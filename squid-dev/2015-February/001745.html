<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using delay pools
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Bug%202907%3A%20high%20CPU%20usage%20on%20CONNECT%20when%0A%20using%20delay%20pools&In-Reply-To=%3C54EB06B2.3080103%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="001732.html">
   <LINK REL="Next"  HREF="001772.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using delay pools</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Bug%202907%3A%20high%20CPU%20usage%20on%20CONNECT%20when%0A%20using%20delay%20pools&In-Reply-To=%3C54EB06B2.3080103%40treenet.co.nz%3E"
       TITLE="[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using delay pools">squid3 at treenet.co.nz
       </A><BR>
    <I>Mon Feb 23 10:53:38 UTC 2015</I>
    <P><UL>
        <LI>Previous message: <A HREF="001732.html">[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using	delay pools
</A></li>
        <LI>Next message: <A HREF="001772.html">[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using delay pools
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1745">[ date ]</a>
              <a href="thread.html#1745">[ thread ]</a>
              <a href="subject.html#1745">[ subject ]</a>
              <a href="author.html#1745">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 21/02/2015 7:20 p.m., Amos Jeffries wrote:
&gt;<i> When delay pools are active on a CONNECT tunnel and the pool is drained
</I>&gt;<i> the I/O loop cycles very often transferring 1 byte until the pool is
</I>&gt;<i> topped-up at the end of the second.
</I>&gt;<i> 
</I>&gt;<i> Instead of looping constantly trying to read 1 byte at a time, add an
</I>&gt;<i> asynchronous event to wait for a few I/O cycles or until more bytes can
</I>&gt;<i> be read.
</I>&gt;<i> 
</I>&gt;<i> To protect against infinite loops of waiting when many tunnels are
</I>&gt;<i> competing for the pool allowance we only delay for a limited number of
</I>&gt;<i> loops before allowing at least 1 byte through. Also, the amount of time
</I>&gt;<i> waited is an odd fraction of 1 second so re-tries naturally spread
</I>&gt;<i> across any given second fairly, with connections rotating closer or
</I>&gt;<i> further from the time when pool topup happens.
</I>
Updated patch attached.

The initial version was using generic_cbdata wrappers needlessly and at
high speeds the event queue could schedule a double-read on an FD.
Using the TunnelStateData directly as the CBDATA parameter and evicting
events from the queue when the state object is destructed prevents the
bad event occurances.

This has now had some (about an hour) production use in a high volume
high performance environment. The initial re-try behaviour still allows
some variance in service times, but overall the CPU consumption and (as
a result) total proxy speed appears to be much improved.

If there are no objections (and no problems in the testing environment)
I will apply this in approx. 3 days.

Amos


-------------- next part --------------
=== modified file 'src/tunnel.cc'
--- src/tunnel.cc	2015-01-16 16:18:05 +0000
+++ src/tunnel.cc	2015-02-23 07:48:39 +0000
@@ -1,34 +1,35 @@
 /*
  * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
  *
  * Squid software is distributed under GPLv2+ license and includes
  * contributions from numerous individuals and organizations.
  * Please see the COPYING and CONTRIBUTORS files for details.
  */
 
 /* DEBUG: section 26    Secure Sockets Layer Proxy */
 
 #include &quot;squid.h&quot;
 #include &quot;acl/FilledChecklist.h&quot;
 #include &quot;base/CbcPointer.h&quot;
 #include &quot;CachePeer.h&quot;
+#include &quot;cbdata.h&quot;
 #include &quot;client_side.h&quot;
 #include &quot;client_side_request.h&quot;
 #include &quot;comm.h&quot;
 #include &quot;comm/Connection.h&quot;
 #include &quot;comm/ConnOpener.h&quot;
 #include &quot;comm/Read.h&quot;
 #include &quot;comm/Write.h&quot;
 #include &quot;errorpage.h&quot;
 #include &quot;fde.h&quot;
 #include &quot;FwdState.h&quot;
 #include &quot;globals.h&quot;
 #include &quot;http.h&quot;
 #include &quot;HttpRequest.h&quot;
 #include &quot;HttpStateFlags.h&quot;
 #include &quot;ip/QosConfig.h&quot;
 #include &quot;LogTags.h&quot;
 #include &quot;MemBuf.h&quot;
 #include &quot;PeerSelectState.h&quot;
 #include &quot;SBuf.h&quot;
 #include &quot;SquidConfig.h&quot;
@@ -98,63 +99,68 @@
     /// Whether we are reading a CONNECT response from a peer.
     bool waitingForConnectResponse() const { return connectRespBuf; }
     /// Whether we are waiting for the CONNECT request/response exchange with the peer.
     bool waitingForConnectExchange() const { return waitingForConnectRequest() || waitingForConnectResponse(); }
 
     /// Whether the client sent a CONNECT request to us.
     bool clientExpectsConnectResponse() const {
 #if USE_OPENSSL
         // We are bumping and we had already send &quot;OK CONNECTED&quot;
         if (http.valid() &amp;&amp; http-&gt;getConn() &amp;&amp; http-&gt;getConn()-&gt;serverBump() &amp;&amp; http-&gt;getConn()-&gt;serverBump()-&gt;step &gt; Ssl::bumpStep1)
             return false;
 #endif
         return !(request != NULL &amp;&amp;
                  (request-&gt;flags.interceptTproxy || request-&gt;flags.intercepted));
     }
 
     class Connection
     {
 
     public:
-        Connection() : len (0), buf ((char *)xmalloc(SQUID_TCP_SO_RCVBUF)), size_ptr(NULL) {}
+        Connection() : len (0), buf ((char *)xmalloc(SQUID_TCP_SO_RCVBUF)), size_ptr(NULL), delayedLoops(0),
+                       readPending(NULL), readPendingFunc(NULL) {}
 
         ~Connection();
 
         int bytesWanted(int lower=0, int upper = INT_MAX) const;
         void bytesIn(int const &amp;);
 #if USE_DELAY_POOLS
 
         void setDelayId(DelayId const &amp;);
 #endif
 
         void error(int const xerrno);
         int debugLevelForError(int const xerrno) const;
         /// handles a non-I/O error associated with this Connection
         void logicError(const char *errMsg);
         void closeIfOpen();
         void dataSent (size_t amount);
         int len;
         char *buf;
         int64_t *size_ptr;      /* pointer to size in an ConnStateData for logging */
 
         Comm::ConnectionPointer conn;    ///&lt; The currently connected connection.
+        uint8_t delayedLoops; ///&lt; how many times a read on this connection has been postponed.
 
+        // XXX: make these an AsyncCall when event API can handle them
+        TunnelStateData *readPending;
+        EVH *readPendingFunc;
     private:
 #if USE_DELAY_POOLS
 
         DelayId delayId;
 #endif
 
     };
 
     Connection client, server;
     int *status_ptr;        ///&lt; pointer for logging HTTP status
     LogTags *logTag_ptr;    ///&lt; pointer for logging Squid processing code
     MemBuf *connectRespBuf; ///&lt; accumulates peer CONNECT response when we need it
     bool connectReqWriting; ///&lt; whether we are writing a CONNECT request to a peer
     SBuf preReadClientData;
 
     void copyRead(Connection &amp;from, IOCB *completion);
 
     /// continue to set up connection to a peer, going async for SSL peers
     void connectToPeer();
 
@@ -193,40 +199,42 @@
     void copy(size_t len, Connection &amp;from, Connection &amp;to, IOCB *);
     void handleConnectResponse(const size_t chunkSize);
     void readServer(char *buf, size_t len, Comm::Flag errcode, int xerrno);
     void readClient(char *buf, size_t len, Comm::Flag errcode, int xerrno);
     void writeClientDone(char *buf, size_t len, Comm::Flag flag, int xerrno);
     void writeServerDone(char *buf, size_t len, Comm::Flag flag, int xerrno);
 
     static void ReadConnectResponseDone(const Comm::ConnectionPointer &amp;, char *buf, size_t len, Comm::Flag errcode, int xerrno, void *data);
     void readConnectResponseDone(char *buf, size_t len, Comm::Flag errcode, int xerrno);
     void copyClientBytes();
 };
 
 static const char *const conn_established = &quot;HTTP/1.1 200 Connection established\r\n\r\n&quot;;
 
 static CNCB tunnelConnectDone;
 static ERCB tunnelErrorComplete;
 static CLCB tunnelServerClosed;
 static CLCB tunnelClientClosed;
 static CTCB tunnelTimeout;
 static PSC tunnelPeerSelectComplete;
+static EVH tunnelDelayedClientRead;
+static EVH tunnelDelayedServerRead;
 static void tunnelConnected(const Comm::ConnectionPointer &amp;server, void *);
 static void tunnelRelayConnectRequest(const Comm::ConnectionPointer &amp;server, void *);
 
 static void
 tunnelServerClosed(const CommCloseCbParams &amp;params)
 {
     TunnelStateData *tunnelState = (TunnelStateData *)params.data;
     debugs(26, 3, HERE &lt;&lt; tunnelState-&gt;server.conn);
     tunnelState-&gt;server.conn = NULL;
 
     if (tunnelState-&gt;request != NULL)
         tunnelState-&gt;request-&gt;hier.stopPeerClock(false);
 
     if (tunnelState-&gt;noConnections()) {
         delete tunnelState;
         return;
     }
 
     if (!tunnelState-&gt;server.len) {
         tunnelState-&gt;client.conn-&gt;close();
@@ -245,53 +253,58 @@
         delete tunnelState;
         return;
     }
 
     if (!tunnelState-&gt;client.len) {
         tunnelState-&gt;server.conn-&gt;close();
         return;
     }
 }
 
 TunnelStateData::TunnelStateData() :
     url(NULL),
     http(),
     request(NULL),
     status_ptr(NULL),
     logTag_ptr(NULL),
     connectRespBuf(NULL),
     connectReqWriting(false)
 {
     debugs(26, 3, &quot;TunnelStateData constructed this=&quot; &lt;&lt; this);
+    client.readPendingFunc = &tunnelDelayedClientRead;
+    server.readPendingFunc = &tunnelDelayedServerRead;
 }
 
 TunnelStateData::~TunnelStateData()
 {
     debugs(26, 3, &quot;TunnelStateData destructed this=&quot; &lt;&lt; this);
     assert(noConnections());
     xfree(url);
     serverDestinations.clear();
     delete connectRespBuf;
 }
 
 TunnelStateData::Connection::~Connection()
 {
+    if (readPending)
+        eventDelete(readPendingFunc, readPending);
+
     safe_free(buf);
 }
 
 int
 TunnelStateData::Connection::bytesWanted(int lowerbound, int upperbound) const
 {
 #if USE_DELAY_POOLS
     return delayId.bytesWanted(lowerbound, upperbound);
 #else
 
     return upperbound;
 #endif
 }
 
 void
 TunnelStateData::Connection::bytesIn(int const &amp;count)
 {
     debugs(26, 3, HERE &lt;&lt; &quot;len=&quot; &lt;&lt; len &lt;&lt; &quot; + count=&quot; &lt;&lt; count);
 #if USE_DELAY_POOLS
     delayId.bytesIn(count);
@@ -314,40 +327,41 @@
         return 3;
 
     return 1;
 }
 
 /* Read from server side and queue it for writing to the client */
 void
 TunnelStateData::ReadServer(const Comm::ConnectionPointer &amp;c, char *buf, size_t len, Comm::Flag errcode, int xerrno, void *data)
 {
     TunnelStateData *tunnelState = (TunnelStateData *)data;
     assert(cbdataReferenceValid(tunnelState));
     debugs(26, 3, HERE &lt;&lt; c);
 
     tunnelState-&gt;readServer(buf, len, errcode, xerrno);
 }
 
 void
 TunnelStateData::readServer(char *, size_t len, Comm::Flag errcode, int xerrno)
 {
     debugs(26, 3, HERE &lt;&lt; server.conn &lt;&lt; &quot;, read &quot; &lt;&lt; len &lt;&lt; &quot; bytes, err=&quot; &lt;&lt; errcode);
+    server.delayedLoops=0;
 
     /*
      * Bail out early on Comm::ERR_CLOSING
      * - close handlers will tidy up for us
      */
 
     if (errcode == Comm::ERR_CLOSING)
         return;
 
     if (len &gt; 0) {
         server.bytesIn(len);
         kb_incr(&amp;(statCounter.server.all.kbytes_in), len);
         kb_incr(&amp;(statCounter.server.other.kbytes_in), len);
     }
 
     if (keepGoingAfterRead(len, errcode, xerrno, server, client))
         copy(len, server, client, WriteClientDone);
 }
 
 /// Called when we read [a part of] CONNECT response from the peer
@@ -459,40 +473,41 @@
     debugs(50, debugLevelForError(xerrno), HERE &lt;&lt; conn &lt;&lt; &quot;: read/write failure: &quot; &lt;&lt; xstrerror());
 
     if (!ignoreErrno(xerrno))
         conn-&gt;close();
 }
 
 /* Read from client side and queue it for writing to the server */
 void
 TunnelStateData::ReadClient(const Comm::ConnectionPointer &amp;, char *buf, size_t len, Comm::Flag errcode, int xerrno, void *data)
 {
     TunnelStateData *tunnelState = (TunnelStateData *)data;
     assert (cbdataReferenceValid (tunnelState));
 
     tunnelState-&gt;readClient(buf, len, errcode, xerrno);
 }
 
 void
 TunnelStateData::readClient(char *, size_t len, Comm::Flag errcode, int xerrno)
 {
     debugs(26, 3, HERE &lt;&lt; client.conn &lt;&lt; &quot;, read &quot; &lt;&lt; len &lt;&lt; &quot; bytes, err=&quot; &lt;&lt; errcode);
+    client.delayedLoops=0;
 
     /*
      * Bail out early on Comm::ERR_CLOSING
      * - close handlers will tidy up for us
      */
 
     if (errcode == Comm::ERR_CLOSING)
         return;
 
     if (len &gt; 0) {
         client.bytesIn(len);
         kb_incr(&amp;(statCounter.client_http.kbytes_in), len);
     }
 
     if (keepGoingAfterRead(len, errcode, xerrno, client, server))
         copy(len, client, server, WriteServerDone);
 }
 
 /// Updates state after reading from client or server.
 /// Returns whether the caller should use the data just read.
@@ -659,47 +674,85 @@
 
 static void
 tunnelTimeout(const CommTimeoutCbParams &amp;io)
 {
     TunnelStateData *tunnelState = static_cast&lt;TunnelStateData *&gt;(io.data);
     debugs(26, 3, HERE &lt;&lt; io.conn);
     /* Temporary lock to protect our own feets (comm_close -&gt; tunnelClientClosed -&gt; Free) */
     CbcPointer&lt;TunnelStateData&gt; safetyLock(tunnelState);
 
     tunnelState-&gt;client.closeIfOpen();
     tunnelState-&gt;server.closeIfOpen();
 }
 
 void
 TunnelStateData::Connection::closeIfOpen()
 {
     if (Comm::IsConnOpen(conn))
         conn-&gt;close();
 }
 
+static void
+tunnelDelayedClientRead(void *data)
+{
+    if (!data)
+        return;
+    TunnelStateData *tunnel = static_cast&lt;TunnelStateData*&gt;(data);
+    if (!tunnel)
+        return;
+    tunnel-&gt;client.readPending = NULL;
+    static uint64_t counter=0;
+    debugs(26, 7, &quot;Client read(2) delayed &quot; &lt;&lt; ++counter &lt;&lt; &quot; times&quot;);
+    tunnel-&gt;copyRead(tunnel-&gt;client, TunnelStateData::ReadClient);
+}
+
+static void
+tunnelDelayedServerRead(void *data)
+{
+    if (!data)
+        return;
+    TunnelStateData *tunnel = static_cast&lt;TunnelStateData*&gt;(data);
+    if (!tunnel)
+        return;
+    tunnel-&gt;server.readPending = NULL;
+    static uint64_t counter=0;
+    debugs(26, 7, &quot;Server read(2) delayed &quot; &lt;&lt; ++counter &lt;&lt; &quot; times&quot;);
+    tunnel-&gt;copyRead(tunnel-&gt;server, TunnelStateData::ReadServer);
+}
+
 void
 TunnelStateData::copyRead(Connection &amp;from, IOCB *completion)
 {
     assert(from.len == 0);
+    // If only the minimum permitted read size is going to be attempted
+    // then we schedule an event to try again in a few I/O cycles.
+    // Allow at least 1 byte to be read every (0.3*10) seconds.
+    int bw = from.bytesWanted(1, SQUID_TCP_SO_RCVBUF);
+    if (bw == 1 &amp;&amp; ++from.delayedLoops &lt; 10) {
+        from.readPending = this;
+        eventAdd(&quot;tunnelDelayedServerRead&quot;, from.readPendingFunc, from.readPending, 0.3, true);
+        return;
+    }
+
     AsyncCall::Pointer call = commCbCall(5,4, &quot;TunnelBlindCopyReadHandler&quot;,
                                          CommIoCbPtrFun(completion, this));
-    comm_read(from.conn, from.buf, from.bytesWanted(1, SQUID_TCP_SO_RCVBUF), call);
+    comm_read(from.conn, from.buf, bw, call);
 }
 
 void
 TunnelStateData::readConnectResponse()
 {
     assert(waitingForConnectResponse());
 
     AsyncCall::Pointer call = commCbCall(5,4, &quot;readConnectResponseDone&quot;,
                                          CommIoCbPtrFun(ReadConnectResponseDone, this));
     comm_read(server.conn, connectRespBuf-&gt;space(),
               server.bytesWanted(1, connectRespBuf-&gt;spaceSize()), call);
 }
 
 void
 TunnelStateData::copyClientBytes()
 {
     if (preReadClientData.length()) {
         size_t copyBytes = preReadClientData.length() &gt; SQUID_TCP_SO_RCVBUF ? SQUID_TCP_SO_RCVBUF : preReadClientData.length();
         memcpy(client.buf, preReadClientData.rawContent(), copyBytes);
         preReadClientData.consume(copyBytes);

</PRE>















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001732.html">[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using	delay pools
</A></li>
	<LI>Next message: <A HREF="001772.html">[squid-dev] [PATCH] Bug 2907: high CPU usage on CONNECT when using delay pools
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1745">[ date ]</a>
              <a href="thread.html#1745">[ thread ]</a>
              <a href="subject.html#1745">[ subject ]</a>
              <a href="author.html#1745">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.squid-cache.org/listinfo/squid-dev">More information about the squid-dev
mailing list</a><br>
</body></html>
