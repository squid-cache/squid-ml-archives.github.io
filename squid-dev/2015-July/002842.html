<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-dev] [PATCH] Simpler and more robust request line parser
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Simpler%20and%20more%20robust%20request%20line%20parser&In-Reply-To=%3C55B17533.4020808%40measurement-factory.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="002834.html">
   <LINK REL="Next"  HREF="002851.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-dev] [PATCH] Simpler and more robust request line parser</H1>
    <B>Alex Rousskov</B> 
    <A HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Simpler%20and%20more%20robust%20request%20line%20parser&In-Reply-To=%3C55B17533.4020808%40measurement-factory.com%3E"
       TITLE="[squid-dev] [PATCH] Simpler and more robust request line parser">rousskov at measurement-factory.com
       </A><BR>
    <I>Thu Jul 23 23:13:55 UTC 2015</I>
    <P><UL>
        <LI>Previous message: <A HREF="002834.html">[squid-dev] [PATCH] Simpler and more robust request line parser
</A></li>
        <LI>Next message: <A HREF="002851.html">[squid-dev] [PATCH] chop() should always clear empty buffers
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2842">[ date ]</a>
              <a href="thread.html#2842">[ thread ]</a>
              <a href="subject.html#2842">[ subject ]</a>
              <a href="author.html#2842">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 07/23/2015 08:16 AM, Amos Jeffries wrote:
&gt;<i> Please note: most of my comments in this long email are informational.
</I>&gt;<i> Things which need to be taken into consideration when touching the parser.
</I>
Noted. I will do my best to treat them as such.


&gt;<i> I request performance testing of the final version of this change vs
</I>&gt;<i> lastest trunk at the time.
</I>
Noted.

The next patch will address comments not discussed in this email. Thank
you for those!

The rest of this email is the usual waste of time discussing
non-existent bugs, arguing about mostly irrelevant issues, burning straw
men, and talking past each other. The only important bit of information
is probably the following paragraph that I will quote here for those who
do not want to waste their time reading everything:

&quot;I am happy to restrict URIs to what older Squids accepted or more.
Recent parser changes broke Squid. There is no need to go beyond what we
were doing before those changes. I will gladly make a simple patch
change to restrict URIs to either that old subset or to the current
trunk subset, whichever is larger. Exactly what subset do you want me to
use?&quot;



&gt;<i> If possible an emulated Slow-Loris DoS attack of up to 1MB of random
</I>&gt;<i> printable ASCII and whitespace characters drip-fed in 1-5 bytes each 15
</I>&gt;<i> sec (followed by 1 LF) would be nice too. Avg socket close rate from
</I>&gt;<i> Squid end is what matters. Though thats a wishlist since I've not done
</I>&gt;<i> such on Squid since ~2010.
</I>
In such a test, Squid connection closing time is determined by the
minimum of

  * mean time of the first LF appearance on the connection and
  * configured request waiting timeout.

Both parameters are outside Squid control. By varying the number of
concurrent connections, I can demonstrate virtually any &quot;socket close
rate from Squid end&quot;. What would such a test tell us?

Needless to say, an implementation that validates partially received
request lines will close random garbage connections earlier, on average.
There is no need to test that.


&gt;<i> On 23/07/2015 4:05 p.m., Alex Rousskov wrote:
</I>&gt;&gt;<i> Hello,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     The attached patch implements a simpler and more robust HTTP request
</I>&gt;<i> 
</I>&gt;<i> It is less robust as many potential attacks are now accepted by the
</I>&gt;<i> Parser as valid HTTP messages. 
</I>
The parser is more robust because it correctly accepts a larger variety
of benign messages (see Robustness Principle). That necessarily includes
a larger variety of &quot;potential attack messages&quot;, of course, because
&quot;potential attack&quot; is such an open-ended term.

FWIW, it would be perfectly reasonable to enhance relaxed_header_parser
(or add a new relaxed_uri_parser directive) to make URI parsing
restrictions more flexible than virtually-all-or-virtually-nothing
approach used today. I would not be surprised if we do see use cases
encouraging such changes. The patched code makes them easier.


&gt;<i> Squid may &quot;work&quot; as a consequence only because this code was the primary
</I>&gt;<i> security protection layer. Internal parsers later on (if any) are
</I>&gt;<i> sometimes less robust and picky.
</I>
I agree that inner layers may receive URIs they have never seen before
and that those URIs may crash those layers. However, they did receive
quite a variety before recent parser restrictions were added, and we
cannot fix bugs [in those other layers] that we do not know about.


&gt;<i> If this Parser is made extremely generic the latter parsing logic is
</I>&gt;<i> going to have to change to find these same problems, which will only
</I>&gt;<i> make Squid reject the same non-compliant traffic - after much wasted
</I>&gt;<i> processing of the message.
</I>
This is too general/vague to argue about: Sure, if some inner code
always rejects URI X, it might make sense to reject X in the parser, but
doing so is not _always_ better from both code quality and performance
points of view. Divide and conquer goes a long way in both directions.


&gt;<i> In these threads we have had around parsing I am getting the impression
</I>&gt;<i> you think of the HTTPbis WG documents as restrictive arbitrary committee
</I>&gt;<i> designs.
</I>
If we are talking about RFCs defining HTTP/1, then I think of them as
protocol specifications. Characterizing a protocol specification written
by a committee as &quot;restrictive arbitrary committee design&quot; does not
really help us do something useful AFAICT.

As an intermediary operating in an imperfect world, Squid has to achieve
the right balance of what to accept and what to send. IETF community has
struggled with finding that balance for a very long time, and I do not
think that struggle is over. Just because there is a real need to accept
some protocol-violating messages does not mean that HTTP RFCs should be
ignored or belittled, of course.


&gt;<i> In the contrary they are explicity tested documents based on what the
</I>&gt;<i> real-world traffic has been actively observed doing over the past 10
</I>&gt;<i> years.
</I>
Very true! However, there are things that a protocol specification does
not and cannot cover. Looking at the specs alone is a doomed approach,
as illustrated by the restricted parser disaster.

I have no problems with Squid identifying protocol-violating messages if
there is some practical need in that. However, my focus is on what to do
with the received message. Blocking *all* protocol-violating messages is
not practical in most environments. Thus, we must consider things that
are not documented in the RFC into the equation and occasionally even
violate the RFC.


&gt;<i>  Where the real-world traffic was tracked down to malicious behaviour or
</I>&gt;<i> causing indirect vulnerability exposure it was explicitly forbidden.
</I>&gt;<i> Intro things like removal of URI user-info segment, explicit
</I>&gt;<i> single-digit HTTP version, forbidding of whitespace after &quot;HTTP/1.1&quot;
</I>&gt;<i> token, single SP delimiter in 'strict' parse, etc.
</I>
And I think Squid follows (or should follow) those protocol changes [at
least up to a point they start causing interoperability problems].


&gt;&gt;<i> line parser. After getting a series of complaints and seeing admins
</I>&gt;&gt;<i> adding more and more exceptional URL characters to the existing trunk
</I>
&gt;<i> I hope you can see the real problem is the admins approach to that
</I>&gt;<i> traffic.
</I>
We disagree on who to blame as the cause of the problem and who should
be responsible for fixing it. I doubt you have any new arguments that
can convince me that Squid should block benign traffic just because it
violates an RFC.


&gt;<i> AFAIK, the only non-permitted URI characters in current trunk are the
</I>&gt;<i> *binary* CTL characters. 
</I>
Sounds good. Could you define &quot;binary CTL characters&quot; so that I can use
the same set in the patched code, please?


&gt;<i> Squid does not even check for correct
</I>&gt;<i> positioning of delimiters in tollerant parse at this stage of the
</I>&gt;<i> processing.
</I>
Bugs notwithstanding, the new parser does require delimiters between
method, URI, and HTTP version.


&gt;<i> Which makes me wonder exactly what these &quot;more and more&quot; extra
</I>&gt;<i> characters the admin are adding are? I hope you are referring to the
</I>&gt;<i> r14157 set, as opposed to people blindly trying to let those
</I>&gt;<i> request-line injection attacks Section 9.2 talks about work through
</I>&gt;<i> their proxies.
</I>
Section 9.2 does not talk about injection attacks; its scope is quite
different from the subject of this thread. You may be thinking of
Sections 9.4 and 9.5. I do not think the relaxed parser makes us more
susceptible to CRLF injection attacks (9.4). As for request smuggling
(9.5), I think that was possible before and is possible after the
changes because it requires two agents to interpret the same request
differently, something that will always be possible as long as there are
malformed requests and buggy agents.

Said that, if there is a known attack vector that we can block without
making things overall worse, then we should block it. Most likely, that
work should be done outside the parser though.


&gt;&gt;<i> parser, I took the plunge and rewrote the existing code using the
</I>&gt;&gt;<i> approach recently discussed on this list.
</I>&gt;<i> 
</I>&gt;<i> This Http1::RequestParser has one purpose, and one only. To identify a
</I>&gt;<i> standards compliant (RFC 1945 or 7230) HTTP/1 request-line.
</I>&gt;<i> 
</I>&gt;<i> All other inputs are undefined behaviour intended to be handled by other
</I>&gt;<i> Parsers. Some in future, some already existing (ie FTP).
</I>
I disagree that Http1::RequestParser purpose is to identify standards
compliant HTTP/1 request-lines. Moreover, I doubt that a parser with
that purpose is really needed in Squid (beyond academic exercises and
experiments).


&gt;&gt;<i> The primary changes are: Removed incremental parsing and revised parsing
</I>&gt;&gt;<i> sequence to accept virtually any URI (by default and also configurable
</I>&gt;&gt;<i> as before).
</I>
&gt;<i> Firstly, I note that you still have incremental parsing in the code
</I>&gt;<i> itself. Just not for fields internal to the request-line.
</I>
The patch is about request-line parser so the description seems accurate
to me. *That* parser is no longer incremental -- it does not preserve
any state while returning the &quot;need more data&quot; result. Are you talking
about some other parsers that this patch does not touch?


&gt;<i> The result of this change is that Squid will become vulnerable to even
</I>&gt;<i> the most trivial of &quot;Slow Loris&quot; attacks.
</I>
No, not the most trivial (random garbage). As for the sophisticated
&quot;Slow Loris&quot; attacks, they are available for any parser that accepts
non-trivial inputs. The defenses against them do not belong to the
request line parser.


&gt;<i> Of course Squid
</I>&gt;<i> without input validation will work faster on most/compliant traffic than
</I>&gt;<i> one that spends time validating inputs.
</I>
With the exception of HTTP method _length_ limit (which will be
restored), the proposed code does not remove validation. For the URI,
the validation changes what characters are acceptable. We can easily
adjust that set as discussed below.


&gt;<i> Squid which re-parse the message from scratch on every byte received are
</I>&gt;<i> vulnerable to Slow Loris even in common compliant-HTTP message parsing.
</I>
So is virtually any parser, including your incremental parser. Virtually
all parsers have to re-parse elements from scratch. It is possible to
avoid that re-parsing completely, but the complexity is usually not
worth the gains. If you want to defend against real Slow Loris attacks,
the defense has to lie elsewhere.


&gt;<i> .. and much more vulnerable Squid. Whole categories of attack against
</I>&gt;<i> both Squid and its backends are now tolerated when they were rejected or
</I>&gt;<i> greatly restricted earlier.
</I>

I am happy to restrict URIs to what older Squids accepted or more.
Recent parser changes broke Squid. There is no need to go beyond what we
were doing before those changes. I will gladly make a simple patch
change to restrict URIs to either that old subset or to the current
trunk subset, whichever is larger. Exactly what subset do you want me to
use?

To minimize iterations, please use character ranges or similar precise
definitions instead of &quot;binary characters&quot; and such. Thank you.


&gt;&gt;<i> * Some unit test case adjustments.
</I>&gt;&gt;<i> * The new parser no longer treats some request lines ending with
</I>&gt;&gt;<i>   &quot;HTTP/1.1&quot; as HTTP/0.9 requests for URIs that end with &quot;HTTP/1.1&quot;.
</I>&gt;<i> 
</I>&gt;<i> Following 'random garbage' with &quot;HTTP/1.1&quot; does not make it HTTP compliant.
</I>
Agreed.


&gt;<i> This adds direct and explicit violations of RFC 1945, 3896 and 7230.
</I>
As you know, some of those violations existed before. I have no desire
to add any new violations compared to the old code that worked OK.


&gt;&gt;<i> Also removed hard-coded 16-character method length limit because I did
</I>&gt;&gt;<i> not know why that limit was needed _and_ there was a TODO to make it
</I>&gt;&gt;<i> configurable. Please let me know if there was a good reason for that
</I>&gt;&gt;<i> limit, and I will restore it.
</I>
&gt;<i> It is there to restrict damage from active malware in HTTP/1. Namely
</I>&gt;<i> Slow Loris, random method cache busting and use of method names as an
</I>&gt;<i> open botnet C&amp;C channel.
</I>
I do not know what the last one means, but the other two you listed do
not seem to be relevant in this context.

I will add a simple 32-character (per your other comment) method
acceptance limit to avoid prolonging this discussion. If you know the
method name that already uses 17 characters, please post so that I can
mention it in a code comment.


&gt;<i> Non-incremental will also re-introduce admin complaints about high CPU
</I>&gt;<i> consumption on traffic where they receive long URLs over a number of
</I>&gt;<i> very small packets. ie the traffic which is normal, valid but looks like
</I>&gt;<i> Slow Loris (or is actually).
</I>
AFAICT, the incremental parser in trunk does not parse URI fields
incrementally so the proposed parser is not making the above situation
worse. A quote from the trunk code:

  tok.reset(buf_);
  if (tok.prefix(line, LfDelim) &amp;&amp; tok.skip('\n')) { ... }
  if (!tok.atEnd()) { ... }
  debugs(74, 5, &quot;Parser needs more data&quot;);
  return 0;

The &quot;...&quot; code does not run while we accumulate the input buffer to find
the end of the URI. Thus, we exit with what we came with until the whole
URI is available. No incremental URI parsing.


&gt;<i> Note that real-world URLs have a *minimum* accepted limit of 4KB now,
</I>&gt;<i> and sightings have been measured with URL as high as 200KB in length
</I>&gt;<i> from some domains. Most origin servers wont accept &gt;32KB though.
</I>
Yes, we must handle long URIs. I already have non-triaged bug reports
that trunk no longer accepts them. That is a separate issue though.



&gt;<i> Consider Squid parsing the pipeline of requests which are delivered in a
</I>&gt;<i> single packet, and thus all exists in buffer simultaneously:
</I>&gt;<i> &quot;
</I>&gt;<i> GET <A HREF="http://example.com/">http://example.com/</A> HTTP/1.1\n
</I>&gt;<i> \n
</I>&gt;<i> GET <A HREF="http://attacker.invalid/">http://attacker.invalid/</A> HTTP/1.1\n
</I>&gt;<i> X-Custom: HTTP/1.1\n
</I>&gt;<i> &quot;
</I>&gt;<i> 
</I>&gt;<i> All Squid default installations use relaxed parse with violations
</I>&gt;<i> enabled. The new tok.prefix(uriFound, UriCharacters()) will treat this
</I>&gt;<i> all as a single request-line:
</I>&gt;<i>   &quot;GET <A HREF="http://example.com/...attacker.invalid/">http://example.com/...attacker.invalid/</A> HTTP/1.1&quot; label.
</I>

No, the code does not work like that. The URI is not parsed until Squid
isolates: the first request line (required), the request method inside
that request line (required), the protocol version inside that request
line (absent in case of HTTP/0), and related delimiters.

In your particular example, the URI parsing code you are worried about
will be called with &quot;<A HREF="http://example.com/">http://example.com/</A>&quot; characters.


&gt;&gt;<i> * Accept virtually any URI (when allowed).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> 1. USE_HTTP_VIOLATIONS now allow any URI characters except spaces.
</I>&gt;<i> 
</I>&gt;<i> Except? Ironic.
</I>
No, just controlled by a different setting -- relaxed_header_parser. The
code is trying to preserve the existing split between
USE_HTTP_VIOLATIONS and relaxed_header_parser scopes, nothing more. I
think you interpret that [undocumented?] split differently, and I will
try to adjust the patch to match _your_ interpretation.


&gt;<i> In any case we can't accept bare LF within URL for practical issues
</I>&gt;<i> around finding the end of line.
</I>
True. The LF character will never get to the URI parser in the proposed
implementation. While the URI parsing code accepts LF, nothing actually
feeds LF to that code. That will be further restricted in the next patch
version as we tighten the relaxed character set as discussed elsewhere.


&gt;&gt;<i> 2. relaxed_header_parser allows spaces in URIs.
</I>
&gt;<i> By &quot;spaces&quot; you mean the extended RFC 7230 compliant whitespace. Good.
</I>

&gt;&gt;<i> URIs in what Squid considers an HTTP/0.9 request obey the same rules.
</I>&gt;&gt;<i> Whether the rules should differ for HTTP/0 is debatable, but the current
</I>&gt;&gt;<i> implementation is the simplest possible one, and the code makes it easy
</I>&gt;&gt;<i> to add complex rules.
</I>
&gt;<i> Which means we can no longer claim RFC 1945 compliance. 
</I>
We can unless USE_HTTP_VIOLATIONS or Config.onoff.relaxed_header_parser
is enabled AFAICT. If somebody really wants RFC 1945 compliance, they
should just configure Squid accordingly. Not that there is any practical
value in doing that, but the patch attempts to preserve that possibility
AFAICT.

This disagreement may be related to how we interpret the split between
USE_HTTP_VIOLATIONS and Config.onoff.relaxed_header_parser
responsibilities. Again, I will do my best to adjust the code to match
your interpretation, which might resolve this issue as well.


&gt;<i> The old RFC is
</I>&gt;<i> remarkably restrictive. MUST level requirements on individual characters
</I>&gt;<i> in the URL (via RFC 1738), their positioning, and method / prefix &quot;GET &quot;
</I>&gt;<i> (with exactly one explicitly SP character delimiter required).
</I>
Squid might accept some invalid HTTP/0 requests even when configured to
be strict, but I do not know of any specific cases (other than those
that are HTTP/1 requests). Since the whole strict HTTP/0 compliance
concern is irrelevant for real HTTP traffic (including real HTTP/0
traffic!), I really doubt we should waste time digging any deeper here.


&gt;<i> NP: RFC 7230 formally dropped the requirement to also support RFC 1945
</I>&gt;<i> HTTP/0.9 simple-request syntax. It has not been seen in any of the
</I>&gt;<i> traffic measured by HTTPbis WG in the past 5 years. So is now optional
</I>&gt;<i> for on-wire traffic parsing.
</I>&gt;<i> 
</I>&gt;<i> I left RFC 1945 compliance to reduce the unnecessary restrictions on
</I>&gt;<i> what Squid accepts. Removing it alone will allow us to dramatically
</I>&gt;<i> simplify the logics in the existing parser (~15% ?) without any of the
</I>&gt;<i> other changes you are trying to get through in this patch.
</I>
Removing HTTP/0 support can simplify any parser. It would be especially
easy in the proposed parser, but let's not discuss that out-of-scope
change here.


&gt;<i> The strict parse not having to bother with 2-pass reverse parsing offers
</I>&gt;<i> faster REST transaction latency.
</I>&gt;<i> 
</I>&gt;<i> As I mentioned when the topic of this re-write came up earlier, I
</I>&gt;<i> request performance testing for this change. We lost a lot of
</I>&gt;<i> performance optimizations in the last re-write and any further polishing
</I>&gt;<i> needs to counter that one way or another.
</I>
Yes, of course. I realize that you will not let my rewrites to cause
known performance regressions. The patched code performance will be tested.


&gt;&gt;<i> * Unit test cases adjustments.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Removal of incremental request line parsing means that we should not
</I>&gt;&gt;<i> check parsed fields when parsing fails or has not completed yet.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Some test cases made arguably weird decisions apparently to accommodate
</I>&gt;&gt;<i> the old parser. The expectations of those test cases are [more] natural now.
</I>&gt;<i> 
</I>&gt;<i> Which ones?
</I>
The examples I remember are the following two requests that test cases
said should be treated as valid HTTP/0 requests [with &quot;HTTP/1.1&quot; as a URI!]:

* GET HTTP/1.1
* GET  HTTP/1.1

They are now treated as invalid requests.


&gt;<i> * I believe we can retain incremental parsing despite your changes here.
</I>&gt;<i> As noted above its use was a fairly major protection against some very
</I>&gt;<i> trivial attack vectors.
</I>&gt;<i>  - I agree the current incremental implementation is not optimal, but
</I>&gt;<i> rather fix than removing completely IMO.
</I>
The old incremental parsing was not offering us anything worth
preserving it for. I agree that it is possible to add *useful*
incremental parsing. That work should be driven by specific scenarios we
want to address though.

I will add &quot;early&quot; method validation to address your other, valid
concerns. This validation is not tied to incremental parsing though.


&gt;<i> * RelaxedCharsPlusWhite includes LF character.
</I>&gt;<i>  - this will result in some traffic parsing the entire Mime header
</I>&gt;<i> portion of messages as part of the URL. see above for details.
</I>
No, it will not, as discussed above.


&gt;<i> in Http::One::RequestParser::skipDelimiter():
</I>&gt;<i> 
</I>&gt;<i> * please note how HTTP Parser use HttpTokenizer for specific skip*()
</I>&gt;<i> things like what this function does.
</I>
So does the patched code: skipDelimiter() is called with specific
skip*() results. This helps avoid code complications related to checking
how many delimiters are allowed depending on the relaxed_header_parser
setting...


&gt;<i> * the &quot;in method&quot; -&gt; &quot;after method&quot; change is incorrect IMO:
</I>&gt;<i>  - the HTTP syntax has SP-delimiter as the end of method.
</I>
HTTP defines method syntax as

     method         = token

Everything else, including what may or may not come AFTER the method
does not define the method.

Note that this is just a wording correction to minimize confusion if the
test case fails (after detecting a perfectly fine method token).

These test cases have not changed except I had to replace LF (0x0A) with
0x16 so that the the test case tests what it claims to test and not a
truncated request.

We already have another test case that tests request truncation, but I
can also restore the 0x0A case in that truncation group if you think
that truncation test is useful.


&gt;<i> * now that you have made both relaxed and strict parsers produce
</I>&gt;<i> identical output for the &quot;GET  HTTP/1.1\r\n&quot; test case.
</I>&gt;<i>  - please add a second test for the remaining case of &quot;GET
</I>&gt;<i> HTTP/9.9\r\n&quot; 
</I>
OK.

&gt;<i>  - note the parser violates RFC 1945 by accepting two sequential SP
</I>&gt;<i> characters as method delimiter. 
</I>
Only when allowed to do so. In strict mode, only one SP is allowed AFAICT.


&gt;<i> I'm happy to see this new code passes all the units with so few changes
</I>&gt;<i> necessary to them. Though that does suggest we need a looped test to
</I>&gt;<i> validate all CTL characters individually instead of just a single
</I>&gt;<i> &quot;GET\x16&quot; method test.
</I>
Yes, the existing test cases are not concerned with URIs that were
broken in trunk (naturally!). The loop to test prohibited characters can
be added later, after we settle on the final relaxed character set.


Again, the specific change requests not discussed above will be
addressed with the next patch version.


Thank you,

Alex.

</PRE>










<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002834.html">[squid-dev] [PATCH] Simpler and more robust request line parser
</A></li>
	<LI>Next message: <A HREF="002851.html">[squid-dev] [PATCH] chop() should always clear empty buffers
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2842">[ date ]</a>
              <a href="thread.html#2842">[ thread ]</a>
              <a href="subject.html#2842">[ subject ]</a>
              <a href="author.html#2842">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.squid-cache.org/listinfo/squid-dev">More information about the squid-dev
mailing list</a><br>
</body></html>
