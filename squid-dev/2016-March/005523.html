<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-dev] [PATCH] Increase request buffer size to 64kb
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Increase%20request%20buffer%20size%20to%2064kb&In-Reply-To=%3C56FBAA71.8050406%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="005522.html">
   <LINK REL="Next"  HREF="005527.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-dev] [PATCH] Increase request buffer size to 64kb</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Increase%20request%20buffer%20size%20to%2064kb&In-Reply-To=%3C56FBAA71.8050406%40treenet.co.nz%3E"
       TITLE="[squid-dev] [PATCH] Increase request buffer size to 64kb">squid3 at treenet.co.nz
       </A><BR>
    <I>Wed Mar 30 10:29:05 UTC 2016</I>
    <P><UL>
        <LI>Previous message: <A HREF="005522.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
        <LI>Next message: <A HREF="005527.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5523">[ date ]</a>
              <a href="thread.html#5523">[ thread ]</a>
              <a href="subject.html#5523">[ subject ]</a>
              <a href="author.html#5523">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 30/03/2016 6:53 p.m., Alex Rousskov wrote:
&gt;<i> On 03/29/2016 10:39 PM, Nathan Hoad wrote:
</I>&gt;&gt;<i> On 30 March 2016 at 12:11, Alex Rousskov wrote:
</I>&gt;&gt;&gt;<i> On 03/29/2016 06:06 PM, Nathan Hoad wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> This (very small) patch increases the request buffer size to 64kb, from 4kb.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> -#define HTTP_REQBUF_SZ  4096
</I>&gt;&gt;&gt;&gt;<i> +#define HTTP_REQBUF_SZ  65535
</I>&gt;&gt;&gt;<i>
</I>
One thing you need to keep in mind with all this is that the above
macros *does not* configure the network I/O buffers.


The network HTTP request buffer is controlled by request_header_max_size
- default 64KB.

The network HTTP reply buffer is controlled by reply_header_max_size -
default 64KB.

The HTTP_REQBUF_SZ macro configures the StoreIOBuffer object size. Which
is mostly used for StoreIOBuffer (client-streams or disk I/O) or local
stack allocated variables. Which is tuned to match the filesystem page
size - default 4KB.

If your system uses non-4KB pages for disk I/O then you should tune that
alignment of course. If you are memory-only caching or even not caching
that object at all - then the memory page size will be the more
important metric to tune it against.

How important I'm not sure. I had thought the relative difference in
memory and network I/O speeds made the smaller size irrelevant (since we
are data-copying from the main network SBuf buffers anyway). But
perhapse not. You may have just found that it needs to be tuned to match
the network I/O buffer default max-size (64KB).

NP: perhapse the real difference is how fast Squid can walk the list of
in-memory buffers that span the object in memory cache. Since it walks
the linked-list from head to position N with each write(2) having larger
steps would be relevant.



&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> In my testing, this increases throughput rather dramatically for
</I>&gt;&gt;&gt;&gt;<i> downloading large files:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Based on your results alone, it is clear that the patch does more than
</I>&gt;&gt;&gt;<i> &quot;increases the request buffer size&quot;(*). IMO, the important initial
</I>&gt;&gt;&gt;<i> questions we need to answer are:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>   1. Why the performance is improved in this micro test?
</I>&gt;&gt;&gt;<i>   2. What are the expected effects on other traffic?
</I>&gt;<i> 
</I>&gt;&gt;<i> I think these are reasonable questions to ask. Answering question one
</I>&gt;&gt;<i> will take time
</I>&gt;<i> 
</I>&gt;<i> Yes, finding the correct answer may not be trivial, although I would
</I>&gt;<i> expect that larger system reads result in fewer reads and, hence, less
</I>&gt;<i> Squid-imposed overhead in a micro test. That hypothesis should be
</I>&gt;<i> relatively easy to semi-confirm by looking at the number of system reads
</I>&gt;<i> before/after the change (if that statistics is already reported).
</I>&gt;<i> Hacking Squid to collect those stats should not be difficult either (if
</I>&gt;<i> that statistics is not already reported).
</I>&gt;<i> 
</I>&gt;<i> Another related angle you may want to consider is comparing 8KB, 16KB,
</I>&gt;<i> and 32KB performance with the already available 4KB and 64KB numbers:
</I>&gt;<i> 
</I>&gt;<i> *  If performance keeps going up with a larger buffer (in a micro test),
</I>&gt;<i> how about 512KB? You will need a large enough file and a fast
</I>&gt;<i> client/server to make sure Squid is the bottleneck, of course.
</I>&gt;<i> 
</I>
Make sure you have plenty of per-process stack space available before
going large. Squid allocates several buffers using this size directly on
the stack. Usually at least 2, maybe a half dozen.


&gt;<i> * If there is a &quot;sweet spot&quot; that is determined by system page size or
</I>&gt;<i> perhaps NIC buffer size, then one can argue that this parameter should
</I>&gt;<i> be configurable to match that size (and perhaps even set based on system
</I>&gt;<i> parameters by default if possible).
</I>&gt;<i> 
</I>
It would be page size (memory pages or disk controller I/O pages). Since
the network is tuned already and defaulting to 64KB.


&gt;<i> 
</I>&gt;&gt;<i> Yes I think you are right in that the constant may be
</I>&gt;&gt;<i> misnamed/misused. Once I've gone through all the places that use it, I
</I>&gt;&gt;<i> suppose reevaluating its name would be a good idea.
</I>&gt;<i> 
</I>&gt;<i> ... and we would have to rename it anyway if we decide to make it
</I>&gt;<i> configurable. Knowing where it is used (and/or isolating its usage)
</I>&gt;<i> would be important for that as well.
</I>
It is used primarily for the disk I/O and Squid internal client-streams
buffers.

In the long-term plan those internal uses will be replaced by SBuf which
are controlled by the existing squid.conf options and actual message
sizes more dynamically.

A new option for tuning disk I/O buffer size might be useful in both
long- and short- terms though.

Amos

</PRE>







<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="005522.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
	<LI>Next message: <A HREF="005527.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5523">[ date ]</a>
              <a href="thread.html#5523">[ thread ]</a>
              <a href="subject.html#5523">[ subject ]</a>
              <a href="author.html#5523">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.squid-cache.org/listinfo/squid-dev">More information about the squid-dev
mailing list</a><br>
</body></html>
