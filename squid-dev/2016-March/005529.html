<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-dev] [PATCH] Increase request buffer size to 64kb
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Increase%20request%20buffer%20size%20to%2064kb&In-Reply-To=%3C56FCFFB3.1020905%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="005527.html">
   <LINK REL="Next"  HREF="005531.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-dev] [PATCH] Increase request buffer size to 64kb</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Increase%20request%20buffer%20size%20to%2064kb&In-Reply-To=%3C56FCFFB3.1020905%40treenet.co.nz%3E"
       TITLE="[squid-dev] [PATCH] Increase request buffer size to 64kb">squid3 at treenet.co.nz
       </A><BR>
    <I>Thu Mar 31 10:45:07 UTC 2016</I>
    <P><UL>
        <LI>Previous message: <A HREF="005527.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
        <LI>Next message: <A HREF="005531.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5529">[ date ]</a>
              <a href="thread.html#5529">[ thread ]</a>
              <a href="subject.html#5529">[ subject ]</a>
              <a href="author.html#5529">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Thank you for some excellent testing (and results :-))


On 31/03/2016 6:50 p.m., Nathan Hoad wrote:
&gt;<i> Responding to both emails, including my findings, so apologies in
</I>&gt;<i> advance for the extremely long email.
</I>&gt;<i> 
</I>&gt;<i> I've gone through the places that use HTTP_REQBUF_SZ, and it seems to
</I>&gt;<i> be Http::Stream::pullData() that's benefiting from this change. To
</I>&gt;<i> simplify all Http::Stream-related uses of HTTP_REQBUF_SZ, I've
</I>&gt;<i> attached a work-in-progress patch that unifies them all into a method
</I>&gt;<i> on Http::Stream and increases only its buffer size, so people are
</I>&gt;<i> welcome to try and replicate my findings.
</I>&gt;<i> 
</I>&gt;<i> Alex, I've tried 8, 16, 32, 128 and 512 KB values - all sizes leading
</I>&gt;<i> up to 64 KB scaled appropriately. 128 and 512 were the same or
</I>&gt;<i> slightly worse than 64, so I think 64 KB is the &quot;best value&quot;.
</I>&gt;<i> 
</I>&gt;<i> My page size and kernel buffer sizes are both stock - I have not
</I>&gt;<i> tweaked anything on this machine.
</I>&gt;<i> 
</I>&gt;<i> $ uname -a
</I>&gt;<i> Linux nhoad-laptop 4.4.3-1-ARCH #1 SMP PREEMPT Fri Feb 26 15:09:29 CET
</I>&gt;<i> 2016 x86_64 GNU/Linux
</I>&gt;<i> $ getconf PAGESIZE
</I>&gt;<i> 4096
</I>&gt;<i> $ cat /proc/sys/net/ipv4/tcp_wmem /proc/sys/net/ipv4/tcp_rmem
</I>&gt;<i> 4096    16384   4194304
</I>&gt;<i> 4096    87380   6291456
</I>&gt;<i> 
</I>&gt;<i> The buffer size on Http::Stream does not grow dynamically, it is a
</I>&gt;<i> simple char[HTTP_REQBUF_SZ]. I could look into making it grow
</I>&gt;<i> dynamically if we're interested in that, but it would be a lot of work
</I>&gt;<i> (to me - feel free to suggest somewhere else this is done and I can
</I>&gt;<i> try to learn from that). I can't definitively say that increasing this
</I>&gt;<i> constant has no impact on smaller objects, however using Apache bench
</I>&gt;<i> indicated no impact in performance, maintaining ~6k requests a second
</I>&gt;<i> pre- and post-patch for a small uncached object.
</I>&gt;<i> 
</I>&gt;<i> Amos, replies inline.
</I>&gt;<i> 
</I>&gt;<i> On 30 March 2016 at 21:29, Amos Jeffries wrote:
</I>&gt;&gt;<i> On 30/03/2016 6:53 p.m., Alex Rousskov wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> One thing you need to keep in mind with all this is that the above
</I>&gt;&gt;<i> macros *does not* configure the network I/O buffers.
</I>&gt;<i> 
</I>&gt;<i> I don't think this is quite true - I don't think it's intentional, but
</I>&gt;<i> I am lead to believe that HTTP_REQBUF_SZ does influence network IO
</I>&gt;<i> buffers in some way. See below.
</I>
Nod. It does seem to be effecting them by being a bottleneck. What I
meant was that it was not the size of those buffers. As your testing
shows, the network I/O buffers can take more data when the bottleneck is
widened.

&gt;<i> 
</I>&gt;&gt;<i> The network HTTP request buffer is controlled by request_header_max_size
</I>&gt;&gt;<i> - default 64KB.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The network HTTP reply buffer is controlled by reply_header_max_size -
</I>&gt;&gt;<i> default 64KB.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The HTTP_REQBUF_SZ macro configures the StoreIOBuffer object size. Which
</I>&gt;&gt;<i> is mostly used for StoreIOBuffer (client-streams or disk I/O) or local
</I>&gt;&gt;<i> stack allocated variables. Which is tuned to match the filesystem page
</I>&gt;&gt;<i> size - default 4KB.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If your system uses non-4KB pages for disk I/O then you should tune that
</I>&gt;&gt;<i> alignment of course. If you are memory-only caching or even not caching
</I>&gt;&gt;<i> that object at all - then the memory page size will be the more
</I>&gt;&gt;<i> important metric to tune it against.
</I>&gt;<i> 
</I>&gt;<i> As shown above, I have 4 KB pages for my memory page size. There is no
</I>&gt;<i> disk cache configured, so disk block size should be irrelevant I think
</I>&gt;<i> - see the end of this mail for the squid.conf I've been using for this
</I>&gt;<i> testing. I also don't have a memory cache configured, so the default
</I>&gt;<i> of 256 MB is being used. Seeing as the object I'm testing is a 5 GB
</I>&gt;<i> file, I don't think the memory cache should be coming into play. To be
</I>&gt;<i> sure, I did also run with `cache_mem none`.
</I>&gt;<i> 
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> How important I'm not sure. I had thought the relative difference in
</I>&gt;&gt;<i> memory and network I/O speeds made the smaller size irrelevant (since we
</I>&gt;&gt;<i> are data-copying from the main network SBuf buffers anyway). But
</I>&gt;&gt;<i> perhapse not. You may have just found that it needs to be tuned to match
</I>&gt;&gt;<i> the network I/O buffer default max-size (64KB).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> NP: perhapse the real difference is how fast Squid can walk the list of
</I>&gt;&gt;<i> in-memory buffers that span the object in memory cache. Since it walks
</I>&gt;&gt;<i> the linked-list from head to position N with each write(2) having larger
</I>&gt;&gt;<i> steps would be relevant.
</I>&gt;<i> 
</I>&gt;<i> Where in the code is this walking done? Investigating this would be
</I>&gt;<i> helpful I think.
</I>
IIRC a function somewhere in mem_node. Might even be called 'walker'.

&gt;<i> 
</I>&gt;&gt;<i> Make sure you have plenty of per-process stack space available before
</I>&gt;&gt;<i> going large. Squid allocates several buffers using this size directly on
</I>&gt;&gt;<i> the stack. Usually at least 2, maybe a half dozen.
</I>&gt;<i> 
</I>&gt;<i> Ensuring I'm being explicit here, in all my testing I haven't messed
</I>&gt;<i> with stack sizes, again using the default on my system, which is:
</I>&gt;<i> 
</I>&gt;<i> Max stack size            8388608              unlimited            bytes
</I>&gt;<i> 
</I>&gt;<i> Which seems to have been enough, I think? What would I see if I had
</I>&gt;<i> run out of stack space? A crash?
</I>&gt;<i> 
</I>
Yes a SEGFAULT crash judging by the recent stack explosions I've been
playing with yesterday.

That 8MB seems to be okay for 512 might even cope with 1MB. But not much
more.


&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> It would be page size (memory pages or disk controller I/O pages). Since
</I>&gt;&gt;<i> the network is tuned already and defaulting to 64KB.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> It is used primarily for the disk I/O and Squid internal client-streams
</I>&gt;&gt;<i> buffers.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> In the long-term plan those internal uses will be replaced by SBuf which
</I>&gt;&gt;<i> are controlled by the existing squid.conf options and actual message
</I>&gt;&gt;<i> sizes more dynamically.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> A new option for tuning disk I/O buffer size might be useful in both
</I>&gt;&gt;<i> long- and short- terms though.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Amos
</I>&gt;&gt;<i>
</I>&gt;<i> 
</I>&gt;<i> Alright, so my findings so far:
</I>&gt;<i> 
</I>&gt;<i> Looking purely at system calls, it shows the reads from the upstream
</I>&gt;<i> server are being read in 16 KB chunks, where as writes to the client
</I>&gt;<i> are done in 4 KB chunks. With the patch, the writes to the client
</I>&gt;<i> increase to 16 KB, so it appears that HTTP_REQBUF_SZ does influence
</I>&gt;<i> network IO in this way.
</I>&gt;<i> 
</I>&gt;<i> Without patch:
</I>&gt;<i> 
</I>&gt;<i> read(14, &quot;...&quot;, 16384) = 16384
</I>&gt;<i> write(11, &quot;...&quot;, 4096) = 4096
</I>&gt;<i> 
</I>&gt;<i> % time     seconds  usecs/call     calls    errors syscall
</I>&gt;<i> ------ ----------- ----------- --------- --------- ----------------
</I>&gt;<i>  86.69    0.050493           0   1310723           write
</I>&gt;<i>  13.31    0.007753           0    327683           read
</I>&gt;<i> ------ ----------- ----------- --------- --------- ----------------
</I>&gt;<i> 100.00    0.058246               1638406           total
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> With patch:
</I>&gt;<i> 
</I>&gt;<i> read(14, &quot;...&quot;, 16384) = 16384
</I>&gt;<i> write(11, &quot;...&quot;, 16384) = 16384
</I>&gt;<i> 
</I>&gt;<i> % time     seconds  usecs/call     calls    errors syscall
</I>&gt;<i> ------ ----------- ----------- --------- --------- ----------------
</I>&gt;<i>  70.55    0.015824           0    327681           write
</I>&gt;<i>  29.45    0.006604           0    327683           read
</I>&gt;<i> ------ ----------- ----------- --------- --------- ----------------
</I>&gt;<i> 100.00    0.022428                655364           total
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> Given that the patch seems to increase the write buffer to 64 KB, the
</I>&gt;<i> 16 KB buffer sizes interested me. So I looked at configuration options
</I>&gt;<i> that default to 16 KB, and found read_ahead_gap. Looking at strace
</I>&gt;<i> output, it showed that increasing this number increased the size of
</I>&gt;<i> the buffer given to the read(2) calls, and with the patch, the
</I>&gt;<i> write(2) calls as well, so I decided to compare read_ahead_gap 16 KB
</I>&gt;<i> and read_ahead_gap 64 KB, with and without the patch.
</I>&gt;<i> 
</I>&gt;<i> Without patch, 16 KB:
</I>&gt;<i> 100 5120M  100 5120M    0     0   104M      0  0:00:48  0:00:48 --:--:-- 96.0M
</I>&gt;<i> 
</I>&gt;<i> Without patch, 64 KB:
</I>&gt;<i> 100 5120M  100 5120M    0     0   102M      0  0:00:50  0:00:50 --:--:-- 91.8M
</I>&gt;<i> 
</I>&gt;<i> With patch, 16 KB:
</I>&gt;<i> 100 5120M  100 5120M    0     0   347M      0  0:00:14  0:00:14 --:--:--  352M
</I>&gt;<i> 
</I>&gt;<i> With patch, 64 KB:*
</I>&gt;<i> 100 5120M  100 5120M    0     0   553M      0  0:00:09  0:00:09 --:--:--  517M
</I>&gt;<i> 
</I>
Nice. :-)

Okay that convinces me we should do this. And change the default
read-ahead gap as well.

&gt;<i> As above shows, this directive does not have much of a performance
</I>&gt;<i> impact pre-patch for this test, as the number and size of write(2)
</I>&gt;<i> calls is still fixed. However post-patch the improvement is quite
</I>&gt;<i> substantial, as the write(2) calls are now using the full 64 KB
</I>&gt;<i> buffer. The strace output above suggests (to me) that the improvement
</I>&gt;<i> in throughput comes from the reduction in syscalls, and possibly less
</I>&gt;<i> work on Squid's behalf. If people are interested in the exact syscall
</I>&gt;<i> numbers, I can show the strace summaries of the above comparisons.
</I>&gt;<i> 
</I>&gt;<i> *: There is quite a lot of variance between runs at this point - the
</I>&gt;<i> averages go from 465MB/s up to 588MB/s.
</I>&gt;<i> 
</I>&gt;<i> Also, the squid.conf I've been using (yes it is 5 lines):
</I>&gt;<i> 
</I>&gt;<i> cache_log /var/log/squid/cache.log
</I>&gt;<i> http_access allow all
</I>&gt;<i> http_port 8080
</I>&gt;<i> cache_effective_user squid
</I>&gt;<i> read_ahead_gap 64 KB
</I>&gt;<i> 
</I>&gt;<i> At this stage, I'm not entirely sure what the best course of action
</I>&gt;<i> is. I'm happy to investigate things further, if people have
</I>&gt;<i> suggestions. read_ahead_gap appears to influence downstream write
</I>&gt;<i> buffer sizes, at least up to the maximum of HTTP_REQBUF_SZ. It would
</I>&gt;<i> be nice if that buffer size was independently run-time configurable
</I>&gt;<i> instead of compile-time, but I don't have a real feel for how much
</I>&gt;<i> work that would be. I'm interested in other people's thoughts here.
</I>
Making it configurable is fairly trivial. Might be useful to do so for
further testing.
Just an entry in SquidConfig.h and src/cf.data.pre, and using the
Config.X member. Perhapse as a MemBuf init() parameter instead of a
straight char[] buffer, they can go up quite large sizes if needed.

The goalpost is to have all the I/O and buffer handling using SBuf
and/or other MemBlob childs for data storage though. That should
completely remove the buffer itself from being a bottleneck limit.

That is quite a lot more work though since the operational design of
StoreIOBuffer is so different to SBuf.

PS. If you like I will take your current patch and merge it tomorrow.

Amos

</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="005527.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
	<LI>Next message: <A HREF="005531.html">[squid-dev] [PATCH] Increase request buffer size to 64kb
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5529">[ date ]</a>
              <a href="thread.html#5529">[ thread ]</a>
              <a href="subject.html#5529">[ subject ]</a>
              <a href="author.html#5529">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.squid-cache.org/listinfo/squid-dev">More information about the squid-dev
mailing list</a><br>
</body></html>
