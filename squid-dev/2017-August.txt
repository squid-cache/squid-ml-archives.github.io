From squid3 at treenet.co.nz  Tue Aug  1 09:42:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Aug 2017 21:42:58 +1200
Subject: [squid-dev] Introduction / SslBump upstream ssl proxy support
In-Reply-To: <334521182.1184657.1500556304973@mail.yahoo.com>
References: <334521182.1184657.1500556304973.ref@mail.yahoo.com>
 <334521182.1184657.1500556304973@mail.yahoo.com>
Message-ID: <c3906f19-57b9-65e6-1036-fb65f0c3c5b4@treenet.co.nz>

On 21/07/17 01:11, Mihai Ene wrote:
> Hello,
> 
> I'm a developer with higher level languages experience very little 
> commercial c++ development on my hands.
> 
> I've been following the SslBump feature for a while now, and this 
> includes source code changes. SslBumping with upstream proxies was 
> completely restricted when bug 3209 was patched in 2011, however, I 
> believe the patch is too restrictive. I agree with Amos's statement that 
> a plaintext information leak is highly unsafe, but the patch also 
> prevents ssl upstream proxies usage.
> 

Hi Mihai,

That bug was 6 years ago, and the comments were specifically about using 
plain-text peer connections. The patch was made to cover all parent 
peers because ...

The problem Squid still has with SSL/TLS peers is not that they leak 
info (they are contacted using TLS after all). It is that explicit-TLS 
proxies use their own certs instead of mimic'd ones so they present 
Squid with a cert other than the origin server cert. That has 
side-effects at the child proxy where bumping cannot mimic the origin 
cert details, and SSL-Bump ends up presenting a clearly invalid cert 
which reasonable clients reject.

In order for the bumping to work without user-visible issues at present 
the best way is for the child proxy to go to its DIRECT or ORIGINAL_DST, 
then get re-intercepted into the parent and re-bumped there. Such that 
the parent mimics the origin cert and it gets to the child proxy, then 
the client.


> In order to prevent plaintext and still use upstream proxies, I propose 
> the following changes (tested in intranet, in production) which enable 
> upstream proxies after ssl bumping, as long as the proxies are ssl 
> themselves:
> 
> - version 4.x 
> https://github.com/randunel/squid4/commit/c91995833370771f9903b374f17a0d774643c2b3
> - version 3.5.x 
> https://github.com/randunel/squid3/commit/a72a47cf0d54bf17faefcfe7692182d82d6520ab
> 

FYI: we are now using github PR system as the only way to accept changes 
to Squid.

Can you please do your submission as a PR request against the 
https://github.com/squid-cache/squid repository master branch. It needs 
to be accepted there before PR against the beta and stable branches code 
will be considered (in that order).

Thank you
Amos

From rousskov at measurement-factory.com  Wed Aug  2 06:49:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Aug 2017 00:49:04 -0600
Subject: [squid-dev] Git-related wiki updates
Message-ID: <e69a268b-2ef1-ffd2-bb53-0dbfa7b7f085@measurement-factory.com>

Hello,

    FYI, I have started updating developer wiki pages to reflect recent
git/GitHub-related changes:

  * http://wiki.squid-cache.org/MergeProcedure
  * http://wiki.squid-cache.org/GitHints
  * http://wiki.squid-cache.org/DeveloperResources

We should not compete with many wonderful git guides and GitHUb manuals,
but there is still a lot of work to be done summarizing Squid-specific
details. Please contribute.


Thank you,

Alex.

From gkinkie at gmail.com  Wed Aug  2 17:26:34 2017
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 2 Aug 2017 18:26:34 +0100
Subject: [squid-dev] Git-related wiki updates
In-Reply-To: <e69a268b-2ef1-ffd2-bb53-0dbfa7b7f085@measurement-factory.com>
References: <e69a268b-2ef1-ffd2-bb53-0dbfa7b7f085@measurement-factory.com>
Message-ID: <761A2F30-7FC0-42D4-9BA0-E180C1746027@gmail.com>

Hi Alex,
	thanks for taking the time!
The updated documents are very helpful and seem quite accurate, I'll try them out and amend if I find any need.

Did you have any challenges with wiki editing?

Thanks!
	Francesco

> On 2 Aug 2017, at 07:49, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> Hello,
> 
>    FYI, I have started updating developer wiki pages to reflect recent
> git/GitHub-related changes:
> 
>  * http://wiki.squid-cache.org/MergeProcedure
>  * http://wiki.squid-cache.org/GitHints
>  * http://wiki.squid-cache.org/DeveloperResources
> 
> We should not compete with many wonderful git guides and GitHUb manuals,
> but there is still a lot of work to be done summarizing Squid-specific
> details. Please contribute.
> 
> 
> Thank you,
> 
> Alex.
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev


From rousskov at measurement-factory.com  Wed Aug  2 17:51:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Aug 2017 11:51:04 -0600
Subject: [squid-dev] Git-related wiki updates
In-Reply-To: <761A2F30-7FC0-42D4-9BA0-E180C1746027@gmail.com>
References: <e69a268b-2ef1-ffd2-bb53-0dbfa7b7f085@measurement-factory.com>
 <761A2F30-7FC0-42D4-9BA0-E180C1746027@gmail.com>
Message-ID: <567e88b6-8b21-381f-d0a5-adeb89b491f3@measurement-factory.com>

On 08/02/2017 11:26 AM, Francesco Chemolli wrote:

> The updated documents are very helpful and seem quite accurate, I'll
> try them out and amend if I find any need.

Please do. They still need a lot of work.


> Did you have any challenges with wiki editing?

Just the 60+ second "your edit was successful" delays and internal
server errors that have been reported to NOC before. I have not lost any
commits yet so this is just an annoyance.


Thank you,

Alex.


>> On 2 Aug 2017, at 07:49, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>>
>> Hello,
>>
>>    FYI, I have started updating developer wiki pages to reflect recent
>> git/GitHub-related changes:
>>
>>  * http://wiki.squid-cache.org/MergeProcedure
>>  * http://wiki.squid-cache.org/GitHints
>>  * http://wiki.squid-cache.org/DeveloperResources
>>
>> We should not compete with many wonderful git guides and GitHUb manuals,
>> but there is still a lot of work to be done summarizing Squid-specific
>> details. Please contribute.
>>
>>
>> Thank you,
>>
>> Alex.
>> _______________________________________________
>> squid-dev mailing list
>> squid-dev at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-dev


From eduard.bagdasaryan at measurement-factory.com  Fri Aug  4 19:52:32 2017
From: eduard.bagdasaryan at measurement-factory.com (Eduard Bagdasaryan)
Date: Fri, 4 Aug 2017 22:52:32 +0300
Subject: [squid-dev] [PATCH] Purge cache entries in SMP-aware caches
In-Reply-To: <ed7c0303-1d97-d813-e1c0-4c69a44ec560@measurement-factory.com>
References: <68a0d7fb-6e14-195c-b167-fc9bdbea4643@measurement-factory.com>
 <ac4e4bf1-dba0-a6e5-db46-16835dd84d1e@measurement-factory.com>
 <a659f750-f53c-875c-c66f-788a2e6aa328@measurement-factory.com>
 <ed7c0303-1d97-d813-e1c0-4c69a44ec560@measurement-factory.com>
Message-ID: <e80f5403-5a1e-664e-9761-edd04943a217@measurement-factory.com>

To avoid this 'going around releaseRequest() API' I would like
to suggest renaming 'RELEASE_REQUEST' flag to something
more suitable, e.g., 'ENTRY_REUSABLE' and adding a helper
API avoiding direct flag manipulation. This would not contradict
with the existing setReleaseFlag() meaning (i.e., "marking the
corresponding entry for eventual removal"), adding an additional
meaning (i.e., since then, the entry is not suggested to be re-used
by others). This intersects a little with the existing 'private entries'
definition. However, private entries may eventually become public,
while an entry with ENTRY_REUSABLE unset, would stay 'private'
until it is removed.


Eduard.

On 24.07.2017 02:00, Eduard Bagdasaryan wrote:
> On 23.07.2017 02:04, Alex Rousskov wrote:
>>> +    if (!flags.cachable)
>>> +        EBIT_SET(e->flags, RELEASE_REQUEST);
>> This release request feels out of place and direct flags setting goes
>> around the existing releaseRequest() API. Please check all callers --
>> perhaps we do not need the above because all callers already do an
>> equivalent action (e.g., makePrivate()) for "uncachable" requests?
>
> I don't think this lines are 'out of place': storeCreatePureEntry() just
> initializes the just created StoreEntry fields (including
> StoreEntry::flags) with correct values.  If we definitely know a
> this moment that 'flags' should have RELEASE_REQUEST set, why do we need
> to postpone this to many callers, hoping that all of them will do that
> work correctly?  There are lots of storeCreateEntry() calls and it is
> hardly possible to track that all of them end up with
> 'releaseRequest()', when flags.cachable is false.  BTW, at the time of
> StoreEntry initialization we do not need to do most of the work
> releaseRequest() does. E.g., there are no connected storages to
> disconnect from, no public keys to make them private, etc. The only
> thing to do is RELEASE_REQUEST flag setting.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20170804/e83fdb87/attachment.html>

From squid3 at treenet.co.nz  Sat Aug  5 05:57:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Aug 2017 17:57:28 +1200
Subject: [squid-dev] [PATCH] Purge cache entries in SMP-aware caches
In-Reply-To: <e80f5403-5a1e-664e-9761-edd04943a217@measurement-factory.com>
References: <68a0d7fb-6e14-195c-b167-fc9bdbea4643@measurement-factory.com>
 <ac4e4bf1-dba0-a6e5-db46-16835dd84d1e@measurement-factory.com>
 <a659f750-f53c-875c-c66f-788a2e6aa328@measurement-factory.com>
 <ed7c0303-1d97-d813-e1c0-4c69a44ec560@measurement-factory.com>
 <e80f5403-5a1e-664e-9761-edd04943a217@measurement-factory.com>
Message-ID: <a427ef97-d141-063a-b542-1781faa2555d@treenet.co.nz>

On 05/08/17 07:52, Eduard Bagdasaryan wrote:
> To avoid this 'going around releaseRequest() API' I would like
> to suggest renaming 'RELEASE_REQUEST' flag to something
> more suitable, e.g., 'ENTRY_REUSABLE' and adding a helper
> API avoiding direct flag manipulation.

I don't see how renaming would help much. Any API control over use of a 
flag can be added regardless of its name.

Also, the 'RELEASE' flag value being true indicates REUSABLE false 
value. So the rename implies a careful and complete inversion of all 
logic conditions relying on or otherwise touching the flag. Which risks 
many new bug additions.

If done at all I think the rename should be left out-of-scope for this 
PR / patching.

Amos

From rousskov at measurement-factory.com  Sat Aug  5 06:09:24 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 5 Aug 2017 00:09:24 -0600
Subject: [squid-dev] [PATCH] Purge cache entries in SMP-aware caches
In-Reply-To: <e80f5403-5a1e-664e-9761-edd04943a217@measurement-factory.com>
References: <68a0d7fb-6e14-195c-b167-fc9bdbea4643@measurement-factory.com>
 <ac4e4bf1-dba0-a6e5-db46-16835dd84d1e@measurement-factory.com>
 <a659f750-f53c-875c-c66f-788a2e6aa328@measurement-factory.com>
 <ed7c0303-1d97-d813-e1c0-4c69a44ec560@measurement-factory.com>
 <e80f5403-5a1e-664e-9761-edd04943a217@measurement-factory.com>
Message-ID: <1c344e68-49d5-accd-0201-524939c7ec63@measurement-factory.com>

On 08/04/2017 01:52 PM, Eduard Bagdasaryan wrote:
> To avoid this 'going around releaseRequest() API' I would like
> to suggest renaming 'RELEASE_REQUEST' flag to something
> more suitable, e.g., 'ENTRY_REUSABLE' and adding a helper
> API avoiding direct flag manipulation. This would not contradict
> with the existing setReleaseFlag() meaning (i.e., "marking the
> corresponding entry for eventual removal"), adding an additional
> meaning (i.e., since then, the entry is not suggested to be re-used
> by others).

Let's focus on the flag meaning. We can always rename it later (or even
in this project). It is the flag meaning that should drive our primary
decisions, not the letters spelling flag's name.

My response has three parts. Most readers may want to skip the first two
parts. I left them to document my thought process in case I missed
something important in my reasoning; the surprising (to me) final
conclusion ended up being rather far from where I started!


== Part I. Simplify by replacing "A or B" with "B" ==

I like the overall direction of your proposal, but I wonder whether we
really have to _add_ the new/proposed "not reusable" meaning to the
old/implied "marked for removal" meaning. Can we just define the flag to
mean "not usable for new Store clients"?

1. The old code that removes idle RELEASE_REQUEST entries from Store
would still work fine: It will be removing idle entries that cannot be
used by anybody. Clearly, removing such entries is logical/natural.

2. Old StoreEntry::release() would still work fine: If it cannot remove
the entry immediately, it will mark it unusable for new clients, knowing
very well that code in #1 will remove such entries (when they become
idle), eventually achieving the result desired by the release() caller.

3. The old StoreEntry::releaseRequest() hack continues to be nothing
more than a temporary hack for callers that are (possibly justifiably!)
afraid of calling release(). No need to discuss it at this high level.

4. The old storeCreatePureEntry() would be able to use the new
StoreEntry::banFutureReuse() or a similar treat-as-private flag-setting
method without raising the same red flag I raised in my review (I will
still argue for moving/changing that code for other reasons, but at
least that it would not look so out of place anymore).

Is there any code that really needs to interpret the RELEASE_REQUEST
flag itself as something other than "unusable for new Store clients"?

If there is no such code, then I have another question: Why do we need
this flag at all? It is answered in Part II.


== Part II. Simplify further by replacing big "B" with small "b" ==

We already have private keys that mean almost the same thing. You have
touched upon this conflict in your email (thank you for carefully
thinking about this!):

> This intersects a little with the existing 'private entries'
> definition. However, private entries may eventually become public,
> while an entry with ENTRY_REUSABLE unset, would stay 'private'
> until it is removed.

I would replace "a little" with something like "98%", but we are
otherwise in agreement :-).

AFAICT, the only difference between a private key and an "unusable for
new Store clients" flag is reversibility of the former. Moreover, IIRC,
all current code paths that set the flag also make (or should make) the
key private (which was a part of my original concern that started this
discussion), tying the two concepts together (naturally).

If this reasoning is correct, then we do not need a "unusable for new
Store clients" flag. We just need a "cannot become public" flag! The
rest of the required functionality/meaning is already covered by the
private key.


== Part III. The new "b" is the old "A"! ==

Part II concluded that we do not need a "unusable for new Store clients"
flag because private keys already address that need. We just need a
"cannot become public" flag. Which reminds me of this old code:

> void
> StoreEntry::makePublic(const KeyScope scope)
> {
>     if (!EBIT_TEST(flags, RELEASE_REQUEST))
>         setPublicKey(scope);
> }

The above code suggests that we may already have that "cannot become
public" flag. It is named differently, it may be set in slightly the
wrong place (separated from the corresponding makePrivate() call), and
some of its tests might need to be adjusted to look for the key status,
but ultimately it can be used correctly as the minimal flag we need to
make a private key permanent!

If you agree with the above reasoning, then it should be easy to adjust
the code to match the newly developed understanding of the
true/desirable RELEASE_REQUEST flag meaning as the "making private keys
permanent" marker. I can suggest specific changes if you prefer, but you
should probably be the one driving this. To reduce noise, I suggest
keeping RELEASE_REQUEST name (at least for now) unless we already have
to change most RELEASE_REQUEST lines for other reasons.


HTH,

Alex.


> On 24.07.2017 02:00, Eduard Bagdasaryan wrote:
>> On 23.07.2017 02:04, Alex Rousskov wrote:
>>>> +    if (!flags.cachable)
>>>> +        EBIT_SET(e->flags, RELEASE_REQUEST);
>>> This release request feels out of place and direct flags setting goes
>>> around the existing releaseRequest() API. Please check all callers --
>>> perhaps we do not need the above because all callers already do an
>>> equivalent action (e.g., makePrivate()) for "uncachable" requests?
>>
>> I don't think this lines are 'out of place': storeCreatePureEntry() just
>> initializes the just created StoreEntry fields (including
>> StoreEntry::flags) with correct values.  If we definitely know a
>> this moment that 'flags' should have RELEASE_REQUEST set, why do we need
>> to postpone this to many callers, hoping that all of them will do that
>> work correctly?  There are lots of storeCreateEntry() calls and it is
>> hardly possible to track that all of them end up with
>> 'releaseRequest()', when flags.cachable is false.  BTW, at the time of
>> StoreEntry initialization we do not need to do most of the work
>> releaseRequest() does. E.g., there are no connected storages to
>> disconnect from, no public keys to make them private, etc. The only
>> thing to do is RELEASE_REQUEST flag setting.
> 
> 
> 
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev
> 


From squid3 at treenet.co.nz  Sat Aug  5 06:52:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Aug 2017 18:52:19 +1200
Subject: [squid-dev] [PATCH] Reuse reserved Negotiate and NTLM helpers
 after an idle timeout.
In-Reply-To: <57113d29-7687-4410-79cd-53d5e6f58536@measurement-factory.com>
References: <708c2c2e-81fd-54cc-36fc-2311aa8b8a07@chtsanti.net>
 <37473742-7656-b95a-1308-87177449a34a@chtsanti.net>
 <6dc858ed-329d-593a-8f3b-a5a5079737ae@treenet.co.nz>
 <f9c36804-a281-a924-b281-1bcf0ec5773f@chtsanti.net>
 <850deed3-72be-92de-9eed-c8763584697a@treenet.co.nz>
 <57113d29-7687-4410-79cd-53d5e6f58536@measurement-factory.com>
Message-ID: <024c3ba4-c5d4-f86e-57fb-b3101e6b2844@treenet.co.nz>

On 01/08/17 04:40, Alex Rousskov wrote:
> On 07/31/2017 09:24 AM, Amos Jeffries wrote:
> 
>>>> To do so otherwise would randomly
>>>> allow replay attacks to succeed
> 
> Please give a specific example where the proposed changes would allow a
> new kind of replay attacks to succeed, given a correctly functioning
> Squid and a correctly functioning helper. I cannot think of any
> realistic examples, but this is not my area of expertise.
> 


In NTLM the TT (type-2) tokens are generated by a particular helper and 
only that helper can authenticate the corresponding KK (type-3) token.

Annoyingly NTLM does not authenticate the client, nor the HTTP message 
it is attached to - it is specifically (and only) authenticating that 
the TCP connection is being used by the specific end-client able to 
generate the KK token proof-of-identity.


With the current code reserved helpers are tied to a particular TCP 
connection through Auth::UserRequest. As such an attacker would have to 
inject replay attempts into the same TCP connection the client was 
using. Which is protected by the TCP SEQNO mechanism - not impossible to 
subvert, but a high difficulty level.
  Any attempt to send the KK on another TCP connection would reach a 
different helper and be rejected as a 'secret' value embedded in the TT 
was reserved by the originating helper.


With the proposed changes all an attacker needs to do is peek at the KK 
token from the client then race it to be the first one to deliver any 
token to the originating helper (which can succeed at or after reuse 
timeout) - using as many other TCP connections as it likes. Which is a 
MUCH easier thing to do than SEQNO subverting.

As far as the TT generating helper is concerned there is no difference 
between an attackers KK token after timeout, or Squid just waiting some 
period timeout+N until the real clients KK token arrived.

And as I mentioned earlier there are other things the attacker can do to 
slow traffic down and bias the race towards itself. Those things do not 
have any effect on the client TCP connection use of SEQNO - so are 
relatively benign with the current code.


Note that under the attack conditions the TT generating helper *does 
not* receive a fresh YR query which it might use to reset its 
reservation state. The victim client generates the YR, then after 
timeout the attacker supplies the KK. The TT goes out on one TCP 
connection, and the KK returns via a different one.


PS. also note that this is only an issue for NTLM. However the existence 
of Negotiate/NTLM flavour of Negotiate makes that interface hit the same 
problem at times. This whole reservation thing is irrelevant with Kerberos.


Amos

From christos at chtsanti.net  Tue Aug  8 15:18:16 2017
From: christos at chtsanti.net (Christos Tsantilas)
Date: Tue, 8 Aug 2017 18:18:16 +0300
Subject: [squid-dev] [PATCH] Reuse reserved Negotiate and NTLM helpers
 after an idle timeout.
In-Reply-To: <024c3ba4-c5d4-f86e-57fb-b3101e6b2844@treenet.co.nz>
References: <708c2c2e-81fd-54cc-36fc-2311aa8b8a07@chtsanti.net>
 <37473742-7656-b95a-1308-87177449a34a@chtsanti.net>
 <6dc858ed-329d-593a-8f3b-a5a5079737ae@treenet.co.nz>
 <f9c36804-a281-a924-b281-1bcf0ec5773f@chtsanti.net>
 <850deed3-72be-92de-9eed-c8763584697a@treenet.co.nz>
 <57113d29-7687-4410-79cd-53d5e6f58536@measurement-factory.com>
 <024c3ba4-c5d4-f86e-57fb-b3101e6b2844@treenet.co.nz>
Message-ID: <bc86c88a-bf30-1a82-4690-5639a360e05d@chtsanti.net>

Στις 05/08/2017 09:52 πμ, ο Amos Jeffries έγραψε:
> On 01/08/17 04:40, Alex Rousskov wrote:
>> On 07/31/2017 09:24 AM, Amos Jeffries wrote:
>>
>>>>> To do so otherwise would randomly
>>>>> allow replay attacks to succeed
>>
>> Please give a specific example where the proposed changes would allow a
>> new kind of replay attacks to succeed, given a correctly functioning
>> Squid and a correctly functioning helper. I cannot think of any
>> realistic examples, but this is not my area of expertise.
>>
> 
> 
> In NTLM the TT (type-2) tokens are generated by a particular helper and 
> only that helper can authenticate the corresponding KK (type-3) token.
> 
> Annoyingly NTLM does not authenticate the client, nor the HTTP message 
> it is attached to - it is specifically (and only) authenticating that 
> the TCP connection is being used by the specific end-client able to 
> generate the KK token proof-of-identity.
> 
> 
> With the current code reserved helpers are tied to a particular TCP 
> connection through Auth::UserRequest. As such an attacker would have to 
> inject replay attempts into the same TCP connection the client was 
> using. Which is protected by the TCP SEQNO mechanism - not impossible to 
> subvert, but a high difficulty level.
>   Any attempt to send the KK on another TCP connection would reach a 
> different helper and be rejected as a 'secret' value embedded in the TT 
> was reserved by the originating helper >
> 
> With the proposed changes all an attacker needs to do is peek at the KK 
> token from the client then race it to be the first one to deliver any 
> token to the originating helper (which can succeed at or after reuse 
> timeout) - using as many other TCP connections as it likes. Which is a 
> MUCH easier thing to do than SEQNO subverting. >
> As far as the TT generating helper is concerned there is no difference 
> between an attackers KK token after timeout, or Squid just waiting some 
> period timeout+N until the real clients KK token arrived.
> 
> And as I mentioned earlier there are other things the attacker can do to 
> slow traffic down and bias the race towards itself. Those things do not 
> have any effect on the client TCP connection use of SEQNO - so are 
> relatively benign with the current code.
> 
> 
> Note that under the attack conditions the TT generating helper *does 
> not* receive a fresh YR query which it might use to reset its 

If I am not wrong this is prevented by ntlm/UserRequest.cc code.

The ntlm/* code for each new connection:
    1) expects a type1 message in Authorization header (and send a YR 
request to helper)
    2) sends back to the client a type2 message in WWW-Authenticate header
    3) expects a type3 message in Authorization header (and send a KK 
request to helper)

If squid did not receive with that order the authentication info from 
the client will just reject/deny the connection.

The ntlm helpers designed to be idiots. They expect a YR and then a KK. 
Every new YR resets their state.
They are not responsible to check for more and must not do it.

Squid code is responsible for this, and I believe this patch does not 
break anything in this area: Any expired reservation results to close 
client connection.
A any new connection still have to repeat the 3  NTLM authentication 
steps to authenticate.

Am I missing something?

> reservation state. The victim client generates the YR, then after 
> timeout the attacker supplies the KK. The TT goes out on one TCP 
> connection, and the KK returns via a different one.
> 
> 
> PS. also note that this is only an issue for NTLM. However the existence 
> of Negotiate/NTLM flavour of Negotiate makes that interface hit the same 
> problem at times. This whole reservation thing is irrelevant with Kerberos.
> 
> 
> Amos
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev


From rousskov at measurement-factory.com  Tue Aug  8 16:01:05 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Aug 2017 10:01:05 -0600
Subject: [squid-dev] [PATCH] Reuse reserved Negotiate and NTLM helpers
 after an idle timeout.
In-Reply-To: <bc86c88a-bf30-1a82-4690-5639a360e05d@chtsanti.net>
References: <708c2c2e-81fd-54cc-36fc-2311aa8b8a07@chtsanti.net>
 <37473742-7656-b95a-1308-87177449a34a@chtsanti.net>
 <6dc858ed-329d-593a-8f3b-a5a5079737ae@treenet.co.nz>
 <f9c36804-a281-a924-b281-1bcf0ec5773f@chtsanti.net>
 <850deed3-72be-92de-9eed-c8763584697a@treenet.co.nz>
 <57113d29-7687-4410-79cd-53d5e6f58536@measurement-factory.com>
 <024c3ba4-c5d4-f86e-57fb-b3101e6b2844@treenet.co.nz>
 <bc86c88a-bf30-1a82-4690-5639a360e05d@chtsanti.net>
Message-ID: <bde1b2a4-1cb8-fb42-4f68-aa5d7e75a61c@measurement-factory.com>

On 08/08/2017 09:18 AM, Christos Tsantilas wrote:
> Στις 05/08/2017 09:52 πμ, ο Amos Jeffries έγραψε:
>> With the proposed changes all an attacker needs to do is peek at the
>> KK token from the client then race it to be the first one to deliver
>> any token to the originating helper (which can succeed at or after
>> reuse timeout)

> If I am not wrong this is prevented by ntlm/UserRequest.cc code.

FWIW, I was hoping that would be the case. If it were not the case, I
would suggest that Squid polices initial tokens to prevent such attacks.
Glad that defense is already implemented.

Amos, if you agree with Christos, please either withdraw your objection
or come up with another attack.


Thank you,

Alex.

From rousskov at measurement-factory.com  Wed Aug  9 01:14:37 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Aug 2017 19:14:37 -0600
Subject: [squid-dev] [RFC] Happy Eyeballs: Parallel TCP connection attempts
Message-ID: <d33824c4-a91b-6ebd-e7cd-cef1e0bd908c@measurement-factory.com>

Hello,

    Since r15240, Squid forwarding code gets a concurrently growing list
of destinations to forward the request to, but it only tries to
establish a connection to one destination at a time, going to the next
one only if the previous connection attempt has failed. Many admins
disable IPv6 because that previous (IPv6) attempt may fail very "slowly"
while it is possible to "quickly" establish a matching (IPv4) connection
instead. Users complain, and IPv6 gets turned off.

To combat this IPv6 adoption barrier, Happy Eyeballs (RFC 6555) suggests
to start establishing a second TCP connection if the first one appears
to be stuck while trying to connect(2). For example, if an IPv6
connect(2) does not succeed in 300ms, Chrome starts an IPv4 connect(2).
A "dumb" implementation of that algorithm is problematic for a proxy
because it can lead to excessive number of connections (in various
states) and might even resemble traffic amplification attacks in extreme
cases.

I propose to implement the following algorithm that follows the Happy
Eyeballs principles while limiting "environmental impact" of opening and
discarding lots of proxy-to-server connections:


* Bootstrapping:

When a new destination is received by FwdState::noteDestination():

0. If there are no in-use destinations, then proceed to use the new
destination as usual. No changes here.

1. If there is one in-use destination, then run the "Spare Connect"
algorithm described below. This is new.

2. If there are two in-use destinations, then add the new destination to
the list as usual. No changes here.

There is no item #3 because there cannot be more than two in-use
destinations at any given time.


* Spare Connect:

If/when allowed (see below), start a second or "spare" TCP connect()
attempt to race against the already in-progress attempt. The first
connect() to succeed wins. The loser is immediately discarded (its
socket is closed) and treated as connect() failure. For example, its
destination is marked as "bad" in the IP cache to prevent subsequent
slow connect() attempts.

Here are the preconditions for the "If/when allowed" part:

0. Both the IP address used by the in-progress connect() and the new
destination IP originate from the same domain name[0] but their address
families differ.

1. The other in-use destination is still in the "TCP connecting" state.
This algorithm works around IP connectivity problems, not slow HTTP(S)
servers.

2. Squid can open new connections and the total number of in-progress
spare connect() attempts (across all same-worker FwdStates) is below the
happy_eyeballs_connect_limit[1]. Please note that this precondition
changes as other spare connect() attempts end. However, I believe we can
ignore (i.e., do not monitor/wait for) such changes for the initial
implementation, for simplicity sake.

3. The last spare connect() attempt (across all same-worker FwdStates)
started at least happy_eyeballs_connect_gap[2] milliseconds ago. Please
note that this precondition changes with time. However, I believe we can
ignore (i.e., do not monitor/wait for) such changes for the initial
implementation, for simplicity sake.

4. The in-progress connect() attempt (for this FwdState object) started
at least happy_eyeballs_connect_timeout[3] milliseconds ago. Please note
that this precondition changes with time. FwdState may need to schedule
a timeout to comply (and re-evaluate preconditions #1-3 after that
timeout). The destination of the spare connect() is considered to be "in
use" while we wait for this timeout to expire.

Here are all the configurable checks and their intended purpose:

  happy_eyeballs_connect_limit: Do not consume too much over time.
  happy_eyeballs_connect_gap: Do not amplify flash crowds (rate limit).
  happy_eyeballs_connect_timeout: Do not try to fix fast-enough cases.


Did I miss any preconditions? Any better ideas?


Footnotes:

[0] The "same domain name" restriction ensures that the "preferred"
destination choices (e.g., cache_peer foo) are still always tried before
the "secondary" ones (e.g., DIRECT).

[1,2,3] The exact names, structure, and the defaults of the new
squid.conf directive(s) are to be determined. Suggestions welcomed. And
if we can hard-code something reasonable for some of them, that is even
better for the initial implementation!


Thank you,

Alex.

From rousskov at measurement-factory.com  Wed Aug  9 20:24:03 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Aug 2017 14:24:03 -0600
Subject: [squid-dev] [RFC] http(s)_port TLS/SSL config redesign
In-Reply-To: <2187f117-97a4-4aed-8a43-426a217745cf@treenet.co.nz>
References: <2187f117-97a4-4aed-8a43-426a217745cf@treenet.co.nz>
Message-ID: <537de842-2ab7-73f8-67f9-5ffc082f0569@measurement-factory.com>

On 07/19/2017 07:27 PM, Amos Jeffries wrote:
> Hi all, Christos and Alex particularly,
> 
> I have been mulling over several ideas for how to improve the config
> parameters on the http(s)_port to make them a bit easier for newbies to
> get right, and pros to do powerfully cool stuff.
> 
> 
> So, the most major change I would like to propose is to move the proxies
> CA for cert generation from cert= parameter to
> generate-host-certificates= parameter. Having it configured with a file
> being the same as generate =on and not configuring it default to =off.
> 
> 
> The matching key= and any CA chain would need to be all bundled in the
> same PEM file. That should be fine since the clients get a separate DER
> file installed, not sharing the PEM file loaded into Squid.
> 
> That will stop confusing newbies have over what should go in cert= for
> what Squid use-case. And will allow pros to configure exactly which
> static cert Squid uses as a fallback when auto-generating others -
> simply by using cert= in the normal non-bumping way.
> 
> Also, we can then easily use the two sets of parameters in identical
> fashion for non-SSL-Bump proxies to auto-generate reverse-proxy certs
> based on SNI, or use a fallback static cert of admins choice.
> 
> Bringing these two different use-cases config into line should vastly
> simplify the complexity of working with Squid TLS certs for everybody,
> including us in the code as we no longer have multiple (8! at least)
> code paths for different cert= possibilities and config error handling
> permutations.
> 
> 
> For backward compatibility concerns with existing SSL-Bump configs I
> think we can use the certificate CA vs non-CA property to auto-detect
> old SSL-Bump configs being loaded and handle the compatibility
> auto-upgrade and warnings. The warning will be useful long-term and not
> just for the transitional period.


I doubt I can grasp all the side effects, but what you are proposing
sounds very good to me in principle. If your arguments are valid, then I
would go further, for exactly the same reasons you are giving above: I
suggest deprecating abused cert/key pair and having a new option for the
regular _port certificate/key. For example:

  https_port port-certificate=... generate-host-certificates=...

and then I would also deprecate generate-host-certificates instead of
abusing a boolean option to specify an essentially required argument,
arriving at something like this:

  https_port port-certificate=... ssl-bump-ca-certificate=...

and refuse to accept non-CA certificates in ssl-bump-ca-certificate.

The "port-" prefix can be dropped but I think having two different
prefixes for two certificates is better.


HTH,

Alex.

From eliezer at ngtech.co.il  Thu Aug 10 00:39:56 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 10 Aug 2017 03:39:56 +0300
Subject: [squid-dev] [RFC] http(s)_port TLS/SSL config redesign
In-Reply-To: <2187f117-97a4-4aed-8a43-426a217745cf@treenet.co.nz>
References: <2187f117-97a4-4aed-8a43-426a217745cf@treenet.co.nz>
Message-ID: <177201d31171$30471da0$90d558e0$@ngtech.co.il>

I have not used v4 yet but the arguments stand for themselves.
+1

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-dev [mailto:squid-dev-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, July 20, 2017 04:27
To: Squid Developers <squid-dev at lists.squid-cache.org>
Subject: [squid-dev] [RFC] http(s)_port TLS/SSL config redesign

Hi all, Christos and Alex particularly,

I have been mulling over several ideas for how to improve the config 
parameters on the http(s)_port to make them a bit easier for newbies to 
get right, and pros to do powerfully cool stuff.


So, the most major change I would like to propose is to move the proxies 
CA for cert generation from cert= parameter to 
generate-host-certificates= parameter. Having it configured with a file 
being the same as generate =on and not configuring it default to =off.


The matching key= and any CA chain would need to be all bundled in the 
same PEM file. That should be fine since the clients get a separate DER 
file installed, not sharing the PEM file loaded into Squid.

That will stop confusing newbies have over what should go in cert= for 
what Squid use-case. And will allow pros to configure exactly which 
static cert Squid uses as a fallback when auto-generating others - 
simply by using cert= in the normal non-bumping way.

Also, we can then easily use the two sets of parameters in identical 
fashion for non-SSL-Bump proxies to auto-generate reverse-proxy certs 
based on SNI, or use a fallback static cert of admins choice.

Bringing these two different use-cases config into line should vastly 
simplify the complexity of working with Squid TLS certs for everybody, 
including us in the code as we no longer have multiple (8! at least) 
code paths for different cert= possibilities and config error handling 
permutations.


For backward compatibility concerns with existing SSL-Bump configs I 
think we can use the certificate CA vs non-CA property to auto-detect 
old SSL-Bump configs being loaded and handle the compatibility 
auto-upgrade and warnings. The warning will be useful long-term and not 
just for the transitional period.


Now would also be a marginally better than usual time to make the change 
since the parameters are migrating to tls-* prefix in v4 and have extra 
admin attention already.


Amos
_______________________________________________
squid-dev mailing list
squid-dev at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-dev


From rousskov at measurement-factory.com  Fri Aug 11 00:13:31 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Aug 2017 18:13:31 -0600
Subject: [squid-dev] Added states to pull requests
Message-ID: <46533964-76ed-63e6-cb4c-fc0c365207d7@measurement-factory.com>

Hello,

    I noticed that GitHub cannot express several important pull request
states. For example, GitHub's "changes requested" PR status means that

* the PR author is expected to make changes based on prior reviewer(s)
feedback; or

* the PR reviewer(s) are expected to verify that all previously
expressed concerns have been addressed by the latest changes.

I added two[1] labels to distinguish these two different states:
S-waiting-for-reviewer and S-waiting-for-author[2].


There is a similar problem with GitHub's "approved" PR status. It could
mean that

* the PR author is expected to either make some minor improvements or
refuse to make them and merge the approved PR as is;

* a committer is expected to merge the approved PR; or

* the PR author is waiting for more reviews, despite getting at least
one positive one.

S-waiting-for-author can already be used to distinguish the first bullet
from the other two[1]. I added two more labels to distinguish the last
two states: S-waiting-for-committer and S-waiting-for-more-reviewers.


Finally, there are similar problems with GitHub's "review required" PR
status. The already added labels seem to be enough[1] to distinguish
various cases for that state.


[1] Ability to cover more use cases is one of the reasons I added two
labels where one would have sufficed (to distinguish two states).
Besides, interpreting the pull request roster is much easier when all
waiting PRs are labeled.

[2] The "S-" prefix stands for "state". Many GitHub projects end up
using lots of labels so it is a good idea (not mine!) to group related
labels using prefixes like that.


HTH,

Alex.

From andreas at canonical.com  Thu Aug 24 14:34:57 2017
From: andreas at canonical.com (Andreas Hasenack)
Date: Thu, 24 Aug 2017 11:34:57 -0300
Subject: [squid-dev] 3.5.27 fails to build with gcc7 on 32 bits
Message-ID: <CANYNYEHsLY1z0Go5Ci6jpr6SQk-jBoKKOFZtRyfjgH2oXkLunA@mail.gmail.com>

Hi,

I'm trying to file a bug about this problem, but bugs.squid-cache.org just
times out with a 504 when I click submit. I did manage to add a comment to
the existing (but closed) 4671 bug, though:
http://bugs.squid-cache.org/show_bug.cgi?id=4671#c3

Here is what would be in the new bug report:
"""
3.5.27 contains many gcc7 build fixes, and they all work when the source is
built on 64bits. But on 32bits it fails:
(...)
Format.cc: In member function ‘void Format::Format::assemble(MemBuf&, const
Pointer&, int) const’:
Format.cc:345:1: error: ‘%0*lld’ directive output may be truncated writing
between 1 and 2147483646 bytes into a region of size 1024
[-Werror=format-truncation=]
 Format::Format::assemble(MemBuf &mb, const AccessLogEntry::Pointer &al,
int logSequenceNumber) const
 ^~~~~~
Format.cc:345:1: note: directive argument in the range [-2147483648,
2147483647]


The v3.5 branch (and the 3.5.27 release as a result) seems to be missing
the 4671 part 4 fix:

https://github.com/squid-cache/squid/commit/6d19fc4dbb47a9b6993057c8599cd57f29d1475a
"""

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20170824/7a7b58e7/attachment.html>

From squid3 at treenet.co.nz  Mon Aug 28 02:50:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 28 Aug 2017 14:50:46 +1200
Subject: [squid-dev] OpenSSL 1.1.0f build issues
Message-ID: <ac6b13ce-b600-3bac-1bab-6c32813a033b@treenet.co.nz>

This OpenSSL has some more breaking API issues.

../../../../src/ssl/support.cc:1373:29: error: invalid conversion from 
‘ASN1_BIT_STRING** {aka asn1_string_st**}’ to ‘const ASN1_BIT_STRING** 
{aka const asn1_string_st**}’ [-fpermissive]
../../../../src/ssl/support.cc:1373:35: error: invalid conversion from 
‘X509_ALGOR** {aka X509_algor_st**}’ to ‘const X509_ALGOR** {aka const 
X509_algor_st**}’ [-fpermissive]

I have a patch in my TLS config branch, if it looks okay with you I will 
make a PR to add it separately.
<https://github.com/yadij/squid/commit/192c5b931a5b0484246e21fd02310974131d32a6>


There also seems to be a regression in the last branch merge with the 
'bio' variable.


../../../../src/ssl/support.cc:1481:10: error: ‘bio’ was not declared in 
this scope
../../../../src/ssl/support.cc:1483:18: error: ‘bio’ was not declared in 
this scope


Amos

From rousskov at measurement-factory.com  Mon Aug 28 19:40:11 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Aug 2017 13:40:11 -0600
Subject: [squid-dev] OpenSSL 1.1.0f build issues
In-Reply-To: <1ce18432-b79e-6fc0-a327-526ff1c62be5@measurement-factory.com>
References: <ac6b13ce-b600-3bac-1bab-6c32813a033b@treenet.co.nz>
 <1ce18432-b79e-6fc0-a327-526ff1c62be5@measurement-factory.com>
Message-ID: <e0a412b7-2c8b-f3fb-5f73-6389e7431525@measurement-factory.com>

On 08/28/2017 08:44 AM, Alex Rousskov wrote:
> On 08/27/2017 08:50 PM, Amos Jeffries wrote:
> 
>> I have a patch in my TLS config branch, if it looks okay with you I will
>> make a PR to add it separately.
>> https://github.com/yadij/squid/commit/192c5b931a5b0484246e21fd02310974131d32a6
> 
>> +   AC_DEFINE_UNQUOTED(const_ASN1_BIT_STRING, [ASN1_BIT_STRING])
>> +   AC_DEFINE_UNQUOTED(const_X509_ALGOR, [X509_ALGOR])
> 
> I do not think we should lie about the actual constness of those types.
> I suggest using something like extracted_ASN1_BIT_STRING and
> extracted_X509_ALGOR names instead.
> 
> 
>> -        X509_ALGOR *sig_alg;
>> +        const_X509_ALGOR *sig_alg;
> 
> Please use this opportunity to initialize sig_alg to nullptr before
> passing it to an external function. The other parameter (sig) is already
> initialized.


Also, please note that printX509Signature() in ssl/gadgets.cc is also
broken in a similar way.

IMHO, we should replace all HAVE_LIBCRYPTO_X509_GET0_SIGNATURE checks
with X509_get0_signaturex() or a similar wrapper that takes care both of
the function presence and its argument constness.


Thank you,

Alex.

From dengke.du at windriver.com  Tue Aug 29 02:02:19 2017
From: dengke.du at windriver.com (Dengke Du)
Date: Tue, 29 Aug 2017 10:02:19 +0800
Subject: [squid-dev] configure: error: Basic auth helper NIS ... found but
	cannot be built
Message-ID: <d99f0e91-5f7f-eb51-6191-6f09433c6c8b@windriver.com>

When the configure check:

|checking for winldap.h... no
|checking for sys/types.h... (cached) yes
|checking for rpc/rpc.h... (cached) no
|checking for rpcsvc/ypclnt.h... yes
|checking for rpcsvc/yp_prot.h... yes
|checking for crypt.h... (cached) yes

|configure: error: Basic auth helper NIS ... found but cannot be built

I am sure the rpc/rpc.h exist in /usr/include

why the configure script can't find it?


From rousskov at measurement-factory.com  Mon Aug 28 14:44:42 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Aug 2017 08:44:42 -0600
Subject: [squid-dev] OpenSSL 1.1.0f build issues
In-Reply-To: <ac6b13ce-b600-3bac-1bab-6c32813a033b@treenet.co.nz>
References: <ac6b13ce-b600-3bac-1bab-6c32813a033b@treenet.co.nz>
Message-ID: <1ce18432-b79e-6fc0-a327-526ff1c62be5@measurement-factory.com>

On 08/27/2017 08:50 PM, Amos Jeffries wrote:

> I have a patch in my TLS config branch, if it looks okay with you I will
> make a PR to add it separately.
> https://github.com/yadij/squid/commit/192c5b931a5b0484246e21fd02310974131d32a6

> +   AC_DEFINE_UNQUOTED(const_ASN1_BIT_STRING, [ASN1_BIT_STRING])
> +   AC_DEFINE_UNQUOTED(const_X509_ALGOR, [X509_ALGOR])

I do not think we should lie about the actual constness of those types.
I suggest using something like extracted_ASN1_BIT_STRING and
extracted_X509_ALGOR names instead.


> -        X509_ALGOR *sig_alg;
> +        const_X509_ALGOR *sig_alg;

Please use this opportunity to initialize sig_alg to nullptr before
passing it to an external function. The other parameter (sig) is already
initialized.


> There also seems to be a regression in the last branch merge with the
> 'bio' variable. 
> 
> ../../../../src/ssl/support.cc:1481:10: error: ‘bio’ was not declared in this scope
> ../../../../src/ssl/support.cc:1483:18: error: ‘bio’ was not declared in this scope

I see the bug and will submit a fix candidate. Evidently, the older CI
tests did not have HAVE_LIBCRYPTO_BIO_METH_NEW defined. Hopefully, the
matrix tests will do better.


Thank you,

Alex.

From rousskov at measurement-factory.com  Mon Aug 28 20:19:55 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Aug 2017 14:19:55 -0600
Subject: [squid-dev] OpenSSL 1.1.0f build issues
In-Reply-To: <1ce18432-b79e-6fc0-a327-526ff1c62be5@measurement-factory.com>
References: <ac6b13ce-b600-3bac-1bab-6c32813a033b@treenet.co.nz>
 <1ce18432-b79e-6fc0-a327-526ff1c62be5@measurement-factory.com>
Message-ID: <5bbaa40f-b8a6-a891-747c-b71bf8ae6e98@measurement-factory.com>

On 08/28/2017 08:44 AM, Alex Rousskov wrote:
> On 08/27/2017 08:50 PM, Amos Jeffries wrote:
>> There also seems to be a regression in the last branch merge with the
>> 'bio' variable. 
>>
>> ../../../../src/ssl/support.cc:1481:10: error: ‘bio’ was not declared in this scope
>> ../../../../src/ssl/support.cc:1483:18: error: ‘bio’ was not declared in this scope

> I see the bug and will submit a fix candidate.

Done: https://github.com/squid-cache/squid/pull/53

Alex.

From eliezer at ngtech.co.il  Wed Aug 30 12:08:59 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 30 Aug 2017 15:08:59 +0300
Subject: [squid-dev] What do you think about a response or continuation for
	this article?
Message-ID: <22b701d32188$c33c63a0$49b52ae0$@ngtech.co.il>

Hey Dev's,

I was looking at some old tutorial at:
https://www.linux.com/news/speed-your-internet-access-using-squids-refresh-p
atterns

>From the search:
https://www.linux.com/search?keyword=squid

And was wondering if it might be a good idea to write something that will be
up-to-date ie 2017-2008=9 so with almost 10 years of progress.
I think it's time to offer an up-to-date article.
What do you all think?

Eliezer

* I think that github gave the development team a good shoulder to lean on
in the development cycle and all the PR's are looking much better then a
"dev list broadcast".

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il





From squid3 at treenet.co.nz  Wed Aug 30 12:43:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Aug 2017 00:43:40 +1200
Subject: [squid-dev] configure: error: Basic auth helper NIS ... found
 but cannot be built
In-Reply-To: <d99f0e91-5f7f-eb51-6191-6f09433c6c8b@windriver.com>
References: <d99f0e91-5f7f-eb51-6191-6f09433c6c8b@windriver.com>
Message-ID: <6bbc9305-7cf1-0cc1-572c-29c7f845ac54@treenet.co.nz>

On 29/08/17 14:02, Dengke Du wrote:
> When the configure check:
> 
> |checking for winldap.h... no
> |checking for sys/types.h... (cached) yes
> |checking for rpc/rpc.h... (cached) no
> |checking for rpcsvc/ypclnt.h... yes
> |checking for rpcsvc/yp_prot.h... yes
> |checking for crypt.h... (cached) yes
> 
> |configure: error: Basic auth helper NIS ... found but cannot be built
> 
> I am sure the rpc/rpc.h exist in /usr/include
> 
> why the configure script can't find it?
> 

Something prevented it being detected.

The config.log produced in the directory you built from which contains 
the full details of "no" results.

Note that "(cached)" word indicates that the same test was performed 
sometime earlier and the result was "no" at that time. So you need to 
look for the first mention of that file in config.log.

Amos

