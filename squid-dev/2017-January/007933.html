<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-dev] [PATCH] Response delay pools
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Response%20delay%20pools&In-Reply-To=%3C588FCFB4.5070608%40measurement-factory.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=utf-8">
   <LINK REL="Previous"  HREF="007811.html">
   <LINK REL="Next"  HREF="007809.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-dev] [PATCH] Response delay pools</H1>
    <B>Eduard Bagdasaryan</B> 
    <A HREF="mailto:squid-dev%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-dev%5D%20%5BPATCH%5D%20Response%20delay%20pools&In-Reply-To=%3C588FCFB4.5070608%40measurement-factory.com%3E"
       TITLE="[squid-dev] [PATCH] Response delay pools">eduard.bagdasaryan at measurement-factory.com
       </A><BR>
    <I>Mon Jan 30 23:43:48 UTC 2017</I>
    <P><UL>
        <LI>Previous message: <A HREF="007811.html">[squid-dev] [PATCH] Response delay pools
</A></li>
        <LI>Next message: <A HREF="007809.html">[squid-dev] [PATCH] Mitigate DoS attacks that use client-initiated SSL/TLS renegotiation.
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7933">[ date ]</a>
              <a href="thread.html#7933">[ thread ]</a>
              <a href="subject.html#7933">[ subject ]</a>
              <a href="author.html#7933">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 22.01.2017 22:52, Amos Jeffries wrote:

 &gt; as I understood it the existing delay pools design is that multiple
 &gt; pools can apply to traffic. In which case on each write() attempt the
 &gt; bucket with smallest available amount determins the write size and all
 &gt; buckets have the actually consumed amount removed.

I assume that you are mixing up &quot;pools&quot; and &quot;buckets&quot;. A pool may have
several &quot;cascading&quot; buckets. The existing server pools have several
buckets for classes &gt; 1.  We can configure several server delay pools,
but only one first 'matched' pool is assigned to the transaction. Then
the &quot;smallest&quot; bucket of the matched pool determines the final speed
limiting (as you correctly noted).  Following this logic, old &quot;client&quot;
and new &quot;response&quot; pools should be independent (as if they would
represent different client pool classes), i.e., finally we
should select only one &quot;first matched&quot; client-side delay pool. Within
this selected pool, we select bucket with the smallest level (individual
or aggregate) similarly as we do for server pools of classes 2
or 3). However we do not do this selection if old client pool matched,
because it has the only one bucket.

 &gt; ClientDelayConfig::parsePoolCount() please use emplace_back instead of
 &gt; push_back.

AFAIK, emplace_back() would have a benefit over push_back if there was
an object itself (thus avoiding copying the object). In this case a
pointer is inserted so the benefit is negligible. I would prefer using
old push_back() here.


 &gt; is there a specific reason this has to be a list of raw-pointers
 &gt; instead of a list of objects like it was?

This change was a consequence of another fix: ClientDelayPool::access member
was not destroyed on shutdown causing &quot;indirectly lost&quot; Valgrind error 
messages.
ClientDelayPool class does not follow copy semantics(and probably should 
not)
due to acl_access pointer, requiring by stl vector. So I had to use 
pointers instead.
However my latest checks with test-builds.sh showed another linker problems,
because SquidConfig got unnecessary dependency on ClientDelayConfig
destructor. Finally I had to re-work ClientDelayConfig module,
introducing new ClientDelayPools class (a storage for ClientDelayPool
objects) and making ClientDelayPool ref-countable.

 &gt;  which brings up a design contradiction: &quot;why are there required
 &gt; 'options'?
 &gt; so the answer is that they should not be required, the value -1 in
 &gt; pools just means &quot;no limit&quot; or &quot;infinity&quot;.
 &gt; - the for-loop checking for missing options should be removed and use
 &gt; of the &quot;none&quot; magic word as value for these options should be supported.

I agree that we should not require using all response_delay_pool
options.  For example, it would be useful to allow only &quot;individual&quot; or
&quot;aggregate&quot; options, e.g.:

#no aggregate limiting
response_delay_pool slowPool1  individual-restore=50000 \
                                   individual-maximum=100000

#no individual limiting, split aggregate bandwidth among clients
response_delay_pool slowPool2  aggregate-restore=100000  \
                                   aggregate-maximum=200000

No need for &quot;none&quot; world: just remove unneeded options (or specify -1
value). Using approach the existing server delay pools
adhere to (&quot;none&quot; for speed/maximum pair), we should not use &quot;restore&quot;
option without &quot;maximum&quot; for both &quot;aggregate&quot; and &quot;individual&quot; cases.
BTW, it is possible to specify &quot;no limitation&quot; at all for a pool
(i.e., no individual and aggregate limitation). The existing server
pools work similarly, allowing to specify &quot;none&quot; for all buckets
(without any warning message).

 &gt; The limitation configured with initial_fill_level= was previously
 &gt; configured through directive client_delay_initial_bucket_level.

&quot;client_delay_initial_bucket_level&quot; relates to only old client delay
pools.  I don't think we should reuse it in the new response pools: one
may need to configure both pools with different initial level parameter.


Thanks for the detailed review. I tried to address all other remarks,
renamed parameters according to the suggested terminology,
merged with latest v5 r15027 and re-attached the patch.


Eduard.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: SQUID-50-response-delay-pools-t3.patch
Type: text/x-patch
Size: 101019 bytes
Desc: not available
URL: &lt;<A HREF="http://lists.squid-cache.org/pipermail/squid-dev/attachments/20170131/7a9b66c7/attachment-0001.bin">http://lists.squid-cache.org/pipermail/squid-dev/attachments/20170131/7a9b66c7/attachment-0001.bin</A>&gt;
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="007811.html">[squid-dev] [PATCH] Response delay pools
</A></li>
	<LI>Next message: <A HREF="007809.html">[squid-dev] [PATCH] Mitigate DoS attacks that use client-initiated SSL/TLS renegotiation.
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7933">[ date ]</a>
              <a href="thread.html#7933">[ thread ]</a>
              <a href="subject.html#7933">[ subject ]</a>
              <a href="author.html#7933">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.squid-cache.org/listinfo/squid-dev">More information about the squid-dev
mailing list</a><br>
</body></html>
