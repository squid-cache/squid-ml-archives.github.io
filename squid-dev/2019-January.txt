From rousskov at measurement-factory.com  Tue Jan  8 03:58:22 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 7 Jan 2019 20:58:22 -0700
Subject: [squid-dev] [RFC] Do we want paranoid_hit_validation?
Message-ID: <b3cbdc99-b5ed-fa49-bea1-67d146182472@measurement-factory.com>

Hello,

    Squid has a few bugs that may result in rock cache corruption.
Factory is working on fixing those bugs. During that work, we have added
support for validating rock disk cache entry metadata at the time of a
cache hit.

This particular validation does not require checksums or other expensive
computations. It does not require disk I/O. The code simply traverses
the chain of disk slot metadata for the entry and compares the sum of
individual slot sizes with the expected total cache entry size. The
validation is able to detect many (but not all) cases of cache index
corruption.

A sketch of configuration directive to control this feature is quoted
further below. The initial (incomplete/unpolished but "working")
implementation is at
https://github.com/measurement-factory/squid/commit/c884c6d775f316e0c2962472fd9f9e7a7f86ff32


Should we add a polished version of this feature to Squid?

* Pros: Can detect and _bypass_ many cache index corruption cases.

* Cons: Requires quite a few extra CPU cycles for larger objects (they
have longer slot lists) but useless for correctly working code because
such code will never corrupt its index. It is essentially a
triage/troubleshooting feature.

The validation cost can be reduced by limiting the slot-scanning loop to
a few iterations by default (as opposed to just turning the feature off
by default) -- in most environments, most hits are small. However, the
sooner we interrupt the loop, the fewer corruption cases we can detect
(the probability of corruption may increases with the chain length).

It can be argued that since this kind of validation can be implemented
as a stand-alone tool (that scans shared memory indexes of a running
Squid), it should not be accepted into Squid [runtime code]. The counter
argument here is that such "external" metadata scans can be very lengthy
for large caches and are, hence, likely to miss fresh corruption cases
(that may have a higher probability of being accessed again!). The
built-in code has the advantage of being executed for disk cache hits
only and being able to convert corrupted hits into benign cache misses.

What do you think?


Thank you,

Alex.

------------- cf.data.pre ----------------
NAME: paranoid_hit_validation
DEFAULT: off
LOC: Config.onoff.paranoid_hit_validation
DOC_START
       Controls whether Squid should perform paranoid validation of
       cache entry metadata integrity every time a cache entry is hit.
       Squid bugs notwithstanding, this low-level validation should
       always succeed. Each failed validation results in a cache miss,
       a BUG line reported to cache.log, and the invalid entry marked
       as unusable (and eventually purged from the cache).
DOC_END

From squid3 at treenet.co.nz  Tue Jan  8 08:50:20 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Jan 2019 21:50:20 +1300
Subject: [squid-dev] [RFC] Do we want paranoid_hit_validation?
In-Reply-To: <b3cbdc99-b5ed-fa49-bea1-67d146182472@measurement-factory.com>
References: <b3cbdc99-b5ed-fa49-bea1-67d146182472@measurement-factory.com>
Message-ID: <5b79e4f1-eb62-8ca0-e16c-c8f5ddbee36a@treenet.co.nz>

On 8/01/19 4:58 pm, Alex Rousskov wrote:
> Hello,
> 
>     Squid has a few bugs that may result in rock cache corruption.
> Factory is working on fixing those bugs. During that work, we have added
> support for validating rock disk cache entry metadata at the time of a
> cache hit.
> 
> This particular validation does not require checksums or other expensive
> computations. It does not require disk I/O. The code simply traverses
> the chain of disk slot metadata for the entry and compares the sum of
> individual slot sizes with the expected total cache entry size. The
> validation is able to detect many (but not all) cases of cache index
> corruption.
> 
...

> What do you think?
> 

Does it have to be a global directive like proposed?

An option of cache_dir would seem better. That would allow admin to work
tune it to match their different cache types and object-size separation
(if any).

Amos

From rousskov at measurement-factory.com  Tue Jan  8 15:01:51 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jan 2019 08:01:51 -0700
Subject: [squid-dev] [RFC] Do we want paranoid_hit_validation?
In-Reply-To: <5b79e4f1-eb62-8ca0-e16c-c8f5ddbee36a@treenet.co.nz>
References: <b3cbdc99-b5ed-fa49-bea1-67d146182472@measurement-factory.com>
 <5b79e4f1-eb62-8ca0-e16c-c8f5ddbee36a@treenet.co.nz>
Message-ID: <ba2f3f32-4fa2-e1f3-ecc9-dda87e6d93ed@measurement-factory.com>

On 1/8/19 1:50 AM, Amos Jeffries wrote:
> On 8/01/19 4:58 pm, Alex Rousskov wrote:
>> This particular validation does not require checksums or other expensive
>> computations. It does not require disk I/O. The code simply traverses
>> the chain of disk slot metadata for the entry and compares the sum of
>> individual slot sizes with the expected total cache entry size. The
>> validation is able to detect many (but not all) cases of cache index
>> corruption.


> Does it have to be a global directive like proposed?

No, it does not. Each validation check only needs access to the index of
the storage where the hit object was found.


> An option of cache_dir would seem better. That would allow admin to work
> tune it to match their different cache types and object-size separation
> (if any).


Yes, this can be implemented as a cache_dir-specific (and, with even
more work, also as a cache_mem-specific) option. Do you think it is a
good idea to add this feature if it is controllable on individual
cache_dirs basis?


Thank you,

Alex.

From gkinkie at gmail.com  Mon Jan 14 22:20:19 2019
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 14 Jan 2019 14:20:19 -0800
Subject: [squid-dev] PVS Studio
Message-ID: <CA+Y8hcNoFyXDoScHOt=PN5iRRm3HWKZrauCzKvJSWDhkLtzWww@mail.gmail.com>

Hi all,
  the team behind PVS studio (static code analysis tool) has decided
to support FOSS projects for free (beer).
  https://hownot2code.com/2019/01/14/free-pvs-studio-for-those-who-develops-open-source-projects/
  Unless there are any concerns, I'll look into integrating our build
pipelines with that service in addition to Coverity.

-- 
    Francesco

From squid3 at treenet.co.nz  Tue Jan 15 03:02:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 Jan 2019 16:02:33 +1300
Subject: [squid-dev] [RFC] Do we want paranoid_hit_validation?
In-Reply-To: <ba2f3f32-4fa2-e1f3-ecc9-dda87e6d93ed@measurement-factory.com>
References: <b3cbdc99-b5ed-fa49-bea1-67d146182472@measurement-factory.com>
 <5b79e4f1-eb62-8ca0-e16c-c8f5ddbee36a@treenet.co.nz>
 <ba2f3f32-4fa2-e1f3-ecc9-dda87e6d93ed@measurement-factory.com>
Message-ID: <952a8ef3-96a1-0bc1-c3eb-ed04bd323129@treenet.co.nz>

On 9/01/19 4:01 am, Alex Rousskov wrote:
> On 1/8/19 1:50 AM, Amos Jeffries wrote:
>> On 8/01/19 4:58 pm, Alex Rousskov wrote:
>>> This particular validation does not require checksums or other expensive
>>> computations. It does not require disk I/O. The code simply traverses
>>> the chain of disk slot metadata for the entry and compares the sum of
>>> individual slot sizes with the expected total cache entry size. The
>>> validation is able to detect many (but not all) cases of cache index
>>> corruption.
> 
> 
>> Does it have to be a global directive like proposed?
> 
> No, it does not. Each validation check only needs access to the index of
> the storage where the hit object was found.
> 
> 
>> An option of cache_dir would seem better. That would allow admin to work
>> tune it to match their different cache types and object-size separation
>> (if any).
> 
> 
> Yes, this can be implemented as a cache_dir-specific (and, with even
> more work, also as a cache_mem-specific) option. Do you think it is a
> good idea to add this feature if it is controllable on individual
> cache_dirs basis?
> 

I think so yes. Long-term I would like to collate these types of tests
into a separate cache management tool. But short of that happening
having some way for Squid to do it is a good ting.

Amos

From rousskov at measurement-factory.com  Tue Jan 15 03:11:48 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Jan 2019 20:11:48 -0700
Subject: [squid-dev] PVS Studio
In-Reply-To: <CA+Y8hcNoFyXDoScHOt=PN5iRRm3HWKZrauCzKvJSWDhkLtzWww@mail.gmail.com>
References: <CA+Y8hcNoFyXDoScHOt=PN5iRRm3HWKZrauCzKvJSWDhkLtzWww@mail.gmail.com>
Message-ID: <b9599c34-ed16-bb3e-d472-58c21f0f92de@measurement-factory.com>

On 1/14/19 3:20 PM, Francesco Chemolli wrote:
> Hi all,
>   the team behind PVS studio (static code analysis tool) has decided
> to support FOSS projects for free (beer).
>   https://hownot2code.com/2019/01/14/free-pvs-studio-for-those-who-develops-open-source-projects/
>   Unless there are any concerns, I'll look into integrating our build
> pipelines with that service in addition to Coverity.

Coverity has not been integrated into our automated testing of pull
requests. I assume you want to look into similar-to-Coverity integration
for PVS studio, and I would welcome that kind of integration if your
initial tests are positive.


Thank you,

Alex.

From gkinkie at gmail.com  Tue Jan 15 12:03:32 2019
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 15 Jan 2019 04:03:32 -0800
Subject: [squid-dev] PVS Studio
In-Reply-To: <b9599c34-ed16-bb3e-d472-58c21f0f92de@measurement-factory.com>
References: <CA+Y8hcNoFyXDoScHOt=PN5iRRm3HWKZrauCzKvJSWDhkLtzWww@mail.gmail.com>
 <b9599c34-ed16-bb3e-d472-58c21f0f92de@measurement-factory.com>
Message-ID: <05A525EC-88AE-4968-9133-635E4D0F3F4B@gmail.com>



> On Jan 14, 2019, at 19:11, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 1/14/19 3:20 PM, Francesco Chemolli wrote:
>> Hi all,
>>  the team behind PVS studio (static code analysis tool) has decided
>> to support FOSS projects for free (beer).
>>  https://hownot2code.com/2019/01/14/free-pvs-studio-for-those-who-develops-open-source-projects/
>>  Unless there are any concerns, I'll look into integrating our build
>> pipelines with that service in addition to Coverity.
> 
> Coverity has not been integrated into our automated testing of pull
> requests. I assume you want to look into similar-to-Coverity integration
> for PVS studio, and I would welcome that kind of integration if your
> initial tests are positive.

Coverity cannot be fully integrated in the pull request workflow: in order not to squander resources on their cloud offering, they ask (do not enforce, though) not to do more than a scan per week. In other words, what we do now is the best we can do with their free tool.
I’ve rechecked PVS studio; their ask for FOSS projects is that a comment be added to every single file acknowledging their support, and I do not think that’s a fair ask of them, so I’m stopping the effort.
I’ve looked into other similar static check tools such as Facebook’s Infer, it’s a pain in the neck to build. clang static analyzer (https://clang-analyzer.llvm.org/) may be our best option if we want per-commit static code analysis.

	Francesco

From rousskov at measurement-factory.com  Tue Jan 15 15:10:41 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Jan 2019 08:10:41 -0700
Subject: [squid-dev] PVS Studio
In-Reply-To: <05A525EC-88AE-4968-9133-635E4D0F3F4B@gmail.com>
References: <CA+Y8hcNoFyXDoScHOt=PN5iRRm3HWKZrauCzKvJSWDhkLtzWww@mail.gmail.com>
 <b9599c34-ed16-bb3e-d472-58c21f0f92de@measurement-factory.com>
 <05A525EC-88AE-4968-9133-635E4D0F3F4B@gmail.com>
Message-ID: <a90fbf70-3f18-7a15-a2db-09bf2525f439@measurement-factory.com>

On 1/15/19 5:03 AM, Francesco Chemolli wrote:

>> On Jan 14, 2019, at 19:11, Alex Rousskov wrote:
>> On 1/14/19 3:20 PM, Francesco Chemolli wrote:
>>>  the team behind PVS studio (static code analysis tool) has decided
>>> to support FOSS projects for free (beer).
>>>  https://hownot2code.com/2019/01/14/free-pvs-studio-for-those-who-develops-open-source-projects/
>>>  Unless there are any concerns, I'll look into integrating our build
>>> pipelines with that service in addition to Coverity.

>> Coverity has not been integrated into our automated testing of pull
>> requests. I assume you want to look into similar-to-Coverity integration
>> for PVS studio, and I would welcome that kind of integration if your
>> initial tests are positive.

> Coverity cannot be fully integrated in the pull request workflow

We have discussed that already. As you know, I disagree that Coverity
restrictions on the number of scans preclude integration of their
service into our pull request tests. Please do not misinterpret my
statement as a veiled push for that integration! I just do not want a
false premise to guide our future decisions.


> I’ve rechecked PVS studio; their ask for FOSS projects is that a
> comment be added to every single file acknowledging their support,

FYI: The following PVS post contradicts your assertion. It sounds like
what you are referring two is a different (old) licensing scheme and
they now have a new one (in addition to the old one):

https://www.viva64.com/en/b/0600/

Alex.

From eliezer at ngtech.co.il  Wed Jan 16 21:22:38 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Wed, 16 Jan 2019 23:22:38 +0200
Subject: [squid-dev] Securtiy_file_gen in a server format development
In-Reply-To: <043e54e0-16f8-9507-847e-77b8979af4a3@measurement-factory.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC1bysTSnlkQKZBElmMsLU7AQAAAAA=@ngtech.co.il>
 <043e54e0-16f8-9507-847e-77b8979af4a3@measurement-factory.com>
Message-ID: <00b301d4ade1$9b7722b0$d2656810$@ngtech.co.il>

OK.

So I would try to run a test of three squid 4.4 instances with a NFS share for /var/squid/ssl_db ontop of NFSv3 and NFSv4.
If the network speed is fast then it's a nice thing to have on a busy cluster to reduce the load of encryption "part" of the CPU (if it's worth something).

The use case of logging certificate generation and couple other related features is security "auditing".
Like SELinux has audit log that can help to decide on specific actions, in a similar way some organizations.

If security auditing is not enough to allow investigation of content leakage or some other scenario I cannot think about another option.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-dev <squid-dev-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Sunday, December 30, 2018 19:08
To: squid-dev at lists.squid-cache.org
Subject: Re: [squid-dev] Securtiy_file_gen in a server format development

On 12/29/18 11:45 PM, Eliezer Croitoru wrote:

> From what I understood until now it seems that the current ssl_db
> directory structure is simple enough that it might be possible to share
> it across a NFS store.

I would expect NFS store to work in environments that support file
locking over NFS. For example, NFS flock(2) does not work with Linux
kernels up to v2.6.11. For the list of environment-specific file locking
system calls used by the certificate generator, see Ssl::Lock::lock().


> Since squid is being used in couple locations as a security software it
> would be good for security admins to be able to have some history logs.

The generated certificate database is just an optimization/cache.
Logging certificate cache operations would probably be as useful/useless
as store.log is for the HTTP cache. It would be good to discuss and
target some specific use cases before designing where and how to log
certificate operations.

Alex.

_______________________________________________
squid-dev mailing list
squid-dev at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-dev


From rousskov at measurement-factory.com  Wed Jan 16 22:04:39 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Jan 2019 15:04:39 -0700
Subject: [squid-dev] Securtiy_file_gen in a server format development
In-Reply-To: <00b301d4ade1$9b7722b0$d2656810$@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC1bysTSnlkQKZBElmMsLU7AQAAAAA=@ngtech.co.il>
 <043e54e0-16f8-9507-847e-77b8979af4a3@measurement-factory.com>
 <00b301d4ade1$9b7722b0$d2656810$@ngtech.co.il>
Message-ID: <267c1de1-604e-5cdc-4f02-8a1f6988966e@measurement-factory.com>

On 1/16/19 2:22 PM, eliezer at ngtech.co.il wrote:

> The use case of logging certificate generation [...] is security "auditing".

I doubt proper security auditing should rely on the log of _second-level
cache_ operations. If you do want to add logging, can you detail your
specific needs a little? Perhaps give a couple of specific usage
examples that are poorly addressed by current access.log information
_and_ should not be addressed by adding more access.log fields.


Thank you,

Alex.


> -----Original Message-----
> From: squid-dev <squid-dev-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
> Sent: Sunday, December 30, 2018 19:08
> To: squid-dev at lists.squid-cache.org
> Subject: Re: [squid-dev] Securtiy_file_gen in a server format development
> 
> On 12/29/18 11:45 PM, Eliezer Croitoru wrote:
> 
>> From what I understood until now it seems that the current ssl_db
>> directory structure is simple enough that it might be possible to share
>> it across a NFS store.
> 
> I would expect NFS store to work in environments that support file
> locking over NFS. For example, NFS flock(2) does not work with Linux
> kernels up to v2.6.11. For the list of environment-specific file locking
> system calls used by the certificate generator, see Ssl::Lock::lock().
> 
> 
>> Since squid is being used in couple locations as a security software it
>> would be good for security admins to be able to have some history logs.
> 
> The generated certificate database is just an optimization/cache.
> Logging certificate cache operations would probably be as useful/useless
> as store.log is for the HTTP cache. It would be good to discuss and
> target some specific use cases before designing where and how to log
> certificate operations.
> 
> Alex.
> 
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev
> 
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev
> 


From squid3 at treenet.co.nz  Sun Jan 27 08:41:22 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 Jan 2019 21:41:22 +1300
Subject: [squid-dev] Squid-5 status update
Message-ID: <e3094579-c62a-8a46-39e5-2a8291e217ae@treenet.co.nz>

Hi all,

 So being January and fielding questions about when v5 will be released
I have taken a look at the state of trunk/HEAD/master code to see
whether or not there is enough change to be worth a new Squid series.

Right now things are looking close but not quite enough.


The details I am looking at right now:

 * 8 new functionality features. Either specifically named features
  that have their own release notes section, or a bunch of related
  UI setting changes that add together to be noteworthy.

  see <https://wiki.squid-cache.org/Squid-5> for the list as of today.

 Ideally what I look for is an arbitrary 10 features. So this is close
enough, but also has room to wait for more.


 * approx. 30k LOC unique to master/v5

Historically there have been around 100k per year. v4 was an oddity with
having to wait for compiler support and some major late-added features.

Higher LOC change tends to mean we have made a lot of progress on the
code polishing and features added actually are significant. If we let it
get too high before beta the expected bug count goes up likewise and
beta testing takes a lot longer.
 If the v3 past is a good indicator of future bug finds, then this 30k
LOC would indicate 8-12 months of beta ahead already.


 * the LOC changes appear to be roughly evenly split in this past two
years between v5 changes, v4 fixes.

This is quite a bit higher than previous cycles and a good indicator
that we are not really ready to stop working on those fixes and move on
to testing stability of the new v5 code.


 * ~6 weeks to accumulate changes sufficient to pass the arbitrary
watermark for new packaging to be worthwhile.

Ideally that series would be stable enough that it takes regularly 8
week turnarounds before focus could be switched to a new series.


These last two are the main reasons I have now to delay starting v5
beta. Next look in a few months to see if/how things have changed.

That said we are getting close, so please start considering what
features you want to finish off to get included.


Amos


