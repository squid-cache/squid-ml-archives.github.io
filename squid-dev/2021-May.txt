From dsuhpublic at gmail.com  Sat May  1 00:04:16 2021
From: dsuhpublic at gmail.com (dsuh)
Date: Fri, 30 Apr 2021 17:04:16 -0700
Subject: [squid-dev] Any code pointers to get sending certificate chain
 from squid reverse proxy with gnutls?
In-Reply-To: <CABWAcDuVUKkxkUGZUmvRhWANtWZ0nvScHYjPCOoG1vYe5s+WbA@mail.gmail.com>
References: <CABWAcDuVUKkxkUGZUmvRhWANtWZ0nvScHYjPCOoG1vYe5s+WbA@mail.gmail.com>
Message-ID: <CABWAcDsQnp3LoGi6RkVt0xQL4LreBDhpt2pofg1uRzYa2L90bw@mail.gmail.com>

I found the fork that fixes this from the end of 2019 and it works for me.
Not sure when this will be merged into master, but this fork is usable for
me.

https://github.com/squid-cache/squid/pull/458
https://github.com/yadij/squid/tree/v5-gnutls-chainload

On Wed, Apr 28, 2021 at 11:09 PM dsuh <dsuhpublic at gmail.com> wrote:

> I have hit a wall as I want to listen on one https_port for 3 different
> server key/cert chains.
> Previous message about 2 years ago says this has not been implemented yet.
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/sending-certificate-chain-from-squid-reverse-proxy-td4687986.html
>
> I am looking at what it would take to get cert chains working.
> I think the credentials are set for gnutls TLS handshake
> with gnutls_credentials_set() call in CreateSession() in Session.cc
> So, I think that ctx is used for session data for gnutls and also for
> context data for openssl?
> I think I kind of got lost on how I can make sure a cert chain (instead of
> just the server cert) is set in ctx.
> Any direction on where the server cert chain should be set for gnutls TLS
> handshake would be appreciated.
>
> David Suh
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210430/8b459d01/attachment.htm>

From gkinkie at gmail.com  Mon May  3 04:41:49 2021
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 3 May 2021 06:41:49 +0200
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
Message-ID: <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>

On Wed, Apr 28, 2021 at 11:34 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 4/28/21 5:12 PM, Amos Jeffries wrote:
> > I'm not sure why this is so controversial still. We have already been
> > over these and have a policy from last time:
>
> Apparently, the recollections of what was agreed upon, if anything,
> during that "last time" differ. If you can provide a pointer to that
> "last time" agreement, please do so.
>

Merge workflows are agreed, and not in discussion. Recent discussions have
highlighted some issues with what's around them, and I'm trying to clarify
that as well

> * dev PR submissions use the volatile 5-pr-test, after test approval by
> > anyone in QA. Check against unstable OS nodes, as many as possible.
> > Kinkie adds/removes from that set as upgrades fix or break at CI end of
> > things.
>
> I do not know how to interpret the last sentence correctly, but, IMO, we
> should not add or change nodes if doing so breaks master tests. AFAICT
> from PR 806 discussion[1], Francesco thinks that it is not a big deal to
> do so. The current discussion is meant to resolve that disagreement.
>

Let me highlight the underlying principles for my proposal: IMO our
objectives are, in descending order of importance (all points should be
intended "as possible given our resources"):
1. ensure we ship healthy code to a maximum number of users
2. have minimal friction in the development workflow

These objectives have a set of consequences:
- we want our QA environment to match what users will use. For this reason,
it is not sensible that we just stop upgrading our QA nodes, or we would
target something that doesn't match our users' experience
- it makes little sense to target unstable distributions (fedora rawhide,
possibly centos stream, gentoo, opensuse tumbleweed, debian unstable) as
first-class citizens of the testing workflow, especially on stages that are
executed often (pr-test)

This means that:
- I periodically weed out distributions that are no longer supported (e.g.
Fedora 31, Ubuntu Xenial) and add current distribution (e.g. Ubuntu
Hirsute, Fedora 34).
I take it on me that when I do that, I need to ensure new compiler features
do not block previously undetected behaviours - I am currently failing
this, see https://build.squid-cache.org/job/trunk-matrix/121/ . I will need
to develop a process with a proper staging phase.
- I believe we should define four tiers of runtime environments, and
reflect these in our test setup:
 1. current and stable (e.g. ubuntu-latest-lts). These are not expected to
change much over a span of years, and to offer non-breaking updates over
their lifetime
 2. current (e.g. fedora 34)
 3. bleeding edge: they may introduce breaking changes which it makes sense
to follow because they might highlight real issues and because they will
eventually trickle down to current and then lts
 4. everything else - this includes freebsd and openbsd (mostly due to the
different virtualization tech they use)

I believe we should focus on the first two tiers for our merge workflow,
but then expect devs to fix any breakages in the third and fourth tiers if
caused by their PR, while I will care for any breakages caused by
dist-upgrades


> [1] https://github.com/squid-cache/squid/pull/806#issuecomment-827937563
>
>
> > * anubis auto branch tested by curated set of LTS stable nodes only.
>
> FWIW, the above list and the original list by Francesco appears to focus
> on distro stability, popularity, and other factors that are largely
> irrelevant to the disagreement at hand. The disagreement is whether it
> is OK to break master (and, hence, all PR) tests by changing CI. It does
> not matter whether that CI change comes from an upgrade of an "LTS
> stable node", "unstable node", or some other source IMO. Breaking
> changes should not be allowed (in the CI environments under our
> control). If they slip through despite careful monitoring for change
> effects, the breaking CI changes should be reverted.
>

I think it depends.
Breakages due to changes in nodes (e.g. introducing a new distro version)
would be on me and would not stop the merge workflow.
What I would place on each individual dev is the case where a PR breaks
something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
trunk-openbsd-matrix, trunk-freebsd-matrix builds, even if the 5-pr-test
and 5-pr-auto builds fail to detect the breakage because it happens on a
unstable or old platform.

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210503/17ff450a/attachment.htm>

From rousskov at measurement-factory.com  Mon May  3 14:29:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 3 May 2021 10:29:31 -0400
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
Message-ID: <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>

On 5/3/21 12:41 AM, Francesco Chemolli wrote:
> - we want our QA environment to match what users will use. For this
> reason, it is not sensible that we just stop upgrading our QA nodes, 

I see flaws in reasoning, but I do agree with the conclusion -- yes, we
should upgrade QA nodes. Nobody has proposed a ban on upgrades AFAICT!

The principles I have proposed allow upgrades that do not violate key
invariants. For example, if a proposed upgrade would break master, then
master has to be changed _before_ that upgrade actually happens, not
after. Upgrades must not break master.

What this means in terms of sysadmin steps for doing upgrades is up to
you. You are doing the hard work here, so you can optimize it the way
that works best for _you_. If really necessary, I would not even object
to trial upgrades (that may break master for an hour or two) as long as
you monitor the results and undo the breaking changes quickly and
proactively (without relying on my pleas to fix Jenkins to detect
breakages). I do not know what is feasible and what the best options
are, but, again, it is up to _you_ how to optimize this (while observing
the invariants).


> - I believe we should define four tiers of runtime environments, and
> reflect these in our test setup:

>  1. current and stable (e.g. ubuntu-latest-lts).
>  2. current (e.g. fedora 34)
>  3. bleeding edge
>  4. everything else - this includes freebsd and openbsd

I doubt this classification is important to anybody _outside_ this
discussion, so I am OK with whatever classification you propose to
satisfy your internal needs.


> I believe we should focus on the first two tiers for our merge workflow,
> but then expect devs to fix any breakages in the third and fourth tiers
> if caused by their PR,

FWIW, I do not understand what "focus" implies in this statement, and
why developers should _not_ "fix any breakages" revealed by the tests in
the first two tiers.

The rules I have in mind use two natural tiers:

* If a PR cannot pass a required CI test, that PR has to change before
it can be merged.

* If a PR cannot pass an optional CI test, it is up to PR author and
reviewers to decide what to do next.

These are very simple rules that do not require developer knowledge of
any complex test node tiers that we might define/use internally.

Needless to say, the rules assume that the tests themselves are correct.
If not, the broken tests need to be fixed (by the Squid Project) before
the first bullet/rule above can be meaningfully applied (the second one
is flexible enough to allow PR author and reviewers to ignore optional
test failures).


> Breakages due to changes in nodes (e.g. introducing a new distro
> version) would be on me and would not stop the merge workflow.

What you do internally to _avoid_ breakage is up to you, but the primary
goal is to _prevent_ CI breakage (rather than to keep CI nodes "up to
date"!). If CI is broken, then it is the responsibility of the Squid
Project to fix CI ASAP. Thank you for assuming that responsibility as
far as Jenkins tests are concerned.

There are many ways to break CI and detect those breakages, of course,
but if master cannot pass required tests after a CI change, then the
change broke CI.


> What I would place on each individual dev is the case where a PR breaks
> something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
> trunk-openbsd-matrix, trunk-freebsd-matrix builds, even if the 5-pr-test
> and 5-pr-auto builds fail to detect the breakage because it happens on a
> unstable or old platform. 

This feels a bit out of topic for me, but I think you are saying that
some CI tests called trunk-matrix, trunk-arm32-matrix,
trunk-arm64-matrix, trunk-openbsd-matrix, trunk-freebsd-matrix should be
classified as _required_. In other words, a PR must pass those CI tests
before it can be merged. Is that the situation today? Or are you
proposing some changes to the list of required CI tests? What are those
changes?


Thank you,

Alex.

From squid3 at treenet.co.nz  Sun May 16 07:31:45 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 16 May 2021 19:31:45 +1200
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
Message-ID: <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>

On 4/05/21 2:29 am, Alex Rousskov wrote:
> On 5/3/21 12:41 AM, Francesco Chemolli wrote:
>> - we want our QA environment to match what users will use. For this
>> reason, it is not sensible that we just stop upgrading our QA nodes,
> 
> I see flaws in reasoning, but I do agree with the conclusion -- yes, we
> should upgrade QA nodes. Nobody has proposed a ban on upgrades AFAICT!
> 
> The principles I have proposed allow upgrades that do not violate key
> invariants. For example, if a proposed upgrade would break master, then
> master has to be changed _before_ that upgrade actually happens, not
> after. Upgrades must not break master.

So ... a node is added/upgraded. It runs and builds master fine. Then 
added to the matrices some of the PRs start failing.

*THAT* is the situation I see happening recently. Master itself working 
fine and "huge amounts of pain, the sky is falling" complaints from a 
couple of people.

Sky is not falling. Master is no more nor less broken and buggy than it 
was before sysadmin touched Jenkins.

The PR itself is no more, nor less, "broken" than it would be if for 
example - it was only tested on Linux nodes and fails to compile on 
Windows. As the case for master *right now* happens to be.


> 
> What this means in terms of sysadmin steps for doing upgrades is up to
> you. You are doing the hard work here, so you can optimize it the way
> that works best for _you_. If really necessary, I would not even object
> to trial upgrades (that may break master for an hour or two) as long as
> you monitor the results and undo the breaking changes quickly and
> proactively (without relying on my pleas to fix Jenkins to detect
> breakages). I do not know what is feasible and what the best options
> are, but, again, it is up to _you_ how to optimize this (while observing
> the invariants).
> 

Uhm. Respectfully, from my perspective the above paragraph conflicts 
directly with actions taken.

 From what I can tell kinkie (as sysadmin) *has* been making a new node 
and testing it first. Not just against master but the main branches and 
most active PRs before adding it for the *post-merge* matrix testing 
snapshot production.

   But still threads like this one with complaints appear.



I understand there is some specific pain you have encountered to trigger 
the complaint. Can we get down to documenting as exactly as possible 
what the particular pain was?

  Much of the processes we are discussing are scripted automation not 
human processing mistakes. Handling such pain points as bugs with 
bugzilla "Project" section would be best. Re-designing the entire system 
policy just moves us all to another set of unknown bugs when the scripts 
are re-coded to meet that policy.


> 
>> - I believe we should define four tiers of runtime environments, and
>> reflect these in our test setup:
> 
>>   1. current and stable (e.g. ubuntu-latest-lts).
>>   2. current (e.g. fedora 34)
>>   3. bleeding edge
>>   4. everything else - this includes freebsd and openbsd
> 
> I doubt this classification is important to anybody _outside_ this
> discussion, so I am OK with whatever classification you propose to
> satisfy your internal needs.
> 

IIRC this is the 5th iteration of ground-up redesign for this wheel.

Test designs that do not fit into our merge and release process sequence 
have proven time and again to be broken and painful to Alex when they 
operate as-designed. For the rest of us it is this constant re-build of 
automation which is the painful part.


A. dev pre-PR testing
    - random individual OS.
    - matrix of everything (anybranch-*-matrix)

B. PR submission testing
    - which OS for master (5-pr-test) ?
    - which OS for beta (5-pr-test) ?
    - which OS for stable (5-pr-test) ?

Are all of those sets the same identical OS+compilers? no.
Why are they forced to be the same matrix test?
   IIRC, policy forced on sysadmin with previous pain complaints.

Are we getting painful experiences from this?
   Yes. Lack of branch-specific testing before D on beta and stable 
causes those branches to break a lot more often at last-minute before 
releases than master. Adding random days/weeks to each scheduled release.


C. merge testing
    - which OS for master (5-pr-auto) ?
    - which OS for beta (5-pr-auto) ?
    - which OS for stable (5-pr-auto) ?
      NP: maintainer does manual override on beta/stable merges.

Are all of those sets the same identical OS+compilers? no.
   Why are they forced to be the same matrix test? Anubis

Are we getting painful experiences from this? yes. see (B).


D. pre-release testing (snapshots + formal)
    - which OS for master (trunk-matrix) ?
    - which OS for beta (5-matrix) ?
    - which OS for stable (4-matrix) ?

Are all of those sets the same identical OS+compilers? no.
Are we forcing them to use the same matrix test? no.
Are we getting painful experiences from this? maybe.
   Most loud complaints have been about "breaking master" which is the 
most volatile branch testing on the most volatile OS.



FTR: the reason all those matrices have '5-' prefix is because several 
redesigns ago the system was that master/trunk had a matrix which the 
sysadmin added nodes to as OS upgraded. During branching vN the 
maintainer would clone/freeze that matrix into an N-foo which would be 
used to test the code against OS+compilers which the code in the vN 
branch was designed to build on.


Can we have the people claiming pain specify exactly what the pain is 
coming from, and let the sysadmin/developer(s) with specialized 
knowledge of the automation in that area decide how best to fix it?


> 
>> I believe we should focus on the first two tiers for our merge workflow,
>> but then expect devs to fix any breakages in the third and fourth tiers
>> if caused by their PR,
> 
> FWIW, I do not understand what "focus" implies in this statement, and
> why developers should _not_ "fix any breakages" revealed by the tests in
> the first two tiers.
> 
> The rules I have in mind use two natural tiers:
> 
> * If a PR cannot pass a required CI test, that PR has to change before
> it can be merged.
> 
> * If a PR cannot pass an optional CI test, it is up to PR author and
> reviewers to decide what to do next.

That is already the case. Already well documented and understood.

I see no need to change anything based on those criteria. Ergo you have 
some undeclared criteria leading to whatever pain triggered this 
discussion. Maybe the pain is some specific bug that does not need a 
whole discussion and re-design by committee?


> 
> These are very simple rules that do not require developer knowledge of
> any complex test node tiers that we might define/use internally.
> 

This is the first I've heard about dev having to have such knowledge. 
Maybe because they are already *how we do things*. Red-herring argument?


> Needless to say, the rules assume that the tests themselves are correct.
> If not, the broken tests need to be fixed (by the Squid Project) before
> the first bullet/rule above can be meaningfully applied (the second one
> is flexible enough to allow PR author and reviewers to ignore optional
> test failures).
> 

There is a hidden assumption here too. About the test being applied 
correctly.

I posit that is the real bug we need to sort out. We could keep on 
"correcting" the node sets (aka tests) back and forward between being 
suitable for master or suitable for release branches. That just shuffles 
the pain from one end of the system to the other.

Make Anubis and Jenkins use different matrix for each branch at the B 
and C process stages above. Only then will discussion of what nodes to 
add to what test/matrix actually make progress.



> 
>> Breakages due to changes in nodes (e.g. introducing a new distro
>> version) would be on me and would not stop the merge workflow.
> 
> What you do internally to _avoid_ breakage is up to you, but the primary
> goal is to _prevent_ CI breakage (rather than to keep CI nodes "up to
> date"!).

The principle ("invariant" in Alex terminology?) with nodes is that they 
represent the OS environment a typical developer can be assumed to be 
running on that OS version+compiler combination.

Distros release security updates to their "stable" versions. Therefore 
to stay true to the goal we require constant small upgrades as an 
ongoing part of sysadmin maintenance.

Adding new nodes with next distro release versions is a manual process 
not related to keeping existing nodes up to date (which is automated?).

 From time to time distros break their own ability to compile things. 
This is to be expected on distros with rolling release and ironically 
LTS release (which get *less* testing of updates than normal releases).
It does not indicate "broken master" nor "broken CI" in any way.


> 
> There are many ways to break CI and detect those breakages, of course,
> but if master cannot pass required tests after a CI change, then the
> change broke CI.

I have yet to see the code in master be corrupted by CI changes in such 
a way that it could not build on peoples development machines.

What we do have going on is network timeouts, DNS resolution, CPU wait 
timeouts, and rarely _automated_ CI upgrades all causing short-term 
failure to pass a test.

A PR fixing newly highlighted bugs gets around the latter. Any pain (eg 
master blocked for 2 days waiting on the fix PR to merge) is a normal 
problem with that QA process and should not be attributed to the CI change.


> 
>> What I would place on each individual dev is the case where a PR breaks
>> something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
>> trunk-openbsd-matrix, trunk-freebsd-matrix builds, even if the 5-pr-test
>> and 5-pr-auto builds fail to detect the breakage because it happens on a
>> unstable or old platform. >
> This feels a bit out of topic for me, but I think you are saying that
> some CI tests called trunk-matrix, trunk-arm32-matrix,
> trunk-arm64-matrix, trunk-openbsd-matrix, trunk-freebsd-matrix should be
> classified as _required_.

That is how I read the statement too.

> In other words, a PR must pass those CI tests
> before it can be merged. Is that the situation today? Or are you
> proposing some changes to the list of required CI tests? What are those
> changes?
> 

No, situation today is that those matrix are new ones only recently 
created by sysadmin and not used for any of the merge or release process 
criteria. The BSD though were once checked in the general 5-pr-test 
required for PR testing.


IMO, it's a good point. We do need to stop the practice of just dropping 
support for any OS where attempting to build finds existing bugs in 
master (aka "breaks master, sky falling"). More focus on fixing those 
bugs to increase portability and grow the Squid community beyond the 
subset of RHEL and Ubuntu users.


Amos

From rousskov at measurement-factory.com  Sun May 16 23:56:01 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 16 May 2021 19:56:01 -0400
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
Message-ID: <5d1ec3b0-f266-8397-4766-69269670d21b@measurement-factory.com>

On 5/16/21 3:31 AM, Amos Jeffries wrote:
> On 4/05/21 2:29 am, Alex Rousskov wrote:
>> On 5/3/21 12:41 AM, Francesco Chemolli wrote:
>>> - we want our QA environment to match what users will use. For this
>>> reason, it is not sensible that we just stop upgrading our QA nodes,
>>
>> I see flaws in reasoning, but I do agree with the conclusion -- yes, we
>> should upgrade QA nodes. Nobody has proposed a ban on upgrades AFAICT!
>>
>> The principles I have proposed allow upgrades that do not violate key
>> invariants. For example, if a proposed upgrade would break master, then
>> master has to be changed _before_ that upgrade actually happens, not
>> after. Upgrades must not break master.
> 
> So ... a node is added/upgraded. It runs and builds master fine. Then
> added to the matrices some of the PRs start failing.

It is easy to misunderstand what is going on because there is no good
visualization of complex PR-master-Jenkins_nodes-Jenkins_failures
relationships. Several kinds of PR test failures are possible. I will
describe the two most relevant to your email:

* PR test failures due to problems introduced by PRs should be welcomed
at any time. CI improvements are allowed to find new bugs in open PRs.
Such findings, even when discovered at the "last minute", should be seen
as an overall positive event or progress -- our CI was able to identify
a problem before it got officially accepted! I do not recall anybody
complaining about such failures recently.

* PR test failures due to the existing master code are not welcomed.
They represent a CI failure. In these cases, if the latest master code
is tested with the same test after the problematic CI change, then that
master test will fail. Nothing a PR can do in this situation can fix
this kind of failure because it is not PR changes that are causing the
failure -- CI changes broke the master branch, not just the PR! This
kind of failures are the responsibility of CI administrators, and PR
authors should complain about them, especially when there are no signs
of CI administrators aware of and working on addressing the problem.

A good example of a failure of the second kind a -Wrange-loop-construct
error in a PR that does not touch any range loops (Jenkins conveniently
deleted the actual failed test, but my GitHub comment and PR contents
may be enough to restore what happened):
https://github.com/squid-cache/squid/pull/806#issuecomment-827924821


[I am skipping the exaggerations (about other people exaggerations) that
were, AFAICT, driven by mis-classifying the failures of the second kind
as regular PR failures of the first kind.]


>> What this means in terms of sysadmin steps for doing upgrades is up to
>> you. You are doing the hard work here, so you can optimize it the way
>> that works best for _you_. If really necessary, I would not even object
>> to trial upgrades (that may break master for an hour or two) as long as
>> you monitor the results and undo the breaking changes quickly and
>> proactively (without relying on my pleas to fix Jenkins to detect
>> breakages). I do not know what is feasible and what the best options
>> are, but, again, it is up to _you_ how to optimize this (while observing
>> the invariants).


> Uhm. Respectfully, from my perspective the above paragraph conflicts
> directly with actions taken.

My paragraph is not really about any taken actions so there cannot be
any such conflict AFAICT.


> From what I can tell kinkie (as sysadmin) *has* been making a new node
> and testing it first. Not just against master but the main branches and
> most active PRs before adding it for the *post-merge* matrix testing
> snapshot production.

The above summary does not match the (second kind of) PR test failures
that I have observed and asked Francesco to address. Those failures were
triggered by the latest master code, not PR changes. No PR changes would
have fixed those test failures. In fact, an "empty" PR that introduces
no code changes at all would have failed as well. See above for one
recent example.


>   But still threads like this one with complaints appear.

This squid-dev thread did not contain any complaints AFAICT. Francesco
is trying to get consensus on how to handle CI upgrades/changes. He is
doing the right thing and nobody is complaining about it.


> I understand there is some specific pain you have encountered to trigger
> the complaint. Can we get down to documenting as exactly as possible
> what the particular pain was?

Well, I apparently do not know what you call "complaint", so I cannot
answer this exact question, but if you are looking for a recent PR test
failure that triggered this squid-dev thread, then please see the GitHub
link above.



> Test designs that do not fit into our merge and release process sequence
> have proven time and again to be broken and painful to Alex when they
> operate as-designed. For the rest of us it is this constant re-build of
> automation which is the painful part.

While I am not sure what you are talking about specifically, I think
that any design that causes pain should be changed. Your phrasing
suggests some dissatisfaction with that natural (IMO) expectation, so I
am probably missing something. Please rephrase if this is important.


> B. PR submission testing
>    - which OS for master (5-pr-test) ?
>    - which OS for beta (5-pr-test) ?
>    - which OS for stable (5-pr-test) ?
> 
> Are all of those sets the same identical OS+compilers? no.
> Why are they forced to be the same matrix test?

I do not understand the question. Are you asking why Jenkins uses the
same 5-pr-test configuration for all three branches (master, beta, _and_
stable)? I do not know the answer.


> IIRC, policy forced on sysadmin with previous pain complaints.

Complaints, even legitimate ones, should not shape a policy. Goals and
principles should do that.

I remember one possibly related discussion where we were trying to
reduce Jenkins/PR wait times by changing which tests are run at what PR
merging stages, but that is probably a different issue because your
question appears to be about a single merging stage.


> C. merge testing
>    - which OS for master (5-pr-auto) ?
>    - which OS for beta (5-pr-auto) ?
>    - which OS for stable (5-pr-auto) ?
>      NP: maintainer does manual override on beta/stable merges.
> 
> Are all of those sets the same identical OS+compilers? no.
>   Why are they forced to be the same matrix test? Anubis

This is too cryptic for me to understand, but Anubis does not force any
tests on anybody -- it simply checks that the required tests have
passed. I am not aware of any Anubis bugs in this area, but please
correct me if I am wrong.


> D. pre-release testing (snapshots + formal)
>    - which OS for master (trunk-matrix) ?
>    - which OS for beta (5-matrix) ?
>    - which OS for stable (4-matrix) ?
> 
> Are all of those sets the same identical OS+compilers? no.
> Are we forcing them to use the same matrix test? no.
> Are we getting painful experiences from this? maybe.
>   Most loud complaints have been about "breaking master" which is the
> most volatile branch testing on the most volatile OS.

FWIW, I think you misunderstood what those "complaints" where about. I
do not know how that relates to the above questions/answers though.


> FTR: the reason all those matrices have '5-' prefix is because several
> redesigns ago the system was that master/trunk had a matrix which the
> sysadmin added nodes to as OS upgraded. During branching vN the
> maintainer would clone/freeze that matrix into an N-foo which would be
> used to test the code against OS+compilers which the code in the vN
> branch was designed to build on.

I think the above description implies that some time ago we were (more)
careful about (not) adding new nodes when testing stable branches. We
did not want a CI change to break a stable branch. That sounds like the
right principle to me (and it should apply to beta and master as well).
How that specific principle is accomplished is not important (to me) so
CI admins should propose whatever technique they think is best.


> Can we have the people claiming pain specify exactly what the pain is
> coming from, and let the sysadmin/developer(s) with specialized
> knowledge of the automation in that area decide how best to fix it?

We can, and that is exactly what is going on in this thread AFAICT. This
particular thread was caused by CI changes breaking master, and
Francesco was discussing how to avoid such breakages in the future.

There are other goals/principles to observe, of course, and it is
possible that Francesco is proposing more changes to optimize something
else as well, but that is something only he can clarify (if needed).

AFAICT, Francesco and I are on the same page regarding not breaking
master anymore -- he graciously agreed to prevent such breakages in the
future, and I am very thankful that he did. Based on your comments
discussing several cases where such master breakage is, in your opinion,
OK, you currently disagree with that principle. I do not know why.


>> I believe we should focus on the first two tiers for our merge workflow,
>> but then expect devs to fix any breakages in the third and fourth tiers
>> if caused by their PR, 

>> The rules I have in mind use two natural tiers:
>>
>> * If a PR cannot pass a required CI test, that PR has to change before
>> it can be merged.
>>
>> * If a PR cannot pass an optional CI test, it is up to PR author and
>> reviewers to decide what to do next.


> That is already the case. Already well documented and understood.

> I see no need to change anything based on those criteria. 

I hope so! I stated those rules in direct response to, what seemed to me
like, a far more complex 4-tier proposal by Francesco. I did not state
them to change some Project policy. I state them in hope to understand
(and/or simplify) his proposal.


>> These are very simple rules that do not require developer knowledge of
>> any complex test node tiers that we might define/use internally.

> This is the first I've heard about dev having to have such knowledge.

Perhaps you interpreted the email I was replying to differently than I
did. I hope the above paragraph clarifies the situation.


>> Needless to say, the rules assume that the tests themselves are correct.
>> If not, the broken tests need to be fixed (by the Squid Project) before
>> the first bullet/rule above can be meaningfully applied (the second one
>> is flexible enough to allow PR author and reviewers to ignore optional
>> test failures).

> There is a hidden assumption here too. About the test being applied
> correctly.

For me, test application (i.e. what tests are applied to what software)
correctness is a part of the overall tests correctness (which naturally
has many components). From that point of view, there is no hidden
assumption here.


> I posit that is the real bug we need to sort out. We could keep on
> "correcting" the node sets (aka tests) back and forward between being
> suitable for master or suitable for release branches. That just shuffles
> the pain from one end of the system to the other.

> Make Anubis and Jenkins use different matrix for each branch at the B
> and C process stages above. Only then will discussion of what nodes to
> add to what test/matrix actually make progress.

Anubis is pretty much a red herring here AFAICT -- IIRC, Anubis just
checks that enough required tests have passed. The test results are
supplied by Jenkins and Semaphore CI.

If you think that each target branch should have its own set of Jenkins
tests for a staged commit, then I see no problem with that per se. You
or Francesco should configure GitHub and Jenkins accordingly.

I recommend grouping all those tests under one name (that single name
can be unique to each target branch), so that Anubis can be told how
many tests to expect using a single number (rather than many
branch-specific settings). If that grouping becomes a serious problem,
we can change Anubis to support a different number of tests for
different target branches.


> The principle ("invariant" in Alex terminology?) with nodes is that they
> represent the OS environment a typical developer can be assumed to be
> running on that OS version+compiler combination.

FWIW, that principle or invariant (I often use those two words
interchangeably) feels a bit unfortunate to me: Given scarce resources,
our CI tests should not target Squid developers. They should target
typical Squid _deployments_ instead. Fortunately, the difference is
often minor.


> Distros release security updates to their "stable" versions. Therefore
> to stay true to the goal we require constant small upgrades as an
> ongoing part of sysadmin maintenance.

Yes, I think we are all in agreement that nodes should be upgraded.
Deployments often do not upgrade their OSes as often as distros do, even
for "security" reasons, but we have to pick the lesser of two evils and
upgrade as often as we can (correctly).

The only possible disagreement that I know of is about _prerequisites_
for an upgrade, as revisited below.


> Adding new nodes with next distro release versions is a manual process
> not related to keeping existing nodes up to date (which is automated?).

Sure. And new nodes should not be added if that addition breaks master.


> From time to time distros break their own ability to compile things.
> It does not indicate "broken master" nor "broken CI" in any way.

Distro changes cannot indicate "broken master" or "broken CI" _until_ we
actually apply them to our CI nodes. If upgrading a node would break or
broke master (as already defined many times), then that upgrade should
not be done or should be undone (until the master is changed to allow
the upgrade to proceed). Keeping master tests successful is a key
upgrade precondition IMO.


>> There are many ways to break CI and detect those breakages, of course,
>> but if master cannot pass required tests after a CI change, then the
>> change broke CI.

> I have yet to see the code in master be corrupted by CI changes in such
> a way that it could not build on peoples development machines.

FWIW, I do not see how the above assertion (or anything about peoples
development machines) is relevant to this discussion about Project CI
and the official merge process.


> What we do have going on is network timeouts, DNS resolution, CPU wait
> timeouts, and rarely _automated_ CI upgrades all causing short-term
> failure to pass a test.

Intermittent failures (DNS and such) are out of scope here.

CI upgrades, automated or not, should not break master. If they
accidentally do, they should be reverted. If reversal is not possible,
they should not be attempted in the first place.


> A PR fixing newly highlighted bugs gets around the latter. Any pain (eg
> master blocked for 2 days waiting on the fix PR to merge) is a normal
> problem with that QA process and should not be attributed to the CI change.

I do not understand why a problem caused by the CI change should not be
attributed to that CI change.



> We do need to stop the practice of just dropping
> support for any OS where attempting to build finds existing bugs in
> master (aka "breaks master, sky falling"). More focus on fixing those
> bugs to increase portability and grow the Squid community beyond the
> subset of RHEL and Ubuntu users.

If somebody can add support for more environments (and adding that
support does not make things worse for Squid), then they should
certainly add that support! I see no relationship with this discussion
-- nobody here wants to prohibit such useful activities, and the "do not
break master" invariant can be upheld while adding such support.


Alex.

From squid3 at treenet.co.nz  Mon May 17 02:19:41 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Mon, 17 May 2021 14:19:41 +1200
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <5d1ec3b0-f266-8397-4766-69269670d21b@measurement-factory.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
 <5d1ec3b0-f266-8397-4766-69269670d21b@measurement-factory.com>
Message-ID: <555533c5fe9916b2bc5de7fcedbadfcc@treenet.co.nz>

On 2021-05-17 11:56, Alex Rousskov wrote:
> On 5/16/21 3:31 AM, Amos Jeffries wrote:
>> On 4/05/21 2:29 am, Alex Rousskov wrote:
>>> On 5/3/21 12:41 AM, Francesco Chemolli wrote:
>>>> - we want our QA environment to match what users will use. For this
>>>> reason, it is not sensible that we just stop upgrading our QA nodes,
>>> 
>>> I see flaws in reasoning, but I do agree with the conclusion -- yes, 
>>> we
>>> should upgrade QA nodes. Nobody has proposed a ban on upgrades 
>>> AFAICT!
>>> 
>>> The principles I have proposed allow upgrades that do not violate key
>>> invariants. For example, if a proposed upgrade would break master, 
>>> then
>>> master has to be changed _before_ that upgrade actually happens, not
>>> after. Upgrades must not break master.
>> 
>> So ... a node is added/upgraded. It runs and builds master fine. Then
>> added to the matrices some of the PRs start failing.
> 
> It is easy to misunderstand what is going on because there is no good
> visualization of complex PR-master-Jenkins_nodes-Jenkins_failures
> relationships. Several kinds of PR test failures are possible. I will
> describe the two most relevant to your email:
> 
> * PR test failures due to problems introduced by PRs should be welcomed
> at any time.

Strawman here. This is both general statement and not relevant to CI 
changes or design(s) we are discussing.

> CI improvements are allowed to find new bugs in open PRs.

IMO the crux is that word "new". CI improvements very rarely find new 
bugs. What it actually finds and intentionally so is *existing bugs* the 
old CI config wrongly ignored.

> Such findings, even when discovered at the "last minute", should be 
> seen
> as an overall positive event or progress -- our CI was able to identify
> a problem before it got officially accepted! I do not recall anybody
> complaining about such failures recently.
> 

Conclusion being that due to the rarity of "new bugs" CI improvements 
very rarely get complained about due to them.


> * PR test failures due to the existing master code are not welcomed.

That is not as black/white as the statement above implies. There are 
some master branch bugs we don't want to block PRs merging, and there 
are some (rarely) we absolutely do not want any PRs to change master 
until fixed.

> They represent a CI failure.

IMO this is absolutely false. The whole point of improving CI is to find 
those "existing" bugs which the previous CI config wrong missed.

e.g. v4+ currently do not build on Windows. We know this, but the 
current CI testing does not show it. Upgrading the CI to include a test 
for Windows is not a "CI failure".


> In these cases, if the latest master code
> is tested with the same test after the problematic CI change, then that
> master test will fail. Nothing a PR can do in this situation can fix
> this kind of failure because it is not PR changes that are causing the
> failure -- CI changes broke the master branch,

Ah. "broke the master branch" is a bit excessive. master is not broken 
any more or less than it already was.

What is *actually* broken is the CI test results.


> not just the PR! This
> kind of failures are the responsibility of CI administrators, and PR
> authors should complain about them, especially when there are no signs
> of CI administrators aware of and working on addressing the problem.
> 

*IF* all the conditions and assumptions contained in that final sentence 
are true I would agree. Such case points to incompetence or neglect on 
part of the sysadmin who broken *the CI test* then abandoned fixing it - 
complaints are reasonable there.

  [ Is kinkie acting incompetently on a regular basis? I think no. ]

Otherwise, short periods between sysadmin thinking it was a safe change 
and reverting as breakage appeared is to be expected. That is why we 
have sysadmin doing advance notices for us all to be aware of CI changes 
planned. Complaints still happen, but not much reason to redesign the 
sysadmin practices and automation (which is yet more CI change, ...).


> A good example of a failure of the second kind a -Wrange-loop-construct
> error in a PR that does not touch any range loops (Jenkins conveniently
> deleted the actual failed test, but my GitHub comment and PR contents
> may be enough to restore what happened):
> https://github.com/squid-cache/squid/pull/806#issuecomment-827924821
> 

Thank you.

I see here two distros which have "rolling release" being updated by 
sysadmin from producing outdated and wrong test results, to producing 
correct test results. This is a correct change in line with the goal of 
our nodes representing what a user running that OS would see building 
Squid master or PRs.
   One distro changed compiler and both turned on a new warning by 
default which exposed existing Squid bugs. Exactly as intended.

IMO we can expect to occur on a regular basis and it is specific to 
"rolling release" distros. We can resolve it by having those OS only 
build in the N-matrix applied before releases, instead of the matrix 
blocking PR tests or merging.

  If we are all agreed, kinkie or I can implement ASAP.


<skip>
> 
>> B. PR submission testing
>>    - which OS for master (5-pr-test) ?
>>    - which OS for beta (5-pr-test) ?
>>    - which OS for stable (5-pr-test) ?
>> 
>> Are all of those sets the same identical OS+compilers? no.
>> Why are they forced to be the same matrix test?
> 
> I do not understand the question. Are you asking why Jenkins uses the
> same 5-pr-test configuration for all three branches (master, beta, 
> _and_
> stable)? I do not know the answer.
> 

So can we agree that they should be different tests?

  If we are all agreed, that can be implemented.

After test separation we have the choice of OS to answer those questions 
I posed.

My idea is to go through distrowatch (see file attached) and sync the 
tests with OS that provide that vN (or lower) of Squid as part of its 
release. Of course, following the sysadmin testing process for any 
additions wanted.


> 
>> IIRC, policy forced on sysadmin with previous pain complaints.
> 
> Complaints, even legitimate ones, should not shape a policy. Goals and
> principles should do that.
> 
> I remember one possibly related discussion where we were trying to
> reduce Jenkins/PR wait times by changing which tests are run at what PR
> merging stages, but that is probably a different issue because your
> question appears to be about a single merging stage.
> 

I think it was the discussion re-inventing the policy prior to that 
performance one.

> 
>> C. merge testing
>>    - which OS for master (5-pr-auto) ?
>>    - which OS for beta (5-pr-auto) ?
>>    - which OS for stable (5-pr-auto) ?
>>      NP: maintainer does manual override on beta/stable merges.
>> 
>> Are all of those sets the same identical OS+compilers? no.
>>   Why are they forced to be the same matrix test? Anubis
> 
> This is too cryptic for me to understand, but Anubis does not force any
> tests on anybody -- it simply checks that the required tests have
> passed. I am not aware of any Anubis bugs in this area, but please
> correct me if I am wrong.
> 

My understanding was that Anubis only has ability to check PRs against 
its auto branch which tracks master. Ability to have it track other 
non-master branches and merge there is not available for use.

If that ability were available, we would need to implement different 
matrix as with N-pr-test to use it without guaranteed pain points.

IMO we should look into this. But it is a technical project for sysadmin 
+ Eduard to coordinate. Not a policy thing.


> 
>> D. pre-release testing (snapshots + formal)
>>    - which OS for master (trunk-matrix) ?
>>    - which OS for beta (5-matrix) ?
>>    - which OS for stable (4-matrix) ?
>> 
>> Are all of those sets the same identical OS+compilers? no.
>> Are we forcing them to use the same matrix test? no.
>> Are we getting painful experiences from this? maybe.
>>   Most loud complaints have been about "breaking master" which is the
>> most volatile branch testing on the most volatile OS.
> 
> FWIW, I think you misunderstood what those "complaints" where about. I
> do not know how that relates to the above questions/answers though.
> 

Maybe. Our different view on what comprises "breaking master" certainly 
confuses interpretations when the phrase is used as the 
problem/complaint/report description.


> 
>> FTR: the reason all those matrices have '5-' prefix is because several
>> redesigns ago the system was that master/trunk had a matrix which the
>> sysadmin added nodes to as OS upgraded. During branching vN the
>> maintainer would clone/freeze that matrix into an N-foo which would be
>> used to test the code against OS+compilers which the code in the vN
>> branch was designed to build on.
> 
> I think the above description implies that some time ago we were (more)
> careful about (not) adding new nodes when testing stable branches. We
> did not want a CI change to break a stable branch. That sounds like the
> right principle to me (and it should apply to beta and master as well).
> How that specific principle is accomplished is not important (to me) so
> CI admins should propose whatever technique they think is best.
> 
> 
>> Can we have the people claiming pain specify exactly what the pain is
>> coming from, and let the sysadmin/developer(s) with specialized
>> knowledge of the automation in that area decide how best to fix it?
> 
> We can, and that is exactly what is going on in this thread AFAICT. 
> This
> particular thread was caused by CI changes breaking master, and
> Francesco was discussing how to avoid such breakages in the future.
> 
> There are other goals/principles to observe, of course, and it is
> possible that Francesco is proposing more changes to optimize something
> else as well, but that is something only he can clarify (if needed).
> 
> AFAICT, Francesco and I are on the same page regarding not breaking
> master anymore -- he graciously agreed to prevent such breakages in the
> future, and I am very thankful that he did. Based on your comments
> discussing several cases where such master breakage is, in your 
> opinion,
> OK, you currently disagree with that principle. I do not know why.
> 

I think we differ in our definitions of "breaking master". You seem to 
be including breakage of things in the CI system itself which I consider 
outside of "master", or expected results of normal sysadmin activity. I 
hope my response to the two use-cases you present at the top of this 
email clarify.


Amos
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: squid_distros_2021.txt
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210517/1ca3ab6e/attachment-0001.txt>

From gkinkie at gmail.com  Mon May 17 06:17:35 2021
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 17 May 2021 08:17:35 +0200
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
Message-ID: <CA+Y8hcPCP2PVEstOeBwARdKmAPW=i1pG4X27OJgHh4UuJbRDoA@mail.gmail.com>

>
>
> Adding new nodes with next distro release versions is a manual process
> not related to keeping existing nodes up to date (which is automated?).
>

Mostly.
Our Linux environments are docker containers on amd64, armv7l and arm64.
On a roughly monthly cadence, I pull from our dockerfiles repo (
https://github.com/kinkie/dockerfiles) and
$ make all push
The resulting docker images are free for everybody to use and test things
on on any docker system
(https://hub.docker.com/r/squidcache/buildfarm). Just
$ docker run -ti --rm -u jenkins squidcache/buildfarm:$(uname -m)-<distro
name> /bin/bash -l
(note: the above command will not preserve any artifacts once the shell
exits)

Adding new Linux distros means copying and tweaking a Dockerfile, testing
things, and updating our Jenkins jobs. I do it roughly every 6 months

FreeBSD, OpenBSD and (hopefully soon) Windows are hand-managed and much
slower changing VMs


> >> What I would place on each individual dev is the case where a PR breaks
> >> something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
> >> trunk-openbsd-matrix, trunk-freebsd-matrix builds, even if the 5-pr-test
> >> and 5-pr-auto builds fail to detect the breakage because it happens on a
> >> unstable or old platform. >
> > This feels a bit out of topic for me, but I think you are saying that
> > some CI tests called trunk-matrix, trunk-arm32-matrix,
> > trunk-arm64-matrix, trunk-openbsd-matrix, trunk-freebsd-matrix should be
> > classified as _required_.
>
> That is how I read the statement too.
>

In a word of infinite resources and very efficient testing, sure.
But in a space where a single os/compiler combo takes 2hrs on Linux and
4hrs on Freebsd or openbsd, and a full 5-pr-test takes 6 hours end to end,
we need to optimize or making any of these requirements blocking would make
these times get 4+ times larger (a full trunk-matrix takes just about a day
on amd64, 2 days on arm64), and the complaint would then be that
development or release is slowed down by the amount of testing done.

My proposal aims to test/land the PR on the systems where we can be
efficient and that are relevant, and fix any remaining after-the-fact
issues with followup, PRs, that remain a soft requirement for the dev
introducing the change. The dev can test any work they're doing with the
anybranch-* jobs, if they don't have access to that OS

Can we do better? Sure.
For the sake of cost-mindedness a lot of the build farm nodes run in my
home - a couple of raspberry PIs, an Intel NUC, I'm in the process of
purchasing a second (and second-hand) NUC that comes with a Windows
license. The set up is meant to be thrifty, I'm mindful of burning
Foundation resources for little gain and I'm running a balancing act
between always-on VMs, on-demand VMs and my own gadgets.

The real game changer would be rethinking how we do things to reduce the
amount of testing needed.

For instance: we currently have a lot of different build-time
configurations meant to save core memory in runtime environments. Is it
maybe time to revisit this decision and move these checks to run time?
Unfortunately, one of the problems we have is that we're running blind. We
don't know what configurations our users deploy; we can only assume and
that makes this conversation much more susceptible to opinions and harder
to build consensus on

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210517/b38ef76f/attachment.htm>

From rousskov at measurement-factory.com  Mon May 17 15:03:23 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 May 2021 11:03:23 -0400
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <555533c5fe9916b2bc5de7fcedbadfcc@treenet.co.nz>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
 <5d1ec3b0-f266-8397-4766-69269670d21b@measurement-factory.com>
 <555533c5fe9916b2bc5de7fcedbadfcc@treenet.co.nz>
Message-ID: <d0f53cf9-4a4d-4543-ee7d-2cf15270dd95@measurement-factory.com>

On 5/16/21 10:19 PM, squid3 at treenet.co.nz wrote:
> On 2021-05-17 11:56, Alex Rousskov wrote:
>> On 5/16/21 3:31 AM, Amos Jeffries wrote:
>>> On 4/05/21 2:29 am, Alex Rousskov wrote:
>>>> The principles I have proposed allow upgrades that do not violate key
>>>> invariants. For example, if a proposed upgrade would break master, then
>>>> master has to be changed _before_ that upgrade actually happens, not
>>>> after. Upgrades must not break master.
>>>
>>> So ... a node is added/upgraded. It runs and builds master fine. Then
>>> added to the matrices some of the PRs start failing.
>>
>> It is easy to misunderstand what is going on because there is no good
>> visualization of complex PR-master-Jenkins_nodes-Jenkins_failures
>> relationships. Several kinds of PR test failures are possible. I will
>> describe the two most relevant to your email:
>>
>> * PR test failures due to problems introduced by PRs should be welcomed
>> at any time.
> 
> Strawman here. This is both general statement and not relevant to CI
> changes or design(s) we are discussing.

The first bullet (out of the two bullets that are meant to be
interpreted together, in their context) is not even close to being a
strawman argument by any definition I can find. Neither is the
combination of those two bullets.


>> CI improvements are allowed to find new bugs in open PRs.

> IMO the crux is that word "new". CI improvements very rarely find new
> bugs. What it actually finds and intentionally so is *existing bugs* the
> old CI config wrongly ignored.

Here, I used "new" to mean bugs that had not been found by the previous
CI version (i.e. "newly discovered" or "new to us"). These bugs existed
before and after CI improvements, of course -- neither master nor the PR
has changed -- but we did not know about them.


>> * PR test failures due to the existing master code are not welcomed.

> That is not as black/white as the statement above implies. There are
> some master branch bugs we don't want to block PRs merging, and there
> are some (rarely) we absolutely do not want any PRs to change master
> until fixed.

Yes, master bugs should not affect PR merging in the vast majority of
cases, but that is not what this bullet is about at all!

This bullet is about (a certain kind of) PR test failures. Hopefully, we
do not need to revisit the debate whether PRs with failed tests should
be merged. They should not be merged, which is exactly why PR test
failures caused by the combination of CI changes and the existing master
code are not welcomed -- they block progress of an innocent PR.


>> They represent a CI failure.
> 
> IMO this is absolutely false. The whole point of improving CI is to find
> those "existing" bugs which the previous CI config wrong missed.

Your second sentence is correct, but it does not make my statement
false. CI should achieve several goals. Finding bugs (i.e. blocking
buggy PRs) is one of them. Merging (i.e. not blocking) PRs that should
be merged is another. And there are a few more goals, of course. The
problem described in my second bullet represents a CI failure to reach
one of the key CI goals or a failure to maintain a critical CI
invariant. It is a CI failure rather than a PR failure (the latter is
covered by the first bullet).


> e.g. v4+ currently do not build on Windows. We know this, but the
> current CI testing does not show it. Upgrading the CI to include a test
> for Windows is not a "CI failure".

If such an upgrade would result in blocking PRs that do not touch
Windows code, then that upgrade would be a CI failure. Or a failure to
properly upgrade CI.


>> In these cases, if the latest master code
>> is tested with the same test after the problematic CI change, then that
>> master test will fail. Nothing a PR can do in this situation can fix
>> this kind of failure because it is not PR changes that are causing the
>> failure -- CI changes broke the master branch,
> 
> Ah. "broke the master branch" is a bit excessive. master is not broken
> any more or less than it already was.

If you can suggest a better short phrase to describe the problem, please
do so! Until then, I will have to continue to use "breaking master" (in
this context) simply for the lack of a better phrase. I tried to explain
what that phrase means to avoid misunderstanding. I cannot find a better
way to compress that explanation into a single phrase.


> What is *actually* broken is the CI test results.

One can easily argue that CI test results are actually "correct" in this
case -- they correctly discover a bug in the code that wants to become
the next master. The problem is in the assignment of responsibility: The
PR did not introduce that bug, so the PR should not be punished for that
bug. The only way to avoid such punishment (given the natural automated
CI limitations) is to avoid breaking master tests, as previously defined.


> Otherwise, short periods between sysadmin thinking it was a safe change
> and reverting as breakage appeared is to be expected.

Well, it is debatable whether frequent breakages should be _expected_ --
there are certainly ways to avoid the vast majority of them, but I have
already agreed that we can survive breaking upgrade attempts, even
relatively frequent ones, provided the admin doing the upgrade monitors
CI and can quickly undo the attempts that break master, as previously
defined.


> I see here two distros which have "rolling release" being updated by
> sysadmin from producing outdated and wrong test results, to producing
> correct test results. This is a correct change in line with the goal of
> our nodes representing what a user running that OS would see building
> Squid master or PRs.

In general, the CI change is incorrect if it results in breaking master,
as previously defined. This particular correctness aspect does not
depend on what tests the CI change was meant to fix.


> IMO we can expect to occur on a regular basis

I hope that Francesco will find a way to avoid creating this persistent
expectation of these CI-caused problems!


> We can resolve it by having those OS only
> build in the N-matrix applied before releases, instead of the matrix
> blocking PR tests or merging.

AFAICT, that would not really _resolve_ the problem, only delay its
manifestation until release time (when the stress related to fixing it
will be extreme, and the maintainer will be tempted to abuse the
pressure to release to push low-quality changes through the review process).


> If we are all agreed, kinkie or I can implement ASAP.

I would not strongly object to delaying N-matrix tests (whatever they
will be) until release time, but delaying the pain until the release
time sounds like a poor solution to me.

I think the best solution here would heavily depend on who is
responsible for adjusting the official code (to allow the anticipated CI
upgrade). Their resources/preferences/capabilities/etc. may determine
the best solution, the set of the required tests, and the corresponding
rules. Today, nobody specific is responsible and many volunteer
developers are often unaware of the failures or cannot quickly address
them. With some luck, the changes that Francesco has started to propose
will improve this situation.

One solution that would be better, IMO, than delaying N-matrix tests
would be to make tests on "rolling" OSes optional instead of required
(but still test all PRs). Splitting merge tests into optional and
required would attract developer attention without blocking innocent PRs.


>>> B. PR submission testing
>>>    - which OS for master (5-pr-test) ?
>>>    - which OS for beta (5-pr-test) ?
>>>    - which OS for stable (5-pr-test) ?
>>>
>>> Are all of those sets the same identical OS+compilers? no.
>>> Why are they forced to be the same matrix test?
>>
>> I do not understand the question. Are you asking why Jenkins uses the
>> same 5-pr-test configuration for all three branches (master, beta, _and_
>> stable)? I do not know the answer.

> So can we agree that they should be different tests?

I, personally, cannot answer that specific question in a meaningful way,
but I doubt I would strongly object to virtually any Jenkins changes
related to stable and beta test sets. I continue to view those branches
(but not their branching points!) as primarily maintainer's
responsibility. If others do not object, you should feel free to make
them different IMO. Just keep GitHub/Jenkins/Anubis integration in mind
when you do so (e.g., by keeping the _number_ of GitHub-visible tests or
"status checks" the same across all branches).


>>> C. merge testing
>>>    - which OS for master (5-pr-auto) ?
>>>    - which OS for beta (5-pr-auto) ?
>>>    - which OS for stable (5-pr-auto) ?
>>>      NP: maintainer does manual override on beta/stable merges.
>>>
>>> Are all of those sets the same identical OS+compilers? no.
>>>   Why are they forced to be the same matrix test? Anubis
>>
>> This is too cryptic for me to understand, but Anubis does not force any
>> tests on anybody -- it simply checks that the required tests have
>> passed. I am not aware of any Anubis bugs in this area, but please
>> correct me if I am wrong.


> My understanding was that Anubis only has ability to check PRs against
> its auto branch which tracks master.

Anubis documentation[1] defines staging_branch (i.e. our "auto") as a
branch for testing PR changes as if they were merged into the PR target
branch. The target branch is defined by each GitHub PR. AFAICT, Anubis
does not really have a high-level concept of a "master" branch. The auto
branch should contain whatever target branch the PR is using.

[1] https://github.com/measurement-factory/anubis/blob/master/README.md


> Ability to have it track other
> non-master branches and merge there is not available for use.

AFAICT, Anubis is tracking v4 PRs but you are ignoring it. For example,
merged PR #815 has a "waiting for more votes" status check added by
Anubis to the last commit (f6828ed):
https://github.com/squid-cache/squid/pull/815

There may be some integration problems that I am not aware of, but I
think everything should work in principle. AFAICT, you are forcing
manual v4/v5 commits instead of following the procedure we use for
master, but that is your decision. Please note that I am not saying that
your decision is right or wrong, just documenting my understanding and
observations.


> IMO we should look into this. But it is a technical project for sysadmin
> + Eduard to coordinate. Not a policy thing.

What tests are blocking and what are optional is a policy thing. Whether
it is OK to break master, as previously defined, is a policy thing. How
to implement those and other policies is usually a technicality indeed.


>> AFAICT, Francesco and I are on the same page regarding not breaking
>> master anymore -- he graciously agreed to prevent such breakages in the
>> future, and I am very thankful that he did. Based on your comments
>> discussing several cases where such master breakage is, in your opinion,
>> OK, you currently disagree with that principle. I do not know why.

> I think we differ in our definitions of "breaking master". You seem to
> be including breakage of things in the CI system itself which I consider
> outside of "master", or expected results of normal sysadmin activity.

I hope my definition of "breaking master" in this CI change context is
clear by now. I still do not know why you oppose to prohibiting doing
that, under any name.


> I hope my response to the two use-cases you present at the top of this
> email clarify.

Unfortunately, those attacks on my choice of words did not help me
understand why you are opposed to the principles those words describe.
You are welcome to propose better phrasing, of course, but that would
not change what actually happened and my desire to prohibit those kinds
of events in the future.

Alex.

From rousskov at measurement-factory.com  Mon May 17 18:31:44 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 May 2021 14:31:44 -0400
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <CA+Y8hcPCP2PVEstOeBwARdKmAPW=i1pG4X27OJgHh4UuJbRDoA@mail.gmail.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
 <CA+Y8hcPCP2PVEstOeBwARdKmAPW=i1pG4X27OJgHh4UuJbRDoA@mail.gmail.com>
Message-ID: <b7087dcc-3b91-54ba-e197-5643fb2249f5@measurement-factory.com>

On 5/17/21 2:17 AM, Francesco Chemolli wrote:

> Our Linux environments are docker containers on amd64, armv7l and arm64.
> On a roughly monthly cadence, I pull from our dockerfiles repo
> (https://github.com/kinkie/dockerfiles) and
> $ make all push

Does that "make push" command automatically switch Jenkins CI to using
the new/pushed containers? Or is that a separate manual step?


>>> What I would place on each individual dev is the case where a PR breaks
>>> something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
>>> trunk-openbsd-matrix, trunk-freebsd-matrix builds

>> I think you are saying that
>> some CI tests called trunk-matrix, trunk-arm32-matrix,
>> trunk-arm64-matrix, trunk-openbsd-matrix, trunk-freebsd-matrix should be
>> classified as _required_.

> That is how I read the statement too.

... but it sounds like that is _not_ what you (Francesco) is actually
proposing because you are essentialy saying "that ideal would be
impractical" in the paragraph below. Assuming that you are not attacking
your own proposal, that means Amos and I do not know what your proposal
is -- we both guessed incorrectly...


> In a word of infinite resources and very efficient testing, sure.
> But in a space where a single os/compiler combo takes 2hrs on Linux and
> 4hrs on Freebsd or openbsd, and a full 5-pr-test takes 6 hours end to
> end, we need to optimize or making any of these requirements blocking
> would make these times get 4+ times larger (a full trunk-matrix takes
> just about a day on amd64, 2 days on arm64), and the complaint would
> then be that development or release is slowed down by the amount of
> testing done.

FWIW, I think that full round of PR tests (i.e. one initial set plus one
staging set) should not exceed ~24 hours, _including_ any wait/queuing
time. This kind of delay should still allow for reasonable progress with
PRs AFAICT. This includes any release PRs, of course. If we exceed these
limits (or whatever limits we can agree on), then we should add testing
resources and/or drop tests.


> My proposal aims to test/land the PR on the systems where we can be
> efficient and that are relevant, and fix any remaining after-the-fact
> issues with followup, PRs, that remain a soft requirement for the dev
> introducing the change.

Here, I think you are proposing to make some tests optional. Which ones?


> For instance: we currently have a lot of different build-time
> configurations meant to save core memory in runtime environments. Is it
> maybe time to revisit this decision and move these checks to run time?

Sure, quality proposals for removing #ifdefs should be welcomed, one
(group of) #ifdefs at a time. We can warn squid-users in advance in case
somebody wants to provide evidence of harm.


> Unfortunately, one of the problems we have is that we're running blind.
> We don't know what configurations our users deploy; we can only assume
> and that makes this conversation much more susceptible to opinions and
> harder to build consensus on

Yes, we are blind, but I doubt we should care much about actual
configuration in this specific context. If we can remove an #ifdef
without serious negative consequences, we should remove it. We can avoid
long discussions by allowing anybody with concrete evidence of problems
to block any particular removal. I bet that conservative approach would
still allow for the removal of many #ifdefs.


FWIW, I do not think reducing the number of #ifdefs will solve our
primary CI problems. I believe we should reduce the number of platforms
we test on and then use Foundation resources to speed up and improve the
remaining tests.


HTH,

Alex.

From gkinkie at gmail.com  Mon May 17 19:32:15 2021
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 17 May 2021 21:32:15 +0200
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <b7087dcc-3b91-54ba-e197-5643fb2249f5@measurement-factory.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
 <CA+Y8hcPCP2PVEstOeBwARdKmAPW=i1pG4X27OJgHh4UuJbRDoA@mail.gmail.com>
 <b7087dcc-3b91-54ba-e197-5643fb2249f5@measurement-factory.com>
Message-ID: <CA+Y8hcNWpzCO3y4w=6XaV-ZK8uK7tmQNzgJY-8AfeZ0pB7vhNQ@mail.gmail.com>

On Mon, May 17, 2021 at 8:32 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 5/17/21 2:17 AM, Francesco Chemolli wrote:
>
> > Our Linux environments are docker containers on amd64, armv7l and arm64.
> > On a roughly monthly cadence, I pull from our dockerfiles repo
> > (https://github.com/kinkie/dockerfiles) and
> > $ make all push
>
> Does that "make push" command automatically switch Jenkins CI to using
> the new/pushed containers? Or is that a separate manual step?
>

Right now that's automatic.
I'm pondering the best way to have a staging step; Docker supports
versioning of images but we're using that feature to implement multiarch.
In order to rely on that I'll need to change the naming conventions we rely
on.


> >>> What I would place on each individual dev is the case where a PR breaks
> >>> something in the trunk-matrix,trunk-arm32-matrix, trunk-arm64-matrix,
> >>> trunk-openbsd-matrix, trunk-freebsd-matrix builds
>
> >> I think you are saying that
> >> some CI tests called trunk-matrix, trunk-arm32-matrix,
> >> trunk-arm64-matrix, trunk-openbsd-matrix, trunk-freebsd-matrix should be
> >> classified as _required_.
>
> > That is how I read the statement too.
>
> ... but it sounds like that is _not_ what you (Francesco) is actually
> proposing because you are essentialy saying "that ideal would be
> impractical" in the paragraph below. Assuming that you are not attacking
> your own proposal, that means Amos and I do not know what your proposal
> is -- we both guessed incorrectly...
>

The overarching objectives are:
- focus on where most users probably are
- allow iterating quickly giving some signal on tracked branches
- allow landing quickly with more signal
- be slow but exhaustive on every other platform we can cover. Do not block
landing new code on less common or harder to test on platforms, but still
rely on devs to own their changes there, too.

> In a word of infinite resources and very efficient testing, sure.
> > But in a space where a single os/compiler combo takes 2hrs on Linux and
> > 4hrs on Freebsd or openbsd, and a full 5-pr-test takes 6 hours end to
> > end, we need to optimize or making any of these requirements blocking
> > would make these times get 4+ times larger (a full trunk-matrix takes
> > just about a day on amd64, 2 days on arm64), and the complaint would
> > then be that development or release is slowed down by the amount of
> > testing done.
>
> FWIW, I think that full round of PR tests (i.e. one initial set plus one
> staging set) should not exceed ~24 hours, _including_ any wait/queuing
> time. This kind of delay should still allow for reasonable progress with
> PRs AFAICT. This includes any release PRs, of course. If we exceed these
> limits (or whatever limits we can agree on), then we should add testing
> resources and/or drop tests.


Is everyone okay with such a slow turnaround?


> > My proposal aims to test/land the PR on the systems where we can be
> > efficient and that are relevant, and fix any remaining after-the-fact
> > issues with followup, PRs, that remain a soft requirement for the dev
> > introducing the change.
>
> Here, I think you are proposing to make some tests optional. Which ones?
>

Not the tests, but the platforms. Things like gentoo, fedora rawhide,
freebsd, openbsd.
Testing is slower there, so results will lag and we need to batch, running
a full test suite every week or so


> > For instance: we currently have a lot of different build-time
> > configurations meant to save core memory in runtime environments. Is it
> > maybe time to revisit this decision and move these checks to run time?
>
> Sure, quality proposals for removing #ifdefs should be welcomed, one
> (group of) #ifdefs at a time. We can warn squid-users in advance in case
> somebody wants to provide evidence of harm.
>

I'm glad this is having buy-in. Are there any other opinions (for or
against)?


> > Unfortunately, one of the problems we have is that we're running blind.
> > We don't know what configurations our users deploy; we can only assume
> > and that makes this conversation much more susceptible to opinions and
> > harder to build consensus on
>
> Yes, we are blind, but I doubt we should care much about actual
> configuration in this specific context. If we can remove an #ifdef
> without serious negative consequences, we should remove it. We can avoid
> long discussions by allowing anybody with concrete evidence of problems
> to block any particular removal. I bet that conservative approach would
> still allow for the removal of many #ifdefs.
>

Sounds good to me

FWIW, I do not think reducing the number of #ifdefs will solve our
> primary CI problems. I believe we should reduce the number of platforms
> we test on and then use Foundation resources to speed up and improve the
> remaining tests.
>

Each test build is right now 4 full builds and test suites:
autodetected, minimal, maximum, everytthing-in-except-for-auth

Can we reduce them to 2? If so, which ones? that would be the guide to
#ifdef removal


> HTH,
>

It does; thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210517/4eb65b93/attachment-0001.htm>

From rousskov at measurement-factory.com  Mon May 17 21:04:15 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 May 2021 17:04:15 -0400
Subject: [squid-dev] Strategy about build farm nodes
In-Reply-To: <CA+Y8hcNWpzCO3y4w=6XaV-ZK8uK7tmQNzgJY-8AfeZ0pB7vhNQ@mail.gmail.com>
References: <CA+Y8hcPL8bQfwfnUyAMhuZ8gwKK-ETZJPYw3WhApC1qBMCWMcA@mail.gmail.com>
 <4ad7be77-2903-61b5-f9da-1e35528f48eb@measurement-factory.com>
 <skbb4h-uyjkyaq2eh9e-cobkd3wscnh4-immygnraz6742804ck-q6a3ttagbfe9v5pbqc159bg8-v28t3ahg46rm-rh7jqfus9643gsz6fm-a1iuwpuu45ja2wj7su-s2y08l-su112h-ezpmst-tdvuc4.1619642950774@email.android.com>
 <baedd943-1a24-bc0e-2cbc-50e6cd34397b@measurement-factory.com>
 <CA+Y8hcP9buYP-DzgADUiX7rNCGiq8Nc7G6Z5WYw+7joiDocxCA@mail.gmail.com>
 <81e781b0-9d9b-9a73-e3ca-d7718c4740ec@measurement-factory.com>
 <a5e9410f-b6e9-0ecd-f83a-e2ba44f7b73a@treenet.co.nz>
 <CA+Y8hcPCP2PVEstOeBwARdKmAPW=i1pG4X27OJgHh4UuJbRDoA@mail.gmail.com>
 <b7087dcc-3b91-54ba-e197-5643fb2249f5@measurement-factory.com>
 <CA+Y8hcNWpzCO3y4w=6XaV-ZK8uK7tmQNzgJY-8AfeZ0pB7vhNQ@mail.gmail.com>
Message-ID: <19cb4b05-71b0-3b8a-bf3f-ef1b74d7cd52@measurement-factory.com>

On 5/17/21 3:32 PM, Francesco Chemolli wrote:
> On Mon, May 17, 2021 at 8:32 PM Alex Rousskov wrote:
> 
>     On 5/17/21 2:17 AM, Francesco Chemolli wrote:
>     > $ make all push
> 
>     Does that "make push" command automatically switch Jenkins CI to using
>     the new/pushed containers? Or is that a separate manual step?

> Right now that's automatic.
> I'm pondering the best way to have a staging step; Docker supports
> versioning of images but we're using that feature to implement
> multiarch. In order to rely on that I'll need to change the naming
> conventions we rely on.

I think you should be free to use whatever versioning/naming scheme you
think works best for supporting safe Jenkins CI upgrades.

Ideally, the new docker images (and any other changes) should be tested
(against official branches) _before_ the official tests are switched to
use them. I hope you find a way to implement that without a lot of work.
If you need to tap into Foundation resources, please say so!


> The overarching objectives are:
> - focus on where most users probably are
> - allow iterating quickly giving some signal on tracked branches
> - allow landing quickly with more signal
> - be slow but exhaustive on every other platform we can cover. Do not
> block landing new code on less common or harder to test on platforms,
> but still rely on devs to own their changes there, too.

What exactly do you mean by "own their changes there"? The rest is
general/vague enough to allow for many interpretations I can agree with,
but that last bit seems rather specific, so I wanted to ask...


>     I think that full round of PR tests (i.e. one initial set plus one
>     staging set) should not exceed ~24 hours, _including_ any wait/queuing
>     time.

> Is everyone okay with such a slow turnaround?

Well, you should not overload the question by calling a 24-hour delay
for an all-green signal slow :-).

It is all relative, of course: Most Squid PRs take a lot longer to
review and fix. There are open source projects where PRs wait for a week
or more, so 24 hours for a full round (and usually just an hour or two
for the first signal!) would feel very fast for them. Etc.

Yes, 24 hours is not "interactive", but it is acceptable for the vast
majority of PRs AFAICT, and you are providing dockers for those who want
to test in a more interactive mode.


>     I think you are proposing to make some tests optional. Which ones?
> 
> 
> Not the tests, but the platforms.

>From my test results consumer point of view, they are
[platform-specific] [build] tests.


> Things like gentoo, fedora rawhide, freebsd, openbsd.
> Testing is slower there, so results will lag and we need to batch,
> running a full test suite every week or so

OK, so you propose to make slow Jenkins platform-specific tests (i.e.
Jenkins tests on platforms where tests run slowly today) optional and
those platforms are gentoo, fedora rawhide, freebsd, openbsd. Got it!

What happens when such a slow test fails and the PR author ignores the
failure of that optional test and/or the PR is already closed? The
answer to that question is the key to evaluating any proposal that
declares any tests optional IMO...


>     FWIW, I do not think reducing the number of #ifdefs will solve our
>     primary CI problems.


> Each test build is right now 4 full builds and test suites:
> autodetected, minimal, maximum, everytthing-in-except-for-auth

And all of that takes less than 10 minutes on decent hardware which we
can rent or buy! To me, it feels like we are creating an unsolvable
problem here (i.e. testing a lot of things quickly on a raspberry Pi)
and then trying extra hard to solve that unsolvable problem. We should
either stop testing a lot of things or we should use appropriate
resources for testing a lot of things quickly.

Yes, we can reduce testing time by removing #ifdefs, but that is a lot
of work and does not really scale. #ifdefs should be removed primarily
for other reasons.


Cheers,

Alex.

From phenom252525 at yandex.ru  Sat May 29 18:58:22 2021
From: phenom252525 at yandex.ru (phenom252525 at yandex.ru)
Date: Sat, 29 May 2021 23:58:22 +0500
Subject: [squid-dev] Compilling squid
Message-ID: <945521622314675@mail.yandex.ru>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-dev/attachments/20210529/28d21873/attachment.htm>

