From squid3 at treenet.co.nz  Sat Nov  1 00:12:28 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 01 Nov 2014 13:12:28 +1300
Subject: [squid-users] SSL bump fails accessing .gov.uk servers
In-Reply-To: <54541690.9060206@urlfilterdb.com>
References: <5453D304.7030909@opendium.com> <20141031200342.GA27156@bloms.de>
 <54541690.9060206@urlfilterdb.com>
Message-ID: <5454256C.4050909@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 1/11/2014 12:09 p.m., Marcus Kool wrote:
> With OpenSSL 1.0.1e-fips :
> 
> openssl s_client -connect www.taxdisc.service.gov.uk:443
> fails (tries TLS1.2) openssl s_client -connect
> www.taxdisc.service.gov.uk:443 -ssl3   works
> 
> The webmail server of my ISP works like this: it uses only TLS1.0,
> so no TLS1.1 or TLS1.2, but when with openssl s_client -connect
> WEBMAIL:443 -tls1_2 the connection is automagically downgraded to
> TLS1.0.  taxdisc does not do this. Taxdisc does not negotiate, so
> the client must guess the desired protocol (SSL3 or TLS1.0) and use
> that.
> 
> I do not know all details about TLS and downgrading rules but the
> server seems broken to me.


It is clearly not supporting TLS at all. TLS mandates that endpoints
offer the highest TLS version they support, and the mutual highest is
used. SSLv3 is not on that scale of TLS 1.0+ versions.

Client implementations usually treat rejection of all TLS versions
down to 1.0 as a signal that SSL handshake is required instead, abort
and retry with SSLv3-only...

> Firefox knows how to deal with it and Squid not yet.

... for now anyway. Firefox will be dropping SSLv3 support Nov 25th.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVCVsAAoJELJo5wb/XPRj1rIIAIacxp8gQYhtIA49/+k9c2D9
cO+vnAhADOsIqg2qwtZKRXCYcpAba/s8IeiiouvcowTV54+6GCZ3yyP7uIztwEY3
x+Li2/VKdRYOSLf6QgFo4JU8y5garf9cMrqZw7eFS+Qo9GaYu+BZOcrtlzbAAehN
DqABCRdHkJ+ZtVIC7obVX1fXTnuPlIC3W/QHzc6uGHp75Qs/QAAaV8ugYBMfPpX9
5G6gYSG5qMwQ1XMJ5nc14vFQxTxjrpydl4BKn0WhNLrGaDCWGZiOQYKi7ERlorNs
7yHzjROpWIxapmUChccHifrFEQIR0vo3vAq5StPad3a3aMMp5SW/scpbGFgW8jw=
=mtZp
-----END PGP SIGNATURE-----


From marcus.kool at urlfilterdb.com  Sat Nov  1 00:39:59 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 31 Oct 2014 22:39:59 -0200
Subject: [squid-users] SSL bump fails accessing .gov.uk servers
In-Reply-To: <5454256C.4050909@treenet.co.nz>
References: <5453D304.7030909@opendium.com> <20141031200342.GA27156@bloms.de>
 <54541690.9060206@urlfilterdb.com> <5454256C.4050909@treenet.co.nz>
Message-ID: <54542BDF.8090309@urlfilterdb.com>



On 10/31/2014 10:12 PM, Amos Jeffries wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 1/11/2014 12:09 p.m., Marcus Kool wrote:
>> With OpenSSL 1.0.1e-fips :
>>
>> openssl s_client -connect www.taxdisc.service.gov.uk:443
>> fails (tries TLS1.2) openssl s_client -connect
>> www.taxdisc.service.gov.uk:443 -ssl3   works
>>
>> The webmail server of my ISP works like this: it uses only TLS1.0,
>> so no TLS1.1 or TLS1.2, but when with openssl s_client -connect
>> WEBMAIL:443 -tls1_2 the connection is automagically downgraded to
>> TLS1.0.  taxdisc does not do this. Taxdisc does not negotiate, so
>> the client must guess the desired protocol (SSL3 or TLS1.0) and use
>> that.
>>
>> I do not know all details about TLS and downgrading rules but the
>> server seems broken to me.
>
>
> It is clearly not supporting TLS at all. TLS mandates that endpoints
> offer the highest TLS version they support, and the mutual highest is
> used. SSLv3 is not on that scale of TLS 1.0+ versions.
>
> Client implementations usually treat rejection of all TLS versions
> down to 1.0 as a signal that SSL handshake is required instead, abort
> and retry with SSLv3-only...

Indeed, but taxdisc supports both SSL3 _and_ TLS1.0 ...
Unfortunately, taxdisc (TLS1.0) and the client (TLS1.2) cannot negotiate to
use TLS1.0.

Although "openssl s_client -connect www.taxdisc.service.gov.uk:443 -tls1_2"
fails, the taxdisc server sends 7 bytes with value 0.
So the negotiation goes wrong, but the question remains what exactly
in the handshake is not understood or undefined.

Marcus

>> Firefox knows how to deal with it and Squid not yet.
>
> ... for now anyway. Firefox will be dropping SSLv3 support Nov 25th.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUVCVsAAoJELJo5wb/XPRj1rIIAIacxp8gQYhtIA49/+k9c2D9
> cO+vnAhADOsIqg2qwtZKRXCYcpAba/s8IeiiouvcowTV54+6GCZ3yyP7uIztwEY3
> x+Li2/VKdRYOSLf6QgFo4JU8y5garf9cMrqZw7eFS+Qo9GaYu+BZOcrtlzbAAehN
> DqABCRdHkJ+ZtVIC7obVX1fXTnuPlIC3W/QHzc6uGHp75Qs/QAAaV8ugYBMfPpX9
> 5G6gYSG5qMwQ1XMJ5nc14vFQxTxjrpydl4BKn0WhNLrGaDCWGZiOQYKi7ERlorNs
> 7yHzjROpWIxapmUChccHifrFEQIR0vo3vAq5StPad3a3aMMp5SW/scpbGFgW8jw=
> =mtZp
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Sat Nov  1 00:55:20 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 01 Nov 2014 13:55:20 +1300
Subject: [squid-users] SSL bump fails accessing .gov.uk servers
In-Reply-To: <54542BDF.8090309@urlfilterdb.com>
References: <5453D304.7030909@opendium.com> <20141031200342.GA27156@bloms.de>
 <54541690.9060206@urlfilterdb.com> <5454256C.4050909@treenet.co.nz>
 <54542BDF.8090309@urlfilterdb.com>
Message-ID: <54542F78.4050503@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 1/11/2014 1:39 p.m., Marcus Kool wrote:
> 
> 
> On 10/31/2014 10:12 PM, Amos Jeffries wrote: On 1/11/2014 12:09
> p.m., Marcus Kool wrote:
>>>> With OpenSSL 1.0.1e-fips :
>>>> 
>>>> openssl s_client -connect www.taxdisc.service.gov.uk:443 
>>>> fails (tries TLS1.2) openssl s_client -connect 
>>>> www.taxdisc.service.gov.uk:443 -ssl3   works
>>>> 
>>>> The webmail server of my ISP works like this: it uses only
>>>> TLS1.0, so no TLS1.1 or TLS1.2, but when with openssl
>>>> s_client -connect WEBMAIL:443 -tls1_2 the connection is
>>>> automagically downgraded to TLS1.0.  taxdisc does not do
>>>> this. Taxdisc does not negotiate, so the client must guess
>>>> the desired protocol (SSL3 or TLS1.0) and use that.
>>>> 
>>>> I do not know all details about TLS and downgrading rules but
>>>> the server seems broken to me.
> 
> 
> It is clearly not supporting TLS at all. TLS mandates that
> endpoints offer the highest TLS version they support, and the
> mutual highest is used. SSLv3 is not on that scale of TLS 1.0+
> versions.
> 
> Client implementations usually treat rejection of all TLS versions 
> down to 1.0 as a signal that SSL handshake is required instead,
> abort and retry with SSLv3-only...
> 
>> Indeed, but taxdisc supports both SSL3 _and_ TLS1.0 ... 
>> Unfortunately, taxdisc (TLS1.0) and the client (TLS1.2) cannot
>> negotiate to use TLS1.0.
> 
>> Although "openssl s_client -connect
>> www.taxdisc.service.gov.uk:443 -tls1_2" fails, the taxdisc server
>> sends 7 bytes with value 0. So the negotiation goes wrong, but
>> the question remains what exactly in the handshake is not
>> understood or undefined.

Probably because the TLS/1.0 handshakes which "work" require RC4-MD5
encryption algorithm. MD5 has been broken for a very long time.

Until POODLE SSLv3 was possibly more secure. I cant confirm that
because none of the tools I work with will use SSLv3 anymore :-P

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVC93AAoJELJo5wb/XPRjILYIAL9xnBgVIe0AnyUSYdzF8lzq
2WZ2RQ/T4AeYg/cagv2Lc01T/hrhS0pORUboHo82Kt2GhXbtXGsEbZPyt21eqPTa
nhP3C8eIT45xcHhuRZcICO43Cyg6drym+YiCOiCPOeaHfmWGYz/UpI/kZDkuyJga
TZCOW95qhA59pkjuwmwoACtQRKXvjBxYWibpQs5ZQrKhSNL6NYRp49CRwIUjWdLl
+I2i7CIfwHEdWWlV/yEG69SKjy3S/M111U6gfgIQlO8noL9llwU35D8AEH0X6e9w
WfOd0u4Gdtk45rlAQ9b7RSXwD6F3xCJ4/K5rphghzwQtZF/h1Oa7K8TRLsUx/FA=
=GzuE
-----END PGP SIGNATURE-----


From joelcordobes at hotmail.com  Sat Nov  1 05:39:12 2014
From: joelcordobes at hotmail.com (Joel Mestres)
Date: Sat, 1 Nov 2014 05:39:12 +0000
Subject: [squid-users] hiding web server port
Message-ID: <DUB125-W32FD56763223FC89D44BB8C29B0@phx.gbl>

hello everyone! I'm trying to configure squid 2.7 so it works with a web server (another machine different from squid proxy server) running apache2 on port 8082 (for instance). Squid is installed on debian. Now I have squid with http_port 3128 transparent but I don't figure out how can I achive my goal. The idea is "hide" the port to clients. I was able to achive this without squid, with iptables.... redirecting 80 to 8082 and 8082 to 80. So is it possible to do something like this using squid proxy. Thanks a lot for your help! 		 	   		  

From huaraz at moeller.plus.com  Sat Nov  1 13:11:37 2014
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 1 Nov 2014 13:11:37 -0000
Subject: [squid-users] Kerberos Authentication Failing for Windows
	7+with BH gss_accept_sec_context() failed
In-Reply-To: <09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E@gmail.com>
References: <40E1E0E7-50C6-4117-94AA-50B06573430A@gmail.com>
 <m2eqo7$cf9$1@ger.gmane.org> <FD6832B9-3F1F-48C6-A76F-47A224F1697B@gmail.com>
 <m2g7l9$1ee$1@ger.gmane.org> <94F74226-F24B-4910-95B7-B86ACE815995@gmail.com>
 <m2m7g0$9l7$1@ger.gmane.org>
 <b4adceec-5a53-4212-b16c-106237fc4504@Pedros-iPhone>
 <m2mbc1$b54$1@ger.gmane.org> <09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E@gmail.com>
Message-ID: <m32m6c$teg$1@ger.gmane.org>

Hi Pedro,

    I looked at your captures and I observed something similar to Victor?s issue.   I see KRB5KRB_AP_ERR_MODIFIED and then the use of the name of the AD object (e.g. proxy$) instead of HTTP/<proxy fqdn>.   I also see that you have more than one AD server and I assume there is a sync problem between your AD servers ( You said it start working after removing an unused AD  server which would support y assumption). 

Regards
Markus

"Pedro Lobo" <palobo at gmail.com> wrote in message news:09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E at gmail.com...
Hi Markus, 

When I get in to the office tomorrow, I'll do that and send you the .cap file. Thanks for all the help so far. 

Pedro Lobo

On 27 Oct 2014, at 20:53, Markus Moeller <huaraz at moeller.plus.com> wrote:


  Hi Pedro,

     Can you capture the traffic from one Windows 7 on XP client on port 88 ( just after the login before access a website via squid until successful or unsuccessful accessing the website) using wireshark ?   Send me the .cap files to check.

  Markus

  "Pedro Lobo" <palobo at gmail.com> wrote in message news:b4adceec-5a53-4212-b16c-106237fc4504 at Pedros-iPhone...
  Hi Markus Moeller,


  Hi Markus,

  Yeah, I'm currently using that option and permissions are correct too. 

  On 27 Oct 2014 19:47, Markus Moeller wrote: 


    Hi Pedro,

      Did you try the ?s GSS_C_NO_NAME option ?

    Markus

    "Pedro Lobo" <palobo at gmail.com> wrote in message news:94F74226-F24B-4910-95B7-B86ACE815995 at gmail.com...
    Hey Everybody,

    Seems as though I celebrated too soon on Saturday. Today things are back to not working for Windows 7+ machines and XP/2003 machines are working just fine.

    I've also checked the permissions on the keytab file and they haven't changed since Saturday, so it's not that... ARGH!!!!

    Craving ideas and solutions right now... Pilot users are less than satisfied ;)

    Cheers,
    Pedro

    On 25 Oct 2014, at 14:13, Markus Moeller wrote:

      Hi Pedro,

      I wonder if he upper case in the name is a problem. Can you try

      auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s GSS_C_NO_NAME

      instead of

      auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s HTTP/proxy01tst.fake.net

      Markus

      "Pedro Lobo" palobo at gmail.com wrote in message news:FD6832B9-3F1F-48C6-A76F-47A224F1697B at gmail.com...
      Hi Markus,

      I used msktutil to create the keytab.

      msktutil -c -s HTTP/proxy01tst.fake.net -h proxy01tst.fake.net -k /etc/squid3/PROXY.keytab --computer-name proxy01-tst --upn HTTP/proxy01tst.fake.net --server srv01.fake.net --verbose
      Output of klist -ekt:

      2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (arcfour-hmac)
      2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (aes128-cts-hmac-sha1-96)
      2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (aes256-cts-hmac-sha1-96)
      2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (arcfour-hmac)
      2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (aes128-cts-hmac-sha1-96)
      2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (aes256-cts-hmac-sha1-96)
      2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (arcfour-hmac)
      2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (aes128-cts-hmac-sha1-96)
      2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (aes256-cts-hmac-sha1-96)
      Yep, using MIT Kerberos

      Thanks in advance for any help.

      Cheers,
      Pedro

      On 25 Oct 2014, at 1:26, Markus Moeller wrote:

      Hi Pedro,

      How did you create your keytab ? What does klist ?ekt <squid.keytab> show ( I assume you use MIT Kerberos) ?

      Markus

      "Pedro Lobo" palobo at gmail.com wrote in message news:40E1E0E7-50C6-4117-94AA-50B06573430A at gmail.com...
      Hi Squid Gurus,

      I'm at my wit's end and in dire need of some squid expertise.

      We've got a production environment with a couple of squid 2.7 servers using NTLM and basic authentication. Recently though, we decided to upgrade and I'm now setting up squid 3.3 with Kerberos and NTLM Fallback. I've followed just about every guide I could find and in my testing environment, things were working great. Now that I've hooked it up to the main domain, things are awry.

      If I use a machine that's not part of the domain, NTLM kicks in and I can surf the web fine. If I use a Windows XP or Windows Server 2003, kerberos works just fine, however, if I use a machine Windows 7, 8 or 2008 server, I keep getting a popup asking me to authenticate and even then, it's and endless loop until it fails. My cache.log is littered with:

      negotiate_kerberos_auth.cc(200): pid=1607 :2014/10/24 23:03:01| negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified GSS failure. Minor code may provide more information.
      2014/10/24 23:03:01| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: Unspecified GSS failure. Minor code may provide more information. '
      The odd thing, is that this has worked before. Help me Obi Wan... You're my only hope! :)

      Current Setup
      Squid 3.3 running on Ubuntu 14.04 server. It's connected to a 2003 server with function level 2000 (I know, we're trying to fase out the older servers).

      krb5.conf

      [libdefaults]
      default_realm = FAKE.NET
      dns_lookup_kdc = yes
      dns_lookup_realm = yes
      ticket_lifetime = 24h
      default_keytab_name = /etc/squid3/PROXY.keytab

      ; for Windows 2003
      default_tgs_enctypes = rc4-hmac des-cbc-crc des-cbc-md5
      default_tkt_enctypes = rc4-hmac des-cbc-crc des-cbc-md5
      permitted_enctypes = rc4-hmac des-cbc-crc des-cbc-md5

      [realms]
      FAKE.NET = {
      kdc = srv01.fake.net
      kdc = srv02.fake.net
      kdc = srv03.fake.net
      admin_server = srv01.fake.net
      default_domain = fake.net
      }

      [domain_realm]
      .fake.net = FAKE.NET
      fake.net = FAKE.NET

      [logging]
      kdc = FILE:/var/log/kdc.log
      admin_server = FILE:/var/log/kadmin.log
      default = FILE:/var/log/krb5lib.log
      squid.conf

      auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s HTTP/proxy01tst.fake.net
      auth_param negotiate children 20 startup=0 idle=1
      auth_param negotiate keep_alive off

      auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=FAKE.NET
      auth_param ntlm children 10
      auth_param ntlm keep_alive off
      Cheers,
      Pedro

      Cumprimentos
      Pedro Lobo
      Solutions Architect | System Engineer

      pedro.lobo at pt.clara.net
      Tlm.: +351 939 528 827 | Tel.: +351 214 127 314

      Claranet Portugal
      Ed. Parque Expo
      Av. D. Jo?o II, 1.07-2.1, 4? Piso
      1998-014 Lisboa
      www.claranet.pt

      Empresa certificada ISO 9001, ISO 20000 e ISO 27001


--------------------------------------------------------------------------

--------------------------------------------------------------------------

      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users


--------------------------------------------------------------------------

      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users

      Cumprimentos
      Pedro Lobo
      Solutions Architect | System Engineer

      pedro.lobo at pt.clara.net
      Tlm.: +351 939 528 827 | Tel.: +351 214 127 314

      Claranet Portugal
      Ed. Parque Expo
      Av. D. Jo?o II, 1.07-2.1, 4? Piso
      1998-014 Lisboa
      www.claranet.pt

      Empresa certificada ISO 9001, ISO 20000 e ISO 27001


--------------------------------------------------------------------------

--------------------------------------------------------------------------

      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users


--------------------------------------------------------------------------

      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users


----------------------------------------------------------------------------
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users

  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users



--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141101/84af5a02/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov  1 14:47:13 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 02 Nov 2014 03:47:13 +1300
Subject: [squid-users] hiding web server port
In-Reply-To: <DUB125-W32FD56763223FC89D44BB8C29B0@phx.gbl>
References: <DUB125-W32FD56763223FC89D44BB8C29B0@phx.gbl>
Message-ID: <5454F271.3010806@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 1/11/2014 6:39 p.m., Joel Mestres wrote:
> hello everyone! I'm trying to configure squid 2.7 so it works with
> a web server (another machine different from squid proxy server) 
> running apache2 on port 8082 (for instance). Squid is installed on 
> debian. Now I have squid with http_port 3128 transparent but I
> don't figure out how can I achive my goal. The idea is "hide" the
> port to clients. I was able to achive this without squid, with
> iptables.... redirecting 80 to 8082 and 8082 to 80. So is it
> possible to do something like this using squid proxy. Thanks a lot
> for your help!

Squid does support this for use in CDN networks. BUT, it is generally
not a good idea to do so in HTTP. The server often embeds the port it
is listening on in URLs generated. That can happen in various places
in the HTTP headers and even the object body content being delivered.

Can you upgrade to squid3 package?

Also, why are you using transparent?

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVPJxAAoJELJo5wb/XPRjzVkIAIwtNinl6m10UMllzkoo0dYd
ma4ivgmyPUJqhRL12Wq/EhHDASil/vIlV6jpTqNVT2mcvLtH8Za/iRkfY2/RMcpf
ZfTQcMqA2HoWduvjc+0iZH4rurY2GPdr+5y+ulkWcxGRJzRuAXjbEDxQQoiZ3TC3
Hh32R9NGtnuDAO6lADqlslIndq74uCAxAFiO0sYEgiMEIa6Q/l4slr1zfB6uOKEM
QA0Ab7UoQ07vPTIYl4dZWzx8pt5GT5JQXrdWXbLFsU5DuD022NRSx3Ekxj9GkyFZ
X03aD/ElE8ShOKFPG6jPsAaYDk/heoEp2GMx3t32fjbG70bYjU74nQQ4YgQu4+w=
=dXlo
-----END PGP SIGNATURE-----


From yogesh_dg at yahoo.com  Sat Nov  1 18:05:29 2014
From: yogesh_dg at yahoo.com (Yogesh Gawankar)
Date: Sat, 1 Nov 2014 18:05:29 +0000 (UTC)
Subject: [squid-users] Squid with wccp and gre return
In-Reply-To: <388078593.74577.1414864063275.JavaMail.yahoo@jws10698.mail.bf1.yahoo.com>
References: <388078593.74577.1414864063275.JavaMail.yahoo@jws10698.mail.bf1.yahoo.com>
Message-ID: <941862424.77969.1414865129540.JavaMail.yahoo@jws10625.mail.bf1.yahoo.com>

Hello


I am trying to get squid to send return traffic to the cisco router via? a gre tunnel.
I am observing that squid returns traffic in gre tunnel? this is ip in ip gre not wccp-gre . The firewall is dropping this packet as expected.
I have gone through many posts online so kindly respond if you know how to get it to send return traffic via wccp-gre (not standard gre) or if you had any suggestions for me in terms of linux? kernel change.

If any further information is needed , do let me know.



 

| ? |
| ? |  | ? | ? | ? | ? | ? |
| Ubuntu 10.04 Transparent squid3 proxy using wccp2 on c...See my last post for more defined questions = ID:37147216 ADSLMODEM-GATEWAY --> 2801-ROUTING-NAT --> 3550-L3-ACL-VLAN --> 3550-L2-SWIT... |
|  |
| View on www.experts-exchan... | Preview by Yahoo |
|  |
| ? |

 ??Thanks and regards

Yogesh Gawankar


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141101/b046c145/attachment.htm>

From palobo at gmail.com  Sat Nov  1 22:12:16 2014
From: palobo at gmail.com (Pedro Lobo)
Date: Sat, 01 Nov 2014 22:12:16 +0000
Subject: [squid-users] Kerberos Authentication Failing for Windows
 7+with BH gss_accept_sec_context() failed
In-Reply-To: <m32m6c$teg$1@ger.gmane.org>
References: <40E1E0E7-50C6-4117-94AA-50B06573430A@gmail.com>
 <m2eqo7$cf9$1@ger.gmane.org> <FD6832B9-3F1F-48C6-A76F-47A224F1697B@gmail.com>
 <m2g7l9$1ee$1@ger.gmane.org> <94F74226-F24B-4910-95B7-B86ACE815995@gmail.com>
 <m2m7g0$9l7$1@ger.gmane.org>
 <b4adceec-5a53-4212-b16c-106237fc4504@Pedros-iPhone>
 <m2mbc1$b54$1@ger.gmane.org> <09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E@gmail.com>
 <m32m6c$teg$1@ger.gmane.org>
Message-ID: <18B6C949-80D5-4494-BEFA-C490270A16CF@gmail.com>

Hi Markus,

Thanks for all your help. I'll do some more testing on monday and I'll let you know how it goes. Hopefully it'll be working as expected once having removed the unused AD servers and sorting out and sync issues.

Cheers and have a great weekend!
Pedro

On 1 Nov 2014, at 13:11, Markus Moeller wrote:

> Hi Pedro,
>
> I looked at your captures and I observed something similar to Victor?s issue.   I see KRB5KRB_AP_ERR_MODIFIED and then the use of the name of the AD object (e.g. proxy$) instead of HTTP/<proxy fqdn>.   I also see that you have more than one AD server and I assume there is a sync problem between your AD servers ( You said it start working after removing an unused AD  server which would support y assumption).
>
> Regards
> Markus
>
> "Pedro Lobo" <palobo at gmail.com> wrote in message news:09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E at gmail.com...
> Hi Markus,
>
> When I get in to the office tomorrow, I'll do that and send you the .cap file. Thanks for all the help so far.
>
> Pedro Lobo
>
> On 27 Oct 2014, at 20:53, Markus Moeller <huaraz at moeller.plus.com> wrote:
>
>
> Hi Pedro,
>
>  Can you capture the traffic from one Windows 7 on XP client on port 88 ( just after the login before access a website via squid until successful or unsuccessful accessing the website) using wireshark ?   Send me the .cap files to check.
>
> Markus
>
> "Pedro Lobo" <palobo at gmail.com> wrote in message news:b4adceec-5a53-4212-b16c-106237fc4504 at Pedros-iPhone...
> Hi Markus Moeller,
>
>
> Hi Markus,
>
> Yeah, I'm currently using that option and permissions are correct too.
>
> On 27 Oct 2014 19:47, Markus Moeller wrote:
>
>
> Hi Pedro,
>
>   Did you try the ?s GSS_C_NO_NAME option ?
>
> Markus
>
> "Pedro Lobo" <palobo at gmail.com> wrote in message news:94F74226-F24B-4910-95B7-B86ACE815995 at gmail.com...
> Hey Everybody,
>
> Seems as though I celebrated too soon on Saturday. Today things are back to not working for Windows 7+ machines and XP/2003 machines are working just fine.
>
> I've also checked the permissions on the keytab file and they haven't changed since Saturday, so it's not that... ARGH!!!!
>
> Craving ideas and solutions right now... Pilot users are less than satisfied ;)
>
> Cheers,
> Pedro
>
> On 25 Oct 2014, at 14:13, Markus Moeller wrote:
>
>   Hi Pedro,
>
>   I wonder if he upper case in the name is a problem. Can you try
>
>   auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s GSS_C_NO_NAME
>
>   instead of
>
>   auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s HTTP/proxy01tst.fake.net
>
>   Markus
>
>   "Pedro Lobo" palobo at gmail.com wrote in message news:FD6832B9-3F1F-48C6-A76F-47A224F1697B at gmail.com...
>   Hi Markus,
>
>   I used msktutil to create the keytab.
>
>   msktutil -c -s HTTP/proxy01tst.fake.net -h proxy01tst.fake.net -k /etc/squid3/PROXY.keytab --computer-name proxy01-tst --upn HTTP/proxy01tst.fake.net --server srv01.fake.net --verbose
>   Output of klist -ekt:
>
>   2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (arcfour-hmac)
>   2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (aes128-cts-hmac-sha1-96)
>   2 10/24/2014 22:59:50 proxy01-tst$@FAKE.NET (aes256-cts-hmac-sha1-96)
>   2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (arcfour-hmac)
>   2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (aes128-cts-hmac-sha1-96)
>   2 10/24/2014 22:59:50 HTTP/proxy01tst.FAKE.net at FAKE.NET (aes256-cts-hmac-sha1-96)
>   2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (arcfour-hmac)
>   2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (aes128-cts-hmac-sha1-96)
>   2 10/24/2014 22:59:50 host/proxy01tst.FAKE.net at FAKE.NET (aes256-cts-hmac-sha1-96)
>   Yep, using MIT Kerberos
>
>   Thanks in advance for any help.
>
>   Cheers,
>   Pedro
>
>   On 25 Oct 2014, at 1:26, Markus Moeller wrote:
>
>   Hi Pedro,
>
>   How did you create your keytab ? What does klist ?ekt <squid.keytab> show ( I assume you use MIT Kerberos) ?
>
>   Markus
>
>   "Pedro Lobo" palobo at gmail.com wrote in message news:40E1E0E7-50C6-4117-94AA-50B06573430A at gmail.com...
>   Hi Squid Gurus,
>
>   I'm at my wit's end and in dire need of some squid expertise.
>
>   We've got a production environment with a couple of squid 2.7 servers using NTLM and basic authentication. Recently though, we decided to upgrade and I'm now setting up squid 3.3 with Kerberos and NTLM Fallback. I've followed just about every guide I could find and in my testing environment, things were working great. Now that I've hooked it up to the main domain, things are awry.
>
>   If I use a machine that's not part of the domain, NTLM kicks in and I can surf the web fine. If I use a Windows XP or Windows Server 2003, kerberos works just fine, however, if I use a machine Windows 7, 8 or 2008 server, I keep getting a popup asking me to authenticate and even then, it's and endless loop until it fails. My cache.log is littered with:
>
>   negotiate_kerberos_auth.cc(200): pid=1607 :2014/10/24 23:03:01| negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified GSS failure. Minor code may provide more information.
>   2014/10/24 23:03:01| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: Unspecified GSS failure. Minor code may provide more information. '
>   The odd thing, is that this has worked before. Help me Obi Wan... You're my only hope! :)
>
>   Current Setup
>   Squid 3.3 running on Ubuntu 14.04 server. It's connected to a 2003 server with function level 2000 (I know, we're trying to fase out the older servers).
>
>   krb5.conf
>
>   [libdefaults]
>   default_realm = FAKE.NET
>   dns_lookup_kdc = yes
>   dns_lookup_realm = yes
>   ticket_lifetime = 24h
>   default_keytab_name = /etc/squid3/PROXY.keytab
>
>   ; for Windows 2003
>   default_tgs_enctypes = rc4-hmac des-cbc-crc des-cbc-md5
>   default_tkt_enctypes = rc4-hmac des-cbc-crc des-cbc-md5
>   permitted_enctypes = rc4-hmac des-cbc-crc des-cbc-md5
>
>   [realms]
>   FAKE.NET = {
>   kdc = srv01.fake.net
>   kdc = srv02.fake.net
>   kdc = srv03.fake.net
>   admin_server = srv01.fake.net
>   default_domain = fake.net
>   }
>
>   [domain_realm]
>   .fake.net = FAKE.NET
>   fake.net = FAKE.NET
>
>   [logging]
>   kdc = FILE:/var/log/kdc.log
>   admin_server = FILE:/var/log/kadmin.log
>   default = FILE:/var/log/krb5lib.log
>   squid.conf
>
>   auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -d -r -s HTTP/proxy01tst.fake.net
>   auth_param negotiate children 20 startup=0 idle=1
>   auth_param negotiate keep_alive off
>
>   auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=FAKE.NET
>   auth_param ntlm children 10
>   auth_param ntlm keep_alive off
>   Cheers,
>   Pedro
>
>   Cumprimentos
>   Pedro Lobo
>   Solutions Architect | System Engineer
>
>   pedro.lobo at pt.clara.net
>   Tlm.: +351 939 528 827 | Tel.: +351 214 127 314
>
>   Claranet Portugal
>   Ed. Parque Expo
>   Av. D. Jo?o II, 1.07-2.1, 4? Piso
>   1998-014 Lisboa
>   www.claranet.pt
>
>   Empresa certificada ISO 9001, ISO 20000 e ISO 27001
>
>
> --------------------------------------------------------------------------
>
> --------------------------------------------------------------------------
>
>   squid-users mailing list
>   squid-users at lists.squid-cache.org
>   http://lists.squid-cache.org/listinfo/squid-users
>
>
> --------------------------------------------------------------------------
>
>   squid-users mailing list
>   squid-users at lists.squid-cache.org
>   http://lists.squid-cache.org/listinfo/squid-users
>
>   Cumprimentos
>   Pedro Lobo
>   Solutions Architect | System Engineer
>
>   pedro.lobo at pt.clara.net
>   Tlm.: +351 939 528 827 | Tel.: +351 214 127 314
>
>   Claranet Portugal
>   Ed. Parque Expo
>   Av. D. Jo?o II, 1.07-2.1, 4? Piso
>   1998-014 Lisboa
>   www.claranet.pt
>
>   Empresa certificada ISO 9001, ISO 20000 e ISO 27001
>
>
> --------------------------------------------------------------------------
>
> --------------------------------------------------------------------------
>
>   squid-users mailing list
>   squid-users at lists.squid-cache.org
>   http://lists.squid-cache.org/listinfo/squid-users
>
>
> --------------------------------------------------------------------------
>
>   squid-users mailing list
>   squid-users at lists.squid-cache.org
>   http://lists.squid-cache.org/listinfo/squid-users
>
>
> ----------------------------------------------------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------------------------------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> --------------------------------------------------------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141101/bdfe1b4e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 536 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141101/bdfe1b4e/attachment.sig>

From squid3 at treenet.co.nz  Sun Nov  2 01:21:24 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 02 Nov 2014 14:21:24 +1300
Subject: [squid-users] Squid with wccp and gre return
In-Reply-To: <941862424.77969.1414865129540.JavaMail.yahoo@jws10625.mail.bf1.yahoo.com>
References: <388078593.74577.1414864063275.JavaMail.yahoo@jws10698.mail.bf1.yahoo.com>
 <941862424.77969.1414865129540.JavaMail.yahoo@jws10625.mail.bf1.yahoo.com>
Message-ID: <54558714.6090805@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 2/11/2014 7:05 a.m., Yogesh Gawankar wrote:
> Hello
> 
> 
> I am trying to get squid to send return traffic to the cisco
> router via  a gre tunnel. I am observing that squid returns traffic
> in gre tunnel  this is ip in ip gre not wccp-gre . The firewall is
> dropping this packet as expected. I have gone through many posts
> online so kindly respond if you know how to get it to send return
> traffic via wccp-gre (not standard gre) or if you had any
> suggestions for me in terms of linux  kernel change.

WCCP-GRE is for transmitting WCCP protocol control packets informing
the router whether the proxy is still available or not. That is all.

The HTTP traffic from router to Squid (wccp_forwarding_method) goes
through the GRE as if it were regular GRE.

The HTTP traffic from Squid to router (wccp_return_method and regular
Squid->Internet connections) is normal traffic and needs routing as
such instead of dropping or diverting back towards Squid.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVYcUAAoJELJo5wb/XPRjYAwH/RQXc3S7bH5w/5lYdLyYRQ/H
Y2GT88IdeM8gz067OMkGR7lTwzIMK4EVmb9GEQJYhLNznZB8hFDYnNDmSL3spB9F
mxHYZ4BVlDTlMLi4qHf05mrpsdEodrhF9A3H8YmzXLWHx5sxyukLfR/R7UJlP14K
S9xZa3KhKE/SERNM8iwPggNdtzdjrBhs6U2AUhmpeNjEgEiOgkgT7XRm724hMj21
9CH5kgGt4qiE5plPfSqTdyiJSBbQF5NL50g8/NSrQ4bRilTD9Fdf+kVRoSnscrAF
ViQKzc5L8s+3YMmVmXyynM0s4YPy5bj9Qgqi5nWjQG9gSBRLeedsyK1rdlcQmbQ=
=CQHA
-----END PGP SIGNATURE-----


From yogesh_dg at yahoo.com  Sun Nov  2 01:41:37 2014
From: yogesh_dg at yahoo.com (Yogesh Gawankar)
Date: Sun, 2 Nov 2014 01:41:37 +0000 (UTC)
Subject: [squid-users] Squid with wccp and gre return
In-Reply-To: <54558714.6090805@treenet.co.nz>
References: <54558714.6090805@treenet.co.nz>
Message-ID: <420932010.112911.1414892497303.JavaMail.yahoo@jws10613.mail.bf1.yahoo.com>

Hello Amos
Thanks for your reply.I don't think I explained my problem well 

With wccp gre redirection, when the tcp syn reaches the cisco router that is set up for wccp redirection , it redirects it to the CE using? wccp gre. Squid will respond with syn-ack and send it back to the router again via wccp-gre. However this is not happening. it sends it back using regular Gre (same as wccp-gre but with no wccp header)


Please refer to my attached capture? .I need to get squid to put that wccp header on the return packet.In squid.conf I already specified gre return.
Are? you saying squid (too!) cannot do gre return for return traffic and does normal ip forwarding?

I got squid version 3 running on ubuntu.

?Thanks and regards

Yogesh Gawankar
 

     On Sunday, November 2, 2014 6:52 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
   

 -----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 2/11/2014 7:05 a.m., Yogesh Gawankar wrote:
> Hello
> 
> 
> I am trying to get squid to send return traffic to the cisco
> router via? a gre tunnel. I am observing that squid returns traffic
> in gre tunnel? this is ip in ip gre not wccp-gre . The firewall is
> dropping this packet as expected. I have gone through many posts
> online so kindly respond if you know how to get it to send return
> traffic via wccp-gre (not standard gre) or if you had any
> suggestions for me in terms of linux? kernel change.

WCCP-GRE is for transmitting WCCP protocol control packets informing
the router whether the proxy is still available or not. That is all.

The HTTP traffic from router to Squid (wccp_forwarding_method) goes
through the GRE as if it were regular GRE.

The HTTP traffic from Squid to router (wccp_return_method and regular
Squid->Internet connections) is normal traffic and needs routing as
such instead of dropping or diverting back towards Squid.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVYcUAAoJELJo5wb/XPRjYAwH/RQXc3S7bH5w/5lYdLyYRQ/H
Y2GT88IdeM8gz067OMkGR7lTwzIMK4EVmb9GEQJYhLNznZB8hFDYnNDmSL3spB9F
mxHYZ4BVlDTlMLi4qHf05mrpsdEodrhF9A3H8YmzXLWHx5sxyukLfR/R7UJlP14K
S9xZa3KhKE/SERNM8iwPggNdtzdjrBhs6U2AUhmpeNjEgEiOgkgT7XRm724hMj21
9CH5kgGt4qiE5plPfSqTdyiJSBbQF5NL50g8/NSrQ4bRilTD9Fdf+kVRoSnscrAF
ViQKzc5L8s+3YMmVmXyynM0s4YPy5bj9Qgqi5nWjQG9gSBRLeedsyK1rdlcQmbQ=
=CQHA
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141102/1cb6c79f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Sample.pcap
Type: application/octet-stream
Size: 880 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141102/1cb6c79f/attachment.obj>

From dan at getbusi.com  Sun Nov  2 02:29:17 2014
From: dan at getbusi.com (Dan Charlesworth)
Date: Sun, 2 Nov 2014 13:29:17 +1100
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <1414670766.2891.14.camel@JamesiMac>
References: <1414670766.2891.14.camel@JamesiMac>
Message-ID: <DA0A43B4-07D2-41AF-A42A-49778C21111A@getbusi.com>

And now I?m going to bump this thread because I too would like to hear from anyone that has an example of Peek/splice functioning in 3.5 beta. 

> On 30 Oct 2014, at 11:06 pm, James Lay <jlay at slave-tothe-box.net> wrote:
> 
> Hello all,
> 
> Here is my complete config for trying out peek/splice.  This currently
> does not work..is there something obvious that I'm mission?  Current
> error is:
> 
> Oct 30 06:03:14 gateway squid: 192.168.1.110 - - [30/Oct/2014:06:03:14
> -0600] "GET https://www.google.com/ HTTP/1.1" 503 3854
> TAG_NONE:HIER_NONE
> 
> and on the page I get a 71 protocol error and a SSL3_WRITE_PENDING:bad
> write retry.
> 
> Thank you.
> 
> James
> 
> acl localnet src 192.168.1.0/24
> 
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> 
> acl CONNECT method CONNECT
> acl allowed_sites url_regex "/opt/etc/squid/url.txt"
> acl all_others dst all
> acl SSL method CONNECT
> 
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow manager localhost
> http_access deny manager
> 
> http_access allow allowed_sites
> http_access deny all_others 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> icp_access deny all
> 
> sslproxy_cert_error allow all
> 
> sslproxy_options ALL
> sslproxy_flags DONT_VERIFY_PEER
> 
> ssl_bump peek all
> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/etc/squid/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> 
> http_port 192.168.1.253:3128 intercept
> 
> https_port 192.168.1.253:3129 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
> options=ALL sslflags=NO_SESSION_REUSE
> 
> always_direct allow all
> 
> 
> access_log syslog:daemon.info common
> 
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (cgi-bin|\?)	0	0%	0
> refresh_pattern .		0	20%	4320
> 
> icp_port 3130
> 
> coredump_dir /opt/var
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From mikes at kuentos.guam.net  Sun Nov  2 06:45:12 2014
From: mikes at kuentos.guam.net (Michael D. Setzer II)
Date: Sun, 02 Nov 2014 16:45:12 +1000
Subject: [squid-users] Squid with CUPS print server?
Message-ID: <5455D2F8.26899.AEF0E@mikes.kuentos.guam.net>

I have a server with both squid and cups server running that works fine, but 
was recently setting up a windows 7 machine to use a printer, but it wouldn't 
even connect until I set the Internet options in IE to not use proxy for the IP 
address of the print server. Rather than having to do this on each machine is 
there an option in squid that would allow the CUPS server to work without 
blocking it on each machine?

Thanks.

I did just add the 631 port to the Safe Ports, but have not checked it after 
since that change.


+----------------------------------------------------------+
  Michael D. Setzer II -  Computer Science Instructor      
  Guam Community College  Computer Center                  
  mailto:mikes at kuentos.guam.net                            
  mailto:msetzerii at gmail.com
  http://www.guam.net/home/mikes
  Guam - Where America's Day Begins                        
  G4L Disk Imaging Project maintainer 
  http://sourceforge.net/projects/g4l/
+----------------------------------------------------------+

http://setiathome.berkeley.edu (Original)
Number of Seti Units Returned:  19,471
Processing time:  32 years, 290 days, 12 hours, 58 minutes
(Total Hours: 287,489)

BOINC at HOME CREDITS
ROSETTA     21182826.617012   |   SETI        36598721.404346
ABC         16613838.513356   |   EINSTEIN    38703585.592899



From yogesh_dg at yahoo.com  Sun Nov  2 06:54:08 2014
From: yogesh_dg at yahoo.com (yogesh_dg at yahoo.com)
Date: Sat, 1 Nov 2014 23:54:08 -0700
Subject: [squid-users] Squid with wccp and gre return
In-Reply-To: <54558714.6090805@treenet.co.nz>
Message-ID: <1414911248.28156.YahooMailAndroidMobile@web141701.mail.bf1.yahoo.com>

I figured out the issue. Thanks all.

Amos : just fyi wccp cp runs over udp never gre.

Wccp gre is for the redirected traffic.

You can read wccp rfc and use tcpdump to learn more.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141101/e1fb2ff9/attachment.htm>

From eliezer at ngtech.co.il  Sun Nov  2 08:42:11 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 02 Nov 2014 10:42:11 +0200
Subject: [squid-users] Squid with CUPS print server?
In-Reply-To: <5455D2F8.26899.AEF0E@mikes.kuentos.guam.net>
References: <5455D2F8.26899.AEF0E@mikes.kuentos.guam.net>
Message-ID: <5455EE63.5010100@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Michael,

You are on the right track.
Since you setup windows on each client with exception you can try to
use wpad\pac files.
There are couple ways to distribute the file.
- - agent which gets\pulls the wpad\pac file or full settings.
- - manually setting it up on each machine(pointing to the pac file)
- - use auto proxy discovery using dns and\or dhcp.

If you are interested in something just ask about it.

Eliezer

On 11/02/2014 08:45 AM, Michael D. Setzer II wrote:
> I have a server with both squid and cups server running that works
> fine, but was recently setting up a windows 7 machine to use a
> printer, but it wouldn't even connect until I set the Internet
> options in IE to not use proxy for the IP address of the print
> server. Rather than having to do this on each machine is there an
> option in squid that would allow the CUPS server to work without 
> blocking it on each machine?
> 
> Thanks.
> 
> I did just add the 631 port to the Safe Ports, but have not checked
> it after since that change.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUVe5jAAoJENxnfXtQ8ZQUy6QH/A7j9JMGdKESMix572cBrCjf
4chEqPIECZk0Dk9TfiCdrm5UA+o235rWJTMwgKsWyp6xMGie5QYS8dYAt26Lo4Hd
VTo8JTzvBn7P0OHZY2BaECYI0Jb5w7f+sfOjSYoRwPZE7NlkFgv/BXvCCLLTVCbo
bt40bIkTnvubMtoq2mab+HwelfPk2ugQjKnM06nflbFSfe/DzF5MzZIhL5q11Y9e
dTF3y05yylD/tWPVTgnnh3U4Adw6gJRtGyujL0KCE6HXdNM/soSMpfXySah1n7dc
drlmDyrf+rxnOltHdDQZdCWrh2wxO4ot/eRDc8LXZO6tLRyoD8qUZtnH8+e/NzU=
=pJYA
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Sun Nov  2 08:51:03 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 02 Nov 2014 10:51:03 +0200
Subject: [squid-users] Squid with wccp and gre return
In-Reply-To: <420932010.112911.1414892497303.JavaMail.yahoo@jws10613.mail.bf1.yahoo.com>
References: <54558714.6090805@treenet.co.nz>
 <420932010.112911.1414892497303.JavaMail.yahoo@jws10613.mail.bf1.yahoo.com>
Message-ID: <5455F077.3080700@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey There,

There are some parts which missing from your picture to me.
First I wanted to ask if you had the chance to look at what I have
written about the subject:
http://wiki.squid-cache.org/ConfigExamples/UbuntuTproxy4Wccp2

It might not answer your question but it's important to first
understand the scenario from both IP layer etc.

Your first email is a "Re" which I am not able track back into the
original thread.

If you can please describe you topology and not like this:
|   |
|   |  |   |   |   |   |   |
| Ubuntu 10.04 Transparent squid3 proxy using wccp2 on c...See my last
post for more defined questions = ID:37147216 ADSLMODEM-GATEWAY -->
2801-ROUTING-NAT --> 3550-L3-ACL-VLAN --> 3550-L2-SWIT... |
|  |


It's not clear to me and probably to some other.
It's cryptic and non informative.

Thanks,
Eliezer

On 11/02/2014 03:41 AM, Yogesh Gawankar wrote:
> Hello Amos Thanks for your reply.I don't think I explained my
> problem well
> 
> With wccp gre redirection, when the tcp syn reaches the cisco
> router that is set up for wccp redirection , it redirects it to the
> CE using  wccp gre. Squid will respond with syn-ack and send it
> back to the router again via wccp-gre. However this is not
> happening. it sends it back using regular Gre (same as wccp-gre but
> with no wccp header)
> 
> 
> Please refer to my attached capture  .I need to get squid to put
> that wccp header on the return packet.In squid.conf I already
> specified gre return. Are  you saying squid (too!) cannot do gre
> return for return traffic and does normal ip forwarding?
> 
> I got squid version 3 running on ubuntu.
> 
> Thanks and regards
> 
> Yogesh Gawankar
> 
> 
> On Sunday, November 2, 2014 6:52 AM, Amos Jeffries
> <squid3 at treenet.co.nz> wrote:
> 
> 
> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
> 
> On 2/11/2014 7:05 a.m., Yogesh Gawankar wrote:
>> Hello
>> 
>> 
>> I am trying to get squid to send return traffic to the cisco 
>> router via  a gre tunnel. I am observing that squid returns
>> traffic in gre tunnel  this is ip in ip gre not wccp-gre . The
>> firewall is dropping this packet as expected. I have gone through
>> many posts online so kindly respond if you know how to get it to
>> send return traffic via wccp-gre (not standard gre) or if you had
>> any suggestions for me in terms of linux  kernel change.
> 
> WCCP-GRE is for transmitting WCCP protocol control packets
> informing the router whether the proxy is still available or not.
> That is all.
> 
> The HTTP traffic from router to Squid (wccp_forwarding_method)
> goes through the GRE as if it were regular GRE.
> 
> The HTTP traffic from Squid to router (wccp_return_method and
> regular Squid->Internet connections) is normal traffic and needs
> routing as such instead of dropping or diverting back towards
> Squid.
> 
> Amos -----BEGIN PGP SIGNATURE----- Version: GnuPG v2.0.22
> (MingW32)
> 
> iQEcBAEBAgAGBQJUVYcUAAoJELJo5wb/XPRjYAwH/RQXc3S7bH5w/5lYdLyYRQ/H 
> Y2GT88IdeM8gz067OMkGR7lTwzIMK4EVmb9GEQJYhLNznZB8hFDYnNDmSL3spB9F 
> mxHYZ4BVlDTlMLi4qHf05mrpsdEodrhF9A3H8YmzXLWHx5sxyukLfR/R7UJlP14K 
> S9xZa3KhKE/SERNM8iwPggNdtzdjrBhs6U2AUhmpeNjEgEiOgkgT7XRm724hMj21 
> 9CH5kgGt4qiE5plPfSqTdyiJSBbQF5NL50g8/NSrQ4bRilTD9Fdf+kVRoSnscrAF 
> ViQKzc5L8s+3YMmVmXyynM0s4YPy5bj9Qgqi5nWjQG9gSBRLeedsyK1rdlcQmbQ= 
> =CQHA -----END PGP SIGNATURE----- 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUVfB3AAoJENxnfXtQ8ZQUs1QH/jfWW/YdERQvb/jZCNj1kxrC
Cq3ryO4NrhcdQcJnUtdb4BqpdwLoTT4A7J3OXiCmbY2hIsD8cC6a1bwldsOaM7Sb
fAhoM1MfoFUvbGXcAZs9EPhbClpmXRfzNqn6JQlt4+xLdh8Fzb6OHj/KtsV//xBN
6uPCHZ7C+ONaXOPREz6ujuHGlWa6LbjGR1Dop1Z/N18w24qoZyajnLnyqfnUnlBl
iNc9MeKKPlXs6ojsdGuUdXKmjm4EBNfKAnejuWKI9UODLeJYGldIjYaCUMycLZ8p
/6LkH1xg/6mWNX30HuOtrHAVgey/VB91kEINrHZ2ZYmE27y3+9ZqSC4HFDAFdDg=
=9Hm6
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Sun Nov  2 11:56:48 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 2 Nov 2014 03:56:48 -0800
Subject: [squid-users] traffic out is less than traffic in with squid 3.4.3 ,
	is that not good ?
Message-ID: <000001cff694$1636b330$42a41990$@netstream.ps>

Hi all ,

 

Sofar im using squid 3.4.3 with only ram caching ,..no cache_dir configured

But...

traffic out is less than traffic in with squid 3.4.3 , is that not good ?

 

from cachemanager it says there is hit ratio and I can see MEM_HIT in
access.log

 

but wondering why the out is less than the in  traffic???

Here is also cachemanager for 5 minutes for my squid

 

sample_start_time = 1414928723.599041 (Sun, 02 Nov 2014 11:45:23 GMT)

sample_end_time = 1414929023.695426 (Sun, 02 Nov 2014 11:50:23 GMT)

client_http.requests = 676.277174/sec

client_http.hits = 110.762874/sec

client_http.errors = 14.836604/sec

client_http.kbytes_in = 673.863808/sec

client_http.kbytes_out = 41258.537372/sec

client_http.all_median_svc_time = 0.152583 seconds

client_http.miss_median_svc_time = 0.200171 seconds

client_http.nm_median_svc_time = 0.000000 seconds

client_http.nh_median_svc_time = 0.059506 seconds

client_http.hit_median_svc_time = 0.000000 seconds

server.all.requests = 578.707579/sec

server.all.errors = 0.000000/sec

server.all.kbytes_in = 41550.242682/sec

server.all.kbytes_out = 665.830506/sec

server.http.requests = 578.707579/sec

server.http.errors = 0.000000/sec

server.http.kbytes_in = 41550.242682/sec

server.http.kbytes_out = 665.830506/sec

server.ftp.requests = 0.000000/sec

server.ftp.errors = 0.000000/sec

server.ftp.kbytes_in = 0.000000/sec

server.ftp.kbytes_out = 0.000000/sec

server.other.requests = 0.000000/sec

server.other.errors = 0.000000/sec

server.other.kbytes_in = 0.000000/sec

server.other.kbytes_out = 0.000000/sec

icp.pkts_sent = 0.000000/sec

icp.pkts_recv = 0.000000/sec

icp.queries_sent = 0.000000/sec

icp.replies_sent = 0.000000/sec

icp.queries_recv = 0.000000/sec

icp.replies_recv = 0.000000/sec

icp.replies_queued = 0.000000/sec

icp.query_timeouts = 0.000000/sec

icp.kbytes_sent = 0.000000/sec

icp.kbytes_recv = 0.000000/sec

icp.q_kbytes_sent = 0.000000/sec

icp.r_kbytes_sent = 0.000000/sec

icp.q_kbytes_recv = 0.000000/sec

icp.r_kbytes_recv = 0.000000/sec

icp.query_median_svc_time = 0.000000 seconds

icp.reply_median_svc_time = 0.000000 seconds

dns.median_svc_time = 0.000000 seconds

unlink.requests = 0.000000/sec

page_faults = 0.000000/sec

select_loops = 20149.994911/sec

select_fds = 22190.891043/sec

average_select_fd_period = 0.000000/fd

median_select_fds = 0.000000

swap.outs = 0.000000/sec

swap.ins = 0.000000/sec

swap.files_cleaned = 0.000000/sec

aborted_requests = 32.319864/sec

syscalls.disk.opens = 11.289952/sec

syscalls.disk.closes = 11.289952/sec

syscalls.disk.reads = 0.000000/sec

syscalls.disk.writes = 0.000000/sec

syscalls.disk.seeks = 0.000000/sec

syscalls.disk.unlinks = 0.000000/sec

syscalls.sock.accepts = 654.237062/sec

syscalls.sock.sockets = 261.768895/sec

syscalls.sock.connects = 261.768895/sec

syscalls.sock.binds = 261.742228/sec

syscalls.sock.closes = 586.447528/sec

syscalls.sock.reads = 8300.327849/sec

syscalls.sock.writes = 12925.184121/sec

syscalls.sock.recvfroms = 165.812608/sec

syscalls.sock.sendtos = 96.559577/sec

cpu_time = 400.387132 seconds

wall_time = 1200.005535 seconds

cpu_usage = 33.365440%

 

 

 

plz help me if I can optimize

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141102/72219db2/attachment.htm>

From squid3 at treenet.co.nz  Sun Nov  2 14:16:35 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 03 Nov 2014 03:16:35 +1300
Subject: [squid-users] traffic out is less than traffic in with squid
 3.4.3 , is that not good ?
In-Reply-To: <000001cff694$1636b330$42a41990$@netstream.ps>
References: <000001cff694$1636b330$42a41990$@netstream.ps>
Message-ID: <54563CC3.8040402@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 12:56 a.m., Ahmed Allzaeem wrote:
> Hi all ,
> 
> 
> 
> Sofar im using squid 3.4.3 with only ram caching ,..no cache_dir
> configured
> 
> But...
> 
> traffic out is less than traffic in with squid 3.4.3 , is that not
> good ?
> 
> 
> 
> from cachemanager it says there is hit ratio and I can see MEM_HIT
> in access.log
> 
> 
> 
> but wondering why the out is less than the in  traffic???

The "out" which is largest is the traffic going to clients. That is fine.

The "in" from servers being larger despite a 110 req/sec HIT-rate is a
bit worrying. Tuning depends on what your traffic is though.


> 
> Here is also cachemanager for 5 minutes for my squid
> 
> 
> 
> sample_start_time = 1414928723.599041 (Sun, 02 Nov 2014 11:45:23
> GMT)
> 
> sample_end_time = 1414929023.695426 (Sun, 02 Nov 2014 11:50:23
> GMT)
> 
> client_http.requests = 676.277174/sec
> 
> client_http.hits = 110.762874/sec
> 
> client_http.errors = 14.836604/sec
> 
> client_http.kbytes_in = 673.863808/sec
> 
> client_http.kbytes_out = 41258.537372/sec
> 
> client_http.all_median_svc_time = 0.152583 seconds
> 
> client_http.miss_median_svc_time = 0.200171 seconds
> 
> client_http.nm_median_svc_time = 0.000000 seconds
> 
> client_http.nh_median_svc_time = 0.059506 seconds
> 
> client_http.hit_median_svc_time = 0.000000 seconds
> 
> server.all.requests = 578.707579/sec
> 
> server.all.errors = 0.000000/sec
> 
> server.all.kbytes_in = 41550.242682/sec
> 
> server.all.kbytes_out = 665.830506/sec
> 
> server.http.requests = 578.707579/sec
> 
> server.http.errors = 0.000000/sec
> 
> server.http.kbytes_in = 41550.242682/sec
> 
> server.http.kbytes_out = 665.830506/sec


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVjzDAAoJELJo5wb/XPRjp3EH/inz+JhTi/Gj2h1HAkU5R9eu
DX3fodA6dCZ04XN5C1F03OyxCMGvSiLjf54V3IuZAaweeHd1jjauMDsu5QJvVZyF
bnBzkhMjFytfAsf8Q4VerF4hrah+bjQ19Ct7aZM3Es/q1e8KWU061MwpQ1p+6NQV
NjyeKma/55Osg6criS7ZF8/5PuQstIOKnkS5nEOMKX+gmzu0rs0mveYOJTvNttHe
h9JaXbql0zS6wJd1j6xwh4l+Ip7ifBdwg6VlN8lkc5n1oy5h4i48qasnM9yYzOM1
LCW2iL5f7dkTWcsaxiIn+c2RA7pI/vvs7+0Bjy3W09mH0hHttTetWkTsfz4v3JQ=
=C8PX
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Sun Nov  2 15:22:03 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 2 Nov 2014 07:22:03 -0800
Subject: [squid-users] traffic out is less than traffic in with squid
	3.4.3 , is that not good ?
In-Reply-To: <54563CC3.8040402@treenet.co.nz>
References: <000001cff694$1636b330$42a41990$@netstream.ps>
 <54563CC3.8040402@treenet.co.nz>
Message-ID: <000a01cff6b0$c29cd9d0$47d68d70$@netstream.ps>

Hi Amos , 
Im wondering wt should I do to investigate this issue ???

The out is less than "IN" traffic ???!!!!

When i was using aufs squid without smp , the "out" was always greater than the "in"

For now , im using smp with memory caching  I mean no hardisks with rock or aufs has been configured.


Can you help me wt should I do ? or how to calculate wt am saving in Mbps right now ??



regards

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, November 2, 2014 6:17 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] traffic out is less than traffic in with squid 3.4.3 , is that not good ?

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 12:56 a.m., Ahmed Allzaeem wrote:
> Hi all ,
> 
> 
> 
> Sofar im using squid 3.4.3 with only ram caching ,..no cache_dir 
> configured
> 
> But...
> 
> traffic out is less than traffic in with squid 3.4.3 , is that not 
> good ?
> 
> 
> 
> from cachemanager it says there is hit ratio and I can see MEM_HIT in 
> access.log
> 
> 
> 
> but wondering why the out is less than the in  traffic???

The "out" which is largest is the traffic going to clients. That is fine.

The "in" from servers being larger despite a 110 req/sec HIT-rate is a bit worrying. Tuning depends on what your traffic is though.


> 
> Here is also cachemanager for 5 minutes for my squid
> 
> 
> 
> sample_start_time = 1414928723.599041 (Sun, 02 Nov 2014 11:45:23
> GMT)
> 
> sample_end_time = 1414929023.695426 (Sun, 02 Nov 2014 11:50:23
> GMT)
> 
> client_http.requests = 676.277174/sec
> 
> client_http.hits = 110.762874/sec
> 
> client_http.errors = 14.836604/sec
> 
> client_http.kbytes_in = 673.863808/sec
> 
> client_http.kbytes_out = 41258.537372/sec
> 
> client_http.all_median_svc_time = 0.152583 seconds
> 
> client_http.miss_median_svc_time = 0.200171 seconds
> 
> client_http.nm_median_svc_time = 0.000000 seconds
> 
> client_http.nh_median_svc_time = 0.059506 seconds
> 
> client_http.hit_median_svc_time = 0.000000 seconds
> 
> server.all.requests = 578.707579/sec
> 
> server.all.errors = 0.000000/sec
> 
> server.all.kbytes_in = 41550.242682/sec
> 
> server.all.kbytes_out = 665.830506/sec
> 
> server.http.requests = 578.707579/sec
> 
> server.http.errors = 0.000000/sec
> 
> server.http.kbytes_in = 41550.242682/sec
> 
> server.http.kbytes_out = 665.830506/sec


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVjzDAAoJELJo5wb/XPRjp3EH/inz+JhTi/Gj2h1HAkU5R9eu
DX3fodA6dCZ04XN5C1F03OyxCMGvSiLjf54V3IuZAaweeHd1jjauMDsu5QJvVZyF
bnBzkhMjFytfAsf8Q4VerF4hrah+bjQ19Ct7aZM3Es/q1e8KWU061MwpQ1p+6NQV
NjyeKma/55Osg6criS7ZF8/5PuQstIOKnkS5nEOMKX+gmzu0rs0mveYOJTvNttHe
h9JaXbql0zS6wJd1j6xwh4l+Ip7ifBdwg6VlN8lkc5n1oy5h4i48qasnM9yYzOM1
LCW2iL5f7dkTWcsaxiIn+c2RA7pI/vvs7+0Bjy3W09mH0hHttTetWkTsfz4v3JQ=
=C8PX
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Sun Nov  2 16:09:43 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 2 Nov 2014 08:09:43 -0800
Subject: [squid-users] traffic out is less than traffic in with
	squid	3.4.3 , is that not good ?
In-Reply-To: <000a01cff6b0$c29cd9d0$47d68d70$@netstream.ps>
References: <000001cff694$1636b330$42a41990$@netstream.ps>
 <54563CC3.8040402@treenet.co.nz>
 <000a01cff6b0$c29cd9d0$47d68d70$@netstream.ps>
Message-ID: <000b01cff6b7$6b43fea0$41cbfbe0$@netstream.ps>

Amos this is a sample from iptraf shot :

IPTraf
+ Statistics for eth0 ---------------------------------------------------------------------------------------------------------------+
|                                                                                                                                    |
|               Total      Total    Incoming   Incoming    Outgoing   Outgoing                                                       |
|             Packets      Bytes     Packets      Bytes     Packets      Bytes                                                       |
| Total:      3442326      3523M     2054295      1765M     1388031      1757M                                                       |
| IPv4:       3442326      3473M     2054295      1735M     1388031      1738M                                                       |
| IPv6:             0          0           0          0           0          0                                                       |
| TCP:        3431872      3471M     2049057      1734M     1382815      1737M                                                       |
| UDP:          10436    1455945        5238    1034448        5198     421497                                                       |
| ICMP:            18       2378           0          0          18       2378                                                       |
| Other IP:         0          0           0          0           0          0                                                       |
| Non-IP:           0          0           0          0           0          0                                                       |
|                                                                                                                                    |
|                                                                                                                                    |
| Total rates:     926677.1 kbits/sec        Broadcast packets:            0                                                         |
|                  112213.8 packets/sec      Broadcast bytes:              0                                                         |
|                                                                                                                                    |
| Incoming rates:  465102.4 kbits/sec                                                                                                |
|                   67284.0 packets/sec                                                                                              |
|                                            IP checksum errors:           0                                                         |
| Outgoing rates:  462996.9 kbits/sec                                                                                                |
|                   44929.8 packets/sec                                                                                              |
|                                                                                                                                    |
|                                                                                                                                    |
|                                                     

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ahmed Allzaeem
Sent: Sunday, November 2, 2014 7:22 AM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] traffic out is less than traffic in with squid 3.4.3 , is that not good ?

Hi Amos ,
Im wondering wt should I do to investigate this issue ???

The out is less than "IN" traffic ???!!!!

When i was using aufs squid without smp , the "out" was always greater than the "in"

For now , im using smp with memory caching  I mean no hardisks with rock or aufs has been configured.


Can you help me wt should I do ? or how to calculate wt am saving in Mbps right now ??



regards

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, November 2, 2014 6:17 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] traffic out is less than traffic in with squid 3.4.3 , is that not good ?

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 12:56 a.m., Ahmed Allzaeem wrote:
> Hi all ,
> 
> 
> 
> Sofar im using squid 3.4.3 with only ram caching ,..no cache_dir 
> configured
> 
> But...
> 
> traffic out is less than traffic in with squid 3.4.3 , is that not 
> good ?
> 
> 
> 
> from cachemanager it says there is hit ratio and I can see MEM_HIT in 
> access.log
> 
> 
> 
> but wondering why the out is less than the in  traffic???

The "out" which is largest is the traffic going to clients. That is fine.

The "in" from servers being larger despite a 110 req/sec HIT-rate is a bit worrying. Tuning depends on what your traffic is though.


> 
> Here is also cachemanager for 5 minutes for my squid
> 
> 
> 
> sample_start_time = 1414928723.599041 (Sun, 02 Nov 2014 11:45:23
> GMT)
> 
> sample_end_time = 1414929023.695426 (Sun, 02 Nov 2014 11:50:23
> GMT)
> 
> client_http.requests = 676.277174/sec
> 
> client_http.hits = 110.762874/sec
> 
> client_http.errors = 14.836604/sec
> 
> client_http.kbytes_in = 673.863808/sec
> 
> client_http.kbytes_out = 41258.537372/sec
> 
> client_http.all_median_svc_time = 0.152583 seconds
> 
> client_http.miss_median_svc_time = 0.200171 seconds
> 
> client_http.nm_median_svc_time = 0.000000 seconds
> 
> client_http.nh_median_svc_time = 0.059506 seconds
> 
> client_http.hit_median_svc_time = 0.000000 seconds
> 
> server.all.requests = 578.707579/sec
> 
> server.all.errors = 0.000000/sec
> 
> server.all.kbytes_in = 41550.242682/sec
> 
> server.all.kbytes_out = 665.830506/sec
> 
> server.http.requests = 578.707579/sec
> 
> server.http.errors = 0.000000/sec
> 
> server.http.kbytes_in = 41550.242682/sec
> 
> server.http.kbytes_out = 665.830506/sec


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVjzDAAoJELJo5wb/XPRjp3EH/inz+JhTi/Gj2h1HAkU5R9eu
DX3fodA6dCZ04XN5C1F03OyxCMGvSiLjf54V3IuZAaweeHd1jjauMDsu5QJvVZyF
bnBzkhMjFytfAsf8Q4VerF4hrah+bjQ19Ct7aZM3Es/q1e8KWU061MwpQ1p+6NQV
NjyeKma/55Osg6criS7ZF8/5PuQstIOKnkS5nEOMKX+gmzu0rs0mveYOJTvNttHe
h9JaXbql0zS6wJd1j6xwh4l+Ip7ifBdwg6VlN8lkc5n1oy5h4i48qasnM9yYzOM1
LCW2iL5f7dkTWcsaxiIn+c2RA7pI/vvs7+0Bjy3W09mH0hHttTetWkTsfz4v3JQ=
=C8PX
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sudakov at sibptus.tomsk.ru  Sun Nov  2 18:00:25 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Mon, 3 Nov 2014 00:00:25 +0600
Subject: [squid-users] Kerberos Authentication Failing for Windows
 7+with BH gss_accept_sec_context() failed
In-Reply-To: <m32m6c$teg$1@ger.gmane.org>
References: <40E1E0E7-50C6-4117-94AA-50B06573430A@gmail.com>
 <m2eqo7$cf9$1@ger.gmane.org>
 <FD6832B9-3F1F-48C6-A76F-47A224F1697B@gmail.com>
 <m2g7l9$1ee$1@ger.gmane.org>
 <94F74226-F24B-4910-95B7-B86ACE815995@gmail.com>
 <m2m7g0$9l7$1@ger.gmane.org>
 <b4adceec-5a53-4212-b16c-106237fc4504@Pedros-iPhone>
 <m2mbc1$b54$1@ger.gmane.org>
 <09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E@gmail.com>
 <m32m6c$teg$1@ger.gmane.org>
Message-ID: <20141102180025.GB23817@admin.sibptus.tomsk.ru>

Markus Moeller wrote:
> Hi Pedro,
> 
>     I looked at your captures and I observed something similar to
>     Victor???s issue.   I see KRB5KRB_AP_ERR_MODIFIED and then the
>     use of the name of the AD object (e.g. proxy$) instead of
>     HTTP/<proxy fqdn>.   

Dear Pedro,

If it is so as Markus wrote, then adding another principal to squid's
keytab (namely 'proxy$@YOUR.REALM' with the same key identical to that
of 'HTTP/<proxy fqdn>@YOUR.REALM' could help you as a workaround. Just
add it manually with ktutil.

However, I am eager to know what could be causing such weird tickets
to be issued, but I think only a Windows expert can tell. After all,
the key in the tickets is correct, only the principal name is changed.
I only suspect that the name is changed when the client sets the
Canonicalize option in the request, and not all clients do that.

<rant>I have not been able to find such an expert, most Windows admins I
know are GUI mouse boys without thorough understanding of Windows
internals.</rant>


> I also see that you have more than one AD
>     server and I assume there is a sync problem between your AD
>     servers ( You said it start working after removing an unused AD
>     server which would support y assumption). 

If it were a DC sync problem, then probably the key/password would be
incorrect too. I blame the Canonicalize flag, but I don't understand
the logic behind it.

-- 
Victor Sudakov 
Tomsk, Russia
Russian Barefoot FAQ at http://www.barefooters.ru/barefoot.txt


From jlay at slave-tothe-box.net  Sun Nov  2 22:12:47 2014
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 02 Nov 2014 15:12:47 -0700
Subject: [squid-users] Assistance with knowing what I'm really trying to do
Message-ID: <1414966367.3415.12.camel@JamesiMac>

A weird question....I guess I need to find out exactly what I'm wanting
before going further with trying to get peek to work.  So here's a small
example of what I currently have.  From my .conf file:

acl broken_sites dst 23.192.0.0/11
http_access allow broken_sites
ssl_bump splice broken_sites

logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
Sh %ssl::>cert_subject

This currently works (no cert_subject though)...log entry shown:

Nov  2 14:23:24 gateway (squid-1): 192.168.1.102 - -
[02/Nov/2014:14:23:24 -0700] "CONNECT 23.211.233.155:443 HTTP/1.1" 200
4229 TCP_TUNNEL:ORIGINAL_DST -

Now this is required as the above will not function if bumped.

At work, we use a commercial proxy which we do not use any ssl
inspection.  These connections show up as, for example:

tcp://www.whateversite.com  TCP_DENIED

And that's what I'm hoping to achieve here...determine what the site is,
and allow or denied it, without having to actually do any SSL
inspection.  Will peek/stare accomplish this?  Or am I restricted to
bump/inspection only, which for a fair amount of sites (facebook,
instagram, google mail, etc) does not work.  Thanks all...I appreciate
any advice.

James



From squid3 at treenet.co.nz  Mon Nov  3 04:22:29 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 03 Nov 2014 17:22:29 +1300
Subject: [squid-users] Assistance with knowing what I'm really trying to
 do
In-Reply-To: <1414966367.3415.12.camel@JamesiMac>
References: <1414966367.3415.12.camel@JamesiMac>
Message-ID: <54570305.1040003@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 11:12 a.m., James Lay wrote:
> A weird question....I guess I need to find out exactly what I'm
> wanting before going further with trying to get peek to work.  So
> here's a small example of what I currently have.  From my .conf
> file:
> 
> acl broken_sites dst 23.192.0.0/11 http_access allow broken_sites 
> ssl_bump splice broken_sites
> 
> logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st
> %Ss:% Sh %ssl::>cert_subject
> 
> This currently works (no cert_subject though)...log entry shown:
> 
> Nov  2 14:23:24 gateway (squid-1): 192.168.1.102 - - 
> [02/Nov/2014:14:23:24 -0700] "CONNECT 23.211.233.155:443 HTTP/1.1"
> 200 4229 TCP_TUNNEL:ORIGINAL_DST -

The TCP_TUNNEL tag shows that no bumping was done. Thus no details
from inside the TLS transaction are available.

"ssl_bump splice" means the same as "ssl_bump none" ... use the
non-bumped CONNECT handling.


> 
> Now this is required as the above will not function if bumped.
> 
> At work, we use a commercial proxy which we do not use any ssl 
> inspection.  These connections show up as, for example:
> 
> tcp://www.whateversite.com  TCP_DENIED
> 
> And that's what I'm hoping to achieve here...determine what the
> site is, and allow or denied it, without having to actually do any
> SSL inspection.  Will peek/stare accomplish this?  Or am I
> restricted to bump/inspection only, which for a fair amount of
> sites (facebook, instagram, google mail, etc) does not work.
> Thanks all...I appreciate any advice.

That depends on how you define "SSL inspection". If the TLS details
are not inspected with peek - then the details you want will not be
available.
 You can see that in the above example.

The ssl_bump access controls are now tested repeatedly in a series of
"steps" with the first matching action which is valid at the step
being performed. So I suspect the only working configurations will use
the at_step ACL type to restrict where the rest of the tests will be
performed.

If you look at the documentation for that ACL it shows the steps are
only before/after the client and server Hello messages.

I think you want to peek at step SslBump1 and splice at step SslBump3.
Or maybe peek at step 1 and 2 then splice at 3.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUVwMDAAoJELJo5wb/XPRjVsIIALi2WxQ6HzUEZcLCWRnDRHIk
m8C/HRZjYGT0ZMs0V6MEWv3ijbta+jeH/1xvNhcMk0y0LiI0Xcw5QKIy4JrBcbJw
IyhGcbIeKOuGiOMsvHAS5mL4FV333ql+aY1Ujp3MTjJ2MymXoInTg/FHZqz1HqaN
M95J4DQuwyz/ZaT/hsp4eTyBcV8ejuyaKDOo0XmSjwon1RapeSUZi8ohZHMGjb3G
R32rfIiqJX8z0PFaDX3wzVASFQ6PpgRPojtjSSjcATcSQm7LPKgXy1+jUPYcogLd
K92WrmeN2Y/P+08dUEe2QhGIiORPXAW5DhxnbxfCMInl9981Fbxm9KjKInL2+RY=
=J9U6
-----END PGP SIGNATURE-----


From andrew_var at yahoo.com  Mon Nov  3 06:35:33 2014
From: andrew_var at yahoo.com (Avery_GoodMan)
Date: Sun, 2 Nov 2014 22:35:33 -0800 (PST)
Subject: [squid-users] Unusually High Average HTTP requests per minute
Message-ID: <1414996533699-4668172.post@n4.nabble.com>

Hi All,

At the very outset , I would like to state that I have only recently started
administering a squid Proxy Server( < 3 months) so any omissions may pls be
condoned. The problem I am facing is this.
>From the Squid manager cache interface I see that the number of "Average
HTTP requests per minute since start" is very high, almost 15000/minute !!.
The normal load is in the region of 800-100( max 1000/minute). also if I
check the " 5 Minute Average of Counters" I see that the
"client_http.requests " also unusually high in the region of 450/sec. What
is perplexing is that in the same page the client_http.errors is also in the
region of 400/sec !! I first suspected a DoS attack and have applied some
patches to the Linux server. But there is no visible change in the above
figures reported. However In spite of all this the Mean Service time is in
the region of .5 to 1.0 secs/request and therefore the user complaints are
minimum. 

Any help in this regard to isolate and rectify the problem is greatly
appreciated.

Avery



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Unusually-High-Average-HTTP-requests-per-minute-tp4668172.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From andrew_var at yahoo.com  Mon Nov  3 07:23:51 2014
From: andrew_var at yahoo.com (Avery_GoodMan)
Date: Sun, 2 Nov 2014 23:23:51 -0800 (PST)
Subject: [squid-users] Very High Average HTTP requests per minute
Message-ID: <1414999431426-4668173.post@n4.nabble.com>

Hi All, 

At the very outset , I would like to state that I have only recently started
administering a squid Proxy Server( < 3 months) so any omissions may pls be
condoned. The problem I am facing is this. 
>From the Squid manager cache interface I see that the number of "Average
HTTP requests per minute since start" is very high, almost 15000/minute !!.
The normal load is in the region of 800-100( max 1000/minute). also if I
check the " 5 Minute Average of Counters" I see that the
"client_http.requests " also unusually high in the region of 450/sec. What
is perplexing is that in the same page the client_http.errors is also in the
region of 400/sec !! I first suspected a DoS attack and have applied some
patches to the Linux server. But there is no visible change in the above
figures reported. However In spite of all this the Mean Service time is in
the region of .5 to 1.0 secs/request and therefore the user complaints are
minimum. 

Any help in this regard to isolate and rectify the problem is greatly
appreciated. 

Squid Version : 3.4.8

Avery



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Very-High-Average-HTTP-requests-per-minute-tp4668173.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Nov  3 09:06:19 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 03 Nov 2014 22:06:19 +1300
Subject: [squid-users] Unusually High Average HTTP requests per minute
In-Reply-To: <1414996533699-4668172.post@n4.nabble.com>
References: <1414996533699-4668172.post@n4.nabble.com>
Message-ID: <5457458B.5040403@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 7:35 p.m., Avery_GoodMan wrote:
> Hi All,
> 
> At the very outset , I would like to state that I have only
> recently started administering a squid Proxy Server( < 3 months) so
> any omissions may pls be condoned. The problem I am facing is
> this. From the Squid manager cache interface I see that the number
> of "Average HTTP requests per minute since start" is very high,
> almost 15000/minute !!. The normal load is in the region of
> 800-100( max 1000/minute). also if I check the " 5 Minute Average
> of Counters" I see that the "client_http.requests " also unusually
> high in the region of 450/sec. What is perplexing is that in the
> same page the client_http.errors is also in the region of 400/sec
> !! I first suspected a DoS attack and have applied some patches to
> the Linux server. But there is no visible change in the above 
> figures reported. However In spite of all this the Mean Service
> time is in the region of .5 to 1.0 secs/request and therefore the
> user complaints are minimum.
> 
> Any help in this regard to isolate and rectify the problem is
> greatly appreciated.

You need to look in the access.log records to find out what those
requests are for exactly.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUV0WLAAoJELJo5wb/XPRj594IANg6ZBfwjKJj4er0X5kMhAD9
uMH7wZg+HOZw62WqXZMH5tuk+xzo1a6DJnP+zqyIz0QhoHf8zQNc2pl838cJAzLr
p36XaAuFOPN1ZPi1XsNjwPWT2Fs2LL86+7CO4Y0KVAra6CSLSa+YQVrSNrOBLMxh
TbCA60jXafeAYf5ymfxvYfsjNFE8PIHiUemyKOhLss+njKJmvg/ThcyOIJz4WOQZ
QqHkGn2DHhspbM34N9bWO5KgTfM8Z06yscDUukO0qE8YBbpZt78vS9g4yyerLidn
7/cTjLmtTvZRbMWM/W0hxhMpH9rj+JPYNqe2vqGJUsyJ5elu5/kb3Re7mDgr080=
=3T1x
-----END PGP SIGNATURE-----


From andrew_var at yahoo.com  Mon Nov  3 10:03:34 2014
From: andrew_var at yahoo.com (Avery_GoodMan)
Date: Mon, 3 Nov 2014 02:03:34 -0800 (PST)
Subject: [squid-users] Unusually High Average HTTP requests per minute
In-Reply-To: <5457458B.5040403@treenet.co.nz>
References: <5457458B.5040403@treenet.co.nz>
Message-ID: <1415009014262-4668175.post@n4.nabble.com>

Hi Amos,

Thank you for your reply.

I did go through the access.log file but the log entries are nothing
unusual. a lot of TCP_DENIED requests ..etc. What intrigues me is the high
client.http_errors field ! Doesn't http_error mean that these are malformed
packets ? Is there a way to configure squid to drop these packets right
away.? 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-Unusually-High-Average-HTTP-requests-per-minute-tp4668174p4668175.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From christos at chtsanti.net  Mon Nov  3 10:24:45 2014
From: christos at chtsanti.net (Christos Tsantilas)
Date: Mon, 03 Nov 2014 12:24:45 +0200
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <1414670766.2891.14.camel@JamesiMac>
References: <1414670766.2891.14.camel@JamesiMac>
Message-ID: <545757ED.5030903@chtsanti.net>

On 10/30/2014 02:06 PM, James Lay wrote:
> Hello all,
>
> Here is my complete config for trying out peek/splice.  This currently
> does not work..is there something obvious that I'm mission?  Current
> error is:
>
> Oct 30 06:03:14 gateway squid: 192.168.1.110 - - [30/Oct/2014:06:03:14
> -0600] "GET https://www.google.com/ HTTP/1.1" 503 3854
> TAG_NONE:HIER_NONE
>
> and on the page I get a 71 protocol error and a SSL3_WRITE_PENDING:bad
> write retry.

- You should use at_step acl to configure different bumping modes on 
each bumping step.

- If you used "peek" mode on SslBump1 and SslBump2 steps then on 
SslBump3 step you should use "splice". If you select "bump" the most 
possible is that you got SSL connection errors.
The "peek" mode on SslBump3 step is interpreted as "bump" mode.

-if you selected peek mode on SslBump1 and SslBump2 steps, in most 
cases, you can select only "terminate" or "splice" for SslBump3 step.

The following configuration should work:

# Bumping steps:
acl step1 at_step  SslBump1
acl step2 at_step  SslBump2
acl step3 at_step  SslBump3

# Selecting bumping mode
ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump splice step3 all

Regards,
     Christos


From squid3 at treenet.co.nz  Mon Nov  3 10:25:18 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 03 Nov 2014 23:25:18 +1300
Subject: [squid-users] Unusually High Average HTTP requests per minute
In-Reply-To: <1415009014262-4668175.post@n4.nabble.com>
References: <5457458B.5040403@treenet.co.nz>
 <1415009014262-4668175.post@n4.nabble.com>
Message-ID: <5457580E.90804@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 3/11/2014 11:03 p.m., Avery_GoodMan wrote:
> Hi Amos,
> 
> Thank you for your reply.
> 
> I did go through the access.log file but the log entries are
> nothing unusual. a lot of TCP_DENIED requests ..etc. What intrigues
> me is the high client.http_errors field ! Doesn't http_error mean
> that these are malformed packets ?

No it means 4xx or 5xx HTTP response count. eg those TCP_DENIED.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUV1gOAAoJELJo5wb/XPRj6jUIAIUSMiw7w2Bknq00kGC5Rrbg
WCoVLJhMdk5EH5RUE7jyhZy8TRuXoxTd1Qd/jW2wjS7tW6rpRqDUQQ0s74BFh5fD
VEmtTcCBTqPnoKNdLMc/rtOJE47EWKBFUT/LqKwMfuPbl+HxzhcV0jxqLWCNsN1N
rJxy+zu5lOXfbbXA1jYuy9kxH6/0MF3AVjBiIkIT0s+zbcuJD3mBXawcq3Bfk2ws
UEUTTlkoSEb3Voed+R8ftAEK7G3KTt/UFfhZvXIoG9IZCcGr1b654UIDsU75/z5L
hkPtdJ+rdpw5F238Gi1COmp2tnA0y3oA7kbL6RdUpGZZ+OKSFIssusVO6mwKsUM=
=jqO5
-----END PGP SIGNATURE-----


From jlay at slave-tothe-box.net  Mon Nov  3 12:39:16 2014
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 03 Nov 2014 05:39:16 -0700
Subject: [squid-users] Assistance with knowing what I'm really trying to
 do
In-Reply-To: <54570305.1040003@treenet.co.nz>
References: <1414966367.3415.12.camel@JamesiMac>
 <54570305.1040003@treenet.co.nz>
Message-ID: <1415018356.2899.0.camel@JamesiMac>

On Mon, 2014-11-03 at 17:22 +1300, Amos Jeffries wrote:
> On 3/11/2014 11:12 a.m., James Lay wrote:
> > A weird question....I guess I need to find out exactly what I'm
> > wanting before going further with trying to get peek to work.  So
> > here's a small example of what I currently have.  From my .conf
> > file:
> > 
> > acl broken_sites dst 23.192.0.0/11 http_access allow broken_sites 
> > ssl_bump splice broken_sites
> > 
> > logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st
> > %Ss:% Sh %ssl::>cert_subject
> > 
> > This currently works (no cert_subject though)...log entry shown:
> > 
> > Nov  2 14:23:24 gateway (squid-1): 192.168.1.102 - - 
> > [02/Nov/2014:14:23:24 -0700] "CONNECT 23.211.233.155:443 HTTP/1.1"
> > 200 4229 TCP_TUNNEL:ORIGINAL_DST -
> 
> The TCP_TUNNEL tag shows that no bumping was done. Thus no details
> from inside the TLS transaction are available.
> 
> "ssl_bump splice" means the same as "ssl_bump none" ... use the
> non-bumped CONNECT handling.
> 
> 
> > 
> > Now this is required as the above will not function if bumped.
> > 
> > At work, we use a commercial proxy which we do not use any ssl 
> > inspection.  These connections show up as, for example:
> > 
> > tcp://www.whateversite.com  TCP_DENIED
> > 
> > And that's what I'm hoping to achieve here...determine what the
> > site is, and allow or denied it, without having to actually do any
> > SSL inspection.  Will peek/stare accomplish this?  Or am I
> > restricted to bump/inspection only, which for a fair amount of
> > sites (facebook, instagram, google mail, etc) does not work.
> > Thanks all...I appreciate any advice.
> 
> That depends on how you define "SSL inspection". If the TLS details
> are not inspected with peek - then the details you want will not be
> available.
>  You can see that in the above example.
> 
> The ssl_bump access controls are now tested repeatedly in a series of
> "steps" with the first matching action which is valid at the step
> being performed. So I suspect the only working configurations will use
> the at_step ACL type to restrict where the rest of the tests will be
> performed.
> 
> If you look at the documentation for that ACL it shows the steps are
> only before/after the client and server Hello messages.
> 
> I think you want to peek at step SslBump1 and splice at step SslBump3.
> Or maybe peek at step 1 and 2 then splice at 3.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks Amos.....looks like peek/splice is where this is going, so I'll
continue this new information of at_step acl in my other thread.

James



From squid3 at treenet.co.nz  Mon Nov  3 12:56:20 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 04 Nov 2014 01:56:20 +1300
Subject: [squid-users] Assistance with knowing what I'm really trying to
 do
In-Reply-To: <1415018356.2899.0.camel@JamesiMac>
References: <1414966367.3415.12.camel@JamesiMac>
 <54570305.1040003@treenet.co.nz> <1415018356.2899.0.camel@JamesiMac>
Message-ID: <54577B74.5010204@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 4/11/2014 1:39 a.m., James Lay wrote:
> On Mon, 2014-11-03 at 17:22 +1300, Amos Jeffries wrote:
>> On 3/11/2014 11:12 a.m., James Lay wrote:
>>> A weird question....I guess I need to find out exactly what
>>> I'm wanting before going further with trying to get peek to
>>> work.  So here's a small example of what I currently have.
>>> From my .conf file:
>>> 
>>> acl broken_sites dst 23.192.0.0/11 http_access allow
>>> broken_sites ssl_bump splice broken_sites
>>> 
>>> logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs
>>> %<st %Ss:% Sh %ssl::>cert_subject
>>> 
>>> This currently works (no cert_subject though)...log entry
>>> shown:
>>> 
>>> Nov  2 14:23:24 gateway (squid-1): 192.168.1.102 - - 
>>> [02/Nov/2014:14:23:24 -0700] "CONNECT 23.211.233.155:443
>>> HTTP/1.1" 200 4229 TCP_TUNNEL:ORIGINAL_DST -
>> 
>> The TCP_TUNNEL tag shows that no bumping was done. Thus no
>> details from inside the TLS transaction are available.
>> 
>> "ssl_bump splice" means the same as "ssl_bump none" ... use the 
>> non-bumped CONNECT handling.
>> 
>> 
>>> 
>>> Now this is required as the above will not function if bumped.
>>> 
>>> At work, we use a commercial proxy which we do not use any ssl
>>>  inspection.  These connections show up as, for example:
>>> 
>>> tcp://www.whateversite.com  TCP_DENIED
>>> 
>>> And that's what I'm hoping to achieve here...determine what
>>> the site is, and allow or denied it, without having to actually
>>> do any SSL inspection.  Will peek/stare accomplish this?  Or am
>>> I restricted to bump/inspection only, which for a fair amount
>>> of sites (facebook, instagram, google mail, etc) does not
>>> work. Thanks all...I appreciate any advice.
>> 
>> That depends on how you define "SSL inspection". If the TLS
>> details are not inspected with peek - then the details you want
>> will not be available. You can see that in the above example.
>> 
>> The ssl_bump access controls are now tested repeatedly in a
>> series of "steps" with the first matching action which is valid
>> at the step being performed. So I suspect the only working
>> configurations will use the at_step ACL type to restrict where
>> the rest of the tests will be performed.
>> 
>> If you look at the documentation for that ACL it shows the steps
>> are only before/after the client and server Hello messages.
>> 
>> I think you want to peek at step SslBump1 and splice at step
>> SslBump3. Or maybe peek at step 1 and 2 then splice at 3.
>> 
>> Amos _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> Thanks Amos.....looks like peek/splice is where this is going, so
> I'll continue this new information of at_step acl in my other
> thread.

It seems Christos has chimed in on the other thread. He is the
ssl-bump author, so take whatever he says as basis.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUV3t0AAoJELJo5wb/XPRjQSgH/R/aeir6MVXtF1IRbu6TxkAN
Yl6N3e8rLWXaXrFup22tvu9/sjynqaQdblSbO+VZEU84t49Pc5z2CSFW3mdFzOlF
JSDXV+LyPjgmux8muOfbjq/cfxzfGTRNjLfzJLQEV8XoYaFYGzB4VUvy4HeoYk9Q
5s/Gv+7/jyy9zdp+3hcfEWp04X2AMnDvZNcSzbKb7oC/ztEnAF0kysdwtXKDigO/
S2TgIHxg2iXX9SRcgj6SnCOOVtyqsFYBTH2AhFMfScUAIgVUvr7chU6gxWOeAJVV
h9rCvm8wqF42UHNNg/abmIurUvkTFaUxMM3OeYi/oaWInjBR+fN/2e15WtL5X8U=
=Q4Mo
-----END PGP SIGNATURE-----


From jlay at slave-tothe-box.net  Mon Nov  3 13:00:00 2014
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 03 Nov 2014 06:00:00 -0700
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <545757ED.5030903@chtsanti.net>
References: <1414670766.2891.14.camel@JamesiMac>
 <545757ED.5030903@chtsanti.net>
Message-ID: <1415019600.2899.11.camel@JamesiMac>

On Mon, 2014-11-03 at 12:24 +0200, Christos Tsantilas wrote:
> On 10/30/2014 02:06 PM, James Lay wrote:
> > Hello all,
> >
> > Here is my complete config for trying out peek/splice.  This currently
> > does not work..is there something obvious that I'm mission?  Current
> > error is:
> >
> > Oct 30 06:03:14 gateway squid: 192.168.1.110 - - [30/Oct/2014:06:03:14
> > -0600] "GET https://www.google.com/ HTTP/1.1" 503 3854
> > TAG_NONE:HIER_NONE
> >
> > and on the page I get a 71 protocol error and a SSL3_WRITE_PENDING:bad
> > write retry.
> 
> - You should use at_step acl to configure different bumping modes on 
> each bumping step.
> 
> - If you used "peek" mode on SslBump1 and SslBump2 steps then on 
> SslBump3 step you should use "splice". If you select "bump" the most 
> possible is that you got SSL connection errors.
> The "peek" mode on SslBump3 step is interpreted as "bump" mode.
> 
> -if you selected peek mode on SslBump1 and SslBump2 steps, in most 
> cases, you can select only "terminate" or "splice" for SslBump3 step.
> 
> The following configuration should work:
> 
> # Bumping steps:
> acl step1 at_step  SslBump1
> acl step2 at_step  SslBump2
> acl step3 at_step  SslBump3
> 
> # Selecting bumping mode
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump splice step3 all
> 
> Regards,
>      Christos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks Christos,

So here's where I'm at...my full test config below:

acl localnet src 192.168.1.0/24

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 777         # multiling http

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

acl CONNECT method CONNECT
acl allowed_sites url_regex "/opt/etc/squid/url.txt"
acl all_others dst all
acl SSL method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow manager localhost
http_access deny manager

http_access allow allowed_sites
http_access deny all_others
http_access allow localnet
http_access allow localhost

http_access deny all
icp_access deny all

sslproxy_cert_error allow all

sslproxy_options ALL
sslproxy_flags DONT_VERIFY_PEER

ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump splice step3 all

http_port 192.168.1.253:3128 intercept
https_port 192.168.1.253:3129 intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
options=ALL sslflags=NO_SESSION_REUSE

always_direct allow all

logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
Sh %ssl::>cert_subject

access_log syslog:daemon.info common

The above works, but allows all sites regardless of what's in url.txt.
Additionally, there's no logging of any kind.  The allow part makes
sense as this is the last ACL, the no logging part is confusing.  If I
add:

acl broken_sites dst 69.25.139.128/25
acl broken_sites dst 65.55.0.0/16
acl broken_sites dst 72.246.0.0/16
acl broken_sites dst 54.224.0.0/12
acl broken_sites dst 17.0.0.0/8
acl broken_sites dst 69.192.0.0/16
acl broken_sites dst 209.59.128.0/18
acl broken_sites dst 173.194.0.0/16
acl broken_sites dst 107.20.0.0/14
acl broken_sites dst 54.72.0.0/13
acl broken_sites dst 54.80.0.0/12
acl broken_sites dst 23.0.0.0/12
acl broken_sites dst 23.192.0.0/11
acl broken_sites dst 8.25.205.0/24
acl broken_sites dst 75.126.0.0/16
acl broken_sites dst 74.125.0.0/16
acl broken_sites dst 192.195.204.0/24
acl broken_sites dst 96.16.0.0/15

and change to
ssl_bump peek step1 broken_sites
ssl_bump peek step2 broken_sites
ssl_bump splice step3 broken_sites

that works, but again...I get no logging, which is worse then "ssl_bump
splice broken_sites", and defeats the purpose of trying to avoid having
to create the broken_sites ACL in the first place.  Lastly, if I try and
change splice to peek or bump it's broken with odd log entries such as:

Nov  3 05:45:23 gateway (squid-1): 192.168.1.110 - -
[03/Nov/2014:05:45:23 -0700] "GET https://www.google.com/ HTTP/1.1" 503
3854 TAG_NONE:HIER_NONE -
Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
[03/Nov/2014:05:45:31 -0700] "CONNECT 206.190.36.45:443 HTTP/1.1" 403
3402 TCP_DENIED:HIER_NONE -
Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
[03/Nov/2014:05:45:31 -0700] "#026#003#001 %BB/%CESsJ%B3%C2%BC%CC%BD%90
HTTP/1.1" 400 3577 TAG_NONE:HIER_NONE -

Is there something I am missing?  I've been really reading through the
squid site, but I can't find any examples of peek splice.  Thank you.

James



From squid3 at treenet.co.nz  Tue Nov  4 04:43:24 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 04 Nov 2014 17:43:24 +1300
Subject: [squid-users] Squid 3.4.9 is available
Message-ID: <5458596C.8060203@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.4.9 release!


This release is a bug fix release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:


* Bug 3803: ident leaks memory on failure

Please note that on Squid which have been configured to send IDENT
queries to WAN visitors this can become a remotely triggerable
security vulnerability. A remote attacker can DoS the Squid service by
sending enough HTTP traffic from hosts not responding to IDENT that
the memory leak overwhelms the Squid server.

IMPORTANT: Correct configuration of IDENT in Squid includes
ident_access ACLs limiting IDENT queries to being sent only to LAN
(localnet) clients.


* Bug 4102: ssl_bump certificate contains only a dot character in key
usage extension

The previous fix for bug 3966 was incorrect. SSL-bump generated
certificates would display with valid version for key exytensions to
exist but have a single "." character as the key extension field contents.

There have been reports that this fix is still incomplete and there
may be further fixes needed on top of this one. However this fix alone
resolves browser issues with many websites using simple key extensions.


* Bug 4088: memory leak in external_acl_type helper

This bug would appear as a memory leak if an external_acl_type helper
is configured with either of the cache=0, ttl=0 or negative_ttl=0
options. Leaked bytes amounted to the size of the helper lookup,
response and HTTP request headers on any helper lookups which were not
cached - that could be several MB per minute on a busy proxy.


* Bug 4024: Bad host/IP ::1 when using IPv4-only environment

This bug would show up as a fatal configuration error processing the
default ::1 localhost address on a system with IPv6 completely
disabled in the host DNS resolver library.

NOTE WELL:
  disabling IPv6 entirely violates the Internet standard BCP 177
  "IPv6 Support Required for All IP-Capable Nodes".

HTTP is one of the protocols where IP addresses are embeded in the
layer-3 protocol syntax. There are no guarantees of correct proxying
operation if the system underlying Squid prevents it correctly
interpreting IPv6 elements within HTTP messages.



 All users of Squid with IDENT are urged to upgrade to this release as
soon as possible.

 All users of Squid with SSL-bump are urged to upgrade to this release
as soon as possible.

 All other users of Squid are encouraged to upgrade to this release as
time permits.



 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.4/RELEASENOTES.html
when you are ready to make the switch to Squid-3.4

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWFlsAAoJELJo5wb/XPRjii8IAJFGBJspa2AFDtWAh+/eddql
xOVCiyqzA2aSM4uMMjlJJNOiwdfgZjgRbEuv2cOrzajvm/ynR5poN0ZE1UKEzZvN
epmAhJd7oCRJIUh6SAKyyZhb1UnnAhfhInrizoMurAJDVV3vBi7LErLXStZ80gMn
MbrW8tzly09dSFytc7yxqRhVLX3tk/mtQzWTeJ9yZ8JQDVrDA3eyXX6wWHRZlOgK
Q+PxqfVOdJsch7s2NJIuneD9K/jv1LIOOcrEzDR5MV1KN8uLKTFY64ItEUas/oEf
TKdfIob+j3kPFPrbTC9yUsSHxT8+y9/VReXLT967DOnJHbpeYEENU5NSjWZH7qU=
=hSni
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Nov  4 04:45:45 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 04 Nov 2014 17:45:45 +1300
Subject: [squid-users] Squid 3.5.0.2 beta is available
Message-ID: <545859F9.1070406@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The Squid Software Foundation is very pleased to announce the
availability of the Squid-3.5.0.2 beta release!


This release is a bug fix and documentation update release resolving
some issues found in the prior Squid releases.


The major changes to be aware of:


* Fix FTP socket opening during reconfigure

The ftp_port directive in earlier 3.5 release was closing and not
reopening the FTP native socket(s) during squid -k reconfigure action.


* Documentation updates

A lot of code documentation and additional copyright blurb additions
have been made.


* Bug fixes shared with 3.4 series

This release includes several bug fixes shared with the 3.4.9 stable
series release.


All users are encouraged to give this Squid release a test run as soon
as time permits. All feedback welcome.


Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
if and when you are ready to make the switch to Squid-3.5

This new release can be downloaded from our HTTP or FTP servers

http://www.squid-cache.org/Versions/v3/3.5/
ftp://ftp.squid-cache.org/pub/squid/
ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

http://www.squid-cache.org/Download/http-mirrors.html
http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/

Amos Jeffries
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWFn5AAoJELJo5wb/XPRjRVAIAMDNPO4FPWBkybvyU+Uwpunh
2w7bWlR+PV4FEB6AnFJTZm7U1e4JzP4XHISofwXcGdq7d4msYFi+v24//y2JVne7
DwPIyj+Qe/l6ZghNbp829iIxkAHw1BV9vRVqb5CVg5U8eUpztVxpiw05uDwiBx6b
/9oSYYjaQK4VudVJOgBsyrkSR2ZsEoSdx7scfynnY9KF4YZgwlSJY1RJwxlgJfLY
ENMdx/SyxAbvvd/fZu3IPdPpXJLZB8yJAsdTMO22j3WIqEL4YYmVES+xjKZ7FKar
ChuF1AJSx2nnfnBStWCR8KDSweQZog+UfF5xHwByAmL2ZioPpW4iS7zdQQb9bzA=
=NYr9
-----END PGP SIGNATURE-----


From sudakov at sibptus.tomsk.ru  Tue Nov  4 07:00:40 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 4 Nov 2014 13:00:40 +0600
Subject: [squid-users] Squid,
 Kerberos and FireFox (Was: Re: leaking memory in squid 3.4.8 and
 3.4.7.)
In-Reply-To: <5448F9D0.4020609@norma.perm.ru>
References: <20141019073258.GA91627@admin.sibptus.tomsk.ru>
 <54444049.1010904@ngtech.co.il>
 <20141023045332.GA23654@admin.sibptus.tomsk.ru>
 <54488C17.9010207@treenet.co.nz>
 <20141023051937.GB23654@admin.sibptus.tomsk.ru>
 <20141023062717.GA26140@admin.sibptus.tomsk.ru>
 <5448D754.2040000@treenet.co.nz>
 <20141023114116.GA28981@admin.sibptus.tomsk.ru>
 <CAHsHsyuHJwb95kCSoh-uJ4MnNM5jrTxHCMxOstGXcQExud-LjQ@mail.gmail.com>
 <5448F9D0.4020609@norma.perm.ru>
Message-ID: <20141104070040.GB30366@admin.sibptus.tomsk.ru>

Colleagues,

I have created a howto in Russian about squid and Kerberos proxy
authentication, addressing also the two problems I personally
encountered while setting all the stuff up.

If any Russian speakers here could review and comment it, I would be
grateful. 

The text is at http://victor-sudakov.dreamwidth.org/302219.html
mirrored at http://victor-sudakov.livejournal.com/273244.html

-- 
Victor Sudakov 
Tomsk, Russia
Russian Barefoot FAQ at http://www.barefooters.ru/barefoot.txt


From sudakov at sibptus.tomsk.ru  Tue Nov  4 08:10:38 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 4 Nov 2014 14:10:38 +0600
Subject: [squid-users] Squid,
 Kerberos and FireFox (Was: Re: leaking memory in squid 3.4.8 and
 3.4.7.)
In-Reply-To: <20141104070040.GB30366@admin.sibptus.tomsk.ru>
References: <54444049.1010904@ngtech.co.il>
 <20141023045332.GA23654@admin.sibptus.tomsk.ru>
 <54488C17.9010207@treenet.co.nz>
 <20141023051937.GB23654@admin.sibptus.tomsk.ru>
 <20141023062717.GA26140@admin.sibptus.tomsk.ru>
 <5448D754.2040000@treenet.co.nz>
 <20141023114116.GA28981@admin.sibptus.tomsk.ru>
 <CAHsHsyuHJwb95kCSoh-uJ4MnNM5jrTxHCMxOstGXcQExud-LjQ@mail.gmail.com>
 <5448F9D0.4020609@norma.perm.ru>
 <20141104070040.GB30366@admin.sibptus.tomsk.ru>
Message-ID: <20141104081037.GA31627@admin.sibptus.tomsk.ru>

Now available at https://bitbucket.org/victor_sudakov/faq/src/tip/FAQ/squid_kerberos.txt

Victor Sudakov wrote:
> Colleagues,
> 
> I have created a howto in Russian about squid and Kerberos proxy
> authentication, addressing also the two problems I personally
> encountered while setting all the stuff up.
> 
> If any Russian speakers here could review and comment it, I would be
> grateful. 
> 
> The text is at http://victor-sudakov.dreamwidth.org/302219.html
> mirrored at http://victor-sudakov.livejournal.com/273244.html
> 
> -- 
> Victor Sudakov 
> Tomsk, Russia
> Russian Barefoot FAQ at http://www.barefooters.ru/barefoot.txt
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From squid3 at treenet.co.nz  Tue Nov  4 08:41:14 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 04 Nov 2014 21:41:14 +1300
Subject: [squid-users] Squid,
 Kerberos and FireFox (Was: Re: leaking memory in squid 3.4.8 and
 3.4.7.)
In-Reply-To: <20141104081037.GA31627@admin.sibptus.tomsk.ru>
References: <54444049.1010904@ngtech.co.il>
 <20141023045332.GA23654@admin.sibptus.tomsk.ru>
 <54488C17.9010207@treenet.co.nz>
 <20141023051937.GB23654@admin.sibptus.tomsk.ru>
 <20141023062717.GA26140@admin.sibptus.tomsk.ru>
 <5448D754.2040000@treenet.co.nz>
 <20141023114116.GA28981@admin.sibptus.tomsk.ru>
 <CAHsHsyuHJwb95kCSoh-uJ4MnNM5jrTxHCMxOstGXcQExud-LjQ@mail.gmail.com>
 <5448F9D0.4020609@norma.perm.ru>
 <20141104070040.GB30366@admin.sibptus.tomsk.ru>
 <20141104081037.GA31627@admin.sibptus.tomsk.ru>
Message-ID: <5458912A.8030700@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 4/11/2014 9:10 p.m., Victor Sudakov wrote:
> Now available at
> https://bitbucket.org/victor_sudakov/faq/src/tip/FAQ/squid_kerberos.txt
>
>  Victor Sudakov wrote:
>> Colleagues,
>> 
>> I have created a howto in Russian about squid and Kerberos proxy 
>> authentication, addressing also the two problems I personally 
>> encountered while setting all the stuff up.
>> 
>> If any Russian speakers here could review and comment it, I would
>> be grateful.
>> 
>> The text is at http://victor-sudakov.dreamwidth.org/302219.html 
>> mirrored at http://victor-sudakov.livejournal.com/273244.html
>> 

I'm not a native Russian speaker but with Google translates help...

If you are going to publish this please use either the official
domains reserved for documentation
(<http://tools.ietf.org/html/rfc2606#section-2>) instead of a *.ru
domain which can be registered by a squatter, or the .local TLD
reserved for LAN internal traffic.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWJEqAAoJELJo5wb/XPRjZggIAL+ekzVzGyv6xeO51njjg4Y1
mhtWDE/1yZcXbFlsJHdj4A3DyTvIjIwVX6IF9nWCnB8BEbWPBZxy0eVtLEwa8zzS
Y4eRTDuc5DFTSbUb7vMITCmv1/fbXS6jzl9vLN9y4As2ugWUdFn/54gG3iiMOfL8
i70ZZZRHAUwSG1kN+bUXNPALku8h4dzqjNZxjj6bjZ67kU5wAXAWJrWO+CqSDyl7
V/jQyPkYEtkLeVrILiuTPIZyRAykFaGL6ycJ5ICNLTk8E+M0RxUut4qIMFEv9XoY
MGZDNAkyRzIyXr1FPFslQgMGmWgFXYyEXOkAZFsaua9Mx90oYNmCV/gUQ4jIZoQ=
=WkSX
-----END PGP SIGNATURE-----


From sudakov at sibptus.tomsk.ru  Tue Nov  4 09:18:08 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 4 Nov 2014 15:18:08 +0600
Subject: [squid-users] Squid,
 Kerberos and FireFox (Was: Re: leaking memory in squid 3.4.8 and
 3.4.7.)
In-Reply-To: <5458912A.8030700@treenet.co.nz>
References: <54488C17.9010207@treenet.co.nz>
 <20141023051937.GB23654@admin.sibptus.tomsk.ru>
 <20141023062717.GA26140@admin.sibptus.tomsk.ru>
 <5448D754.2040000@treenet.co.nz>
 <20141023114116.GA28981@admin.sibptus.tomsk.ru>
 <CAHsHsyuHJwb95kCSoh-uJ4MnNM5jrTxHCMxOstGXcQExud-LjQ@mail.gmail.com>
 <5448F9D0.4020609@norma.perm.ru>
 <20141104070040.GB30366@admin.sibptus.tomsk.ru>
 <20141104081037.GA31627@admin.sibptus.tomsk.ru>
 <5458912A.8030700@treenet.co.nz>
Message-ID: <20141104091808.GA32585@admin.sibptus.tomsk.ru>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Amos Jeffries wrote:
> On 4/11/2014 9:10 p.m., Victor Sudakov wrote:
> > Now available at
> > https://bitbucket.org/victor_sudakov/faq/src/tip/FAQ/squid_kerberos.txt

[dd]

> 
> If you are going to publish this please use either the official
> domains reserved for documentation
> (<http://tools.ietf.org/html/rfc2606#section-2>) instead of a *.ru
> domain which can be registered by a squatter, or the .local TLD
> reserved for LAN internal traffic.

Good point. I have changed the TLD to .example

- -- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWJnQAAoJEA2k8lmbXsY0ABgH/A/i0LgoMa+Kfettd4eajY/X
7T92+K3RXz/L8LOZgFgRlzGSjxP3W+tbgzi1bsiSJarZHYOmNLmtzZeRb7iwgXvz
ZF0Qyta0594RCHIO5s5Xy45jX3dlvlQ/MJ9MfRIomcQXEDXl6qVnxi2G2SA7oYIj
fceTXehx9gZ4SPLa7vv9jgtTTMzY8AlSf3YHd82QBuMcOTYoIMCZUdkwB1ZrZ5ZM
qblld1OdUdwCO5QlQaHnMubuAAw4plAc5zEO8FCE5nbnJjLYknA+7WtbBGZoirIz
P0V1i9dy7VtU3EZaIsxfWxAdD3k5Lfdg4OzE6vKvzYtO4nGJXVV4QG5utU97Nj0=
=5lNO
-----END PGP SIGNATURE-----


From steve at opendium.com  Tue Nov  4 10:17:58 2014
From: steve at opendium.com (Steve Hill)
Date: Tue, 04 Nov 2014 10:17:58 +0000
Subject: [squid-users] RFC2616 headers in bumped requests
Message-ID: <5458A7D6.3090502@opendium.com>


Squid (correctly) inserts Via and X-Forwarded-For headers into requests
that it is proxying.  However, in the case of encrypted traffic, the
server and client are expecting the traffic to reach the other end
as-is, since usually this could not be intercepted.  With SSL bumped
requests this is no longer true - the proxy can (and does) modify the
traffic, by inserting these headers.

So I'm asking the question: is this behavior considered desirable, or
should we be attempting to modify the request as little as possible for
compatibility reasons?

I've just come across a web server that throws its toys out of the pram
when it sees a Via header in an HTTPS request, and unfortunately it's
quite a big one - Yahoo.  See this request:

-----
GET /news/degrees-lead-best-paid-careers-141513989.html HTTP/1.1
Host: uk.finance.yahoo.com
Via: 1.1

HTTP/1.1 301 Moved Permanently
Date: Tue, 04 Nov 2014 09:55:40 GMT
Via: http/1.1 yts212.global.media.ir2.yahoo.com (ApacheTrafficServer [c
s f ]), http/1.1 r04.ycpi.ams.yahoo.net (ApacheTrafficServer [cMsSfW])
Server: ATS
Strict-Transport-Security: max-age=172800
Location:
https://uk.finance.yahoo.com/news/degrees-lead-best-paid-careers-141513989.html
Content-Length: 0
Age: 0
Connection: keep-alive
-----

Compare to:

-----
GET /news/degrees-lead-best-paid-careers-141513989.html HTTP/1.1
Host: uk.finance.yahoo.com

HTTP/1.1 200 OK
...
-----


Note that the 301 that they return when a Via header is present just
points back at the same URI, so the client never gets the object it
requested.

For now I have worked around it with:
  request_header_access Via deny https
  request_header_access X-Forwarded-For deny https
But it does make me wonder if inserting the headers into bumped traffic
is a sensible thing to do.

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From steve at opendium.com  Tue Nov  4 10:24:07 2014
From: steve at opendium.com (Steve Hill)
Date: Tue, 04 Nov 2014 10:24:07 +0000
Subject: [squid-users] SSL bump fails accessing .gov.uk servers
In-Reply-To: <20141031200342.GA27156@bloms.de>
References: <5453D304.7030909@opendium.com> <20141031200342.GA27156@bloms.de>
Message-ID: <5458A947.4020905@opendium.com>

On 31/10/14 20:03, Dieter Bloms wrote:

> but when the server is broken, it will not work.
> Have a look at:
> 
> https://www.ssllabs.com/ssltest/analyze.html?d=www.taxdisc.service.gov.uk
> 
>> It works correctly when FireFox connects directly to the web server
>> rather than going through the proxy.
> 
> yes the browsers have a workaround and try with different cipher suites,
> when the first connect fails.
> 
>> So my question is: is the web server broken, or am I misunderstanding
>> something?
> 
> The webserver is broken.

Many thanks for this - I have emailed them, which I fully expect them to
ignore  :)

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From christos at chtsanti.net  Tue Nov  4 10:32:14 2014
From: christos at chtsanti.net (Christos Tsantilas)
Date: Tue, 04 Nov 2014 12:32:14 +0200
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <1415019600.2899.11.camel@JamesiMac>
References: <1414670766.2891.14.camel@JamesiMac>
 <545757ED.5030903@chtsanti.net> <1415019600.2899.11.camel@JamesiMac>
Message-ID: <5458AB2E.9060905@chtsanti.net>

On 11/03/2014 03:00 PM, James Lay wrote:
>
> Thanks Christos,
>
> So here's where I'm at...my full test config below:
> ......
>......
>
> logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
> Sh %ssl::>cert_subject

The %ssl::>cert_subject will print the subject of the client 
certificate, if there is any. In most cases the client does not sent any 
certificate.
Logging the server certificate subject is not yet implemented.

>
> The above works, but allows all sites regardless of what's in url.txt.

If you want to use a list of urls to restrict sites which should bumped 
you should use an external_acl helper.
You can send to the external_acl helpers the client SNI informations (on 
at_step SslBump2) and/OR the server certificate subject (on at_step 
SslBump3).

> Additionally, there's no logging of any kind.  The allow part makes
> sense as this is the last ACL, the no logging part is confusing.  If I
> add:
>
> acl broken_sites dst 69.25.139.128/25
> acl .....
 > .....
> and change to
> ssl_bump peek step1 broken_sites
> ssl_bump peek step2 broken_sites
> ssl_bump splice step3 broken_sites

This is will splice any connection to broken_sites and will not bump any 
other request.

>
> that works, but again...I get no logging, which is worse then "ssl_bump
> splice broken_sites", and defeats the purpose of trying to avoid having
> to create the broken_sites ACL in the first place.  Lastly, if I try and
> change splice to peek or bump it's broken with odd log entries such as:

Will help if you describe what are you trying to do.
The acl broken_sites includes only IP addresses. Looks that the 
peek-and-splice is not needed in your application.
You can just use "ssl_bump none broken_sites"

>
> Nov  3 05:45:23 gateway (squid-1): 192.168.1.110 - -
> [03/Nov/2014:05:45:23 -0700] "GET https://www.google.com/ HTTP/1.1" 503
> 3854 TAG_NONE:HIER_NONE -
> Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
> [03/Nov/2014:05:45:31 -0700] "CONNECT 206.190.36.45:443 HTTP/1.1" 403
> 3402 TCP_DENIED:HIER_NONE -
> Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
> [03/Nov/2014:05:45:31 -0700] "#026#003#001 %BB/%CESsJ%B3%C2%BC%CC%BD%90
> HTTP/1.1" 400 3577 TAG_NONE:HIER_NONE -
>
> Is there something I am missing?  I've been really reading through the
> squid site, but I can't find any examples of peek splice.  Thank you.
>
> James


From jlay at slave-tothe-box.net  Tue Nov  4 12:26:24 2014
From: jlay at slave-tothe-box.net (James Lay)
Date: Tue, 04 Nov 2014 05:26:24 -0700
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <5458AB2E.9060905@chtsanti.net>
References: <1414670766.2891.14.camel@JamesiMac>
 <545757ED.5030903@chtsanti.net> <1415019600.2899.11.camel@JamesiMac>
 <5458AB2E.9060905@chtsanti.net>
Message-ID: <1415103984.2837.13.camel@JamesiMac>

On Tue, 2014-11-04 at 12:32 +0200, Christos Tsantilas wrote:
> On 11/03/2014 03:00 PM, James Lay wrote:
> >
> > Thanks Christos,
> >
> > So here's where I'm at...my full test config below:
> > ......
> >......
> >
> > logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
> > Sh %ssl::>cert_subject
> 
> The %ssl::>cert_subject will print the subject of the client 
> certificate, if there is any. In most cases the client does not sent any 
> certificate.
> Logging the server certificate subject is not yet implemented.
> 
> >
> > The above works, but allows all sites regardless of what's in url.txt.
> 
> If you want to use a list of urls to restrict sites which should bumped 
> you should use an external_acl helper.
> You can send to the external_acl helpers the client SNI informations (on 
> at_step SslBump2) and/OR the server certificate subject (on at_step 
> SslBump3).
> 
> > Additionally, there's no logging of any kind.  The allow part makes
> > sense as this is the last ACL, the no logging part is confusing.  If I
> > add:
> >
> > acl broken_sites dst 69.25.139.128/25
> > acl .....
>  > .....
> > and change to
> > ssl_bump peek step1 broken_sites
> > ssl_bump peek step2 broken_sites
> > ssl_bump splice step3 broken_sites
> 
> This is will splice any connection to broken_sites and will not bump any 
> other request.
> 
> >
> > that works, but again...I get no logging, which is worse then "ssl_bump
> > splice broken_sites", and defeats the purpose of trying to avoid having
> > to create the broken_sites ACL in the first place.  Lastly, if I try and
> > change splice to peek or bump it's broken with odd log entries such as:
> 
> Will help if you describe what are you trying to do.
> The acl broken_sites includes only IP addresses. Looks that the 
> peek-and-splice is not needed in your application.
> You can just use "ssl_bump none broken_sites"
> 
> >
> > Nov  3 05:45:23 gateway (squid-1): 192.168.1.110 - -
> > [03/Nov/2014:05:45:23 -0700] "GET https://www.google.com/ HTTP/1.1" 503
> > 3854 TAG_NONE:HIER_NONE -
> > Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
> > [03/Nov/2014:05:45:31 -0700] "CONNECT 206.190.36.45:443 HTTP/1.1" 403
> > 3402 TCP_DENIED:HIER_NONE -
> > Nov  3 05:45:31 gateway (squid-1): 192.168.1.110 - -
> > [03/Nov/2014:05:45:31 -0700] "#026#003#001 %BB/%CESsJ%B3%C2%BC%CC%BD%90
> > HTTP/1.1" 400 3577 TAG_NONE:HIER_NONE -
> >
> > Is there something I am missing?  I've been really reading through the
> > squid site, but I can't find any examples of peek splice.  Thank you.
> >
> > James

Thanks a bunch Christos,

That list of IP's is things like apple.com, textnow.me, and windows
updates...IP's that simply don't bump well.  My setup is a linux box
that's a router...one NIC internal IP, the other external IP.  Via
iptables redirect, I'm transparently intercepting the web traffic of a
few devices, only allowing them access to the list of sites in url.txt.
At issue with using the broken_sites list, is that I have to just
specify large chucks of netblocks, which I lose control and visibility
of.  What I'm really hoping for is for a way for squid to be able to, in
my case at least, look at either the server_name extension in the Client
Hello, or, if that's not present, look at the dNSName of certificate
being sent, check the access against url.txt, and either allow or deny.

Ssl_bump does work well for most sites...and I understand we are
performing a man in the middle attack so it's not supposed to be easy.
Again my hope isn't really to perform a mitm...more of an access control
type thing.  Thanks again Christos...I hope I explained this well
enough.

James





From marcus.kool at urlfilterdb.com  Tue Nov  4 12:32:36 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 04 Nov 2014 10:32:36 -0200
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <545859F9.1070406@treenet.co.nz>
References: <545859F9.1070406@treenet.co.nz>
Message-ID: <5458C764.7090006@urlfilterdb.com>

The release notes have a link for ecap documentation that points to http://wiki.squid-cache.org/Features/BLAH which does not exist.

The squid wiki refers to an old version of ecap so
I tried searching for "ecap" on the home page but it throws an error:
Not Found
The requested URL /cgi-bin/swish-query.cgi was not found on this server.

Is there already more documentation about ecap v1.0?

Thanks
Marcus


On 11/04/2014 02:45 AM, Amos Jeffries wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> The Squid Software Foundation is very pleased to announce the
> availability of the Squid-3.5.0.2 beta release!
>
>
> This release is a bug fix and documentation update release resolving
> some issues found in the prior Squid releases.
>
>
> The major changes to be aware of:
>
>
> * Fix FTP socket opening during reconfigure
>
> The ftp_port directive in earlier 3.5 release was closing and not
> reopening the FTP native socket(s) during squid -k reconfigure action.
>
>
> * Documentation updates
>
> A lot of code documentation and additional copyright blurb additions
> have been made.
>
>
> * Bug fixes shared with 3.4 series
>
> This release includes several bug fixes shared with the 3.4.9 stable
> series release.
>
>
> All users are encouraged to give this Squid release a test run as soon
> as time permits. All feedback welcome.
>
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
> if and when you are ready to make the switch to Squid-3.5
>
> This new release can be downloaded from our HTTP or FTP servers
>
> http://www.squid-cache.org/Versions/v3/3.5/
> ftp://ftp.squid-cache.org/pub/squid/
> ftp://ftp.squid-cache.org/pub/archive/3.5/
>
> or the mirrors. For a list of mirror sites see
>
> http://www.squid-cache.org/Download/http-mirrors.html
> http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
> http://bugs.squid-cache.org/
>
> Amos Jeffries
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUWFn5AAoJELJo5wb/XPRjRVAIAMDNPO4FPWBkybvyU+Uwpunh
> 2w7bWlR+PV4FEB6AnFJTZm7U1e4JzP4XHISofwXcGdq7d4msYFi+v24//y2JVne7
> DwPIyj+Qe/l6ZghNbp829iIxkAHw1BV9vRVqb5CVg5U8eUpztVxpiw05uDwiBx6b
> /9oSYYjaQK4VudVJOgBsyrkSR2ZsEoSdx7scfynnY9KF4YZgwlSJY1RJwxlgJfLY
> ENMdx/SyxAbvvd/fZu3IPdPpXJLZB8yJAsdTMO22j3WIqEL4YYmVES+xjKZ7FKar
> ChuF1AJSx2nnfnBStWCR8KDSweQZog+UfF5xHwByAmL2ZioPpW4iS7zdQQb9bzA=
> =NYr9
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Tue Nov  4 13:59:44 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 05 Nov 2014 02:59:44 +1300
Subject: [squid-users] RFC2616 headers in bumped requests
In-Reply-To: <5458A7D6.3090502@opendium.com>
References: <5458A7D6.3090502@opendium.com>
Message-ID: <5458DBD0.2050306@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 4/11/2014 11:17 p.m., Steve Hill wrote:
> 
> Squid (correctly) inserts Via and X-Forwarded-For headers into
> requests that it is proxying.  However, in the case of encrypted
> traffic, the server and client are expecting the traffic to reach
> the other end as-is, since usually this could not be intercepted.
> With SSL bumped requests this is no longer true - the proxy can
> (and does) modify the traffic, by inserting these headers.
> 
> So I'm asking the question: is this behavior considered desirable,
> or should we be attempting to modify the request as little as
> possible for compatibility reasons?

It is desirable. More so for intercepted SSL traffic than for regular
HTTP traffic. These headers convey the important information about the
path the traffic took to reach each endpoint. Only with full
information can the endpoints accurately assign security trust/context
on the message.

> 
> I've just come across a web server that throws its toys out of the
> pram when it sees a Via header in an HTTPS request, and
> unfortunately it's quite a big one - Yahoo.  See this request:
> 
> ----- GET /news/degrees-lead-best-paid-careers-141513989.html
> HTTP/1.1 Host: uk.finance.yahoo.com Via: 1.1
> 

That is unfortunately an invalid HTTP Via header. It is mandatory to
contain the host field even if it contains a host alias for the real
FQDN. If that is what is actually being transfered the server is right
in complaining.


> HTTP/1.1 301 Moved Permanently Date: Tue, 04 Nov 2014 09:55:40 GMT 
> Via: http/1.1 yts212.global.media.ir2.yahoo.com
> (ApacheTrafficServer [c s f ]), http/1.1 r04.ycpi.ams.yahoo.net
> (ApacheTrafficServer [cMsSfW])

As you can see here the upstream server itself is inserting Via
headers to HTTPS traffic due to a CDN at the server end.

It is reporting that the HTTPS request is being sent unencrypted over
their portion of the network. Not terrible if we assume its their
internal LAN or VPN network, but it cearly demonstrates how realistic
is that end-to-end encryption assumption you say the client and server
have.


> Server: ATS Strict-Transport-Security: max-age=172800 Location: 
> https://uk.finance.yahoo.com/news/degrees-lead-best-paid-careers-141513989.html
>
> 
Content-Length: 0
> Age: 0 Connection: keep-alive -----
> 
> Compare to:
> 
> ----- GET /news/degrees-lead-best-paid-careers-141513989.html
> HTTP/1.1 Host: uk.finance.yahoo.com
> 
> HTTP/1.1 200 OK ... -----
> 
> 
> Note that the 301 that they return when a Via header is present
> just points back at the same URI, so the client never gets the
> object it requested.
> 
> For now I have worked around it with: request_header_access Via
> deny https request_header_access X-Forwarded-For deny https But it
> does make me wonder if inserting the headers into bumped traffic is
> a sensible thing to do.
> 

If you can please chek that Via header being emitted by your Squid
when things break. And also whether your Squid is contacting their
server on an HTTPS or HTTP port.
 If your Squid is contacting their HTTP port for un-encrypted traffic
this redirect is competely expected.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWNvPAAoJELJo5wb/XPRjmsoH/3d9CswaasIZRKlEuu1dQ0a2
/KFZmxhKJDHKYT4yQva+l0og6/FwhFdaUNr0s6xhVfyfUG/fRX2KALCElXl9Bbbe
y3qKrFVSGBJXdXZ/Pysv5ZixKBGBXeiHF4akGZZpK3zexgUjb/+NYg5CrDVi7omi
NbbZE/cTmYa2ZlrlG0F0v5hxlIhjTmyhxJNBR5PWKHWCkYZMnc4cynjrCnNzSmnX
LuY1pFSE9SCRovE3kUgZRB5tDfdk7bNOLfjMHBW1B9p/INRrmnTTVxhxKmPadzWn
9I0IOlusNSYrgYE9mQCp+TYQhVhDRuTN9LubIuuYsC5mHmSdW2WMUs0ExkyOhOM=
=lJ19
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Nov  4 14:18:41 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 05 Nov 2014 03:18:41 +1300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <5458C764.7090006@urlfilterdb.com>
References: <545859F9.1070406@treenet.co.nz> <5458C764.7090006@urlfilterdb.com>
Message-ID: <5458E041.4040808@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 5/11/2014 1:32 a.m., Marcus Kool wrote:
> The release notes have a link for ecap documentation that points
> to http://wiki.squid-cache.org/Features/BLAH which does not exist.

Oops, that should be pointing at
<http://wiki.squid-cache.org/Features/eCAP> though I see Alex has not
updated it yet for eCAP 1.0.

There may be something more at http://www.e-cap.org/

- From the Squid user perspective it is configured the same AFAIK. The
big changes are in the plugin developer API presented by eCAP.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWOBAAAoJELJo5wb/XPRjrugIANRIudXPSXAmUNzP9rI4lFt6
PlglRKfN6F3sHxthgeorSr4oJ5MtAe7OdJZvTJ5+Bwdta+onjgsl51ZPZtqSnglM
5V/3hlMTInD9/seRZD4adPhnASJ54XHgkXUHpZ9e+19Qw76IJQF8X0MLFsOKvpha
6KWXz1k02MQEh47iurA7eVGQmenVmYRbQBt6CTTYCVvhH+YixMjZlCyAw7dII1ty
RnYl0NyhakWn2pRyy/chv29Be8mqT+hOZKUqZRZVI2sIyXvi+c1RQowY8M1c/f+T
IAYLVXv6Owm3k6uA1mRLHDv0lqQUlFbjuM3V5xcseTSM+eXgmHRSOyN5ote3nnU=
=ia1B
-----END PGP SIGNATURE-----


From odhiambo at gmail.com  Tue Nov  4 15:46:33 2014
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Tue, 4 Nov 2014 18:46:33 +0300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <545859F9.1070406@treenet.co.nz>
References: <545859F9.1070406@treenet.co.nz>
Message-ID: <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>

On 4 November 2014 07:45, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> The Squid Software Foundation is very pleased to announce the
> availability of the Squid-3.5.0.2 beta release!
>
>

3.5.0.1 beta failed to compile successfully so I left it at that.
3.5.0.2 has also failed, but I am not leaving it.

<cut>
gmake[3]: Leaving directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/adaptation'
Making all in esi
gmake[3]: Entering directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi'
gmake[3]: Nothing to be done for `all'.
gmake[3]: Leaving directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi'
gmake[3]: Entering directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src'
gmake[3]: *** No rule to make target `/usp/include/c++/4.2/vector', needed
by `DiskIO/Blocking/BlockingIOStrategy.o'.  Stop.
gmake[3]: Leaving directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src'
gmake[2]: *** [all-recursive] Error 1
gmake[2]: Leaving directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src'
gmake[1]: *** [all] Error 2
gmake[1]: Leaving directory
`/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src'
gmake: *** [all-recursive] Error 1
</cut>

Platform is FreeBSD 9.1-RELEASE and my configure options are:

./configure --prefix=/opt/squid35 \
        --enable-removal-policies="lru heap" \
        --disable-epoll \
        --enable-auth \
        --enable-auth-basic="DB NCSA PAM MSNT PAM POP3 SMB SSPI MSNT" \
        --enable-external-acl-helpers="session unix_group wbinfo_group
file_userip" \
        --enable-auth-ntlm="smb_lm SSPI" \
        --enable-auth-negotiate="SSPI kerberos" \
        --with-pthreads \
        --enable-storeio="ufs diskd aufs" \
        --enable-delay-pools \
        --enable-snmp  \
        --with-openssl=/usr \
        --enable-forw-via-db \
        --enable-cache-digests \
        --enable-wccpv2 \
        --enable-follow-x-forwarded-for \
        --with-large-files \
        --enable-large-cache-files \
        --enable-error_default_language=English \
        --enable-esi \
        --enable-kqueue \
        --enable-icap-client \
        --enable-kill-parent-hack \
        --enable-ssl \
        --enable-leakfinder \
        --enable-ssl-crtd \
        --enable-url-rewrite-helpers \
        --enable-xmalloc-statistics \
        --enable-stacktraces \
        --enable-zph-qos \
        --enable-eui \
        --enable-pf-transparent \
        --enable-ipf-transparent


Maybe I am being overzealous with something, or not reading the available
options right, but I have used these with 3.4.8 which is what I am running.



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254733744121/+254722743223
"I can't hear you -- I'm using the scrambler."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141104/3d07c00a/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov  4 18:27:14 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 05 Nov 2014 07:27:14 +1300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
Message-ID: <54591A82.5060608@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 5/11/2014 4:46 a.m., Odhiambo Washington wrote:
> On 4 November 2014 07:45, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>> 
>> The Squid Software Foundation is very pleased to announce the 
>> availability of the Squid-3.5.0.2 beta release!
>> 
>> 
> 
> 3.5.0.1 beta failed to compile successfully so I left it at that. 
> 3.5.0.2 has also failed, but I am not leaving it.
> 

Thank you.


> <cut> gmake[3]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/adaptation' 
> Making all in esi gmake[3]: Entering directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi' gmake[3]:
> Nothing to be done for `all'. gmake[3]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi' gmake[3]:
> Entering directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[3]: *** No
> rule to make target `/usp/include/c++/4.2/vector', needed by
> `DiskIO/Blocking/BlockingIOStrategy.o'.  Stop.

It looks like your system is missing the <vector> C++ standard library
file.
Which is kind of strange, since the build should have died far earlier
on other Squid object files that also need it.

> gmake[3]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[2]: ***
> [all-recursive] Error 1 gmake[2]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[1]: ***
> [all] Error 2 gmake[1]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake: ***
> [all-recursive] Error 1 </cut>
> 
> Platform is FreeBSD 9.1-RELEASE and my configure options are:
> 
> ./configure --prefix=/opt/squid35 \ --enable-removal-policies="lru
> heap" \ --disable-epoll \ --enable-auth \ --enable-auth-basic="DB
> NCSA PAM MSNT PAM POP3 SMB SSPI MSNT" \ 
> --enable-external-acl-helpers="session unix_group wbinfo_group 
> file_userip" \ --enable-auth-ntlm="smb_lm SSPI" \ 
> --enable-auth-negotiate="SSPI kerberos" \ --with-pthreads \ 
> --enable-storeio="ufs diskd aufs" \ --enable-delay-pools \ 
> --enable-snmp  \ --with-openssl=/usr \ --enable-forw-via-db \ 
> --enable-cache-digests \ --enable-wccpv2 \ 
> --enable-follow-x-forwarded-for \ --with-large-files \ 
> --enable-large-cache-files \ 
> --enable-error_default_language=English \ --enable-esi \ 
> --enable-kqueue \ --enable-icap-client \ --enable-kill-parent-hack
> \ --enable-ssl \ --enable-leakfinder \ --enable-ssl-crtd \ 
> --enable-url-rewrite-helpers \ --enable-xmalloc-statistics \ 
> --enable-stacktraces \ --enable-zph-qos \ --enable-eui \ 
> --enable-pf-transparent \ --enable-ipf-transparent
> 
> 
> Maybe I am being overzealous with something, or not reading the
> available options right, but I have used these with 3.4.8 which is
> what I am running.
> 
> 
> 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWRqCAAoJELJo5wb/XPRjQZ4IALWqUF7f1Q/eGquJO9Tm3bqr
aLQfHAPDeHFcnJl79M5h8KUnxwDSwGdkvOYgUVJs7lTeUNa8NK4S0ZFCQ5+1Fjlg
oK2ZiF5Nz41gEU5xRwakOnJsnV9dwkFPgnhWwtl7XyuZILPeWFMw4Q466/Ezrk/Z
TobdvDQmkaKAuKrV2Ja2srpabjk4d0CfjAp5ggj7zIRc8kjP5qjk27yUUfMPAAEs
k/5ZPM6ZhuSeo7OiSxn47DnkUGHOdPw8Qj005n1sAu+c6pE3vh9EuJYunARdzq7/
Z/uz9ej6ODxgPaiXDiTLTG9IHq9LVknSfQoJZAI1csNNINH+4ghHoYlGOBilipQ=
=eyj9
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Wed Nov  5 05:40:27 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Tue, 4 Nov 2014 21:40:27 -0800
Subject: [squid-users] possible SYN flooding on port 80. Sending cookies.
Message-ID: <001301cff8bb$0966f970$1c34ec50$@netstream.ps>

Hi ,

I got new logs with 
squid-squid kernel: possible SYN flooding on port 80. Sending cookies.

 

Then server is about to be killing the browsing

 

Im asking wt should I do in kernel config to tun it so that it handle that
traffic ?

 

I just want to make sure no limitation in kernel and will try to increase
cpus on the Vm server

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141104/afbfa7c7/attachment.htm>

From odhiambo at gmail.com  Tue Nov  4 20:05:31 2014
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Tue, 4 Nov 2014 23:05:31 +0300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <54591A82.5060608@treenet.co.nz>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <54591A82.5060608@treenet.co.nz>
Message-ID: <CAAdA2WOVg16LN704=MfY8FkTGndTWKY5da4wtTTSBUU7LM+2+w@mail.gmail.com>

How should that be fixed?

On 4 November 2014 21:27, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 5/11/2014 4:46 a.m., Odhiambo Washington wrote:
> > On 4 November 2014 07:45, Amos Jeffries <squid3 at treenet.co.nz>
> > wrote:
> >
> >> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
> >>
> >> The Squid Software Foundation is very pleased to announce the
> >> availability of the Squid-3.5.0.2 beta release!
> >>
> >>
> >
> > 3.5.0.1 beta failed to compile successfully so I left it at that.
> > 3.5.0.2 has also failed, but I am not leaving it.
> >
>
> Thank you.
>
>
> > <cut> gmake[3]: Leaving directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/adaptation'
> > Making all in esi gmake[3]: Entering directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi' gmake[3]:
> > Nothing to be done for `all'. gmake[3]: Leaving directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/esi' gmake[3]:
> > Entering directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[3]: *** No
> > rule to make target `/usp/include/c++/4.2/vector', needed by
> > `DiskIO/Blocking/BlockingIOStrategy.o'.  Stop.
>
> It looks like your system is missing the <vector> C++ standard library
> file.
> Which is kind of strange, since the build should have died far earlier
> on other Squid object files that also need it.
>
> > gmake[3]: Leaving directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[2]: ***
> > [all-recursive] Error 1 gmake[2]: Leaving directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[1]: ***
> > [all] Error 2 gmake[1]: Leaving directory
> > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake: ***
> > [all-recursive] Error 1 </cut>
> >
> > Platform is FreeBSD 9.1-RELEASE and my configure options are:
> >
> > ./configure --prefix=/opt/squid35 \ --enable-removal-policies="lru
> > heap" \ --disable-epoll \ --enable-auth \ --enable-auth-basic="DB
> > NCSA PAM MSNT PAM POP3 SMB SSPI MSNT" \
> > --enable-external-acl-helpers="session unix_group wbinfo_group
> > file_userip" \ --enable-auth-ntlm="smb_lm SSPI" \
> > --enable-auth-negotiate="SSPI kerberos" \ --with-pthreads \
> > --enable-storeio="ufs diskd aufs" \ --enable-delay-pools \
> > --enable-snmp  \ --with-openssl=/usr \ --enable-forw-via-db \
> > --enable-cache-digests \ --enable-wccpv2 \
> > --enable-follow-x-forwarded-for \ --with-large-files \
> > --enable-large-cache-files \
> > --enable-error_default_language=English \ --enable-esi \
> > --enable-kqueue \ --enable-icap-client \ --enable-kill-parent-hack
> > \ --enable-ssl \ --enable-leakfinder \ --enable-ssl-crtd \
> > --enable-url-rewrite-helpers \ --enable-xmalloc-statistics \
> > --enable-stacktraces \ --enable-zph-qos \ --enable-eui \
> > --enable-pf-transparent \ --enable-ipf-transparent
> >
> >
> > Maybe I am being overzealous with something, or not reading the
> > available options right, but I have used these with 3.4.8 which is
> > what I am running.
> >
> >
> >
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUWRqCAAoJELJo5wb/XPRjQZ4IALWqUF7f1Q/eGquJO9Tm3bqr
> aLQfHAPDeHFcnJl79M5h8KUnxwDSwGdkvOYgUVJs7lTeUNa8NK4S0ZFCQ5+1Fjlg
> oK2ZiF5Nz41gEU5xRwakOnJsnV9dwkFPgnhWwtl7XyuZILPeWFMw4Q466/Ezrk/Z
> TobdvDQmkaKAuKrV2Ja2srpabjk4d0CfjAp5ggj7zIRc8kjP5qjk27yUUfMPAAEs
> k/5ZPM6ZhuSeo7OiSxn47DnkUGHOdPw8Qj005n1sAu+c6pE3vh9EuJYunARdzq7/
> Z/uz9ej6ODxgPaiXDiTLTG9IHq9LVknSfQoJZAI1csNNINH+4ghHoYlGOBilipQ=
> =eyj9
> -----END PGP SIGNATURE-----
>



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254733744121/+254722743223
"I can't hear you -- I'm using the scrambler."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141104/e86b6e2f/attachment.htm>

From hno at squid-cache.org  Tue Nov  4 20:10:20 2014
From: hno at squid-cache.org (Henrik Nordstrom)
Date: Tue, 04 Nov 2014 21:10:20 +0100
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <CAAdA2WOVg16LN704=MfY8FkTGndTWKY5da4wtTTSBUU7LM+2+w@mail.gmail.com>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <54591A82.5060608@treenet.co.nz>
 <CAAdA2WOVg16LN704=MfY8FkTGndTWKY5da4wtTTSBUU7LM+2+w@mail.gmail.com>
Message-ID: <1415131820.9159.1.camel@localhost>

tis 2014-11-04 klockan 23:05 +0300 skrev Odhiambo Washington:


>         > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[3]:
>         *** No
>         > rule to make target `/usp/include/c++/4.2/vector', needed by
>         > `DiskIO/Blocking/BlockingIOStrategy.o'.  Stop.

What is /usp/?


Regards
Henrik




From dan at getbusi.com  Wed Nov  5 00:42:51 2014
From: dan at getbusi.com (dan at getbusi.com)
Date: Tue, 04 Nov 2014 16:42:51 -0800 (PST)
Subject: [squid-users] assertion failed: client_side.cc:1515:
 "connIsUsable(http->getConn())
Message-ID: <1415148171389.37537c0e@Nodemailer>

Hi folks


I think I raised this crash a few months ago, but I can?t locate the thread now, so I?m starting a new one now that I actually have a gdb backtrace to present.


I suspect it?s to do with one of our external ACLs (we use a few), but the backtrace I?ve attached is most undecipherable for me?hopefully someone else here can understand it.


Thanks as always
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141104/2d1330df/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: squid-assertion-failed-gdb-bt.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141104/2d1330df/attachment.txt>

From eliezer at ngtech.co.il  Wed Nov  5 07:44:22 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 05 Nov 2014 09:44:22 +0200
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
Message-ID: <5459D556.9080608@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Odhiambo,

The cut might be pretty important.
For example I would like to get the full build environmental variables
etc.
If you can please provide all the relevant details about the build
node and procedure to ensure we are looking at the same picture.

Eliezer

On 11/04/2014 05:46 PM, Odhiambo Washington wrote:
> 3.5.0.1 beta failed to compile successfully so I left it at that. 
> 3.5.0.2 has also failed, but I am not leaving it.
> 
> <cut> gmake[3]: Leaving directory 
> `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src/adaptation' 
> Making all in esi gmake[3]: Entering directory



-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWdVVAAoJENxnfXtQ8ZQUD9cH/2LVLH/BdmdZyAKj4MY+v3RK
ypsyK5iBjgEHeIhMmyXKVo7F3w3u6tVzvoq/CW8pgS2PDhrcjolXoNViy/4CBMfQ
gaMFtwLCO0PU7lIdNVmp+GGxvEbr+icPHZhe23UZMCTL+n8ItrANB0KXRF9V74eY
5/A9z2UBL9xtDLkYknnGH9JUh0jbFslRzgzS7zP0ub27ggFc3p0tIHfbkckSe2c6
NE7TSLPq2NzbRJ+RZ2HSooPZbHq96KI8noQtWjeJhomjXTHyv0xLkVQXMDEo0jlW
2V+XiZaNBNmLCVUc9qtrhnTjdYJabkH2bFBVw2nse1fLSCnrDTIl5K8b22Erc8w=
=5vv6
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Wed Nov  5 07:52:06 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 05 Nov 2014 09:52:06 +0200
Subject: [squid-users] possible SYN flooding on port 80. Sending cookies.
In-Reply-To: <001301cff8bb$0966f970$1c34ec50$@netstream.ps>
References: <001301cff8bb$0966f970$1c34ec50$@netstream.ps>
Message-ID: <5459D726.5080703@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/05/2014 07:40 AM, Ahmed Allzaeem wrote:
> I just want to make sure no limitation in kernel and will try to
> increase cpus on the Vm server

Hey Ahmed,

Kernel is not squid related issue so you first ask in other places.
Since you have asked you should know that a busy server using
syn-cookies defence inside the kernel might show what you see now.

All The Bests,
Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWdcmAAoJENxnfXtQ8ZQUrGoH/3xmQ4ndFzxYmTzZkRBTiNin
eC+4uaHE8a97fWYIcl3xpZ3lN+zi1LAjsLE/Ftc2YV6eUQ+xM//LkRecsh+A+kYB
mpJtpiPXaxCX862LNw3CWn/5O05zN44rnATafP1l9JN/2Ck+t2o2mibLb2kjfPRm
wNJJJGaFZFqMMlLRGxG+cXljT1+VKhHPt8p0pMTBtuQaYnaaWNHdSX8bKusMqJOS
jLZxDsYLCYo8LAumqMAhGysQIR6i3BE6Xp91W/oyMzXrt1MKPtOnJH8MDTOGXFqN
UacGNFeP2M+VmW1Bzyvrp3cYWzQh7hs9ANgvhGqUGtabwlSItZPRSihSgo7Iou0=
=61ou
-----END PGP SIGNATURE-----


From odhiambo at gmail.com  Wed Nov  5 08:51:41 2014
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 5 Nov 2014 11:51:41 +0300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <1415131820.9159.1.camel@localhost>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <54591A82.5060608@treenet.co.nz>
 <CAAdA2WOVg16LN704=MfY8FkTGndTWKY5da4wtTTSBUU7LM+2+w@mail.gmail.com>
 <1415131820.9159.1.camel@localhost>
Message-ID: <CAAdA2WNewhZkqS=5_hCfKnqVy91D7kqu1egm+_BWkyf-5z2QeA@mail.gmail.com>

On 4 November 2014 23:10, Henrik Nordstrom <hno at squid-cache.org> wrote:

> tis 2014-11-04 klockan 23:05 +0300 skrev Odhiambo Washington:
>
>
> >         > `/usr/home/wash/Tools/Squid/3.5/squid-3.5.0.2/src' gmake[3]:
> >         *** No
> >         > rule to make target `/usp/include/c++/4.2/vector', needed by
> >         > `DiskIO/Blocking/BlockingIOStrategy.o'.  Stop.
>
> What is /usp/?
>
>

Did that come from my system??

Strange.

-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254733744121/+254722743223
"I can't hear you -- I'm using the scrambler."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141105/22bd742e/attachment.htm>

From eliezer at ngtech.co.il  Wed Nov  5 09:00:10 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 05 Nov 2014 11:00:10 +0200
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <545859F9.1070406@treenet.co.nz>
References: <545859F9.1070406@treenet.co.nz>
Message-ID: <5459E71A.6020601@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/04/2014 06:45 AM, Amos Jeffries wrote:
> The Squid Software Foundation is very pleased to announce the 
> availability of the Squid-3.5.0.2 beta release!

Later next week a new build node will be available for CentOS 6.5 RPMs.
Sorry for the delay!!

There will be a subject for the RPMs release as usual.

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWecaAAoJENxnfXtQ8ZQUgxEH/249BVSDDlyO2xsOYPqxQcmz
CMEOXNDT/uftiOQ8UBcr6V/ISFNlzoyTucxkkudhsOTQLqKWzDQ4OWoOO9YzgGUz
k8f2c3WdnMtPJxjfjIqYO2nTDWDdwdvzuGMBrPabuce8dMiRZTgS5BAnkcZKhKPS
PyAa/7rNdblW6zaTh6KYjpm1PSduNE1y3Myx1+oy/gz3Xa4NNnh7fPZJNu+tkMD2
o3Mbu6v8blBPdqzrU2Vyi3qk1qPOaeEEDyw61+NFLGWRafizwKqF/3dZ3CR9lDyW
0DHYyXR3ANUZ16kWjonubKdEX5BuNIg6JY+s1Hd4XNDMMp8n+zfQAUWT1ykuKSE=
=Yan0
-----END PGP SIGNATURE-----


From odhiambo at gmail.com  Wed Nov  5 09:01:03 2014
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 5 Nov 2014 12:01:03 +0300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <5459D556.9080608@ngtech.co.il>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <5459D556.9080608@ngtech.co.il>
Message-ID: <CAAdA2WM8tpinZxwGy-EzpuEuxAzrVXYO-_Kude+VDgK-BretzA@mail.gmail.com>

On 5 November 2014 10:44, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hey Odhiambo,
>
> The cut might be pretty important.
> For example I would like to get the full build environmental variables
> etc.
> If you can please provide all the relevant details about the build
> node and procedure to ensure we are looking at the same picture.
>
> Eliezer
>
>
Hi Eliezer,

For some strange reason, the compile has succeeded. I am not sure what is
going on to be honest. I build everything as unprivileged user and only
used sudo to install.
I have captured the whole process in the file
http://196.200.26.114/~wash/squid-3.5.0.2-build.txt

Now I am not even sure what had happened, since I haven't changed anything
on the system.

-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254733744121/+254722743223
"I can't hear you -- I'm using the scrambler."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141105/d0b81c21/attachment.htm>

From eliezer at ngtech.co.il  Wed Nov  5 09:15:55 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 05 Nov 2014 11:15:55 +0200
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <CAAdA2WM8tpinZxwGy-EzpuEuxAzrVXYO-_Kude+VDgK-BretzA@mail.gmail.com>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <5459D556.9080608@ngtech.co.il>
 <CAAdA2WM8tpinZxwGy-EzpuEuxAzrVXYO-_Kude+VDgK-BretzA@mail.gmail.com>
Message-ID: <5459EACB.2030602@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Odhiambo,

I will try to build it on freebsd and see what happens to get the
bigger picture.

Will this:
http://ftp.freebsd.org/%2Fpub/FreeBSD/releases/ISO-IMAGES/9.1/FreeBSD-9.1-RELEASE-amd64-dvd1.iso

be the link for the relevant release?

Eliezer

On 11/05/2014 11:01 AM, Odhiambo Washington wrote:
> Hi Eliezer,
> 
> For some strange reason, the compile has succeeded. I am not sure
> what is going on to be honest. I build everything as unprivileged
> user and only used sudo to install. I have captured the whole
> process in the file 
> http://196.200.26.114/~wash/squid-3.5.0.2-build.txt
> 
> Now I am not even sure what had happened, since I haven't changed
> anything on the system.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWerLAAoJENxnfXtQ8ZQUN1wH/0+7nbKg0h1gR0ih5RKE6E//
Vvw66GeFkljsPejM5Vn32AUCjWfJbd5Lu6zOGGwEoRDqnQtrTgyLI9M8oDhQjK6w
rqnBRTwe4ukZypCKiOcokxT7nE4LDvqzd5iywe690fBjm/yGqkxVVH17fnOl8eMi
VfZLZvSfL/4cSu4k2B4omdLk2VMEtXhovVjeMG7fgvQ2ITRQPB4Ic7p3qimR7jf/
rcE0qerlgpDoKF+7ZW6sEpUJeytvi+PDKqjx+EB9upvyKn1x04VkSCSn2HhzyGrh
mypahJx89t2w+IoqvoFsC3DuSPKDEdOmZwNhAqyCyQgvB7TX7qOFOmHNcWkEavo=
=PKUm
-----END PGP SIGNATURE-----


From navari.lorenzo at gmail.com  Wed Nov  5 09:39:19 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Wed, 5 Nov 2014 01:39:19 -0800 (PST)
Subject: [squid-users] TCP_DENIED/403
Message-ID: <1415180359160-4668210.post@n4.nabble.com>

Good day today.
I' m configuring a Squid Web Proxy Cache 
and  I apply the deny policy to some sites l 
this is the problem:

when people accesses sites with GET they have the right html error page
ERR_ACCES_DENIED 
(LOG = TCP_DENIED/403 4069 GET http://www.sex.com/ - HIER_NONE/- text/html)

when people accesses sites with CONNECT they DON'T  have the right html
error page 
but te message CONNECTION REFUSED BY PROXY SERVER
(LOG = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/- text/html)

I would like to have the same error page for all.

Can anyone help ??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Nov  5 09:52:10 2014
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 5 Nov 2014 10:52:10 +0100
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <1415180359160-4668210.post@n4.nabble.com>
References: <1415180359160-4668210.post@n4.nabble.com>
Message-ID: <201411051052.11267.Antony.Stone@squid.open.source.it>

On Wednesday 05 November 2014 at 10:39:19 (EU time), navari.lorenzo at gmail.com 
wrote:

> I'm configuring a Squid Web Proxy Cache and I apply the deny policy to some
> sites.  This is the problem:
> 
> when people access sites with GET they have the right html error page
> ERR_ACCES_DENIED
> (LOG = TCP_DENIED/403 4069 GET http://www.sex.com/ - HIER_NONE/- text/html)
> 
> when people access sites with CONNECT they DON'T have the right html
> error page
> but te message CONNECTION REFUSED BY PROXY SERVER
> (LOG = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/-
> text/html)
> 
> I would like to have the same error page for all.
> 
> Can anyone help ??

Please tell us the version of Squid you are using, and show us the rules you 
have implemented for the "deny policy".


Regards,


Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rafael.akchurin at diladele.com  Wed Nov  5 09:57:05 2014
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 5 Nov 2014 09:57:05 +0000
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <1415180359160-4668210.post@n4.nabble.com>
References: <1415180359160-4668210.post@n4.nabble.com>
Message-ID: <95e445b7848c4c9582386bf5b869c559@AM3PR04MB450.eurprd04.prod.outlook.com>

Hello Navari,

Just my two cents - http://docs.diladele.com/faq/squid.html#why-i-see-cannot-connect-to-site-using-https-browser-message-instead-of-usual-site-is-blocked

Raf

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of navari.lorenzo at gmail.com
Sent: Wednesday, November 5, 2014 10:39 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] TCP_DENIED/403

Good day today.
I' m configuring a Squid Web Proxy Cache and  I apply the deny policy to some sites l this is the problem:

when people accesses sites with GET they have the right html error page ERR_ACCES_DENIED (LOG = TCP_DENIED/403 4069 GET http://www.sex.com/ - HIER_NONE/- text/html)

when people accesses sites with CONNECT they DON'T  have the right html error page but te message CONNECTION REFUSED BY PROXY SERVER (LOG = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/- text/html)

I would like to have the same error page for all.

Can anyone help ??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From odhiambo at gmail.com  Wed Nov  5 09:56:47 2014
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 5 Nov 2014 12:56:47 +0300
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <5459EACB.2030602@ngtech.co.il>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <5459D556.9080608@ngtech.co.il>
 <CAAdA2WM8tpinZxwGy-EzpuEuxAzrVXYO-_Kude+VDgK-BretzA@mail.gmail.com>
 <5459EACB.2030602@ngtech.co.il>
Message-ID: <CAAdA2WMQr5quVSC20fD4781dfOpbknToDMNotP8qVV_HXai=KA@mail.gmail.com>

Hi Eliezer,

That link should be fine, although my system is actually PC-BSD. The
version is the same though an old version.
My exact version is:
ftp://ftp.pcbsd.org/pub/archived/9.1-RELEASE/amd64/PCBSD9.1-RELEASE-p19-09-07-2013-x64-DVD.iso

On 5 November 2014 12:15, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hey Odhiambo,
>
> I will try to build it on freebsd and see what happens to get the
> bigger picture.
>
> Will this:
>
> http://ftp.freebsd.org/%2Fpub/FreeBSD/releases/ISO-IMAGES/9.1/FreeBSD-9.1-RELEASE-amd64-dvd1.iso
>
> be the link for the relevant release?
>
> Eliezer
>
> On 11/05/2014 11:01 AM, Odhiambo Washington wrote:
> > Hi Eliezer,
> >
> > For some strange reason, the compile has succeeded. I am not sure
> > what is going on to be honest. I build everything as unprivileged
> > user and only used sudo to install. I have captured the whole
> > process in the file
> > http://196.200.26.114/~wash/squid-3.5.0.2-build.txt
> >
> > Now I am not even sure what had happened, since I haven't changed
> > anything on the system.
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUWerLAAoJENxnfXtQ8ZQUN1wH/0+7nbKg0h1gR0ih5RKE6E//
> Vvw66GeFkljsPejM5Vn32AUCjWfJbd5Lu6zOGGwEoRDqnQtrTgyLI9M8oDhQjK6w
> rqnBRTwe4ukZypCKiOcokxT7nE4LDvqzd5iywe690fBjm/yGqkxVVH17fnOl8eMi
> VfZLZvSfL/4cSu4k2B4omdLk2VMEtXhovVjeMG7fgvQ2ITRQPB4Ic7p3qimR7jf/
> rcE0qerlgpDoKF+7ZW6sEpUJeytvi+PDKqjx+EB9upvyKn1x04VkSCSn2HhzyGrh
> mypahJx89t2w+IoqvoFsC3DuSPKDEdOmZwNhAqyCyQgvB7TX7qOFOmHNcWkEavo=
> =PKUm
> -----END PGP SIGNATURE-----
>



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254733744121/+254722743223
"I can't hear you -- I'm using the scrambler."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141105/5a7f794c/attachment.htm>

From navari.lorenzo at gmail.com  Wed Nov  5 09:55:30 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Wed, 5 Nov 2014 01:55:30 -0800 (PST)
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <95e445b7848c4c9582386bf5b869c559@AM3PR04MB450.eurprd04.prod.outlook.com>
References: <1415180359160-4668210.post@n4.nabble.com>
 <95e445b7848c4c9582386bf5b869c559@AM3PR04MB450.eurprd04.prod.outlook.com>
Message-ID: <5459F4FE.4030800@gmail.com>

thank now i read


Il 05/11/2014 10:53, Rafael Akchurin [via Squid Web Proxy Cache] ha scritto:
> Hello Navari,
>
> Just my two cents - 
> http://docs.diladele.com/faq/squid.html#why-i-see-cannot-connect-to-site-using-https-browser-message-instead-of-usual-site-is-blocked
>
> Raf
>
> -----Original Message-----
> From: squid-users [mailto:[hidden email] 
> </user/SendEmail.jtp?type=node&node=4668212&i=0>] On Behalf Of [hidden 
> email] </user/SendEmail.jtp?type=node&node=4668212&i=1>
> Sent: Wednesday, November 5, 2014 10:39 AM
> To: [hidden email] </user/SendEmail.jtp?type=node&node=4668212&i=2>
> Subject: [squid-users] TCP_DENIED/403
>
> Good day today.
> I' m configuring a Squid Web Proxy Cache and  I apply the deny policy 
> to some sites l this is the problem:
>
> when people accesses sites with GET they have the right html error 
> page ERR_ACCES_DENIED (LOG = TCP_DENIED/403 4069 GET 
> http://www.sex.com/ - HIER_NONE/- text/html)
>
> when people accesses sites with CONNECT they DON'T  have the right 
> html error page but te message CONNECTION REFUSED BY PROXY SERVER (LOG 
> = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/- text/html)
>
> I would like to have the same error page for all.
>
> Can anyone help ??
>
>
>
> -- 
> View this message in context: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=4668212&i=3>
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=4668212&i=4>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210p4668212.html 
>
> To unsubscribe from TCP_DENIED/403, click here 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4668210&code=bmF2YXJpLmxvcmVuem9AZ21haWwuY29tfDQ2NjgyMTB8MTQyOTk5NTkyNw==>.
> NAML 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210p4668214.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From navari.lorenzo at gmail.com  Wed Nov  5 10:13:15 2014
From: navari.lorenzo at gmail.com (Lorenzo)
Date: Wed, 05 Nov 2014 11:13:15 +0100
Subject: [squid-users] Fwd: Re:  TCP_DENIED/403
In-Reply-To: <5459F4CC.507@gmail.com>
References: <5459F4CC.507@gmail.com>
Message-ID: <5459F83B.2040002@gmail.com>




thank for replay

SO = CentOS 7

squid-3.4.6-1.el7.centos.x86_64

=====================================================

[root at lv-034-005 squid]# cat squid.conf
#
# Recommended minimum configuration:
#

#Default: debug_options ALL,1
#more    : debug_options ALL,1 33,2 28,9
debug_options ALL,1 28,3

visible_hostname proxy.usl1.toscana.it
append_domain .usl1.toscana.it
ftp_passive on
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern .            0 20% 4320
error_directory /etc/squid/errors
maximum_object_size 8192 KB
cache_dir ufs /var/spool/squid 8192 16 128
cache_log none
cache_store_log none
cache_mem 512 MB

#####################################################################################

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
#acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443 563 445 8080 10443 27443 28443
#acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 22        # telnet
acl Safe_ports port 443    563 445 # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
##################################################################

## ACL definition ##
acl siti_proibiti  url_regex  "/etc/squid/siti.proibiti.squid"

## ACL application ##
http_access deny siti_proibiti

##################################################################

# LDAP AUTH -
auth_param basic program /usr/lib64/squid/basic_ldap_auth -b
ou=users,dc=usl1,dc=toscana,dc=it -H ldap://portale3.usl1.toscana.it/
auth_param basic children 5
auth_param basic realm Accesso ad Internet
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off

acl user_authentication proxy_auth REQUIRED
http_access allow user_authentication

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320
[root at lv-034-005 squid]#

================================================================
[root at lv-034-005 squid]# cat siti.proibiti.squid
facebook\.(com|org|info|net|it)
twitter\.(com|org|info|net|it)
\.sex\.(com|org|info|net|it)
google-analytics\.com
youporn\.com
adultfriend\.com
pornhub\.com
sex
porn
games
giochi
gaytube\.com
amantesorgias\.com
porno\.com
www\.topgirlsitalia\.it
\.twitter\.com
\.myspace\.com
\.hi5\.com
\.teamviewer\.com
fbcdn-sphotos-e-a.akamaihd.net
[root at lv-034-005 squid]#






Il 05/11/2014 10:52, Antony Stone ha scritto:
> On Wednesday 05 November 2014 at 10:39:19 (EU time), navari.lorenzo at gmail.com
> wrote:
>
>> I'm configuring a Squid Web Proxy Cache and I apply the deny policy to some
>> sites.  This is the problem:
>>
>> when people access sites with GET they have the right html error page
>> ERR_ACCES_DENIED
>> (LOG = TCP_DENIED/403 4069 GET http://www.sex.com/ - HIER_NONE/- text/html)
>>
>> when people access sites with CONNECT they DON'T have the right html
>> error page
>> but te message CONNECTION REFUSED BY PROXY SERVER
>> (LOG = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/-
>> text/html)
>>
>> I would like to have the same error page for all.
>>
>> Can anyone help ??
> Please tell us the version of Squid you are using, and show us the rules you
> have implemented for the "deny policy".
>
>
> Regards,
>
>
> Antony.
>





From navari.lorenzo at gmail.com  Wed Nov  5 10:10:47 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Wed, 5 Nov 2014 02:10:47 -0800 (PST)
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <201411051052.11267.Antony.Stone@squid.open.source.it>
References: <1415180359160-4668210.post@n4.nabble.com>
 <201411051052.11267.Antony.Stone@squid.open.source.it>
Message-ID: <1415182247480-4668216.post@n4.nabble.com>

thank for replay

SO = CentOS 7

squid-3.4.6-1.el7.centos.x86_64

=====================================================

[root at lv-034-005 squid]# cat squid.conf
#
# Recommended minimum configuration:
#

#Default: debug_options ALL,1
#more    : debug_options ALL,1 33,2 28,9
debug_options ALL,1 28,3

visible_hostname proxy.usl1.toscana.it
append_domain .usl1.toscana.it
ftp_passive on
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern .            0 20% 4320
error_directory /etc/squid/errors
maximum_object_size 8192 KB
cache_dir ufs /var/spool/squid 8192 16 128
cache_log none
cache_store_log none
cache_mem 512 MB

#####################################################################################

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
#acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443 563 445 8080 10443 27443 28443
#acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 22        # telnet
acl Safe_ports port 443    563 445 # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
##################################################################

## ACL definition ##
acl siti_proibiti  url_regex  "/etc/squid/siti.proibiti.squid"

## ACL application ##
http_access deny siti_proibiti

##################################################################

# LDAP AUTH -
auth_param basic program /usr/lib64/squid/basic_ldap_auth -b
ou=users,dc=usl1,dc=toscana,dc=it -H ldap://portale3.usl1.toscana.it/
auth_param basic children 5
auth_param basic realm Accesso ad Internet
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off

acl user_authentication proxy_auth REQUIRED
http_access allow user_authentication

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320
[root at lv-034-005 squid]#

================================================================
[root at lv-034-005 squid]# cat siti.proibiti.squid
facebook\.(com|org|info|net|it)
twitter\.(com|org|info|net|it)
\.sex\.(com|org|info|net|it)
google-analytics\.com
youporn\.com
adultfriend\.com
pornhub\.com
sex
porn
games
giochi
gaytube\.com
amantesorgias\.com
porno\.com
www\.topgirlsitalia\.it
\.twitter\.com
\.myspace\.com
\.hi5\.com
\.teamviewer\.com
fbcdn-sphotos-e-a.akamaihd.net
[root at lv-034-005 squid]#





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210p4668216.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Nov  5 10:22:46 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 05 Nov 2014 12:22:46 +0200
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <95e445b7848c4c9582386bf5b869c559@AM3PR04MB450.eurprd04.prod.outlook.com>
References: <1415180359160-4668210.post@n4.nabble.com>
 <95e445b7848c4c9582386bf5b869c559@AM3PR04MB450.eurprd04.prod.outlook.com>
Message-ID: <5459FA76.8090603@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The project 100% needs a wiki article with the relevant information
about the issue.(the page is wonderful!!!)

I can write the page but it will take time to finish.
- - Adding into the todo list.

I am unsure but almost sure a bugzilla report about it in the wiki
section of the project should be the starting grounds for the subject.

Eliezer

On 11/05/2014 11:57 AM, Rafael Akchurin wrote:
> Hello Navari,
> 
> Just my two cents -
> http://docs.diladele.com/faq/squid.html#why-i-see-cannot-connect-to-site-using-https-browser-message-instead-of-usual-site-is-blocked
>
>  Raf

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUWfp2AAoJENxnfXtQ8ZQUxcUH/0kuZtMVOu8ktTddZ4je1vSu
Rl90hYLUfjIvS4G2A3B3IUw8DZtAW58r0XyqMzRj+wJ7i64q5XQMOhBmYiJqJ689
ZZAG4NO2cRg+f0/zQ1NhFCn2J3jkHwqLJdumUgVVT/fLKz0ENBk3hhNAyY88IrTF
AB1nNQ7UUx7iNFObnSkihWmRrOIXKBljHzlwVWdDkst6qgxR0wXrE8BJjnWsGvN5
HBUprYzLVd3aOPjXNyRMnLVHAsknU2ogdQakdJHNSiwrt78RyGthaneLEu7mtFCF
LW/SXOFIxYtN3mwlzE7FHEpB1G89Zf/MhzisnuxwdpZXZfn+wk6hu4k+UkijV2I=
=BZsz
-----END PGP SIGNATURE-----


From christos at chtsanti.net  Wed Nov  5 10:24:24 2014
From: christos at chtsanti.net (Christos Tsantilas)
Date: Wed, 05 Nov 2014 12:24:24 +0200
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <1415103984.2837.13.camel@JamesiMac>
References: <1414670766.2891.14.camel@JamesiMac>
 <545757ED.5030903@chtsanti.net> <1415019600.2899.11.camel@JamesiMac>
 <5458AB2E.9060905@chtsanti.net> <1415103984.2837.13.camel@JamesiMac>
Message-ID: <5459FAD8.7010005@chtsanti.net>

On 11/04/2014 02:26 PM, James Lay wrote:
>
> Thanks a bunch Christos,
>
> That list of IP's is things like apple.com, textnow.me, and windows
> updates...IP's that simply don't bump well.  My setup is a linux box
> that's a router...one NIC internal IP, the other external IP.  Via
> iptables redirect, I'm transparently intercepting the web traffic of a
> few devices, only allowing them access to the list of sites in url.txt.
> At issue with using the broken_sites list, is that I have to just
> specify large chucks of netblocks, which I lose control and visibility
> of.  What I'm really hoping for is for a way for squid to be able to, in
> my case at least, look at either the server_name extension in the Client

You need to build your own external_acl helper which will take as input 
the client sni (server_name extension). Read squid wiki for informations 
about external acl helpers:
  http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29

It is easy to build one in perl or as a shell script. I am suggesting to 
build an external_acl helper which return "OK" when the sni matches or 
no sni information exist.

You can use the following configuration or similar:
#
external_acl_type EXTACL %ssl::>sni /path-to-my/external-acl-helper.sh
acl EXTACL external EXTACL

acl step1 at_step  SslBump1
acl step2 at_step  SslBump2
acl step3 at_step  SslBump3

# At first step peek all
ssl_bump peek step1 all
ssl_bump splice step2 EXTACL
ssl_bump bump all


> Hello, or, if that's not present, look at the dNSName of certificate
> being sent, check the access against url.txt, and either allow or deny.

In your case the server certificate informations will not work well. At 
the time this information is available:
     1) in peek mode, you can not bump any more
     2) in stare mode, you can not splice any more.
There are exceptions to the above rules (for example in case the client 
uses the same SSL library with squid) but the SSL protocol is enough 
safe to not allow us to make something better than this.

Regards,
    Christos

>
> Ssl_bump does work well for most sites...and I understand we are
> performing a man in the middle attack so it's not supposed to be easy.
> Again my hope isn't really to perform a mitm...more of an access control
> type thing.  Thanks again Christos...I hope I explained this well
> enough.
>
> James
>


From squid3 at treenet.co.nz  Wed Nov  5 10:29:01 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 05 Nov 2014 23:29:01 +1300
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <1415180359160-4668210.post@n4.nabble.com>
References: <1415180359160-4668210.post@n4.nabble.com>
Message-ID: <5459FBED.8070905@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 5/11/2014 10:39 p.m., navari.lorenzo at gmail.com wrote:
> Good day today. I' m configuring a Squid Web Proxy Cache and  I
> apply the deny policy to some sites l this is the problem:
> 
> when people accesses sites with GET they have the right html error
> page ERR_ACCES_DENIED (LOG = TCP_DENIED/403 4069 GET
> http://www.sex.com/ - HIER_NONE/- text/html)
> 
> when people accesses sites with CONNECT they DON'T  have the right
> html error page but te message CONNECTION REFUSED BY PROXY SERVER 
> (LOG = TCP_DENIED/403 3681 CONNECT facebook.com:443 - HIER_NONE/-
> text/html)
> 
> I would like to have the same error page for all.
> 
> Can anyone help ??
> 

Sorry, the answer there is no.

If you look at the TCP packets being sent back by Squid, adn also
shown by those log entries. Squid *is* sending back the same 403 error
in both cases to the client browser.

What is happening is that the browser treates CONNECT and GET
differently. Specifically they refuse to show any remotely generated
(by Squid) content in the even of a 4xx or 5xx response to CONNECT.

To get this changed you will have to discuss it with the browser
people. They do it this way because of past hstory with malicious
payloads being delivered back in 4xx errors to CONNECT.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWfvtAAoJELJo5wb/XPRjiMgH/iBOO2WiuZ8QnWrcxz/spKpe
pf8KAGwzvtqJMuP0ZUWSLzmfVXEHWf1HlljiwoP+2n73zg3JS51iVSd7f6L3rbGM
eWhZhZ+syWJQ3LBretZHgpvXMuyjiu74PF7m/LkL61G4j/41oVvZeBIt8DQ54ml0
8yC196NIVEAjf2PXasHywgO+Is8L839l4uEh+WVudrDt9VCGEO7V1TJAt6qXiF6j
nwzJEbzzjCLfTosqwdBW1I/QuXVjj230XRVRVT38x/SSu7C1DFY68AvrkIK4DKae
YD0h2ElYEvg2O97KFo6BUugbbUeT/SNz+NGTKNvbz0di0AlwNVVyGj6nuTKayEg=
=Ol/T
-----END PGP SIGNATURE-----


From jlay at slave-tothe-box.net  Wed Nov  5 12:40:21 2014
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 05 Nov 2014 05:40:21 -0700
Subject: [squid-users] Correctly implementing peak-splice
In-Reply-To: <5459FAD8.7010005@chtsanti.net>
References: <1414670766.2891.14.camel@JamesiMac>
 <545757ED.5030903@chtsanti.net> <1415019600.2899.11.camel@JamesiMac>
 <5458AB2E.9060905@chtsanti.net> <1415103984.2837.13.camel@JamesiMac>
 <5459FAD8.7010005@chtsanti.net>
Message-ID: <1415191221.2837.23.camel@JamesiMac>

On Wed, 2014-11-05 at 12:24 +0200, Christos Tsantilas wrote:
> On 11/04/2014 02:26 PM, James Lay wrote:
> >
> > Thanks a bunch Christos,
> >
> > That list of IP's is things like apple.com, textnow.me, and windows
> > updates...IP's that simply don't bump well.  My setup is a linux box
> > that's a router...one NIC internal IP, the other external IP.  Via
> > iptables redirect, I'm transparently intercepting the web traffic of a
> > few devices, only allowing them access to the list of sites in url.txt.
> > At issue with using the broken_sites list, is that I have to just
> > specify large chucks of netblocks, which I lose control and visibility
> > of.  What I'm really hoping for is for a way for squid to be able to, in
> > my case at least, look at either the server_name extension in the Client
> 
> You need to build your own external_acl helper which will take as input 
> the client sni (server_name extension). Read squid wiki for informations 
> about external acl helpers:
>   http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
> 
> It is easy to build one in perl or as a shell script. I am suggesting to 
> build an external_acl helper which return "OK" when the sni matches or 
> no sni information exist.
> 
> You can use the following configuration or similar:
> #
> external_acl_type EXTACL %ssl::>sni /path-to-my/external-acl-helper.sh
> acl EXTACL external EXTACL
> 
> acl step1 at_step  SslBump1
> acl step2 at_step  SslBump2
> acl step3 at_step  SslBump3
> 
> # At first step peek all
> ssl_bump peek step1 all
> ssl_bump splice step2 EXTACL
> ssl_bump bump all
> 
> 
> > Hello, or, if that's not present, look at the dNSName of certificate
> > being sent, check the access against url.txt, and either allow or deny.
> 
> In your case the server certificate informations will not work well. At 
> the time this information is available:
>      1) in peek mode, you can not bump any more
>      2) in stare mode, you can not splice any more.
> There are exceptions to the above rules (for example in case the client 
> uses the same SSL library with squid) but the SSL protocol is enough 
> safe to not allow us to make something better than this.
> 
> Regards,
>     Christos
> 
> >
> > Ssl_bump does work well for most sites...and I understand we are
> > performing a man in the middle attack so it's not supposed to be easy.
> > Again my hope isn't really to perform a mitm...more of an access control
> > type thing.  Thanks again Christos...I hope I explained this well
> > enough.
> >
> > James
> >
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks so much Christos for taking time with this.  I'll give the helper
a go and report my results here.

James



From Jason_Haar at trimble.com  Wed Nov  5 22:35:37 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 6 Nov 2014 11:35:37 +1300
Subject: [squid-users] could sslbump handle client certs better?
Message-ID: <545AA639.3010901@trimble.com>

I haven't tested this so I may be embarrassing myself, but I doubt
client certs and sslbump play nicely together as the end-server would
never see any possible client cert interaction

I was wondering how quickly the need of a client cert is announced?
Could/does squid notice the server requirement for client certs and fall
back into passthrough mode? It would certainly be a great option to
have. ie force most https traffic through sslbump, but allow squid to
bypass it for the (very) few sites that require client certs. Some may
want to turn off such a feature, but most would probably be like me and
purely interested in using sslbump for enabling SSL content filtering,
and I really doubt we'll be seeing many viruses via client-cert
protected https any time soon ;-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From dan at getbusi.com  Wed Nov  5 22:56:12 2014
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 6 Nov 2014 09:56:12 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <1415148171389.37537c0e@Nodemailer>
References: <1415148171389.37537c0e@Nodemailer>
Message-ID: <CAN8nrKB+9oz8nk3sKwTKRgFNovzWtMMb5YbyfgAqP+Bfm2pVcg@mail.gmail.com>

Terribly sorry, but I only attached half the traceback before. I've
attached two *full* tracebacks this time; one from today and one from
yesterday.

This is from a forward proxy at a school, where this crash happens exactly
once per day between 9:00 and 9:30.



On 5 November 2014 11:42, <dan at getbusi.com> wrote:

>  Hi folks
>
> I think I raised this crash a few months ago, but I can?t locate the
> thread now, so I?m starting a new one now that I actually have a gdb
> backtrace to present.
>
> I suspect it?s to do with one of our external ACLs (we use a few), but the
> backtrace I?ve attached is most undecipherable for me?hopefully someone
> else here can understand it.
>
> Thanks as always
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141106/889a55b3/attachment.htm>
-------------- next part --------------
#0  0x0000003f28832625 in raise () from /lib64/libc.so.6
No symbol table info available.
#1  0x0000003f28833e05 in abort () from /lib64/libc.so.6
No symbol table info available.
#2  0x000000000056525f in xassert (msg=0x80ba03 "connIsUsable(http->getConn())", file=0x80a8f0 "client_side.cc", line=1515) at debug.cc:566
        __FUNCTION__ = "xassert"
#3  0x000000000053de3f in clientSocketRecipient (node=0x1c306118, http=0x1c30f038, rep=0x1e1c8e90, receivedData=...) at client_side.cc:1515
        context = {p_ = 0x1c30f908}
        mustSendLastChunk = <value optimized out>
#4  0x0000000000545d77 in clientReplyContext::processReplyAccessResult (this=0x1c3109b8, accessAllowed=<value optimized out>) at client_side_reply.cc:2058
        __FUNCTION__ = "processReplyAccessResult"
        localTempBuffer = {flags = {error = <value optimized out>}, length = 0, offset = <value optimized out>, data = 0x1c30fa17 ""}
        buf = 0x1c30f928 "HTTP/1.1 200 OK\r\nServer: nginx/1.6.0\r\nContent-Type: image/jpeg\r\nContent-Length: 1312\r\nLast-Modified: Fri, 27 Sep 2013 03:57:18 GMT\r\nETag: \"5245021e-520\"\r\nAccept-Ranges: bytes\r\nDate: Tue, 04 Nov 2014 2"...
        body_buf = <value optimized out>
        body_size = <value optimized out>
#5  0x00000000005461b3 in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=<value optimized out>) at client_side_reply.cc:1961
        me = <value optimized out>
#6  0x00000000006ea36b in ACLChecklist::checkCallback (this=0x6ebbed8, answer=<value optimized out>) at Checklist.cc:161
        callback_ = 0x5461a0 <clientReplyContext::ProcessReplyAccessResult(allow_t, void*)>
        cbdata_ = 0x1c3109b8
        __FUNCTION__ = "checkCallback"
#7  0x000000000058a779 in externalAclHandleReply (data=<value optimized out>, reply=<value optimized out>) at external_acl.cc:1422
        cbdata = 0x6ebbed8
        state = 0x120a3f88
        __FUNCTION__ = "externalAclHandleReply"
        next = <value optimized out>
        entryData = {result = {code = ACCESS_DENIED, kind = 0}, notes = {<Lock> = {_vptr.Lock = 0xaeaa80, count_ = 0}, _vptr.NotePairs = 0xaeaa58, entries = {
              capacity = 16, count = 1, items = 0x1e492000}}, user = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, password = {
            static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, message = {static npos = 18446744073709551615, size_ = 0, len_ = 0, 
            buf_ = 0x0}, tag = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, log = {static npos = 18446744073709551615, size_ = 512, 
            len_ = 148, 
            buf_ = 0x1e4cd340 "{\"deny_list\":0,\"deny_set\":2,\"categories\":\"[253]\",\"policy_group_id\":\"6\",\"user\":\"17lhilde\",\"user_group\":\"stu2017\",\"deny_type\":\"filesize\",\"set_id\":\"2\"}"}}
        entry = 0x1e4d9988
        label = <value optimized out>
#8  0x00000000005bb63a in helperReturnBuffer (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1b0afb8) at helper.cc:858
        response = {result = HelperReply::Error, notes = {<Lock> = {_vptr.Lock = 0xaeaa80, count_ = 0}, _vptr.NotePairs = 0xaeaa58, entries = {capacity = 16, 
              count = 1, items = 0x1de24510}}, whichServer = {cbc = 0x0, lock = 0x0}, other_ = {buf = 0x109a7120 "", size = 0, max_capacity = 247, 
            capacity = 2048, stolen = 0, static CBDATA_MemBuf = 14}}
        callback = 0x58a500 <externalAclHandleReply(void*, HelperReply const&)>
        cbdata = 0x120a3f88
        r = 0x110a7f00
#9  helperHandleRead (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1b0afb8) at helper.cc:951
        msg = 0x1b0b170 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A2%2C%22categories%22%3A%22%5B253%5D%22%2C%22policy_group_id%22%3A%226%22%2C%22user%22%3A%2217lhilde%22%2C%22user_group%22%3A%22stu2017%22%2C%22deny_ty"...
        i = 5809408
---Type <return> to continue, or q <return> to quit---
        skip = 1
        t = 0x1b0b26a ""
        srv = 0x1b0afb8
        hlp = 0x1b0adb8
        __FUNCTION__ = "helperHandleRead"
#10 0x00000000006ed746 in AsyncCall::make (this=0x1e3b8790) at AsyncCall.cc:32
        __FUNCTION__ = "make"
#11 0x00000000006f0802 in AsyncCallQueue::fireNext (this=<value optimized out>) at AsyncCallQueue.cc:52
        call = {p_ = 0x1e3b8790}
        __FUNCTION__ = "fireNext"
#12 0x00000000006f0b50 in AsyncCallQueue::fire (this=0x19d9a00) at AsyncCallQueue.cc:38
        made = true
#13 0x00000000005845a4 in EventLoop::runOnce (this=0x7fff578e09a0) at EventLoop.cc:135
        sawActivity = <value optimized out>
        waitingEngine = 0x7fff578e0a20
        __FUNCTION__ = "runOnce"
#14 0x00000000005846f8 in EventLoop::run (this=0x7fff578e09a0) at EventLoop.cc:99
No locals.
#15 0x0000000000602928 in SquidMain (argc=<value optimized out>, argv=0x7fff578e0b98) at main.cc:1528
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        signalEngine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaea790}, loop = @0x7fff578e09a0}
        store_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaea7f0}, <No data fields>}
        comm_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaf23b0}, <No data fields>}
        mainLoop = {errcount = 0, last_loop = false, engines = {capacity = 16, count = 4, items = 0x1f874a0}, timeService = 0x7fff578e0a10, 
          primaryEngine = 0x7fff578e0a20, loop_delay = 0, error = false, runOnceResult = false}
        time_engine = {_vptr.TimeEngine = 0xaecb90}
#16 0x00000000006032b8 in SquidMainSafe (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1260
No locals.
#17 main (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1252
No locals.
-------------- next part --------------
#0  0x0000003f28832625 in raise () from /lib64/libc.so.6
#1  0x0000003f28833e05 in abort () from /lib64/libc.so.6
#2  0x000000000056525f in xassert (msg=0x80ba03 "connIsUsable(http->getConn())", file=0x80a8f0 "client_side.cc", line=1515) at debug.cc:566
#3  0x000000000053de3f in clientSocketRecipient (node=0x11671f68, http=0xac1a798, rep=0x60494200, receivedData=...) at client_side.cc:1515
#4  0x0000000000545d77 in clientReplyContext::processReplyAccessResult (this=0xac1be98, accessAllowed=<value optimized out>) at client_side_reply.cc:2058
#5  0x00000000005461b3 in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=<value optimized out>) at client_side_reply.cc:1961
#6  0x00000000006ea36b in ACLChecklist::checkCallback (this=0x563b5948, answer=<value optimized out>) at Checklist.cc:161
#7  0x000000000058a779 in externalAclHandleReply (data=<value optimized out>, reply=<value optimized out>) at external_acl.cc:1422
#8  0x00000000005bb63a in helperReturnBuffer (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1c467c8) at helper.cc:858
#9  helperHandleRead (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1c467c8) at helper.cc:951
#10 0x00000000006ed746 in AsyncCall::make (this=0x4e888920) at AsyncCall.cc:32
#11 0x00000000006f0802 in AsyncCallQueue::fireNext (this=<value optimized out>) at AsyncCallQueue.cc:52
#12 0x00000000006f0b50 in AsyncCallQueue::fire (this=0x1b347b0) at AsyncCallQueue.cc:38
#13 0x00000000005845a4 in EventLoop::runOnce (this=0x7fff37193f70) at EventLoop.cc:135
#14 0x00000000005846f8 in EventLoop::run (this=0x7fff37193f70) at EventLoop.cc:99
#15 0x0000000000602928 in SquidMain (argc=<value optimized out>, argv=0x7fff37194168) at main.cc:1528
#16 0x00000000006032b8 in SquidMainSafe (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1260
#17 main (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1252
(gdb) bt full
#0  0x0000003f28832625 in raise () from /lib64/libc.so.6
No symbol table info available.
#1  0x0000003f28833e05 in abort () from /lib64/libc.so.6
No symbol table info available.
#2  0x000000000056525f in xassert (msg=0x80ba03 "connIsUsable(http->getConn())", file=0x80a8f0 "client_side.cc", line=1515) at debug.cc:566
        __FUNCTION__ = "xassert"
#3  0x000000000053de3f in clientSocketRecipient (node=0x11671f68, http=0xac1a798, rep=0x60494200, receivedData=...) at client_side.cc:1515
        context = {p_ = 0xac1ade8}
        mustSendLastChunk = <value optimized out>
#4  0x0000000000545d77 in clientReplyContext::processReplyAccessResult (this=0xac1be98, accessAllowed=<value optimized out>) at client_side_reply.cc:2058
        __FUNCTION__ = "processReplyAccessResult"
        localTempBuffer = {flags = {error = <value optimized out>}, length = 0, offset = <value optimized out>, data = 0xac1af7f ""}
        buf = 0xac1ae08 "HTTP/1.1 200 OK\r\nAccess-Control-Allow-Origin: *\r\nDate: Wed, 05 Nov 2014 22:22:08 GMT\r\nPragma: no-cache\r\nExpires: Fri, 01 Jan 1990 00:00:00 GMT\r\nCache-Control: no-cache, no-store, must-revalidate\r\nLast"...
        body_buf = <value optimized out>
        body_size = <value optimized out>
#5  0x00000000005461b3 in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=<value optimized out>) at client_side_reply.cc:1961
        me = <value optimized out>
#6  0x00000000006ea36b in ACLChecklist::checkCallback (this=0x563b5948, answer=<value optimized out>) at Checklist.cc:161
        callback_ = 0x5461a0 <clientReplyContext::ProcessReplyAccessResult(allow_t, void*)>
        cbdata_ = 0xac1be98
        __FUNCTION__ = "checkCallback"
#7  0x000000000058a779 in externalAclHandleReply (data=<value optimized out>, reply=<value optimized out>) at external_acl.cc:1422
        cbdata = 0x563b5948
        state = 0x5676ae88
        __FUNCTION__ = "externalAclHandleReply"
        next = <value optimized out>
        entryData = {result = {code = ACCESS_DENIED, kind = 0}, notes = {<Lock> = {_vptr.Lock = 0xaeaa80, count_ = 0}, _vptr.NotePairs = 0xaeaa58, entries = {
              capacity = 0, count = 0, items = 0x0}}, user = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, password = {
            static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, message = {static npos = 18446744073709551615, size_ = 0, len_ = 0, 
            buf_ = 0x0}, tag = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, log = {static npos = 18446744073709551615, size_ = 0, 
            len_ = 0, buf_ = 0x0}}
        entry = 0x60d9e4a8
        label = <value optimized out>
#8  0x00000000005bb63a in helperReturnBuffer (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1c467c8) at helper.cc:858
        response = {result = HelperReply::Error, notes = {<Lock> = {_vptr.Lock = 0xaeaa80, count_ = 0}, _vptr.NotePairs = 0xaeaa58, entries = {capacity = 0, 
              count = 0, items = 0x0}}, whichServer = {cbc = 0x0, lock = 0x0}, other_ = {buf = 0x5e01bf90 "", size = 0, max_capacity = 1, capacity = 2048, 
            stolen = 0, static CBDATA_MemBuf = 14}}
        callback = 0x58a500 <externalAclHandleReply(void*, HelperReply const&)>
        cbdata = 0x5676ae88
        r = 0x13161c80
#9  helperHandleRead (conn=<value optimized out>, buf=<value optimized out>, len=<value optimized out>, flag=<value optimized out>, 
    xerrno=<value optimized out>, data=0x1c467c8) at helper.cc:951
        msg = 0x1c46980 "ERR"
        i = 5809408
        skip = 1
        t = 0x1c46983 ""
        srv = 0x1c467c8
---Type <return> to continue, or q <return> to quit---
        hlp = 0x1c465c8
        __FUNCTION__ = "helperHandleRead"
#10 0x00000000006ed746 in AsyncCall::make (this=0x4e888920) at AsyncCall.cc:32
        __FUNCTION__ = "make"
#11 0x00000000006f0802 in AsyncCallQueue::fireNext (this=<value optimized out>) at AsyncCallQueue.cc:52
        call = {p_ = 0x4e888920}
        __FUNCTION__ = "fireNext"
#12 0x00000000006f0b50 in AsyncCallQueue::fire (this=0x1b347b0) at AsyncCallQueue.cc:38
        made = true
#13 0x00000000005845a4 in EventLoop::runOnce (this=0x7fff37193f70) at EventLoop.cc:135
        sawActivity = <value optimized out>
        waitingEngine = 0x7fff37193ff0
        __FUNCTION__ = "runOnce"
#14 0x00000000005846f8 in EventLoop::run (this=0x7fff37193f70) at EventLoop.cc:99
No locals.
#15 0x0000000000602928 in SquidMain (argc=<value optimized out>, argv=0x7fff37194168) at main.cc:1528
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        signalEngine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaea790}, loop = @0x7fff37193f70}
        store_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaea7f0}, <No data fields>}
        comm_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xaf23b0}, <No data fields>}
        mainLoop = {errcount = 0, last_loop = false, engines = {capacity = 16, count = 4, items = 0x20dd930}, timeService = 0x7fff37193fe0, 
          primaryEngine = 0x7fff37193ff0, loop_delay = 0, error = false, runOnceResult = false}
        time_engine = {_vptr.TimeEngine = 0xaecb90}
#16 0x00000000006032b8 in SquidMainSafe (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1260
No locals.
#17 main (argc=<value optimized out>, argv=<value optimized out>) at main.cc:1252
No locals.

From doc.holliday at usa.com  Thu Nov  6 01:33:22 2014
From: doc.holliday at usa.com (doc.holliday at usa.com)
Date: Thu, 6 Nov 2014 02:33:22 +0100
Subject: [squid-users] Behind enemy lines (squid behind proxy)
Message-ID: <trinity-9b4bdd56-ca1b-4b9e-9ea0-f028f2b9ecc8-1415237602023@3capp-mailcom-lxa03>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141106/2746cea6/attachment.htm>

From doc.holliday at usa.com  Thu Nov  6 01:43:12 2014
From: doc.holliday at usa.com (doc.holliday at usa.com)
Date: Thu, 6 Nov 2014 02:43:12 +0100
Subject: [squid-users] Behind enemy lines (squid behind proxy)
Message-ID: <trinity-337ee6a9-d92b-4bfb-8891-d15b0fedef7d-1415238191509@3capp-mailcom-lxa06>

[plain text version; sorry for the inconvenience]

I've searched through the internets and tried various things... to no avail. Hopefully someone here can point me in the right direction.
?
I am sitting behind a proxy, which accepts http/https. Everything else is blocked. If I instruct my browser to use this proxy,
everything works dandy. Both http and https.
?
The problem is, I have a few apps that don't have an option to set proxy. So, my idea was to set up squid on the local machine
that would transparently redirect http/https to the proxy. Eg something like this:
?
[ local_box: app (http or https) ---> squid ] ? ----->?? [ the_proxy ] ? ----->?? ...?? ----->?? [ internets ]
?
I have no control of the proxy, nor do I know what goes on after it.
?
I have the following iptables rules:

*nat
:PREROUTING ACCEPT [1:89]
:INPUT ACCEPT [1:89]
:OUTPUT ACCEPT [549:34321]
:POSTROUTING ACCEPT [624:38821]
-A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130
COMMIT
?
And my squid.conf is mostly garden variety:

acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777???????? # multiling http
acl Safe_ports port 901???????? # SWAT
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
coredump_dir /var/cache/squid
refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
refresh_pattern .?????????????? 0?????? 20%???? 4320
?
cache deny all
?
cache_peer proxy parent 3128 0 no-query no-digest default
never_direct allow all
?
http_port 3128
http_port 3129 intercept
https_port 3130 intercept ssl-bump generate-host-certificates=on cert=/etc/ssl/squid/cert.pem key=/etc/ssl/squid/key.pem
?
I've generated the certs and ran ssl_crtd to init ssl db dirs.
?
To verify squid is working, I've changed my browser proxy settings to 127.0.0.1:3128 for http and https.
Everything works like a charm.
?
This is where the "fun" begins:
?
==========
Without the proxy settings http also works just fine -- in both the browser and wget command. Https on the other hand is fubar.
In the browser I get "Unsupported Request Method and Protocol" error (after accepting the "invalid" certificate).
With wget I get:

local_box [~] wget https://google.com --no-check-certificate
--2014-11-05 20:21:12--? https://google.com/[https://google.com/]
Resolving google.com... 74.125.196.138, 74.125.196.139, 74.125.196.101, ...
Connecting to google.com|74.125.196.138|:443... connected.
WARNING: cannot verify google.com's certificate, issued by ?/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd?:
? Self-signed certificate encountered.
??? WARNING: certificate common name ?? doesn't match requested host name ?google.com?.
HTTP request sent, awaiting response... 501 Not Implemented
2014-11-05 20:21:12 ERROR 501: Not Implemented.
?

access.log says:
1415236731.852???? 19 10.0.0.13 TCP_MISS/501 4255 GET https://www.google.com/[https://www.google.com/] - FIRSTUP_PARENT/10.64.252.14 text/html
?
==========
If I add 'ssl_bump server-first all' to squid.conf. Whenever I try to pull up an https page, it barfs with:

2014/11/05 20:22:28| assertion failed: forward.cc:785: "peer->use_ssl"
Aborted
?
==========
If I change it to 'ssl_bump client-first all', I get "Unable to forward this request at this time" in the browser.
And wget says:

local_box [~] wget https://google.com[https://google.com] --no-check-certificate
--2014-11-05 20:26:53--? https://google.com/[https://google.com/]
Resolving google.com... 74.125.196.101, 74.125.196.100, 74.125.196.139, ...
Connecting to google.com|74.125.196.101|:443... connected.
WARNING: cannot verify google.com's certificate, issued by ?/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd?:
? Self-signed certificate encountered.
??? WARNING: certificate common name ?74.125.196.101? doesn't match requested host name ?google.com?.
HTTP request sent, awaiting response... 503 Service Unavailable
2014-11-05 20:26:53 ERROR 503: Service Unavailable.
?
access.log says:
1415237271.133????? 0 10.0.0.13 TCP_MISS/503 3840 GET https://google.com/[https://google.com/] - FIRSTUP_PARENT/10.64.252.14 text/html
?
==========
And so after endless searching and searching and trying various things I came here. Could please help me figure out why it is not working?
?
Thank you.
?
-D


From squid3 at treenet.co.nz  Thu Nov  6 03:48:53 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 06 Nov 2014 16:48:53 +1300
Subject: [squid-users] Behind enemy lines (squid behind proxy)
In-Reply-To: <trinity-9b4bdd56-ca1b-4b9e-9ea0-f028f2b9ecc8-1415237602023@3capp-mailcom-lxa03>
References: <trinity-9b4bdd56-ca1b-4b9e-9ea0-f028f2b9ecc8-1415237602023@3capp-mailcom-lxa03>
Message-ID: <545AEFA5.9030903@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 6/11/2014 2:33 p.m., doc.holliday at usa.com wrote:
> I've searched through the internets and tried various things... to
> no avail. Hopefully someone here can point me in the right
> direction. I am sitting behind a proxy, which accepts http/https.
> Everything else is blocked. If I instruct my browser to use this
> proxy, everything works dandy. Both http and https. The problem is,
> I have a few apps that don't have an option to set proxy. So, my 
> idea was to set up squid on the local machine that would
> transparently redirect http/https to the proxy. Eg something like
> this: [ local_box: app (http or https) ---> squid ]   ----->   [
> the_proxy ] ----->   ...   ----->   [ internets ] I have no control
> of the proxy, nor do I know what goes on after it.

<snip>
> And so after endless searching and searching and trying various
> things I came here. Could please help me figure out why it is not
> working?

HTTPS is supposed to offer end-to-end security. Squid attempts to
simulate that behaviour even when bumping. So by default bumped
traffic will be re-encrypted and sent directly to the origin server as
per DNS records.

What you have configured forces that not to happen then sends the
de-crypted traffic to the peer proxy as HTTP. The peer is rejecting
the un-encrypted protocol containing https:// URLs with a 503 for
whatever reason.

If the other peer is another Squid then chances are still fairly high
that it has been built without OpenSSL support and so literally cannot
open the TLS connection to deliver the https:// request to the origin.

Generating new CONNECT tunnels over peer proxies has not yet been
coded for Squid. Nobody seems willing sponsor its development, despite
all these problems bumping is now causing.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWu+lAAoJELJo5wb/XPRjBfQH/iXUPYRkq16T4cuSsrex2OIH
8XldBtMkzAYl4HjtiFHeK0uT5lV/BHvelP9hfKSs5zFa4Y1JjQeLYV3rLTXWIWME
VSsk7Q/PDUCMdEShenCDPFaSCHr8pSU8Ey7wCgco86yMm5SMEtjCxP0pmaauuUvq
PNH8CceJUWgVQFtVuRZq57MueNX0xSIxIzVdiOn22ajST4ZQoWthoHvsTybbup/l
4iBXp7wygwCOFmVs6/WFw7CA/e7yuxj6oWhL3q4vg4pFP23zA7uknERf4h6QVkQw
Ms3/QfXJDVvnRhya3jbqm1Eme/vIC4LDYKO1a++YBxHg8b4yxtN+D6oqF2Vlz1k=
=oPE/
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov  6 04:11:25 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 06 Nov 2014 17:11:25 +1300
Subject: [squid-users] could sslbump handle client certs better?
In-Reply-To: <545AA639.3010901@trimble.com>
References: <545AA639.3010901@trimble.com>
Message-ID: <545AF4ED.9050005@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 6/11/2014 11:35 a.m., Jason Haar wrote:
> I haven't tested this so I may be embarrassing myself, but I doubt 
> client certs and sslbump play nicely together as the end-server
> would never see any possible client cert interaction

SSL-bump in which Squid version?

There is an arms race going on between browsers, site owners and
bumping proxies. Each major series of Squid has had a different
variantion on what bumping can do and what breaks.


> 
> I was wondering how quickly the need of a client cert is
> announced?

see http://tools.ietf.org/html/rfc5246#section-7.4.6

> Could/does squid notice the server requirement for client certs and
> fall back into passthrough mode?

Maybe yes maybe no. As I understand things right now it is part of the
crypto which follows the 3rd (final?) peek-n-splice "step".


 It would certainly be a great option to
> have. ie force most https traffic through sslbump, but allow squid
> to bypass it for the (very) few sites that require client certs.

The ServerHelo has an explicit request for client-cert. So this demand
from the server should be detectable during SSL-bump step3 ACL
processing, even though the client cert itself is probably unavailable.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWvTtAAoJELJo5wb/XPRjuNMH/jUWy/neh2yqGeJKrayRnwPz
0WI1m9+433eVNE2vyBalFdEgdBCop+gdFPHYIZDB0neC+jjy/m9bnKquE7RUm1pi
Tw7qJVOBaf5f89tmYwX1YuTX46TUFkzQ7I588JsU50rNxe+db6VoHIuJ3JZyS0tm
g4kYkZ1XO4Hbh+6Bs/iDZu/jvxCRDudVAUM/lkQzYkPPP1KCiqLAHOxujHJO8sud
cmVFnl3X+wOGHOAkAs60RWfvrR4MSGBz18WpWprBJ+rPUXi0JAvwdgiIoZmTRx4S
xij3f2TkFK678YIobZguHWHojg8zFWmzjX2WZ7m1SLqHuFjeUXeFtKZBVae8kBg=
=odgE
-----END PGP SIGNATURE-----


From ric.castellani at alice.it  Thu Nov  6 07:58:49 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Thu, 6 Nov 2014 08:58:49 +0100 (CET)
Subject: [squid-users] true sizeof squid cache
Message-ID: <149841cf22e.ric.castellani@alice.it>

I'm installing new machine as Squid server and I need to understand what 
criteria to estimate the 'cache size', I'm not speaking about extra space for 
swap/temporary files or fragmentation but I'm saying about the cache of size 
that is 3rd cache_dir argument.
Server will receive requests from about 2000-
3000 computers.
I'd like interesting to find documents where shows suggestions 
to assign right size.


From navari.lorenzo at gmail.com  Thu Nov  6 08:45:20 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Thu, 6 Nov 2014 00:45:20 -0800 (PST)
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <5459FBED.8070905@treenet.co.nz>
References: <1415180359160-4668210.post@n4.nabble.com>
 <5459FBED.8070905@treenet.co.nz>
Message-ID: <1415263520725-4668228.post@n4.nabble.com>

hello boys,
excuse my bad english

there is something i don't understand.
If i write an URL into a browser which use Squid (for example www.xxx.com)
(denied whith an acl)

I expect that Squid answer saying:  you cannot access this url because it is
a denied url.
This should happen without squid goes to look for that url.

What's wrong ? 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210p4668228.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Nov  6 09:16:33 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 06 Nov 2014 22:16:33 +1300
Subject: [squid-users] true sizeof squid cache
In-Reply-To: <149841cf22e.ric.castellani@alice.it>
References: <149841cf22e.ric.castellani@alice.it>
Message-ID: <545B3C71.4060400@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 6/11/2014 8:58 p.m., Riccardo Castellani wrote:
> I'm installing new machine as Squid server and I need to understand
> what criteria to estimate the 'cache size', I'm not speaking about
> extra space for swap/temporary files or fragmentation but I'm
> saying about the cache of size that is 3rd cache_dir argument. 
> Server will receive requests from about 2000- 3000 computers. I'd
> like interesting to find documents where shows suggestions to
> assign right size.

Assign the amount of disk you want to use. "right size" all depends on
what your clients patterns of usage are. That varies between networks
and even over time within one network. Also whether you want to focus
on bandwidth reduction or request reduction. Some networks run with no
disk cache, others with Terabytes.

So ... start with a guess that seems reasonable to you (tens of GB? or
hundreds of MB?) and see what happens before tuning it up/down to try
and reach a better hit ratio(s) and longer LRU object timespan.

BTW, the disk cache_dir *is* "swap" file space for Squid. All caching
is temporary files.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWzxxAAoJELJo5wb/XPRjXA4H/RGYCLaPhR8+4Nnhenf3eAj7
kAkNud1zbIhghfYkX2FIBZt51E9ZP/h+w89jC21V1dt1nitE7b4zD6mcZq8eTxm5
asjnwtoH2U70gBYsvFaVUDLE9pv1yxa5LGxV7OiKa81OQmsBtSjbsUjpY2LTrQE8
m53Q1xdHtg4FeX4TIvlj8dLYc3lgSm2BKERHdV08kZAlFPQtkBx8ZrN+dXUto+QO
WfyNRYKuhyq4TbmdNM19KIqUmBjAIa/L8N02IZ5+FCmt1/Pn8jsLvKEl112aE0yT
8QxcyX5m+sBQIcG5IyFzhSGxN3pDL4m5btd/9XhgDwqJwFz4E3KyfWRkkuQeBBg=
=VfOs
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov  6 09:23:48 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 06 Nov 2014 22:23:48 +1300
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <1415263520725-4668228.post@n4.nabble.com>
References: <1415180359160-4668210.post@n4.nabble.com>
 <5459FBED.8070905@treenet.co.nz> <1415263520725-4668228.post@n4.nabble.com>
Message-ID: <545B3E24.8080303@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 6/11/2014 9:45 p.m., navari.lorenzo wrote:
> hello boys, excuse my bad english
> 
> there is something i don't understand. If i write an URL into a
> browser which use Squid (for example www.xxx.com) (denied whith an
> acl)
> 
> I expect that Squid answer saying:  you cannot access this url
> because it is a denied url. This should happen without squid goes
> to look for that url.
> 
> What's wrong ?

Your thinking is wrong.

1) When you enter an HTTP URI the browser passes *the URL* to Squid
and asks for it.
 -> Squid replies 403 Forbidden (to access URL)

2) when you enter an HTTPS URL the browser tries to open a CONNECT
tunnel through the proxy.
 -> Squid replies 403 Forbidden (to setup tunnel).

*** HTTPS URL is never seen by Squid (unseen URL cannot be forbidden)

 -> Browser leaves your https:// URL in the address bar.

 ** Browser people think its false to display the 403 Forbidden (about
CONNET) as the page, because the URL in address bar was not forbidden.
Which is understandable though annoying - only the CONNECT tunnel is
known to be forbidden.

So what does the browser display for HTTPS URLs?
  "Cannot CONNECT to server" or something like that.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUWz4jAAoJELJo5wb/XPRj0fQH/0K3+WFy9ld2g7div1ygGjbA
Y8xHFSTEXlw1QcuFHGDudKaAKVsGrnbLk0MTKSVcG0zooWpvqHKM3gvtkRt2da9K
vPMNGgI1cZ97NZqoJOOYlI8aXSpVUT6Bx/1+cPRLdmIpbD8OZSIhZ3UfPBtL3VIN
gFtgkMAGj5quQmjHIFOJbXZgkc+zZY9rbt0oBhji8CtK6B9DfK2yRZe25gneDnz4
rt9TzUoaFnExvdsJUKWRGTqn0fEc9N5JDaK1kx/UBgjNPpszbVUVZQFVPipe/Mrn
Gz4bff49EeQ2ZsAoCrSr7dalB1dcH2fPs2ROGYNtPG7HxxYf8AN+t85jTP9NUsA=
=fNhm
-----END PGP SIGNATURE-----


From navari.lorenzo at gmail.com  Thu Nov  6 09:54:31 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Thu, 6 Nov 2014 01:54:31 -0800 (PST)
Subject: [squid-users] TCP_DENIED/403
In-Reply-To: <545B3E24.8080303@treenet.co.nz>
References: <1415180359160-4668210.post@n4.nabble.com>
 <5459FBED.8070905@treenet.co.nz> <1415263520725-4668228.post@n4.nabble.com>
 <545B3E24.8080303@treenet.co.nz>
Message-ID: <1415267671945-4668231.post@n4.nabble.com>

Thank You.
Now I understand.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-DENIED-403-tp4668210p4668231.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Thu Nov  6 11:58:36 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 06 Nov 2014 13:58:36 +0200
Subject: [squid-users] Squid 3.5.0.2 beta is available
In-Reply-To: <CAAdA2WMQr5quVSC20fD4781dfOpbknToDMNotP8qVV_HXai=KA@mail.gmail.com>
References: <545859F9.1070406@treenet.co.nz>
 <CAAdA2WP4RtujsjPnQmuTKyHWKu-1og_2On-r0AtutgJNv_3AJw@mail.gmail.com>
 <5459D556.9080608@ngtech.co.il>
 <CAAdA2WM8tpinZxwGy-EzpuEuxAzrVXYO-_Kude+VDgK-BretzA@mail.gmail.com>
 <5459EACB.2030602@ngtech.co.il>
 <CAAdA2WMQr5quVSC20fD4781dfOpbknToDMNotP8qVV_HXai=KA@mail.gmail.com>
Message-ID: <545B626C.8040107@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/05/2014 11:56 AM, Odhiambo Washington wrote:
> Hi Eliezer,
> 
> That link should be fine, although my system is actually PC-BSD.
> The version is the same though an old version. My exact version
> is: 
> ftp://ftp.pcbsd.org/pub/archived/9.1-RELEASE/amd64/PCBSD9.1-RELEASE-p19-09-07-2013-x64-DVD.iso

OK,

This
> 
is a very important missing details.
I can test it but with all the needed details since it works fine on
freebsd 9.stable(9.2).

Can you provide a link to the PC-BSD download?

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUW2JsAAoJENxnfXtQ8ZQUcJkH/3WmFtyT4xZ6jtQkOeRZvf4F
6Xg1RIEmSIisJqI3dgfXr4mZ2SJlKiR5bogo+r+kh/YJomQaPhlrKmmB4G4iirS+
WP73OcWOed7koqHgRF7twilRLVrAAm7bJ8SFafijUmHIGO5RKFG+4bX1YLPhPp8o
rJlGrX8VNDogKsowcis0xvV+25cbsc7jlm0fE9oB3Kfix0Sy6lHUUe/msWi0ifdJ
Mpus6qBC3vkq4PfcwbxAKAYegRvdzky83umkIKJ9WhMkUQv+vTtNEqcHVYBmnNZG
Z1ROX5w5UFWf9LtMu1YfUEi/jx4Q1++NXNEJYTJhh0tso/0BxAew8UDGDzY021M=
=q1Vk
-----END PGP SIGNATURE-----


From tomtux007 at gmail.com  Thu Nov  6 13:50:15 2014
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 6 Nov 2014 14:50:15 +0100
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
Message-ID: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>

Hi

After migration from squid 3.3.13 to 3.4.4, I recognized a
performance-issue. Squid is configured with 4 workers. They often have
a CPU-Utilization between 50%-90% (each worker). With squid 3.3.13
(same configuration), the CPU-Utilization was never a problem. I
installed squid 3.4.9 and had the same issue. No warnings/errors in
the cache.log

I saw, that someone other reported a similar issue:
http://www.squid-cache.org/mail-archive/squid-users/201407/0500.html

Concerning the post above: Yes, we have external_auth-helpers
(ext_kerberos_ldap_group_acl) and no, we do not use delay_pools. The
high cpu-usage comes not from the auth-helper - it comes from the 4
squid-worker-processes.
Any hints? Is this a known problem? Probably solved in 3.5?

Kind regards,
Tom


From sudakov at sibptus.tomsk.ru  Thu Nov  6 15:50:11 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Thu, 6 Nov 2014 21:50:11 +0600
Subject: [squid-users] Kerberos Authentication Failing for Windows
 7+with BH gss_accept_sec_context() failed
In-Reply-To: <20141102180025.GB23817@admin.sibptus.tomsk.ru>
References: <m2eqo7$cf9$1@ger.gmane.org>
 <FD6832B9-3F1F-48C6-A76F-47A224F1697B@gmail.com>
 <m2g7l9$1ee$1@ger.gmane.org>
 <94F74226-F24B-4910-95B7-B86ACE815995@gmail.com>
 <m2m7g0$9l7$1@ger.gmane.org>
 <b4adceec-5a53-4212-b16c-106237fc4504@Pedros-iPhone>
 <m2mbc1$b54$1@ger.gmane.org>
 <09275CEC-ABC1-4BC6-B4F3-546E8C5D3B7E@gmail.com>
 <m32m6c$teg$1@ger.gmane.org>
 <20141102180025.GB23817@admin.sibptus.tomsk.ru>
Message-ID: <20141106155011.GA10616@admin.sibptus.tomsk.ru>

Victor Sudakov wrote:
> 
> However, I am eager to know what could be causing such weird tickets
> to be issued, but I think only a Windows expert can tell. After all,
> the key in the tickets is correct, only the principal name is changed.
> I only suspect that the name is changed when the client sets the
> Canonicalize option in the request, and not all clients do that.
> 
> <rant>I have not been able to find such an expert, most Windows admins I
> know are GUI mouse boys without thorough understanding of Windows
> internals.</rant>

I have found a Windows expert who suggested editing the registry:

REGEDIT4

[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Lsa\Kerberos\Domains\DOMAIN]
"RealmFlags"=dword:00000000

[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Lsa\Kerberos\Domains\domain.example]
"RealmFlags"=dword:00000000

The link to Windows docs where "RealmFlags" is mentioned: 
http://msdn.microsoft.com/en-us/library/cc233855.aspx

I have updated the Russian Howto accordingly 
https://bitbucket.org/victor_sudakov/faq/src/tip/FAQ/squid_kerberos.txt

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From doc.holliday at usa.com  Thu Nov  6 22:30:28 2014
From: doc.holliday at usa.com (doc.holliday at usa.com)
Date: Thu, 6 Nov 2014 23:30:28 +0100
Subject: [squid-users] Behind enemy lines (squid behind proxy)
Message-ID: <trinity-de15a38f-07f4-4fee-94b1-4a2ecf861000-1415313027728@3capp-mailcom-lxa05>


> Sent:?Wednesday, November 05, 2014 at 10:48 PM
> From:?"Amos Jeffries" <squid3 at treenet.co.nz>

> On 6/11/2014 2:33 p.m., doc.holliday at usa.com wrote:
>>
>> I've searched through the internets and tried various things... to
>> no avail. Hopefully someone here can point me in the right
>> direction. I am sitting behind a proxy, which accepts http/https.
>> Everything else is blocked. If I instruct my browser to use this
>> proxy, everything works dandy. Both http and https. The problem is,
>> I have a few apps that don't have an option to set proxy. So, my
>> idea was to set up squid on the local machine that would
>> transparently redirect http/https to the proxy. Eg something like
>> this: [ local_box: app (http or https) ---> squid ] -----> [
>> the_proxy ] -----> ... -----> [ internets ] I have no control
>> of the proxy, nor do I know what goes on after it.
>
> What you have configured forces that not to happen then sends the
> de-crypted traffic to the peer proxy as HTTP. The peer is rejecting
> the un-encrypted protocol containing https:// URLs with a 503 for
> whatever reason.
>
> If the other peer is another Squid then chances are still fairly high
> that it has been built without OpenSSL support and so literally cannot
> open the TLS connection to deliver the https:// request to the origin.
>
> Generating new CONNECT tunnels over peer proxies has not yet been
> coded for Squid. Nobody seems willing sponsor its development, despite
> all these problems bumping is now causing.
>
> Amos

Thanks Amos. It makes sense... mostly. :)

One thing I am wondering though is, if I set my browser to use the proxy
(in the browser settings) for both http and https, both work fine. So, it
seems the proxy server supports both http and https over CONNECT tunnels.

So, if the squid on the local_box is not talking to the_proxy (it's cache_peer)
via CONNECT, what does it use?

-D
?


From dan at getbusi.com  Thu Nov  6 23:25:36 2014
From: dan at getbusi.com (dan at getbusi.com)
Date: Thu, 06 Nov 2014 15:25:36 -0800 (PST)
Subject: [squid-users] assertion failed: client_side.cc:1515:
 "connIsUsable(http->getConn())
In-Reply-To: <CAN8nrKB+9oz8nk3sKwTKRgFNovzWtMMb5YbyfgAqP+Bfm2pVcg@mail.gmail.com>
References: <CAN8nrKB+9oz8nk3sKwTKRgFNovzWtMMb5YbyfgAqP+Bfm2pVcg@mail.gmail.com>
Message-ID: <1415316336554.2e048ede@Nodemailer>

Bumping this with another backtrace. Happened at 16:05 this time, when the system was not very very busy.




It?s causing squid to crash in such a way that I actually have to `kill -9` the process in order to get things restarted properly.




Would really appreciate any feedback at all from anyone who can understand these back traces.

On Thu, Nov 6, 2014 at 9:56 AM, Dan Charlesworth <dan at getbusi.com> wrote:

> Terribly sorry, but I only attached half the traceback before. I've
> attached two *full* tracebacks this time; one from today and one from
> yesterday.
> This is from a forward proxy at a school, where this crash happens exactly
> once per day between 9:00 and 9:30.
> On 5 November 2014 11:42, <dan at getbusi.com> wrote:
>>  Hi folks
>>
>> I think I raised this crash a few months ago, but I can?t locate the
>> thread now, so I?m starting a new one now that I actually have a gdb
>> backtrace to present.
>>
>> I suspect it?s to do with one of our external ACLs (we use a few), but the
>> backtrace I?ve attached is most undecipherable for me?hopefully someone
>> else here can understand it.
>>
>> Thanks as always
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141106/5bf60da4/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: squid-assertion-failed-gdb-bt-4.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141106/5bf60da4/attachment.txt>

From squid at icshk.com  Fri Nov  7 03:25:36 2014
From: squid at icshk.com (squid at icshk.com)
Date: Fri, 7 Nov 2014 11:25:36 +0800
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
Message-ID: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>

Hello all,

 

As our company policy only allow some machines to access to some SSL website
URL(eg. https://www.google.com/maps). However, they do not have access to
https://www.google.com/ Before, we tried to implement authentication,
everything works fine. We try to allow https access to
https://www.google.com/maps and "CONNECT" request to www.google.com
<http://www.google.com>  using SSL bump. Now, I want to preserve this
config, and let user to authenicate to access to any website. Access to
google maps(https://www.google.com/maps) should prevent any authentication
need. However, I am not success to figure this out. I have tried different
kinds of configuration, some will prompt for authentication. Some will not
allow the authenticated users to access to https://www.google.com. From the
access log, after I authenticate and try to access to
https://www.google.com, the authentication information is not displayed.
Seems squid do not use the authentication information when matching the this
rule: "http_access     allow   CONNECT                 google".

The "CONNECT" method is success. Then, the squid will continue use no
authentication information to process the "GET" command, causing the
authenticated user to denied access to https://www.google.com.

Can I make squid always use the authentication information if already
authenticate ? Or any suggestion to implement this policy.

Thanks.

 

Here is an extracted version of config which should state the related
configuration:

 

auth_param basic children 5

auth_param basic realm Welcome to Our Website!

auth_param basic program /usr/lib64/squid/basic_ncsa_auth
/etc/squid/squid_user

auth_param basic credentialsttl 2 hours

auth_param basic casesensitive off

 

acl my_auth proxy_auth REQUIRED

 

acl SSL_ports port 443

acl Safe_ports port 443         # https

acl CONNECT method CONNECT

 

acl     GoogleMaps           url_regex -i    ^https://www.google.com/maps*.

acl     test_net                 src             192.168.1.253/32

acl     google                    dstdomain    www.google.com
<http://www.google.com> 

http_access deny CONNECT !SSL_ports

 

http_access     allow                           GoogleMaps

 

http_access     allow   CONNECT                 google

http_access     deny    CONNECT                 google
my_auth

#http_access    allow   CONNECT                 test_net
google

 

http_access     allow   my_auth                all

 

http_access     deny                            all

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141107/767b6b1c/attachment.htm>

From ric.castellani at alice.it  Fri Nov  7 05:23:50 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Fri, 7 Nov 2014 06:23:50 +0100
Subject: [squid-users] true sizeof squid cache
In-Reply-To: <545B3C71.4060400@treenet.co.nz>
References: <149841cf22e.ric.castellani@alice.it>
 <545B3C71.4060400@treenet.co.nz>
Message-ID: <54BEAC6A-8DD2-47A1-B655-0BE099759D33@alice.it>

Does it exist guideline to tune up/down cache_dir size?
What do you think about use of ram disk as cache_dir vs hard disk ?

On 06 Nov 2014, at 10:16, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> On 6/11/2014 8:58 p.m., Riccardo Castellani wrote:
>> I'm installing new machine as Squid server and I need to understand
>> what criteria to estimate the 'cache size', I'm not speaking about
>> extra space for swap/temporary files or fragmentation but I'm
>> saying about the cache of size that is 3rd cache_dir argument. 
>> Server will receive requests from about 2000- 3000 computers. I'd
>> like interesting to find documents where shows suggestions to
>> assign right size.
> 
> Assign the amount of disk you want to use. "right size" all depends on
> what your clients patterns of usage are. That varies between networks
> and even over time within one network. Also whether you want to focus
> on bandwidth reduction or request reduction. Some networks run with no
> disk cache, others with Terabytes.
> 
> So ... start with a guess that seems reasonable to you (tens of GB? or
> hundreds of MB?) and see what happens before tuning it up/down to try
> and reach a better hit ratio(s) and longer LRU object timespan.
> 
> BTW, the disk cache_dir *is* "swap" file space for Squid. All caching
> is temporary files.
> 
> Amos
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
> 
> iQEcBAEBAgAGBQJUWzxxAAoJELJo5wb/XPRjXA4H/RGYCLaPhR8+4Nnhenf3eAj7
> kAkNud1zbIhghfYkX2FIBZt51E9ZP/h+w89jC21V1dt1nitE7b4zD6mcZq8eTxm5
> asjnwtoH2U70gBYsvFaVUDLE9pv1yxa5LGxV7OiKa81OQmsBtSjbsUjpY2LTrQE8
> m53Q1xdHtg4FeX4TIvlj8dLYc3lgSm2BKERHdV08kZAlFPQtkBx8ZrN+dXUto+QO
> WfyNRYKuhyq4TbmdNM19KIqUmBjAIa/L8N02IZ5+FCmt1/Pn8jsLvKEl112aE0yT
> 8QxcyX5m+sBQIcG5IyFzhSGxN3pDL4m5btd/9XhgDwqJwFz4E3KyfWRkkuQeBBg=
> =VfOs
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid at visolve.com  Fri Nov  7 07:35:42 2014
From: squid at visolve.com (squid-list)
Date: Fri, 07 Nov 2014 13:05:42 +0530
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
In-Reply-To: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
References: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
Message-ID: <545C764E.5040007@visolve.com>

Hi,
*
**"Access to google maps(https://www.google.com/maps) should prevent any 
authentication need"*

I could understand that all users should be able to access the google 
maps link without any authentication. For this you could add the site 
acl before the authentication part in the squid conf. So that users will 
not prompt for the authentication when the user try to access the google 
map site. But when they try to access any other site authentication will 
be prompted.

(i.e)
acl GoogleMaps           url_regex -i ^https://www.google.com/maps*.
         acl allow GoogleMaps all

auth_param basic children 5

auth_param basic realm Welcome to Our Website!

auth_param basic program /usr/lib64/squid/basic_ncsa_auth 
/etc/squid/squid_user

auth_param basic credentialsttl 2 hours

         auth_param basic casesensitive off

         ....
         ....

I am not clear about the remaining part of the content.

Regards,
ViSolve Squid

On 11/07/2014 08:55 AM, squid at icshk.com wrote:
>
> Hello all,
>
> As our company policy only allow some machines to access to some SSL 
> website URL(eg. https://www.google.com/maps). However, they do not 
> have access to https://www.google.com/ Before, we tried to implement 
> authentication, everything works fine. We try to allow https access to 
> https://www.google.com/maps and ?CONNECT? request to www.google.com 
> <http://www.google.com> using SSL bump. Now, I want to preserve this 
> config, and let user to authenicate to access to any website. Access 
> to google maps(https://www.google.com/maps) should prevent any 
> authentication need. However, I am not success to figure this out. I 
> have tried different kinds of configuration, some will prompt for 
> authentication. Some will not allow the authenticated users to access 
> to https://www.google.com. From the access log, after I authenticate 
> and try to access to https://www.google.com, the authentication 
> information is not displayed. Seems squid do not use the 
> authentication information when matching the this rule: 
> ?http_access     allow   CONNECT google?.
>
> The ?CONNECT? method is success. Then, the squid will continue use no 
> authentication information to process the ?GET? command, causing the 
> authenticated user to denied access to https://www.google.com.
>
> Can I make squid always use the authentication information if already 
> authenticate ? Or any suggestion to implement this policy.
>
> Thanks.
>
> Here is an extracted version of config which should state the related 
> configuration:
>
> auth_param basic children 5
>
> auth_param basic realm Welcome to Our Website!
>
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth 
> /etc/squid/squid_user
>
> auth_param basic credentialsttl 2 hours
>
> auth_param basic casesensitive off
>
> acl my_auth proxy_auth REQUIRED
>
> acl SSL_ports port 443
>
> acl Safe_ports port 443         # https
>
> acl CONNECT method CONNECT
>
> acl GoogleMaps           url_regex -i ^https://www.google.com/maps*.
>
> acl test_net                 src             192.168.1.253/32
>
> acl google                    dstdomain www.google.com 
> <http://www.google.com>
>
> http_access deny CONNECT !SSL_ports
>
> http_access allow                           GoogleMaps
>
> http_access allow   CONNECT                 google
>
> http_access deny    CONNECT                 google my_auth
>
> #http_access allow   CONNECT                 test_net google
>
> http_access allow   my_auth                all
>
> http_access deny                            all
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141107/8c10f040/attachment.htm>

From squid at icshk.com  Fri Nov  7 07:51:41 2014
From: squid at icshk.com (squid at icshk.com)
Date: Fri, 7 Nov 2014 15:51:41 +0800
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
In-Reply-To: <545C764E.5040007@visolve.com>
References: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
 <545C764E.5040007@visolve.com>
Message-ID: <004d01cffa5f$aae5d5a0$00b180e0$@icshk.com>

Hi,

 

Sorry for my poor English. I think maybe I have figure out how to do this. Seems I need to explicitly specific allow ?google? for ?my_auth? users.

 

Add this:

http_access     allow   google                 my_auth

before

http_access     allow   my_auth                all

 

auth_param basic children 5

auth_param basic realm Welcome to Our Website!

auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_user

auth_param basic credentialsttl 2 hours

auth_param basic casesensitive off

 

acl my_auth proxy_auth REQUIRED

 

acl SSL_ports port 443

acl Safe_ports port 443         # https

acl CONNECT method CONNECT

 

acl     GoogleMaps           url_regex -i    ^https://www.google.com/maps*.

acl     test_net                 src             192.168.1.253/32

acl     google                    dstdomain    www.google.com <http://www.google.com> 

http_access deny CONNECT !SSL_ports

 

http_access     allow                           GoogleMaps

 

http_access     allow   CONNECT                 google

http_access     deny    CONNECT                 google                 my_auth

#http_access    allow   CONNECT                 test_net                 google

 

http_access     allow   google                 my_auth

http_access     allow   my_auth                all

 

http_access     deny                            all

 

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-list
Sent: Friday, November 07, 2014 3:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid ACL, SSL-BUMP and authentication questions

 

Hi,

"Access to google maps(https://www.google.com/maps) should prevent any authentication need"

I could understand that all users should be able to access the google maps link without any authentication. For this you could add the site acl before the authentication part in the squid conf. So that users will not prompt for the authentication when the user try to access the google map site. But when they try to access any other site authentication will be prompted.

(i.e)
        acl     GoogleMaps           url_regex -i    ^https://www.google.com/maps*.
        acl allow GoogleMaps all

        auth_param basic children 5

        auth_param basic realm Welcome to Our Website!

        auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_user

        auth_param basic credentialsttl 2 hours

        auth_param basic casesensitive off    

        ....
        ....

I am not clear about the remaining part of the content.

Regards,
ViSolve Squid

On 11/07/2014 08:55 AM, squid at icshk.com <mailto:squid at icshk.com>  wrote:

Hello all,

 

As our company policy only allow some machines to access to some SSL website URL(eg. https://www.google.com/maps). However, they do not have access to https://www.google.com/ Before, we tried to implement authentication, everything works fine. We try to allow https access to https://www.google.com/maps and ?CONNECT? request to www.google.com <http://www.google.com>  using SSL bump. Now, I want to preserve this config, and let user to authenicate to access to any website. Access to google maps(https://www.google.com/maps) should prevent any authentication need. However, I am not success to figure this out. I have tried different kinds of configuration, some will prompt for authentication. Some will not allow the authenticated users to access to https://www.google.com. From the access log, after I authenticate and try to access to https://www.google.com, the authentication information is not displayed. Seems squid do not use the authentication information when matching the this rule: ?http_access     allow   CONNECT                 google?.

The ?CONNECT? method is success. Then, the squid will continue use no authentication information to process the ?GET? command, causing the authenticated user to denied access to https://www.google.com.

Can I make squid always use the authentication information if already authenticate ? Or any suggestion to implement this policy.

Thanks.

 

Here is an extracted version of config which should state the related configuration:

 

auth_param basic children 5

auth_param basic realm Welcome to Our Website!

auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_user

auth_param basic credentialsttl 2 hours

auth_param basic casesensitive off

 

acl my_auth proxy_auth REQUIRED

 

acl SSL_ports port 443

acl Safe_ports port 443         # https

acl CONNECT method CONNECT

 

acl     GoogleMaps           url_regex -i    ^https://www.google.com/maps*.

acl     test_net                 src             192.168.1.253/32

acl     google                    dstdomain    www.google.com <http://www.google.com> 

http_access deny CONNECT !SSL_ports

 

http_access     allow                           GoogleMaps

 

http_access     allow   CONNECT                 google

http_access     deny    CONNECT                 google                 my_auth

#http_access    allow   CONNECT                 test_net                 google

 

http_access     allow   my_auth                all

 

http_access     deny                            all

 





_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141107/98c4be01/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov  7 08:28:33 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 07 Nov 2014 21:28:33 +1300
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
In-Reply-To: <545C764E.5040007@visolve.com>
References: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
 <545C764E.5040007@visolve.com>
Message-ID: <545C82B1.9040105@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 8:35 p.m., squid-list wrote:
> Hi, * **"Access to google maps(https://www.google.com/maps) should
> prevent any authentication need"*
> 
> I could understand that all users should be able to access the
> google maps link without any authentication. For this you could add
> the site acl before the authentication part in the squid conf. So
> that users will not prompt for the authentication when the user try
> to access the google map site. But when they try to access any
> other site authentication will be prompted.

This cannot be done.

You can authenticate the user setting up a CONNECT tunnel, OR you can
bypass authentication for them.

That authentication choice applies equally all requests sent over the
tunnel. Whether they are for maps or for any other Google service. And
it must be made *before* the tunnel is setup. Thus *before* the URL
inside the tunnel becomes known.


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXIKwAAoJELJo5wb/XPRjMoMH/2yCMjxisbxWBAYnp+96908O
W46taJk7kqwUbtv76aOsSEcPpc3cBl4E+nFv7cQofRqgobcR2wTsJtgRupjuIgSb
SYPQKqJolbs/7wF5nhxbggewSfRU7B21aULKStkXV7BUWNlUIaV1vUsv+J1JV8OP
U/HkcVeXny1khCjF9nEKeXNUpOioUQ0LpPboAOrLnfZZfY098NkGubJF04/stUCQ
QXIErZ8cwX7yJ1x+yIwlVw4KVbtGaBJ8dd8PH4q3DknzAVxfJ0LZgYJC3nKTQMZ3
vUTMV33Rf94Y9x/yNrs6AVWcR3rLl08GkpFv3owqItkHa1hi7yFCuEg5e3bOFFA=
=AMi0
-----END PGP SIGNATURE-----


From ric.castellani at alice.it  Fri Nov  7 09:20:03 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Fri, 7 Nov 2014 10:20:03 +0100 (CET)
Subject: [squid-users] TCP_DENIED/411
Message-ID: <149898dab3d.ric.castellani@alice.it>

Can I bypass this "TCP_DENIE/D411" error in Squid?
I read this error is about 
"Length Required" field but we can permit Squid can handle this request POST ?



From dm at belkam.com  Fri Nov  7 09:29:17 2014
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 07 Nov 2014 13:29:17 +0400
Subject: [squid-users] 3.4.9 and tls 1.2
Message-ID: <545C90ED.9080200@belkam.com>

Hello!

I just found that I have problems with using google maps , i.e. 
https://maps.google.com with firefox 33.0 ovr squid 3.4.9-
it works extremely slow, sometimes street view can't be loaded and 
showed black screen.
This is wit default  security.tls.version.max=3, i.e. tls 1.2 , but if I 
change this to 2, i.e. tls 1.1 all works fine.
Direct, i.e. without squid, connection always works OK, so, I guess this 
is squid problem.
Any ideas how to solve this?

Thank you!



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141107/bd412680/attachment.htm>

From ric.castellani at alice.it  Fri Nov  7 09:32:02 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Fri, 7 Nov 2014 10:32:02 +0100 (CET)
Subject: [squid-users] R:  TCP_DENIED/411
Message-ID: <1498998a60f.ric.castellani@alice.it>

Problem is in POST operation

>----Messaggio originale----
>Da: ric.
castellani at alice.it
>Data: 7-nov-2014 10.20
>A: 
>Ogg: [squid-users] 
TCP_DENIED/411
>
>Can I bypass this "TCP_DENIE/D411" error in Squid?
>I read 
this error is about 
>"Length Required" field but we can permit Squid can 
handle this request POST ?
>
>_______________________________________________

>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users>




From squid3 at treenet.co.nz  Fri Nov  7 09:36:16 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 07 Nov 2014 22:36:16 +1300
Subject: [squid-users] TCP_DENIED/411
In-Reply-To: <149898dab3d.ric.castellani@alice.it>
References: <149898dab3d.ric.castellani@alice.it>
Message-ID: <545C9290.7010000@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 10:20 p.m., Riccardo Castellani wrote:
> Can I bypass this "TCP_DENIE/D411" error in Squid? I read this
> error is about "Length Required" field but we can permit Squid can
> handle this request POST ?

If you send a message without saying how big its attached object is.
When is the server supposed to know that it has finished and the next
one starting?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXJKQAAoJELJo5wb/XPRjw3QIAIsuzGjrIserp22emYlYow0x
tsI7fnjDWi+BMa15gSDL1ZA8jujngErSkpfLRsniTxmVC21G8BTyNfPvYbNHBB+v
iQMqebkZMLiP6cUc+PNxPMBrdmaxbWfc276LYhvUr7AI83EWzNNJKckFHw5D98zZ
37KyYuwQlN0CYjgg7YzxtFeg9XWSvaiMTdenXXkfS6Yb/7sSDr2VKs1ue/PBwKu/
juDKV5DunmaG74pY5hM+ijS9RhMDbuaMYcrgrzpgZ1mgq13qkG84z4OD7YEAUJkM
1Y1DlmpJuCu1SlWX87CtDKuLWZwoL2s6ciJCuVNfHDwOZowiC0RVJd5acUgX4vo=
=WzpB
-----END PGP SIGNATURE-----


From ric.castellani at alice.it  Fri Nov  7 09:56:04 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Fri, 7 Nov 2014 10:56:04 +0100 (CET)
Subject: [squid-users] R: Re:  TCP_DENIED/411
Message-ID: <14989aea4c1.ric.castellani@alice.it>

Is there way to permit this POST operation?
I agree you are saying but it's 
problem to block these requests nows


>----Messaggio originale----
>Da: 
squid3 at treenet.co.nz
>Data: 7-nov-2014 10.36
>A: 
>Ogg: Re: [squid-users] 
TCP_DENIED/411
>
>-----BEGIN PGP SIGNED MESSAGE-----
>Hash: SHA1
>
>On 
7/11/2014 10:20 p.m., Riccardo Castellani wrote:
>> Can I bypass this 
"TCP_DENIE/D411" error in Squid? I read this
>> error is about "Length 
Required" field but we can permit Squid can
>> handle this request POST ?
>
>If 
you send a message without saying how big its attached object is.
>When is the 
server supposed to know that it has finished and the next
>one starting?
>

>Amos
>
>-----BEGIN PGP SIGNATURE-----
>Version: GnuPG v2.0.22 (MingW32)
>

>iQEcBAEBAgAGBQJUXJKQAAoJELJo5wb/XPRjw3QIAIsuzGjrIserp22emYlYow0x

>tsI7fnjDWi+BMa15gSDL1ZA8jujngErSkpfLRsniTxmVC21G8BTyNfPvYbNHBB+v

>iQMqebkZMLiP6cUc+PNxPMBrdmaxbWfc276LYhvUr7AI83EWzNNJKckFHw5D98zZ

>37KyYuwQlN0CYjgg7YzxtFeg9XWSvaiMTdenXXkfS6Yb/7sSDr2VKs1ue/PBwKu/

>juDKV5DunmaG74pY5hM+ijS9RhMDbuaMYcrgrzpgZ1mgq13qkG84z4OD7YEAUJkM

>1Y1DlmpJuCu1SlWX87CtDKuLWZwoL2s6ciJCuVNfHDwOZowiC0RVJd5acUgX4vo=
>=WzpB
>-----
END PGP SIGNATURE-----
>_______________________________________________
>squid-
users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users>




From squid at icshk.com  Fri Nov  7 10:04:57 2014
From: squid at icshk.com (squid at icshk.com)
Date: Fri, 7 Nov 2014 18:04:57 +0800
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
In-Reply-To: <545C82B1.9040105@treenet.co.nz>
References: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
 <545C764E.5040007@visolve.com> <545C82B1.9040105@treenet.co.nz>
Message-ID: <005e01cffa72$48fefd40$dafcf7c0$@icshk.com>

Hi Amos,

The configuration I post last time still cannot accomplish the tasks. So, you mean the "CONNECT" ACL and must pair with normal "GET" command ACL to be evaluated by squid ? 

Best,
Kelvin Yip

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, November 07, 2014 4:29 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid ACL, SSL-BUMP and authentication questions

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 8:35 p.m., squid-list wrote:
> Hi, * **"Access to google maps(https://www.google.com/maps) should 
> prevent any authentication need"*
> 
> I could understand that all users should be able to access the google 
> maps link without any authentication. For this you could add the site 
> acl before the authentication part in the squid conf. So that users 
> will not prompt for the authentication when the user try to access the 
> google map site. But when they try to access any other site 
> authentication will be prompted.

This cannot be done.

You can authenticate the user setting up a CONNECT tunnel, OR you can bypass authentication for them.

That authentication choice applies equally all requests sent over the tunnel. Whether they are for maps or for any other Google service. And it must be made *before* the tunnel is setup. Thus *before* the URL inside the tunnel becomes known.


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXIKwAAoJELJo5wb/XPRjMoMH/2yCMjxisbxWBAYnp+96908O
W46taJk7kqwUbtv76aOsSEcPpc3cBl4E+nFv7cQofRqgobcR2wTsJtgRupjuIgSb
SYPQKqJolbs/7wF5nhxbggewSfRU7B21aULKStkXV7BUWNlUIaV1vUsv+J1JV8OP
U/HkcVeXny1khCjF9nEKeXNUpOioUQ0LpPboAOrLnfZZfY098NkGubJF04/stUCQ
QXIErZ8cwX7yJ1x+yIwlVw4KVbtGaBJ8dd8PH4q3DknzAVxfJ0LZgYJC3nKTQMZ3
vUTMV33Rf94Y9x/yNrs6AVWcR3rLl08GkpFv3owqItkHa1hi7yFCuEg5e3bOFFA=
=AMi0
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Nov  7 10:52:22 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 07 Nov 2014 23:52:22 +1300
Subject: [squid-users] Squid ACL, SSL-BUMP and authentication questions
In-Reply-To: <005e01cffa72$48fefd40$dafcf7c0$@icshk.com>
References: <002901cffa3a$7f1eb600$7d5c2200$@icshk.com>
 <545C764E.5040007@visolve.com> <545C82B1.9040105@treenet.co.nz>
 <005e01cffa72$48fefd40$dafcf7c0$@icshk.com>
Message-ID: <545CA466.5050508@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 11:04 p.m., squid at icshk.com wrote:
> Hi Amos,
> 
> The configuration I post last time still cannot accomplish the
> tasks.

I said the task was not possible.

You are trying to decide whether to authenticate, based on details
that will not be known until authentication has finished.


I recommend you just authenticate and be done with it.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXKRlAAoJELJo5wb/XPRjElEH/1N96B09dmvqHZjxi0b/7doo
EXu8f+t11lckpmc3Y+iE8QCzFacrJe2osv/SSPH+Z4vXU351kKRUxA/fXkcWmtvt
A1eW+NVOVViS5uYXgfpR1w/8JvL/FgO4axuBbgPg3x/LQlk0eI5QMVFYjdkzBleL
x2rqHGkccdspmNm40Msok0snuAR3zmn0em0tFLjjR4peIOY/UPPuO9pt7YDOvsz/
vZCjxZ5A+sffU8ZDnKeJ1DIxXb+gk8N0W0YPEkBRYQjEuhNiF1z1J8Mu8z+GFJ+c
8ROzeI++XVOkndK3hiVCPXLFfkhgj5bRmutJb0kBltCiPidv/N0raxNSm2Y4Aqw=
=q7Dd
-----END PGP SIGNATURE-----


From tomtux007 at gmail.com  Fri Nov  7 13:07:36 2014
From: tomtux007 at gmail.com (Tom Tom)
Date: Fri, 7 Nov 2014 14:07:36 +0100
Subject: [squid-users] FTP-Prompt-Behaviour changed between 3.3.11 and
	>=3.3.13
Message-ID: <CACLJR+PKRCjhgYaLJNQxiJnvDP8VA7f6MBR5FfWKLRXb0_DkAA@mail.gmail.com>

Hi

Within squid 3.3.11 and 3.3.13 (and of course squid >3.3.13) changed
something concerning browser-behaviour while accessing ftp-sites:

squid 3.3.11
============
ftp://ftp.xxx.xxx -> User is prompted for username/password
(TCP_DENIED/401), when anonymous-access is not allowed


squid 3.3.13 (same config as in 3.3.11)
=======================================
ftp://ftp.xxx.xxx -> User is *not* prompted for username/password
(also TCP_DENIED/401), when anonymous-access is not allowed.

Tested with IE and FF.

Any hints for this behaviour? Is there a way, to enforce the ftp-prompt?
Kind regards,
Tom


From eliezer at ngtech.co.il  Fri Nov  7 13:31:55 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 07 Nov 2014 15:31:55 +0200
Subject: [squid-users] true sizeof squid cache
In-Reply-To: <149841cf22e.ric.castellani@alice.it>
References: <149841cf22e.ric.castellani@alice.it>
Message-ID: <545CC9CB.5040107@ngtech.co.il>

Hey Riccardo,

2000-3000 computers is partially relevant.
It means that the proxy machine will probably need some more ram then a
tiny machine and it's a CACHE and not filtering machine.
Since it's 2-3k clients you will use SMP to make sure that the load will
be distributed on top of couple CPUs.
And since it's more then one CPU you will want to use rock cache_dir.
UFS cache_dir doesn't support SMP yet and there is a 3.5 beta version of
"large rock".
It will limit the cache size but remember that it's cache and not a storage.

I do not remember the numbers by heart but there is a limit on how much
a machine can handle(Amos might remember it).
The main bottle neck is the DISK.
The spinning disk can be used with maximum 300 IOPS (on a very fast one).

When implementing a proxy you will need to first make a small assessment
on ram only and get some load statistics.
It has been my suggestion for a very long time now and some do not like it.
Squid as a software with no cache can take one load and with ram cache
can take another.
There is a complexity for it and it depends on the nature of the
environment.

For an enterprise class network like yours it won't take too long to
collect the logs and statistics to make sure that the proxy match your
needs.

All The Bests,
Eliezer

On 11/06/2014 09:58 AM, Riccardo Castellani wrote:
> I'm installing new machine as Squid server and I need to understand what 
> criteria to estimate the 'cache size', I'm not speaking about extra space for 
> swap/temporary files or fragmentation but I'm saying about the cache of size 
> that is 3rd cache_dir argument.
> Server will receive requests from about 2000-
> 3000 computers.
> I'd like interesting to find documents where shows suggestions 
> to assign right size.



From squid3 at treenet.co.nz  Fri Nov  7 17:20:23 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 08 Nov 2014 06:20:23 +1300
Subject: [squid-users] FTP-Prompt-Behaviour changed between 3.3.11 and
 >=3.3.13
In-Reply-To: <CACLJR+PKRCjhgYaLJNQxiJnvDP8VA7f6MBR5FfWKLRXb0_DkAA@mail.gmail.com>
References: <CACLJR+PKRCjhgYaLJNQxiJnvDP8VA7f6MBR5FfWKLRXb0_DkAA@mail.gmail.com>
Message-ID: <545CFF57.4040306@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 8/11/2014 2:07 a.m., Tom Tom wrote:
> Hi
> 
> Within squid 3.3.11 and 3.3.13 (and of course squid >3.3.13)
> changed something concerning browser-behaviour while accessing
> ftp-sites:
> 
> squid 3.3.11 ============ ftp://ftp.xxx.xxx -> User is prompted for
> username/password (TCP_DENIED/401), when anonymous-access is not
> allowed
> 
> 
> squid 3.3.13 (same config as in 3.3.11) 
> ======================================= ftp://ftp.xxx.xxx -> User
> is *not* prompted for username/password (also TCP_DENIED/401), when
> anonymous-access is not allowed.
> 
> Tested with IE and FF.
> 
> Any hints for this behaviour? Is there a way, to enforce the
> ftp-prompt?

Take a look at the HTTP response headers Squid is sending back to the
browser in those 401.

And see if there is a followup request with credentials. It may just
be the browser is doing SSO with the authentication now.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXP9WAAoJELJo5wb/XPRjbXQH/21x8hke4Ot872IlbFMIlVru
NtLME6wnOB3rk+u/2e1gSiE7cdBWp6iGOIi1LWARncL84qmOb6AmsZYBNx3zpLXh
4znWM8KTSUIHWDEkbV/wJVR5XYBSbGR2onyjISc1DqA4QHkXR/+GO+ZIGhkJ/Vhf
QipOJQQQJC/9ByvtnPSXJBa5QO+h8RI7dPgkTuuImQ/vQV7X4BKFhMkoI2p3QoIV
zYXYyi9dUTV7ohdN+MxkIOSYMxgRUpED4b1bcM/Jxy4gNfwjIc2QAub1P0TNvqZ4
GCCZD4tPnAgwbb0FiEI6At5Ya/6Y5kF9zhc7OT420QFqzOUgU/i4Rn2ll8HGp3Y=
=x/fE
-----END PGP SIGNATURE-----


From nam.nh at nd24.net  Sat Nov  8 02:57:51 2014
From: nam.nh at nd24.net (Neddy, NH. Nam)
Date: Sat, 8 Nov 2014 09:57:51 +0700
Subject: [squid-users] www.quid-cache.org is down.
Message-ID: <CAA44a76V7eGaG4agAYfCFw_jgn=upbprRj-Z0svwoJbd-_5C9g@mail.gmail.com>

Hi,
I was going to download new tar ball of 3.5 beta but the whole website
is down now.
Does anybody know any mirror?

Thanks.
~Nedd


From squid3 at treenet.co.nz  Sat Nov  8 03:34:51 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 08 Nov 2014 16:34:51 +1300
Subject: [squid-users] www.quid-cache.org is down.
In-Reply-To: <CAA44a76V7eGaG4agAYfCFw_jgn=upbprRj-Z0svwoJbd-_5C9g@mail.gmail.com>
References: <CAA44a76V7eGaG4agAYfCFw_jgn=upbprRj-Z0svwoJbd-_5C9g@mail.gmail.com>
Message-ID: <545D8F5B.2090005@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 8/11/2014 3:57 p.m., Neddy, NH. Nam wrote:
> Hi, I was going to download new tar ball of 3.5 beta but the whole
> website is down now. Does anybody know any mirror?

These two are working:
 http://www.eu.squid-cache.org/
 http://west.squid-cache.org/

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXY9bAAoJELJo5wb/XPRjqZUH/1Zv5VC6clEKFKomhWBIRv5L
4VSFdwAubcJ95HiMaW42QBA+yMpMRzYk83WYmM1r6xKNq8KPQCqObokFe93HvEKO
MWqfnj2Rb9TH4FJ6dkN7vObOp7OH9tNP8I25utT+k6jMsq8HdPNkXg4sZhBStL3m
X86CQw5iMSl9N2+uV0Ck3Ozo4v8Rw5DbZF0lsU8I4jYeaFQCfFgt5pNLOBGR4rfO
GunYWcuIHQBywYAwhsTnXWh21zLiNbypStPVPmaC2U7efTJYeexPDYsWpTHYwd6T
hPMjF7VneHSvAUt/BLon2kuewsT/v1L0ottY8/1TpnQeDnPCpNU475zGnk4dHt0=
=d2ij
-----END PGP SIGNATURE-----


From nam.nh at nd24.net  Sat Nov  8 06:32:46 2014
From: nam.nh at nd24.net (Neddy, NH. Nam)
Date: Sat, 8 Nov 2014 13:32:46 +0700
Subject: [squid-users] www.quid-cache.org is down.
In-Reply-To: <545D8F5B.2090005@treenet.co.nz>
References: <CAA44a76V7eGaG4agAYfCFw_jgn=upbprRj-Z0svwoJbd-_5C9g@mail.gmail.com>
 <545D8F5B.2090005@treenet.co.nz>
Message-ID: <CAA44a76TNogY_=bXyARO-Lw76qZZAugY1+CCGRkpk75cri_4gg@mail.gmail.com>

I downloaded. Thanks.



On Sat, Nov 8, 2014 at 10:34 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 8/11/2014 3:57 p.m., Neddy, NH. Nam wrote:
>> Hi, I was going to download new tar ball of 3.5 beta but the whole
>> website is down now. Does anybody know any mirror?
>
> These two are working:
>  http://www.eu.squid-cache.org/
>  http://west.squid-cache.org/
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUXY9bAAoJELJo5wb/XPRjqZUH/1Zv5VC6clEKFKomhWBIRv5L
> 4VSFdwAubcJ95HiMaW42QBA+yMpMRzYk83WYmM1r6xKNq8KPQCqObokFe93HvEKO
> MWqfnj2Rb9TH4FJ6dkN7vObOp7OH9tNP8I25utT+k6jMsq8HdPNkXg4sZhBStL3m
> X86CQw5iMSl9N2+uV0Ck3Ozo4v8Rw5DbZF0lsU8I4jYeaFQCfFgt5pNLOBGR4rfO
> GunYWcuIHQBywYAwhsTnXWh21zLiNbypStPVPmaC2U7efTJYeexPDYsWpTHYwd6T
> hPMjF7VneHSvAUt/BLon2kuewsT/v1L0ottY8/1TpnQeDnPCpNU475zGnk4dHt0=
> =d2ij
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ric.castellani at alice.it  Sat Nov  8 08:05:15 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Sat, 8 Nov 2014 09:05:15 +0100
Subject: [squid-users] TCP_DENIED/411
In-Reply-To: <545C9290.7010000@treenet.co.nz>
References: <149898dab3d.ric.castellani@alice.it>
 <545C9290.7010000@treenet.co.nz>
Message-ID: <9C700C21-8E96-4A83-B7DD-BCD380A257DD@alice.it>

Squid (we are using 2.7 version) checks inside http request to verify message is compliant to rfc but I ask myself if there is way to stop this check for specific site/client, al least temporarily? to exclude firewall problems too.

On 07 Nov 2014, at 10:36, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> On 7/11/2014 10:20 p.m., Riccardo Castellani wrote:
>> Can I bypass this "TCP_DENIE/D411" error in Squid? I read this
>> error is about "Length Required" field but we can permit Squid can
>> handle this request POST ?
> 
> If you send a message without saying how big its attached object is.
> When is the server supposed to know that it has finished and the next
> one starting?
> 
> Amos
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
> 
> iQEcBAEBAgAGBQJUXJKQAAoJELJo5wb/XPRjw3QIAIsuzGjrIserp22emYlYow0x
> tsI7fnjDWi+BMa15gSDL1ZA8jujngErSkpfLRsniTxmVC21G8BTyNfPvYbNHBB+v
> iQMqebkZMLiP6cUc+PNxPMBrdmaxbWfc276LYhvUr7AI83EWzNNJKckFHw5D98zZ
> 37KyYuwQlN0CYjgg7YzxtFeg9XWSvaiMTdenXXkfS6Yb/7sSDr2VKs1ue/PBwKu/
> juDKV5DunmaG74pY5hM+ijS9RhMDbuaMYcrgrzpgZ1mgq13qkG84z4OD7YEAUJkM
> 1Y1DlmpJuCu1SlWX87CtDKuLWZwoL2s6ciJCuVNfHDwOZowiC0RVJd5acUgX4vo=
> =WzpB
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Sat Nov  8 20:18:16 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sat, 8 Nov 2014 12:18:16 -0800
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does not
	support the forwarding method specified, only GRE supported
Message-ID: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>

Hi ,

Im trying to implemnte wccp/tproxy between squid & cisco

I have :

wccp2HandleUdp: fatal error - A WCCP router does not support the forwarding
method specified, only GRE supported

 

but they are in same subnety and I don't need GRE tunnel ??

 

how can I fix this issue ?

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141108/db030d82/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov  8 11:38:40 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 09 Nov 2014 00:38:40 +1300
Subject: [squid-users] TCP_DENIED/411
In-Reply-To: <9C700C21-8E96-4A83-B7DD-BCD380A257DD@alice.it>
References: <149898dab3d.ric.castellani@alice.it>
 <545C9290.7010000@treenet.co.nz>
 <9C700C21-8E96-4A83-B7DD-BCD380A257DD@alice.it>
Message-ID: <545E00C0.3070000@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 8/11/2014 9:05 p.m., Riccardo Castellani wrote:
> Squid (we are using 2.7 version) checks inside http request to
> verify message is compliant to rfc but I ask myself if there is way
> to stop this check for specific site/client, al least temporarily?
> to exclude firewall problems too.
> 

Don't, just don't. Seriously.

The proxy gets screwed over:
https://www.owasp.org/index.php/Improper_Data_Validation

Then the origin server risks getting screwed over:
https://www.owasp.org/index.php/Cross-User_Defacement
https://www.owasp.org/index.php/Improper_Data_Validation

Being a POST the application itself riks getting screwed over with
infinite-length input:
https://www.owasp.org/index.php/Improper_Data_Validation
https://www.owasp.org/index.php/Process_Control
https://www.owasp.org/index.php/Unsafe_Reflection

And then side effects can echo right back out to the proxy to trigger
further rounds of nastiness at random times in the future:
https://www.owasp.org/index.php/HTTP_Response_Splitting
https://www.owasp.org/index.php/Cross-User_Defacement
https://www.owasp.org/index.php/Cache_Poisoning


The 411 respone is telling you that the client sending the proxy a
request message is broken. Many of the above attack side effects could
be happening in other software already as a result of this client
Squid caught out. It really, really needs to be fixed ASAP.


Now, there is a small posibility that the client is using HTTP/1.1
Transfer-Encoding Squid-2.7 does not understand. The first fix for
that is to upgrade to a HTTP/1.1 compliant Squid (which 2.7 is *not*).

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXgDAAAoJELJo5wb/XPRjNZ0IANsinW8QFF8ssHA9SeepEBf3
4T/219SAC7GvpTJsBkVC3pQiMxNvngwC6gS3ssTpzcFjWJUi0LI25BAvV7KjuyHk
rpdQN0U2jAblAFthzFtX9xZHbkBF6pwbMNTLH+zB0imWMnZ8TdGpvjYU4onrh/DD
pYxgZOqF8ThRIqaB5kjowCC+VO1wmAOa2TsUfTGDRks29wK8yAva2bmhpQrFOEFN
En1iXuxcCSAhPkBMNM6a4a+h+zgPJkhKL4c0IXJ9I6BnAuJ0VxD8PA6eJTiTcIkK
V2Lzp2acOLINoMw2HpYiKfn0+HuWRLNedOST4rFqP0YEENkYIqbCgQ/+4fTIZZU=
=+k8q
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sat Nov  8 11:41:46 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 09 Nov 2014 00:41:46 +1300
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does
 not support the forwarding method specified, only GRE supported
In-Reply-To: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
References: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
Message-ID: <545E017A.5050302@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 9/11/2014 9:18 a.m., Ahmed Allzaeem wrote:
> Hi ,
> 
> Im trying to implemnte wccp/tproxy between squid & cisco
> 
> I have :
> 
> wccp2HandleUdp: fatal error - A WCCP router does not support the
> forwarding method specified, only GRE supported
> 
> 
> 
> but they are in same subnety and I don't need GRE tunnel ??

Apparently you do need GRE tunnel. For reason that your router does
not support L2 forwarding method for WCCP.


> 
> how can I fix this issue ?

Setup the GRE tunnel between the machines and configure that in your
wccp2_forwarding_method.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXgF6AAoJELJo5wb/XPRjYokH/j8gHje9o7ZE757oq2ob2o3D
OtrcXkuxt/8EtBbCWmc8T+PkV/W6Y2D2pXPjvPoihfAlI/sKCyBFzFZjks5c0rYf
DljSkATv81Vswd+z+TYmCNyqUGAagzn7k5GclgqRkKXKIsSJwVc/rIopkMjfzwNv
iCmR9qCgVrexeF9vUZkjN0KJ2bNvq5wblew/zgNXJkaPjqTqhdiJazqlxmm/k4Fl
06LEvacAP+dyAdyoM6b2gsvGJs/FyFk/euHKOthIybT6nf2+5DROiIQryUD6Jui2
H3sZTkMb5JY06F+uJcbPF71wQB1fgV6hipJYlnCR5x/ZooTkF01eRNy3fhI7h88=
=5UGT
-----END PGP SIGNATURE-----


From david at articatech.com  Sat Nov  8 16:06:54 2014
From: david at articatech.com (David Touzeau)
Date: Sat, 08 Nov 2014 17:06:54 +0100
Subject: [squid-users] Squid 3.5: Delay parameters bungled what changes ?
Message-ID: <545E3F9E.1040001@articatech.com>

Hi
Have created a simple delay pool

delay_pools 1
delay_class 1 1
delay_parameters 1 64000/64000
#Rules Access definitions
delay_access 1 zall
delay_initial_bucket_level 50

Here are the logs
What is missing ?

5:12.400| MemBlob.cc(57) MemBlob: constructed, this=0x136cf70 id=blob65 
reserveSize=6
2014/11/08 10:35:12.400| MemBlob.cc(102) memAlloc: blob65 memAlloc: 
requested=6, received=40
2014/11/08 10:35:12.400| MemBlob.cc(83) ~MemBlob: destructed, 
this=0x136cd00 id=blob64 capacity=40 size=6
2014/11/08 10:35:12.400| SBuf.cc(889) reAlloc: new store capacity: 40
2014/11/08 10:35:12.400| cache_cf.cc(518) parseOneConfigFile: 
Processing: delay_class 1 1
2014/11/08 10:35:12.400| SBuf.cc(168) rawSpace: reserving 1 for SBuf5
2014/11/08 10:35:12.400| SBuf.cc(910) cow: new size:6
2014/11/08 10:35:12.400| SBuf.cc(880) reAlloc: new size: 6
2014/11/08 10:35:12.400| MemBlob.cc(57) MemBlob: constructed, 
this=0x136cd00 id=blob66 reserveSize=6
2014/11/08 10:35:12.400| MemBlob.cc(102) memAlloc: blob66 memAlloc: 
requested=6, received=40
2014/11/08 10:35:12.400| MemBlob.cc(83) ~MemBlob: destructed, 
this=0x136cf70 id=blob65 capacity=40 size=6
2014/11/08 10:35:12.400| SBuf.cc(889) reAlloc: new store capacity: 40
2014/11/08 10:35:12.400| cache_cf.cc(518) parseOneConfigFile: 
Processing: delay_parameters 1 64000/64000
2014/11/08 10:35:12.400| tools.cc(543) leave_suid: leave_suid: PID 27503 
called
2014/11/08 10:35:12.400| tools.cc(565) leave_suid: leave_suid: PID 27503 
giving up root, becoming 'squid'
FATAL: Bungled /etc/squid3/squid.conf line 68: delay_parameters 1 
64000/64000
Squid Cache (Version 3.5.0.2-20141031-r13657): Terminated abnormally.
CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
Maximum Resident Size: 32208 KB
Page faults with physical i/o: 4
2014/11/08 10:35:12.403| SBuf.cc(124) ~SBuf: SBuf39 destructed

best regards




From vdoctor at neuf.fr  Sun Nov  9 08:42:03 2014
From: vdoctor at neuf.fr (Stakres)
Date: Sun, 9 Nov 2014 00:42:03 -0800 (PST)
Subject: [squid-users] Squid 3.5: Delay parameters bungled what changes ?
In-Reply-To: <545E3F9E.1040001@articatech.com>
References: <545E3F9E.1040001@articatech.com>
Message-ID: <1415522523118-4668260.post@n4.nabble.com>

Hi David, All,

Same results from my tests, Squid 3.5.0.2 and same nice error on the
"delay_parameters" option.

It would be nice to have a quick fix on this 

BYe Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-Delay-parameters-bungled-what-changes-tp4668259p4668260.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Sun Nov  9 23:19:45 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 9 Nov 2014 15:19:45 -0800
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does
	not support the forwarding method specified, only GRE supported
In-Reply-To: <545E017A.5050302@treenet.co.nz>
References: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
 <545E017A.5050302@treenet.co.nz>
Message-ID: <00d101cffc73$a7cecbe0$f76c63a0$@netstream.ps>

Thanks Amos , 
Im trying to implement gre , AND  I followed 
http://wiki.squid-cache.org/ConfigExamples/UbuntuTproxy4Wccp2

But still wondering , 

Don?t we need to create GRE config for cisco ??

I see only gre config for squid only ??!!

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, November 8, 2014 3:42 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does not support the forwarding method specified, only GRE supported

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 9/11/2014 9:18 a.m., Ahmed Allzaeem wrote:
> Hi ,
> 
> Im trying to implemnte wccp/tproxy between squid & cisco
> 
> I have :
> 
> wccp2HandleUdp: fatal error - A WCCP router does not support the 
> forwarding method specified, only GRE supported
> 
> 
> 
> but they are in same subnety and I don't need GRE tunnel ??

Apparently you do need GRE tunnel. For reason that your router does not support L2 forwarding method for WCCP.


> 
> how can I fix this issue ?

Setup the GRE tunnel between the machines and configure that in your wccp2_forwarding_method.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUXgF6AAoJELJo5wb/XPRjYokH/j8gHje9o7ZE757oq2ob2o3D
OtrcXkuxt/8EtBbCWmc8T+PkV/W6Y2D2pXPjvPoihfAlI/sKCyBFzFZjks5c0rYf
DljSkATv81Vswd+z+TYmCNyqUGAagzn7k5GclgqRkKXKIsSJwVc/rIopkMjfzwNv
iCmR9qCgVrexeF9vUZkjN0KJ2bNvq5wblew/zgNXJkaPjqTqhdiJazqlxmm/k4Fl
06LEvacAP+dyAdyoM6b2gsvGJs/FyFk/euHKOthIybT6nf2+5DROiIQryUD6Jui2
H3sZTkMb5JY06F+uJcbPF71wQB1fgV6hipJYlnCR5x/ZooTkF01eRNy3fhI7h88=
=5UGT
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ajm.martinez at gmail.com  Sun Nov  9 14:35:09 2014
From: ajm.martinez at gmail.com (Alejandro Martinez)
Date: Sun, 9 Nov 2014 12:35:09 -0200
Subject: [squid-users] Proposal for deny_info
Message-ID: <CAFEXmxjxCRX=Cdgc+p1VVavCm9MxMCDrWYtwaRFKAKXkncAhoA@mail.gmail.com>

Hi all,

I'm trying to setup deny_info for denied sites using CONNECT method.
This is something that doesn't work 100% depending on browser, etc.

Could be possible to change the 30X:http://x.x.x.x/deny.html to something
based in DNS replies ?

Squid uses its own directive "dns_nameserver" to configure which name
server is going to use.

I was thinking on something like this

dns_nameserver_deny 172.16.1.1  <- IP of dnsmasq server
acl deniedsites dstdomain "/list/of/denied/domains" (.youtube.com , .
facebook.com
)
http_access deny deniedsites

but instead of

deny_info deniedsites 307:http://172.16.1.1/deny.html

something like this

deny_dns_info deniedsites 172.16.1.1

and 172.16.1.1 is going to resolv:

172.16.1.1 youtube.com facebook.com, etc

It is possible ?

based on destination domain, the IP to return, so if I ask for facebook.com
I'll get 172.16.1.1 and the certificate warning appears, but the error
(Denied Site) too.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141109/21d70915/attachment.htm>

From gkinkie at gmail.com  Sun Nov  9 16:04:55 2014
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 9 Nov 2014 17:04:55 +0100
Subject: [squid-users] [squid-dev] When i redirected to more traffic to
 squid box for testing goal via web-polygraph . find error info "OS probably
 ran out of ephemeral ports at 192.168.2.1:0"
In-Reply-To: <545B250F.6030901@yahoo.com>
References: <545B250F.6030901@yahoo.com>
Message-ID: <CA+Y8hcNMY68LDMWR3NFmh-7TsA3gvFTNzNjtEJTmwORc_taUgg@mail.gmail.com>

You want to ask this question to the polygraph authors; squid users
and developers are not really the right place to ask.
I don't know about the details, but that error message probably means
that your bots are overwhelming the TCP stack, which can't free
ephemeral ports quickly enough due to TCP timeouts.

On Thu, Nov 6, 2014 at 8:36 AM, johnzeng <johnzeng2013 at yahoo.com> wrote:
>
> Hello :
>
> i meet a problem , When i redirected to more traffic to squid box for
> testing goal via web-polygraph .
>
> squidbox ip is 192.168.2.2 web-polygraph_box ip is 192.168.2.3
>
> /polygraph-client --config
> /accerater/testtool/share/polygraph/workloads/simple.pg --proxy
> 192.168.2.2:80 --verb_lvl 10
>
> ./polygraph-server --config
> /accerater/testtool/share/polygraph/workloads/simple.pg --verb_lvl 10
>
> When testing traffic is 1500request/sec , i found more error info ,
>
> but my os setting is net.ipv4.ip_local_port_range = 1024 65535
> open files (-n) 65536
> max user processes (-u) 10000
> /proc/sys/fs/file-max 6815744
>
>
> and i found these error info still , how will i do ? if possible , give
> me some help or advisement please .
>
> ******************************
> error info
> ******************************
>
> EphPortMgr.cc:23: error: 34920/69877 (s98) Address already in use
> 005.28| OS probably ran out of ephemeral ports at 192.168.2.3:0
> 005.28| Client.cc:347: error: 34920/69878 (c63) failed to establish a
> connection
> 005.28| 192.168.2.3 failed to connect to 192.168.2.2:80
> 005.31| i-dflt 104811 0.00 -1 -1.00 3904 32336
> 005.59| PolyApp.cc:189: error: 39/75599 (c58) internal timers may be
> getting beh
>
> 005.90| EphPortMgr.cc:23: error: 64/129 (s98) Address already in use
> 005.90| OS probably ran out of ephemeral ports at 192.168.2.1:0
> 005.90| Client.cc:347: error: 64/130 (c63) failed to establish a connection
> 005.90| 192.168.2.1 failed to connect to 192.168.2.2:80
> 005.90| PolyApp.cc:189: error: 4/180 (c58) internal timers may be
> getting behind
> 005.90| record level of timing drift: 179msec; last check was 3msec ago
> 005.90| EphPortMgr.cc:23: error: 128/260 (s98) Address already in use
> 005.90| OS probably ran out of ephemeral ports at 192.168.2.1:0
> 005.90| Client.cc:347: error: 128/261 (c63) failed to establish a connection
> 005.90| 192.168.2.1 failed to connect to 192.168.2.2:80
> 005.91| PolyApp.cc:189: error: 8/460 (c58) internal timers may be
> getting behind
> 005.91| record level of timing drift: 383msec; last check was 3msec ago
>
>
>
>
>
>
>
>
>
> ***********************************
> Web-polygraph configration
> ***********************************
>
>
> Content SimpleContent = {
> size = exp(13KB); // response sizes distributed exponentially
> cachable = 80%; // 20% of content is uncachable
> };
>
> // a primitive server cleverly labeled "S101"
> // normally, you would specify more properties,
> // but we will mostly rely on defaults for now
> Server S = {
> kind = "S101";
> contents = [ SimpleContent ];
> direct_access = contents;
>
> addresses = ['192.168.2.1:9090']; // where to create these server agents
> };
>
> // a primitive robot
> Robot R = {
> kind = "R101";
> pop_model = { pop_distr = popUnif(); };
> recurrence = 55% / SimpleContent.cachable; // adjusted to get 55% DHR
> req_rate = 1600/sec;
>
>
> origins = S.addresses; // where the origin servers are
> addresses = ['192.168.2.1']; // where these robot agents will be created
> };
>
>
>
>
> **************************************
> sysctl.conf
> **************************************
>
>
> fs.file-max = 6815744
> fs.aio-max-nr = 1048576
> kernel.shmmax = 4294967295
> kernel.threads-max = 212992
> kernel.sem = 250 256000 100 1024
> net.core.rmem_max=5165824
> net.core.wmem_max=262144
> net.ipv4.tcp_rmem=5165824 5165824 5165824
> net.ipv4.tcp_wmem=262144 262144 262144
> net.core.rmem_default = 5165824
> net.core.wmem_default = 262144
> net.core.optmem_max = 25165824
> net.ipv4.ip_local_port_range = 1024 65535
> net.nf_conntrack_max = 6553600
> net.netfilter.nf_conntrack_tcp_timeout_established = 1200
>
>
> net.ipv4.tcp_tw_recycle = 1
> net.ipv4.tcp_tw_reuse = 1
> net.ipv4.tcp_fin_timeout = 10
> net.ipv4.tcp_orphan_retries = 3
> net.ipv4.tcp_retries2 = 5
> net.ipv4.tcp_keepalive_intvl = 15
> net.ipv4.tcp_syn_retries = 5
> net.ipv4.tcp_synack_retries = 2
> net.ipv4.tcp_keepalive_time = 1800
> net.core.netdev_max_backlog = 3000000
> net.ipv4.tcp_max_syn_backlog = 262144000
> net.ipv4.tcp_max_tw_buckets = 500000000
> net.core.somaxconn = 262144000
> net.ipv4.tcp_sack = 1
> net.ipv4.tcp_fack = 1
>
> net.ipv4.tcp_timestamps = 0
> net.ipv4.tcp_window_scaling = 1
> net.ipv4.tcp_syncookies = 1
> net.ipv4.tcp_no_metrics_save=1
> net.ipv4.tcp_max_orphans = 26214400
> net.ipv4.tcp_synack_retries = 2
> net.ipv4.tcp_low_latency = 1
> net.ipv4.tcp_rfc1337 = 1
>
> _______________________________________________
> squid-dev mailing list
> squid-dev at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-dev



-- 
    Francesco


From squid3 at treenet.co.nz  Sun Nov  9 17:07:26 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Nov 2014 06:07:26 +1300
Subject: [squid-users] Proposal for deny_info
In-Reply-To: <CAFEXmxjxCRX=Cdgc+p1VVavCm9MxMCDrWYtwaRFKAKXkncAhoA@mail.gmail.com>
References: <CAFEXmxjxCRX=Cdgc+p1VVavCm9MxMCDrWYtwaRFKAKXkncAhoA@mail.gmail.com>
Message-ID: <545F9F4E.1000500@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 3:35 a.m., Alejandro Martinez wrote:
> Hi all,
> 
> I'm trying to setup deny_info for denied sites using CONNECT
> method. This is something that doesn't work 100% depending on
> browser, etc.
> 
> Could be possible to change the 30X:http://x.x.x.x/deny.html to
> something based in DNS replies ?
> 
> Squid uses its own directive "dns_nameserver" to configure which
> name server is going to use.

It only has that behaviour if you restrict the list to a single NS entry.

dns_nameserver overrides and replaces all the OS /etc/resolv.conf
settings. It is meant to contain a *set* of DNS servers to select from
("no less than 2, no more than 7" is the BCP standard guideline).

> 
> I was thinking on something like this
> 
> dns_nameserver_deny 172.16.1.1  <- IP of dnsmasq server acl
> deniedsites dstdomain "/list/of/denied/domains" (.youtube.com , . 
> facebook.com ) http_access deny deniedsites
> 
> but instead of
> 
> deny_info deniedsites 307:http://172.16.1.1/deny.html
> 
> something like this
> 
> deny_dns_info deniedsites 172.16.1.1
> 
> and 172.16.1.1 is going to resolv:
> 
> 172.16.1.1 youtube.com facebook.com, etc
> 
> It is possible ?

Sounds horribly complicated and confusing. If you are willing to put
on the coding time almost anything is "possible". Whether it works or
not is a different question.

With Squid-3.2 or later you can use %o in the deny_info. That gets
filled in with the message= value received back from external ACL
helper. You should experiment with that first.

But remember what it comes down to is how the individual browser
handles non-200 responses to a CONNECT request. Simply tweaking the
Content-Location header will not affect that in any significant way
unless it is *already* acting on that header.

> 
> based on destination domain, the IP to return, so if I ask for
> facebook.com I'll get 172.16.1.1 and the certificate warning
> appears, but the error (Denied Site) too.
> 

If deny_info is used on the CONNECT there should not be a certificate
warning at all. Because TLS is never involved. Any of the HTTPS
traffic, being "inside the tunnel" at that point becomes just so many
garbage bytes dropped on the floor when the TCP connection is
necessarily closed.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUX59OAAoJELJo5wb/XPRjSRQIAJF4nE93Wyao6vxYCbXactsZ
oyAE+oiMLiWmocqSN4HnYQH8ykSf5BTYw/FlUsg/LQdhAeiM//UHIig6mN+j2eFx
SkMwTHNc5XkmR0muYP2SfltkQHH3/ZvODZH8W0M4Xv3f9bePqYLwm15N24gmX2GI
EdTeM2P/HEHzOiaWLZ7iDxB3ePcAPlPkScgzO92Jrn1lCfenxy7mxk/h0R6AHwtB
GXGcBhJPtLl/MyBlm2l2fCm6nUWrsKd80p36UMT5eqjZK8AQspZ7o7uDz82P5gnc
Za3dMwnao14LUu7U/ibmzckIn+mecEDpOcgHcktzPnnFBYGnCMUH/2/0GDKd+Sk=
=TKGa
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sun Nov  9 17:17:28 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Nov 2014 06:17:28 +1300
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does
 not support the forwarding method specified, only GRE supported
In-Reply-To: <00d101cffc73$a7cecbe0$f76c63a0$@netstream.ps>
References: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
 <545E017A.5050302@treenet.co.nz>
 <00d101cffc73$a7cecbe0$f76c63a0$@netstream.ps>
Message-ID: <545FA1A8.1080309@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 12:19 p.m., Ahmed Allzaeem wrote:
> Thanks Amos , Im trying to implement gre , AND  I followed 
> http://wiki.squid-cache.org/ConfigExamples/UbuntuTproxy4Wccp2
> 
> But still wondering ,
> 
> Don?t we need to create GRE config for cisco ??

I've not seen any of the config examples needing to specify it
specifically at the Cisco end. I suspect it is implicit in the
enabling of "wccp" modules.

GRE vs L2 use also depends on the device hardware type (router or
switch, PIX or ASA) and how recent the IOS version is.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUX6GoAAoJELJo5wb/XPRjLbcIAK6UZYYVVUw3XZLANqJ/x4Ph
XppIXXr2nMKYyAhtSqN8+uPdHu9+pdvAVeyvnx+irp44wTDtj6+ePB7R0PMmhBTC
JTw2/egFeYZmdEI0mCsq4Tmsx4NVbQmHRYok5SYCGdkJvZGbneNgEABHMt6LQxaZ
P0B7kbLLDqMs64BNxtDBBznQCRUCZjRsnBBd8sBnMK0YX0nh6d/oAfIg1GGszW7f
bH4Mth6rpDcFzRiWnbBmAYLzPF5buhvZYw7ivzIrhQHqX0ZmBTTkCMeaEuMCr/rh
budLJRl3MwsYll/w6zoywPebGQImgzhj99ZtyKeqQQNKDPGyEOIGcmAt8HsvVNo=
=5zVY
-----END PGP SIGNATURE-----


From diego at woitasen.com.ar  Sun Nov  9 22:53:38 2014
From: diego at woitasen.com.ar (Diego Woitasen)
Date: Sun, 9 Nov 2014 19:53:38 -0300
Subject: [squid-users] 3.3.x -> 3.4.x: huge performance regression
In-Reply-To: <544BD4DC.7070900@ngtech.co.il>
References: <5447486B.3010205@norma.perm.ru> <5448C7FB.1080104@ngtech.co.il>
 <1FCF9DA5B29068478ECF15896F19F0840138A509A8@Y011008.bk.fin.local>
 <544A4390.3090903@norma.perm.ru> <544A4AD5.6040804@ngtech.co.il>
 <CAFKaCCSS_Mo0MWfxtM_Zp3fs-8iVrZcE7-EaO9x4aC+iVjYn8Q@mail.gmail.com>
 <CAA7UhcmzjjT=nyu-Ep6CUn09-GAE=f2DjEJxS1KQVk3K7zAeqw@mail.gmail.com>
 <544BD4DC.7070900@ngtech.co.il>
Message-ID: <CAA7UhckbzXbgwT7kVwjm2R=-aeCmX-YwpVxNZZDodWtaorF9BA@mail.gmail.com>

Hi,
  I have more information. The testing environment has a few users. We
switched to basic authencation and it's been working for a week without any
issues. A couple of days ago we enabled NTLM again and the issue appeared
again.

I 'm on mobile now. I'll add more info in the bug report.

Regards,
  Diego
On Oct 25, 2014 1:51 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hey Diego,
>
> Can you take a look at the bug report and help pinpoint the issue please?
> http://bugs.squid-cache.org/show_bug.cgi?id=3997
>
> I am pretty sure it's unique to auth only but I want to verify that
> external_acl helpers do not affect this issue.
>
> Also if you can share the testing environment details or we can get
> some help with testing from your IT testing team?
>
> Thanks,
> Eliezer
>
>
> On 10/25/2014 06:17 PM, Diego Woitasen wrote:
> > Same problem here. New users, only a few users from IT testing it
> > and CPU usage is really high from time to time.
> >
> > Switched to basic auth for a few days. Looks like everybody is
> > having issues with NTLM/SPNEGO.
> >
> > Keep in touch and we'll fix it :)
> >
> > Regards, Diego
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUS9TcAAoJENxnfXtQ8ZQU4EEIAIKeKjvzrPSlj8UlGUaWHhT+
> 64ontOl7wiYdyo1rjU1MWZxg+6erlVVYg5p46Ki/bznes/on70peU6UndzInLA0K
> JACZEq0P6eQBDQjP0eVfRbSVo4QeMA/+1prDZY8GAwyI3ugSWndeAT2dqVQFkVdt
> x3OxXc5ch4nfV9ZF4HPAMKRp6mey4LJjixTToIw9CsoDpcAE7UAWuXi//JOHMqmp
> b6ZONdhOBCJajWebhEHbUwNbciZVeCgGWXJGuyVA8kp0ChkFTtBnC7BpNjWRC3hL
> rH5cJcfJXyFLoG67qZaPTueakk5aII8Aj2DkPauK2ofQAOjlLL6gh45GiO1oeJ0=
> =sV5l
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141109/0fc31ee5/attachment.htm>

From dm at belkam.com  Mon Nov 10 06:46:09 2014
From: dm at belkam.com (Dmitry Melekhov)
Date: Mon, 10 Nov 2014 10:46:09 +0400
Subject: [squid-users] 3.4.9 and tls 1.2
In-Reply-To: <545C90ED.9080200@belkam.com>
References: <545C90ED.9080200@belkam.com>
Message-ID: <54605F31.4060507@belkam.com>

07.11.2014 13:29, Dmitry Melekhov ?????:
> Hello!
>
> I just found that I have problems with using google maps , i.e. 
> https://maps.google.com with firefox 33.0 ovr squid 3.4.9-
> it works extremely slow, sometimes street view can't be loaded and 
> showed black screen.
> This is wit default  security.tls.version.max=3, i.e. tls 1.2 , but if 
> I change this to 2, i.e. tls 1.1 all works fine.
> Direct, i.e. without squid, connection always works OK, so, I guess 
> this is squid problem.
> Any ideas how to solve this?
>
> Thank you!
>
>
>
btw, I see in squid log that there is no reply from server
1415601857.368  60019 192.168.22.229 TCP_MISS/503 0 CONNECT 
maps.google.com:443 dm HIER_NONE/- -

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/794ecd2d/attachment.htm>

From ric.castellani at alice.it  Mon Nov 10 07:53:14 2014
From: ric.castellani at alice.it (Riccardo Castellani)
Date: Mon, 10 Nov 2014 08:53:14 +0100 (CET)
Subject: [squid-users] R: Re:  TCP_DENIED/411
Message-ID: <14998b14365.ric.castellani@alice.it>

I think the request is http/1.1 because I captured it and it shows in the 
'Hypertext Transfer Protocol' in the POST section, the field 'Request version' 
is HTTP/1.1
I understand Squid 2.7 is not able to understand http/1.1, but I 
ask myself if 'content-length' field was missing in the http/1.1 request and 
Squid was compliant to http/1.1( squid 3.x version ) , what Squid would return 
'DENIED/411' again?



>----Messaggio originale----
>Da: squid3 at treenet.co.nz

>Data: 8-nov-2014 12.38
>A: 
>Ogg: Re: [squid-users] TCP_DENIED/411
>
>-----
BEGIN PGP SIGNED MESSAGE-----
>Hash: SHA1
>
>On 8/11/2014 9:05 p.m., Riccardo 
Castellani wrote:
>> Squid (we are using 2.7 version) checks inside http 
request to
>> verify message is compliant to rfc but I ask myself if there is 
way
>> to stop this check for specific site/client, al least temporarily?
>> to 
exclude firewall problems too.
>> 
>
>Don't, just don't. Seriously.
>
>The 
proxy gets screwed over:
>https://www.owasp.org/index.php/Improper_Data_Validation>
>Then the origin 
server risks getting screwed over:
>https://www.owasp.org/index.php/Cross-User_Defacement>https://www.owasp.org/index.php/Improper_Data_Validation
>
>Being a POST the application itself riks getting screwed over with

>infinite-length input:
>https://www.owasp.org/index.php/Improper_Data_Validation>https://www.owasp.org/index.php/Process_Control
>https://www.owasp.org/index.php/Unsafe_Reflection>
>And then side effects can 
echo right back out to the proxy to trigger
>further rounds of nastiness at 
random times in the future:
>https://www.owasp.org/index.php/HTTP_Response_Splitting>https://www.owasp.org/index.php/Cross-User_Defacement
>https://www.owasp.org/index.php/Cache_Poisoning>
>
>The 411 respone is 
telling you that the client sending the proxy a
>request message is broken. 
Many of the above attack side effects could
>be happening in other software 
already as a result of this client
>Squid caught out. It really, really needs 
to be fixed ASAP.
>
>
>Now, there is a small posibility that the client is 
using HTTP/1.1
>Transfer-Encoding Squid-2.7 does not understand. The first fix 
for
>that is to upgrade to a HTTP/1.1 compliant Squid (which 2.7 is *not*).
>

>Amos
>-----BEGIN PGP SIGNATURE-----
>Version: GnuPG v2.0.22 (MingW32)
>

>iQEcBAEBAgAGBQJUXgDAAAoJELJo5wb/XPRjNZ0IANsinW8QFF8ssHA9SeepEBf3

>4T/219SAC7GvpTJsBkVC3pQiMxNvngwC6gS3ssTpzcFjWJUi0LI25BAvV7KjuyHk

>rpdQN0U2jAblAFthzFtX9xZHbkBF6pwbMNTLH+zB0imWMnZ8TdGpvjYU4onrh/DD

>pYxgZOqF8ThRIqaB5kjowCC+VO1wmAOa2TsUfTGDRks29wK8yAva2bmhpQrFOEFN

>En1iXuxcCSAhPkBMNM6a4a+h+zgPJkhKL4c0IXJ9I6BnAuJ0VxD8PA6eJTiTcIkK

>V2Lzp2acOLINoMw2HpYiKfn0+HuWRLNedOST4rFqP0YEENkYIqbCgQ/+4fTIZZU=
>=+k8q
>-----
END PGP SIGNATURE-----
>_______________________________________________
>squid-
users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users>




From eliezer at ngtech.co.il  Mon Nov 10 07:58:12 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 10 Nov 2014 09:58:12 +0200
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does
	not support the forwarding method specified, only GRE supported
In-Reply-To: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
References: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
Message-ID: <54607014.20107@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Two things,

- - What cisco device? what IOS?
- - What docs in cisco have you tried to use?

Eliezer

On 11/08/2014 10:18 PM, Ahmed Allzaeem wrote:
> Hi ,
> 
> Im trying to implemnte wccp/tproxy between squid & cisco
> 
> I have :
> 
> wccp2HandleUdp: fatal error - A WCCP router does not support the
> forwarding method specified, only GRE supported
> 
> 
> 
> but they are in same subnety and I don't need GRE tunnel ??
> 
> 
> 
> how can I fix this issue ?
> 
> 
> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYHAUAAoJENxnfXtQ8ZQU+QIH/AkOirxkWe+1h1wWGpherr3N
fyIl7Va/6qpRPACSnkRDp3wo5tXq41II6OVWvJchX7HUFS3zo6edtriqGbHwWDAv
expwVcp5/vzfa4UHcXeVmAkhCJfInlcqED9Fe4CSL2OoCwU2yIQwEa6hdsW0+Fns
AhhkT72H+TQXVBxS+WWEKgJmRe7pCXL4mXTXzzK+mWjE0/euej8fqQZNCPdKnGTp
nkCPKYjQT4J1D6I2Q73NpzeJX31v1B3o3q7L5bb9Sqb62N3m6NcXI/VggFzr/kH5
ep7mUEy/btnyLKKAJxcUA7Fm0E/5kzLAg3xf3ozfJjvIKe7w6KX1d7r+a90azc0=
=NMlB
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 10 08:40:58 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Nov 2014 21:40:58 +1300
Subject: [squid-users] R: Re:  TCP_DENIED/411
In-Reply-To: <14998b14365.ric.castellani@alice.it>
References: <14998b14365.ric.castellani@alice.it>
Message-ID: <54607A1A.1050609@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 8:53 p.m., Riccardo Castellani wrote:
> I think the request is http/1.1 because I captured it and it shows
> in the 'Hypertext Transfer Protocol' in the POST section, the field
> 'Request version' is HTTP/1.1 I understand Squid 2.7 is not able to
> understand http/1.1, but I ask myself if 'content-length' field was
> missing in the http/1.1 request and Squid was compliant to
> http/1.1( squid 3.x version ) , what Squid would return 
> 'DENIED/411' again?

Can you produce a copy of those HTTP headers to clarify what we are
discussing?

HTTP/1.1 allows the possibility of Content-Length not being present
yet the request being valid. Squid-2.7 has just enough 1.1 compliance
to perform checks like that and respond with the appropriate 411 or
accept.

HTTP/1.0 requires terminating the TCP connection in these circumstances.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYHoZAAoJELJo5wb/XPRjzsQIAK0inM0JrQVMSyfqdtZPfpmR
D4nO6+Xs888obdVh4mGciCePSwzHcFd7HCwb/RIqCit/v1ocQhdJxzXSwCiIgCud
eVNDYQUKO08LKlIMVg0zyQqBbEQlaxEv8hfa0CLnk7KNB74s4e0Lv0PDbHrOSDsm
iQ1jvCKlgcTq8owm2ITXoYWoIkze/BbrUegdMmgSqjHebQ+5Gk7by0TAlQCXC4Ct
uS1fnZHmlkzX9HcaBwvnh8IjMXuRYd09ZF5ASwi0Puo1XZpyjMFD4svsEpTA00Rb
9wb2C0O6R7h1wWoIwmsu3JfjuufPRPZ+HUyyMA0Ba+xXv3/g34zmd7UzgTzYfyA=
=ykSC
-----END PGP SIGNATURE-----


From Jason_Haar at trimble.com  Mon Nov 10 09:17:17 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 10 Nov 2014 22:17:17 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept mode?
Message-ID: <5460829D.9000104@trimble.com>

Hi there, I've googled about for this but I think most of the squid
intercept stuff refers to 3.2 and I think things have changed since then?

I have squid-3.4.9 running with sslbump, and when I configure my browser
to use it as a proxy, it bumps the certs nicely, signing "fake"
certs/etc. I then added an iptables run to redirect outbound tcp/80 onto
port 3129 (see below) and that transparently proxies all port 80 -
great. I then went through the same exercise with sslbump, but when I
put in an iptables rule to redirect outbound tcp/443 traffic onto 3127,
it doesn't bump - it acts like a TCP forwarder instead. I get a "CONNECT
ip.add.ress:443" log record - no sign of the hostname and no bumping

http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
http_port 3129 transparent
https_port 3127 transparent ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL

acl SSL_nonHTTPS_sites dstdom_regex "/etc/squid/SSL_nonHTTPS_sites.txt"
acl SSL_noIntercept_sites dstdom_regex
"/etc/squid/SSL_noIntercept_sites.txt"
ssl_bump none SSL_nonHTTPS_sites
ssl_bump none SSL_noIntercept_sites
ssl_bump server-first all

So these older search-engine pages I came across claimed this should
work with squid, but either I am missing something, or this doesn't work
in 3.4.9?

Thanks

 

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From dm at belkam.com  Mon Nov 10 09:25:06 2014
From: dm at belkam.com (Dmitry Melekhov)
Date: Mon, 10 Nov 2014 13:25:06 +0400
Subject: [squid-users] 3.4.9 and tls 1.2
In-Reply-To: <54605F31.4060507@belkam.com>
References: <545C90ED.9080200@belkam.com> <54605F31.4060507@belkam.com>
Message-ID: <54608472.50907@belkam.com>

10.11.2014 10:46, Dmitry Melekhov ?????:
> 07.11.2014 13:29, Dmitry Melekhov ?????:
>> Hello!
>>
>> I just found that I have problems with using google maps , i.e. 
>> https://maps.google.com with firefox 33.0 ovr squid 3.4.9-
>> it works extremely slow, sometimes street view can't be loaded and 
>> showed black screen.
>> This is wit default  security.tls.version.max=3, i.e. tls 1.2 , but 
>> if I change this to 2, i.e. tls 1.1 all works fine.
>> Direct, i.e. without squid, connection always works OK, so, I guess 
>> this is squid problem.
>> Any ideas how to solve this?
>>
>> Thank you!
>>
>>
>>
> btw, I see in squid log that there is no reply from server
> 1415601857.368  60019 192.168.22.229 TCP_MISS/503 0 CONNECT 
> maps.google.com:443 dm HIER_NONE/- -
>
Ooops.
Just  informed by my colleague that he experiences the problem with the 
same ISP, and, yes, switching to another links solved problem.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/05c0753b/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 10 10:02:23 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Nov 2014 23:02:23 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <5460829D.9000104@trimble.com>
References: <5460829D.9000104@trimble.com>
Message-ID: <54608D2F.5050704@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 10:17 p.m., Jason Haar wrote:
> Hi there, I've googled about for this but I think most of the
> squid intercept stuff refers to 3.2 and I think things have changed
> since then?
> 
> I have squid-3.4.9 running with sslbump, and when I configure my
> browser to use it as a proxy, it bumps the certs nicely, signing
> "fake" certs/etc. I then added an iptables run to redirect outbound
> tcp/80 onto port 3129 (see below) and that transparently proxies
> all port 80 - great. I then went through the same exercise with
> sslbump, but when I put in an iptables rule to redirect outbound
> tcp/443 traffic onto 3127, it doesn't bump - it acts like a TCP
> forwarder instead. I get a "CONNECT ip.add.ress:443" log record -
> no sign of the hostname and no bumping

Two critical details:

1) TCP packet headers do not contain hostnames. The "ip.add.ress:443"
you see is the tcp/443 dst-IP field on the intercepted traffic.


2) ssl_bump is a "fast" group ACL test. It does not hold up traffic
waiting for reverse-DNS lookups on the IP:port details. It just tests
the dst-IP against your regex rules and uses the resulting
match/non-match to decide between bumping or forwarding.

> 
> http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert 
> capath=/etc/ssl/certs/ generate-host-certificates=on 
> dynamic_cert_mem_cache_size=256MB options=ALL http_port 3129
> transparent https_port 3127 transparent ssl-bump
> cert=/etc/squid/squid-CA.cert capath=/etc/ssl/certs/
> generate-host-certificates=on dynamic_cert_mem_cache_size=256MB
> options=ALL
> 
> acl SSL_nonHTTPS_sites dstdom_regex
> "/etc/squid/SSL_nonHTTPS_sites.txt" acl SSL_noIntercept_sites
> dstdom_regex "/etc/squid/SSL_noIntercept_sites.txt" ssl_bump none
> SSL_nonHTTPS_sites ssl_bump none SSL_noIntercept_sites ssl_bump
> server-first all
> 
> So these older search-engine pages I came across claimed this
> should work with squid, but either I am missing something, or this
> doesn't work in 3.4.9?

The TCP forwarding behaviour occurs when your "ssl_bump none" rules
match the IP address of the intercepted tcp/443 traffic.

So it comes down to what your regex files contain and what TCP dst-IPs
your Squid is processing. Both of the details you have elided from
your description.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYI0uAAoJELJo5wb/XPRjgJYIAOGC63EWkAgnxBnUv0nO9mMK
iFmrirjVS6bH0i7tao8meAqEc0npz0h5h/6IFvwt+NVeks0sdq0zFN5624SZKD4M
sb4flKyDZdvnCMl9tVxKnVGQDLZU/wDV2xoEFA+nsIo2mwurn3+5o1YEZ2eCV14T
MXfdt4d7M1L2ReQGL/s12wcNnLLXyHdw1Se4wqZEYOn2+t3H7s6+q2gfe5/pqs8k
KMVfLc3EkaUnCeNduJ/W9sNJ4zb2Oa7m3vpzDjLR2/2c+lt/HfnrurXhZQdx+Tb5
EbBaI1yOrqPOGP7bfsA6kgECy+Qn5rJHXM2Db768DWCEFJSOf7kdopclGjRLhpQ=
=qeWj
-----END PGP SIGNATURE-----


From Jason_Haar at trimble.com  Mon Nov 10 10:26:43 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 10 Nov 2014 23:26:43 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <54608D2F.5050704@treenet.co.nz>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
Message-ID: <546092E3.20607@trimble.com>

On 10/11/14 23:02, Amos Jeffries wrote:
> > acl SSL_nonHTTPS_sites dstdom_regex
> > "/etc/squid/SSL_nonHTTPS_sites.txt" acl SSL_noIntercept_sites
> > dstdom_regex "/etc/squid/SSL_noIntercept_sites.txt" ssl_bump none
> > SSL_nonHTTPS_sites ssl_bump none SSL_noIntercept_sites ssl_bump
> > server-first all
>
> The TCP forwarding behaviour occurs when your "ssl_bump none" rules
> match the IP address of the intercepted tcp/443 traffic.
>
> So it comes down to what your regex files contain and what TCP dst-IPs
> your Squid is processing. Both of the details you have elided from
> your description.
>

Ha! You're dead right. I had "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" in
SSL_nonHTTPS_sites.txt so that Skype could work (skype will auto-detect
proxies if needed and uses CONNECT statements to peer IP addresses
instead of hostnames). So that whitelisted the bumps!

I've commented out  the SSL_nonHTTPS_sites rule and now it's broken
HTTPS all together. Now "telnet 1.2.3.4 443" connects and immediately
drops. cache.log shows squid crashing and restarting. If I comment out
"https_port", the crashing stops, so it looks like my config is OK for
"normal" proxy-bumping, but something is wrong for intercept. (this is a
Centos-6 box self-compiled 3.4.9)

2014/11/10 23:20:43 kid1| Closing HTTP port 0.0.0.0:3126
2014/11/10 23:20:43 kid1| Closing HTTP port 0.0.0.0:3129
2014/11/10 23:20:43 kid1| Closing HTTPS port 0.0.0.0:3127
FATAL: xstrdup: tried to dup a NULL pointer!

Squid Cache (Version 3.4.9): Terminated abnormally.
CPU Usage: 0.077 seconds = 0.049 user + 0.028 sys
Maximum Resident Size: 71088 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
    total space in arena:    9332 KB
    Ordinary blocks:         9264 KB      6 blks
    Small blocks:               0 KB      1 blks
    Holding blocks:         10068 KB      6 blks
    Free Small blocks:          0 KB
    Free Ordinary blocks:      67 KB
    Total in use:           19332 KB 207%
    Total free:                67 KB 1%
2014/11/10 23:20:43 kid1| storeDirWriteCleanLogs: Starting...
2014/11/10 23:20:43 kid1|   Finished.  Wrote 9466 entries.
2014/11/10 23:20:43 kid1|   Took 0.01 seconds (732549.14 entries/sec).
2014/11/10 23:20:46 kid1| Set Current Directory to /var/spool/squid
2014/11/10 23:20:46 kid1| Starting Squid Cache version 3.4.9 for
x86_64-redhat-linux-gnu...



-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From eliezer at ngtech.co.il  Mon Nov 10 10:43:21 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 10 Nov 2014 12:43:21 +0200
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
	mode?
In-Reply-To: <5460829D.9000104@trimble.com>
References: <5460829D.9000104@trimble.com>
Message-ID: <546096C9.4060109@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Can you send all ssl_bump related settings?
There are some missing parts in the settings.
If there is a bug\error the full details are needed to analyze the
subject.
I need:
- - OS details
- - machine details
- - network topology
- - cache logs
- - access logs

Eliezer

On 11/10/2014 11:17 AM, Jason Haar wrote:
> Hi there, I've googled about for this but I think most of the
> squid intercept stuff refers to 3.2 and I think things have changed
> since then?
> 
> I have squid-3.4.9 running with sslbump, and when I configure my
> browser to use it as a proxy, it bumps the certs nicely, signing
> "fake" certs/etc. I then added an iptables run to redirect outbound
> tcp/80 onto port 3129 (see below) and that transparently proxies
> all port 80 - great. I then went through the same exercise with
> sslbump, but when I put in an iptables rule to redirect outbound
> tcp/443 traffic onto 3127, it doesn't bump - it acts like a TCP
> forwarder instead. I get a "CONNECT ip.add.ress:443" log record -
> no sign of the hostname and no bumping
> 
> http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert 
> capath=/etc/ssl/certs/ generate-host-certificates=on 
> dynamic_cert_mem_cache_size=256MB options=ALL http_port 3129
> transparent https_port 3127 transparent ssl-bump
> cert=/etc/squid/squid-CA.cert capath=/etc/ssl/certs/
> generate-host-certificates=on dynamic_cert_mem_cache_size=256MB
> options=ALL
> 
> acl SSL_nonHTTPS_sites dstdom_regex
> "/etc/squid/SSL_nonHTTPS_sites.txt" acl SSL_noIntercept_sites
> dstdom_regex "/etc/squid/SSL_noIntercept_sites.txt" ssl_bump none
> SSL_nonHTTPS_sites ssl_bump none SSL_noIntercept_sites ssl_bump
> server-first all
> 
> So these older search-engine pages I came across claimed this
> should work with squid, but either I am missing something, or this
> doesn't work in 3.4.9?
> 
> Thanks

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYJbJAAoJENxnfXtQ8ZQUcAwH/RFRxy4Rk+TliEEPzcgT+BLu
Yu4n5I1XiOBMIixR+4qckV/f0j0Y51eWSvczs082Ow/vfOMlmImLtdWS8lswpTBX
cRQq3jhV9+MeFVDjDr8/owGXtf9TY5Aj1Jcmxvg+lR9TJvj4IzG5tp6t+SsW1Y0C
ulXdvKBYr+KGILSrUsIKb+Px+pSZHB/yRx1GHClQFVDrkHG1djSTT74SlRnTNREs
1Ewzm6CtNF5lYD5sHpgUAaI3fsDGbAmvebwyk4nzxyDj6o3Ow1tl3/z3gND8Tv++
WMoziJphFPPDAYhCpk5f6fSCPgM1nNaxdIDs0Z+i9wd/Nw2A5TWeW9U+JPAehqU=
=y/Dr
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 10 11:06:20 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 00:06:20 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <546092E3.20607@trimble.com>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
 <546092E3.20607@trimble.com>
Message-ID: <54609C2C.5090105@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 11:26 p.m., Jason Haar wrote:
> On 10/11/14 23:02, Amos Jeffries wrote:
>>> acl SSL_nonHTTPS_sites dstdom_regex 
>>> "/etc/squid/SSL_nonHTTPS_sites.txt" acl SSL_noIntercept_sites 
>>> dstdom_regex "/etc/squid/SSL_noIntercept_sites.txt" ssl_bump
>>> none SSL_nonHTTPS_sites ssl_bump none SSL_noIntercept_sites
>>> ssl_bump server-first all
>> 
>> The TCP forwarding behaviour occurs when your "ssl_bump none"
>> rules match the IP address of the intercepted tcp/443 traffic.
>> 
>> So it comes down to what your regex files contain and what TCP
>> dst-IPs your Squid is processing. Both of the details you have
>> elided from your description.
>> 
> 
> Ha! You're dead right. I had "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" in 
> SSL_nonHTTPS_sites.txt so that Skype could work (skype will
> auto-detect proxies if needed and uses CONNECT statements to peer
> IP addresses instead of hostnames). So that whitelisted the bumps!
> 
> I've commented out  the SSL_nonHTTPS_sites rule and now it's
> broken HTTPS all together. Now "telnet 1.2.3.4 443" connects and
> immediately drops. cache.log shows squid crashing and restarting.
> If I comment out "https_port", the crashing stops, so it looks like
> my config is OK for "normal" proxy-bumping, but something is wrong
> for intercept. (this is a Centos-6 box self-compiled 3.4.9)
> 
> 2014/11/10 23:20:43 kid1| Closing HTTP port 0.0.0.0:3126 2014/11/10
> 23:20:43 kid1| Closing HTTP port 0.0.0.0:3129 2014/11/10 23:20:43
> kid1| Closing HTTPS port 0.0.0.0:3127 FATAL: xstrdup: tried to dup
> a NULL pointer!


Grr, strdup bites again. Backtrace please if you can.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYJwrAAoJELJo5wb/XPRjTZgIAMW+5hICK87MXODECe1+qLud
sbQLjxhpE5OGyl6urmad9cfk0B+pQAdK8sYYq/rQaIYaGvjLtAxGX94x53dIP+Z/
BEd4u0IFVZ/LZpv6bhu+yN5zmXSns5s2vhupQHreeCTgcgbqylnuwVjFoB8aqdez
5TRS25LETGBBAL7L+8n5wQ//VXkz5Q8/vX8lQS5YTAJ5AhFc15/W2R2k0PtAKeXx
nCarsQjmMTO/lDTu2E5dgcpEWD1QuyMJO9YVl2oXL7YlO/t1vHzxg2xdpaKAO5Ri
q4xjodYEgOo6oFZRbkiaJwPZpowUNgbOzGGqJ/nHIwc8WpJTv5XPiJvN69HjMb8=
=4agm
-----END PGP SIGNATURE-----


From jbhasin83 at gmail.com  Mon Nov 10 11:08:11 2014
From: jbhasin83 at gmail.com (Jatin Bhasin)
Date: Mon, 10 Nov 2014 22:08:11 +1100
Subject: [squid-users] SslBump Squid - Dropbox client does not work
Message-ID: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>

Hello,

I am using squid 3.4.9 and the Dropbox client does not work with
SSLBump feature of squid. Dropbox client gives a message that it
cannot make a secure connection. Does anyone know fix or workaround or
this issue?


Thanks,
Jatin


From jbhasin83 at gmail.com  Mon Nov 10 11:08:11 2014
From: jbhasin83 at gmail.com (Jatin Bhasin)
Date: Mon, 10 Nov 2014 22:08:11 +1100
Subject: [squid-users] SslBump Squid - Dropbox client does not work
Message-ID: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>

Hello,

I am using squid 3.4.9 and the Dropbox client does not work with
SSLBump feature of squid. Dropbox client gives a message that it
cannot make a secure connection. Does anyone know fix or workaround or
this issue?


Thanks,
Jatin


From merhabakendim at gmail.com  Mon Nov 10 11:22:25 2014
From: merhabakendim at gmail.com (Efe)
Date: Mon, 10 Nov 2014 13:22:25 +0200
Subject: [squid-users] Squid3 config on Ubuntu remains even after uninstall
 and ignore the new config
Message-ID: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>

OS: Ubuntu 14.04 LTS

After i installed the squid3 package for the 1st time, i've add a list of
domains to be blocked in squid.conf:

    acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
    http_access deny myrule

where domainblock.txt is

    someaddress.com
    blockthis.net

Which worked fine and redirect them to localhost running on my LAMP

    Index of /

    Name   Last modified   Size   Description

    html   2014/04/10
    ------------------------------
----------------------
    Apache/2.4.7 (Ubuntu) Server at google-analytics.com Port 80

Later i purged it by:

    sudo apt-get remove --purge squid3*

and removed every file&folder the command "locate squid" gave, including
the "/etc/squid3" folder then reboot. But i still couldnt access the
websites in "domainblock.txt" even though it doesnt exist anymore.

Then i re-installed with sudo apt-get install squid3 this time with the
config to allow those websites in the list:

    acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
    http_access allow myrule

But still no luck. I guess some configurations remain even after removing
the squid in the system. So what should i do now?

Note: The default "squid.conf" is huge to add here, but i just added these
lines above & changed nothing.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/bd87184d/attachment.htm>

From Jason_Haar at trimble.com  Mon Nov 10 11:25:56 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 11 Nov 2014 00:25:56 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <54609C2C.5090105@treenet.co.nz>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
 <546092E3.20607@trimble.com> <54609C2C.5090105@treenet.co.nz>
Message-ID: <5460A0C4.4010000@trimble.com>

On 11/11/14 00:06, Amos Jeffries wrote:
> Grr, strdup bites again. Backtrace please if you can.
I'm not a developer, so here's my attempt, let me know if I need to do
something else

(gdb) run
Starting program: /usr/sbin/squid -N
[Thread debugging using libthread_db enabled]
Detaching after fork from child process 29759.
Detaching after fork from child process 29760.
Detaching after fork from child process 29761.
Detaching after fork from child process 29762.
Detaching after fork from child process 29763.
Detaching after fork from child process 29764.
Detaching after fork from child process 29765.
Detaching after fork from child process 29766.
Detaching after fork from child process 29767.
Detaching after fork from child process 29768.
Detaching after fork from child process 29769.
Detaching after fork from child process 29770.
Detaching after fork from child process 29771.

Program received signal SIGABRT, Aborted.
0x0000003f40032625 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003f40032625 in raise () from /lib64/libc.so.6
#1  0x0000003f40033e05 in abort () from /lib64/libc.so.6
#2  0x000000000059cbbb in fatal_dump(char const*) ()
#3  0x000000000082a6bb in xstrdup ()
#4  0x00000000006b528c in ACLUrlPathStrategy::match(ACLData<char
const*>*&, ACLFilledChecklist*, ACLFlags&) ()
#5  0x00000000006f9478 in ACL::matches(ACLChecklist*) const ()
#6  0x00000000006fbbbb in ACLChecklist::matchChild(Acl::InnerNode
const*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*,
std::allocator<ACL*> > >, ACL const*) ()
#7  0x00000000006faeb3 in Acl::AndNode::doMatch(ACLChecklist*,
__gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*,
std::allocator<ACL*> > >) const ()
#8  0x00000000006f9478 in ACL::matches(ACLChecklist*) const ()
#9  0x00000000006fbbbb in ACLChecklist::matchChild(Acl::InnerNode
const*, __gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*,
std::allocator<ACL*> > >, ACL const*) ()
#10 0x00000000006fae2e in Acl::OrNode::doMatch(ACLChecklist*,
__gnu_cxx::__normal_iterator<ACL* const*, std::vector<ACL*,
std::allocator<ACL*> > >) const ()
#11 0x00000000006f9478 in ACL::matches(ACLChecklist*) const ()
#12 0x00000000006fc474 in ACLChecklist::matchAndFinish() ()
#13 0x00000000006fce90 in ACLChecklist::nonBlockingCheck(void
(*)(allow_t, void*), void*) ()
#14 0x0000000000635f1a in ?? ()
#15 0x00000000005bc2b8 in FwdState::Start(RefCount<Comm::Connection>
const&, StoreEntry*, HttpRequest*, RefCount<AccessLogEntry> const&) ()
#16 0x00000000005bc706 in FwdState::fwdStart(RefCount<Comm::Connection>
const&, StoreEntry*, HttpRequest*) ()
#17 0x000000000053c572 in ConnStateData::switchToHttps(HttpRequest*,
Ssl::BumpMode) ()
#18 0x000000000053cde9 in ?? ()
#19 0x000000000054860f in ?? ()
#20 0x00000000006fc63b in ACLChecklist::checkCallback(allow_t) ()
#21 0x000000000054df1a in ?? ()
#22 0x00000000006ffa46 in AsyncCall::make() ()
#23 0x0000000000702b02 in AsyncCallQueue::fireNext() ()
#24 0x0000000000702e50 in AsyncCallQueue::fire() ()
#25 0x0000000000593cf4 in EventLoop::runOnce() ()
#26 0x0000000000593e48 in EventLoop::run() ()
#27 0x0000000000613e48 in SquidMain(int, char**) ()
#28 0x00000000006147d8 in main ()
(gdb) quit
A debugging session is active.

    Inferior 1 [process 29756] will be killed.

Quit anyway? (y or n) y


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From Jason_Haar at trimble.com  Mon Nov 10 11:26:23 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 11 Nov 2014 00:26:23 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <546096C9.4060109@ngtech.co.il>
References: <5460829D.9000104@trimble.com> <546096C9.4060109@ngtech.co.il>
Message-ID: <5460A0DF.4020105@trimble.com>

On 10/11/14 23:43, Eliezer Croitoru wrote:
> Can you send all ssl_bump related settings?
> There are some missing parts in the settings.

How's this?

# egrep '^(https?_port|ssl)' /etc/squid/squid.conf
http_port 3128
http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
http_port 3129 intercept
https_port 3127 intercept ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 32 startup=5 idle=1
ssl_bump server-first all


This is a CentOS-6 64bit server with 8G RAM and two Ethernet cards - one
internal and one external. iptables is used to redirect outbound tcp
port 80/443 (on internal network) onto squid port 3129/3127
respectively. I've removed the two ACLs I had and they haven't caused
any change, so they are not related to the problem

access.log does not show any entries (the crash occurs before they can
write I guess) and the cache.log  shows the following whenever I "telnet
1.2.3.4 443" (I've appended the cache.log from the start, through the
crash to the next start)

2014/11/11 00:14:02 kid1| Starting Squid Cache version 3.4.9 for
x86_64-redhat-linux-gnu...
2014/11/11 00:14:02 kid1| Process ID 25288
2014/11/11 00:14:02 kid1| Process Roles: worker
2014/11/11 00:14:02 kid1| With 16384 file descriptors available
2014/11/11 00:14:02 kid1| Initializing IP Cache...
2014/11/11 00:14:02 kid1| DNS Socket created at 0.0.0.0, FD 7
2014/11/11 00:14:02 kid1| Adding domain xx.org from /etc/resolv.conf
2014/11/11 00:14:02 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2014/11/11 00:14:02 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
processes
2014/11/11 00:14:02 kid1| helperOpenServers: Starting 5/20 'squidguard'
processes
2014/11/11 00:14:02 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2014/11/11 00:14:02 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2014/11/11 00:14:02 kid1| Unlinkd pipe opened on FD 33
2014/11/11 00:14:02 kid1| Local cache digest enabled; rebuild/rewrite
every 3600/3600 sec
2014/11/11 00:14:02 kid1| Store logging disabled
2014/11/11 00:14:02 kid1| Swap maxSize 1024000 + 524288 KB, estimated
119099 objects
2014/11/11 00:14:02 kid1| Target number of buckets: 5954
2014/11/11 00:14:02 kid1| Using 8192 Store buckets
2014/11/11 00:14:02 kid1| Max Mem  size: 524288 KB
2014/11/11 00:14:02 kid1| Max Swap size: 1024000 KB
2014/11/11 00:14:02 kid1| Rebuilding storage in /var/spool/squid (clean log)
2014/11/11 00:14:02 kid1| Using Least Load store dir selection
2014/11/11 00:14:02 kid1| Set Current Directory to /var/spool/squid
2014/11/11 00:14:02 kid1| Finished loading MIME types and icons.
2014/11/11 00:14:02 kid1| HTCP Disabled.
2014/11/11 00:14:02 kid1| Squid plugin modules loaded: 0
2014/11/11 00:14:02 kid1| Adaptation support is off.
2014/11/11 00:14:02 kid1| Accepting HTTP Socket connections at
local=0.0.0.0:3128 remote=[::] FD 36 flags=9
2014/11/11 00:14:02 kid1| Accepting SSL bumped HTTP Socket connections
at local=0.0.0.0:3126 remote=[::] FD 37 flags=9
2014/11/11 00:14:02 kid1| Accepting NAT intercepted HTTP Socket
connections at local=0.0.0.0:3129 remote=[::] FD 38 flags=41
2014/11/11 00:14:02 kid1| Accepting NAT intercepted SSL bumped HTTPS
Socket connections at local=0.0.0.0:3127 remote=[::] FD 39 flags=41
2014/11/11 00:14:02 kid1| Store rebuilding is 42.19% complete
2014/11/11 00:14:02 kid1| Done reading /var/spool/squid swaplog (9479
entries)
2014/11/11 00:14:02 kid1| Finished rebuilding storage from disk.
2014/11/11 00:14:02 kid1|      9479 Entries scanned
2014/11/11 00:14:02 kid1|         0 Invalid entries.
2014/11/11 00:14:02 kid1|         0 With invalid flags.
2014/11/11 00:14:02 kid1|      9479 Objects loaded.
2014/11/11 00:14:02 kid1|         0 Objects expired.
2014/11/11 00:14:02 kid1|         0 Objects cancelled.
2014/11/11 00:14:02 kid1|         0 Duplicate URLs purged.
2014/11/11 00:14:02 kid1|         0 Swapfile clashes avoided.
2014/11/11 00:14:02 kid1|   Took 0.06 seconds (147560.63 objects/sec).
2014/11/11 00:14:02 kid1| Beginning Validation Procedure
2014/11/11 00:14:02 kid1|   Completed Validation Procedure
2014/11/11 00:14:02 kid1|   Validated 9479 Entries
2014/11/11 00:14:02 kid1|   store_swap_size = 920980.00 KB
2014/11/11 00:14:03 kid1| storeLateRelease: released 0 objects
2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3128
2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3126
2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3129
2014/11/11 00:14:09 kid1| Closing HTTPS port 0.0.0.0:3127
FATAL: xstrdup: tried to dup a NULL pointer!

Squid Cache (Version 3.4.9): Terminated abnormally.
CPU Usage: 0.077 seconds = 0.054 user + 0.023 sys
Maximum Resident Size: 70912 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
    total space in arena:    9328 KB
    Ordinary blocks:         9228 KB      5 blks
    Small blocks:               0 KB      1 blks
    Holding blocks:         10068 KB      6 blks
    Free Small blocks:          0 KB
    Free Ordinary blocks:      99 KB
    Total in use:           19296 KB 207%
    Total free:                99 KB 1%
2014/11/11 00:14:09 kid1| storeDirWriteCleanLogs: Starting...
2014/11/11 00:14:09 kid1|   Finished.  Wrote 9479 entries.
2014/11/11 00:14:09 kid1|   Took 0.04 seconds (240455.59 entries/sec).
2014/11/11 00:14:12 kid1| Set Current Directory to /var/spool/squid
2014/11/11 00:14:12 kid1| Starting Squid Cache version 3.4.9 for
x86_64-redhat-linux-gnu...

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From merhabakendim at gmail.com  Mon Nov 10 11:35:11 2014
From: merhabakendim at gmail.com (Efe)
Date: Mon, 10 Nov 2014 13:35:11 +0200
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
	mode?
In-Reply-To: <5460A0DF.4020105@trimble.com>
References: <5460829D.9000104@trimble.com> <546096C9.4060109@ngtech.co.il>
 <5460A0DF.4020105@trimble.com>
Message-ID: <CA+-XJgV9g7rE1cMZoqB2Em=amg0nS-Ear-9yGwfWGjSxrZNPjw@mail.gmail.com>

Here are the outputs:

$ egrep '^(https?_port|ssl)' /etc/squid3/squid.conf

http_port 3128

---------------------------------------------------------------------------------------------------
$ /usr/sbin/squid3 -N

WARNING: Cannot write log file: /var/log/squid3/cache.log
/var/log/squid3/cache.log: Permission denied
         messages will be sent to 'stderr'.
WARNING: Cannot write log file: /var/log/squid3/cache.log
/var/log/squid3/cache.log: Permission denied
         messages will be sent to 'stderr'.
2014/11/10 13:30:29| WARNING: Closing open FD    2
2014/11/10 13:30:29| Starting Squid Cache version 3.3.8 for
i686-pc-linux-gnu...
2014/11/10 13:30:29| Process ID 24524
2014/11/10 13:30:29| Process Roles: master worker
2014/11/10 13:30:29| With 65536 file descriptors available
2014/11/10 13:30:29| Initializing IP Cache...
2014/11/10 13:30:29| DNS Socket created at [::], FD 4
2014/11/10 13:30:29| DNS Socket created at 0.0.0.0, FD 5
2014/11/10 13:30:29| Adding nameserver 127.0.1.1 from /etc/resolv.conf
2014/11/10 13:30:29| Adding domain mynet from /etc/resolv.conf
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_ACCESS_DENIED': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_ACCESS_DENIED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_CACHE_ACCESS_DENIED': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_CACHE_ACCESS_DENIED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_CACHE_MGR_ACCESS_DENIED': (2) No
such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_CACHE_MGR_ACCESS_DENIED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FORWARDING_DENIED': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FORWARDING_DENIED
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_NO_RELAY': (2)
No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_NO_RELAY
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_CANNOT_FORWARD': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_CANNOT_FORWARD
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_READ_TIMEOUT':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_READ_TIMEOUT
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_LIFETIME_EXP':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_LIFETIME_EXP
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_READ_ERROR':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_READ_ERROR
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_WRITE_ERROR':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_WRITE_ERROR
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_CONNECT_FAIL':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_CONNECT_FAIL
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_SECURE_CONNECT_FAIL': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_SECURE_CONNECT_FAIL
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_SOCKET_FAILURE': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_SOCKET_FAILURE
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_DNS_FAIL': (2)
No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_DNS_FAIL
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_URN_RESOLVE':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_URN_RESOLVE
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_ONLY_IF_CACHED_MISS': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_ONLY_IF_CACHED_MISS
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_TOO_BIG': (2)
No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_TOO_BIG
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_INVALID_RESP':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_INVALID_RESP
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_UNSUP_HTTPVERSION': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_UNSUP_HTTPVERSION
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_INVALID_REQ':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_INVALID_REQ
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_UNSUP_REQ':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_UNSUP_REQ
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_INVALID_URL':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_INVALID_URL
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_ZERO_SIZE_OBJECT': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_ZERO_SIZE_OBJECT
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_PRECONDITION_FAILED': (2) No such
file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_PRECONDITION_FAILED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_CONFLICT_HOST': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_CONFLICT_HOST
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_FTP_DISABLED':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_DISABLED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_UNAVAILABLE': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_UNAVAILABLE
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_FTP_FAILURE':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_FAILURE
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_PUT_ERROR': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_PUT_ERROR
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_NOT_FOUND': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_NOT_FOUND
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_FORBIDDEN': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_FORBIDDEN
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_PUT_CREATED': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_PUT_CREATED
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_FTP_PUT_MODIFIED': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_FTP_PUT_MODIFIED
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_ESI': (2) No
such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file ERR_ESI
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_ICAP_FAILURE':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_ICAP_FAILURE
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_GATEWAY_FAILURE': (2) No such file
or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_GATEWAY_FAILURE
2014/11/10 13:30:29| '/usr/share/squid3/errors/templates/ERR_DIR_LISTING':
(2) No such file or directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_DIR_LISTING
2014/11/10 13:30:29|
'/usr/share/squid3/errors/templates/ERR_SHUTTING_DOWN': (2) No such file or
directory
2014/11/10 13:30:29| WARNING: failed to find or read error text file
ERR_SHUTTING_DOWN
2014/11/10 13:30:29| Logfile: opening log daemon:/var/log/squid3/access.log
2014/11/10 13:30:29| Logfile Daemon: opening log /var/log/squid3/access.log
2014/11/10 13:30:29| WARNING: no_suid: setuid(0): (1) Operation not
permitted
2014/11/10 13:30:29| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2014/11/10 13:30:29| Store logging disabled
2014/11/10 13:30:29| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2014/11/10 13:30:29| Target number of buckets: 1008
2014/11/10 13:30:29| Using 8192 Store buckets
2014/11/10 13:30:29| Max Mem  size: 262144 KB
2014/11/10 13:30:29| Max Swap size: 0 KB
2014/11/10 13:30:29| Using Least Load store dir selection
2014/11/10 13:30:29| chdir: /var/spool/squid3: (2) No such file or directory
2014/11/10 13:30:29| Current Directory is /home/myhome
fopen: Permission denied
2014/11/10 13:30:29| Loaded Icons.
2014/11/10 13:30:29| commBind: Cannot bind socket FD 8 to [::]:3128: (98)
Address already in use
2014/11/10 13:30:29| HTCP Disabled.
2014/11/10 13:30:29| WARNING: no_suid: setuid(0): (1) Operation not
permitted
2014/11/10 13:30:29| Pinger socket opened on FD 10
2014/11/10 13:30:29| /var/run/squid3.pid: (13) Permission denied
2014/11/10 13:30:29| WARNING: Could not write pid file
2014/11/10 13:30:29| Squid plugin modules loaded: 0
2014/11/10 13:30:29| Adaptation support is off.
2014/11/10 13:30:29| Closing HTTP port [::]:3128
2014/11/10 13:30:29| storeDirWriteCleanLogs: Starting...
2014/11/10 13:30:29|   Finished.  Wrote 0 entries.
2014/11/10 13:30:29|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: Unable to open HTTP Socket
Squid Cache (Version 3.3.8): Terminated abnormally.
CPU Usage: 0.052 seconds = 0.048 user + 0.004 sys
Maximum Resident Size: 105920 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
    total space in arena:   15512 KB
    Ordinary blocks:        15407 KB      4 blks
    Small blocks:               0 KB      1 blks
    Holding blocks:         27420 KB      8 blks
    Free Small blocks:          0 KB
    Free Ordinary blocks:     104 KB
    Total in use:           42827 KB 276%
    Total free:               104 KB 1%
2014/11/10 13:30:29| Closing Pinger socket on FD 10
myhome at firstcom:~$ 2014/11/10 13:30:29| pinger: Initialising ICMP pinger ...
2014/11/10 13:30:29| pinger: ICMP socket opened.
2014/11/10 13:30:29| pinger: ICMPv6 socket opened
2014/11/10 13:30:29| Pinger exiting.

---------------------------------------------------------------------------------------------------


On Mon, Nov 10, 2014 at 1:26 PM, Jason Haar <Jason_Haar at trimble.com> wrote:

> On 10/11/14 23:43, Eliezer Croitoru wrote:
> > Can you send all ssl_bump related settings?
> > There are some missing parts in the settings.
>
> How's this?
>
> # egrep '^(https?_port|ssl)' /etc/squid/squid.conf
> http_port 3128
> http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert
> capath=/etc/ssl/certs/ generate-host-certificates=on
> dynamic_cert_mem_cache_size=256MB options=ALL
> http_port 3129 intercept
> https_port 3127 intercept ssl-bump cert=/etc/squid/squid-CA.cert
> capath=/etc/ssl/certs/ generate-host-certificates=on
> dynamic_cert_mem_cache_size=256MB options=ALL
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> sslcrtd_children 32 startup=5 idle=1
> ssl_bump server-first all
>
>
> This is a CentOS-6 64bit server with 8G RAM and two Ethernet cards - one
> internal and one external. iptables is used to redirect outbound tcp
> port 80/443 (on internal network) onto squid port 3129/3127
> respectively. I've removed the two ACLs I had and they haven't caused
> any change, so they are not related to the problem
>
> access.log does not show any entries (the crash occurs before they can
> write I guess) and the cache.log  shows the following whenever I "telnet
> 1.2.3.4 443" (I've appended the cache.log from the start, through the
> crash to the next start)
>
> 2014/11/11 00:14:02 kid1| Starting Squid Cache version 3.4.9 for
> x86_64-redhat-linux-gnu...
> 2014/11/11 00:14:02 kid1| Process ID 25288
> 2014/11/11 00:14:02 kid1| Process Roles: worker
> 2014/11/11 00:14:02 kid1| With 16384 file descriptors available
> 2014/11/11 00:14:02 kid1| Initializing IP Cache...
> 2014/11/11 00:14:02 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2014/11/11 00:14:02 kid1| Adding domain xx.org from /etc/resolv.conf
> 2014/11/11 00:14:02 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
> 2014/11/11 00:14:02 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
> processes
> 2014/11/11 00:14:02 kid1| helperOpenServers: Starting 5/20 'squidguard'
> processes
> 2014/11/11 00:14:02 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> 2014/11/11 00:14:02 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> 2014/11/11 00:14:02 kid1| Unlinkd pipe opened on FD 33
> 2014/11/11 00:14:02 kid1| Local cache digest enabled; rebuild/rewrite
> every 3600/3600 sec
> 2014/11/11 00:14:02 kid1| Store logging disabled
> 2014/11/11 00:14:02 kid1| Swap maxSize 1024000 + 524288 KB, estimated
> 119099 objects
> 2014/11/11 00:14:02 kid1| Target number of buckets: 5954
> 2014/11/11 00:14:02 kid1| Using 8192 Store buckets
> 2014/11/11 00:14:02 kid1| Max Mem  size: 524288 KB
> 2014/11/11 00:14:02 kid1| Max Swap size: 1024000 KB
> 2014/11/11 00:14:02 kid1| Rebuilding storage in /var/spool/squid (clean
> log)
> 2014/11/11 00:14:02 kid1| Using Least Load store dir selection
> 2014/11/11 00:14:02 kid1| Set Current Directory to /var/spool/squid
> 2014/11/11 00:14:02 kid1| Finished loading MIME types and icons.
> 2014/11/11 00:14:02 kid1| HTCP Disabled.
> 2014/11/11 00:14:02 kid1| Squid plugin modules loaded: 0
> 2014/11/11 00:14:02 kid1| Adaptation support is off.
> 2014/11/11 00:14:02 kid1| Accepting HTTP Socket connections at
> local=0.0.0.0:3128 remote=[::] FD 36 flags=9
> 2014/11/11 00:14:02 kid1| Accepting SSL bumped HTTP Socket connections
> at local=0.0.0.0:3126 remote=[::] FD 37 flags=9
> 2014/11/11 00:14:02 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=0.0.0.0:3129 remote=[::] FD 38 flags=41
> 2014/11/11 00:14:02 kid1| Accepting NAT intercepted SSL bumped HTTPS
> Socket connections at local=0.0.0.0:3127 remote=[::] FD 39 flags=41
> 2014/11/11 00:14:02 kid1| Store rebuilding is 42.19% complete
> 2014/11/11 00:14:02 kid1| Done reading /var/spool/squid swaplog (9479
> entries)
> 2014/11/11 00:14:02 kid1| Finished rebuilding storage from disk.
> 2014/11/11 00:14:02 kid1|      9479 Entries scanned
> 2014/11/11 00:14:02 kid1|         0 Invalid entries.
> 2014/11/11 00:14:02 kid1|         0 With invalid flags.
> 2014/11/11 00:14:02 kid1|      9479 Objects loaded.
> 2014/11/11 00:14:02 kid1|         0 Objects expired.
> 2014/11/11 00:14:02 kid1|         0 Objects cancelled.
> 2014/11/11 00:14:02 kid1|         0 Duplicate URLs purged.
> 2014/11/11 00:14:02 kid1|         0 Swapfile clashes avoided.
> 2014/11/11 00:14:02 kid1|   Took 0.06 seconds (147560.63 objects/sec).
> 2014/11/11 00:14:02 kid1| Beginning Validation Procedure
> 2014/11/11 00:14:02 kid1|   Completed Validation Procedure
> 2014/11/11 00:14:02 kid1|   Validated 9479 Entries
> 2014/11/11 00:14:02 kid1|   store_swap_size = 920980.00 KB
> 2014/11/11 00:14:03 kid1| storeLateRelease: released 0 objects
> 2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3128
> 2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3126
> 2014/11/11 00:14:09 kid1| Closing HTTP port 0.0.0.0:3129
> 2014/11/11 00:14:09 kid1| Closing HTTPS port 0.0.0.0:3127
> FATAL: xstrdup: tried to dup a NULL pointer!
>
> Squid Cache (Version 3.4.9): Terminated abnormally.
> CPU Usage: 0.077 seconds = 0.054 user + 0.023 sys
> Maximum Resident Size: 70912 KB
> Page faults with physical i/o: 0
> Memory usage for squid via mallinfo():
>     total space in arena:    9328 KB
>     Ordinary blocks:         9228 KB      5 blks
>     Small blocks:               0 KB      1 blks
>     Holding blocks:         10068 KB      6 blks
>     Free Small blocks:          0 KB
>     Free Ordinary blocks:      99 KB
>     Total in use:           19296 KB 207%
>     Total free:                99 KB 1%
> 2014/11/11 00:14:09 kid1| storeDirWriteCleanLogs: Starting...
> 2014/11/11 00:14:09 kid1|   Finished.  Wrote 9479 entries.
> 2014/11/11 00:14:09 kid1|   Took 0.04 seconds (240455.59 entries/sec).
> 2014/11/11 00:14:12 kid1| Set Current Directory to /var/spool/squid
> 2014/11/11 00:14:12 kid1| Starting Squid Cache version 3.4.9 for
> x86_64-redhat-linux-gnu...
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/9b6e2d74/attachment.htm>

From Martin.Sperl at amdocs.com  Mon Nov 10 11:57:00 2014
From: Martin.Sperl at amdocs.com (Martin Sperl)
Date: Mon, 10 Nov 2014 11:57:00 +0000
Subject: [squid-users] eCap + lua integration
Message-ID: <ED4469082EFD4F479C4E4B4D469697ADD15C7E53@ILRAADAGBE4.corp.amdocs.com>

Hi!

Is there any eCap lua integration module available that one could use for filtering - similar to apache mod_lua?

It then could get easily used as a URL-rewrite engine or to handle session-affinity and similar without any context switches that is needed with url_rewrite_program...

Thanks,
                Martin

This message and the information contained herein is proprietary and confidential and subject to the Amdocs policy statement,
you may review at http://www.amdocs.com/email_disclaimer.asp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/56ff39c6/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 10 12:18:59 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 10 Nov 2014 14:18:59 +0200
Subject: [squid-users] eCap + lua integration
In-Reply-To: <ED4469082EFD4F479C4E4B4D469697ADD15C7E53@ILRAADAGBE4.corp.amdocs.com>
References: <ED4469082EFD4F479C4E4B4D469697ADD15C7E53@ILRAADAGBE4.corp.amdocs.com>
Message-ID: <5460AD33.3010705@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Martin,

As far as I can tell there is non of these ECAP modules available.
Also using ECAP for this might be very weird since the helpers modules
do exists.
If you want to use lua specifically it's doable to write a helper for
the current squid interfaces(ICAP,external_acl,url_rewrite).
I have written myself an external_acl helper that help to filter
specific sites and have used deny_info as the user interface for a
"block" state.
There is however the overhead of using the external apache server that
has to "know" things about the sites which are blocked.

All The Bests,
Eliezer

On 11/10/2014 01:57 PM, Martin Sperl wrote:
> Hi!
> 
> Is there any eCap lua integration module available that one could
> use for filtering - similar to apache mod_lua?
> 
> It then could get easily used as a URL-rewrite engine or to handle
> session-affinity and similar without any context switches that is
> needed with url_rewrite_program...
> 
> Thanks, Martin

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYK0zAAoJENxnfXtQ8ZQUVoYH/0HiGcPgAz/JtU9Eyh5obNDr
b93gUx38dusvrfcNQ4QN30oFnmHkZx5vNPipcSn6Q5Rs48XMqu8SrHKwlYOLFIB/
UM/FB/LqZ1h7MXsXJtFU/BgY+4T9BEBmRPOX6UzLhbr0gg3JQGCy1w0fkoP5gmlK
8weJE6YoJWD4uZRU0Egh0521rELZry8SaIJG9wVI5K4C8SCStfiUwBYyIHu/vmmv
oLTml17lRnWNteOCCW4DI85ogZ1qYlB2eimDGUcsfK5hybSBg3Dg2PXGutQS7uJm
cQoC5kk9WK8tUi590+Fssoc+6NsBHTfYX5NJRc/nYfqiMKTIWfrfBpsA7hp5lHU=
=nCTs
-----END PGP SIGNATURE-----


From diego at woitasen.com.ar  Mon Nov 10 12:32:09 2014
From: diego at woitasen.com.ar (Diego Woitasen)
Date: Mon, 10 Nov 2014 09:32:09 -0300
Subject: [squid-users] 3.3.x -> 3.4.x: huge performance regression
In-Reply-To: <CAA7UhckbzXbgwT7kVwjm2R=-aeCmX-YwpVxNZZDodWtaorF9BA@mail.gmail.com>
References: <5447486B.3010205@norma.perm.ru> <5448C7FB.1080104@ngtech.co.il>
 <1FCF9DA5B29068478ECF15896F19F0840138A509A8@Y011008.bk.fin.local>
 <544A4390.3090903@norma.perm.ru> <544A4AD5.6040804@ngtech.co.il>
 <CAFKaCCSS_Mo0MWfxtM_Zp3fs-8iVrZcE7-EaO9x4aC+iVjYn8Q@mail.gmail.com>
 <CAA7UhcmzjjT=nyu-Ep6CUn09-GAE=f2DjEJxS1KQVk3K7zAeqw@mail.gmail.com>
 <544BD4DC.7070900@ngtech.co.il>
 <CAA7UhckbzXbgwT7kVwjm2R=-aeCmX-YwpVxNZZDodWtaorF9BA@mail.gmail.com>
Message-ID: <CAA7Uhc=efu5SjYw_X4Lp3AY938bAT+5EMOnHsR6xx_oS=0qsuQ@mail.gmail.com>

Info added to the bug report.

On Sun, Nov 9, 2014 at 7:53 PM, Diego Woitasen <diego at woitasen.com.ar> wrote:
> Hi,
>   I have more information. The testing environment has a few users. We
> switched to basic authencation and it's been working for a week without any
> issues. A couple of days ago we enabled NTLM again and the issue appeared
> again.
>
> I 'm on mobile now. I'll add more info in the bug report.
>
> Regards,
>   Diego
>
>
> On Oct 25, 2014 1:51 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Hey Diego,
>>
>> Can you take a look at the bug report and help pinpoint the issue please?
>> http://bugs.squid-cache.org/show_bug.cgi?id=3997
>>
>> I am pretty sure it's unique to auth only but I want to verify that
>> external_acl helpers do not affect this issue.
>>
>> Also if you can share the testing environment details or we can get
>> some help with testing from your IT testing team?
>>
>> Thanks,
>> Eliezer
>>
>>
>> On 10/25/2014 06:17 PM, Diego Woitasen wrote:
>> > Same problem here. New users, only a few users from IT testing it
>> > and CPU usage is really high from time to time.
>> >
>> > Switched to basic auth for a few days. Looks like everybody is
>> > having issues with NTLM/SPNEGO.
>> >
>> > Keep in touch and we'll fix it :)
>> >
>> > Regards, Diego
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v1
>>
>> iQEcBAEBAgAGBQJUS9TcAAoJENxnfXtQ8ZQU4EEIAIKeKjvzrPSlj8UlGUaWHhT+
>> 64ontOl7wiYdyo1rjU1MWZxg+6erlVVYg5p46Ki/bznes/on70peU6UndzInLA0K
>> JACZEq0P6eQBDQjP0eVfRbSVo4QeMA/+1prDZY8GAwyI3ugSWndeAT2dqVQFkVdt
>> x3OxXc5ch4nfV9ZF4HPAMKRp6mey4LJjixTToIw9CsoDpcAE7UAWuXi//JOHMqmp
>> b6ZONdhOBCJajWebhEHbUwNbciZVeCgGWXJGuyVA8kp0ChkFTtBnC7BpNjWRC3hL
>> rH5cJcfJXyFLoG67qZaPTueakk5aII8Aj2DkPauK2ofQAOjlLL6gh45GiO1oeJ0=
>> =sV5l
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



-- 
Diego Woitasen
Infrastructure Developer, DevOps Engineer, Linux and Open Source expert
http://www.woitasen.com.ar


From squid3 at treenet.co.nz  Mon Nov 10 13:20:07 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 02:20:07 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <5460A0C4.4010000@trimble.com>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
 <546092E3.20607@trimble.com> <54609C2C.5090105@treenet.co.nz>
 <5460A0C4.4010000@trimble.com>
Message-ID: <5460BB87.5010403@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 12:25 a.m., Jason Haar wrote:
> On 11/11/14 00:06, Amos Jeffries wrote:
>> Grr, strdup bites again. Backtrace please if you can.
> I'm not a developer, so here's my attempt, let me know if I need to
> do something else
> 

> #3  0x000000000082a6bb in xstrdup () #4  0x00000000006b528c in
> ACLUrlPathStrategy::match(ACLData<char const*>*&,
> ACLFilledChecklist*, ACLFlags&) ()

Aha. Thats great. Thank you.

You have an urlpath_regex ACL test depending on URIs containing paths.
Which is not the case with CONNECT.

The attached patch should fix the crash.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYLt2AAoJELJo5wb/XPRjdVoIAIzRQmx2gWvYs2IuaiUU7uox
qxl8VId/AwixD1abowYJzoWmDxeXn8WSbeQn0hRU7HIEfBzsyW2rG2XLW/SICwPF
2JU8U1cCvuwzaT5Jmw7ofpRCYaLj0qE3D02/eW6NRVR+qXV7qg6aiWZVKqivw2F3
r2OT9hlUTKH8lq3vqn4147UH67slHNvABjDNMhuvNJWvHQZRGMffr31JCM1UhPTK
4LoBRyBQ+QcAEtXhMt0TrQg2pBmpbXXITXVNR6PCF7VHx95zM0sPMeEyA1Px6uBg
1dD2MNKt7Bb8JqqwfKGyZw81wlgm2mcichPFSIQQsYnDYiFvJQ0MM5BP+0OYwYs=
=P8l+
-----END PGP SIGNATURE-----
-------------- next part --------------
=== modified file 'src/acl/UrlPath.cc'
--- src/acl/UrlPath.cc	2013-01-27 17:35:07 +0000
+++ src/acl/UrlPath.cc	2014-11-10 13:10:39 +0000
@@ -25,34 +25,37 @@
  *  GNU General Public License for more details.
  *
  *  You should have received a copy of the GNU General Public License
  *  along with this program; if not, write to the Free Software
  *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111, USA.
  *
  *
  * Copyright (c) 2003, Robert Collins <robertc at squid-cache.org>
  */
 
 #include "squid.h"
 #include "acl/UrlPath.h"
 #include "acl/Checklist.h"
 #include "acl/RegexData.h"
 #include "HttpRequest.h"
 #include "rfc1738.h"
 
 int
 ACLUrlPathStrategy::match (ACLData<char const *> * &data, ACLFilledChecklist *checklist, ACLFlags &)
 {
+    if (!checklist->request->urlpath.size())
+        return -1;
+
     char *esc_buf = xstrdup(checklist->request->urlpath.termedBuf());
     rfc1738_unescape(esc_buf);
     int result = data->match(esc_buf);
     safe_free(esc_buf);
     return result;
 }
 
 ACLUrlPathStrategy *
 ACLUrlPathStrategy::Instance()
 {
     return &Instance_;
 }
 
 ACLUrlPathStrategy ACLUrlPathStrategy::Instance_;

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Segfault_aclUrlPath_mk0.patch.sig
Type: application/octet-stream
Size: 287 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141111/775255fd/attachment.obj>

From squid3 at treenet.co.nz  Mon Nov 10 13:22:22 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 02:22:22 +1300
Subject: [squid-users] SslBump Squid - Dropbox client does not work
In-Reply-To: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>
References: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>
Message-ID: <5460BC0E.6000803@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 12:08 a.m., Jatin Bhasin wrote:
> Hello,
> 
> I am using squid 3.4.9 and the Dropbox client does not work with 
> SSLBump feature of squid. Dropbox client gives a message that it 
> cannot make a secure connection. Does anyone know fix or workaround
> or this issue?

Please start by finding out what is the problem. "Cannot connect" is
not sufficient to diagnose anything.
Your proxy logs should be able to help out there.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYLwOAAoJELJo5wb/XPRjQCkIAI2Zb+sryeBziaDZlR98uzvS
p2y8GZiUK8qzXyMwkyL5D2KUzdrC1LJGwc8NpBErX+r0VZrT0ryAzzmxgcnp+AWr
LbaVv5hmy/+AArKv9sjuxnOFTZlS0tmnJkmCJx3zo4B0LhZKEqB+NnhgeryjsNDs
PsUtU1A2ZoLjCb7w9Lc/ZvxcrEhvRMQPoEBj8Z4jX7XFkoJk3N6fy+QGhO6iM4cm
kgziG0fFi3mY1ppKuTc5J24/9+I4rRfPg9NhgWp+CWi5eqlHpV/PeMXmRAnSU6I+
IrcJ6OU9Lxd0XsivA8HksCtjUWvq+6EcCFRtXRWbT+fz+wcRom52EUh7oy5FqtE=
=bQ3E
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 10 13:32:51 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 02:32:51 +1300
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
Message-ID: <5460BE83.9090100@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 12:22 a.m., Efe wrote:
> OS: Ubuntu 14.04 LTS
> 
> After i installed the squid3 package for the 1st time, i've add a
> list of domains to be blocked in squid.conf:
> 
> acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> deny myrule
> 
> where domainblock.txt is
> 
> someaddress.com blockthis.net

For domain names like this you should be using "dstdomain" ACL type
instead of a regex.

> 
> Which worked fine and redirect them to localhost running on my
> LAMP

?? there is nothing about a "redirect" in that squid.conf snippet you
showed.

Can you please display the whole squid.conf. Without all the
documentation #comment or empty lines though.


> 
> Index of /
> 
> Name   Last modified   Size   Description
> 
> html   2014/04/10 ------------------------------ 
> ---------------------- Apache/2.4.7 (Ubuntu) Server at
> google-analytics.com Port 80
> 
> Later i purged it by:
> 
> sudo apt-get remove --purge squid3*
> 
> and removed every file&folder the command "locate squid" gave,
> including the "/etc/squid3" folder then reboot. But i still couldnt
> access the websites in "domainblock.txt" even though it doesnt
> exist anymore.

"it" being squid I assume?

There is almost always other configuration you had to add in the
network or browser causing the browser to use the proxy. This will
have probably broken your test result.

If you did actually not have any of that, then you just proved that
Squid was not involved with the problem.

> 
> Then i re-installed with sudo apt-get install squid3 this time with
> the config to allow those websites in the list:
> 
> acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> allow myrule
> 
> But still no luck. I guess some configurations remain even after
> removing the squid in the system. So what should i do now?

What do you want to achieve exactly?

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYL6CAAoJELJo5wb/XPRjF/sIANt7v8/BpcZ4taHWpRZyXbul
mdr6Cq7rfNb8VRiR41GYnEi0qDuSsSb31ciCpVAVoaeIBF48FufleqQ+QH3kn8M3
ETvNuUHis2Wd8gNXKd3pWtqBx+AbVyPzgKpJebBma0KeOw1eE3Jwevsqbjh/yMBq
KLNfDH0CgL63wYkbNquP3AjDkQggv/w/YvD9bFQY1JfMsaTb64CEdP6NdtEbgnhi
PBo9p9/T1HBkfBf2kg+uElR+TMur1OoHaztxx8g+iiqfBHSocaXhWRCtayhqg35X
8DCmysOYNjY9FyQHZAKFaSeb4WyD02On/KOchH3/5ZHCA1P4HG9zF8V4KYmlgfc=
=iena
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 10 13:36:26 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 02:36:26 +1300
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
Message-ID: <5460BF5A.6030400@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 2:50 a.m., Tom Tom wrote:
> Hi
> 
> After migration from squid 3.3.13 to 3.4.4, I recognized a 
> performance-issue. Squid is configured with 4 workers. They often
> have a CPU-Utilization between 50%-90% (each worker). With squid
> 3.3.13 (same configuration), the CPU-Utilization was never a
> problem. I installed squid 3.4.9 and had the same issue. No
> warnings/errors in the cache.log
> 
> I saw, that someone other reported a similar issue: 
> http://www.squid-cache.org/mail-archive/squid-users/201407/0500.html
>
>  Concerning the post above: Yes, we have external_auth-helpers 
> (ext_kerberos_ldap_group_acl) and no, we do not use delay_pools.
> The high cpu-usage comes not from the auth-helper - it comes from
> the 4 squid-worker-processes. Any hints? Is this a known problem?
> Probably solved in 3.5?

Are you able to find out any specific details about what the workers
are doing that uses so much extra CPU?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYL9ZAAoJELJo5wb/XPRj8B8H/3/sgvNAKBmRBAffyXY+gbkC
KNgcfD2e2d/6BDglKJrlj8BCtIFg/2DBN6j6S+y/DETdu8K9Rwfgc4za0eRt7bMu
V6In4lYCdh2Bws6gXSy9ONKbBCzJnwtfZr80kIFvXH0m7Pa5RiQ8kJIMvXh/w9tH
cRi4vDE9DWTNiDwW2bc/Li15NyDtzzLTyzkrhdJ+joDZeaWgXBEDYwdt67S5eEbD
WWiilu8rv3hdHjguYUwqvp2Z5u3Rfck/XaQFH1I6ez/0zrkd2J7Gv2J2CDQLGv4j
aK+JhTsyhvVtIJuWApGSwlmFJWWwPIR3tV59ZVRT9uZ0lZNROdaSMuIaKKNvbss=
=cjcF
-----END PGP SIGNATURE-----


From merhabakendim at gmail.com  Mon Nov 10 13:58:42 2014
From: merhabakendim at gmail.com (Efe)
Date: Mon, 10 Nov 2014 15:58:42 +0200
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <5460BE83.9090100@treenet.co.nz>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460BE83.9090100@treenet.co.nz>
Message-ID: <CA+-XJgUDPErO67zo_CwpCeS5jGamer6HJHKdWSRXZq+o_H2juA@mail.gmail.com>

Thank you for your reply. I've managed to retrieve uncommented config lines:

$ grep -P '^\s*\w' /etc/squid3/squid.conf

acl localnet src 192.168.0.101  # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 21      # ftp
acl Safe_ports port 443     # https
acl Safe_ports port 70      # gopher
acl Safe_ports port 210     # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280     # http-mgmt
acl Safe_ports port 488     # gss-http
acl Safe_ports port 591     # filemaker
acl Safe_ports port 777     # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
http_access allow myrule
cache deny all
http_access allow localhost manager
http_access deny manager
acl Purge method PURGE
http_access deny Purge
http_access allow localhost
http_port 3128
coredump_dir /var/spool/squid3
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .       0   20% 4320

If helps, iptables is empty and netstat status is
$ sudo netstat -nltp | grep squid
tcp6       0      0 :::3128                 :::*
LISTEN      20292/(squid-1)

Version info:
$ sudo apt-cache policy squid3
squid3:
  Installed: 3.3.8-1ubuntu6.1
  Candidate: 3.3.8-1ubuntu6.1
  Version table:
 *** 3.3.8-1ubuntu6.1 0
        500 http://archive.ubuntu.com/ubuntu/ trusty-updates/main i386
Packages
        500 http://archive.ubuntu.com/ubuntu/ trusty-security/main i386
Packages
        100 /var/lib/dpkg/status
     3.3.8-1ubuntu6 0
        500 http://archive.ubuntu.com/ubuntu/ trusty/main i386 Packages

Proof that squid is running:
$ ps ax | grep squid
20290 ?        Ss     0:00 squid3
20292 ?        S      0:06 (squid-1)
31535 ?        S      0:00 (logfile-daemon) /var/log/squid3/access.log
31720 pts/28   S+     0:00 grep --color=auto squid

Maybe i used the wrong terminology as "redirect". B/c whenever the website
in the blocklist is called, localhost page of my LAMP shows up.

So, what i want to achieve in the end is blocking and sometimes unblocking
a list of websites based on their domain name. Problem is even the config
is changed to "http_access allow myrule" it doesnt reflect allow/deny
options accordingly anymore. At this moment, the websites in the list are
still non-accessible.

On Mon, Nov 10, 2014 at 3:32 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 11/11/2014 12:22 a.m., Efe wrote:
> > OS: Ubuntu 14.04 LTS
> >
> > After i installed the squid3 package for the 1st time, i've add a
> > list of domains to be blocked in squid.conf:
> >
> > acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> > deny myrule
> >
> > where domainblock.txt is
> >
> > someaddress.com blockthis.net
>
> For domain names like this you should be using "dstdomain" ACL type
> instead of a regex.
>
> >
> > Which worked fine and redirect them to localhost running on my
> > LAMP
>
> ?? there is nothing about a "redirect" in that squid.conf snippet you
> showed.
>
> Can you please display the whole squid.conf. Without all the
> documentation #comment or empty lines though.
>
>
> >
> > Index of /
> >
> > Name   Last modified   Size   Description
> >
> > html   2014/04/10 ------------------------------
> > ---------------------- Apache/2.4.7 (Ubuntu) Server at
> > google-analytics.com Port 80
> >
> > Later i purged it by:
> >
> > sudo apt-get remove --purge squid3*
> >
> > and removed every file&folder the command "locate squid" gave,
> > including the "/etc/squid3" folder then reboot. But i still couldnt
> > access the websites in "domainblock.txt" even though it doesnt
> > exist anymore.
>
> "it" being squid I assume?
>
> There is almost always other configuration you had to add in the
> network or browser causing the browser to use the proxy. This will
> have probably broken your test result.
>
> If you did actually not have any of that, then you just proved that
> Squid was not involved with the problem.
>
> >
> > Then i re-installed with sudo apt-get install squid3 this time with
> > the config to allow those websites in the list:
> >
> > acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> > allow myrule
> >
> > But still no luck. I guess some configurations remain even after
> > removing the squid in the system. So what should i do now?
>
> What do you want to achieve exactly?
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUYL6CAAoJELJo5wb/XPRjF/sIANt7v8/BpcZ4taHWpRZyXbul
> mdr6Cq7rfNb8VRiR41GYnEi0qDuSsSb31ciCpVAVoaeIBF48FufleqQ+QH3kn8M3
> ETvNuUHis2Wd8gNXKd3pWtqBx+AbVyPzgKpJebBma0KeOw1eE3Jwevsqbjh/yMBq
> KLNfDH0CgL63wYkbNquP3AjDkQggv/w/YvD9bFQY1JfMsaTb64CEdP6NdtEbgnhi
> PBo9p9/T1HBkfBf2kg+uElR+TMur1OoHaztxx8g+iiqfBHSocaXhWRCtayhqg35X
> 8DCmysOYNjY9FyQHZAKFaSeb4WyD02On/KOchH3/5ZHCA1P4HG9zF8V4KYmlgfc=
> =iena
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/81d3a185/attachment.htm>

From Martin.Sperl at amdocs.com  Mon Nov 10 14:14:37 2014
From: Martin.Sperl at amdocs.com (Martin Sperl)
Date: Mon, 10 Nov 2014 14:14:37 +0000
Subject: [squid-users] eCap + lua integration
In-Reply-To: <5460AD33.3010705@ngtech.co.il>
References: <ED4469082EFD4F479C4E4B4D469697ADD15C7E53@ILRAADAGBE4.corp.amdocs.com>
 <5460AD33.3010705@ngtech.co.il>
Message-ID: <ED4469082EFD4F479C4E4B4D469697ADD15C7EC0@ILRAADAGBE4.corp.amdocs.com>

Well, the problem with the helper-modules is that you have context switches (icap, external_acl and reqwrite) and scheduler delays.
This is (as far as I understood) the great advantage of eCap: it does not run in an external context with the corresponding context switches.
That was why I was asking if a lua binding was available for eCap...

The drawback is probably that lua is quite a bit slower than native C (unless luajit is used) and thus probably makes the event loop slower reducing the throughput.
This would probably need to get tested and compared against an icap implementation which comes with the 

Thanks,
	Martin

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Eliezer Croitoru
> Sent: Montag, 10. November 2014 13:19
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] eCap + lua integration
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Hey Martin,
> 
> As far as I can tell there is non of these ECAP modules available.
> Also using ECAP for this might be very weird since the helpers modules
> do exists.
> If you want to use lua specifically it's doable to write a helper for
> the current squid interfaces(ICAP,external_acl,url_rewrite).
> I have written myself an external_acl helper that help to filter
> specific sites and have used deny_info as the user interface for a
> "block" state.
> There is however the overhead of using the external apache server that
> has to "know" things about the sites which are blocked.
> 
> All The Bests,
> Eliezer
> 
> On 11/10/2014 01:57 PM, Martin Sperl wrote:
> > Hi!
> >
> > Is there any eCap lua integration module available that one could
> > use for filtering - similar to apache mod_lua?
> >
> > It then could get easily used as a URL-rewrite engine or to handle
> > session-affinity and similar without any context switches that is
> > needed with url_rewrite_program...
> >
> > Thanks, Martin
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
> 
> iQEcBAEBAgAGBQJUYK0zAAoJENxnfXtQ8ZQUVoYH/0HiGcPgAz/JtU9Eyh5obND
> r
> b93gUx38dusvrfcNQ4QN30oFnmHkZx5vNPipcSn6Q5Rs48XMqu8SrHKwlYOLFI
> B/
> UM/FB/LqZ1h7MXsXJtFU/BgY+4T9BEBmRPOX6UzLhbr0gg3JQGCy1w0fkoP5gm
> lK
> 8weJE6YoJWD4uZRU0Egh0521rELZry8SaIJG9wVI5K4C8SCStfiUwBYyIHu/vmmv
> oLTml17lRnWNteOCCW4DI85ogZ1qYlB2eimDGUcsfK5hybSBg3Dg2PXGutQS7u
> Jm
> cQoC5kk9WK8tUi590+Fssoc+6NsBHTfYX5NJRc/nYfqiMKTIWfrfBpsA7hp5lHU=
> =nCTs
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

This message and the information contained herein is proprietary and confidential and subject to the Amdocs policy statement,
you may review at http://www.amdocs.com/email_disclaimer.asp

From guy.helmer at palisadesystems.com  Mon Nov 10 14:40:02 2014
From: guy.helmer at palisadesystems.com (Guy Helmer)
Date: Mon, 10 Nov 2014 08:40:02 -0600
Subject: [squid-users] SslBump Squid - Dropbox client does not work
In-Reply-To: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>
References: <CAGRJihV9gkg=WmVtmyhNobxxN2D9_ffC6Opxr1kogWcg7XAjxQ@mail.gmail.com>
Message-ID: <E1BAF1D9-B0DE-4497-9139-2FFEE42BF388@palisadesystems.com>


> On Nov 10, 2014, at 5:08 AM, Jatin Bhasin <jbhasin83 at gmail.com> wrote:
> 
> Hello,
> 
> I am using squid 3.4.9 and the Dropbox client does not work with
> SSLBump feature of squid. Dropbox client gives a message that it
> cannot make a secure connection. Does anyone know fix or workaround or
> this issue?
> 
> 
> Thanks,
> Jatin

I?ve researched this and even contacted Dropbox tech support ? Dropbox seems to use its own SSL library and doesn?t provide a way to add a trusted root cert. I haven?t found an easy way to work around this when using intercept mode. I would like to try out 3.5?s peek & splice to see if it can help.

Guy




From ahmed.zaeem at netstream.ps  Tue Nov 11 00:41:29 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Mon, 10 Nov 2014 16:41:29 -0800
Subject: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does
	not support the forwarding method specified, only GRE supported
In-Reply-To: <545FA1A8.1080309@treenet.co.nz>
References: <002f01cffb91$22b4fcf0$681ef6d0$@netstream.ps>
 <545E017A.5050302@treenet.co.nz>
 <00d101cffc73$a7cecbe0$f76c63a0$@netstream.ps>
 <545FA1A8.1080309@treenet.co.nz>
Message-ID: <00f701cffd48$40581220$c1083660$@netstream.ps>

Thanks 

I have some achievement , but still no access.log

I have seen hits from router to wccp0 gre port , but cant see log file and no browsing ??

I have from tcpdump traffic goin to wccp0 :

tcpdump -i wccp0
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on wccp0, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes
05:45:57.042756 IP xxxx > xxxx.http: Flags [S], seq 3127016785, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK], length 0
05:45:57.293315 IP xxxx > xxx.http: Flags [S], seq 3529638186, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK], length 0
05:46:00.036110 IP xxxx > xxxxhttp: Flags [S], seq 3127016785, win 8192, options [mss 1460,nop,wscale 8,nop,nop,sackOK], length 0



[root at localhost ~]# ifconfig
enp4s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet xxxx  netmask 255.255.255.252  broadcast xxxx
        inet6 fe80::1e6f:65ff:fe25:9230  prefixlen 64  scopeid 0x20<link>
        ether 1c:6f:65:25:92:30  txqueuelen 1000  (Ethernet)
        RX packets 1748  bytes 204497 (199.7 KiB)
        RX errors 0  dropped 62  overruns 0  frame 0
        TX packets 1365  bytes 242105 (236.4 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 0  (Local Loopback)
        RX packets 10  bytes 968 (968.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 10  bytes 968 (968.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

wccp0: flags=209<UP,POINTOPOINT,RUNNING,NOARP>  mtu 1476
        inet 127.0.1.1  netmask 0.0.0.0  destination 127.0.1.1
        inet6 fe80::200:5efe:4ddd:7da6  prefixlen 64  scopeid 0x20<link>
        unspec 4D-DD-7D-A6-00-00-F0-00-00-00-00-00-00-00-00-00  txqueuelen 0  (UNSPEC)
        RX packets 212  bytes 10732 (10.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3  bytes 180 (180.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

need something  to be done in  routing ?

regards

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, November 9, 2014 9:17 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] wccp2HandleUdp: fatal error - A WCCP router does not support the forwarding method specified, only GRE supported

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 10/11/2014 12:19 p.m., Ahmed Allzaeem wrote:
> Thanks Amos , Im trying to implement gre , AND  I followed
> http://wiki.squid-cache.org/ConfigExamples/UbuntuTproxy4Wccp2
> 
> But still wondering ,
> 
> Don?t we need to create GRE config for cisco ??

I've not seen any of the config examples needing to specify it specifically at the Cisco end. I suspect it is implicit in the enabling of "wccp" modules.

GRE vs L2 use also depends on the device hardware type (router or switch, PIX or ASA) and how recent the IOS version is.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUX6GoAAoJELJo5wb/XPRjLbcIAK6UZYYVVUw3XZLANqJ/x4Ph
XppIXXr2nMKYyAhtSqN8+uPdHu9+pdvAVeyvnx+irp44wTDtj6+ePB7R0PMmhBTC
JTw2/egFeYZmdEI0mCsq4Tmsx4NVbQmHRYok5SYCGdkJvZGbneNgEABHMt6LQxaZ
P0B7kbLLDqMs64BNxtDBBznQCRUCZjRsnBBd8sBnMK0YX0nh6d/oAfIg1GGszW7f
bH4Mth6rpDcFzRiWnbBmAYLzPF5buhvZYw7ivzIrhQHqX0ZmBTTkCMeaEuMCr/rh
budLJRl3MwsYll/w6zoywPebGQImgzhj99ZtyKeqQQNKDPGyEOIGcmAt8HsvVNo=
=5zVY
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From tomtux007 at gmail.com  Mon Nov 10 14:46:40 2014
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 10 Nov 2014 15:46:40 +0100
Subject: [squid-users] FTP-Prompt-Behaviour changed between 3.3.11 and
	>=3.3.13
In-Reply-To: <545CFF57.4040306@treenet.co.nz>
References: <CACLJR+PKRCjhgYaLJNQxiJnvDP8VA7f6MBR5FfWKLRXb0_DkAA@mail.gmail.com>
 <545CFF57.4040306@treenet.co.nz>
Message-ID: <CACLJR+N8BkVJ6eP0TOYWRhvwKY9f0C=cGZh=fYZJoZsRy-oHaQ@mail.gmail.com>

Traces are showing, that in the 401-response from squid, which
provides the ftp-prompt (3.3.11), the header-field 'WWW-Authenticate:
Basic realm="FTP Access"' exists. In the newer squid-version (ex.
3.3.13), the prompt doesn't appear and the header-field
"WWW-Authenticate" is not existent. Why does squid in newer versions
"eats" this header-field? Is there a configuration-directive for
squid, not to delete this field?

On Fri, Nov 7, 2014 at 6:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 8/11/2014 2:07 a.m., Tom Tom wrote:
>> Hi
>>
>> Within squid 3.3.11 and 3.3.13 (and of course squid >3.3.13)
>> changed something concerning browser-behaviour while accessing
>> ftp-sites:
>>
>> squid 3.3.11 ============ ftp://ftp.xxx.xxx -> User is prompted for
>> username/password (TCP_DENIED/401), when anonymous-access is not
>> allowed
>>
>>
>> squid 3.3.13 (same config as in 3.3.11)
>> ======================================= ftp://ftp.xxx.xxx -> User
>> is *not* prompted for username/password (also TCP_DENIED/401), when
>> anonymous-access is not allowed.
>>
>> Tested with IE and FF.
>>
>> Any hints for this behaviour? Is there a way, to enforce the
>> ftp-prompt?
>
> Take a look at the HTTP response headers Squid is sending back to the
> browser in those 401.
>
> And see if there is a followup request with credentials. It may just
> be the browser is doing SSO with the authentication now.
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUXP9WAAoJELJo5wb/XPRjbXQH/21x8hke4Ot872IlbFMIlVru
> NtLME6wnOB3rk+u/2e1gSiE7cdBWp6iGOIi1LWARncL84qmOb6AmsZYBNx3zpLXh
> 4znWM8KTSUIHWDEkbV/wJVR5XYBSbGR2onyjISc1DqA4QHkXR/+GO+ZIGhkJ/Vhf
> QipOJQQQJC/9ByvtnPSXJBa5QO+h8RI7dPgkTuuImQ/vQV7X4BKFhMkoI2p3QoIV
> zYXYyi9dUTV7ohdN+MxkIOSYMxgRUpED4b1bcM/Jxy4gNfwjIc2QAub1P0TNvqZ4
> GCCZD4tPnAgwbb0FiEI6At5Ya/6Y5kF9zhc7OT420QFqzOUgU/i4Rn2ll8HGp3Y=
> =x/fE
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From markus.rietzler at fv.nrw.de  Mon Nov 10 15:12:10 2014
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, SG 324 / <RIETZLER_SOFTWARE>))
Date: Mon, 10 Nov 2014 15:12:10 +0000
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <5460BF5A.6030400@treenet.co.nz>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
 <5460BF5A.6030400@treenet.co.nz>
Message-ID: <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>

> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im
> Auftrag von Amos Jeffries
> Gesendet: Montag, 10. November 2014 14:36
> An: squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] High CPU-Usage with squid 3.4.9 (and/or
> 3.4.4)
> 
> On 7/11/2014 2:50 a.m., Tom Tom wrote:
> > Hi
> >
> > After migration from squid 3.3.13 to 3.4.4, I recognized a
> > performance-issue. Squid is configured with 4 workers. They often
> > have a CPU-Utilization between 50%-90% (each worker). With squid
> > 3.3.13 (same configuration), the CPU-Utilization was never a
> > problem. I installed squid 3.4.9 and had the same issue. No
> > warnings/errors in the cache.log
> >
> > I saw, that someone other reported a similar issue:
> > http://www.squid-cache.org/mail-archive/squid-users/201407/0500.html
> >
> >  Concerning the post above: Yes, we have external_auth-helpers
> > (ext_kerberos_ldap_group_acl) and no, we do not use delay_pools.
> > The high cpu-usage comes not from the auth-helper - it comes from
> > the 4 squid-worker-processes. Any hints? Is this a known problem?
> > Probably solved in 3.5?
> 
> Are you able to find out any specific details about what the workers
> are doing that uses so much extra CPU?
> 
> Amos
> 
during our last tests (with 3.4.x) we also tried the worker option. it does not matter if workers are enabled or not. with more workers the cpu rise seems to be somewhat slower. so it is not connected to (smp)workers. it is the external auth helper - although the squid process and not the helper does consume all the cpu...

markus

From schinken at hackerspace-bamberg.de  Mon Nov 10 15:13:54 2014
From: schinken at hackerspace-bamberg.de (schinken)
Date: Mon, 10 Nov 2014 16:13:54 +0100
Subject: [squid-users] NTLM Auth fails while using DNS instead of IP address
Message-ID: <5460D632.10704@hackerspace-bamberg.de>

Hi,

i recently configured a squid 3.3.8 on an ubuntu 14.04 trusty host using
NTLM/Kerberos auth. My configuration is mostly based on a config example
on squid-cache.org [1].

It took me a day to figure out why the following messages appeared in my
/var/log/squid3/cache.log:

> 2014/11/10 06:32:16| negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified GSS failure.  Minor code may provide more information.
> 2014/11/10 06:32:16| ERROR: Negotiate Authentication validating user. Error returned 'BH gss_accept_sec_context() failed: Unspecified GSS failure.  Minor code may provide more information.'
> 2014/11/10 06:32:16| negotiate_wrapper: Return 'BH gss_accept_sec_context() failed: Unspecified GSS failure.  Minor code may provide more information.'


After debugging and running each command seperately i discored that the
problem lies within the client proxy settings.

If i use the IP address instead of the hostname of the squid server, the
authentication works fine.

What could that possibly be? The error message is kind of "unhelpful".

[1]
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory

Thanks for help in advance,
Schinken

---
Backspace e.V.
http://hackerspace-bamberg.de

mail: schinken at hackerspace-bamberg.de
xmpp: schinken at tai-wahn.de (otr)
GPG: FFB7 E40D B2DD D24C C9B7 B5C5 703C F8B8 882C 871E


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/ac347450/attachment.sig>

From squid3 at treenet.co.nz  Mon Nov 10 15:45:34 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 04:45:34 +1300
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <CA+-XJgUDPErO67zo_CwpCeS5jGamer6HJHKdWSRXZq+o_H2juA@mail.gmail.com>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460BE83.9090100@treenet.co.nz>
 <CA+-XJgUDPErO67zo_CwpCeS5jGamer6HJHKdWSRXZq+o_H2juA@mail.gmail.com>
Message-ID: <5460DD9E.8080201@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 2:58 a.m., Efe wrote:
> Thank you for your reply. I've managed to retrieve uncommented
> config lines:
> 
> $ grep -P '^\s*\w' /etc/squid3/squid.conf
> 
> acl localnet src 192.168.0.101  # RFC1918 possible internal
> network acl SSL_ports port 443 acl Safe_ports port 80      # http 
> acl Safe_ports port 21      # ftp acl Safe_ports port 443     #
> https acl Safe_ports port 70      # gopher acl Safe_ports port 210
> # wais acl Safe_ports port 1025-65535  # unregistered ports acl
> Safe_ports port 280     # http-mgmt acl Safe_ports port 488     #
> gss-http acl Safe_ports port 591     # filemaker acl Safe_ports
> port 777     # multiling http acl CONNECT method CONNECT 
> http_access deny !Safe_ports http_access deny CONNECT !SSL_ports 
> acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> allow myrule cache deny all http_access allow localhost manager 
> http_access deny manager acl Purge method PURGE http_access deny
> Purge http_access allow localhost http_port 3128 coredump_dir
> /var/spool/squid3 refresh_pattern ^ftp:       1440    20% 10080 
> refresh_pattern ^gopher:    1440    0%  1440 refresh_pattern -i
> (/cgi-bin/|\?) 0 0%  0 refresh_pattern (Release|Packages(.gz)*)$
> 0       20%     2880 refresh_pattern .       0   20% 4320
> 
> If helps, iptables is empty and netstat status is $ sudo netstat
> -nltp | grep squid tcp6       0      0 :::3128
> :::* LISTEN      20292/(squid-1)
> 
> Version info: $ sudo apt-cache policy squid3 squid3: Installed:
> 3.3.8-1ubuntu6.1 Candidate: 3.3.8-1ubuntu6.1 Version table: ***
> 3.3.8-1ubuntu6.1 0 500 http://archive.ubuntu.com/ubuntu/
> trusty-updates/main i386 Packages 500
> http://archive.ubuntu.com/ubuntu/ trusty-security/main i386 
> Packages 100 /var/lib/dpkg/status 3.3.8-1ubuntu6 0 500
> http://archive.ubuntu.com/ubuntu/ trusty/main i386 Packages
> 
> Proof that squid is running: $ ps ax | grep squid 20290 ?        Ss
> 0:00 squid3 20292 ?        S      0:06 (squid-1) 31535 ?        S
> 0:00 (logfile-daemon) /var/log/squid3/access.log 31720 pts/28   S+
> 0:00 grep --color=auto squid
> 
> Maybe i used the wrong terminology as "redirect". B/c whenever the
> website in the blocklist is called, localhost page of my LAMP shows
> up.
> 
> So, what i want to achieve in the end is blocking and sometimes
> unblocking a list of websites based on their domain name. Problem
> is even the config is changed to "http_access allow myrule" it
> doesnt reflect allow/deny options accordingly anymore. At this
> moment, the websites in the list are still non-accessible.
> 

Let me guess,  you are testing this with a browser URL
http://192.168.whatever:3128/ or even just http://192.168.whatever/
and it shows your LAMP server page?

You seem to have missed out all the bits of the setup which make the
browser use the proxy rather than just connecting directly to Apache
in the LAMP stack.
You may need to read through this:
 http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYN2bAAoJELJo5wb/XPRj5VMIANLLVIb8kjqk5BiycDBAH1MR
5qA896B3hhcoVEgFIH2lxSzFVBXJBFSwcjXnZN9nkqf5b7/t6k58hY3+56+UfZSF
xO7tHOy/mvtCNA+ol7JlyVz+MvgaKMRqzXdAnJdP3OrhQ4g75WfGKCxoCBBwPNgU
5gD7rSSQq8PpD0uBNuHV8SDwwKkYaoYtoqAs1OWD5p+WbyAylYZB7cJKKgex1+d9
nPqGIlaRLaWRJzcTDFUW0C3B0zIggIv5GRNsO50gqqQZ0Xb9F3Iy5aDOwyiyiCYn
LmlRADaoqf4MWfBh+nmmufcwUcfsAGknI7tStk3dXCzNQNA9O2gy3e7s+H+7Poo=
=A5vS
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 10 15:59:13 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 04:59:13 +1300
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
 <5460BF5A.6030400@treenet.co.nz>
 <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>
Message-ID: <5460E0D1.6000303@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 4:12 a.m., Rietzler, Markus (RZF, SG 324 /
<RIETZLER_SOFTWARE>) wrote:
>> -----Urspr?ngliche Nachricht----- Von: squid-users 
>> [mailto:squid-users-bounces at lists.squid-cache.org] Im Auftrag
>> von Amos Jeffries Gesendet: Montag, 10. November 2014 14:36 An: 
>> squid-users at lists.squid-cache.org Betreff: Re: [squid-users]
>> High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
>> 
>> On 7/11/2014 2:50 a.m., Tom Tom wrote:
>>> Hi
>>> 
>>> After migration from squid 3.3.13 to 3.4.4, I recognized a 
>>> performance-issue. Squid is configured with 4 workers. They 
>>> often have a CPU-Utilization between 50%-90% (each worker).
>>> With squid 3.3.13 (same configuration), the CPU-Utilization was
>>> never a problem. I installed squid 3.4.9 and had the same
>>> issue. No warnings/errors in the cache.log
>>> 
>>> I saw, that someone other reported a similar issue: 
>>> http://www.squid-cache.org/mail-archive/squid-users/201407/0500.html
>>>
>>>
>>>
>>> 
Concerning the post above: Yes, we have external_auth-helpers
>>> (ext_kerberos_ldap_group_acl) and no, we do not use
>>> delay_pools. The high cpu-usage comes not from the auth-helper
>>> - it comes from the 4 squid-worker-processes. Any hints? Is
>>> this a known problem? Probably solved in 3.5?
>> 
>> Are you able to find out any specific details about what the 
>> workers are doing that uses so much extra CPU?
>> 
>> Amos
>> 
> during our last tests (with 3.4.x) we also tried the worker
> option. it does not matter if workers are enabled or not. with more
> workers the cpu rise seems to be somewhat slower. so it is not
> connected to (smp)workers. it is the external auth helper -
> although the squid process and not the helper does consume all the
> cpu...

The only difference between SMP and non-SMP mode here is that non-SMP
has 1 worker doing all the work with one CPU core, whereas SMP mode
has several workers. They can all hit the same issues independently
for the same reason(s).


I am of the understanding that the code associated with the helper
processe is using a lot of CPU doing *something* that consumes a lot
of cycles.

There is a bunch of code doing cache lookups on previous helper
queries, queueing new lookups, generating and parsing strings in the
I/O, and even sometimes running whole trees of ACL logics when the
helper(s) respond.
 So to get anywhere on this complaint it is important to know what
(from the above set of things) exactly the CPU is doing.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYODRAAoJELJo5wb/XPRjvrkH/RBEitKpvLcWypNPCCrMTtw6
tKftf9gz4og6GOkUiipE0qNLTMgWvV7Fk9/By3vhEGNL+WpQG5UEWCbfpc2h2RdL
H5tIWJGnXcsV1PGwYI1cuyDpanNs6EnSvKnSGTZ2DdWabiFEOPr9FR/8QtVqpUdS
EK3uMpnmZ0mbo7auDIPxwa7CYh44tC/C3VMZSto+peB1ikiDonU9B0tXVEFCheeE
B0IWs8FaoYByVd54lL6cYPz7HcOtyt2Hb6uyPJyQVrrEJs2JuI4ZQh0X7B2mbzAi
HK8wBbDcyC4ZKagq4ABQIYHsxwqxiNFD6v9ntXBjZpORG1opXLMSBAh9K0Ycq5s=
=5pNQ
-----END PGP SIGNATURE-----


From schinken at hackerspace-bamberg.de  Mon Nov 10 16:09:41 2014
From: schinken at hackerspace-bamberg.de (schinken)
Date: Mon, 10 Nov 2014 17:09:41 +0100
Subject: [squid-users] NTLM Auth fails while using DNS instead of IP
	address
In-Reply-To: <5460D632.10704@hackerspace-bamberg.de>
References: <5460D632.10704@hackerspace-bamberg.de>
Message-ID: <5460E345.3050400@hackerspace-bamberg.de>

Hi again,

just for documentation: I figured out what the problem was. According to
the previously mentioned configuration example [1] one can use these
encryption modes inside /etc/krb5.conf:

> ; for Windows 2003
>
> default_tgs_enctypes = rc4-hmac des-cbc-crc des-cbc-md
> default_tkt_enctypes = rc4-hmac des-cbc-crc des-cbc-md
> permitted_enctypes = rc4-hmac des-cbc-crc des-cbc-md5

or

> ; for Windows 2008 with AES
>
> default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc
des-cbc-md5
> default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc
des-cbc-md5
> permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc
des-cbc-md5


Actually, if you use the old method (without aes and --enctypes 28), you
only can use the IP adress for your squid server instead of a DNS name.

Btw: One shouldn't use the old method if it's not needed - at least for
security reasons.

[1]
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory

Best,
Schinken

---
Backspace e.V.
http://hackerspace-bamberg.de

mail: schinken at hackerspace-bamberg.de
xmpp: schinken at tai-wahn.de (otr)
GPG: FFB7 E40D B2DD D24C C9B7 B5C5 703C F8B8 882C 871E

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/614f2ebe/attachment.sig>

From merhabakendim at gmail.com  Mon Nov 10 16:12:23 2014
From: merhabakendim at gmail.com (Efe)
Date: Mon, 10 Nov 2014 18:12:23 +0200
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <5460DD9E.8080201@treenet.co.nz>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460BE83.9090100@treenet.co.nz>
 <CA+-XJgUDPErO67zo_CwpCeS5jGamer6HJHKdWSRXZq+o_H2juA@mail.gmail.com>
 <5460DD9E.8080201@treenet.co.nz>
Message-ID: <CA+-XJgU0g9CDFrweoG5hd9E9WeRnNqmWOocxv3n7ouiTeE_TUw@mail.gmail.com>

It's the websites i want to block & unblock occasionaly as in:
squid.conf:
    acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
    http_access deny myrule

where domainblock.txt is

    someaddress.com
    blockthis.net

Now whenever i enter someaddress.com url & hit enter, it (squid or
something on the network level) redirects to the localhost page. That's
fine as long as i want to restrict access to that website.

Problem is, changing config "http_access allow myrule" and reconfiguring,
parsing and restarting squid and then the computer does not change
anything, i.e i still cant access someaddress.com. Removed the settings
from config, no luck again.

As i said, iptables is empty. So what might be the reason then?



On Mon, Nov 10, 2014 at 5:45 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 11/11/2014 2:58 a.m., Efe wrote:
> > Thank you for your reply. I've managed to retrieve uncommented
> > config lines:
> >
> > $ grep -P '^\s*\w' /etc/squid3/squid.conf
> >
> > acl localnet src 192.168.0.101  # RFC1918 possible internal
> > network acl SSL_ports port 443 acl Safe_ports port 80      # http
> > acl Safe_ports port 21      # ftp acl Safe_ports port 443     #
> > https acl Safe_ports port 70      # gopher acl Safe_ports port 210
> > # wais acl Safe_ports port 1025-65535  # unregistered ports acl
> > Safe_ports port 280     # http-mgmt acl Safe_ports port 488     #
> > gss-http acl Safe_ports port 591     # filemaker acl Safe_ports
> > port 777     # multiling http acl CONNECT method CONNECT
> > http_access deny !Safe_ports http_access deny CONNECT !SSL_ports
> > acl myrule dstdom_regex "/etc/squid3/domainblock.txt" http_access
> > allow myrule cache deny all http_access allow localhost manager
> > http_access deny manager acl Purge method PURGE http_access deny
> > Purge http_access allow localhost http_port 3128 coredump_dir
> > /var/spool/squid3 refresh_pattern ^ftp:       1440    20% 10080
> > refresh_pattern ^gopher:    1440    0%  1440 refresh_pattern -i
> > (/cgi-bin/|\?) 0 0%  0 refresh_pattern (Release|Packages(.gz)*)$
> > 0       20%     2880 refresh_pattern .       0   20% 4320
> >
> > If helps, iptables is empty and netstat status is $ sudo netstat
> > -nltp | grep squid tcp6       0      0 :::3128
> > :::* LISTEN      20292/(squid-1)
> >
> > Version info: $ sudo apt-cache policy squid3 squid3: Installed:
> > 3.3.8-1ubuntu6.1 Candidate: 3.3.8-1ubuntu6.1 Version table: ***
> > 3.3.8-1ubuntu6.1 0 500 http://archive.ubuntu.com/ubuntu/
> > trusty-updates/main i386 Packages 500
> > http://archive.ubuntu.com/ubuntu/ trusty-security/main i386
> > Packages 100 /var/lib/dpkg/status 3.3.8-1ubuntu6 0 500
> > http://archive.ubuntu.com/ubuntu/ trusty/main i386 Packages
> >
> > Proof that squid is running: $ ps ax | grep squid 20290 ?        Ss
> > 0:00 squid3 20292 ?        S      0:06 (squid-1) 31535 ?        S
> > 0:00 (logfile-daemon) /var/log/squid3/access.log 31720 pts/28   S+
> > 0:00 grep --color=auto squid
> >
> > Maybe i used the wrong terminology as "redirect". B/c whenever the
> > website in the blocklist is called, localhost page of my LAMP shows
> > up.
> >
> > So, what i want to achieve in the end is blocking and sometimes
> > unblocking a list of websites based on their domain name. Problem
> > is even the config is changed to "http_access allow myrule" it
> > doesnt reflect allow/deny options accordingly anymore. At this
> > moment, the websites in the list are still non-accessible.
> >
>
> Let me guess,  you are testing this with a browser URL
> http://192.168.whatever:3128/ or even just http://192.168.whatever/
> and it shows your LAMP server page?
>
> You seem to have missed out all the bits of the setup which make the
> browser use the proxy rather than just connecting directly to Apache
> in the LAMP stack.
> You may need to read through this:
>  http://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUYN2bAAoJELJo5wb/XPRj5VMIANLLVIb8kjqk5BiycDBAH1MR
> 5qA896B3hhcoVEgFIH2lxSzFVBXJBFSwcjXnZN9nkqf5b7/t6k58hY3+56+UfZSF
> xO7tHOy/mvtCNA+ol7JlyVz+MvgaKMRqzXdAnJdP3OrhQ4g75WfGKCxoCBBwPNgU
> 5gD7rSSQq8PpD0uBNuHV8SDwwKkYaoYtoqAs1OWD5p+WbyAylYZB7cJKKgex1+d9
> nPqGIlaRLaWRJzcTDFUW0C3B0zIggIv5GRNsO50gqqQZ0Xb9F3Iy5aDOwyiyiCYn
> LmlRADaoqf4MWfBh+nmmufcwUcfsAGknI7tStk3dXCzNQNA9O2gy3e7s+H+7Poo=
> =A5vS
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141110/ac9f8f4e/attachment.htm>

From garthl at dcdata.co.za  Mon Nov 10 16:25:41 2014
From: garthl at dcdata.co.za (Garth Lancaster)
Date: Mon, 10 Nov 2014 18:25:41 +0200 (SAST)
Subject: [squid-users] Squid Ecap - Centos 6.6 x86_64
In-Reply-To: <30141292.8127.1415636301296.JavaMail.garthl@Garth>
Message-ID: <21300148.8135.1415636729821.JavaMail.garthl@Garth>

Hi All

Has anyone come across a compile issues for Squid 3+ and Ecap? I've tried the following below.

http://www.e-cap.org/Documentation

squid-3.1 to squid-3.4 with libecap-0.2.0 and libecap-1.0.0

./configure --enable-ecap
./configure --enable-ecap --with-included-ltdl

When I use "make" I keep getting this error. The folder versions just change when I try other versions. I have yum installed many packages I could find just in-case. I have rebooted.

Host.cc: In static member function 'static void Adaptation::Ecap::Host::Register()':
Host.cc:130: error: cannot allocate an object of abstract type 'Adaptation::Ecap::Host'
../../../src/adaptation/ecap/Host.h:17: note:   because the following virtual functions are pure within 'Adaptation::Ecap::Host':
/usr/local/include/libecap/host/host.h:25: note:        virtual void libecap::host::Host::noteVersionedService(const char*, const std::tr1::weak_ptr<libecap::adapter::Service>&)
make[4]: *** [libsquid_ecap_la-Host.lo] Error 1
make[4]: Leaving directory `/usr/src/squid-3.4.9/src/adaptation/ecap'
make[3]: *** [all-recursive] Error 1
make[3]: Leaving directory `/usr/src/squid-3.4.9/src/adaptation'
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory `/usr/src/squid-3.4.9/src'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/usr/src/squid-3.4.9/src'
make: *** [all-recursive] Error 1

Regards
Garth


From Antony.Stone at squid.open.source.it  Mon Nov 10 16:34:11 2014
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 10 Nov 2014 17:34:11 +0100
Subject: [squid-users] Squid3 config on Ubuntu remains even after
	uninstall and ignore the new config
In-Reply-To: <CA+-XJgU0g9CDFrweoG5hd9E9WeRnNqmWOocxv3n7ouiTeE_TUw@mail.gmail.com>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460DD9E.8080201@treenet.co.nz>
 <CA+-XJgU0g9CDFrweoG5hd9E9WeRnNqmWOocxv3n7ouiTeE_TUw@mail.gmail.com>
Message-ID: <201411101734.11744.Antony.Stone@squid.open.source.it>

On Monday 10 November 2014 at 17:12:23 (EU time), Efe wrote:

>     acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
>     http_access deny myrule
> 
> where domainblock.txt is
> 
>     someaddress.com
>     blockthis.net

As Amos said, use dstdomain instead of dstdom_regex.

> Now whenever i enter someaddress.com url & hit enter, it (squid or
> something on the network level) redirects to the localhost page. That's
> fine as long as i want to restrict access to that website.
> 
> Problem is, changing config "http_access allow myrule" and reconfiguring,
> parsing and restarting squid and then the computer does not change
> anything, i.e i still cant access someaddress.com. Removed the settings
> from config, no luck again.
> 
> As i said, iptables is empty. So what might be the reason then?

Local browser cache on the client machine?

Try testing with client A, reconfigure, and then re-test from client B (where A 
and B are either different computers or different browsers, or both).


Regards,


Antony.

-- 
"I find the whole business of religion profoundly interesting.  But it does 
mystify me that otherwise intelligent people take it seriously."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From marcus.kool at urlfilterdb.com  Mon Nov 10 17:41:32 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 10 Nov 2014 15:41:32 -0200
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <5460E0D1.6000303@treenet.co.nz>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
 <5460BF5A.6030400@treenet.co.nz>
 <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>
 <5460E0D1.6000303@treenet.co.nz>
Message-ID: <5460F8CC.5020303@urlfilterdb.com>


>> during our last tests (with 3.4.x) we also tried the worker
>> option. it does not matter if workers are enabled or not. with more
>> workers the cpu rise seems to be somewhat slower. so it is not
>> connected to (smp)workers. it is the external auth helper -
>> although the squid process and not the helper does consume all the
>> cpu...
>
> The only difference between SMP and non-SMP mode here is that non-SMP
> has 1 worker doing all the work with one CPU core, whereas SMP mode
> has several workers. They can all hit the same issues independently
> for the same reason(s).
>
>
> I am of the understanding that the code associated with the helper
> processe is using a lot of CPU doing *something* that consumes a lot
> of cycles.
>
> There is a bunch of code doing cache lookups on previous helper
> queries, queueing new lookups, generating and parsing strings in the
> I/O, and even sometimes running whole trees of ACL logics when the
> helper(s) respond.
>   So to get anywhere on this complaint it is important to know what
> (from the above set of things) exactly the CPU is doing.

Indeed but setting debug_options to ALL,9 does not work since the log file
already is too big and unmanageable even before Squid begins to do
thing that consumes CPU time.

I have a script for a daemon that I wrote. The script is executes when
the daemon receives a fatal signal (e.g. SIGSEGV):  the daemon catches
SIGSEV and executes the script which saves a stack trace (using gdb)
of all threads in a file, and then finally the daemon exits.  It is a
nice debug tool.
Maybe we can make a similar script for Squid that does something
like this:
collect the pids of all processes and then for all pids run gdb
that attaches to the process, dumps a stack trace and detaches.
The script can even do it 25 times to have a better insight in what
squid does.
An administrator can then run the script at the time that CPU peaks
and send the output to the Squid developers.
Do you like the idea?

Another idea is to implement a user-defined signal handler, say
on receipt of SIGUSR2, squid sets the debug_options to ALL,9
without rereading the config file, and after 3 seconds sets
the debug_options back to the configured value.
This way you get a 3-second sample of what Squid is doing at a
specific time and the log file will have a reasonable size.

Marcus

> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUYODRAAoJELJo5wb/XPRjvrkH/RBEitKpvLcWypNPCCrMTtw6
> tKftf9gz4og6GOkUiipE0qNLTMgWvV7Fk9/By3vhEGNL+WpQG5UEWCbfpc2h2RdL
> H5tIWJGnXcsV1PGwYI1cuyDpanNs6EnSvKnSGTZ2DdWabiFEOPr9FR/8QtVqpUdS
> EK3uMpnmZ0mbo7auDIPxwa7CYh44tC/C3VMZSto+peB1ikiDonU9B0tXVEFCheeE
> B0IWs8FaoYByVd54lL6cYPz7HcOtyt2Hb6uyPJyQVrrEJs2JuI4ZQh0X7B2mbzAi
> HK8wBbDcyC4ZKagq4ABQIYHsxwqxiNFD6v9ntXBjZpORG1opXLMSBAh9K0Ycq5s=
> =5pNQ
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Mon Nov 10 18:15:21 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 10 Nov 2014 20:15:21 +0200
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <5460F8CC.5020303@urlfilterdb.com>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
 <5460BF5A.6030400@treenet.co.nz>
 <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>
 <5460E0D1.6000303@treenet.co.nz> <5460F8CC.5020303@urlfilterdb.com>
Message-ID: <546100B9.7000102@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/10/2014 07:41 PM, Marcus Kool wrote:
> 
> Indeed but setting debug_options to ALL,9 does not work since the
> log file already is too big and unmanageable even before Squid
> begins to do thing that consumes CPU time.

I have suggested a full one request cycle comparison but not everybody
can follow this path easily.

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYQC5AAoJENxnfXtQ8ZQUgKcH/1fblv01RrTrXMkViqPYKVSr
zLfjwV+4EsGjmMc3BC3qPXljcVJj2GI/8++TBzdPZzPvqzVoES/ZpySOpfhZ8A5N
+vVKOIoxwi8008Vkp0P3q8snxdcVLWlcWM5FCxYvWpSuy7XNdcYm3BPefkZU+HM4
utvpvE35JBeejp3PtwOVya1J8nVX18lvusjExZSgIHkepcwS429dXCu3cNedQ5RZ
0yKdKLcYlwh7EHJ7zpAf1NZJxg7NwX4i46tmkWJBDxsae+x4UkLCNiJCUwrdaVuQ
Jc4RhblQjWqH1ZWHFHyfKpeOMy5xpjAe2jbqDrLE+f2sz0X54Ci2mvuNyninmSY=
=HP6z
-----END PGP SIGNATURE-----


From Jason_Haar at trimble.com  Mon Nov 10 20:19:01 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 11 Nov 2014 09:19:01 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <5460BB87.5010403@treenet.co.nz>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
 <546092E3.20607@trimble.com> <54609C2C.5090105@treenet.co.nz>
 <5460A0C4.4010000@trimble.com> <5460BB87.5010403@treenet.co.nz>
Message-ID: <54611DB5.2050207@trimble.com>

I applied the patch and now it works! I can transparently access port
443-based websites with ssl-bump :-)

Thanks Amos :-)


On 11/11/14 02:20, Amos Jeffries wrote:
>
> You have an urlpath_regex ACL test depending on URIs containing paths.
> Which is not the case with CONNECT.
>
> The attached patch should fix the crash.
>
> Amos


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From oleg at ims.ca  Mon Nov 10 20:28:28 2014
From: oleg at ims.ca (Oleg Chomenko)
Date: Mon, 10 Nov 2014 15:28:28 -0500
Subject: [squid-users] Squid 3.3.12, Multiple process,
	requests serviced by process.
Message-ID: <CAHTi2bF0Z01igs1hfUV9WjXWT5YJg+HnFTu2JGxgAYsqm1qTOA@mail.gmail.com>

Hello,

We use a squid cache for our robots to collects an information from
client's web sites.

The squid running on FreeBSD 9.3 , squid version 3.3.13

the configuration is like this:

if ${process_number} = 1
http_port 3001
cache_peer 1.1.1.1 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.2 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.3 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.4 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.5 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
endif

if ${process_number} = 2
http_port 3001
cache_peer 1.1.1.1 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.2 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.3 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.4 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.1.5 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
endif


if ${process_number} = 3
http_port 3002
cache_peer 1.1.2.1 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.2 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.3 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.4 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.5 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
endif

if ${process_number} = 4
http_port 3002
cache_peer 1.1.2.1 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.2 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.3 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.4 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
cache_peer 1.1.2.5 parent 4567 0 no-query no-digest no-netdb-exchange
round-robin connect-fail-limit=3
endif

.....
# COORDINATOR
if ${process_number} = 16
http_port 3099
endif

workers 15


in total 15+1 processes is running, traffic load over 100 Mbit; around
50K req/min (total #)

Problem is:
when we restart the squid all request to port 3001 do serve only
upstream proxy defined for this process. after couple hours, we see
request served by upstream cache NOT belonged to this 3001 ports.  (
like in example above can served by 1.1.2.4)

The rate depend on the load, up to 15% all requests can be served by
others upstream proxy NOT belonged to this port
we use a java application and our website to logging all requests we
generating and passing trough the cache server.

This behavior is a serious trouble for us .....

Thanks in advance for any tips to solve it (Thinking it an internal
request distribution mechanism produce a fault )

-- 
This electronic message, including any attachments, may contain 
proprietary, confidential or privileged information for the sole use of the 
intended recipient(s). You are hereby notified that any unauthorized 
disclosure, copying, distribution, or use of this message is prohibited. 
 If you have received this message in error, please immediately notify the 
sender by reply e-mail and delete it.


From squid3 at treenet.co.nz  Tue Nov 11 03:25:10 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 16:25:10 +1300
Subject: [squid-users] sslbump working with 3.4.9 but not in intercept
 mode?
In-Reply-To: <54611DB5.2050207@trimble.com>
References: <5460829D.9000104@trimble.com> <54608D2F.5050704@treenet.co.nz>
 <546092E3.20607@trimble.com> <54609C2C.5090105@treenet.co.nz>
 <5460A0C4.4010000@trimble.com> <5460BB87.5010403@treenet.co.nz>
 <54611DB5.2050207@trimble.com>
Message-ID: <54618196.30308@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 9:19 a.m., Jason Haar wrote:
> I applied the patch and now it works! I can transparently access
> port 443-based websites with ssl-bump :-)
> 
> Thanks Amos :-)
> 
> 

Thank you for the report and quick feedback.
This will be fixed in the next release.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYYGWAAoJELJo5wb/XPRja0AH/AqPkb2XzAy5wNbvhy7PbAcz
jqq51gpNXsIkJ+btzmCIYD61OqnC9nairqiUA/KoRBPtP7QO9e/xrblcs4GfTkS+
KLXFFGGSBUqKbQ5PYqMLfvjIvmKzJCfIIdDDCvOMUY+pcCHTtGFqZ02sXxAgiTSk
4F4/sauTRlkmAkZpL11l9Di31NVI4vkoptyF4PMHq5/TmHp4qya4GDSOdYRq5KF9
/lgWLHeMRkhpvXOwuoJ08Cn7/4p8p3aeI+fwhFa7aDLW1u01OHLhmscEaHG42gpP
Fcguuvu8n+UR2ARC/+5c6iZz2yIxYnrFT1JC4iAEA722ew/wohEvsrumzasViiA=
=s0aP
-----END PGP SIGNATURE-----


From Jason_Haar at trimble.com  Tue Nov 11 04:00:37 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 11 Nov 2014 17:00:37 +1300
Subject: [squid-users] https intercept breaks non-HTTPS port 443 traffic?
Message-ID: <546189E5.1010702@trimble.com>

Hi there

Now that I've got ssl-bump working with port 443 intercept, I now find
non-HTTPS apps that operate on port 443 no longer work. eg for ssl-bump
in standard proxy mode I had an ACL to disable bump when an application
(like Skype, which doesn't use HTTPS) tried CONNECT-ing to ip addresses,
but with intercept mode that needed to be removed as all outbound https
intercepted sessions begin with them being to an ip address.

I just brought up a remote SSH server on port 443 and when I try to
telnet to it, instead of getting the OpenSSH banner, I see nothing, but
the remote server receives a SSL transaction from squid. All makes
sense, but is there a way for bump to "fail open" on non-SSL traffic? I
see squid 3.5 mentions "peek" and "at_step" - are those components going
to be the mechanism to solve this issue? Just curious, I'm only
testing/playing with intercepting port 443, but it's interesting to see
where this is going

Finally, when I attempted this connection, cache.log reported

fwdNegotiateSSL: Error negotiating SSL connection on FD 25:
error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol (1/-1/0)

I guess that's it squealing about getting non-SSL content back from the
server (ie the SSH banner). Shouldn't that be a bit more verbose - to
help sysadmins figure out what was behind it. eg

fwdNegotiateSSL: Error negotiating SSL connection from
192.168.22.11:44382 -> 1.2.3.4:443 (FD 25): error:140770FC:SSL
routines:SSL23_GET_SERVER_HELLO:unknown protocol (1/-1/0)

At the very least, with that I could have a cronjob grep through my
cache.log to auto-create a "bump none" acl ;-)

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Tue Nov 11 04:42:19 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Nov 2014 17:42:19 +1300
Subject: [squid-users] https intercept breaks non-HTTPS port 443 traffic?
In-Reply-To: <546189E5.1010702@trimble.com>
References: <546189E5.1010702@trimble.com>
Message-ID: <546193AB.4090108@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 5:00 p.m., Jason Haar wrote:
> Hi there
> 
> Now that I've got ssl-bump working with port 443 intercept, I now
> find non-HTTPS apps that operate on port 443 no longer work. eg for
> ssl-bump in standard proxy mode I had an ACL to disable bump when
> an application (like Skype, which doesn't use HTTPS) tried
> CONNECT-ing to ip addresses, but with intercept mode that needed to
> be removed as all outbound https intercepted sessions begin with
> them being to an ip address.
> 
> I just brought up a remote SSH server on port 443 and when I try
> to telnet to it, instead of getting the OpenSSH banner, I see
> nothing, but the remote server receives a SSL transaction from
> squid. All makes sense, but is there a way for bump to "fail open"
> on non-SSL traffic? I see squid 3.5 mentions "peek" and "at_step" -
> are those components going to be the mechanism to solve this issue?
> Just curious, I'm only testing/playing with intercepting port 443,
> but it's interesting to see where this is going

Since you are experimenting and learning ... it is very important at
present to know where the HTTPS support is coming from as well as
going to.

In particular that for classical HTTPS support the proxy offloads all
SSL configuration setup and port 443 traffic to the OpenSSL library.
So Squid-3.1 and older never had anything at all to do with the
handshakes.

Bumping is gradually changing that, with Squid taking on more and more
responsibility for ever lower levels of the TLS encryption. Initial
client-first mode jumped in to replace the ServerHello wth a static
cert. Then client-first/server-first +mimic taking responsibility for
the high level certificate exchange process. Now 3.5 peek-n-splice
makes Squid responsible for the individual low-level bytes being
transferred around in TLS - though mostly as a spy-in-the-middle.
 Its an arms race, with Squid (in the persons of Measurement Factory
and sponsors) vs the entire TLS security community.

Personally I think we should have just applied QoS limiting on CONNECT
tunnels or started issuing 426 responses mandating TLS to the proxy.
But sponsors pay the bills and that kind of paradigm shift leads to
some obvious pain, whereas ssl-bump hides the pain until reality bites
back.

> 
> Finally, when I attempted this connection, cache.log reported
> 
> fwdNegotiateSSL: Error negotiating SSL connection on FD 25: 
> error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
> (1/-1/0)
> 
> I guess that's it squealing about getting non-SSL content back from
> the server (ie the SSH banner). Shouldn't that be a bit more
> verbose - to help sysadmins figure out what was behind it. eg
> 
> fwdNegotiateSSL: Error negotiating SSL connection from 
> 192.168.22.11:44382 -> 1.2.3.4:443 (FD 25): error:140770FC:SSL 
> routines:SSL23_GET_SERVER_HELLO:unknown protocol (1/-1/0)
> 
> At the very least, with that I could have a cronjob grep through
> my cache.log to auto-create a "bump none" acl ;-)

That is a direct result specific to the server-first logic in your
Squid version being applied to the traffic. It will change if
client-first is applied, and as version upgrades change the logic
itself. Neat idea though :-)

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYZOoAAoJELJo5wb/XPRjfRYIAJQ4CjWUpj+ZMw8ViLgeeu3D
hQHPbx9ln1NXAyk9KwiEGdwEBuVe6rTk4X3q+fCvdl8/1kHdVrRw3mNeIIJJRW8h
gbH6h0E+Oi6tL6jBs4pMsTqSjCsNK01nQtkDylHIVDKPLo7KUUyhJWBjiy4npxoq
VC37ufs8vtqrU76d7seTBGOCRWHQ+DoOAv+kPzJDQ3Z9YOfd/peD8qxfhHiHbw3R
fmkOh8n66f8qDwKUtnJjTUqvww3/akrAKFlIFnTcWvsED9vwHnxrNChTg8rdq27n
DgVF9GY5QZ3bFsScmMCQbl96lPE12SE+jIZU6aYrKOxCEC/D5izGBb5Dsmt3peI=
=+JVQ
-----END PGP SIGNATURE-----


From schinken at hackerspace-bamberg.de  Tue Nov 11 09:33:40 2014
From: schinken at hackerspace-bamberg.de (schinken)
Date: Tue, 11 Nov 2014 10:33:40 +0100
Subject: [squid-users] Fallback auth method
In-Reply-To: <5461D689.7050609@hackerspace-bamberg.de>
References: <5461D689.7050609@hackerspace-bamberg.de>
Message-ID: <5461D7F4.1040302@hackerspace-bamberg.de>


Hi there,

i'm trying to use basic_ncsa_auth as a fallback to my ntlm/kerberos and
LDAP authentification.


The problem here is, that even if my user is successfully authenticated
by ncsa_auth, its denied by the memberof external_acl rule.

Is there a way to skip this acl rule if ncsa_auth was the authenticator?


My configuration looks like this:

> # Negotiate Kerberos and NTLM
> 
> auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=NT-DOMAINNAME --kerberos /usr/lib/squid3/negotiate_kerberos_auth -s GSS_C_NO_NAME
> ...
> 
> # NTLM Authentication
> 
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=NT-DOMAINNAME
> ...
> 
> # LDAP/ActiveDirectory
> 
> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "dc=COMPANY,dc=int" -D squid at company.int -W /etc/
> squid3/ldappass.txt -f sAMAccountName=%s -h ad.company.int,ad3.company.int
> ....
> 
> # basic-auth
> 
> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd
> 
> 
> # AD memberof check
> 
> external_acl_type memberof ttl=300 negative_ttl=300 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -b "dc=COMPANY,dc=i
> nt" -D squid at company.int -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)(memberof:1.2.
> 840.113556.1.4.1941:=cn=%g,ou=Groups,ou=foobar,dc=COMPANY,dc=int))" -h ad.company.int,ad3.company.int
>
> acl auth proxy_auth REQUIRED
> http_access deny !auth
> http_access allow auth
>
> acl AllowedMemberOf external memberof "/etc/squid3/memberof_allow.txt
> acl BlockedMemberOf external memberof "/etc/squid3/memberof_deny.txt"
>
> http_access allow AllowedMemberOf all
> http_access deny BlockedMemberOf all

Best,
Schinken

---
Backspace e.V.
http://hackerspace-bamberg.de

mail: schinken at hackerspace-bamberg.de
xmpp: schinken at tai-wahn.de (otr)
GPG: FFB7 E40D B2DD D24C C9B7 B5C5 703C F8B8 882C 871E





-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141111/ca9bcfa2/attachment.sig>

From vdoctor at neuf.fr  Tue Nov 11 10:33:08 2014
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 11 Nov 2014 02:33:08 -0800 (PST)
Subject: [squid-users] Squid 3.4.x Videos/Music Booster
In-Reply-To: <1412234862102-4667630.post@n4.nabble.com>
References: <1401445017211-4666154.post@n4.nabble.com>
 <1402342920545-4666272.post@n4.nabble.com>
 <CAOoxthOFdUyjpPcyNYx6maQ=8EJuX8LCm6EjFRQ6ySt4fUaRXQ@mail.gmail.com>
 <CAOoxthNhLP26KUWVy+OpPobQgb6oeU-Y7cQ+dMvghXXWfNrz=w@mail.gmail.com>
 <1402470426333-4666302.post@n4.nabble.com>
 <1402558228027-4666310.post@n4.nabble.com>
 <1412234862102-4667630.post@n4.nabble.com>
Message-ID: <1415701988960-4668310.post@n4.nabble.com>

Hi All,

New  release 1.07 <https://sourceforge.net/projects/squidvideosbooster/>  
including the Netflix video streams, all countries...

Enjoy 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-4-x-Videos-Music-Booster-tp4666154p4668310.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From merhabakendim at gmail.com  Tue Nov 11 12:37:38 2014
From: merhabakendim at gmail.com (Efe)
Date: Tue, 11 Nov 2014 14:37:38 +0200
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <201411101734.11744.Antony.Stone@squid.open.source.it>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460DD9E.8080201@treenet.co.nz>
 <CA+-XJgU0g9CDFrweoG5hd9E9WeRnNqmWOocxv3n7ouiTeE_TUw@mail.gmail.com>
 <201411101734.11744.Antony.Stone@squid.open.source.it>
Message-ID: <CA+-XJgWzg1M=7_6yFs0EfXSv9FUvbWoUsw5YYqOaTMmd9hPjKw@mail.gmail.com>

I tried on different browser but it's the same. Clearing browser cache and
stopping the service made no difference either. I don't know if Squid does
change Ubuntu network settings. Any particular place/file to check it out?

On Mon, Nov 10, 2014 at 6:34 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Monday 10 November 2014 at 17:12:23 (EU time), Efe wrote:
>
> >     acl myrule dstdom_regex "/etc/squid3/domainblock.txt"
> >     http_access deny myrule
> >
> > where domainblock.txt is
> >
> >     someaddress.com
> >     blockthis.net
>
> As Amos said, use dstdomain instead of dstdom_regex.
>
> > Now whenever i enter someaddress.com url & hit enter, it (squid or
> > something on the network level) redirects to the localhost page. That's
> > fine as long as i want to restrict access to that website.
> >
> > Problem is, changing config "http_access allow myrule" and reconfiguring,
> > parsing and restarting squid and then the computer does not change
> > anything, i.e i still cant access someaddress.com. Removed the settings
> > from config, no luck again.
> >
> > As i said, iptables is empty. So what might be the reason then?
>
> Local browser cache on the client machine?
>
> Try testing with client A, reconfigure, and then re-test from client B
> (where A
> and B are either different computers or different browsers, or both).
>
>
> Regards,
>
>
> Antony.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141111/7240dc19/attachment.htm>

From Job at colliniconsulting.it  Tue Nov 11 14:20:32 2014
From: Job at colliniconsulting.it (Job)
Date: Tue, 11 Nov 2014 15:20:32 +0100
Subject: [squid-users] Problem with Squid 3.4 and transparent SSL proxy
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>

Hello,

i initialize correctly SSL Bump with Squid 3.4.4, following some guides.
In iptables i redirect 80 and 443 ports to squid ports.

Squid starts with no error, lines involving SSL bump are the following:

http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key

But no request arrives to squid.

If i telnet, from Linux machine, this:

telnet localhost 3128 or telnet localhost 3129, even though the socket is open (netstat -avn | grep 3128 and 3129), connection close immediately.

I see no errors in cache.log, access.log and messages.

Thank you
Francesco

From eliezer at ngtech.co.il  Tue Nov 11 14:31:03 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 11 Nov 2014 16:31:03 +0200
Subject: [squid-users] Problem with Squid 3.4 and transparent SSL proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>
Message-ID: <54621DA7.2060002@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey,

Your configuration seems to not include any iptables and other
relevant details.
What is this machine details?

Eliezer

On 11/11/2014 04:20 PM, Job wrote:
> Hello,
> 
> i initialize correctly SSL Bump with Squid 3.4.4, following some
> guides. In iptables i redirect 80 and 443 ports to squid ports.
> 
> Squid starts with no error, lines involving SSL bump are the
> following:
> 
> http_port 3128 intercept https_port 3129 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
> 
> But no request arrives to squid.
> 
> If i telnet, from Linux machine, this:
> 
> telnet localhost 3128 or telnet localhost 3129, even though the
> socket is open (netstat -avn | grep 3128 and 3129), connection
> close immediately.
> 
> I see no errors in cache.log, access.log and messages.
> 
> Thank you Francesco

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYh2nAAoJENxnfXtQ8ZQUoEEIAI71G38BNCtTTyeGeNB0VHu8
0r2ta5KZKcFLcI3NxcyHN6ygKatSk1zkZQu8uzFOlPIrrAQ1bvql1shpq5vhPjLw
8T8IGEeiULrhx5ms+6ErgvB8sg3wkq1Z+jyJ4Q40lgcPU07ncXzBOyWV5ODaSFXC
zYPII8hrtVH0taPgJpW35XcNb/0htyjxdtXbEs3ZCoAmXLwJQsRfHmdeSdn0Am+Y
swDybjHpMsaf90SJUVFZN3uDLVxKOcMBVLhbCpWt50g+lsJcQeNCZ4xo2QaRURxT
c2lfQD4h1k3ck52r/70dtMZzwTYnoSymyfEGp5zUh8yYSzmd2moDC2z89PEGSQI=
=2uuM
-----END PGP SIGNATURE-----


From Job at colliniconsulting.it  Tue Nov 11 15:06:57 2014
From: Job at colliniconsulting.it (Job)
Date: Tue, 11 Nov 2014 16:06:57 +0100
Subject: [squid-users]  Problem with Squid 3.4 and transparent SSL proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>

Hello Elizier,

first of all thank you for your patience and help!
I use this directives in iptables:

iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3128 (for http)
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3129 (for https)

In a normal http-only transparent proxy everything works fine, but i would like to implement ssl bump for proxying transparently https connection.

When telnetting 3128 or 3129 mode, from Linux machine shell, it seems that connection fails.
When telnetting 3128 port not in interception mode (for standard http transparent proxying), the socket opens and stay connected!

The squid.conf seciont regarding SSL:

http_port 3128
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
sslcrtd_children 50 startup=5 idle=1
ssl_bump server-first all


Thank you again,
Francesco


________________________________________
Da: squid-users [squid-users-bounces at lists.squid-cache.org] per conto di Eliezer Croitoru [eliezer at ngtech.co.il]
Inviato: marted? 11 novembre 2014 15.31
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] Problem with Squid 3.4 and transparent SSL proxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey,

Your configuration seems to not include any iptables and other
relevant details.
What is this machine details?

Eliezer

On 11/11/2014 04:20 PM, Job wrote:
> Hello,
>
> i initialize correctly SSL Bump with Squid 3.4.4, following some
> guides. In iptables i redirect 80 and 443 ports to squid ports.
>
> Squid starts with no error, lines involving SSL bump are the
> following:
>
> http_port 3128 intercept https_port 3129 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>
> But no request arrives to squid.
>
> If i telnet, from Linux machine, this:
>
> telnet localhost 3128 or telnet localhost 3129, even though the
> socket is open (netstat -avn | grep 3128 and 3129), connection
> close immediately.
>
> I see no errors in cache.log, access.log and messages.
>
> Thank you Francesco

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUYh2nAAoJENxnfXtQ8ZQUoEEIAI71G38BNCtTTyeGeNB0VHu8
0r2ta5KZKcFLcI3NxcyHN6ygKatSk1zkZQu8uzFOlPIrrAQ1bvql1shpq5vhPjLw
8T8IGEeiULrhx5ms+6ErgvB8sg3wkq1Z+jyJ4Q40lgcPU07ncXzBOyWV5ODaSFXC
zYPII8hrtVH0taPgJpW35XcNb/0htyjxdtXbEs3ZCoAmXLwJQsRfHmdeSdn0Am+Y
swDybjHpMsaf90SJUVFZN3uDLVxKOcMBVLhbCpWt50g+lsJcQeNCZ4xo2QaRURxT
c2lfQD4h1k3ck52r/70dtMZzwTYnoSymyfEGp5zUh8yYSzmd2moDC2z89PEGSQI=
=2uuM
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Tue Nov 11 15:23:36 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 04:23:36 +1300
Subject: [squid-users] Squid3 config on Ubuntu remains even after
 uninstall and ignore the new config
In-Reply-To: <CA+-XJgWzg1M=7_6yFs0EfXSv9FUvbWoUsw5YYqOaTMmd9hPjKw@mail.gmail.com>
References: <CA+-XJgX21YU+mko+N4MOiHsVxDHOsAh6zZYq804KaGZPPG5hCg@mail.gmail.com>
 <5460DD9E.8080201@treenet.co.nz>
 <CA+-XJgU0g9CDFrweoG5hd9E9WeRnNqmWOocxv3n7ouiTeE_TUw@mail.gmail.com>
 <201411101734.11744.Antony.Stone@squid.open.source.it>
 <CA+-XJgWzg1M=7_6yFs0EfXSv9FUvbWoUsw5YYqOaTMmd9hPjKw@mail.gmail.com>
Message-ID: <546229F8.2060608@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 1:37 a.m., Efe wrote:
> I tried on different browser but it's the same. Clearing browser
> cache and stopping the service made no difference either. I don't
> know if Squid does change Ubuntu network settings. Any particular
> place/file to check it out?

Lots of places.

Right now you sound like someone guessing blindly about things and
making assumptions. It is clear that at least one of those assumptions
about how things work or break is wrong.

I suggest you dig deep and follow the TCP packets. Find out where they
are actually going. Starting with your browser (its network config
settings) and working outward across the network checking if you can
see the packets flowing in the place you expect. Take it slow, one hop
at a time.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYin4AAoJELJo5wb/XPRj+X0H/jlCIrKkIQiOrD7KLDzs79mC
fTVdyndUjLfHFwQTf+FBE8RsSd2pjNYwgMFytYhIP3h2qK8A0xIxGcS0wPGa+UYe
hW0lNyPOeledybOJ47I5/EiNTWafDyv5vq5PVj7nOMdoIlyAfwKTuKNOACFYqfR3
pMvvh+NzqWv2l9DUarqaksnqMiJxv2S+muMmSyMwJgBpp2Cu8tQAj28N95FguQZQ
GiOr0w13MnDx/qi3dncALEZMZmTaqJDG/8UJRDGK7b9BFafhXyt1ewUSfiFjSdMu
uomDI4ZpYqUYsdAF6wXYXfWWYx26mihvQ3iKNy8rIfn30tF1YDzj/DASAOsO0fU=
=6mR9
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Nov 11 15:30:38 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 04:30:38 +1300
Subject: [squid-users] Problem with Squid 3.4 and transparent SSL proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>
Message-ID: <54622B9E.6010308@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 4:06 a.m., Job wrote:
> Hello Elizier,
> 
> first of all thank you for your patience and help! I use this
> directives in iptables:
> 
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT
> --to-port 3128 (for http) iptables -t nat -A PREROUTING -p tcp
> --dport 443 -j REDIRECT --to-port 3129 (for https)
> 
> In a normal http-only transparent proxy everything works fine, but
> i would like to implement ssl bump for proxying transparently https
> connection.
> 
> When telnetting 3128 or 3129 mode, from Linux machine shell, it
> seems that connection fails. When telnetting 3128 port not in
> interception mode (for standard http transparent proxying), the
> socket opens and stay connected!

Do you mean you are telnet'ing *directly* to a port which is expecting
to receive NAT'd traffic and is also configured to use the NAT packet
details to contact some server?

With proxy of all types you MUST test them from the same
context/perspective the real clients traffic would be using.

That means in your case avoid directly connecting to the intercepting
port. Connect to port 80/443 on some Internet server instead and see
if the packets are properly delivered through Squid.
 Also, avoid telnet for the 443 tests. Use an HTTPS client.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYiueAAoJELJo5wb/XPRj3V8H/3w8d6ahHFfiXIxIa8Y9MyJJ
BziDhnD7CAWnrfuiVWq05Ubr3ZASLh9vJQze0gq7jaJ/sptm92jjF6gkxRWhKNM+
sBDYIA5zyfr9BhYolIus39BeHaQGAu0640gAGd4SeGV4zp5ZugZMa0BV+dNlACIW
m+qe+SvXX8Uz9aiPLCvOdDKykAD7PP9FrqQCzJ0veYqOlVq0inXMf3Jt3hlN5djp
fAu7n7WdG/E5SRki+BaG2RAZ9MQr3uf0WhyNfKLfcvFAyOTCTTLQOeONFDsaMO9C
r2PEk7pfwMss5Q8tu9hBIKoJWEx411Z7wMZsWcoEuXS5a7VrXBoLy2lmMUr6cPw=
=nnfH
-----END PGP SIGNATURE-----


From Job at colliniconsulting.it  Tue Nov 11 16:40:45 2014
From: Job at colliniconsulting.it (Job)
Date: Tue, 11 Nov 2014 17:40:45 +0100
Subject: [squid-users] R: Problem with Squid 3.4 and transparent SSL proxy
In-Reply-To: <54622B9E.6010308@treenet.co.nz>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>

>That means in your case avoid directly connecting to the intercepting
>port. Connect to port 80/443 on some Internet server instead and see
if> the packets are properly delivered through Squid.
 >Also, avoid telnet for the 443 tests. Use an HTTPS client.

Hello Amos and thank you, first of all.

I started squid in debug mode and now i see it:

2014/11/11 17:40:17| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=192.168.10.254:3130 remote=192.168.10.109:52024 FD 12 flags=33: (92) Protocol not available

192.168.10.254 is lan-firewall gateway
192.168.10.109 is the workstation where i am trying to surfing on 443 port

When redirecting the 443 port to squid https_port, errors appears.

Thank you!
Francesco

From pag at nanosec.com  Tue Nov 11 18:47:19 2014
From: pag at nanosec.com (Peter Gross)
Date: Tue, 11 Nov 2014 11:47:19 -0700
Subject: [squid-users] Problem with https://www.google.com and squid
	interception
Message-ID: <546259B7.3020103@nanosec.com>

Hi,
I am a new user of Squid and would first like to thank the developers 
for this excellent software. This is my first post to the mailing list 
... I have been tasked with setting up quite restrictive web access 
control at work. I plan to use an intercepting squid proxy with SSL 
bump. There will also be WCCPv2 to/from a Cisco IOS router. Since this 
is quite a bit of complexity, I though it prudent to start slowly, in 
steps. So first -- to get my feet wet -- I set up squid (version 3.4.8, 
built using rpmbuild from the src rpm from ngtech) on a home linux 
server (Centos 5.11 -- no Cisco at home) which is also the firewall 
router for my home network. I also decided to start out with plain 
vanilla proxying (no interception -- use browser setting). This worked 
fine. I then tested HTTP interception by changing squid.conf from:
http_port 3128
   -to-
http_port 3128 intercept

and adding the following rule to my shorewall firewall:
REDIRECT:info   loc:192.168.101.9       3128    tcp     http

I wanted to test intercepting just one host before turning it on for all 
hosts and wireless devices in my network.

192.168.101.9 is another Centos PC on my network. Squid is running on 
192.168.101.253.

The interception seemed to work fine ... access.log showed lots of 
successful proxy activity. Then came the problem: going to 
https://www.google.com failed (not every time, but frequently). If I 
turned off the REDIRECT line in the shorewall rules file and restarted 
shorewall, no problem. This seemed very peculiar because no HTTPS 
traffic should be redirected to the proxy. Here are the errors that 
showed up in cache.log when redirection (NAT-ing) was on:

2014/11/11 11:03:42 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on 
local=192.168.101.253:3128 remote=192.168.101.9:34165 FD 11 flags=33: 
(92) Protocol not available

Note that other HTTPS sites worked fine! It appears to be confined to a 
google specific issue.

Thanks for any comments/suggestions you might have,
--peter





From squid3 at treenet.co.nz  Wed Nov 12 03:16:37 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 16:16:37 +1300
Subject: [squid-users] Squid 3.3.12, Multiple process,
 requests serviced by process.
In-Reply-To: <CAHTi2bF0Z01igs1hfUV9WjXWT5YJg+HnFTu2JGxgAYsqm1qTOA@mail.gmail.com>
References: <CAHTi2bF0Z01igs1hfUV9WjXWT5YJg+HnFTu2JGxgAYsqm1qTOA@mail.gmail.com>
Message-ID: <5462D115.1090507@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/11/2014 9:28 a.m., Oleg Chomenko wrote:
> Hello,
> 
> We use a squid cache for our robots to collects an information
> from client's web sites.
> 
> The squid running on FreeBSD 9.3 , squid version 3.3.13
> 
> the configuration is like this:
> 
> if ${process_number} = 1 http_port 3001 cache_peer 1.1.1.1 parent
> 4567 0 no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 cache_peer 1.1.1.2 parent 4567 0 no-query
> no-digest no-netdb-exchange round-robin connect-fail-limit=3 
> cache_peer 1.1.1.3 parent 4567 0 no-query no-digest
> no-netdb-exchange round-robin connect-fail-limit=3 cache_peer
> 1.1.1.4 parent 4567 0 no-query no-digest no-netdb-exchange 
> round-robin connect-fail-limit=3 cache_peer 1.1.1.5 parent 4567 0
> no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 endif
> 
> if ${process_number} = 2 http_port 3001 cache_peer 1.1.1.1 parent
> 4567 0 no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 cache_peer 1.1.1.2 parent 4567 0 no-query
> no-digest no-netdb-exchange round-robin connect-fail-limit=3 
> cache_peer 1.1.1.3 parent 4567 0 no-query no-digest
> no-netdb-exchange round-robin connect-fail-limit=3 cache_peer
> 1.1.1.4 parent 4567 0 no-query no-digest no-netdb-exchange 
> round-robin connect-fail-limit=3 cache_peer 1.1.1.5 parent 4567 0
> no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 endif
> 
> 
> if ${process_number} = 3 http_port 3002 cache_peer 1.1.2.1 parent
> 4567 0 no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 cache_peer 1.1.2.2 parent 4567 0 no-query
> no-digest no-netdb-exchange round-robin connect-fail-limit=3 
> cache_peer 1.1.2.3 parent 4567 0 no-query no-digest
> no-netdb-exchange round-robin connect-fail-limit=3 cache_peer
> 1.1.2.4 parent 4567 0 no-query no-digest no-netdb-exchange 
> round-robin connect-fail-limit=3 cache_peer 1.1.2.5 parent 4567 0
> no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 endif
> 
> if ${process_number} = 4 http_port 3002 cache_peer 1.1.2.1 parent
> 4567 0 no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 cache_peer 1.1.2.2 parent 4567 0 no-query
> no-digest no-netdb-exchange round-robin connect-fail-limit=3 
> cache_peer 1.1.2.3 parent 4567 0 no-query no-digest
> no-netdb-exchange round-robin connect-fail-limit=3 cache_peer
> 1.1.2.4 parent 4567 0 no-query no-digest no-netdb-exchange 
> round-robin connect-fail-limit=3 cache_peer 1.1.2.5 parent 4567 0
> no-query no-digest no-netdb-exchange round-robin
> connect-fail-limit=3 endif
> 
> ..... # COORDINATOR if ${process_number} = 16 http_port 3099 endif
> 
> workers 15
> 
> 
> in total 15+1 processes is running, traffic load over 100 Mbit;
> around 50K req/min (total #)
> 
> Problem is: when we restart the squid all request to port 3001 do
> serve only upstream proxy defined for this process. after couple
> hours, we see request served by upstream cache NOT belonged to this
> 3001 ports.  ( like in example above can served by 1.1.2.4)
> 
> The rate depend on the load, up to 15% all requests can be served
> by others upstream proxy NOT belonged to this port we use a java
> application and our website to logging all requests we generating
> and passing trough the cache server.

Are these 15% HITs ?
 Do you have shared memory cache enabled ?

If a worker 1 transaction through cache_peer 1.1.1.4 gets cached and
shared between workers its response might be re-used by worker 3 or 4
when they get the same URL requested.

or, are they actually reaching those backend cache_peer servers from a
worker ?

> 
> This behavior is a serious trouble for us .....
> 
> Thanks in advance for any tips to solve it (Thinking it an
> internal request distribution mechanism produce a fault )
> 

If you need the shared caching, but to have it split between worker
1&2 and 3&4 then you will want to look at the named services feature
in Squid-3.5.
 http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.3

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYtEVAAoJELJo5wb/XPRjr40H/30boJ5IM/w/vtezpIbR+kv1
B5ae+vPyAwEa5a53Q6yRxgoK+qQ7sZWi6bsCtpjyimSMuzp4HVwrZ+ebz1GTZdKb
47Uyeu9IfTu0xB2cZi8usNB/cLfox15pk48gZJqoB3Vzav749PVvm/Y1gR0IunEO
MuqRLr4DkUMBROHwKNIfZLHTG6wfNtMT/hwFgZt5dX37J3PAh4Z+ev0kAt0nx3eY
kgS09lHyrsiQkrsaVgqppNqbM2yEA5h69wddYYP/gjpFd0fJl7wJfsilvyj8oo4L
jkvSux5JV/kJNKBnQLpMdnrJTW1LJPwZH0DPdaOkFSmGkhm/rYifqJzabnTxDRg=
=Idr+
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 12 03:25:41 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 16:25:41 +1300
Subject: [squid-users] R: Problem with Squid 3.4 and transparent SSL
	proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>
Message-ID: <5462D335.2030701@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 5:40 a.m., Job wrote:
>> That means in your case avoid directly connecting to the
>> intercepting port. Connect to port 80/443 on some Internet server
>> instead and see
> if> the packets are properly delivered through Squid.
>> Also, avoid telnet for the 443 tests. Use an HTTPS client.
> 
> Hello Amos and thank you, first of all.
> 
> I started squid in debug mode and now i see it:
> 
> 2014/11/11 17:40:17| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.10.254:3130 remote=192.168.10.109:52024 FD 12
> flags=33: (92) Protocol not available
> 

That means that the NAT system has no record of the transaction being
intercepted.

The kind of error which shows up when you deliver traffic directly
from client 192.168.10.109 to an "http_port 3130 intercept" port on
Squid without going through NAT on the Squid box.


> 192.168.10.254 is lan-firewall gateway 192.168.10.109 is the
> workstation where i am trying to surfing on 443 port
> 
> When redirecting the 443 port to squid https_port, errors appears.

Details are critical. Please feel free to flood us with details. Some
of them will be important and we dont know which until we have them.
It is very hard to help an any useful way without lots of details
about what you are doing *exactly*, whats happening *exactly*, and
whats wrong with the happening.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYtM0AAoJELJo5wb/XPRjHzsH/ip3kd7kv8PSgBBAtiVVZ3ws
8ACmAd3upZs4gZy0WRDRGRiL3uQtnWW7DBte7qWOWWMqdmos+5YNG9WH8hFZ+ZzY
awCG6EvtCjVAzuWGRMMe5FkX4fa8yhutoNFZbOYT33CKfWDQTw5tbljR8PH5PIXc
9h0p8MBqPMZyTJUv13szaGzZENZl88xZ3Chg/OMd7DHdEhTi+Ko8qC2n9mTnhFpg
mnChkgG+Y4XRGKTLECTJGOk7OoxFknPmAWpuPZwAcgQXtr1r3rwnCDjfnp9rSWr/
Gz9wQ4Yt2qcB7rIkDtfbnAjLWOtyn2b958sM0h9xdHFY7legYLNDwN/RkbZ/hEA=
=pNAq
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 12 03:31:59 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 16:31:59 +1300
Subject: [squid-users] Problem with https://www.google.com and squid
	interception
In-Reply-To: <546259B7.3020103@nanosec.com>
References: <546259B7.3020103@nanosec.com>
Message-ID: <5462D4AF.6060700@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 7:47 a.m., Peter Gross wrote:
> Hi, I am a new user of Squid and would first like to thank the
> developers for this excellent software. This is my first post to
> the mailing list ... I have been tasked with setting up quite
> restrictive web access control at work. I plan to use an
> intercepting squid proxy with SSL bump. There will also be WCCPv2
> to/from a Cisco IOS router. Since this is quite a bit of
> complexity, I though it prudent to start slowly, in steps. So first
> -- to get my feet wet -- I set up squid (version 3.4.8, built using
> rpmbuild from the src rpm from ngtech) on a home linux server
> (Centos 5.11 -- no Cisco at home) which is also the firewall router
> for my home network. I also decided to start out with plain vanilla
> proxying (no interception -- use browser setting). This worked 
> fine. I then tested HTTP interception by changing squid.conf from: 
> http_port 3128 -to- http_port 3128 intercept

Please use a second http_port with a different number for each input
"mode". Squid needs at least one forward-proxy port for things like
icons URLs in error pages - 3128 is the port number W3C allocated for
that.

> 
> and adding the following rule to my shorewall firewall: 
> REDIRECT:info   loc:192.168.101.9       3128    tcp     http
> 
> I wanted to test intercepting just one host before turning it on
> for all hosts and wireless devices in my network.
> 
> 192.168.101.9 is another Centos PC on my network. Squid is running
> on 192.168.101.253.
> 
> The interception seemed to work fine ... access.log showed lots of 
> successful proxy activity. Then came the problem: going to 
> https://www.google.com failed (not every time, but frequently). If
> I turned off the REDIRECT line in the shorewall rules file and
> restarted shorewall, no problem. This seemed very peculiar because
> no HTTPS traffic should be redirected to the proxy.

Correct. Very perculiar.

> Here are the errors that showed up in cache.log when redirection
> (NAT-ing) was on:
> 
> 2014/11/11 11:03:42 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed
> on local=192.168.101.253:3128 remote=192.168.101.9:34165 FD 11
> flags=33: (92) Protocol not available

That is forward-proxy traffic going to the port with intercept on it.
Separating the ports should help you identify if this is error pages
embeded objects or not.


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYtSvAAoJELJo5wb/XPRjcJ0H/jeEbRpFJYp0v5nfTWlwdOTV
ajA2SpQ+3GY0s8iPX0FJnl2mFSfNVs5V+515/iOSGfEUhIS/+qGyWCqIjA79tTHy
dTx9eKOc1boF/7nadgfFl/60rpJprNfuosh9iIhkDShC3bNzz3y5lcPVyPi4bhKD
wkG/dT5GdUdVTVn1aA1cojebrJ2SUMe/NMyFdEADIyOLSNsDT000MMB4Mr3VnVBA
68nq6wdGdnK0/ydm/OvErruVgPqQGP/IvpdLaHw0+2ck2fYRlzgB9+6P1an24rDA
hgsn0sZ/MfIxJd/biC5Pk0gnNzapVI+n4E2NIx+F0aN/y2Bgn0+AncvEqKnSS3U=
=tp1A
-----END PGP SIGNATURE-----


From yogesh_dg at yahoo.com  Wed Nov 12 04:05:37 2014
From: yogesh_dg at yahoo.com (Yogesh Gawankar)
Date: Wed, 12 Nov 2014 04:05:37 +0000 (UTC)
Subject: [squid-users] Problem with https://www.google.com and
	squid	interception
In-Reply-To: <5462D4AF.6060700@treenet.co.nz>
References: <5462D4AF.6060700@treenet.co.nz>
Message-ID: <1536817904.20202.1415765137757.JavaMail.yahoo@jws10610.mail.bf1.yahoo.com>

hello peter
can you check if your squid does gre return? 


It is unrelated to your issue though :)?Thanks and regards

Yogesh Gawankar
 

     On Wednesday, November 12, 2014 9:02 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
   

 -----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 7:47 a.m., Peter Gross wrote:
> Hi, I am a new user of Squid and would first like to thank the
> developers for this excellent software. This is my first post to
> the mailing list ... I have been tasked with setting up quite
> restrictive web access control at work. I plan to use an
> intercepting squid proxy with SSL bump. There will also be WCCPv2
> to/from a Cisco IOS router. Since this is quite a bit of
> complexity, I though it prudent to start slowly, in steps. So first
> -- to get my feet wet -- I set up squid (version 3.4.8, built using
> rpmbuild from the src rpm from ngtech) on a home linux server
> (Centos 5.11 -- no Cisco at home) which is also the firewall router
> for my home network. I also decided to start out with plain vanilla
> proxying (no interception -- use browser setting). This worked 
> fine. I then tested HTTP interception by changing squid.conf from: 
> http_port 3128 -to- http_port 3128 intercept

Please use a second http_port with a different number for each input
"mode". Squid needs at least one forward-proxy port for things like
icons URLs in error pages - 3128 is the port number W3C allocated for
that.

> 
> and adding the following rule to my shorewall firewall: 
> REDIRECT:info? loc:192.168.101.9? ? ? 3128? ? tcp? ? http
> 
> I wanted to test intercepting just one host before turning it on
> for all hosts and wireless devices in my network.
> 
> 192.168.101.9 is another Centos PC on my network. Squid is running
> on 192.168.101.253.
> 
> The interception seemed to work fine ... access.log showed lots of 
> successful proxy activity. Then came the problem: going to 
> https://www.google.com failed (not every time, but frequently). If
> I turned off the REDIRECT line in the shorewall rules file and
> restarted shorewall, no problem. This seemed very peculiar because
> no HTTPS traffic should be redirected to the proxy.

Correct. Very perculiar.

> Here are the errors that showed up in cache.log when redirection
> (NAT-ing) was on:
> 
> 2014/11/11 11:03:42 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed
> on local=192.168.101.253:3128 remote=192.168.101.9:34165 FD 11
> flags=33: (92) Protocol not available

That is forward-proxy traffic going to the port with intercept on it.
Separating the ports should help you identify if this is error pages
embeded objects or not.


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYtSvAAoJELJo5wb/XPRjcJ0H/jeEbRpFJYp0v5nfTWlwdOTV
ajA2SpQ+3GY0s8iPX0FJnl2mFSfNVs5V+515/iOSGfEUhIS/+qGyWCqIjA79tTHy
dTx9eKOc1boF/7nadgfFl/60rpJprNfuosh9iIhkDShC3bNzz3y5lcPVyPi4bhKD
wkG/dT5GdUdVTVn1aA1cojebrJ2SUMe/NMyFdEADIyOLSNsDT000MMB4Mr3VnVBA
68nq6wdGdnK0/ydm/OvErruVgPqQGP/IvpdLaHw0+2ck2fYRlzgB9+6P1an24rDA
hgsn0sZ/MfIxJd/biC5Pk0gnNzapVI+n4E2NIx+F0aN/y2Bgn0+AncvEqKnSS3U=
=tp1A
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141112/bffb1813/attachment.htm>

From Jason_Haar at trimble.com  Wed Nov 12 04:49:18 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 12 Nov 2014 17:49:18 +1300
Subject: [squid-users] connecting directly to ssl-bump intercept port causes
	runaway CPU
Message-ID: <5462E6CE.7040506@trimble.com>

Hi there

I was reading this list about the issue with google.com and was playing
around - and I used telnet to connect directly to the intercept ssl-bump
port. End result was squid immediately went to 99% CPU, and the
cache.log started reporting

WARNING! Your cache is running out of filedescriptors
WARNING! Your cache is running out of filedescriptors
WARNING! Your cache is running out of filedescriptors

The box staggered to it's knees, so I had to kill squid. Restarted it
and everything is fine - until I do that again. If I let the network
redirecting work (ie make outbound port 443 connections), this doesn't
happen - it's only if I directly connect to the intercept port

I have my "http_port intercept" and "https_port intercept" set
identically (except for the ssl stuff of course), and yet if I telnet to
the http_port set to intercept, this does NOT happen - it works fine...

Any ideas where I should look to see what's causing the grief? This is
squid-3.4.9. "127.0.0.1" is in /etc/squid/SSL_noIntercept_sites.txt, but
not the ethernet IP nor hostname of the proxy if that matters.

#egrep '^(https?_port|ssl)|SSL_nonHTTPS|SSL_noInter' /etc/squid/squid.conf
http_port 3128
http_port 3126 ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
http_port 3129 intercept
https_port 3127 intercept ssl-bump cert=/etc/squid/squid-CA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 32 startup=5 idle=1
acl SSL_nonHTTPS_sites dstdom_regex "/etc/squid/SSL_nonHTTPS_sites.txt"
acl SSL_noIntercept_sites dstdom_regex
"/etc/squid/SSL_noIntercept_sites.txt"
ssl_bump none SSL_nonHTTPS_sites
ssl_bump none SSL_noIntercept_sites
ssl_bump server-first all
sslproxy_cert_error allow SSL_nonHTTPS_sites
sslproxy_cert_error allow all

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Wed Nov 12 05:59:41 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Nov 2014 18:59:41 +1300
Subject: [squid-users] connecting directly to ssl-bump intercept port
 causes runaway CPU
In-Reply-To: <5462E6CE.7040506@trimble.com>
References: <5462E6CE.7040506@trimble.com>
Message-ID: <5462F74D.8060609@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 5:49 p.m., Jason Haar wrote:
> Hi there
> 
> I was reading this list about the issue with google.com and was
> playing around - and I used telnet to connect directly to the
> intercept ssl-bump port. End result was squid immediately went to
> 99% CPU, and the cache.log started reporting
> 
> WARNING! Your cache is running out of filedescriptors WARNING! Your
> cache is running out of filedescriptors WARNING! Your cache is
> running out of filedescriptors
> 
> The box staggered to it's knees, so I had to kill squid. Restarted
> it and everything is fine - until I do that again. If I let the
> network redirecting work (ie make outbound port 443 connections),
> this doesn't happen - it's only if I directly connect to the
> intercept port


Otherwise known as a "Forwarding loop" or "Denial of Service"
depending on whether it breaks the client request, the server, or your
whole network connectivity.


Short answer:

That being one of the "NAT security vulnerabilities" mentioned as
reason for mangle table rules.
  http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
  http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

... and a core reason for using Via header.

... and incidentally why Squid complains about not having a
forward-proxy port to use in some configurations.


Long answer:
What do you think "server-first" means ? (hint step 3 below).

1) Your telnet opened a connection.

2) The TCP packet IPs told Squid the server you were connecting to was
itself on port 3126.

3) Squid connected there to fetch the SSL certificate details.

4) The TCP connection from Squid to Squid:3126 was opened.

5) go back to #2


HTTP has the Via header mechanism to restrict the loop to protect
against the DoS becoming a wide problem. But notice how no HTTP is
taking place, Squid is still trying to perform the TLS/SSL operations.
There is no protection at the TCP layer.


> 
> I have my "http_port intercept" and "https_port intercept" set 
> identically (except for the ssl stuff of course), and yet if I
> telnet to the http_port set to intercept, this does NOT happen - it
> works fine...


No. "works fine" is a wrong conclusion. This could only "work" if you
sent Squid a test HTTP message with forward-proxy URL syntax. When you
start actually using the port for real traffic with port-80 URL syntax
it will break with forwarding loops like above.


Please stop using telnet and manual data entry to test HTTP services.
If you knew enough about the protocol to type it into telnet correctly
the above loop problems would not be surprising you.

A proper HTTP client software like squidclient, wget, curl, or even a
full browser should be used.


Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYvdMAAoJELJo5wb/XPRjL04H+gKhrYJ20swYkWCOsVrjyRnl
/t3up7PhwKSEaguNnbdkCaT9B/5t3uft+hp9VpiEZkEuTub+JYPWn/3Z2ugCSqKf
k1uiRjHboex+AEILNc5o5fny1CKyXRdCzOArqXvrrTBLOeWGx+Sc5QL+8tiC3g27
ZJ3ldgWS9O7ewdWhp24ReCjgGufq0oWz0PC6BsCVy0qilLjBK4yk+E/xI0Ht3nVj
YTU6lNUzli4Sb0ElMHpJkGbF8lh0e48iEvyIyHdYvTMmbf3gR/lD51mS9XNzrxMu
CiDX7Ok4M/ELdadNG7kgWvKzAZVlySb+63Xaix1CyGzJlWHSwISaH2QLnsASbP0=
=bnBC
-----END PGP SIGNATURE-----


From Job at colliniconsulting.it  Wed Nov 12 08:55:47 2014
From: Job at colliniconsulting.it (Job)
Date: Wed, 12 Nov 2014 09:55:47 +0100
Subject: [squid-users] R: R: Problem with Squid 3.4 and transparent SSL proxy
In-Reply-To: <5462D335.2030701@treenet.co.nz>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>,
 <5462D335.2030701@treenet.co.nz>
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE9@W2008DC01.ColliniConsulting.lan>

Thank you Amos, for everything.

I route with REDIRECT all outgoing connection to port tcp/443 from my LAN:

iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3130

in squid, i have these configurations:

http_port 3128
http_port 3129 intercept
https_port 3130 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

Do you think my iptables rule is wrong?

Thank you!
Francesco


________________________________________
Da: Amos Jeffries [squid3 at treenet.co.nz]
Inviato: mercoled? 12 novembre 2014 4.25
A: Job; squid-users at lists.squid-cache.org
Oggetto: Re: R: [squid-users] Problem with Squid 3.4 and transparent SSL proxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 5:40 a.m., Job wrote:
>> That means in your case avoid directly connecting to the
>> intercepting port. Connect to port 80/443 on some Internet server
>> instead and see
> if> the packets are properly delivered through Squid.
>> Also, avoid telnet for the 443 tests. Use an HTTPS client.
>
> Hello Amos and thank you, first of all.
>
> I started squid in debug mode and now i see it:
>
> 2014/11/11 17:40:17| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.10.254:3130 remote=192.168.10.109:52024 FD 12
> flags=33: (92) Protocol not available
>

That means that the NAT system has no record of the transaction being
intercepted.

The kind of error which shows up when you deliver traffic directly
from client 192.168.10.109 to an "http_port 3130 intercept" port on
Squid without going through NAT on the Squid box.


> 192.168.10.254 is lan-firewall gateway 192.168.10.109 is the
> workstation where i am trying to surfing on 443 port
>
> When redirecting the 443 port to squid https_port, errors appears.

Details are critical. Please feel free to flood us with details. Some
of them will be important and we dont know which until we have them.
It is very hard to help an any useful way without lots of details
about what you are doing *exactly*, whats happening *exactly*, and
whats wrong with the happening.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUYtM0AAoJELJo5wb/XPRjHzsH/ip3kd7kv8PSgBBAtiVVZ3ws
8ACmAd3upZs4gZy0WRDRGRiL3uQtnWW7DBte7qWOWWMqdmos+5YNG9WH8hFZ+ZzY
awCG6EvtCjVAzuWGRMMe5FkX4fa8yhutoNFZbOYT33CKfWDQTw5tbljR8PH5PIXc
9h0p8MBqPMZyTJUv13szaGzZENZl88xZ3Chg/OMd7DHdEhTi+Ko8qC2n9mTnhFpg
mnChkgG+Y4XRGKTLECTJGOk7OoxFknPmAWpuPZwAcgQXtr1r3rwnCDjfnp9rSWr/
Gz9wQ4Yt2qcB7rIkDtfbnAjLWOtyn2b958sM0h9xdHFY7legYLNDwN/RkbZ/hEA=
=pNAq
-----END PGP SIGNATURE-----

From rafael.akchurin at diladele.com  Wed Nov 12 09:36:06 2014
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 12 Nov 2014 09:36:06 +0000
Subject: [squid-users] Troubles compiling latest Squid 3.5 on Windows 7 with
	Cygwin
Message-ID: <60be352dd06947188dae0f509e5b54a2@AM3PR04MB450.eurprd04.prod.outlook.com>

Hello all,

I am struggling to compile Squid 3.5 on windows 7 x64 using latest Cygwin. 

During configuration (./configure --disable-wccp --disable-wccpv2)
The following error occurs:

checking for ldap.h... (cached) yes
checking winldap.h usability... no
checking winldap.h presence... yes
configure: WARNING: winldap.h: present but cannot be compiled
configure: WARNING: winldap.h: check for missing prerequisite headers?
configure: WARNING: winldap.h: see the Autoconf documentation
configure: WARNING: winldap.h: section "Present But Cannot Be Compiled"
configure: WARNING: winldap.h: proceeding with the compiler's result
configure: WARNING: ## ------------------------------------------- ##
configure: WARNING: ## Report this to http://bugs.squid-cache.org/ ##
configure: WARNING: ## ------------------------------------------- ##
checking for winldap.h... no
checking w32api/windows.h usability... yes

Similar with mswsock:

checking mswsock.h usability... no
checking mswsock.h presence... yes
configure: WARNING: mswsock.h: present but cannot be compiled
configure: WARNING: mswsock.h: check for missing prerequisite headers?
configure: WARNING: mswsock.h: see the Autoconf documentation
configure: WARNING: mswsock.h: section "Present But Cannot Be Compiled"
configure: WARNING: mswsock.h: proceeding with the compiler's result
configure: WARNING: ## ------------------------------------------- ##
configure: WARNING: ## Report this to http://bugs.squid-cache.org/ ##
configure: WARNING: ## ------------------------------------------- ##
checking for mswsock.h... no

If?I run "make" after that it fails at start:

Making all in compat
make[1]: Entering directory '/usr/src/squid-3.5.0.1/compat'
/bin/sh ../libtool? --tag=CXX?? --mode=compile g++ -DHAVE_CONFIG_H?? -I.. -I../include -I../lib -I../src -I../include???? -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT -g -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF .deps/assert.Tpo -c -o assert.lo assert.cc
libtool: compile:? g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT -g -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF .deps/assert.Tpo -c assert.cc? -DDLL_EXPORT -DPIC -o .libs/assert.o
In file included from /usr/include/w32api/winsock2.h:56:0,
???????????????? from /usr/include/w32api/ws2spi.h:13,
???????????????? from ../compat/os/mswindows.h:306,
???????????????? from ../compat/compat.h:73,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:
/usr/include/w32api/psdk_inc/_fd_types.h:100:2: error: #warning "fd_set and associated macros have been defined in sys/types.????? This can cause runtime problems with W32 sockets" [-Werror=cpp]
?#warning "fd_set and associated macros have been defined in sys/types.? \
? ^
In file included from ../compat/compat.h:73:0,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:
../compat/os/mswindows.h:417:0: error: "FOPEN" redefined [-Werror]
?#define FOPEN?????????? 0x01??? /* file handle open */
?^
In file included from /usr/include/sys/fcntl.h:3:0,
???????????????? from /usr/include/fcntl.h:14,
???????????????? from ../compat/os/mswindows.h:41,
???????????????? from ../compat/compat.h:73,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:
/usr/include/sys/_default_fcntl.h:98:0: note: this is the location of the previous definition
?#define FOPEN? _FOPEN
?^
In file included from /usr/include/w32api/ws2spi.h:13:0,
???????????????? from ../compat/os/mswindows.h:306,
???????????????? from ../compat/compat.h:73,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:
/usr/include/w32api/winsock2.h:995:123: error: declaration of C function 'int select(int, _types_fd_set*, _types_fd_set*, _types_fd_set*, PTIMEVAL)' conflicts with
?? WINSOCK_API_LINKAGE int WSAAPI select(int nfds,fd_set *readfds,fd_set *writefds,fd_set *exceptfds,const PTIMEVAL timeout);
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????? ^
In file included from ../compat/types.h:41:0,
???????????????? from ../compat/compat.h:59,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:
/usr/include/sys/select.h:31:5: error: previous declaration 'int select(int, _types_fd_set*, _types_fd_set*, _types_fd_set*, timeval*)' here
?int select __P ((int __n, fd_set *__readfds, fd_set *__writefds,
???? ^
In file included from ../compat/compat.h:73:0,
???????????????? from ../include/squid.h:43,
???????????????? from assert.cc:9:

Is this a known issue or am I doing something wrong?
Please note the same commands compile Squid 3.3.8 (from Cygwin) without any problems.

Thank you very much!
Rafael Akchurin
Diladele B.V.



From hussam at visp.net.lb  Wed Nov 12 11:17:35 2014
From: hussam at visp.net.lb (Hussam Al-Tayeb)
Date: Wed, 12 Nov 2014 13:17:35 +0200
Subject: [squid-users] Cannot purge items that are not upstream anymore
Message-ID: <1630701.7i1R41SJLv@hades>

Hello. I have a problem with 'squidclient -m PURGE' and also the purge 
command.
They won't purge urls from disk that are not available online anymore or 
redirect to other links.


For example, 
http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg
which corresponds to /home/squid/00/BB/0000BBC0

Even "purge -e "static.firedrive.com" -c /etc/squid/squid.conf -P  0x01" reads 
it but will not really remove it from disk.
Are such files stuck on disk forever?
What would the correct way to clear them?


From squid3 at treenet.co.nz  Wed Nov 12 12:17:36 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 01:17:36 +1300
Subject: [squid-users] Troubles compiling latest Squid 3.5 on Windows 7
 with Cygwin
In-Reply-To: <60be352dd06947188dae0f509e5b54a2@AM3PR04MB450.eurprd04.prod.outlook.com>
References: <60be352dd06947188dae0f509e5b54a2@AM3PR04MB450.eurprd04.prod.outlook.com>
Message-ID: <54634FE0.1020907@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 10:36 p.m., Rafael Akchurin wrote:
> Hello all,
> 
> I am struggling to compile Squid 3.5 on windows 7 x64 using latest
> Cygwin.
> 
> During configuration (./configure --disable-wccp --disable-wccpv2)

NP: the current 3.5 should need NO special options to build on Windows.


> The following error occurs:
> 
> checking for ldap.h... (cached) yes checking winldap.h usability...
> no checking winldap.h presence... yes configure: WARNING:
> winldap.h: present but cannot be compiled configure: WARNING:
> winldap.h: check for missing prerequisite headers? configure:
> WARNING: winldap.h: see the Autoconf documentation configure:
> WARNING: winldap.h: section "Present But Cannot Be Compiled" 
> configure: WARNING: winldap.h: proceeding with the compiler's
> result configure: WARNING: ##
> ------------------------------------------- ## configure: WARNING:
> ## Report this to http://bugs.squid-cache.org/ ## configure:
> WARNING: ## ------------------------------------------- ## checking
> for winldap.h... no checking w32api/windows.h usability... yes
> 
> Similar with mswsock:
> 
> checking mswsock.h usability... no checking mswsock.h presence...
> yes configure: WARNING: mswsock.h: present but cannot be compiled 
> configure: WARNING: mswsock.h: check for missing prerequisite
> headers? configure: WARNING: mswsock.h: see the Autoconf
> documentation configure: WARNING: mswsock.h: section "Present But
> Cannot Be Compiled" configure: WARNING: mswsock.h: proceeding with
> the compiler's result configure: WARNING: ##
> ------------------------------------------- ## configure: WARNING:
> ## Report this to http://bugs.squid-cache.org/ ## configure:
> WARNING: ## ------------------------------------------- ## checking
> for mswsock.h... no
> 

Sigh. OS headers not including their own basic dependencies. Evil
practice even in C.

The winldap.h not being found usable should only prevent the LDAP
helpers being built, not a big issue perhapse.

The absence of mswsock.h is probably involved with those later build
errors. It usually contains a lot of definitions needed for the FD and
socket API. If you can find out what it requires to be icluded before
it in the .cc file, please let me know.


> If I run "make" after that it fails at start:
> 
> Making all in compat make[1]: Entering directory
> '/usr/src/squid-3.5.0.1/compat' /bin/sh ../libtool  --tag=CXX
> --mode=compile g++ -DHAVE_CONFIG_H   -I.. -I../include -I../lib
> -I../src -I../include     -Wall -Wpointer-arith -Wwrite-strings
> -Wcomments -Wshadow -Werror -pipe -D_REENTRANT -g -O2 -march=native
> -std=c++11 -MT assert.lo -MD -MP -MF .deps/assert.Tpo -c -o
> assert.lo assert.cc libtool: compile:  g++ -DHAVE_CONFIG_H -I..
> -I../include -I../lib -I../src -I../include -Wall -Wpointer-arith
> -Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT -g
> -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF
> .deps/assert.Tpo -c assert.cc  -DDLL_EXPORT -DPIC -o
> .libs/assert.o In file included from
> /usr/include/w32api/winsock2.h:56:0, from
> /usr/include/w32api/ws2spi.h:13, from
> ../compat/os/mswindows.h:306, from ../compat/compat.h:73, from
> ../include/squid.h:43, from assert.cc:9: 
> /usr/include/w32api/psdk_inc/_fd_types.h:100:2: error: #warning
> "fd_set and associated macros have been defined in sys/types.
> This can cause runtime problems with W32 sockets" [-Werror=cpp] 
> #warning "fd_set and associated macros have been defined in
> sys/types.  \ ^

No idea at this point what the correct fix is here. There is a bit of
a catch-22 situation going on with fd_set. With any luck is and the
below might disappear once mswsock.h is found.


> In file included from ../compat/compat.h:73:0, from
> ../include/squid.h:43, from assert.cc:9: 
> ../compat/os/mswindows.h:417:0: error: "FOPEN" redefined [-Werror] 
> #define FOPEN           0x01    /* file handle open */

Fixed this one.

> ^ In file included from /usr/include/w32api/ws2spi.h:13:0, from
> ../compat/os/mswindows.h:306, from ../compat/compat.h:73, from
> ../include/squid.h:43, from assert.cc:9: 
> /usr/include/w32api/winsock2.h:995:123: error: declaration of C
> function 'int select(int, _types_fd_set*, _types_fd_set*,
> _types_fd_set*, PTIMEVAL)' conflicts with WINSOCK_API_LINKAGE int
> WSAAPI select(int nfds,fd_set *readfds,fd_set *writefds,fd_set
> *exceptfds,const PTIMEVAL timeout); ^ In file included from
> ../compat/types.h:41:0, from ../compat/compat.h:59, from
> ../include/squid.h:43, from assert.cc:9: 
> /usr/include/sys/select.h:31:5: error: previous declaration 'int
> select(int, _types_fd_set*, _types_fd_set*, _types_fd_set*,
> timeval*)' here int select __P ((int __n, fd_set *__readfds, fd_set
> *__writefds, ^ In file included from ../compat/compat.h:73:0, from
> ../include/squid.h:43, from assert.cc:9:
> 
> Is this a known issue or am I doing something wrong? Please note
> the same commands compile Squid 3.3.8 (from Cygwin) without any
> problems.
> 

No to both questions.

A lot has changed in the Windows support since 3.3. The
Windows-specific code has had a lot of shuffling around and testing
under MinGW. You are the first Cygwin person to report anything about
issues in the post-3.3 versions.

The Cygwin situation is kind of being tracked in
http://bugs.squid-cache.org/show_bug.cgi?id=4073. If you can update
the bug report it would be a help.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY0/fAAoJELJo5wb/XPRjyf8H/jTYfUq1b9LDgJmDtAGsuyVA
4l7i3Tn6sZEfIny6iS7rPIT55cdB6ku/7A81f85tcY2nzfYbRcH00ccLtPhtvEb0
WcwYrxMYOWXOGy9lT5SO0h+aT4a5eoaxcgGR34+Ekiphji2Hs8INYtcXVo0tnlap
GZ8wV3WNNyqXhTkzdqZO8S98LcBDGYxBp1qeRtcsk9+syW9mapC5vJcqzEf2rwCr
FbU0sE+yNvA67pVGxTqxokcRX0bU3VZtwgNCdtmNGXZ+6UFtDrUe5ewNJeW0I34B
rAcoAoZ1grI4iA0TN9evGBM4XtAIlKZTV82Qtz5WGm5bfopzCtXIAKNd9Q9IkJo=
=S5SL
-----END PGP SIGNATURE-----


From vdoctor at neuf.fr  Wed Nov 12 12:16:45 2014
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 12 Nov 2014 04:16:45 -0800 (PST)
Subject: [squid-users] Squid 3.5: Delay parameters bungled what changes ?
In-Reply-To: <1415522523118-4668260.post@n4.nabble.com>
References: <545E3F9E.1040001@articatech.com>
 <1415522523118-4668260.post@n4.nabble.com>
Message-ID: <1415794605821-4668329.post@n4.nabble.com>

Hi,

Any news ? feedbacks ? nobody interested ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-Delay-parameters-bungled-what-changes-tp4668259p4668329.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov 12 12:39:27 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 01:39:27 +1300
Subject: [squid-users] Cannot purge items that are not upstream anymore
In-Reply-To: <1630701.7i1R41SJLv@hades>
References: <1630701.7i1R41SJLv@hades>
Message-ID: <546354FF.7070505@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 12:17 a.m., Hussam Al-Tayeb wrote:
> Hello. I have a problem with 'squidclient -m PURGE' and also the
> purge command. They won't purge urls from disk that are not
> available online anymore or redirect to other links.
> 

PURGE was designed for use in HTTP/1.0. It does not handle HTTP/1.1
negotiated content / variants at all well.

> 
> For example, 
> http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg
>
> 
which corresponds to /home/squid/00/BB/0000BBC0
> 
> Even "purge -e "static.firedrive.com" -c /etc/squid/squid.conf -P
> 0x01" reads it but will not really remove it from disk. Are such
> files stuck on disk forever?

No. Cache is a temporary storage location. Think of it as a buffer in
the network.

They will exist only until a) the storage space is needed for
something else, or b) the timestamp in their Expires: header, or c) an
origin server informs Squid they are no longer existing.

PURGE method was a way to fake (c).


> What would the correct way to clear them?

By obeying HTTP protocol. HTTP has built-in mechanisms for doing that
automatically.

Or you can just delete the disk file. Squid will auto-detect the
removal at some point when it needs to use or delete the object. Until
then it will just under-count the amount of used disk.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY1T+AAoJELJo5wb/XPRjVC0IALcAfgH78kZ8o+xA1h4mYc/N
4YfFSGLuTa1j4Bqkrd9pBthGhYzjrk0FHo60XvsNB2A7f9vuXlNs/M+/ImkIukde
qgsGIQ16vcAPLhI+cNaH+iVl+mcLL8tvpkkF6WXBSb0rLgpNuij4lAzUCtYHi3/h
JZ5nDXlXoUIJqJn6JEJyqIAeB7zqHjGqx14qwBTCUEmY11KiqhCyBurIvNS0ziWR
E7C5sRwn0ZNXjKJ9lDMrXJLvjtnYRb/gUzdvU/VicrNmh32kaBuFi1Opm4N1F2g+
J1GBDZCUCOijaOleIO5JZB++dOwWGcmReSugoIapou8Oh7thYoQJ1Haq3PHXymI=
=rHzR
-----END PGP SIGNATURE-----


From hussam at visp.net.lb  Wed Nov 12 12:55:24 2014
From: hussam at visp.net.lb (Hussam Al-Tayeb)
Date: Wed, 12 Nov 2014 14:55:24 +0200
Subject: [squid-users] Cannot purge items that are not upstream anymore
In-Reply-To: <546354FF.7070505@treenet.co.nz>
References: <1630701.7i1R41SJLv@hades> <546354FF.7070505@treenet.co.nz>
Message-ID: <8429980.HusbksOlJk@hades>

On Thursday 13 November 2014 01:39:27 Amos Jeffries wrote:
> On 13/11/2014 12:17 a.m., Hussam Al-Tayeb wrote:
> > Hello. I have a problem with 'squidclient -m PURGE' and also the
> > purge command. They won't purge urls from disk that are not
> > available online anymore or redirect to other links.
> 
> PURGE was designed for use in HTTP/1.0. It does not handle HTTP/1.1
> negotiated content / variants at all well.
> 
> > For example,
> > http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64
> > e8d563.jpg
> which corresponds to /home/squid/00/BB/0000BBC0
> 
> > Even "purge -e "static.firedrive.com" -c /etc/squid/squid.conf -P
> > 0x01" reads it but will not really remove it from disk. Are such
> > files stuck on disk forever?
> 
> No. Cache is a temporary storage location. Think of it as a buffer in
> the network.
> 
> They will exist only until a) the storage space is needed for
> something else, or b) the timestamp in their Expires: header, or c) an
> origin server informs Squid they are no longer existing.
> 
> PURGE method was a way to fake (c).
> 
> > What would the correct way to clear them?
> 
> By obeying HTTP protocol. HTTP has built-in mechanisms for doing that
> automatically.
> 
> Or you can just delete the disk file. Squid will auto-detect the
> removal at some point when it needs to use or delete the object. Until
> then it will just under-count the amount of used disk.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


head /home/squid/00/BB/0000BBC0 -n20

?u??(???,?|?S?|
?S??f7?P`Uhttp://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg
R"accept-encoding="gzip,%20deflate"HTTP/1.1 200 OK
Date: Tue, 26 Aug 2014 12:25:13 GMT
Content-Type: image/jpeg
Last-Modified: Wed, 31 Oct 2012 06:12:07 GMT
ETag: "5090c137-2efb"
Expires: Fri, 23 Aug 2024 12:25:13 GMT
Cache-Control: public, max-age=315360000
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET, POST, OPTIONS
Access-Control-Allow-Headers: Origin,Referer,Accept-Encoding,Accept-
Language,Accept,DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-
Modified-Since,Cache-Control,Content-Type,X-Forwarded-For,X-Forwarded-Proto
CF-Cache-Status: HIT
Vary: Accept-Encoding
Accept-Ranges: bytes
Server: cloudflare-nginx
CF-RAY: 160002c2e4730887-FRA
Content-Length: 12027
Connection: Keep-Alive
Set-Cookie: __cfduid=dfcf568ed46ef827243b3d6c342b3bdc41409055913424; 
expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/; domain=.firedrive.com; HttpOnly


The Expires header is in the past.  
"http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg"
Sending HTTP request ... done.
HTTP/1.1 404 Not Found
Server: squid
Mime-Version: 1.0
Date: Wed, 12 Nov 2014 12:54:13 GMT
Content-Length: 0
X-Cache: MISS from SERV1
X-Cache-Lookup: NONE from SERV1:3129
Via: 1.1 SERV1 (squid)
Connection: close

Is that why  squidclient -m PURGE -h 127.0.0.1 -p 3129 says not found in 
cache?



From ahmed.zaeem at netstream.ps  Wed Nov 12 22:55:23 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Wed, 12 Nov 2014 14:55:23 -0800
Subject: [squid-users] cache peer problem with two squid one Tproxy
	--->normal Porxy
Message-ID: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>

Hi all

I have two proxies

 

1(tproxy) and configured it to get from another normal proxy

So , my topology is as below

 

 

Tproxy- listen on 6000------------------->normal proxy listen 3127

 

The problem is done on the normal proxy , I sont see hit or access logs but
I can see logs as below :

 

 

2014/11/12 15:17:25 kid1| WARNING: Forwarding loop detected for:

GET /favicon.ico HTTP/1.1

Host: 108.61.172.74

User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101
Firefox/33.0

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8

Accept-Language: en-US,en;q=0.5

Accept-Encoding: gzip, deflate

Via: 1.1 localhost.localdomain (squid/3.4.3)

X-Forwarded-For: 176.58.67.238

Cache-Control: max-age=259200

Connection: keep-alive

 

 

2014/11/12 15:17:25 kid1| WARNING: Forwarding loop detected for:

GET /favicon.ico HTTP/1.1

Host: 108.61.172.74

User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101
Firefox/33.0

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8

Accept-Language: en-US,en;q=0.5

Accept-Encoding: gzip, deflate

Via: 1.1 localhost.localdomain (squid/3.4.3)

X-Forwarded-For: 176.58.67.238

Cache-Control: max-age=259200

Connection: keep-alive

 

 

 

 

As we see , the request reach from the tproxy to the normal proxy , but not
processed well @ the normal proxy.

 

 

Here are config file for the tproxy for the cache peer :

cache_peer xxxxxx  parent 3127 0 default

 

on the normal proxy , I have allowed the ip of the tproxy  there and here is
squid.conf file :

[root at localhost ~]# cat /etc/squid/squid.conf

#

# Recommended minimum configuration:

#

 

# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl localnet src 77.221.96.0/19 176.58.67.238/32

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

 

#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

 

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

 

# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager

 

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost

 

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

 

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost

 

# And finally deny all other access to this proxy

http_access deny all

 

# Squid normally listens to port 3128

http_port 3127

 

# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/cache/squid 100 16 256

 

# Leave coredumps in the first cache dir

coredump_dir /var/cache/squid

 

#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

[root at localhost ~]#

 

 

 

 

Squid is 3.4.3 on both squid machines and here is the compilation options :

 

# squid -v

Squid Cache: Version 3.4.3

configure options:  '--build=i486-linux-gnu' '--prefix=/usr'
'--includedir=/include' '--mandir=/share/man' '--infodir=/share/info'
'--sysconfdir=/etc' '--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'build_alias=i486-linux-gnu' 'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS='
'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g -Wall -O2' '--enable-ltdl-convenience'

 

 

 

wish to help

 

regards

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141112/e57c474c/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 12 13:23:12 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 02:23:12 +1300
Subject: [squid-users] Cannot purge items that are not upstream anymore
In-Reply-To: <8429980.HusbksOlJk@hades>
References: <1630701.7i1R41SJLv@hades> <546354FF.7070505@treenet.co.nz>
 <8429980.HusbksOlJk@hades>
Message-ID: <54635F40.105@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 1:55 a.m., Hussam Al-Tayeb wrote:
> On Thursday 13 November 2014 01:39:27 Amos Jeffries wrote:
>> On 13/11/2014 12:17 a.m., Hussam Al-Tayeb wrote:
>>> Hello. I have a problem with 'squidclient -m PURGE' and also
>>> the purge command. They won't purge urls from disk that are
>>> not available online anymore or redirect to other links.
>> 
>> PURGE was designed for use in HTTP/1.0. It does not handle
>> HTTP/1.1 negotiated content / variants at all well.
>> 
>>> For example, 
>>> http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64
>>>
>>> 
e8d563.jpg
>> which corresponds to /home/squid/00/BB/0000BBC0
>> 
>>> Even "purge -e "static.firedrive.com" -c /etc/squid/squid.conf
>>> -P 0x01" reads it but will not really remove it from disk. Are
>>> such files stuck on disk forever?
>> 
>> No. Cache is a temporary storage location. Think of it as a
>> buffer in the network.
>> 
>> They will exist only until a) the storage space is needed for 
>> something else, or b) the timestamp in their Expires: header, or
>> c) an origin server informs Squid they are no longer existing.
>> 
>> PURGE method was a way to fake (c).
>> 
>>> What would the correct way to clear them?
>> 
>> By obeying HTTP protocol. HTTP has built-in mechanisms for doing
>> that automatically.
>> 
>> Or you can just delete the disk file. Squid will auto-detect the 
>> removal at some point when it needs to use or delete the object.
>> Until then it will just under-count the amount of used disk.
>> 
>> Amos
>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> head /home/squid/00/BB/0000BBC0 -n20
> 
> ?u??(???,?|?S?| 
> ?S??f7?P`Uhttp://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg
>
> 
R"accept-encoding="gzip,%20deflate"HTTP/1.1 200 OK

... notice and remember the accept-encoding=* value in quotes above.
We shall get back to that later.

Now for a little tale of woe...

On this Date:

> Date: Tue, 26 Aug 2014 12:25:13 GMT

... the

> Server: cloudflare-nginx

... representing static.firedrive.com has expicitly with complete
authority state that the:

> Content-Type: image/jpeg

... otherwise known as:

> ETag: "5090c137-2efb"

... should remain in cache until:

> Expires: Fri, 23 Aug 2024 12:25:13 GMT

... should remain in cache until Aug 2024.

Furthermore that:

> Last-Modified: Wed, 31 Oct 2012 06:12:07 GMT Cache-Control: public,
> max-age=315360000

... there is no need for any recipient storing the object to even
check for validity again until Oct 2022.


> Access-Control-Allow-Origin: * Access-Control-Allow-Methods: GET,
> POST, OPTIONS Access-Control-Allow-Headers:
> Origin,Referer,Accept-Encoding,Accept- 
> Language,Accept,DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-
>
> 
Modified-Since,Cache-Control,Content-Type,X-Forwarded-For,X-Forwarded-Proto
> CF-Cache-Status: HIT Vary: Accept-Encoding Accept-Ranges: bytes 
> CF-RAY: 160002c2e4730887-FRA Content-Length: 12027 Connection:
> Keep-Alive Set-Cookie:
> __cfduid=dfcf568ed46ef827243b3d6c342b3bdc41409055913424; 
> expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/;
> domain=.firedrive.com; HttpOnly

... and that the Cookies it served up shall remain on the users
machine until Dec 2019. Thereafter this object shall be served without
any Cookie.

> 
> 
> The Expires header is in the past. 
> "http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b64e8d563.jpg"
>
> 
Sending HTTP request ... done.
> HTTP/1.1 404 Not Found Server: squid Mime-Version: 1.0 Date: Wed,
> 12 Nov 2014 12:54:13 GMT Content-Length: 0 X-Cache: MISS from
> SERV1 X-Cache-Lookup: NONE from SERV1:3129 Via: 1.1 SERV1 (squid) 
> Connection: close
> 
> Is that why  squidclient -m PURGE -h 127.0.0.1 -p 3129 says not
> found in cache?

The Vary: header is why your PURGE is not doing anything. It means you
have to find and send in the exactly right value for Accept-Encoding,
in order for PURGE to find and erase the actual object.

The request *not* having an Accept-Encoding header is one of the
possible variations of values. In particular it is the one which is
foudn to already be absent from your cache by your PURGE command.

This is where that quoted string up top of the object file comes in.
What is cached on disk is the object for:
squidclient -m PURGE -p 3129 -H 'Accept-Encoding: gzip, deflate\n'

The purge tool has not been updated in some years and does not support
these types of variant commonly found in HTTP/1.1 traffic.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY18/AAoJELJo5wb/XPRjEOMH+wXFvkHoSGbZMLKMZHmGfk6m
8CtmBoICMAhHsQ34vKNNn5ENZU79MuaI2OeTyDXK/xekr0/WnRmyAL+iQYRD+/B+
D9L4TJxNznx/8io5AAJ1rTU0Ua81Ey3mCgB9af7zabUICNb2pnSFx53ju8PMc6WR
FIre5qbBT+eeSxY3IHlyp61eqcvKqf1yYOnutAXSDarodvlUsydspjEMfxBYjpUT
8zAQq7cCxDb2H2jamFFaQhh4x+AR5yOEQ8jRCIhQgP8M2E0Cm6lorAVpeVS9sTv3
82DKXKNN+NaLKtiAhMMBx4LVeXmjQU6farLSz2Xj94WFLWsI7uE7fTyDzX2AaAs=
=RPQc
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 12 13:38:10 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 02:38:10 +1300
Subject: [squid-users] cache peer problem with two squid one Tproxy
 --->normal Porxy
In-Reply-To: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
Message-ID: <546362C2.5010708@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 11:55 a.m., Ahmed Allzaeem wrote:
> Hi all
> 
> I have two proxies
> 
> 
> 
> 1(tproxy) and configured it to get from another normal proxy
> 
> So , my topology is as below
> 
> 
> 
> 
> 
> Tproxy- listen on 6000------------------->normal proxy listen 3127
> 
> 
> 
> The problem is done on the normal proxy , I sont see hit or access
> logs but I can see logs as below :
> 
> 
> 
> 
> 
> 2014/11/12 15:17:25 kid1| WARNING: Forwarding loop detected for:
> 
> GET /favicon.ico HTTP/1.1
> 
> Host: 108.61.172.74
> 
> User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0)
> Gecko/20100101 Firefox/33.0
> 
> Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> 
> Accept-Language: en-US,en;q=0.5
> 
> Accept-Encoding: gzip, deflate
> 
> Via: 1.1 localhost.localdomain (squid/3.4.3)

Problem #1: The *public* domain name for your proxy is apparently
"localhost".

Any traffic it receives from any other proxy calling itself or
resolving to the domain "localhost" will die horribly.

Since Squid claims a forwarding loop I guess your proxies are either
both configured to call themselves "localhost", or possibly there
really is a forwarding loop. You wont be able to know for sure until
you fix the proxy servers hostname.

> 
> As we see , the request reach from the tproxy to the normal proxy ,
> but not processed well @ the normal proxy.
> 
> Here are config file for the tproxy for the cache peer :
> 
> cache_peer xxxxxx  parent 3127 0 default
> 

Problem #2:
 Your cache_peer directive is missing the no-tproxy option.

As a result the tproxy Squid is sending TCP packets to the peer using
the client address as src-IP.



Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY2LBAAoJELJo5wb/XPRj3A0H/1oXy6AGSF1jCMa5nI4dptNn
6nuWV2t4c3NbMhFo2JzkhXQiSFjpZX+/UH7yxm5aMuU90jLC4bpXD7hfbhMUyl6P
2nHOrDu2iK/pwMUAFg6fBJEUYsVzb032FEI0FHk2HySEDEmj/Svk1MB6/RVG8D5P
flNFIyzQ5x/RBuRiVHqwnq1o0KSBf8NQAWRteSWVLr7vNnYTi5xRWdjkJEall7nc
2fV3ye6UUm6vwJFjyAXhqsi808D1nvnvGcnBzecl04ZuD5n4Xb2g0BbmfPR/Rh1O
s/kjpzx25VlMFUQz/nyHOwCDRN9egRiAFstwKlSoGcmodtJbej4QrKXnRPIRhq8=
=lMrh
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Thu Nov 13 00:02:09 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Wed, 12 Nov 2014 16:02:09 -0800
Subject: [squid-users] cache peer problem with two squid one Tproxy
	--->normal Porxy
In-Reply-To: <546362C2.5010708@treenet.co.nz>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
Message-ID: <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>

Thanks amos

I have added option notproxy on the tproxy one
It became :
cache_peer 77.221.104.97  parent 3127 0 no-tproxy  


also I changed hostnames in /etc.hosts for both servers and added visible hostname  squid for both

now on the normal proxy I can see the logs access 

but still not traffic
it give me access denied from the parent proxy ?!!
any help ?


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, November 12, 2014 5:38 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer problem with two squid one Tproxy --->normal Porxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 11:55 a.m., Ahmed Allzaeem wrote:
> Hi all
> 
> I have two proxies
> 
> 
> 
> 1(tproxy) and configured it to get from another normal proxy
> 
> So , my topology is as below
> 
> 
> 
> 
> 
> Tproxy- listen on 6000------------------->normal proxy listen 3127
> 
> 
> 
> The problem is done on the normal proxy , I sont see hit or access 
> logs but I can see logs as below :
> 
> 
> 
> 
> 
> 2014/11/12 15:17:25 kid1| WARNING: Forwarding loop detected for:
> 
> GET /favicon.ico HTTP/1.1
> 
> Host: 108.61.172.74
> 
> User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0)
> Gecko/20100101 Firefox/33.0
> 
> Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> 
> Accept-Language: en-US,en;q=0.5
> 
> Accept-Encoding: gzip, deflate
> 
> Via: 1.1 localhost.localdomain (squid/3.4.3)

Problem #1: The *public* domain name for your proxy is apparently "localhost".

Any traffic it receives from any other proxy calling itself or resolving to the domain "localhost" will die horribly.

Since Squid claims a forwarding loop I guess your proxies are either both configured to call themselves "localhost", or possibly there really is a forwarding loop. You wont be able to know for sure until you fix the proxy servers hostname.

> 
> As we see , the request reach from the tproxy to the normal proxy , 
> but not processed well @ the normal proxy.
> 
> Here are config file for the tproxy for the cache peer :
> 
> cache_peer xxxxxx  parent 3127 0 default
> 

Problem #2:
 Your cache_peer directive is missing the no-tproxy option.

As a result the tproxy Squid is sending TCP packets to the peer using the client address as src-IP.



Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY2LBAAoJELJo5wb/XPRj3A0H/1oXy6AGSF1jCMa5nI4dptNn
6nuWV2t4c3NbMhFo2JzkhXQiSFjpZX+/UH7yxm5aMuU90jLC4bpXD7hfbhMUyl6P
2nHOrDu2iK/pwMUAFg6fBJEUYsVzb032FEI0FHk2HySEDEmj/Svk1MB6/RVG8D5P
flNFIyzQ5x/RBuRiVHqwnq1o0KSBf8NQAWRteSWVLr7vNnYTi5xRWdjkJEall7nc
2fV3ye6UUm6vwJFjyAXhqsi808D1nvnvGcnBzecl04ZuD5n4Xb2g0BbmfPR/Rh1O
s/kjpzx25VlMFUQz/nyHOwCDRN9egRiAFstwKlSoGcmodtJbej4QrKXnRPIRhq8=
=lMrh
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From hussam at visp.net.lb  Wed Nov 12 14:04:04 2014
From: hussam at visp.net.lb (Hussam Al-Tayeb)
Date: Wed, 12 Nov 2014 16:04:04 +0200
Subject: [squid-users] Cannot purge items that are not upstream anymore
In-Reply-To: <54635F40.105@treenet.co.nz>
References: <1630701.7i1R41SJLv@hades> <8429980.HusbksOlJk@hades>
 <54635F40.105@treenet.co.nz>
Message-ID: <2620417.F4s5CzuQaP@hades>

On Thursday 13 November 2014 02:23:12 Amos Jeffries wrote:
> On 13/11/2014 1:55 a.m., Hussam Al-Tayeb wrote:
> > On Thursday 13 November 2014 01:39:27 Amos Jeffries wrote:
> >> On 13/11/2014 12:17 a.m., Hussam Al-Tayeb wrote:
> >>> Hello. I have a problem with 'squidclient -m PURGE' and also
> >>> the purge command. They won't purge urls from disk that are
> >>> not available online anymore or redirect to other links.
> >> 
> >> PURGE was designed for use in HTTP/1.0. It does not handle
> >> HTTP/1.1 negotiated content / variants at all well.
> >> 
> >>> For example,
> >>> http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b
> >>> 64
> 
> e8d563.jpg
> 
> >> which corresponds to /home/squid/00/BB/0000BBC0
> >> 
> >>> Even "purge -e "static.firedrive.com" -c /etc/squid/squid.conf
> >>> -P 0x01" reads it but will not really remove it from disk. Are
> >>> such files stuck on disk forever?
> >> 
> >> No. Cache is a temporary storage location. Think of it as a
> >> buffer in the network.
> >> 
> >> They will exist only until a) the storage space is needed for
> >> something else, or b) the timestamp in their Expires: header, or
> >> c) an origin server informs Squid they are no longer existing.
> >> 
> >> PURGE method was a way to fake (c).
> >> 
> >>> What would the correct way to clear them?
> >> 
> >> By obeying HTTP protocol. HTTP has built-in mechanisms for doing
> >> that automatically.
> >> 
> >> Or you can just delete the disk file. Squid will auto-detect the
> >> removal at some point when it needs to use or delete the object.
> >> Until then it will just under-count the amount of used disk.
> >> 
> >> Amos
> >> 
> >> _______________________________________________ squid-users
> >> mailing list squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> > 
> > head /home/squid/00/BB/0000BBC0 -n20
> > 
> > ?u??(???,?|?S?|
> > ?S??f7?P`Uhttp://static.firedrive.com/dynamic/previews/75/27577be2d6d86af2
> > 0265734b64e8d563.jpg
> R"accept-encoding="gzip,%20deflate"HTTP/1.1 200 OK
> 
> ... notice and remember the accept-encoding=* value in quotes above.
> We shall get back to that later.
> 
> Now for a little tale of woe...
> 
> On this Date:
> > Date: Tue, 26 Aug 2014 12:25:13 GMT
> 
> ... the
> 
> > Server: cloudflare-nginx
> 
> ... representing static.firedrive.com has expicitly with complete
> 
> authority state that the:
> > Content-Type: image/jpeg
> 
> ... otherwise known as:
> > ETag: "5090c137-2efb"
> 
> ... should remain in cache until:
> > Expires: Fri, 23 Aug 2024 12:25:13 GMT
> 
> ... should remain in cache until Aug 2024.
> 
> Furthermore that:
> > Last-Modified: Wed, 31 Oct 2012 06:12:07 GMT Cache-Control: public,
> > max-age=315360000
> 
> ... there is no need for any recipient storing the object to even
> check for validity again until Oct 2022.
> 
> > Access-Control-Allow-Origin: * Access-Control-Allow-Methods: GET,
> > POST, OPTIONS Access-Control-Allow-Headers:
> > Origin,Referer,Accept-Encoding,Accept-
> > Language,Accept,DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,I
> > f-
> Modified-Since,Cache-Control,Content-Type,X-Forwarded-For,X-Forwarded-Proto
> 
> > CF-Cache-Status: HIT Vary: Accept-Encoding Accept-Ranges: bytes
> > CF-RAY: 160002c2e4730887-FRA Content-Length: 12027 Connection:
> > Keep-Alive Set-Cookie:
> > __cfduid=dfcf568ed46ef827243b3d6c342b3bdc41409055913424;
> > expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/;
> > domain=.firedrive.com; HttpOnly
> 
> ... and that the Cookies it served up shall remain on the users
> machine until Dec 2019. Thereafter this object shall be served without
> any Cookie.
> 
> > The Expires header is in the past.
> > "http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b6
> > 4e8d563.jpg"
> Sending HTTP request ... done.
> 
> > HTTP/1.1 404 Not Found Server: squid Mime-Version: 1.0 Date: Wed,
> > 12 Nov 2014 12:54:13 GMT Content-Length: 0 X-Cache: MISS from
> > SERV1 X-Cache-Lookup: NONE from SERV1:3129 Via: 1.1 SERV1 (squid)
> > Connection: close
> > 
> > Is that why  squidclient -m PURGE -h 127.0.0.1 -p 3129 says not
> > found in cache?
> 
> The Vary: header is why your PURGE is not doing anything. It means you
> have to find and send in the exactly right value for Accept-Encoding,
> in order for PURGE to find and erase the actual object.
> 
> The request *not* having an Accept-Encoding header is one of the
> possible variations of values. In particular it is the one which is
> foudn to already be absent from your cache by your PURGE command.
> 
> This is where that quoted string up top of the object file comes in.
> What is cached on disk is the object for:
> squidclient -m PURGE -p 3129 -H 'Accept-Encoding: gzip, deflate\n'
> 
> The purge tool has not been updated in some years and does not support
> these types of variant commonly found in HTTP/1.1 traffic.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Ok, thank you for the explanation. seems wget and curl also say 
"X-Cache: MISS from SERV1
X-Cache-Lookup: NONE from SERV1:3129"

So the current natural ways of purging the object is waiting till squid runs 
out of space and rotates old objects or reaching the Expire date of 2024, 
correct?
Both options are ok. I just wanted to make sure squid is self cleaning even if 
things take time.


From squid3 at treenet.co.nz  Wed Nov 12 14:36:35 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 03:36:35 +1300
Subject: [squid-users] cache peer problem with two squid one Tproxy
 --->normal Porxy
In-Reply-To: <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
 <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
Message-ID: <54637073.1090801@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 1:02 p.m., Ahmed Allzaeem wrote:
> Thanks amos
> 
> I have added option notproxy on the tproxy one It became : 
> cache_peer 77.221.104.97  parent 3127 0 no-tproxy
> 
> 
> also I changed hostnames in /etc.hosts for both servers and added
> visible hostname  squid for both

/etc/hosts is for resolving *other* machines IPs/names.

/etc/hostname is where the server host name is configured. Or if you
are using a GUI it may need to be done via one of the control panels
to ensure it gets set properly.


> 
> now on the normal proxy I can see the logs access
> 
> but still not traffic it give me access denied from the parent
> proxy ?!! any help ?

Find out why the denial. It could be the hostname still, or the first
proxies outbound IP vs. your second proxies http_access rules, or it
could even be coming from the origin server out on the Internet.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY3BzAAoJELJo5wb/XPRjhvYH/1T5cl+N8CSRzFTvcDyW7gmZ
mIohmmQJUOS1seudbO0NrMjOuwHkaWZKNz2Obrwyk3vLTeV2OYPaM9CB4XlYOuDM
XrwyPAdSexKvtimrSVO2sGjpYy4tx2vZ58cTIgkS0LPqVDCY4+QTCRgX1UbDrUQ+
SALieUD1JTJDcE9L7l/xA/oRndINTeT3/J3aeNRvx32l5gOsMRmreGgmHiCLZo/1
N2Dfl2xDxzD+1Z1OMY5DaV/zh7kA5jpKUgOz3W7kMSq6kdKtmZSzcXz8tlyoKsP8
/WjQqr/eIfV4kFuLj2Ymw4euAZPUJfL926uFoEqhBdz24RgS/g9T1KW7mp0deeI=
=IJ+G
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 12 14:43:04 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 03:43:04 +1300
Subject: [squid-users] Cannot purge items that are not upstream anymore
In-Reply-To: <2620417.F4s5CzuQaP@hades>
References: <1630701.7i1R41SJLv@hades> <8429980.HusbksOlJk@hades>
 <54635F40.105@treenet.co.nz> <2620417.F4s5CzuQaP@hades>
Message-ID: <546371F8.7070409@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 3:04 a.m., Hussam Al-Tayeb wrote:
> On Thursday 13 November 2014 02:23:12 Amos Jeffries wrote:
>> On 13/11/2014 1:55 a.m., Hussam Al-Tayeb wrote:
>>> On Thursday 13 November 2014 01:39:27 Amos Jeffries wrote:
>>>> On 13/11/2014 12:17 a.m., Hussam Al-Tayeb wrote:
>>>>> Hello. I have a problem with 'squidclient -m PURGE' and
>>>>> also the purge command. They won't purge urls from disk
>>>>> that are not available online anymore or redirect to other
>>>>> links.
>>>> 
>>>> PURGE was designed for use in HTTP/1.0. It does not handle 
>>>> HTTP/1.1 negotiated content / variants at all well.
>>>> 
>>>>> For example, 
>>>>> http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b
>>>>>
>>>>> 
64
>> 
>> e8d563.jpg
>> 
>>>> which corresponds to /home/squid/00/BB/0000BBC0
>>>> 
>>>>> Even "purge -e "static.firedrive.com" -c
>>>>> /etc/squid/squid.conf -P 0x01" reads it but will not really
>>>>> remove it from disk. Are such files stuck on disk forever?
>>>> 
>>>> No. Cache is a temporary storage location. Think of it as a 
>>>> buffer in the network.
>>>> 
>>>> They will exist only until a) the storage space is needed
>>>> for something else, or b) the timestamp in their Expires:
>>>> header, or c) an origin server informs Squid they are no
>>>> longer existing.
>>>> 
>>>> PURGE method was a way to fake (c).
>>>> 
>>>>> What would the correct way to clear them?
>>>> 
>>>> By obeying HTTP protocol. HTTP has built-in mechanisms for
>>>> doing that automatically.
>>>> 
>>>> Or you can just delete the disk file. Squid will auto-detect
>>>> the removal at some point when it needs to use or delete the
>>>> object. Until then it will just under-count the amount of
>>>> used disk.
>>>> 
>>>> Amos
>>>> 
>>>> _______________________________________________ squid-users 
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> 
>>> head /home/squid/00/BB/0000BBC0 -n20
>>> 
>>> ?u??(???,?|?S?| 
>>> ?S??f7?P`Uhttp://static.firedrive.com/dynamic/previews/75/27577be2d6d86af2
>>>
>>> 
0265734b64e8d563.jpg
>> R"accept-encoding="gzip,%20deflate"HTTP/1.1 200 OK
>> 
>> ... notice and remember the accept-encoding=* value in quotes
>> above. We shall get back to that later.
>> 
>> Now for a little tale of woe...
>> 
>> On this Date:
>>> Date: Tue, 26 Aug 2014 12:25:13 GMT
>> 
>> ... the
>> 
>>> Server: cloudflare-nginx
>> 
>> ... representing static.firedrive.com has expicitly with
>> complete
>> 
>> authority state that the:
>>> Content-Type: image/jpeg
>> 
>> ... otherwise known as:
>>> ETag: "5090c137-2efb"
>> 
>> ... should remain in cache until:
>>> Expires: Fri, 23 Aug 2024 12:25:13 GMT
>> 
>> ... should remain in cache until Aug 2024.
>> 
>> Furthermore that:
>>> Last-Modified: Wed, 31 Oct 2012 06:12:07 GMT Cache-Control:
>>> public, max-age=315360000
>> 
>> ... there is no need for any recipient storing the object to
>> even check for validity again until Oct 2022.
>> 
>>> Access-Control-Allow-Origin: * Access-Control-Allow-Methods:
>>> GET, POST, OPTIONS Access-Control-Allow-Headers: 
>>> Origin,Referer,Accept-Encoding,Accept- 
>>> Language,Accept,DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,I
>>>
>>> 
f-
>> Modified-Since,Cache-Control,Content-Type,X-Forwarded-For,X-Forwarded-Proto
>>
>>>
>> 
CF-Cache-Status: HIT Vary: Accept-Encoding Accept-Ranges: bytes
>>> CF-RAY: 160002c2e4730887-FRA Content-Length: 12027 Connection: 
>>> Keep-Alive Set-Cookie: 
>>> __cfduid=dfcf568ed46ef827243b3d6c342b3bdc41409055913424; 
>>> expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/; 
>>> domain=.firedrive.com; HttpOnly
>> 
>> ... and that the Cookies it served up shall remain on the users 
>> machine until Dec 2019. Thereafter this object shall be served
>> without any Cookie.
>> 
>>> The Expires header is in the past. 
>>> "http://static.firedrive.com/dynamic/previews/75/27577be2d6d86af20265734b6
>>>
>>> 
4e8d563.jpg"
>> Sending HTTP request ... done.
>> 
>>> HTTP/1.1 404 Not Found Server: squid Mime-Version: 1.0 Date:
>>> Wed, 12 Nov 2014 12:54:13 GMT Content-Length: 0 X-Cache: MISS
>>> from SERV1 X-Cache-Lookup: NONE from SERV1:3129 Via: 1.1 SERV1
>>> (squid) Connection: close
>>> 
>>> Is that why  squidclient -m PURGE -h 127.0.0.1 -p 3129 says
>>> not found in cache?
>> 
>> The Vary: header is why your PURGE is not doing anything. It
>> means you have to find and send in the exactly right value for
>> Accept-Encoding, in order for PURGE to find and erase the actual
>> object.
>> 
>> The request *not* having an Accept-Encoding header is one of the 
>> possible variations of values. In particular it is the one which
>> is foudn to already be absent from your cache by your PURGE
>> command.
>> 
>> This is where that quoted string up top of the object file comes
>> in. What is cached on disk is the object for: squidclient -m
>> PURGE -p 3129 -H 'Accept-Encoding: gzip, deflate\n'
>> 
>> The purge tool has not been updated in some years and does not
>> support these types of variant commonly found in HTTP/1.1
>> traffic.
>> 
>> Amos
> 
> Ok, thank you for the explanation. seems wget and curl also say 
> "X-Cache: MISS from SERV1 X-Cache-Lookup: NONE from SERV1:3129"

Expected. It's hard to set exactly the right header contents with
those tools. Thus squidclient is recommended when manually doing
fine-grained testing.

> 
> So the current natural ways of purging the object is waiting till
> squid runs out of space and rotates old objects or reaching the
> Expire date of 2024, correct? Both options are ok. I just wanted to
> make sure squid is self cleaning even if things take time.

Yes, it is.

To be more precise Squid currently will revalidate objects more than 1
year old if things actually stay in cache that long. Most times the
traffic throughput is big enough to push objects out of cache after
only a month or two even when configuring huge cache_dir spaces.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY3H3AAoJELJo5wb/XPRjr3sH/15KC9dV0VlqTdS6b/QKcBOM
LhCcNOhod/qVibtX0j4W1R+FtPvW+ZZqrgl/d3+XA50PF65Bsp6UAG9Greq1Kwav
cqu1tkhV/aokLgxxi9YdUcphLqq15hGdEusx/+XwwH59sPDqR9+PpIbRnWyLwBG9
b7TGgZFjNXvK4/8CLXcOPgjm0L5ML4mo0DPf4AYPCKo6kRJ7/AWNKBt93aanQcFx
dA6UIjvdWf8T86uF5NJXSHQf4T75fTRiTAZSDIiN8Q8TogeEG1ys8IUa5XZi008d
nst0I4EopqZalbh2e4yQ1taCNpR0wjIRdbwXzQVu1WxytnyrfRql/1gunTYGq8A=
=vDUl
-----END PGP SIGNATURE-----


From lorenzo.gollinelli at gmail.com  Wed Nov 12 16:47:40 2014
From: lorenzo.gollinelli at gmail.com (Lorenzo Gollinelli)
Date: Wed, 12 Nov 2014 08:47:40 -0800
Subject: [squid-users] Squid 3.4.6 POST upload problem
Message-ID: <CABMqo8-gx5d88MiDUTKQCR1VQVwprnN3X0dZAy7aAcK=WBXTnA@mail.gmail.com>

Hello,

we have squid 3.4.6 talking to websense over icap.
We have problems in uploading files larger than 55 kB.

this is the icap.log when file is correctly uploaded (<55kB):

1415810436.490      0 192.168.x.x TAG_NONE/000 0 POST
http://www.csm-testcenter.org/test DOMAIN/user HIER_NONE/- -

this is the icap.log when file upload hangs:

1415810513.657      0 192.168.x.x TCP_MISS/000 0 POST
http://www.csm-testcenter.org/test DOMAIN/user HIER_DIRECT/85.214.28.69 -

It looks like there is something somewhere that behaves differently
according to the uploaed file size.
We only use cache_mem (no disk), 5 workers, and kerberos/ntlm
authentication (bypassed without luck).

Any idea?

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141112/5ee8d752/attachment.htm>

From pag at nanosec.com  Wed Nov 12 17:13:16 2014
From: pag at nanosec.com (Peter Gross)
Date: Wed, 12 Nov 2014 10:13:16 -0700
Subject: [squid-users] Problem with https://www.google.com and
	squid	interception
In-Reply-To: <1536817904.20202.1415765137757.JavaMail.yahoo@jws10610.mail.bf1.yahoo.com>
References: <5462D4AF.6060700@treenet.co.nz>
 <1536817904.20202.1415765137757.JavaMail.yahoo@jws10610.mail.bf1.yahoo.com>
Message-ID: <5463952C.3010001@nanosec.com>

On 11/11/2014 9:05 PM, Yogesh Gawankar wrote:
> hello peter
>
> can you check if your squid does gre return?

Yohesh -- there are no Cisco routers in my home network, so no GRE. I 
will be using GRE when/if I configure squid at work since we have Cisco 
routers/switches in that network.

> It is unrelated to your issue though :)
> *Thanks and regards
>
> Yogesh Gawankar*
>
> **
>
>
> On Wednesday, November 12, 2014 9:02 AM, Amos Jeffries
> <squid3 at treenet.co.nz> wrote:
>
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 12/11/2014 7:47 a.m., Peter Gross wrote:
>  > Hi, I am a new user of Squid and would first like to thank the
>  > developers for this excellent software. This is my first post to
>  > the mailing list ... I have been tasked with setting up quite
>  > restrictive web access control at work. I plan to use an
>  > intercepting squid proxy with SSL bump. There will also be WCCPv2
>  > to/from a Cisco IOS router. Since this is quite a bit of
>  > complexity, I though it prudent to start slowly, in steps. So first
>  > -- to get my feet wet -- I set up squid (version 3.4.8, built using
>  > rpmbuild from the src rpm from ngtech) on a home linux server
>  > (Centos 5.11 -- no Cisco at home) which is also the firewall router
>  > for my home network. I also decided to start out with plain vanilla
>  > proxying (no interception -- use browser setting). This worked
>  > fine. I then tested HTTP interception by changing squid.conf from:
>  > http_port 3128 -to- http_port 3128 intercept
>
> Please use a second http_port with a different number for each input
> "mode". Squid needs at least one forward-proxy port for things like
> icons URLs in error pages - 3128 is the port number W3C allocated for
> that.
>
>  >
>  > and adding the following rule to my shorewall firewall:
>  > REDIRECT:info  loc:192.168.101.9      3128    tcp    http
>  >
>  > I wanted to test intercepting just one host before turning it on
>  > for all hosts and wireless devices in my network.
>  >
>  > 192.168.101.9 is another Centos PC on my network. Squid is running
>  > on 192.168.101.253.
>  >
>  > The interception seemed to work fine ... access.log showed lots of
>  > successful proxy activity. Then came the problem: going to
>  > https://www.google.com <https://www.google.com/>failed (not every
> time, but frequently). If
>  > I turned off the REDIRECT line in the shorewall rules file and
>  > restarted shorewall, no problem. This seemed very peculiar because
>  > no HTTPS traffic should be redirected to the proxy.
>
> Correct. Very perculiar.
>
>  > Here are the errors that showed up in cache.log when redirection
>  > (NAT-ing) was on:
>  >
>  > 2014/11/11 11:03:42 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed
>  > on local=192.168.101.253:3128 remote=192.168.101.9:34165 FD 11
>  > flags=33: (92) Protocol not available
>
> That is forward-proxy traffic going to the port with intercept on it.
> Separating the ports should help you identify if this is error pages
> embeded objects or not.
>
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUYtSvAAoJELJo5wb/XPRjcJ0H/jeEbRpFJYp0v5nfTWlwdOTV
> ajA2SpQ+3GY0s8iPX0FJnl2mFSfNVs5V+515/iOSGfEUhIS/+qGyWCqIjA79tTHy
> dTx9eKOc1boF/7nadgfFl/60rpJprNfuosh9iIhkDShC3bNzz3y5lcPVyPi4bhKD
> wkG/dT5GdUdVTVn1aA1cojebrJ2SUMe/NMyFdEADIyOLSNsDT000MMB4Mr3VnVBA
> 68nq6wdGdnK0/ydm/OvErruVgPqQGP/IvpdLaHw0+2ck2fYRlzgB9+6P1an24rDA
> hgsn0sZ/MfIxJd/biC5Pk0gnNzapVI+n4E2NIx+F0aN/y2Bgn0+AncvEqKnSS3U=
> =tp1A
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From pag at nanosec.com  Wed Nov 12 18:27:27 2014
From: pag at nanosec.com (Peter Gross)
Date: Wed, 12 Nov 2014 11:27:27 -0700
Subject: [squid-users] Problem with https://www.google.com and squid
	interception
In-Reply-To: <546259B7.3020103@nanosec.com>
References: <546259B7.3020103@nanosec.com>
Message-ID: <5463A68F.8090002@nanosec.com>

On 11/11/2014 11:47 AM, Peter Gross wrote:
> Hi,
> I am a new user of Squid and would first like to thank the developers
> for this excellent software. This is my first post to the mailing list
> ... I have been tasked with setting up quite restrictive web access
> control at work. I plan to use an intercepting squid proxy with SSL
> bump. There will also be WCCPv2 to/from a Cisco IOS router. Since this
> is quite a bit of complexity, I though it prudent to start slowly, in
> steps. So first -- to get my feet wet -- I set up squid (version 3.4.8,
> built using rpmbuild from the src rpm from ngtech) on a home linux
> server (Centos 5.11 -- no Cisco at home) which is also the firewall
> router for my home network. I also decided to start out with plain
> vanilla proxying (no interception -- use browser setting). This worked
> fine. I then tested HTTP interception by changing squid.conf from:
> http_port 3128
>    -to-
> http_port 3128 intercept

Based on a suggestion from Amos Jeffries, I changed my squid.conf 
http_port directives to:

http_port 192.168.101.253:3128
http_port 192.168.101.253:3129 intercept
#https_port 192.168.101.253:3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem

Note that the https_port directive is commented out for now because I 
just wanted to get HTTP interception working first. I have changed to 
using the default proxy port of 3128 as a (no-intercept) forwarding port 
and changed the intercept port to use 3129. Also -- and this may be 
important in my case -- I added the squid server internal LAN IP address 
to the http_port directives. This host has multiple NICs and I wonder if 
not specifying the eth1 address (eth0 is for internet access) that 
something weird happened before. In any case going to 
https://www.google.com now works every time (keeping fingers crossed).


> and adding the following rule to my shorewall firewall:
> REDIRECT:info   loc:192.168.101.9       3128    tcp     http
>
> I wanted to test intercepting just one host before turning it on for all
> hosts and wireless devices in my network.
>
> 192.168.101.9 is another Centos PC on my network. Squid is running on
> 192.168.101.253.
>
> The interception seemed to work fine ... access.log showed lots of
> successful proxy activity. Then came the problem: going to
> https://www.google.com failed (not every time, but frequently). If I
> turned off the REDIRECT line in the shorewall rules file and restarted
> shorewall, no problem. This seemed very peculiar because no HTTPS
> traffic should be redirected to the proxy. Here are the errors that
> showed up in cache.log when redirection (NAT-ing) was on:
>
> 2014/11/11 11:03:42 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.101.253:3128 remote=192.168.101.9:34165 FD 11 flags=33:
> (92) Protocol not available
>
> Note that other HTTPS sites worked fine! It appears to be confined to a
> google specific issue.
>
> Thanks for any comments/suggestions you might have,
> --peter


From Jason_Haar at trimble.com  Wed Nov 12 19:15:18 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 13 Nov 2014 08:15:18 +1300
Subject: [squid-users] connecting directly to ssl-bump intercept port
 causes runaway CPU
In-Reply-To: <5462F74D.8060609@treenet.co.nz>
References: <5462E6CE.7040506@trimble.com> <5462F74D.8060609@treenet.co.nz>
Message-ID: <5463B1C6.801@trimble.com>

On 12/11/14 18:59, Amos Jeffries wrote:
>
> That being one of the "NAT security vulnerabilities" mentioned as
> reason for mangle table rules.

Sorry, I should have said that if I remove the iptables 443 redirect
rule, it still occurs!

>
> 3) Squid connected there to fetch the SSL certificate details.
>
> 4) The TCP connection from Squid to Squid:3126 was opened.
>
> 5) go back to #2
>
>
> HTTP has the Via header mechanism to restrict the loop to protect
> against the DoS becoming a wide problem. But notice how no HTTP is
> taking place, Squid is still trying to perform the TLS/SSL operations.
> There is no protection at the TCP layer.
>

Ah! Yeah - nasty

>
> Please stop using telnet and manual data entry to test HTTP services.
> If you knew enough about the protocol to type it into telnet correctly
> the above loop problems would not be surprising you.
>

So you're saying this is expected behaviour? Squid with HTTPS intercept
enabled is vulnerable to DoS attacks with a simple 3-way?

Now that you have explained it to me, I can see the issue, but I think
squid needs to do something about it. eg when squid starts it could make
a note of all it's own IP addresses:ports, then if someone asks squid to
connect to those combinations, reject it. There is no valid case where
squid should ever be used to connect to itself that I can think of

I just made the following changes

acl localSquidServer    src 127.0.0.1 proxy.ip.address
deny_info ERR_LOCALSQUID    localSquidServer
http_access deny localSquidServer

Now when I connect to the HTTP (not https!) intercept port, I see the
html error page ERR_LOCALSQUID - great - works like it says on the box.
However, that deny doesn't trigger for https (it's the first http_access
entry BTW). Shouldn't that be a mechanism to filter this DoS out? If
not, shouldn't there be "https_access" to achieve the same thing? I also
tried using iptables to exclude such connections, but my proxy is my
default gateway and I couldn't come up with a rule that worked without
breaking redirection all together.

> A proper HTTP client software like squidclient, wget, curl, or even a
> full browser should be used.

I explicit mentioned telnet (nc is the same) because it means it
triggers on any successful TCP 3-way handshake - no data needs to flow
for the fault to trigger. If I call "curl http://proxy.server:3127" it
also triggers the runaway CPU (3127 being my https intercept port of
course).

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From Jason_Haar at trimble.com  Wed Nov 12 19:23:00 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 13 Nov 2014 08:23:00 +1300
Subject: [squid-users] connecting directly to ssl-bump intercept port
 causes runaway CPU
In-Reply-To: <5462F74D.8060609@treenet.co.nz>
References: <5462E6CE.7040506@trimble.com> <5462F74D.8060609@treenet.co.nz>
Message-ID: <5463B394.2030502@trimble.com>

Typical, I figured out an iptables workaround within seconds of sending
my last email

I still think squid needs to be able to stop this DoS, but this will
stop the issue occurring

iptables -t nat -A PREROUTING -d proxy.ip -i lan.interface -p tcp -m tcp
--dport 3127 -j REDIRECT --to-ports 9876 #9876 has nothing running on it
iptables -t nat -A PREROUTING  ! -d lan.subnet/netmask  -i lan.interface
-p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3127 #3127 is our ssl
intercept port

So I get "connection refused" when I try to connect to the proxy on port
3127, but https intercept still works for anything else. Now squid never
sees the direct 3127 connection and so never goes into a loop

Jason

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From santosh.pai at vigyanlabs.com  Wed Nov 12 19:27:57 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Wed, 12 Nov 2014 11:27:57 -0800 (PST)
Subject: [squid-users] Forceful Reauthentication
Message-ID: <1415820477635-4668344.post@n4.nabble.com>

Hello Team,

I'm trying to reauthenticate the user once he visits google as per this url 
https://workaround.org/squid-acls <https://workaround.org/squid-acls>   but
it doesnt seem to reprompt the credentials when i access google  , below are
my rules let me know where i'm going wrong 


#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

#conatins acl to block sites 
acl bad_url url_regex "/etc/squid3/badsites.conf"

auth_param basic program /usr/lib/squid3/squid_ldap_auth -b
"dc=example,dc=com" -f "uid=%s" -h example.com
acl ldapauth proxy_auth REQUIRED
acl reauth dstdomain .google.co.in
http_access deny bad_url
http_access allow ldapauth
http_access deny  reauth ldapauth
http_access deny all





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Forceful-Reauthentication-tp4668344.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Thu Nov 13 06:39:11 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Wed, 12 Nov 2014 22:39:11 -0800
Subject: [squid-users] cache peer problem with two squid one Tproxy
	--->normal Porxy
In-Reply-To: <54637073.1090801@treenet.co.nz>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
 <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
 <54637073.1090801@treenet.co.nz>
Message-ID: <001201cfff0c$8a09eb30$9e1dc190$@netstream.ps>

Hi amos 

I have changed the both hostnames on two servers :

[root at tproxy ~]# hostname
tproxy.com


[root at parent ~]# hostname
parent.com


but , as I told u last time
I can see traffic "miss" on the normal proxy , and "miss" on the tproxy server.

But it says access denied  from normal proxy 

I mean on the normal proxy "parent" there is only miss and no Denied hits , but it give me error access denid.

Also I made sure that the ip of tproxy is allowed by acl on the normal proxy"parent"


Again , here is the  cache log @ the parent proxy , still says a loop occurring :

2014/11/12 23:33:24 kid1| WARNING: Forwarding loop detected for:
GET / HTTP/1.1
Host: abc.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Via: 1.1 squid (squid/3.4.3)
X-Forwarded-For: 176.58.79.248
Cache-Control: max-age=259200
Connection: keep-alive


2014/11/12 23:33:24 kid1| WARNING: Forwarding loop detected for:
GET /Artwork/SN.png HTTP/1.1
Host: www.squid-cache.org
User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0
Accept: image/png,image/*;q=0.8,*/*;q=0.5
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Referer: http://abc.com/
Via: 1.1 squid (squid/3.4.3)
X-Forwarded-For: 176.58.79.248
Cache-Control: max-age=259200
Connection: keep-alive


Why there is a loop still ???








-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Wednesday, November 12, 2014 6:37 AM
To: Ahmed Allzaeem; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer problem with two squid one Tproxy --->normal Porxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 1:02 p.m., Ahmed Allzaeem wrote:
> Thanks amos
> 
> I have added option notproxy on the tproxy one It became : 
> cache_peer 77.221.104.97  parent 3127 0 no-tproxy
> 
> 
> also I changed hostnames in /etc.hosts for both servers and added 
> visible hostname  squid for both

/etc/hosts is for resolving *other* machines IPs/names.

/etc/hostname is where the server host name is configured. Or if you are using a GUI it may need to be done via one of the control panels to ensure it gets set properly.


> 
> now on the normal proxy I can see the logs access
> 
> but still not traffic it give me access denied from the parent proxy 
> ?!! any help ?

Find out why the denial. It could be the hostname still, or the first proxies outbound IP vs. your second proxies http_access rules, or it could even be coming from the origin server out on the Internet.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUY3BzAAoJELJo5wb/XPRjhvYH/1T5cl+N8CSRzFTvcDyW7gmZ
mIohmmQJUOS1seudbO0NrMjOuwHkaWZKNz2Obrwyk3vLTeV2OYPaM9CB4XlYOuDM
XrwyPAdSexKvtimrSVO2sGjpYy4tx2vZ58cTIgkS0LPqVDCY4+QTCRgX1UbDrUQ+
SALieUD1JTJDcE9L7l/xA/oRndINTeT3/J3aeNRvx32l5gOsMRmreGgmHiCLZo/1
N2Dfl2xDxzD+1Z1OMY5DaV/zh7kA5jpKUgOz3W7kMSq6kdKtmZSzcXz8tlyoKsP8
/WjQqr/eIfV4kFuLj2Ymw4euAZPUJfL926uFoEqhBdz24RgS/g9T1KW7mp0deeI=
=IJ+G
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Wed Nov 12 20:44:17 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 12 Nov 2014 22:44:17 +0200
Subject: [squid-users] connecting directly to ssl-bump intercept port
	causes runaway CPU
In-Reply-To: <5463B1C6.801@trimble.com>
References: <5462E6CE.7040506@trimble.com> <5462F74D.8060609@treenet.co.nz>
 <5463B1C6.801@trimble.com>
Message-ID: <5463C6A1.6090709@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Jason,

Indeed it is nasty.
I do not remember now how I advised in the past to defend against this
issue.
There is a "risk" in every system operation and this is one of them.
You indeed found this "bug" or security vulnerability!

Specially on linux the main way to handle this issue is disabling
direct access to the intercepting port.
The options on linux are basically iptables mangle table rules.
There is however the specific case that iptables do not cover and it's
the case is:(sysadmin R rated knowledge ahead)
using a remote ip with a specific address such as localhost which is
defined on the proxy.
practical example:
# nc -v 8.8.8.8 80
> GET http://localhost/ HTTP/1.1 Host: localhost
...
##END of example
The above example was tested in the past on too many systems and was
and still on some there.
Some will call it a bug and others will call it a feature.
I would say something similar to what I have seen in the old movie "3
ninjas":
"with power comes responsibility"

Squid has the means to defend against such vulnerability but only a
good admin will be able to use them.

All The Bests,
Eliezer

* 3 ninjas(PG) - http://www.imdb.com/title/tt0103596/


On 11/12/2014 09:15 PM, Jason Haar wrote:
> Ah! Yeah - nasty
> 
>>> 
>>> Please stop using telnet and manual data entry to test HTTP
>>> services. If you knew enough about the protocol to type it into
>>> telnet correctly the above loop problems would not be
>>> surprising you.
>>> 
> So you're saying this is expected behaviour? Squid with HTTPS
> intercept enabled is vulnerable to DoS attacks with a simple
> 3-way?
> 
> Now that you have explained it to me, I can see the issue, but I
> think squid needs to do something about it. eg when squid starts it
> could make a note of all it's own IP addresses:ports, then if
> someone asks squid to connect to those combinations, reject it.
> There is no valid case where squid should ever be used to connect
> to itself that I can think of
> 
> I just made the following changes
> 
> acl localSquidServer    src 127.0.0.1 proxy.ip.address deny_info
> ERR_LOCALSQUID    localSquidServer http_access deny
> localSquidServer
> 
> Now when I connect to the HTTP (not https!) intercept port, I see
> the html error page ERR_LOCALSQUID - great - works like it says on
> the box. However, that deny doesn't trigger for https (it's the
> first http_access entry BTW). Shouldn't that be a mechanism to
> filter this DoS out? If not, shouldn't there be "https_access" to
> achieve the same thing? I also tried using iptables to exclude such
> connections, but my proxy is my default gateway and I couldn't come
> up with a rule that worked without breaking redirection all
> together.
> 
>>> A proper HTTP client software like squidclient, wget, curl, or
>>> even a full browser should be used.
> I explicit mentioned telnet (nc is the same) because it means it 
> triggers on any successful TCP 3-way handshake - no data needs to
> flow for the fault to trigger. If I call "curl
> http://proxy.server:3127" it also triggers the runaway CPU (3127
> being my https intercept port of course).
> 
> Thanks

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUY8agAAoJENxnfXtQ8ZQUVZwIAIYMgkYvUwvBG346VIeTFso5
b5qyzM9iTk6M/xWKO9Ml3ZfNh2EE4yFcdSyj3du+dRwXYadqRmOOn+FOskEWHIrd
4S5K/7wF3fsXFXJbie54QwY1o32rS8R4ZA/3NANrJt+n5/kHTccp/7UtQm5rCL/8
9eKAvQPBqvx/hfB1nfZkRpbMn5bHJKG0LUQDigmCIKBMkgjFoBRfpiFz5TNHcohS
/n7JZAUalCLqpfI9n1TSZ43wfvzKj+/f0ToEEdyxXH7K6+/QfjKeQYhAkhmNNpky
T1/ceT4/QQGzcClxnv4Lm/ZOLk37s+UWHCevvvJGlfGjUIO/5D9JAtLSfY6gG4E=
=AiTp
-----END PGP SIGNATURE-----


From david at articatech.com  Wed Nov 12 21:37:55 2014
From: david at articatech.com (David Touzeau)
Date: Wed, 12 Nov 2014 22:37:55 +0100
Subject: [squid-users] High CPU-Usage with squid 3.4.9 (and/or 3.4.4)
In-Reply-To: <546100B9.7000102@ngtech.co.il>
References: <CACLJR+PFGgwCDd1Fm9Lf8iSdtRhqupSoQaAua9RZtu6-E9O_1g@mail.gmail.com>
 <5460BF5A.6030400@treenet.co.nz>
 <1FCF9DA5B29068478ECF15896F19F0840138A54E5E@Y011008.bk.fin.local>
 <5460E0D1.6000303@treenet.co.nz> <5460F8CC.5020303@urlfilterdb.com>
 <546100B9.7000102@ngtech.co.il>
Message-ID: <5463D333.90409@articatech.com>

-----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1 On 11/10/2014 07:41 PM, 
Marcus Kool wrote:
>> Indeed but setting debug_options to ALL,9 does not work since the
>> log file already is too big and unmanageable even before Squid
>> begins to do thing that consumes CPU time.
> I have suggested a full one request cycle comparison but not everybody
> can follow this path easily.
>
> Eliezer
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUYQC5AAoJENxnfXtQ8ZQUgKcH/1fblv01RrTrXMkViqPYKVSr
> zLfjwV+4EsGjmMc3BC3qPXljcVJj2GI/8++TBzdPZzPvqzVoES/ZpySOpfhZ8A5N
> +vVKOIoxwi8008Vkp0P3q8snxdcVLWlcWM5FCxYvWpSuy7XNdcYm3BPefkZU+HM4
> utvpvE35JBeejp3PtwOVya1J8nVX18lvusjExZSgIHkepcwS429dXCu3cNedQ5RZ
> 0yKdKLcYlwh7EHJ7zpAf1NZJxg7NwX4i46tmkWJBDxsae+x4UkLCNiJCUwrdaVuQ
> Jc4RhblQjWqH1ZWHFHyfKpeOMy5xpjAe2jbqDrLE+f2sz0X54Ci2mvuNyninmSY=
> =HP6z
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Hi

We confirm, we using PHP helpers and Active Directory connection.
We have tested 3.4.9 on 50 users only.
Kids turn to 98% CPU after few minutes.

Here the htop screenshot
http://articatech.net/img/squid-high-cpu-level.png

Returning back to 3.3.13 fix the issue.
CPU run no more 5% and load is about 0.7





From Jason_Haar at trimble.com  Wed Nov 12 22:55:52 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 13 Nov 2014 11:55:52 +1300
Subject: [squid-users] OT: why does openssl-1.0.1f not like
	https://www.bnz.co.nz/?
Message-ID: <5463E578.5060408@trimble.com>

Hi there

I just found I cannot connect to https://www.bnz.co.nz/ using curl on
Ubuntu (7.35 compiled against openssl-1.0.1f), whereas
https://www.kiwibank.co.nz/ works fine. I first thought it was due to my
messing around with ssl-bump, but it happens when I don't go through
squid too

I have a CentOS-6 server with curl-7.19 (compiled against 1.0.1e) and it
works fine. The same happens with "openssl s_client": it works on CentOS
but not on Ubuntu - so I think it's the root cause (unless I call it
with either "-ssl3" or "-tls1" - explicitly asking for protocols seems
to get around the issue with 1.0.1f). It looks like www.bnz.co.nz
doesn't negotiate SSL/TLS correctly?

Any SSL guru out there willing to explain why newer command line tools
don't like www.bnz.co.nz (whereas browsers do - but I hear it's because
they "double try" in certain error conditions and basically workaround
this kind of issue)

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From dw.andrew at gmail.com  Thu Nov 13 01:07:04 2014
From: dw.andrew at gmail.com (andrew williams)
Date: Thu, 13 Nov 2014 11:07:04 +1000
Subject: [squid-users] Squid not using all cache_mem/ Increase TCP_MEM_HIT
	squid 2.6
Message-ID: <CAGmjFR-CLMSwbvHbGgAn4ZUo30ZNLj5-E_TMnJh8AJ+zY3Ym1w@mail.gmail.com>

Hi,
I'm getting what I think is too low of MEM_HIT ratio..  I would like squid
to use all of the cache_mem, thus increasing MEM_HIT?

Cache information for squid:

Request Hit Ratios: 5min: 83.2%, 60min: 81.7%

Byte Hit Ratios: 5min: 85.6%, 60min: 69.4%

Request Memory Hit Ratios: 5min: 31.0%, 60min: 33.5%

Request Disk Hit Ratios: 5min: 41.9%, 60min: 40.5%

Storage Swap size: 13824000 KB

Storage Mem size: 401884 KB

Mean Object Size: 61.24 KB

Requests given to unlinkd: 0


Memory usage for squid via mallinfo():

Total space in arena:  583740 KB

Ordinary blocks:       580311 KB  51632 blks

Small blocks:               0 KB      0 blks

Holding blocks:          4588 KB      3 blks

Free Small blocks:          0 KB

Free Ordinary blocks:    3428 KB

Total in use:          584899 KB 99%

Total free:              3428 KB 1%

Total size:            588328 KB

Memory accounted for:

Total accounted:       517938 KB

memPoolAlloc calls: 604503322

memPoolFree calls: 602248266



Config:
cache_replacement_policy heap LFUDA

memory_replacement_policy heap LFUDA

cache_dir aufs /var/squid/cache 15000 16 256

cache_mem 4096 MB


Why is squid not using all 4096 MB allocated? it's only using 590MB
according to mgr:info.
Is there something extra I need to do?  To me the HIT rate is reasonable...
they hit's are just not coming from memory

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/38fd9973/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 13 02:04:21 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 15:04:21 +1300
Subject: [squid-users] OT: why does openssl-1.0.1f not like
	https://www.bnz.co.nz/?
In-Reply-To: <5463E578.5060408@trimble.com>
References: <5463E578.5060408@trimble.com>
Message-ID: <546411A5.1030309@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 11:55 a.m., Jason Haar wrote:
> Hi there
> 
> I just found I cannot connect to https://www.bnz.co.nz/ using curl
> on Ubuntu (7.35 compiled against openssl-1.0.1f), whereas 
> https://www.kiwibank.co.nz/ works fine. I first thought it was due
> to my messing around with ssl-bump, but it happens when I don't go
> through squid too
> 
> I have a CentOS-6 server with curl-7.19 (compiled against 1.0.1e)
> and it works fine. The same happens with "openssl s_client": it
> works on CentOS but not on Ubuntu - so I think it's the root cause
> (unless I call it with either "-ssl3" or "-tls1" - explicitly
> asking for protocols seems to get around the issue with 1.0.1f). It
> looks like www.bnz.co.nz doesn't negotiate SSL/TLS correctly?

Sounds to me like they are using SSLv3 in their server.

> 
> Any SSL guru out there willing to explain why newer command line
> tools don't like www.bnz.co.nz (whereas browsers do - but I hear
> it's because they "double try" in certain error conditions and
> basically workaround this kind of issue)

Lookup "SSLv3 POODLE" for what is happening in that area.

FYI: The browsers all announced deadlines of their next regular update
cycle[1][2][3] for dropping or disabling SSLv3 support. It's a dead
duck walking right now, should be buried by the end of this year.

[1] MSIE 6+ - (not sure exactly if this made it into the Patch Tuesday
set on 12 Nov)
  <https://support.microsoft.com/kb/3009008>

[2] Firefox 34 - 25 Nov

<https://blog.mozilla.org/security/2014/10/14/the-poodle-attack-and-the-end-of-ssl-3-0/>

[3] Chrome 39,40 - 'next few months'

<https://groups.google.com/a/chromium.org/forum/#!topic/security-dev/Vnhy9aKM_l4>

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZBGeAAoJELJo5wb/XPRj8ywH/0OMZzuTDHpVGGWpHR19AlTi
Qvl/XYbhoDGdSjeLqpkvMT3vrElk2ukznOV1cNxjZY8GL1vgmxObJl3fSu2mBW1O
pHh3j5WJDnNyXS9l1+9FyGRZo38Y0wZ56jjGRwPfhWr4rB5qDHNQU0w5MxXL3noS
rCm/yuQgeX791Jv9pe9toq4nGSpBCc0SmNIKLZiQnsS8qZKbKlZYEeh7x4V2TiME
6niFKHaQP58+xiJrlGQL/1GFZkem0Hu4U09tr+4Ru6PNWnumgd19/doznRk2dS6r
JX3F5+HdwZVbkfgjFEWcIaHaTq+YAOI1iMNq4CDjNaevjkSUIFgEYf6BCAhY3nM=
=GWhN
-----END PGP SIGNATURE-----


From Jason_Haar at trimble.com  Thu Nov 13 02:22:36 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 13 Nov 2014 15:22:36 +1300
Subject: [squid-users] OT: why does openssl-1.0.1f not like
	https://www.bnz.co.nz/?
In-Reply-To: <546411A5.1030309@treenet.co.nz>
References: <5463E578.5060408@trimble.com> <546411A5.1030309@treenet.co.nz>
Message-ID: <546415EC.7040906@trimble.com>

On 13/11/14 15:04, Amos Jeffries wrote:
> Sounds to me like they are using SSLv3 in their server.

Yes but "openssl s_client -tls1" also works, it just appears that
openssl cannot negotiate it - it has to be hardwired

> Lookup "SSLv3 POODLE" for what is happening in that area.

I thought it would be related, so it appears the newest version of
openssl cannot talk to some servers that "only" support TLSv1.0? That
doesn't sound right...

But as you say, once the browsers start breaking, I bet sites will
rapidly get upgraded. Hard to believe, but right now  the Bank of New
Zealand doesn't support TLSv1.1, let alone TLSv1.2!



-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From squid3 at treenet.co.nz  Thu Nov 13 02:33:55 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 15:33:55 +1300
Subject: [squid-users] Squid not using all cache_mem/ Increase
 TCP_MEM_HIT squid 2.6
In-Reply-To: <CAGmjFR-CLMSwbvHbGgAn4ZUo30ZNLj5-E_TMnJh8AJ+zY3Ym1w@mail.gmail.com>
References: <CAGmjFR-CLMSwbvHbGgAn4ZUo30ZNLj5-E_TMnJh8AJ+zY3Ym1w@mail.gmail.com>
Message-ID: <54641893.2080306@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 2:07 p.m., andrew williams wrote:
> Hi, I'm getting what I think is too low of MEM_HIT ratio..  I would
> like squid to use all of the cache_mem, thus increasing MEM_HIT?

You are running a 32-bit build of a Squid version deprecated more than
6 years ago.

Please upgrade to a current release. Today that would be 3.4.9
(stable) or 3.5.0.2 (beta).


<snip>
> 
> Why is squid not using all 4096 MB allocated? it's only using
> 590MB according to mgr:info. Is there something extra I need to do?
> To me the HIT rate is reasonable... they hit's are just not coming
> from memory

Your average object size is almost 64KB yet squid-2.6 only stores up
to 8 KB objects in memory by default.
<http://www.squid-cache.org/Versions/v2/2.6/cfgman/maximum_object_size_in_memory.html>

One of the many benefits of upgrading is that the defaults are
occasionally updated to reflect modern web reality. Todays Squid store
objects up to 512KB by default in a default 256MB cache_mem.

PS. Todays Squid also properly store and manage objects with
Cache-Control:no-cache.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZBiSAAoJELJo5wb/XPRjAmAH/R2XqFbj0FHTcTS1Bxix9BvW
5+BOY0s8Iyo54xcQnaZZn8AxFbNC/ksYN1Eh3yulTbMj17Y5IK9zS09eMkv0DXG+
0sTmzvw2Dj88lWQw/wrwEM8eEq4rkAVWYaE+puD+rOBGNMU9MkcJ4/auYjcUAJuY
ENFZ+4CzLQLZnzKRlVjxNb9FVyC6qMbvKi7pui/t9DItBGhT+KmTXC8W3G3JUG9T
riQT5Ma9ccYEyDQv2mfK4tDhTRGPXtaO3e5XWtZ777z9yDih9K7MAXx2wn+Q2Xn0
rbxkU+BD9h0i25I31FLG7eWekMJ7ZOBXLie37P2IbA5FERW3evrh0j/Z4tfXCSQ=
=g0N5
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 02:37:05 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 15:37:05 +1300
Subject: [squid-users] OT: why does openssl-1.0.1f not like
	https://www.bnz.co.nz/?
In-Reply-To: <546415EC.7040906@trimble.com>
References: <5463E578.5060408@trimble.com> <546411A5.1030309@treenet.co.nz>
 <546415EC.7040906@trimble.com>
Message-ID: <54641951.1050408@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 3:22 p.m., Jason Haar wrote:
> On 13/11/14 15:04, Amos Jeffries wrote:
>> Sounds to me like they are using SSLv3 in their server.
> 
> Yes but "openssl s_client -tls1" also works, it just appears that
> openssl cannot negotiate it - it has to be hardwired
> 
>> Lookup "SSLv3 POODLE" for what is happening in that area.
> 
> I thought it would be related, so it appears the newest version of
> openssl cannot talk to some servers that "only" support TLSv1.0? That
> doesn't sound right...

If the server responds to either the -ssl3 option or the -tls1 option
then it is performing some form of SSLv3 / TLS1.0 compatibility logics.
Which are probably screwed up when SSLv3 disappears out of the equation.

> 
> But as you say, once the browsers start breaking, I bet sites will
> rapidly get upgraded. Hard to believe, but right now  the Bank of New
> Zealand doesn't support TLSv1.1, let alone TLSv1.2!

Following along in the footsteps of the UK tax dept. They hit this last
week. :-)

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZBlRAAoJELJo5wb/XPRjcX8IANycoz70clz5mjhcMgwxABaa
33i4HrMciwhQU+AtgK22COd4OxK/L2GGFCV6Aapa4xcggsvVQ7B7BvcSSdAX3woF
ubhAhQOBc3NY5ZykDDSXnfVUfLIwfkB5xH225wTAmUZM4AWLk4QE/BrH7Q8qUGzh
6pBzlCetI3GqoHPtKCrQPuBt7t4zoAwRPvE23PWSxcwygdIJuUgQN1ZTcjdiGNcm
BqW3LxkNTFqE2w5RLaQmLpfD6vOH+CZyrTwW9INOb3vVqsUw2oj2DHPQUoRBvb6x
ZhGjnoQ+ta/sRNsbdUL6qVexXcf/+loVRHkhwgmhvIPXHhXrzrYzVsnmvmgKL9s=
=4k9a
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 02:55:22 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 15:55:22 +1300
Subject: [squid-users] cache peer problem with two squid one Tproxy
 --->normal Porxy
In-Reply-To: <001201cfff0c$8a09eb30$9e1dc190$@netstream.ps>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
 <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
 <54637073.1090801@treenet.co.nz>
 <001201cfff0c$8a09eb30$9e1dc190$@netstream.ps>
Message-ID: <54641D9A.8050107@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 7:39 p.m., Ahmed Allzaeem wrote:
> Hi amos
> 
> I have changed the both hostnames on two servers :
> 
> [root at tproxy ~]# hostname tproxy.com
> 
> 
> [root at parent ~]# hostname parent.com
> 
> 

Good.

> but , as I told u last time I can see traffic "miss" on the normal
> proxy , and "miss" on the tproxy server.
> 
> But it says access denied  from normal proxy
> 
> I mean on the normal proxy "parent" there is only miss and no
> Denied hits , but it give me error access denid.

Just to double check. The access.log records a TCP_MISS/403 ?

 That is a "Access denied" error coming from the origin server.


PS. DENIED is rejection. HIT is acceptance. A single proxy cannot
accept and reject at the same time.

> 
> Also I made sure that the ip of tproxy is allowed by acl on the
> normal proxy"parent"
> 

Good.

> 
> Again , here is the  cache log @ the parent proxy , still says a
> loop occurring :
> 
> 2014/11/12 23:33:24 kid1| WARNING: Forwarding loop detected for: 
> GET / HTTP/1.1

URL "/" is *not* a forward-proxy syntax URL. It is an origin server
syntax URL.

This URL syntax should only ever be seen as the first (tproxy)
configured proxy. Never at the parent. For the parent to receive this
message syntax is an "Invalid Request" error.

This is therefore very, very strange.


> Host: abc.com User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64;
> rv:33.0) Gecko/20100101 Firefox/33.0 Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 
> Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate Via:
> 1.1 squid (squid/3.4.3)

Notice how this is neither "tproxy.com" nor "parent.com" which your
hostname is set to.

Lets try the shortcut for now and set visible_hostname in both proxies
to the relevant tproxy.com//parent.com/


> X-Forwarded-For: 176.58.79.248 Cache-Control: max-age=259200 
> Connection: keep-alive
> 

The *only* ways a normal forward-proxy parent could be recording
forwarding loops is:

1) Via header already contains its hostname.

2) the URL domain:port resolves in DNS to the proxy listening IP:port.

3) the parent proxy is configured to use itself as a cache_peer.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZB2aAAoJELJo5wb/XPRjV7AIAI6OZaoaftNd2QoTVHb/6FB8
9rFKQLc1zRfLHTBCO0QM1tq4eph751Gk0SFnzzr0gNw9Mzbg6Tkbkrtkt1jtu33m
I0dQ5YOzJpcYhuZ1ufXoMXjV1ihcw33BQit1w80QV/rclQqlYSqMcHXfK1t0bR5n
B4oplYBSVxZ+1ttAAUFdVNp//yT7vrNGQezudEsxhkvqOpiaajZcIK5S3AT8msi1
/TYtOoWhVa/nkZDUxMa/BGzAaeq2SED/RQdgCZcCrvCRfbahzFc4nGAtcDho0HVZ
yFIYCN5vmEhYfg/0kEkLj4jgiJA9VpfwTOdAX9lGPEHGzO36f8h94lFZoPEFMJU=
=+YST
-----END PGP SIGNATURE-----


From albertok at gmail.com  Thu Nov 13 03:11:54 2014
From: albertok at gmail.com (Alberto Klocker)
Date: Thu, 13 Nov 2014 14:11:54 +1100
Subject: [squid-users] mgr:info question
Message-ID: <CAGrhr77_iS75YLkv40r3Hyxfn=i9iSpzSLmc9zqAxKiytE1SQQ@mail.gmail.com>

Looking at the squidclient mgr:info command output I was wondering what the
difference between these two entries are?

Cache information for squid:
        Hits as % of all requests:      5min: 0.7%, 60min: 0.3%
        Hits as % of bytes sent:        5min: 51.3%, 60min: 25.5%


I can guess the first one means all requests but I'm stumped on the wording
of the second entry.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/00a95c54/attachment.htm>

From ahmed.zaeem at netstream.ps  Thu Nov 13 13:27:15 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Thu, 13 Nov 2014 05:27:15 -0800
Subject: [squid-users] cache peer problem with two squid one Tproxy
	--->normal Porxy
In-Reply-To: <54641D9A.8050107@treenet.co.nz>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
 <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
 <54637073.1090801@treenet.co.nz>
 <001201cfff0c$8a09eb30$9e1dc190$@netstream.ps>
 <54641D9A.8050107@treenet.co.nz>
Message-ID: <001d01cfff45$8c493cf0$a4dbb6d0$@netstream.ps>

Hi Amos , thanks for all explanation.

But the problem solved when I added the following directives to the tproxy server :
##############################
forwarded_for off
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access X-Forwarded-For deny all
request_header_access Via deny all
request_header_access All allow all
#############################


Now everything is working fine with with me

But one last thing I need.

I need the tproxy server forward the packet with the original ip of the clients .... I mean I want to still keeping the tproxy function whereas now all cliewnts to to the peer with the ip of the tproxy server.

I need each user go to the parent proxy with the original ip

Can I do it with directive ?

Again , here is the directive I put on the tproxy to go to to parent :

cache_peer 77.221.104.97  parent 3127 0 no-query no-digest no-tproxy proxy-only


thank you alot

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Wednesday, November 12, 2014 6:55 PM
To: Ahmed Allzaeem; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache peer problem with two squid one Tproxy --->normal Porxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 7:39 p.m., Ahmed Allzaeem wrote:
> Hi amos
> 
> I have changed the both hostnames on two servers :
> 
> [root at tproxy ~]# hostname tproxy.com
> 
> 
> [root at parent ~]# hostname parent.com
> 
> 

Good.

> but , as I told u last time I can see traffic "miss" on the normal 
> proxy , and "miss" on the tproxy server.
> 
> But it says access denied  from normal proxy
> 
> I mean on the normal proxy "parent" there is only miss and no Denied 
> hits , but it give me error access denid.

Just to double check. The access.log records a TCP_MISS/403 ?

 That is a "Access denied" error coming from the origin server.


PS. DENIED is rejection. HIT is acceptance. A single proxy cannot accept and reject at the same time.

> 
> Also I made sure that the ip of tproxy is allowed by acl on the normal 
> proxy"parent"
> 

Good.

> 
> Again , here is the  cache log @ the parent proxy , still says a loop 
> occurring :
> 
> 2014/11/12 23:33:24 kid1| WARNING: Forwarding loop detected for: 
> GET / HTTP/1.1

URL "/" is *not* a forward-proxy syntax URL. It is an origin server syntax URL.

This URL syntax should only ever be seen as the first (tproxy) configured proxy. Never at the parent. For the parent to receive this message syntax is an "Invalid Request" error.

This is therefore very, very strange.


> Host: abc.com User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64;
> rv:33.0) Gecko/20100101 Firefox/33.0 Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate Via:
> 1.1 squid (squid/3.4.3)

Notice how this is neither "tproxy.com" nor "parent.com" which your hostname is set to.

Lets try the shortcut for now and set visible_hostname in both proxies to the relevant tproxy.com//parent.com/


> X-Forwarded-For: 176.58.79.248 Cache-Control: max-age=259200
> Connection: keep-alive
> 

The *only* ways a normal forward-proxy parent could be recording
forwarding loops is:

1) Via header already contains its hostname.

2) the URL domain:port resolves in DNS to the proxy listening IP:port.

3) the parent proxy is configured to use itself as a cache_peer.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZB2aAAoJELJo5wb/XPRjV7AIAI6OZaoaftNd2QoTVHb/6FB8
9rFKQLc1zRfLHTBCO0QM1tq4eph751Gk0SFnzzr0gNw9Mzbg6Tkbkrtkt1jtu33m
I0dQ5YOzJpcYhuZ1ufXoMXjV1ihcw33BQit1w80QV/rclQqlYSqMcHXfK1t0bR5n
B4oplYBSVxZ+1ttAAUFdVNp//yT7vrNGQezudEsxhkvqOpiaajZcIK5S3AT8msi1
/TYtOoWhVa/nkZDUxMa/BGzAaeq2SED/RQdgCZcCrvCRfbahzFc4nGAtcDho0HVZ
yFIYCN5vmEhYfg/0kEkLj4jgiJA9VpfwTOdAX9lGPEHGzO36f8h94lFZoPEFMJU=
=+YST
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Thu Nov 13 03:49:03 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 16:49:03 +1300
Subject: [squid-users] mgr:info question
In-Reply-To: <CAGrhr77_iS75YLkv40r3Hyxfn=i9iSpzSLmc9zqAxKiytE1SQQ@mail.gmail.com>
References: <CAGrhr77_iS75YLkv40r3Hyxfn=i9iSpzSLmc9zqAxKiytE1SQQ@mail.gmail.com>
Message-ID: <54642A2F.8090407@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 4:11 p.m., Alberto Klocker wrote:
> Looking at the squidclient mgr:info command output I was wondering
> what the difference between these two entries are?
> 
> Cache information for squid: Hits as % of all requests:      5min:
> 0.7%, 60min: 0.3%z Hits as % of bytes sent:        5min: 51.3%,
> 60min: 25.5%
> 
> 
> I can guess the first one means all requests but I'm stumped on the
> wording of the second entry.

HTTP messages consist of a variable amount of bytes each. A HIT on a
1KB object saves less bandwidth than a HIT on a 1MB object. The only
(relatively) predictable thing based on message count is consumed CPU
and network sockets.


"% of all requests" is counting by messages.
 - higher values means less CPU consumed/wasted upstream (server
latency), less contention for TCP sockets (HTTP pipeline congestion).

"% of bytes sent" is counting by bandwidth used.
 - higher values means less bandwidth consumed/wasted upstream (TCP
congestion).

So ... in the past 60min 0.3% of your clients were responded to using
the cache, which reduced your upstream bandwidth usage/cost by 25.5%.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZCouAAoJELJo5wb/XPRjzJIIAM499MGEXNvMGI60Vul/jShC
2eP5W7O/6rDygQQBkSpI/h9ph0xcgRc4eG+F/jbvUIY828fYOkIZirAOknr8NxPD
hV3xB3nZw859SyOfB6+Iz0X2Fp7VWIZZiRg27XxGuWcWsLGF7qmA7Kjh6S0jy21c
mCSZYKU5Rgx6FMNNBac7NxztogD0ZD2z2JQwJsSK1JI4JFkw2eImu+NTPPU6aQUG
pdiRBGTzATTL0lDXOBaM96s7o3XwkKift4OzZgetBoG+b3LQ2XX4Iw9caBnK/Fr8
MS2nLsvISUI+XrvpqFTYFdTlRVOEYjzfOGPzOwxgNdl86TlzTMh1ExOTmWd92RE=
=pN5q
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 04:14:16 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 17:14:16 +1300
Subject: [squid-users] cache peer problem with two squid one Tproxy
 --->normal Porxy
In-Reply-To: <001d01cfff45$8c493cf0$a4dbb6d0$@netstream.ps>
References: <000001cffecb$bf89f7a0$3e9de6e0$@netstream.ps>
 <546362C2.5010708@treenet.co.nz>
 <000c01cffed5$12ae0f80$380a2e80$@netstream.ps>
 <54637073.1090801@treenet.co.nz>
 <001201cfff0c$8a09eb30$9e1dc190$@netstream.ps>
 <54641D9A.8050107@treenet.co.nz>
 <001d01cfff45$8c493cf0$a4dbb6d0$@netstream.ps>
Message-ID: <54643018.9000601@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 2:27 a.m., Ahmed Allzaeem wrote:
> Hi Amos , thanks for all explanation.
> 
> But the problem solved when I added the following directives to the
> tproxy server : ############################## forwarded_for off

* that breaks any possibility of the parent proxy identifying what the
client IP was.

> request_header_access Via deny all

This alone breaks the forwarding loop detection. Just prevents you
seeing whats going on.


> Now everything is working fine with with me
> 

Whatever looping was going on is still happening out of sight and
could bite at any time.

> But one last thing I need.
> 
> I need the tproxy server forward the packet with the original ip of
> the clients .... I mean I want to still keeping the tproxy function
> whereas now all cliewnts to to the peer with the ip of the tproxy
> server.
> 
> I need each user go to the parent proxy with the original ip

 user != client.

In the context of TPROXY a client is a piece of machinery or software.
A User remains a person or logical identity.

When traffic arrives at the parent proxy the user remains whoever
started the transaction the *client* however actually *is* the tproxy
regardless of what the IPs say.

> 
> Can I do it with directive ?

Spoofing arbitrary outgoing IPs is not supported behaviour. (It is
also actively illegal in places.)

If the parent proxy is not receiving TPROXY packets directly it cannot
spoof the outgoing.

To do what you ask will require *both* proxies to be setup as TPROXY,
no cache_peer link between them. The network routing must pass packets
from machine A (users) through machine B (child proxy) through machine
C (parent proxy) as if they were regular routers in a chain.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDAYAAoJELJo5wb/XPRjllEIAMo1zAQvRy1cSJaxy64i2ZKy
GaMnSwe14c255aDV2Pmr8tPTWv9udA/g+t1D25fM3RMEiut2aN5n2g6ArWABPpXX
bJOjPZiq+SkaZZq1JLP4ncTfk5TyLxVXxuRJnAAVyGZpX0lyoD/EoXAvBLpZf3EN
Fhx3EnKq0baf/pHtu1UAnuCdU0eVHElAfk/srLpSS42O8O56RAzjjZ24QltIWmys
e1nUIYnbzRhF1krD3QLKTWR14Tq76Ww2syB3TpRlHrH2SH3JNMa2wA+u9pYSKGO8
URSoMguyYjQkF/S6mWxfXHpvJ/hl0uvs8RoMzWVSI7pLP17y3nM7FDaqmlmJqGk=
=NJbJ
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 04:39:17 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 17:39:17 +1300
Subject: [squid-users] Forceful Reauthentication
In-Reply-To: <1415820477635-4668344.post@n4.nabble.com>
References: <1415820477635-4668344.post@n4.nabble.com>
Message-ID: <546435F5.4000904@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 8:27 a.m., santosh wrote:
> Hello Team,
> 
> I'm trying to reauthenticate the user once he visits google as per
> this url https://workaround.org/squid-acls but it doesnt seem to
> reprompt the credentials when i access google  , below are my rules
> let me know where i'm going wrong


The Tutorial is flat wrong.

> 
> 
> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
> #
> 
> #conatins acl to block sites acl bad_url url_regex
> "/etc/squid3/badsites.conf"
> 
> auth_param basic program /usr/lib/squid3/squid_ldap_auth -b 
> "dc=example,dc=com" -f "uid=%s" -h example.com acl ldapauth
> proxy_auth REQUIRED acl reauth dstdomain .google.co.in http_access
> deny bad_url http_access allow ldapauth

The above line says that if they authenticate they are in. No matter what.

The following line will never be tested.

> http_access deny  reauth ldapauth http_access deny all
> 


Lets go back to the Squid FAQ, which that tutorial apparently was
trying to re-write ...

<wiki.squid-cache.org/Features/Authentication#How_do_I_ask_for_authentication_of_an_already_authenticated_user.3F>

It lists a particular sequence of ACL tests:
1. http_access deny google !google_users
2. http_access allow my_auth
3. http_access deny all


line 1 tests for google and requires authentication challenge if they
are a) un-authenticated, or b) using "wrong" user accounts.

line 2 checks authentication and allows anyone who can login with
*any* credentials. Unauthenticated users will be challenged.

line 3 rejects anyone who cannot login at all.

   Order Is Important.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDX1AAoJELJo5wb/XPRj9bAIAJqPgkB+jhvuKL/gj/q07Crk
mZTUuOLPep4E0/+ahGxV1xyEt3w1iAOysTa8vf8DEaJ40zHr+nRt9Oi6MGK39SNN
tURRSrDLtRkn4OtSWE0Yw9yKf4l1FJfZswAqZxh8HruF6Apr6bc3GZSya2x0qcbF
CKXTt7C7pyy/FeKdkfju3jp8spazTmpBcx/Ib6F4GIBoQXhqALh3ZluzbmV+ws4l
LzNDZ4AcPUrF41aBwkZkyvYXyAHzCmkJPBCkDNqUu37msmNye2IrA5poNWrEKWyp
6jSVvYehPLFqU3DMVhGja+blJ3GA6FYuHurR3+0kwIudGvw32c6EVZWDUyOmaug=
=waRG
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 04:45:43 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 17:45:43 +1300
Subject: [squid-users] Squid 3.4.6 POST upload problem
In-Reply-To: <CABMqo8-gx5d88MiDUTKQCR1VQVwprnN3X0dZAy7aAcK=WBXTnA@mail.gmail.com>
References: <CABMqo8-gx5d88MiDUTKQCR1VQVwprnN3X0dZAy7aAcK=WBXTnA@mail.gmail.com>
Message-ID: <54643777.6090406@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 5:47 a.m., Lorenzo Gollinelli wrote:
> Hello,
> 
> we have squid 3.4.6 talking to websense over icap. We have problems
> in uploading files larger than 55 kB.
> 
> this is the icap.log when file is correctly uploaded (<55kB):
> 
> 1415810436.490      0 192.168.x.x TAG_NONE/000 0 POST 
> http://www.csm-testcenter.org/test DOMAIN/user HIER_NONE/- -
> 
> this is the icap.log when file upload hangs:
> 
> 1415810513.657      0 192.168.x.x TCP_MISS/000 0 POST 
> http://www.csm-testcenter.org/test DOMAIN/user
> HIER_DIRECT/85.214.28.69 -
> 
> It looks like there is something somewhere that behaves
> differently according to the uploaed file size. We only use
> cache_mem (no disk), 5 workers, and kerberos/ntlm authentication
> (bypassed without luck).
> 
> Any idea?

Look at the HTTP message being sent to Squid. You may be encountering
a known issue with Expect:100-continue messages vs. ICAP.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDd3AAoJELJo5wb/XPRjgT4H/iGu4v0j9FjOUGi7BNc8Zb6/
R4Hd4097z2N1kF2hVLQqninR8F4u/OMHN1oATCIXEaM9lYoWl1MulxA1XeZT4Zk7
b/6X/q9MDB1P9liq26hKhaUXnZGgbltK3brkUZ3M7Qf78zFqEtvaMl0iDDJmElJn
SdBE+4oW28s+b+T5c3XB1uWLjKzeKiaxsJ8mnYiYf7dE0mqKibWvO6sc95EeCxS4
8g3R6aR+Jw/W5D+M/ILF8e0Sf7V/b+NPkW0wdsmPdFYzSoGJhg9InSI9oXermBf5
L6fzojQ4uiUHs+xR/D4dRCn7eTqjFmDhrLpQabDsxM6SuXWLARMVu5UW+LFpiO4=
=c4Y9
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 04:51:14 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 17:51:14 +1300
Subject: [squid-users] R: R: Problem with Squid 3.4 and transparent SSL
 proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE9@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>,
 <5462D335.2030701@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE9@W2008DC01.ColliniConsulting.lan>
Message-ID: <546438C2.4080205@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 9:55 p.m., Job wrote:
> Thank you Amos, for everything.
> 
> I route with REDIRECT all outgoing connection to port tcp/443 from
> my LAN:
> 
> iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT
> --to-port 3130
> 
> in squid, i have these configurations:
> 
> http_port 3128 http_port 3129 intercept https_port 3130 intercept
> ssl-bump connection-auth=off generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem
> key=/etc/squid/ssl/squid.key
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>
>  Do you think my iptables rule is wrong?

The iptables looks fine.

Peter G, in a recent thread added the IP address Squid was being
contacted on to the port details. Maybe that will work for you too.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDjCAAoJELJo5wb/XPRjse0IAIL7VDPvPvConqwAxSBP1O79
D8uBOW8D1WVxLARD4HmW9To6qSSten3QwYfJYcwhk0BRDyzh0h0PCiduhWe50H8b
MdK/TXbBdo79q8AobaHkycBQeKwYWKjnHd5IvEp+fPzNK5izqIoWcvdnfFOnSZVh
ULuus+CmKnkykgcYBClxwRlnDo30SPSVUWUS5dgT2Z6r4xnvAANTlpwCJxodcqz/
9zq6vn8dnYYdtIgvuz7SgI49bSDxNo0aa+tizl2P0sKSIxfw5vnnaaj8VXWdeS+r
cpD4H0Wju7CXIyGXfgkDBl/BP3gVUjGVyWJkXN5XYx3Qyu4kKEg4absRTR5+tYc=
=c8G3
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 13 05:00:42 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 18:00:42 +1300
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <1415316336554.2e048ede@Nodemailer>
References: <CAN8nrKB+9oz8nk3sKwTKRgFNovzWtMMb5YbyfgAqP+Bfm2pVcg@mail.gmail.com>
 <1415316336554.2e048ede@Nodemailer>
Message-ID: <54643AFA.4000006@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 12:25 p.m., dan wrote:
> Bumping this with another backtrace. Happened at 16:05 this time,
> when the system was not very very busy.
> 
> It?s causing squid to crash in such a way that I actually have to
> `kill -9` the process in order to get things restarted properly.
> 
> Would really appreciate any feedback at all from anyone who can
> understand these back traces.


Any hints, like what release version of Squid you are using?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDr6AAoJELJo5wb/XPRjxL8H/iQO8pG5twVV2lcXxlFEgLuE
NR0c4ezkPOiwgM3iHzrVxcDtCRLHB2YrwT9GuapslmSkcTOP6sBKxekOHsZmWtlK
Sd8A/jK6l/GXFqPpdUHjut6g1aEUwTfJxsRr2NxtMW2f7a91qyOE9f31WYKQq73m
odjtt6ayc+yA2jMEfHaIHaqXhIzAxV21ipN8GH5CWhrfMfo6IxpP4326z8SMa1am
6HXJQhTkt1qqV4jCjGdYQ4BkAZygBtsHNb2AKgJ5Wmb4OCsM4MZlbIiPmWqWWCfY
8ccyLVvodfpPVtjCBgStcJTkWxamu6BaHNhy8qCV03zAa4faxqcYVwAvSA5Q2sg=
=tHfy
-----END PGP SIGNATURE-----


From dan at getbusi.com  Thu Nov 13 05:02:20 2014
From: dan at getbusi.com (dan at getbusi.com)
Date: Wed, 12 Nov 2014 21:02:20 -0800 (PST)
Subject: [squid-users] assertion failed: client_side.cc:1515:
 "connIsUsable(http->getConn())
In-Reply-To: <54643AFA.4000006@treenet.co.nz>
References: <54643AFA.4000006@treenet.co.nz>
Message-ID: <1415854939929.9166aac8@Nodemailer>

Oh sure, sorry:





Squid Cache: Version 3.4.8

configure options:? '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,AD_group,session' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log' '--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-maxfd=65535' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

On Thu, Nov 13, 2014 at 4:01 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> On 7/11/2014 12:25 p.m., dan wrote:
>> Bumping this with another backtrace. Happened at 16:05 this time,
>> when the system was not very very busy.
>> 
>> It?s causing squid to crash in such a way that I actually have to
>> `kill -9` the process in order to get things restarted properly.
>> 
>> Would really appreciate any feedback at all from anyone who can
>> understand these back traces.
> Any hints, like what release version of Squid you are using?
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
> iQEcBAEBAgAGBQJUZDr6AAoJELJo5wb/XPRjxL8H/iQO8pG5twVV2lcXxlFEgLuE
> NR0c4ezkPOiwgM3iHzrVxcDtCRLHB2YrwT9GuapslmSkcTOP6sBKxekOHsZmWtlK
> Sd8A/jK6l/GXFqPpdUHjut6g1aEUwTfJxsRr2NxtMW2f7a91qyOE9f31WYKQq73m
> odjtt6ayc+yA2jMEfHaIHaqXhIzAxV21ipN8GH5CWhrfMfo6IxpP4326z8SMa1am
> 6HXJQhTkt1qqV4jCjGdYQ4BkAZygBtsHNb2AKgJ5Wmb4OCsM4MZlbIiPmWqWWCfY
> 8ccyLVvodfpPVtjCBgStcJTkWxamu6BaHNhy8qCV03zAa4faxqcYVwAvSA5Q2sg=
> =tHfy
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141112/a4b07e72/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 13 05:19:20 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 18:19:20 +1300
Subject: [squid-users] Behind enemy lines (squid behind proxy)
In-Reply-To: <trinity-de15a38f-07f4-4fee-94b1-4a2ecf861000-1415313027728@3capp-mailcom-lxa05>
References: <trinity-de15a38f-07f4-4fee-94b1-4a2ecf861000-1415313027728@3capp-mailcom-lxa05>
Message-ID: <54643F58.5030809@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 7/11/2014 11:30 a.m., doc.holliday at usa.com wrote:
> 
>> Sent: Wednesday, November 05, 2014 at 10:48 PM From: "Amos
>> Jeffries"
> 
>> On 6/11/2014 2:33 p.m., doc.holliday wrote:
>>> 
>>> I've searched through the internets and tried various things...
>>> to no avail. Hopefully someone here can point me in the right 
>>> direction. I am sitting behind a proxy, which accepts
>>> http/https. Everything else is blocked. If I instruct my
>>> browser to use this proxy, everything works dandy. Both http
>>> and https. The problem is, I have a few apps that don't have an
>>> option to set proxy. So, my idea was to set up squid on the
>>> local machine that would transparently redirect http/https to
>>> the proxy. Eg something like this: [ local_box: app (http or
>>> https) ---> squid ] -----> [ the_proxy ] -----> ... -----> [
>>> internets ] I have no control of the proxy, nor do I know what
>>> goes on after it.
>> 
>> What you have configured forces that not to happen then sends
>> the de-crypted traffic to the peer proxy as HTTP. The peer is
>> rejecting the un-encrypted protocol containing https:// URLs with
>> a 503 for whatever reason.
>> 
>> If the other peer is another Squid then chances are still fairly
>> high that it has been built without OpenSSL support and so
>> literally cannot open the TLS connection to deliver the https://
>> request to the origin.
>> 
>> Generating new CONNECT tunnels over peer proxies has not yet
>> been coded for Squid. Nobody seems willing sponsor its
>> development, despite all these problems bumping is now causing.
>> 
>> Amos
> 
> Thanks Amos. It makes sense... mostly. :)
> 
> One thing I am wondering though is, if I set my browser to use the
> proxy (in the browser settings) for both http and https, both work
> fine. So, it seems the proxy server supports both http and https
> over CONNECT tunnels.

That is all just HTTP. The HTTPS is just a sequence of bytes inside
the CONNECT tunnel "payload". When the browser generates the
CONNECT+TLS parts your proxy can relay the CONNECT message as-is to
the peer, which needs not understand the HTTPS in any way to pump it
as raw TCP bytes to the origin.

Generating new CONNECT+TLS wrappers from scratch to secure a TCP
connection is what Squid does not yet do. It's completely possible,
just nobody has done (or sponsored) the implementation work yet.

> 
> So, if the squid on the local_box is not talking to the_proxy (it's
> cache_peer) via CONNECT, what does it use?

Squid talks to peers using HTTP on top of either TLS or TCP. When TLS
is used its called "HTTPS" regardless of whether the peer is a proxy
or origin. HTTP messages with https:// URLs are only permitted to be
sent over TLS.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZD9XAAoJELJo5wb/XPRjId8H/RxjTtJ8k3ephVIF62bMoz5F
8TCkK9f71XDoSTrot7R8c05tbtlO+Erg8o7xGxIU7XsQqXoGWVkZYysp/R4DB1Ox
0cYPxG3gYdMNEKcjk3Nwgv+8RPpgq/q6cnhPG5Y0kHX8V0eZ3CNQooVLu26dulno
QfwJlswILfDavaJZ8jvHklIHCKsscuzHT653EGd7WYzf6TpV3slaPhJ/JcGeQo4Z
RyTeJLaL2Vk3OhQ2tfnMtTIpaGC5FF1rNztfat8bzAzHZtLZMHRpKkrKcM6SsV5p
9sjLCEr4KUd/ruqO9WajbJ7+JHNX17444/s5HTljNSeI8zqR/Xm1lMLQVCcN2Bo=
=mVP4
-----END PGP SIGNATURE-----


From dw.andrew at gmail.com  Thu Nov 13 06:16:20 2014
From: dw.andrew at gmail.com (andrew williams)
Date: Thu, 13 Nov 2014 16:16:20 +1000
Subject: [squid-users] Squid not using all cache_mem/ Increase
 TCP_MEM_HIT squid 2.6
In-Reply-To: <54641893.2080306@treenet.co.nz>
References: <CAGmjFR-CLMSwbvHbGgAn4ZUo30ZNLj5-E_TMnJh8AJ+zY3Ym1w@mail.gmail.com>
 <54641893.2080306@treenet.co.nz>
Message-ID: <CAGmjFR8=7bDNupOW_3OMBWodDka70nKXbAZWb6PixtdszGXZsA@mail.gmail.com>

Thank you Amos.
maximum_object_size_in_memory must be the special setting that I had
missed.  Memory hits now up to 60%

Cache information for squid:

Request Hit Ratios: 5min: 84.4%, 60min: 78.6%

Byte Hit Ratios: 5min: 80.6%, 60min: 67.9%

Request Memory Hit Ratios: 5min: 63.1%, 60min: 66.1%

Request Disk Hit Ratios: 5min: 8.7%, 60min: 10.2%

Storage Swap size: 13823916 KB

Storage Mem size: 488276 KB

Mean Object Size: 61.36 KB

Requests given to unlinkd: 0


And yes I'm very keen to get to squid 3.. hopefully soon!


thanks!



On Thu, Nov 13, 2014 at 12:33 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 13/11/2014 2:07 p.m., andrew williams wrote:
> > Hi, I'm getting what I think is too low of MEM_HIT ratio..  I would
> > like squid to use all of the cache_mem, thus increasing MEM_HIT?
>
> You are running a 32-bit build of a Squid version deprecated more than
> 6 years ago.
>
> Please upgrade to a current release. Today that would be 3.4.9
> (stable) or 3.5.0.2 (beta).
>
>
> <snip>
> >
> > Why is squid not using all 4096 MB allocated? it's only using
> > 590MB according to mgr:info. Is there something extra I need to do?
> > To me the HIT rate is reasonable... they hit's are just not coming
> > from memory
>
> Your average object size is almost 64KB yet squid-2.6 only stores up
> to 8 KB objects in memory by default.
> <
> http://www.squid-cache.org/Versions/v2/2.6/cfgman/maximum_object_size_in_memory.html
> >
>
> One of the many benefits of upgrading is that the defaults are
> occasionally updated to reflect modern web reality. Todays Squid store
> objects up to 512KB by default in a default 256MB cache_mem.
>
> PS. Todays Squid also properly store and manage objects with
> Cache-Control:no-cache.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUZBiSAAoJELJo5wb/XPRjAmAH/R2XqFbj0FHTcTS1Bxix9BvW
> 5+BOY0s8Iyo54xcQnaZZn8AxFbNC/ksYN1Eh3yulTbMj17Y5IK9zS09eMkv0DXG+
> 0sTmzvw2Dj88lWQw/wrwEM8eEq4rkAVWYaE+puD+rOBGNMU9MkcJ4/auYjcUAJuY
> ENFZ+4CzLQLZnzKRlVjxNb9FVyC6qMbvKi7pui/t9DItBGhT+KmTXC8W3G3JUG9T
> riQT5Ma9ccYEyDQv2mfK4tDhTRGPXtaO3e5XWtZ777z9yDih9K7MAXx2wn+Q2Xn0
> rbxkU+BD9h0i25I31FLG7eWekMJ7ZOBXLie37P2IbA5FERW3evrh0j/Z4tfXCSQ=
> =g0N5
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/c7dedb6f/attachment.htm>

From jonathan.courtois at gmail.com  Thu Nov 13 08:39:39 2014
From: jonathan.courtois at gmail.com (jcourtois)
Date: Thu, 13 Nov 2014 00:39:39 -0800 (PST)
Subject: [squid-users] Enable to create a encrypted connexion between
	browser and squid
Message-ID: <1415867979898-4668366.post@n4.nabble.com>

Hi everyone,

I've been trying to create a simple encrypted connexion in between a browser
(that support https proxy connexion, I use Firefox 33 to do my test because
it's supose to support it:
https://bugzilla.mozilla.org/show_bug.cgi?id=378637) and my squid located
ona remote server.

I don't want to use ssl_bump (if it's not necessary, I don't want to
uncrypt, read, or make any rule related to the requests. I just want the
tunnel in between the browser and the squid server to be encrypted and the
rest (squid to internet) depending of the request http/https.

I always receive the same error with a little difference in the error code
if it's an http or https request.

I've been reading a lot about this errors on the internet and try many
things without success. One thing I'm not sure is do I need to use ssl_bump
or is it something else ? From what I read, I think we can create a https
proxy connexion without ssl_bump.



On Firefox I use this configuration for all protocol : mysquid.com:443 (I
replace my real domain name that have a wild card SSL certificate by this
fake domain to not publish here the real name).

And on my server I have the following configuration. Also I have an
authentication of user (login/password).



Thank you very much for your help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Enable-to-create-a-encrypted-connexion-between-browser-and-squid-tp4668366.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From lorenzo.gollinelli at gmail.com  Thu Nov 13 09:10:58 2014
From: lorenzo.gollinelli at gmail.com (Lorenzo Gollinelli)
Date: Thu, 13 Nov 2014 01:10:58 -0800
Subject: [squid-users] Squid 3.4.6 POST upload problem
Message-ID: <CABMqo8-9sdeMi1V=b5R=CeUDvde7kAwcqGzqSroLFR=_u7MtEg@mail.gmail.com>

Thank you very much Amos,

why then do we have the problem only if file is > 55kB? The bug your are
referring (#4067) to is not listed in version 3.4 known bugs

Here are the headers for a working transaction (< 55kB):

HTTP/1.1 100 Continue
Connection: keep-alive

HTTP/1.1 200 OK
Server: gunicorn/18.0
Date: Thu, 13 Nov 2014 08:54:34 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 2
Sponsored-By: https://www.runscope.com
X-Cache: MISS from squidserver.domain.lc
Via: 1.1 vegur, 1.1 squidserver.domain.lc (squid/3.4.6)
Connection: keep-alive

and here are the headers for non working one (> 55kB):

HTTP/1.1 100 Continue
Connection: keep-alive

HTTP/1.1 503 Service Unavailable
Server: Cowboy
Date: Thu, 13 Nov 2014 08:53:39 GMT
Content-Length: 484
Content-Type: text/html; charset=utf-8
Cache-Control: no-cache, no-store
X-Cache: MISS from squidserver.domain.lc
Via: 1.1 squidserver.domain.lc (squid/3.4.6)
Connection: close
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/caad181f/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 13 09:31:40 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Nov 2014 22:31:40 +1300
Subject: [squid-users] Enable to create a encrypted connexion between
 browser and squid
In-Reply-To: <1415867979898-4668366.post@n4.nabble.com>
References: <1415867979898-4668366.post@n4.nabble.com>
Message-ID: <54647A7C.4000700@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 9:39 p.m., jcourtois wrote:
> Hi everyone,
> 
> I've been trying to create a simple encrypted connexion in between
> a browser (that support https proxy connexion, I use Firefox 33 to
> do my test because it's supose to support it: 
> https://bugzilla.mozilla.org/show_bug.cgi?id=378637) and my squid
> located ona remote server.
> 

Squid supports receiving TLS connectinos from browsers using a plain
and simple old https_port with certificate.
  https_port 3129 cert=...


Firefox and Chrome are only known to take advantage of that capablity
*if* you configure them with a PAC file returning an https:// URL as
the proxy auto-config details.
...
 return "PROXY https://example.com:3129/";
...

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZHpyAAoJELJo5wb/XPRjd5wH/0im+RmcSd6vvoes0IN/2HTY
zuOVkJt+7EkJjaMoEWbF6Xp8esQ0R2invl7qDf/wvgbdW1U/EQe6/uHxSJIp8i0R
ytufdVJEtRJGgKymRc5n73VNwZcmS/CQoyaxB6GTuyAnJBEguBvmFXCUnPPEIRHO
rSHol13F+npJc77iXUHTaWe966TWLdCIOpF1UbCjOgHMF/4ZxRM/7Uqa0sHEzdKH
RB7iQUb9WHi4P33WzkyZLsen+z6E6UN9pV3jRiR4RmRduWCzGOQdELWK01vjG8ed
Exn1gTFpwJmvvuOwmV52NCGwQZLwgNpWb0l2aukhtqV5JFum+cPCJd2NzDpWrf4=
=E+wb
-----END PGP SIGNATURE-----


From garthl at dcdata.co.za  Thu Nov 13 10:29:24 2014
From: garthl at dcdata.co.za (Garth Lancaster)
Date: Thu, 13 Nov 2014 12:29:24 +0200 (SAST)
Subject: [squid-users] Icap Squid Https/Http
In-Reply-To: <20116210.1085.1415874327324.JavaMail.garthl@Garth>
Message-ID: <31134040.1090.1415874552017.JavaMail.garthl@Garth>

Hi There 

Is anyone able to confirm that https requests to squid proxy will be sent on to the icap service? I am able to get normal http requests into icap which displays a banner on the page. 
I have tried the whole transparent ssl-bump route as well. 

Thanks 
Garth 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/6af0110c/attachment.htm>

From jonathan.courtois at gmail.com  Thu Nov 13 10:28:11 2014
From: jonathan.courtois at gmail.com (jcourtois)
Date: Thu, 13 Nov 2014 02:28:11 -0800 (PST)
Subject: [squid-users] Enable to create a encrypted connexion between
 browser and squid
In-Reply-To: <54647A7C.4000700@treenet.co.nz>
References: <1415867979898-4668366.post@n4.nabble.com>
 <54647A7C.4000700@treenet.co.nz>
Message-ID: <1415874491428-4668370.post@n4.nabble.com>

Thanks for your reply.

I used a PAC file with the following, it appears to be "HTTPS" instead of
"PROXY" to use:

function FindProxyForURL(url, host)
{
return "HTTPS https://mysquid.com:443/";
}

But now when I navigate to any website (http or https), I receive a "This
connexion is not secured" message, with a go away buton and add exception
button. But of course it can't find certificates, especially for http://
website.

If I used return "PROXY https://mysquid.com:443/"; as you advice, I arrive
to the main page of my proxy server, the proxy doesn't appear to be used.

I must be doing something wrong.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Enable-to-create-a-encrypted-connexion-between-browser-and-squid-tp4668366p4668370.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Thu Nov 13 11:23:29 2014
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 13 Nov 2014 11:23:29 +0000
Subject: [squid-users] Icap Squid Https/Http
In-Reply-To: <31134040.1090.1415874552017.JavaMail.garthl@Garth>
References: <20116210.1085.1415874327324.JavaMail.garthl@Garth>,
 <31134040.1090.1415874552017.JavaMail.garthl@Garth>
Message-ID: <1415877815645.51049@diladele.com>

Hello Garth,


We are happily doing ICAP HTTPS filtering, see sample instructions at http://docs.diladele.com/tutorials/transparently_filtering_https_centos/index.html. It is even more simple if you do not need "intercept" style proxying.


Best regards,

Rafael Akchurin

Diladele B.V.


________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Garth Lancaster <garthl at dcdata.co.za>
Sent: Thursday, November 13, 2014 11:29 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Icap Squid Https/Http

Hi There

Is anyone able to confirm that https requests to squid proxy will be sent on to the icap service? I am able to get normal http requests into icap which displays a banner on the page.
I have tried the whole transparent ssl-bump route as well.

Thanks
Garth


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/6bdc62a7/attachment.htm>

From sergios at greeklug.gr  Thu Nov 13 13:50:04 2014
From: sergios at greeklug.gr (Sergey Tsabolov ( aka linuxman ))
Date: Thu, 13 Nov 2014 15:50:04 +0200
Subject: [squid-users] Icap Squid Https/Http
In-Reply-To: <1415877815645.51049@diladele.com>
References: <20116210.1085.1415874327324.JavaMail.garthl@Garth>,
 <31134040.1090.1415874552017.JavaMail.garthl@Garth>
 <1415877815645.51049@diladele.com>
Message-ID: <5464B70C.9090808@greeklug.gr>

Hello,
Nice way to filtering, I read them.
Only problem is supported by 64bit not 32bit, ok this is not big problem 
we can change the hardware.
But I need ask, what Pricing Plans you choose for this, on Our None 
Profit organization is not big problem but if you know the way without 
pricing inform me please.
Thank you.

On 13/11/2014 01:23 ??, Rafael Akchurin wrote:
>
> Hello Garth,
>
>
> We are happily doing ICAP HTTPS filtering, see sample instructions at 
> http://docs.diladele.com/tutorials/transparently_filtering_https_centos/index.html. 
> It is even more simple if you do not need "intercept" style proxying.
>
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
>
> ------------------------------------------------------------------------
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> on 
> behalf of Garth Lancaster <garthl at dcdata.co.za>
> *Sent:* Thursday, November 13, 2014 11:29 AM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Icap Squid Https/Http
> Hi There
>
> Is anyone able to confirm that https requests to squid proxy will be 
> sent on to the icap service? I am able to get normal http requests 
> into icap which displays a banner on the page.
> I have tried the whole transparent ssl-bump route as well.
>
> Thanks
> Garth
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
--------------------------------------------------------------------------------------
Don't send me documents in .doc , .docx, .xls, .ppt . , .pptx
Send it with ODF format : .odt , .odp , .ods or .pdf .
Try to use Open Document Format : http://el.libreoffice.org/
Save you money   &  use GNU/Linux Distro http://distrowatch.com/
-----------------------------------------------------------------------------------------
First they ignore you, then they ridicule you, then they fight you, then you win!!!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/c67a671b/attachment.htm>

From rafael.akchurin at diladele.com  Thu Nov 13 14:01:12 2014
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 13 Nov 2014 14:01:12 +0000
Subject: [squid-users] Icap Squid Https/Http
In-Reply-To: <5464B70C.9090808@greeklug.gr>
References: <20116210.1085.1415874327324.JavaMail.garthl@Garth>,
 <31134040.1090.1415874552017.JavaMail.garthl@Garth>
 <1415877815645.51049@diladele.com>,<5464B70C.9090808@greeklug.gr>
Message-ID: <1415887276984.48479@diladele.com>

?I do not pay a cent, I do it myself :)


________________________________
From: Sergey Tsabolov ( aka linuxman ) <sergios at greeklug.gr>
Sent: Thursday, November 13, 2014 2:50 PM
To: Rafael Akchurin; Garth Lancaster; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Icap Squid Https/Http

Hello,
Nice way to filtering, I read them.
Only problem is supported by 64bit not 32bit, ok this is not big problem we can change the hardware.
But I need ask, what Pricing Plans you choose for this, on Our None Profit organization is not big problem but if you know the way without pricing inform me please.
Thank you.

On 13/11/2014 01:23 ??, Rafael Akchurin wrote:

Hello Garth,


We are happily doing ICAP HTTPS filtering, see sample instructions at http://docs.diladele.com/tutorials/transparently_filtering_https_centos/index.html. It is even more simple if you do not need "intercept" style proxying.


Best regards,

Rafael Akchurin

Diladele B.V.


________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org><mailto:squid-users-bounces at lists.squid-cache.org> on behalf of Garth Lancaster <garthl at dcdata.co.za><mailto:garthl at dcdata.co.za>
Sent: Thursday, November 13, 2014 11:29 AM
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] Icap Squid Https/Http

Hi There

Is anyone able to confirm that https requests to squid proxy will be sent on to the icap service? I am able to get normal http requests into icap which displays a banner on the page.
I have tried the whole transparent ssl-bump route as well.

Thanks
Garth





_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users



--
--------------------------------------------------------------------------------------
Don't send me documents in .doc , .docx, .xls, .ppt . , .pptx
Send it with ODF format : .odt , .odp , .ods or .pdf .
Try to use Open Document Format : http://el.libreoffice.org/
Save you money   &  use GNU/Linux Distro http://distrowatch.com/
-----------------------------------------------------------------------------------------
First they ignore you, then they ridicule you, then they fight you, then you win!!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/e199eba9/attachment.htm>

From santosh.pai at vigyanlabs.com  Thu Nov 13 18:39:06 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Thu, 13 Nov 2014 10:39:06 -0800 (PST)
Subject: [squid-users] Squid Log file rotation
Message-ID: <1415903946072-4668374.post@n4.nabble.com>

Hello Team ,

I have a doubt with squid log file rotation ,my squid log file rotation is
configured this way in  /etc/logrotate.d/squid3

/var/log/squid3/*.log {
        daily
        compress
        delaycompress
        rotate 2
        missingok
        nocreate
        sharedscripts
        prerotate
                test ! -x /usr/sbin/sarg-reports || /usr/sbin/sarg-reports
        endscript
        postrotate
                test ! -e /var/run/squid3.pid || /usr/sbin/squid3 -k rotate
        endscript
}


by this i can understand that the the log is rotated daily and once the log
is rotated Old log files are renamed with numeric extensions. For example,
when a log  rotates , Squid renames log.6 to log.7, then log.5 to log.6, and
so on and it compresses too .

earlier i had logs from Oct 25 th to Oct 27th and i didnt use the server for
long and started using it from 7th november till today ie Nov 13th , i have
log files only from 7th till today where did the old log files go ? .

As per my understanding they should be archived in the squid log directory
itself rite ? . how to archive and keep the old log files ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Log-file-rotation-tp4668374.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hectorchan at gmail.com  Thu Nov 13 18:50:36 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Thu, 13 Nov 2014 13:50:36 -0500
Subject: [squid-users] Squid going through another forward proxy
Message-ID: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>

Hi,

Does anyone have any idea how to setup squid (reverse proxy) behind a
forward proxy ?

Thanks,
Hector
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/7dd6f35c/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Nov 13 19:49:59 2014
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 13 Nov 2014 20:49:59 +0100
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
Message-ID: <201411132049.59585.Antony.Stone@squid.open.source.it>

On Thursday 13 November 2014 at 19:50:36 (EU time), Hector Chan wrote:

> Hi,
> 
> Does anyone have any idea how to setup squid (reverse proxy) behind a
> forward proxy ?

1. Set up Squid as a forward proxy on machine A for the clients.

2. Set up Squid as a reverse proxy on machine B for the server/s.

What am I missing here?

The fact that you have two Squids doing different things on different machines 
simply means you set them up totally independently of each other.


Antony.


-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From mcsnv96 at afo.net  Thu Nov 13 20:49:45 2014
From: mcsnv96 at afo.net (Mike)
Date: Thu, 13 Nov 2014 14:49:45 -0600
Subject: [squid-users] Squid Log file rotation
In-Reply-To: <1415903946072-4668374.post@n4.nabble.com>
References: <1415903946072-4668374.post@n4.nabble.com>
Message-ID: <54651969.3050603@afo.net>

"rotate 2" means rotate 2 logs and delete anything older, so this is 
equivalent to 2 days.

With my job, most of our servers, we use rotate 7 or rotate 14 for 1 or 
2 weeks worth. Without drive scrubbing software, there is no easy way to 
get those files back.

Also it may help to put the prerotate entry above "daily" so it runs 
each line in proper order, sometimes the prerotate further down on 
certain systems works more like a "delay rotate".

Mike



On 11/13/2014 12:39 PM, santosh wrote:
> Hello Team ,
>
> I have a doubt with squid log file rotation ,my squid log file rotation is
> configured this way in  /etc/logrotate.d/squid3
>
> /var/log/squid3/*.log {
>          daily
>          compress
>          delaycompress
>          rotate 2
>          missingok
>          nocreate
>          sharedscripts
>          prerotate
>                  test ! -x /usr/sbin/sarg-reports || /usr/sbin/sarg-reports
>          endscript
>          postrotate
>                  test ! -e /var/run/squid3.pid || /usr/sbin/squid3 -k rotate
>          endscript
> }
>
>
> by this i can understand that the the log is rotated daily and once the log
> is rotated Old log files are renamed with numeric extensions. For example,
> when a log  rotates , Squid renames log.6 to log.7, then log.5 to log.6, and
> so on and it compresses too .
>
> earlier i had logs from Oct 25 th to Oct 27th and i didnt use the server for
> long and started using it from 7th november till today ie Nov 13th , i have
> log files only from 7th till today where did the old log files go ? .
>
> As per my understanding they should be archived in the squid log directory
> itself rite ? . how to archive and keep the old log files ?
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Log-file-rotation-tp4668374.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From hectorchan at gmail.com  Thu Nov 13 21:36:09 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Thu, 13 Nov 2014 16:36:09 -0500
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <201411132049.59585.Antony.Stone@squid.open.source.it>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
Message-ID: <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>

Basically, what I am looking for is whether it's possible to set up the
following:

Client (A) --> Squid as Reverse Proxy (B) --> Squid as Forward Proxy (C)
--> Origin Servers Depending on Client Request URI (D)

Depending on the client request from (A), (B) could route the request to
different origin servers (multiple cache_peer lines).  The request has to
go through (C) to reach (D), as (C) is the only way out of the network to
the internet.  Moreover, the logic of figuring out where to go to lies in
(C).

I know it's kind of a weird setup, but, unfortunately, we do not have
access or control of (A) and (C).

Thanks again,
Hector


On Thu, Nov 13, 2014 at 2:49 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Thursday 13 November 2014 at 19:50:36 (EU time), Hector Chan wrote:
>
> > Hi,
> >
> > Does anyone have any idea how to setup squid (reverse proxy) behind a
> > forward proxy ?
>
> 1. Set up Squid as a forward proxy on machine A for the clients.
>
> 2. Set up Squid as a reverse proxy on machine B for the server/s.
>
> What am I missing here?
>
> The fact that you have two Squids doing different things on different
> machines
> simply means you set them up totally independently of each other.
>
>
> Antony.
>
>
> --
> "The problem with television is that the people must sit and keep their
> eyes
> glued on a screen; the average American family hasn't time for it."
>
>  - New York Times, following a demonstration at the 1939 World's Fair.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/2f366cda/attachment.htm>

From Job at colliniconsulting.it  Thu Nov 13 22:16:50 2014
From: Job at colliniconsulting.it (Job)
Date: Thu, 13 Nov 2014 23:16:50 +0100
Subject: [squid-users] R: R: R: Problem with Squid 3.4 and transparent SSL
 proxy
In-Reply-To: <546438C2.4080205@treenet.co.nz>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>,
 <5462D335.2030701@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE9@W2008DC01.ColliniConsulting.lan>,
 <546438C2.4080205@treenet.co.nz>
Message-ID: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEEA@W2008DC01.ColliniConsulting.lan>

Hello Amos, thank you!

I solved with this configuration:

http_port 3128
http_port 192.168.10.254:3129 intercept
https_port 192.168.10.254:3130 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH options=NO_SSLv2

as you told me to find Peter G.'s thread! Now it works i think good.

Just a question: both transparent and explicited proxy, can cohexist with interception and ssl bump?
Or i have to duplicated configurations of host and ports in squid.conf?

Thank you again,
Francesco


________________________________________
Da: Amos Jeffries [squid3 at treenet.co.nz]
Inviato: gioved? 13 novembre 2014 5.51
A: Job; squid-users at lists.squid-cache.org
Oggetto: Re: R: R: [squid-users] Problem with Squid 3.4 and transparent SSL proxy

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 12/11/2014 9:55 p.m., Job wrote:
> Thank you Amos, for everything.
>
> I route with REDIRECT all outgoing connection to port tcp/443 from
> my LAN:
>
> iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT
> --to-port 3130
>
> in squid, i have these configurations:
>
> http_port 3128 http_port 3129 intercept https_port 3130 intercept
> ssl-bump connection-auth=off generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem
> key=/etc/squid/ssl/squid.key
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>
>  Do you think my iptables rule is wrong?

The iptables looks fine.

Peter G, in a recent thread added the IP address Squid was being
contacted on to the port details. Maybe that will work for you too.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZDjCAAoJELJo5wb/XPRjse0IAIL7VDPvPvConqwAxSBP1O79
D8uBOW8D1WVxLARD4HmW9To6qSSten3QwYfJYcwhk0BRDyzh0h0PCiduhWe50H8b
MdK/TXbBdo79q8AobaHkycBQeKwYWKjnHd5IvEp+fPzNK5izqIoWcvdnfFOnSZVh
ULuus+CmKnkykgcYBClxwRlnDo30SPSVUWUS5dgT2Z6r4xnvAANTlpwCJxodcqz/
9zq6vn8dnYYdtIgvuz7SgI49bSDxNo0aa+tizl2P0sKSIxfw5vnnaaj8VXWdeS+r
cpD4H0Wju7CXIyGXfgkDBl/BP3gVUjGVyWJkXN5XYx3Qyu4kKEg4absRTR5+tYc=
=c8G3
-----END PGP SIGNATURE-----

From squid3 at treenet.co.nz  Fri Nov 14 00:13:12 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 13:13:12 +1300
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
Message-ID: <54654918.2090003@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 10:36 a.m., Hector Chan wrote:
> Basically, what I am looking for is whether it's possible to set up
> the following:
> 
> Client (A) --> Squid as Reverse Proxy (B) --> Squid as Forward
> Proxy (C) --> Origin Servers Depending on Client Request URI (D)
> 
> Depending on the client request from (A), (B) could route the
> request to different origin servers (multiple cache_peer lines).
> The request has to go through (C) to reach (D), as (C) is the only
> way out of the network to the internet.  Moreover, the logic of
> figuring out where to go to lies in (C).
> 
> I know it's kind of a weird setup, but, unfortunately, we do not
> have access or control of (A) and (C).

Nothing weird about that at all. It is a perfectly normal CDN proxy
chain. The only potentially tricky bit will be getting the domain
owner to add your reverse-proxy IP address to the domain.

Forward/reverse are *input* modes, indicating the HTTP syntax expected
and resulting behaviour. Any type of proxy can be configured with a
cache_peer for *output*.

Only the interception proxy have problems with cache_peer sometimes,
because their very existence is a violation of the protocol.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZUkXAAoJELJo5wb/XPRj3F8H/2OIGI+RGJuJCWmH13cCmbdZ
jA4GZDYU8QgGPx6eUJ5kCVQPi31nnkNKtfv5WFfhetj7syYj40ow+C0DDaUCsPT/
udVBLMqA/qaoEYkT6q6mTu7D5xio/Gsd+SjBmPG7w6mJx6YiAceoP7/61/7EKZNA
vMTFfCuubh8a0IDSOajMUlf/ZRlRXkYNEnZMeGTliCKz2vnA/gjasu5fJcDPUunl
6JpYa4DRfYZ2S3vElyCnkInN53XkQDcTLKBR4jKIXvsjfVuc4mYxDf2TQj8hhIy0
5TGzAtMq4D53LuyH3mGN2mK0OO6itGO5k+ALHIiDgGoP6AoZU6oIqjuaLPV117k=
=NIK1
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Fri Nov 14 00:19:57 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 13:19:57 +1300
Subject: [squid-users] R: R: R: Problem with Squid 3.4 and transparent
 SSL proxy
In-Reply-To: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEEA@W2008DC01.ColliniConsulting.lan>
References: <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE5@W2008DC01.ColliniConsulting.lan>,
 <54621DA7.2060002@ngtech.co.il>,
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE6@W2008DC01.ColliniConsulting.lan>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE7@W2008DC01.ColliniConsulting.lan>,
 <54622B9E.6010308@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE8@W2008DC01.ColliniConsulting.lan>,
 <5462D335.2030701@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEE9@W2008DC01.ColliniConsulting.lan>,
 <546438C2.4080205@treenet.co.nz>
 <88EF58F000EC4B4684700C2AA3A73D7A04F5323FBEEA@W2008DC01.ColliniConsulting.lan>
Message-ID: <54654AAD.6090505@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 11:16 a.m., Job wrote:
> Hello Amos, thank you!
> 
> I solved with this configuration:
> 
> http_port 3128 http_port 192.168.10.254:3129 intercept https_port
> 192.168.10.254:3130 intercept ssl-bump connection-auth=off
> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
> options=NO_SSLv2
> 
> as you told me to find Peter G.'s thread! Now it works i think
> good.

Yay. Now I amm interested in finding out exactly why NAT fails with
the port-only config. What OS are you using? and have you done
anything special regarding IPv4/IPv6 to it?


> 
> Just a question: both transparent and explicited proxy, can
> cohexist with interception and ssl bump? Or i have to duplicated
> configurations of host and ports in squid.conf?

Yes. ssl-bump only occurs when there is TLS/SSL to decrypt. That is
separate from the traffic syntax/mode.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZUqtAAoJELJo5wb/XPRju/IH/04IR4RiiPkycROZZGlCnONp
KC1ujoh1eEz1cUvaqBzNVwm+9DFWI+JINWCo9Za0oj7qYfi2FVZRnncf4XXx4sJo
9lSrlaNOKT7ReWS7caNfszb83dsZi0pJ95NlDMS3mpuFCUaDCB1UTEsGp2jNW3d+
kLEYYNyAOtcIItAe9KT3zBeqZzk29HKmSWYozAu3jnVju3+af22bkdjgHMBtxvYQ
Zav9iITws7Pkp6Tr54b37NwWDzgQUAhJn8Ao402dZGVZNHkWvLbIcxViAHTUoW+n
Eq0qJzB86gBBe1YqPAIWYQdCIgvYJebVSY1Ep0Z08psEMKxCTSdTE80I+2G3BtI=
=XqLv
-----END PGP SIGNATURE-----


From hectorchan at gmail.com  Fri Nov 14 01:14:16 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Thu, 13 Nov 2014 17:14:16 -0800
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <54654918.2090003@treenet.co.nz>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
Message-ID: <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>

Hi Amos,

Thanks for your reply.  Let's say I have the following cache_peer lines in
(B), and the address for (C) is "forward-proxy.example.com:3128".

cache_peer    origin-x.example.com    parent 443 0 no-query originserver ssl
cache_peer    origin-y.example.com    parent 443 0 no-query originserver ssl
cache_peer    origin-z.example.com    parent 443 0 no-query originserver ssl

What would be the syntax to configure (B) to use "
forward-proxy.example.com:3128" (C) as the forward proxy to the origin
servers (D) ?

Thanks again,
Hector



On Thu, Nov 13, 2014 at 4:13 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 14/11/2014 10:36 a.m., Hector Chan wrote:
> > Basically, what I am looking for is whether it's possible to set up
> > the following:
> >
> > Client (A) --> Squid as Reverse Proxy (B) --> Squid as Forward
> > Proxy (C) --> Origin Servers Depending on Client Request URI (D)
> >
> > Depending on the client request from (A), (B) could route the
> > request to different origin servers (multiple cache_peer lines).
> > The request has to go through (C) to reach (D), as (C) is the only
> > way out of the network to the internet.  Moreover, the logic of
> > figuring out where to go to lies in (C).
> >
> > I know it's kind of a weird setup, but, unfortunately, we do not
> > have access or control of (A) and (C).
>
> Nothing weird about that at all. It is a perfectly normal CDN proxy
> chain. The only potentially tricky bit will be getting the domain
> owner to add your reverse-proxy IP address to the domain.
>
> Forward/reverse are *input* modes, indicating the HTTP syntax expected
> and resulting behaviour. Any type of proxy can be configured with a
> cache_peer for *output*.
>
> Only the interception proxy have problems with cache_peer sometimes,
> because their very existence is a violation of the protocol.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUZUkXAAoJELJo5wb/XPRj3F8H/2OIGI+RGJuJCWmH13cCmbdZ
> jA4GZDYU8QgGPx6eUJ5kCVQPi31nnkNKtfv5WFfhetj7syYj40ow+C0DDaUCsPT/
> udVBLMqA/qaoEYkT6q6mTu7D5xio/Gsd+SjBmPG7w6mJx6YiAceoP7/61/7EKZNA
> vMTFfCuubh8a0IDSOajMUlf/ZRlRXkYNEnZMeGTliCKz2vnA/gjasu5fJcDPUunl
> 6JpYa4DRfYZ2S3vElyCnkInN53XkQDcTLKBR4jKIXvsjfVuc4mYxDf2TQj8hhIy0
> 5TGzAtMq4D53LuyH3mGN2mK0OO6itGO5k+ALHIiDgGoP6AoZU6oIqjuaLPV117k=
> =NIK1
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/0956e356/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 01:36:33 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 14:36:33 +1300
Subject: [squid-users] Squid 3.4.6 POST upload problem
In-Reply-To: <CABMqo8-9sdeMi1V=b5R=CeUDvde7kAwcqGzqSroLFR=_u7MtEg@mail.gmail.com>
References: <CABMqo8-9sdeMi1V=b5R=CeUDvde7kAwcqGzqSroLFR=_u7MtEg@mail.gmail.com>
Message-ID: <54655CA1.7040704@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 13/11/2014 10:10 p.m., Lorenzo Gollinelli wrote:
> Thank you very much Amos,
> 
> why then do we have the problem only if file is > 55kB? The bug
> your are referring (#4067) to is not listed in version 3.4 known
> bugs

I thinnks the size weirdness is related to Preview: ICAP feature.

#4067 does not show up as 3.4 new bug because ICAP is a 3.2+ feature.
The bug exists in all 3.2+ versions.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZVygAAoJELJo5wb/XPRjk7MH/RI/4juPYD3KPIAo9EVyfFnc
gwjnuXClgRLfk1aIEtTxouwWzoTwvGUvQ6z1lsFserNkDV5tfcV1+YV38sKPFGgk
lhuaLDMgC5fyguk92s/9AomPVtxv0bp3qaKPaH5g7gCJX7a0QvGK5Z1DW7vf+1KS
6N9mE/Ygh2gy13YLH+NJPm6M/OwKJ+ab2/s0VRiaiDms2kgSVJ18F3WLytOj0WmP
rj0nNwSSCm9xxjpfT0mJxq6A2lVnQdOQ/n4CWn1yrMDXYji3o9tCG6oEmBJkJIxr
X5YoPtJRmOQq3ZE3Y1uOMzSe4drEsd/b/jGuDh0zHVKKJL8WbDGcbJhOS8efNCo=
=iN0S
-----END PGP SIGNATURE-----


From hectorchan at gmail.com  Fri Nov 14 03:03:04 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Thu, 13 Nov 2014 19:03:04 -0800
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
Message-ID: <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>

Ah, I think I have a typo in my question.  Originally, I mentioned the
following:

> the logic of figuring out where to go to lies in (C).

What I actually meant is "the logic that figuring out where to go lies in
(B)" (not C).

On Thu, Nov 13, 2014 at 5:14 PM, Hector Chan <hectorchan at gmail.com> wrote:

> Hi Amos,
>
> Thanks for your reply.  Let's say I have the following cache_peer lines in
> (B), and the address for (C) is "forward-proxy.example.com:3128".
>
> cache_peer    origin-x.example.com    parent 443 0 no-query originserver
> ssl
> cache_peer    origin-y.example.com    parent 443 0 no-query originserver
> ssl
> cache_peer    origin-z.example.com    parent 443 0 no-query originserver
> ssl
>
> What would be the syntax to configure (B) to use "
> forward-proxy.example.com:3128" (C) as the forward proxy to the origin
> servers (D) ?
>
> Thanks again,
> Hector
>
>
>
> On Thu, Nov 13, 2014 at 4:13 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> On 14/11/2014 10:36 a.m., Hector Chan wrote:
>> > Basically, what I am looking for is whether it's possible to set up
>> > the following:
>> >
>> > Client (A) --> Squid as Reverse Proxy (B) --> Squid as Forward
>> > Proxy (C) --> Origin Servers Depending on Client Request URI (D)
>> >
>> > Depending on the client request from (A), (B) could route the
>> > request to different origin servers (multiple cache_peer lines).
>> > The request has to go through (C) to reach (D), as (C) is the only
>> > way out of the network to the internet.  Moreover, the logic of
>> > figuring out where to go to lies in (C).
>> >
>> > I know it's kind of a weird setup, but, unfortunately, we do not
>> > have access or control of (A) and (C).
>>
>> Nothing weird about that at all. It is a perfectly normal CDN proxy
>> chain. The only potentially tricky bit will be getting the domain
>> owner to add your reverse-proxy IP address to the domain.
>>
>> Forward/reverse are *input* modes, indicating the HTTP syntax expected
>> and resulting behaviour. Any type of proxy can be configured with a
>> cache_peer for *output*.
>>
>> Only the interception proxy have problems with cache_peer sometimes,
>> because their very existence is a violation of the protocol.
>>
>> Amos
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2.0.22 (MingW32)
>>
>> iQEcBAEBAgAGBQJUZUkXAAoJELJo5wb/XPRj3F8H/2OIGI+RGJuJCWmH13cCmbdZ
>> jA4GZDYU8QgGPx6eUJ5kCVQPi31nnkNKtfv5WFfhetj7syYj40ow+C0DDaUCsPT/
>> udVBLMqA/qaoEYkT6q6mTu7D5xio/Gsd+SjBmPG7w6mJx6YiAceoP7/61/7EKZNA
>> vMTFfCuubh8a0IDSOajMUlf/ZRlRXkYNEnZMeGTliCKz2vnA/gjasu5fJcDPUunl
>> 6JpYa4DRfYZ2S3vElyCnkInN53XkQDcTLKBR4jKIXvsjfVuc4mYxDf2TQj8hhIy0
>> 5TGzAtMq4D53LuyH3mGN2mK0OO6itGO5k+ALHIiDgGoP6AoZU6oIqjuaLPV117k=
>> =NIK1
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/7f887fad/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 03:38:02 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 16:38:02 +1300
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
 <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
Message-ID: <5465791A.4080500@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 4:03 p.m., Hector Chan wrote:
> Ah, I think I have a typo in my question.  Originally, I mentioned
> the following:
> 
>> the logic of figuring out where to go to lies in (C).
> 
> What I actually meant is "the logic that figuring out where to go
> lies in (B)" (not C).
> 

Actually both require that logic. Relative to the traffic each is
receiving.



> On Thu, Nov 13, 2014 at 5:14 PM, Hector Chan wrote:
> 
>> Hi Amos,
>> 
>> Thanks for your reply.  Let's say I have the following cache_peer
>> lines in (B), and the address for (C) is
>> "forward-proxy.example.com:3128".
>> 
>> cache_peer    origin-x.example.com    parent 443 0 no-query
>> originserver ssl cache_peer    origin-y.example.com    parent 443
>> 0 no-query originserver ssl cache_peer    origin-z.example.com
>> parent 443 0 no-query originserver ssl
>> 
>> What would be the syntax to configure (B) to use " 
>> forward-proxy.example.com:3128" (C) as the forward proxy to the
>> origin servers (D) ?

those lines you specify above go in (C). *if* they are needed at all.

In (B) goes:

  cache_peer forward-proxy.example.com parent 3128 0 name=C

  acl sendToC dstdomain origin-x.example.com origin-y.example.com
origin-z.example.com
  cache_peer_access C sendToC


OR, since you say C is the *only* way to access the Internet you can
omit the acl and cache_peer_access lines. Which will cause all traffic
to try and go through C.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZXkaAAoJELJo5wb/XPRjNaYH/0rD/r4hepby40bDQnLTzJyr
BP5CY/WP6tqBvavJHpJE41TpRhhx6HilZSWccAnhe5Dk7O+wit2rbp4lxZqXID01
6sxzPb6IjXldgXd6vmjCzkK8QiDFdFjEev2+26LAX6cPuQOEuiyzCiFFHzI6TK7c
lXGqDGbkTmBtIplHVAz/7u4IJlb2TNbT23wkzt6jJHUBYxhhY23g3x1sT4Tc045U
oseTZnazY+Ehrglf/NdNJfBAkcl71Lp65rxDLvo2ayUNffiXeg6bNtd7oGHyZ0S3
MMUxsF2XA7kDxgPCtD5qhVFnxnhTKCMawz3gf6PRxRlSqeCxB+FrWx01frblx2s=
=VsYA
-----END PGP SIGNATURE-----


From hectorchan at gmail.com  Fri Nov 14 05:22:44 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Thu, 13 Nov 2014 21:22:44 -0800
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <5465791A.4080500@treenet.co.nz>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
 <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
 <5465791A.4080500@treenet.co.nz>
Message-ID: <CAEhCwUw3aZsdNeMLAPBBojELPTWWCMZc9sbz4SXUOabaU-aJ2A@mail.gmail.com>

Hi Amos,

> those lines you specify above go in (C). *if* they are needed at all.

But I don't have control over (C).  It's off limits.

> In (B) goes:
>
>  cache_peer forward-proxy.example.com parent 3128 0 name=C
>
> acl sendToC dstdomain origin-x.example.com origin-y.example.com
origin-z.example.com
> cache_peer_access C sendToC

The requests reaching (B) (reverse-proxy.example.com) are in the form:
     http://reverse-proxy.example.com/goto-origin-x
     http://reverse-proxy.example.com/goto-origin-y
     http://reverse-proxy.example.com/goto-origin-z

and I have a couple of cache_peer_access acls (urlpath regex) to send them
to origin-x, origin-y, and origin-z.  How would the above dstdomain acl
work with these rules?

On Thu, Nov 13, 2014 at 7:38 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 14/11/2014 4:03 p.m., Hector Chan wrote:
> > Ah, I think I have a typo in my question.  Originally, I mentioned
> > the following:
> >
> >> the logic of figuring out where to go to lies in (C).
> >
> > What I actually meant is "the logic that figuring out where to go
> > lies in (B)" (not C).
> >
>
> Actually both require that logic. Relative to the traffic each is
> receiving.
>
>
>
> > On Thu, Nov 13, 2014 at 5:14 PM, Hector Chan wrote:
> >
> >> Hi Amos,
> >>
> >> Thanks for your reply.  Let's say I have the following cache_peer
> >> lines in (B), and the address for (C) is
> >> "forward-proxy.example.com:3128".
> >>
> >> cache_peer    origin-x.example.com    parent 443 0 no-query
> >> originserver ssl cache_peer    origin-y.example.com    parent 443
> >> 0 no-query originserver ssl cache_peer    origin-z.example.com
> >> parent 443 0 no-query originserver ssl
> >>
> >> What would be the syntax to configure (B) to use "
> >> forward-proxy.example.com:3128" (C) as the forward proxy to the
> >> origin servers (D) ?
>
> those lines you specify above go in (C). *if* they are needed at all.
>
> In (B) goes:
>
>   cache_peer forward-proxy.example.com parent 3128 0 name=C
>
>   acl sendToC dstdomain origin-x.example.com origin-y.example.com
> origin-z.example.com
>   cache_peer_access C sendToC
>
>
> OR, since you say C is the *only* way to access the Internet you can
> omit the acl and cache_peer_access lines. Which will cause all traffic
> to try and go through C.
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUZXkaAAoJELJo5wb/XPRjNaYH/0rD/r4hepby40bDQnLTzJyr
> BP5CY/WP6tqBvavJHpJE41TpRhhx6HilZSWccAnhe5Dk7O+wit2rbp4lxZqXID01
> 6sxzPb6IjXldgXd6vmjCzkK8QiDFdFjEev2+26LAX6cPuQOEuiyzCiFFHzI6TK7c
> lXGqDGbkTmBtIplHVAz/7u4IJlb2TNbT23wkzt6jJHUBYxhhY23g3x1sT4Tc045U
> oseTZnazY+Ehrglf/NdNJfBAkcl71Lp65rxDLvo2ayUNffiXeg6bNtd7oGHyZ0S3
> MMUxsF2XA7kDxgPCtD5qhVFnxnhTKCMawz3gf6PRxRlSqeCxB+FrWx01frblx2s=
> =VsYA
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141113/0afce38b/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 06:35:02 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 19:35:02 +1300
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUw3aZsdNeMLAPBBojELPTWWCMZc9sbz4SXUOabaU-aJ2A@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
 <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
 <5465791A.4080500@treenet.co.nz>
 <CAEhCwUw3aZsdNeMLAPBBojELPTWWCMZc9sbz4SXUOabaU-aJ2A@mail.gmail.com>
Message-ID: <5465A296.8010109@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 6:22 p.m., Hector Chan wrote:
> Hi Amos,
> 
>> those lines you specify above go in (C). *if* they are needed at
>> all.
> 
> But I don't have control over (C).  It's off limits.

Then you have to trust that the admin in charge of it set it up right.

> 
>> In (B) goes:
>> 
>> cache_peer forward-proxy.example.com parent 3128 0 name=C
>> 
>> acl sendToC dstdomain origin-x.example.com origin-y.example.com
> origin-z.example.com
>> cache_peer_access C sendToC
> 
> The requests reaching (B) (reverse-proxy.example.com) are in the
> form: http://reverse-proxy.example.com/goto-origin-x 
> http://reverse-proxy.example.com/goto-origin-y 
> http://reverse-proxy.example.com/goto-origin-z
> 
> and I have a couple of cache_peer_access acls (urlpath regex) to
> send them to origin-x, origin-y, and origin-z.  How would the above
> dstdomain acl work with these rules?

You have now stopped using HTTP and started using some strange
URL-embeded protocol.

An HTTP proxy cannot help you there. You require a proxy that
understands and acts on the URL-embeded protocol messages.

It is possible to extend Squid with URL-rewrite helpers that can
translate it into different HTTP URL for passing to (C). BUT, there is
no guarantee of what origin (C) will use to fetch that resource. You
have to *trust* that (C) uses the origin best suited to any request
that it is given, according to the criteria its own admin has set for
"best".

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZaKVAAoJELJo5wb/XPRjdpQH/iBh1HQcAZQr0gqK7FS8nZ9x
v0fzAOx/L0HCG5MTT7drwvvEVltxMRYoVniM8VJSqUw3cFAlI+2VEScIr3oOFjcr
qAdjxyjer7sxVgmQM80Oa+n40RK7mvZejvhEV9/0Gc0XTmAjL3PrBptKpumslhVh
rq40LUX50rg5xaAfA02WCy4mYS99uH7qBABWIXeeESVdvGLVRTaTlthqaKW8JTFh
pjmS9OKVnk5CeEi6cyJ8VV7edBOgv2rpgUH8Wjap66mmIjVHq8alNU53obRAMk7p
Pd/bPfPFERnoBymbYmYfFBd3Mfddgc49Wpz9gggAWgXE8bq6CbXQHpj5GvUayaE=
=mS+q
-----END PGP SIGNATURE-----


From jkillimangalam at in.rm.com  Fri Nov 14 07:02:21 2014
From: jkillimangalam at in.rm.com (John Killimangalam Jacob)
Date: Fri, 14 Nov 2014 07:02:21 +0000
Subject: [squid-users] Is it safe to set number of sslcrtd_children to 50?
Message-ID: <8da9a2404901440185b5a52f82ce9e74@DBXPR04MB493.eurprd04.prod.outlook.com>

Hi All,

For my configuration to use the ssl bump, I am setting the number of sslcrtd_children to 50. But in the documentation it is written that   "The maximum this may be safely set to is 32" . When I set it to 32, I am getting warning that all 32/32 helpers are busy, consider increasing the number of helpers. So I increased it to 50 and the warnings are no longer appearing. So is it safe to fix  the number of sslcrtd helpers to 50?I am aware that this setting may take more resources compared to the recommended one. Also  is there any restriction from the squid code/implementation on the maximum number of sslcrtd children?

Please find below the configuration line.

sslcrtd_children 50 startup=5 idle=1

Thanks in Advance,
John

Visit our Website at www.rmesi.co.in<http://www.rmesi.co.in>

This message is confidential. You should not copy it or disclose its contents to anyone. You may use and apply the information for the intended purpose only. Internet communications are not secure; therefore, RMESI does not accept legal responsibility for the contents of this message. Any views or opinions presented are those of the author only and not of RMESI. If this email has come to you in error, please delete it, along with any attachments. Please note that RMESI may intercept incoming and outgoing email communications.

Freedom of Information Act 2000
This email and any attachments may contain confidential information belonging to RMESI. Where the email and any attachments do contain information of a confidential nature, including without limitation information relating to trade secrets, special terms or prices these shall be deemed for the purpose of the Freedom of Information Act 2000 as information provided in confidence by RMESI and the disclosure of which would be prejudicial to RMESI's commercial interests.

This email has been scanned for viruses by Trend ScanMail.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/68b83439/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 07:37:57 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Nov 2014 20:37:57 +1300
Subject: [squid-users] Is it safe to set number of sslcrtd_children to
 50?
In-Reply-To: <8da9a2404901440185b5a52f82ce9e74@DBXPR04MB493.eurprd04.prod.outlook.com>
References: <8da9a2404901440185b5a52f82ce9e74@DBXPR04MB493.eurprd04.prod.outlook.com>
Message-ID: <5465B155.6040901@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14/11/2014 8:02 p.m., John Killimangalam Jacob wrote:
> Hi All,
> 
> For my configuration to use the ssl bump, I am setting the number
> of sslcrtd_children to 50. But in the documentation it is written
> that "The maximum this may be safely set to is 32" . When I set it
> to 32, I am getting warning that all 32/32 helpers are busy,
> consider increasing the number of helpers. So I increased it to 50
> and the warnings are no longer appearing. So is it safe to fix  the
> number of sslcrtd helpers to 50?I am aware that this setting may
> take more resources compared to the recommended one. Also  is there
> any restriction from the squid code/implementation on the maximum
> number of sslcrtd children?

The only limits internal to Squid are rather much higher than woudl be
reasonable to use. The lower restrictions that exist on helpers come
from the APIs or protocols they are using.

For the ssl_crtd helpers I suspect it is based around how many
concurrent uses of the cert database can safelly co-exist and entropy
the OS random generator can produce.

Christos workign on behalf of Measurement Factory wrote that text
though, so perhapse he can answer your question directly.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZbFVAAoJELJo5wb/XPRjZuQIAN0kueGDzqxNQtc2VgaAOQM6
RklXqy2lsm99capalMgBWUjzvN57UpYAyX6RZ/Thi0OjzVNGiM0OYF1DViHSJc07
EfuguSnYmpS/buIBYnvC3Vy35dO9lZ9wEsxWwDE+hKA0+q0lFjaENscetRVJ862F
Wg2UxvXW/96PnhRoUkMBwvAfGd+itjFIl+m/4iIWopHJcrhOs7qpc46XLDvAFyaz
5WiQf/ruxI69K8TjrhjJNXn5Az+Mk/qGO+2SAhrGdUlnyvnCVkRuVUaGwzwUn0T6
1z0o2eFvunbFy2oiPKjOHpmFAbldJ0QWT/n5jm+TIXJzj82weH5uPv8a9bvFhrE=
=yqzN
-----END PGP SIGNATURE-----


From santosh.pai at vigyanlabs.com  Fri Nov 14 10:16:06 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Fri, 14 Nov 2014 02:16:06 -0800 (PST)
Subject: [squid-users] Squid Log file rotation
In-Reply-To: <1415903946072-4668374.post@n4.nabble.com>
References: <1415903946072-4668374.post@n4.nabble.com>
Message-ID: <1415960166555-4668390.post@n4.nabble.com>

Do we have to make any updates in /etc/logrotate.conf or this is sufficient ?
I have now changed the settings as below as I want to keep 6 months logs .

/var/log/squid3/*.log {
        weekly
        compress
        delaycompress
        rotate 24
        missingok
        nocreate
        sharedscripts
        prerotate
                test ! -x /usr/sbin/sarg-reports || /usr/sbin/sarg-reports
        endscript
        postrotate
                test ! -e /var/run/squid3.pid || test ! -x /usr/sbin/squid3
|| /usr/sbin/squid3 -k rotate
        endscript
}



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Log-file-rotation-tp4668374p4668390.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From schinken at hackerspace-bamberg.de  Fri Nov 14 11:33:11 2014
From: schinken at hackerspace-bamberg.de (schinken)
Date: Fri, 14 Nov 2014 12:33:11 +0100
Subject: [squid-users] Fallback auth method
In-Reply-To: <5461D7F4.1040302@hackerspace-bamberg.de>
References: <5461D689.7050609@hackerspace-bamberg.de>
 <5461D7F4.1040302@hackerspace-bamberg.de>
Message-ID: <5465E877.1000200@hackerspace-bamberg.de>

Maybe i'll try to simplify my question ;)


Is it possible to skip the:

> http_access allow AllowedMemberOf all

if auth_param basic was the authenticator (instead of the other
authentications like NTLM/LDAP)?


Condensed config:

>> # basic-auth
>>
>> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd
>>
>>
>> # AD memberof check
>>
>> external_acl_type memberof ttl=300 negative_ttl=300 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -b "dc=COMPANY,dc=i
>> nt" -D squid at company.int -W /etc/squid3/ldappass.txt -f "(&(objectclass=person)(sAMAccountName=%v)(memberof:1.2.
>> 840.113556.1.4.1941:=cn=%g,ou=Groups,ou=foobar,dc=COMPANY,dc=int))" -h ad.company.int,ad3.company.int
>>
>> acl auth proxy_auth REQUIRED
>> http_access deny !auth
>> http_access allow auth
>>
>> acl AllowedMemberOf external memberof "/etc/squid3/memberof_allow.txt
>> acl BlockedMemberOf external memberof "/etc/squid3/memberof_deny.txt"
>>
>> http_access allow AllowedMemberOf all
>> http_access deny BlockedMemberOf all



-- 
Schinken

Backspace e.V.
http://hackerspace-bamberg.de

mail: schinken at hackerspace-bamberg.de
xmpp: schinken at tai-wahn.de (otr)
GPG: FFB7 E40D B2DD D24C C9B7 B5C5 703C F8B8 882C 871E

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/a1c64308/attachment.sig>

From squid3 at treenet.co.nz  Fri Nov 14 12:00:49 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 01:00:49 +1300
Subject: [squid-users] Fallback auth method
In-Reply-To: <5465E877.1000200@hackerspace-bamberg.de>
References: <5461D689.7050609@hackerspace-bamberg.de>
 <5461D7F4.1040302@hackerspace-bamberg.de>
 <5465E877.1000200@hackerspace-bamberg.de>
Message-ID: <5465EEF1.8090704@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 12:33 a.m., schinken wrote:
> Maybe i'll try to simplify my question ;)
> 
> 
> Is it possible to skip the:
> 
>> http_access allow AllowedMemberOf all
> 
> if auth_param basic was the authenticator (instead of the other 
> authentications like NTLM/LDAP)?

They are already being skipped by the "http_access allow auth" line.
Which allows all clients through as long as they can provide
credentials from any of the configured auth schemes.


What you asked for is done like this:

  acl basicAuth req_header Proxy-Authorization ^Basic

  http_access allow !basicAuth AllowedMemberOf all
  http_access deny !basicAuth BlockedMemberOf all

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZe7uAAoJELJo5wb/XPRj1ZsH/2F4LzWInFlKJO+DgtmQ4j2n
nExTcKZj0C1REwUPpGTE3umv5KNXRY36AeWMOEuVAH9hVA//rR4PkX8eaP72qFn8
vEwHCQL0+SBSTlo8ftg5yrFNS6CLL/MilsU6Jg0H8gFKqdK2BEJG8DCX+0kGN/EE
JXM78DsHK2ZWZRcRmIewT6jv2FDhyIoQM++EtCd+q4E2/HBYY3HyisJxqhr+WaP6
mqjjkSRnY3TlmIgh6nC+SQQnJREeKg++qgiw4B2vJUPELE+6nk5BzeHeuGPXFeU7
Eq/48jILAGMvKhX17Ze0KLqQlQ5HRL/0bTbvzD1jyfUz8OiYRm0gKfkYbRc3nZE=
=QhEo
-----END PGP SIGNATURE-----


From ulises at vianetcon.com.ar  Fri Nov 14 14:41:18 2014
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Fri, 14 Nov 2014 11:41:18 -0300
Subject: [squid-users] How to make squid proxy server cache response with
	vary: * in header?
Message-ID: <5466148E.4020000@vianetcon.com.ar>

HI All!!!

I'm building a system based on squid 2.7 for cache all software updates. 
I have a problem when the origin server response with vary:* in header, 
for example for this url (Chrome updates)

http://r20---sn-x1x7sn7r.c.pack.google.com/edgedl/chrome/win/BD7EFFAED0C46EB9/38.0.2125.111_chrome_installer.exe

Without  squid, use direct download, the response header has

HTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Length: 41100368
Content-Type: application/x-msdos-program
Etag: "4a5cd"
Server: downloads
Vary: *
x-content-type-options: nosniff
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
Date: Sat, 08 Nov 2014 05:41:10 GMT
Alternate-Protocol: 80:quic,p=0.01
Last-Modified: Mon, 27 Oct 2014 18:10:00 GMT
Connection: keep-alive

If i use Squid in transparent proxy mode in the middle, i don't have 
hits for this url. ?Can squid ignore the Vary header for store object in 
cache? Of course my refresh pattern has

# Cache Update
refresh_pattern -i pack\.google\.com\/.*\.(exe|crx) 10080 80% 43200 
override-expire override-lastmod ignore-no-cache  ignore-reload 
reload-into-ims ignore-private

#Specific App
refresh_pattern -i 
\.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|pdf|tiff)$ 10080 90% 
43200 override-expire ignore-no-cache ignore-private

Thanks in Advance,

Ulises


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/4c2d3136/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 15:22:33 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 04:22:33 +1300
Subject: [squid-users] How to make squid proxy server cache response
 with vary: * in header?
In-Reply-To: <5466148E.4020000@vianetcon.com.ar>
References: <5466148E.4020000@vianetcon.com.ar>
Message-ID: <54661E39.2090009@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 3:41 a.m., Ulises Nicolini wrote:
> HI All!!!
> 
> I'm building a system based on squid 2.7 for cache all software
> updates.

Not possible. Squid-2.7 does not support enough HTTP/1.1 features to
cache all content properly. Upgrade to a current release of Squid.

> I have a problem when the origin server response with vary:* in
> header,

I'll bet. The * means that the URL represents multiple different
versions of the object being delivered, which one is in this response
depends on something outside the HTTP request used to fetch it. There
is no way your proxy can identify or replicate that variance decision.

Example; the chrome D/L server may be delivering .exe containing
built-in translations based on your personal account G' preferences.
 How would you like to find an automated upgrade made your browser
suddenly display only Klingon because the first person going to update
through this proxy was a Trekkie ?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZh44AAoJELJo5wb/XPRjiUoIALK58V1ktnrTBAs4XDwNmJs4
JTheP81PxmR5nvr85GE36aX5RvRVNy7SGaxhzGB3+oUhcPZkaJruTJHYKMy6K6jV
WvfDxGDTBmj6t+W8yyFD/CEj7WC/IuEVPBdscy4l47KvHfxk7q+VQ8LwWdZGy1O2
s03Qqzc2AM7mbcpGwZKQcQN3uNqq3rbJDa/seQ1tZnQ08RO9tHMHq4tR+/gF+pjU
Y4VgQ2eN5yZ3ERk38/JBDExzlnNMl9DDol8QyeSdLyxNfw8Rz9IEVLZzEPuSg/jK
+FB5OumaZe6A0uvyd3LQUFcteoiJqeEkGAXkAnc8zv27wmzZ2o9AIKpqiYgtwP0=
=dxhi
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Sat Nov 15 01:35:38 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Fri, 14 Nov 2014 17:35:38 -0800
Subject: [squid-users] using squid 3.head for large rock ,
	but i still have mean object size is 32 !!!!!
Message-ID: <000a01d00074$77027340$650759c0$@netstream.ps>

 

Hi ,

 

I migrated from squid 3.4.3 so that I wish to have a chance to save bw.

 

Im using : Squid Cache: Version 3.HEAD-20141105-r13687

 

 

With options below :

 

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS=' 'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g
-Wall -O2' '--enable-ltdl-convenience'

 

 

I have 16 cores and I have set 6 workes and used aufs cahe dir for bw saving
but still no luck ... the out traffc in general is less than in traffic.

 

This idea is getting me a headache !!!!

 

Here is my cache manager :

Connection information for squid:

        Number of clients accessing cache:    8967

        Number of HTTP requests received:     455542

        Number of ICP messages received:      0

        Number of ICP messages sent:   0

        Number of queued ICP replies:  0

        Number of HTCP messages received:     0

        Number of HTCP messages sent:  0

        Request failure ratio: 0.00

        Average HTTP requests per minute since start: 35706.3

        Average ICP messages per minute since start:  0.0

        Select loop called: 14737492 times, 0.345 ms avg

Cache information for squid:

        Hits as % of all requests:     5min: 10.4%, 60min: 10.6%

        Hits as % of bytes sent:       5min: -0.6%, 60min: -0.8%

        Memory hits as % of hit requests:     5min: 37.1%, 60min: 36.9%

        Disk hits as % of hit requests:       5min: 28.0%, 60min: 28.4%

        Storage Swap size:     29253956 KB

        Storage Swap capacity: 10.6% used, 89.4% free

        Storage Mem size:      2434400 KB

        Storage Mem capacity:  39.6% used, 60.4% free

        Mean Object Size:      32.60 KB

        Requests given to unlinkd:     0

Median Service Times (seconds)  5 min    60 min:

        HTTP Requests (All):   0.15616  0.15748

        Cache Misses:          0.18340  0.19003

        Cache Hits:            0.00030  0.00030

        Near Hits:             0.08938  0.08686

        Not-Modified Replies:  0.00000  0.00000

        DNS Lookups:           0.00000  0.00000

        ICP Queries:           0.00000  0.00000

Resource usage for squid:

        UP Time:       765.486 seconds

        CPU Time:      1333.285 seconds

        CPU Usage:     174.18%

        CPU Usage, 5 minute avg:       176.56%

        CPU Usage, 60 minute avg:      176.15%

        Maximum Resident Size: 22667056 KB

        Page faults with physical i/o: 0

Memory accounted for:

        Total accounted:       1568707 KB

        memPoolAlloc calls:      1830

        memPoolFree calls:  133080611

File descriptor usage for squid:

        Maximum number of file descriptors:   393216

        Largest file desc currently in use:   6574

        Number of file desc currently in use: 23510

        Files queued for open:                   0

        Available number of file descriptors: 369706

        Reserved number of file descriptors:   600

        Store Disk files open:                  37

Internal Data Structures:

        899673 StoreEntries

          2442 StoreEntries with MemObjects

        39600 Hot Object Cache Items

        897283 on-disk objects

 

 

 

And here is iptraf :

 

| Total rates:     752700.3 kbits/sec        Broadcast packets:            0
|

|                   90466.8 packets/sec      Broadcast bytes:              0
|

|
|

| Incoming rates:  378217.1 kbits/sec
|

|                   54377.4 packets/sec
|

|                                            IP checksum errors:           0
|

| Outgoing rates:  375640.1 kbits/sec
|

|                   36089.4 packets/sec
|

|
|

|                                            

 

 

Any help ?????

 

BTW , I used both squid3.4.3 and squid3.head and same issue !!!! why I cant
save bw !!!

 

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/b40beaef/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 14 15:51:17 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 04:51:17 +1300
Subject: [squid-users] using squid 3.head for large rock ,
 but i still have mean object size is 32 !!!!!
In-Reply-To: <000a01d00074$77027340$650759c0$@netstream.ps>
References: <000a01d00074$77027340$650759c0$@netstream.ps>
Message-ID: <546624F5.3010400@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The storedir cache manager report has some more accurate details on
the cache(s). What does it say?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZiT0AAoJELJo5wb/XPRjHloH/3qLSypfU6LU7IdPFITWUjK/
dfpmZl3dsIAxyHuZ3YWujXZe9u1+GrBX9gFlJ9QQV73aesm/44K6qsOkfhUrtVSk
iRf8+nK/5wCeeHZP24Hmeglb+EtGfMypJuuWOCT76SNoJ/dZHxy2du9yenicMHTz
Z13htqW4N84iSnpzOzO53qODTTfEmuXFjHl6pS4mdzvh6kxfl1kYW3VJ7yq/IgQ3
H95tDRh65xzqG4rEiPXaouQGCIz33R2T9lmHq+B3LKZDAD0dCVpV1RLqgvZi24AQ
Wge4ZgnfuIZAJkloZ8Mn1CeUqi+/TZdi6bzxaHgberBuX+O28VcnA4MMHv3niW0=
=765q
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Sat Nov 15 04:14:38 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Fri, 14 Nov 2014 20:14:38 -0800
Subject: [squid-users] using squid 3.head for large rock ,
	but i still have mean object size is 32 !!!!!
In-Reply-To: <01c901d0003f$9768c5b0$c63a5110$@zaeem@netstream.ps>
References: <000a01d00074$77027340$650759c0$@netstream.ps>
 <546624F5.3010400@treenet.co.nz>
 <01c901d0003f$9768c5b0$c63a5110$@zaeem@netstream.ps>
Message-ID: <001901d0008a$ad4bd7a0$07e386e0$@netstream.ps>



Hi Amos :


by kid1 {
Store Directory Statistics:
Store Entries          : 599
Maximum Swap Size      : 0 KB
Current Store Swap Size: 0.00 KB
Current Capacity       : 0.00% used, 0.00% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700672.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146896 76.51%
} by kid1

by kid2 {
Store Directory Statistics:
Store Entries          : 675875
Maximum Swap Size      : 92160000 KB
Current Store Swap Size: 22297020.00 KB
Current Capacity       : 24.19% used, 75.81% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700672.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146896 76.51%

Store Directory #0 (aufs): /ssd1
FS Block Size 4096 Bytes
First level subdirectories: 64
Second level subdirectories: 256
Maximum Size: 92160000 KB
Current Size: 22297020.00 KB
Percent Used: 24.19%
Filemap bits in use: 675307 of 1048576 (64%)
Filesystem Space in use: 22623348/103216920 KB (22%)
Filesystem Inodes in use: 691386/6561792 (11%)
Flags: SELECTED
Removal policy: lru
LRU reference age: 10.48 days
} by kid2

by kid3 {
Store Directory Statistics:
Store Entries          : 482
Maximum Swap Size      : 0 KB
Current Store Swap Size: 0.00 KB
Current Capacity       : 0.00% used, 0.00% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700704.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146897 76.51%
} by kid3

by kid4 {
Store Directory Statistics:
Store Entries          : 581658
Maximum Swap Size      : 92160000 KB
Current Store Swap Size: 19763536.00 KB
Current Capacity       : 21.44% used, 78.56% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700704.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146897 76.51%

Store Directory #0 (aufs): /ssd3
FS Block Size 4096 Bytes
First level subdirectories: 64
Second level subdirectories: 256
Maximum Size: 92160000 KB
Current Size: 19763536.00 KB
Percent Used: 21.44%
Filemap bits in use: 581137 of 1048576 (55%)
Filesystem Space in use: 20082784/103216920 KB (19%)
Filesystem Inodes in use: 597324/6561792 (9%)
Flags: SELECTED
Removal policy: lru
LRU reference age: 6.39 days
} by kid4

by kid5 {
Store Directory Statistics:
Store Entries          : 300
Maximum Swap Size      : 0 KB
Current Store Swap Size: 0.00 KB
Current Capacity       : 0.00% used, 0.00% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700704.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146897 76.51%
} by kid5

by kid6 {
Store Directory Statistics:
Store Entries          : 488014
Maximum Swap Size      : 92160000 KB
Current Store Swap Size: 16140540.00 KB
Current Capacity       : 17.51% used, 82.49% free


Shared Memory Cache
Maximum Size: 6144000 KB
Current Size: 4700704.00 KB 76.51%
Maximum entries:    192000
Current entries: 71708 37.35%
Maximum slots:      192000
Used slots:         146897 76.51%

Store Directory #0 (aufs): /ssd2
FS Block Size 4096 Bytes
First level subdirectories: 64
Second level subdirectories: 256
Maximum Size: 92160000 KB
Current Size: 16140540.00 KB
Percent Used: 17.51%
Filemap bits in use: 487699 of 524288 (93%)
Filesystem Space in use: 16448388/103216920 KB (16%)
Filesystem Inodes in use: 503916/6561792 (8%)
Flags: SELECTED
Removal policy: lru
LRU reference age: 10.48 days
} by kid6




-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, November 14, 2014 5:51 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] using squid 3.head for large rock , but i still have mean object size is 32 !!!!!

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The storedir cache manager report has some more accurate details on
the cache(s). What does it say?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZiT0AAoJELJo5wb/XPRjHloH/3qLSypfU6LU7IdPFITWUjK/
dfpmZl3dsIAxyHuZ3YWujXZe9u1+GrBX9gFlJ9QQV73aesm/44K6qsOkfhUrtVSk
iRf8+nK/5wCeeHZP24Hmeglb+EtGfMypJuuWOCT76SNoJ/dZHxy2du9yenicMHTz
Z13htqW4N84iSnpzOzO53qODTTfEmuXFjHl6pS4mdzvh6kxfl1kYW3VJ7yq/IgQ3
H95tDRh65xzqG4rEiPXaouQGCIz33R2T9lmHq+B3LKZDAD0dCVpV1RLqgvZi24AQ
Wge4ZgnfuIZAJkloZ8Mn1CeUqi+/TZdi6bzxaHgberBuX+O28VcnA4MMHv3niW0=
=765q
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From santosh.pai at vigyanlabs.com  Sat Nov 15 04:21:20 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Fri, 14 Nov 2014 20:21:20 -0800 (PST)
Subject: [squid-users] Removing cache credentials
Message-ID: <1416025280653-4668398.post@n4.nabble.com>

Hello Team,

I have set-up squid proxy server with ldap authentication , the
infrastructure is setup in such a way that users have to access the internet
through the proxy .In Internet explorer there's an option to save the
credntials and once its saved during the prompt squid wont ask for
credentials the user will have direct access to internet . I understand this
is an browser issue is there a way in squid which can prevent the caching of
credentials or give a timeout so that the user is prompted to reauthenticate
from squid . I have searched on web all i get is help on disabling the
credential cache of windows .

Any help or suggestions to solve this problem would be great .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Removing-cache-credentials-tp4668398.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Nov 15 05:28:22 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 18:28:22 +1300
Subject: [squid-users] Removing cache credentials
In-Reply-To: <1416025280653-4668398.post@n4.nabble.com>
References: <1416025280653-4668398.post@n4.nabble.com>
Message-ID: <5466E476.8030800@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 5:21 p.m., santosh wrote:
> Hello Team,
> 
> I have set-up squid proxy server with ldap authentication , the 
> infrastructure is setup in such a way that users have to access the
> internet through the proxy .In Internet explorer there's an option
> to save the credntials and once its saved during the prompt squid
> wont ask for credentials the user will have direct access to
> internet .

Wrong and wrong.
HTTP (thus Squid) is stateless. Each and every single request requires
the credentials necessary to pass that request through the proxy.

Whenever the credentials are missing or invalid Squid WILL ask. The
browser and only the browser can decide what credentials (if any) are
sent on the request.

> I understand this is an browser issue is there a way in squid which
> can prevent the caching of credentials

Do you have a time machine? The browser actions happen long before
Squid ever hears about the credentials for the first time.

> or give a timeout so that the user is prompted to reauthenticate 
> from squid . I have searched on web all i get is help on disabling
> the credential cache of windows .

Re-authenticate is another matter. The only way to do this is to tell
the browser the credentials it is sending are invalid.

Timeout is one way to trigger re-authentication. Validity TTL is the
business of the background authentication system you are using to
check the credentials validity (the auth helper).


So why are you trying to force frustration and annoyance on your users?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZuR2AAoJELJo5wb/XPRj9fUH/RQBEu/whtwjCWuQHjujyREo
HXWjrKKgVM/yXccgYhozJryCuFbErJgQLHit8IoKOFFO1/e6tWSq0xc5dB+KUtag
KY1Ugg5AxveVJmGKak/QV4h0+JHL5s8xX4pjEYRk1EY/H7ScIob2/LtphHQMEUWv
ikSy7rEGlGF9mgOjF4mIZDPwqhKCssgdu0hA9YNShHjX2o+jqlaQVV/J1A1qI/BR
jbmKP+QH0Bf1h/zIDe1CAOhqSAZFiAEJ+MidxevPwQzA6ovgFH+4yJbZNnt+cvVt
MHzS/Wbut1o/UWgJ0CrXtrhNkPJRByRBKxo3e5IVaYaDOJZ/ok4Pc6qwKR2skhc=
=AYUH
-----END PGP SIGNATURE-----


From sudakov at sibptus.tomsk.ru  Sat Nov 15 06:33:44 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Sat, 15 Nov 2014 12:33:44 +0600
Subject: [squid-users] Removing cache credentials
In-Reply-To: <5466E476.8030800@treenet.co.nz>
References: <1416025280653-4668398.post@n4.nabble.com>
 <5466E476.8030800@treenet.co.nz>
Message-ID: <20141115063344.GA32943@admin.sibptus.tomsk.ru>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Amos Jeffries wrote:
> > 
> > I have set-up squid proxy server with ldap authentication , the 
> > infrastructure is setup in such a way that users have to access the
> > internet through the proxy .In Internet explorer there's an option
> > to save the credntials and once its saved during the prompt squid
> > wont ask for credentials the user will have direct access to
> > internet .
> 
> Wrong and wrong.
> HTTP (thus Squid) is stateless. Each and every single request requires
> the credentials necessary to pass that request through the proxy.

Once you mentioned it, I have a question. 

If we speak about Kerberos authentication. On the very first request,
the browser receives a "407 Proxy Authentication Required" reply and
learns that it is expected to provide credentials. For a certain amount
of time, the browser knows that it should send the credentials with
every request without waiting for an 407 reply.

How long is this amount of time? Is it like forever?  Is there ever a
limit after which the browser will try again to send a request without
credentials? Maybe after a browser restart or what?

- -- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUZvPIAAoJEA2k8lmbXsY0qn8IAIoxZnizrSzrx6mozpqDo64n
zjwuYNPO7RmEcXWJx8/YBYH1/dPMJ+ya5oHq588jOij0zOmxKu2NcA7h7RvBGRxt
2L9CJWLG+9iQrndoPZwghG5/f3AhTz8sq9QuOUB2Cw2+DCFi2VWYTFhlcvK9INsl
AngytS3e8xlrdnQ9S8VGA5bayG7Qt3eoVorSc0GJ+tb+fHM5rs1TfdJExr4ALLeX
UQkOuIKFlyzcS4wfRIcmjHxW3Fce7+E8tZQnoyic8ZFOf5GZspBRi1BLRJ31cDuJ
k/VB7erdd8VhDrydB2p0IJsaeE9hNGyH1vgecBU39wAZg4uU1HzQL3ThC/CAWAA=
=APpH
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sat Nov 15 06:47:42 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 19:47:42 +1300
Subject: [squid-users] Removing cache credentials
In-Reply-To: <20141115063344.GA32943@admin.sibptus.tomsk.ru>
References: <1416025280653-4668398.post@n4.nabble.com>
 <5466E476.8030800@treenet.co.nz>
 <20141115063344.GA32943@admin.sibptus.tomsk.ru>
Message-ID: <5466F70E.2020204@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 7:33 p.m., Victor Sudakov wrote:
> Amos Jeffries wrote:
>>> 
>>> I have set-up squid proxy server with ldap authentication , the
>>>  infrastructure is setup in such a way that users have to
>>> access the internet through the proxy .In Internet explorer
>>> there's an option to save the credntials and once its saved
>>> during the prompt squid wont ask for credentials the user will
>>> have direct access to internet .
> 
>> Wrong and wrong. HTTP (thus Squid) is stateless. Each and every
>> single request requires the credentials necessary to pass that
>> request through the proxy.
> 
> Once you mentioned it, I have a question.
> 
> If we speak about Kerberos authentication. On the very first
> request, the browser receives a "407 Proxy Authentication Required"
> reply and learns that it is expected to provide credentials. For a
> certain amount of time, the browser knows that it should send the
> credentials with every request without waiting for an 407 reply.
> 
> How long is this amount of time? Is it like forever?  Is there ever
> a limit after which the browser will try again to send a request
> without credentials? Maybe after a browser restart or what?
> 

Negotiate/Kerberos (and NTLM) do not authenticate the request. They
abuse HTTP to authenticate the TCP connection underneath HTTP. So the
credentials must be re-used for the entire lifetime of that TCP
connection. Changing credentials means tearing down that whole TCP
connection.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZvcOAAoJELJo5wb/XPRjuPwIANbYcqUEBtwt5MmMr0Rc5oM9
o9DW6e+Blm5hMClwa8i31zBg6pcww0/ixEb4DwwBgBr+NcCPr4jP/dHMZQ0vh+rx
IOH2n7LGZwQ6phaltIavYFQouqJjUL0gtFRpoYjClobm8coi/jxv/3qZMwfrGB53
/A9l8cmBs7v7C5vzEKLLlpTZQ85wYtc+qC8i1W1FVK8jcpypd5ql8xSbodMumtUH
vItOJdKRZFseOZc6rk9EJG24VZluRD7rmab4XQWQdbL/eVabXDDIqQq2agaf7DTZ
8F9bSEuqjAoSnsf/gl5RGdWNUN1h5tTWO/DYvyn1MI5vYEhExGeW1YrsF2sWPpA=
=DvPW
-----END PGP SIGNATURE-----


From santosh.pai at vigyanlabs.com  Sat Nov 15 07:40:39 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Fri, 14 Nov 2014 23:40:39 -0800 (PST)
Subject: [squid-users] Removing cache credentials
In-Reply-To: <1416025280653-4668398.post@n4.nabble.com>
References: <1416025280653-4668398.post@n4.nabble.com>
Message-ID: <1416037239007-4668402.post@n4.nabble.com>

Hello Amos,

You mentioned that "Timeout is one way to trigger re-authentication" , is
there an configuration directive in squid.conf to do that .If its there let
me know so that i can see it for experimental purpose .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Removing-cache-credentials-tp4668398p4668402.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From mikes at kuentos.guam.net  Sat Nov 15 08:03:25 2014
From: mikes at kuentos.guam.net (Michael D. Setzer II)
Date: Sat, 15 Nov 2014 18:03:25 +1000
Subject: [squid-users] Question on setting up squid with Parent that is
	off-site?
Message-ID: <546708CD.13314.BD78F@mikes.kuentos.guam.net>

I was showing my students the process of using a squid server and building 
linux kernels. Have done this in the past, but today it didn't work the same.
The squid server is setup differently, so there may be something I missed in 
setting up the parent that caused the difference.

In the past, it was just a local squid server, and would cache on the same 
1000M network of my classroom. 192.168.7.0 with squid server on 
192.168.7.3. But had changed the configuration to use one of our ISPs proxy 
server as a parent due to geographic issues. When doing updates using our 
local college IP addresses, we would connect to Austrial and/or New Zealand 
repos that are really slow from Guam. The one ISP has a squid server at 
proxy.guam.net and allows us to use it, and it has a US based address, so it 
connects to US sites which are much faster. 

Normally, I would download the file on a machine locally, so it was in the 
local squid servers cache, but now that it uses the ISPs server as a parent, it 
didn't seem to cache the file on the local server, but only on the ISPs server. 
Instead of downloading the file via the local server via 1000M connection, it 
seemed to be downloading multiple copies through the 20M connection to 
the ISP taking about 30 minutes with about 10 machines downloading the 
70+M 3.17.3 kernel file from kernel.org?

Is there an option that would have the local squid server cache a file locally 
thru the parent so other request would come from the local squid server.

Thanks.

+----------------------------------------------------------+
  Michael D. Setzer II -  Computer Science Instructor      
  Guam Community College  Computer Center                  
  mailto:mikes at kuentos.guam.net                            
  mailto:msetzerii at gmail.com
  http://www.guam.net/home/mikes
  Guam - Where America's Day Begins                        
  G4L Disk Imaging Project maintainer 
  http://sourceforge.net/projects/g4l/
+----------------------------------------------------------+

http://setiathome.berkeley.edu (Original)
Number of Seti Units Returned:  19,471
Processing time:  32 years, 290 days, 12 hours, 58 minutes
(Total Hours: 287,489)

BOINC at HOME CREDITS
ROSETTA     21741707.098117   |   SETI        37660772.805449
ABC         16613838.513356   |   EINSTEIN    40346326.419899



From squid3 at treenet.co.nz  Sat Nov 15 08:27:02 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 21:27:02 +1300
Subject: [squid-users] Removing cache credentials
In-Reply-To: <1416037239007-4668402.post@n4.nabble.com>
References: <1416025280653-4668398.post@n4.nabble.com>
 <1416037239007-4668402.post@n4.nabble.com>
Message-ID: <54670E56.1010906@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 8:40 p.m., santosh wrote:
> Hello Amos,
> 
> You mentioned that "Timeout is one way to trigger
> re-authentication" , is there an configuration directive in
> squid.conf to do that .If its there let me know so that i can see
> it for experimental purpose .

Credential validity is determined by your backend authentication
system. You need to find a way for it to inform Squid that credentials
are invalid after the timeout period.

Once that is setup, the auth_param credentialsttl in Squid needs to be
a short dividend of the backend TTL value.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZw5WAAoJELJo5wb/XPRj46cH+gOQZlLk3Y2NOzGY9oyHVVv7
MLcrKNjEz9SRNI2u5r6OL4otHq8ecqiS4oTq0fkjomv26lgnjH35kbzxB1TY5g0n
Sfu4k963mC1yQc+9z2rSi7HggIFbfhx4KF6YGPqh/T5ian4+8N2XVu6A5tup6GQ9
9n/00YaO3uVVjaQQaFBl2gnLvMF6PVUpKDWAl7kyFoF+nV+RXFa2eS5OQCs95Ckg
vZwtiGdFZLA0HqdARENGLyfiHBll7yFqKQAViWzlrCK+tRW3iEv/6mMXalgQ+uPw
9ImLifqHi3+UOxNtWlf2PnlQjgQ4QFgsh1Ek9Dw3/fu3rYt/aCYlrG8hFX1Svq8=
=I/yK
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sat Nov 15 08:32:53 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Nov 2014 21:32:53 +1300
Subject: [squid-users] Question on setting up squid with Parent that is
 off-site?
In-Reply-To: <546708CD.13314.BD78F@mikes.kuentos.guam.net>
References: <546708CD.13314.BD78F@mikes.kuentos.guam.net>
Message-ID: <54670FB5.9020501@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 15/11/2014 9:03 p.m., Michael D. Setzer II wrote:
> Normally, I would download the file on a machine locally, so it was
> in the local squid servers cache, but now that it uses the ISPs
> server as a parent, it didn't seem to cache the file on the local
> server, but only on the ISPs server. Instead of downloading the
> file via the local server via 1000M connection, it seemed to be
> downloading multiple copies through the 20M connection to the ISP
> taking about 30 minutes with about 10 machines downloading the 70+M
> 3.17.3 kernel file from kernel.org?
> 

Run the URL through the tool at http://redbot.org/ to see if there are
any cacheability problems with the response(s).

> Is there an option that would have the local squid server cache a
> file locally thru the parent so other request would come from the
> local squid server.

If your Squid version is new enough use "collapsed_forwarding on".

And DO NOT use the "proxy-only" option on your upstream cache_peer
directive. It will force the local proxy never to cache anything
arriving from that peer.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZw+1AAoJELJo5wb/XPRjzYkH/iCU47EF33oz3VfqjKYM1kN7
biBtlcnzFNmv2ulZg1A44nhqT7RDVCbxM3GFNyUIYq4hvWNWCxH6RjbFRr4yA2Om
3WVLapKKqIJ1NGTjpFDzrb8L5Uhz7Lw7ICSp5SFrdO/G5331vFoLAb7mixchme/U
O4B6v3P1Q4UT4cAA6Y0Ck9PP1cRUwnhtUiDqmyhueLs+2/OkBrv0u64hUx5SC8XL
JwAQEVSfvoX4w64iWBEM5MH99c74XH7zscLa1ixxxyfy+P6vAJ6W4Mq/Dqw8bnFb
a6DA5VJa9Cv+nXoaKPRTDWMazfxwk5zTKuWd8uukAlTuExR5fZzU2LWIv4eHKk8=
=c5NH
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Sun Nov 16 07:07:45 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sat, 15 Nov 2014 23:07:45 -0800
Subject: [squid-users] squid cache Large rock with aufs optimization for
	bandwidth saving
Message-ID: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>

Hi developers .

 

Im using squid 
Squid Cache: Version 3.HEAD-20141105-r13687

 

 

With options :

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS=' 'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g
-Wall -O2' '--enable-ltdl-convenience'

 

 

and my squid.conf is :

 

 

 

 

# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl mysubnet src xxxxxxxxxxxxx

acl xxxxxxxxxx

acl localip     src     xxxxxxx

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

######################################################

# Recommended minimum Access Permission configuration:

# Deny requests to certain unsafe ports

##############################################################

http_access allow xxx

http_access allow manager xxx

http_access deny !Safe_ports

http_access allow localnet

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

#######################################################

###########SMP oPTIONS#####################

dns_v4_first on 

workers 6

cache_mem 6000 MB

maximum_object_size 4 MB

maximum_object_size_in_memory 4 MB

minimum_object_size 50 KB

########################################################

#cpu_affinity_map process_numbers=5,6,7 cores=1,2,7

http_port 127.0.0.1:400${process_number}

if ${process_number} = 2 

###################################

cache_mem 25000 MB

maximum_object_size 4 MB

maximum_object_size_in_memory 4 MB

minimum_object_size 50 KB

###################################

cache_dir aufs /ssd1 90000 64 256

cache_log stdio:/var/log/squid/cache5.log

access_log /var/log/squid/access5.log  

endif 

####################

####################

if ${process_number} = 6

###################################

cache_mem 6000 MB

maximum_object_size 4 MB

maximum_object_size_in_memory 4 MB

minimum_object_size 50 KB

################################### 

cache_dir aufs /ssd2 90000 64 256

cache_log stdio:/var/log/squid/cache6.log

access_log /var/log/squid/access6.log

endif 

#################

#################

if ${process_number} = 4

###################################

 

###################################

cache_dir aufs /ssd3 90000 64 256

cache_log stdio:/var/log/squid/cache7.log

access_log /var/log/squid/access7.log

endif 

#######################################

if ${process_number} = 5

###################################

cache_mem 6000 MB

maximum_object_size 4 MB

maximum_object_size_in_memory 4 MB

minimum_object_size 50 KB

###################################

cache_dir aufs /ssd4 40000 64 256

cache_log stdio:/var/log/squid/cache8.log

access_log /var/log/squid/access8.log

endif

#########################################

visible_hostname Largerock-squid

####################################################

####Filterring##############

acl blockkeywords dstdomain "/etc/squid/koko.acl"

http_access deny blockkeywords

#################################################

##Regix filtering##########

acl xacl dstdom_regex "/etc/squid/squid-regex.acl"

http_access deny xacl

##############################

cache_log /var/log/squid/cache.log

access_log /var/log/squid/access.log 

############################################### 

http_port xxxxx

http_port xxxxxxx tproxy

###

http_access allow mysubnet

# add user authentication and similar options here

http_access allow manager localhost

http_access allow manager localip

http_access allow manager mysubnet

http_access allow mysubnet manager

http_access deny manager

##########################

cache_mem 6000 MB

maximum_object_size 4 MB

maximum_object_size_in_memory 4 MB

minimum_object_size 0 KB

cache_swap_low 90

cache_swap_high 95

###########################################################

#########################################################

quick_abort_pct 95

fqdncache_size 65535

ipcache_size 65535

###########################################################

ipcache_low 98

ipcache_high 99

#####################################################

########### WCCP2 Config#############

wccp2_router xxxxxx

wccp2_rebuild_wait off   

wccp_version 2

wccp2_forwarding_method 2

wccp2_return_method 2

wccp2_assignment_method 2

wccp2_service dynamic 80

wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=250 ports=80

wccp2_service dynamic 90

wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
priority=250 ports=80

######################################################

cache_effective_user squid

cache_effective_group squid

#######################################################

memory_replacement_policy heap GDSF

cache_replacement_policy heap LFUDA

#####################################################

dns_nameservers x.x.x.x

####################################################### 

http_access deny all

##############################

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

######

memory_pools off 

pconn_timeout 2 minutes

persistent_request_timeout 1 minute 

read_ahead_gap 128 KB

 

 

[root at Largerock-squid ~]# df -h

Filesystem      Size  Used Avail Use% Mounted on

/dev/sda1        50G   13G   35G  27% /

tmpfs           100G  5.2G   95G   6% /dev/shm

/dev/sdb1        99G   82G   12G  88% /ssd1

/dev/sdc1        99G   56G   38G  60% /ssd2

/dev/sdd1        99G   75G   19G  81% /ssd3

/dev/sde1        45G   28G   15G  67% /ssd4

 

 

But still wondering , why after im using squid3.head that support large rock
and using aufs with 4 processes with object size about 4  M configured in
each prcess .the mean size is about 38 KB ???

 

This is my biggest problem

 

I could load the traffic which is about 1000 req/second among my cores
equally   ..but I cant save a lot of BW ???!!

 

Here is cache mamager info :

Cache Manager menu
<http://176.58.67.242:9090/cgi-bin/cachemgr.cgi?host=176.58.67.242&port=6500
0&user_name=a at a&operation=menu&auth=MTc2LjU4LjY3LjI0MnwxNDE2MDg1MzcwfGFAYXxh
> 

  _____  


  _____  

 
Squid Object Cache: Version 3.HEAD-20141105-r13687
Build Info: 
Service Name: squid

Start Time:

Sat, 15 Nov 2014 06:47:15 GMT


Current Time:

Sat, 15 Nov 2014 21:02:50 GMT

 
Connection information for squid:
        Number of clients accessing cache:    29344
        Number of HTTP requests received:     36422429
        Number of ICP messages received:      0
        Number of ICP messages sent:   0
        Number of queued ICP replies:  0
        Number of HTCP messages received:     0
        Number of HTCP messages sent:  0
        Request failure ratio:  0.00
        Average HTTP requests per minute since start: 42570.5
        Average ICP messages per minute since start:  0.0
        Select loop called: 1117931059 times, 0.303 ms avg
Cache information for squid:
        Hits as % of all requests:     5min: 15.8%, 60min: 16.0%
        Hits as % of bytes sent:       5min: 3.0%, 60min: 2.6%
        Memory hits as % of hit requests:     5min: 27.1%, 60min: 28.2%
        Disk hits as % of hit requests:       5min: 39.5%, 60min: 40.2%
        Storage Swap size:     250049956 KB
        Storage Swap capacity: 78.8% used, 21.2% free
        Storage Mem size:      4697152 KB
        Storage Mem capacity:  76.5% used, 23.5% free
        Mean Object Size:      38.16 KB
        Requests given to unlinkd:     0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.18416  0.17590
        Cache Misses:          0.25616  0.24208
        Cache Hits:            0.00061  0.00061
        Near Hits:             0.07278  0.08295
        Not-Modified Replies:  0.00015  0.00030
        DNS Lookups:           0.00000  0.00000
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:       51334.692 seconds
        CPU Time:      95558.407 seconds
        CPU Usage:     186.15%
        CPU Usage, 5 minute avg:       269.60%
        CPU Usage, 60 minute avg:      270.21%
        Maximum Resident Size: 145364816 KB
        Page faults with physical i/o: 5
Memory accounted for:
        Total accounted:       3392882 KB
        memPoolAlloc calls:      1736
        memPoolFree calls:  10645847682
File descriptor usage for squid:
        Maximum number of file descriptors:   786432
        Largest file desc currently in use:   8453
        Number of file desc currently in use: 38409
        Files queued for open:                   0
        Available number of file descriptors: 748023
        Reserved number of file descriptors:   600
        Store Disk files open:                 124
Internal Data Structures:
        6556333 StoreEntries
          3738 StoreEntries with MemObjects
         73065 Hot Object Cache Items
        6552722 on-disk objects

 

 

Any help ??????

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141115/7bc6a6a4/attachment.htm>

From squid3 at treenet.co.nz  Sun Nov 16 00:11:23 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 16 Nov 2014 13:11:23 +1300
Subject: [squid-users] squid cache Large rock with aufs optimization for
 bandwidth saving
In-Reply-To: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
References: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
Message-ID: <5467EBAB.7040206@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 16/11/2014 8:07 p.m., Ahmed Allzaeem wrote:
> Hi developers .
> 

The developers are not here. This is the user mailing list.

...
> But still wondering , why after im using squid3.head that support
> large rock and using aufs with 4 processes with object size about 4
> M configured in each prcess .the mean size is about 38 KB ???


1) while your Squid supports large objects in rock stores you are not
actually using that. So your Squid workers are still stuck with only
the memory cache shared, and disk caches separate.

2) you have several different cache_mem values spread across the
workers. Only the last cache_mem matters so you have only 6GB used by
any worker. Not the 25GB you may think some are using.

3) you are doign similar things with minimum_object_size. But again
oly the last configured value matters. So its 0 for all workers.


The mean object size is just an average of all the cacheable traffic
your proxy is processing. Bandwidth savings is more closely related to
*total* cacheable traffic than an average of individual object sizes.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZ+uqAAoJELJo5wb/XPRjzZ4H/3wMteFcHint7RRowwZvtR5x
lwo0Z/u2DsWAfF+lXmvH/PpDhyzEKDzBJh9tVZR1xB5fQ3bT9ydhQrnkoXClq8mC
lqYwmMJy57JG4YGCZnaw8XXt1D248r+RywlNzPXNukyNDTGFBDWkiTBuT2b+Jlxn
ofwj0mjFMuiyxC7Oe7o/f3uqOD+5lpS9spBX8DD8weQzcfKFb/7IrZnb9Hpv9FBY
LhMTWFu8sOBndyaYedelLcUriUL4kthg59sqwp68oDg2SLBUv//UdX5If9RAWm4G
AAYacYMrsOn+OX8wEjscwinY3TNhH+QjAAb1oOZKSfJEtDbIvLp9I2qprDi34o8=
=bgGV
-----END PGP SIGNATURE-----


From james at ejbdigital.com.au  Sun Nov 16 01:12:02 2014
From: james at ejbdigital.com.au (James Harper)
Date: Sun, 16 Nov 2014 01:12:02 +0000
Subject: [squid-users] ssl callout helper
Message-ID: <07a61e8144ec4e75888e8ce16dcc8c2e@HKNPR04MB0388.apcprd04.prod.outlook.com>

I've written a little helper to do ssl callouts to determine if the server is running ssl at all (eg not tunnelling over ssl), and also to be able to do limited ACL on CN/SAN. The main limitation is the way larger organisations will often have one SSL cert that covers many URLS (eg google cert also covers google.com, youtube.com etc).

Currently I need to do it like:

external_acl_type cert_callout %DST %PORT /usr/local/squid/libexec/ext_cert_callout_acl

acl banks dstdomain .bigbank.com
acl banks dstdomain .otherbank.com

acl is_ssl external cert_callout IS_SSL
acl banks_callout external cert_callout SAN .bigbank.com
acl banks_callout external cert_callout SAN .otherbank.com

ssl_bump splice !is_ssl
ssl_bump splice banks
ssl_bump splice banks_callout
ssl_bump bump all

But I'd rather not have to maintain the banks and the banks_callout lists separately when they are identical. Apart from sticking them in a separate file, are there any shortcuts I can take?

Also, it would be good if squid could make use of the CN from the certificate for logging, so instead of "CONNECT <IP>:<PORT>", I could log "CONNECT <CN>:<PORT>", which would really clean up the logs (apart from the cases mentioned above). I think I can use tag= or log=, but that would preclude me from using them for anything else (I'm not using them for anything else at the moment but still...)

Thanks

James


From ahmed.zaeem at netstream.ps  Sun Nov 16 13:24:38 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 16 Nov 2014 05:24:38 -0800
Subject: [squid-users] squid cache Large rock with aufs optimization for
	bandwidth saving
In-Reply-To: <5467EBAB.7040206@treenet.co.nz>
References: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
 <5467EBAB.7040206@treenet.co.nz>
Message-ID: <002801d001a0$acfe81a0$06fb84e0$@netstream.ps>

Hi Amos , thank you

Really sorry to ask you  , wts needed to be done to use largerock support ?

I mean wt wring im doing ?

Also about the cace_mem directive , as I understood this value  per worker or per process.
And here im not using rock , so its not shared , I have about 32 Gram total.


Can you correct nme about setting cache_mem  directive ? also guide me wts needed to enable caching large object size like 512k - 1 M or even 2 M


Thanks 

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, November 15, 2014 4:11 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid cache Large rock with aufs optimization for bandwidth saving

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 16/11/2014 8:07 p.m., Ahmed Allzaeem wrote:
> Hi developers .
> 

The developers are not here. This is the user mailing list.

...
> But still wondering , why after im using squid3.head that support 
> large rock and using aufs with 4 processes with object size about 4 M 
> configured in each prcess .the mean size is about 38 KB ???


1) while your Squid supports large objects in rock stores you are not actually using that. So your Squid workers are still stuck with only the memory cache shared, and disk caches separate.

2) you have several different cache_mem values spread across the workers. Only the last cache_mem matters so you have only 6GB used by any worker. Not the 25GB you may think some are using.

3) you are doign similar things with minimum_object_size. But again oly the last configured value matters. So its 0 for all workers.


The mean object size is just an average of all the cacheable traffic your proxy is processing. Bandwidth savings is more closely related to
*total* cacheable traffic than an average of individual object sizes.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUZ+uqAAoJELJo5wb/XPRjzZ4H/3wMteFcHint7RRowwZvtR5x
lwo0Z/u2DsWAfF+lXmvH/PpDhyzEKDzBJh9tVZR1xB5fQ3bT9ydhQrnkoXClq8mC
lqYwmMJy57JG4YGCZnaw8XXt1D248r+RywlNzPXNukyNDTGFBDWkiTBuT2b+Jlxn
ofwj0mjFMuiyxC7Oe7o/f3uqOD+5lpS9spBX8DD8weQzcfKFb/7IrZnb9Hpv9FBY
LhMTWFu8sOBndyaYedelLcUriUL4kthg59sqwp68oDg2SLBUv//UdX5If9RAWm4G
AAYacYMrsOn+OX8wEjscwinY3TNhH+QjAAb1oOZKSfJEtDbIvLp9I2qprDi34o8=
=bgGV
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Nov 16 06:06:39 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 16 Nov 2014 19:06:39 +1300
Subject: [squid-users] squid cache Large rock with aufs optimization for
 bandwidth saving
In-Reply-To: <002801d001a0$acfe81a0$06fb84e0$@netstream.ps>
References: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
 <5467EBAB.7040206@treenet.co.nz>
 <002801d001a0$acfe81a0$06fb84e0$@netstream.ps>
Message-ID: <54683EEF.5090701@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 2:24 a.m., Ahmed Allzaeem wrote:
> Hi Amos , thank you
> 
> Really sorry to ask you  , wts needed to be done to use largerock
> support ?

Example 5GB rock cache;

 cache_dir rock /ssd1 5000

> 
> I mean wt wring im doing ?
> 
> Also about the cace_mem directive , as I understood this value  per
> worker or per process. And here im not using rock , so its not
> shared , I have about 32 Gram total.

http://www.squid-cache.org/Doc/config/memory_cache_shared/

You are using workers, so it is shared unless you configure:
  memory_cache_shared off

Note this: "Currently, entities exceeding 32KB in size cannot be shared.".
 I am not sure if that was fixed when large-rock support was added.
Should have been, but maybe not.


For most of the per-worker settings you have are undone by the global
settings at the end of the config between http_port and WCCP config.


> 
> Can you correct nme about setting cache_mem  directive ? also guide
> me wts needed to enable caching large object size like 512k - 1 M
> or even 2 M
> 

Try "memory_cache_shared off" at the top of your config.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaD7vAAoJELJo5wb/XPRjDfYH/1SsTIG4RUrU5NXB7C4Vojva
iZdt9hPLW7QTkUWjjB3tr6Y84I4DRCd9/N6h1LBlDY3lNmjhsYQEwCEUmm6Dm2Uo
hj1P1Nvn6cBMOjDZswFrz3imo37UAnvIUAqqwMC2SJF3uglvgtrd8IdeFVPRIoSR
bgjsiZB1yAx+SBCEOPs2sEhpqdi0215FeiF/ujTS5A7JeEPS+jPVcZoYT/xRoe9L
oDXkITcfBlOa/0bY1lo5LAqPHmVDh5BZxVn8ToMMvki21J9UscRdvT04JVoc6iqP
Uouo9VxZJDJfxF0BtK4wJSG1nKtcTiFYrBqK862L2c7zwYPB9WIPpUflKRx/aFQ=
=82cL
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Mon Nov 17 00:26:35 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Sun, 16 Nov 2014 16:26:35 -0800
Subject: [squid-users] squid cache Large rock with aufs optimization for
	bandwidth saving
In-Reply-To: <54683EEF.5090701@treenet.co.nz>
References: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
 <5467EBAB.7040206@treenet.co.nz>
 <002801d001a0$acfe81a0$06fb84e0$@netstream.ps>
 <54683EEF.5090701@treenet.co.nz>
Message-ID: <002d01d001fd$26473960$72d5ac20$@netstream.ps>

Hi Amos , thanks for reply.

But I think using large rock will let the memory cache size be >32 ??? am  I correct ?

I mean although im not using rock store , but I was thinking will have memory size >32Kb

So , I keep the aufs size which I assume will help me and will let me break the 32kB limitation .. but it seems not like I was planning !!!!

So m my previous config was using aufs with large rock needed in memory size which is not what I was planning ?!!!! can you plz explain me why my previous config don?t have luck with memory >> 32KB??




So for now , as u told me I removed all aufs drivers and added only rock dir as below :

cache_dir rock /ssd1 90000 max-size=32768 swap-timeout=350 max-swap-rate=350
cache_dir rock /ssd2 90000 max-size=32768 swap-timeout=350 max-swap-rate=350
cache_dir rock /ssd3 90000 max-size=32768 swap-timeout=350 max-swap-rate=350 
cache_dir rock /ssd4 30000 max-size=32768 swap-timeout=350 max-swap-rate=350  


And only mean size I can see now is 18 Kb ??!!!!

Why ???



I had tried another trial , I put memory_cache_shared off , squid cant be started all all kids is being killed and stared again  and so on !

Sample of logs :
FATAL: Received Segment Violation...dying.
2014/11/16 09:18:28 kid1| Closing HTTP port xxxx
2014/11/16 09:18:28 kid1| Closing HTTP port xxxxxx
2014/11/16 09:18:28 kid1| storeDirWriteCleanLogs: Starting...
2014/11/16 09:18:28 kid1|   Finished.  Wrote 0 entries.
2014/11/16 09:18:28 kid1| storeDirWriteCleanLogs: Starting...
2014/11/16 09:18:28 kid1|   Finished.  Wrote 0 entries.
2014/11/16 09:18:28 kid1|   Took 0.00 seconds (  0.00 entries/sec).
CPU Usage: 8.235 seconds = 8.049 user + 0.186 sys
Maximum Resident Size: 381440 KB
Page faults with physical i/o: 1
2014/11/16 09:18:30 kid5| Current Directory is /root
2014/11/16 09:18:30 kid5| Starting Squid Cache version 3.HEAD-20141105-r13687 for x86_64-unknown-linux-gnu...
2014/11/16 09:18:30 kid5| Service Name: squid
2014/11/16 09:18:30 kid5| Process ID 2925
2014/11/16 09:18:30 kid5| Process Roles: worker
2014/11/16 09:18:30 kid5| Process Roles: worker
2014/11/16 09:18:30 kid5| With 131072 file descriptors available
2014/11/16 09:18:30 kid5| With 131072 file descriptors available
2014/11/16 09:18:30 kid5| Initializing IP Cache...
2014/11/16 09:18:30 kid5| Initializing IP Cache...
2014/11/16 09:18:30 kid5| DNS Socket created at [::], FD 10
2014/11/16 09:18:30 kid5| DNS Socket created at 0.0.0.0, FD 11



I wish you help me with a good squid.conf file that can help me in saving bw as I can !

regards


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Saturday, November 15, 2014 10:07 PM
To: Ahmed Allzaeem
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid cache Large rock with aufs optimization for bandwidth saving

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 2:24 a.m., Ahmed Allzaeem wrote:
> Hi Amos , thank you
> 
> Really sorry to ask you  , wts needed to be done to use largerock 
> support ?

Example 5GB rock cache;

 cache_dir rock /ssd1 5000

> 
> I mean wt wring im doing ?
> 
> Also about the cace_mem directive , as I understood this value  per 
> worker or per process. And here im not using rock , so its not shared 
> , I have about 32 Gram total.

http://www.squid-cache.org/Doc/config/memory_cache_shared/

You are using workers, so it is shared unless you configure:
  memory_cache_shared off

Note this: "Currently, entities exceeding 32KB in size cannot be shared.".
 I am not sure if that was fixed when large-rock support was added.
Should have been, but maybe not.


For most of the per-worker settings you have are undone by the global settings at the end of the config between http_port and WCCP config.


> 
> Can you correct nme about setting cache_mem  directive ? also guide me 
> wts needed to enable caching large object size like 512k - 1 M or even 
> 2 M
> 

Try "memory_cache_shared off" at the top of your config.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaD7vAAoJELJo5wb/XPRjDfYH/1SsTIG4RUrU5NXB7C4Vojva
iZdt9hPLW7QTkUWjjB3tr6Y84I4DRCd9/N6h1LBlDY3lNmjhsYQEwCEUmm6Dm2Uo
hj1P1Nvn6cBMOjDZswFrz3imo37UAnvIUAqqwMC2SJF3uglvgtrd8IdeFVPRIoSR
bgjsiZB1yAx+SBCEOPs2sEhpqdi0215FeiF/ujTS5A7JeEPS+jPVcZoYT/xRoe9L
oDXkITcfBlOa/0bY1lo5LAqPHmVDh5BZxVn8ToMMvki21J9UscRdvT04JVoc6iqP
Uouo9VxZJDJfxF0BtK4wJSG1nKtcTiFYrBqK862L2c7zwYPB9WIPpUflKRx/aFQ=
=82cL
-----END PGP SIGNATURE-----



From alberto.furia at gmail.com  Sun Nov 16 15:54:32 2014
From: alberto.furia at gmail.com (alberto)
Date: Sun, 16 Nov 2014 16:54:32 +0100
Subject: [squid-users] Centralized Squid - design and implementation
Message-ID: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>

Hello everyone,
first of all thanks to the community of squid for such a great job.

I'm writing because I have to revise the current implementation of squid in
my company so I would like to share with you some design ideas and possibly
have some suggestions from you.

The group I work for has six offices/branches in six different italian
cities and the networking infrastructure is based on a "hub and spoke"
paradgima (ie https://www.checkpoint.com/products/vpn-1_power/images/vpn
-1_pro_oneclick_star.gif) where every branch, the spoke, is part of the
main datacenter, the hub.
Now the cache/forward proxy runs - for each branch - inside the branch
office on a pair of squid nodes balanced by wpad/javascript on ip address
base (even ip/odd ip).

For obvious reasons of maintenance and for other technical reasons we
intend to move the proxy navigation centralizing it to the datacenter hub
on a couple (how many?!) of squid nodes.

I have some questions that I would like to share with you:

1. I would like to leave the solution we are using now (wpad balancing). In
a situation like the one I have described, centralized squid serving the
spokes/branches, which is the best solution for clustering/HA? If one of
the centralized nodes had to "die" I would like client machines not to
remain "hanging" but to continue working on an active node without
disruption. A hierarchy of proxy would be the solution?

2. Bearing in mind that all users will be AD authenticated, which url
filtering/blacklist solution do you suggest?
In the past I have worked a lot with squidguard and dansguardian but now
they don't seem to be the state of the art anymore.
I've been thinking about two different solutions:
  2a. To use the native acl squid with the squidblacklist.org lists (
http://www.squidblacklist.org/)
  2b. To use urlfilterdb (http://www.urlfilterdb.com/products/overview.html)

3. Which GNU/Linux distro do you suggest me? I was thinking about Debian
Jessie (just frozen) or CentOS7.

Thank you to everyone for reading so far.

Regards,
a.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/7022189e/attachment.htm>

From gkinkie at gmail.com  Sun Nov 16 16:22:43 2014
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 16 Nov 2014 17:22:43 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
Message-ID: <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>

On Sun, Nov 16, 2014 at 4:54 PM, alberto <alberto.furia at gmail.com> wrote:
> Hello everyone,
> first of all thanks to the community of squid for such a great job.

Hello Alberto,

[...]

> I have some questions that I would like to share with you:
>
> 1. I would like to leave the solution we are using now (wpad balancing). In
> a situation like the one I have described, centralized squid serving the
> spokes/branches, which is the best solution for clustering/HA? If one of the
> centralized nodes had to "die" I would like client machines not to remain
> "hanging" but to continue working on an active node without disruption. A
> hierarchy of proxy would be the solution?

If you want to maximize the efficiency of your balancing solution, you
probably want a slightly different approach: instead of using the
client-ip as hashing mechanism, you want to hash on the destination
host.
e.g. have a pac-file like (untested, and to be adjusted):

function FindProxyForURL(url, host) {
   var dest_ip = dnsResolve(host);
   var dest_hash= dest_ip.slice(-1) % 2;
   if (dest_hash)
     return "PROXY local_proxy1:port; PROXY local_proxy2:port; DIRECT";
   return "PROXY local_proxy2:port; PROXY local_proxy1:port; DIRECT"
}
This will balance by the final digit of the destination IP of the
service. The downside is that it requires DNS lookups by the clients,
and that if the primary local proxy fails, it takes a few seconds (up
to 30) for clients to give up and fail over to secondary.

local_proxies can then either go direct to the origin server (if
intranet) or use a balancing mechanism such as carp (see the
documentation for the cache_peer directive in squid) to maximize
efficiency, especially for Internet destinations.

The only single-point-of-failure at the HTTP level in this design is
the PACfile server, it'll be up to you to make that reliable.

> 2. Bearing in mind that all users will be AD authenticated, which url
> filtering/blacklist solution do you suggest?
> In the past I have worked a lot with squidguard and dansguardian but now
> they don't seem to be the state of the art anymore.
> I've been thinking about two different solutions:
>   2a. To use the native acl squid with the squidblacklist.org lists
> (http://www.squidblacklist.org/)
>   2b. To use urlfilterdb (http://www.urlfilterdb.com/products/overview.html)

I don't know, sorry.

> 3. Which GNU/Linux distro do you suggest me? I was thinking about Debian
> Jessie (just frozen) or CentOS7.

http://wiki.squid-cache.org/BestOsForSquid

-- 
    Francesco


From bpk678 at gmail.com  Sun Nov 16 16:51:43 2014
From: bpk678 at gmail.com (Brendan Kearney)
Date: Sun, 16 Nov 2014 11:51:43 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
Message-ID: <1416156703.4763.12.camel@desktop.bpk2.com>

On Sun, 2014-11-16 at 17:22 +0100, Kinkie wrote:
> On Sun, Nov 16, 2014 at 4:54 PM, alberto <alberto.furia at gmail.com> wrote:
> > Hello everyone,
> > first of all thanks to the community of squid for such a great job.
> 
> Hello Alberto,
> 
> [...]
> 
> > I have some questions that I would like to share with you:
> >
> > 1. I would like to leave the solution we are using now (wpad balancing). In
> > a situation like the one I have described, centralized squid serving the
> > spokes/branches, which is the best solution for clustering/HA? If one of the
> > centralized nodes had to "die" I would like client machines not to remain
> > "hanging" but to continue working on an active node without disruption. A
> > hierarchy of proxy would be the solution?
> 
> If you want to maximize the efficiency of your balancing solution, you
> probably want a slightly different approach: instead of using the
> client-ip as hashing mechanism, you want to hash on the destination
> host.
> e.g. have a pac-file like (untested, and to be adjusted):
> 
> function FindProxyForURL(url, host) {
>    var dest_ip = dnsResolve(host);
>    var dest_hash= dest_ip.slice(-1) % 2;
>    if (dest_hash)
>      return "PROXY local_proxy1:port; PROXY local_proxy2:port; DIRECT";
>    return "PROXY local_proxy2:port; PROXY local_proxy1:port; DIRECT"
> }
> This will balance by the final digit of the destination IP of the
> service. The downside is that it requires DNS lookups by the clients,
> and that if the primary local proxy fails, it takes a few seconds (up
> to 30) for clients to give up and fail over to secondary.
> 
> local_proxies can then either go direct to the origin server (if
> intranet) or use a balancing mechanism such as carp (see the
> documentation for the cache_peer directive in squid) to maximize
> efficiency, especially for Internet destinations.
> 
> The only single-point-of-failure at the HTTP level in this design is
> the PACfile server, it'll be up to you to make that reliable.
> 
> > 2. Bearing in mind that all users will be AD authenticated, which url
> > filtering/blacklist solution do you suggest?
> > In the past I have worked a lot with squidguard and dansguardian but now
> > they don't seem to be the state of the art anymore.
> > I've been thinking about two different solutions:
> >   2a. To use the native acl squid with the squidblacklist.org lists
> > (http://www.squidblacklist.org/)
> >   2b. To use urlfilterdb (http://www.urlfilterdb.com/products/overview.html)
> 
> I don't know, sorry.
> 
> > 3. Which GNU/Linux distro do you suggest me? I was thinking about Debian
> > Jessie (just frozen) or CentOS7.
> 
> http://wiki.squid-cache.org/BestOsForSquid
> 

i have all my squid instances (only 2 right now) share their caches:
cache_peer 192.168.25.1 sibling 3128    4827    htcp=no-clr
and
cache_peer 192.168.50.1 sibling 3128    4827    htcp=no-clr

which allows for anything cached to be served from local cache or a
sibling, instead of from the internet.  the likelihood of the sibling
cache being faster than the internet is high.

i use HAProxy to load balance based on the least number of connections
associated with the a pool member.  since i am sharing caches, i dont
need to pin a client or request to any particular proxy, at all or for
only a period of time.  with HAProxy, i only see a couple of seconds
interruption when one proxy goes offline.  generally this is trivial in
the end user experience.  i have it logging when instances go offline or
come back online, and the stats web interface is handy for quickly
checking status.

while i dont have any suggestions about which filtering option to use, i
will note that DansGuardian versions i have found are only HTTP/1.0
compliant, so you are likely losing gzip compression at the protocol
layer, and caching is likely affected, too.



From gkinkie at gmail.com  Sun Nov 16 17:27:44 2014
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 16 Nov 2014 18:27:44 +0100
Subject: [squid-users] Fwd:  Centralized Squid - design and implementation
In-Reply-To: <CA+Y8hcPzOsZc-==8u_=EOOTnUECEqg-=k7OWqDtNHDbtb02s+Q@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
 <CANEbR8=k8DiZSu4iLPykkw_CZGDtcx02ejtFk74ytxkxXP5ubg@mail.gmail.com>
 <CA+Y8hcPzOsZc-==8u_=EOOTnUECEqg-=k7OWqDtNHDbtb02s+Q@mail.gmail.com>
Message-ID: <CA+Y8hcPwJPyBh9AVf3rEcL5CMiqVEQkzwWBX=KfGqqKD0x8XEA@mail.gmail.com>

Forwarding, as it may be useful to others.


---------- Forwarded message ----------
From: Kinkie <gkinkie at gmail.com>
Date: Sun, Nov 16, 2014 at 6:27 PM
Subject: Re: [squid-users] Centralized Squid - design and implementation
To: alberto <alberto.furia at gmail.com>


On Sun, Nov 16, 2014 at 5:53 PM, alberto <alberto.furia at gmail.com> wrote:
> Hi Kinkie
>
> On Sun, Nov 16, 2014 at 5:22 PM, Kinkie <gkinkie at gmail.com> wrote:
>>
>>    if (dest_hash)
>>      return "PROXY local_proxy1:port; PROXY local_proxy2:port; DIRECT";
>>    return "PROXY local_proxy2:port; PROXY local_proxy1:port; DIRECT"
>> }
>> This will balance by the final digit of the destination IP of the
>> service.
>
>
> With this configuration i can only balance between two nodes in normal
> situation right?
> Whati if i would like to have more nodes balancing the traffic? In case of
> very high load for example.

The hashing is a bit simplicistic. You could do something like (again:
untested):

// if the code works, this is the only tuneable needed. Everything
else self-adjusts
var proxies = ["PROXY proxy1:port1", "PROXY proxy2:port2", "PROXY
proxy3.port3"];

function hash(host, buckets) { // returns a host-dependent integer
between 0 and buckets
  var hostip = dnsResolve(host);
  if (!hostip) // dns resolution failure
    return 0;
  return hostip.slice(hostip.lastIndexOf(".")) % buckets;
}

function FindProxyForURL(url, host) {
  var h = hash(host, proxies.length+1);
  var p = proxies;
  for (var j = 0; j < h; ++j)
    p.unshift(p.pop()); // rotate the "p" array
  return p.join("; ") + "; DIRECT";
}



--
    Francesco


-- 
    Francesco


From alberto.furia at gmail.com  Sun Nov 16 17:32:34 2014
From: alberto.furia at gmail.com (alberto)
Date: Sun, 16 Nov 2014 18:32:34 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <1416156703.4763.12.camel@desktop.bpk2.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
 <1416156703.4763.12.camel@desktop.bpk2.com>
Message-ID: <CANEbR8n4A-nPq9G6bEWgjKCqw_ee82JZDQRFp+T9+WXq5On15w@mail.gmail.com>

Hi Brendan

On Sun, Nov 16, 2014 at 5:51 PM, Brendan Kearney <bpk678 at gmail.com> wrote:

> i use HAProxy to load balance based on the least number of connections
>

Do you use kerberos/AD authentication?
Any issues with HAPROXY in front of the squid nodes?

Thx,
a.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/ea17d52f/attachment.htm>

From bpk678 at gmail.com  Sun Nov 16 19:00:53 2014
From: bpk678 at gmail.com (brendan kearney)
Date: Sun, 16 Nov 2014 14:00:53 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CANEbR8n4A-nPq9G6bEWgjKCqw_ee82JZDQRFp+T9+WXq5On15w@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
 <1416156703.4763.12.camel@desktop.bpk2.com>
 <CANEbR8n4A-nPq9G6bEWgjKCqw_ee82JZDQRFp+T9+WXq5On15w@mail.gmail.com>
Message-ID: <CAARxGtgUP4Ps_zXS1TcQrube==6WXUqe+Ad4JaEfUm=bOE0bOQ@mail.gmail.com>

I use kerberos auth and do not have issues.  You have to pay attention to
the details with kerberos auth (dns name and principals need to match,
specific  options set in squid configs), but it is working very well for me
On Nov 16, 2014 12:32 PM, "alberto" <alberto.furia at gmail.com> wrote:

> Hi Brendan
>
> On Sun, Nov 16, 2014 at 5:51 PM, Brendan Kearney <bpk678 at gmail.com> wrote:
>
>> i use HAProxy to load balance based on the least number of connections
>>
>
> Do you use kerberos/AD authentication?
> Any issues with HAPROXY in front of the squid nodes?
>
> Thx,
> a.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/a9100541/attachment.htm>

From squid3 at treenet.co.nz  Sun Nov 16 20:19:17 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Nov 2014 09:19:17 +1300
Subject: [squid-users] squid cache Large rock with aufs optimization for
 bandwidth saving
In-Reply-To: <002d01d001fd$26473960$72d5ac20$@netstream.ps>
References: <001c01d0016c$0b192f40$214b8dc0$@netstream.ps>
 <5467EBAB.7040206@treenet.co.nz>
 <002801d001a0$acfe81a0$06fb84e0$@netstream.ps>
 <54683EEF.5090701@treenet.co.nz>
 <002d01d001fd$26473960$72d5ac20$@netstream.ps>
Message-ID: <546906C5.8070001@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 1:26 p.m., Ahmed Allzaeem wrote:
> Hi Amos , thanks for reply.
> 
> But I think using large rock will let the memory cache size be >32
> ??? am  I correct ?
> 

Maybe no, maybe yes. Shared memory is a separate feature to large
rock. They previously had the same size limitations. largerock solved
it for rock caches. But the documentation does not show any change for
shared memory.


> I mean although im not using rock store , but I was thinking will
> have memory size >32Kb
> 
> So , I keep the aufs size which I assume will help me and will let
> me break the 32kB limitation .. but it seems not like I was
> planning !!!!
> 
> So m my previous config was using aufs with large rock needed in
> memory size which is not what I was planning ?!!!! can you plz
> explain me why my previous config don?t have luck with memory >>
> 32KB??
> 

Explained that in tha last email. AUFS is per-worker caching and
memory_cache_shared probably still has the 32KB limit.

> 
> So for now , as u told me I removed all aufs drivers and added only
> rock dir as below :
> 
> cache_dir rock /ssd1 90000 max-size=32768 swap-timeout=350
> max-swap-rate=350 cache_dir rock /ssd2 90000 max-size=32768
> swap-timeout=350 max-swap-rate=350 cache_dir rock /ssd3 90000
> max-size=32768 swap-timeout=350 max-swap-rate=350 cache_dir rock
> /ssd4 30000 max-size=32768 swap-timeout=350 max-swap-rate=350
> 
> 
> And only mean size I can see now is 18 Kb ??!!!!

This has manually limited the rock dirs to ~32KB (max-size=32768).

> 
> I had tried another trial , I put memory_cache_shared off , squid
> cant be started all all kids is being killed and stared again  and
> so on !
> 
> Sample of logs : FATAL: Received Segment Violation...dying.

Do you have gdb?

http://wiki.squid-cache.org/SquidFaq/BugReporting has details on how
to use it to get a backtrace we will need to fix this bug.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaQbFAAoJELJo5wb/XPRj5JQH/03JULTbnGNgX5gz3wEw00eU
oTfcC/zJlwvItiuw/aOSsPoJ0zm9nTKjT/2BLcdFSS0hlk9gOczH0ea9OPAxDRvx
1u3MDNMcINAXo/YB8MXTgqp1fxS30OOkzILv0eoIQ+RJ/xtdIAJdvpxSy5DGHZnh
9b3VFXafAimOt+ZkWfML4b16Q5QAnHX1nHgWb9NHvubUtiB3B5eYw8+nHIOPbEtK
bVZrqVKQkxVbP3Bp+9NFrCrE6uhFw/Z9Xudc/hfBTec/8Moo/cw9pV1K0kqbzx8G
Vy06PI0RZBWRMZyBsXDY22aTrsslKtMyfbehuPCoVRbEETxY3jv8itntOT+kb7E=
=Vmca
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sun Nov 16 20:40:11 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Nov 2014 09:40:11 +1300
Subject: [squid-users] ssl callout helper
In-Reply-To: <07a61e8144ec4e75888e8ce16dcc8c2e@HKNPR04MB0388.apcprd04.prod.outlook.com>
References: <07a61e8144ec4e75888e8ce16dcc8c2e@HKNPR04MB0388.apcprd04.prod.outlook.com>
Message-ID: <54690BAB.2090800@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 16/11/2014 2:12 p.m., James Harper wrote:
> I've written a little helper to do ssl callouts to determine if
> the server is running ssl at all (eg not tunnelling over ssl), and
> also to be able to do limited ACL on CN/SAN. The main limitation is
> the way larger organisations will often have one SSL cert that
> covers many URLS (eg google cert also covers google.com,
> youtube.com etc).
> 
> Currently I need to do it like:
> 
> external_acl_type cert_callout %DST %PORT
> /usr/local/squid/libexec/ext_cert_callout_acl
> 
> acl banks dstdomain .bigbank.com acl banks dstdomain
> .otherbank.com
> 
> acl is_ssl external cert_callout IS_SSL acl banks_callout external
> cert_callout SAN .bigbank.com acl banks_callout external
> cert_callout SAN .otherbank.com
> 
> ssl_bump splice !is_ssl ssl_bump splice banks ssl_bump splice
> banks_callout ssl_bump bump all
> 
> But I'd rather not have to maintain the banks and the
> banks_callout lists separately when they are identical. Apart from
> sticking them in a separate file, are there any shortcuts I can
> take?

Return a tag=X parameter from your helper and use the note ACL type in
ssl_bump.

However, ssl_bump is a "fast" type ACL and as such is not really
capable of running external ACL tests.

> 
> Also, it would be good if squid could make use of the CN from the 
> certificate for logging, so instead of "CONNECT <IP>:<PORT>", I
> could log "CONNECT <CN>:<PORT>", which would really clean up the
> logs (apart from the cases mentioned above). I think I can use tag=
> or log=, but that would preclude me from using them for anything
> else (I'm not using them for anything else at the moment but
> still...)

<http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>

Also note that for splice the server cert details are probably not
available. If the IP address reverse-DNS points at any of your "banks"
ACL entries the connection will be spliced without even seeing the
ClientHello details.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaQurAAoJELJo5wb/XPRjHncH/3c15UBEzppVaR3l9+HuBADp
WEQMPntl+wi8Mmekzo5zZPxbvvPEORHENpXuZQvZNT0iXfZwYmaJnAdgOp+LlMgh
6iA4F3HFOkg7geXvcgef8e7HVJ8XaszsMMycoppg1NMhm6FlxnQIkLbGBjvWC2i0
2GDD7kV7uR44IHwoAjhnhE4JiHUgbrfSA8Cy1NynZ73X3bY//kcY5iKx9LN7IhIc
ks7hw69KODPbqPyFZ+H9axbGlDNIz3O9Q/1I1OzgW23V/7m3UPk1zXapsfmCK17u
8GRJRv9nDOZmOe8KAC7FsRN47I7e9tAmvikJrjDd9vm5G4zIdUM+ycgPpKKoY1Q=
=70hO
-----END PGP SIGNATURE-----


From alberto.furia at gmail.com  Sun Nov 16 20:58:41 2014
From: alberto.furia at gmail.com (alberto)
Date: Sun, 16 Nov 2014 21:58:41 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAARxGtgUP4Ps_zXS1TcQrube==6WXUqe+Ad4JaEfUm=bOE0bOQ@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
 <1416156703.4763.12.camel@desktop.bpk2.com>
 <CANEbR8n4A-nPq9G6bEWgjKCqw_ee82JZDQRFp+T9+WXq5On15w@mail.gmail.com>
 <CAARxGtgUP4Ps_zXS1TcQrube==6WXUqe+Ad4JaEfUm=bOE0bOQ@mail.gmail.com>
Message-ID: <CANEbR8mXqgmr7St+W5_YSDqEjvAa-Og6D--Mw-qqEE4ofaeoFg@mail.gmail.com>

Ok, thank you very much. I think this is a good solution, maybe with an
active/passive HAProxy with keepalived.
Are you able to serve also https without any problem through HAProxy or
only http request?

regards,
a.



On Sun, Nov 16, 2014 at 8:00 PM, brendan kearney <bpk678 at gmail.com> wrote:

> I use kerberos auth and do not have issues.  You have to pay attention to
> the details with kerberos auth (dns name and principals need to match,
> specific  options set in squid configs), but it is working very well for me
> On Nov 16, 2014 12:32 PM, "alberto" <alberto.furia at gmail.com> wrote:
>
>> Hi Brendan
>>
>> On Sun, Nov 16, 2014 at 5:51 PM, Brendan Kearney <bpk678 at gmail.com>
>> wrote:
>>
>>> i use HAProxy to load balance based on the least number of connections
>>>
>>
>> Do you use kerberos/AD authentication?
>> Any issues with HAPROXY in front of the squid nodes?
>>
>> Thx,
>> a.
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/8573fcab/attachment.htm>

From bpk678 at gmail.com  Sun Nov 16 21:51:01 2014
From: bpk678 at gmail.com (brendan kearney)
Date: Sun, 16 Nov 2014 16:51:01 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CANEbR8mXqgmr7St+W5_YSDqEjvAa-Og6D--Mw-qqEE4ofaeoFg@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CA+Y8hcOwYY+x0Q=M16GJo-Y7BekJgGBfkojeVucZ1O3G5zuEng@mail.gmail.com>
 <1416156703.4763.12.camel@desktop.bpk2.com>
 <CANEbR8n4A-nPq9G6bEWgjKCqw_ee82JZDQRFp+T9+WXq5On15w@mail.gmail.com>
 <CAARxGtgUP4Ps_zXS1TcQrube==6WXUqe+Ad4JaEfUm=bOE0bOQ@mail.gmail.com>
 <CANEbR8mXqgmr7St+W5_YSDqEjvAa-Og6D--Mw-qqEE4ofaeoFg@mail.gmail.com>
Message-ID: <CAARxGtgeavjAppKbwojkavqKYU4xTnKS_3BcDSz+UYmHOnpGow@mail.gmail.com>

Https is no issue.  The ssl session will persist to the same proxy for the
duration of the session.  I have no problems at all.
On Nov 16, 2014 3:58 PM, "alberto" <alberto.furia at gmail.com> wrote:

> Ok, thank you very much. I think this is a good solution, maybe with an
> active/passive HAProxy with keepalived.
> Are you able to serve also https without any problem through HAProxy or
> only http request?
>
> regards,
> a.
>
>
>
> On Sun, Nov 16, 2014 at 8:00 PM, brendan kearney <bpk678 at gmail.com> wrote:
>
>> I use kerberos auth and do not have issues.  You have to pay attention to
>> the details with kerberos auth (dns name and principals need to match,
>> specific  options set in squid configs), but it is working very well for me
>> On Nov 16, 2014 12:32 PM, "alberto" <alberto.furia at gmail.com> wrote:
>>
>>> Hi Brendan
>>>
>>> On Sun, Nov 16, 2014 at 5:51 PM, Brendan Kearney <bpk678 at gmail.com>
>>> wrote:
>>>
>>>> i use HAProxy to load balance based on the least number of connections
>>>>
>>>
>>> Do you use kerberos/AD authentication?
>>> Any issues with HAPROXY in front of the squid nodes?
>>>
>>> Thx,
>>> a.
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/29985b75/attachment.htm>

From monahbaki at gmail.com  Sun Nov 16 23:03:19 2014
From: monahbaki at gmail.com (Monah Baki)
Date: Sun, 16 Nov 2014 18:03:19 -0500
Subject: [squid-users] Squid 3.4.9 on OpenBSD 5.6 on Sparc64
Message-ID: <CALP3=x-=YtQ-j+Uqwf1opJNrU-Otoj1fVZxnj-utL8sHrSPr_g@mail.gmail.com>

Hi all,


./configure  '--prefix=/usr/local/squid' '--enable-pf-transparent'
'--enable-follow-x-forwarded-for' '--with-large-files' '--enable-ssl'
'--disable-ipv6' '--enable-esi' '--enable-kill-parent-hack' '--enable-snmp'
'--with-pthreads' '--with-filedescriptors=65535'




$ make
Making all in compat
/bin/sh ../libtool  --tag=CXX    --mode=compile g++ -DHAVE_CONFIG_H   -I..
-I../include -I../lib  -I../src -I../include    -I../libltdl    -Wall
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe
-D_REENTRANT -g -O2 -MT assert.lo -MD -MP -MF .deps/assert.Tpo -c -o
assert.lo assert.cc
libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src
-I../include -I../libltdl -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -g -O2 -MT assert.lo -MD -MP -MF
.deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o .libs/assert.o
In file included from ../compat/compat.h:98,
                 from ../include/squid.h:66,
                 from assert.cc:32:
../include/squid_endian.h:129:1: error: "le16toh" redefined
In file included from /usr/include/sys/types.h:45,
                 from ../compat/types.h:59,
                 from ../compat/compat.h:51,
                 from ../include/squid.h:66,
                 from assert.cc:32:
/usr/include/sys/endian.h:63:1: error: this is the location of the previous
definition
In file included from ../compat/compat.h:98,
                 from ../include/squid.h:66,
                 from assert.cc:32:
../include/squid_endian.h:130:1: error: "le32toh" redefined
In file included from /usr/include/sys/types.h:45,
                 from ../compat/types.h:59,
                 from ../compat/compat.h:51,
                 from ../include/squid.h:66,
                 from assert.cc:32:
/usr/include/sys/endian.h:64:1: error: this is the location of the previous
definition
*** Error 1 in compat (Makefile:898 'assert.lo')
*** Error 1 in /home/mbaki/squid-3.4.9 (Makefile:587 'all-recursive')



Any help will be appreciated.


Thanks
Monah
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141116/91abd492/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 17 02:00:58 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Nov 2014 15:00:58 +1300
Subject: [squid-users] Squid 3.4.9 on OpenBSD 5.6 on Sparc64
In-Reply-To: <CALP3=x-=YtQ-j+Uqwf1opJNrU-Otoj1fVZxnj-utL8sHrSPr_g@mail.gmail.com>
References: <CALP3=x-=YtQ-j+Uqwf1opJNrU-Otoj1fVZxnj-utL8sHrSPr_g@mail.gmail.com>
Message-ID: <546956DA.60805@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 12:03 p.m., Monah Baki wrote:
> Hi all,
> 
> 
> ./configure  '--prefix=/usr/local/squid' '--enable-pf-transparent'
> '--enable-follow-x-forwarded-for' '--with-large-files' '--enable-ssl'
> '--disable-ipv6' '--enable-esi' '--enable-kill-parent-hack' '--enable-snmp'
> '--with-pthreads' '--with-filedescriptors=65535'
> 
> 
> 
> 
> $ make
> Making all in compat
> /bin/sh ../libtool  --tag=CXX    --mode=compile g++ -DHAVE_CONFIG_H   -I..
> -I../include -I../lib  -I../src -I../include    -I../libltdl    -Wall
> -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe
> -D_REENTRANT -g -O2 -MT assert.lo -MD -MP -MF .deps/assert.Tpo -c -o
> assert.lo assert.cc
> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib -I../src
> -I../include -I../libltdl -Wall -Wpointer-arith -Wwrite-strings -Wcomments
> -Wshadow -Werror -pipe -D_REENTRANT -g -O2 -MT assert.lo -MD -MP -MF
> .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o .libs/assert.o
> In file included from ../compat/compat.h:98,
>                  from ../include/squid.h:66,
>                  from assert.cc:32:
> ../include/squid_endian.h:129:1: error: "le16toh" redefined
> In file included from /usr/include/sys/types.h:45,
>                  from ../compat/types.h:59,
>                  from ../compat/compat.h:51,
>                  from ../include/squid.h:66,
>                  from assert.cc:32:
> /usr/include/sys/endian.h:63:1: error: this is the location of the previous
> definition
> In file included from ../compat/compat.h:98,
>                  from ../include/squid.h:66,
>                  from assert.cc:32:
> ../include/squid_endian.h:130:1: error: "le32toh" redefined
> In file included from /usr/include/sys/types.h:45,
>                  from ../compat/types.h:59,
>                  from ../compat/compat.h:51,
>                  from ../include/squid.h:66,
>                  from assert.cc:32:
> /usr/include/sys/endian.h:64:1: error: this is the location of the previous
> definition
> *** Error 1 in compat (Makefile:898 'assert.lo')
> *** Error 1 in /home/mbaki/squid-3.4.9 (Makefile:587 'all-recursive')
> 
> 
> 
> Any help will be appreciated.

Please report bugs through Bugzilla.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaVbaAAoJELJo5wb/XPRj870IAOFKQ4IowlUEVGOPCOrV9B8t
jRbJTQvPePtdAmDHGbSyv+4Ajwd7AASx/u2vsnmINWX+nicIUAjol9Wgv34hoqrX
k2Ipqdcw6YCBzFOsDbL81j2XH3zI1FWt2iP9rUwHb6d7nIPqtkbHLR5BwjvNzSXA
WGGxU0uMmCyzQsJgKXSDeG+B4uTiNbkWCTnLHA9gTBVUngc3swmj1nfkFPCXBtct
RWqA4zIfrQwNobRmAtxyBdovX+hdhxPduG0ZCnRpQLawxZ7LUcIFPlfyVVOw24yf
tLaP+hFO/3ZALGzv6OcXmJ2H8GUQDnplhdXcizcz7uCpURLyNmeSTPQJN21nGhY=
=cQCl
-----END PGP SIGNATURE-----


From marcus.kool at urlfilterdb.com  Mon Nov 17 02:04:59 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 17 Nov 2014 00:04:59 -0200
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
Message-ID: <546957CB.40608@urlfilterdb.com>

Let me start to say that I am biased since I am the author of ufdbGuard.
If you have worked with squidGuard than you will find that ufdbGuard is an excellent replacement since ufdbGuard was forked in 2005 from squidGuard and has since gained many features.
And I suggest to apply for a trial license of the URL database at www.URLfilterDB.com.  You will probably see that there is a difference with other URL databases.

Best regards

Marcus

On 11/16/2014 01:54 PM, alberto wrote:
> Hello everyone,
> first of all thanks to the community of squid for such a great job.
>
> I'm writing because I have to revise the current implementation of squid in my company so I would like to share with you some design ideas and possibly have some suggestions from you.
>
> The group I work for has six offices/branches in six different italian cities and the networking infrastructure is based on a "hub and spoke" paradgima (ie
> https://www.checkpoint.com/products/vpn-1_power/images/vpn -1_pro_oneclick_star.gif) where every branch, the spoke, is part of the main datacenter, the hub.
> Now the cache/forward proxy runs - for each branch - inside the branch office on a pair of squid nodes balanced by wpad/javascript on ip address base (even ip/odd ip).
>
> For obvious reasons of maintenance and for other technical reasons we intend to move the proxy navigation centralizing it to the datacenter hub on a couple (how many?!) of squid nodes.
>
> I have some questions that I would like to share with you:
>
> 1. I would like to leave the solution we are using now (wpad balancing). In a situation like the one I have described, centralized squid serving the spokes/branches, which is the best solution for
> clustering/HA? If one of the centralized nodes had to "die" I would like client machines not to remain "hanging" but to continue working on an active node without disruption. A hierarchy of proxy
> would be the solution?
>
> 2. Bearing in mind that all users will be AD authenticated, which url filtering/blacklist solution do you suggest?
> In the past I have worked a lot with squidguard and dansguardian but now they don't seem to be the state of the art anymore.
> I've been thinking about two different solutions:
>    2a. To use the native acl squid with the squidblacklist.org <http://squidblacklist.org> lists (http://www.squidblacklist.org/)
>    2b. To use urlfilterdb (http://www.urlfilterdb.com/products/overview.html)
>
> 3. Which GNU/Linux distro do you suggest me? I was thinking about Debian Jessie (just frozen) or CentOS7.
>
> Thank you to everyone for reading so far.
>
> Regards,
> a.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From sudakov at sibptus.tomsk.ru  Mon Nov 17 04:26:30 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Mon, 17 Nov 2014 10:26:30 +0600
Subject: [squid-users] Removing cache credentials
In-Reply-To: <5466F70E.2020204@treenet.co.nz>
References: <1416025280653-4668398.post@n4.nabble.com>
 <5466E476.8030800@treenet.co.nz>
 <20141115063344.GA32943@admin.sibptus.tomsk.ru>
 <5466F70E.2020204@treenet.co.nz>
Message-ID: <20141117042630.GA62737@admin.sibptus.tomsk.ru>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Amos Jeffries wrote:
> > 
> > If we speak about Kerberos authentication. On the very first
> > request, the browser receives a "407 Proxy Authentication Required"
> > reply and learns that it is expected to provide credentials. For a
> > certain amount of time, the browser knows that it should send the
> > credentials with every request without waiting for an 407 reply.
> > 
> > How long is this amount of time? Is it like forever?  Is there ever
> > a limit after which the browser will try again to send a request
> > without credentials? Maybe after a browser restart or what?
> > 
> 
> Negotiate/Kerberos (and NTLM) do not authenticate the request. They
> abuse HTTP to authenticate the TCP connection underneath HTTP. So the
> credentials must be re-used for the entire lifetime of that TCP
> connection. Changing credentials means tearing down that whole TCP
> connection.

As far as I understood you, there would be a "407 Proxy Authentication
Required" and "Proxy-Authorization: Negotiate" pair in each TCP
connection between browser and proxy.

If the connection is used for several requests, only the first HTTP
request in the connection would contain authentication info.  But each
new TCP connection is re-authenticated by HTTP. Is this correct?

- -- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUaXj2AAoJEA2k8lmbXsY05EgH/ji2X1LaocbTZ+mnL1A/ejBY
8sToM1NwBvzmk+lO1Ezrq91DuZOKTUSGiCv/973Dg0aNrCRpQZ1XzC+jsJ6F/sjo
eaBdmF5X74IG7cVgozfZJFXPjA1Ld0h1boTCsqibnoz85IUB4cJwU1rVvVsFzLEZ
O8DcpPf7KDbFdFJLH6Niu7rZ0vLoNi4hqNRSAmmdBXb7ck8wEM7o0G/YC3IwzhLW
c+8D5rfGVYxAwYN0H7hIo/VGMsD7gTZVAYjWhWEORczDyEFtnT7NprDa3RMVEQXX
LwQfaY7g0KUqpoCKqYIqCyXrmMh26nK84z2k/UCfbUUYttLD/ae7NPnyOI36DhE=
=XBm/
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 17 06:50:57 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Nov 2014 19:50:57 +1300
Subject: [squid-users] Removing cache credentials
In-Reply-To: <20141117042630.GA62737@admin.sibptus.tomsk.ru>
References: <1416025280653-4668398.post@n4.nabble.com>
 <5466E476.8030800@treenet.co.nz>
 <20141115063344.GA32943@admin.sibptus.tomsk.ru>
 <5466F70E.2020204@treenet.co.nz>
 <20141117042630.GA62737@admin.sibptus.tomsk.ru>
Message-ID: <54699AD1.8030504@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 5:26 p.m., Victor Sudakov wrote:
> Amos Jeffries wrote:
>>> 
>>> If we speak about Kerberos authentication. On the very first 
>>> request, the browser receives a "407 Proxy Authentication
>>> Required" reply and learns that it is expected to provide
>>> credentials. For a certain amount of time, the browser knows
>>> that it should send the credentials with every request without
>>> waiting for an 407 reply.
>>> 
>>> How long is this amount of time? Is it like forever?  Is there
>>> ever a limit after which the browser will try again to send a
>>> request without credentials? Maybe after a browser restart or
>>> what?
>>> 
> 
>> Negotiate/Kerberos (and NTLM) do not authenticate the request.
>> They abuse HTTP to authenticate the TCP connection underneath
>> HTTP. So the credentials must be re-used for the entire lifetime
>> of that TCP connection. Changing credentials means tearing down
>> that whole TCP connection.
> 
> As far as I understood you, there would be a "407 Proxy
> Authentication Required" and "Proxy-Authorization: Negotiate" pair
> in each TCP connection between browser and proxy.

407 is repeated as many times as necesary until the client starts
sending valid credentials. Proxy-Authorization is used on every
request containing any credentials. That is the basic requirement for
any HTTP auth schemes.

They are not a pair. Since there is no requirement for anything to
follow the 407. Nor is there even a requirement for the two messages
to be sent on the same TCP connection (eg "auth_param ... keep_alive
off"). Statelessness is fun sometimes.

> 
> If the connection is used for several requests, only the first
> HTTP request in the connection would contain authentication info.

No. Once authentication is accepted on a connection the credentials
token MUST be sent on all following requests.

 - So far that is basic HTTP auth requirements. Now things get weird...

Lack of Negotiate credentials on any request is a sign of injection
attack being performed and the TCP connection must be torn down.

To do that tear-down Squid can send 407 challenge with
Connection:close such that the client can resume with
re-authentication on new TCP connection(s) without waiting for any 407.


> But each new TCP connection is re-authenticated by HTTP. Is this
> correct?

Not really. A TCP connection may be used for multiple requests before
one needs to authenticate and kicks out a 407.

If you offer Basic Digest, Bearer or other properly HTTP compliant
auth schemes there is even the possibility that multiple different
types of credentials were happily being multiplexed over the single
connection until NTLM/Negotiate gets in the way.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaZrRAAoJELJo5wb/XPRjCNIH/2/Shwg113XcpKnyF+CATG8C
vxpa88xuajuqiwHRBtPd6caQDbnQi05HXuoD+RUmS+bhLeHhFg/5AZPswnCLLp7N
SmoOcc7yB80+0/Cmy31CpfO0QWm5GnkMHM40Kj7Lftms2AUKdOtmxoBITJYKTmPf
hXEktNtD78K8rB0qHg5s38Z/wd3K7YNvGTBlNacbpLM0TYx1CELkccA7ifx+t3Ut
rLKqZJHUbSWAhmBeHs6eLu2KXp+0ZA1gp5HqDmg7XaRtzWAJpy9K0+zpPEVdVZcz
rYPsCjdOuesHrWc99pNp4d3RFggXQ6298eJHthhNwqdVwMhhkfRGcp1fyNNIx2E=
=8j6B
-----END PGP SIGNATURE-----


From sudakov at sibptus.tomsk.ru  Mon Nov 17 08:52:30 2014
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Mon, 17 Nov 2014 14:52:30 +0600
Subject: [squid-users] Removing cache credentials
In-Reply-To: <54699AD1.8030504@treenet.co.nz>
References: <1416025280653-4668398.post@n4.nabble.com>
 <5466E476.8030800@treenet.co.nz>
 <20141115063344.GA32943@admin.sibptus.tomsk.ru>
 <5466F70E.2020204@treenet.co.nz>
 <20141117042630.GA62737@admin.sibptus.tomsk.ru>
 <54699AD1.8030504@treenet.co.nz>
Message-ID: <20141117085230.GA70086@admin.sibptus.tomsk.ru>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Amos Jeffries wrote:

[dd]

> > 
> > As far as I understood you, there would be a "407 Proxy
> > Authentication Required" and "Proxy-Authorization: Negotiate" pair
> > in each TCP connection between browser and proxy.
> 
> 407 is repeated as many times as necesary until the client starts
> sending valid credentials. Proxy-Authorization is used on every
> request containing any credentials. That is the basic requirement for
> any HTTP auth schemes.

During one TCP session from browser to Squid, I see requests both with
and without the Proxy-Authorization header. The requests without the
Proxy-Authorization header are also satisfied by the proxy. I don't
understand the logic behind this, that's why I am asking.

If there were a Proxy-Authorization header on every request, or only
on the first request in a TCP session, or if every credentialless
request were followed by a 407, I would not be surprised.

> They are not a pair. Since there is no requirement for anything to
> follow the 407. Nor is there even a requirement for the two messages
> to be sent on the same TCP connection (eg "auth_param ... keep_alive
> off"). Statelessness is fun sometimes.
> 
> > 
> > If the connection is used for several requests, only the first
> > HTTP request in the connection would contain authentication info.
> 
> No. Once authentication is accepted on a connection the credentials
> token MUST be sent on all following requests.

However, as I am looking at a single TCP session between squid and browser
(filtered out by WireShark), I don't see this happening. The 407 reply is sent
only once, and then there are some requests following, some of them
contain the Proxy-Authorization header but most don't.

> 
>  - So far that is basic HTTP auth requirements. Now things get weird...
> 
> Lack of Negotiate credentials on any request is a sign of injection
> attack being performed and the TCP connection must be torn down.

There are plenty of such requests in the packet dump, and they are
happily answered with a "200 OK" and relevant content.

> 
> To do that tear-down Squid can send 407 challenge with
> Connection:close such that the client can resume with
> re-authentication on new TCP connection(s) without waiting for any 407.
> 
> 
> > But each new TCP connection is re-authenticated by HTTP. Is this
> > correct?
> 
> Not really. A TCP connection may be used for multiple requests before
> one needs to authenticate and kicks out a 407.

But each request, you say, must contain the credentials? Well, it does
not seem to be happening.


- -- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUabdOAAoJEA2k8lmbXsY02GYH/iYQDJwXd/2iQlBEbCMA5EUN
2ou/0fIkiQkbtcZtln38AbIzzP70/9TNXRfaRnGJYpPr28jKhxbXEWLDStL4ZgV0
TCocf8OGJb1Y16GPjuO+w004dCiQsqibY1pf3WYU0Ru7sAqtmpvkIBh+3I+I3+yo
KOh4onzALSj4A26pi3VIEtYv/4I/ufOibO1gJU43W9RcR9E33Cb3WZUTVeBniMkN
gIfW7+87iOQtln3oI2SJhr5jegH/bR0H+kAimQGMfqTNh9Rgs3cDVNIcf0KmAdGh
09p3vgnZWTY+wvnCe2g37NBlsZk1DIyw8nD0oEIlolMWdi53tU8XtMDZI1FuiG4=
=HcLA
-----END PGP SIGNATURE-----


From alberto.furia at gmail.com  Mon Nov 17 09:08:06 2014
From: alberto.furia at gmail.com (alberto)
Date: Mon, 17 Nov 2014 10:08:06 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <546957CB.40608@urlfilterdb.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
Message-ID: <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>

On Mon, Nov 17, 2014 at 3:04 AM, Marcus Kool <marcus.kool at urlfilterdb.com>
wrote:

> Let me start to say that I am biased since I am the author of ufdbGuard.
> If you have worked with squidGuard than you will find that ufdbGuard is an
> excellent replacement since ufdbGuard was forked in 2005 from squidGuard
> and has since gained many features.
>

Hi Marcus,
thank you for your reply.
I know you (i'm an old lurker of the squid list :-)) and the urlfilterdb
project.
I am very interested in the project and I will give it a chance without any
doubt, starting from the trial license.
FYI, there are about 1000 users in total.
Thank you to everyone, i'll come back soon!:-)

a.

--
https://qa.debian.org/developer.php?login=straluna at email.it
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141117/2b6c5958/attachment.htm>

From steve at opendium.com  Mon Nov 17 10:25:47 2014
From: steve at opendium.com (Steve Hill)
Date: Mon, 17 Nov 2014 10:25:47 +0000
Subject: [squid-users] RFC2616 headers in bumped requests
In-Reply-To: <5458DBD0.2050306@treenet.co.nz>
References: <5458A7D6.3090502@opendium.com> <5458DBD0.2050306@treenet.co.nz>
Message-ID: <5469CD2B.8080802@opendium.com>

On 04/11/14 13:59, Amos Jeffries wrote:

>> I've just come across a web server that throws its toys out of the
>> pram when it sees a Via header in an HTTPS request, and
>> unfortunately it's quite a big one - Yahoo.  See this request:
> 
>> ----- GET /news/degrees-lead-best-paid-careers-141513989.html
>> HTTP/1.1 Host: uk.finance.yahoo.com Via: 1.1
> 
> That is unfortunately an invalid HTTP Via header. It is mandatory to
> contain the host field even if it contains a host alias for the real
> FQDN. If that is what is actually being transfered the server is right
> in complaining.

It looks like I copied and pasted this wrong in my original email, I
have just retested and squid sends:
  Via: 1.1 iceni2.opendium.net (squid/3.4.9)

>> For now I have worked around it with: request_header_access Via
>> deny https request_header_access X-Forwarded-For deny https But it
>> does make me wonder if inserting the headers into bumped traffic is
>> a sensible thing to do.
> 
> If you can please chek that Via header being emitted by your Squid
> when things break. And also whether your Squid is contacting their
> server on an HTTPS or HTTP port.
>  If your Squid is contacting their HTTP port for un-encrypted traffic
> this redirect is competely expected.

This is definitely occurring when contacting the server on HTTPS with a
valid Via header:

$ openssl s_client -connect uk.finance.yahoo.com:443 -servername
uk.finance.yahoo.com
CONNECTED(00000003)
depth=3 C = US, O = "VeriSign, Inc.", OU = Class 3 Public Primary
Certification Authority
verify return:1
depth=2 C = US, O = "VeriSign, Inc.", OU = VeriSign Trust Network, OU =
"(c) 2006 VeriSign, Inc. - For authorized use only", CN = VeriSign Class
3 Public Primary Certification Authority - G5
verify return:1
depth=1 C = US, O = "VeriSign, Inc.", OU = VeriSign Trust Network, OU =
Terms of use at https://www.verisign.com/rpa (c)10, CN = VeriSign Class
3 Secure Server CA - G3
verify return:1
depth=0 C = US, ST = California, L = Sunnyvale, O = Yahoo Inc., CN =
www.yahoo.com
verify return:1
---
Certificate chain
 0 s:/C=US/ST=California/L=Sunnyvale/O=Yahoo Inc./CN=www.yahoo.com
   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=Terms of use at
https://www.verisign.com/rpa (c)10/CN=VeriSign Class 3 Secure Server CA - G3
 1 s:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=Terms of use at
https://www.verisign.com/rpa (c)10/CN=VeriSign Class 3 Secure Server CA - G3
   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
 2 s:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
   i:/C=US/O=VeriSign, Inc./OU=Class 3 Public Primary Certification
Authority
---
[certificate removed]
---
GET /news/degrees-lead-best-paid-careers-141513989.html HTTP/1.1
Host: uk.finance.yahoo.com
Via: 1.1 iceni2.opendium.net (squid/3.4.9)

HTTP/1.1 301 Moved Permanently
Date: Mon, 17 Nov 2014 10:20:57 GMT
Via: http/1.1 yts272.global.media.ir2.yahoo.com (ApacheTrafficServer [c
s f ]), http/1.1 r15.ycpi.dee.yahoo.net (ApacheTrafficServer [cMsSfW])
Server: ATS
Strict-Transport-Security: max-age=172800
Location:
https://uk.finance.yahoo.com/news/degrees-lead-best-paid-careers-141513989.html
Content-Length: 0
Age: 0
Connection: keep-alive

-- 

 - Steve

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From navari.lorenzo at gmail.com  Mon Nov 17 11:09:21 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Mon, 17 Nov 2014 03:09:21 -0800 (PST)
Subject: [squid-users] WARNING: deny_mime_type ACL is used in context
 without an HTTP response. Assuming mismatch.
Message-ID: <1416222561289-4668430.post@n4.nabble.com>

hello, 

in my cache.log i have many of these

==> /var/log/squid/cache.log <==
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.
2014/11/17 12:08:07 kid1| WARNING: deny_mime_type ACL is used in context
without an HTTP response. Assuming mismatch.


what could I do to correct.


vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

## ACL definition ##

acl user_authentication proxy_auth REQUIRED

acl server_no_auth src                  "/etc/squid/server_noauth.txt"
acl direct_hosts dst                    "/etc/squid/direct_hosts.txt"
acl deny_host src                       "/etc/squid/deny_host.txt"
acl direct_urls dstdom_regex            "/etc/squid/direct_urls.txt"
acl deny_client src                     "/etc/squid/deny_client.txt"
acl deny_users proxy_auth               "/etc/squid/deny_users.txt"
acl siti_proibiti  url_regex            "/etc/squid/siti.proibiti.squid"
acl deny_extensions urlpath_regex -i    "/etc/squid/deny_extensions.txt"
acl deny_mime_type rep_mime_type -i     "/etc/squid/deny_mime.txt"
acl allowed_extensions urlpath_regex -i "/etc/squid/allowed_extensions.txt"
acl allowed_assistenza_users proxy_auth
"/etc/squid/allowed_assistenza_users.txt"
acl allowed_sitesmime dstdom_regex      "/etc/squid/allowed_sitesmime.txt"
acl allowed_usermime proxy_auth         "/etc/squid/allowed_usermime.txt"

## ACL application ##

http_access allow server_no_auth
http_access allow direct_hosts
http_access       allow deny_mime_type direct_hosts
http_reply_access allow deny_mime_type direct_hosts
http_access       allow deny_extensions direct_hosts
http_reply_access allow deny_extensions direct_hosts

http_access allow direct_urls
http_access       allow deny_mime_type direct_urls
http_reply_access allow deny_mime_type direct_urls
http_access       allow deny_extensions direct_urls
http_reply_access allow deny_extensions direct_urls

deny_info ERR_DENY_EXTENSIONS deny_extensions
deny_info ERR_DENY_MIME deny_mime_type

http_access allow deny_client allowed_assistenza_users
http_access allow deny_extensions allowed_assistenza_users

http_access       allow deny_mime_type allowed_assistenza_users
http_reply_access allow deny_mime_type allowed_assistenza_users

http_access       allow deny_mime_type allowed_usermime
http_reply_access allow deny_mime_type allowed_usermime
http_access       allow deny_extensions allowed_usermime
http_reply_access allow deny_extensions allowed_usermime

http_access       allow deny_mime_type allowed_sitesmime
http_reply_access allow deny_mime_type allowed_sitesmime
http_access       allow deny_extensions allowed_sitesmime
http_reply_access allow deny_extensions allowed_sitesmime

http_access deny deny_mime_type
http_reply_access deny deny_mime_type
http_access deny deny_extensions
http_access deny deny_client
http_access deny deny_users
http_access deny siti_proibiti
http_access allow user_authentication
http_access allow allowed_extensions
##################################################################








--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-deny-mime-type-ACL-is-used-in-context-without-an-HTTP-response-Assuming-mismatch-tp4668430.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From carlosdefoe at gmail.com  Mon Nov 17 11:39:15 2014
From: carlosdefoe at gmail.com (Carlos Defoe)
Date: Mon, 17 Nov 2014 08:39:15 -0300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
Message-ID: <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>

Use a load balancer. HAproxy will do the trick, if you don't want to
spend some money on a professional load balancer like F5 big-ip.

Don't drop the use of wpad. You can send the balancer name (eg.
proxy.your.domain) as a default for every client, and send the names
of the proxy nodes as a failover.

On Mon, Nov 17, 2014 at 6:08 AM, alberto <alberto.furia at gmail.com> wrote:
> On Mon, Nov 17, 2014 at 3:04 AM, Marcus Kool <marcus.kool at urlfilterdb.com>
> wrote:
>>
>> Let me start to say that I am biased since I am the author of ufdbGuard.
>> If you have worked with squidGuard than you will find that ufdbGuard is an
>> excellent replacement since ufdbGuard was forked in 2005 from squidGuard and
>> has since gained many features.
>
>
> Hi Marcus,
> thank you for your reply.
> I know you (i'm an old lurker of the squid list :-)) and the urlfilterdb
> project.
> I am very interested in the project and I will give it a chance without any
> doubt, starting from the trial license.
> FYI, there are about 1000 users in total.
> Thank you to everyone, i'll come back soon!:-)
>
> a.
>
> --
> https://qa.debian.org/developer.php?login=straluna at email.it
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From mrshahyn at gmail.com  Mon Nov 17 15:36:18 2014
From: mrshahyn at gmail.com (Shaheen)
Date: Mon, 17 Nov 2014 20:36:18 +0500
Subject: [squid-users] Please do not send any email on this email address.
Message-ID: <422A7C4E-61B7-48AF-A379-63C243798879@gmail.com>


Hi
Please do not send me any email on this email id. I feel boring. I dont know why lot of emails come to my inbox.  Please do not send me any 
Sent from my iPhone

From alex at samad.com.au  Mon Nov 17 21:01:29 2014
From: alex at samad.com.au (Alexander Samad)
Date: Tue, 18 Nov 2014 08:01:29 +1100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
Message-ID: <CAJ+Q1PUUetkAyFPub=r59n_d-OJdpOSG+gSeQa-wJad0kJC--g@mail.gmail.com>

Why haproxy instead of a pacemaker. I have 2 dmz boxes I setup in a
cluster. so I have 2 vips for the squid proxies. and dns setup to
round robin to the vip's.

I see sort of even distribution but I don't have a single point of
failure. if 1 node failes the vip moves over to the other node..





On 17 November 2014 22:39, Carlos Defoe <carlosdefoe at gmail.com> wrote:
> Use a load balancer. HAproxy will do the trick, if you don't want to
> spend some money on a professional load balancer like F5 big-ip.
>
> Don't drop the use of wpad. You can send the balancer name (eg.
> proxy.your.domain) as a default for every client, and send the names
> of the proxy nodes as a failover.
>
> On Mon, Nov 17, 2014 at 6:08 AM, alberto <alberto.furia at gmail.com> wrote:
>> On Mon, Nov 17, 2014 at 3:04 AM, Marcus Kool <marcus.kool at urlfilterdb.com>
>> wrote:
>>>
>>> Let me start to say that I am biased since I am the author of ufdbGuard.
>>> If you have worked with squidGuard than you will find that ufdbGuard is an
>>> excellent replacement since ufdbGuard was forked in 2005 from squidGuard and
>>> has since gained many features.
>>
>>
>> Hi Marcus,
>> thank you for your reply.
>> I know you (i'm an old lurker of the squid list :-)) and the urlfilterdb
>> project.
>> I am very interested in the project and I will give it a chance without any
>> doubt, starting from the trial license.
>> FYI, there are about 1000 users in total.
>> Thank you to everyone, i'll come back soon!:-)
>>
>> a.
>>
>> --
>> https://qa.debian.org/developer.php?login=straluna at email.it
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Antony.Stone at squid.open.source.it  Mon Nov 17 21:17:12 2014
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 17 Nov 2014 22:17:12 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAJ+Q1PUUetkAyFPub=r59n_d-OJdpOSG+gSeQa-wJad0kJC--g@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <CAJ+Q1PUUetkAyFPub=r59n_d-OJdpOSG+gSeQa-wJad0kJC--g@mail.gmail.com>
Message-ID: <201411172217.13254.Antony.Stone@squid.open.source.it>

On Monday 17 November 2014 at 22:01:29 (EU time), Alexander Samad wrote:

> Why haproxy instead of a pacemaker. I have 2 dmz boxes I setup in a
> cluster. so I have 2 vips for the squid proxies. and dns setup to
> round robin to the vip's.
> 
> I see sort of even distribution but I don't have a single point of
> failure. if 1 node failes the vip moves over to the other node..

Pacemaker is a fairly "dumb" (no offence meant, see below) network-level 
failover system, and if you do master-master failover, it can end up doing 
load balancing for you.

However, it only knows about node availability, whereas HAproxy can monitor 
many more things about your nodes, and also very easily expand to more than 
two nodes, doing true load balancing based on node availability, node load, 
node response times, number of connections to each node... it's a lot more 
"intelligient" (maybe "aware" is a better term) than pacemaker.

The downside of HAproxy is that you need an HAproxy machine in addition to the 
(Squid, in this case) nodes, and for real High Availability you should have 
two HAproxy nodes running Pacemaker between them, to avoid the HAproxy itself 
being a Single Point of Failure.  It doesn't need to be a big machine, though.

However the benefits of being able to send new connections to the machine with 
the lowest load, the fastest response, the fewest current connections, or 
several other things, means it's a lot more flexible, not to mention expandable 
if you decide to grow your Squid farm to 3, 4 or however many more servers.

> On 17 November 2014 22:39, Carlos Defoe <carlosdefoe at gmail.com> wrote:
> > Use a load balancer. HAproxy will do the trick, if you don't want to
> > spend some money on a professional load balancer like F5 big-ip.
> > 
> > Don't drop the use of wpad. You can send the balancer name (eg.
> > proxy.your.domain) as a default for every client, and send the names
> > of the proxy nodes as a failover.
> > 
> > On Mon, Nov 17, 2014 at 6:08 AM, alberto <alberto.furia at gmail.com> wrote:
> >> On Mon, Nov 17, 2014 at 3:04 AM, Marcus Kool
> >> <marcus.kool at urlfilterdb.com>
> >> 
> >> wrote:
> >>> Let me start to say that I am biased since I am the author of
> >>> ufdbGuard. If you have worked with squidGuard than you will find that
> >>> ufdbGuard is an excellent replacement since ufdbGuard was forked in
> >>> 2005 from squidGuard and has since gained many features.
> >> 
> >> Hi Marcus,
> >> thank you for your reply.
> >> I know you (i'm an old lurker of the squid list :-)) and the urlfilterdb
> >> project.
> >> I am very interested in the project and I will give it a chance without
> >> any doubt, starting from the trial license.
> >> FYI, there are about 1000 users in total.
> >> Thank you to everyone, i'll come back soon!:-)

Regards,


Antony.

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Mon Nov 17 21:36:32 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Nov 2014 10:36:32 +1300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
Message-ID: <546A6A60.3080303@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 18/11/2014 12:39 a.m., Carlos Defoe wrote:
> Use a load balancer. HAproxy will do the trick, if you don't want
> to spend some money on a professional load balancer like F5
> big-ip.

Or even, taddah ... Squid!

see cache_peer for the many load balancing algorithms available.

> 
> Don't drop the use of wpad. You can send the balancer name (eg. 
> proxy.your.domain) as a default for every client, and send the
> names of the proxy nodes as a failover.

But, but, WPAD *is* the first layer of load balancing.
Kinkie already posted the way to do clean loadbalancing with failover
between proxies. Without any need for a bottlneck at a balancer server
or IP.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUampfAAoJELJo5wb/XPRjqmoH/1Uofs/AnILu8GePdCHBLu3R
7O6dXpjcI8GTLtjXw12YWVQG5tAW6SBu3S6iLiwLLIxfcHxok/bcn9n+AnD5aBxh
deALwoYavG+iG28uj6DA65eqg02mOgps0HNbLOIk5kZS6G50mKRIoXVIS1JaqESb
797L9VhDLdckgotn8XGEAXOsPT6ZptgSoHvs/6X1YOx3iV51criH8Nt4O1UsiSY9
R/YjhfPbtDuK5UG0lU8w1BN1NaJBH2ZQzWu318kUFkGQ6a1eXFIEVTUZG+7APVrd
KOYbgGv99HaTo13a77BYb2Yr5wviVjG41B5rF6Y3LpRCAvOJ9GTb5WDzKPrgeJo=
=/QNG
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 17 21:47:13 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Nov 2014 10:47:13 +1300
Subject: [squid-users] WARNING: deny_mime_type ACL is used in context
 without an HTTP response. Assuming mismatch.
In-Reply-To: <1416222561289-4668430.post@n4.nabble.com>
References: <1416222561289-4668430.post@n4.nabble.com>
Message-ID: <546A6CE1.1020607@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 18/11/2014 12:09 a.m., navari.lorenzo at gmail.com wrote:
> hello,
> 
> in my cache.log i have many of these
> 
> ==> /var/log/squid/cache.log <== 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch. 2014/11/17 12:08:07 kid1| WARNING:
> deny_mime_type ACL is used in context without an HTTP response.
> Assuming mismatch.
> 
> 
> what could I do to correct.

Stop trying to decide whether to permit requests, based on the mime
type of the response that would come back at some point in the future.

Your deny_mime_type ACL is not usable in the http_access directives.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUamzhAAoJELJo5wb/XPRjwl4H/1JI3+on9saLle7VxB44wWbk
BieHYGHxMZ1aUcJd2DfLSBCzwj+mO+fLRR6QeUiiSyqtLDLMEOJs6oeHxaJSQKhg
14VJlspWRs2t2RFCQd+GGYV/YnB/Uxw69KT2s0+6OW27bamcTT1DJm/uhcngbJSn
PLlxMc7itOFp620nDqELlYrR99a2HLHItVPcOFQYnaNF05uhI5y1Y1kdmtgUazVZ
knqoIo704Ynza/mTYUFEap2KeKYFuUYcvQnKEWnSBHRmQkoFNmmBXH72+jTLG6Ji
wEC+rX+OMaDysSSnuVrc6Y3qJkAFjg1ZmZgG7aNlOMQtRIAV1ODIRex383FUMhU=
=c645
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Nov 17 22:05:05 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Nov 2014 11:05:05 +1300
Subject: [squid-users] RFC2616 headers in bumped requests
In-Reply-To: <5469CD2B.8080802@opendium.com>
References: <5458A7D6.3090502@opendium.com> <5458DBD0.2050306@treenet.co.nz>
 <5469CD2B.8080802@opendium.com>
Message-ID: <546A7111.70605@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 17/11/2014 11:25 p.m., Steve Hill wrote:
> On 04/11/14 13:59, Amos Jeffries wrote:
> 
>>> I've just come across a web server that throws its toys out of
>>> the pram when it sees a Via header in an HTTPS request, and 
>>> unfortunately it's quite a big one - Yahoo.  See this request:
>> 
>>> ----- GET /news/degrees-lead-best-paid-careers-141513989.html 
>>> HTTP/1.1 Host: uk.finance.yahoo.com Via: 1.1
>> 
>> That is unfortunately an invalid HTTP Via header. It is mandatory
>> to contain the host field even if it contains a host alias for
>> the real FQDN. If that is what is actually being transfered the
>> server is right in complaining.
> 
> It looks like I copied and pasted this wrong in my original email,
> I have just retested and squid sends: Via: 1.1 iceni2.opendium.net
> (squid/3.4.9)
> 
>>> For now I have worked around it with: request_header_access
>>> Via deny https request_header_access X-Forwarded-For deny https
>>> But it does make me wonder if inserting the headers into bumped
>>> traffic is a sensible thing to do.
>> 
>> If you can please chek that Via header being emitted by your
>> Squid when things break. And also whether your Squid is
>> contacting their server on an HTTPS or HTTP port. If your Squid
>> is contacting their HTTP port for un-encrypted traffic this
>> redirect is competely expected.
> 
> This is definitely occurring when contacting the server on HTTPS
> with a valid Via header:
> 

Would you mind running an experiment for me?

To see what happens if Squid delivers either of these Via headers
instead of its current output:

  Via: HTTPS/1.1 iceni2.opendium.net (squid/3.4.9)

  Via: TLS/1.2 iceni2.opendium.net (squid/3.4.9)

Setting it with request_header_access/replace should do.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUanERAAoJELJo5wb/XPRjbX0IAIsacfWnhx0zsP8AIzjXFvIr
bg1c19Hbgk2OmcxpBMA3b5cWggqPnZskUkQ/SLZphjt9z/tIbMa5Mgl0Ih7vTg5X
Z9GhX+gf3YoM2WLMymWnvzCRzQ6NwZKs856TFWYtM0gV8HPRFlVyGBp8cxya4yYh
rdGcp++yAC2LmvIGmELnQtXf74XyaIBw+exWwXCokHPh3MTD1CmsrD8rm1WJ2tBC
JnTxT5p8QL2NcuCAQqw9uZuckG9aVUsAOOdxSO8l7rkcQnuRJZKm3ZO7y4/kYrcU
XO1riDW0Ow0Xx0HAF/HMkz+pux2sPVvMeDa3JSP07sIVrcc8eaISZPXaC3n8FBQ=
=Xwwe
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Mon Nov 17 22:09:38 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Nov 2014 00:09:38 +0200
Subject: [squid-users] Squid 3.4.9 RPM release
In-Reply-To: <5458596C.8060203@treenet.co.nz>
References: <5458596C.8060203@treenet.co.nz>
Message-ID: <546A7222.90001@ngtech.co.il>

HTML version at: http://www1.ngtech.co.il/repo/release-3.4.9.html
I am happy to release the new RPMs of squid 3.4.9 and 3.5.0.2 beta for
Centos 6.6 64bit.
The new release includes couple bug fixes and improvements.
The links to the RPMs are at the bottom of the article.

?The sales man?
You get into a shop.. you got a sales man.
You have a meeting, you got a sales man + IT guy.
For a very long time I tried to understand the sales man place in the
shoe store
and in the IT area but it always was a mystery to me.
Some think that the sales man is the one that his job is to take some
product from
the inventory and put it in your hands and make sure you will pay for it.
Others thinks that his job is to make sure you will pay or at least will
think about it.
So in any case it's a very hard job!
I never have been a salesman but I have seen lots and lots of them.
There was a nice guy here and there but I will never forget ?The sales
man?!!
Indeed I must admit it is a rare character to find but ?The sales man?
do exist.
The one that wants your own good and to not just put anything in your hands.
I was very surprised to understand that even though his job was to sell
he told me
?I don't want you to buy the product! It's not for you!?.
I know some think it might be a myth but ?The sales man? is a very nice
guy which wants your own good.
There are crooks or bullies but non of them are real sales man.
These are crooks and always will be named this way.

For example if I will want to buy a new cell phone I will want him
to first understand my needs and according to these offer me what
he has and tell me if he doesn't have what I am looking for.
I expect this since I had a very good experience with more then one
of these sales man in the past.
I was looking for this type of sales man for many years until not long ago
and then I have found one.
I gladly asked him in a very honest way ?Is this the way you sell
everybody??
without any hesitation he answered me ?Yes I am!!!?.
He showed me how he sells to couple customers and
I was fascinated by ?The art of the sales man?.
He actually could sell a brick wall something but he just couldn't do that.
When I asked him about the reason he is not selling a Popsicle to Eskimos
he started to ask me couple questions and eventually told me
?I do not want to sell anything to anyone since I do not want others to
sell me anything the same way?.
This is one of the extraordinary things I have learned from ?The sales man?.

* Any notes and comments are wanted and welcome!

= RPMs release notifications
For now the RPM for the new release has started.
I have built until now two RPMs one for 3.4.9 stable and the other for
3.5.0.2 beta.
There are three packages:
- squid-%version%.rpm which contains the squid binary
- squid-helpers-%version%.rpm which contains the squid helpers
- squid-debuginfo-%version%.rpm which contains the debug symbols for the
other packages

For now the RPMs was built only for 64 bit version of CentOS 6.6 but
later as the time allows me I will build also for 32 bit CentOS 6.6 and
oracle linux 64 bit.

In this beta release I have tested:
- basic RPM installation
- simple forward proxy usage
- basic large rock functionality (creation, storage, recovery from
couple cases)
This release repo is at:
http://www1.ngtech.co.il/rpm/centos/6/x86_64/

This release files are:
http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-3.4.9-2.el6.x86_64.rpm
http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-helpers-3.4.9-2.el6.x86_64.rpm
http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-debuginfo-3.4.9-2.el6.x86_64.rpm
The beta repo was move to:
http://www1.ngtech.co.il/repo/centos/6/beta/x86_64/
The beta release files are:
http://www1.ngtech.co.il/repo/centos/6/beta/x86_64/squid-3.5.0.2-1.el6.x86_64.rpm
http://www1.ngtech.co.il/repo/centos/6/beta/x86_64/squid-helpers-3.5.0.2-1.el6.x86_64.rpm
http://www1.ngtech.co.il/repo/centos/6/beta/x86_64/squid-debuginfo-3.5.0.2-1.el6.x86_64.rpm

To Each and everyone of them there is an *asc* file which contains PGP and
MD5 SHA1 SHA2 SHA256 SHA384 SHA512 hashes.

All The Bests,
Eliezer Croitoru


From carlos.e.fernandez-touzon at uscis.dhs.gov  Mon Nov 17 22:34:53 2014
From: carlos.e.fernandez-touzon at uscis.dhs.gov (Fernandez-Touzon, Carlos E (CTR))
Date: Mon, 17 Nov 2014 22:34:53 +0000
Subject: [squid-users] using request_header_replace option
Message-ID: <309A75C3302E384DBE47D4D6EAA8F0F7B62B99@D2ASEPREA001>

I have two instances of Squid running:


-        Squid v3.3.13 on Fedora 20

-        Squid v3.1.10-29 on RHEL 6.6

Both instances are configured with the following options:

request_header_access User-Agent deny all
request_header_replace User-Agent someagent; (squid proxy header rewrite DID-IT)

The Fedora instance running v3.3.13 works just fine and rewrites headers like a champ.

The RHEL instance running v3.1.10 fails when I start squid with the following message:

2014/11/17 17:28:18| cache_cf.cc(364) parseOneConfigFile: squid.conf:61 unrecognized: 'request_header_replace'

I suspected that it was because I am missing the -enable-http-violations option.  However, neither instance displays --enable-http-violations or -disable-http-violations when I run "squid -v".  Moreover, the squid.spec files for both RPMs did not include any reference to -enable-http-violation or -disable-http-violation.  Is "-enable-http-violations" the default config option?

Thoughts?

Carlos

==FEDORA==
$ squid -v
Squid Cache: Version 3.3.13
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches  -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches  -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

==RHEL==
$ squid -v
Squid Cache: Version 3.1.10
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-internal-dns' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-arp-acl' '--enable-follow-x-forwarded-for' '--enable-auth=basic,digest,ntlm,negotiate' '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SMB,YP,getpwnam,multi-domain-NTLM,SASL,DB,POP3,squid_radius_auth' '--enable-ntlm-auth-helpers=smb_lm,no_check,fakeauth' '--enable-digest-auth-helpers=password,ldap,eDirectory' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log' '--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'LDFLAGS=-pie' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' --with-squid=/builddir/build/BUILD/squid-3.1.10Squid Cache: Version 3.1.10
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-internal-dns' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-arp-acl' '--enable-follow-x-forwarded-for' '--enable-auth=basic,digest,ntlm,negotiate' '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SMB,YP,getpwnam,multi-domain-NTLM,SASL,DB,POP3,squid_radius_auth' '--enable-ntlm-auth-helpers=smb_lm,no_check,fakeauth' '--enable-digest-auth-helpers=password,ldap,eDirectory' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-storeio=aufs,diskd,ufs' '--enable-useragent-log' '--enable-wccpv2' '--enable-esi' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'LDFLAGS=-pie' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' --with-squid=/builddir/build/BUILD/squid-3.1.10

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141117/d3cc75e7/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 17 23:40:19 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Nov 2014 12:40:19 +1300
Subject: [squid-users] using request_header_replace option
In-Reply-To: <309A75C3302E384DBE47D4D6EAA8F0F7B62B99@D2ASEPREA001>
References: <309A75C3302E384DBE47D4D6EAA8F0F7B62B99@D2ASEPREA001>
Message-ID: <546A8763.1000700@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 18/11/2014 11:34 a.m., Fernandez-Touzon, Carlos E (CTR) wrote:
> I have two instances of Squid running:
> 
> 
> -        Squid v3.3.13 on Fedora 20
> 
> -        Squid v3.1.10-29 on RHEL 6.6
> 
> Both instances are configured with the following options:
> 
> request_header_access User-Agent deny all request_header_replace
> User-Agent someagent; (squid proxy header rewrite DID-IT)
> 
> The Fedora instance running v3.3.13 works just fine and rewrites
> headers like a champ.
> 
> The RHEL instance running v3.1.10 fails when I start squid with the
> following message:
> 
> 2014/11/17 17:28:18| cache_cf.cc(364) parseOneConfigFile:
> squid.conf:61 unrecognized: 'request_header_replace'
> 
> I suspected that it was because I am missing the
> -enable-http-violations option.  However, neither instance displays
> --enable-http-violations or -disable-http-violations when I run
> "squid -v".  Moreover, the squid.spec files for both RPMs did not
> include any reference to -enable-http-violation or
> -disable-http-violation.  Is "-enable-http-violations" the default
> config option?
> 
> Thoughts?
> 

The directive was added in 3.1.12.

Please upgrade. 3.1 is very old and buggy. Including a handful of
security CVE, though I think the -29 indicates they have patched those
out.

Details on how to upgrade beyond the outdated RHEL (and Fedora)
packages can be found here:
  http://wiki.squid-cache.org/KnowledgeBase/RedHat

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaodiAAoJELJo5wb/XPRjWKEH/1+u6rt2pdmp2McXbPorGDRE
xZ0tNfUxku2eqY4OsACbjgGGpAeYmAb5lsrV2jYi+ev3zE5MC34736GYhJPEZ6Ma
SRSuqfHXMyRfr/ih2oprLnxxw2Fnyif2Q4bFH0AHVKT/oAF/7qksEYC8Tx3JnIXj
hFanxEQVh69SSARzXN/4y7uREIsvQ7kmEQZ9/Rqx3IxJx9PZfJpJxPqluoffEzkT
wn3FmDNIELE66OUceySHifQXYeoVrZfCpXi4VucWmzvzwvdQCUDIw+VjMapu9tXv
l6i/3GefHdIX+KfOaRwKPsiGWVzevwK0/LBJJt6YMyk8robVeObGhiFK+mpMjrs=
=RfjJ
-----END PGP SIGNATURE-----


From carlos.e.fernandez-touzon at uscis.dhs.gov  Tue Nov 18 02:58:13 2014
From: carlos.e.fernandez-touzon at uscis.dhs.gov (Fernandez-Touzon, Carlos E (CTR))
Date: Tue, 18 Nov 2014 02:58:13 +0000
Subject: [squid-users] using request_header_replace option
In-Reply-To: <546A8763.1000700@treenet.co.nz>
References: <309A75C3302E384DBE47D4D6EAA8F0F7B62B99@D2ASEPREA001>
 <546A8763.1000700@treenet.co.nz>
Message-ID: <309A75C3302E384DBE47D4D6EAA8F0F7B62BCA@D2ASEPREA001>

Amos,

Thanks!  You probably saved me a day of trying to track that down.  I am going to try to use the 3.4 release that  Eliezer Croitoru maintains.

Carlos

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, November 17, 2014 6:40 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] using request_header_replace option

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 18/11/2014 11:34 a.m., Fernandez-Touzon, Carlos E (CTR) wrote:
> I have two instances of Squid running:
> 
> 
> -        Squid v3.3.13 on Fedora 20
> 
> -        Squid v3.1.10-29 on RHEL 6.6
> 
> Both instances are configured with the following options:
> 
> request_header_access User-Agent deny all request_header_replace 
> User-Agent someagent; (squid proxy header rewrite DID-IT)
> 
> The Fedora instance running v3.3.13 works just fine and rewrites 
> headers like a champ.
> 
> The RHEL instance running v3.1.10 fails when I start squid with the 
> following message:
> 
> 2014/11/17 17:28:18| cache_cf.cc(364) parseOneConfigFile:
> squid.conf:61 unrecognized: 'request_header_replace'
> 
> I suspected that it was because I am missing the 
> -enable-http-violations option.  However, neither instance displays 
> --enable-http-violations or -disable-http-violations when I run "squid 
> -v".  Moreover, the squid.spec files for both RPMs did not include any 
> reference to -enable-http-violation or -disable-http-violation.  Is 
> "-enable-http-violations" the default config option?
> 
> Thoughts?
> 

The directive was added in 3.1.12.

Please upgrade. 3.1 is very old and buggy. Including a handful of security CVE, though I think the -29 indicates they have patched those out.

Details on how to upgrade beyond the outdated RHEL (and Fedora) packages can be found here:
  http://wiki.squid-cache.org/KnowledgeBase/RedHat

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUaodiAAoJELJo5wb/XPRjWKEH/1+u6rt2pdmp2McXbPorGDRE
xZ0tNfUxku2eqY4OsACbjgGGpAeYmAb5lsrV2jYi+ev3zE5MC34736GYhJPEZ6Ma
SRSuqfHXMyRfr/ih2oprLnxxw2Fnyif2Q4bFH0AHVKT/oAF/7qksEYC8Tx3JnIXj
hFanxEQVh69SSARzXN/4y7uREIsvQ7kmEQZ9/Rqx3IxJx9PZfJpJxPqluoffEzkT
wn3FmDNIELE66OUceySHifQXYeoVrZfCpXi4VucWmzvzwvdQCUDIw+VjMapu9tXv
l6i/3GefHdIX+KfOaRwKPsiGWVzevwK0/LBJJt6YMyk8robVeObGhiFK+mpMjrs=
=RfjJ
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From carlosdefoe at gmail.com  Tue Nov 18 03:07:01 2014
From: carlosdefoe at gmail.com (Carlos Defoe)
Date: Tue, 18 Nov 2014 00:07:01 -0300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <546A6A60.3080303@treenet.co.nz>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
Message-ID: <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>

I don't meant to use wpad as a load balancer. I would not do it, wpad
and pac are not designed for doing that, although it is (roughly)
possible to do it.

The load balancer device, if there is one, have one and only name, eg,
"proxy.your.domain". All the clients must point to that very same
name, and let the balancer do its job, that is, balancing between
nodes with a chosen algorithm, as a proxy itself.

With such a device in the scenario, wpad is only useful to send the
node names as a failover, for the (very very rare) case in which the
balancer is offline.

As for my scenario, I also use wpad to configure some exceptions, some
clients that will use a completely different proxy, etc...

But, in fact, when using wpad, you have a new point of failure, that
is, the webserver that is serving the pac (wpad.dat) file. As a fair
solution, every proxy node can have a light webserver serve the pac
file and let the DNS balance the requests for the name
"http://wpad.your.domain/wpad.dat".

On Mon, Nov 17, 2014 at 6:36 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 18/11/2014 12:39 a.m., Carlos Defoe wrote:
>> Use a load balancer. HAproxy will do the trick, if you don't want
>> to spend some money on a professional load balancer like F5
>> big-ip.
>
> Or even, taddah ... Squid!
>
> see cache_peer for the many load balancing algorithms available.
>
>>
>> Don't drop the use of wpad. You can send the balancer name (eg.
>> proxy.your.domain) as a default for every client, and send the
>> names of the proxy nodes as a failover.
>
> But, but, WPAD *is* the first layer of load balancing.
> Kinkie already posted the way to do clean loadbalancing with failover
> between proxies. Without any need for a bottlneck at a balancer server
> or IP.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUampfAAoJELJo5wb/XPRjqmoH/1Uofs/AnILu8GePdCHBLu3R
> 7O6dXpjcI8GTLtjXw12YWVQG5tAW6SBu3S6iLiwLLIxfcHxok/bcn9n+AnD5aBxh
> deALwoYavG+iG28uj6DA65eqg02mOgps0HNbLOIk5kZS6G50mKRIoXVIS1JaqESb
> 797L9VhDLdckgotn8XGEAXOsPT6ZptgSoHvs/6X1YOx3iV51criH8Nt4O1UsiSY9
> R/YjhfPbtDuK5UG0lU8w1BN1NaJBH2ZQzWu318kUFkGQ6a1eXFIEVTUZG+7APVrd
> KOYbgGv99HaTo13a77BYb2Yr5wviVjG41B5rF6Y3LpRCAvOJ9GTb5WDzKPrgeJo=
> =/QNG
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Jason_Haar at trimble.com  Tue Nov 18 06:31:26 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 18 Nov 2014 19:31:26 +1300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
Message-ID: <546AE7BE.50700@trimble.com>

On 18/11/14 16:07, Carlos Defoe wrote:
> As for my scenario, I also use wpad to configure some exceptions, some
> clients that will use a completely different proxy, etc...
Our "wpad.dat" is actually a PHP script which tests that the "official"
proxy (per client subnet) is actually working (with caching of the
results for performance reasons of course), if not it flicks them off to
another site's proxy server. Much better than trying to do dynamic DNS
tricks with a local HAproxy. ie if you have actually lost local Internet
access due to an ISP outage, HAproxy isn't going to help. But if WPAD
knows that a WAN-connected proxy is still working - why not point your
users at that instead

We've been doing this for 10+ years, 99% of the time it's never needed,
but when it's needed, it works :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From navari.lorenzo at gmail.com  Tue Nov 18 09:11:38 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Tue, 18 Nov 2014 01:11:38 -0800 (PST)
Subject: [squid-users] WARNING: deny_mime_type ACL is used in context
 without an HTTP response. Assuming mismatch.
In-Reply-To: <546A6CE1.1020607@treenet.co.nz>
References: <1416222561289-4668430.post@n4.nabble.com>
 <546A6CE1.1020607@treenet.co.nz>
Message-ID: <1416301898690-4668444.post@n4.nabble.com>

thank you very much.
it's right
i apologize



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-deny-mime-type-ACL-is-used-in-context-without-an-HTTP-response-Assuming-mismatch-tp4668430p4668444.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From schinken at hackerspace-bamberg.de  Tue Nov 18 11:30:57 2014
From: schinken at hackerspace-bamberg.de (schinken)
Date: Tue, 18 Nov 2014 12:30:57 +0100
Subject: [squid-users] Using LDAP and NCSA auth
Message-ID: <546B2DF1.1020606@hackerspace-bamberg.de>

Hi there,

i'm currently trying to do authentication against LDAP and NCSA auth -
but it looks like, the user is never checked against NCSA if auth
against LDAP failed (because the user doesn't exist):

> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b "dc=COMPANY,dc=int" -D squid at company.int -W /etc/squid3/ldappass.txt -f sAMAccountName=%s -h ldap.company.int
> auth_param basic children 100
> auth_param basic realm Internet Proxy
> auth_param basic credentialsttl 5 minute
> 
> auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd
> auth_param basic realm Internet Proxy Basic
> 
> acl auth proxy_auth REQUIRED

If i try ncsa auth manually, it works:

> root at proxy:~# /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd
> nikola testla
> OK


The same is true for LDAP auth. But i can't get a fallback working. How
could i solve this?

-- 
Schinken

Backspace e.V.
http://hackerspace-bamberg.de

mail: schinken at hackerspace-bamberg.de
xmpp: schinken at tai-wahn.de (otr)
GPG: FFB7 E40D B2DD D24C C9B7 B5C5 703C F8B8 882C 871E

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141118/352dd543/attachment.sig>

From carlosdefoe at gmail.com  Tue Nov 18 11:35:39 2014
From: carlosdefoe at gmail.com (Carlos Defoe)
Date: Tue, 18 Nov 2014 08:35:39 -0300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <546AE7BE.50700@trimble.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
Message-ID: <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>

Well, you just wrote a load balancer in PHP, with a load balancing
algorithm in it. It serves the same purpose as HAproxy (I don't really
use HAproxy, so I don't know, but I use the F5 big-ip which is
perfectly capable of testing Internet links behind squid). In you
scheme, WPAD is being used to tell the clients where the load balancer
(a webserver with a php script) is, and PAC probably as the answer
format, which returns a currently valid proxy node address directly to
the client. But as far as I know, once the client gets the PAC answer,
it willl not refresh until the browser is restarted, so it might be a
small problem there.

But it is a good solution, as proved by your decade of using it, and
much cheaper than a F5. As for the DNS trick, it is intended to
increase high availability of the web servers that are serving
wpad.dat (or your php script), because if it runs on only one
webserver, at some point no clients will find anything at all.

Well, there's a lot of ways of doing the same thing, including ucarp,
squid cache_peer as Amos said... It's just a matter of picking the one
that fits.

On Tue, Nov 18, 2014 at 3:31 AM, Jason Haar <Jason_Haar at trimble.com> wrote:
> On 18/11/14 16:07, Carlos Defoe wrote:
>> As for my scenario, I also use wpad to configure some exceptions, some
>> clients that will use a completely different proxy, etc...
> Our "wpad.dat" is actually a PHP script which tests that the "official"
> proxy (per client subnet) is actually working (with caching of the
> results for performance reasons of course), if not it flicks them off to
> another site's proxy server. Much better than trying to do dynamic DNS
> tricks with a local HAproxy. ie if you have actually lost local Internet
> access due to an ISP outage, HAproxy isn't going to help. But if WPAD
> knows that a WAN-connected proxy is still working - why not point your
> users at that instead
>
> We've been doing this for 10+ years, 99% of the time it's never needed,
> but when it's needed, it works :-)
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Nov 18 12:12:22 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Nov 2014 01:12:22 +1300
Subject: [squid-users] Using LDAP and NCSA auth
In-Reply-To: <546B2DF1.1020606@hackerspace-bamberg.de>
References: <546B2DF1.1020606@hackerspace-bamberg.de>
Message-ID: <546B37A6.9010902@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 12:30 a.m., schinken wrote:
> Hi there,
> 
> i'm currently trying to do authentication against LDAP and NCSA
> auth - but it looks like, the user is never checked against NCSA if
> auth against LDAP failed (because the user doesn't exist):
> 
>> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b
>> "dc=COMPANY,dc=int" -D squid at company.int -W
>> /etc/squid3/ldappass.txt -f sAMAccountName=%s -h
>> ldap.company.int auth_param basic children 100 auth_param basic
>> realm Internet Proxy auth_param basic credentialsttl 5 minute
>> 
>> auth_param basic program /usr/lib/squid3/basic_ncsa_auth
>> /etc/squid3/passwd auth_param basic realm Internet Proxy Basic
>> 
>> acl auth proxy_auth REQUIRED
> 
> If i try ncsa auth manually, it works:
> 
>> root at proxy:~# /usr/lib/squid3/basic_ncsa_auth /etc/squid3/passwd 
>> nikola testla OK
> 
> 
> The same is true for LDAP auth. But i can't get a fallback working.
> How could i solve this?

Two points:

1) Squid does not do authentication. What it does do is send
credentials to a helper and uses the OK/ERR response that comes back
to determine whether to serve the client request. That is all.

2) Each authentication scheme may only have one helper queried. Its
answer is absolute regarding the validity of the credentials sent to it.


Since you decided to write your own authentication system {check
against A, if ERR check against B} you also need to write a helper
that can do the authentication using that system logic.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUazekAAoJELJo5wb/XPRj6p0IANu5XsqkLFlj8pamP60LsXfp
VNxMbRHFBADauR7yaWUIbz+3Wif1ojr/nQg3tiXhb+1skUDOi1iIziPi3C9QvewI
FBlmcHBgIVHm+GfYHm4rfALnyi7lVXPX0Q9uJy4R+0xGzQw0mqgCRQ9QnYD+SUyB
euITq/X6AjDXKKT1fLbJ82DfiAYbukVTLXdXoBFKQ24semNcmHztoeBPuSeyFtBO
Yfu3SkR7J2zXolBt7o/q8nFXJSNzspuwjmIeIZGY7DliBoQtzSII/psiKI/QHusk
Q5/qb3S74uxhmvFhIbuflL0smYgIqezlILHqulZL60ob0tHAGrRnoYVnIYjVd/k=
=Ujkt
-----END PGP SIGNATURE-----


From bpk678 at gmail.com  Tue Nov 18 12:39:37 2014
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 18 Nov 2014 07:39:37 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
Message-ID: <1416314377.4763.35.camel@desktop.bpk2.com>

On Tue, 2014-11-18 at 08:35 -0300, Carlos Defoe wrote:
> Well, you just wrote a load balancer in PHP, with a load balancing
> algorithm in it. It serves the same purpose as HAproxy (I don't really
> use HAproxy, so I don't know, but I use the F5 big-ip which is
> perfectly capable of testing Internet links behind squid). In you
> scheme, WPAD is being used to tell the clients where the load balancer
> (a webserver with a php script) is, and PAC probably as the answer
> format, which returns a currently valid proxy node address directly to
> the client. But as far as I know, once the client gets the PAC answer,
> it willl not refresh until the browser is restarted, so it might be a
> small problem there.
> 
> But it is a good solution, as proved by your decade of using it, and
> much cheaper than a F5. As for the DNS trick, it is intended to
> increase high availability of the web servers that are serving
> wpad.dat (or your php script), because if it runs on only one
> webserver, at some point no clients will find anything at all.
> 
> Well, there's a lot of ways of doing the same thing, including ucarp,
> squid cache_peer as Amos said... It's just a matter of picking the one
> that fits.
> 
> On Tue, Nov 18, 2014 at 3:31 AM, Jason Haar <Jason_Haar at trimble.com> wrote:
> > On 18/11/14 16:07, Carlos Defoe wrote:
> >> As for my scenario, I also use wpad to configure some exceptions, some
> >> clients that will use a completely different proxy, etc...
> > Our "wpad.dat" is actually a PHP script which tests that the "official"
> > proxy (per client subnet) is actually working (with caching of the
> > results for performance reasons of course), if not it flicks them off to
> > another site's proxy server. Much better than trying to do dynamic DNS
> > tricks with a local HAproxy. ie if you have actually lost local Internet
> > access due to an ISP outage, HAproxy isn't going to help. But if WPAD
> > knows that a WAN-connected proxy is still working - why not point your
> > users at that instead
> >
> > We've been doing this for 10+ years, 99% of the time it's never needed,
> > but when it's needed, it works :-)
> >
> > --
> > Cheers
> >
> > Jason Haar
> > Corporate Information Security Manager, Trimble Navigation Ltd.
> > Phone: +1 408 481 8171
> > PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

web servers providing pac/wpad dont need to be a single point of
failure, given that multiple instances of web servers can be behind a
load balancer, just like squid.  i have this arrangement, and get plenty
of reliability out of it.  it scales well too.

i have setup my VIP for the proxies in such a way that if you hit port
8080 you get load balanced to the pool with all members in it.  if you
hit the VIP on port 8081, you get load balanced to a pool with only the
first proxy in it, 8082 goes to the second proxy, etc.  this allows me
to test each proxy individually, and because the VIP name is the same,
the same kerberos ticket satisfies the auth requests.  at work, we have
F5s as well, and as a service check we attempt to GET some content we
host, and attempt to GET google or cnn.  the check requires that at
least one of the GETs succeed, in order to mark the device up.  i dont
have the external check in my HAProxy configs, but might have to look
into it.

as for my pac/wpad script, i have logic in it to send requests proxied
or unproxied, based on my design or security decisions.  i have logic
for direct access domains, direct access hosts, direct access networks,
proxied domains (forces the use of the proxy, overriding any other
logic), proxied hosts (again, override logic), and hosts that are forced
via a specific proxy by sending the request to a specific port on the
VIP.

the bulk of my access will be proxied, and i return the VIP on port 8080
as the primary proxy, and then ports 8081, 8082, etc as secondary,
tertiary, and so on.  that way the browser will always get all possible
avenues for access, should something be wrong with one or more of the
VIPs.  what i am not sure of is if HAProxy will reply with a RST when no
pool member(s) is/are available for a given VIP/pool.  we have this
setup at work on the F5s, and i'm not sure if i have it in HAProxy (or
if i can do it at all).

i would suggest that if you use a pac/wpad solution, you look into
pactester, which is a google summer of code project that executes pac
files and provides output indicating what actions would be returned to
the browser, given a URL.  so, with my setup if i call pactester and
give it http://www.google.com, it returns to me:

PROXY proxy.bpk2.com:8080; PROXY proxy.bpk2.com:8081; PROXY
proxy.bpk2.com:8082

if i call pactester with http://www.bpk2.com, it returns to me:

DIRECT

with a bit of scripting and a couple of files with URLs in them, i can
quickly evaluate my proxy script, validate the logic and perform a
rudimentary syntax and punctuation check on any changes i make to the
script.



From steve at opendium.com  Tue Nov 18 16:02:39 2014
From: steve at opendium.com (Steve Hill)
Date: Tue, 18 Nov 2014 16:02:39 +0000
Subject: [squid-users] Assertion failure: DestinationIp.cc:60
Message-ID: <546B6D9F.10408@opendium.com>

I'm seeing a lot of this in both 3.4.6 and 3.4.9:

2014/11/18 15:08:48 kid1| assertion failed: DestinationIp.cc:60:
"checklist->conn() && checklist->conn()->clientConnection != NULL"

I've looked through Bugzilla and couldn't see anything regarding this -
is this a known bug?

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From frank at cronomagic.com  Tue Nov 18 17:59:42 2014
From: frank at cronomagic.com (Frank)
Date: Tue, 18 Nov 2014 12:59:42 -0500
Subject: [squid-users] squid-3.4.8  intercept
Message-ID: <546B890E.7000605@cronomagic.com>

     Hi,

     Since upgrading from 3.1.22 to 3.4.8 I have been unable to get the 
transparent mode
to accept my IP.  I am seeing permission denied in the transaction when 
I do a packet dump.
I have read the documentation making changes for 3.4.8.
I even allowed everything and no go.

I also compiled squid and here is my configure script:

./configure \
   --prefix=/usr/share/squid-3.4.8  \
   --libdir=/usr/lib${LIBDIRSUFFIX} \
   --sysconfdir=/etc/squid \
   --localstatedir=/var/log/squid \
   --datadir=/usr/share/squid-3.4.8 \
   --with-pidfile=/var/run/squid/squid.pid \
   --mandir=/usr/man \
   --with-logdir=/var/log/squid \
   --enable-snmp \
   --enable-ipf-transparent \
   --enable-ipfw-transparent
#  --enable-auth="basic" \
#  --enable-basic-auth-helpers="NCSA" \
#  --enable-linux-netfilter \
#  --enable-async-io \
#  --disable-strict-error-checking

My machine the browser is on:

66.159.32.31

The machine that is running squid:

66.159.47.22

Here is my squid.conf

===================================================================================

#
# Recommended minimum configuration:
#

cache_effective_user  squid
cache_effective_group  squid

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src all    # RFC1918 possible internal network
#acl localnet src 66.159.32.0/24        # RFC1918 possible internal network
#acl localnet src 108.161.167.0/24      # RFC1918 possible internal network
#acl localnet src 66.159.47.0/24        # RFC1918 possible internal network
#acl localnet src 127.0.0.0/24  # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
########http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
###########http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
###############http_access allow localhost manager

###############http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
############http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
#http_access deny all
http_access allow all

# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept

always_direct allow all

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /usr/share/squid/cache 100 32 512

# Leave coredumps in the first cache dir
coredump_dir /var/log/squid/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

==================================================================================

And I have configured my browser to use HTTP Proxy 66.159.47.22 Port 3129

I also setup iptables on my machine as follows and that didn't work either. Same permission
denied.


/sbin/iptables -t nat -A OUTPUT -p tcp -s 66.159.32.31 --dport 80 -j DNAT --to 66.159.47.22:3129

Let me know if further info is needed.   Any help would be greatly appreciated.

-- 
Regards,
Frank Torontour
Network Administrator
frank at cronomagic.com
514-341-1579 EXT-214
1-800-427-6012 Ext-214

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141118/620ed9c9/attachment.htm>

From cassiano at polaco.pro.br  Tue Nov 18 18:14:35 2014
From: cassiano at polaco.pro.br (Cassiano Martin)
Date: Tue, 18 Nov 2014 16:14:35 -0200
Subject: [squid-users] squid-3.4.8 intercept
In-Reply-To: <546B890E.7000605@cronomagic.com>
References: <546B890E.7000605@cronomagic.com>
Message-ID: <CAOoxthMPmo6DTtppXR11wJ=dSH0Z0Z4aEpQ8gUpZdzKQNVqJYA@mail.gmail.com>

Thats because you have not set your local network to squid. You have
to allow your network  range 66.159.32.0/24

2014-11-18 15:59 GMT-02:00 Frank <frank at cronomagic.com>:
>     Hi,
>
>     Since upgrading from 3.1.22 to 3.4.8 I have been unable to get the
> transparent mode
> to accept my IP.  I am seeing permission denied in the transaction when I do
> a packet dump.
> I have read the documentation making changes for 3.4.8.
> I even allowed everything and no go.
>
> I also compiled squid and here is my configure script:
>
> ./configure \
>   --prefix=/usr/share/squid-3.4.8  \
>   --libdir=/usr/lib${LIBDIRSUFFIX} \
>   --sysconfdir=/etc/squid \
>   --localstatedir=/var/log/squid \
>   --datadir=/usr/share/squid-3.4.8 \
>   --with-pidfile=/var/run/squid/squid.pid \
>   --mandir=/usr/man \
>   --with-logdir=/var/log/squid \
>   --enable-snmp \
>   --enable-ipf-transparent \
>   --enable-ipfw-transparent
> #  --enable-auth="basic" \
> #  --enable-basic-auth-helpers="NCSA" \
> #  --enable-linux-netfilter \
> #  --enable-async-io \
> #  --disable-strict-error-checking
>
> My machine the browser is on:
>
> 66.159.32.31
>
> The machine that is running squid:
>
> 66.159.47.22
>
> Here is my squid.conf
>
> ===================================================================================
>
> #
> # Recommended minimum configuration:
> #
>
> cache_effective_user  squid
> cache_effective_group  squid
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src all    # RFC1918 possible internal network
> #acl localnet src 66.159.32.0/24        # RFC1918 possible internal network
> #acl localnet src 108.161.167.0/24      # RFC1918 possible internal network
> #acl localnet src 66.159.47.0/24        # RFC1918 possible internal network
> #acl localnet src 127.0.0.0/24  # RFC1918 possible internal network
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> ########http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> ###########http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> ###############http_access allow localhost manager
>
> ###############http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> ############http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
>
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> #http_access deny all
> http_access allow all
>
> # Squid normally listens to port 3128
> http_port 3128
> http_port 3129 intercept
>
> always_direct allow all
>
> # Uncomment and adjust the following to add a disk cache directory.
> cache_dir ufs /usr/share/squid/cache 100 32 512
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/log/squid/cache/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> ==================================================================================
>
> And I have configured my browser to use HTTP Proxy 66.159.47.22 Port 3129
>
> I also setup iptables on my machine as follows and that didn't work either.
> Same permission
> denied.
>
>
> /sbin/iptables -t nat -A OUTPUT -p tcp -s 66.159.32.31 --dport 80 -j DNAT
> --to 66.159.47.22:3129
>
> Let me know if further info is needed.   Any help would be greatly
> appreciated.
>
> --
> Regards,
> Frank Torontour
> Network Administrator
> frank at cronomagic.com
> 514-341-1579 EXT-214
> 1-800-427-6012 Ext-214
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From cassiano at polaco.pro.br  Tue Nov 18 18:35:38 2014
From: cassiano at polaco.pro.br (Cassiano Martin)
Date: Tue, 18 Nov 2014 16:35:38 -0200
Subject: [squid-users] squid-3.4.8 intercept
In-Reply-To: <546B8FA1.6050200@cronomagic.com>
References: <546B890E.7000605@cronomagic.com>
 <CAOoxthMPmo6DTtppXR11wJ=dSH0Z0Z4aEpQ8gUpZdzKQNVqJYA@mail.gmail.com>
 <546B8FA1.6050200@cronomagic.com>
Message-ID: <CAOoxthN-69bw7NifP_7zKqU3Z2V5sY0NWmUy49v5sfbDG++Nxg@mail.gmail.com>

Spam detection software, running on the system "master.squid-cache.org",
has identified this incoming email as possible spam.  The original
message has been attached to this so you can view it or label
similar future email.  If you have any questions, see
@@CONTACT_ADDRESS@@ for details.

Content preview:  From what I know, localnet resumes to 10.x.x.x 172.16.x.x
  and 192.168.x.x RFC net ranges. You might try creating a new ACL and allow
   it. acl mynet src 66.159.32.0/24 http_access allow mynet and check your browser,
   you should point it to port 3128 and not 3129, which is set to iptables intercept
   mode only. [...] 

Content analysis details:   (6.5 points, 5.0 required)

 pts rule name              description
---- ---------------------- --------------------------------------------------
 0.0 RCVD_IN_DNSWL_BLOCKED  RBL: ADMINISTRATOR NOTICE: The query to DNSWL
                            was blocked.  See
                            http://wiki.apache.org/spamassassin/DnsBlocklists#dnsbl-block
                             for more information.
                            [187.113.211.85 listed in list.dnswl.org]
 1.6 RCVD_IN_BRBL_LASTEXT   RBL: No description available.
                            [187.113.211.85 listed in bb.barracudacentral.org]
 3.6 RCVD_IN_PBL            RBL: Received via a relay in Spamhaus PBL
                            [187.113.211.85 listed in zen.spamhaus.org]
 0.0 URIBL_BLOCKED          ADMINISTRATOR NOTICE: The query to URIBL was blocked.
                            See
                            http://wiki.apache.org/spamassassin/DnsBlocklists#dnsbl-block
                             for more information.
                            [URIs: cronomagic.com]
 0.0 UNPARSEABLE_RELAY      Informational: message has unparseable relay lines
 1.3 RDNS_NONE              Delivered to internal network by a host with no rDNS


-------------- next part --------------
An embedded message was scrubbed...
From: Cassiano Martin <cassiano at polaco.pro.br>
Subject: Re: [squid-users] squid-3.4.8 intercept
Date: Tue, 18 Nov 2014 16:35:38 -0200
Size: 7727
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141118/5603cd2a/attachment.eml>

From Jason_Haar at trimble.com  Tue Nov 18 20:44:31 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 19 Nov 2014 09:44:31 +1300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <1416314377.4763.35.camel@desktop.bpk2.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com>
Message-ID: <546BAFAF.2070502@trimble.com>

On 19/11/14 01:39, Brendan Kearney wrote:
> i would suggest that if you use a pac/wpad solution, you look into
> pactester, which is a google summer of code project that executes pac
> files and provides output indicating what actions would be returned to
> the browser, given a URL. 
couldn't agree more. We have it built into our QA to run before we ever
roll out any change to our WPAD php script (a bug in there means
everyone loses Internet access - so we have to be careful).

Auto-generating a PAC script per client allows us to change behaviour
based on User-Agent, client IP, proxy and destination - and allows us to
control what web services should be DIRECT and what should be proxied.
There is no other way of achieving those outcomes.

Oh yes, and now that both Chrome and Firefox support proxies over HTTPS,
I'm starting to ponder putting up some form of proxy on the Internet for
our staff to use (authenticated of course!) - WPAD makes that something
we could implement with no client changes - pretty cool :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From eliezer at ngtech.co.il  Tue Nov 18 21:15:54 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Nov 2014 23:15:54 +0200
Subject: [squid-users] squid-3.4.8  intercept
In-Reply-To: <546B890E.7000605@cronomagic.com>
References: <546B890E.7000605@cronomagic.com>
Message-ID: <546BB70A.1010906@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Frank,

To understand the issue better I am missing couple things.
I filtered the squid.conf (which is a basic thing to do) and the
content can be seen here:
http://www1.ngtech.co.il/paste/1216/

It seems like you do not understand what and how.
the localnet you are using is kind of illegal and probably the reason
it's not working for you.
What you should do is to remove or comment the line "acl localnet src all"
and then remove\comment all http_access lines and add only one line:
http_access allow all
##END

when you have used this and only this config line you should be able
to use the proxy from anywhere on the ipv4 planet and if you have ipv6
also from there.
So use it with at least a basic firewall to not make this proxy public
and to black list your server.

This test is one of the biggest binary search while troubleshooting
with squid.

As I have mentioned before you should also provide your OS.
If you are using CentOS for example there is a nice RPM that will
might reduce your self compilation efforts.

All The Bests,
Eliezer

On 11/18/2014 07:59 PM, Frank wrote:
> Hi,
> 
> Since upgrading from 3.1.22 to 3.4.8 I have been unable to get the 
> transparent mode to accept my IP.  I am seeing permission denied in
> the transaction when I do a packet dump. I have read the
> documentation making changes for 3.4.8. I even allowed everything
> and no go.
> 
> I also compiled squid and here is my configure script:
> 
> ./configure \ --prefix=/usr/share/squid-3.4.8  \ 
> --libdir=/usr/lib${LIBDIRSUFFIX} \ --sysconfdir=/etc/squid \ 
> --localstatedir=/var/log/squid \ --datadir=/usr/share/squid-3.4.8
> \ --with-pidfile=/var/run/squid/squid.pid \ --mandir=/usr/man \ 
> --with-logdir=/var/log/squid \ --enable-snmp \ 
> --enable-ipf-transparent \ --enable-ipfw-transparent #
> --enable-auth="basic" \ #  --enable-basic-auth-helpers="NCSA" \ #
> --enable-linux-netfilter \ #  --enable-async-io \ #
> --disable-strict-error-checking
> 
> My machine the browser is on:
> 
> 66.159.32.31
> 
> The machine that is running squid:
> 
> 66.159.47.22
> 
> Here is my squid.conf
> 
> ===================================================================================
>
> 
> 
> # # Recommended minimum configuration: #
> 
> cache_effective_user  squid cache_effective_group  squid
> 
> # Example rule allowing access from your local networks. # Adapt to
> list your (internal) IP networks from where browsing # should be
> allowed acl localnet src all    # RFC1918 possible internal
> network #acl localnet src 66.159.32.0/24        # RFC1918 possible
> internal network #acl localnet src 108.161.167.0/24      # RFC1918
> possible internal network #acl localnet src 66.159.47.0/24        #
> RFC1918 possible internal network #acl localnet src 127.0.0.0/24  #
> RFC1918 possible internal network
> 
> acl SSL_ports port 443 acl Safe_ports port 80          # http acl
> Safe_ports port 21          # ftp acl Safe_ports port 443         #
> https acl Safe_ports port 70          # gopher acl Safe_ports port
> 210         # wais acl Safe_ports port 1025-65535  # unregistered
> ports acl Safe_ports port 280         # http-mgmt acl Safe_ports
> port 488         # gss-http acl Safe_ports port 591         #
> filemaker acl Safe_ports port 777         # multiling http acl
> CONNECT method CONNECT
> 
> # # Recommended minimum Access Permission configuration: # # Deny
> requests to certain unsafe ports ########http_access deny
> !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports 
> ###########http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost 
> ###############http_access allow localhost manager
> 
> ###############http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect
> innocent # web applications running on the proxy server who think
> the only # one who can access services on "localhost" is a local
> user ############http_access deny to_localhost
> 
> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
> # # Example rule allowing access from your local networks. # Adapt
> localnet in the ACL section to list your (internal) IP networks #
> from where browsing should be allowed
> 
> http_access allow localnet http_access allow localhost
> 
> # And finally deny all other access to this proxy #http_access deny
> all http_access allow all
> 
> # Squid normally listens to port 3128 http_port 3128 http_port 3129
> intercept
> 
> always_direct allow all
> 
> # Uncomment and adjust the following to add a disk cache
> directory. cache_dir ufs /usr/share/squid/cache 100 32 512
> 
> # Leave coredumps in the first cache dir coredump_dir
> /var/log/squid/cache/squid
> 
> # # Add any of your own refresh_pattern entries above these. # 
> refresh_pattern ^ftp:           1440    20%     10080 
> refresh_pattern ^gopher:        1440    0%      1440 
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0 refresh_pattern .
> 0       20%     4320
> 
> ==================================================================================
>
> 
> 
> And I have configured my browser to use HTTP Proxy 66.159.47.22
> Port 3129
> 
> I also setup iptables on my machine as follows and that didn't
> work either. Same permission denied.
> 
> 
> /sbin/iptables -t nat -A OUTPUT -p tcp -s 66.159.32.31 --dport 80
> -j DNAT --to 66.159.47.22:3129
> 
> Let me know if further info is needed.   Any help would be greatly 
> appreciated.
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUa7cKAAoJENxnfXtQ8ZQU1vgH+wRtttwPLpxAgF0+wma0u7w4
iqwcZgHG2+996qpFOpI8tzDPo+xKQFPkJX3SthDU2AUnOkDYMT1JplT4vmCik6d5
4lfNlC9kAqMVTq2iF1WGbujgfbFeceImOxP0gfuS3ox5XjXeWUmdmGi57kzNGLMn
cZ0Ct+YYzyvMWebi7bpDerkEYD+5Qr4zAk1n16Xw3d5OCBOclFsvTZlI3ivZ/9Y6
kFVBKFI3q7/Ae8tpmi7jtN6uvebM9QZttyBwHgF/qtfY/QiloFilnEbZQQNRde1V
OZMxUF2rDRDCR+iOBJMIKFmNzn7YbPjF/g02V1/h2hZMF4OY3nTA1BbGO7BVhRw=
=Lpd/
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Tue Nov 18 22:13:16 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 19 Nov 2014 00:13:16 +0200
Subject: [squid-users] Squid 3.4.9 RPM release
In-Reply-To: <546A7222.90001@ngtech.co.il>
References: <5458596C.8060203@treenet.co.nz> <546A7222.90001@ngtech.co.il>
Message-ID: <546BC47C.9010008@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


On 11/18/2014 12:09 AM, Eliezer Croitoru wrote:
> HTML version at: http://www1.ngtech.co.il/repo/release-3.4.9.html I
> am happy to release the new RPMs of squid 3.4.9 and 3.5.0.2 beta
> for Centos 6.6 64bit.

> All The Bests, Eliezer Croitoru


Addition of the centos i686 rpm release.

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUa8R8AAoJENxnfXtQ8ZQU5PUH+wZGrwf3tBIB8DwfQ7CAPZTV
QsFlIyW7vazHkcu3vXinvNyraKi71m/WTUpHViFbPI7rmI8zc6Jy1760EAxG7eoV
8HrzKHyPoCSvtpF/djbWif7ol+9496rFeZU8p3w34PMiFuSpWGb0Uv8rDUe9nY5Q
I310Ysr33I/NvKZ0Bm6VmRYZKNfxCsizFcx/c54oIr0vvfCz382a9YOKUBHaMJg/
eOeih7VxHd1jUlhNSVjnc4FI7ZNKVMyP3wmxu7PJYhw3fE0C92RB9Gyqkr0/sxow
rPtylyab8ORzOQXgfkDASyDdPGrfS8lqaKVUBMqkqv6tEgJereRfHI7PtUKlGdg=
=3OM3
-----END PGP SIGNATURE-----


From phalen at gmail.com  Wed Nov 19 03:39:58 2014
From: phalen at gmail.com (Kendrick .)
Date: Tue, 18 Nov 2014 22:39:58 -0500
Subject: [squid-users] Squid url_rewrite_program crashing/stalling
Message-ID: <CAHuVzom+BUc22457HvdNBA9Youwt_V+-NeLhwATpDiCnJtmbuA@mail.gmail.com>

I am using centos7 with a 3.4 squid package.  I have tried a
commercial and a couple user made url_rewrite_program items.  They
have all not ran properly.  I had to disable selinux as it was not
playing nice with several things.

How does one go about trouble shooting what squid/url_rewrite_program
are doing?  I have upped the debug levels and not found any thing
usefull.  9 spewed out so much that i wouldent see the tree for the
forest though.  If a url does not hit the rewrite filter it works just
fine but any url that matches the patern times out. Nothing showed up
in any of my logs either.

Thanks
Kendrick.


From squid3 at treenet.co.nz  Wed Nov 19 05:04:54 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Nov 2014 18:04:54 +1300
Subject: [squid-users] Squid url_rewrite_program crashing/stalling
In-Reply-To: <CAHuVzom+BUc22457HvdNBA9Youwt_V+-NeLhwATpDiCnJtmbuA@mail.gmail.com>
References: <CAHuVzom+BUc22457HvdNBA9Youwt_V+-NeLhwATpDiCnJtmbuA@mail.gmail.com>
Message-ID: <546C24F6.2050208@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 4:39 p.m., Kendrick . wrote:
> I am using centos7 with a 3.4 squid package.  I have tried a 
> commercial and a couple user made url_rewrite_program items.  They 
> have all not ran properly.  I had to disable selinux as it was not 
> playing nice with several things.
> 
> How does one go about trouble shooting what
> squid/url_rewrite_program are doing?  I have upped the debug levels
> and not found any thing usefull.  9 spewed out so much that i
> wouldent see the tree for the forest though.  If a url does not hit
> the rewrite filter it works just fine but any url that matches the
> patern times out. Nothing showed up in any of my logs either.

Run the helper from the command line manually entering the URLs.
Whatever is goig wrong should show up then.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbCT1AAoJELJo5wb/XPRjBpYIAMCGFRRLkZoOUVdqhcNJPT8M
On/h+V0EEImjhQBbItVWlpSkh7+v72VV2FemMw3UVXpsB0VFkkAGuxc2hekNt+6R
tCKgXc0mKENKlVfCpYwC9Zjj1eAIN5s4zpgNK87xpcTwziWK068U2NvEl+jDcqXE
Le+qqJOb6SIyfP2jojOTwiMZptrPb3yA1DJRxHeA31zWOLYZAgbLm+q8hQqJWLsN
kQu9jOU6hKLwFfqMbXN3OT6bmas7Mp3HKC3jnTROF6+gd5RBq2SNiwWGPGGznb9J
C3OfVCmJbDvUbbP+FUjNBwEVQeVG8keGuZ1USlx7+/EFl3XPkoWAsqW88ZuzPe4=
=7aQW
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 19 05:18:19 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Nov 2014 18:18:19 +1300
Subject: [squid-users] squid-3.4.8  intercept
In-Reply-To: <546B890E.7000605@cronomagic.com>
References: <546B890E.7000605@cronomagic.com>
Message-ID: <546C281B.5010103@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 6:59 a.m., Frank wrote:
> Hi,
> 
> Since upgrading from 3.1.22 to 3.4.8 I have been unable to get the 
> transparent mode to accept my IP.  I am seeing permission denied in
> the transaction when I do a packet dump. I have read the
> documentation making changes for 3.4.8. I even allowed everything
> and no go.
> 
> I also compiled squid and here is my configure script:
> 
> ./configure \ --prefix=/usr/share/squid-3.4.8  \ 
> --libdir=/usr/lib${LIBDIRSUFFIX} \ --sysconfdir=/etc/squid \ 
> --localstatedir=/var/log/squid \ --datadir=/usr/share/squid-3.4.8
> \ --with-pidfile=/var/run/squid/squid.pid \ --mandir=/usr/man \ 
> --with-logdir=/var/log/squid \ --enable-snmp \ 
> --enable-ipf-transparent \ --enable-ipfw-transparent #
> --enable-auth="basic" \ #  --enable-basic-auth-helpers="NCSA" \ #
> --enable-linux-netfilter \ #  --enable-async-io \ #
> --disable-strict-error-checking
> 
> My machine the browser is on:
> 
> 66.159.32.31
> 
> The machine that is running squid:
> 
> 66.159.47.22
> 
> Here is my squid.conf
> 
> ===================================================================================
>
> 
> 
> # # Recommended minimum configuration: #
> 
> cache_effective_user  squid cache_effective_group  squid
> 
> # Example rule allowing access from your local networks. # Adapt to
> list your (internal) IP networks from where browsing # should be
> allowed acl localnet src all    # RFC1918 possible internal
> network

You are thus naming yourself the owner and operator with the *entire*
Internet as your LAN.


> #acl localnet src 66.159.32.0/24        # RFC1918 possible internal
> network #acl localnet src 108.161.167.0/24      # RFC1918 possible
> internal network #acl localnet src 66.159.47.0/24        # RFC1918
> possible internal network #acl localnet src 127.0.0.0/24  # RFC1918
> possible internal network
> 
> acl SSL_ports port 443 acl Safe_ports port 80          # http acl
> Safe_ports port 21          # ftp acl Safe_ports port 443         #
> https acl Safe_ports port 70          # gopher acl Safe_ports port
> 210         # wais acl Safe_ports port 1025-65535  # unregistered
> ports acl Safe_ports port 280         # http-mgmt acl Safe_ports
> port 488         # gss-http acl Safe_ports port 591         #
> filemaker acl Safe_ports port 777         # multiling http acl
> CONNECT method CONNECT
> 
> # # Recommended minimum Access Permission configuration: # # Deny
> requests to certain unsafe ports ########http_access deny
> !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports 
> ###########http_access deny CONNECT !SSL_ports
> 

Undo the above disabling. When accepting traffic from anywhere on the
Internt it becomes more critical than ever that you block malicious
port usage.


> # Only allow cachemgr access from localhost 
> ###############http_access allow localhost manager
> 
> ###############http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect
> innocent # web applications running on the proxy server who think
> the only # one who can access services on "localhost" is a local
> user ############http_access deny to_localhost
> 
> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
> # # Example rule allowing access from your local networks. # Adapt
> localnet in the ACL section to list your (internal) IP networks #
> from where browsing should be allowed
> 
> http_access allow localnet http_access allow localhost
> 
> # And finally deny all other access to this proxy #http_access deny
> all http_access allow all

You already allow localnet (aka 'all'). Undo this.

> 
> # Squid normally listens to port 3128 http_port 3128 http_port 3129
> intercept
> 
> always_direct allow all

This is irrelevant. You have no cache_peer setting to bypass.

<snip>
> ==================================================================================
>
> 
> 
> And I have configured my browser to use HTTP Proxy 66.159.47.22
> Port 3129

Boom!. There is the problem #1.

* "http_port 3128" is for explicitly configured browsers.

* "http_port 3129 intercept" is for NAT intercepted traffic.

You MUST NOT combine the two traffic types on one port. The
reject/deny you are seeing are just the tip of the iceberg of problems
when that happens.

> 
> I also setup iptables on my machine as follows and that didn't
> work either. Same permission denied.
> 
> 
> /sbin/iptables -t nat -A OUTPUT -p tcp -s 66.159.32.31 --dport 80
> -j DNAT --to 66.159.47.22:3129

DNAT/REDIRECT of traffic entering Squid *MUST* be performed on the
Squid machine itself.

Here is the config you shoudld be using:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>


Use normal routing to get the traffic from your users machine(s) to
the proxy machine. There are some examples here:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute>

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbCgJAAoJELJo5wb/XPRjvioIALkqGbHFtXCZpww3/i3IzuAC
PpcRaYAXSqJxKcdncoSxfBtHiig1OxyARdSDqJ/PWRLEuVBkOG/uL5fHD19gwRaN
6aOsO/IZu8+Fg7Fl+DLVkoVsOKeRj4guyJOBVyPAQXRyOEzpBe6N6WjCpDHVzh3c
iT5qVbHs9iHDZjy809OqUB5+bzL/PmgKeDrbnYpUlBKniehgP7LL86aim3tE9mc7
Kr00oltPEEGZ/gyuAp9zy2DiLYj+IYlE9NrkZE9/larCrZNLl4Vd4rRznxqhowQQ
6nJslTpOm63zg/f4UwGzOfCW7XdqZC0ZXunn04rQKbN/Z18teWvare0al/HUxyA=
=2boS
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 19 05:29:59 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Nov 2014 18:29:59 +1300
Subject: [squid-users] Assertion failure: DestinationIp.cc:60
In-Reply-To: <546B6D9F.10408@opendium.com>
References: <546B6D9F.10408@opendium.com>
Message-ID: <546C2AD7.9080905@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 5:02 a.m., Steve Hill wrote:
> I'm seeing a lot of this in both 3.4.6 and 3.4.9:
> 
> 2014/11/18 15:08:48 kid1| assertion failed: DestinationIp.cc:60: 
> "checklist->conn() && checklist->conn()->clientConnection != NULL"
> 
> I've looked through Bugzilla and couldn't see anything regarding
> this - is this a known bug?
> 

Possibly not.

What is your config? In particular anything using ACLs.


Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbCrXAAoJELJo5wb/XPRjqEgH/2aEfHYZHjZpjJ4qQZ2oIz32
s5L3jHgvFEM5cTwLwsbsrDzKJXtb7vAJXq15fNIl3GniZ6e6d2tAEh7ZyoRwDpxp
Ik91QtcKOXJyieqF3jZxq4oeYnuOwEdtaFLmhX7I6B5qCGxBkJp2Pr2tzs+onQmw
mWi3IFTScA7yKKixWLsFTnyvcfPVS/pCa5BUawD9CbR6gi7KyjrAVvzA6dFrpGbC
y3Txl890MlABXPerO8FAd1z9Wzslh6LTr/atDOc3SSaYfgOeEfNPz+lfJ1tg28pU
wOmM9aPKliluY/fWDRBPqqaDX/OkJPtQqsij8eF51RXqTdcc8NKdZEvFjmvY3Ao=
=rKzX
-----END PGP SIGNATURE-----


From santosh.pai at vigyanlabs.com  Wed Nov 19 09:36:49 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Wed, 19 Nov 2014 01:36:49 -0800 (PST)
Subject: [squid-users] Unable to get username in logs for access denied(HTTP
	407)
Message-ID: <1416389809018-4668460.post@n4.nabble.com>

Hello Team,

We have setup squid proxy server and the backend authentication is through
Open LDAP . each user is given with an unique id and password . We have been
tracking the logs for accessdenied results , it has been found that squid
hasn't been logging the username ,in the place there is  - HIER_NONE/- ,
below are sample log .What could be the reason ?

1414651057.911      0 192.168.4.101 TCP_DENIED/407 3787 CONNECT
ie.search.yahoo.com:443 - HIER_NONE/- text/html
1414651057.913      0 192.168.4.101 TCP_DENIED/407 3787 CONNECT
ie.search.yahoo.com:443 - HIER_NONE/- text/html
1414651057.922      0 192.168.4.101 TCP_DENIED/407 3787 CONNECT
ie.search.yahoo.com:443 - HIER_NONE/- text/html
1414651074.126      1 192.168.4.101 TCP_DENIED/407 4394 GET
http://www.search.ask.com/? - HIER_NONE/- text/html
1414651082.981      0 192.168.4.101 TCP_DENIED/407 4572 GET
http://www.search.ask.com/? irfan HIER_NONE/- text/html
1414651087.111      0 192.168.4.101 TCP_DENIED/407 4572 GET
http://www.search.ask.com/? irfan HIER_NONE/- text/html
1414651093.587    987 192.168.4.101 TCP_MISS/200 8357 GET
http://www.search.ask.com/? irfan HIER_DIRECT/184.30.62.3 text/html
1414651094.041     10 192.168.4.101 TCP_MISS/204 360 GET
http://b.scorecardresearch.com/b? irfan HIER_DIRECT/122.178.225.18 -
1414651101.412      1 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.83:443 - HIER_NONE/- text/html
1414651101.412      1 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.83:443 - HIER_NONE/- text/html
1414651103.077      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.83:443 - HIER_NONE/- text/html
1414651108.953      0 192.168.4.101 TCP_DENIED/407 4165 GET
http://www.google.co.in/ - HIER_NONE/- text/html
1414651116.389    116 192.168.4.101 TCP_MISS/302 684 GET
http://www.google.co.in/ irfan HIER_DIRECT/74.125.236.88 text/html
1414651117.243      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.77:443 - HIER_NONE/- text/html
1414651117.291      1 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.77:443 - HIER_NONE/- text/html
1414651117.291      1 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.77:443 - HIER_NONE/- text/html
1414651120.944      1 192.168.4.101 TCP_DENIED/407 3519 CONNECT
62.128.100.131:443 - HIER_NONE/- text/html
1414651120.944      1 192.168.4.101 TCP_DENIED/407 3519 CONNECT
62.128.100.131:443 - HIER_NONE/- text/html
1414651123.223      0 192.168.4.101 TCP_DENIED/407 3519 CONNECT
62.128.100.109:443 - HIER_NONE/- text/html
1414651127.362      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
38.113.165.86:443 - HIER_NONE/- text/html
1414651128.701  12257 192.168.4.101 TCP_MISS/200 38964 CONNECT
www.google.co.in:443 irfan HIER_DIRECT/74.125.236.88 -
1414651129.651      0 192.168.4.101 TCP_DENIED/407 3522 CONNECT
202.177.216.227:443 - HIER_NONE/- text/html
1414651131.947      0 192.168.4.101 TCP_DENIED/407 3522 CONNECT
202.177.216.236:443 - HIER_NONE/- text/html
1414651132.294      0 192.168.4.101 TCP_DENIED/407 4165 GET
http://www.google.co.in/ - HIER_NONE/- text/html
1414651132.452    104 192.168.4.101 TCP_MISS/302 684 GET
http://www.google.co.in/ irfan HIER_DIRECT/74.125.236.88 text/html
1414651134.307      0 192.168.4.101 TCP_DENIED/407 3522 CONNECT
202.177.216.230:443 - HIER_NONE/- text/html
1414651162.016      0 192.168.4.101 TCP_DENIED/407 3719 CONNECT
iecvlist.microsoft.com:443 - HIER_NONE/- text/html
1414651221.771      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
62.128.100.37:443 - HIER_NONE/- text/html
1414651224.051      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
62.128.100.45:443 - HIER_NONE/- text/html
1414651231.091      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
62.128.100.45:443 - HIER_NONE/- text/html
1414651247.463      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
62.128.100.35:443 - HIER_NONE/- text/html
1414651247.474      0 192.168.4.101 TCP_DENIED/407 3516 CONNECT
62.128.100.45:443 - HIER_NONE/- text/html




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Unable-to-get-username-in-logs-for-access-denied-HTTP-407-tp4668460.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Nov 19 09:57:17 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Nov 2014 22:57:17 +1300
Subject: [squid-users] Unable to get username in logs for access
 denied(HTTP 407)
In-Reply-To: <1416389809018-4668460.post@n4.nabble.com>
References: <1416389809018-4668460.post@n4.nabble.com>
Message-ID: <546C697D.3050008@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 10:36 p.m., santosh wrote:
> Hello Team,
> 
> We have setup squid proxy server and the backend authentication is
> through Open LDAP . each user is given with an unique id and
> password . We have been tracking the logs for accessdenied results
> , it has been found that squid hasn't been logging the username ,in
> the place there is  - HIER_NONE/- , below are sample log .What
> could be the reason ?
> 

There is no authenticated username for that transaction.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbGl8AAoJELJo5wb/XPRjs/MIANgHDrIcA6lB0uO0Zk1zKWnM
zACOs7yB7hZfwwdkTG+CkG2W99cRLdy+o22B4NHpujwpXV1jlKbQY7BSKLe2rX+Y
YNlePWbWOr8uYXOF168rlS0nXde98WGcCpD/N2+B1gGuaHbp3+YpRT6CW9nQwf6p
rTNT9D8jFMmmxJoPYP3nSeJAapkj4LXL1sq3I0WK6z9btbzxSQOZcN0IPleqi6MF
G3BgX2Rzn+o3xduU2UdYI9f0xBPHoELt9iNDUDPHyLAu+o3+uys1GY6RvsgVrSxq
Gl5Qm0VFM9NwU3z8IPAQk5EMyWRWNP1yDMYr4qzq/paAiOzTvvrQeVPwwoPGoxs=
=x9lk
-----END PGP SIGNATURE-----


From santosh.pai at vigyanlabs.com  Wed Nov 19 10:19:51 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Wed, 19 Nov 2014 02:19:51 -0800 (PST)
Subject: [squid-users] Unable to get username in logs for access
	denied(HTTP 407)
In-Reply-To: <546C697D.3050008@treenet.co.nz>
References: <1416389809018-4668460.post@n4.nabble.com>
 <546C697D.3050008@treenet.co.nz>
Message-ID: <1416392391448-4668462.post@n4.nabble.com>

I have got fresh set of logs my username is spai and i 'm already
authenticated and one of the site www.flipkart.com is blocked and i accessed
it for testing purpose ,but the user name is still not shown . 


1416392601.192   2427 192.168.4.7 TCP_MISS/302 874 GET
http://cc.chango.com/c/o? spai HIER_DIRECT/184.30.51.146 text/html
1416392601.243     46 192.168.4.7 TCP_MISS/302 1100 GET
http://cm.g.doubleclick.net/pixel? spai HIER_DIRECT/74.125.236.185 text/html
1416392601.808    562 192.168.4.7 TCP_MISS/200 704 GET
http://gcm.chango.com/collector/relator? spai HIER_DIRECT/173.192.202.135
image/gif
1416392605.309 209269 192.168.4.12 TCP_MISS/200 5107 CONNECT
0.client-channel.google.com:443 - HIER_DIRECT/74.125.130.189 -
1416392606.919  23334 192.168.4.6 TCP_MISS/200 2074 GET
http://www.cricbuzz.com/cbz_pub/fetch? irfan HIER_DIRECT/119.81.109.21
application/octet-stream
1416392607.728      0 192.168.4.7 TCP_DENIED/403 3945 GET
http://www.flipkart.com/ - HIER_NONE/- text/html
1416392607.928     24 192.168.4.7 TCP_HIT/200 13092 GET
http://www.squid-cache.org/Artwork/SN.png spai HIER_NONE/- image/png
1416392607.945      0 192.168.4.7 TCP_DENIED/403 3839 GET
http://www.flipkart.com/favicon.ico - HIER_NONE/- text/html



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Unable-to-get-username-in-logs-for-access-denied-HTTP-407-tp4668460p4668462.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gkinkie at gmail.com  Wed Nov 19 11:17:51 2014
From: gkinkie at gmail.com (Kinkie)
Date: Wed, 19 Nov 2014 12:17:51 +0100
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <546BAFAF.2070502@trimble.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com>
 <546BAFAF.2070502@trimble.com>
Message-ID: <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>

One word of caution: pactester uses the Firefox JavaScript engine, which is
more forgiving than MSIE's. So while it is a very useful tool, it may let
some errors slip through.
On Nov 18, 2014 9:45 PM, "Jason Haar" <Jason_Haar at trimble.com> wrote:

> On 19/11/14 01:39, Brendan Kearney wrote:
> > i would suggest that if you use a pac/wpad solution, you look into
> > pactester, which is a google summer of code project that executes pac
> > files and provides output indicating what actions would be returned to
> > the browser, given a URL.
> couldn't agree more. We have it built into our QA to run before we ever
> roll out any change to our WPAD php script (a bug in there means
> everyone loses Internet access - so we have to be careful).
>
> Auto-generating a PAC script per client allows us to change behaviour
> based on User-Agent, client IP, proxy and destination - and allows us to
> control what web services should be DIRECT and what should be proxied.
> There is no other way of achieving those outcomes.
>
> Oh yes, and now that both Chrome and Firefox support proxies over HTTPS,
> I'm starting to ponder putting up some form of proxy on the Internet for
> our staff to use (authenticated of course!) - WPAD makes that something
> we could implement with no client changes - pretty cool :-)
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141119/d3ed3b27/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 19 11:44:07 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Nov 2014 00:44:07 +1300
Subject: [squid-users] Unable to get username in logs for access
 denied(HTTP 407)
In-Reply-To: <1416392391448-4668462.post@n4.nabble.com>
References: <1416389809018-4668460.post@n4.nabble.com>
 <546C697D.3050008@treenet.co.nz> <1416392391448-4668462.post@n4.nabble.com>
Message-ID: <546C8287.5090809@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 19/11/2014 11:19 p.m., santosh wrote:
> I have got fresh set of logs my username is spai and i 'm already 
> authenticated and one of the site www.flipkart.com is blocked and i
> accessed it for testing purpose ,but the user name is still not
> shown .

No login was necessary to deny those requests. So there is no need for
Squid to waste time decoding the HTTP headers where the credentials
were stored.

If you need the credentials to always be logged then move the blocked
sites denial down below the http_access lines which require
authentication. Note that this will slow your proxy down as it does
all the extra credentials checking work.


Something like so:
  http_access deny !auth
  http_access deny blockedSites
  ...

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbIKHAAoJELJo5wb/XPRjmj4H/0c3vhjMxaTtYNFmBjO6VVjK
/UQpLf2RMkK0YeEx4E+014vKftv5XUGIQhAEplJU0vb5DasHgml/jiO4hh5Gzgw5
PcnZLYWlpMaxmXqb6AqKedS5PfHJnEGRszIMzRuwhwz6F7XM5XgGsAexbtyeblnI
0ishLx+01OU7xh1Bh3pQfbRuwnvEpCpIJWLtaMyP96i3UP3uD+s0TOcfIeOH4irZ
ZJCHePyOkvFaBYVqd4Og4rJBmTuh4+dzZQTRHgH8DO+4+ERW4388NKg4aUFDL35L
WMCn/5n2X1BJhy+Ywswh+ECkM1xZp+EChl0QP5pYmU/mJnh637M8oAHWgf1r8Uw=
=CJlV
-----END PGP SIGNATURE-----


From bpk678 at gmail.com  Wed Nov 19 13:11:44 2014
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 19 Nov 2014 08:11:44 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com>
 <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
Message-ID: <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>

Yes and it seems java is even more sensitive.  I had an array member
defined on a line that was not terminated with a semicolon and browsers did
not throw errors, but java did.  Pactester did not catch this.  Missing
curly braces and I think quotes are caught.

Also of note, you have to set the content type header for a pac file or
else you run into weird issues.  I found that browsers are forgiving and
will execute the script and take its output if the header is not set.
Flash does not do this.  It might call for the script but does not use it
if the Content-Type header is not set to
"application/x-ns-proxy-autoconfig".

GoToMeeting has also pissed me off.  The client parses the script and takes
any value found in it, before executing the script and taking the output of
the execution. This has the result of finding inappropriate proxies to use,
when you are in a corporate environment and have proxies dedicated to
client access or other functions that should not be leveraged in all
cases.  I got their technical team on a call because we have a large citrix
install base (both products have the same parent company) and complained to
no avail.  I had to write a doc on how to correct the client config for
anyone needing to use GoTo... products.
On Nov 19, 2014 6:18 AM, "Kinkie" <gkinkie at gmail.com> wrote:

> One word of caution: pactester uses the Firefox JavaScript engine, which
> is more forgiving than MSIE's. So while it is a very useful tool, it may
> let some errors slip through.
> On Nov 18, 2014 9:45 PM, "Jason Haar" <Jason_Haar at trimble.com> wrote:
>
>> On 19/11/14 01:39, Brendan Kearney wrote:
>> > i would suggest that if you use a pac/wpad solution, you look into
>> > pactester, which is a google summer of code project that executes pac
>> > files and provides output indicating what actions would be returned to
>> > the browser, given a URL.
>> couldn't agree more. We have it built into our QA to run before we ever
>> roll out any change to our WPAD php script (a bug in there means
>> everyone loses Internet access - so we have to be careful).
>>
>> Auto-generating a PAC script per client allows us to change behaviour
>> based on User-Agent, client IP, proxy and destination - and allows us to
>> control what web services should be DIRECT and what should be proxied.
>> There is no other way of achieving those outcomes.
>>
>> Oh yes, and now that both Chrome and Firefox support proxies over HTTPS,
>> I'm starting to ponder putting up some form of proxy on the Internet for
>> our staff to use (authenticated of course!) - WPAD makes that something
>> we could implement with no client changes - pretty cool :-)
>>
>> --
>> Cheers
>>
>> Jason Haar
>> Corporate Information Security Manager, Trimble Navigation Ltd.
>> Phone: +1 408 481 8171
>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141119/1f958a1c/attachment.htm>

From codemarauder at gmail.com  Wed Nov 19 13:32:41 2014
From: codemarauder at gmail.com (Nishant Sharma)
Date: Wed, 19 Nov 2014 19:02:41 +0530
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com> <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
 <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
Message-ID: <0A678568-D54C-4664-8262-2D25997441F1@gmail.com>



On 19 November 2014 6:41:44 pm IST, brendan kearney <bpk678 at gmail.com> wrote:
>Yes and it seems java is even more sensitive.  I had an array member
>defined on a line that was not terminated with a semicolon and browsers
>did
>not throw errors, but java did.  Pactester did not catch this.  Missing
>curly braces and I think quotes are caught.
>
>Also of note, you have to set the content type header for a pac file or
>else you run into weird issues.  I found that browsers are forgiving
>and
>will execute the script and take its output if the header is not set.
>Flash does not do this.  It might call for the script but does not use
>it
>if the Content-Type header is not set to
>"application/x-ns-proxy-autoconfig".
>
>GoToMeeting has also pissed me off.  The client parses the script and
>takes
>any value found in it, before executing the script and taking the
>output of
>the execution. This has the result of finding inappropriate proxies to
>use,
>when you are in a corporate environment and have proxies dedicated to
>client access or other functions that should not be leveraged in all
>cases.  I got their technical team on a call because we have a large
>citrix
>install base (both products have the same parent company) and
>complained to
>no avail.  I had to write a doc on how to correct the client config for
>anyone needing to use GoTo... products.
>On Nov 19, 2014 6:18 AM, "Kinkie" <gkinkie at gmail.com> wrote:
>
>> One word of caution: pactester uses the Firefox JavaScript engine,
>which
>> is more forgiving than MSIE's. So while it is a very useful tool, it
>may
>> let some errors slip through.
>> On Nov 18, 2014 9:45 PM, "Jason Haar" <Jason_Haar at trimble.com> wrote:
>>
>>> On 19/11/14 01:39, Brendan Kearney wrote:
>>> > i would suggest that if you use a pac/wpad solution, you look into
>>> > pactester, which is a google summer of code project that executes
>pac
>>> > files and provides output indicating what actions would be
>returned to
>>> > the browser, given a URL.
>>> couldn't agree more. We have it built into our QA to run before we
>ever
>>> roll out any change to our WPAD php script (a bug in there means
>>> everyone loses Internet access - so we have to be careful).
>>>
>>> Auto-generating a PAC script per client allows us to change
>behaviour
>>> based on User-Agent, client IP, proxy and destination - and allows
>us to
>>> control what web services should be DIRECT and what should be
>proxied.
>>> There is no other way of achieving those outcomes.
>>>
>>> Oh yes, and now that both Chrome and Firefox support proxies over
>HTTPS,
>>> I'm starting to ponder putting up some form of proxy on the Internet
>for
>>> our staff to use (authenticated of course!) - WPAD makes that
>something
>>> we could implement with no client changes - pretty cool :-)
>>>
>>> --
>>> Cheers
>>>
>>> Jason Haar
>>> Corporate Information Security Manager, Trimble Navigation Ltd.
>>> Phone: +1 408 481 8171
>>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141119/66ae62bc/attachment.htm>

From codemarauder at gmail.com  Wed Nov 19 13:36:49 2014
From: codemarauder at gmail.com (Nishant Sharma)
Date: Wed, 19 Nov 2014 19:06:49 +0530
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com> <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
 <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
Message-ID: <EB9A7FA8-FFC7-4B45-B272-F2624D8B0FBE@gmail.com>



On 19 November 2014 6:41:44 pm IST, brendan kearney <bpk678 at gmail.com> wrote:

>it
>if the Content-Type header is not set to
>"application/x-ns-proxy-autoconfig".
>

Ah so that is why most of the java applets don't honour PAC settings and I was blaming poor coding of those applets.

I usually serve PAC file with uhttpd or lighttpd servers running on the gateways and never bothered to set correct content-type headers.

Would be great if you could include that in your document too.

Regards,
Nishant

>GoToMeeting has also pissed me off.  The client parses the script and
>takes
>any value found in it, before executing the script and taking the
>output of
>the execution. This has the result of finding inappropriate proxies to
>use,
>when you are in a corporate environment and have proxies dedicated to
>client access or other functions that should not be leveraged in all
>cases.  I got their technical team on a call because we have a large
>citrix
>install base (both products have the same parent company) and
>complained to
>no avail.  I had to write a doc on how to correct the client config for
>anyone needing to use GoTo... products.
>On Nov 19, 2014 6:18 AM, "Kinkie" <gkinkie at gmail.com> wrote:
>
>> One word of caution: pactester uses the Firefox JavaScript engine,
>which
>> is more forgiving than MSIE's. So while it is a very useful tool, it
>may
>> let some errors slip through.
>> On Nov 18, 2014 9:45 PM, "Jason Haar" <Jason_Haar at trimble.com> wrote:
>>
>>> On 19/11/14 01:39, Brendan Kearney wrote:
>>> > i would suggest that if you use a pac/wpad solution, you look into
>>> > pactester, which is a google summer of code project that executes
>pac
>>> > files and provides output indicating what actions would be
>returned to
>>> > the browser, given a URL.
>>> couldn't agree more. We have it built into our QA to run before we
>ever
>>> roll out any change to our WPAD php script (a bug in there means
>>> everyone loses Internet access - so we have to be careful).
>>>
>>> Auto-generating a PAC script per client allows us to change
>behaviour
>>> based on User-Agent, client IP, proxy and destination - and allows
>us to
>>> control what web services should be DIRECT and what should be
>proxied.
>>> There is no other way of achieving those outcomes.
>>>
>>> Oh yes, and now that both Chrome and Firefox support proxies over
>HTTPS,
>>> I'm starting to ponder putting up some form of proxy on the Internet
>for
>>> our staff to use (authenticated of course!) - WPAD makes that
>something
>>> we could implement with no client changes - pretty cool :-)
>>>
>>> --
>>> Cheers
>>>
>>> Jason Haar
>>> Corporate Information Security Manager, Trimble Navigation Ltd.
>>> Phone: +1 408 481 8171
>>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From squid3 at treenet.co.nz  Wed Nov 19 13:37:58 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Nov 2014 02:37:58 +1300
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com> <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
 <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
Message-ID: <546C9D36.8070205@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 20/11/2014 2:11 a.m., brendan kearney wrote:
> Yes and it seems java is even more sensitive.  I had an array
> member defined on a line that was not terminated with a semicolon
> and browsers did not throw errors, but java did.  Pactester did not
> catch this.  Missing curly braces and I think quotes are caught.
> 
> Also of note, you have to set the content type header for a pac
> file or else you run into weird issues.  I found that browsers are
> forgiving and will execute the script and take its output if the
> header is not set. Flash does not do this.  It might call for the
> script but does not use it if the Content-Type header is not set
> to "application/x-ns-proxy-autoconfig".
> 
> GoToMeeting has also pissed me off.  The client parses the script
> and takes any value found in it, before executing the script and
> taking the output of the execution. This has the result of finding
> inappropriate proxies to use, when you are in a corporate
> environment and have proxies dedicated to client access or other
> functions that should not be leveraged in all cases.  I got their
> technical team on a call because we have a large citrix install
> base (both products have the same parent company) and complained
> to no avail.  I had to write a doc on how to correct the client
> config for anyone needing to use GoTo... products.

Ouch. Thank you for that. I've had questions but not had access to
systems to find out.

This is stuff that should be published on http://findproxyforurl.com/.
If you can find the author of that site (Peter Hayes?) please let them
know these.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbJ0xAAoJELJo5wb/XPRj4SYIAIUklyLTyHN5VnerwL4cuxGE
D1UOe7JF1xZMvLjbnZqKc0tOiy5PxabSbWREUTpQSKfFGwn21KPTL4bn2IMIkjM4
00zMucVr2xu7KRX99QFCWbOgU2cu7CGCkAQd1BmPaoQr5gE4VC5eo2b/MCz4fTx6
F0HjlKWlyJYhHcRhlnPLrkQV+gwHgJBpum4riATsd+TlnLH4C0zbKsy9/VYTzzWP
Lcm5wONkn1Ja1sSxagarDqeCbb94577b+s8sV2Iz9jYGYAHkVILxc0yBAuloG/PO
LD/N3Vhh+AGYk22raX9MlBuueTHCcz8rg/wWt6q5OQFfuaTuegjsDgKj8XE169A=
=EQKG
-----END PGP SIGNATURE-----


From wmunny at mail.com  Wed Nov 19 15:21:05 2014
From: wmunny at mail.com (wmunny william)
Date: Wed, 19 Nov 2014 16:21:05 +0100
Subject: [squid-users] Problem with digest authentification and credential
	backend
Message-ID: <trinity-83fc6663-456f-4f83-823a-0360a27ae695-1416410465373@3capp-mailcom-lxa14>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141119/0b1232b2/attachment.htm>

From bpk678 at gmail.com  Thu Nov 20 00:39:09 2014
From: bpk678 at gmail.com (Brendan Kearney)
Date: Wed, 19 Nov 2014 19:39:09 -0500
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <EB9A7FA8-FFC7-4B45-B272-F2624D8B0FBE@gmail.com>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com> <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
 <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
 <EB9A7FA8-FFC7-4B45-B272-F2624D8B0FBE@gmail.com>
Message-ID: <1416443949.6715.4.camel@desktop.bpk2.com>

On Wed, 2014-11-19 at 19:06 +0530, Nishant Sharma wrote:
> 
> On 19 November 2014 6:41:44 pm IST, brendan kearney <bpk678 at gmail.com> wrote:
> 
> >it
> >if the Content-Type header is not set to
> >"application/x-ns-proxy-autoconfig".
> >
> 
> Ah so that is why most of the java applets don't honour PAC settings and I was blaming poor coding of those applets.
> 
> I usually serve PAC file with uhttpd or lighttpd servers running on the gateways and never bothered to set correct content-type headers.
> 
> Would be great if you could include that in your document too.
> 
> Regards,
> Nishant
> 
> >GoToMeeting has also pissed me off.  The client parses the script and
> >takes
> >any value found in it, before executing the script and taking the
> >output of
> >the execution. This has the result of finding inappropriate proxies to
> >use,
> >when you are in a corporate environment and have proxies dedicated to
> >client access or other functions that should not be leveraged in all
> >cases.  I got their technical team on a call because we have a large
> >citrix
> >install base (both products have the same parent company) and
> >complained to
> >no avail.  I had to write a doc on how to correct the client config for
> >anyone needing to use GoTo... products.
> >On Nov 19, 2014 6:18 AM, "Kinkie" <gkinkie at gmail.com> wrote:
> >
> >> One word of caution: pactester uses the Firefox JavaScript engine,
> >which
> >> is more forgiving than MSIE's. So while it is a very useful tool, it
> >may
> >> let some errors slip through.
> >> On Nov 18, 2014 9:45 PM, "Jason Haar" <Jason_Haar at trimble.com> wrote:
> >>
> >>> On 19/11/14 01:39, Brendan Kearney wrote:
> >>> > i would suggest that if you use a pac/wpad solution, you look into
> >>> > pactester, which is a google summer of code project that executes
> >pac
> >>> > files and provides output indicating what actions would be
> >returned to
> >>> > the browser, given a URL.
> >>> couldn't agree more. We have it built into our QA to run before we
> >ever
> >>> roll out any change to our WPAD php script (a bug in there means
> >>> everyone loses Internet access - so we have to be careful).
> >>>
> >>> Auto-generating a PAC script per client allows us to change
> >behaviour
> >>> based on User-Agent, client IP, proxy and destination - and allows
> >us to
> >>> control what web services should be DIRECT and what should be
> >proxied.
> >>> There is no other way of achieving those outcomes.
> >>>
> >>> Oh yes, and now that both Chrome and Firefox support proxies over
> >HTTPS,
> >>> I'm starting to ponder putting up some form of proxy on the Internet
> >for
> >>> our staff to use (authenticated of course!) - WPAD makes that
> >something
> >>> we could implement with no client changes - pretty cool :-)
> >>>
> >>> --
> >>> Cheers
> >>>
> >>> Jason Haar
> >>> Corporate Information Security Manager, Trimble Navigation Ltd.
> >>> Phone: +1 408 481 8171
> >>> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >>
> >
> >
> >------------------------------------------------------------------------
> >
> >_______________________________________________
> >squid-users mailing list
> >squid-users at lists.squid-cache.org
> >http://lists.squid-cache.org/listinfo/squid-users
> 

i didn't mean to get your hopes up about the document i wrote.  i wrote
it for my employer and its details are specific to our environment.  i
am sure i could create something if people would want it, but i am not
sure which topic to provide documentation for.  is it the web server /
pac file stuff or the GoToMeeting stuff?



From Andreas.Reschke at mahle.com  Thu Nov 20 09:57:58 2014
From: Andreas.Reschke at mahle.com (Andreas.Reschke at mahle.com)
Date: Thu, 20 Nov 2014 10:57:58 +0100
Subject: [squid-users] probs with squid and url forwarding
Message-ID: <OF54A836E1.3FB2C3A0-ONC1257D96.0036BF26-C1257D96.0036BF2A@mahle.com>

Hi there,

we're using 3 squid proxy (version 3.3.-13, no caching, no logging) at our LAN (4500 User) in front of a squid proxy at the DMZ. Now there are a lot of troubles with portals for the costumer (like www.covisint.com). After register at this portal and select the application there is a forward to another domain. At this point squid can't connect the other domain with the error: this side isn't reachable.

How to fix this?



Mit freundlichen Gr??en / Kind regards

Mr. Andreas Reschke
andreas.reschke at mahle.com


From squid3 at treenet.co.nz  Thu Nov 20 10:07:57 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Nov 2014 23:07:57 +1300
Subject: [squid-users] probs with squid and url forwarding
In-Reply-To: <OF54A836E1.3FB2C3A0-ONC1257D96.0036BF26-C1257D96.0036BF2A@mahle.com>
References: <OF54A836E1.3FB2C3A0-ONC1257D96.0036BF26-C1257D96.0036BF2A@mahle.com>
Message-ID: <546DBD7D.2060602@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 20/11/2014 10:57 p.m., Andreas.Reschke wrote:
> Hi there,
> 
> we're using 3 squid proxy (version 3.3.-13, no caching, no
> logging) at our LAN (4500 User) in front of a squid proxy at the
> DMZ. Now there are a lot of troubles with portals for the costumer
> (like www.covisint.com). After register at this portal and select
> the application there is a forward to another domain. At this point
> squid can't connect the other domain with the error: this side
> isn't reachable.
> 
> How to fix this?

What is the HTTP message being sent from the client to Squid when it
"breaks"?

And what error details is Squid reporting about the problem in cache.log?
 NP: cache.log is mandatory, only access.log/store.log are optional.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbb19AAoJELJo5wb/XPRja0gH/jtExNBSJLU/bHplHeZ8F1Ik
k+nhEf3OvbfrcF1juHRueHW+3bXtQq+CGbh5i7Fq8dGcg84boXEWJ1ViNzBRiy6k
CJgCKT7XKVIrCm9f5EMa4kBwgioXaj0Jhpu7A0vQSjC36Orsw0S4pBrfxus3RSie
gsHmJpuhLNxETOBFFKYDBtCD6aZQtmbuEKbKkLotk/OIkpeGQWuIE2Os4F9eSJ5o
Ujr0dRUoThIBT2VMnI3lLcw3bskQHI4oGWlDKV2VO2sDB9AnbGEQU6uyYXhD6Z+/
HtFvyxYQCW53J0JJO6n7otbuEFN0XneDUDxBxhFBgnfrFwNeskW/DbUajQTxxG8=
=1Wl/
-----END PGP SIGNATURE-----


From steve at opendium.com  Thu Nov 20 10:51:11 2014
From: steve at opendium.com (Steve Hill)
Date: Thu, 20 Nov 2014 10:51:11 +0000
Subject: [squid-users] RFC2616 headers in bumped requests
In-Reply-To: <546A7111.70605@treenet.co.nz>
References: <5458A7D6.3090502@opendium.com> <5458DBD0.2050306@treenet.co.nz>
 <5469CD2B.8080802@opendium.com> <546A7111.70605@treenet.co.nz>
Message-ID: <546DC79F.5040300@opendium.com>

On 17/11/14 22:05, Amos Jeffries wrote:

> Would you mind running an experiment for me?
> 
> To see what happens if Squid delivers either of these Via headers
> instead of its current output:
> 
>   Via: HTTPS/1.1 iceni2.opendium.net (squid/3.4.9)

The HTTPS/1.1 one appears to work correctly.

>   Via: TLS/1.2 iceni2.opendium.net (squid/3.4.9)

The web server produces the same broken redirect as before when I send
TLS/1.2.

> Setting it with request_header_access/replace should do.

I've tested this in Squid with request_header_access/replace and
confirmed with openssl's s_client directly.

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From steve at opendium.com  Thu Nov 20 11:00:55 2014
From: steve at opendium.com (Steve Hill)
Date: Thu, 20 Nov 2014 11:00:55 +0000
Subject: [squid-users] Assertion failure: DestinationIp.cc:60
In-Reply-To: <546C2AD7.9080905@treenet.co.nz>
References: <546B6D9F.10408@opendium.com> <546C2AD7.9080905@treenet.co.nz>
Message-ID: <546DC9E7.7090505@opendium.com>

On 19/11/14 05:29, Amos Jeffries wrote:

> What is your config? In particular anything using ACLs.

auth_param basic program /usr/lib64/squid/basic_pam_auth -r
auth_param basic children 50
auth_param basic realm Iceni Web Proxy
auth_param basic credentialsttl 2 hours

workers 4
shutdown_lifetime 3 seconds
forward_max_tries 40
icap_service_failure_limit -1
host_verify_strict off
spoof_client_ip deny all

logformat iceni %tg.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a
%mt "%{User-Agent}>h" %lp
access_log stdio:/var/log/squid-nocache/access.log iceni
cache_log /var/log/squid-nocache/cache.log
cache_store_log none
pid_filename /var/run/squid-nocache.pid
coredump_dir /var/spool/squid-nocache
state_dir /var/run/squid-nocache


external_acl_type preauth children-max=1 concurrency=100 ttl=60
negative_ttl=0 %SRC %>{User-Agent} %URI %METHOD /usr/sbin/squid-preauth
/etc/iceni/authcached/authcached.psk
acl preauth		external preauth
acl preauth_tproxy	external preauth transparent
acl preauth_ok		note auth_tag preauth_ok
acl preauth_done	note auth_tag preauth_done
acl need_http_auth	note auth_tag http_auth
acl need_cp_auth	note auth_tag cp_auth
acl need_postauth_sync	note auth_tag postauth_sync
acl need_postauth_async	note auth_tag postauth_async

external_acl_type postauth_async children-max=1 concurrency=100 ttl=0
grace=100 %SRC %>{User-Agent} %LOGIN %EXT_USER /usr/sbin/squid-postauth
/etc/iceni/authcached/authcached.psk
external_acl_type postauth_sync cache=0 children-max=1 concurrency=100
ttl=0 grace=0 %SRC %>{User-Agent} %LOGIN %EXT_USER
/usr/sbin/squid-postauth /etc/iceni/authcached/authcached.psk
acl postauth_async	external postauth_async
acl postauth_sync	external postauth_sync

acl show_login_page	src all
deny_info
302:https://%h/webproxy/captive_portal/captive_portal_login?c=%o
show_login_page

# A bodge to ensure accesses to this machine aren't authenticated
# /etc/squid/local_ips is automatically updated by the init script when
# Squid starts or reloads, so Squid should be reloaded whenever the
# machine's IPs change (yuck!).
acl local_ips		dst "/etc/squid/local_ips"

acl SSL_ports		port 443

acl Safe_ports		port 80		# http
acl Safe_ports		port 21		# ftp
acl Safe_ports		port 443	# https
acl Safe_ports		port 70		# gopher
acl Safe_ports		port 210	# wais
acl Safe_ports		port 1025-65535	# unregistered ports
acl Safe_ports		port 280	# http-mgmt
acl Safe_ports		port 488	# gss-http
acl Safe_ports		port 591	# filemaker
acl Safe_ports		port 777	# multiling http

acl CONNECT		method CONNECT
acl https		proto https

acl proxy_auth		proxy_auth REQUIRED
acl tproxy		myportname tproxy
acl tproxy_ssl		myportname tproxy_ssl

acl dstdomain_localhost	dstdomain localhost


######
# Start of http_access access control.
######

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny to_localhost

# Unauthenticated access to the local server
http_access allow local_ips
http_access allow !tproxy !tproxy_ssl !https preauth
http_access allow !preauth_done preauth_tproxy
http_access allow need_http_auth need_postauth_sync proxy_auth postauth_sync
http_access allow need_http_auth need_postauth_async proxy_auth
postauth_async
http_access allow need_http_auth proxy_auth postauth_async

http_access deny preauth_ok show_login_page
http_access deny all
icp_access deny all
htcp_access deny all


acl icap_says_bump req_header X-SSL-Bump -i Yes
ssl_bump server-first icap_says_bump
ssl_bump server-first tproxy_ssl
sslproxy_cert_error allow all
request_header_access Via deny https
request_header_access X-Forwarded-For deny https


######
# Listening ports
######

http_port 3128 ssl-bump generate-host-certificates=on
cert=/etc/pki/tls/certs/squid-sslbump.crt
key=/etc/pki/tls/private/squid-sslbump.key dynamic_cert_mem_cache_size=128KB
http_port 8080 ssl-bump generate-host-certificates=on
cert=/etc/pki/tls/certs/squid-sslbump.crt
key=/etc/pki/tls/private/squid-sslbump.key dynamic_cert_mem_cache_size=128KB
http_port 3130 tproxy name=tproxy
https_port 3131 ssl-bump generate-host-certificates=on
cert=/etc/pki/tls/certs/squid-sslbump.crt
key=/etc/pki/tls/private/squid-sslbump.key tproxy name=tproxy_ssl
dynamic_cert_mem_cache_size=128KB
tcp_outgoing_mark 0x2 tproxy
tcp_outgoing_mark 0x2 tproxy_ssl
cache_peer [::1] parent 3129 0 proxy-only no-query no-digest no-tproxy
name=caching
cache_peer_access caching deny CONNECT
cache_peer_access caching deny https
cache_peer_access caching deny tproxy_ssl
cache_peer_access caching deny to_localhost
cache_peer_access caching deny dstdomain_localhost
cache_peer_access caching allow all

cache_mem 0
cache deny all
never_direct deny CONNECT
never_direct deny https
never_direct deny tproxy_ssl
never_direct deny to_localhost
never_direct deny dstdomain_localhost
never_direct allow all

icap_enable on
icap_service_revival_delay 30
icap_preview_enable on
icap_preview_size 50000
icap_send_client_ip on
icap_send_client_username on

icap_service iceni_reqmod_precache reqmod_precache 0
icap://localhost6:1344/reqmod_precache
icap_service iceni_respmod_postcache respmod_precache 0
icap://localhost6:1344/respmod_postcache

adaptation_service_set iceni_reqmod_precache iceni_reqmod_precache
adaptation_service_set iceni_respmod_postcache iceni_respmod_postcache

adaptation_access iceni_reqmod_precache deny local_ips
adaptation_access iceni_reqmod_precache deny to_localhost
adaptation_access iceni_reqmod_precache deny dstdomain_localhost
adaptation_access iceni_reqmod_precache allow all

adaptation_access iceni_respmod_postcache deny local_ips
adaptation_access iceni_respmod_postcache deny to_localhost
adaptation_access iceni_respmod_postcache deny dstdomain_localhost
adaptation_access iceni_respmod_postcache allow all

-- 

 - Steve

-- 

 - Steve Hill
   Technical Director
   Opendium Limited     http://www.opendium.com

Direct contacts:
   Instant messager: xmpp:steve at opendium.com
   Email:            steve at opendium.com
   Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
   Email:            sales at opendium.com
   Phone:            +44-1792-825748 / sip:sales at opendium.com

Support contacts:
   Email:            support at opendium.com
   Phone:            +44-1792-824568 / sip:support at opendium.com


From Andreas.Reschke at mahle.com  Thu Nov 20 11:43:36 2014
From: Andreas.Reschke at mahle.com (Andreas.Reschke at mahle.com)
Date: Thu, 20 Nov 2014 12:43:36 +0100
Subject: [squid-users] probs with squid and url forwarding
In-Reply-To: <546DBD7D.2060602@treenet.co.nz>
References: <546DBD7D.2060602@treenet.co.nz>,
 <OF54A836E1.3FB2C3A0-ONC1257D96.0036BF26-C1257D96.0036BF2A@mahle.com>
Message-ID: <OF49605B29.9D1E68F3-ONC1257D96.00406AE7-C1257D96.00406AE8@mahle.com>

"squid-users" <squid-users-bounces at lists.squid-cache.org> schrieb am 20.11.2014 11:07:57:

> Von: Amos Jeffries <squid3 at treenet.co.nz>
> An: squid-users at lists.squid-cache.org
> Datum: 20.11.2014 11:08
> Betreff: Re: [squid-users] probs with squid and url forwarding
> Gesendet von: "squid-users" <squid-users-bounces at lists.squid-cache.org>
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> On 20/11/2014 10:57 p.m., Andreas.Reschke wrote:
> > Hi there,
> > 
> > we're using 3 squid proxy (version 3.3.-13, no caching, no
> > logging) at our LAN (4500 User) in front of a squid proxy at the
> > DMZ. Now there are a lot of troubles with portals for the costumer
> > (like www.covisint.com). After register at this portal and select
> > the application there is a forward to another domain. At this point
> > squid can't connect the other domain with the error: this side
> > isn't reachable.
> > 
> > How to fix this?
> 
> What is the HTTP message being sent from the client to Squid when it
> "breaks"?
> 
> And what error details is Squid reporting about the problem in cache.log?
>  NP: cache.log is mandatory, only access.log/store.log are optional.
> 
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
> 
> iQEcBAEBAgAGBQJUbb19AAoJELJo5wb/XPRja0gH/jtExNBSJLU/bHplHeZ8F1Ik
> k+nhEf3OvbfrcF1juHRueHW+3bXtQq+CGbh5i7Fq8dGcg84boXEWJ1ViNzBRiy6k
> CJgCKT7XKVIrCm9f5EMa4kBwgioXaj0Jhpu7A0vQSjC36Orsw0S4pBrfxus3RSie
> gsHmJpuhLNxETOBFFKYDBtCD6aZQtmbuEKbKkLotk/OIkpeGQWuIE2Os4F9eSJ5o
> Ujr0dRUoThIBT2VMnI3lLcw3bskQHI4oGWlDKV2VO2sDB9AnbGEQU6uyYXhD6Z+/
> HtFvyxYQCW53J0JJO6n7otbuEFN0XneDUDxBxhFBgnfrFwNeskW/DbUajQTxxG8=
> =1Wl/
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users 

Hi Amos,
access.log is off because of we aren't allowed to log the user. Cache.log contains only LDAP-User-Auth and -groups.
The only method is read the stream with netcat, but there aren't nice data because of https.

Here are the sites:
1. https://fsp.portal.covisint.com/web/portal/applications
goes to
2. https://fim.covisint.com/ap/ford?TARGET=https://www.sim.ford.com
goes to
3. https://www.wsl-federation.ford.com/Federation/Redirect/Covisint
and then

Connection error
Fehler: Verbindung fehlgeschlagen

Firefox kann keine Verbindung zu dem Server unter www.wsl-federation.ford.com aufbauen.


Mit freundlichen Gr??en / Kind regards

Mr. Andreas Reschke
andreas.reschke at mahle.com


From lupick at gmail.com  Thu Nov 20 11:38:26 2014
From: lupick at gmail.com (Lupick)
Date: Thu, 20 Nov 2014 03:38:26 -0800 (PST)
Subject: [squid-users] squid basic ntlm auth error after upgrade to 3.3.8
Message-ID: <1416483506229-4668477.post@n4.nabble.com>

Hi 
I've a problem authenticating users outside my AD domain after the upgrade
to squid 3.3.8.

All the domain logged user are able to authenticate without any issue. 

The local user or user of a non domain computer have a username/password
prompt as expected.

If I provide the right doamin\username and password the promt appear over
and over.

BUT after the first time if I click cancel qnd I retry i'm able to browse
internet.  This happen cause the credential provided  are stored under the
windows credentian manager in the control panel.

no problem using centos 6 and squid 3.3.1, the problem appears after an
upgrade to centos 7 and squid 3.3.8.

this is my section on squid.conf:

auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 45
#auth_param ntlm max_challenge_reuses 0
#auth_param ntlm max_challenge_lifetime 2 minutes

auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 5 hours





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-basic-ntlm-auth-error-after-upgrade-to-3-3-8-tp4668477.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From swapneel at patnekar.com  Thu Nov 20 12:18:09 2014
From: swapneel at patnekar.com (Swapneel Patnekar)
Date: Thu, 20 Nov 2014 17:48:09 +0530
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
Message-ID: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>

Hi there,

I need to pull/fetch high level URL requests from the Squid access.log i.e
URL requests which were typed by the user in the browser.

For example, if the user had typed facebook.com, I want to pull/fetch only
facebook.com from the access.log and not
https://fbstatic-a.akamaihd.net/rsrc.php/v2/yV/r/aXwjx2fqSf4.css etc which
was not typed by the user in the browser but was referenced by facebook.com
for the CSS.

Can this be done ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141120/ecc64401/attachment.htm>

From ssivaprakash666 at gmail.com  Thu Nov 20 13:25:47 2014
From: ssivaprakash666 at gmail.com (Siva Prakash)
Date: Thu, 20 Nov 2014 18:55:47 +0530
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
Message-ID: <CAEuU1adtdgHJKzRb+4Sk_Lra-AUMCf8qbbLW_nDpbcRpqE00RQ@mail.gmail.com>

Hi,

i think this cannot be done.

In squid, we have option for custom log format, but in that also request
for .css or .js  or img will be logged.  We cannot specifically log only
the user typed url in the acces log.

- Siva Prakash

On 11/20/2014 05:48 PM, Swapneel Patnekar wrote:

Hi there,

I need to pull/fetch high level URL requests from the Squid access.log i.e
URL requests which were typed by the user in the browser.

For example, if the user had typed facebook.com, I want to pull/fetch only
facebook.com from the access.log and not
https://fbstatic-a.akamaihd.net/rsrc.php/v2/yV/r/aXwjx2fqSf4.css etc which
was not typed by the user in the browser but was referenced by facebook.com
for the CSS.

Can this be done ?



_______________________________________________
squid-users mailing
listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141120/6e662ca8/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 20 14:07:29 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Nov 2014 03:07:29 +1300
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
In-Reply-To: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>
References: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>
Message-ID: <546DF5A1.4080805@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 21/11/2014 1:18 a.m., Swapneel Patnekar wrote:
> Hi there,
> 
> I need to pull/fetch high level URL requests from the Squid
> access.log i.e URL requests which were typed by the user in the
> browser.
> 
> For example, if the user had typed facebook.com, I want to
> pull/fetch only facebook.com from the access.log and not 
> https://fbstatic-a.akamaihd.net/rsrc.php/v2/yV/r/aXwjx2fqSf4.css
> etc which was not typed by the user in the browser but was
> referenced by facebook.com for the CSS.
> 
> Can this be done ?

No it can't.

There is absolutely no way for Squid to identify what the user (if one
even exists) has done with their keyboard (or shortcuts, or bookmarks,
or search bar or...) that started the HTTP to happen.

You can log the "Referer" header contents, requests with no Referer
value are usually "first" requests. But that header is not always sent
or is sent when users change "page". So the accuracy is very low for
what you are asking to get out of it.

FWIW: "facebook.com" is a rarely visited page. Most of the users
search for it by company name and click the results, or use history
and bookmarks, even the emailed links FB sends out. All of which dump
them straight into the middle of some sub-section of the FB site.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbfWhAAoJELJo5wb/XPRjaeQH/0uWTCtDq9DNvNNXUVo103g4
nFAvaT5kcaEJxRNOCTMerLwSAIrqyhT+SpqnmbSJURskwmW8vZRodIAnQPXPfiCj
QuAazwDywVq0n7SyAtyIzyK0I6qlVtuKD+3VHcCJ30AXMv4RUv3ne8WenVLYggOq
KGfTAS5rXUQvnAKpSz+jRGY4ZS7ZJ7dxrrPZwUxsBqXiNAwJpesZVScnxMtiXdsN
Ko+/CMUDA4i35pAsc/l/GtGQozPtlsMtiXm7V5Vg+p9r01gIIaUWA6DuatBuB6RR
QqMsMWKm66HpeD8Kw/MMx81yKOXpEZYRpuGxoxs9CU/rrAd1uugjUR/3D1QLcgs=
=7n1w
-----END PGP SIGNATURE-----


From swapneel at patnekar.com  Thu Nov 20 14:31:14 2014
From: swapneel at patnekar.com (Swapneel Patnekar)
Date: Thu, 20 Nov 2014 20:01:14 +0530
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
In-Reply-To: <546DF5A1.4080805@treenet.co.nz>
References: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>
 <546DF5A1.4080805@treenet.co.nz>
Message-ID: <CAL4Goxy7ztn22s5CrdO-h1hofizQ_=VT-dRF224uX-j7FdX22A@mail.gmail.com>

Dear Amos,

Thank you for your input.

The reason I had this requirement is to build a analytics app on top of
Squid which will only show typed URL's so that the end user has the precise
information w.r.t evaluating web access.

Currently, since everything gets logged into the access.log, there is no
way to differentiate between typed/bookmark accessed URL's and URL's which
are references for the main URL i.e CDN URL's for js, css etc.

Sorry if I am pressing here, but do you have any suggestions/alternate
solutions for how I could achieve this ?

On Thu, Nov 20, 2014 at 7:37 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 21/11/2014 1:18 a.m., Swapneel Patnekar wrote:
> > Hi there,
> >
> > I need to pull/fetch high level URL requests from the Squid
> > access.log i.e URL requests which were typed by the user in the
> > browser.
> >
> > For example, if the user had typed facebook.com, I want to
> > pull/fetch only facebook.com from the access.log and not
> > https://fbstatic-a.akamaihd.net/rsrc.php/v2/yV/r/aXwjx2fqSf4.css
> > etc which was not typed by the user in the browser but was
> > referenced by facebook.com for the CSS.
> >
> > Can this be done ?
>
> No it can't.
>
> There is absolutely no way for Squid to identify what the user (if one
> even exists) has done with their keyboard (or shortcuts, or bookmarks,
> or search bar or...) that started the HTTP to happen.
>
> You can log the "Referer" header contents, requests with no Referer
> value are usually "first" requests. But that header is not always sent
> or is sent when users change "page". So the accuracy is very low for
> what you are asking to get out of it.
>
> FWIW: "facebook.com" is a rarely visited page. Most of the users
> search for it by company name and click the results, or use history
> and bookmarks, even the emailed links FB sends out. All of which dump
> them straight into the middle of some sub-section of the FB site.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUbfWhAAoJELJo5wb/XPRjaeQH/0uWTCtDq9DNvNNXUVo103g4
> nFAvaT5kcaEJxRNOCTMerLwSAIrqyhT+SpqnmbSJURskwmW8vZRodIAnQPXPfiCj
> QuAazwDywVq0n7SyAtyIzyK0I6qlVtuKD+3VHcCJ30AXMv4RUv3ne8WenVLYggOq
> KGfTAS5rXUQvnAKpSz+jRGY4ZS7ZJ7dxrrPZwUxsBqXiNAwJpesZVScnxMtiXdsN
> Ko+/CMUDA4i35pAsc/l/GtGQozPtlsMtiXm7V5Vg+p9r01gIIaUWA6DuatBuB6RR
> QqMsMWKm66HpeD8Kw/MMx81yKOXpEZYRpuGxoxs9CU/rrAd1uugjUR/3D1QLcgs=
> =7n1w
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141120/7142d20c/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 20 14:43:41 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Nov 2014 03:43:41 +1300
Subject: [squid-users] probs with squid and url forwarding
In-Reply-To: <OF49605B29.9D1E68F3-ONC1257D96.00406AE7-C1257D96.00406AE8@mahle.com>
References: <546DBD7D.2060602@treenet.co.nz>,
 <OF54A836E1.3FB2C3A0-ONC1257D96.0036BF26-C1257D96.0036BF2A@mahle.com>
 <OF49605B29.9D1E68F3-ONC1257D96.00406AE7-C1257D96.00406AE8@mahle.com>
Message-ID: <546DFE1D.4050202@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 21/11/2014 12:43 a.m., Andreas.Reschke wrote:
>> Von: Amos Jeffries
> On 20/11/2014 10:57 p.m., Andreas.Reschke wrote:
>>>> Hi there,
>>>> 
>>>> we're using 3 squid proxy (version 3.3.-13, no caching, no 
>>>> logging) at our LAN (4500 User) in front of a squid proxy at
>>>> the DMZ. Now there are a lot of troubles with portals for the
>>>> costumer (like www.covisint.com). After register at this
>>>> portal and select the application there is a forward to
>>>> another domain. At this point squid can't connect the other
>>>> domain with the error: this side isn't reachable.
>>>> 
>>>> How to fix this?
> 
> What is the HTTP message being sent from the client to Squid when
> it "breaks"?
> 
> And what error details is Squid reporting about the problem in
> cache.log? NP: cache.log is mandatory, only access.log/store.log
> are optional.
> 
> Amos
> 
> Hi Amos, access.log is off because of we aren't allowed to log the
> user. Cache.log contains only LDAP-User-Auth and -groups. The only
> method is read the stream with netcat, but there aren't nice data
> because of https.
> 

If you are unable to get any meaningful information about what is
going wrong then you will be unable to solve the problem.

Perhapse you can (temporarily) log only the request made and what
Squid tried to do with it? no details about the users necessary.

  logformat debug %Ss/%03>Hs %<st %rm %ru %Sh/%<a
  access_log stdio:/var/log/squid/debug_access.log debug


I suspect the HTTPS sites are sending logged-in users to some strange
port number or hostname which your squid.conf forbids access to. That
will show up in the above log as a CONNECT with TCP_DENIED/403 and
strange port number that will need to be added to the SSL_Ports ACL.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbf4dAAoJELJo5wb/XPRjMHIH/2/JQCpwGMyPHhbFfmmmYSp9
3BO8kGh9ZOexfgiouVeYcNYviuVhg1+roz7uI9Uo3S2PXJFNv5WT+alpTqHPPIQx
obaLK5GsxeXcEgjvtXK9sJhbTrepuO4XXK1THxtKMacT06QtubZyaK5gjLGiLTML
25+IaNrkttpha2jFuMfZRlnKXC/ENRQc+Yp/FzKI1BO98VFuir/mTMn9/8CSD95L
FsnVFwUe/pGUmdlvLpcbUZevgcX13ma3BfytMdjYGF4kDqg4444hygQFzT569P6Q
PA5nMJwY/vZIa0xKnleXkXCBQU05CclNdC9+hAmrxCd9dA1Y9KFKbhX3lVfnvjg=
=Uuek
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 20 14:58:31 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Nov 2014 03:58:31 +1300
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
In-Reply-To: <CAL4Goxy7ztn22s5CrdO-h1hofizQ_=VT-dRF224uX-j7FdX22A@mail.gmail.com>
References: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>	<546DF5A1.4080805@treenet.co.nz>
 <CAL4Goxy7ztn22s5CrdO-h1hofizQ_=VT-dRF224uX-j7FdX22A@mail.gmail.com>
Message-ID: <546E0197.8000709@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 21/11/2014 3:31 a.m., Swapneel Patnekar wrote:
> Dear Amos,
> 
> Thank you for your input.
> 
> The reason I had this requirement is to build a analytics app on
> top of Squid which will only show typed URL's so that the end user
> has the precise information w.r.t evaluating web access.
> 
> Currently, since everything gets logged into the access.log, there
> is no way to differentiate between typed/bookmark accessed URL's
> and URL's which are references for the main URL i.e CDN URL's for
> js, css etc.
> 
> Sorry if I am pressing here, but do you have any
> suggestions/alternate solutions for how I could achieve this ?

No sorry. What you ae asking for is not possible. The information you
are seeking is private to the browser. It rarely goes anywhere else,
and applications that attempt to make it get a CVE security
vulnerability of "information leakage" classification thrown at them
for doing it.

You will have to go with imprecise data and heuristic algorithms
estimating actions based on the logged URLs and their relative timing.


It may help if you discard the idea that you can point at a single URL
and say that is "the page". There are no such things as "pages" in
HTTP. "Page" is one of several abstract concepts representing groups
of URL fetches. In other words a "page" is a whole collection of
transactions in the HTTP log.
 Each time a user views a "page". Some *part* of the URLs forming that
page are fetched. Some parts of the collection may not be fetched. Or
different parts fetched at different "page" views. It depends on what
the browser needs to fetch at the time of each different viewing
(since it has its own cache).

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbgGXAAoJELJo5wb/XPRj24QH+wXf8G02wlscUiMO2s99q4I2
ROM1PA16Ye6EieD4I0p+vydIint4nIh10qiuuV+BnsMPgDK95ADdWgl6p2HFWYyK
ZwxzYZ7k4HW9cvJujkGKyJEgIv9UEQmduzljJu35yig50fNIbdYhbMNch4lmjZHg
CYs3TTglE0VOXh5+vYxKLRBR7CzZcYcHzQ2epj5kuSkCfFQSRTyZTsO2bZsTmv38
CufUrkzH6i0XnhYpp9tTwFdXBNPJs2yJI5I0UcDbPBI44IAH+bMCq0HJ83dV1CJA
c4+4EXE+vnWuc67HzmsSWx9KveQvx/2sddex3HqLtWwSXYp328JqHp22oTErwXE=
=dTiW
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 20 15:22:39 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Nov 2014 04:22:39 +1300
Subject: [squid-users] Problem with digest authentification and
 credential backend
In-Reply-To: <trinity-83fc6663-456f-4f83-823a-0360a27ae695-1416410465373@3capp-mailcom-lxa14>
References: <trinity-83fc6663-456f-4f83-823a-0360a27ae695-1416410465373@3capp-mailcom-lxa14>
Message-ID: <546E073E.4060003@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 20/11/2014 4:21 a.m., wmunny william wrote:
> Hi, After some strange authentication issues, I saw that my problem
> occurs when a admin sets a new password or removes a user. The
> backend authentification is only checked in logon at first popup,
> if something changes in LDAP the browser still always connected
> unlike basic ident. Maybe I forgot something like authenticate_ttl
> ? I already saw this -> 
> http://www.squid-cache.org/mail-archive/squid-users/201107/0259.html
> but without any answer. Regards Wm
> 

http://bugs.squid-cache.org/show_bug.cgi?id=4066

It is getting high on my todo list again to investigate this more
deeply. Meanwhile the patch in the bug report apparently works.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbgc9AAoJELJo5wb/XPRjY68IAK28OEU900sNEsvypzp9jqVX
qojm9L8FqHpdS8gwugRiIAgirjLlWFFsvbxBj9HxWoA0IoeYwMqeggQzVe/oua10
N3xbCiCWsyl3WaDyz2cicuS50oI7NKGSreEGXDItbu6V+EPMTnVl7mzR4Rh0YPly
AlIG7BBNONDjxK/H2q+3KSdGMZeQ4hFlJdbQcHegk0ffObEM0/uSBU5uEq/GlD4v
MxFyccOeLdu0Rvk98bCC+gC60KEX07Q/KCQELztm5RdG3K6cPQbi2WWDWXMwBDy6
x5cLLu3PlZ9CcVOHrt9UrY2Rni3SyRAFLaWe4QcO7g0Wwm9yqIH/wvsd42j1HBY=
=G79m
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Thu Nov 20 15:53:58 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Nov 2014 04:53:58 +1300
Subject: [squid-users] squid basic ntlm auth error after upgrade to 3.3.8
In-Reply-To: <1416483506229-4668477.post@n4.nabble.com>
References: <1416483506229-4668477.post@n4.nabble.com>
Message-ID: <546E0E96.3010303@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 21/11/2014 12:38 a.m., Lupick wrote:
> Hi I've a problem authenticating users outside my AD domain after
> the upgrade to squid 3.3.8.

3.3.8 is far from the latest Squid. There is information about where
to find updated packages for CentOS at
<http://wiki.squid-cache.org/KnowledgeBase/CentOS>

> 
> All the domain logged user are able to authenticate without any
> issue.
> 
> The local user or user of a non domain computer have a
> username/password prompt as expected.
> 
> If I provide the right doamin\username and password the promt
> appear over and over.

By "right" you mean the Basic or NTLM credentials?

Which popup is the browser selecting to display?
 - the realm value configured in squid.conf is displayed as part of
the Basic auth popup, IIRC the proxy hostname or DOMAIN is listed in
teh NTLM popup. So you should be able to tell which its asking for.

NTLM requires machines to be signed into the domain to get the correct
credentials crypto tokens from the DC to login with. Any attempt to
use NTLM credentials without being signed onto the domain will fail.

Basic auth only requires the domain\user:password combo gets delivered.


> 
> BUT after the first time if I click cancel qnd I retry i'm able to
> browse internet.  This happen cause the credential provided  are
> stored under the windows credentian manager in the control panel.
> 
> no problem using centos 6 and squid 3.3.1, the problem appears
> after an upgrade to centos 7 and squid 3.3.8.
> 
> this is my section on squid.conf:
> 
> auth_param ntlm program /usr/bin/ntlm_auth 
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 45 
> #auth_param ntlm max_challenge_reuses 0 #auth_param ntlm
> max_challenge_lifetime 2 minutes
> 
> auth_param basic program /usr/bin/ntlm_auth 
> --helper-protocol=squid-2.5-basic auth_param basic children 5 
> auth_param basic realm Squid proxy-caching web server auth_param
> basic credentialsttl 5 hours
> 

PS. Have you considered migrating to Kerberos? it has a lot less
problems than NTLM.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUbg6UAAoJELJo5wb/XPRjHzYIALBvTG3mVsl0QX0I1MzYdM2w
h9Cz2ShzpYEJWP+JcqeyQsp8xd8eWcxC8jsnibTat60belprPjcG7HLVVKHnKacT
jwQUQFId5B3KfuIad5MD887CxLwfujT3yoiBB2vFFki+bGWkkEDoOPzkcNY7TsUs
pSAqlynOpHNWH6UTahzG7L/xvxcHMTv8Wd2n1XxKFSGrdShwkWixLP1x3zA/CB3q
qckN8H5R/rOnMSBmWNCZ5VDFelPZTItXaxf4HmSbLw4XySxwLkthd8kHO9o/sv4E
SwiOihvxVMcXD/GPyG+bW9aXDN1p51aPX0SIisUuznuhh6vTTrhCJTqCDU1o9mM=
=pGgC
-----END PGP SIGNATURE-----


From jorgeburgos at inaipyucatan.org.mx  Thu Nov 20 19:35:23 2014
From: jorgeburgos at inaipyucatan.org.mx (=?UTF-8?Q?Jorge_Iv=C3=A1n_Burgos_Aguilar?=)
Date: Thu, 20 Nov 2014 13:35:23 -0600
Subject: [squid-users] External ACL with an HTTP reply header format doesn't
Message-ID: <CAGCqWQQ9fE+nVw55=rKakoXk4X2+Z5NwSaVosm=pi1=WnrsWMQ@mail.gmail.com>

Hi,

First of all my setup information
Distribution: CentOS Linux release 7.0.1406 (Core)
Kernel Version: Linux 3.10.0-123.8.1.el7.x86_64 x86_64
RPM Package Version: squid-3.3.8-12.el7_0.x86_64
Squid Cache: Version 3.3.8
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--disable-strict-error-checking'
'--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-eui'
'--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos'
'--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
'--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
'--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
-g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'


I'm currently working in a external acl (python based) to get the Source IP
and the HTTP Reply Content-Type header
in order to make a mime based filter based on the source ip (client ip)
with a database backend.
As the documentation for external_acl_type[1] says, i just need to use the
format %<{Header}, for my specific
need it should be%<h{Content-Type} (added the "h" because after running
squid -k parse it states that i should use
this form instead of the one described in the documentation).
But after some testing it doesn't work...

So m defining it in squid.conf like (Lower TTL's only for testing purposes):
------ relevant lines /etc/squid/squid.conf ------
external_acl_type testfilter ttl=120 negative_ttl=120 children-max=5
children-idle=1 ipv4 %SRC %<h{Content-Type} /usr/bin/python -u
/external_acl/test.py
acl test_filter external testfilter
http_reply_access deny test_filter
------ relevant lines /etc/squid/squid.conf ------

Considering that testfilter is simple a python script to log everything to
%ea:
------ /external_acl/test.py ------
#!/usr/bin/python -u
import sys
from urllib import quote

EOF = False
while not EOF:
    line = sys.stdin.readline()
    if not line:
        EOF = True
        continue

    sys.stdout.write('{0} log={1}\n'.format('ERR', quote(line)))

------ /external_acl/test.py ------

After configuring the logs everything is working fine except that the
Content-Type is never presented to the external ACL
it always show "-" instead of text/html as you can see in this example log
line (squid format + %ea):
1416507449.579    814 10.0.0.100 TCP_MISS/200 8823 GET
http://www.squid-cache.org/ - HIER_DIRECT/77.93.254.178 text/html
10.0.0.100%20-%0A

After testing with other reply headers, not a single one is working (even
the very common date or server reply header) all send the same "-"
to the external_acl.

So someone here managed to workout this? or have a different solution for
the same problem? or is it bug in squid? or in the package provided by
centos?

Kindest Regards

[1] http://www.squid-cache.org/Versions/v3/3.3/cfgman/external_acl_type.html
-- 
Jorge Iv?n Burgos Aguilar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141120/7d9d396c/attachment.htm>

From wmunny at mail.com  Fri Nov 21 08:17:23 2014
From: wmunny at mail.com (wmunny william)
Date: Fri, 21 Nov 2014 09:17:23 +0100
Subject: [squid-users] Problem with digest authentification and
 credential backend
In-Reply-To: <546E073E.4060003@treenet.co.nz>
References: <trinity-83fc6663-456f-4f83-823a-0360a27ae695-1416410465373@3capp-mailcom-lxa14>,
 <546E073E.4060003@treenet.co.nz>
Message-ID: <trinity-a1058ad0-6843-4434-bdd6-fe4c6d32c403-1416557842903@3capp-mailcom-lxa14>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141121/cca0597e/attachment.htm>

From info at far-galaxy.de  Fri Nov 21 14:57:27 2014
From: info at far-galaxy.de (Sebastian Fohler)
Date: Fri, 21 Nov 2014 15:57:27 +0100
Subject: [squid-users] Disable SSLv3 on Squid doesn't seem to work
Message-ID: <546F52D7.9030804@far-galaxy.de>

I've disabled SSLv3 with this option set in my squid.conf file:

sslproxy_options NO_SSLv3 NO_SSLv2

But despite that fact, the squid proxy accepted the configuration 
without any problems, I still get SSLv3 connections working.
I've sniffed the traffice on that interface on the proxy port and if I 
do a SSLv3 connection from the browser and do a poodle check, the 
sniffing protocol shows an established SSLv3 connection.

Can someone tell me if I missed something here?
Is there some option which could override the sslproxy_options setting?
How can I check if the sslproxy_options are really being used?

Thank you in advance.
Best regards
Sebastian


From squid3 at treenet.co.nz  Fri Nov 21 15:29:26 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Nov 2014 04:29:26 +1300
Subject: [squid-users] Disable SSLv3 on Squid doesn't seem to work
In-Reply-To: <546F52D7.9030804@far-galaxy.de>
References: <546F52D7.9030804@far-galaxy.de>
Message-ID: <546F5A56.8010001@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 22/11/2014 3:57 a.m., Sebastian Fohler wrote:
> I've disabled SSLv3 with this option set in my squid.conf file:
> 
> sslproxy_options NO_SSLv3 NO_SSLv2
> 
> But despite that fact, the squid proxy accepted the configuration 
> without any problems, I still get SSLv3 connections working. I've
> sniffed the traffice on that interface on the proxy port and if I 
> do a SSLv3 connection from the browser and do a poodle check, the 
> sniffing protocol shows an established SSLv3 connection.

The connection between browser and Squid is controlled by the *_port
settings.

sslproxy_* directives are purely for DIRECT or ORIGINAL_DST server
connections.

> 
> Can someone tell me if I missed something here?

The sslproxy_options setting is an OpenSSL format string. Which is a
list of comma (',') or colon (':') separated OpenSSL option names.


What you need to configure is something like these:

 # to prevent SSL on inbound traffic
 https_port ...  options=NO_SSLv3:NO_SSLv2
 http_port ... ssl-bump options=NO_SSLv3:NO_SSLv2

 # to prevent SSL on direct server traffic
 sslproxy_options NO_SSLv3:NO_SSLv2

 # to prevent SSL on relayed peer connections
 cache_peer ... ssloptions=NO_SSLv3:NO_SSLv2


> Is there some option which could override the sslproxy_options
> setting?

If anything the OpenSSL library configuration may have such options.
But AFAIK that is for configuring the defaults and squid.conf settings
are overriding them.


> How can I check if the sslproxy_options are really being used?

Good question. I'm not aware of anything in particular. If there is an
SSL/TLS testing website connecting to it through Squid should tell you.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUb1pVAAoJELJo5wb/XPRjTPAIAJiboRyQ7kwCTW9bByF8yT99
oD/u8W23DQ5p6sl1bfvKGeZBwUIkn5qX6pzF8RDZIWFrz/Fu1N0b7KMpdqQYqsFC
W/dfyXywucWSmnTj32e47Wa9q1Y4u/r1oa6tDUBCsUM9Dh4iVS2UI6akyy1HkuEk
Zpxl7iF9UcPyRBZ7cvTl7iZSFHRgPEokdaXNo+qKLDQUpNg5XlK82wf4JY+EUyt1
AvBz32cCIVz9ErQ5RckCTCV3XTLOUFoAXrbOiApGe07Gum746yAnRzuB07LYCwwY
16XL5N+mjw5Gj+70pMGVfaieoQHK7W9L7qJPDLy+JqL7Z2r81GjD4tb6O0txAgo=
=NbHW
-----END PGP SIGNATURE-----


From info at far-galaxy.de  Fri Nov 21 16:07:06 2014
From: info at far-galaxy.de (Sebastian Fohler)
Date: Fri, 21 Nov 2014 17:07:06 +0100
Subject: [squid-users] Disable SSLv3 on Squid doesn't seem to work
In-Reply-To: <546F5A56.8010001@treenet.co.nz>
References: <546F52D7.9030804@far-galaxy.de> <546F5A56.8010001@treenet.co.nz>
Message-ID: <546F632A.7030601@far-galaxy.de>

Thank you Amos,

I've implemented http_port 80 ssl-bump options=NO_SSLv3:NO_SSLv2
Yet still the proxy accepts SSLv3 connections in the sniffing protocol.

Something is still wrong.

Best regards
Sebastian

On 21.11.2014 16:29, Amos Jeffries wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 22/11/2014 3:57 a.m., Sebastian Fohler wrote:
>> I've disabled SSLv3 with this option set in my squid.conf file:
>>
>> sslproxy_options NO_SSLv3 NO_SSLv2
>>
>> But despite that fact, the squid proxy accepted the configuration
>> without any problems, I still get SSLv3 connections working. I've
>> sniffed the traffice on that interface on the proxy port and if I
>> do a SSLv3 connection from the browser and do a poodle check, the
>> sniffing protocol shows an established SSLv3 connection.
>
> The connection between browser and Squid is controlled by the *_port
> settings.
>
> sslproxy_* directives are purely for DIRECT or ORIGINAL_DST server
> connections.
>
>>
>> Can someone tell me if I missed something here?
>
> The sslproxy_options setting is an OpenSSL format string. Which is a
> list of comma (',') or colon (':') separated OpenSSL option names.
>
>
> What you need to configure is something like these:
>
>   # to prevent SSL on inbound traffic
>   https_port ...  options=NO_SSLv3:NO_SSLv2
>   http_port ... ssl-bump options=NO_SSLv3:NO_SSLv2
>
>   # to prevent SSL on direct server traffic
>   sslproxy_options NO_SSLv3:NO_SSLv2
>
>   # to prevent SSL on relayed peer connections
>   cache_peer ... ssloptions=NO_SSLv3:NO_SSLv2
>
>
>> Is there some option which could override the sslproxy_options
>> setting?
>
> If anything the OpenSSL library configuration may have such options.
> But AFAIK that is for configuring the defaults and squid.conf settings
> are overriding them.
>
>
>> How can I check if the sslproxy_options are really being used?
>
> Good question. I'm not aware of anything in particular. If there is an
> SSL/TLS testing website connecting to it through Squid should tell you.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUb1pVAAoJELJo5wb/XPRjTPAIAJiboRyQ7kwCTW9bByF8yT99
> oD/u8W23DQ5p6sl1bfvKGeZBwUIkn5qX6pzF8RDZIWFrz/Fu1N0b7KMpdqQYqsFC
> W/dfyXywucWSmnTj32e47Wa9q1Y4u/r1oa6tDUBCsUM9Dh4iVS2UI6akyy1HkuEk
> Zpxl7iF9UcPyRBZ7cvTl7iZSFHRgPEokdaXNo+qKLDQUpNg5XlK82wf4JY+EUyt1
> AvBz32cCIVz9ErQ5RckCTCV3XTLOUFoAXrbOiApGe07Gum746yAnRzuB07LYCwwY
> 16XL5N+mjw5Gj+70pMGVfaieoQHK7W9L7qJPDLy+JqL7Z2r81GjD4tb6O0txAgo=
> =NbHW
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From hectorchan at gmail.com  Sat Nov 22 00:15:51 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Fri, 21 Nov 2014 16:15:51 -0800
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <5465A296.8010109@treenet.co.nz>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
 <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
 <5465791A.4080500@treenet.co.nz>
 <CAEhCwUw3aZsdNeMLAPBBojELPTWWCMZc9sbz4SXUOabaU-aJ2A@mail.gmail.com>
 <5465A296.8010109@treenet.co.nz>
Message-ID: <CAEhCwUxT-H5mjFHqHvOLPn8r8RjUV43BOK2tsaTfcYqCEbpumg@mail.gmail.com>

Hi Amos,

For the following cache_peer:

> cache_peer forward-proxy.example.com parent 3128 0 name=C

Would squid do the proper HTTP CONNECT before forwarding the request there ?

Thanks,
Hector

On Thu, Nov 13, 2014 at 10:35 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 14/11/2014 6:22 p.m., Hector Chan wrote:
> > Hi Amos,
> >
> >> those lines you specify above go in (C). *if* they are needed at
> >> all.
> >
> > But I don't have control over (C).  It's off limits.
>
> Then you have to trust that the admin in charge of it set it up right.
>
> >
> >> In (B) goes:
> >>
> >> cache_peer forward-proxy.example.com parent 3128 0 name=C
> >>
> >> acl sendToC dstdomain origin-x.example.com origin-y.example.com
> > origin-z.example.com
> >> cache_peer_access C sendToC
> >
> > The requests reaching (B) (reverse-proxy.example.com) are in the
> > form: http://reverse-proxy.example.com/goto-origin-x
> > http://reverse-proxy.example.com/goto-origin-y
> > http://reverse-proxy.example.com/goto-origin-z
> >
> > and I have a couple of cache_peer_access acls (urlpath regex) to
> > send them to origin-x, origin-y, and origin-z.  How would the above
> > dstdomain acl work with these rules?
>
> You have now stopped using HTTP and started using some strange
> URL-embeded protocol.
>
> An HTTP proxy cannot help you there. You require a proxy that
> understands and acts on the URL-embeded protocol messages.
>
> It is possible to extend Squid with URL-rewrite helpers that can
> translate it into different HTTP URL for passing to (C). BUT, there is
> no guarantee of what origin (C) will use to fetch that resource. You
> have to *trust* that (C) uses the origin best suited to any request
> that it is given, according to the criteria its own admin has set for
> "best".
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUZaKVAAoJELJo5wb/XPRjdpQH/iBh1HQcAZQr0gqK7FS8nZ9x
> v0fzAOx/L0HCG5MTT7drwvvEVltxMRYoVniM8VJSqUw3cFAlI+2VEScIr3oOFjcr
> qAdjxyjer7sxVgmQM80Oa+n40RK7mvZejvhEV9/0Gc0XTmAjL3PrBptKpumslhVh
> rq40LUX50rg5xaAfA02WCy4mYS99uH7qBABWIXeeESVdvGLVRTaTlthqaKW8JTFh
> pjmS9OKVnk5CeEi6cyJ8VV7edBOgv2rpgUH8Wjap66mmIjVHq8alNU53obRAMk7p
> Pd/bPfPFERnoBymbYmYfFBd3Mfddgc49Wpz9gggAWgXE8bq6CbXQHpj5GvUayaE=
> =mS+q
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141121/b4be3edf/attachment.htm>

From hectorchan at gmail.com  Sat Nov 22 00:49:38 2014
From: hectorchan at gmail.com (Hector Chan)
Date: Fri, 21 Nov 2014 16:49:38 -0800
Subject: [squid-users] Squid going through another forward proxy
In-Reply-To: <CAEhCwUxT-H5mjFHqHvOLPn8r8RjUV43BOK2tsaTfcYqCEbpumg@mail.gmail.com>
References: <CAEhCwUwfRv9zNvbVYvsneswPvVbh0qXOCnhtpWRBWyExCbR8qQ@mail.gmail.com>
 <201411132049.59585.Antony.Stone@squid.open.source.it>
 <CAEhCwUx45drX2rSxL_F7v609a6Qcp3oSPX=Xqk8uNxH19VnjQg@mail.gmail.com>
 <54654918.2090003@treenet.co.nz>
 <CAEhCwUwNW-o1qSGhd2vUMfk9yLoj-iDhEmF=EAs3eV+5n9015g@mail.gmail.com>
 <CAEhCwUy+Vy46dYwknPzKqfaKj1iHZ7Jh+bKt8L6m9agH-XHuzQ@mail.gmail.com>
 <5465791A.4080500@treenet.co.nz>
 <CAEhCwUw3aZsdNeMLAPBBojELPTWWCMZc9sbz4SXUOabaU-aJ2A@mail.gmail.com>
 <5465A296.8010109@treenet.co.nz>
 <CAEhCwUxT-H5mjFHqHvOLPn8r8RjUV43BOK2tsaTfcYqCEbpumg@mail.gmail.com>
Message-ID: <CAEhCwUwvNo39me+rHsODv4-iisoEXpNuhECFoJDhA2zixud=jA@mail.gmail.com>

And if doesn't do the proper HTTP CONNECT, is there any way I can make
squid to use HTTP CONNECT and establish a proxy channel?  The reason I ask
is because we use HTTP BasicAuth with the origin server and it needs to be
encrypted end-to-end.

Thanks again,
Hector


On Fri, Nov 21, 2014 at 4:15 PM, Hector Chan <hectorchan at gmail.com> wrote:

> Hi Amos,
>
> For the following cache_peer:
>
> > cache_peer forward-proxy.example.com parent 3128 0 name=C
>
> Would squid do the proper HTTP CONNECT before forwarding the request there
> ?
>
> Thanks,
> Hector
>
> On Thu, Nov 13, 2014 at 10:35 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> On 14/11/2014 6:22 p.m., Hector Chan wrote:
>> > Hi Amos,
>> >
>> >> those lines you specify above go in (C). *if* they are needed at
>> >> all.
>> >
>> > But I don't have control over (C).  It's off limits.
>>
>> Then you have to trust that the admin in charge of it set it up right.
>>
>> >
>> >> In (B) goes:
>> >>
>> >> cache_peer forward-proxy.example.com parent 3128 0 name=C
>> >>
>> >> acl sendToC dstdomain origin-x.example.com origin-y.example.com
>> > origin-z.example.com
>> >> cache_peer_access C sendToC
>> >
>> > The requests reaching (B) (reverse-proxy.example.com) are in the
>> > form: http://reverse-proxy.example.com/goto-origin-x
>> > http://reverse-proxy.example.com/goto-origin-y
>> > http://reverse-proxy.example.com/goto-origin-z
>> >
>> > and I have a couple of cache_peer_access acls (urlpath regex) to
>> > send them to origin-x, origin-y, and origin-z.  How would the above
>> > dstdomain acl work with these rules?
>>
>> You have now stopped using HTTP and started using some strange
>> URL-embeded protocol.
>>
>> An HTTP proxy cannot help you there. You require a proxy that
>> understands and acts on the URL-embeded protocol messages.
>>
>> It is possible to extend Squid with URL-rewrite helpers that can
>> translate it into different HTTP URL for passing to (C). BUT, there is
>> no guarantee of what origin (C) will use to fetch that resource. You
>> have to *trust* that (C) uses the origin best suited to any request
>> that it is given, according to the criteria its own admin has set for
>> "best".
>>
>> Amos
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2.0.22 (MingW32)
>>
>> iQEcBAEBAgAGBQJUZaKVAAoJELJo5wb/XPRjdpQH/iBh1HQcAZQr0gqK7FS8nZ9x
>> v0fzAOx/L0HCG5MTT7drwvvEVltxMRYoVniM8VJSqUw3cFAlI+2VEScIr3oOFjcr
>> qAdjxyjer7sxVgmQM80Oa+n40RK7mvZejvhEV9/0Gc0XTmAjL3PrBptKpumslhVh
>> rq40LUX50rg5xaAfA02WCy4mYS99uH7qBABWIXeeESVdvGLVRTaTlthqaKW8JTFh
>> pjmS9OKVnk5CeEi6cyJ8VV7edBOgv2rpgUH8Wjap66mmIjVHq8alNU53obRAMk7p
>> Pd/bPfPFERnoBymbYmYfFBd3Mfddgc49Wpz9gggAWgXE8bq6CbXQHpj5GvUayaE=
>> =mS+q
>> -----END PGP SIGNATURE-----
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141121/287687fb/attachment.htm>

From swapneel at patnekar.com  Sat Nov 22 06:58:51 2014
From: swapneel at patnekar.com (Swapneel Patnekar)
Date: Sat, 22 Nov 2014 12:28:51 +0530
Subject: [squid-users] Pull/Fetch high level URL requests from Squid
 access.log without getting all the object hits
In-Reply-To: <546E0197.8000709@treenet.co.nz>
References: <CAL4Goxw95U5GpnfjWx2rCFaB=gvhv_xq2rxboqV5KQOCBBzy6g@mail.gmail.com>
 <546DF5A1.4080805@treenet.co.nz>
 <CAL4Goxy7ztn22s5CrdO-h1hofizQ_=VT-dRF224uX-j7FdX22A@mail.gmail.com>
 <546E0197.8000709@treenet.co.nz>
Message-ID: <CAL4GoxxMGi=3uRmde1-kpVWB4=t+_brkQdkNNUWRXu0rUQUw5A@mail.gmail.com>

Thank you Amos for your detailed explanation. That helped. Appreciate it.

On Thu, Nov 20, 2014 at 8:28 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 21/11/2014 3:31 a.m., Swapneel Patnekar wrote:
> > Dear Amos,
> >
> > Thank you for your input.
> >
> > The reason I had this requirement is to build a analytics app on
> > top of Squid which will only show typed URL's so that the end user
> > has the precise information w.r.t evaluating web access.
> >
> > Currently, since everything gets logged into the access.log, there
> > is no way to differentiate between typed/bookmark accessed URL's
> > and URL's which are references for the main URL i.e CDN URL's for
> > js, css etc.
> >
> > Sorry if I am pressing here, but do you have any
> > suggestions/alternate solutions for how I could achieve this ?
>
> No sorry. What you ae asking for is not possible. The information you
> are seeking is private to the browser. It rarely goes anywhere else,
> and applications that attempt to make it get a CVE security
> vulnerability of "information leakage" classification thrown at them
> for doing it.
>
> You will have to go with imprecise data and heuristic algorithms
> estimating actions based on the logged URLs and their relative timing.
>
>
> It may help if you discard the idea that you can point at a single URL
> and say that is "the page". There are no such things as "pages" in
> HTTP. "Page" is one of several abstract concepts representing groups
> of URL fetches. In other words a "page" is a whole collection of
> transactions in the HTTP log.
>  Each time a user views a "page". Some *part* of the URLs forming that
> page are fetched. Some parts of the collection may not be fetched. Or
> different parts fetched at different "page" views. It depends on what
> the browser needs to fetch at the time of each different viewing
> (since it has its own cache).
>
> Amos
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUbgGXAAoJELJo5wb/XPRj24QH+wXf8G02wlscUiMO2s99q4I2
> ROM1PA16Ye6EieD4I0p+vydIint4nIh10qiuuV+BnsMPgDK95ADdWgl6p2HFWYyK
> ZwxzYZ7k4HW9cvJujkGKyJEgIv9UEQmduzljJu35yig50fNIbdYhbMNch4lmjZHg
> CYs3TTglE0VOXh5+vYxKLRBR7CzZcYcHzQ2epj5kuSkCfFQSRTyZTsO2bZsTmv38
> CufUrkzH6i0XnhYpp9tTwFdXBNPJs2yJI5I0UcDbPBI44IAH+bMCq0HJ83dV1CJA
> c4+4EXE+vnWuc67HzmsSWx9KveQvx/2sddex3HqLtWwSXYp328JqHp22oTErwXE=
> =dTiW
> -----END PGP SIGNATURE-----
>



-- 
Best,
Swapneel Patnekar
+91 98866 37820
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141122/91a9defa/attachment.htm>

From david at articatech.com  Sun Nov 23 11:01:17 2014
From: david at articatech.com (David Touzeau)
Date: Sun, 23 Nov 2014 12:01:17 +0100
Subject: [squid-users] squid 3.5x: Active Directory accounts with space issue
Message-ID: <5471BE7D.6030206@articatech.com>

Hi

We have connected 3.5.0.2-20141121-r13666 with Active Directory.
It seems where there are spaces in login account squid use only the last 
argument.

For example for an account "Jhon smith" squid use "smith" only
For example for an account "Dr Jhon smith" squid use "smith" only

In 3.3.13 there is no such issue, a "Jhon smith" account is logged as 
"Jhon smith" and sended as Jhon%20smith to helpers

Best regards


From santosh.pai at vigyanlabs.com  Mon Nov 24 07:08:20 2014
From: santosh.pai at vigyanlabs.com (santosh)
Date: Sun, 23 Nov 2014 23:08:20 -0800 (PST)
Subject: [squid-users] Unable to get username in logs for access
	denied(HTTP 403)
In-Reply-To: <1416389809018-4668460.post@n4.nabble.com>
References: <1416389809018-4668460.post@n4.nabble.com>
Message-ID: <1416812900748-4668496.post@n4.nabble.com>

Hello Amos,

Sorry for the late reply . I have made changes as you said here the
authentication is working fine and even blocking too is working as expected
but the logging doesnt seem to work .

auth_param basic program /usr/lib/squid3/basic_ldap_auth -b
"dc=example,dc=com" -f "((uid=%s))" -h example.com

acl ldapauth proxy_auth REQUIRED

acl workhours time  M T W H F 8:00-18:00

http_access deny !ldapauth
http_access deny  bad_url

http_access deny blocked_sites workhours

http_access allow ldapauth



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Unable-to-get-username-in-logs-for-access-denied-HTTP-403-tp4668460p4668496.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Mon Nov 24 09:38:02 2014
From: fredbmail at free.fr (FredB)
Date: Mon, 24 Nov 2014 10:38:02 +0100 (CET)
Subject: [squid-users] Problem with digest authentification and
 credential backend
In-Reply-To: <trinity-a1058ad0-6843-4434-bdd6-fe4c6d32c403-1416557842903@3capp-mailcom-lxa14>
Message-ID: <379284887.174632909.1416821882975.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> 
> And it works great ! Thank you Amos for your patch.
> In previous Squid 3.3.x DIGEST was very buggy , crash, 407, banners,
> but now it seems very stable. Perhaps there are some little bugs
> like this, but now it's usable.
> Thanks for your works
> 


Hi,

William to be more clear this patch is not related at all with authenticate_ttl directive.
authenticate_ttl doesn't works with Digest, but with basic and maybe another (ntlm, kerberos ?) there is no precision here http://www.squid-cache.org/Doc/config/authenticate_ttl/

The patch works like this:

At first banner Squid store the login/password HASH http://en.wikipedia.org/wiki/Digest_access_authentication http://wiki.squid-cache.org/KnowledgeBase/LdapBackedDigestAuthentication 

When nonce is stalled (nonce_max_count reached) the helper compare the account stored in memory with a request to Ldap or/and when the nonce is expired, the helper makes the same thing.

In this two cases there are two possibilities, the account is right or wrong -> Bad password or/and bad login

- If the return is right Squid return a new nonce and there is no impact for the user, I mean no banner.
- If the return is wrong Squid present the authentication realm to the user and the browser prompt for a username and password.

There is also an another situation - if squid is restarted - the browser returns is HASH without banner (if the account is right of course)

So, without any change in LDAP the banner never appear, except when the browser start.

Fred 

PS: About Digest you are right it's almost good now, still also a little problem with nonce count but not related with this 


From eliezer at ngtech.co.il  Mon Nov 24 10:25:14 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 24 Nov 2014 12:25:14 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
Message-ID: <5473078A.3060506@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I do know that pac files contains some form of JS and in the past I
have seen couple complex PAC files but unsure about the options.
I want to know if a PAC file can be used for
Authentication\Authorization, maybe even working against another
external system to get a token?

Thanks,
Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUcweKAAoJENxnfXtQ8ZQUy7oH/ieegXDfKslc8NPYgzkRfpRW
JVYcRB9gqVEQSEpphznVz3s4PTuspYYKmNnr1uWMnUQRC906GPaa326j+EMtQ9Eq
mcPc2dBU7jyMkj5V4EUAJlMZ+29YzDFKSAAJkf4/cYX5ik1JKOMyIljaKF5O4PQU
HNhSUVrQ+/9nkDE8puzALYYFygKn+u8exN2pr9ikobAgsGhoMMsULJxQi90st67S
W9/Be12+2KiBxGWBwnTCNTZjRs5xAg/8xsLTOuMMzKPF0ihpDRcDFQFYZYF22uKM
BQAZCG1VJWz8wwDrDN8Pmy7AbII2ygFvKu/8s6S7ZAdq7mragGVsyhJzVoQzqJc=
=l9Ue
-----END PGP SIGNATURE-----


From Andreas.Reschke at mahle.com  Mon Nov 24 12:02:43 2014
From: Andreas.Reschke at mahle.com (Andreas.Reschke at mahle.com)
Date: Mon, 24 Nov 2014 13:02:43 +0100
Subject: [squid-users] citrix client auth with squid
Message-ID: <OF4113E55B.443B888D-ONC1257D9A.00422AF4-C1257D9A.00422AF9@mahle.com>

Hi there,
does anybody use citrix clients in browser to connect citrix server of another company and squid as a proxy? 
How does the authentication work at the citrix client?




Mit freundlichen Gr??en / Kind regards

Mr. Andreas Reschke
andreas.reschke at mahle.com


From gkinkie at gmail.com  Mon Nov 24 12:43:12 2014
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 24 Nov 2014 13:43:12 +0100
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <5473078A.3060506@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
Message-ID: <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>

Hi Eliezer,
  I don't think so. PACfiles have no access to the DOM or facilities
like AJAX, and are very limited in what they can return or affect as
side-effects. In theory it could be possible to do something, but in
practice it would be only advisory and not secure: a pacfile must by
definition be in a publicly-accessible URL, so anyone can read it and
interpret it.

On Mon, Nov 24, 2014 at 11:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> I do know that pac files contains some form of JS and in the past I
> have seen couple complex PAC files but unsure about the options.
> I want to know if a PAC file can be used for
> Authentication\Authorization, maybe even working against another
> external system to get a token?
>
> Thanks,
> Eliezer
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUcweKAAoJENxnfXtQ8ZQUy7oH/ieegXDfKslc8NPYgzkRfpRW
> JVYcRB9gqVEQSEpphznVz3s4PTuspYYKmNnr1uWMnUQRC906GPaa326j+EMtQ9Eq
> mcPc2dBU7jyMkj5V4EUAJlMZ+29YzDFKSAAJkf4/cYX5ik1JKOMyIljaKF5O4PQU
> HNhSUVrQ+/9nkDE8puzALYYFygKn+u8exN2pr9ikobAgsGhoMMsULJxQi90st67S
> W9/Be12+2KiBxGWBwnTCNTZjRs5xAg/8xsLTOuMMzKPF0ihpDRcDFQFYZYF22uKM
> BQAZCG1VJWz8wwDrDN8Pmy7AbII2ygFvKu/8s6S7ZAdq7mragGVsyhJzVoQzqJc=
> =l9Ue
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From eliezer at ngtech.co.il  Mon Nov 24 12:46:25 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 24 Nov 2014 14:46:25 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
Message-ID: <547328A1.6040100@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/24/2014 02:43 PM, Kinkie wrote:
> Hi Eliezer, I don't think so. PACfiles have no access to the DOM or
> facilities like AJAX, and are very limited in what they can return
> or affect as side-effects. In theory it could be possible to do
> something, but in practice it would be only advisory and not
> secure: a pacfile must by definition be in a publicly-accessible
> URL, so anyone can read it and interpret it.

So a small question:
Can I put the pac file on a https site with basic authentication?

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUcyihAAoJENxnfXtQ8ZQUbKwH/2iF+hTc0HO14k74+lw0ftj0
tllr3v2uOIG5a3905B27sSxPvJa+XQ7mTOa0dRdvbmL9klyh0njdyKYVrs0ZjBB4
6VFUqMRsimu7gSpjuFZZIySMDy35XM+S3EyluQehiQJpwOfidgxHbF7iiehfd/+1
yn0kk/AoxnisDMvRvlpKZAwnvTuZFjZoj+zMs0GfZgJ/skcNu2YDAKANAPon+uhm
MIEf6Gi2zbwPCrsOnQXySPTG17trWPMGUvH3nVXbxFd+8amHSdBWy6O8iEhsFEPZ
hXP9XyjfXbWIwrYvqhI+0lVc3BEr52tjIdpVV1pu5h9jsBfjJMFTQXvlhhCHHiw=
=sVZf
-----END PGP SIGNATURE-----


From gkinkie at gmail.com  Mon Nov 24 13:05:10 2014
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 24 Nov 2014 14:05:10 +0100
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <547328A1.6040100@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
Message-ID: <CA+Y8hcPNNBkz3M=YerGU7MDfF4H3ADL1nRcsRbuqXf9FW_Qbig@mail.gmail.com>

Still it'd be semi-public; you'd have to replicate the access control
rules on the proxy anyway.

On Mon, Nov 24, 2014 at 1:46 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 11/24/2014 02:43 PM, Kinkie wrote:
>> Hi Eliezer, I don't think so. PACfiles have no access to the DOM or
>> facilities like AJAX, and are very limited in what they can return
>> or affect as side-effects. In theory it could be possible to do
>> something, but in practice it would be only advisory and not
>> secure: a pacfile must by definition be in a publicly-accessible
>> URL, so anyone can read it and interpret it.
>
> So a small question:
> Can I put the pac file on a https site with basic authentication?
>
> Eliezer
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUcyihAAoJENxnfXtQ8ZQUbKwH/2iF+hTc0HO14k74+lw0ftj0
> tllr3v2uOIG5a3905B27sSxPvJa+XQ7mTOa0dRdvbmL9klyh0njdyKYVrs0ZjBB4
> 6VFUqMRsimu7gSpjuFZZIySMDy35XM+S3EyluQehiQJpwOfidgxHbF7iiehfd/+1
> yn0kk/AoxnisDMvRvlpKZAwnvTuZFjZoj+zMs0GfZgJ/skcNu2YDAKANAPon+uhm
> MIEf6Gi2zbwPCrsOnQXySPTG17trWPMGUvH3nVXbxFd+8amHSdBWy6O8iEhsFEPZ
> hXP9XyjfXbWIwrYvqhI+0lVc3BEr52tjIdpVV1pu5h9jsBfjJMFTQXvlhhCHHiw=
> =sVZf
> -----END PGP SIGNATURE-----



-- 
    Francesco


From eliezer at ngtech.co.il  Mon Nov 24 13:13:52 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 24 Nov 2014 15:13:52 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <CA+Y8hcPNNBkz3M=YerGU7MDfF4H3ADL1nRcsRbuqXf9FW_Qbig@mail.gmail.com>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
 <CA+Y8hcPNNBkz3M=YerGU7MDfF4H3ADL1nRcsRbuqXf9FW_Qbig@mail.gmail.com>
Message-ID: <54732F10.1010608@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/24/2014 03:05 PM, Kinkie wrote:
> Still it'd be semi-public; you'd have to replicate the access
> control rules on the proxy anyway.
So we can use a http backend for authentication via a radius server
and then get a token or something else while the proxy authenticate
the user via the radius server(on a ip level).

Eliezer

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUcy8QAAoJENxnfXtQ8ZQUZ0UH/3+bPBj2VIGnlgbU/p4MruMR
O8cBsiNCPtldaMA8kMeZl4A/D5ETEXw/NmutEASqSJZiQJWdYNu6C0gCt+rQPPA1
9ae4d3zUfJuCyiYFcl9IqlP5YtBIvry8J2ml9f5eSlEfpGwkddLZ2PKtfkixaDva
TmNSBmsKgW410Wtyd24YipbpVyoOc8eXxwfH8b/1Evm4hRsDZdSg6H274yC5kTqc
C3OxFXfej8uZQT9lUw0qKwsqwOu0e82fIuUxqzcxsAlH3MlIIze2LyLIgtdTRo+F
iYcOjTSmMO92B3okbO79SI8ssABclF0LVARi1PdTqJhC0qic/WHgrKhomvXSkiM=
=hxiL
-----END PGP SIGNATURE-----


From gkinkie at gmail.com  Mon Nov 24 13:24:41 2014
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 24 Nov 2014 14:24:41 +0100
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <54732F10.1010608@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
 <CA+Y8hcPNNBkz3M=YerGU7MDfF4H3ADL1nRcsRbuqXf9FW_Qbig@mail.gmail.com>
 <54732F10.1010608@ngtech.co.il>
Message-ID: <CA+Y8hcMtwxqa79soyKhth_3ZNBHsfaS_D4As0Y1RADH0SHj7Rw@mail.gmail.com>

But what if multiple users share the same IP (e.g. Citrix, X11)?

On Mon, Nov 24, 2014 at 2:13 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 11/24/2014 03:05 PM, Kinkie wrote:
>> Still it'd be semi-public; you'd have to replicate the access
>> control rules on the proxy anyway.
> So we can use a http backend for authentication via a radius server
> and then get a token or something else while the proxy authenticate
> the user via the radius server(on a ip level).
>
> Eliezer
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1
>
> iQEcBAEBAgAGBQJUcy8QAAoJENxnfXtQ8ZQUZ0UH/3+bPBj2VIGnlgbU/p4MruMR
> O8cBsiNCPtldaMA8kMeZl4A/D5ETEXw/NmutEASqSJZiQJWdYNu6C0gCt+rQPPA1
> 9ae4d3zUfJuCyiYFcl9IqlP5YtBIvry8J2ml9f5eSlEfpGwkddLZ2PKtfkixaDva
> TmNSBmsKgW410Wtyd24YipbpVyoOc8eXxwfH8b/1Evm4hRsDZdSg6H274yC5kTqc
> C3OxFXfej8uZQT9lUw0qKwsqwOu0e82fIuUxqzcxsAlH3MlIIze2LyLIgtdTRo+F
> iYcOjTSmMO92B3okbO79SI8ssABclF0LVARi1PdTqJhC0qic/WHgrKhomvXSkiM=
> =hxiL
> -----END PGP SIGNATURE-----



-- 
    Francesco


From eliezer at ngtech.co.il  Mon Nov 24 16:43:32 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 24 Nov 2014 18:43:32 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <CA+Y8hcMtwxqa79soyKhth_3ZNBHsfaS_D4As0Y1RADH0SHj7Rw@mail.gmail.com>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
 <CA+Y8hcPNNBkz3M=YerGU7MDfF4H3ADL1nRcsRbuqXf9FW_Qbig@mail.gmail.com>
 <54732F10.1010608@ngtech.co.il>
 <CA+Y8hcMtwxqa79soyKhth_3ZNBHsfaS_D4As0Y1RADH0SHj7Rw@mail.gmail.com>
Message-ID: <54736034.2060507@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/24/2014 03:24 PM, Kinkie wrote:
> But what if multiple users share the same IP (e.g. Citrix, X11)?
This is another situation which requires authentication...
Two users can use the same pac files and be authorized as another
user(a regular forward proxy).
While I do like the Radius solution we can use external_acl in some
way to check if the specific user is allowed to authenticate from this
specific IP.
In a more strict environments a Radius server can help with this issue
to disallow one username to be allow the usage from two src IP at the
same time.

Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUc2A0AAoJENxnfXtQ8ZQUW6UH/2HM/FHijhGozGUlCHMc3hpi
IdBQhEWjkKAYmTmI44ZhULPn/Nc76qBbHW8cRBhU4ziOJLyQ/Wq9yFKa8g4HS7IS
pJBzQum+fP2eVbUVz1kqjggq2sPnEe8SNtLr99sF25UoocTf/thl3ssoKOqHgudb
1vgRIQoDMB1SMqQ5XAQ/rwiFg969OITs65AKUHEu50FfbGeorqO64+32mLtGcoUG
hDKGThLMHnZeFsNafCecm2bvvJjcCjBV3Uj4tbwbYF2uRRRdbT3aa9vTnWUH6na0
exA6Cj7UQQp42eakWZl7LKd9xRTn4kReGgLxRa+v+VO/hwNkd4z7wyhUQWpE3mA=
=fzkF
-----END PGP SIGNATURE-----


From tomtux007 at gmail.com  Mon Nov 24 18:30:36 2014
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 24 Nov 2014 19:30:36 +0100
Subject: [squid-users] citrix client auth with squid
In-Reply-To: <OF4113E55B.443B888D-ONC1257D9A.00422AF4-C1257D9A.00422AF9@mahle.com>
References: <OF4113E55B.443B888D-ONC1257D9A.00422AF4-C1257D9A.00422AF9@mahle.com>
Message-ID: <CACLJR+OgaB-CBCAeAU6eWfW+-9vGHzmDT19AbB_LKYejnGig=Q@mail.gmail.com>

Some of our internal users are connecting via squid and ica-webclient
(java-applet) to the remote citrix-server. Because of our
kerberos-authentication (java resp. ica-webclient seems not to know
kerberos) we allowed the destination (citrix)-site without
authentication, but based on the user-agent java/1.7 resp. java/1.8.

Kind regards,
Tom

On Mon, Nov 24, 2014 at 1:02 PM,  <Andreas.Reschke at mahle.com> wrote:
> Hi there,
> does anybody use citrix clients in browser to connect citrix server of another company and squid as a proxy?
> How does the authentication work at the citrix client?
>
>
>
>
> Mit freundlichen Gr??en / Kind regards
>
> Mr. Andreas Reschke
> andreas.reschke at mahle.com
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Jason_Haar at trimble.com  Mon Nov 24 20:06:02 2014
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 25 Nov 2014 09:06:02 +1300
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <5473078A.3060506@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
Message-ID: <54738FAA.9090309@trimble.com>

On 24/11/14 23:25, Eliezer Croitoru wrote:
> I do know that pac files contains some form of JS and in the past I
> have seen couple complex PAC files but unsure about the options.
> I want to know if a PAC file can be used for
> Authentication\Authorization, maybe even working against another
> external system to get a token?

I think you are confusing proxy authentication with WPAD/PAC files. WPAD
knows nothing about proxy authentication: browsers do

ie you use WPAD to tell browsers where/if they need to use a proxy and
under what circumstances, and when they then attempt to do so, the
BROWSER will have to respond to authentication issues surrounding
authentication proxies.

BTW, I'm sorry I didn't keep the link, but just a few days ago I was 
reading a Microsoft technet article about requiring authentication to
access the actual WPAD/PAC file, and Microsoft said it was "sort of"
supported for some versions of MSIE, but that they recommend WPAD never
be placed behind an authenticating web server because it causes
problems... ie don't protect the WPAD data, protect the proxies the WPAD
points at

BTW, over the weekend I was playing with using WPAD to enable users to
access our squid proxy *over the Internet*. Firefox and Chrome now
support proxy-over-SSL, so I was looking at creating WPAD DNS records on
the Internet to point to an Internet-based proxy server so that our
people could interact with the Internet "safely" (ie with content
filtering including AV). Obviously an open proxy on the Internet isn't a
sane option, so it had authentication enabled, but then I discovered
Chrome doesn't allow you to "save password" for proxy authentication (at
least over SSL, didn't try over "raw"). That sort of put an end to that
experiment, as I was anticipating a standalone account database with
randomly generated 20char passwords :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1




From dougs at dawnsign.com  Mon Nov 24 20:06:52 2014
From: dougs at dawnsign.com (Doug Sampson)
Date: Mon, 24 Nov 2014 20:06:52 +0000
Subject: [squid-users] Memory Leak Squid 3.4.9 on FreeBSD 10.0 x64
Message-ID: <E6B2517F8D6DBF4CABB8F38ACA367E783431AF29@Draco.dawnsign.com>

Recently due to squid 2.7 being EOL'ed, we migrated our squid server to version 3.4.9 on a FreeBSD 10.0-RELEASE running on 64-bit hardware. We started seeing paging file being swapped out eventually running out of available memory. From the time squid gets started it usually takes about two days before we see these entries in /var/log/messages as follows:

+swap_pager_getswapspace(16): failed
+swap_pager_getswapspace(16): failed
+swap_pager_getswapspace(16): failed
+swap_pager_getswapspace(12): failed
+swap_pager_getswapspace(16): failed
+swap_pager_getswapspace(12): failed
+swap_pager_getswapspace(6): failed
+swap_pager_getswapspace(16): failed

Looking at the 'top' results, I see that the swap file has been totally exhausted. Memory used by squid hovers around 2.3GB out of the total 3GB of system memory.

I am not sure what is causing these memory leaks. After rebooting, squid-internal-mgr/info shows the following statistics:

Squid Object Cache: Version 3.4.9
Build Info: 
Start Time:	Mon, 24 Nov 2014 18:39:08 GMT
Current Time:	Mon, 24 Nov 2014 19:39:13 GMT
Connection information for squid:
	Number of clients accessing cache:	18
	Number of HTTP requests received:	10589
	Number of ICP messages received:	0
	Number of ICP messages sent:	0
	Number of queued ICP replies:	0
	Number of HTCP messages received:	0
	Number of HTCP messages sent:	0
	Request failure ratio:	 0.00
	Average HTTP requests per minute since start:	176.2
	Average ICP messages per minute since start:	0.0
	Select loop called: 763993 times, 4.719 ms avg
Cache information for squid:
	Hits as % of all requests:	5min: 3.2%, 60min: 17.0%
	Hits as % of bytes sent:	5min: 2.0%, 60min: 6.7%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 37.2%
	Disk hits as % of hit requests:	5min: 22.2%, 60min: 33.2%
	Storage Swap size:	7361088 KB
	Storage Swap capacity:	58.5% used, 41.5% free
	Storage Mem size:	54348 KB
	Storage Mem capacity:	 3.9% used, 96.1% free
	Mean Object Size:	23.63 KB
	Requests given to unlinkd:	1
Median Service Times (seconds)  5 min    60 min:
	HTTP Requests (All):   0.10857  0.19742
	Cache Misses:          0.10857  0.32154
	Cache Hits:            0.08265  0.01387
	Near Hits:             0.15048  0.12106
	Not-Modified Replies:  0.00091  0.00091
	DNS Lookups:           0.05078  0.05078
	ICP Queries:           0.00000  0.00000
Resource usage for squid:
	UP Time:	3605.384 seconds
	CPU Time:	42.671 seconds
	CPU Usage:	1.18%
	CPU Usage, 5 minute avg:	0.72%
	CPU Usage, 60 minute avg:	1.17%
	Maximum Resident Size: 845040 KB
	Page faults with physical i/o: 20
Memory accounted for:
	Total accounted:       105900 KB
	memPoolAlloc calls:   2673353
	memPoolFree calls:    2676487
File descriptor usage for squid:
	Maximum number of file descriptors:   87516
	Largest file desc currently in use:    310
	Number of file desc currently in use:  198
	Files queued for open:                   0
	Available number of file descriptors: 87318
	Reserved number of file descriptors:   100
	Store Disk files open:                   0
Internal Data Structures:
	311543 StoreEntries
	  4421 StoreEntries with MemObjects
	  4416 Hot Object Cache Items
	311453 on-disk objects

I will post another one tomorrow that will indicate growing memory/swapfile consumption.

Here is my squid.conf:

# OPTIONS FOR AUTHENTICATION
# -----------------------------------------------------------------------------
# 1st four lines for 
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
#  next three lines for kerberos authentication (needed to use usernames)
#  used in conjunction with "acl auth proxy_auth" line below
#auth_param negotiate program /usr/local/libexec/squid/negotiate_kerberos_auth -i
#auth_param negotiate children 50 startup=10 idle=5
#auth_param negotiate keep_alive on


# ACCESS CONTROLS
# -----------------------------------------------------------------------------
# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
#acl manager proto cache_object 
acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
acl adminhost src 192.168.1.149
acl localnet src 192.168.1.0/24	# RFC1918 possible internal network
acl localnet src fc00::/7           # RFC 4193 local private network range
acl localnet src fe80::/10          # RFC 4291 link-local (directly plugged) machines
acl webserver src 198.168.1.35
acl some_big_clients src 192.168.1.149/32 #CI53

# We want to limit downloads of these type of files
# Put this all in one line
acl magic_words url_regex -i ftp .exe .mp3 .vqf .tar.gz .gz .rpm .zip .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav .dmg .mp4 .img
# We don't block .html, .gif, .jpg and similar files, because they
# generally don't consume much bandwidth

acl SSL_ports port 443
acl SSL_ports port 8443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

acl CONNECT method CONNECT
acl Winupdate dstdomain .microsoft.com     # Dot is important
acl social-sites dstdomain .facebook.com .fbcdn.net
acl ZipInfo dstdomain .dial-a-zip.com

# in conjunction with negotiate_kerberos_auth line above
#acl auth proxy_auth REQUIRED

#
# Recommended minimum Access Permission configuration:
#
http_access allow manager localnet
http_access allow manager localhost
http_access allow manager webserver
http_access allow manager adminhost
http_access deny manager
acl PURGE method PURGE
http_access allow PURGE localhost
http_access deny PURGE
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
redirector_access deny Winupdate

# in conjunction with negotiate_kerberos_auth line above
#http_access deny !auth
#http_access allow auth

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost
http_access allow ZipInfo localnet
# And finally deny all other access to this proxy
http_access deny all

# NETWORK OPTIONS
# -----------------------------------------------------------------------------
# Squid normally listens to port 3128
http_port 3128

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# -----------------------------------------------------------------------------
hierarchy_stoplist cgi-bin ?

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
cache_mem 1366 MB
#cache_mem 2134 MB
#maximum_object_size_in_memory 64 KB
maximum_object_size_in_memory 128 KB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
cache_replacement_policy heap LFUDA
cache_dir aufs /data/squid/aufs_cache 4096 16 256 min-size=131073
cache_dir diskd /data/squid/diskd_cache 8192 16 256 Q1=64 Q2=72 max-size=131072
#maximum_object_size 122880 KB
maximum_object_size 153600 KB
cache_swap_low 90
cache_swap_high 95

# LOGFILE OPTIONS
# -----------------------------------------------------------------------------
access_log daemon:/data/squid/logs/access.log
cache_store_log daemon:/data/squid/logs/store.log
cache_swap_log /var/spool/squid/%s
logfile_rotate 28

# OPTIONS FOR TROUBLESHOOTING
# -----------------------------------------------------------------------------
cache_log /data/squid/logs/cache.log
# Leave coredumps in the first cache dir
coredump_dir /data/squid

# OPTIONS FOR EXTERNAL SUPPORT PROGRAMS
# -----------------------------------------------------------------------------
diskd_program /usr/local/libexec/squid/diskd

# OPTIONS FOR TUNING THE CACHE
# -----------------------------------------------------------------------------
refresh_pattern http://.*\.windowsupdate\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://office\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://windowsupdate\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://w?xpsp[0-9]\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://w2ksp[0-9]\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://download\.microsoft\.com/ 0 80% 20160 
refresh_pattern http://download\.macromedia\.com/ 0 80% 20160 
refresh_pattern http://ftp\.software\.ibm\.com/ 0 80% 20160 
refresh_pattern         cgi-bin         1 20% 2
refresh_pattern         \.asp$          1 20% 2
refresh_pattern         \.acgi$         1 20% 2
refresh_pattern         \.cgi$          1 20% 2
refresh_pattern         \.pl$           1 20% 2
refresh_pattern         \.shtml$        1 20% 2
refresh_pattern         \.php3$         1 20% 2
refresh_pattern         \?              1 20% 2
refresh_pattern         \.gif$          10080   90%     43200 
refresh_pattern         \.png$          10080   90%     43200 
refresh_pattern         \.jpg$          10080   90%     43200 
refresh_pattern         \.ico$          10080   90%     43200 
refresh_pattern         \.bom\.gov\.au     30   20%       120 
refresh_pattern         \.html$           480   50%     22160 
refresh_pattern         \.htm$            480   50%     22160 
refresh_pattern         \.css$            480   50%     22160 
refresh_pattern         \.js$             480   50%     22160 
refresh_pattern         \.class$        10080   90%     43200 
refresh_pattern         \.zip$          10080   90%     43200 
refresh_pattern         \.jpeg$         10080   90%     43200 
refresh_pattern         \.mid$          10080   90%     43200 
refresh_pattern         \.shtml$          480   50%     22160 
refresh_pattern         \.exe$          10080   90%     43200 
refresh_pattern         \.thm$          10080   90%     43200 
refresh_pattern         \.wav$          10080   90%     43200 
refresh_pattern         \.mp4$          10080   90%     43200 
refresh_pattern         \.txt$          10080   90%     43200 
refresh_pattern         \.cab$          10080   90%     43200 
refresh_pattern         \.au$           10080   90%     43200 
refresh_pattern         \.mov$          10080   90%     43200 
refresh_pattern         \.xbm$          10080   90%     43200 
refresh_pattern         \.ram$          10080   90%     43200 
refresh_pattern         \.iso$          10080   90%     43200 
refresh_pattern         \.avi$          10080   90%     43200 
refresh_pattern         \.chtml$          480   50%     22160 
refresh_pattern         \.thb$          10080   90%     43200 
refresh_pattern         \.dcr$          10080   90%     43200 
refresh_pattern         \.bmp$          10080   90%     43200 
refresh_pattern         \.phtml$          480   50%     22160 
refresh_pattern         \.mpg$          10080   90%     43200 
refresh_pattern         \.pdf$          10080   90%     43200 
refresh_pattern         \.art$          10080   90%     43200 
refresh_pattern         \.swf$          10080   90%     43200 
refresh_pattern         \.flv$          10080   90%     43200 
refresh_pattern         \.x-flv$        10080   90%     43200 
refresh_pattern         \.mp3$          10080   90%     43200 
refresh_pattern         \.ra$           10080   90%     43200 
refresh_pattern         \.spl$          10080   90%     43200 
refresh_pattern         \.viv$          10080   90%     43200 
refresh_pattern         \.doc$          10080   90%     43200 
refresh_pattern         \.gz$           10080   90%     43200 
refresh_pattern         \.Z$            10080   90%     43200 
refresh_pattern         \.tgz$          10080   90%     43200 
refresh_pattern         \.tar$          10080   90%     43200 
refresh_pattern         \.vrm$          10080   90%     43200 
refresh_pattern         \.vrml$         10080   90%     43200 
refresh_pattern         \.aif$          10080   90%     43200 
refresh_pattern         \.aifc$         10080   90%     43200 
refresh_pattern         \.aiff$         10080   90%     43200 
refresh_pattern         \.arj$          10080   90%     43200 
refresh_pattern         \.c$            10080   90%     43200 
refresh_pattern         \.cpt$          10080   90%     43200 
refresh_pattern         \.dir$          10080   90%     43200 
refresh_pattern         \.dxr$          10080   90%     43200 
refresh_pattern         \.hqx$          10080   90%     43200 
refresh_pattern         \.jpe$          10080   90%     43200 
refresh_pattern         \.lha$          10080   90%     43200 
refresh_pattern         \.lzh$          10080   90%     43200 
refresh_pattern         \.midi$         10080   90%     43200 
refresh_pattern         \.movie$        10080   90%     43200 
refresh_pattern         \.mp2$          10080   90%     43200 
refresh_pattern         \.mpe$          10080   90%     43200 
refresh_pattern         \.mpeg$         10080   90%     43200 
refresh_pattern         \.mpga$         10080   90%     43200 
refresh_pattern         \.pl$           10080   90%     43200 
refresh_pattern         \.ppt$          10080   90%     43200 
refresh_pattern         \.ps$           10080   90%     43200 
refresh_pattern         \.qt$           10080   90%     43200 
refresh_pattern         \.qtm$          10080   90%     43200 
refresh_pattern         \.rar$          10080   90%     43200 
refresh_pattern         \.ras$          10080   90%     43200 
refresh_pattern         \.sea$          10080   90%     43200 
refresh_pattern         \.sit$          10080   90%     43200 
refresh_pattern         \.tif$          10080   90%     43200 
refresh_pattern         \.tiff$         10080   90%     43200 
refresh_pattern         \.snd$          10080   90%     43200 
refresh_pattern         \.wrl$          10080   90%     43200 
refresh_pattern         ^ftp:           1440    60%     22160
refresh_pattern         ^gopher:        1440    20%     1440
refresh_pattern         -i (cgi-bin|\?) 0       0%      0
refresh_pattern         .               480     50%     22160 

# ADMINISTRATIVE PARAMETERS
# -----------------------------------------------------------------------------
cache_mgr admin at example.com
mail_from squid at example.com
cache_effective_user squid
cache_effective_group squid

# DELAY POOL PARAMETERS
# -----------------------------------------------------------------------------
delay_pools 2 
delay_class 1 2
# When big_files are being downloaded, the first 5MB (625000 * 8 bits) are
# downloaded at max network speed. Once the file size limit of 5MB is reached,
# download speed drops to 438,000 bits or 3,504,000 MB per sec. Current
# contracted Internet connection speed w/ TP is at 7MB per sec.
delay_parameters 1 750000/750000 438000/625000
acl big_files url_regex -i ftp .exe .mp3 .vqf .tar.gz .gz .rpm .zip .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav .dmg .mp4 .img .flv .wmv .divx .mov .bz2 .deb
delay_access 1 allow big_files 
delay_access 1 deny all
delay_class 2 2
# Any files other than big_files are downloaded at wire speed (currently 7MB)
# until 6MB file size limit is reached and thereafter at 6MB per sec (750,000
# * 8 bits)
delay_parameters 2 750000/750000 750000/750000
delay_access 2 allow localnet 
delay_access 2 deny big_files 
delay_access 2 deny all
delay_initial_bucket_level 25

# DNS OPTIONS
# -----------------------------------------------------------------------------
dns_nameservers 192.168.1.1
append_domain .example.com

# MISCELLANEOUS
# -----------------------------------------------------------------------------
memory_pools on
memory_pools_limit none
cachemgr_passwd none all


Squid was compiled with the following options:

===> The following configuration options are available for squid-3.4.9:
     ARP_ACL=on: ARP/MAC/EUI based authentification
     AUTH_KERB=on: Install Kerberos authentication helpers
     AUTH_LDAP=on: Install LDAP authentication helpers
     AUTH_NIS=off: Install NIS/YP authentication helpers
     AUTH_SASL=off: Install SASL authentication helpers
     AUTH_SMB=on: Install SMB auth. helpers (req. Samba)
     AUTH_SQL=off: Install SQL based auth (uses MySQL)
     CACHE_DIGESTS=on: Use cache digests
     DEBUG=off: Build with extended debugging support
     DELAY_POOLS=on: Delay pools (bandwidth limiting)
     DNS_HELPER=on: Use external dnsserver processes for DNS
     DOCS=on: Build and/or install documentation
     ECAP=off: Loadable content adaptation modules
     ESI=off: ESI support
     EXAMPLES=on: Build and/or install examples
     FOLLOW_XFF=on: Support for the X-Following-For header
     FS_AUFS=on: AUFS (threaded-io) support
     FS_DISKD=on: DISKD storage engine controlled by separate service
     FS_ROCK=off: ROCK (unstable)
     HTCP=on: HTCP support
     ICAP=off: the ICAP client
     ICMP=off: ICMP pinging and network measurement
     IDENT=on: Ident lookups (RFC 931)
     IPV6=on: IPv6 protocol support
     KQUEUE=on: Kqueue(2) support
     LARGEFILE=on: Support large (>2GB) cache and log files
     LAX_HTTP=off: Do not enforce strict HTTP compliance
     SNMP=on: SNMP support
     SSL=on: SSL gatewaying support
     SSL_CRTD=off: Use ssl_crtd to handle SSL cert requests
     STACKTRACES=off: Enable automatic backtraces on fatal errors
     TP_IPF=off: Transparent proxying with IPFilter
     TP_IPFW=off: Transparent proxying with IPFW
     TP_PF=off: Transparent proxying with PF
     VIA_DB=off: Forward/Via database
     WCCP=on: Web Cache Coordination Protocol
     WCCPV2=on: Web Cache Coordination Protocol v2


Intially, I set mem_cache=2134MB and after noticing these memory leaks, I dropped it down to 1344MB. Memory leaks are still occurring.

Am I using anything that is known to cause memory leaks?

If there is additional information that you need, please do not hesitate to ask! Thanks.

~Doug






From Walter.H at mathemainzel.info  Mon Nov 24 20:26:45 2014
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 24 Nov 2014 21:26:45 +0100
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <5473078A.3060506@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
Message-ID: <54739485.3070601@mathemainzel.info>

Hi,

a sample of a .pac-File

function FindProxyForURL( url, host )
{
   var strURL = ( decodeURI( url ) ).toLowerCase( );

   var strHost = host.toLowerCase( );

   // Redirect to proxy with these URLs
   if ( ( strURL == "http://flickr.com/images/spaceball.gif" ) ||
        ( strURL == "..." ) )
     return "PROXY host:port;";

   // Don't use a proxy with these domains/hosts
   if ( dnsDomainIs( strHost, "www.domain.tld" ) ||
        dnsDomainIs( strHost, "..." ) )
     return "DIRECT";

   // Bad words or combinations, use localhost as proxy
   if ( ( strURL.search(/advert/) == -1 ) &&
        ( strURL.search(/banner/) == -1 ) &&
        ( strURL.search(/realmedia/) == -1 ) &&
        ( strURL.search(/[^a-z0-9]ads/) == -1 ) &&
        ( strURL.search(/[a-z][c,k]lick/) == -1 ) );
   else
     return "PROXY ntwaldivm-001.waldinet.home:8080;";

   // Use a proxy on higher domain levels,
   // except for educational or academical domains
   if ( dnsDomainLevels( strHost ) > 3 )
     if ( ( strHost.search(/.ac.[a-z][a-z]/) == ( strHost.length - 6 ) ) ||
          ( strHost.search(/.edu.[a-z][a-z]/) == ( strHost.length - 7 ) ) ||
          ( strHost.search(/.mail.yahoo.com/) == ( strHost.length - 15 ) 
) ||
          ( strHost.search(/.edu/) == ( strHost.length - 4 ) ) )
       return "PROXY host:port;";
     else
       return "DIRECT";

   // DIRECT connections to non-FQDN hosts
   if ( isPlainHostName( strHost ) )
     return "DIRECT";

   // Bad words or combinations in host, use proxy
   if ( ( strHost.search(/stat[0-9s]/) == -1 ) &&
        ( strHost.search(/media/) == -1 ) &&
        ( strHost.search(/spy/) == -1 ) &&
        ( strHost.search(/track/) == -1 ) &&
        ( strHost.search(/trade/) == -1 ) &&
        ( strHost.search(/klamm/) == -1 ) &&
        ( strHost.search(/trade/) == -1 ) &&
        ( strHost.search(/meter/) == -1 ) &&
        ( strHost.search(/poker/) == -1 ) &&
        ( strHost.search(/sex/) == -1 ) &&
        ( strHost.search(/xxx/) == -1 ) &&
        ( strHost.search(/cash/) == -1 ) &&
        ( strHost.search(/pay/) == -1 ) &&
        ( strHost.search(/popup/) == -1 ) &&
        ( strHost.search(/^ad./) == -1 ) &&
        ( strHost.search(/^www.ad./) == -1 ) &&
        ( strHost.search(/^www.free/) == -1 ) &&
        ( strHost.search(/^www.web/) == -1 ) );
   else
     return "PROXY host:port;";

   //  If nothing above matches, then go directly there,
   //  in case of a DNS failure use proxy
   if ( isResolvable( strHost ) )
     return "DIRECT";
   else
     return "PROXY host:port;";
}

any authorization to a proxy in this .pac-File is the same as if the 
proxy were
configured directly in the browser;
with such a setup you can use different proxies for e.g. specific sites
  even with SSL without SSL-bump ...

Walter


On 24.11.2014 11:25, Eliezer Croitoru wrote:
> I do know that pac files contains some form of JS and in the past I
> have seen couple complex PAC files but unsure about the options.
> I want to know if a PAC file can be used for
> Authentication\Authorization, maybe even working against another
> external system to get a token?
>
> Thanks,
> Eliezer


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141124/38c4c3a0/attachment.bin>

From james at ejbdigital.com.au  Mon Nov 24 20:42:59 2014
From: james at ejbdigital.com.au (James Harper)
Date: Mon, 24 Nov 2014 20:42:59 +0000
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <547328A1.6040100@ngtech.co.il>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
Message-ID: <HKNPR04MB19383C10ECC7830A250A2FAE8720@HKNPR04MB193.apcprd04.prod.outlook.com>

> 
> On 11/24/2014 02:43 PM, Kinkie wrote:
> > Hi Eliezer, I don't think so. PACfiles have no access to the DOM or
> > facilities like AJAX, and are very limited in what they can return
> > or affect as side-effects. In theory it could be possible to do
> > something, but in practice it would be only advisory and not
> > secure: a pacfile must by definition be in a publicly-accessible
> > URL, so anyone can read it and interpret it.
> 
> So a small question:
> Can I put the pac file on a https site with basic authentication?
> 

Seems like the sort of thing you could test with a minimum of effort...

James

From jorgeburgos at inaipyucatan.org.mx  Mon Nov 24 20:53:54 2014
From: jorgeburgos at inaipyucatan.org.mx (=?UTF-8?Q?Jorge_Iv=C3=A1n_Burgos_Aguilar?=)
Date: Mon, 24 Nov 2014 14:53:54 -0600
Subject: [squid-users] External ACL with an HTTP reply header format
	doesn't
In-Reply-To: <CAGCqWQQ9fE+nVw55=rKakoXk4X2+Z5NwSaVosm=pi1=WnrsWMQ@mail.gmail.com>
References: <CAGCqWQQ9fE+nVw55=rKakoXk4X2+Z5NwSaVosm=pi1=WnrsWMQ@mail.gmail.com>
Message-ID: <CAGCqWQSd0T8ZtBDKAhL3OkwRD54385VzAiS9kZYMbHEUqr58vA@mail.gmail.com>

Hi again,

Solved by using %<{Content-Type} log format instead of the one with an
additional h with it (recommend while running squid -k parse).
Actually is a bug in the code handling the parsing of the external acl's
but here is not the place to discuss it, i will open a new one bug report
asap on bugs.squid-cache.org

Kindest Regards

2014-11-20 13:35 GMT-06:00 Jorge Iv?n Burgos Aguilar <
jorgeburgos at inaipyucatan.org.mx>:

> Hi,
>
> First of all my setup information
> Distribution: CentOS Linux release 7.0.1406 (Core)
> Kernel Version: Linux 3.10.0-123.8.1.el7.x86_64 x86_64
> RPM Package Version: squid-3.3.8-12.el7_0.x86_64
> Squid Cache: Version 3.3.8
> configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--disable-strict-error-checking'
> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=$(localstatedir)/log/squid'
> '--with-pidfile=$(localstatedir)/run/squid.pid'
> '--disable-dependency-tracking' '--enable-eui'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos'
> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter'
> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
> '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
> -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>
>
> I'm currently working in a external acl (python based) to get the Source
> IP and the HTTP Reply Content-Type header
> in order to make a mime based filter based on the source ip (client ip)
> with a database backend.
> As the documentation for external_acl_type[1] says, i just need to use the
> format %<{Header}, for my specific
> need it should be%<h{Content-Type} (added the "h" because after running
> squid -k parse it states that i should use
> this form instead of the one described in the documentation).
> But after some testing it doesn't work...
>
> So m defining it in squid.conf like (Lower TTL's only for testing
> purposes):
> ------ relevant lines /etc/squid/squid.conf ------
> external_acl_type testfilter ttl=120 negative_ttl=120 children-max=5
> children-idle=1 ipv4 %SRC %<h{Content-Type} /usr/bin/python -u
> /external_acl/test.py
> acl test_filter external testfilter
> http_reply_access deny test_filter
> ------ relevant lines /etc/squid/squid.conf ------
>
> Considering that testfilter is simple a python script to log everything to
> %ea:
> ------ /external_acl/test.py ------
> #!/usr/bin/python -u
> import sys
> from urllib import quote
>
> EOF = False
> while not EOF:
>     line = sys.stdin.readline()
>     if not line:
>         EOF = True
>         continue
>
>     sys.stdout.write('{0} log={1}\n'.format('ERR', quote(line)))
>
> ------ /external_acl/test.py ------
>
> After configuring the logs everything is working fine except that the
> Content-Type is never presented to the external ACL
> it always show "-" instead of text/html as you can see in this example log
> line (squid format + %ea):
> 1416507449.579    814 10.0.0.100 TCP_MISS/200 8823 GET
> http://www.squid-cache.org/ - HIER_DIRECT/77.93.254.178 text/html
> 10.0.0.100%20-%0A
>
> After testing with other reply headers, not a single one is working (even
> the very common date or server reply header) all send the same "-"
> to the external_acl.
>
> So someone here managed to workout this? or have a different solution for
> the same problem? or is it bug in squid? or in the package provided by
> centos?
>
> Kindest Regards
>
> [1]
> http://www.squid-cache.org/Versions/v3/3.3/cfgman/external_acl_type.html
> --
> Jorge Iv?n Burgos Aguilar
>



-- 
Jorge Iv?n Burgos Aguilar
Instituto Estatal de Acceso a la Informaci?n P?blica del Estado de Yucat?n
www.inaipyucatan.org.mx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141124/4d72f262/attachment.htm>

From eliezer at ngtech.co.il  Mon Nov 24 22:38:20 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Nov 2014 00:38:20 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <HKNPR04MB19383C10ECC7830A250A2FAE8720@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <5473078A.3060506@ngtech.co.il>
 <CA+Y8hcMGFzbiuX4skFg-=vPc_5ybdR2k=u-JHpFoc9EbGCNtgQ@mail.gmail.com>
 <547328A1.6040100@ngtech.co.il>
 <HKNPR04MB19383C10ECC7830A250A2FAE8720@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <5473B35C.1040003@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Well james you are 100% right.
But as you may know not everybody can test all day long.
Some have day jobs and someone else knows the answer already and can
assist here and there.
This is the basic foundation of this mailing list.
Anyone anytime can ask a question and others will be able to respond.

Specifically squid-users is for squid-cache proxy but many will agree
with me that you can ask here things which are not directly related to
squid and find that others already knows the answer and you do not
need to run and start a Philosophy Dr research about it.

All The Bests,
Eliezer Croitoru

On 11/24/2014 10:42 PM, James Harper wrote:
> Seems like the sort of thing you could test with a minimum of
> effort...
> 
> James

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUc7NcAAoJENxnfXtQ8ZQUb0AH/j1b5RjHNRDVWrLyaItl0Xh0
FhIiDtSTDCX14VQ3HaUUTpey46Ov7gqUKNhw/A/hPHzksDCWvPPdi82jn72Pwfw2
mflKTZ5iA6ZI3txikpK/n7SerTzaLxP+mnFHvPdgxAkPsIH/yE22rCq4Kylqj18k
eaqK6JPF+zFPE8/x8KeAENoYraPJuOrDjbAly7dlvPe/gEmPYdbRJyWTPiVXS1jS
Lwx/w7yUWfAa5Ck27CamCgwK5tFNIKtURXBc4amfex04jvFhG+xEwceotU6urstk
P+os2+6uJ+yhYHch0RRnGbPKVcH3ttUJYYz0UBkz1xeCJ+GHlR0OlQStxc+aq/U=
=oxc/
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Nov 25 01:53:05 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Nov 2014 14:53:05 +1300
Subject: [squid-users] Memory Leak Squid 3.4.9 on FreeBSD 10.0 x64
In-Reply-To: <E6B2517F8D6DBF4CABB8F38ACA367E783431AF29@Draco.dawnsign.com>
References: <E6B2517F8D6DBF4CABB8F38ACA367E783431AF29@Draco.dawnsign.com>
Message-ID: <5473E101.4010907@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 25/11/2014 9:06 a.m., Doug Sampson wrote:
> Recently due to squid 2.7 being EOL'ed, we migrated our squid
> server to version 3.4.9 on a FreeBSD 10.0-RELEASE running on 64-bit
> hardware. We started seeing paging file being swapped out
> eventually running out of available memory. From the time squid
> gets started it usually takes about two days before we see these
> entries in /var/log/messages as follows:
> 
> +swap_pager_getswapspace(16): failed +swap_pager_getswapspace(16):
> failed +swap_pager_getswapspace(16): failed 
> +swap_pager_getswapspace(12): failed +swap_pager_getswapspace(16):
> failed +swap_pager_getswapspace(12): failed 
> +swap_pager_getswapspace(6): failed +swap_pager_getswapspace(16):
> failed
> 
> Looking at the 'top' results, I see that the swap file has been
> totally exhausted. Memory used by squid hovers around 2.3GB out of
> the total 3GB of system memory.
> 
> I am not sure what is causing these memory leaks. After rebooting,
> squid-internal-mgr/info shows the following statistics:
> 
> Squid Object Cache: Version 3.4.9 Build Info: Start Time:	Mon, 24
> Nov 2014 18:39:08 GMT Current Time:	Mon, 24 Nov 2014 19:39:13 GMT 
> Connection information for squid: Number of clients accessing
> cache:	18 Number of HTTP requests received:	10589 Number of ICP
> messages received:	0 Number of ICP messages sent:	0 Number of
> queued ICP replies:	0 Number of HTCP messages received:	0 Number of
> HTCP messages sent:	0 Request failure ratio:	 0.00 Average HTTP
> requests per minute since start:	176.2 Average ICP messages per
> minute since start:	0.0 Select loop called: 763993 times, 4.719 ms
> avg Cache information for squid: Hits as % of all requests:	5min:
> 3.2%, 60min: 17.0% Hits as % of bytes sent:	5min: 2.0%, 60min:
> 6.7% Memory hits as % of hit requests:	5min: 0.0%, 60min: 37.2% 
> Disk hits as % of hit requests:	5min: 22.2%, 60min: 33.2% Storage
> Swap size:	7361088 KB Storage Swap capacity:	58.5% used, 41.5%
> free Storage Mem size:	54348 KB Storage Mem capacity:	 3.9% used,
> 96.1% free Mean Object Size:	23.63 KB Requests given to unlinkd:	1 
> Median Service Times (seconds)  5 min    60 min: HTTP Requests
> (All):   0.10857  0.19742 Cache Misses:          0.10857  0.32154 
> Cache Hits:            0.08265  0.01387 Near Hits:
> 0.15048  0.12106 Not-Modified Replies:  0.00091  0.00091 DNS
> Lookups:           0.05078  0.05078 ICP Queries:           0.00000
> 0.00000 Resource usage for squid: UP Time:	3605.384 seconds CPU
> Time:	42.671 seconds CPU Usage:	1.18% CPU Usage, 5 minute avg:
> 0.72% CPU Usage, 60 minute avg:	1.17% Maximum Resident Size: 845040
> KB Page faults with physical i/o: 20 Memory accounted for: Total
> accounted:       105900 KB memPoolAlloc calls:   2673353 
> memPoolFree calls:    2676487 File descriptor usage for squid: 
> Maximum number of file descriptors:   87516 Largest file desc
> currently in use:    310 Number of file desc currently in use:
> 198 Files queued for open:                   0 Available number of
> file descriptors: 87318 Reserved number of file descriptors:   100 
> Store Disk files open:                   0 Internal Data
> Structures: 311543 StoreEntries 4421 StoreEntries with MemObjects 
> 4416 Hot Object Cache Items 311453 on-disk objects
> 
> I will post another one tomorrow that will indicate growing
> memory/swapfile consumption.
> 
> Here is my squid.conf:
> 
> # OPTIONS FOR AUTHENTICATION #
> -----------------------------------------------------------------------------
>
> 
# 1st four lines for
> auth_param basic children 5 auth_param basic realm Squid
> proxy-caching web server auth_param basic credentialsttl 2 hours 
> auth_param basic casesensitive off #  next three lines for kerberos
> authentication (needed to use usernames) #  used in conjunction
> with "acl auth proxy_auth" line below #auth_param negotiate program
> /usr/local/libexec/squid/negotiate_kerberos_auth -i #auth_param
> negotiate children 50 startup=10 idle=5 #auth_param negotiate
> keep_alive on
> 
> 
> # ACCESS CONTROLS #
> -----------------------------------------------------------------------------
>
> 
# Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing #
> should be allowed #acl manager proto cache_object acl manager
> url_regex -i ^cache_object:// /squid-internal-mgr/ acl adminhost
> src 192.168.1.149 acl localnet src 192.168.1.0/24	# RFC1918
> possible internal network acl localnet src fc00::/7           # RFC
> 4193 local private network range acl localnet src fe80::/10
> # RFC 4291 link-local (directly plugged) machines acl webserver src
> 198.168.1.35 acl some_big_clients src 192.168.1.149/32 #CI53
> 
> # We want to limit downloads of these type of files # Put this all
> in one line acl magic_words url_regex -i ftp .exe .mp3 .vqf .tar.gz
> .gz .rpm .zip .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav
> .dmg .mp4 .img # We don't block .html, .gif, .jpg and similar
> files, because they # generally don't consume much bandwidth

But you do. Whenever the domain name or path contains any of the byte
sequences in that regex above. The entire websites
http://www.divx.com/  and http://isohunt.com/ for example.

And whats wrong with adding more HITs ? even if they are small enough
not to use much cache space.

<snip>
> 
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM #
> -----------------------------------------------------------------------------
>
> 
hierarchy_stoplist cgi-bin ?


... but you dont have neighbours. This is also deprecated anyway.

> 
> # MEMORY CACHE OPTIONS #
> -----------------------------------------------------------------------------
>
> 
cache_mem 1366 MB
> #cache_mem 2134 MB #maximum_object_size_in_memory 64 KB 
> maximum_object_size_in_memory 128 KB
> 
> # DISK CACHE OPTIONS #
> -----------------------------------------------------------------------------
>
> 
cache_replacement_policy heap LFUDA
> cache_dir aufs /data/squid/aufs_cache 4096 16 256 min-size=131073 
> cache_dir diskd /data/squid/diskd_cache 8192 16 256 Q1=64 Q2=72
> max-size=131072

Why the segregation between diskd and aufs?

The only difference between these cache types is the method if I/O
performed accessing the disk. AUFS is threaded SMP, diskd is
multi-process SMP.

NP: FreeBSD 10 seem to have resolved the issues Squid AUFS has with
older BSD and people are now noticing the speed issues with diskd.

The official recommendation is currently to use AUFS with FreeBSD 10+
and diskd with older FreeBSD.


> #maximum_object_size 122880 KB maximum_object_size 153600 KB 
> cache_swap_low 90 cache_swap_high 95
> 
> # LOGFILE OPTIONS #
> -----------------------------------------------------------------------------
>
> 
access_log daemon:/data/squid/logs/access.log
> cache_store_log daemon:/data/squid/logs/store.log cache_swap_log
> /var/spool/squid/%s

What is this %s ??

> logfile_rotate 28
> 
> # OPTIONS FOR TROUBLESHOOTING #
> -----------------------------------------------------------------------------
>
> 
cache_log /data/squid/logs/cache.log
> # Leave coredumps in the first cache dir coredump_dir /data/squid
> 
> # OPTIONS FOR EXTERNAL SUPPORT PROGRAMS #
> -----------------------------------------------------------------------------
>
> 
diskd_program /usr/local/libexec/squid/diskd
> 

Unless you are replacing this helper with a custom-built one with
strange name this should not be configured explicitly in Squid-3.


> # OPTIONS FOR TUNING THE CACHE #
> -----------------------------------------------------------------------------
>
> 
refresh_pattern http://.*\.windowsupdate\.microsoft\.com/ 0 80% 20160
> refresh_pattern http://office\.microsoft\.com/ 0 80% 20160 
> refresh_pattern http://windowsupdate\.microsoft\.com/ 0 80% 20160 
> refresh_pattern http://w?xpsp[0-9]\.microsoft\.com/ 0 80% 20160 
> refresh_pattern http://w2ksp[0-9]\.microsoft\.com/ 0 80% 20160 
> refresh_pattern http://download\.microsoft\.com/ 0 80% 20160 
> refresh_pattern http://download\.macromedia\.com/ 0 80% 20160 
> refresh_pattern http://ftp\.software\.ibm\.com/ 0 80% 20160 
> refresh_pattern         cgi-bin         1 20% 2 refresh_pattern
> \.asp$          1 20% 2 refresh_pattern         \.acgi$         1
> 20% 2 refresh_pattern         \.cgi$          1 20% 2 
> refresh_pattern         \.pl$           1 20% 2 refresh_pattern
> \.shtml$        1 20% 2 refresh_pattern         \.php3$         1
> 20% 2 refresh_pattern         \?              1 20% 2 
> refresh_pattern         \.gif$          10080   90%     43200 
> refresh_pattern         \.png$          10080   90%     43200 
> refresh_pattern         \.jpg$          10080   90%     43200 
> refresh_pattern         \.ico$          10080   90%     43200 
> refresh_pattern         \.bom\.gov\.au     30   20%       120 
> refresh_pattern         \.html$           480   50%     22160 
> refresh_pattern         \.htm$            480   50%     22160 
> refresh_pattern         \.css$            480   50%     22160 
> refresh_pattern         \.js$             480   50%     22160 
> refresh_pattern         \.class$        10080   90%     43200 
> refresh_pattern         \.zip$          10080   90%     43200 
> refresh_pattern         \.jpeg$         10080   90%     43200 
> refresh_pattern         \.mid$          10080   90%     43200 
> refresh_pattern         \.shtml$          480   50%     22160 
> refresh_pattern         \.exe$          10080   90%     43200 
> refresh_pattern         \.thm$          10080   90%     43200 
> refresh_pattern         \.wav$          10080   90%     43200 
> refresh_pattern         \.mp4$          10080   90%     43200 
> refresh_pattern         \.txt$          10080   90%     43200 
> refresh_pattern         \.cab$          10080   90%     43200 
> refresh_pattern         \.au$           10080   90%     43200 
> refresh_pattern         \.mov$          10080   90%     43200 
> refresh_pattern         \.xbm$          10080   90%     43200 
> refresh_pattern         \.ram$          10080   90%     43200 
> refresh_pattern         \.iso$          10080   90%     43200 
> refresh_pattern         \.avi$          10080   90%     43200 
> refresh_pattern         \.chtml$          480   50%     22160 
> refresh_pattern         \.thb$          10080   90%     43200 
> refresh_pattern         \.dcr$          10080   90%     43200 
> refresh_pattern         \.bmp$          10080   90%     43200 
> refresh_pattern         \.phtml$          480   50%     22160 
> refresh_pattern         \.mpg$          10080   90%     43200 
> refresh_pattern         \.pdf$          10080   90%     43200 
> refresh_pattern         \.art$          10080   90%     43200 
> refresh_pattern         \.swf$          10080   90%     43200 
> refresh_pattern         \.flv$          10080   90%     43200 
> refresh_pattern         \.x-flv$        10080   90%     43200 
> refresh_pattern         \.mp3$          10080   90%     43200 
> refresh_pattern         \.ra$           10080   90%     43200 
> refresh_pattern         \.spl$          10080   90%     43200 
> refresh_pattern         \.viv$          10080   90%     43200 
> refresh_pattern         \.doc$          10080   90%     43200 
> refresh_pattern         \.gz$           10080   90%     43200 
> refresh_pattern         \.Z$            10080   90%     43200 
> refresh_pattern         \.tgz$          10080   90%     43200 
> refresh_pattern         \.tar$          10080   90%     43200 
> refresh_pattern         \.vrm$          10080   90%     43200 
> refresh_pattern         \.vrml$         10080   90%     43200 
> refresh_pattern         \.aif$          10080   90%     43200 
> refresh_pattern         \.aifc$         10080   90%     43200 
> refresh_pattern         \.aiff$         10080   90%     43200 
> refresh_pattern         \.arj$          10080   90%     43200 
> refresh_pattern         \.c$            10080   90%     43200 
> refresh_pattern         \.cpt$          10080   90%     43200 
> refresh_pattern         \.dir$          10080   90%     43200 
> refresh_pattern         \.dxr$          10080   90%     43200 
> refresh_pattern         \.hqx$          10080   90%     43200 
> refresh_pattern         \.jpe$          10080   90%     43200 
> refresh_pattern         \.lha$          10080   90%     43200 
> refresh_pattern         \.lzh$          10080   90%     43200 
> refresh_pattern         \.midi$         10080   90%     43200 
> refresh_pattern         \.movie$        10080   90%     43200 
> refresh_pattern         \.mp2$          10080   90%     43200 
> refresh_pattern         \.mpe$          10080   90%     43200 
> refresh_pattern         \.mpeg$         10080   90%     43200 
> refresh_pattern         \.mpga$         10080   90%     43200 
> refresh_pattern         \.pl$           10080   90%     43200 
> refresh_pattern         \.ppt$          10080   90%     43200 
> refresh_pattern         \.ps$           10080   90%     43200 
> refresh_pattern         \.qt$           10080   90%     43200 
> refresh_pattern         \.qtm$          10080   90%     43200 
> refresh_pattern         \.rar$          10080   90%     43200 
> refresh_pattern         \.ras$          10080   90%     43200 
> refresh_pattern         \.sea$          10080   90%     43200 
> refresh_pattern         \.sit$          10080   90%     43200 
> refresh_pattern         \.tif$          10080   90%     43200 
> refresh_pattern         \.tiff$         10080   90%     43200 
> refresh_pattern         \.snd$          10080   90%     43200 
> refresh_pattern         \.wrl$          10080   90%     43200 
> refresh_pattern         ^ftp:           1440    60%     22160 
> refresh_pattern         ^gopher:        1440    20%     1440 
> refresh_pattern         -i (cgi-bin|\?) 0       0%      0 
> refresh_pattern         .               480     50%     22160
> 

That is a LOT of regex comparisions the proxy is having to do at least
once per-request.

The special rules you have up the top for "cgi-bin" and "\?" are also
violating HTTP safe behaviour. The default rule we provide is highly
tuned to handle caching of those responses safely without breaking old
legacy scripts.


At least most of them end with $ anchor point to prevent random URLs
matching.


> # ADMINISTRATIVE PARAMETERS #
> -----------------------------------------------------------------------------
>
> 
cache_mgr admin at example.com
> mail_from squid at example.com cache_effective_user squid 
> cache_effective_group squid
> 
> # DELAY POOL PARAMETERS #
> -----------------------------------------------------------------------------
>
> 
delay_pools 2
> delay_class 1 2 # When big_files are being downloaded, the first
> 5MB (625000 * 8 bits) are # downloaded at max network speed. Once
> the file size limit of 5MB is reached, # download speed drops to
> 438,000 bits or 3,504,000 MB per sec. Current # contracted Internet
> connection speed w/ TP is at 7MB per sec. delay_parameters 1
> 750000/750000 438000/625000

> acl big_files url_regex -i ftp .exe .mp3 .vqf .tar.gz .gz .rpm .zip
> .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav .dmg .mp4
> .img .flv .wmv .divx .mov .bz2 .deb

Another long list of regex patterns. Notice how these are permitted to
match anywhere in the entie URL. Including domain names.

FTP traffic in particular is not guaranteed to be "big files".

<snip>
> Intially, I set mem_cache=2134MB and after noticing these memory
> leaks, I dropped it down to 1344MB. Memory leaks are still
> occurring.
> 
> Am I using anything that is known to cause memory leaks?
> 
> If there is additional information that you need, please do not
> hesitate to ask! Thanks.

A copy of the manager "mem" report would be very useful to see whats
using the memory.
 Note that it is a TSV format, so please save as .tsv file and attach.
rather than cut-n-pasting inline.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUc+EAAAoJELJo5wb/XPRj7TMH/jko8cuy/iURLknvFadt6Yn7
QKh8LSV63yLClh6dWxVGvHLbFcF+GQDEn2SGaZ9ZGxt5yEjAJjwoSZDTpFcFNZPf
ocROu8/R1Ys69PCGHMLJh8DzRaXrLW1/OPrt1hcSuogWKUnNCEbgg+g3MpQO4AmM
4AgwIQyx8O3kE39CHTSKy5luCFzj8pMB/qr20AwjWiM4eG+MV81OWQpJL+AnhH1s
5LYZdLXtQ16BR0TT8uokYmnzS2B+B51VC9HYKEJdWz9BALgjMHQhcDtYtRZh7pV/
ppdARfj984xYk9l4wsmkFMIlBp4aDNMhJiIZcNp8t96MKwLeDgoCaaGh6dDBa/Q=
=wKvq
-----END PGP SIGNATURE-----


From matt_m at me.com  Tue Nov 25 14:23:01 2014
From: matt_m at me.com (Mathew Marulla)
Date: Tue, 25 Nov 2014 09:23:01 -0500
Subject: [squid-users] Persistent Connections - only one side
In-Reply-To: <EADFA361-A1CA-4462-9B9B-7DF26FC8FD9B@me.com>
References: <EADFA361-A1CA-4462-9B9B-7DF26FC8FD9B@me.com>
Message-ID: <9E0D4AC4-D2E3-42F1-87FA-C0948669DA2B@me.com>

Need to know if this is even possible?

We have a PHP app that uses CURL to send requests/get responses from servers of various business partners.  The connections to these partner servers are not currently persistent.  Setting keep-alive headers would be ineffective because the connections are necessarily closed when the PHP code exists, at the end of each transaction.  Something like this:

http://marulla.com/files/proxy1.png

Now, we have a potential partner that requires persistent, re-used connections.  Changing the system architecture so that the PHP code remains running is not an option.  I was wondering if a proxy server, running on the same server as the PHP code, could keep the outside half persistent, even after the inside half closes.  Like this:

http://marulla.com/files/proxy2.png

Is this possible?  Is there a config option that might achieve this? 

Thanks!



From nobody at hushmail.com  Tue Nov 25 16:38:13 2014
From: nobody at hushmail.com (HaxNobody)
Date: Tue, 25 Nov 2014 08:38:13 -0800 (PST)
Subject: [squid-users] Existing root certificate not working with SSL Bump
	(squid 3.3.10)
Message-ID: <1416933493483-4668515.post@n4.nabble.com>

Hello,

We are trying to configure Squid with SSL bump in order to filter traffic
with a content filter. We have an existing self-signed root certificate and
private key that we use successfully with other similar proxy software, and
we wish to re-use it with Squid so that we don't have to distribute a new
root certificate to our clients.

However, when we try to use our existing root with Squid, we get SSL errors
from the browser and we are quite stumped as to why they are happening. We
have provided the certificate and private key to Squid and it is
successfully decrypting and re-encrypting the traffic after sending it
through our filter. The message we get from Firefox is as follows:

"www.google.com uses an invalid security certificate. The certificate is not
trusted because no issuer chain was provided. (Error code:
sec_error_unknown_issuer)"

Chrome says:

"NET::ERR_CERT_AUTHORITY_INVALID"

Please note that this happens with any SSL site, not just Google. If we go
and view the certificate that the browser sees, it appears to be valid and
it shows the issuer as our custom root certificate, as expected. As I
mentioned previously, this root is already installed into Firefox (and into
the windows trusted root store as well). Internet Explorer and Google Chrome
give similar errors.

I have used openssl to verify that the modulus on the key and the
certificate match (they do). Oddly, we can generate a new certificate and it
will work correctly as expected, although that would require us to
distribute a new root. Does anyone have any idea why our existing root will
not work, but a new one will? Does Squid require certain extensions or other
things that our existing certificate might not have?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Existing-root-certificate-not-working-with-SSL-Bump-squid-3-3-10-tp4668515.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vrogoziansky.squid at gmail.com  Tue Nov 25 18:22:07 2014
From: vrogoziansky.squid at gmail.com (Vadim Rogoziansky)
Date: Tue, 25 Nov 2014 20:22:07 +0200
Subject: [squid-users] Transparent proxy with Peek and Splice feature.
Message-ID: <5474C8CF.6030404@gmail.com>

Hello All.

My goal is to do ssl bumping in transparent proxy mode with domain 
exclude possibility.
Let me tell you about squid's strange behaviour when I'm trying to do it.

In browsers it says something like this:
/This server could not prove that it is www.ukr.net; its security 
certificate is from212.42.76.253. This may be caused by a 
misconfiguration or an attacker intercepting your connection.//
//NET::ERR_CERT_COMMON_NAME_INVALID//
//Subject: 212.42.76.253//
/
Looks like squid takes the CN from the certificate as IP address of the 
destination domain.
But, everything works smoothly when I use proxy in non transparent mode 
and put it to the browser directly . I can successfully bypass bad sites 
and do ssl bumping on others. There are no certificate errors except of 
some of them, you know)

My OS is /Centos 6.5 //2.6.32-358.6.2.el6.x86_64/
My squid's version:
//opt/squid/sbin/squid -v//
//Squid Cache: Version 3.5.0.2//
//Service Name: squid//
//configure options:  '--with-openssl' '--enable-linux-netfilter' 
'--disable-ipv6' '--enable-icap-client' '--enable-ssl-crtd' 
'--prefix=/opt/squid' '--enable-external-acl-helpers=none' 
'--enable-auth-negotiate=none' '--enable-follow-x-forwarded-for' 
'--disable-auth-ntlm' '--disable-arch-native' '--enable-wccpv2' 
'--enable-snmp' 
'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig' 
--enable-ltdl-convenience//
/
My iptables which is doing redirecting to internal squid ports: /
//Table: nat//
//Chain PREROUTING (policy ACCEPT)//
//num  target     prot opt source               destination//
//1    ACCEPT     tcp  --  0.0.0.0/0 192.168.0.121       tcp dpt:443 /* 
accept connection *///
//2    REDIRECT   tcp  --  192.168.0.0/24 0.0.0.0/0           tcp 
dpt:443 /* redirect */ redir ports 3132//
//3    ACCEPT     tcp  --  0.0.0.0/0 192.168.0.121       tcp dpt:80 /* 
accept connection *///
//4    REDIRECT   tcp  --  192.168.0.0/24 0.0.0.0/0           tcp dpt:80 
/* redirect */ redir ports 3131/

Here is my squid configuration file:
___________________________
visible_hostname local.local
always_direct allow all
dns_nameservers 8.8.8.8

acl step2 at_step SslBump2
ssl_bump stare step2 all
acl sslBumpDeniedDstDomain dstdomain ukr.net www.ukr.net
ssl_bump splice sslBumpDeniedDstDomain
ssl_bump bump all

http_port 3128 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/opt/squid/var/ssl_cert/cert.pem

http_port 3131 transparent
https_port 3132 transparent ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/opt/squid/var/ssl_cert/cert.pem

http_access allow all

sslcrtd_program /opt/squid/libexec/ssl_crtd -s /opt/squid/var/ssl_db -M 4MB
sslcrtd_children 15

logformat logaccess  [%{%d/%b/%Y  %H:%M:%S}tl] %>a %Ss/%03>Hs %<st %rm 
%ru %un %Sh/%<A %mt
access_log daemon:/opt/squid/var/logs/access.log logaccess
______________________________________________________

Also, I've run squid like this *//opt/squid/sbin/squid -N -X -d 2/*  and 
got interesting strings like:
/2014/11/26 04:28:08.622| client_side.cc(3849) 
httpsSslBumpAccessCheckDone: sslBump needed 
for//*local=212.42.76.246:443*////*remote=192.168.0.122:63719*//FD 40 
flags=33 method 5/

Here, the local and remote IP addresses are switched (I checked such 
lines when went through the squid directly).

Please, tell me what can be wrong in configuration or squid.  I can 
provide you with any logs which you may need.
BTW, cache.log is clean.
________________
Best regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141125/0954f8d9/attachment.htm>

From eliezer at ngtech.co.il  Tue Nov 25 18:25:33 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Nov 2014 20:25:33 +0200
Subject: [squid-users] Persistent Connections - only one side
In-Reply-To: <9E0D4AC4-D2E3-42F1-87FA-C0948669DA2B@me.com>
References: <EADFA361-A1CA-4462-9B9B-7DF26FC8FD9B@me.com>
 <9E0D4AC4-D2E3-42F1-87FA-C0948669DA2B@me.com>
Message-ID: <5474C99D.7000904@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Mathew,

The defaults of squid should do what you are asking for as long as we
are talking about the same src domain and IP.
There are settings to make the persistent connections more effective
but I do not remember them by heart.
The start point would be here:
http://www.squid-cache.org/Doc/config/server_persistent_connections/

and everything else should at the official docs at:
http://www.squid-cache.org/Doc/config/

Notice that persistent connections are not the answer to everything
because these have limitation and constrains.
In many networks a overhead even a dozen and more TCP connection would
be more effective then persistent connections.
Any case should be tested first to understand the nature of the demand
and to decide if it's the best path.
The client doesn't always right...but he is still the client so..

Eliezer

On 11/25/2014 04:23 PM, Mathew Marulla wrote:
> Need to know if this is even possible?
> 
> We have a PHP app that uses CURL to send requests/get responses
> from servers of various business partners.  The connections to
> these partner servers are not currently persistent.  Setting
> keep-alive headers would be ineffective because the connections are
> necessarily closed when the PHP code exists, at the end of each
> transaction.  Something like this:
> 
> http://marulla.com/files/proxy1.png
> 
> Now, we have a potential partner that requires persistent, re-used
> connections.  Changing the system architecture so that the PHP code
> remains running is not an option.  I was wondering if a proxy
> server, running on the same server as the PHP code, could keep the
> outside half persistent, even after the inside half closes.  Like
> this:
> 
> http://marulla.com/files/proxy2.png
> 
> Is this possible?  Is there a config option that might achieve
> this?
> 
> Thanks!

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUdMmdAAoJENxnfXtQ8ZQUbJYH/007hNd9AHopz/y0XVvV8sls
lJdPMStt7+IY1HVzix8g7zDuplVIvT+6/dRG4eMddzTK/XtJ5C7l+aPCY8LQsLo0
qLRFhdFYREEyNXU7UwGQHLZNxJ+bPT/G8mSwX5sUkJYU8wO6XALq4bhJCO+mnvBh
TLB867Ty8TU2Sixlc4CZdVaFdoiv9G91zgUPNRJy0N192vXZOccjiZ/3eXjWpcbi
n6ZNydsAhlKS6jz0ta9qDxiA57qbbBzFg2+wESHf9dMo5G6vPeXQ7pVUklfYZlxc
tf0p0DHeRdHNT4UeH+X1cwd9/304hryCZaVF8Kk1vYR7sCvi4trpVAQczqABbIY=
=Qc/b
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Tue Nov 25 18:33:16 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Nov 2014 20:33:16 +0200
Subject: [squid-users] Authentication\Authorization using a PAC file?
In-Reply-To: <54738FAA.9090309@trimble.com>
References: <5473078A.3060506@ngtech.co.il> <54738FAA.9090309@trimble.com>
Message-ID: <5474CB6C.6050505@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 11/24/2014 10:06 PM, Jason Haar wrote:
> I think you are confusing proxy authentication with WPAD/PAC files.
> WPAD knows nothing about proxy authentication: browsers do
> 
> ie you use WPAD to tell browsers where/if they need to use a proxy
> and under what circumstances, and when they then attempt to do so,
> the BROWSER will have to respond to authentication issues
> surrounding authentication proxies.

Thanks Jason for the referrals.
WPAD purpose is solid but there is an option to create a WPAD/PAC file
on the fly and on the background authorize the client user name with a
specific ip.
Some restrictions can be applied on the use of a specific user to be
used only with one IP at a time which is similar to some AD features.

WPAD can be used with authorization and it should be like any other
http\s resource and the user + password can be written inside the url
similar to ftp URL.
Windows specifically has a daemon\script\service that does the
WPAD\PAC file and there for all the limitations are to this service only.
In any other system these limitations might be the same but I didn't
tested any of these yet.

I might add this to my TODO list.

Thanks,
Eliezer
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUdMtsAAoJENxnfXtQ8ZQUgsIIAIl5CUYcvYWIY4rNhG8ZTBWe
4oB08L0yHWPt7lbN/PEksxp+8dsJo5ZD0VTXMHD7CwRh+sIy8Nb01HaW+Rpb8hDD
CzFJjAnscEw99kJr+0Yv9WZ+Ba+O6JfbmzFdozV6fHrnD4fd4L8Kni3EXCUcL1KS
iUHepiOkLq/Bf4MmSo9Eo49Ao7cRNAKEAC5t8I6wkLOCWo/ijxmBZimpnRwNMba9
z6KUUcRp1biwSyFfaUPqGCRtwaWxQPLmgkZI1ABJvO+mWyiMX1Droq8+Zz8iioJs
mRBrydNnBIqBf+LjADkYJsNnQeLSJ2hsHJe2O0USM1+l3JhNzE7R/o3Y5HnVC3E=
=HwV7
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 26 09:33:32 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Nov 2014 22:33:32 +1300
Subject: [squid-users] Transparent proxy with Peek and Splice feature.
In-Reply-To: <5474C8CF.6030404@gmail.com>
References: <5474C8CF.6030404@gmail.com>
Message-ID: <54759E6C.5080404@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 26/11/2014 7:22 a.m., Vadim Rogoziansky wrote:
> Hello All.
> 
> My goal is to do ssl bumping in transparent proxy mode with domain 
> exclude possibility. Let me tell you about squid's strange
> behaviour when I'm trying to do it.
> 
> In browsers it says something like this: /This server could not
> prove that it is www.ukr.net; its security certificate is
> from212.42.76.253. This may be caused by a misconfiguration or an
> attacker intercepting your connection.// 
> //NET::ERR_CERT_COMMON_NAME_INVALID// //Subject: 212.42.76.253// / 
> Looks like squid takes the CN from the certificate as IP address of
> the destination domain.

Squid takes the IP address from the TCP packet. Which is all that is
available in NAT intercepted traffic at bumping step #1.

The ACLs you have therefore determine that "bump" action is to happen.
Correct?

The cert details are therefore mimic'ed from what gets delivered by
the server.

It may be that the server is depending on SNI to generate its own
cert, but since Squid deos not have that domain name already an
IP-based cert comes back.

It may also be that some ISP upstream of you is bumping the encryption
with client-first method.



> But, everything works smoothly when I use proxy in non transparent
> mode and put it to the browser directly .

In which case the browser sends domain name to the proxy in its
CONNECT message starting the HTTPS. The possible results are very
different.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUdZ5sAAoJELJo5wb/XPRj0qIIANBjuFvq45hPmcaj/NYL6bza
7ttt5Gn+tn8E5KH7T4wfQhUXr91UIsYWfOswfnVAAlBevIO/iFVoDN5hAOveuhIl
ra/0eGti1EpZ3LHJiAqmo0mHsrz3v9+PAduVrXgUJLyYDiM0xctg0nRhj2u166VX
j0IL3g8CKEw+KiWVJM9HdLaDEz9fYtHBO8UHhKDDE94O9yxScIvB+GAhN4YlTtrE
z65VJkSCEw+3vH6XcrrkF2aEnB20jeEGiV5puO2cPoJpgcg3ic8sMVEfa/Z1qwqa
KCkj2XI28wBCIovCV+AfBhpvW0o8eVFbt4ESodLTmwjUvU+m8zxky/9cjO5kyLE=
=kgug
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 26 09:45:08 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Nov 2014 22:45:08 +1300
Subject: [squid-users] Existing root certificate not working with SSL
 Bump (squid 3.3.10)
In-Reply-To: <1416933493483-4668515.post@n4.nabble.com>
References: <1416933493483-4668515.post@n4.nabble.com>
Message-ID: <5475A124.4050208@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 26/11/2014 5:38 a.m., HaxNobody wrote:
> Hello,
> 
> We are trying to configure Squid with SSL bump in order to filter
> traffic with a content filter. We have an existing self-signed root
> certificate and private key that we use successfully with other
> similar proxy software, and we wish to re-use it with Squid so that
> we don't have to distribute a new root certificate to our clients.
> 
> However, when we try to use our existing root with Squid, we get
> SSL errors from the browser and we are quite stumped as to why they
> are happening.

The story begins here:
https://www.imperialviolet.org/2011/05/04/pinning.html

.. the other browsers picked up and also started pinning domain
certificates some time ago.

The rest of the story is that Squid 3.3 is now quite old and in terms
of ssl-bump specifically is it outright obsolete technology. Your best
chance is to upgrade to the latest release and try again. A fix will
only be worth fixing (or even investigating) if the problem persists
with the latest Squid-3.5 (beta) ssl-bump features.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUdaEkAAoJELJo5wb/XPRjCqEH/AhtJLeDaFEJfNDodZkcPLU/
KlvBtvPKQBkint01uNYONNSH5VEIRGBwoDcLmMeczswforgUjQPB6RfQEFbf0KU0
6vGT2c7i2l+vYHY4OBEkCFN1DklW/Z/caPjKfN8C2bJw863CtYLoMi3LUHH46txC
3xLeRHGerWY6AGUcSwvw0V33zGrhxXHgPugii6iTQ6juaCOJxpKiEyftwYGuCZxa
y1r4htpskSUjlJBX1N6Fj1cSuJ8L9rpsubEts/ENDeuPWj/YXHPX/N9iFhLQ6Trr
bMH9zc/CHOpxYJNJQIjnowQNMh2oeEc3pISnSRSgoEDEXZ28kg9qi97SdeR8ayQ=
=N4Au
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 26 10:08:18 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Nov 2014 23:08:18 +1300
Subject: [squid-users] External ACL with an HTTP reply header format
	doesn't
In-Reply-To: <CAGCqWQSd0T8ZtBDKAhL3OkwRD54385VzAiS9kZYMbHEUqr58vA@mail.gmail.com>
References: <CAGCqWQQ9fE+nVw55=rKakoXk4X2+Z5NwSaVosm=pi1=WnrsWMQ@mail.gmail.com>
 <CAGCqWQSd0T8ZtBDKAhL3OkwRD54385VzAiS9kZYMbHEUqr58vA@mail.gmail.com>
Message-ID: <5475A692.2060805@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 25/11/2014 9:53 a.m., Jorge Iv?n Burgos Aguilar wrote:
> Hi again,
> 
> Solved by using %<{Content-Type} log format instead of the one with
> an additional h with it (recommend while running squid -k parse). 
> Actually is a bug in the code handling the parsing of the external
> acl's but here is not the place to discuss it, i will open a new
> one bug report asap on bugs.squid-cache.org
> 

For the record Jorge opened
http://bugs.squid-cache.org/show_bug.cgi?id=4148

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUdaaRAAoJELJo5wb/XPRj7nkH/jVAKc4/IQx1MdfUbSQvmtSy
1GpkJOW0XzqoYgU4tU7v5d1pbPHHGHlE1d5lNx5hgUfEW8bTlOz5KkZJe6CdLja4
z7g96hzz8ih5XvVlwKLD+Vxp+eSR7YuZUpHr60wQPOqxn7GA7eh04Ch7kXmVx7nk
G3W4o3BBJwlFC16H+xAP+B7YU3V20Rn4/NagH4LGWDkcuunLZVZosSWb8r4w68tK
X7kbdIHpQ9zJjuHEvq4LZ9EfJKM4vo+NZgUpfq3JHxmRq9QoUvianpjG7B4enNON
MNIXiAtphD0vI3r7kUY5I2AYd90DyHvP4EJ2kUJJgT6YxPHFck3Zn832HL0/iHs=
=/vKz
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Nov 26 10:27:25 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Nov 2014 23:27:25 +1300
Subject: [squid-users] squid 3.5x: Active Directory accounts with space
 issue
In-Reply-To: <5471BE7D.6030206@articatech.com>
References: <5471BE7D.6030206@articatech.com>
Message-ID: <5475AB0D.4030608@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 24/11/2014 12:01 a.m., David Touzeau wrote:
> Hi
> 
> We have connected 3.5.0.2-20141121-r13666 with Active Directory. It
> seems where there are spaces in login account squid use only the
> last argument.
> 
> For example for an account "Jhon smith" squid use "smith" only For
> example for an account "Dr Jhon smith" squid use "smith" only
> 
> In 3.3.13 there is no such issue, a "Jhon smith" account is logged
> as "Jhon smith" and sended as Jhon%20smith to helpers

Any information about the auth Scheme being performed?
 the helpers being used?
 and what is being sent to/from the helpers in 3.5 different from the
3.3 version?

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUdasMAAoJELJo5wb/XPRjRPUH/2aVKrtNdmJzupzsN9JtcOK0
1e+NIxNSaDiyu9R03eJrwlAy7g9zFGEj+0dI1HgJz36Mf2i03ahbyinD4GwFDVPh
a6iYyCPrhy2XDeL16qcSqsX0i2e8yXO/WRbFTJymKMOFhVDS05Bg6KuE1FroNjHG
OkhpzN/T3O1fUW2k0XSRZEWFV1YnriwcCLdKXdsXEXEIIA3J9ZN0WQZ8I/oGXfWV
S4xHKh4jnDFJCEO5lwYxT1CDe53CCHnPfV9Uf1Dhq6AkKnDZAR8U53Uyhji4V6ck
UzwZEPMAtK73O3uXn0J2l2S9v0ga5ymHRhiWADG2jC/8dyAc0ICaWFjK7o6wMfE=
=GaV2
-----END PGP SIGNATURE-----


From wmunny at mail.com  Wed Nov 26 10:45:45 2014
From: wmunny at mail.com (wmunny william)
Date: Wed, 26 Nov 2014 11:45:45 +0100
Subject: [squid-users] Problem with digest authentification and
 credential backend
In-Reply-To: <379284887.174632909.1416821882975.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <379284887.174632909.1416821882975.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <trinity-210917d8-5fac-4e6a-a24a-59876a624501-1416998745431@3capp-mailcom-lxa10>


> 
> William to be more clear this patch is not related at all with authenticate_ttl directive.
> authenticate_ttl doesn't works with Digest, but with basic and maybe another (ntlm, kerberos ?) there is no precision here http://www.squid-cache.org/Doc/config/authenticate_ttl/
> 
> The patch works like this:
> 
> At first banner Squid store the login/password HASH http://en.wikipedia.org/wiki/Digest_access_authentication http://wiki.squid-cache.org/KnowledgeBase/LdapBackedDigestAuthentication 
> 
> When nonce is stalled (nonce_max_count reached) the helper compare the account stored in memory with a request to Ldap or/and when the nonce is expired, the helper makes the same thing.
> 
> In this two cases there are two possibilities, the account is right or wrong -> Bad password or/and bad login
> 
> - If the return is right Squid return a new nonce and there is no impact for the user, I mean no banner.
> - If the return is wrong Squid present the authentication realm to the user and the browser prompt for a username and password.
> 
> There is also an another situation - if squid is restarted - the browser returns is HASH without banner (if the account is right of course)
> 
> So, without any change in LDAP the banner never appear, except when the browser start.
> 
> Fred 
> 
> PS: About Digest you are right it's almost good now, still also a little problem with nonce count but not related with this 
> 

Hi,

Ok, thanks,

Tested with both nonce_count and nonce_max_duration, no problem. Do you known if it works with squid 3.5 ?



From giorgi at mia.gov.ge  Wed Nov 26 12:25:30 2014
From: giorgi at mia.gov.ge (Giorgi Tepnadze)
Date: Wed, 26 Nov 2014 16:25:30 +0400
Subject: [squid-users] Problem with access.log and when using SMP
Message-ID: <5475C6BA.5000008@mia.gov.ge>

Hello,

I have own squid log parsing software which worked flawlessly before we 
moved to squid 3.3.8 (old was 2.7). Log parser stopped working. This is 
the problem I found.

Problem lines:
1416971036.802 184393 10.xx.xx.xx TCP_MISS/200 10963 CONNECT 
pixel.facebook.com:1416971036.390      0 10.yy.yy.yy TCP_DENIED/407 3751 
CONNECT clients4.google.com:443 - HIER_NONE/- text/html
1416971036.645   7164 10.zz.zz.zz TCP_MISS/200 55438 GET 
http://s40.radikal.ru/i089/1107/9b/2fbf1779ead9.jpg user at domain.com 
HIER_DIRECT/81.176.238.141 image/jpeg
443 user2 at domainc.com HIER_DIRECT/31.13.93.3 -

As it should be:
1416971036.390      0 10.yy.yy.yy TCP_DENIED/407 3751 CONNECT 
clients4.google.com:443 - HIER_NONE/- text/html
1416971036.802 184393 10.xx.xx.xx TCP_MISS/200 10963 CONNECT 
pixel.facebook.com:443 user2 at domainc.com HIER_DIRECT/31.13.93.3 -
1416971036.645   7164 10.zz.zz.zz TCP_MISS/200 55438 GET 
http://s40.radikal.ru/i089/1107/9b/2fbf1779ead9.jpg user at domain.com 
HIER_DIRECT/81.176.238.141 image/jpeg

I think that's because of SMP as I have 4 squid kids running and they 
all write to same access.log file and there is concurrency problem. 
Sometimes one is writing log line when previous log line is not finished 
yet or something like this. Such errors are rare but enough to stop my 
parser with  error.

Is it a bug? Is there any workaround or solution?

-- George Tepnadze


From holger.hoffstaette at googlemail.com  Wed Nov 26 14:02:54 2014
From: holger.hoffstaette at googlemail.com (Holger =?iso-8859-1?q?Hoffst=E4tte?=)
Date: Wed, 26 Nov 2014 14:02:54 +0000 (UTC)
Subject: [squid-users] Minor nit with cachemgr.cgi in 3.5.0.2
Message-ID: <pan.2014.11.26.14.02.54@googlemail.com>

Spam detection software, running on the system "master.squid-cache.org",
has identified this incoming email as possible spam.  The original
message has been attached to this so you can view it or label
similar future email.  If you have any questions, see
@@CONTACT_ADDRESS@@ for details.

Content preview:  Hi, I just thought I'd give 3.5.0.2 a workout and so far it
   seems to be working fine. One minor nit I found was on the initial page of
   cachemgr.cgi: the "server" field now contains two blank lines, whereas previously
   it would always contain only the (correct) localhost. [...] 

Content analysis details:   (7.3 points, 5.0 required)

 pts rule name              description
---- ---------------------- --------------------------------------------------
 3.6 RCVD_IN_PBL            RBL: Received via a relay in Spamhaus PBL
                            [79.245.130.44 listed in zen.spamhaus.org]
 1.3 RCVD_IN_RP_RNBL        RBL: Relay in RNBL,
                            https://senderscore.org/blacklistlookup/
                            [79.245.130.44 listed in bl.score.senderscore.com]
 0.2 CK_HELO_GENERIC        Relay used name indicative of a Dynamic Pool or
                            Generic rPTR
 0.0 FREEMAIL_FROM          Sender email is commonly abused enduser mail provider
                            (holger.hoffstaette[at]googlemail.com)
 0.9 SPF_FAIL               SPF: sender does not match SPF record (fail)
[SPF failed: Please see http://www.openspf.org/Why?s=mfrom;id=gcwsg-squid-users%40m.gmane.org;ip=79.245.130.44;r=master.squid-cache.org]
 0.0 T_HEADER_FROM_DIFFERENT_DOMAINS From and EnvelopeFrom 2nd level mail
                            domains are different
 0.0 UNPARSEABLE_RELAY      Informational: message has unparseable relay lines
 0.0 T_FREEMAIL_FORGED_FROMDOMAIN 2nd level domains in From and
                            EnvelopeFrom freemail headers are different
 1.3 RDNS_NONE              Delivered to internal network by a host with no rDNS


-------------- next part --------------
An embedded message was scrubbed...
From: Holger =?iso-8859-1?q?Hoffst=E4tte?=
	<holger.hoffstaette at googlemail.com>
Subject: Minor nit with cachemgr.cgi in 3.5.0.2
Date: Wed, 26 Nov 2014 14:02:54 +0000 (UTC)
Size: 2084
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141126/f5ff69cb/attachment.eml>

From nobody at hushmail.com  Wed Nov 26 17:03:01 2014
From: nobody at hushmail.com (HaxNobody)
Date: Wed, 26 Nov 2014 09:03:01 -0800 (PST)
Subject: [squid-users] Existing root certificate not working with SSL
 Bump (squid 3.3.10)
In-Reply-To: <5475A124.4050208@treenet.co.nz>
References: <1416933493483-4668515.post@n4.nabble.com>
 <5475A124.4050208@treenet.co.nz>
Message-ID: <1417021381827-4668526.post@n4.nabble.com>

Thanks for the reply. I'm aware of pinning, but this problem is happening on
small and/or insignificant sites that are certainly not pinned, as well as
the larger sites. In addition, our clients are not getting errors due to
pinning on our existing proxy setup, so we're doing something correctly
there.

Unfortunately, the squid version that I have is something that I can't
change, because it's supplied on a hardware appliance by our vendor. I can
try to get them to update it, but I don't think I will get very far. As it
is, they have done some extensive custom configuration for us, specifically
relating to the ability to use both HTTP and HTTPS traffic over the same
port while retaining full SSL interception capabilities.

The annoying thing is that none of the browsers I am using will give me any
useful information as to why they are hating my setup. I don't really know
the best way to validate the output of my proxy server. Openssl would seem
like a good place to start - is there any way to tell it to use a proxy when
I want to try using the s_client feature and see how the certificate
validates? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Existing-root-certificate-not-working-with-SSL-Bump-squid-3-3-10-tp4668515p4668526.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From nobody at hushmail.com  Wed Nov 26 17:45:20 2014
From: nobody at hushmail.com (HaxNobody)
Date: Wed, 26 Nov 2014 09:45:20 -0800 (PST)
Subject: [squid-users] Existing root certificate not working with SSL
 Bump (squid 3.3.10)
In-Reply-To: <1417021381827-4668526.post@n4.nabble.com>
References: <1416933493483-4668515.post@n4.nabble.com>
 <5475A124.4050208@treenet.co.nz> <1417021381827-4668526.post@n4.nabble.com>
Message-ID: <1417023920196-4668527.post@n4.nabble.com>

Alright, I figured out a possible cause. I downloaded the certificate that
the browsers were complaining about, and used openssl verify to verify
against the root certificate that I have. I got error 20, indicating that
squid must not be using the correct root certificate to generate the client
certificate on the fly, or that it is being generated incorrectly. The
generated certificate shows all the correct properties of the root
certificate that I am using, so my conclusion is that squid is incorrectly
generating the client certificate.

Question: Under what circumstances might squid incorrectly generate a bump
certificate?
Another question: Why might it be working when I use a different root
certificate?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Existing-root-certificate-not-working-with-SSL-Bump-squid-3-3-10-tp4668515p4668527.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rino at melsa.net.id  Thu Nov 27 04:38:26 2014
From: rino at melsa.net.id (Rino M Nur)
Date: Thu, 27 Nov 2014 11:38:26 +0700
Subject: [squid-users] Cannot display page correctly with SSL-Bump
Message-ID: <CA+zetAT=B6otujNQGWCrs7dacc2zOY4G-rk5GutiryEJ0MQbxQ@mail.gmail.com>

Hi,

Im trying to get ssl bump work correctly but when i get a site with https
then browser display the page with no CSS or javascript.
log :
1417149172.053    175 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443
- HIER_DIRECT/74.125.130.102 -
1417149172.145    194 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443
- HIER_DIRECT/74.125.130.102 -
1417149172.181    156 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443
- HIER_DIRECT/74.125.130.102 -
1417149172.220    169 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443
- HIER_DIRECT/74.125.130.102 -
1417149172.299    348 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443
- HIER_DIRECT/74.125.130.102 -

my configuration :
http_port 3130 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB key=/etc/squid/cert/private.pem
cert=/etc/squid/cert/public.pem
http_port 3128
http_port 192.168.10.50:3129 intercept

squid version :
Squid Cache: Version 3.4.9
configure options:  '--prefix=/usr' '--exec_prefix=/usr'
'--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--sysconfdir=/etc/squid' '--localstatedir=/var/spool/squid'
'--datadir=/usr/share/squid' '--enable-http-gzip' '--enable-async-io=24'
'--with-aufs-threads=24' '--with-pthreads' '--enable-storeio=aufs'
'--enable-linux-netfilter' '--enable-arp-acl' '--enable-epoll'
'--enable-removal-policies=heap' '--with-aio' '--enable-snmp'
'--enable-delay-pools' '--enable-htcp' '--enable-cache-digests'
'--disable-unlinkd' '--enable-large-cache-files'
'--enable-err-languages=English' '--enable-default-err-language=English'
'--with-maxfd=65536' '--enable-ssl-crtd' '--enable-zph-qos'
'--with-default-user=proxy' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-swapdir=/var/spool/squid'
'--with-aufs-threads=32' '--with-dl' '--with-large-files' '--with-openssl'
'--enable-ssl' 'CFLAGS=-march=nocona -O2 -pipe' --enable-ltdl-convenience

os :
debian wheezy


the page (youtube ) is displaying like this :


   -        What to Watch
   - My Subscriptions <https://www.youtube.com/feed/subscriptions>
   - Music <https://www.youtube.com/feed/music>


   -
      -   What to Watch  <https://www.youtube.com/>
      -   My Channel  <https://www.youtube.com/create_channel>
      -   My Subscriptions  <https://www.youtube.com/feed/subscriptions>
      -   History  <https://www.youtube.com/feed/history>
      -   Watch Later  1  <https://www.youtube.com/playlist?list=WL>
    ------------------------------
    -  Playlists
   <https://www.youtube.com/channel/UCsZ9qk2nbDNYZE8XWfC0ZMA/playlists>
    -   Liked videos
      <https://www.youtube.com/playlist?list=LLsZ9qk2nbDNYZE8XWfC0ZMA>
     More
      ------------------------------
    -  Subscriptions <https://www.youtube.com/subscription_manager>
     -        JackThammarat
      <https://www.youtube.com/channel/UCfy4XIsYPJ3ubslsPwwIe-g>
     More
      ------------------------------
    -
      -   Browse channels  <https://www.youtube.com/channels>
      -  <https://www.youtube.com/subscription_manager>
      <https://www.youtube.com/subscription_manager> Manage subscriptions
               <https://www.youtube.com/subscription_manager>
      <https://www.youtube.com/subscription_manager>
      <https://www.youtube.com/subscription_manager>
      -
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141127/97afbd42/attachment.htm>

From navari.lorenzo at gmail.com  Thu Nov 27 08:57:06 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Thu, 27 Nov 2014 00:57:06 -0800 (PST)
Subject: [squid-users] WARNING: there are more than 100 regular expressions
Message-ID: <1417078626013-4668529.post@n4.nabble.com>

Good day,
i have these Warnings  

 squid -k parse

..........
2014/11/27 09:36:22| Processing: acl direct_urls dstdom_regex 	
"/etc/squid/direct_urls.txt"
2014/11/27 09:36:22| /etc/squid/squid.conf line 86: acl direct_urls
dstdom_regex 		"/etc/squid/direct_urls.txt"
2014/11/27 09:36:22| WARNING: there are more than 100 regular expressions.
Consider using less REs or use rules without expressions like 'dstdomain'.
....................
2014/11/27 09:36:22| Processing: acl allowed_sitesmime dstdom_regex 
"/etc/squid/allowed_sitesmime.txt"
2014/11/27 09:36:22| /etc/squid/squid.conf line 94: acl allowed_sitesmime
dstdom_regex 	"/etc/squid/allowed_sitesmime.txt"
2014/11/27 09:36:22| WARNING: there are more than 100 regular expressions.
Consider using less REs or use rules without expressions like 'dstdomain'.


What can i do ?

Thank to everybody.







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Hullen at t-online.de  Thu Nov 27 09:17:00 2014
From: Hullen at t-online.de (Helmut Hullen)
Date: Thu, 27 Nov 2014 10:17:00 +0100
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <1417078626013-4668529.post@n4.nabble.com>
Message-ID: <D5dthyFPCXB@helmut.hullen.de>

Hallo, navari.lorenzo at gmail.com,

Du meintest am 27.11.14:

> i have these Warnings

>  squid -k parse

> ..........
> 2014/11/27 09:36:22| Processing: acl direct_urls dstdom_regex
> "/etc/squid/direct_urls.txt"
> 2014/11/27 09:36:22| /etc/squid/squid.conf line 86: acl direct_urls
> dstdom_regex 		"/etc/squid/direct_urls.txt"
> 2014/11/27 09:36:22| WARNING: there are more than 100 regular
> expressions.Consider using less REs or use rules without expressions
> like 'dstdomain'.....................

[...]

> What can i do ?

"Consider using less REs ..."

Viele Gruesse!
Helmut



From navari.lorenzo at gmail.com  Thu Nov 27 09:59:06 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Thu, 27 Nov 2014 01:59:06 -0800 (PST)
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <D5dthyFPCXB@helmut.hullen.de>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de>
Message-ID: <1417082346062-4668531.post@n4.nabble.com>

"Consider using less REs ..." is not possible.

if there is no other solution
i will break the files in many files with less then 100 entries.

Probably will have the same problem with black list.

Thank 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529p4668531.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From christian.molecki at stala.bwl.de  Thu Nov 27 10:24:41 2014
From: christian.molecki at stala.bwl.de (christianmolecki)
Date: Thu, 27 Nov 2014 02:24:41 -0800 (PST)
Subject: [squid-users] Cascading different authentification methods
Message-ID: <1417083881838-4668532.post@n4.nabble.com>

Hello everyone,

we are using squid 3.4.6 with ntlm authentification.
Depending on ActiveDirectory group memberships, the user is able to use
different protocols.
This works very well.

Now we need for some websites an additional basic authentication.
So I configured the basic ncsa_auth helper.
This works also, but only if the ntlm_auth helper is disables.

How I can authentificate via ntlm + basic?

Is this generally possible?


Best Regards
Christian



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cascading-different-authentification-methods-tp4668532.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Hullen at t-online.de  Thu Nov 27 10:33:00 2014
From: Hullen at t-online.de (Helmut Hullen)
Date: Thu, 27 Nov 2014 11:33:00 +0100
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <1417082346062-4668531.post@n4.nabble.com>
Message-ID: <D5dtqA5eCXB@helmut.hullen.de>

Hallo, navari.lorenzo at gmail.com,

Du meintest am 27.11.14:

> "Consider using less REs ..." is not possible.

Then try something like "squidguard" with lots of user defined domains  
and URLs.

Viele Gruesse!
Helmut



From marcus.kool at urlfilterdb.com  Thu Nov 27 12:03:32 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 27 Nov 2014 10:03:32 -0200
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <D5dtqA5eCXB@helmut.hullen.de>
References: <D5dtqA5eCXB@helmut.hullen.de>
Message-ID: <54771314.5040702@urlfilterdb.com>

how many REs do you have ?
and do you intend to use REs for blacklisting?

Marcus

On 11/27/2014 08:33 AM, Helmut Hullen wrote:
> Hallo, navari.lorenzo at gmail.com,
>
> Du meintest am 27.11.14:
>
>> "Consider using less REs ..." is not possible.
>
> Then try something like "squidguard" with lots of user defined domains
> and URLs.
>
> Viele Gruesse!
> Helmut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From navari.lorenzo at gmail.com  Thu Nov 27 12:01:15 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Thu, 27 Nov 2014 04:01:15 -0800 (PST)
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <54771314.5040702@urlfilterdb.com>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
 <D5dtqA5eCXB@helmut.hullen.de> <54771314.5040702@urlfilterdb.com>
Message-ID: <1417089675959-4668535.post@n4.nabble.com>

ok 
i don't intend to use REs for blacklisting but only for blocking some sites
like facebook twitter...
In the other file i have about 120 - 150 REs.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529p4668535.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From leolistas at solutti.com.br  Thu Nov 27 12:15:43 2014
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Thu, 27 Nov 2014 10:15:43 -0200
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <1417082346062-4668531.post@n4.nabble.com>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
Message-ID: <547715EF.5050703@solutti.com.br>

On 27/11/14 07:59, navari.lorenzo at gmail.com wrote:
> "Consider using less REs ..." is not possible.
>
>
     so dont worry about this WARNING message. This is just a warning, 
not an error. If you're aware that using lots of REs can hit hard on the 
CPU usage, just go for it.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From marcus.kool at urlfilterdb.com  Thu Nov 27 13:18:33 2014
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 27 Nov 2014 11:18:33 -0200
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <1417089675959-4668535.post@n4.nabble.com>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
 <D5dtqA5eCXB@helmut.hullen.de> <54771314.5040702@urlfilterdb.com>
 <1417089675959-4668535.post@n4.nabble.com>
Message-ID: <547724A9.9000407@urlfilterdb.com>

blocking facebook and twitter can be done with ACLs based on dstdomain.
they are much faster than REs.

Marcus

On 11/27/2014 10:01 AM, navari.lorenzo at gmail.com wrote:
> ok
> i don't intend to use REs for blacklisting but only for blocking some sites
> like facebook twitter...
> In the other file i have about 120 - 150 REs.
>
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529p4668535.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From vrogoziansky.squid at gmail.com  Thu Nov 27 13:48:37 2014
From: vrogoziansky.squid at gmail.com (Vadim Rogoziansky)
Date: Thu, 27 Nov 2014 15:48:37 +0200
Subject: [squid-users] Transparent proxy with Peek and Splice feature.
In-Reply-To: <54759E6C.5080404@treenet.co.nz>
References: <5474C8CF.6030404@gmail.com> <54759E6C.5080404@treenet.co.nz>
Message-ID: <54772BB5.5010101@gmail.com>

Hello Amos.

Thank you for answer.

There was made an investigation related to squid's peek and splice 
issues in transparent mode.
One-line explanation is as follows - in intercept mode squid can't get a 
server host name from the request header and uses clent IP address 
instead for both fake cert generation and as a SNI record in server bump 
SSL handshaking. This is the root of the problem. However this can be 
fixed if squid uses SNI field taken from client TLS Hello message for 
that purposes. Can you hack squid in this way? What do you think?

Many thanks.


11/26/2014 11:33 AM, Amos Jeffries ???????(??):
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 26/11/2014 7:22 a.m., Vadim Rogoziansky wrote:
>> Hello All.
>>
>> My goal is to do ssl bumping in transparent proxy mode with domain
>> exclude possibility. Let me tell you about squid's strange
>> behaviour when I'm trying to do it.
>>
>> In browsers it says something like this: /This server could not
>> prove that it is www.ukr.net; its security certificate is
>> from212.42.76.253. This may be caused by a misconfiguration or an
>> attacker intercepting your connection.//
>> //NET::ERR_CERT_COMMON_NAME_INVALID// //Subject: 212.42.76.253// /
>> Looks like squid takes the CN from the certificate as IP address of
>> the destination domain.
> Squid takes the IP address from the TCP packet. Which is all that is
> available in NAT intercepted traffic at bumping step #1.
>
> The ACLs you have therefore determine that "bump" action is to happen.
> Correct?
>
> The cert details are therefore mimic'ed from what gets delivered by
> the server.
>
> It may be that the server is depending on SNI to generate its own
> cert, but since Squid deos not have that domain name already an
> IP-based cert comes back.
>
> It may also be that some ISP upstream of you is bumping the encryption
> with client-first method.
>
>
>
>> But, everything works smoothly when I use proxy in non transparent
>> mode and put it to the browser directly .
> In which case the browser sends domain name to the proxy in its
> CONNECT message starting the HTTPS. The possible results are very
> different.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUdZ5sAAoJELJo5wb/XPRj0qIIANBjuFvq45hPmcaj/NYL6bza
> 7ttt5Gn+tn8E5KH7T4wfQhUXr91UIsYWfOswfnVAAlBevIO/iFVoDN5hAOveuhIl
> ra/0eGti1EpZ3LHJiAqmo0mHsrz3v9+PAduVrXgUJLyYDiM0xctg0nRhj2u166VX
> j0IL3g8CKEw+KiWVJM9HdLaDEz9fYtHBO8UHhKDDE94O9yxScIvB+GAhN4YlTtrE
> z65VJkSCEw+3vH6XcrrkF2aEnB20jeEGiV5puO2cPoJpgcg3ic8sMVEfa/Z1qwqa
> KCkj2XI28wBCIovCV+AfBhpvW0o8eVFbt4ESodLTmwjUvU+m8zxky/9cjO5kyLE=
> =kgug
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From bpk678 at gmail.com  Thu Nov 27 16:16:37 2014
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 27 Nov 2014 11:16:37 -0500
Subject: [squid-users] Cascading different authentification methods
In-Reply-To: <1417083881838-4668532.post@n4.nabble.com>
References: <1417083881838-4668532.post@n4.nabble.com>
Message-ID: <1417104997.2953.1.camel@desktop.bpk2.com>

On Thu, 2014-11-27 at 02:24 -0800, christianmolecki wrote:
> Hello everyone,
> 
> we are using squid 3.4.6 with ntlm authentification.
> Depending on ActiveDirectory group memberships, the user is able to use
> different protocols.
> This works very well.
> 
> Now we need for some websites an additional basic authentication.
> So I configured the basic ncsa_auth helper.
> This works also, but only if the ntlm_auth helper is disables.
> 
> How I can authentificate via ntlm + basic?
> 
> Is this generally possible?
> 
> 
> Best Regards
> Christian
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cascading-different-authentification-methods-tp4668532.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

why do you need to authenticate the users differently, for different
sites?  the auth for the proxies (indicated by an HTTP/407 status code)
is completely different than the auth for a web site/server (indicated
by an HTTP/401 status code).



From matt_m at me.com  Tue Nov 25 02:16:07 2014
From: matt_m at me.com (Mathew Marulla)
Date: Mon, 24 Nov 2014 21:16:07 -0500
Subject: [squid-users] Persistent Connections - only one side
Message-ID: <EADFA361-A1CA-4462-9B9B-7DF26FC8FD9B@me.com>

Need to know if this is even possible?

We have a PHP app that uses CURL to send requests/get responses from servers of various business partners.  The connections to these partner servers are not currently persistent.  Setting keep-alive headers would be ineffective because the connections are necessarily closed when the PHP code exists, at the end of each transaction.  Something like this:



Now, we have a potential partner that requires persistent, re-used connections.  Changing the system architecture so that the PHP code remains running is not an option.  I was wondering if a proxy server, running on the same server as the PHP code, could keep the outside half persistent, even after the inside half closes.  Like this:



Is this possible?  Is there a config option that might achieve this? 

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141124/45878419/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2014-11-24 at 9.00.30 PM.png
Type: image/png
Size: 35517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141124/45878419/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2014-11-24 at 9.00.37 PM.png
Type: image/png
Size: 34388 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141124/45878419/attachment-0001.png>

From dougs at dawnsign.com  Tue Nov 25 19:59:27 2014
From: dougs at dawnsign.com (Doug Sampson)
Date: Tue, 25 Nov 2014 19:59:27 +0000
Subject: [squid-users] Memory Leak Squid 3.4.9 on FreeBSD 10.0 x64
In-Reply-To: <5473E101.4010907@treenet.co.nz>
References: <E6B2517F8D6DBF4CABB8F38ACA367E783431AF29@Draco.dawnsign.com>
 <5473E101.4010907@treenet.co.nz>
Message-ID: <E6B2517F8D6DBF4CABB8F38ACA367E7834342AF7@Draco.dawnsign.com>

> On 25/11/2014 9:06 a.m., Doug Sampson wrote:
> > Recently due to squid 2.7 being EOL'ed, we migrated our squid
> > server to version 3.4.9 on a FreeBSD 10.0-RELEASE running on 64-bit
> > hardware. We started seeing paging file being swapped out
> > eventually running out of available memory. From the time squid
> > gets started it usually takes about two days before we see these
> > entries in /var/log/messages as follows:
> >
> > +swap_pager_getswapspace(16): failed +swap_pager_getswapspace(16):
> > failed +swap_pager_getswapspace(16): failed
> > +swap_pager_getswapspace(12): failed +swap_pager_getswapspace(16):
> > failed +swap_pager_getswapspace(12): failed
> > +swap_pager_getswapspace(6): failed +swap_pager_getswapspace(16):
> > failed
> >
> > Looking at the 'top' results, I see that the swap file has been
> > totally exhausted. Memory used by squid hovers around 2.3GB out of
> > the total 3GB of system memory.
> >
> > I am not sure what is causing these memory leaks. After rebooting,
> > squid-internal-mgr/info shows the following statistics:
> >
> > Squid Object Cache: Version 3.4.9 Build Info: Start Time:	Mon, 24
> > Nov 2014 18:39:08 GMT Current Time:	Mon, 24 Nov 2014 19:39:13 GMT
> > Connection information for squid: Number of clients accessing
> > cache:	18 Number of HTTP requests received:	10589 Number of ICP
> > messages received:	0 Number of ICP messages sent:	0 Number of
> > queued ICP replies:	0 Number of HTCP messages received:	0 Number of
> > HTCP messages sent:	0 Request failure ratio:	 0.00 Average HTTP
> > requests per minute since start:	176.2 Average ICP messages per
> > minute since start:	0.0 Select loop called: 763993 times, 4.719 ms
> > avg Cache information for squid: Hits as % of all requests:	5min:
> > 3.2%, 60min: 17.0% Hits as % of bytes sent:	5min: 2.0%, 60min:
> > 6.7% Memory hits as % of hit requests:	5min: 0.0%, 60min: 37.2%
> > Disk hits as % of hit requests:	5min: 22.2%, 60min: 33.2% Storage
> > Swap size:	7361088 KB Storage Swap capacity:	58.5% used, 41.5%
> > free Storage Mem size:	54348 KB Storage Mem capacity:	 3.9%
> used,
> > 96.1% free Mean Object Size:	23.63 KB Requests given to unlinkd:	1
> > Median Service Times (seconds)  5 min    60 min: HTTP Requests
> > (All):   0.10857  0.19742 Cache Misses:          0.10857  0.32154
> > Cache Hits:            0.08265  0.01387 Near Hits:
> > 0.15048  0.12106 Not-Modified Replies:  0.00091  0.00091 DNS
> > Lookups:           0.05078  0.05078 ICP Queries:           0.00000
> > 0.00000 Resource usage for squid: UP Time:	3605.384 seconds CPU
> > Time:	42.671 seconds CPU Usage:	1.18% CPU Usage, 5 minute avg:
> > 0.72% CPU Usage, 60 minute avg:	1.17% Maximum Resident Size: 845040
> > KB Page faults with physical i/o: 20 Memory accounted for: Total
> > accounted:       105900 KB memPoolAlloc calls:   2673353
> > memPoolFree calls:    2676487 File descriptor usage for squid:
> > Maximum number of file descriptors:   87516 Largest file desc
> > currently in use:    310 Number of file desc currently in use:
> > 198 Files queued for open:                   0 Available number of
> > file descriptors: 87318 Reserved number of file descriptors:   100
> > Store Disk files open:                   0 Internal Data
> > Structures: 311543 StoreEntries 4421 StoreEntries with MemObjects
> > 4416 Hot Object Cache Items 311453 on-disk objects
> >
> > I will post another one tomorrow that will indicate growing
> > memory/swapfile consumption.
> >
> > Here is my squid.conf:
> >
> > # OPTIONS FOR AUTHENTICATION #
> > ------------------------------------------------------------------------
> -----
> >
> >
> # 1st four lines for
> > auth_param basic children 5 auth_param basic realm Squid
> > proxy-caching web server auth_param basic credentialsttl 2 hours
> > auth_param basic casesensitive off #  next three lines for kerberos
> > authentication (needed to use usernames) #  used in conjunction
> > with "acl auth proxy_auth" line below #auth_param negotiate program
> > /usr/local/libexec/squid/negotiate_kerberos_auth -i #auth_param
> > negotiate children 50 startup=10 idle=5 #auth_param negotiate
> > keep_alive on
> >
> >
> > # ACCESS CONTROLS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> # Example rule allowing access from your local networks.
> > # Adapt to list your (internal) IP networks from where browsing #
> > should be allowed #acl manager proto cache_object acl manager
> > url_regex -i ^cache_object:// /squid-internal-mgr/ acl adminhost
> > src 192.168.1.149 acl localnet src 192.168.1.0/24	# RFC1918
> > possible internal network acl localnet src fc00::/7           # RFC
> > 4193 local private network range acl localnet src fe80::/10
> > # RFC 4291 link-local (directly plugged) machines acl webserver src
> > 198.168.1.35 acl some_big_clients src 192.168.1.149/32 #CI53
> >
> > # We want to limit downloads of these type of files # Put this all
> > in one line acl magic_words url_regex -i ftp .exe .mp3 .vqf .tar.gz
> > .gz .rpm .zip .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav
> > .dmg .mp4 .img # We don't block .html, .gif, .jpg and similar
> > files, because they # generally don't consume much bandwidth
> 
> But you do. Whenever the domain name or path contains any of the byte
> sequences in that regex above. The entire websites
> http://www.divx.com/  and http://isohunt.com/ for example.
> 
> And whats wrong with adding more HITs ? even if they are small enough
> not to use much cache space.
> 
> <snip>
> >
> > # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM #
> > ------------------------------------------------------------------------
> -----
> >
> >
> hierarchy_stoplist cgi-bin ?
> 
> 
> ... but you dont have neighbours. This is also deprecated anyway.
> 
> >
> > # MEMORY CACHE OPTIONS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> cache_mem 1366 MB
> > #cache_mem 2134 MB #maximum_object_size_in_memory 64 KB
> > maximum_object_size_in_memory 128 KB
> >
> > # DISK CACHE OPTIONS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> cache_replacement_policy heap LFUDA
> > cache_dir aufs /data/squid/aufs_cache 4096 16 256 min-size=131073
> > cache_dir diskd /data/squid/diskd_cache 8192 16 256 Q1=64 Q2=72
> > max-size=131072
> 
> Why the segregation between diskd and aufs?
> 
> The only difference between these cache types is the method if I/O
> performed accessing the disk. AUFS is threaded SMP, diskd is
> multi-process SMP.
> 
> NP: FreeBSD 10 seem to have resolved the issues Squid AUFS has with
> older BSD and people are now noticing the speed issues with diskd.
> 
> The official recommendation is currently to use AUFS with FreeBSD 10+
> and diskd with older FreeBSD.
> 
> 
> > #maximum_object_size 122880 KB maximum_object_size 153600 KB
> > cache_swap_low 90 cache_swap_high 95
> >
> > # LOGFILE OPTIONS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> access_log daemon:/data/squid/logs/access.log
> > cache_store_log daemon:/data/squid/logs/store.log cache_swap_log
> > /var/spool/squid/%s
> 
> What is this %s ??
> 
> > logfile_rotate 28
> >
> > # OPTIONS FOR TROUBLESHOOTING #
> > ------------------------------------------------------------------------
> -----
> >
> >
> cache_log /data/squid/logs/cache.log
> > # Leave coredumps in the first cache dir coredump_dir /data/squid
> >
> > # OPTIONS FOR EXTERNAL SUPPORT PROGRAMS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> diskd_program /usr/local/libexec/squid/diskd
> >
> 
> Unless you are replacing this helper with a custom-built one with
> strange name this should not be configured explicitly in Squid-3.
> 
> 
> > # OPTIONS FOR TUNING THE CACHE #
> > ------------------------------------------------------------------------
> -----
> >
> >
> refresh_pattern http://.*\.windowsupdate\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://office\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://windowsupdate\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://w?xpsp[0-9]\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://w2ksp[0-9]\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://download\.microsoft\.com/ 0 80% 20160
> > refresh_pattern http://download\.macromedia\.com/ 0 80% 20160
> > refresh_pattern http://ftp\.software\.ibm\.com/ 0 80% 20160
> > refresh_pattern         cgi-bin         1 20% 2 refresh_pattern
> > \.asp$          1 20% 2 refresh_pattern         \.acgi$         1
> > 20% 2 refresh_pattern         \.cgi$          1 20% 2
> > refresh_pattern         \.pl$           1 20% 2 refresh_pattern
> > \.shtml$        1 20% 2 refresh_pattern         \.php3$         1
> > 20% 2 refresh_pattern         \?              1 20% 2
> > refresh_pattern         \.gif$          10080   90%     43200
> > refresh_pattern         \.png$          10080   90%     43200
> > refresh_pattern         \.jpg$          10080   90%     43200
> > refresh_pattern         \.ico$          10080   90%     43200
> > refresh_pattern         \.bom\.gov\.au     30   20%       120
> > refresh_pattern         \.html$           480   50%     22160
> > refresh_pattern         \.htm$            480   50%     22160
> > refresh_pattern         \.css$            480   50%     22160
> > refresh_pattern         \.js$             480   50%     22160
> > refresh_pattern         \.class$        10080   90%     43200
> > refresh_pattern         \.zip$          10080   90%     43200
> > refresh_pattern         \.jpeg$         10080   90%     43200
> > refresh_pattern         \.mid$          10080   90%     43200
> > refresh_pattern         \.shtml$          480   50%     22160
> > refresh_pattern         \.exe$          10080   90%     43200
> > refresh_pattern         \.thm$          10080   90%     43200
> > refresh_pattern         \.wav$          10080   90%     43200
> > refresh_pattern         \.mp4$          10080   90%     43200
> > refresh_pattern         \.txt$          10080   90%     43200
> > refresh_pattern         \.cab$          10080   90%     43200
> > refresh_pattern         \.au$           10080   90%     43200
> > refresh_pattern         \.mov$          10080   90%     43200
> > refresh_pattern         \.xbm$          10080   90%     43200
> > refresh_pattern         \.ram$          10080   90%     43200
> > refresh_pattern         \.iso$          10080   90%     43200
> > refresh_pattern         \.avi$          10080   90%     43200
> > refresh_pattern         \.chtml$          480   50%     22160
> > refresh_pattern         \.thb$          10080   90%     43200
> > refresh_pattern         \.dcr$          10080   90%     43200
> > refresh_pattern         \.bmp$          10080   90%     43200
> > refresh_pattern         \.phtml$          480   50%     22160
> > refresh_pattern         \.mpg$          10080   90%     43200
> > refresh_pattern         \.pdf$          10080   90%     43200
> > refresh_pattern         \.art$          10080   90%     43200
> > refresh_pattern         \.swf$          10080   90%     43200
> > refresh_pattern         \.flv$          10080   90%     43200
> > refresh_pattern         \.x-flv$        10080   90%     43200
> > refresh_pattern         \.mp3$          10080   90%     43200
> > refresh_pattern         \.ra$           10080   90%     43200
> > refresh_pattern         \.spl$          10080   90%     43200
> > refresh_pattern         \.viv$          10080   90%     43200
> > refresh_pattern         \.doc$          10080   90%     43200
> > refresh_pattern         \.gz$           10080   90%     43200
> > refresh_pattern         \.Z$            10080   90%     43200
> > refresh_pattern         \.tgz$          10080   90%     43200
> > refresh_pattern         \.tar$          10080   90%     43200
> > refresh_pattern         \.vrm$          10080   90%     43200
> > refresh_pattern         \.vrml$         10080   90%     43200
> > refresh_pattern         \.aif$          10080   90%     43200
> > refresh_pattern         \.aifc$         10080   90%     43200
> > refresh_pattern         \.aiff$         10080   90%     43200
> > refresh_pattern         \.arj$          10080   90%     43200
> > refresh_pattern         \.c$            10080   90%     43200
> > refresh_pattern         \.cpt$          10080   90%     43200
> > refresh_pattern         \.dir$          10080   90%     43200
> > refresh_pattern         \.dxr$          10080   90%     43200
> > refresh_pattern         \.hqx$          10080   90%     43200
> > refresh_pattern         \.jpe$          10080   90%     43200
> > refresh_pattern         \.lha$          10080   90%     43200
> > refresh_pattern         \.lzh$          10080   90%     43200
> > refresh_pattern         \.midi$         10080   90%     43200
> > refresh_pattern         \.movie$        10080   90%     43200
> > refresh_pattern         \.mp2$          10080   90%     43200
> > refresh_pattern         \.mpe$          10080   90%     43200
> > refresh_pattern         \.mpeg$         10080   90%     43200
> > refresh_pattern         \.mpga$         10080   90%     43200
> > refresh_pattern         \.pl$           10080   90%     43200
> > refresh_pattern         \.ppt$          10080   90%     43200
> > refresh_pattern         \.ps$           10080   90%     43200
> > refresh_pattern         \.qt$           10080   90%     43200
> > refresh_pattern         \.qtm$          10080   90%     43200
> > refresh_pattern         \.rar$          10080   90%     43200
> > refresh_pattern         \.ras$          10080   90%     43200
> > refresh_pattern         \.sea$          10080   90%     43200
> > refresh_pattern         \.sit$          10080   90%     43200
> > refresh_pattern         \.tif$          10080   90%     43200
> > refresh_pattern         \.tiff$         10080   90%     43200
> > refresh_pattern         \.snd$          10080   90%     43200
> > refresh_pattern         \.wrl$          10080   90%     43200
> > refresh_pattern         ^ftp:           1440    60%     22160
> > refresh_pattern         ^gopher:        1440    20%     1440
> > refresh_pattern         -i (cgi-bin|\?) 0       0%      0
> > refresh_pattern         .               480     50%     22160
> >
> 
> That is a LOT of regex comparisions the proxy is having to do at least
> once per-request.
> 
> The special rules you have up the top for "cgi-bin" and "\?" are also
> violating HTTP safe behaviour. The default rule we provide is highly
> tuned to handle caching of those responses safely without breaking old
> legacy scripts.
> 
> 
> At least most of them end with $ anchor point to prevent random URLs
> matching.
> 
> 
> > # ADMINISTRATIVE PARAMETERS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> cache_mgr admin at example.com
> > mail_from squid at example.com cache_effective_user squid
> > cache_effective_group squid
> >
> > # DELAY POOL PARAMETERS #
> > ------------------------------------------------------------------------
> -----
> >
> >
> delay_pools 2
> > delay_class 1 2 # When big_files are being downloaded, the first
> > 5MB (625000 * 8 bits) are # downloaded at max network speed. Once
> > the file size limit of 5MB is reached, # download speed drops to
> > 438,000 bits or 3,504,000 MB per sec. Current # contracted Internet
> > connection speed w/ TP is at 7MB per sec. delay_parameters 1
> > 750000/750000 438000/625000
> 
> > acl big_files url_regex -i ftp .exe .mp3 .vqf .tar.gz .gz .rpm .zip
> > .rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav .dmg .mp4
> > .img .flv .wmv .divx .mov .bz2 .deb
> 
> Another long list of regex patterns. Notice how these are permitted to
> match anywhere in the entie URL. Including domain names.
> 
> FTP traffic in particular is not guaranteed to be "big files".
> 
> <snip>
> > Intially, I set mem_cache=2134MB and after noticing these memory
> > leaks, I dropped it down to 1344MB. Memory leaks are still
> > occurring.
> >
> > Am I using anything that is known to cause memory leaks?
> >
> > If there is additional information that you need, please do not
> > hesitate to ask! Thanks.
> 
> A copy of the manager "mem" report would be very useful to see whats
> using the memory.
>  Note that it is a TSV format, so please save as .tsv file and attach.
> rather than cut-n-pasting inline.
> 

Thanks, Amos, for your pointers. 

I've commented out all the fresh_patterns lines appearing above the last two lines. 

I also have dropped diskd in favor of using aufs exclusively, taking out the min-size parameter. I've commented out the diskd_program support option. In the previous version of squid (2.7) I had split the cache_dir into two types with great success using coss and aufs. Previously I had only aufs and performance wasn't where I wanted it. Apparently coss is no longer supported in the 3.x version of squid atop FreeBSD.

The pathname for the cache swap logs have been fixed. Apparently this came from a squid.conf example that I copied in parts. Would this be the reason why we are seeing the error messages in /var/log/messages regarding swapping mentioned in my original post?

The hierarchy_stoplist line has been stripped out as you say it is deprecated.

The mem .TSV file is attached herewith.

Currently I have the cache_dir located on the OS disk and all of the cache logging files on a second drive. Is this the optimal setup of cache-dir and logs?

Your comments are much appreciated!

~Doug
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: squid-internal-mgr_MEM.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141125/29500fc5/attachment.txt>

From carlos.e.fernandez-touzon at uscis.dhs.gov  Tue Nov 18 23:33:09 2014
From: carlos.e.fernandez-touzon at uscis.dhs.gov (Fernandez-Touzon, Carlos E (CTR))
Date: Tue, 18 Nov 2014 23:33:09 +0000
Subject: [squid-users] cannot start Squid 3.4.9
Message-ID: <309A75C3302E384DBE47D4D6EAA8F0F7B63047@D2ASEPREA001>

RHEL 6.6 x86_64
Squid v3.4.9

I downloaded the 3.4.9 RPM from http://www1.ngtech.co.il/repo/centos/6/x86_64/squid-3.4.9-2.el6.x86_64.rpm

The squid service fails to start.

$ sudo service squid start
Starting squid: ....................                       [FAILED]

Squid doesn't write anything to squid.out.  Once I add the -X option to SQUID_OPTS, squid outputs a ton of messages to squid.out ... but nothing seems out of order.

Any thoughts on how I should track down this issue?

For completeness I have added the full debug log output

Thanks

C

==DEBUG LOG==
2014/11/18 18:22:35.457| debug.cc(425) parseOptions: command-line -X overrides: ALL,7
2014/11/18 18:22:35.467| cache_manager.cc(102) registerProfile: registering legacy mem
2014/11/18 18:22:35.467| cache_manager.cc(136) findAction: CacheManager::findAction: looking for action mem
2014/11/18 18:22:35.468| cache_manager.cc(144) findAction: Action not found.
2014/11/18 18:22:35.468| cache_manager.cc(87) registerProfile: registered profile: mem
2014/11/18 18:22:35.468| cache_manager.cc(102) registerProfile: registering legacy squidaio_counts
2014/11/18 18:22:35.468| cache_manager.cc(136) findAction: CacheManager::findAction: looking for action squidaio_counts
2014/11/18 18:22:35.468| cache_manager.cc(144) findAction: Action not found.
2014/11/18 18:22:35.468| cache_manager.cc(87) registerProfile: registered profile: squidaio_counts
2014/11/18 18:22:35.468| cache_manager.cc(136) findAction: CacheManager::findAction: looking for action diskd
2014/11/18 18:22:35.468| cache_manager.cc(144) findAction: Action not found.
2014/11/18 18:22:35.468| cache_manager.cc(87) registerProfile: registered profile: diskd
2014/11/18 18:22:35.468| rock/RockStoreFileSystem.cc(44) setup: Will use Rock FS
2014/11/18 18:22:35.468| Startup: Initializing Authentication Schemes ...
2014/11/18 18:22:35.468| Startup: Initialized Authentication Scheme 'basic'
2014/11/18 18:22:35.468| Startup: Initialized Authentication Scheme 'digest'
2014/11/18 18:22:35.468| Startup: Initialized Authentication Scheme 'negotiate'
2014/11/18 18:22:35.468| Startup: Initialized Authentication Scheme 'ntlm'
2014/11/18 18:22:35.468| Startup: Initialized Authentication.
2014/11/18 18:22:35.468| tools.cc(59) ProbeTransport: IPv6 not supported on this machine. Auto-Disabled.
2014/11/18 18:22:35.468| Config.cc(39) registerTokens:  register format tokens for 'adapt'
2014/11/18 18:22:35.468| Config.cc(39) registerTokens:  register format tokens for 'icap'
2014/11/18 18:22:35.468| Config.cc(39) registerTokens:  register format tokens for 'ssl'
2014/11/18 18:22:35.468| cache_cf.cc(608) parseConfigFile:
2014/11/18 18:22:35.468| cf_parser.cci(4061) free_all:
2014/11/18 18:22:35.468| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type ssl_error
2014/11/18 18:22:35.468| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.468| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certHasExpired'
2014/11/18 18:22:35.468| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.468| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'ssl::certHasExpired'
2014/11/18 18:22:35.468| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'ssl_error'
2014/11/18 18:22:35.468| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117d688
2014/11/18 18:22:35.468| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type ssl_error
2014/11/18 18:22:35.468| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.468| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certNotYetValid'
2014/11/18 18:22:35.468| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.468| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'ssl::certNotYetValid'
2014/11/18 18:22:35.468| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'ssl_error'
2014/11/18 18:22:35.468| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117d6b8
2014/11/18 18:22:35.468| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type ssl_error
2014/11/18 18:22:35.468| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.468| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certDomainMismatch'
2014/11/18 18:22:35.468| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.469| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'ssl::certDomainMismatch'
2014/11/18 18:22:35.469| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'ssl_error'
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117d868
2014/11/18 18:22:35.469| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type ssl_error
2014/11/18 18:22:35.469| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.469| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certUntrusted'
2014/11/18 18:22:35.469| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.469| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'ssl::certUntrusted'
2014/11/18 18:22:35.469| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'ssl_error'
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117da88
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117da48
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117dab8
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117d818
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117dae8
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117db18
2014/11/18 18:22:35.469| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type ssl_error
2014/11/18 18:22:35.469| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.469| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certSelfSigned'
2014/11/18 18:22:35.469| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.469| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'ssl::certSelfSigned'
2014/11/18 18:22:35.469| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'ssl_error'
2014/11/18 18:22:35.469| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x117dc58
2014/11/18 18:22:35.469| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type src
2014/11/18 18:22:35.469| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.469| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.469| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.469| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'all'
2014/11/18 18:22:35.469| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'src'
2014/11/18 18:22:35.469| Ip.cc(260) FactoryParse: aclIpParseIpData: all
2014/11/18 18:22:35.469| Ip.cc(264) FactoryParse: aclIpParseIpData: magic 'all' found.
2014/11/18 18:22:35.469| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type url_regex
2014/11/18 18:22:35.469| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.469| Acl.cc(118) FindByName: ACL::FindByName 'manager'
2014/11/18 18:22:35.469| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.469| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'manager'
2014/11/18 18:22:35.469| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'url_regex'
2014/11/18 18:22:35.469| RegexData.cc(323) aclParseRegexList: aclParseRegexList: new Regex line or file
2014/11/18 18:22:35.469| RegexData.cc(331) aclParseRegexList: aclParseRegexList: buffering RE '-i'
2014/11/18 18:22:35.469| RegexData.cc(331) aclParseRegexList: aclParseRegexList: buffering RE '^cache_object://'
2014/11/18 18:22:35.469| RegexData.cc(331) aclParseRegexList: aclParseRegexList: buffering RE '+i'
2014/11/18 18:22:35.469| RegexData.cc(331) aclParseRegexList: aclParseRegexList: buffering RE '^https?://[^/]+/squid-internal-mgr/'
2014/11/18 18:22:35.469| RegexData.cc(214) compileOptimisedREs: compileOptimisedREs: -i
2014/11/18 18:22:35.469| RegexData.cc(238) compileOptimisedREs: compileOptimisedREs: adding RE '^cache_object://'
2014/11/18 18:22:35.469| RegexData.cc(228) compileOptimisedREs: compileOptimisedREs: +i
2014/11/18 18:22:35.469| RegexData.cc(173) compileRE: compileRE: compiled '(^cache_object://)' with flags 11
2014/11/18 18:22:35.469| RegexData.cc(238) compileOptimisedREs: compileOptimisedREs: adding RE '^https?://[^/]+/squid-internal-mgr/'
2014/11/18 18:22:35.469| RegexData.cc(173) compileRE: compileRE: compiled '(^https?://[^/]+/squid-internal-mgr/)' with flags 9
2014/11/18 18:22:35.470| RegexData.cc(281) compileOptimisedREs: compileOptimisedREs: 2 REs are optimised into one RE.
2014/11/18 18:22:35.470| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type src
2014/11/18 18:22:35.470| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.470| Acl.cc(118) FindByName: ACL::FindByName 'localhost'
2014/11/18 18:22:35.470| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.470| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'localhost'
2014/11/18 18:22:35.470| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'src'
2014/11/18 18:22:35.470| Ip.cc(260) FactoryParse: aclIpParseIpData: 127.0.0.1/32
2014/11/18 18:22:35.470| Ip.cc(368) FactoryParse: aclIpParseIpData: '127.0.0.1/32' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.470| Ip.cc(505) FactoryParse: Parsed: 127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff](/128)
2014/11/18 18:22:35.470| Ip.cc(260) FactoryParse: aclIpParseIpData: ::1
2014/11/18 18:22:35.470| Ip.cc(405) FactoryParse: aclIpParseIpData: Lookup Host/IP ::1
2014/11/18 18:22:35.470| Ip.cc(444) FactoryParse: aclIpParseIpData: Located host/IP: '[::1]'
2014/11/18 18:22:35.470| Ip.cc(451) FactoryParse: ::1 --> [::1]
2014/11/18 18:22:35.470| Ip.cc(437) FactoryParse: aclIpParseIpData: Duplicate host/IP: '[::1]' dropped.
2014/11/18 18:22:35.470| Ip.cc(437) FactoryParse: aclIpParseIpData: Duplicate host/IP: '[::1]' dropped.
2014/11/18 18:22:35.470| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 127.0.0.1/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (127.0.0.1)  vs [::1]-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2014/11/18 18:22:35.470| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: [::1]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] ([::1])  vs 127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2014/11/18 18:22:35.470| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type dst
2014/11/18 18:22:35.470| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.470| Acl.cc(118) FindByName: ACL::FindByName 'to_localhost'
2014/11/18 18:22:35.470| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.470| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'to_localhost'
2014/11/18 18:22:35.470| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'dst'
2014/11/18 18:22:35.470| Ip.cc(260) FactoryParse: aclIpParseIpData: 127.0.0.0/8
2014/11/18 18:22:35.470| Ip.cc(368) FactoryParse: aclIpParseIpData: '127.0.0.0/8' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.470| Ip.cc(505) FactoryParse: Parsed: 127.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0](/104)
2014/11/18 18:22:35.470| Ip.cc(260) FactoryParse: aclIpParseIpData: 0.0.0.0/32
2014/11/18 18:22:35.470| Ip.cc(368) FactoryParse: aclIpParseIpData: '0.0.0.0/32' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.470| Ip.cc(505) FactoryParse: Parsed: 0.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff](/128)
2014/11/18 18:22:35.470| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 127.0.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (127.0.0.0)  vs 0.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2014/11/18 18:22:35.470| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 0.0.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0] (0.0.0.0)  vs 127.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0]
2014/11/18 18:22:35.470| Ip.cc(260) FactoryParse: aclIpParseIpData: ::1
2014/11/18 18:22:35.471| Ip.cc(405) FactoryParse: aclIpParseIpData: Lookup Host/IP ::1
2014/11/18 18:22:35.471| Ip.cc(444) FactoryParse: aclIpParseIpData: Located host/IP: '[::1]'
2014/11/18 18:22:35.471| Ip.cc(451) FactoryParse: ::1 --> [::1]
2014/11/18 18:22:35.471| Ip.cc(437) FactoryParse: aclIpParseIpData: Duplicate host/IP: '[::1]' dropped.
2014/11/18 18:22:35.471| Ip.cc(437) FactoryParse: aclIpParseIpData: Duplicate host/IP: '[::1]' dropped.
2014/11/18 18:22:35.471| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 0.0.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (0.0.0.0)  vs [::1]-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2014/11/18 18:22:35.471| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: [::1]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] ([::1])  vs 0.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2014/11/18 18:22:35.471| Processing Configuration File: /etc/squid/squid.conf (depth 0)
2014/11/18 18:22:35.472| cache_cf.cc(556) parseOneConfigFile: Processing: acl localnet src 10.0.0.0/8           # RFC1918 possible internal network
2014/11/18 18:22:35.472| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type src
2014/11/18 18:22:35.472| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.472| Acl.cc(118) FindByName: ACL::FindByName 'localnet'
2014/11/18 18:22:35.472| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.472| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'localnet'
2014/11/18 18:22:35.472| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'src'
2014/11/18 18:22:35.472| Ip.cc(260) FactoryParse: aclIpParseIpData: 10.0.0.0/8
2014/11/18 18:22:35.472| Ip.cc(368) FactoryParse: aclIpParseIpData: '10.0.0.0/8' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.472| Ip.cc(505) FactoryParse: Parsed: 10.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0](/104)
2014/11/18 18:22:35.472| cache_cf.cc(556) parseOneConfigFile: Processing: acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
2014/11/18 18:22:35.472| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type src
2014/11/18 18:22:35.472| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.472| Acl.cc(118) FindByName: ACL::FindByName 'localnet'
2014/11/18 18:22:35.472| Acl.cc(270) ParseAclLine: aclParseAclLine: Appending to 'localnet'
2014/11/18 18:22:35.472| Ip.cc(260) FactoryParse: aclIpParseIpData: 172.16.0.0/12
2014/11/18 18:22:35.472| Ip.cc(368) FactoryParse: aclIpParseIpData: '172.16.0.0/12' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.472| Ip.cc(505) FactoryParse: Parsed: 172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0](/108)
2014/11/18 18:22:35.472| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 10.0.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0] (10.0.0.0)  vs 172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
2014/11/18 18:22:35.472| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 172.16.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0] (172.0.0.0)  vs 10.0.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ff00:0]
2014/11/18 18:22:35.472| cache_cf.cc(556) parseOneConfigFile: Processing: acl localnet src 192.168.0.0/16  # RFC1918 possible internal network
2014/11/18 18:22:35.472| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type src
2014/11/18 18:22:35.472| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.472| Acl.cc(118) FindByName: ACL::FindByName 'localnet'
2014/11/18 18:22:35.472| Acl.cc(270) ParseAclLine: aclParseAclLine: Appending to 'localnet'
2014/11/18 18:22:35.472| Ip.cc(260) FactoryParse: aclIpParseIpData: 192.168.0.0/16
2014/11/18 18:22:35.472| Ip.cc(368) FactoryParse: aclIpParseIpData: '192.168.0.0/16' matched: SCAN3-v4: %[0123456789.]/%[0123456789.]
2014/11/18 18:22:35.472| Ip.cc(505) FactoryParse: Parsed: 192.168.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0](/112)
2014/11/18 18:22:35.472| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 172.16.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0] (172.16.0.0)  vs 192.168.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
2014/11/18 18:22:35.472| Ip.cc(134) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare: 192.168.0.0/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0] (192.160.0.0)  vs 172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
2014/11/18 18:22:35.472| cache_cf.cc(556) parseOneConfigFile: Processing: acl SSL_ports port 443                              # https
2014/11/18 18:22:35.472| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type port
2014/11/18 18:22:35.472| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.472| Acl.cc(118) FindByName: ACL::FindByName 'SSL_ports'
2014/11/18 18:22:35.472| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.472| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'SSL_ports'
2014/11/18 18:22:35.472| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'port'
2014/11/18 18:22:35.472| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1180fb8
2014/11/18 18:22:35.473| cache_cf.cc(556) parseOneConfigFile: Processing: acl Safe_ports port 80                              # http
2014/11/18 18:22:35.473| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type port
2014/11/18 18:22:35.473| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.473| Acl.cc(118) FindByName: ACL::FindByName 'Safe_ports'
2014/11/18 18:22:35.473| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.473| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'Safe_ports'
2014/11/18 18:22:35.473| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'port'
2014/11/18 18:22:35.473| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1180218
2014/11/18 18:22:35.473| cache_cf.cc(556) parseOneConfigFile: Processing: acl Safe_ports port 443                              # https
2014/11/18 18:22:35.473| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type port
2014/11/18 18:22:35.473| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.473| Acl.cc(118) FindByName: ACL::FindByName 'Safe_ports'
2014/11/18 18:22:35.473| Acl.cc(270) ParseAclLine: aclParseAclLine: Appending to 'Safe_ports'
2014/11/18 18:22:35.473| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1181148
2014/11/18 18:22:35.473| cache_cf.cc(556) parseOneConfigFile: Processing: acl purge method PURGE
2014/11/18 18:22:35.473| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type method
2014/11/18 18:22:35.473| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.473| Acl.cc(118) FindByName: ACL::FindByName 'purge'
2014/11/18 18:22:35.473| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.473| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'purge'
2014/11/18 18:22:35.473| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'method'
2014/11/18 18:22:35.473| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1182178
2014/11/18 18:22:35.473| cache_cf.cc(556) parseOneConfigFile: Processing: acl CONNECT method CONNECT
2014/11/18 18:22:35.473| Acl.cc(425) Registered: ACL::Prototype::Registered: invoked for type method
2014/11/18 18:22:35.473| Acl.cc(429) Registered: ACL::Prototype::Registered:    yes
2014/11/18 18:22:35.473| Acl.cc(118) FindByName: ACL::FindByName 'CONNECT'
2014/11/18 18:22:35.473| Acl.cc(124) FindByName: ACL::FindByName found no match
2014/11/18 18:22:35.473| Acl.cc(259) ParseAclLine: aclParseAclLine: Creating ACL 'CONNECT'
2014/11/18 18:22:35.473| Acl.cc(461) Factory: ACL::Prototype::Factory: cloning an object for type 'method'
2014/11/18 18:22:35.473| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1182318
2014/11/18 18:22:35.473| cache_cf.cc(556) parseOneConfigFile: Processing: http_access allow manager localhost
2014/11/18 18:22:35.473| InnerNode.cc(50) lineParse: looking for ACL manager
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'manager'
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL localhost
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'localhost'
2014/11/18 18:22:35.474| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1182e88
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access deny manager
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL manager
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'manager'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access allow purge localhost
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL purge
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'purge'
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL localhost
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'localhost'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access deny purge
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL purge
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'purge'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access deny !Safe_ports
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL Safe_ports
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'Safe_ports'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access deny CONNECT !SSL_ports
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL CONNECT
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'CONNECT'
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL SSL_ports
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'SSL_ports'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access allow localhost
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL localhost
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'localhost'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access allow localnet
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL localnet
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'localnet'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_access deny all
2014/11/18 18:22:35.474| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.474| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: http_port 80
2014/11/18 18:22:35.474| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1183e18
2014/11/18 18:22:35.474| cache_cf.cc(3580) parsePortSpecification: http_port: found Listen on Port: 80
2014/11/18 18:22:35.474| cache_cf.cc(3596) parsePortSpecification: http_port: found Listen on wildcard address: *:80
2014/11/18 18:22:35.474| cbdata.cc(419) cbdataInternalLock: cbdataLock: 0x1183e18=1
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: access_log /var/log/squid/access.log squid
2014/11/18 18:22:35.474| cache_cf.cc(4165) setLogformat: possible /var/log/squid/access.log logformat: squid
2014/11/18 18:22:35.474| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x11842e8
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern              ^ftp:                                    1440      20%       10080
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern     ^gopher:                                            1440      0%          1440
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern              -i (/cgi-bin/|\?)                     0             0%          0
2014/11/18 18:22:35.474| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern     (Release|Package(.gz)*)$              0             20%       2880
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern              \.rpm$                                 1440    20%         10080
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: hosts_file /etc/hosts
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: cache_dir ufs /var/spool/squid 100 16 256
2014/11/18 18:22:35.475| filemap.cc(61) FileMap: creating space for 16384 files
2014/11/18 18:22:35.475| filemap.cc(62) FileMap: --> 256 words of 8 bytes each
2014/11/18 18:22:35.475| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1183688
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: coredump_dir /var/spool/squid
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: refresh_pattern deb$ 1440 20% 10080
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: maximum_object_size 1024 MB
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: cache_mem 2048 MB
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: cache_peer proxy.dhs.gov parent 80 0 no-query default no-digest
2014/11/18 18:22:35.475| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x11899f8
2014/11/18 18:22:35.475| event.cc(346) schedule: schedule: Adding 'peerClearRR', in 300.00 seconds
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: never_direct allow all
2014/11/18 18:22:35.475| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.475| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.475| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1189e68
2014/11/18 18:22:35.475| cache_cf.cc(556) parseOneConfigFile: Processing: request_header_access User-Agent deny all
2014/11/18 18:22:35.475| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.475| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.475| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x118c2d8
2014/11/18 18:22:35.476| cache_cf.cc(556) parseOneConfigFile: Processing: request_header_replace User-Agent MYUSERAGENT; (squid proxy header rewrite DID-IT)
2014/11/18 18:22:35.476| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.476| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.476| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x118c3f8
2014/11/18 18:22:35.476| wccp2.cc(532) wccp2_add_service_list: wccp2_add_service_list: added service id 0
2014/11/18 18:22:35.476| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.476| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.476| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x1180e58
2014/11/18 18:22:35.476| InnerNode.cc(50) lineParse: looking for ACL ssl::certUntrusted
2014/11/18 18:22:35.476| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certUntrusted'
2014/11/18 18:22:35.476| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x118fc98
2014/11/18 18:22:35.476| InnerNode.cc(50) lineParse: looking for ACL ssl::certSelfSigned
2014/11/18 18:22:35.476| Acl.cc(118) FindByName: ACL::FindByName 'ssl::certSelfSigned'
2014/11/18 18:22:35.476| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x118fe98
2014/11/18 18:22:35.476| InnerNode.cc(50) lineParse: looking for ACL all
2014/11/18 18:22:35.476| Acl.cc(118) FindByName: ACL::FindByName 'all'
2014/11/18 18:22:35.476| cbdata.cc(324) cbdataInternalAlloc: cbdataAlloc: 0x118fb68
2014/11/18 18:22:35.476| tools.cc(631) uniqueHostname:  Config: '
2014/11/18 18:22:35.476| tools.cc(606) getMyHostname: getMyHostname: 'somehost.uscis.dhs.gov' has DNS resolution.
2014/11/18 18:22:35.477| tools.cc(631) uniqueHostname:  Config: '
2014/11/18 18:22:35.477| Initializing https proxy context
2014/11/18 18:22:35.478| support.cc(1055) sslCreateClientContext: Using SSLv2/SSLv3.
2014/11/18 18:22:35.479| support.cc(1107) sslCreateClientContext: Setting RSA key generation callback.
2014/11/18 18:22:35.479| support.cc(1114) sslCreateClientContext: Setting certificate verification callback.
2014/11/18 18:22:35.479| support.cc(1118) sslCreateClientContext: Setting CA certificate locations.
2014/11/18 18:22:35.501| tools.cc(643) leave_suid: leave_suid: PID 22996 called
2014/11/18 18:22:35.501| tools.cc(665) leave_suid: leave_suid: PID 22996 giving up root, becoming 'squid'
2014/11/18 18:22:35.501| debug.cc(425) parseOptions: command-line -X overrides: ALL,1


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141118/880a289f/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Nov  7 20:07:45 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Fri, 07 Nov 2014 20:07:45 -0000
Subject: [squid-users] bad kernel logs on high traffic server
Message-ID: <002301cffb1a$3c8b3030$b5a19090$@netstream.ps>

Hi guys , I have server with about 1000 users and its okay with 2 workers

 

Now I added more 1000 and its now 2000 users with 4 workers

 

But sometime some process of squid get killed suddenly and here is log below
, 

 

Here  is sysctl.conf

[root at Largerock-squid ~]# cat  /etc/sysctl.conf     

# Kernel sysctl configuration file for Red Hat Linux

#

# For binary values, 0 is disabled, 1 is enabled.  See sysctl(8) and

# sysctl.conf(5) for more details.

 

# Controls IP packet forwarding

################net.ipv4.ip_forward = 0

net.ipv4.ip_forward = 1

# Controls source route verification

#################net.ipv4.conf.default.rp_filter = 1

net.ipv4.conf.default.rp_filter = 0

###################################

net.ipv4.conf.all.rp_filter = 0

net.ipv4.conf.eth0.rp_filter = 0

###############################

# Do not accept source routing

net.ipv4.conf.default.accept_source_route = 0

 

# Controls the System Request debugging functionality of the kernel

kernel.sysrq = 0

 

# Controls whether core dumps will append the PID to the core filename.

# Useful for debugging multi-threaded applications.

kernel.core_uses_pid = 1

 

# Controls the use of TCP syncookies

net.ipv4.tcp_syncookies = 1

 

# Disable netfilter on bridges.

net.bridge.bridge-nf-call-ip6tables = 0

net.bridge.bridge-nf-call-iptables = 0

net.bridge.bridge-nf-call-arptables = 0

 

# Controls the default maxmimum size of a mesage queue

kernel.msgmnb = 65536

 

# Controls the maximum size of a message, in bytes

kernel.msgmax = 65536

 

# Controls the maximum shared segment size, in bytes

kernel.shmmax = 68719476736

 

# Controls the maximum number of shared memory segments, in pages

kernel.shmall = 4294967296

################################

###############################

fs.file-max = 1131072

net.local.dgram.recvspace = 1262144

net.local.dgram.maxdgram = 116384

net.netfilter.nf_conntrack_max = 1130000

########################################

net.nf_conntrack_max = 1131072

#########################################

####kernel: possible SYN flooding on port 80. Sending cookies.#####

net.ipv4.tcp_syncookies=0

 

 

im wondering wt should I do ,   or modify my kernel linux

 

the error code is below :

 

 

 

Nov  7 14:52:16 Largerock-squid snmpd[1576]: Connection from UDP:
[xxx]:57067xxx]

Nov  7 14:52:17 Largerock-squid kernel: snmpd invoked oom-killer:
gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0

Nov  7 14:52:17 Largerock-squid kernel: snmpd cpuset=/ mems_allowed=0

Nov  7 14:52:17 Largerock-squid kernel: Pid: 1576, comm: snmpd Not tainted
2.6.32-431.el6.x86_64 #1

Nov  7 14:52:17 Largerock-squid kernel: Call Trace:

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff810d05b1>] ?
cpuset_print_task_mems_allowed+0x91/0xb0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81122960>] ?
dump_header+0x90/0x1b0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8122798c>] ?
security_real_capable_noaudit+0x3c/0x70

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81122de2>] ?
oom_kill_process+0x82/0x2a0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81122d21>] ?
select_bad_process+0xe1/0x120

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81123220>] ?
out_of_memory+0x220/0x3c0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8112fb3c>] ?
__alloc_pages_nodemask+0x8ac/0x8d0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81167a9a>] ?
alloc_pages_current+0xaa/0x110

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8111fd57>] ?
__page_cache_alloc+0x87/0x90

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8111f73e>] ?
find_get_page+0x1e/0xa0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81120cf7>] ?
filemap_fault+0x1a7/0x500

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8114a084>] ?
__do_fault+0x54/0x530

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8114a657>] ?
handle_pte_fault+0xf7/0xb00

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81447c54>] ?
move_addr_to_kernel+0x64/0x70

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff810129de>] ?
copy_user_generic+0xe/0x20

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8114b28a>] ?
handle_mm_fault+0x22a/0x300

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8104a8d8>] ?
__do_page_fault+0x138/0x480

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff81448119>] ?
sys_sendto+0x139/0x190

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff810149b9>] ?
read_tsc+0x9/0x20

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff810a70a1>] ?
ktime_get_ts+0xb1/0xf0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8119fb18>] ?
poll_select_copy_remaining+0xf8/0x150

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8152d45e>] ?
do_page_fault+0x3e/0xa0

Nov  7 14:52:17 Largerock-squid kernel: [<ffffffff8152a815>] ?
page_fault+0x25/0x30

Nov  7 14:52:17 Largerock-squid kernel: Mem-Info:

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA per-cpu:

Nov  7 14:52:17 Largerock-squid kernel: CPU    0: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    1: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    2: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    3: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    4: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    5: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    6: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: CPU    7: hi:    0, btch:   1 usd:
0

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA32 per-cpu:

Nov  7 14:52:17 Largerock-squid kernel: CPU    0: hi:  186, btch:  31 usd:
77

Nov  7 14:52:17 Largerock-squid kernel: CPU    1: hi:  186, btch:  31 usd:
34

Nov  7 14:52:17 Largerock-squid kernel: CPU    2: hi:  186, btch:  31 usd:
31

Nov  7 14:52:17 Largerock-squid kernel: CPU    3: hi:  186, btch:  31 usd:
33

Nov  7 14:52:17 Largerock-squid kernel: CPU    4: hi:  186, btch:  31 usd:
24

Nov  7 14:52:17 Largerock-squid kernel: CPU    5: hi:  186, btch:  31 usd:
24

Nov  7 14:52:17 Largerock-squid kernel: CPU    6: hi:  186, btch:  31 usd:
22

Nov  7 14:52:17 Largerock-squid kernel: CPU    7: hi:  186, btch:  31 usd:
17

Nov  7 14:52:17 Largerock-squid kernel: Node 0 Normal per-cpu:

Nov  7 14:52:17 Largerock-squid kernel: CPU    0: hi:  186, btch:  31 usd:
148

Nov  7 14:52:17 Largerock-squid kernel: CPU    1: hi:  186, btch:  31 usd:
59

Nov  7 14:52:17 Largerock-squid kernel: CPU    2: hi:  186, btch:  31 usd:
64

Nov  7 14:52:17 Largerock-squid kernel: CPU    3: hi:  186, btch:  31 usd:
34

Nov  7 14:52:17 Largerock-squid kernel: CPU    4: hi:  186, btch:  31 usd:
21

Nov  7 14:52:17 Largerock-squid kernel: CPU    5: hi:  186, btch:  31 usd:
22

Nov  7 14:52:17 Largerock-squid kernel: CPU    6: hi:  186, btch:  31 usd:
50

Nov  7 14:52:17 Largerock-squid kernel: CPU    7: hi:  186, btch:  31 usd:
55

Nov  7 14:52:17 Largerock-squid kernel: active_anon:3847274
inactive_anon:2365500 isolated_anon:0

Nov  7 14:52:17 Largerock-squid kernel: active_file:77 inactive_file:0
isolated_file:32

Nov  7 14:52:17 Largerock-squid kernel: unevictable:0 dirty:1 writeback:0
unstable:0

Nov  7 14:52:17 Largerock-squid kernel: free:41667 slab_reclaimable:28547
slab_unreclaimable:280933

Nov  7 14:52:17 Largerock-squid kernel: mapped:2365635 shmem:2365544
pagetables:48036 bounce:0

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA free:15548kB min:36kB
low:44kB high:52kB active_anon:0kB inactive_anon:0kB active_file:0kB
inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB
present:15156kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB
slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB
unstable:0kB bounce:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable?
yes

Nov  7 14:52:17 Largerock-squid kernel: lowmem_reserve[]: 0 3000 26482 26482

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA32 free:101504kB
min:7652kB low:9564kB high:11476kB active_anon:1681480kB
inactive_anon:47812kB active_file:8kB inactive_file:0kB unevictable:0kB
isolated(anon):0kB isolated(file):0kB present:3072160kB mlocked:0kB
dirty:0kB writeback:0kB mapped:47836kB shmem:47812kB
slab_reclaimable:18296kB slab_unreclaimable:591000kB kernel_stack:56kB
pagetables:3520kB unstable:0kB bounce:0kB writeback_tmp:0kB pages_scanned:64
all_unreclaimable? yes

Nov  7 14:52:17 Largerock-squid kernel: lowmem_reserve[]: 0 0 23482 23482

Nov  7 14:52:17 Largerock-squid kernel: Node 0 Normal free:49616kB
min:59892kB low:74864kB high:89836kB active_anon:13707616kB
inactive_anon:9414188kB active_file:300kB inactive_file:0kB unevictable:0kB
isolated(anon):0kB isolated(file):128kB present:24046080kB mlocked:0kB
dirty:4kB writeback:0kB mapped:9414704kB shmem:9414364kB
slab_reclaimable:95892kB slab_unreclaimable:533024kB kernel_stack:1648kB
pagetables:188624kB unstable:0kB bounce:0kB writeback_tmp:0kB
pages_scanned:458 all_unreclaimable? yes

Nov  7 14:52:17 Largerock-squid kernel: lowmem_reserve[]: 0 0 0 0

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA: 1*4kB 1*8kB 1*16kB
1*32kB 2*64kB 0*128kB 0*256kB 0*512kB 1*1024kB 1*2048kB 3*4096kB = 15548kB

Nov  7 14:52:17 Largerock-squid kernel: Node 0 DMA32: 23974*4kB 1*8kB 0*16kB
0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 1*2048kB 1*4096kB = 102048kB

Nov  7 14:52:17 Largerock-squid kernel: Node 0 Normal: 11163*4kB 0*8kB
0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 1*4096kB =
48748kB

Nov  7 14:52:17 Largerock-squid kernel: 2365580 total pagecache pages

Nov  7 14:52:17 Largerock-squid kernel: 0 pages in swap cache

Nov  7 14:52:17 Largerock-squid kernel: Swap cache stats: add 0, delete 0,
find 0/0

Nov  7 14:52:17 Largerock-squid kernel: Free swap  = 0kB

Nov  7 14:52:17 Largerock-squid kernel: Total swap = 0kB

Nov  7 14:52:17 Largerock-squid kernel: 6881264 pages RAM

Nov  7 14:52:17 Largerock-squid kernel: 147804 pages reserved

Nov  7 14:52:17 Largerock-squid kernel: 4911130 pages shared

Nov  7 14:52:17 Largerock-squid kernel: 4272662 pages non-shared

Nov  7 14:52:17 Largerock-squid kernel: [ pid ]   uid  tgid total_vm
rss cpu oom_adj oom_score_adj name

Nov  7 14:52:17 Largerock-squid kernel: [  500]     0   500     2737
179   5     -17         -1000 udevd

Nov  7 14:52:17 Largerock-squid kernel: [  921]     0   921     2662
107   6     -17         -1000 udevd

Nov  7 14:52:17 Largerock-squid kernel: [  922]     0   922     2662
107   7     -17         -1000 udevd

Nov  7 14:52:17 Largerock-squid kernel: [ 1230]     0  1230    23294
68   4     -17         -1000 auditd

Nov  7 14:52:17 Largerock-squid kernel: [ 1254]     0  1254     1540
24   4       0             0 portreserve

Nov  7 14:52:17 Largerock-squid kernel: [ 1261]     0  1261    63856
235   1       0             0 rsyslogd

Nov  7 14:52:17 Largerock-squid kernel: [ 1290]     0  1290     2738
62   6       0             0 irqbalance

Nov  7 14:52:17 Largerock-squid kernel: [ 1304]    32  1304     4744
62   6       0             0 rpcbind

Nov  7 14:52:17 Largerock-squid kernel: [ 1322]    29  1322     5837
114   1       0             0 rpc.statd

Nov  7 14:52:17 Largerock-squid kernel: [ 1432]    81  1432     5351
71   2       0             0 dbus-daemon

Nov  7 14:52:17 Largerock-squid kernel: [ 1448]     0  1448    47336
223   4       0             0 cupsd

Nov  7 14:52:17 Largerock-squid kernel: [ 1473]     0  1473     1020
27   5       0             0 acpid

Nov  7 14:52:17 Largerock-squid kernel: [ 1482]    68  1482     9476
234   1       0             0 hald

Nov  7 14:52:17 Largerock-squid kernel: [ 1483]     0  1483     5082
54   4       0             0 hald-runner

Nov  7 14:52:17 Largerock-squid kernel: [ 1512]     0  1512     5612
41   5       0             0 hald-addon-inpu

Nov  7 14:52:17 Largerock-squid kernel: [ 1530]    68  1530     4484
36   4       0             0 hald-addon-acpi

Nov  7 14:52:17 Largerock-squid kernel: [ 1551]     0  1551    96434
124   1       0             0 automount

Nov  7 14:52:17 Largerock-squid kernel: [ 1567]     0  1567     1570
32   4       0             0 mcelog

Nov  7 14:52:17 Largerock-squid kernel: [ 1576]     0  1576    49460
955   5       0             0 snmpd

Nov  7 14:52:17 Largerock-squid kernel: [ 1588]     0  1588    16655
177   4     -17         -1000 sshd

Nov  7 14:52:17 Largerock-squid kernel: [ 1664]     0  1664    20326
226   6       0             0 master

Nov  7 14:52:17 Largerock-squid kernel: [ 1677]    89  1677    20389
221   0       0             0 qmgr

Nov  7 14:52:17 Largerock-squid kernel: [ 1688]     0  1688    27580
35   1       0             0 abrtd

Nov  7 14:52:17 Largerock-squid kernel: [ 1703]     0  1703    29324
152   2       0             0 crond

Nov  7 14:52:17 Largerock-squid kernel: [ 1717]   500  1717  5166698
14730   4       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1719]   500  1719  5168582
21614   0       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1721]   500  1721  6119473
2180266   6       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1722]   500  1722  6136601
2165217   4       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1723]   500  1723  6252323
2575690   4       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1724]   500  1724  5950985
1737617   0       0             0 squid

Nov  7 14:52:17 Largerock-squid kernel: [ 1742]     0  1742     5385
46   1       0             0 atd

Nov  7 14:52:17 Largerock-squid kernel: [ 1754]     0  1754    15587
89   4       0             0 certmonger

Nov  7 14:52:17 Largerock-squid kernel: [ 1770]     0  1770     1016
18   4       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 1772]     0  1772     1016
18   3       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 1774]     0  1774     1016
19   6       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 1776]     0  1776     1016
19   2       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 1778]     0  1778     1016
17   6       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 1780]     0  1780     1016
19   5       0             0 mingetty

Nov  7 14:52:17 Largerock-squid kernel: [ 7149]    89  7149    20346
220   4       0             0 pickup

Nov  7 14:52:17 Largerock-squid kernel: [ 7295]     0  7295    25095
264   6       0             0 sshd

Nov  7 14:52:17 Largerock-squid kernel: [ 7297]     0  7297    27085
90   7       0             0 bash

Nov  7 14:52:17 Largerock-squid kernel: [ 7316]     0  7316    28210
211   0       0             0 htop

Nov  7 14:52:17 Largerock-squid kernel: [ 7339]     0  7339    27085
91   5       0             0 bash

Nov  7 14:52:17 Largerock-squid kernel: [ 7360]     0  7360    25224
31   6       0             0 tailf

Nov  7 14:52:17 Largerock-squid kernel: [ 7361]     0  7361    27085
93   3       0             0 bash

Nov  7 14:52:17 Largerock-squid kernel: [ 7380]     0  7380    27057
220   4       0             0 nano

Nov  7 14:52:17 Largerock-squid kernel: [ 7383]     0  7383    27085
89   5       0             0 bash

Nov  7 14:52:17 Largerock-squid kernel: [ 7402]     0  7402    27590
226   0       0             0 iptraf

Nov  7 14:52:17 Largerock-squid kernel: [ 7412]     0  7412    27085
91   4       0             0 bash

Nov  7 14:52:17 Largerock-squid kernel: [ 7431]     0  7431    25224
30   5       0             0 tailf

Nov  7 14:52:17 Largerock-squid kernel: Out of memory: Kill process 1723
(squid) score 384 or sacrifice child

Nov  7 14:52:17 Largerock-squid kernel: Killed process 1723, UID 500,
(squid) total-vm:25009292kB, anon-rss:4356424kB, file-rss:5946336kB

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141107/f13fe65d/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Nov 14 13:46:01 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Fri, 14 Nov 2014 13:46:01 -0000
Subject: [squid-users] using squid 3.head for large rock ,
	but i still have mean object size is 32 !!!!!
Message-ID: <000001d00065$101828a0$304879e0$@netstream.ps>

Hi ,

 

I migrated from squid 3.4.3 so that I wish to have a chance to save bw.

 

Im using : Squid Cache: Version 3.HEAD-20141105-r13687

 

 

With options below :

 

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS=' 'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g
-Wall -O2' '--enable-ltdl-convenience'

 

 

I have 16 cores and I have set 6 workes and used aufs cahe dir for bw saving
but still no luck ... the out traffc in general is less than in traffic.

 

This idea is getting me a headache !!!!

 

Here is my cache manager :

Connection information for squid:

        Number of clients accessing cache:    8967

        Number of HTTP requests received:     455542

        Number of ICP messages received:      0

        Number of ICP messages sent:   0

        Number of queued ICP replies:  0

        Number of HTCP messages received:     0

        Number of HTCP messages sent:  0

        Request failure ratio: 0.00

        Average HTTP requests per minute since start: 35706.3

        Average ICP messages per minute since start:  0.0

        Select loop called: 14737492 times, 0.345 ms avg

Cache information for squid:

        Hits as % of all requests:     5min: 10.4%, 60min: 10.6%

        Hits as % of bytes sent:       5min: -0.6%, 60min: -0.8%

        Memory hits as % of hit requests:     5min: 37.1%, 60min: 36.9%

        Disk hits as % of hit requests:       5min: 28.0%, 60min: 28.4%

        Storage Swap size:     29253956 KB

        Storage Swap capacity: 10.6% used, 89.4% free

        Storage Mem size:      2434400 KB

        Storage Mem capacity:  39.6% used, 60.4% free

        Mean Object Size:      32.60 KB

        Requests given to unlinkd:     0

Median Service Times (seconds)  5 min    60 min:

        HTTP Requests (All):   0.15616  0.15748

        Cache Misses:          0.18340  0.19003

        Cache Hits:            0.00030  0.00030

        Near Hits:             0.08938  0.08686

        Not-Modified Replies:  0.00000  0.00000

        DNS Lookups:           0.00000  0.00000

        ICP Queries:           0.00000  0.00000

Resource usage for squid:

        UP Time:       765.486 seconds

        CPU Time:      1333.285 seconds

        CPU Usage:     174.18%

        CPU Usage, 5 minute avg:       176.56%

        CPU Usage, 60 minute avg:      176.15%

        Maximum Resident Size: 22667056 KB

        Page faults with physical i/o: 0

Memory accounted for:

        Total accounted:       1568707 KB

        memPoolAlloc calls:      1830

        memPoolFree calls:  133080611

File descriptor usage for squid:

        Maximum number of file descriptors:   393216

        Largest file desc currently in use:   6574

        Number of file desc currently in use: 23510

        Files queued for open:                   0

        Available number of file descriptors: 369706

        Reserved number of file descriptors:   600

        Store Disk files open:                  37

Internal Data Structures:

        899673 StoreEntries

          2442 StoreEntries with MemObjects

        39600 Hot Object Cache Items

        897283 on-disk objects

 

 

 

 

 

 

 

 

 

sample_start_time = 1415972068.352725 (Fri, 14 Nov 2014 13:34:28 GMT)

sample_end_time = 1415972368.441291 (Fri, 14 Nov 2014 13:39:28 GMT)

client_http.requests = 624.049293/sec

client_http.hits = 52.839655/sec

client_http.errors = 8.046615/sec

client_http.kbytes_in = 481.063567/sec

client_http.kbytes_out = 42844.925855/sec

client_http.all_median_svc_time = 0.154761 seconds

client_http.miss_median_svc_time = 0.181403 seconds

client_http.nm_median_svc_time = 0.000000 seconds

client_http.nh_median_svc_time = 0.094540 seconds

client_http.hit_median_svc_time = 0.000304 seconds

server.all.requests = 556.379734/sec

server.all.errors = 0.000000/sec

server.all.kbytes_in = 43258.073243/sec

server.all.kbytes_out = 494.860147/sec

server.http.requests = 556.379734/sec

server.http.errors = 0.000000/sec

server.http.kbytes_in = 43258.073243/sec

server.http.kbytes_out = 494.860147/sec

server.ftp.requests = 0.000000/sec

server.ftp.errors = 0.000000/sec

server.ftp.kbytes_in = 0.000000/sec

server.ftp.kbytes_out = 0.000000/sec

server.other.requests = 0.000000/sec

server.other.errors = 0.000000/sec

server.other.kbytes_in = 0.000000/sec

server.other.kbytes_out = 0.000000/sec

icp.pkts_sent = 0.000000/sec

icp.pkts_recv = 0.000000/sec

icp.queries_sent = 0.000000/sec

icp.replies_sent = 0.000000/sec

icp.queries_recv = 0.000000/sec

icp.replies_recv = 0.000000/sec

icp.replies_queued = 0.000000/sec

icp.query_timeouts = 0.000000/sec

icp.kbytes_sent = 0.000000/sec

icp.kbytes_recv = 0.000000/sec

icp.q_kbytes_sent = 0.000000/sec

icp.r_kbytes_sent = 0.000000/sec

icp.q_kbytes_recv = 0.000000/sec

icp.r_kbytes_recv = 0.000000/sec

icp.query_median_svc_time = 0.000000 seconds

icp.reply_median_svc_time = 0.000000 seconds

dns.median_svc_time = 0.000000 seconds

unlink.requests = 0.000000/sec

page_faults = 0.000000/sec

select_loops = 20058.805484/sec

select_fds = 22262.722249/sec

average_select_fd_period = 0.000000/fd

median_select_fds = 0.000000

swap.outs = 48.353008/sec

swap.ins = 111.065932/sec

swap.files_cleaned = 0.000000/sec

aborted_requests = 38.496416/sec

syscalls.disk.opens = 109.649271/sec

syscalls.disk.closes = 213.635244/sec

syscalls.disk.reads = 133.399114/sec

syscalls.disk.writes = 910.827293/sec

syscalls.disk.seeks = 0.000000/sec

syscalls.disk.unlinks = 3.456643/sec

syscalls.sock.accepts = 684.672141/sec

syscalls.sock.sockets = 241.881767/sec

syscalls.sock.connects = 241.695102/sec

syscalls.sock.binds = 241.671769/sec

syscalls.sock.closes = 542.913161/sec

syscalls.sock.reads = 7940.295208/sec

syscalls.sock.writes = 13137.561543/sec

syscalls.sock.recvfroms = 318.947897/sec

syscalls.sock.sendtos = 180.778809/sec

cpu_time = 524.449272 seconds

wall_time = 1800.012392 seconds

cpu_usage = 29.135870%

 

  _____  

 

 

Generated Fri, 14 Nov 2014 13:39:36 GMT, by
cachemgr.cgi/3.4.3 at Largerock-squid

 

 

 

And here is iptraf :

 

| Total rates:     752700.3 kbits/sec        Broadcast packets:            0
|

|                   90466.8 packets/sec      Broadcast bytes:              0
|

|
|

| Incoming rates:  378217.1 kbits/sec
|

|                   54377.4 packets/sec
|

|                                            IP checksum errors:           0
|

| Outgoing rates:  375640.1 kbits/sec
|

|                   36089.4 packets/sec
|

|
|

|                                            

 

 

Any help ?????

 

BTW , I used both squid3.4.3 and squid3.head and same issue !!!! why I cant
save bw !!!

 

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/d0295591/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Nov 14 15:39:08 2014
From: ahmed.zaeem at netstream.ps (Ahmed Allzaeem)
Date: Fri, 14 Nov 2014 15:39:08 -0000
Subject: [squid-users] using squid 3.head for large rock ,
	but i still have mean object size is 32 !!!!!
In-Reply-To: <000a01d00074$77027340$650759c0$@netstream.ps>
References: <000a01d00074$77027340$650759c0$@netstream.ps>
Message-ID: <001401d00074$e1299730$a37cc590$@netstream.ps>

 

More info ...

 

 

sample_start_time = 1415972068.352725 (Fri, 14 Nov 2014 13:34:28 GMT)

sample_end_time = 1415972368.441291 (Fri, 14 Nov 2014 13:39:28 GMT)

client_http.requests = 624.049293/sec

client_http.hits = 52.839655/sec

client_http.errors = 8.046615/sec

client_http.kbytes_in = 481.063567/sec

client_http.kbytes_out = 42844.925855/sec

client_http.all_median_svc_time = 0.154761 seconds

client_http.miss_median_svc_time = 0.181403 seconds

client_http.nm_median_svc_time = 0.000000 seconds

client_http.nh_median_svc_time = 0.094540 seconds

client_http.hit_median_svc_time = 0.000304 seconds

server.all.requests = 556.379734/sec

server.all.errors = 0.000000/sec

server.all.kbytes_in = 43258.073243/sec

server.all.kbytes_out = 494.860147/sec

server.http.requests = 556.379734/sec

server.http.errors = 0.000000/sec

server.http.kbytes_in = 43258.073243/sec

server.http.kbytes_out = 494.860147/sec

server.ftp.requests = 0.000000/sec

server.ftp.errors = 0.000000/sec

server.ftp.kbytes_in = 0.000000/sec

server.ftp.kbytes_out = 0.000000/sec

server.other.requests = 0.000000/sec

server.other.errors = 0.000000/sec

server.other.kbytes_in = 0.000000/sec

server.other.kbytes_out = 0.000000/sec

icp.pkts_sent = 0.000000/sec

icp.pkts_recv = 0.000000/sec

icp.queries_sent = 0.000000/sec

icp.replies_sent = 0.000000/sec

icp.queries_recv = 0.000000/sec

icp.replies_recv = 0.000000/sec

icp.replies_queued = 0.000000/sec

icp.query_timeouts = 0.000000/sec

icp.kbytes_sent = 0.000000/sec

icp.kbytes_recv = 0.000000/sec

icp.q_kbytes_sent = 0.000000/sec

icp.r_kbytes_sent = 0.000000/sec

icp.q_kbytes_recv = 0.000000/sec

icp.r_kbytes_recv = 0.000000/sec

icp.query_median_svc_time = 0.000000 seconds

icp.reply_median_svc_time = 0.000000 seconds

dns.median_svc_time = 0.000000 seconds

unlink.requests = 0.000000/sec

page_faults = 0.000000/sec

select_loops = 20058.805484/sec

select_fds = 22262.722249/sec

average_select_fd_period = 0.000000/fd

median_select_fds = 0.000000

swap.outs = 48.353008/sec

swap.ins = 111.065932/sec

swap.files_cleaned = 0.000000/sec

aborted_requests = 38.496416/sec

syscalls.disk.opens = 109.649271/sec

syscalls.disk.closes = 213.635244/sec

syscalls.disk.reads = 133.399114/sec

syscalls.disk.writes = 910.827293/sec

syscalls.disk.seeks = 0.000000/sec

syscalls.disk.unlinks = 3.456643/sec

syscalls.sock.accepts = 684.672141/sec

syscalls.sock.sockets = 241.881767/sec

syscalls.sock.connects = 241.695102/sec

syscalls.sock.binds = 241.671769/sec

syscalls.sock.closes = 542.913161/sec

syscalls.sock.reads = 7940.295208/sec

syscalls.sock.writes = 13137.561543/sec

syscalls.sock.recvfroms = 318.947897/sec

syscalls.sock.sendtos = 180.778809/sec

cpu_time = 524.449272 seconds

wall_time = 1800.012392 seconds

cpu_usage = 29.135870%

 

  _____  

 

 

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Ahmed Allzaeem
Sent: Friday, November 14, 2014 5:36 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] using squid 3.head for large rock , but i still have
mean object size is 32 !!!!!

 

 

Hi ,

 

I migrated from squid 3.4.3 so that I wish to have a chance to save bw.

 

Im using : Squid Cache: Version 3.HEAD-20141105-r13687

 

 

With options below :

 

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS=' 'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g
-Wall -O2' '--enable-ltdl-convenience'

 

 

I have 16 cores and I have set 6 workes and used aufs cahe dir for bw saving
but still no luck ... the out traffc in general is less than in traffic.

 

This idea is getting me a headache !!!!

 

Here is my cache manager :

Connection information for squid:

        Number of clients accessing cache:    8967

        Number of HTTP requests received:     455542

        Number of ICP messages received:      0

        Number of ICP messages sent:   0

        Number of queued ICP replies:  0

        Number of HTCP messages received:     0

        Number of HTCP messages sent:  0

        Request failure ratio: 0.00

        Average HTTP requests per minute since start: 35706.3

        Average ICP messages per minute since start:  0.0

        Select loop called: 14737492 times, 0.345 ms avg

Cache information for squid:

        Hits as % of all requests:     5min: 10.4%, 60min: 10.6%

        Hits as % of bytes sent:       5min: -0.6%, 60min: -0.8%

        Memory hits as % of hit requests:     5min: 37.1%, 60min: 36.9%

        Disk hits as % of hit requests:       5min: 28.0%, 60min: 28.4%

        Storage Swap size:     29253956 KB

        Storage Swap capacity: 10.6% used, 89.4% free

        Storage Mem size:      2434400 KB

        Storage Mem capacity:  39.6% used, 60.4% free

        Mean Object Size:      32.60 KB

        Requests given to unlinkd:     0

Median Service Times (seconds)  5 min    60 min:

        HTTP Requests (All):   0.15616  0.15748

        Cache Misses:          0.18340  0.19003

        Cache Hits:            0.00030  0.00030

        Near Hits:             0.08938  0.08686

        Not-Modified Replies:  0.00000  0.00000

        DNS Lookups:           0.00000  0.00000

        ICP Queries:           0.00000  0.00000

Resource usage for squid:

        UP Time:       765.486 seconds

        CPU Time:      1333.285 seconds

        CPU Usage:     174.18%

        CPU Usage, 5 minute avg:       176.56%

        CPU Usage, 60 minute avg:      176.15%

        Maximum Resident Size: 22667056 KB

        Page faults with physical i/o: 0

Memory accounted for:

        Total accounted:       1568707 KB

        memPoolAlloc calls:      1830

        memPoolFree calls:  133080611

File descriptor usage for squid:

        Maximum number of file descriptors:   393216

        Largest file desc currently in use:   6574

        Number of file desc currently in use: 23510

        Files queued for open:                   0

        Available number of file descriptors: 369706

        Reserved number of file descriptors:   600

        Store Disk files open:                  37

Internal Data Structures:

        899673 StoreEntries

          2442 StoreEntries with MemObjects

        39600 Hot Object Cache Items

        897283 on-disk objects

 

 

 

And here is iptraf :

 

| Total rates:     752700.3 kbits/sec        Broadcast packets:            0
|

|                   90466.8 packets/sec      Broadcast bytes:              0
|

|
|

| Incoming rates:  378217.1 kbits/sec
|

|                   54377.4 packets/sec
|

|                                            IP checksum errors:           0
|

| Outgoing rates:  375640.1 kbits/sec
|

|                   36089.4 packets/sec
|

|
|

|                                            

 

 

Any help ?????

 

BTW , I used both squid3.4.3 and squid3.head and same issue !!!! why I cant
save bw !!!

 

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141114/b425475d/attachment.htm>

From james at ejbdigital.com.au  Sun Nov 30 20:44:20 2014
From: james at ejbdigital.com.au (James Harper)
Date: Sun, 30 Nov 2014 20:44:20 +0000
Subject: [squid-users] squid-3.5.0.2-20141031-r13657 crashes
Message-ID: <HKNPR04MB193FD4B698E424FC11690E5E87C0@HKNPR04MB193.apcprd04.prod.outlook.com>

I've been getting squid crashes with squid-3.5.0.2-20141031-r13657. Basically I think my cache got corrupt - started seeing TCP_SWAPFAIL_MISS and md5 mismatches.
Config is cache_dir ufs /usr/local/squid/var/cache/squid 102400 16 256

It's possible that at one point I might have started 2 instances of squid running at once... could that cause corruption?

And if it happens again, what sort of things should I collect to better diagnose the problem? As I see it there are two problems:
1. that the cache got corrupt in the first place
2. that a corrupt cache can crash squid

Unfortunately I did the stupid thing and deleted the cache without taking a copy for post-mortem... the best I can do is:

[31072.428922] squid[6317]: segfault at 58 ip 000000000061a6f9 sp 00007fff8b9e2d40 error 4 in squid[400000+4e9000]
[31654.707792] squid[6329]: segfault at 58 ip 000000000061a6f9 sp 00007fff54358fe0 error 4 in squid[400000+4e9000]
[31783.399832] squid[6465]: segfault at 58 ip 000000000061a6f9 sp 00007fff82af0aa0 error 4 in squid[400000+4e9000]
[31984.470507] squid[6509]: segfault at 58 ip 000000000061a6f9 sp 00007fff028a6640 error 4 in squid[400000+4e9000]
[32178.270298] squid[6576]: segfault at 58 ip 000000000061a6f9 sp 00007fffe64a07e0 error 4 in squid[400000+4e9000]
[32789.635935] squid[6626]: segfault at 58 ip 000000000061a6f9 sp 00007ffff6932960 error 4 in squid[400000+4e9000]

addr2line -e /usr/local/squid/sbin/squid 000000000061a6f9
/usr/local/src/squid-3.5.0.2-20141031-r13657/src/store.cc:962

James




From navari.lorenzo at gmail.com  Fri Nov 28 09:23:34 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Fri, 28 Nov 2014 01:23:34 -0800 (PST)
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <547715EF.5050703@solutti.com.br>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
 <547715EF.5050703@solutti.com.br>
Message-ID: <1417166614113-4668542.post@n4.nabble.com>

I saw that the error does not preclude the use of the lines over the 100. I
have no problem with the CPU ( 7 % ) . Only I do not like to see " Warning"



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529p4668542.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Nov 29 04:17:19 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Nov 2014 17:17:19 +1300
Subject: [squid-users] Transparent proxy with Peek and Splice feature.
In-Reply-To: <54772BB5.5010101@gmail.com>
References: <5474C8CF.6030404@gmail.com> <54759E6C.5080404@treenet.co.nz>
 <54772BB5.5010101@gmail.com>
Message-ID: <547948CF.5040408@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 28/11/2014 2:48 a.m., Vadim Rogoziansky wrote:
> Hello Amos.
> 
> Thank you for answer.
> 
> There was made an investigation related to squid's peek and splice 
> issues in transparent mode. One-line explanation is as follows - in
> intercept mode squid can't get a server host name from the request
> header and uses clent IP address instead for both fake cert
> generation and as a SNI record in server bump SSL handshaking. This
> is the root of the problem. However this can be fixed if squid uses
> SNI field taken from client TLS Hello message for that purposes.
> Can you hack squid in this way? What do you think?

I think peek-n-splice is supposed to already be doing that.

However it does depend on whether you are bumping the connection at
step 1 (before ClientHello), step 2 (after ClientHello, before
ServerHello), or step 3 (after both ClientHello and ServerHello) of
the TLS handshake whether the SNI details are present.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUeUjPAAoJELJo5wb/XPRj6QEIAOHrR8wmDcjkfgUh2UtPwpHP
vVkPMEuIrUq9Gxx3uSojCZjlFJPuCQ2UafS1p8LuxcEQ+TRmUFbAu4AkKoO2RoZ5
7fCGoiXTwn4TzFf0pLh9SPBq9j12OJ3uT28EEqbILrT0sbKP02xK/qiJfCLR61Ev
vprAdggapbKg/ns1l1H3BBgZR2A4W/abQPIq6/Eu/r+7nYK6L2oOdqPDWTJjudMV
8D9sdOD9mYYryrdptU0GLh9Q/V5QEhipSkuA936iZ0Dfa2ZSr4gphJyaRAFWSMf3
q502lZy+ASkDa2vAbjALRBgn3VwYWl8KBQcypUKF4UXtaLtF0EIrLMun+p4QxUM=
=44aG
-----END PGP SIGNATURE-----


From hno at squid-cache.org  Thu Nov 27 16:44:44 2014
From: hno at squid-cache.org (Henrik Nordstrom)
Date: Thu, 27 Nov 2014 17:44:44 +0100
Subject: [squid-users] WARNING: there are more than 100 regular
 expressions
In-Reply-To: <1417082346062-4668531.post@n4.nabble.com>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
Message-ID: <1417106684.11342.4.camel@localhost>


tor 2014-11-27 klockan 01:59 -0800 skrev navari.lorenzo at gmail.com:
> "Consider using less REs ..." is not possible.
> 
> if there is no other solution
> i will break the files in many files with less then 100 entries.
> 
> Probably will have the same problem with black list.

How many REs do you need in your blacklist? Most blacklists I have seen
is domain based, not pattern based.

If you don't know what I mean then read up on the difference bewteen
dstdom_regex and dstdomain ACL types in Squid.

Regards
Henrik




From chio1990 at gmail.com  Sun Nov 30 07:17:31 2014
From: chio1990 at gmail.com (k simon)
Date: Sun, 30 Nov 2014 15:17:31 +0800
Subject: [squid-users] long standing bug about aufs on freebsd 10
Message-ID: <547AC48B.8040800@gmail.com>

Hi, Lists,
   AUFS can not run stable one day and report:

2014/11/30 07:10:15 kid1| WARNING: swapfile header inconsistent with 
available data
2014/11/30 07:10:15 kid1| Could not parse headers from on disk object
2014/11/30 07:10:15 kid1| BUG 3279: HTTP reply without Date:
2014/11/30 07:10:15 kid1| StoreEntry->key: 553ABDC02632452B7204639E5DDA66D8
2014/11/30 07:10:15 kid1| StoreEntry->next: 0
2014/11/30 07:10:15 kid1| StoreEntry->mem_obj: 0x82178ed00
2014/11/30 07:10:15 kid1| StoreEntry->timestamp: -1
2014/11/30 07:10:15 kid1| StoreEntry->lastref: 1417302615
2014/11/30 07:10:15 kid1| StoreEntry->expires: -1
2014/11/30 07:10:15 kid1| StoreEntry->lastmod: -1
2014/11/30 07:10:15 kid1| StoreEntry->swap_file_sz: 0
2014/11/30 07:10:15 kid1| StoreEntry->refcount: 1
2014/11/30 07:10:15 kid1| StoreEntry->flags: 
CACHABLE,PRIVATE,FWD_HDR_WAIT,VALIDATED
2014/11/30 07:10:15 kid1| StoreEntry->swap_dirn: -1
2014/11/30 07:10:15 kid1| StoreEntry->swap_filen: -1
2014/11/30 07:10:15 kid1| StoreEntry->lock_count: 2
2014/11/30 07:10:15 kid1| StoreEntry->mem_status: 0
2014/11/30 07:10:15 kid1| StoreEntry->ping_status: 2
2014/11/30 07:10:15 kid1| StoreEntry->store_status: 1
2014/11/30 07:10:15 kid1| StoreEntry->swap_status: 0
2014/11/30 07:10:15 kid1| assertion failed: store.cc:1876: "isEmpty()"

   How can I workaround it ?




Simon


From squid3 at treenet.co.nz  Sun Nov 30 06:20:31 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Nov 2014 19:20:31 +1300
Subject: [squid-users] Problem with access.log and when using SMP
In-Reply-To: <5475C6BA.5000008@mia.gov.ge>
References: <5475C6BA.5000008@mia.gov.ge>
Message-ID: <547AB72F.9030303@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 27/11/2014 1:25 a.m., Giorgi Tepnadze wrote:
> Hello,
> 
> I have own squid log parsing software which worked flawlessly
> before we moved to squid 3.3.8 (old was 2.7). Log parser stopped
> working. This is the problem I found.
> 
> Problem lines: 1416971036.802 184393 10.xx.xx.xx TCP_MISS/200 10963
> CONNECT pixel.facebook.com:1416971036.390      0 10.yy.yy.yy
> TCP_DENIED/407 3751 CONNECT clients4.google.com:443 - HIER_NONE/-
> text/html 1416971036.645   7164 10.zz.zz.zz TCP_MISS/200 55438 GET 
> http://s40.radikal.ru/i089/1107/9b/2fbf1779ead9.jpg
> user at domain.com HIER_DIRECT/81.176.238.141 image/jpeg 443
> user2 at domainc.com HIER_DIRECT/31.13.93.3 -
> 
> As it should be: 1416971036.390      0 10.yy.yy.yy TCP_DENIED/407
> 3751 CONNECT clients4.google.com:443 - HIER_NONE/- text/html 
> 1416971036.802 184393 10.xx.xx.xx TCP_MISS/200 10963 CONNECT 
> pixel.facebook.com:443 user2 at domainc.com HIER_DIRECT/31.13.93.3 - 
> 1416971036.645   7164 10.zz.zz.zz TCP_MISS/200 55438 GET 
> http://s40.radikal.ru/i089/1107/9b/2fbf1779ead9.jpg
> user at domain.com HIER_DIRECT/81.176.238.141 image/jpeg
> 
> I think that's because of SMP as I have 4 squid kids running and
> they all write to same access.log file and there is concurrency
> problem. Sometimes one is writing log line when previous log line
> is not finished yet or something like this. Such errors are rare
> but enough to stop my parser with  error.
> 
> Is it a bug? Is there any workaround or solution?

What logging module are you using to write?

Squid log lines are generated into a buffer and emitted in what should
be close to atomic (but not exactly locking) per-line write() actions.

You could write a custom daemon to receive the log lines and send them
directly through your parser system as units. This has the added
benefit of being a lot closer to real-time log processing.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUercvAAoJELJo5wb/XPRjZVEH/RtrMBTH5XfPi6MmIVf2/Yzb
DZg+ObG64J/SmWMc99YvTL8Bg5SrB4GNKZEt5JM3jKYOkjS1lg7G90JUfwmHi8xI
xnX/+iayr19QVmdRb5LNZdpgn1spCoodq9RXQKVLKQnxvYVrV5SCnPx51xeevG2g
X7gBa+l/nltEQy1cc2Dc5UScvMHdXh8tShG8CINX/oyXr6XM0qa8jCe3Tczq966K
GsEnicp017lwSDVSiXPU866Gf/3xexXGUzPIxlWe3OIbEwp/LP0U63jaF2lUz6cu
xY10bAUbOTGE76UmhHRSmePlhZcWwsHHwbXy7Z5OQCe2UX/MRIZYzN7GkqSVgf4=
=iKx3
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sun Nov 30 07:40:28 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Nov 2014 20:40:28 +1300
Subject: [squid-users] RFC2616 headers in bumped requests
In-Reply-To: <546DC79F.5040300@opendium.com>
References: <5458A7D6.3090502@opendium.com> <5458DBD0.2050306@treenet.co.nz>
 <5469CD2B.8080802@opendium.com> <546A7111.70605@treenet.co.nz>
 <546DC79F.5040300@opendium.com>
Message-ID: <547AC9EC.6010800@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 20/11/2014 11:51 p.m., Steve Hill wrote:
> On 17/11/14 22:05, Amos Jeffries wrote:
> 
>> Would you mind running an experiment for me?
>> 
>> To see what happens if Squid delivers either of these Via
>> headers instead of its current output:
>> 
>> Via: HTTPS/1.1 iceni2.opendium.net (squid/3.4.9)
> 
> The HTTPS/1.1 one appears to work correctly.
> 
>> Via: TLS/1.2 iceni2.opendium.net (squid/3.4.9)
> 
> The web server produces the same broken redirect as before when I
> send TLS/1.2.
> 
>> Setting it with request_header_access/replace should do.
> 
> I've tested this in Squid with request_header_access/replace and 
> confirmed with openssl's s_client directly.
> 


Just to followup, there will not be a permanent change made to Squid
because:

1) "HTTPS" is a common name for an entire stack of protocols. Since it
is a whole stack of protocols (HTTP-on-TLS-on-TCP-on-IP...) it is not
being registered by IANA as a label for an individual protocol.

2) the Via headers indicates the single top-level protocol. Which is
actually HTTP for both port 80 and 443 traffic, even though port 443
is HTTP being transmitted over TLS connections. Thus Squid Via header
is correct.


The ATS server has at least three bugs;

A) it is emitting some unknown "http/1.1" protocol. The "HTTP"
protocol label is case-sensitive as defined in RFC 7230.

B) it is attempting to determine security from the Via header. As the
server operators themselves should know (due to the "http/1.1" usage
by their own server) the presence of any top-level HTTP is no
indicator for or against security of the underlying network connection.

C) it is redirecting to the same https:// URI which is being delivered
to it. The server itself is uniquely in a position to be aware of
these types of loop and so expected not to cause them. (Squid opening
a port 443 connection is dead giveaway they are getting https:// even
if it is proxied).


PS. that said, the workaround should be enough to get things going
again until the ATS people fix their bugs.

Cheers
Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUesnsAAoJELJo5wb/XPRj5RYIAIzYJF0nbjG24jR3i73rVQSl
BUcUdwsfwo/KFDSDmqHBlgiN5qcxAt2pZcKzmyGevqmY+nwUQSBUwCvigWXh5tT1
vhrjAB4iuJfFefQqHac4ZtflVID5ft4hSLcwfxdlRwcld5XvNubU5L4bBLNkOuja
1JAezYn+EJtonhQsC7ZxecWPiDCMo/sUgtDjWjoYu3Awtn/A0mNQpzmPfsUyQyjI
c/2hwTZFPcPruwleZ6kB4/XXcfSRCKVpdI/U/nuPeoEXraO+n6ZhU6Y+6LfaHO26
osmgBf3DM2NirHSI67Ewgk9++JeFAd0v0MASFdzlH97da5SxIGy8yva1bl38Ii0=
=6EbN
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sat Nov 29 04:13:57 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Nov 2014 17:13:57 +1300
Subject: [squid-users] Cannot display page correctly with SSL-Bump
In-Reply-To: <CA+zetAT=B6otujNQGWCrs7dacc2zOY4G-rk5GutiryEJ0MQbxQ@mail.gmail.com>
References: <CA+zetAT=B6otujNQGWCrs7dacc2zOY4G-rk5GutiryEJ0MQbxQ@mail.gmail.com>
Message-ID: <54794805.1040107@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 27/11/2014 5:38 p.m., Rino M Nur wrote:
> Hi,
> 
> Im trying to get ssl bump work correctly but when i get a site with
> https then browser display the page with no CSS or javascript. log
> : 1417149172.053    175 192.168.10.10 TAG_NONE/200 0 CONNECT
> i.ytimg.com:443 - HIER_DIRECT/74.125.130.102 - 1417149172.145
> 194 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443 -
> HIER_DIRECT/74.125.130.102 - 1417149172.181    156 192.168.10.10
> TAG_NONE/200 0 CONNECT i.ytimg.com:443 - HIER_DIRECT/74.125.130.102
> - 1417149172.220    169 192.168.10.10 TAG_NONE/200 0 CONNECT
> i.ytimg.com:443 - HIER_DIRECT/74.125.130.102 - 1417149172.299
> 348 192.168.10.10 TAG_NONE/200 0 CONNECT i.ytimg.com:443 -
> HIER_DIRECT/74.125.130.102 -

Hmm, I think that is what gets logged when the CONNECT gets bumped.
There should be other log lines for the decrypted requests from inside
the tunnel.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUeUgFAAoJELJo5wb/XPRjY3UIAMKMnbuvYhWsAivco1zt7uQ8
z0VMKdxPoYWYSoL1k8nPigUQWg60Chjoql0RWLT8ZeIRVvtPa0lPqMJBneEQJetj
s8QTnbqEOqr1Su/aUFPUiuWJfhjlg2AqD0PiJi4Jvl+6n8cKxabTpR1eD8uqoPTU
q5aPsNxYbMWRUBd26kADfmdu9bvhR3TZEOtb6RZxyDr8xUTCT9rOVDIbZnMKGpiW
EqjwJvRjFSgOAZJjpvSccvRV0tYzAY6/Ru2qg/Y/RCgHDDZeJQZv1OF+ncBhxPBl
RQhUUnYjG7MviVR1Ek0hB4n0XIac81gRZVYd0QQXqYT2OgqiDSq4fns3G0Nv6PM=
=zDDS
-----END PGP SIGNATURE-----


From m.riede at babiel.com  Fri Nov 28 13:40:25 2014
From: m.riede at babiel.com (Mark Riede)
Date: Fri, 28 Nov 2014 13:40:25 +0000
Subject: [squid-users] 2.7.STABLE9 & Error with option deny_info from local
	requests
Message-ID: <166D4EE577088449BA7DD4367005D8642E1BF4AA@s015011.office.babiel.com>

Hello,

I have a strange behavior with Squid 2.7.STABLE9 and local requests which should be intercept by the option deny_info.

I am using Squid as a reverse proxy.
I have configured a list of subdomains (i.e. subdomain.domain.tld) in a file via the option dstdomain, which will be forwarded to the defined cache peer.
There is an additional list of domains (i.e. *.domain.tld) which match via wildcard to all other domains, which are not absolutely defined yet and will be forwarded to a custom error page via the option deny_info.

The problem is that requests forwarded to the ip of the server, i.e. 192.168.0.1, will be catched up by the option deny_info.
But, when the request is forwarded to the ip of the localhost (127.0.0.1), the option deny_info will not match.
Now the strange behaviour is that requests to the ip of the localhost but with the destination domain subdomain.domain.tld will be answered successfully.
I need a fix because clients get the custom error page for requests via http (NAT to 192.168.0.1) but not the same response via https (nginx to 127.0.0.1). 
I don?t know where or how I can fix this problem or do more debugging.


# Config
http_access allow localhost
acl foo dstdomain "/file"
acl foo_deny dstdom_regex "/ file _deny"
http_access allow foo
cache_peer 127.0.0.1 parent 8080 0 no-query originserver name=srv1 login=PASS
cache_peer_access srv1 allow foo
cache_peer_access srv1 deny all
deny_info ERR_FOO foo_deny
http_access deny foo_deny
http_access deny all


# Error via curl
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"> <html><head> <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> <title>ERROR: The requested URL could not be retrieved</title> <style type="text/css"><!--   %l  body :lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya, sans-serif; float: right; } :lang(he) { direction: rtl; }  --></style> </head><body id=ERR_CANNOT_FORWARD> <div id="titles"> <h1>ERROR</h1> <h2>The requested URL could not be retrieved</h2> </div> <hr>  <div id="content"> <p>The following error was encountered while trying to retrieve the URL: <a href="http://subdomain.domain.tld/">http://subdomain.domain.tld/</a></p>  <blockquote id="error"> <p><b>Unable to forward this request at this time.</b></p> </blockquote>  <p>This request could not be forwarded to the origin server or to any parent caches.</p>  <p>Some possible problems are:</p> <ul> <li id="network-down">An Internet connection needed to access this domains origin servers may be down.</li> <li id="no-peer">All configured parent caches may be currently unreachable.</li> <li id="permission-denied">The administrator may not allow this cache to make direct connections to origin servers.</li> </ul>  <p>Your cache administrator is <a href="mailto:service at babiel.com">service at babiel.com</a>.</p>  <br> </div>  <hr> <div id="footer"> <p>Generated Fri, 28 Nov 2014 13:29:22 GMT by squid (squid)</p> <!-- ERR_CANNOT_FORWARD -->

# Error from log
1417181439.852 RELEASE -1 FFFFFFFF B41394C6D2C0281301E5137947DE34E0  504 1417181439        -1        -1 text/html 1509/1509 GET http://subdomain.domain.tld/

Best regards,
Mark


From david at articatech.com  Sat Nov 29 11:52:08 2014
From: david at articatech.com (David Touzeau)
Date: Sat, 29 Nov 2014 12:52:08 +0100
Subject: [squid-users] squid 3.5x: Active Directory accounts with space
 issue
In-Reply-To: <5475AB0D.4030608@treenet.co.nz>
References: <5471BE7D.6030206@articatech.com> <5475AB0D.4030608@treenet.co.nz>
Message-ID: <5479B368.4070908@articatech.com>


Le 26/11/2014 11:27, Amos Jeffries a ?crit :
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 24/11/2014 12:01 a.m., David Touzeau wrote:
>> Hi
>>
>> We have connected 3.5.0.2-20141121-r13666 with Active Directory. It
>> seems where there are spaces in login account squid use only the
>> last argument.
>>
>> For example for an account "Jhon smith" squid use "smith" only For
>> example for an account "Dr Jhon smith" squid use "smith" only
>>
>> In 3.3.13 there is no such issue, a "Jhon smith" account is logged
>> as "Jhon smith" and sended as Jhon%20smith to helpers
> Any information about the auth Scheme being performed?
>   the helpers being used?
>   and what is being sent to/from the helpers in 3.5 different from the
> 3.3 version?
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUdasMAAoJELJo5wb/XPRjRPUH/2aVKrtNdmJzupzsN9JtcOK0
> 1e+NIxNSaDiyu9R03eJrwlAy7g9zFGEj+0dI1HgJz36Mf2i03ahbyinD4GwFDVPh
> a6iYyCPrhy2XDeL16qcSqsX0i2e8yXO/WRbFTJymKMOFhVDS05Bg6KuE1FroNjHG
> OkhpzN/T3O1fUW2k0XSRZEWFV1YnriwcCLdKXdsXEXEIIA3J9ZN0WQZ8I/oGXfWV
> S4xHKh4jnDFJCEO5lwYxT1CDe53CCHnPfV9Uf1Dhq6AkKnDZAR8U53Uyhji4V6ck
> UzwZEPMAtK73O3uXn0J2l2S9v0ga5ymHRhiWADG2jC/8dyAc0ICaWFjK7o6wMfE=
> =GaV2
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
Hi

I'm using this method

auth_param ntlm program /usr/bin/ntlm_auth --domain=TOUZEAU.BIZ 
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 25 startup=5 idle=1
auth_param ntlm keep_alive off
#Dynamic ACLs groups Enabled: [1]
external_acl_type ads_group ttl=3600 children-max=5 children-startup=1 
children-idle=1 %LOGIN /usr/share/artica-postfix/external_acl_squid_ldap.php
#Other settings
authenticate_ttl 1 hour
authenticate_cache_garbage_interval 10 seconds
authenticate_ip_ttl 60 seconds
# END NTLM Parameters --------------------------------
#Basic authentication for other browser that did not supports NTLM: 
(KerbAuthMethod =  )
auth_param basic program /usr/bin/ntlm_auth 
--helper-protocol=squid-2.5-basic
auth_param basic children 3 startup=1 idle=1
auth_param basic realm Basic Identification
auth_param basic credentialsttl 2 hours


On 3.3.13, everything works as expected.
On 3.5x LOGIN are truncated where there is space on account.

I have tested by removing external_acl_type ads_group, no change issue 
is still displayed.








From squid3 at treenet.co.nz  Sat Nov 29 04:18:32 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Nov 2014 17:18:32 +1300
Subject: [squid-users] Minor nit with cachemgr.cgi in 3.5.0.2
In-Reply-To: <pan.2014.11.26.14.02.54@googlemail.com>
References: <pan.2014.11.26.14.02.54@googlemail.com>
Message-ID: <54794918.10202@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Please report through bugzilla so we do not loose track of this.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUeUkXAAoJELJo5wb/XPRjUDcIAIUQIhNmd1OMCSax99wbCTBz
LmSYPLwzeu7qE+YWMJiUXwnPxHD07f8SMFwYdczJpeuVDMYGOrbC6UHdac+EA8sl
65E3jgqsf0LwjwXCER8DmDPe2f39ve8pICYVKoxe4JxX88oPNmNw5fh7TSI+ZEi6
6cVsSpa9bDEMrJUJxsDomGCV9L+TSOBJJPO9UwdBhe3wtFPPI0S2OknkmmRwXL5l
WWKhz8+yu/wnSZ/BTZfgLp3COQ08ZVBZxFArF89HTmEMUH+L9eZn9cB72BrNoKo0
LzB6pIJPQbyZDnqlZ+lj6HlfPA9F+530XvWbWNJKVzj91vMxaOxYV23sF/CBFQ8=
=4U3q
-----END PGP SIGNATURE-----


From james at ejbdigital.com.au  Sat Nov 29 23:36:34 2014
From: james at ejbdigital.com.au (James Harper)
Date: Sat, 29 Nov 2014 23:36:34 +0000
Subject: [squid-users] squid-3.5.0.2-20141031-r13657 crashes
Message-ID: <HKNPR04MB1933529F3384F902F2583EAE87F0@HKNPR04MB193.apcprd04.prod.outlook.com>

This has happened again a day or so after wiping the cache directory. Core dump this time:

#0  StoreEntry::checkCachable (this=this at entry=0x284c440) at store.cc:962
962                         getReply()->content_length > store_maxobjsize) ||
(gdb) bt
#0  StoreEntry::checkCachable (this=this at entry=0x284c440) at store.cc:962
#1  0x0000000000619edf in StoreEntry::memoryCachable (this=this at entry=0x284c440) at store.cc:1418
#2  0x00000000006255d2 in StoreController::keepForLocalMemoryCache (this=<optimized out>, e=...) at store_dir.cc:798
#3  0x0000000000625a75 in StoreController::handleIdleEntry (this=0x23a48d0, e=...) at store_dir.cc:891
#4  0x000000000061b091 in StoreEntry::unlock (this=0x284c440, context=context at entry=0x806224 "clientReplyContext::forgetHit") at store.cc:543
#5  0x0000000000535356 in clientReplyContext::forgetHit (this=this at entry=0x28eb5e8) at client_side_reply.cc:1586
#6  0x00000000005390be in clientReplyContext::identifyFoundObject (this=0x28eb5e8, newEntry=<optimized out>) at client_side_reply.cc:1675
#7  0x000000000053ed0d in ClientHttpRequest::httpStart (this=this at entry=0x28e9ed8) at client_side_request.cc:1517
#8  0x00000000005402b7 in ClientHttpRequest::processRequest (this=this at entry=0x28e9ed8) at client_side_request.cc:1503
#9  0x00000000005420d5 in ClientHttpRequest::doCallouts (this=0x28e9ed8) at client_side_request.cc:1818
#10 0x0000000000545bd7 in ClientRequestContext::clientAccessCheckDone (this=this at entry=0x28603d8, answer=...) at client_side_request.cc:821
#11 0x0000000000546801 in ClientRequestContext::clientAccessCheck2 (this=0x28603d8) at client_side_request.cc:718
#12 0x00000000005427bc in ClientHttpRequest::doCallouts (this=0x28e9ed8) at client_side_request.cc:1711
#13 0x0000000000545bd7 in ClientRequestContext::clientAccessCheckDone (this=this at entry=0x28603d8, answer=...) at client_side_request.cc:821
#14 0x00000000005466c5 in clientAccessCheckDoneWrapper (answer=..., data=0x28603d8) at client_side_request.cc:730
#15 0x00000000006c369b in ACLChecklist::checkCallback (this=0x28eda18, answer=...) at Checklist.cc:167
#16 0x00000000006c3ea4 in ACLChecklist::completeNonBlocking (this=<optimized out>) at Checklist.cc:52
#17 0x00000000006c43a3 in ACLChecklist::nonBlockingCheck (this=<optimized out>,
    callback_=callback_ at entry=0x5466a0 <clientAccessCheckDoneWrapper(allow_t, void*)>, callback_data_=callback_data_ at entry=0x28603d8) at Checklist.cc:255
#18 0x0000000000546171 in ClientRequestContext::clientAccessCheck (this=0x28603d8) at client_side_request.cc:698
#19 0x00000000005426ca in ClientHttpRequest::doCallouts (this=0x28e9ed8) at client_side_request.cc:1682
#20 0x0000000000544b90 in ClientRequestContext::hostHeaderIpVerify (this=0x28603d8, ia=0x2860cc0, dns=...) at client_side_request.cc:526
#21 0x00000000005ca5d4 in ipcacheCallback (i=i at entry=0x2860ca0, wait=wait at entry=330) at ipcache.cc:325
#22 0x00000000005cae74 in ipcacheHandleReply (data=<optimized out>, answers=<optimized out>, na=<optimized out>, error_message=<optimized out>) at ipcache.cc:475
#23 0x000000000055b4a1 in idnsCallback (q=0x28561e8, q at entry=0x2862ff8, error=error at entry=0x0) at dns_internal.cc:1097
#24 0x000000000055f78f in idnsGrokReply (buf=buf at entry=0xb0d100 <idnsRead(int, void*)::rbuf> "\262<\201\200", sz=sz at entry=156, from_ns=<optimized out>)
    at dns_internal.cc:1266
#25 0x00000000005601b5 in idnsRead (fd=7, data=<optimized out>) at dns_internal.cc:1353
#26 0x0000000000752223 in Comm::DoSelect (msec=<optimized out>) at ModEpoll.cc:277
#27 0x00000000006d3f7f in CommSelectEngine::checkEvents (this=<optimized out>, timeout=<optimized out>) at comm.cc:1835
#28 0x000000000056893a in EventLoop::checkEngine (this=this at entry=0x7fffee8124b0, engine=engine at entry=0x7fffee812440, primary=primary at entry=true)
    at EventLoop.cc:35
#29 0x0000000000568b27 in EventLoop::runOnce (this=this at entry=0x7fffee8124b0) at EventLoop.cc:114
#30 0x0000000000568ce8 in EventLoop::run (this=this at entry=0x7fffee8124b0) at EventLoop.cc:82
#31 0x00000000005d0753 in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1508
#32 0x00000000004e378c in SquidMainSafe (argv=<optimized out>, argc=<optimized out>) at main.cc:1240
#33 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233

James


From squid3 at treenet.co.nz  Sun Nov 30 06:13:53 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Nov 2014 19:13:53 +1300
Subject: [squid-users] Existing root certificate not working with SSL
 Bump (squid 3.3.10)
In-Reply-To: <1417023920196-4668527.post@n4.nabble.com>
References: <1416933493483-4668515.post@n4.nabble.com>
 <5475A124.4050208@treenet.co.nz> <1417021381827-4668526.post@n4.nabble.com>
 <1417023920196-4668527.post@n4.nabble.com>
Message-ID: <547AB5A1.3030702@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 27/11/2014 6:45 a.m., HaxNobody wrote:
> Alright, I figured out a possible cause. I downloaded the
> certificate that the browsers were complaining about, and used
> openssl verify to verify against the root certificate that I have.
> I got error 20, indicating that squid must not be using the correct
> root certificate to generate the client certificate on the fly, or
> that it is being generated incorrectly. The generated certificate
> shows all the correct properties of the root certificate that I am
> using, so my conclusion is that squid is incorrectly generating the
> client certificate.
> 
> Question: Under what circumstances might squid incorrectly generate
> a bump certificate?

In all circumstances involving client-first bumping, or a bug in Squid.

Other circumstances depend on your definitinon of "correct". Squid
3.3+ will mimic certificates *including errors* delivered by servers.


Also, Squid does not generate client certificates. It generates server
certificates. I assume that is what you are talking about.


> Another question: Why might it be working when I use a different
> root certificate?

a) possibly the client trusts only one out of the two root certificates.

b) possibly the non-working certificate is not properly installed in
the client.

c) possibly the non-trusted root certificate is part of a chain which
the client is not able to locate all the pieces for (leading to 'a').

d) possibly the root certificate has key extensions or usage
restrictions prohibiting what Squid usage requires (leading to 'a').


You will need to get a content dump of the certificates emitted by
Squid and a working system to see what the difference(s) are.

Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUerWgAAoJELJo5wb/XPRjOnoH/ROsdsAnwe837rrCSgvmlb7N
y51KKl6axftQZs6HQKToYNZ4BkB1Hzgpn5mPxT9NlsbQm8yRGA42mhjHOWvJX4R7
WEsW6OlF+HNd/FVhahkJHSGmS/isSKRCK0B5fXuq0KX3dnTrZz6/53oNYXMXeyl+
j89d9JHSKUPVmvtEUfLEPYW5VDmaZfcmFL8WkUQ7Hi/ZOubnbL5gQPr67DF0r6qE
maZucqIHs5j0xP3ItLbcOxZQ5iCjmTmyNrxh0gyjZ3/OOTp1qpyRZQ6UPGqtnswt
UIGPgvayerMDNN+rAp82qZyLm70A4mmcHVY42d6haG4hGWb/WweEEhCZm6wS/TI=
=+5Ty
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Sun Nov 30 08:08:10 2014
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Nov 2014 21:08:10 +1300
Subject: [squid-users] squid 3.5x: Active Directory accounts with space
 issue
In-Reply-To: <5479B368.4070908@articatech.com>
References: <5471BE7D.6030206@articatech.com> <5475AB0D.4030608@treenet.co.nz>
 <5479B368.4070908@articatech.com>
Message-ID: <547AD06A.5000301@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 30/11/2014 12:52 a.m., David Touzeau wrote:
> 
> Le 26/11/2014 11:27, Amos Jeffries a ?crit : On 24/11/2014 12:01
> a.m., David Touzeau wrote:
>>>> Hi
>>>> 
>>>> We have connected 3.5.0.2-20141121-r13666 with Active
>>>> Directory. It seems where there are spaces in login account
>>>> squid use only the last argument.
>>>> 
>>>> For example for an account "Jhon smith" squid use "smith"
>>>> only For example for an account "Dr Jhon smith" squid use
>>>> "smith" only
>>>> 
>>>> In 3.3.13 there is no such issue, a "Jhon smith" account is
>>>> logged as "Jhon smith" and sended as Jhon%20smith to helpers
> Any information about the auth Scheme being performed? the helpers
> being used? and what is being sent to/from the helpers in 3.5
> different from the 3.3 version?
> 
> Amos
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> Hi
> 
> I'm using this method
> 
> auth_param ntlm program /usr/bin/ntlm_auth --domain=TOUZEAU.BIZ 
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 25
> startup=5 idle=1 auth_param ntlm keep_alive off #Dynamic ACLs
> groups Enabled: [1] external_acl_type ads_group ttl=3600
> children-max=5 children-startup=1 children-idle=1 %LOGIN 
> /usr/share/artica-postfix/external_acl_squid_ldap.php #Other
> settings authenticate_ttl 1 hour 
> authenticate_cache_garbage_interval 10 seconds authenticate_ip_ttl
> 60 seconds # END NTLM Parameters -------------------------------- 
> #Basic authentication for other browser that did not supports
> NTLM: (KerbAuthMethod =  ) auth_param basic program
> /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic auth_param
> basic children 3 startup=1 idle=1 auth_param basic realm Basic
> Identification auth_param basic credentialsttl 2 hours
> 
> 
> On 3.3.13, everything works as expected. On 3.5x LOGIN are
> truncated where there is space on account.

By "LOGIN" are you meaning the log entries for user name labels?
 the %LOGIN format code delivered to the external ACL helper?
 the user=X labels delivered by the NTLM helper to Squid?
 or the generic "login" concept?

The 'old' helper protocol was whitespace delimited set of fields with
fixed meaning for each column/field. If the helper is delivering an
un-encoded SP character inside an old-style response to Squid it will
be parsed as two values.
 The 3.4+ helpers are parsing that protocol and upgrading it to the
new kv-pair protocol automatically. Garbage fields are discarded from
the input.

It looks like the 2-column AF (NTLM) response being confused for a
3-column AF (Kerberos) response. Since the only difference between the
two helpers outputs is the presence of a "token" column before the
username field.

You can workaround it with a script to convert the protocol explicitly
before delivering to Squid.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJUetBqAAoJELJo5wb/XPRja6YH/1PpeTPb+BcfvWTKnsxDcy1O
deM+KEBK3nPz2IjTj6In73cH/UIkoFZaKIOViSR8MyjFtg517mz54tQcWWMkLIUQ
CId00veZcSlbpI1oJlg/eds6o0UXj+TZ4KpFGzLCnxLrAzwW93bneRuj6VeGUlpY
wlWwutZKFFlY1mHfIzlOkCE0f3AJZ/bK6XKP0x6UOfCzXjX4V/MW8KyhwCJXE0rz
Vr04GoJbMxSKR5JhMVZJV2uPteW9qFvX2efEkZA4coyV/E78YEp800et07eE+hRO
3O5Wswq7Lh+aZ0cMrjbdV/l4jcC/1UQnd9lM9rkiqoA3aXn63i5aUjxpbJJ9PWk=
=uEUQ
-----END PGP SIGNATURE-----


From navari.lorenzo at gmail.com  Fri Nov 28 09:19:43 2014
From: navari.lorenzo at gmail.com (navari.lorenzo at gmail.com)
Date: Fri, 28 Nov 2014 01:19:43 -0800 (PST)
Subject: [squid-users] WARNING: there are more than 100 regular
	expressions
In-Reply-To: <547724A9.9000407@urlfilterdb.com>
References: <1417078626013-4668529.post@n4.nabble.com>
 <D5dthyFPCXB@helmut.hullen.de> <1417082346062-4668531.post@n4.nabble.com>
 <D5dtqA5eCXB@helmut.hullen.de> <54771314.5040702@urlfilterdb.com>
 <1417089675959-4668535.post@n4.nabble.com> <547724A9.9000407@urlfilterdb.com>
Message-ID: <1417166383583-4668541.post@n4.nabble.com>

 this is the error: WARNING: there are more than 100 regular expressions.
Consider using less REs or use rules without expressions like 'dstdomain'. 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/WARNING-there-are-more-than-100-regular-expressions-tp4668529p4668541.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stephen.baynes at smoothwall.net  Fri Nov 28 10:28:43 2014
From: stephen.baynes at smoothwall.net (Stephen Baynes)
Date: Fri, 28 Nov 2014 10:28:43 +0000
Subject: [squid-users] Squid WCCP with multiple workers
Message-ID: <CABrJVozUY6m0yF0E=yBX7u51Te5qNqPnOKXwdGC7oZbYfD=+7g@mail.gmail.com>

Is WCCP supposed to work with Squid multiple workers?

It works with 1 worker. If we change the number of workers from 1 to 2
we see it fail. The router no longer is aware of Squid and does not
reroute the data to the Squid box.

This can be seen on the router with "show ip wccp" . The "Number of
Service Group Clients" and "Number of Service Group Routers"  stops
being 1 and become 0.

The wccp lines in the Squid config are:

wccp2_router 10.0.5.201
wccp2_forwarding_method 1
wccp2_return_method 1
wccp2_assignment_method 1
wccp2_service standard 0 password=PPPPP
wccp2_service dynamic 70 password=PPPPP
wccp2_weight 1
wccp2_service_info 70 protocol=tcp flags=src_ip_hash priority=240 ports=443

Squid version 3.4.7


With 2 workers we see the following in the cache log:
2014/11/28 10:20:25 kid3| Accepting WCCPv2 messages on port 2048, FD 14.
2014/11/28 10:20:25 kid3| Initialising all WCCPv2 lists

With 2 worker we see the following in the cache log:
2014/11/28 09:44:09 kid1| Accepting WCCPv2 messages on port 2048, FD 16.
2014/11/28 09:44:09 kid1| Initialising all WCCPv2 lists


Any thoughts?

Thanks
-- 
Stephen Baynes CEng MBCS CITP


From rgelfand2 at gmail.com  Sun Nov 30 01:39:54 2014
From: rgelfand2 at gmail.com (Roman Gelfand)
Date: Sat, 29 Nov 2014 20:39:54 -0500
Subject: [squid-users] Multiple SSL Domains on Reverse Proxy
Message-ID: <CAJbW+rn6-7XnZnSXWevSoTPmCZ4VJF89SMK6SEiXz0gv-0QO1Q@mail.gmail.com>

Is it possible to listen on port 443 for requests for multiple domains
ie... www.xyz.com, www.mno.com, etc...?

If yes, could you point me to sample config.

Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20141129/112b4d5b/attachment.htm>

From eliezer at ngtech.co.il  Thu Nov 27 21:45:43 2014
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 27 Nov 2014 23:45:43 +0200
Subject: [squid-users] Centralized Squid - design and implementation
In-Reply-To: <546C9D36.8070205@treenet.co.nz>
References: <CANEbR8=4=fHvdP1zwAVrmtJ9Abk310pv-wjAQ5vBcPHmsPQF-Q@mail.gmail.com>
 <546957CB.40608@urlfilterdb.com>
 <CANEbR8nVP6k2eOB5mdHiNgqeiO0i4JaenSq5Sx6o8=3Mytz9tg@mail.gmail.com>
 <CAHsHsytaqvDcUXL_H3ac7YT9pM=K_4WqWePfXvWZNy0PEs8Fcg@mail.gmail.com>
 <546A6A60.3080303@treenet.co.nz>
 <CAHsHsyv6=OynWA=+xb1o=qEHTrZSB=XP+g6NDsWWuR8AV7bW6g@mail.gmail.com>
 <546AE7BE.50700@trimble.com>
 <CAHsHsyvMm_ojaN_SUrmCy3uwypiv8mVYBjZrBj0jtw3djeio1A@mail.gmail.com>
 <1416314377.4763.35.camel@desktop.bpk2.com> <546BAFAF.2070502@trimble.com>
 <CA+Y8hcOUzQpHNbTVNF2miqCJ4fd43us0Cra3t0_1AAfdJjxb_g@mail.gmail.com>
 <CAARxGtgjC2TPMw5+U7LK=4MydPnnaXn9mM26TBWca7vs-+xX_A@mail.gmail.com>
 <546C9D36.8070205@treenet.co.nz>
Message-ID: <54779B87.2030101@ngtech.co.il>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

A simple post here about pac files is welcomed as far as my
understanding goes.
I noticed that the last time that the site http://findproxyforurl.com/
was updated was a little bit more then a year ago.

Eliezer

On 11/19/2014 03:37 PM, Amos Jeffries wrote:
> Ouch. Thank you for that. I've had questions but not had access to 
> systems to find out.
> 
> This is stuff that should be published on
> http://findproxyforurl.com/. If you can find the author of that
> site (Peter Hayes?) please let them know these.
> 
> Amos

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJUd5uHAAoJENxnfXtQ8ZQUvGcH/0/tGzBVhounFtNJnyg+WUs7
W/g5T/iLrlOV3UzXwMXSTMuh0G+fsp9qCVfOguJnikdjhT8UD2sje2X4xJZScqTE
y9RzjQjPDfObR68wwX8Uw4R98yBAqXvk5E7CUPaQR1S2xmvnmAgLdhzyeiK6dVII
XNNphE47rrqZyna3mUVl8TsRloXRxjmd2zFLrWjXaBMUC+9mcorl1bOKI9w/KR53
HK3XNrqYlqn1fUKa2XgKJPLbUuDOXEvylmnLvjnSC3W4lsa6/TpmUFdhqv0OODNL
QAir8QKlWsN+4vnh3KI+H3x+PxDy35hKnn1WrrvxCjjGIeNC8HNKXgN+UtC9djo=
=4vfo
-----END PGP SIGNATURE-----


