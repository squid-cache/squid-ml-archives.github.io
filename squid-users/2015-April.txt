From squid3 at treenet.co.nz  Wed Apr  1 01:07:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Apr 2015 14:07:39 +1300
Subject: [squid-users] squid with slow client
In-Reply-To: <CAEhCwUyrQZZKzgW0HJENsppybFn4bSy28VdgnuE-YJUNrXiAbw@mail.gmail.com>
References: <CAEhCwUyrQZZKzgW0HJENsppybFn4bSy28VdgnuE-YJUNrXiAbw@mail.gmail.com>
Message-ID: <551B44DB.5040006@treenet.co.nz>

On 1/04/2015 11:18 a.m., Hector Chan wrote:
> Hi all,
> 
> How does squid behave when it is downloading a 5+GB file with a slow
> client?  I see my client (curl) exited with error code 18 (
> CURLE_PARTIAL_FILE) when downloading a 5+GB file from squid.  It was a
> cache miss, so the file was actually being fetched from the origin server.
> When it is cache hit, I don't see the curl error.
> 
> What I observed was that squid was downloading from the origin server at
> about 750 KB/s, and the client was downloading from squid at about 10 to 50
> KB/s.  The client was geographically far from squid.

Depends on several factors:

* whether the Content-Length, Transfer-Encoding, or neither is presented
by the server in headers, and

* what the cacheable object size limits are for Squid, and

* how far down into the object the transaction has reached, and

* whether collapsed forwarding is in effect, and

* whether ICAP is being used on it, and

* whether range processing is in effect


Squid has a 64KB server read buffer plus a client write buffer. These
two are limited by the readahead_gap directive if it is smaller. In
general that readahead_gap amount is what Squid has buffered for the
client and will not read from server any more than has been sent to the
client (TCP buffering and I/O latency can make things get a bit higher -
but for Squid purposes that bit is already "sent").

If the size is known from Content-Length (or a Range offset) Squid can
select the appropriate place to store the data from server (as well as
delivering to the client). It could be cache_mem (RAM), or cache_dir
(disk), or transient / not cached (RAM).

If the content-length header is absent (none or Transfer-Encoding used)
all objects start off assuming they can be cached in memory, which is
fast. Once it gets past maximum_object_size_in_memory Squid pushes it
out to disk (which is quite a bit slower), and if it gets so large it
cant even store there it should get updated to transient memory (fast
again). In transient memory the bits sent to the client(s) are discarded
and only the readahead_gap window is retained in RAM.

If Range processing is in effect the range_offset_limit and
quick_abort_* directives determine whether Squid fetches data from the
server before/after the client requested range. This will be fetched at
full speed of the connection between Squid and server.

If collapsed forwarding is in effect the client which initiated the
first fetch is in control of how much is read and how fast. Other
clients are just tagging along receiving duplicate copies of what Squid
has available.

If ICAP is being used there are ICAP I/O buffers of 64KB also affecting
delivery and readahead_gap covers what has not yet been sent to client.


> 
> I am using the ufs disk cache:
> 
> cache_replacement_policy lru
> minimum_object_size 1 bytes
> cache_dir ufs /data/squid/cache 130000 16 256 max-size=26843545600

I expect you would be better served with using the min-size= parameter
on the cache_dir line instead of setting the global Squid minimum object
size. HTTP/1.1 commonly has a fair amount of 0-byte messages (30x for
example) that are cacheable and might be stored in cache_mem quite
easily for good performance.

> 
> Here is the error I found in cache.log:
> 
> 2015/03/18 16:53:07.263| client_side_reply.cc(1185) replyStatus:
> clientReplyStatus: transfer is DONE
> 2015/03/18 16:53:07.263| client_side_reply.cc(1201) replyStatus:
> clientReplyStatus: client didn't get all it expected
> 2015/03/18 16:53:07.263| cbdata.cc(510) cbdataReferenceValid:
> cbdataReferenceValid: 0x1707a48
> 2015/03/18 16:53:07.263| client_side.cc(1917) stopSending: sending error
> (local=127.0.0.1:8443 remote=127.0.0.1:54359 FD 8 flags=1):
> STREAM_UNPLANNED_COMPLETE; old receiving erro
> r: none

I read this as something happened with the server communication that
aborted the delivery. eg the server disconnecting with a TCP RST/abort
instead of a normal connection closure.
Although the trace above only indicates that the client was not finished
receiving when the dosconnect happened, no details on what caused it to
happen.

Amos



From windflower1201 at gmail.com  Wed Apr  1 07:06:35 2015
From: windflower1201 at gmail.com (Yu-Hsuan Liao)
Date: Wed, 1 Apr 2015 15:06:35 +0800
Subject: [squid-users] ssl_bump problem with tw.bid.yahoo.com in transparent
	proxy
Message-ID: <CAGob8wBxOY5xxRnfzZc9gBqGFOzBLi8=pCCckQAz7XOR6cKs_g@mail.gmail.com>

Hello Everyone,

I got  'ssl_error_bad_cert_domain' message from browser when I was trying
to bump tw.bid.yahoo.com in transparent mode

I found that the certificate is signed to tw.otplogin.reg.yahoo.com, which
should be signed to tw.bid.yahoo.com

but for now I can't bypass using the following configure:

acl yahoo_url tw.otplogin.reg.yahoo.com tw.bid.yahoo.com
ssl_bump none yahoo_url

yet everything is OK when I use forward proxy, the certificate is correct
signed to tw.bid.yahoo.com

any ideas?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150401/60e99d8c/attachment.htm>

From yvoinov at gmail.com  Wed Apr  1 09:34:45 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 01 Apr 2015 15:34:45 +0600
Subject: [squid-users] ssl_bump problem with tw.bid.yahoo.com in
 transparent proxy
In-Reply-To: <CAGob8wBxOY5xxRnfzZc9gBqGFOzBLi8=pCCckQAz7XOR6cKs_g@mail.gmail.com>
References: <CAGob8wBxOY5xxRnfzZc9gBqGFOzBLi8=pCCckQAz7XOR6cKs_g@mail.gmail.com>
Message-ID: <551BBBB5.6010603@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
What version of Squid you are using?

01.04.15 13:06, Yu-Hsuan Liao ?????:
> Hello Everyone,
>
> I got  'ssl_error_bad_cert_domain' message from browser when I was trying
> to bump tw.bid.yahoo.com in transparent mode
>
> I found that the certificate is signed to tw.otplogin.reg.yahoo.com, which
> should be signed to tw.bid.yahoo.com
>
> but for now I can't bypass using the following configure:
>
> acl yahoo_url tw.otplogin.reg.yahoo.com tw.bid.yahoo.com
> ssl_bump none yahoo_url
>
> yet everything is OK when I use forward proxy, the certificate is correct
> signed to tw.bid.yahoo.com
>
> any ideas?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVG7u1AAoJENNXIZxhPexGiZwH/19TdE+jGhb29JPXqvf1cVqv
HAjmuq7nj9dQt/SmW2CM+rPeS6pgHuJIH2/rVsxU/ydbDhuomNBmOuZyhguaUBM0
xke1UBjHFbPsTHczfmlaW3/q+V1wg1BJ0Le8lNnJ4dZMxH5rK/O6L0zb6HwS7SMJ
Nn15VpqGWY6cESWMvV3ZYrdQ2dgiQRO9CEQkpXSAy5xV4C+5B4L10FfsN1JeMPZF
NZ/trRZFpZha2cQk65zYE4oBuiT137I4EKv+ldLu3uWhkGS8oqKSiPxjSmckzjhw
jFUONqSKGOxbT4HSBQSjZgmEvPLg/HKlVR99eH+Vyc/kOfGh7rt63bQ6AUYM3Jc=
=+MVl
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150401/0e2a30e1/attachment.htm>

From vrogoziansky.squid at gmail.com  Wed Apr  1 14:10:39 2015
From: vrogoziansky.squid at gmail.com (Vadim Rogoziansky)
Date: Wed, 01 Apr 2015 17:10:39 +0300
Subject: [squid-users] ssl_bump problem with tw.bid.yahoo.com in
 transparent proxy
In-Reply-To: <551BBBB5.6010603@gmail.com>
References: <CAGob8wBxOY5xxRnfzZc9gBqGFOzBLi8=pCCckQAz7XOR6cKs_g@mail.gmail.com>
 <551BBBB5.6010603@gmail.com>
Message-ID: <551BFC5F.2030903@gmail.com>

Hello Yuri,

I have the same problem with transparent proxy (can't bypass bad web 
sites) and as I know squid guys did not fix SNI issue yet. Forward proxy 
works smoothly.
Tell me something if I was wrong)

My configuration is following:
/
acl step1 at_step SslBump1//
//ssl_bump stare step1 all//
//acl sslBumpDeniedDstDomain dstdomain .google.com//
//ssl_bump splice sslBumpDeniedDstDomain//
//ssl_bump bump all//
/
And sqiud version is
/Squid Cache: Version 3.5.3//
//Service Name: squid//
//configure options:  '--with-openssl' '--enable-linux-netfilter' 
'--disable-ipv6' '--enable-icap-client' '--enable-ssl-crtd' 
'--prefix=/opt/squid' '--enable-external-acl-helpers=none' 
'--enable-auth-negotiate=none' '--enable-follow-x-forwarded-for' 
'--disable-auth-ntlm' '--disable-arch-native' '--enable-wccpv2' 
'--enable-snmp' 
'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig' 
--enable-ltdl-convenience/

Regards

On 4/1/2015 12:34 PM, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> What version of Squid you are using?
>
> 01.04.15 13:06, Yu-Hsuan Liao ?????:
> > Hello Everyone,
> >
> > I got  'ssl_error_bad_cert_domain' message from browser when I was 
> trying
> > to bump tw.bid.yahoo.com in transparent mode
> >
> > I found that the certificate is signed to tw.otplogin.reg.yahoo.com, 
> which
> > should be signed to tw.bid.yahoo.com
> >
> > but for now I can't bypass using the following configure:
> >
> > acl yahoo_url tw.otplogin.reg.yahoo.com tw.bid.yahoo.com
> > ssl_bump none yahoo_url
> >
> > yet everything is OK when I use forward proxy, the certificate is 
> correct
> > signed to tw.bid.yahoo.com
> >
> > any ideas?
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVG7u1AAoJENNXIZxhPexGiZwH/19TdE+jGhb29JPXqvf1cVqv
> HAjmuq7nj9dQt/SmW2CM+rPeS6pgHuJIH2/rVsxU/ydbDhuomNBmOuZyhguaUBM0
> xke1UBjHFbPsTHczfmlaW3/q+V1wg1BJ0Le8lNnJ4dZMxH5rK/O6L0zb6HwS7SMJ
> Nn15VpqGWY6cESWMvV3ZYrdQ2dgiQRO9CEQkpXSAy5xV4C+5B4L10FfsN1JeMPZF
> NZ/trRZFpZha2cQk65zYE4oBuiT137I4EKv+ldLu3uWhkGS8oqKSiPxjSmckzjhw
> jFUONqSKGOxbT4HSBQSjZgmEvPLg/HKlVR99eH+Vyc/kOfGh7rt63bQ6AUYM3Jc=
> =+MVl
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150401/d78d3766/attachment.htm>

From yvoinov at gmail.com  Wed Apr  1 15:12:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 01 Apr 2015 21:12:51 +0600
Subject: [squid-users] ssl_bump problem with tw.bid.yahoo.com in
 transparent proxy
In-Reply-To: <551BFC5F.2030903@gmail.com>
References: <CAGob8wBxOY5xxRnfzZc9gBqGFOzBLi8=pCCckQAz7XOR6cKs_g@mail.gmail.com>
 <551BBBB5.6010603@gmail.com> <551BFC5F.2030903@gmail.com>
Message-ID: <551C0AF3.7010802@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


01.04.15 20:10, Vadim Rogoziansky ?????:
> Hello Yuri,
>
> I have the same problem with transparent proxy (can't bypass bad web
sites) and as I know squid guys did not fix SNI issue yet. Forward proxy
works smoothly.

This is the reason that I still use 3.4.12. Bug 4188 still not fixed.

> Tell me something if I was wrong)
>
> My configuration is following:
> /
> acl step1 at_step SslBump1//
> //ssl_bump stare step1 all//
> //acl sslBumpDeniedDstDomain dstdomain .google.com//
> //ssl_bump splice sslBumpDeniedDstDomain//
> //ssl_bump bump all//
> /
> And sqiud version is
> /Squid Cache: Version 3.5.3//
> //Service Name: squid//
> //configure options:  '--with-openssl' '--enable-linux-netfilter'
'--disable-ipv6' '--enable-icap-client' '--enable-ssl-crtd'
'--prefix=/opt/squid' '--enable-external-acl-helpers=none'
'--enable-auth-negotiate=none' '--enable-follow-x-forwarded-for'
'--disable-auth-ntlm' '--disable-arch-native' '--enable-wccpv2'
'--enable-snmp'
'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
--enable-ltdl-convenience/

Looks like all ok.

>
> Regards
>
> On 4/1/2015 12:34 PM, Yuri Voinov wrote:
>>
> What version of Squid you are using?
>
> 01.04.15 13:06, Yu-Hsuan Liao ?????:
> > Hello Everyone,
>
> > I got  'ssl_error_bad_cert_domain' message from browser when I was
trying
> > to bump tw.bid.yahoo.com in transparent mode
>
> > I found that the certificate is signed to tw.otplogin.reg.yahoo.com,
which
> > should be signed to tw.bid.yahoo.com
>
> > but for now I can't bypass using the following configure:
>
> > acl yahoo_url tw.otplogin.reg.yahoo.com tw.bid.yahoo.com
> > ssl_bump none yahoo_url
>
> > yet everything is OK when I use forward proxy, the certificate is
correct
> > signed to tw.bid.yahoo.com
>
> > any ideas?
>
>
>
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVHArzAAoJENNXIZxhPexGuBsH/RYdXW7iKAbLz55Hfi/O7pJJ
ouPIZ5Gf+ApP/Aopt7/W433Uf6vudDI+xRsLfbmlPa8rdp18+wczCCbTRr7PP3uv
ULErMqaDdm5TEUTFXvR1i9XJt4I0zAcp9npRGGa4Xi9dVTQB5n7xCnL+freKT+KB
mE7VVOSBq+yq8E2+7khNRS68B5bgvuhMWdh/2pbWNvT83zwSt692R/VPq7H8rkZY
MDP8j19LaBeuvI9HIB8saPtQA0/0ptgvrwHNzTGHTPhYJJhaZsWWW35J+yj7U7Jh
hy84Pm+/xzEdEaP0j2qReBU8D38XWYuYc8BOmRTOHI1gdf8Yep0sS28W8+7xiAQ=
=S3+z
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150401/ad2e08fe/attachment.htm>

From vdoctor at neuf.fr  Wed Apr  1 16:45:14 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 1 Apr 2015 09:45:14 -0700 (PDT)
Subject: [squid-users] assertion failed: Read.cc:205: "params.data == data"
Message-ID: <1427906714415-4670624.post@n4.nabble.com>

Hi All,

Strange problem during surf, squid 3.5.3, 64bits, Debian 7.8:
*2015/04/01 19:19:06 kid3| assertion failed: Read.cc:205: "params.data ==
data"*

caches:
workers 3
cache_dir rock /var/spool/squid3r1 166400 min-size=0 max-size=65536
swap-timeout=500 max-swap-rate=200/sec
if ${process_number} = 1
cache_dir diskd /var/spool/squid3w1 453282 16 256 min-size=65536
max-size=1250301952
endif
if ${process_number} = 2
cache_dir diskd /var/spool/squid3w2 453282 16 256 min-size=65536
max-size=1250301952
endif
if ${process_number} = 3
cache_dir diskd /var/spool/squid3w3 453282 16 256 min-size=65536
max-size=1250301952
endif

/dev/shm exists, correct rights.
/var/run/squid exists, correct rights.

Anyone with experience about this error ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-Read-cc-205-params-data-data-tp4670624.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From windflower1201 at gmail.com  Thu Apr  2 00:52:45 2015
From: windflower1201 at gmail.com (Hsuan Yu)
Date: Thu, 2 Apr 2015 08:52:45 +0800
Subject: [squid-users] ssl_bump problem with tw.bid.yahoo.com in
	transparent proxy
Message-ID: <CAGob8wAkmYY64Ef8KHJNhJ4hRcCDZhFg9kAS4gZMrOvjj=Agyw@mail.gmail.com>

I've tried squid 4.0.0 / squid 3.5.3 / squid 3.3.5
these have the same problem.


>What version of Squid you are using?

>01.04.15 13:06, Yu-Hsuan Liao ?????:
>>* Hello Everyone,
*>>>>* I got  'ssl_error_bad_cert_domain' message from browser when I was trying
*>>* to bump tw.bid.yahoo.com <http://tw.bid.yahoo.com> in transparent mode
*>>>>* I found that the certificate is signed to
tw.otplogin.reg.yahoo.com <http://tw.otplogin.reg.yahoo.com>, which
*>>* should be signed to tw.bid.yahoo.com <http://tw.bid.yahoo.com>
*>>>>* but for now I can't bypass using the following configure:
*>>>>* acl yahoo_url tw.otplogin.reg.yahoo.com
<http://tw.otplogin.reg.yahoo.com> tw.bid.yahoo.com
<http://tw.bid.yahoo.com>
*>>* ssl_bump none yahoo_url
*>>>>* yet everything is OK when I use forward proxy, the certificate is correct
*>>* signed to tw.bid.yahoo.com <http://tw.bid.yahoo.com>
*>>>>* any ideas?*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150402/bb3880c1/attachment.htm>

From nathan at getoffmalawn.com  Thu Apr  2 05:45:33 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Thu, 2 Apr 2015 16:45:33 +1100
Subject: [squid-users] assertion failed: Read.cc:205: "params.data ==
	data"
In-Reply-To: <1427906714415-4670624.post@n4.nabble.com>
References: <1427906714415-4670624.post@n4.nabble.com>
Message-ID: <CAGUJm7Zt9n8NBg4EhVSZPWY66vAMy89y79DQkyEiiba_=QLj4Q@mail.gmail.com>

Hello,

Do you have a core dump? If so, if you could get a backtrace as
described here http://wiki.squid-cache.org/SquidFaq/BugReporting and
post it here?

Thank you,

Nathan.

On 2 April 2015 at 03:45, Stakres <vdoctor at neuf.fr> wrote:
> Hi All,
>
> Strange problem during surf, squid 3.5.3, 64bits, Debian 7.8:
> *2015/04/01 19:19:06 kid3| assertion failed: Read.cc:205: "params.data ==
> data"*
>
> caches:
> workers 3
> cache_dir rock /var/spool/squid3r1 166400 min-size=0 max-size=65536
> swap-timeout=500 max-swap-rate=200/sec
> if ${process_number} = 1
> cache_dir diskd /var/spool/squid3w1 453282 16 256 min-size=65536
> max-size=1250301952
> endif
> if ${process_number} = 2
> cache_dir diskd /var/spool/squid3w2 453282 16 256 min-size=65536
> max-size=1250301952
> endif
> if ${process_number} = 3
> cache_dir diskd /var/spool/squid3w3 453282 16 256 min-size=65536
> max-size=1250301952
> endif
>
> /dev/shm exists, correct rights.
> /var/run/squid exists, correct rights.
>
> Anyone with experience about this error ?
>
> Bye Fred
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-Read-cc-205-params-data-data-tp4670624.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From johnzeng2013 at yahoo.com  Sat Apr  4 15:26:39 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 04 Apr 2015 23:26:39 +0800
Subject: [squid-users] a question about Dns lookup
Message-ID: <552002AF.6090308@yahoo.com>


Hello All

I deploy squid 3.5.2 and Bind9 at same box , and redirect full dns
request to Bind server via setting /etc/resolv.conf nameserver 127.0.0.1

and i use tproxy and bridge mode , but when traffic thourgh squid , and
i found dns resolving rate will be slow than privious status .

Whether i need configure --/disable/-internal-/dns/ or other ??


if possible , please give me some advisement .


Best Regards

john


From alberto2perez at gmail.com  Sat Apr  4 18:20:58 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Sat, 4 Apr 2015 14:20:58 -0400
Subject: [squid-users] issue with tcp_outgoing_address,
	trying to balace traffic based on username
Message-ID: <CAMZauGo-6w+j5YV2SS81kbnjT5zE23kLVOEPJ16F+DRVQBiMhA@mail.gmail.com>

Hi everyone
I've been trying to make a traffic load balancing between two links
based on username using tcp_outgoing_address

My squid setup only use authorization with an external_acl which
returns the username based on the client ip.

In my first failure trying to setup this, I found (with the help of
Amos) that tcp_outgoing_address only works with fast acls, so ext_user
acl doesn't work there, Amos recommend me to use NOTE acl for matching
annotations in transactions, at first it looks to work fine but now I
am realizing that only a very small part of the traffic is been going
through the correct link.

I can't find any good documentation related to note acl with some
samples or cases of usage, so I hope some one can correct my config
and/or point me the right direction to achieve this.


Inside my external acl I mark some users like this.
// if username is one of fast users
fwrite(STDOUT, "OK user=$username clt_conn_tag=55\n");

So theoretically both user and clt_conn_tag values can be matched with
NOTE ACL, as far as i know, I use also clt_conn_tag for testing but it
should be enough with user mark.


#this is fast users declaration tests ive tried with both commented
and uncommented acl.
acl nodo_users ext_user "/etc/squid3/users/nodo_users"
#acl fast_users note clt_conn_tag 55
acl fast_users note user "/etc/squid3/users/nodo_users"

nodo_users is a list of fast users,  the tcp_outgoing_address is used
like this

#fast link for fast users
tcp_outgoing_address xx.xx.xx.01 fast_users
# default slow link
tcp_outgoing_address xx.xx.xx.02

Traffic of fast users is intermitent between the two links and much
more using the slow link than the fast link (opposite than expected).
External ACL TTL is 3 sec, so I asume that the correct balancing is
made only when squid checks de acl and mark the request, but this mark
isnt persistent.

Please, any help with this will be appreciated.

Thanks
Alberto


From vdoctor at neuf.fr  Mon Apr  6 08:10:01 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 6 Apr 2015 01:10:01 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <54F6D2AC.6050203@treenet.co.nz>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz>
Message-ID: <1428307801410-4670629.post@n4.nabble.com>

Hi Amos,

We have done additional tests in production with ISPs and the ORIGINAL_DST
in tproxy cannot be cached.
In normal mode (not tproxy), ORIGINAL_DST can be cached, no problem.
But once in tproxy (http_port 3128 tproxy), no way, it's impossible to get
TCP_HIT.

We have played with the client_dst_passthru and the host_verify_strict, many
combinaisons on/off.
By settings client_dst_passthru ON and host_verify_strict OFF, we can reduce
the number of ORIGINAL_DST (generating DNS "alerts" in the cache.log) but it
makes issues with HTTPS websites (facebook, hotmail, gmail, etc...).
We have also tried many DNS servers (internals and/or externals), same
issue.

I read what you explain in your previous email but it seems there is
something weird.
The problem is that the ORIGINAL_DST could be up to 25% of the traffic with
some installations meaning this part is "out-of-control" in term of cache
potential.

All help is welcome here 
Thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4670629.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Mon Apr  6 12:14:55 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 6 Apr 2015 05:14:55 -0700 (PDT)
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <55093D09.2070909@gmail.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com>
Message-ID: <1428322495420-4670630.post@n4.nabble.com>

Hi All, Yury,

Facing the same problem at the moment with the squid 3.5.3, around 150
req/sec.
The SSL crash 5 min later with the error.

index.txt:
V	150623000000Z		7EE07E84896D06865495B87A061C4C55D03E428D	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.appspot.com+Sign=signTrusted+SignHash=SHA256
V	150617000000Z		4E50C8790541265060E8796852D2E1D2878D7089	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=google.com+Sign=signTrusted+SignHash=SHA256
V	150617000000Z		16802607779EC137D972E9731A3D8DD1D65F1819	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=accounts.google.com+Sign=signTrusted+SignHash=SHA256
V	150617000000Z		736D922E14C3E8E573141AC6E3E79C4218B1B541	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.google-analytics.com+Sign=signTrusted+SignHash=SHA256
V	150617000000Z		0A1D58F2065EA701CD60D874325AFB4D76602922	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.googleusercontent.com+Sign=signTrusted+SignHash=SHA256
V	150617000000Z		213231FB70E633CA37606F717BBD1A92AEA97D7B	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.google.com+Sign=signTrusted+SignHash=SHA256
SHA256

The last line is wrong 

Tested with 1 worker, 1 DISKD cache.

https_port 8189 intercept ssl-bump generate-host-certificates=on
cert=/etc/squid3/mycert.pem key=/etc/squid3/mycert.pem
sslproxy_capath /etc/ssl/certs
ssl_bump server-first all
sslcrtd_program /usr/local/squid3/lib/ssl_crtd -s /var/lib/ssl_db -M 16MB
sslcrtd_children 32 startup=5 idle=1

/var/lib/ssl_db is using the correct rights, all controled many times.

Any idea ?

Bye Fred





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670630.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Apr  6 12:53:59 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 06 Apr 2015 18:53:59 +0600
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1428322495420-4670630.post@n4.nabble.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
Message-ID: <552281E7.8060405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Note: This line:

ssl_bump server-first all


in config is from 3.4.x, in 3.5.x you must use new bump syntax.

Also:

- -what openssl/gnutls version used to build squid?
- -Is sslproxy_capath contains really complete set of root/intermediate
CA's certificates?

This is probably not the main reason. But highly provocative, at least
on my 3.4.12 was so.

06.04.15 18:14, Stakres ?????:
> ssl_bump server-first all

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVIoHmAAoJENNXIZxhPexGnkAH/A9yQzd0V62l9y1rCeKhXRQb
rrAhx088aEIH+/o240fHe+hobJPrCrlbPgI2PigQuX08gsYlVCyYXdeqiI27kY4v
S6f3vTkyT/K3IoPFFu56xRlzxAykDAXHuqHTqEVHKjnbv/52Sb7WauKj7uYTBuJq
0cNz/IzWODpArlOIJ15XrfhiS0pEZxveYG8LwCok07iN939uyUUkvScJe0zKZw28
1T8Yae2X4nI3pCT/X/habLia0BforOeuHjSIHPwUC2bqtqQMa9M7ou+2N3HCwLWZ
DRIEjghyjlZLrylW3P4L/uucoYo1X+Ug/eUON8cm+pyweU2tIRQCRsDpAld13CU=
=CcxE
-----END PGP SIGNATURE-----



From geekguy at geek-guy.com  Mon Apr  6 20:29:26 2015
From: geekguy at geek-guy.com (Lawrence Pingree)
Date: Mon, 6 Apr 2015 13:29:26 -0700
Subject: [squid-users] Possible-Spam  a question about Dns lookup
In-Reply-To: <552002AF.6090308@yahoo.com>
References: <552002AF.6090308@yahoo.com>
Message-ID: <06d701d070a8$61b08780$25119680$@geek-guy.com>


Unless you are doing authoritative DNS lookups, you may only want to use a caching forwarder like dnsmasq. Also, squid's ipcache_size parameter can be tweaked to cache more dns responses. Disabling internal DNS would actually hinder performance in most cases.



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of johnzeng
Sent: Saturday, April 4, 2015 8:27 AM
To: squid-users at lists.squid-cache.org; Amos Jeffries
Subject: Possible-Spam [squid-users] a question about Dns lookup


Hello All

I deploy squid 3.5.2 and Bind9 at same box , and redirect full dns request to Bind server via setting /etc/resolv.conf nameserver 127.0.0.1

and i use tproxy and bridge mode , but when traffic thourgh squid , and i found dns resolving rate will be slow than privious status .

Whether i need configure --/disable/-internal-/dns/ or other ??


if possible , please give me some advisement .


Best Regards

john
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From yvoinov at gmail.com  Mon Apr  6 20:32:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 07 Apr 2015 02:32:15 +0600
Subject: [squid-users] Possible-Spam  a question about Dns lookup
In-Reply-To: <06d701d070a8$61b08780$25119680$@geek-guy.com>
References: <552002AF.6090308@yahoo.com>
 <06d701d070a8$61b08780$25119680$@geek-guy.com>
Message-ID: <5522ED4F.9090302@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yep,

especially with Unbound :)

07.04.15 2:29, Lawrence Pingree ?????:
>
> Unless you are doing authoritative DNS lookups, you may only want to
use a caching forwarder like dnsmasq. Also, squid's ipcache_size
parameter can be tweaked to cache more dns responses. Disabling internal
DNS would actually hinder performance in most cases.
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of johnzeng
> Sent: Saturday, April 4, 2015 8:27 AM
> To: squid-users at lists.squid-cache.org; Amos Jeffries
> Subject: Possible-Spam [squid-users] a question about Dns lookup
>
>
> Hello All
>
> I deploy squid 3.5.2 and Bind9 at same box , and redirect full dns
request to Bind server via setting /etc/resolv.conf nameserver 127.0.0.1
>
> and i use tproxy and bridge mode , but when traffic thourgh squid ,
and i found dns resolving rate will be slow than privious status .
>
> Whether i need configure --/disable/-internal-/dns/ or other ??
>
>
> if possible , please give me some advisement .
>
>
> Best Regards
>
> john
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVIu1OAAoJENNXIZxhPexGs4cH+wRs5V1Fo4q/U44X/8eOdqRm
s26VQ6ewdEZv06G988pvXYzpwo/XcrMlS10LsGrsS4LfFb9lpVfQjGMxslCQYeoZ
1JTDxEEx/mAfgQ4DnGVqMqS1UAyPjvkvCo4vcsYNClHe/+dkx7GE9eWPhGlVZIo9
vXXADq6oK6a5dnGMmPGSX5GdDATRoxDJIZDHT0nRr07YIislBbbqVhYwIofN9fS2
uQ4IZPc25cdB26lhcICWTqzcgaCVfZLas2PjblJBUnCsTD1aG0ecfU5pQCYNIAEN
3PCtL/+g5bNts/pQqrnhdn2aeioOyWPMrhUDO1I5zm6KPZegN+COq3D7vRuNfwo=
=dvBL
-----END PGP SIGNATURE-----



From alberto2perez at gmail.com  Mon Apr  6 21:01:26 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Mon, 6 Apr 2015 17:01:26 -0400
Subject: [squid-users] help with tcp_outgoing_address trying to balace
	traffic based on username
Message-ID: <CAMZauGoVTsfOMHU2xT_gv5hM68w3BtEjFO8-soNDbB29fvpGXQ@mail.gmail.com>

Hi everyone
I've been trying to make a traffic load balancing between two links
based on username using tcp_outgoing_address

My squid setup only use authorization with an external_acl which
returns the username based on the client ip.

In my first failure trying to setup this, I found (with the help of
Amos) that tcp_outgoing_address only works with fast acls, so ext_user
acl doesn't work there, Amos recommend me to use NOTE acl for matching
annotations in transactions, at first it looks to work fine but now I
am realizing that only a very small part of the traffic is been going
through the correct link.

I can't find any good documentation related to note acl with some
samples or cases of usage, so I hope some one can correct my config
and/or point me the right direction to achieve this.


Inside my external acl I mark some users like this.
// if username is one of fast users
fwrite(STDOUT, "OK user=$username clt_conn_tag=55\n");

So theoretically both user and clt_conn_tag values can be matched with
NOTE ACL, as far as i know, I use also clt_conn_tag for testing but it
should be enough with user mark.


#this is fast users declaration tests ive tried with both commented
and uncommented acl.
acl nodo_users ext_user "/etc/squid3/users/nodo_users"
#acl fast_users note clt_conn_tag 55
acl fast_users note user "/etc/squid3/users/nodo_users"

nodo_users is a list of fast users,  the tcp_outgoing_address is used
like this

#fast link for fast users
tcp_outgoing_address xx.xx.xx.01 fast_users
# default slow link
tcp_outgoing_address xx.xx.xx.02

Traffic of fast users is intermitent between the two links and much
more using the slow link than the fast link (opposite than expected).
External ACL TTL is 3 sec, so I asume that the correct balancing is
made only when squid checks de acl and mark the request, but this mark
isnt persistent.

Please, any help with this will be appreciated.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150406/70c9ca0b/attachment.htm>

From johnzeng2013 at yahoo.com  Tue Apr  7 05:46:09 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Tue, 07 Apr 2015 13:46:09 +0800
Subject: [squid-users] hi ,
 i don't receive any info from squid-users@lists.squid-cache.org now
Message-ID: <55236F21.9060608@yahoo.com>


HI Amos :

hi , i don't receive any info from squid-users at lists.squid-cache.org now ,

if possible , please help me to check .


Best Regards

john


From dweimer at dweimer.net  Tue Apr  7 15:16:09 2015
From: dweimer at dweimer.net (dweimer)
Date: Tue, 07 Apr 2015 10:16:09 -0500
Subject: [squid-users] Strange message when doing a squid -k parse or
	reconfigure
Message-ID: <8c52d230b0ea454a350522b679348975@dweimer.net>

My Squid Process seems to be working fine, but I noticed an unusual 
message when testing the squid configuration

squid: environment corrupt; missing value for https_pr

Any Ideas? Its a forward only proxy not doing reverse proxy or anything. 
Its running on FreeBSD 10.1-RELEASE-p8, installed from ports Squid 
version is 3.4.12. I don't have any problems accessing http or https 
sites through it.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From gkinkie at gmail.com  Tue Apr  7 15:19:47 2015
From: gkinkie at gmail.com (Kinkie)
Date: Tue, 7 Apr 2015 17:19:47 +0200
Subject: [squid-users] Strange message when doing a squid -k parse or
	reconfigure
In-Reply-To: <8c52d230b0ea454a350522b679348975@dweimer.net>
References: <8c52d230b0ea454a350522b679348975@dweimer.net>
Message-ID: <CA+Y8hcM1GzWjFYfkjDM7ARBjK88Ba-JBo=DPCLcW9vmEGXty=A@mail.gmail.com>

Hi,
  I've searched for these strings in squid, couldn't find them.
Maybe this is emitted by some library?

On Tue, Apr 7, 2015 at 5:16 PM, dweimer <dweimer at dweimer.net> wrote:
> My Squid Process seems to be working fine, but I noticed an unusual message
> when testing the squid configuration
>
> squid: environment corrupt; missing value for https_pr
>
> Any Ideas? Its a forward only proxy not doing reverse proxy or anything. Its
> running on FreeBSD 10.1-RELEASE-p8, installed from ports Squid version is
> 3.4.12. I don't have any problems accessing http or https sites through it.
>
> --
> Thanks,
>    Dean E. Weimer
>    http://www.dweimer.net/
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From johnzeng2013 at yahoo.com  Tue Apr  7 15:21:31 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Tue, 07 Apr 2015 23:21:31 +0800
Subject: [squid-users] Possible-Spam  a question about Dns lookup
In-Reply-To: <06d701d070a8$61b08780$25119680$@geek-guy.com>
References: <552002AF.6090308@yahoo.com>
 <06d701d070a8$61b08780$25119680$@geek-guy.com>
Message-ID: <5523F5FB.8020402@yahoo.com>

Hello Lawrence:

                           Thanks , Maybe  we must add 
tcp_outgoing_address   at bridge mode + tproxy .

                           and Box can send dns request to internet via 
the setting .

                           i feel accessing rate will be fast than 
previous seting .


                           John


? 2015?04?07? 04:29, Lawrence Pingree ??:
> Unless you are doing authoritative DNS lookups, you may only want to use a caching forwarder like dnsmasq. Also, squid's ipcache_size parameter can be tweaked to cache more dns responses. Disabling internal DNS would actually hinder performance in most cases.
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of johnzeng
> Sent: Saturday, April 4, 2015 8:27 AM
> To: squid-users at lists.squid-cache.org; Amos Jeffries
> Subject: Possible-Spam [squid-users] a question about Dns lookup
>
>
> Hello All
>
> I deploy squid 3.5.2 and Bind9 at same box , and redirect full dns request to Bind server via setting /etc/resolv.conf nameserver 127.0.0.1
>
> and i use tproxy and bridge mode , but when traffic thourgh squid , and i found dns resolving rate will be slow than privious status .
>
> Whether i need configure --/disable/-internal-/dns/ or other ??
>
>
> if possible , please give me some advisement .
>
>
> Best Regards
>
> john
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>



From sam at idsdoc.com  Wed Apr  8 03:26:47 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Tue, 7 Apr 2015 21:26:47 -0600
Subject: [squid-users] Increase number of ext_ldap_group_acl processes
In-Reply-To: <1426694704086-4670488.post@n4.nabble.com>
References: <1426689385646-4670484.post@n4.nabble.com>
 <550993B7.5030801@treenet.co.nz>
 <1426694704086-4670488.post@n4.nabble.com>
Message-ID: <CAP6yRXiEnpSJ21XLJVdg=VFb7QiOrBCrtk1PNQs9XxFK6zOAvg@mail.gmail.com>

Try this:

external_acl_type internet_domain_group children-startup=15 children-max=15
children-idle=2 %LOGIN



On Wed, Mar 18, 2015 at 10:05 AM, Rich549 <Richard.Aspley at hammonds-uk.com>
wrote:

> Thanks! I've added the children-startup=15 to my config but it seems to be
> ignoring it. An excerpt of my config is:
>
> external_acl_type internet_domain_group children-startup=15 %LOGIN
> /usr/lib/squid3/ext_ldap_group_acl -R -P -b (this then goes on to provide
> details of AD structure etc).
>
> Have I missed something?
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Increase-number-of-ext-ldap-group-acl-processes-tp4670484p4670488.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150407/eda57b38/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr  8 04:00:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Apr 2015 16:00:14 +1200
Subject: [squid-users] Possible-Spam  a question about Dns lookup
In-Reply-To: <5523F5FB.8020402@yahoo.com>
References: <552002AF.6090308@yahoo.com>
 <06d701d070a8$61b08780$25119680$@geek-guy.com> <5523F5FB.8020402@yahoo.com>
Message-ID: <5524A7CE.4080103@treenet.co.nz>

On 8/04/2015 3:21 a.m., johnzeng wrote:
> Hello Lawrence:
> 
>                           Thanks , Maybe  we must add
> tcp_outgoing_address   at bridge mode + tproxy .
> 
>                           and Box can send dns request to internet via
> the setting .
> 
>                           i feel accessing rate will be fast than
> previous seting .
> 
> 
>                           John

Bridging, TPROXY and tcp_outgoing_address are *alternative* features.

Bridging works with TPROXY, because TPOXY is all about using the client
IP and not the local device (bridge) IP. Since bridge devices do not
have an IP of their own to use in tcp_outgoing_address this works fine.

TPROXY works without bridging, because bridging is just a TCP-only layer
feature and Squid operates at HTTP layer.

tcp_outgoing_address does *not* work with TPROXY (or bridging), because
it is all about selecting which of the *Squid device* IPs are to be used
by the *routing* systems.


NP: You *can* use tcp_outgoing_address selection on traffic received in
a TPROXY port, but only if spoofing is disabled using the
spoof_client_ip directive.
(<http://www.squid-cache.org/Doc/config/spoof_client_ip/>)


Running Squid (or any other loclahost software) on a bridge device needs
that device to *also* have some routing capability and IPs for use.
Squid DNS traffic will use the device IP as source address so packets
get back to *it* properly. This has nothing to do with the HTTP layer
bridging or TPROXY or tcp_outgoing_address behaviours.

This situation can make it appear as if strange things are going on if
you are thinking of the box as *only* a bridge - because its not a
bridge its a bridge+router.

HTH
Amos



From squid3 at treenet.co.nz  Wed Apr  8 04:50:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Apr 2015 16:50:52 +1200
Subject: [squid-users] help with tcp_outgoing_address trying to balace
 traffic based on username
In-Reply-To: <CAMZauGoVTsfOMHU2xT_gv5hM68w3BtEjFO8-soNDbB29fvpGXQ@mail.gmail.com>
References: <CAMZauGoVTsfOMHU2xT_gv5hM68w3BtEjFO8-soNDbB29fvpGXQ@mail.gmail.com>
Message-ID: <5524B3AC.10805@treenet.co.nz>

On 7/04/2015 9:01 a.m., Alberto Perez wrote:
> Hi everyone
> I've been trying to make a traffic load balancing between two links
> based on username using tcp_outgoing_address
> 
> My squid setup only use authorization with an external_acl which
> returns the username based on the client ip.
> 
> In my first failure trying to setup this, I found (with the help of
> Amos) that tcp_outgoing_address only works with fast acls, so ext_user
> acl doesn't work there, Amos recommend me to use NOTE acl for matching
> annotations in transactions, at first it looks to work fine but now I
> am realizing that only a very small part of the traffic is been going
> through the correct link.
> 
> I can't find any good documentation related to note acl with some
> samples or cases of usage, so I hope some one can correct my config
> and/or point me the right direction to achieve this.
> 
> 
> Inside my external acl I mark some users like this.
> // if username is one of fast users
> fwrite(STDOUT, "OK user=$username clt_conn_tag=55\n");
> 
> So theoretically both user and clt_conn_tag values can be matched with
> NOTE ACL, as far as i know, I use also clt_conn_tag for testing but it
> should be enough with user mark.

Yes and no...

 The user= is a per-message note. Each message needs to be individually
checked by the helpers ACL to get the note attached.

 The clt_conn_tag= is a per-connection note. Once its assigned all
traffic received[**] on that same persistent client connection will be
tagged with it. Any future checks of the helper will only update the
notes value (if anything).

[**] its not quite as reliable as it seems. Multiple requests may be
parsed off the connection and processed in parallel before the first one
gets tagged. What happens for tagging (or not) on the parallel ones is
not easily determined. Usually they end up tagged by the same logics
that caused the initial mesage tag to be set on the connection, but not
guaranteed.


> 
> #this is fast users declaration tests ive tried with both commented
> and uncommented acl.
> acl nodo_users ext_user "/etc/squid3/users/nodo_users"
> #acl fast_users note clt_conn_tag 55
> acl fast_users note user "/etc/squid3/users/nodo_users"
> 
> nodo_users is a list of fast users,  the tcp_outgoing_address is used
> like this
> 
> #fast link for fast users
> tcp_outgoing_address xx.xx.xx.01 fast_users
> # default slow link
> tcp_outgoing_address xx.xx.xx.02
> 
> Traffic of fast users is intermitent between the two links and much
> more using the slow link than the fast link (opposite than expected).

The relationship between what network link is used and tcp_outgoing_* is
a lot more fuzzy than most people seems to think. The whole system NAT
infrastructure, routing and other TCP stack mechanisms are sitting in
between the Squid request for binding that IP and what actually leaves
the device. Those systems could be changing the packet in any way, or
even just routing it out the other NIC.


> External ACL TTL is 3 sec, so I asume that the correct balancing is
> made only when squid checks de acl and mark the request, but this mark
> isnt persistent.

Yes, Squid needs to check the external ACL and mark each request with
the notes. That should happen in the http_access rules though. The
"user=" note is set by both external ACL helper and authentication helpers.
 If you have a mix of those helpers then weird stuff can appear to be
happening depending on order they are checked.
 If you are bypassing the helper check for any traffic weird things can
happen for that traffic.


> 
> Please, any help with this will be appreciated.

What is your current squid.conf content?

Amos


From johnzeng2013 at yahoo.com  Wed Apr  8 05:32:40 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 08 Apr 2015 13:32:40 +0800
Subject: [squid-users] Possible-Spam  a question about Dns lookup
In-Reply-To: <5524A7CE.4080103@treenet.co.nz>
References: <552002AF.6090308@yahoo.com>
 <06d701d070a8$61b08780$25119680$@geek-guy.com> <5523F5FB.8020402@yahoo.com>
 <5524A7CE.4080103@treenet.co.nz>
Message-ID: <5524BD78.80301@yahoo.com>

Hello Dear Amos:

                             Thanks for your reply , and We redirect 
traffics based port 80 via ebtables -t broute ....   at bridge mode .

                             and Tproxy is located route layer or ip 
layer , you are right , we must set default route  and return route for 
different subnet client .

                             But Dns request can be sent via 
tcp_outgoing_address ,because we enable interna ldns .

                             Maybe you are right , if we don't set 
tcp_outgoing_address , dns request packed can be sent to internet too ,

                             i feel accessing rate will be fast than 
previous seting .


                              There are some detail about dns config

                             /etc/resolv.conf    ----  nameserver 127.0.0.1

                             a part of Bind config is                 
forwarders{ xxx.xxx.xxx.xxx ; xxx.xxx.xxx.xxx ; 8.8.8.8;  };



                              Thanks for your reply

                              have a good day with you



                              Bye


? 2015?04?08? 12:00, Amos Jeffries ??:
> s situation can make it appear as if strange things are going on if
> you are thinking of the box as*only*  a bridge - because i



From jaykbvt at gmail.com  Wed Apr  8 09:20:51 2015
From: jaykbvt at gmail.com (Jaydeep Kubavat)
Date: Wed, 8 Apr 2015 14:50:51 +0530
Subject: [squid-users] Transparent Proxy
Message-ID: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>

Hi,

I've configured a transparent squid proxy on a centos 6.6 with single NIC.

There is Cisco ISG in between with L4 redirection on www traffic.

The requests are coming on port 80 from client and ISG forwards that to
port 80 on my squid server.

So there is no iptables configured on squid server.

Client requests are not reaching upto my squid instance.

I'm getting the following in pcap on squid box.

=========================

"129","79.114808","10.210.83.246","10.58.200.33","TCP","76","39546?80 [SYN]
Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1 TSval=2686675 TSecr=0 WS=64"

"130","79.114946","10.58.200.33","10.210.83.246","TCP","76","80?39546 [SYN,
ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=509402603
TSecr=2686675 WS=64"

"145","82.115674","10.210.83.246","10.58.200.33","TCP","76","[TCP Spurious
Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1
TSval=2686976 TSecr=0 WS=64"

"146","82.115748","10.58.200.33","10.210.83.246","TCP","76","[TCP
Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
SACK_PERM=1 TSval=509405604 TSecr=2686675 WS=64"

"151","83.113859","10.58.200.33","10.210.83.246","TCP","76","[TCP
Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
SACK_PERM=1 TSval=509406603 TSecr=2686675 WS=64"

"165","88.145376","10.210.83.246","10.58.200.33","TCP","76","[TCP Spurious
Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1
TSval=2687578 TSecr=0 WS=64"

"166","88.145450","10.58.200.33","10.210.83.246","TCP","76","[TCP
Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
SACK_PERM=1 TSval=509411634 TSecr=2686675 WS=64"

"176","89.113837","10.58.200.33","10.210.83.246","TCP","76","[TCP
Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
SACK_PERM=1 TSval=509412603 TSecr=2686675 WS=64"

"285","101.113833","10.58.200.33","10.210.83.246","TCP","76","[TCP
Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
SACK_PERM=1 TSval=509424603 TSecr=2686675 WS=64"

=========================

my squid is configured default, only

http_port 3130
http_port 80 intercept

are changed.



-- 
Thanks & Regards
Jaykbvt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/0a8cac11/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr  8 09:51:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Apr 2015 21:51:49 +1200
Subject: [squid-users] Transparent Proxy
In-Reply-To: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>
References: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>
Message-ID: <5524FA35.8020907@treenet.co.nz>

On 8/04/2015 9:20 p.m., Jaydeep Kubavat wrote:
> Hi,
> 
> I've configured a transparent squid proxy on a centos 6.6 with single NIC.
> 
> There is Cisco ISG in between with L4 redirection on www traffic.
> 
> The requests are coming on port 80 from client and ISG forwards that to
> port 80 on my squid server.

No, no it does not.

If you configured the remote router coorrectly:

It passes the packet to your Squid box for handling. The packet still
says port 80 *on some other server*.

Once the TCP SYN packet reaches the Squid box ...

> 
> So there is no iptables configured on squid server.
> 

... nothing happens to it. "Dropped on the floor.", etc.


If you configured the router badly:
 ... many varied things (all nasty) could happen.


Please have a read through:
<http://wiki.squid-cache.org/SquidFaq/InterceptionProxy>
in particular the sections:
* "Concepts of Interception Caching"
* "Requirements and methods for Interception Caching"
* "Getting your traffic to the right port on your Squid Cache"


<snip>
> 
> my squid is configured default, only
> 
> http_port 3130

Port 3130 is generally used for ICP (which is a UDP based protocol)

> http_port 80 intercept

This has no use other than to potentially prevent your Squid being able
to open the listening port (unless the worker has root privileges - not
good).

Any port will do and a randomly selected port number higher than 1024 is
better. Only Squid and the machines TCP stack systems will have anything
to do with it - not the packets nor any external system.


Amos


From jaykbvt at gmail.com  Wed Apr  8 11:50:17 2015
From: jaykbvt at gmail.com (Jaydeep Kubavat)
Date: Wed, 8 Apr 2015 17:20:17 +0530
Subject: [squid-users] Transparent Proxy
In-Reply-To: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>
References: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>
Message-ID: <CAA8_PU6TaKQqM6AvA=O+gN-g+UZm7JQ7LfO-cCrYi0YbPZOfPw@mail.gmail.com>

Hi,

As suggested by Amos...I've configured squid box with bellow mentioned
config.

I followed this doc
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

1. Configured iptables as:

Table: filter
Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
num  target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination

Table: mangle
Chain PREROUTING (policy ACCEPT)
num  target     prot opt source               destination
1    DROP       tcp  --  0.0.0.0/0            0.0.0.0/0           tcp
dpt:3129

Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
num  target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
num  target     prot opt source               destination

Table: nat
Chain PREROUTING (policy ACCEPT)
num  target     prot opt source               destination
1    ACCEPT     tcp  --  10.58.200.33         0.0.0.0/0           tcp
dpt:80
2    DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0           tcp
dpt:80 to:10.58.200.33:3129

Chain POSTROUTING (policy ACCEPT)
num  target     prot opt source               destination
1    MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination


2. squid with http_port 3129 intercept

3. PCAP result

"3","1.539609","10.210.83.247","10.58.200.33","TCP","68","28754?80 [SYN]
Seq=0 Win=8192 Len=0 MSS=1360 WS=256 SACK_PERM=1"

"4","1.539680","10.58.200.33","10.210.83.247","TCP","68","80?28754 [SYN,
ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460 SACK_PERM=1 WS=64"

"19","2.717863","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

"31","7.613768","10.210.83.247","10.58.200.33","TCP","64","[TCP Spurious
Retransmission] 28754?80 [SYN] Seq=0 Win=8192 Len=0 MSS=1360 SACK_PERM=1"

"32","7.613835","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

"43","8.917825","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

"167","20.917840","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

"485","44.917837","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

"962","93.117870","10.58.200.33","10.210.83.247","TCP","68","[TCP
Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
SACK_PERM=1 WS=64"

-- 
Thanks & Regards
Jaykbvt

On Wed, Apr 8, 2015 at 2:50 PM, Jaydeep Kubavat <jaykbvt at gmail.com> wrote:

> Hi,
>
> I've configured a transparent squid proxy on a centos 6.6 with single NIC.
>
> There is Cisco ISG in between with L4 redirection on www traffic.
>
> The requests are coming on port 80 from client and ISG forwards that to
> port 80 on my squid server.
>
> So there is no iptables configured on squid server.
>
> Client requests are not reaching upto my squid instance.
>
> I'm getting the following in pcap on squid box.
>
> =========================
>
> "129","79.114808","10.210.83.246","10.58.200.33","TCP","76","39546?80
> [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1 TSval=2686675 TSecr=0
> WS=64"
>
> "130","79.114946","10.58.200.33","10.210.83.246","TCP","76","80?39546
> [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=509402603
> TSecr=2686675 WS=64"
>
> "145","82.115674","10.210.83.246","10.58.200.33","TCP","76","[TCP Spurious
> Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1
> TSval=2686976 TSecr=0 WS=64"
>
> "146","82.115748","10.58.200.33","10.210.83.246","TCP","76","[TCP
> Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> SACK_PERM=1 TSval=509405604 TSecr=2686675 WS=64"
>
> "151","83.113859","10.58.200.33","10.210.83.246","TCP","76","[TCP
> Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> SACK_PERM=1 TSval=509406603 TSecr=2686675 WS=64"
>
> "165","88.145376","10.210.83.246","10.58.200.33","TCP","76","[TCP Spurious
> Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1
> TSval=2687578 TSecr=0 WS=64"
>
> "166","88.145450","10.58.200.33","10.210.83.246","TCP","76","[TCP
> Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> SACK_PERM=1 TSval=509411634 TSecr=2686675 WS=64"
>
> "176","89.113837","10.58.200.33","10.210.83.246","TCP","76","[TCP
> Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> SACK_PERM=1 TSval=509412603 TSecr=2686675 WS=64"
>
> "285","101.113833","10.58.200.33","10.210.83.246","TCP","76","[TCP
> Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> SACK_PERM=1 TSval=509424603 TSecr=2686675 WS=64"
>
> =========================
>
> my squid is configured default, only
>
> http_port 3130
> http_port 80 intercept
>
> are changed.
>
>
>
> --
> Thanks & Regards
> Jaykbvt
>



-- 
Thanks & Regards
Jaykbvt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/3f59f6d4/attachment.htm>

From alex at imaginers.org  Wed Apr  8 12:26:18 2015
From: alex at imaginers.org (alex at imaginers.org)
Date: Wed, 8 Apr 2015 14:26:18 +0200 (CEST)
Subject: [squid-users] Transparent Proxy
In-Reply-To: <CAA8_PU6TaKQqM6AvA=O+gN-g+UZm7JQ7LfO-cCrYi0YbPZOfPw@mail.gmail.com>
References: <CAA8_PU5R3D9-9sRk=2Q5U_ob4zZFMj9SMot3wowTfX=UBEtKjw@mail.gmail.com>
 <CAA8_PU6TaKQqM6AvA=O+gN-g+UZm7JQ7LfO-cCrYi0YbPZOfPw@mail.gmail.com>
Message-ID: <1054110223.45555.1428495978562.JavaMail.open-xchange@oxbaltgw06.schlund.de>

Hi,
first of all what error do you get at client side? Timeout? Blank Page?
I'm also running squid in an ISG setup, my squid version is Squid Cache: Version
3.1.10 on Centos 6.5
Few things to check:
1) please ensure the iptables-rules are hit correctly by issuing .f.e:
iptables -t mangle -vnL
 
2)if you see packets please make sure you do not have a redirect-loop, run squid
in debug mode or enable logging.
an example error can be found here:
http://www.squid-cache.org/mail-archive/squid-users/201004/0538.html
 
3) it's enough to configure port redirection once, you can do it with iptables
on the squid box (as you did below) or directly at the ISG, if you have defined
a server Pool it will look like that (probably ;))
redirect server-group REDIRECT_SERVERS
 server ip xx.xx.xx.xx port 80
for iptables-redirect
or
server ip xx.xx.xx.xx port 3129
for isg redirect
 
4) All problems I had with that setup basically were router configuration
errors. If L4 redirect does not work did you try next-hop rerouting without
altering the ports?
In a Cisco ISG setup make sure the squid box uses the ISG for the return traffic
and can't reach the clients directly, also make sure you are capturing the right
traffic and not blocking the return packets ect.
 
HTH,
Alex
 
 

> Jaydeep Kubavat <jaykbvt at gmail.com> hat am 8. April 2015 um 13:50 geschrieben:
> 
>  Hi,
>   
>  As suggested by Amos...I've configured squid box with bellow mentioned
> config.
>   
>  I followed this doc
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
>   
>  1. Configured iptables as:
>   
>  Table: filter
>  Chain INPUT (policy ACCEPT)
>  num target prot opt source destination
>   
>  Chain FORWARD (policy ACCEPT)
>  num target prot opt source destination
>   
>  Chain OUTPUT (policy ACCEPT)
>  num target prot opt source destination
>   
>  Table: mangle
>  Chain PREROUTING (policy ACCEPT)
>  num target prot opt source destination
>  1 DROP tcp --<http://0.0.0.0/0><http://0.0.0.0/0> tcp dpt:3129
>   
>  Chain INPUT (policy ACCEPT)
>  num target prot opt source destination
>   
>  Chain FORWARD (policy ACCEPT)
>  num target prot opt source destination
>   
>  Chain OUTPUT (policy ACCEPT)
>  num target prot opt source destination
>   
>  Chain POSTROUTING (policy ACCEPT)
>  num target prot opt source destination
>   
>  Table: nat
>  Chain PREROUTING (policy ACCEPT)
>  num target prot opt source destination
>  1 ACCEPT tcp -- 10.58.200.33<http://0.0.0.0/0> tcp dpt:80
>  2 DNAT tcp --<http://0.0.0.0/0><http://0.0.0.0/0> tcp dpt:80
> to:<http://10.58.200.33:3129>
>   
>  Chain POSTROUTING (policy ACCEPT)
>  num target prot opt source destination
>  1 MASQUERADE all --<http://0.0.0.0/0><http://0.0.0.0/0>
>   
>  Chain OUTPUT (policy ACCEPT)
>  num target prot opt source destination
>   
>   
>  2. squid with http_port 3129 intercept
>   
>  3. PCAP result
>   
>  "3","1.539609","10.210.83.247","10.58.200.33","TCP","68","28754?80 [SYN]
> Seq=0 Win=8192 Len=0 MSS=1360 WS=256 SACK_PERM=1"
>   
>  "4","1.539680","10.58.200.33","10.210.83.247","TCP","68","80?28754 [SYN, ACK]
> Seq=0 Ack=1 Win=14600 Len=0 MSS=1460 SACK_PERM=1 WS=64"
>   
>  "19","2.717863","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  "31","7.613768","10.210.83.247","10.58.200.33","TCP","64","[TCP Spurious
> Retransmission] 28754?80 [SYN] Seq=0 Win=8192 Len=0 MSS=1360 SACK_PERM=1"
>   
>  "32","7.613835","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  "43","8.917825","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  "167","20.917840","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  "485","44.917837","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  "962","93.117870","10.58.200.33","10.210.83.247","TCP","68","[TCP
> Retransmission] 80?28754 [SYN, ACK] Seq=0 Ack=1 Win=14600 Len=0 MSS=1460
> SACK_PERM=1 WS=64"
>   
>  --
>  Thanks & Regards
>  Jaykbvt
> 
>  On Wed, Apr 8, 2015 at 2:50 PM, Jaydeep Kubavat <jaykbvt at gmail.com
> <mailto:jaykbvt at gmail.com> > wrote:
>    > >    Hi,
> >     
> >    I've configured a transparent squid proxy on a centos 6.6 with single
> > NIC.
> > 
> >    There is Cisco ISG in between with L4 redirection on www traffic.
> >     
> >    The requests are coming on port 80 from client and ISG forwards that to
> > port 80 on my squid server.
> >     
> >    So there is no iptables configured on squid server.
> >     
> >    Client requests are not reaching upto my squid instance.
> >     
> >    I'm getting the following in pcap on squid box.
> >     
> >    =========================
> >     
> >    "129","79.114808","10.210.83.246","10.58.200.33","TCP","76","39546?80
> > [SYN] Seq=0 Win=14600 Len=0 MSS=1360 SACK_PERM=1 TSval=2686675 TSecr=0
> > WS=64"
> >     
> >    "130","79.114946","10.58.200.33","10.210.83.246","TCP","76","80?39546
> > [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=509402603
> > TSecr=2686675 WS=64"
> >     
> >    "145","82.115674","10.210.83.246","10.58.200.33","TCP","76","[TCP
> > Spurious Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360
> > SACK_PERM=1 TSval=2686976 TSecr=0 WS=64"
> >     
> >    "146","82.115748","10.58.200.33","10.210.83.246","TCP","76","[TCP
> > Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> > SACK_PERM=1 TSval=509405604 TSecr=2686675 WS=64"
> >     
> >    "151","83.113859","10.58.200.33","10.210.83.246","TCP","76","[TCP
> > Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> > SACK_PERM=1 TSval=509406603 TSecr=2686675 WS=64"
> >     
> >    "165","88.145376","10.210.83.246","10.58.200.33","TCP","76","[TCP
> > Spurious Retransmission] 39546?80 [SYN] Seq=0 Win=14600 Len=0 MSS=1360
> > SACK_PERM=1 TSval=2687578 TSecr=0 WS=64"
> >     
> >    "166","88.145450","10.58.200.33","10.210.83.246","TCP","76","[TCP
> > Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> > SACK_PERM=1 TSval=509411634 TSecr=2686675 WS=64"
> >     
> >    "176","89.113837","10.58.200.33","10.210.83.246","TCP","76","[TCP
> > Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> > SACK_PERM=1 TSval=509412603 TSecr=2686675 WS=64"
> >     
> >    "285","101.113833","10.58.200.33","10.210.83.246","TCP","76","[TCP
> > Retransmission] 80?39546 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460
> > SACK_PERM=1 TSval=509424603 TSecr=2686675 WS=64"
> >     
> >    =========================
> >     
> >    my squid is configured default, only
> >     
> >    http_port 3130
> >    http_port 80 intercept
> >     
> >    are changed.
> >     
> >     
> >     
> >    --
> >    Thanks & Regards
> >    Jaykbvt
> >  > 
> 
>   
>  --
>  Thanks & Regards
>  Jaykbvt
>  _______________________________________________
>  squid-users mailing list
>  squid-users at lists.squid-cache.org
>  http://lists.squid-cache.org/listinfo/squid-users
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/c9eae1be/attachment.htm>

From sam at idsdoc.com  Wed Apr  8 15:20:45 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Wed, 8 Apr 2015 09:20:45 -0600
Subject: [squid-users] NTLM authentication problems with HTTP 1.1
Message-ID: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>

Hello all,


I'm having a problem where HTTP 1.1 connect requests do not authenticate
using NTLM. Browsing the internet works fine in all major browsers, I
mostly see this occurring in programs that are installed locally on a users
computer. Using wireshark I'm able to follow the TCP stream and I can see
that the server returns the error (407 Proxy Authentication Required). I am
able to work around this problem by explicitly bypassing a domain from
requiring authentication, however I really don't want to do that. Any ideas
would be appreciated very much.

Thanks,


Below is the content summery of some of the network packets that I'm
working with along with my config file

TCP Stream Content

####################
CONNECT batch.internetpostage.com:443 HTTP/1.1
Host: batch.internetpostage.com
Proxy-Connection: Keep-Alive


HTTP/1.1 407 Proxy Authentication Required
Server: squid/3.3.8
Mime-Version: 1.0
Date: Tue, 07 Apr 2015 21:02:24 GMT
Content-Type: text/html
Content-Length: 3208
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Proxy-Authenticate: Negotiate
Proxy-Authenticate: NTLM
X-Cache: MISS from squid2.****.local
X-Cache-Lookup: NONE from squid2.****.local:3128
Via: 1.1 squid2.****.local (squid/3.3.8)
Connection: close
####################

CONFIG File

####################

#Kerberos and NTLM authentication

auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=****.LOCAL --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d
-s GSS_C_NO_NAME
auth_param negotiate children 30
auth_param negotiate keep_alive off

auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --domain=****
auth_param ntlm children 30
auth_param ntlm keep_alive off

# AD group membership lookup

external_acl_type ldap_group ttl=60 children-startup=10 children-max=50
children-idle=2 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
"DC=****,DC=local" -D "CN=SQUID,OU=**** Service Accounts,DC=****,DC=local"
-w "****" -f "(&(objectclass=person)
(sAMAccountname=%v)(memberof=CN=%a,OU=PROXY,ou=ALL **** Groups,DC=****
,DC=local))" -h dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local

# auth required

acl auth proxy_auth REQUIRED
http_access deny !auth all

####################

-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/e5302192/attachment.htm>

From bpk678 at gmail.com  Wed Apr  8 17:20:35 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 8 Apr 2015 13:20:35 -0400
Subject: [squid-users] NTLM authentication problems with HTTP 1.1
In-Reply-To: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>
References: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>
Message-ID: <CAARxGtii5UR_6YrmAab3t1COPks0LFdfrhyzrJ-xzHy0APYGEg@mail.gmail.com>

Note the lack of a user-agent string.  This is likely an app that cannot
authenticate.

My standard for Auth Bypass is source IP, user-agent string and destination
URL.  Generally the source is preferred to be statically assigned otherwise
you need to allow the entire dhcp pool or range.  Because there is no
user-agent you can drop the requirement or force it with some sort of
negated logic (!any)
On Apr 8, 2015 11:21 AM, "Samuel Anderson" <sam at idsdoc.com> wrote:

> Hello all,
>
>
> I'm having a problem where HTTP 1.1 connect requests do not authenticate
> using NTLM. Browsing the internet works fine in all major browsers, I
> mostly see this occurring in programs that are installed locally on a users
> computer. Using wireshark I'm able to follow the TCP stream and I can see
> that the server returns the error (407 Proxy Authentication Required). I am
> able to work around this problem by explicitly bypassing a domain from
> requiring authentication, however I really don't want to do that. Any ideas
> would be appreciated very much.
>
> Thanks,
>
>
> Below is the content summery of some of the network packets that I'm
> working with along with my config file
>
> TCP Stream Content
>
> ####################
> CONNECT batch.internetpostage.com:443 HTTP/1.1
> Host: batch.internetpostage.com
> Proxy-Connection: Keep-Alive
>
>
> HTTP/1.1 407 Proxy Authentication Required
> Server: squid/3.3.8
> Mime-Version: 1.0
> Date: Tue, 07 Apr 2015 21:02:24 GMT
> Content-Type: text/html
> Content-Length: 3208
> X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
> Proxy-Authenticate: Negotiate
> Proxy-Authenticate: NTLM
> X-Cache: MISS from squid2.****.local
> X-Cache-Lookup: NONE from squid2.****.local:3128
> Via: 1.1 squid2.****.local (squid/3.3.8)
> Connection: close
> ####################
>
> CONFIG File
>
> ####################
>
> #Kerberos and NTLM authentication
>
> auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
> --domain=****.LOCAL --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d
> -s GSS_C_NO_NAME
> auth_param negotiate children 30
> auth_param negotiate keep_alive off
>
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --domain=****
> auth_param ntlm children 30
> auth_param ntlm keep_alive off
>
> # AD group membership lookup
>
> external_acl_type ldap_group ttl=60 children-startup=10 children-max=50
> children-idle=2 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> "DC=****,DC=local" -D "CN=SQUID,OU=**** Service Accounts,DC=****,DC=local"
> -w "****" -f "(&(objectclass=person)
> (sAMAccountname=%v)(memberof=CN=%a,OU=PROXY,ou=ALL **** Groups,DC=****
> ,DC=local))" -h dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local
>
> # auth required
>
> acl auth proxy_auth REQUIRED
> http_access deny !auth all
>
> ####################
>
> --
> Samuel Anderson  |  Information Technology Administrator  |  International
> Document Services
>
> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
>
>
> CONFIDENTIALITY NOTICE:
> This e-mail and any attachments are confidential. If you are not an
> intended recipient, please contact the sender to report the error and
> delete all copies of this message from your system.  Any unauthorized
> review, use, disclosure or distribution is prohibited.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/b5b5df16/attachment.htm>

From sam at idsdoc.com  Wed Apr  8 17:42:48 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Wed, 8 Apr 2015 13:42:48 -0400
Subject: [squid-users] NTLM authentication problems with HTTP 1.1
In-Reply-To: <CAARxGtii5UR_6YrmAab3t1COPks0LFdfrhyzrJ-xzHy0APYGEg@mail.gmail.com>
References: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>
 <CAARxGtii5UR_6YrmAab3t1COPks0LFdfrhyzrJ-xzHy0APYGEg@mail.gmail.com>
Message-ID: <CAP6yRXjvNzQ1DFSHMQjmio8ov75=qT3ZYbqJcUkBvbS8KokJPg@mail.gmail.com>

Oh okay, that makes sense.

Thanks brendan

On Wed, Apr 8, 2015 at 1:20 PM, brendan kearney <bpk678 at gmail.com> wrote:

> Note the lack of a user-agent string.  This is likely an app that cannot
> authenticate.
>
> My standard for Auth Bypass is source IP, user-agent string and
> destination URL.  Generally the source is preferred to be statically
> assigned otherwise you need to allow the entire dhcp pool or range.
> Because there is no user-agent you can drop the requirement or force it
> with some sort of negated logic (!any)
> On Apr 8, 2015 11:21 AM, "Samuel Anderson" <sam at idsdoc.com> wrote:
>
>> Hello all,
>>
>>
>> I'm having a problem where HTTP 1.1 connect requests do not authenticate
>> using NTLM. Browsing the internet works fine in all major browsers, I
>> mostly see this occurring in programs that are installed locally on a users
>> computer. Using wireshark I'm able to follow the TCP stream and I can see
>> that the server returns the error (407 Proxy Authentication Required). I am
>> able to work around this problem by explicitly bypassing a domain from
>> requiring authentication, however I really don't want to do that. Any ideas
>> would be appreciated very much.
>>
>> Thanks,
>>
>>
>> Below is the content summery of some of the network packets that I'm
>> working with along with my config file
>>
>> TCP Stream Content
>>
>> ####################
>> CONNECT batch.internetpostage.com:443 HTTP/1.1
>> Host: batch.internetpostage.com
>> Proxy-Connection: Keep-Alive
>>
>>
>> HTTP/1.1 407 Proxy Authentication Required
>> Server: squid/3.3.8
>> Mime-Version: 1.0
>> Date: Tue, 07 Apr 2015 21:02:24 GMT
>> Content-Type: text/html
>> Content-Length: 3208
>> X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
>> Proxy-Authenticate: Negotiate
>> Proxy-Authenticate: NTLM
>> X-Cache: MISS from squid2.****.local
>> X-Cache-Lookup: NONE from squid2.****.local:3128
>> Via: 1.1 squid2.****.local (squid/3.3.8)
>> Connection: close
>> ####################
>>
>> CONFIG File
>>
>> ####################
>>
>> #Kerberos and NTLM authentication
>>
>> auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
>> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
>> --domain=****.LOCAL --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d
>> -s GSS_C_NO_NAME
>> auth_param negotiate children 30
>> auth_param negotiate keep_alive off
>>
>> auth_param ntlm program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-ntlmssp --domain=****
>> auth_param ntlm children 30
>> auth_param ntlm keep_alive off
>>
>> # AD group membership lookup
>>
>> external_acl_type ldap_group ttl=60 children-startup=10 children-max=50
>> children-idle=2 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
>> "DC=****,DC=local" -D "CN=SQUID,OU=**** Service Accounts,DC=****,DC=local"
>> -w "****" -f "(&(objectclass=person)
>> (sAMAccountname=%v)(memberof=CN=%a,OU=PROXY,ou=ALL **** Groups,DC=****
>> ,DC=local))" -h dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local
>>
>> # auth required
>>
>> acl auth proxy_auth REQUIRED
>> http_access deny !auth all
>>
>> ####################
>>
>> --
>> Samuel Anderson  |  Information Technology Administrator  |
>>  International Document Services
>>
>> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
>>
>>
>> CONFIDENTIALITY NOTICE:
>> This e-mail and any attachments are confidential. If you are not an
>> intended recipient, please contact the sender to report the error and
>> delete all copies of this message from your system.  Any unauthorized
>> review, use, disclosure or distribution is prohibited.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>


-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150408/03d2f9e8/attachment.htm>

From squid3 at treenet.co.nz  Thu Apr  9 01:31:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Apr 2015 13:31:20 +1200
Subject: [squid-users] NTLM authentication problems with HTTP 1.1
In-Reply-To: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>
References: <CAP6yRXjmMusAVAD0YR1Bpq3Dk_Vg4_bh=ehxHp0OK7c1b1KJ5A@mail.gmail.com>
Message-ID: <5525D668.90902@treenet.co.nz>

On 9/04/2015 3:20 a.m., Samuel Anderson wrote:
> Hello all,
> 
> 
> I'm having a problem where HTTP 1.1 connect requests do not authenticate
> using NTLM. Browsing the internet works fine in all major browsers, I
> mostly see this occurring in programs that are installed locally on a users
> computer. Using wireshark I'm able to follow the TCP stream and I can see
> that the server returns the error (407 Proxy Authentication Required). I am
> able to work around this problem by explicitly bypassing a domain from
> requiring authentication, however I really don't want to do that. Any ideas
> would be appreciated very much.
> 
> Thanks,
> 
> 
> Below is the content summery of some of the network packets that I'm
> working with along with my config file
> 
> TCP Stream Content
> 
> ####################
> CONNECT batch.internetpostage.com:443 HTTP/1.1
> Host: batch.internetpostage.com
> Proxy-Connection: Keep-Alive

Note the absence of Proxy-Authorization. This is the very start of the
auth sequence...

> 
> HTTP/1.1 407 Proxy Authentication Required
> Server: squid/3.3.8
> Mime-Version: 1.0
> Date: Tue, 07 Apr 2015 21:02:24 GMT
> Content-Type: text/html
> Content-Length: 3208
> X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
> Proxy-Authenticate: Negotiate
> Proxy-Authenticate: NTLM
> X-Cache: MISS from squid2.****.local
> X-Cache-Lookup: NONE from squid2.****.local:3128
> Via: 1.1 squid2.****.local (squid/3.3.8)
> Connection: close

... Squid responds by indicating auth is required and what types may be
used.

This is a perfectly normal and working stage-1 of any authentication
type. Including the fact the TCP connection is closed.

NTLM handshake stage-2 begins on a new TCP connection with the client
sending a repeat of the CONNECT request but containing the type-1
credentials token. Squid responds with a type-2 credentials token. Then
the client repeats the CONNECT for a third time with at type-3
credentials token, which Squid either accepts or rejects.


> ####################
> 
> CONFIG File
> 
> ####################
> 
<snip>
> # auth required
> 
> acl auth proxy_auth REQUIRED
> http_access deny !auth all

The "all" here is probably why your authetication is failing. What it
does is prevent Squid emitting the stage-3 NTLM handshake response.

Amos


From h.wahl at ifw-dresden.de  Thu Apr  9 06:47:29 2015
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Thu, 09 Apr 2015 08:47:29 +0200
Subject: [squid-users] State of www1.ngtech.co.il
Message-ID: <55262081.1070200@ifw-dresden.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi list,
does anybody know what is the matter with www1.ngtech.co.il? This is
the source for RPM packages of squid but it seems to be dried up for
some days now.
Regards

- -- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Kaufm?nnische Direktorin i. V.
Dipl.-Kffr. Friederike Jaeger
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlUmIIEACgkQnmb3Nh+6CUIEYACcDuQKyYq7FIqA5Kr+Ykbf90k4
bh8AnjYEaXryCQ8q/Ki2JOXHDyjyYALk
=HhFG
-----END PGP SIGNATURE-----


From alex at samad.com.au  Thu Apr  9 07:01:59 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 9 Apr 2015 17:01:59 +1000
Subject: [squid-users] State of www1.ngtech.co.il
In-Reply-To: <55262081.1070200@ifw-dresden.de>
References: <55262081.1070200@ifw-dresden.de>
Message-ID: <CAJ+Q1PVLpxjVitCv-KJKdqcxjgSb9zb6+vpd3Dj70jPdJAGvYw@mail.gmail.com>

What I found, was I couldn't yum install . yum update but I would
directly download the rpm with wget (with out a proxy as well !).
strange !



On 9 April 2015 at 16:47, Henri Wahl <h.wahl at ifw-dresden.de> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi list,
> does anybody know what is the matter with www1.ngtech.co.il? This is
> the source for RPM packages of squid but it seems to be dried up for
> some days now.
> Regards
>
> - --
> Henri Wahl
>
> IT Department
> Leibniz-Institut fuer Festkoerper- u.
> Werkstoffforschung Dresden
>
> tel: +49 (3 51) 46 59 - 797
> email: h.wahl at ifw-dresden.de
> https://www.ifw-dresden.de
>
> Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de
>
> DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de
>
> S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
> PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc
>
> IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
> VR Dresden Nr. 1369
> Vorstand: Prof. Dr. Manfred Hennecke, Kaufm?nnische Direktorin i. V.
> Dipl.-Kffr. Friederike Jaeger
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iEYEARECAAYFAlUmIIEACgkQnmb3Nh+6CUIEYACcDuQKyYq7FIqA5Kr+Ykbf90k4
> bh8AnjYEaXryCQ8q/Ki2JOXHDyjyYALk
> =HhFG
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From vdoctor at neuf.fr  Thu Apr  9 07:29:29 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 9 Apr 2015 00:29:29 -0700 (PDT)
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <552281E7.8060405@gmail.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com>
Message-ID: <1428564569076-4670656.post@n4.nabble.com>

Hi Yuri,

We have checked the sslproxy_capath, all certifs updated.
OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)

Additional point, the auto-signed certif is a 1024, could it be the problem
?
Maybe we need to use the ssl_crtd with the option "-b 1024"
what do you think ?

example of corrupted db:
*V	250402155004Z		7307E4A4E7FC6483C2B1D533821A7D2356DF1B88	unknown
/CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
V	250402155004Z		2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3	unknown
/CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
6
*

the squid crash when the index.txt becomes wrong... weird...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Apr  9 12:48:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 09 Apr 2015 18:48:16 +0600
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1428564569076-4670656.post@n4.nabble.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
Message-ID: <55267510.5020305@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think this is critical. What is native fs block size?

09.04.15 13:29, Stakres ?????:
> Hi Yuri,
>
> We have checked the sslproxy_capath, all certifs updated.
> OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)
>
> Additional point, the auto-signed certif is a 1024, could it be the
problem
> ?
> Maybe we need to use the ssl_crtd with the option "-b 1024"
> what do you think ?
>
> example of corrupted db:
> *V    250402155004Z        7307E4A4E7FC6483C2B1D533821A7D2356DF1B88   
unknown
> /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
> V    250402155004Z        2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3   
unknown
> /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
> 6
> *
>
> the squid crash when the index.txt becomes wrong... weird...
>
> Bye Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJnUQAAoJENNXIZxhPexGzxIH/j+jLbLNjGDPn1D12zsIv60r
nEO+wMAIcz6pVOS9x+Dy7fIlj+SXMZBmgywWiI0qhsgycVQstG+wTJige4WbtIG8
uQlLYMF1lBQb/ZCyimOfh6tGr0cYxckBFy+LQ6eC6KMyV6HtQ3Cik8zjVMJwtbKW
/QDQ8Nl5SR4sVxGOEWrWNMhw1gwR7cx5Wd1fxNnhCXJZu+dFtqFj9a/xuD4SyKwK
x+VbGdRaWveTwMj+SqgGnVr0KhyKaVeQwNIZ8ZJANN7ZUZM6y58FtrolnO3Akfic
KM4qVWKHcdb402up9y+ktGRFSJ1UEcCaw+TTZsv4eZBZMe+k/1qm7b4ndCEzzGQ=
=aiSN
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Thu Apr  9 12:51:45 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 09 Apr 2015 18:51:45 +0600
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1428564569076-4670656.post@n4.nabble.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
Message-ID: <552675E1.6090502@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think this is critical. What is native fs block size?

09.04.15 13:29, Stakres ?????:
> Hi Yuri,
>
> We have checked the sslproxy_capath, all certifs updated.
> OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)
>
> Additional point, the auto-signed certif is a 1024, could it be the
problem
> ?
> Maybe we need to use the ssl_crtd with the option "-b 1024"
> what do you think ?
>
> example of corrupted db:
> *V    250402155004Z        7307E4A4E7FC6483C2B1D533821A7D2356DF1B88   
unknown
> /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
> V    250402155004Z        2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3   
unknown
> /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
> 6
> *
>
> the squid crash when the index.txt becomes wrong... weird...
>
> Bye Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJnXhAAoJENNXIZxhPexGUp0IAK5Q7MlBDnZOL5jaCGqZo6L5
gbqIUwMfBf8DMD86b0INI/n1nM4OyzcZnHv2NdfatWwJ2qxDs5W0ciYUXbqxtTer
h45AV78FmaQxabk6rU0oxPOMMZz1o9tmGaaPtaGl3eykeA4Fv6jhZFA6C2dAiX87
ALgC7VPyLnwJNIljWwNqI+n2LzTJmsJEGu1xvEJT248XgKExlVTzB2qOLBri4Eaw
r3GNfSjO8lG4sjmW/H1UqaneigHOn0/CRDQk2/PRViWl15T/DndmoSWD+S3k+H0n
qj1iEGOfPsVvY86mljCaYCgNiVXsLYIsvoif7FZ1JYoSvtITeUihmew8soKlewI=
=D1nG
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Thu Apr  9 13:00:26 2015
From: vdoctor at neuf.fr (Vdoctor)
Date: Thu, 9 Apr 2015 15:00:26 +0200
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <552675E1.6090502@gmail.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
 <552675E1.6090502@gmail.com>
Message-ID: <029d01d072c5$25e5b800$71b12800$@fr>

Yury,

I checked the source code (3.4/3.5) ssl_crtd, the default size is 2048.
    -b fs_block_size     File system block size in bytes. Need for processing
                         natural size of certificate on disk. Default value is
                         2048 bytes."

/**
 \ingroup ssl_crtd
 * This is the external ssl_crtd process.
 */
int main(int argc, char *argv[])
{
    try {
        size_t max_db_size = 0;
        size_t fs_block_size = 2048;


But the crazy thing is the index.txt (last line) is wrong, not complete. It seems the tool writes/saves wrong data that's why it becomes corrupted and crash the Squid.

We have tried with a single ssl_crtd in the squid.conf, then one per worker, the same corruption.

Bye Fred

-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Yuri Voinov
Envoy? : jeudi 9 avril 2015 14:52
? : squid-users at lists.squid-cache.org
Objet : ***SPAM*** Re: [squid-users] Random SSL bump DB corruption


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think this is critical. What is native fs block size?

09.04.15 13:29, Stakres ?????:
> Hi Yuri,
>
> We have checked the sslproxy_capath, all certifs updated.
> OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)
>
> Additional point, the auto-signed certif is a 1024, could it be the
problem
> ?
> Maybe we need to use the ssl_crtd with the option "-b 1024"
> what do you think ?
>
> example of corrupted db:
> *V    250402155004Z        7307E4A4E7FC6483C2B1D533821A7D2356DF1B88   
unknown
> /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
> V    250402155004Z        2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3   
unknown
> /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
> 6
> *
>
> the squid crash when the index.txt becomes wrong... weird...
>
> Bye Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJnXhAAoJENNXIZxhPexGUp0IAK5Q7MlBDnZOL5jaCGqZo6L5
gbqIUwMfBf8DMD86b0INI/n1nM4OyzcZnHv2NdfatWwJ2qxDs5W0ciYUXbqxtTer
h45AV78FmaQxabk6rU0oxPOMMZz1o9tmGaaPtaGl3eykeA4Fv6jhZFA6C2dAiX87
ALgC7VPyLnwJNIljWwNqI+n2LzTJmsJEGu1xvEJT248XgKExlVTzB2qOLBri4Eaw
r3GNfSjO8lG4sjmW/H1UqaneigHOn0/CRDQk2/PRViWl15T/DndmoSWD+S3k+H0n
qj1iEGOfPsVvY86mljCaYCgNiVXsLYIsvoif7FZ1JYoSvtITeUihmew8soKlewI=
=D1nG
-----END PGP SIGNATURE-----

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Thu Apr  9 13:03:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 09 Apr 2015 19:03:53 +0600
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <029d01d072c5$25e5b800$71b12800$@fr>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>	<7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>	<55093D09.2070909@gmail.com>
 <1428322495420-4670630.post@n4.nabble.com>	<552281E7.8060405@gmail.com>
 <1428564569076-4670656.post@n4.nabble.com>	<552675E1.6090502@gmail.com>
 <029d01d072c5$25e5b800$71b12800$@fr>
Message-ID: <552678B9.3090309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
- From my experience, it may occur as a result of forming the fake
certificate zero length (in the case of the SQUID can not complete its
formation for any reason).

In turn, the formation of such a certificate occurs in particular due to
any error in the code of the SQUID characteristics or if server
certificate. In particular, one of these servers is iTunes.

09.04.15 19:00, Vdoctor ?????:
> Yury,
>
> I checked the source code (3.4/3.5) ssl_crtd, the default size is 2048.
>     -b fs_block_size     File system block size in bytes. Need for
processing
>                          natural size of certificate on disk. Default
value is
>                          2048 bytes."
>
> /**
>  \ingroup ssl_crtd
>  * This is the external ssl_crtd process.
>  */
> int main(int argc, char *argv[])
> {
>     try {
>         size_t max_db_size = 0;
>         size_t fs_block_size = 2048;
>
>
> But the crazy thing is the index.txt (last line) is wrong, not
complete. It seems the tool writes/saves wrong data that's why it
becomes corrupted and crash the Squid.
>
> We have tried with a single ssl_crtd in the squid.conf, then one per
worker, the same corruption.
>
> Bye Fred
>
> -----Message d'origine-----
> De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De
la part de Yuri Voinov
> Envoy? : jeudi 9 avril 2015 14:52
> ? : squid-users at lists.squid-cache.org
> Objet : ***SPAM*** Re: [squid-users] Random SSL bump DB corruption
>
>
> Don't think this is critical. What is native fs block size?
>
> 09.04.15 13:29, Stakres ?????:
> > Hi Yuri,
>
> > We have checked the sslproxy_capath, all certifs updated.
> > OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)
>
> > Additional point, the auto-signed certif is a 1024, could it be the
> problem
> > ?
> > Maybe we need to use the ssl_crtd with the option "-b 1024"
> > what do you think ?
>
> > example of corrupted db:
> > *V    250402155004Z        7307E4A4E7FC6483C2B1D533821A7D2356DF1B88  
> unknown
> > /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
> > V    250402155004Z        2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3  
> unknown
> > /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
> > 6
> > *
>
> > the squid crash when the index.txt becomes wrong... weird...
>
> > Bye Fred
>
>
>
> > --
> > View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
> > Sent from the Squid - Users mailing list archive at Nabble.com.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJni5AAoJENNXIZxhPexGTAkIAIx0ar6l6z84snTTem8XXZtD
oO/MnUvMb5FB+2IRp74dq7cO5KzlcZUeOvbbmsMsXR2CAraPqiLyTb3m3/eKqLS1
QdDRZZIuvV2GKyNizEzwwCV1W7QRjApbELc36rZC8fXVv5WArisDg3kk/Ycu3OeF
p0TBHhMNBvFKd+8Ve8xUqKQn3J6fYAYB8FHBzpssmfGaaGK7PeDmZ3LofeYHlqDP
eY7WKCzBQ7wOkezWJopBqkZH72OorLYHxOSanrNlbZ+5n2iO5wbuocm03F/QMJBc
uTN71irqNwHiqGd95ThQjSlhOXHvUSHEKssALUgmfHWEtIUy1PhLQvCksLm2510=
=ai9y
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150409/925ea307/attachment.htm>

From vdoctor at neuf.fr  Thu Apr  9 13:09:40 2015
From: vdoctor at neuf.fr (Vdoctor)
Date: Thu, 9 Apr 2015 15:09:40 +0200
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <552678B9.3090309@gmail.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
 <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com>
Message-ID: <02ad01d072c6$70601320$51203960$@fr>

Yuri,

 

So what?s next ?

Do you mean we must ?do-not-ssl-bump? wrong certificats ?

And if a certificate not yet identified is requested by an user it?ll crash the Squid ?

 

Any idea how to fix that issue ?

 

Thanks in advance.

Bye Fred

 

De : Yuri Voinov [mailto:yvoinov at gmail.com] 
Envoy? : jeudi 9 avril 2015 15:04
? : Vdoctor; squid-users at lists.squid-cache.org
Objet : Re: ***SPAM*** Re: [squid-users] Random SSL bump DB corruption

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
- From my experience, it may occur as a result of forming the fake certificate zero length (in the case of the SQUID can not complete its formation for any reason).

In turn, the formation of such a certificate occurs in particular due to any error in the code of the SQUID characteristics or if server certificate. In particular, one of these servers is iTunes.

09.04.15 19:00, Vdoctor ?????:
> Yury,



      >



      > I checked the source code (3.4/3.5) ssl_crtd, the default

      size is 2048.



      >     -b fs_block_size     File system block size in bytes.

      Need for processing



      >                          natural size of certificate on disk.

      Default value is



      >                          2048 bytes."



      >



      > /**



      >  \ingroup ssl_crtd



      >  * This is the external ssl_crtd process.



      >  */



      > int main(int argc, char *argv[])



      > {



      >     try {



      >         size_t max_db_size = 0;



      >         size_t fs_block_size = 2048;



      >



      >



      > But the crazy thing is the index.txt (last line) is wrong,

      not complete. It seems the tool writes/saves wrong data that's why

      it becomes corrupted and crash the Squid.



      >



      > We have tried with a single ssl_crtd in the squid.conf, then

      one per worker, the same corruption.



      >



      > Bye Fred



      >



      > -----Message d'origine-----



      > De : squid-users

      [mailto:squid-users-bounces at lists.squid-cache.org] De la part de

      Yuri Voinov



      > Envoy? : jeudi 9 avril 2015 14:52



      > ? : squid-users at lists.squid-cache.org



      > Objet : ***SPAM*** Re: [squid-users] Random SSL bump DB

      corruption



      >



      >



      > Don't think this is critical. What is native fs block size?



      >



      > 09.04.15 13:29, Stakres ?????:



      > > Hi Yuri,



      >



      > > We have checked the sslproxy_capath, all certifs

      updated.



      > > OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)



      >



      > > Additional point, the auto-signed certif is a 1024,

      could it be the



      > problem



      > > ?



      > > Maybe we need to use the ssl_crtd with the option "-b

      1024"



      > > what do you think ?



      >



      > > example of corrupted db:



      > > *V    250402155004Z       

      7307E4A4E7FC6483C2B1D533821A7D2356DF1B88   



      > unknown



      > >

      /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256



      > > V    250402155004Z       

      2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3   



      > unknown



      > > /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256



      > > 6



      > > *



      >



      > > the squid crash when the index.txt becomes wrong...

      weird...



      >



      > > Bye Fred



      >



      >



      >



      > > --



      > > View this message in context:



      >

http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html



      > > Sent from the Squid - Users mailing list archive at

      Nabble.com.



      > > _______________________________________________



      > > squid-users mailing list



      > > squid-users at lists.squid-cache.org



      > > http://lists.squid-cache.org/listinfo/squid-users



      >



      >



      > _______________________________________________



      > squid-users mailing list



      > squid-users at lists.squid-cache.org



      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJVJni5AAoJENNXIZxhPexGTAkIAIx0ar6l6z84snTTem8XXZtD 
oO/MnUvMb5FB+2IRp74dq7cO5KzlcZUeOvbbmsMsXR2CAraPqiLyTb3m3/eKqLS1 
QdDRZZIuvV2GKyNizEzwwCV1W7QRjApbELc36rZC8fXVv5WArisDg3kk/Ycu3OeF 
p0TBHhMNBvFKd+8Ve8xUqKQn3J6fYAYB8FHBzpssmfGaaGK7PeDmZ3LofeYHlqDP 
eY7WKCzBQ7wOkezWJopBqkZH72OorLYHxOSanrNlbZ+5n2iO5wbuocm03F/QMJBc 
uTN71irqNwHiqGd95ThQjSlhOXHvUSHEKssALUgmfHWEtIUy1PhLQvCksLm2510= 
=ai9y 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150409/3b5cf0ca/attachment.htm>

From yvoinov at gmail.com  Thu Apr  9 13:14:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 09 Apr 2015 19:14:46 +0600
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <02ad01d072c6$70601320$51203960$@fr>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>	<7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>	<55093D09.2070909@gmail.com>
 <1428322495420-4670630.post@n4.nabble.com>	<552281E7.8060405@gmail.com>
 <1428564569076-4670656.post@n4.nabble.com>	<552675E1.6090502@gmail.com>
 <029d01d072c5$25e5b800$71b12800$@fr>	<552678B9.3090309@gmail.com>
 <02ad01d072c6$70601320$51203960$@fr>
Message-ID: <55267B46.2010804@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think,first  you can try new stage-based SSL bump with 3.5.x. To do
that you must identify problem sites.

If there is no results, you can simple bypass problem sites without bump.

Whole server-first bump, on Squid 3.5.x especially, is not so good idea,
I think. Especially on provider-level proxies.

09.04.15 19:09, Vdoctor ?????:
> Yuri,
>
> 
>
> So what?s next ?
>
> Do you mean we must ?do-not-ssl-bump? wrong certificats ?
>
> And if a certificate not yet identified is requested by an user it?ll
crash the Squid ?
>
> 
>
> Any idea how to fix that issue ?
>
> 
>
> Thanks in advance.
>
> Bye Fred
>
> 
>
> De : Yuri Voinov [mailto:yvoinov at gmail.com]
> Envoy? : jeudi 9 avril 2015 15:04
> ? : Vdoctor; squid-users at lists.squid-cache.org
> Objet : Re: ***SPAM*** Re: [squid-users] Random SSL bump DB corruption
>
> 
>
>
> - From my experience, it may occur as a result of forming the fake
certificate zero length (in the case of the SQUID can not complete its
formation for any reason).
>
> In turn, the formation of such a certificate occurs in particular due
to any error in the code of the SQUID characteristics or if server
certificate. In particular, one of these servers is iTunes.
>
> 09.04.15 19:00, Vdoctor ?????:
> > Yury,
>
>
>
>
>
>
>
>       > I checked the source code (3.4/3.5) ssl_crtd, the default
>
>       size is 2048.
>
>
>
>       >     -b fs_block_size     File system block size in bytes.
>
>       Need for processing
>
>
>
>       >                          natural size of certificate on disk.
>
>       Default value is
>
>
>
>       >                          2048 bytes."
>
>
>
>
>
>
>
>       > /**
>
>
>
>       >  \ingroup ssl_crtd
>
>
>
>       >  * This is the external ssl_crtd process.
>
>
>
>       >  */
>
>
>
>       > int main(int argc, char *argv[])
>
>
>
>       > {
>
>
>
>       >     try {
>
>
>
>       >         size_t max_db_size = 0;
>
>
>
>       >         size_t fs_block_size = 2048;
>
>
>
>
>
>
>
>
>
>
>
>       > But the crazy thing is the index.txt (last line) is wrong,
>
>       not complete. It seems the tool writes/saves wrong data that's why
>
>       it becomes corrupted and crash the Squid.
>
>
>
>
>
>
>
>       > We have tried with a single ssl_crtd in the squid.conf, then
>
>       one per worker, the same corruption.
>
>
>
>
>
>
>
>       > Bye Fred
>
>
>
>
>
>
>
>       > -----Message d'origine-----
>
>
>
>       > De : squid-users
>
>       [mailto:squid-users-bounces at lists.squid-cache.org] De la part de
>
>       Yuri Voinov
>
>
>
>       > Envoy? : jeudi 9 avril 2015 14:52
>
>
>
>       > ? : squid-users at lists.squid-cache.org
>
>
>
>       > Objet : ***SPAM*** Re: [squid-users] Random SSL bump DB
>
>       corruption
>
>
>
>
>
>
>
>
>
>
>
>       > Don't think this is critical. What is native fs block size?
>
>
>
>
>
>
>
>       > 09.04.15 13:29, Stakres ?????:
>
>
>
>       > > Hi Yuri,
>
>
>
>
>
>
>
>       > > We have checked the sslproxy_capath, all certifs
>
>       updated.
>
>
>
>       > > OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013 (Debian 7.8)
>
>
>
>
>
>
>
>       > > Additional point, the auto-signed certif is a 1024,
>
>       could it be the
>
>
>
>       > problem
>
>
>
>       > > ?
>
>
>
>       > > Maybe we need to use the ssl_crtd with the option "-b
>
>       1024"
>
>
>
>       > > what do you think ?
>
>
>
>
>
>
>
>       > > example of corrupted db:
>
>
>
>       > > *V    250402155004Z      
>
>       7307E4A4E7FC6483C2B1D533821A7D2356DF1B88  
>
>
>
>       > unknown
>
>
>
>       > >
>
>      
/CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256
>
>
>
>       > > V    250402155004Z      
>
>       2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3  
>
>
>
>       > unknown
>
>
>
>       > > /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256
>
>
>
>       > > 6
>
>
>
>       > > *
>
>
>
>
>
>
>
>       > > the squid crash when the index.txt becomes wrong...
>
>       weird...
>
>
>
>
>
>
>
>       > > Bye Fred
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > > --
>
>
>
>       > > View this message in context:
>
>
>
>
>
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html
>
>
>
>       > > Sent from the Squid - Users mailing list archive at
>
>       Nabble.com.
>
>
>
>       > > _______________________________________________
>
>
>
>       > > squid-users mailing list
>
>
>
>       > > squid-users at lists.squid-cache.org
>
>
>
>       > > http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJntGAAoJENNXIZxhPexGu5cIAK17uOKYtdAvuZsGUFEd43pS
eSpzm5mjO9HqIejFis55Ahz5xSHiZLBb++yb/+oV5I/m0CoEOO7Y17qtWAjO56Ni
D/QRCmdCudrb4uoXWu0AY/+qwECJmAAsAYkigepVS+6u/kw2R1aU1oXt816EgFhq
XLyh3/92OvArDbn7HxAAMZRQ5Wqdgc7pdI8Bah6iElMHQrcd5FEuK/yyfoxUTdWf
F4HQa0EFC4Z3xY1AYfTskTcuVIEyZt9N9s5na/b9TcxktxzbPnTon2yg6CtohAqM
v2u28VIpToDETq8N8qv7DxQtbGz9cXuGsBj6HDYIUZB8NzEA5ETc+BOzG+DxOPQ=
=rC2l
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150409/037c87c2/attachment.htm>

From vdoctor at neuf.fr  Thu Apr  9 14:14:35 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 9 Apr 2015 07:14:35 -0700 (PDT)
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <55267B46.2010804@gmail.com>
References: <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
 <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com>
Message-ID: <02d601d072d1$390562d0$ab102870$@fr>

Yuri,

 

We?re trying that :

-          Tproxy

-          ssl_bump bump all

does not work.

 

We have followed the squid wiki regarding iptables rules, sysctl, etc?

Instead ?ssl_bump bump all?, if we use ?ssl_bump server-first all? , it works, the https is decrypted.

 

So is the tproxy compatible with the new squid 3.5.x ssl_bump options ?

 

Bye Fred

 

De : Yuri Voinov [via Squid Web Proxy Cache] [mailto:ml-node+s1019090n4670662h55 at n4.nabble.com] 
Envoy? : jeudi 9 avril 2015 15:03
? : Stakres
Objet : Re: ***SPAM*** Re: Random SSL bump DB corruption

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
I think,first  you can try new stage-based SSL bump with 3.5.x. To do that you must identify problem sites.

If there is no results, you can simple bypass problem sites without bump.

Whole server-first bump, on Squid 3.5.x especially, is not so good idea, I think. Especially on provider-level proxies.

09.04.15 19:09, Vdoctor ?????:
> Yuri,



      >



      >  



      >



      > So what?s next ?



      >



      > Do you mean we must ?do-not-ssl-bump? wrong certificats ?



      >



      > And if a certificate not yet identified is requested by an

      user it?ll crash the Squid ?



      >



      >  



      >



      > Any idea how to fix that issue ?



      >



      >  



      >



      > Thanks in advance.



      >



      > Bye Fred



      >



      >  



      >



      > De : Yuri Voinov [[hidden email]] 



      > Envoy? : jeudi 9 avril 2015 15:04



      > ? : Vdoctor; [hidden email]



      > Objet : Re: ***SPAM*** Re: [squid-users] Random SSL bump DB

      corruption



      >



      >  



      >



      >



      > - From my experience, it may occur as a result of forming the

      fake certificate zero length (in the case of the SQUID can not

      complete its formation for any reason).



      >



      > In turn, the formation of such a certificate occurs in

      particular due to any error in the code of the SQUID

      characteristics or if server certificate. In particular, one of

      these servers is iTunes.



      >



      > 09.04.15 19:00, Vdoctor ?????:



      > > Yury,



      >



      >



      >



      >



      >



      >



      >



      >       > I checked the source code (3.4/3.5) ssl_crtd, the

      default



      >



      >       size is 2048.



      >



      >



      >



      >       >     -b fs_block_size     File system block size in

      bytes.



      >



      >       Need for processing



      >



      >



      >



      >       >                          natural size of

      certificate on disk.



      >



      >       Default value is



      >



      >



      >



      >       >                          2048 bytes."



      >



      >



      >



      >



      >



      >



      >



      >       > /**



      >



      >



      >



      >       >  \ingroup ssl_crtd



      >



      >



      >



      >       >  * This is the external ssl_crtd process.



      >



      >



      >



      >       >  */



      >



      >



      >



      >       > int main(int argc, char *argv[])



      >



      >



      >



      >       > {



      >



      >



      >



      >       >     try {



      >



      >



      >



      >       >         size_t max_db_size = 0;



      >



      >



      >



      >       >         size_t fs_block_size = 2048;



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > But the crazy thing is the index.txt (last line)

      is wrong,



      >



      >       not complete. It seems the tool writes/saves wrong data

      that's why



      >



      >       it becomes corrupted and crash the Squid.



      >



      >



      >



      >



      >



      >



      >



      >       > We have tried with a single ssl_crtd in the

      squid.conf, then



      >



      >       one per worker, the same corruption.



      >



      >



      >



      >



      >



      >



      >



      >       > Bye Fred



      >



      >



      >



      >



      >



      >



      >



      >       > -----Message d'origine-----



      >



      >



      >



      >       > De : squid-users



      >



      >       [[hidden email]] De

      la part de



      >



      >       Yuri Voinov



      >



      >



      >



      >       > Envoy? : jeudi 9 avril 2015 14:52



      >



      >



      >



      >       > ? : [hidden email]



      >



      >



      >



      >       > Objet : ***SPAM*** Re: [squid-users] Random SSL

      bump DB



      >



      >       corruption



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > Don't think this is critical. What is native fs

      block size?



      >



      >



      >



      >



      >



      >



      >



      >       > 09.04.15 13:29, Stakres ?????:



      >



      >



      >



      >       > > Hi Yuri,



      >



      >



      >



      >



      >



      >



      >



      >       > > We have checked the sslproxy_capath, all

      certifs



      >



      >       updated.



      >



      >



      >



      >       > > OpenSSL is: OpenSSL 1.0.1e 11 Feb 2013

      (Debian 7.8)



      >



      >



      >



      >



      >



      >



      >



      >       > > Additional point, the auto-signed certif is a

      1024,



      >



      >       could it be the



      >



      >



      >



      >       > problem



      >



      >



      >



      >       > > ?



      >



      >



      >



      >       > > Maybe we need to use the ssl_crtd with the

      option "-b



      >



      >       1024"



      >



      >



      >



      >       > > what do you think ?



      >



      >



      >



      >



      >



      >



      >



      >       > > example of corrupted db:



      >



      >



      >



      >       > > *V    250402155004Z       



      >



      >       7307E4A4E7FC6483C2B1D533821A7D2356DF1B88   



      >



      >



      >



      >       > unknown



      >



      >



      >



      >       > >



      >



      >      

      /CN=r2---sn-q4f7sn7z.googlevideo.com+Sign=signTrusted+SignHash=SHA256



      >



      >



      >



      >       > > V    250402155004Z       



      >



      >       2D1FC87E26AC4D8AB1E6F3B45E2C69EB36C7F8D3   



      >



      >



      >



      >       > unknown



      >



      >



      >



      >       > >

      /CN=seal.verisign.com+Sign=signTrusted+SignHash=SHA256



      >



      >



      >



      >       > > 6



      >



      >



      >



      >       > > *



      >



      >



      >



      >



      >



      >



      >



      >       > > the squid crash when the index.txt becomes

      wrong...



      >



      >       weird...



      >



      >



      >



      >



      >



      >



      >



      >       > > Bye Fred



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > > --



      >



      >



      >



      >       > > View this message in context:



      >



      >



      >



      >



      >



      >

http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670656.html



      >



      >



      >



      >       > > Sent from the Squid - Users mailing list

      archive at



      >



      >       Nabble.com.



      >



      >



      >



      >       > >

      _______________________________________________



      >



      >



      >



      >       > > squid-users mailing list



      >



      >



      >



      >       > > [hidden email]



      >



      >



      >



      >       > >

      http://lists.squid-cache.org/listinfo/squid-users



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >



      >       > _______________________________________________



      >



      >



      >



      >       > squid-users mailing list



      >



      >



      >



      >       > [hidden email]



      >



      >



      >



      >       > http://lists.squid-cache.org/listinfo/squid-users



      >



      >



      >

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJVJntGAAoJENNXIZxhPexGu5cIAK17uOKYtdAvuZsGUFEd43pS 
eSpzm5mjO9HqIejFis55Ahz5xSHiZLBb++yb/+oV5I/m0CoEOO7Y17qtWAjO56Ni 
D/QRCmdCudrb4uoXWu0AY/+qwECJmAAsAYkigepVS+6u/kw2R1aU1oXt816EgFhq 
XLyh3/92OvArDbn7HxAAMZRQ5Wqdgc7pdI8Bah6iElMHQrcd5FEuK/yyfoxUTdWf 
F4HQa0EFC4Z3xY1AYfTskTcuVIEyZt9N9s5na/b9TcxktxzbPnTon2yg6CtohAqM 
v2u28VIpToDETq8N8qv7DxQtbGz9cXuGsBj6HDYIUZB8NzEA5ETc+BOzG+DxOPQ= 
=rC2l 
-----END PGP SIGNATURE----- 


_______________________________________________ 
squid-users mailing list 
[hidden email] 
http://lists.squid-cache.org/listinfo/squid-users



  _____  

If you reply to this email, your message will be added to the discussion below:

http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670662.html 

To start a new topic under Squid - Users, email ml-node+s1019090n1019091h54 at n4.nabble.com 
To unsubscribe from Squid Web Proxy Cache, click here <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1019090&code=dmRvY3RvckBuZXVmLmZyfDEwMTkwOTB8OTE5NjEzNjUz> .
 <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> NAML 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670663.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Apr 10 02:03:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Apr 2015 14:03:51 +1200
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <02d601d072d1$390562d0$ab102870$@fr>
References: <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
 <55093D09.2070909@gmail.com> <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
 <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com> <02d601d072d1$390562d0$ab102870$@fr>
Message-ID: <55272F87.40607@treenet.co.nz>

On 10/04/2015 2:14 a.m., Stakres wrote:
> Yuri,
> 
>  
> 
> We?re trying that :
> 
> -          Tproxy
> 
> -          ssl_bump bump all
> 
> does not work.
> 
>  
> 
> We have followed the squid wiki regarding iptables rules, sysctl, etc?
> 
> Instead ?ssl_bump bump all?, if we use ?ssl_bump server-first all? , it works, the https is decrypted.
> 
>  
> 
> So is the tproxy compatible with the new squid 3.5.x ssl_bump options ?

With intercept / tproxy you may need to peek first to get the
ClientHello details. Those are needed not just for any ssl_bump
directive ACLs, but also for generating the correct ClientHello to be
delivered to the server. Without it Squid only has the raw-IP details
from TCP to work with.

Amos


From fmeini at esseweb.eu  Fri Apr 10 08:48:09 2015
From: fmeini at esseweb.eu (Fiorenza Meini)
Date: Fri, 10 Apr 2015 10:48:09 +0200
Subject: [squid-users] ACL to block installation program
Message-ID: <55278E49.4060106@esseweb.eu>

Hi,
is there a way to filter and block update programs which come from 
Internet, for example java update or windows update , withouth using the 
url of the web site, but working with  header/mime types ?

Thanks and regards

Fiorenza Meini
-- 
Spazio Web S.r.l.
V. Dante, 10
13900 Biella
Tel.: +39 015 2431982
Fax.: +39 015 2522600
Numero d'Iscrizione al Registro Imprese presso CCIAA Biella, Cod.Fisc.e 
P.Iva: 02414430021
Iscriz. REA: BI - 188936 Cap. Soc.: ?. 30.000 i.v.


From yvoinov at gmail.com  Fri Apr 10 09:00:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 15:00:25 +0600
Subject: [squid-users] ACL to block installation program
In-Reply-To: <55278E49.4060106@esseweb.eu>
References: <55278E49.4060106@esseweb.eu>
Message-ID: <55279129.9000508@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes


10.04.15 14:48, Fiorenza Meini ?????:
> Hi,
> is there a way to filter and block update programs which come from
Internet, for example java update or windows update , withouth using the
url of the web site, but working with  header/mime types ?
>
> Thanks and regards
>
> Fiorenza Meini

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJ5EoAAoJENNXIZxhPexGJR4H/2z4EeMqAzLUda+uvtYwseCY
xxUwFOMw6Pj5/5RYaqoLjio07RxbOsSS3Tw4NsKlbtGoBD5NZ1haKKak6dn4iGg2
fd0KlkqxG7XgKAtFJhS8+Gk3lMPRENOX6gM09GmY0Ua4Q6nvtH75xvBXqNhG/wut
eS8AXINbwTDi6ciWbtUXLgyeAhQwE9C++8zvTq3UPHv359H8s7HyA6CT6QFvhJtS
pHEqlzbt8daU+wyiiHA9k/DUfjynjHJJgYGUtOiTHrScWFymtqnqo9sKvfR4UbFb
HNi3gfRPtV2pw0igof69m8AGKrihEQqt6YUxgDtHjsogvaFZFc8MIyui64zC6Sg=
=uMVo
-----END PGP SIGNATURE-----



From tomtux007 at gmail.com  Fri Apr 10 09:06:07 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Fri, 10 Apr 2015 11:06:07 +0200
Subject: [squid-users] Stats about used acl/http_access-directives?
Message-ID: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>

Hi

Is there a way to get stats about matching http_access/acl-directives
in recent squid-versions?

In a way like "http_access allow localhost" matched 1 time, 2 times,
n-times.I would be interesting, to see, which
acl/http_access-directives are used and which ones are configured but
not used.

I searched within squidclient, but didn't found something like this.

Kind regards,
Tom


From yvoinov at gmail.com  Fri Apr 10 09:08:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 15:08:28 +0600
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
Message-ID: <5527930C.6080104@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/SquidFaq/SquidAcl
http://i.imgur.com/1Q87Q2i.png

10.04.15 15:06, Tom Tom ?????:
> Hi
>
> Is there a way to get stats about matching http_access/acl-directives
> in recent squid-versions?
>
> In a way like "http_access allow localhost" matched 1 time, 2 times,
> n-times.I would be interesting, to see, which
> acl/http_access-directives are used and which ones are configured but
> not used.
>
> I searched within squidclient, but didn't found something like this.
>
> Kind regards,
> Tom
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJ5MMAAoJENNXIZxhPexGDUMH/0H5lV24SjqarnEQMk4X46RF
fEpdi4AcG9kjOwLnVl1NtwGe70eESeQlEvTIQLwX8dOlesMeK2fde5CrRATZquPG
TfoFK5Bt2iVVSGhUKQzNP7fpva+QWAN4ckIG80wZxoA23sgiZCmbjlON8U879LPY
n4lBna/iK9TiHrJlAOrf4sa8h/L/XNHyDr4fEwmx1HULQHB0kEGuJ3ek6MKMWp6H
37t3rzn2vLq6W6BM91vjB17YrTlcvHxPSgEpRk0M+jOcjhOXf8JgrwkKLEwlYeRQ
C2h58502Qt7m54qRpmKrkoJ0vX33j/sCsf4+6o0cta7NXmA1T1l86nW7rKLOyEU=
=Sxjk
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Fri Apr 10 09:09:24 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 15:09:24 +0600
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
Message-ID: <55279344.7010107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And this one:

https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/

10.04.15 15:06, Tom Tom ?????:
> Hi
>
> Is there a way to get stats about matching http_access/acl-directives
> in recent squid-versions?
>
> In a way like "http_access allow localhost" matched 1 time, 2 times,
> n-times.I would be interesting, to see, which
> acl/http_access-directives are used and which ones are configured but
> not used.
>
> I searched within squidclient, but didn't found something like this.
>
> Kind regards,
> Tom
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJ5NEAAoJENNXIZxhPexGim0IAIrB79MPat9tqfxJebpKdwd+
Lv88+vZ+irzt47R/H4sBEYEd7Guwuejse7V3LKygo+W3cFl6iJIbDj/SgtGxpReS
uKZsKmnBUJMyrzp5mRbRaYCV1plopQuTqYWqssKmXPvzwapbAUJlpiRmFvqi6Nxv
cb7h44mPFlpnKZtUvp7lt97cCR7OUDD3nJix35lyfUcdK5IlRV5sS8wDeb6zxcMh
Q7ZdgiGulCCMyDwA6biQIbqDlqSrMw2cyJ/7YRuMMqmrrdgBUHIB1Cj8SrNdiF7h
CZxPxmw0jbivRZR60wfLDuv696+nOpDsn9sTe8IKZcTXrqIQU50ZSTkF7Njy1yc=
=4sG+
-----END PGP SIGNATURE-----



From tomtux007 at gmail.com  Fri Apr 10 12:26:54 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Fri, 10 Apr 2015 14:26:54 +0200
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <55279344.7010107@gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
 <55279344.7010107@gmail.com>
Message-ID: <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>

Thank you.

This gives too much output and the cache.log will grow too fast. Other
hints for better tracking the matching/used http_access and
acl-directives?

On Fri, Apr 10, 2015 at 11:09 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> And this one:
>
> https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/
>
> 10.04.15 15:06, Tom Tom ?????:
>> Hi
>>
>> Is there a way to get stats about matching http_access/acl-directives
>> in recent squid-versions?
>>
>> In a way like "http_access allow localhost" matched 1 time, 2 times,
>> n-times.I would be interesting, to see, which
>> acl/http_access-directives are used and which ones are configured but
>> not used.
>>
>> I searched within squidclient, but didn't found something like this.
>>
>> Kind regards,
>> Tom
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVJ5NEAAoJENNXIZxhPexGim0IAIrB79MPat9tqfxJebpKdwd+
> Lv88+vZ+irzt47R/H4sBEYEd7Guwuejse7V3LKygo+W3cFl6iJIbDj/SgtGxpReS
> uKZsKmnBUJMyrzp5mRbRaYCV1plopQuTqYWqssKmXPvzwapbAUJlpiRmFvqi6Nxv
> cb7h44mPFlpnKZtUvp7lt97cCR7OUDD3nJix35lyfUcdK5IlRV5sS8wDeb6zxcMh
> Q7ZdgiGulCCMyDwA6biQIbqDlqSrMw2cyJ/7YRuMMqmrrdgBUHIB1Cj8SrNdiF7h
> CZxPxmw0jbivRZR60wfLDuv696+nOpDsn9sTe8IKZcTXrqIQU50ZSTkF7Njy1yc=
> =4sG+
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Fri Apr 10 12:44:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 18:44:25 +0600
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>	<55279344.7010107@gmail.com>
 <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>
Message-ID: <5527C5A9.6060006@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Google tells us, there is no other way.

10.04.15 18:26, Tom Tom ?????:
> Thank you.
>
> This gives too much output and the cache.log will grow too fast. Other
> hints for better tracking the matching/used http_access and
> acl-directives?
>
> On Fri, Apr 10, 2015 at 11:09 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
> And this one:
>
> https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/
>
> 10.04.15 15:06, Tom Tom ?????:
> >>> Hi
> >>>
> >>> Is there a way to get stats about matching http_access/acl-directives
> >>> in recent squid-versions?
> >>>
> >>> In a way like "http_access allow localhost" matched 1 time, 2 times,
> >>> n-times.I would be interesting, to see, which
> >>> acl/http_access-directives are used and which ones are configured but
> >>> not used.
> >>>
> >>> I searched within squidclient, but didn't found something like this.
> >>>
> >>> Kind regards,
> >>> Tom
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJ8WoAAoJENNXIZxhPexGJkUIAJtILEqHMhKWBiZTXBAZ7RdB
U7pFz8tpmOpenKxd2L+qcoJTiq0jpyyvRTAeepkIKGAkcjVxbD2k6cDm1ovm93cD
pyvwAMvVQSV/PfXXPl105J2wGT12Zz2MplXw61AYwn97dXPgVcvti1Mb44QB55Iq
ufPZeVAWqT0oJQcnx/DO/obzlPilH9i2dolt32r0cQDXCpzXF5F24AFUBWl4lrHr
NhLTZ/CVuSARDQwj1o50tz19BJXOknXPZkkN+CGLntqqxpvcFaJXrVTmr0nYwbOz
k6LRLauL5ZaFfrB7FzxmJ/wobGKm9aGSbxYfLJ+yF3rUm543ca4lKEnMI0bbPlc=
=EjxJ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/f7e61765/attachment.htm>

From fmeini at esseweb.eu  Fri Apr 10 13:51:19 2015
From: fmeini at esseweb.eu (Fiorenza Meini)
Date: Fri, 10 Apr 2015 15:51:19 +0200
Subject: [squid-users] Client delay pools ...doesn't work
Message-ID: <5527D557.8070407@esseweb.eu>

Hi,
I'm testing on a 3.4 squid release the client_delay_poolfunctionality.
It seems that isn't working: on my browser I receive the error that 
proxy isn't reachable, and in log file I can't see nothing useful.

Has anyone configured this functionality successfully ?

Regards

Fiorenza Meini
-- 
Spazio Web S.r.l.
V. Dante, 10
13900 Biella
Tel.: +39 015 2431982
Fax.: +39 015 2522600
Numero d'Iscrizione al Registro Imprese presso CCIAA Biella, Cod.Fisc.e 
P.Iva: 02414430021
Iscriz. REA: BI - 188936 Cap. Soc.: ?. 30.000 i.v.


From tomtux007 at gmail.com  Fri Apr 10 13:52:24 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Fri, 10 Apr 2015 15:52:24 +0200
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <5527C5A9.6060006@gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
 <55279344.7010107@gmail.com>
 <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>
 <5527C5A9.6060006@gmail.com>
Message-ID: <CACLJR+OrvAbZtCjW5St8tQhfpORpYHGsA046qv4MU-yo56M8bg@mail.gmail.com>

Something planned in this direction in future releases?

On Fri, Apr 10, 2015 at 2:44 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Google tells us, there is no other way.
>
> 10.04.15 18:26, Tom Tom ?????:
>> Thank you.
>>
>> This gives too much output and the cache.log will grow too fast. Other
>> hints for better tracking the matching/used http_access and
>> acl-directives?
>>
>> On Fri, Apr 10, 2015 at 11:09 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>>
>> And this one:
>>
>> https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/
>>
>> 10.04.15 15:06, Tom Tom ?????:
>> >>> Hi
>> >>>
>> >>> Is there a way to get stats about matching http_access/acl-directives
>> >>> in recent squid-versions?
>> >>>
>> >>> In a way like "http_access allow localhost" matched 1 time, 2 times,
>> >>> n-times.I would be interesting, to see, which
>> >>> acl/http_access-directives are used and which ones are configured but
>> >>> not used.
>> >>>
>> >>> I searched within squidclient, but didn't found something like this.
>> >>>
>> >>> Kind regards,
>> >>> Tom
>> >>> _______________________________________________
>> >>> squid-users mailing list
>> >>> squid-users at lists.squid-cache.org
>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVJ8WoAAoJENNXIZxhPexGJkUIAJtILEqHMhKWBiZTXBAZ7RdB
> U7pFz8tpmOpenKxd2L+qcoJTiq0jpyyvRTAeepkIKGAkcjVxbD2k6cDm1ovm93cD
> pyvwAMvVQSV/PfXXPl105J2wGT12Zz2MplXw61AYwn97dXPgVcvti1Mb44QB55Iq
> ufPZeVAWqT0oJQcnx/DO/obzlPilH9i2dolt32r0cQDXCpzXF5F24AFUBWl4lrHr
> NhLTZ/CVuSARDQwj1o50tz19BJXOknXPZkkN+CGLntqqxpvcFaJXrVTmr0nYwbOz
> k6LRLauL5ZaFfrB7FzxmJ/wobGKm9aGSbxYfLJ+yF3rUm543ca4lKEnMI0bbPlc=
> =EjxJ
> -----END PGP SIGNATURE-----
>


From ashish.patil at shreshtait.com  Fri Apr 10 14:22:37 2015
From: ashish.patil at shreshtait.com (Ashish Patil)
Date: Fri, 10 Apr 2015 19:52:37 +0530
Subject: [squid-users] Peek and Splice for websites using HSTS
Message-ID: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>

Hello,

I am trying to set up Peek and Splice using Squid 3.5.3. I'm facing issues
setting it up for website that have HSTS enabled, like google.com and
twitter.com.

My squid.conf is:
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl/myCA.pem
acl step3 at_step SslBump3
acl sslBumpAllowedDstDomain dstdomain google.co.in
ssl_bump peek step3 all
ssl_bump splice sslBumpAllowedDstDomain
ssl_bump bump all


The output of access.log is:
1428674512.281    511 192.168.3.31 TCP_MISS/301 634 GET http://google.co.in/
- ORIGINAL_DST/173.194.117.23 text/html
1428674512.703    348 192.168.3.31 TCP_MISS/302 1106 GET
http://www.google.co.in/ - ORIGINAL_DST/173.194.117.24 text/html
1428674512.706      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.24:443
- HIER_NONE/- -
1428674512.711      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.24:443
- HIER_NONE/- -
1428674515.883      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674515.956      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674515.965      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674516.006      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674526.310      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674526.327      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674526.335      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -
1428674526.411      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
- HIER_NONE/- -


Any input would be welcome.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/2fd15965/attachment.htm>

From yvoinov at gmail.com  Fri Apr 10 16:32:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 22:32:54 +0600
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <CACLJR+OrvAbZtCjW5St8tQhfpORpYHGsA046qv4MU-yo56M8bg@mail.gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>	<55279344.7010107@gmail.com>	<CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>	<5527C5A9.6060006@gmail.com>
 <CACLJR+OrvAbZtCjW5St8tQhfpORpYHGsA046qv4MU-yo56M8bg@mail.gmail.com>
Message-ID: <5527FB36.2000607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
For what?

Don't think so.

10.04.15 19:52, Tom Tom ?????:
> Something planned in this direction in future releases?
>
> On Fri, Apr 10, 2015 at 2:44 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
> Google tells us, there is no other way.
>
> 10.04.15 18:26, Tom Tom ?????:
> >>> Thank you.
> >>>
> >>> This gives too much output and the cache.log will grow too fast. Other
> >>> hints for better tracking the matching/used http_access and
> >>> acl-directives?
> >>>
> >>> On Fri, Apr 10, 2015 at 11:09 AM, Yuri Voinov <yvoinov at gmail.com>
wrote:
> >>>>
> >>> And this one:
> >>>
> >>> https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/
> >>>
> >>> 10.04.15 15:06, Tom Tom ?????:
> >>>>>> Hi
> >>>>>>
> >>>>>> Is there a way to get stats about matching
http_access/acl-directives
> >>>>>> in recent squid-versions?
> >>>>>>
> >>>>>> In a way like "http_access allow localhost" matched 1 time, 2
times,
> >>>>>> n-times.I would be interesting, to see, which
> >>>>>> acl/http_access-directives are used and which ones are
configured but
> >>>>>> not used.
> >>>>>>
> >>>>>> I searched within squidclient, but didn't found something like
this.
> >>>>>>
> >>>>>> Kind regards,
> >>>>>> Tom
> >>>>>> _______________________________________________
> >>>>>> squid-users mailing list
> >>>>>> squid-users at lists.squid-cache.org
> >>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
>
>>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVJ/s1AAoJENNXIZxhPexGOqcIAIx8xxxkRIIkuBNVf2Ds6nWK
DDcJifdHWtjftzF7CmIrznQc/t8BoxsC999Py+tTWeseHnPme9E/WkkphN00rCQ2
+xYwTzp+zKLuPShTylGO34KxZjeOXMikvJF3BTTyimsRil3e/YotisloY/rvkF/l
GAFGFk9ff0ce4yD2Ph5LSHeyjMLqRBpScNglQ7DGqFtVJgYBvXguLW278dR8owG7
JTtMGKH0yvHuCnqdTMJT0+tSCG1neNVVc0YJIUVe8cRVBgc4w4IKYxsrzvI+8cwc
wTIckvIg5fie4/RLFdAEQvstK2eumU2T5n/BviXoIIp5AxHym2lvYtxA+/vUItA=
=orxN
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/57e87e23/attachment.htm>

From yvoinov at gmail.com  Fri Apr 10 17:03:01 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 23:03:01 +0600
Subject: [squid-users] Peek and Splice for websites using HSTS
In-Reply-To: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
References: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
Message-ID: <55280245.7090106@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It is generally at least on anything work? Peek stage 3 - the original!

10.04.15 20:22, Ashish Patil ?????:
> Hello,
>
> I am trying to set up Peek and Splice using Squid 3.5.3. I'm facing issues
> setting it up for website that have HSTS enabled, like google.com and
> twitter.com.
>
> My squid.conf is:
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl/myCA.pem
> acl step3 at_step SslBump3
> acl sslBumpAllowedDstDomain dstdomain google.co.in
> ssl_bump peek step3 all
> ssl_bump splice sslBumpAllowedDstDomain
> ssl_bump bump all
>
>
> The output of access.log is:
> 1428674512.281    511 192.168.3.31 TCP_MISS/301 634 GET
http://google.co.in/
> - ORIGINAL_DST/173.194.117.23 text/html
> 1428674512.703    348 192.168.3.31 TCP_MISS/302 1106 GET
> http://www.google.co.in/ - ORIGINAL_DST/173.194.117.24 text/html
> 1428674512.706      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.24:443
> - HIER_NONE/- -
> 1428674512.711      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.24:443
> - HIER_NONE/- -
> 1428674515.883      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.956      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.965      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674516.006      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.310      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.327      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.335      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.411      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
>
>
> Any input would be welcome.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVKAJEAAoJENNXIZxhPexGTegIAMRDbEbF1Ku4OAJVxrv0xnzt
6mn4vN1hHuEPfd0d5XwpyGQpEJmQ4AErZM6M1CY6Pl5GTexEEwQa9aGptV1g4dnN
x34Xi/08RjMy3bBufFInXPjPYTFJhSv7e91jolGa4xaLyoZLegxFrL4F0Gl2eC1P
6khXVxw/1uzNZspEKdxSVSskBRj6dA/PZmCEkb9A9Lf/zsqBY/gQmXkbnpDIkNqy
t2RqitKsJJUHhf9gmK8lJjc+3EGmGTzZ5dQ/nQQVYknMbyrOAu9gwKukyOZx/Wwm
f6EoBW+l5cvUl1bwsAESWYbVWuqBGdee2VNWHngYwe7afCfOLa8ja9/N4ZU6hMM=
=6Fs0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/25cff66c/attachment.htm>

From yvoinov at gmail.com  Fri Apr 10 17:05:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 23:05:09 +0600
Subject: [squid-users] Peek and Splice for websites using HSTS
In-Reply-To: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
References: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
Message-ID: <552802C5.4010503@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
BTW, man, splice in most cases means "no bump".

Why do you expect that will happen bumping, if your URL is in splice ACL?

10.04.15 20:22, Ashish Patil ?????:
> Hello,
>
> I am trying to set up Peek and Splice using Squid 3.5.3. I'm facing issues
> setting it up for website that have HSTS enabled, like google.com and
> twitter.com.
>
> My squid.conf is:
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl/myCA.pem
> acl step3 at_step SslBump3
> acl sslBumpAllowedDstDomain dstdomain google.co.in
> ssl_bump peek step3 all
> ssl_bump splice sslBumpAllowedDstDomain
> ssl_bump bump all
>
>
> The output of access.log is:
> 1428674512.281    511 192.168.3.31 TCP_MISS/301 634 GET
http://google.co.in/
> - ORIGINAL_DST/173.194.117.23 text/html
> 1428674512.703    348 192.168.3.31 TCP_MISS/302 1106 GET
> http://www.google.co.in/ - ORIGINAL_DST/173.194.117.24 text/html
> 1428674512.706      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.24:443
> - HIER_NONE/- -
> 1428674512.711      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.24:443
> - HIER_NONE/- -
> 1428674515.883      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.956      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.965      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674516.006      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.310      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.327      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.335      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.411      0 192.168.3.31 TAG_NONE/200 0 CONNECT
173.194.117.22:443
> - HIER_NONE/- -
>
>
> Any input would be welcome.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVKALFAAoJENNXIZxhPexGAwAH/jTH4eX6W1RDp12zwGC4Fu8P
68eLUveFGb+pjtlML/fvBBmihp6QOi1sU/CswbqaowFw/A/dXLmZhdo/nZI474up
iYpiqZZ2nH2muvXjSU746p6LcjGAv0bHqXkXHQpDqfXnob7v1wJdNYVnthWw+t3Y
sCxBlTetuvyTO7iCYGZ7bB9oVspb7q4Vd4t7T079KCT2CkuyBOZrcB7IWAqigYoZ
BnJef33wZ45YCTzRmsbVpUZMZgFsNCtkTuVAXOfBewlwBORxoZ/sIXsecDTKRrJ6
0QntexRv4f+CBZiXJJvFdyA4U57yw5FHDgLcEFIPdfhW7xnRPxrgU3t9WXclDkc=
=mGMV
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/3081e010/attachment.htm>

From bpk678 at gmail.com  Fri Apr 10 17:32:12 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Fri, 10 Apr 2015 13:32:12 -0400
Subject: [squid-users] ACL to block installation program
In-Reply-To: <55279129.9000508@gmail.com>
References: <55278E49.4060106@esseweb.eu>
	<55279129.9000508@gmail.com>
Message-ID: <CAARxGtgMXGE+5CzruvNrNgmvab68Tqg90Xp+0k+8d=VyJaShVg@mail.gmail.com>

Be warned...  a web server can be configured to send an arbitrary mime type
for any file.  You may find .jar files with a mime type of html/text.  Also
zipping a jar circumvents this check.  Some ICAP servers have a "true
content type" check that does not rely on the headers which can be forged,
but actually looks at the file that was requested.
On Apr 10, 2015 5:00 AM, "Yuri Voinov" <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> http://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes
>
>
> 10.04.15 14:48, Fiorenza Meini ?????:
> > Hi,
> > is there a way to filter and block update programs which come from
> Internet, for example java update or windows update , withouth using the
> url of the web site, but working with  header/mime types ?
> >
> > Thanks and regards
> >
> > Fiorenza Meini
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVJ5EoAAoJENNXIZxhPexGJR4H/2z4EeMqAzLUda+uvtYwseCY
> xxUwFOMw6Pj5/5RYaqoLjio07RxbOsSS3Tw4NsKlbtGoBD5NZ1haKKak6dn4iGg2
> fd0KlkqxG7XgKAtFJhS8+Gk3lMPRENOX6gM09GmY0Ua4Q6nvtH75xvBXqNhG/wut
> eS8AXINbwTDi6ciWbtUXLgyeAhQwE9C++8zvTq3UPHv359H8s7HyA6CT6QFvhJtS
> pHEqlzbt8daU+wyiiHA9k/DUfjynjHJJgYGUtOiTHrScWFymtqnqo9sKvfR4UbFb
> HNi3gfRPtV2pw0igof69m8AGKrihEQqt6YUxgDtHjsogvaFZFc8MIyui64zC6Sg=
> =uMVo
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/1a045f07/attachment.htm>

From yvoinov at gmail.com  Fri Apr 10 17:40:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 10 Apr 2015 23:40:51 +0600
Subject: [squid-users] ACL to block installation program
In-Reply-To: <CAARxGtgMXGE+5CzruvNrNgmvab68Tqg90Xp+0k+8d=VyJaShVg@mail.gmail.com>
References: <55278E49.4060106@esseweb.eu>	<55279129.9000508@gmail.com>
 <CAARxGtgMXGE+5CzruvNrNgmvab68Tqg90Xp+0k+8d=VyJaShVg@mail.gmail.com>
Message-ID: <55280B23.6050708@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I would never have such an idea had not occurred. The man asked - I
answered. I know what you're talking about, and I would use ACL to URL
for this.

10.04.15 23:32, brendan kearney ?????:
> Be warned...  a web server can be configured to send an arbitrary mime type
> for any file.  You may find .jar files with a mime type of html/text. 
Also
> zipping a jar circumvents this check.  Some ICAP servers have a "true
> content type" check that does not rely on the headers which can be forged,
> but actually looks at the file that was requested.
> On Apr 10, 2015 5:00 AM, "Yuri Voinov" <yvoinov at gmail.com> wrote:
>
>>
> http://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes
>
>
> 10.04.15 14:48, Fiorenza Meini ?????:
> >>> Hi,
> >>> is there a way to filter and block update programs which come from
> Internet, for example java update or windows update , withouth using the
> url of the web site, but working with  header/mime types ?
> >>>
> >>> Thanks and regards
> >>>
> >>> Fiorenza Meini
>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVKAsjAAoJENNXIZxhPexGXY8H/jeSErby+EvjHyFQ1SNNFg1F
lrxZEVIPYNvNkv8cGCYC7Ye7JyIBOGmOjL6agOXXkwn6ch0qjb8ABP0LQYX/AfKV
GQ76E/OQjO57I5QwFgt8a0T/EoR0Mpu2lcRDi/uLzcSnt5a7djeQCr0RI+GChNEc
IbwjaI/SE2zeZfQGpiGFiUBtDs6W+bfS2QdhL75Y0+i/0r1d6Wc2CFndE41KGq9P
OIwwdqXbWdhZh254amAWs9FWoqqhxM0HONksbds6DLTdwwHeRt8mdLx0WIrgG4uO
py8r07Ml9tCQL55CcCHYpKOKtiJUZLizZRyptFZaYUiWaaW+m418CUJptDvChvU=
=iCuZ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/603a8c69/attachment.htm>

From bpk678 at gmail.com  Fri Apr 10 17:57:00 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Fri, 10 Apr 2015 13:57:00 -0400
Subject: [squid-users] ACL to block installation program
In-Reply-To: <55280B23.6050708@gmail.com>
References: <55278E49.4060106@esseweb.eu> <55279129.9000508@gmail.com>
 <CAARxGtgMXGE+5CzruvNrNgmvab68Tqg90Xp+0k+8d=VyJaShVg@mail.gmail.com>
 <55280B23.6050708@gmail.com>
Message-ID: <CAARxGtgzaDWAP9V3XcBvbn7Ue65iOqO4K=qjPk1G4BGyO9nDxA@mail.gmail.com>

I am in a policy enforcement role, and our policy making / auditing team
approached me about why they could download a jar file from a site that was
not explicitly allowed to provide java content (I.e. not on the
whitelist).  It was because the mime type not being accurate.
On Apr 10, 2015 1:40 PM, "Yuri Voinov" <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> I would never have such an idea had not occurred. The man asked - I
> answered. I know what you're talking about, and I would use ACL to URL for
> this.
>
> 10.04.15 23:32, brendan kearney ?????:
> > Be warned...  a web server can be configured to send an arbitrary mime
> type
> > for any file.  You may find .jar files with a mime type of html/text.
> Also
> > zipping a jar circumvents this check.  Some ICAP servers have a "true
> > content type" check that does not rely on the headers which can be
> forged,
> > but actually looks at the file that was requested.
> > On Apr 10, 2015 5:00 AM, "Yuri Voinov" <yvoinov at gmail.com>
> <yvoinov at gmail.com> wrote:
> >
> >>
> > http://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes
> >
> >
> > 10.04.15 14:48, Fiorenza Meini ?????:
> > >>> Hi,
> > >>> is there a way to filter and block update programs which come from
> > Internet, for example java update or windows update , withouth using the
> > url of the web site, but working with  header/mime types ?
> > >>>
> > >>> Thanks and regards
> > >>>
> > >>> Fiorenza Meini
> >
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVKAsjAAoJENNXIZxhPexGXY8H/jeSErby+EvjHyFQ1SNNFg1F
> lrxZEVIPYNvNkv8cGCYC7Ye7JyIBOGmOjL6agOXXkwn6ch0qjb8ABP0LQYX/AfKV
> GQ76E/OQjO57I5QwFgt8a0T/EoR0Mpu2lcRDi/uLzcSnt5a7djeQCr0RI+GChNEc
> IbwjaI/SE2zeZfQGpiGFiUBtDs6W+bfS2QdhL75Y0+i/0r1d6Wc2CFndE41KGq9P
> OIwwdqXbWdhZh254amAWs9FWoqqhxM0HONksbds6DLTdwwHeRt8mdLx0WIrgG4uO
> py8r07Ml9tCQL55CcCHYpKOKtiJUZLizZRyptFZaYUiWaaW+m418CUJptDvChvU=
> =iCuZ
> -----END PGP SIGNATURE-----
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150410/44bf872b/attachment.htm>

From yvoinov at gmail.com  Fri Apr 10 18:40:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Apr 2015 00:40:31 +0600
Subject: [squid-users] ACL to block installation program
In-Reply-To: <CAARxGtgzaDWAP9V3XcBvbn7Ue65iOqO4K=qjPk1G4BGyO9nDxA@mail.gmail.com>
References: <55278E49.4060106@esseweb.eu>	<55279129.9000508@gmail.com>	<CAARxGtgMXGE+5CzruvNrNgmvab68Tqg90Xp+0k+8d=VyJaShVg@mail.gmail.com>	<55280B23.6050708@gmail.com>
 <CAARxGtgzaDWAP9V3XcBvbn7Ue65iOqO4K=qjPk1G4BGyO9nDxA@mail.gmail.com>
Message-ID: <5528191F.2070400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Agreed.

10.04.15 23:57, brendan kearney ?????:
> I am in a policy enforcement role, and our policy making / auditing team
> approached me about why they could download a jar file from a site
that was
> not explicitly allowed to provide java content (I.e. not on the
> whitelist).  It was because the mime type not being accurate.
> On Apr 10, 2015 1:40 PM, "Yuri Voinov" <yvoinov at gmail.com> wrote:
>
>>
> I would never have such an idea had not occurred. The man asked - I
> answered. I know what you're talking about, and I would use ACL to URL for
> this.
>
> 10.04.15 23:32, brendan kearney ?????:
> >>> Be warned...  a web server can be configured to send an arbitrary mime
> type
> >>> for any file.  You may find .jar files with a mime type of html/text.
> Also
> >>> zipping a jar circumvents this check.  Some ICAP servers have a "true
> >>> content type" check that does not rely on the headers which can be
> forged,
> >>> but actually looks at the file that was requested.
> >>> On Apr 10, 2015 5:00 AM, "Yuri Voinov" <yvoinov at gmail.com>
> <yvoinov at gmail.com> wrote:
> >>>
> >>>>
> >>> http://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes
> >>>
> >>>
> >>> 10.04.15 14:48, Fiorenza Meini ?????:
> >>>>>> Hi,
> >>>>>> is there a way to filter and block update programs which come from
> >>> Internet, for example java update or windows update , withouth
using the
> >>> url of the web site, but working with  header/mime types ?
> >>>>>>
> >>>>>> Thanks and regards
> >>>>>>
> >>>>>> Fiorenza Meini
> >>>
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>
>
>>
>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVKBkfAAoJENNXIZxhPexGQD4H/2U2jQtNqkVS1Hk3gxkyWXeq
nf6ge0Kd+W92WtBWs4Hkf1vbifF9Z/TDckEaAH+SLQaDTr4/O+EeEtQTLLyFNj7Z
5G/RuuGJ+Y1CFwo8zG3x9qqP1ga3Q9PKKjf64k3zlZrEqgWamMksbSoWIEHaQat9
aDi+iGOTGeuF6RxRBFjw1G8nxtRGQAPIs2/B0WDDlY/sQuz7na7R5vDSZCD8O+6X
ywr6Fe3s3CsLrb6F5xxTEzQiofCDerZtszZ9A/OOOTz0XLdPvOqNQAmmhHYk4xQb
CdRREdz6K0wiecM7NFn+jocnet6ZnYP/Q7C5IB7PfiG2N+S0djueWHrmVqP7IVg=
=gWJ2
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150411/fbd2c014/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Apr 10 18:46:15 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 10 Apr 2015 20:46:15 +0200
Subject: [squid-users] ACL to block installation program
In-Reply-To: <CAARxGtgzaDWAP9V3XcBvbn7Ue65iOqO4K=qjPk1G4BGyO9nDxA@mail.gmail.com>
References: <55278E49.4060106@esseweb.eu> <55280B23.6050708@gmail.com>
 <CAARxGtgzaDWAP9V3XcBvbn7Ue65iOqO4K=qjPk1G4BGyO9nDxA@mail.gmail.com>
Message-ID: <201504102046.16111.Antony.Stone@squid.open.source.it>

On Friday 10 April 2015 at 19:57:00 (EU time), brendan kearney wrote:

> I am in a policy enforcement role, and our policy making / auditing team
> approached me about why they could download a jar file from a site that was
> not explicitly allowed to provide java content (I.e. not on the
> whitelist).  It was because the mime type not being accurate.

In that case you need to implement content filtering (checking the actual 
content type) and not trusting the reported content from the sender.

I can't quote a specific implementation, but there must be ways of connecting 
(at the very least) the "file" command for identifying content, and very likely 
ways of creating signatures for .jar files which would trigger ClamAV etc.


Antony.

-- 
The words "e pluribus unum" on the Great Seal of the United States are from a 
poem by Virgil entitled "Moretum", which is about cheese and garlic salad 
dressing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sat Apr 11 00:02:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Apr 2015 12:02:54 +1200
Subject: [squid-users] Stats about used acl/http_access-directives?
In-Reply-To: <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>
References: <CACLJR+PqJaW6nmELGttHfR0W7q_abBL3qMiLeGs26-og0TzD1w@mail.gmail.com>
 <55279344.7010107@gmail.com>
 <CACLJR+OPa0XNP19CZphm4dXVZTjWL9VdsagQpmKkKzfC+EVe2A@mail.gmail.com>
Message-ID: <552864AE.5020602@treenet.co.nz>

On 11/04/2015 12:26 a.m., Tom Tom wrote:
> Thank you.
> 
> This gives too much output and the cache.log will grow too fast. Other
> hints for better tracking the matching/used http_access and
> acl-directives?
> 
> On Fri, Apr 10, 2015 at 11:09 AM, Yuri Voinov wrote:
>>
> And this one:
> 
> https://forums.freebsd.org/threads/tip-debug-squid-acl-matches.5966/
> 
> 10.04.15 15:06, Tom Tom ?????:
>>>> Hi
>>>>
>>>> Is there a way to get stats about matching http_access/acl-directives
>>>> in recent squid-versions?
>>>>
>>>> In a way like "http_access allow localhost" matched 1 time, 2 times,
>>>> n-times.I would be interesting, to see, which
>>>> acl/http_access-directives are used and which ones are configured but
>>>> not used.


Theres nothing like that at present. I've some ideas for a mgr report
but its way down on the TODO list unless someone sponsors it.

Amos


From squid3 at treenet.co.nz  Sat Apr 11 00:23:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Apr 2015 12:23:55 +1200
Subject: [squid-users] Peek and Splice for websites using HSTS
In-Reply-To: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
References: <CANQnQxTpc2tY75s4yav9jPzZHt-kyjfsbcEpdYvh0k94pE9yvw@mail.gmail.com>
Message-ID: <5528699B.8090107@treenet.co.nz>

On 11/04/2015 2:22 a.m., Ashish Patil wrote:
> Hello,
> 
> I am trying to set up Peek and Splice using Squid 3.5.3. I'm facing issues
> setting it up for website that have HSTS enabled, like google.com and
> twitter.com.

Those websites dont just have HSTS enabled. Those two in particular have
HTTP/2, SPDY, and WebSockets operating over port 443 as well.

> 
> My squid.conf is:
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl/myCA.pem
> acl step3 at_step SslBump3
> acl sslBumpAllowedDstDomain dstdomain google.co.in
> ssl_bump peek step3 all
> ssl_bump splice sslBumpAllowedDstDomain
> ssl_bump bump all

What you have configured is to:

* splice (tunnel) if the reverse-DNS of 173.194.117.24 == google.co.in.
  - I doubt this will ever match. Because google reverse-DNS usually
names machines from the *.1e100.net domain.

NOTE: google.* domains are their public forward-DNS entries from the
encrypted URLs. You have to decrypt/bump to see those.

* bump everything else.

* peek will never happen because the connection was spliced or bumped at
step 1. step3 is never reached.

> 
> The output of access.log is:
> 1428674512.281    511 192.168.3.31 TCP_MISS/301 634 GET http://google.co.in/
> - ORIGINAL_DST/173.194.117.23 text/html
> 1428674512.703    348 192.168.3.31 TCP_MISS/302 1106 GET
> http://www.google.co.in/ - ORIGINAL_DST/173.194.117.24 text/html

Two HTTP requests were intercepted, using port 3128 by the looks of your
config.


> 1428674512.706      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.24:443
> - HIER_NONE/- -
> 1428674512.711      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.24:443
> - HIER_NONE/- -
> 1428674515.883      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.956      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674515.965      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674516.006      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.310      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.327      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.335      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -
> 1428674526.411      0 192.168.3.31 TAG_NONE/200 0 CONNECT 173.194.117.22:443
> - HIER_NONE/- -

10 other connections were bumped or spliced. Your log does not continue
long enough to say what was in them, so maybe spliced or they just took
>14 seconds to first request completion.

Amos


From mattatrmc at gmail.com  Sat Apr 11 18:09:16 2015
From: mattatrmc at gmail.com (mattatrmc)
Date: Sat, 11 Apr 2015 11:09:16 -0700 (PDT)
Subject: [squid-users] Error negotiating SSL connection on FD 20:
 error:00000000:lib(0):func(0):reason(0) (5/-1/131)
In-Reply-To: <54CA0581.3050605@gmail.com>
References: <54C677E5.3050400@gmail.com> <54C67C68.6040107@gmail.com>
 <1422305395708-4669351.post@n4.nabble.com> <54C76D26.5020404@gmail.com>
 <1422485696025-4669397.post@n4.nabble.com> <54CA0581.3050605@gmail.com>
Message-ID: <1428775755999-4670687.post@n4.nabble.com>

Hi Yuri,

I just got my Squid up and running on a fairly active test environment, and
am seeing the same error printed.  Did you ever figure out what was causing
the errors?  It appears that the pages that are requested are still loading,
it just appears to be happening more slowly.

Cheers,

Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-connection-on-FD-20-error-00000000-lib-0-func-0-reason-0-5-1-131-tp4669338p4670687.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sat Apr 11 18:24:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 12 Apr 2015 00:24:28 +0600
Subject: [squid-users] Error negotiating SSL connection on FD 20:
 error:00000000:lib(0):func(0):reason(0) (5/-1/131)
In-Reply-To: <1428775755999-4670687.post@n4.nabble.com>
References: <54C677E5.3050400@gmail.com> <54C67C68.6040107@gmail.com>
 <1422305395708-4669351.post@n4.nabble.com> <54C76D26.5020404@gmail.com>
 <1422485696025-4669397.post@n4.nabble.com> <54CA0581.3050605@gmail.com>
 <1428775755999-4670687.post@n4.nabble.com>
Message-ID: <552966DC.108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm ignoring this message.

All pages are loading. This is error 131 from openssl.

12.04.15 0:09, mattatrmc ?????:
> Hi Yuri,
>
> I just got my Squid up and running on a fairly active test
environment, and
> am seeing the same error printed.  Did you ever figure out what was
causing
> the errors?  It appears that the pages that are requested are still
loading,
> it just appears to be happening more slowly.
>
> Cheers,
>
> Matt
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-connection-on-FD-20-error-00000000-lib-0-func-0-reason-0-5-1-131-tp4669338p4670687.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVKWbcAAoJENNXIZxhPexGEW8H/2srWmq7poXspJQHklX94Sdc
fe2NhKQ6UzhDq/9kShjnWGImMhBc4znWitmfuvntAOBLTpW/QZ9klAkk5cZn8rNW
vsTyBg89XTg3Kbt3oBbW2EYR5HZ86do81T8bvTqXwWEU8Ftksdm599DTyQi407NW
DxmUD9GATf2GthJ5DHmWVt5pUS/S+QteR629ymUUVGGaugDXfjnYM58YMn+PxceG
pD70LP9bf87wuvgKkmHnvaTZfMFfkGSoRp0jakiw4E/eXqW/T4neA/Ab9YXXup0d
Wi53I2lVGK8kVXnC+06KsYRVBHAQKES4dUxnaEqyI4p9VaUJjEG6ENsHryQuNrI=
=ENdA
-----END PGP SIGNATURE-----



From mattatrmc at gmail.com  Sat Apr 11 22:11:25 2015
From: mattatrmc at gmail.com (mattatrmc)
Date: Sat, 11 Apr 2015 15:11:25 -0700 (PDT)
Subject: [squid-users] Specify sslproxy_cipher for one site
Message-ID: <1428790285233-4670689.post@n4.nabble.com>

I've been troubleshoot a site that I haven't been able to load using the
squid proxy.  Based on the information provided I was able to determine it
was an issue with the cipher that the proxy was trying to use.  

When I add sslproxy_cipher RCA-MD5 it allows the site to open.  

Now my concern is that since this isn't a secure encryption option I would
only like to make it available for the one site, however I can't seem to
figure out how to do it with acl rules.  Is it possible to do, or do I have
to leave it open for everyone?

Thanks

Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Specify-sslproxy-cipher-for-one-site-tp4670689.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Apr 12 03:31:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Apr 2015 15:31:41 +1200
Subject: [squid-users] Specify sslproxy_cipher for one site
In-Reply-To: <1428790285233-4670689.post@n4.nabble.com>
References: <1428790285233-4670689.post@n4.nabble.com>
Message-ID: <5529E71D.2070401@treenet.co.nz>

On 12/04/2015 10:11 a.m., mattatrmc wrote:
> I've been troubleshoot a site that I haven't been able to load using the
> squid proxy.  Based on the information provided I was able to determine it
> was an issue with the cipher that the proxy was trying to use.  
> 
> When I add sslproxy_cipher RCA-MD5 it allows the site to open.  

RCA ? do you mean RC4 ?

> 
> Now my concern is that since this isn't a secure encryption option I would
> only like to make it available for the one site, however I can't seem to
> figure out how to do it with acl rules.  Is it possible to do, or do I have
> to leave it open for everyone?

No its not possible. And no you should be very, very careful about
enabling it at all. RC4 requires a minimum 2048-bit key to have any
amount of security these days (lesser key sizes can be cracked in near
realtime), and even then it requires connections to be completed/closed
relatively quickly before attacker gets enough info to decipher the keys.

The either your end or the remote site really, really needs an upgrade
to TLSv1.2 if that is the only mutually supported cipher. Now that
RFC7465 prohibits RC4 usage entirely you will find a growing number of
software not supporting it at all.

Amos



From jimdoxin at gmail.com  Sun Apr 12 04:02:39 2015
From: jimdoxin at gmail.com (jimdo x)
Date: Sun, 12 Apr 2015 14:02:39 +1000
Subject: [squid-users] Problem with squid3
Message-ID: <CAN0+XjchKyUMn1fiv8-5j=J7Pg-wpHr9AosFd8S_K5hqpJxqGg@mail.gmail.com>

Hi,

I'm using squid3 on Ubuntu.
It's very good and stable.

But one question is, when using wechat, with windows phone it connect to
mmsns.qpic.cn to retrieve pictures, it ok. When on ios, it trying to
connect ip address 203.213.33.75 to retrieve picture, squid returned 400
error, on ios client side, nothing comes up.

I tried to tune squid, but still no luck.

Any help will be appreciated.

Thank you very much!!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150412/c4f0ba0f/attachment.htm>

From squid3 at treenet.co.nz  Sun Apr 12 04:33:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Apr 2015 16:33:33 +1200
Subject: [squid-users] Problem with squid3
In-Reply-To: <CAN0+XjchKyUMn1fiv8-5j=J7Pg-wpHr9AosFd8S_K5hqpJxqGg@mail.gmail.com>
References: <CAN0+XjchKyUMn1fiv8-5j=J7Pg-wpHr9AosFd8S_K5hqpJxqGg@mail.gmail.com>
Message-ID: <5529F59D.4050908@treenet.co.nz>

On 12/04/2015 4:02 p.m., jimdo x wrote:
> Hi,
> 
> I'm using squid3 on Ubuntu.
> It's very good and stable.
> 
> But one question is, when using wechat, with windows phone it connect to
> mmsns.qpic.cn to retrieve pictures, it ok. When on ios, it trying to
> connect ip address 203.213.33.75 to retrieve picture, squid returned 400
> error, on ios client side, nothing comes up.
> 

So what you are saying is that the *client* is doing two different
things, and one does not work? How would that be Squids fault?


Amos



From anatole.v.farci at intel.com  Sun Apr 12 04:41:58 2015
From: anatole.v.farci at intel.com (Farci, Anatole V)
Date: Sun, 12 Apr 2015 04:41:58 +0000
Subject: [squid-users] T3/T3S Protocol
Message-ID: <6E4E92190C68E34688E8385B138CA417A6558480@ORSMSX108.amr.corp.intel.com>

Hi,

I have a JavaClient that uses T3S:443 to connect to Oracle's WLS application server. WLS is in DMZ and I have Squid proxy between the DMZ and our Intranet (in its own DMZ) to fwd all requests to WLS. The ports (443) is open since the browsers can talk to the WLS but it appears that the T3S is not going thru the proxy. I have searched to see what I can add to allow this T3 (RMI protocol) to go thru and our Squid configuration is very simple and have a whitelist and allows all traffic on port 80 and 443 to go thru.

On the client side, I get this error:
javax.naming.CommunicationException [Root exception is java.net.ConnectException: t3s://xxxx.yyy.intel.com:443: Destination xxx.yyy.zzz.www, 443 unreachable; nested exception is:
        java.net.ConnectException: Connection timed out: connect; No available router to destination]

on the Squid Acccess.log where <dns> and <fqdn> are the correct values and using a browser, I can open reach the WLS with either of them using HTTPS:443
1428776399.835  27238 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776414.999  15117 10.254.98.83 TCP_MISS/200 2199 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776430.068  27768 10.254.98.83 TCP_MISS/200 9658 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776445.200  15085 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776460.396  15118 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776480.270  15211 10.254.98.83 TCP_MISS/200 9722 CONNECT <FQDN>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
1428776495.293  27207 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -

Store.log has this one entry only:
1428773672.888 RELEASE -1 FFFFFFFF 93F32BC091B147DF27B4355731396BC9  200 1428770072 1428770072 1428773672 application/cache-digest 144/144 GET internal://proxy..intel.com/squid-internal-periodic/store_digest

and the squid config looks like this:
visible_hostname proxy.intel.com
http_port 912

logfile_rotate 30
cache_access_log C:/squid/var/logs/access.log

acl all src 0.0.0.0/0.0.0.0
acl whitelist dstdomain .intel.com
acl http proto http t3
acl port_80 port 80
acl port_443 port 443
acl port_23791 port 23791
acl CONNECT method CONNECT


# rules allowing non-authenticated users
http_access allow http port_80 whitelist
http_access allow CONNECT port_443 whitelist
http_access allow CONNECT port_23791 whitelist


I've tested that the ACL is open from the squid DMZ to WLS DMZ but running the JavaClient on the Squid server.

Any help is appreciated.

Thanks

Anatole



Anatole V. Farci
Product Development IT (PDIT) - Integrated Lifecycle Solutions (ILS)
503-696-2917
Mobile # available on outlook





From squid3 at treenet.co.nz  Sun Apr 12 07:17:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Apr 2015 19:17:49 +1200
Subject: [squid-users] T3/T3S Protocol
In-Reply-To: <6E4E92190C68E34688E8385B138CA417A6558480@ORSMSX108.amr.corp.intel.com>
References: <6E4E92190C68E34688E8385B138CA417A6558480@ORSMSX108.amr.corp.intel.com>
Message-ID: <552A1C1D.8070100@treenet.co.nz>

On 12/04/2015 4:41 p.m., Farci, Anatole V wrote:
> Hi,
> 
> I have a JavaClient that uses T3S:443 to connect to Oracle's WLS
application server. WLS is in DMZ and I have Squid proxy between the DMZ
and our Intranet (in its own DMZ) to fwd all requests to WLS. The ports
(443) is open since the browsers can talk to the WLS but it appears that
the T3S is not going thru the proxy. I have searched to see what I can
add to allow this T3 (RMI protocol) to go thru and our Squid
configuration is very simple and have a whitelist and allows all traffic
on port 80 and 443 to go thru.
> 
> On the client side, I get this error:
> javax.naming.CommunicationException [Root exception is java.net.ConnectException: t3s://xxxx.yyy.intel.com:443: Destination xxx.yyy.zzz.www, 443 unreachable; nested exception is:
>         java.net.ConnectException: Connection timed out: connect; No available router to destination]

Does this Java application support HTTP proxying ?

Squid is an HTTP proxy, use of any other protocol over it has to be via
HTTP mechanisms. CONNECT requests in this case. It wont help at all if
the Java application cannot do HTTP enough to perform an HTTP CONNECT
request.

> 
> on the Squid Acccess.log where <dns> and <fqdn> are the correct values and using a browser, I can open reach the WLS with either of them using HTTPS:443
> 1428776399.835  27238 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776414.999  15117 10.254.98.83 TCP_MISS/200 2199 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776430.068  27768 10.254.98.83 TCP_MISS/200 9658 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776445.200  15085 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776460.396  15118 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776480.270  15211 10.254.98.83 TCP_MISS/200 9722 CONNECT <FQDN>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 1428776495.293  27207 10.254.98.83 TCP_MISS/200 2439 CONNECT <dns>.intel.com:443 - DIRECT/xxx.yyy.zzz.www -
> 

Yes. However note that HTTPS != T3S


> Store.log has this one entry only:
> 1428773672.888 RELEASE -1 FFFFFFFF 93F32BC091B147DF27B4355731396BC9  200 1428770072 1428770072 1428773672 application/cache-digest 144/144 GET internal://proxy..intel.com/squid-internal-periodic/store_digest
> 

CONNECT are not cacheable. There is nothing that can be stored to disk.


> and the squid config looks like this:
> visible_hostname proxy.intel.com
> http_port 912

APEX and HTTP protocols are not safe to be mixing up. Use of 3128 is
sufficient for Squid proxy (its even registered for Squid use).

> 
> logfile_rotate 30
> cache_access_log C:/squid/var/logs/access.log

That should be:
 access_log C:/squid/var/logs/access.log


> 
> acl all src 0.0.0.0/0.0.0.0
> acl whitelist dstdomain .intel.com
> acl http proto http t3
> acl port_80 port 80
> acl port_443 port 443
> acl port_23791 port 23791
> acl CONNECT method CONNECT
> 
> 
> # rules allowing non-authenticated users
> http_access allow http port_80 whitelist
> http_access allow CONNECT port_443 whitelist
> http_access allow CONNECT port_23791 whitelist

Highly dangerous. Please use the recommended defaults:

 acl SSL_ports port 443
 acl Safe_ports port 80          # http
 acl Safe_ports port 21          # ftp
 acl Safe_ports port 443         # https
 acl Safe_ports port 70          # gopher
 acl Safe_ports port 210         # wais
 acl Safe_ports port 1025-65535  # unregistered ports
 acl Safe_ports port 280         # http-mgmt
 acl Safe_ports port 488         # gss-http
 acl Safe_ports port 591         # filemaker
 acl Safe_ports port 777         # multiling http

 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports

 # Then your bit...
 http_access allow whitelist

 http_access deny all


Amos


From gkinkie at gmail.com  Sun Apr 12 14:37:05 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 12 Apr 2015 16:37:05 +0200
Subject: [squid-users] Client delay pools ...doesn't work
In-Reply-To: <5527D557.8070407@esseweb.eu>
References: <5527D557.8070407@esseweb.eu>
Message-ID: <CA+Y8hcNaCSMeEwHyu+fVj4owCivVUTq45HigisLwyZWLd6P3Pg@mail.gmail.com>

Hi Fiorenza,
  does your browser display the same error when you remove that config
line and reconfigure squid?

On Fri, Apr 10, 2015 at 3:51 PM, Fiorenza Meini <fmeini at esseweb.eu> wrote:
> Hi,
> I'm testing on a 3.4 squid release the client_delay_poolfunctionality.
> It seems that isn't working: on my browser I receive the error that proxy
> isn't reachable, and in log file I can't see nothing useful.
>
> Has anyone configured this functionality successfully ?
>
> Regards
>
> Fiorenza Meini
> --
> Spazio Web S.r.l.
> V. Dante, 10
> 13900 Biella
> Tel.: +39 015 2431982
> Fax.: +39 015 2522600
> Numero d'Iscrizione al Registro Imprese presso CCIAA Biella, Cod.Fisc.e
> P.Iva: 02414430021
> Iscriz. REA: BI - 188936 Cap. Soc.: EURO. 30.000 i.v.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From monahbaki at gmail.com  Sun Apr 12 14:55:31 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Sun, 12 Apr 2015 10:55:31 -0400
Subject: [squid-users] BUG 3279: HTTP reply without Date:
Message-ID: <CALP3=x9O7GTf5h=GE98wB2x2hKc2LEfc8XJFLmb4J_yOyAVOaA@mail.gmail.com>

Hi all,

Compiled squid 3.5.2 on CentOS 6.6 as follows:
$ ./configure --prefix=/home/cache --enable-follow-x-forwarded-for
--with-large-files --enable-ssl --disable-ipv6 --enable-esi
--enable-kill-parent-hack --enable-snmp --with-pthreads
--with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
--enable-storeio=ufs,aufs,diskd,rock

After approx 24 hours I am seeing this error on my squid 3.5.2 with one
user connected for testing:

2015/04/11 15:02:58| Logfile: closing log
daemon:/home/cache/var/logs/access.log
2015/04/11 15:02:58| Logfile Daemon: closing log
daemon:/home/cache/var/logs/access.log
2015/04/11 15:02:58| Open FD UNSTARTED     0 stdin
2015/04/11 15:02:58| Open FD UNSTARTED     1 stdout
2015/04/11 15:02:58| Open FD UNSTARTED     2 stderr
2015/04/11 15:02:58| Open FD UNSTARTED     8 DNS Socket IPv4
2015/04/11 15:02:58| Open FD UNSTARTED     9 IPC UNIX STREAM Parent
2015/04/11 15:02:58| Squid Cache (Version 3.5.2): Exiting normally.
2015/04/11 15:06:52| Set Current Directory to
/usr/local/squid/var/cache/squid
2015/04/11 15:06:52| Starting Squid Cache version 3.5.2 for
x86_64-unknown-linux-gnu...
2015/04/11 15:06:52| Service Name: squid
2015/04/11 15:06:52| Process ID 2005
2015/04/11 15:06:52| Process Roles: master worker
2015/04/11 15:06:52| With 65536 file descriptors available
2015/04/11 15:06:52| Initializing IP Cache...
2015/04/11 15:06:52| DNS Socket created at 0.0.0.0, FD 8
2015/04/11 15:06:52| Adding nameserver 8.8.8.8 from squid.conf
2015/04/11 15:06:52| Adding nameserver 41.78.211.30 from squid.conf
2015/04/11 15:06:52| Logfile: opening log
daemon:/home/cache/var/logs/access.log
2015/04/11 15:06:52| Logfile Daemon: opening log
/home/cache/var/logs/access.log
2015/04/11 15:06:52| Store logging disabled
2015/04/11 15:06:52| Swap maxSize 358400000 + 9437184 KB, estimated
28295168 objects
2015/04/11 15:06:52| Target number of buckets: 1414758
2015/04/11 15:06:52| Using 2097152 Store buckets
2015/04/11 15:06:52| Max Mem  size: 9437184 KB
2015/04/11 15:06:52| Max Swap size: 358400000 KB
2015/04/11 15:06:52| Rebuilding storage in /home/cache/var/cache/squid
(clean log)
2015/04/11 15:06:52| Using Least Load store dir selection
2015/04/11 15:06:52| Set Current Directory to
/usr/local/squid/var/cache/squid
2015/04/11 15:06:52| Finished loading MIME types and icons.
2015/04/11 15:06:52| HTCP Disabled.
2015/04/11 15:06:52| Sending SNMP messages from 0.0.0.0:3401
2015/04/11 15:06:52| Squid plugin modules loaded: 0
2015/04/11 15:06:52| Adaptation support is off.
2015/04/11 15:06:52| Accepting HTTP Socket connections at local=0.0.0.0:3128
remote=[::] FD 13 flags=9
2015/04/11 15:06:52| Accepting NAT intercepted HTTP Socket connections at
local=0.0.0.0:3129 remote=[::] FD 14 flags=41
2015/04/11 15:06:52| Accepting SNMP messages on 0.0.0.0:3401
2015/04/11 15:06:52| Done reading /home/cache/var/cache/squid swaplog (94
entries)
2015/04/11 15:06:52| Finished rebuilding storage from disk.
2015/04/11 15:06:52|        94 Entries scanned
2015/04/11 15:06:52|         0 Invalid entries.
2015/04/11 15:06:52|         0 With invalid flags.
2015/04/11 15:06:52|        94 Objects loaded.
2015/04/11 15:06:52|         0 Objects expired.
2015/04/11 15:06:52|         0 Objects cancelled.
2015/04/11 15:06:52|         0 Duplicate URLs purged.
2015/04/11 15:06:52|         0 Swapfile clashes avoided.
2015/04/11 15:06:52|   Took 0.05 seconds (2036.97 objects/sec).
2015/04/11 15:06:52| Beginning Validation Procedure
2015/04/11 15:06:52|   Completed Validation Procedure
2015/04/11 15:06:52|   Validated 94 Entries
2015/04/11 15:06:52|   store_swap_size = 2000.00 KB
2015/04/11 15:06:53| storeLateRelease: released 0 objects
2015/04/11 15:48:51| WARNING: 1 swapin MD5 mismatches
2015/04/11 15:48:51| Could not parse headers from on disk object
2015/04/11 15:48:51| BUG 3279: HTTP reply without Date:
2015/04/11 15:48:51| StoreEntry->key: 039CA6C6725D0A9F31B498354995DE50
2015/04/11 15:48:51| StoreEntry->next: 0
2015/04/11 15:48:51| StoreEntry->mem_obj: 0x21ecd40
2015/04/11 15:48:51| StoreEntry->timestamp: -1
2015/04/11 15:48:51| StoreEntry->lastref: 1428763731
2015/04/11 15:48:51| StoreEntry->expires: -1
2015/04/11 15:48:51| StoreEntry->lastmod: -1
2015/04/11 15:48:51| StoreEntry->swap_file_sz: 0
2015/04/11 15:48:51| StoreEntry->refcount: 1
2015/04/11 15:48:51| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/04/11 15:48:51| StoreEntry->swap_dirn: -1
2015/04/11 15:48:51| StoreEntry->swap_filen: -1
2015/04/11 15:48:51| StoreEntry->lock_count: 2
2015/04/11 15:48:51| StoreEntry->mem_status: 0
2015/04/11 15:48:51| StoreEntry->ping_status: 2
2015/04/11 15:48:51| StoreEntry->store_status: 1
2015/04/11 15:48:51| StoreEntry->swap_status: 0
2015/04/11 15:49:55| Could not parse headers from on disk object
2015/04/11 20:10:06| BUG 3279: HTTP reply without Date:
2015/04/11 20:10:06| StoreEntry->key: 8749EF6C14DB515AA7E09A4ED2019298
2015/04/11 20:10:06| StoreEntry->next: 0
2015/04/11 20:10:06| StoreEntry->mem_obj: 0x224f3f0
2015/04/11 20:10:06| StoreEntry->timestamp: -1
2015/04/11 20:10:06| StoreEntry->lastref: 1428779406
2015/04/11 20:10:06| StoreEntry->expires: -1
2015/04/11 20:10:06| StoreEntry->lastmod: -1
2015/04/11 20:10:06| StoreEntry->swap_file_sz: 0
2015/04/11 20:10:06| StoreEntry->refcount: 1
2015/04/11 20:10:06| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/04/11 20:10:06| StoreEntry->swap_dirn: -1
2015/04/11 20:10:06| StoreEntry->swap_filen: -1
2015/04/11 20:10:06| StoreEntry->lock_count: 2
2015/04/11 20:10:06| StoreEntry->mem_status: 0
2015/04/11 20:10:06| StoreEntry->ping_status: 2
2015/04/11 20:10:06| StoreEntry->store_status: 1
2015/04/11 20:10:06| StoreEntry->swap_status: 0
2015/04/12 03:54:21| Could not parse headers from on disk object
2015/04/12 03:54:21| BUG 3279: HTTP reply without Date:
2015/04/12 03:54:21| StoreEntry->key: 2664F79F89A842E097DCD721C4417644
2015/04/12 03:54:21| StoreEntry->next: 0
2015/04/12 03:54:21| StoreEntry->mem_obj: 0x23686e0
2015/04/12 03:54:21| StoreEntry->timestamp: -1
2015/04/12 03:54:21| StoreEntry->lastref: 1428807261
2015/04/12 03:54:21| StoreEntry->expires: -1
2015/04/12 03:54:21| StoreEntry->lastmod: -1
2015/04/12 03:54:21| StoreEntry->swap_file_sz: 0
2015/04/12 03:54:21| StoreEntry->refcount: 1
2015/04/12 03:54:21| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/04/12 03:54:21| StoreEntry->swap_dirn: -1
2015/04/12 03:54:21| StoreEntry->swap_filen: -1
2015/04/12 03:54:21| StoreEntry->lock_count: 2
2015/04/12 03:54:21| StoreEntry->mem_status: 0
2015/04/12 03:54:21| StoreEntry->ping_status: 2
2015/04/12 03:54:21| StoreEntry->store_status: 1
2015/04/12 03:54:21| StoreEntry->swap_status: 0
2015/04/12 03:55:24| Could not parse headers from on disk object
2015/04/12 03:56:24| StoreEntry->swap_status: 0
2015/04/12 03:56:24| assertion failed: store.cc:1885: "isEmpty()"




Thank you
Monah
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150412/1d2dc3bd/attachment.htm>

From stan.prescott at gmail.com  Sun Apr 12 18:12:56 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 12 Apr 2015 14:12:56 -0400
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
	certain sites
Message-ID: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>

I would like to give my users the ability to "not bump" certain sites. I
tried to use the examples given on the SSLPeekandSplice wiki page but can't
get it to work.

This is a snippet of my squid.conf file.

*https_port 192.168.10.1:808 <http://192.168.10.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*http_port 192.168.20.1:800 <http://192.168.20.1:800> intercept*
*https_port 192.168.20.1:808 <http://192.168.20.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*http_port 127.0.0.1:800 <http://127.0.0.1:800> intercept*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*sslproxy_session_cache_size 4 MB*

*acl serverIsBank dstdomain wellsfargo.com <http://wellsfargo.com>*

*ssl_bump server-first all*

*ssl_bump none localhostgreen*
*ssl_bump none localhostpurple*

*ssl_bump splice serverIsBank*
*ssl_bump peek all*
*ssl_bump bump all*
*sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
*sslcrtd_children 5*


When I start squid I don't get any error messages and all pages, http and
https, load properly. The problem is, using the example above, the
*https://www.wellsfargo.com
<https://www.wellsfargo.com> *website is still getting bumped, evidenced by
the appearance of the ssl website in the web proxy access logs. When I
don't have ssl_bump enabled then no https websites appear in the access
logs, as it should be. But, enabling ssl_bump and peek and splice, web
sites that I am trying not to bump still seem to be getting bumped.

Any suggestions on how to properly "not bump" certain websites.

Thanks,

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150412/1b8ae287/attachment.htm>

From nathan at getoffmalawn.com  Mon Apr 13 00:25:28 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 13 Apr 2015 10:25:28 +1000
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
Message-ID: <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>

Hi Stan,

For peek and splice, you need to decide based on the SNI name, not the
domain name, which for 3.5 means you need to use an external ACL
helper that processes %ssl::>sni. For 4.0 there will be a server_name
ACL you can use instead.

On top of that, you also need to make sure this external ACL helper
runs at the correct "bump step", with the at_step ACL, e.g...

external_acl_type sni ttl=30 concurrency=X children-max=Y
children-startup=Z %ssl::>sni /path/to/your/helper

acl sni_exclusions external sni
acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2

ssl_bump peek tcp_level all
ssl_bump splice client_hello_peeked sni_exclusions
ssl_bump bump all

Hope that helps,

Nathan.

On 13 April 2015 at 04:12, Stanford Prescott <stan.prescott at gmail.com> wrote:
> I would like to give my users the ability to "not bump" certain sites. I
> tried to use the examples given on the SSLPeekandSplice wiki page but can't
> get it to work.
>
> This is a snippet of my squid.conf file.
>
> https_port 192.168.10.1:808 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>
> http_port 192.168.20.1:800 intercept
> https_port 192.168.20.1:808 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>
> http_port 127.0.0.1:800 intercept
>
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_session_cache_size 4 MB
>
> acl serverIsBank dstdomain wellsfargo.com
>
> ssl_bump server-first all
>
> ssl_bump none localhostgreen
> ssl_bump none localhostpurple
>
> ssl_bump splice serverIsBank
> ssl_bump peek all
> ssl_bump bump all
> sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
> sslcrtd_children 5
>
>
> When I start squid I don't get any error messages and all pages, http and
> https, load properly. The problem is, using the example above, the
> https://www.wellsfargo.com website is still getting bumped, evidenced by the
> appearance of the ssl website in the web proxy access logs. When I don't
> have ssl_bump enabled then no https websites appear in the access logs, as
> it should be. But, enabling ssl_bump and peek and splice, web sites that I
> am trying not to bump still seem to be getting bumped.
>
> Any suggestions on how to properly "not bump" certain websites.
>
> Thanks,
>
> Stan
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From nathan at getoffmalawn.com  Mon Apr 13 02:37:27 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 13 Apr 2015 12:37:27 +1000
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CANLNtGSgY4P0xGBYW9A=quVrAMq3cyt_0_J-nugt8bB3kVYKVg@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
 <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>
 <CANLNtGSgY4P0xGBYW9A=quVrAMq3cyt_0_J-nugt8bB3kVYKVg@mail.gmail.com>
Message-ID: <CAGUJm7aCWmacrsufrj9aKw+Le+kF50GEtz2XB9tUbVNJqBW6cw@mail.gmail.com>

Hi Stan,

So one of the things that peek and splice added was support for the
Server Name Indication SSL extension, which let's Squid make bumping
decisions more accurately based on the hostname, rather than the IP
address. Prior to this, bumping on only the IP address caused issues
for virtual hosting and such.

As for a good write-up, this is about the best you can get, which
covers the protocol itself:
http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29

Essentially external ACLs are processes that Squid will write
"requests" to, which are line-based and configured according to the
format specifiers in your external_acl_type directive. The helper
process should read the request and decide if it's a "match", then
write back to Squid, which Squid will take action on. All
communication is over standard input/output.

Writing external ACLs is usually quite specialised to the situation,
so it's difficult to find concrete examples that will do what you
want. Using the configuration I mentioned earlier, you could write a
simple helper like so (this one is in Python):

import sys

line = sys.stdin.read()

# run loop until an empty read, which indicates the process should shut down.
while line:
    concurrency_id, sni = line.split()

    if sni == 'wellsfargo.com':
        sys.stdout.write('%s OK\n' % concurrency_id)
    else:
        sys.stdout.write('%s ERR\n' % concurrency_id)

    line = sys.stdin.read()

So when Squid sends a request to the helper process, if the servername
is for 'wellsfargo.com', it will be considered a match, i.e. it will
not be bumped, else it won't match, so it will be bumped later in the
SSL bump process.

If you need clarification on anything, feel free to ask! This is quite
a bit of information to absorb I know :)

Hope that helps,

Nathan.

On 13 April 2015 at 12:01, Stanford Prescott <stan.prescott at gmail.com> wrote:
> Thanks for your response, Nathan. I'm sure what you suggest would be very
> helpful, if I knew anything about ACL helpers and how to use them to not
> bump certain sites. I'm thinking the sni is what is actually used to
> identify the sites to not bump?
>
> Is there a good write-up somewhere of how to create these ACL helpers and
> how to use them?
>
> On Sun, Apr 12, 2015 at 8:25 PM, Nathan Hoad <nathan at getoffmalawn.com>
> wrote:
>>
>> Hi Stan,
>>
>> For peek and splice, you need to decide based on the SNI name, not the
>> domain name, which for 3.5 means you need to use an external ACL
>> helper that processes %ssl::>sni. For 4.0 there will be a server_name
>> ACL you can use instead.
>>
>> On top of that, you also need to make sure this external ACL helper
>> runs at the correct "bump step", with the at_step ACL, e.g...
>>
>> external_acl_type sni ttl=30 concurrency=X children-max=Y
>> children-startup=Z %ssl::>sni /path/to/your/helper
>>
>> acl sni_exclusions external sni
>> acl tcp_level at_step SslBump1
>> acl client_hello_peeked at_step SslBump2
>>
>> ssl_bump peek tcp_level all
>> ssl_bump splice client_hello_peeked sni_exclusions
>> ssl_bump bump all
>>
>> Hope that helps,
>>
>> Nathan.
>>
>> On 13 April 2015 at 04:12, Stanford Prescott <stan.prescott at gmail.com>
>> wrote:
>> > I would like to give my users the ability to "not bump" certain sites. I
>> > tried to use the examples given on the SSLPeekandSplice wiki page but
>> > can't
>> > get it to work.
>> >
>> > This is a snippet of my squid.conf file.
>> >
>> > https_port 192.168.10.1:808 intercept ssl-bump
>> > generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB
>> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >
>> > http_port 192.168.20.1:800 intercept
>> > https_port 192.168.20.1:808 intercept ssl-bump
>> > generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB
>> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >
>> > http_port 127.0.0.1:800 intercept
>> >
>> > sslproxy_cert_error allow all
>> > sslproxy_flags DONT_VERIFY_PEER
>> > sslproxy_session_cache_size 4 MB
>> >
>> > acl serverIsBank dstdomain wellsfargo.com
>> >
>> > ssl_bump server-first all
>> >
>> > ssl_bump none localhostgreen
>> > ssl_bump none localhostpurple
>> >
>> > ssl_bump splice serverIsBank
>> > ssl_bump peek all
>> > ssl_bump bump all
>> > sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
>> > /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
>> > sslcrtd_children 5
>> >
>> >
>> > When I start squid I don't get any error messages and all pages, http
>> > and
>> > https, load properly. The problem is, using the example above, the
>> > https://www.wellsfargo.com website is still getting bumped, evidenced by
>> > the
>> > appearance of the ssl website in the web proxy access logs. When I don't
>> > have ssl_bump enabled then no https websites appear in the access logs,
>> > as
>> > it should be. But, enabling ssl_bump and peek and splice, web sites that
>> > I
>> > am trying not to bump still seem to be getting bumped.
>> >
>> > Any suggestions on how to properly "not bump" certain websites.
>> >
>> > Thanks,
>> >
>> > Stan
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>
>


From squid3 at treenet.co.nz  Mon Apr 13 03:21:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Apr 2015 15:21:22 +1200
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CAGUJm7aCWmacrsufrj9aKw+Le+kF50GEtz2XB9tUbVNJqBW6cw@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
 <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>
 <CANLNtGSgY4P0xGBYW9A=quVrAMq3cyt_0_J-nugt8bB3kVYKVg@mail.gmail.com>
 <CAGUJm7aCWmacrsufrj9aKw+Le+kF50GEtz2XB9tUbVNJqBW6cw@mail.gmail.com>
Message-ID: <552B3632.8010709@treenet.co.nz>

On 13/04/2015 2:37 p.m., Nathan Hoad wrote:
> Hi Stan,
> 
> So one of the things that peek and splice added was support for the
> Server Name Indication SSL extension, which let's Squid make bumping
> decisions more accurately based on the hostname, rather than the IP
> address. Prior to this, bumping on only the IP address caused issues
> for virtual hosting and such.
> 


FYI; Christos has now also created a ssl::server_name ACL which tests
the SNI values (amongst other things) for this use case. I'm just in the
process of back porting it now, so future 3.5 snapshots and releases
should have it available.

Amos



From jimdoxin at gmail.com  Mon Apr 13 03:58:42 2015
From: jimdoxin at gmail.com (jimdo x)
Date: Mon, 13 Apr 2015 13:58:42 +1000
Subject: [squid-users] Problem with squid3.3.8 on ubuntu and windows 8.1
	store
Message-ID: <CAN0+Xjd2BaW032qkw8pV_PMGWPfRa25jRj8We-ptnYkCCfwqnA@mail.gmail.com>

Hi all,

Anyone knows why with squid3 as proxy, the windows store load contents
fine, but when comes to download apps, it always saying, you have a slow
connection? or just can not download.
Without squid as proxy, like connect directly to internet, it downloads
fine.

Thanks!!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150413/35d2d48b/attachment.htm>

From fmeini at esseweb.eu  Mon Apr 13 07:43:38 2015
From: fmeini at esseweb.eu (Fiorenza Meini)
Date: Mon, 13 Apr 2015 09:43:38 +0200
Subject: [squid-users] Client delay pools ...doesn't work
In-Reply-To: <CA+Y8hcNaCSMeEwHyu+fVj4owCivVUTq45HigisLwyZWLd6P3Pg@mail.gmail.com>
References: <5527D557.8070407@esseweb.eu>
 <CA+Y8hcNaCSMeEwHyu+fVj4owCivVUTq45HigisLwyZWLd6P3Pg@mail.gmail.com>
Message-ID: <552B73AA.5030206@esseweb.eu>

Il 12/04/2015 16:37, Kinkie ha scritto:
> Hi Fiorenza,
>    does your browser display the same error when you remove that config
> line and reconfigure squid?
>

Hi,
if I remove lines of squid client_delay section, browser works

Regards
Fiorenza


-- 
Spazio Web S.r.l.
V. Dante, 10
13900 Biella
Tel.: +39 015 2431982
Fax.: +39 015 2522600
Numero d'Iscrizione al Registro Imprese presso CCIAA Biella, Cod.Fisc.e 
P.Iva: 02414430021
Iscriz. REA: BI - 188936 Cap. Soc.: ?. 30.000 i.v.


From markus.rietzler at fv.nrw.de  Mon Apr 13 08:39:15 2015
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, SG 324 / <RIETZLER_SOFTWARE>))
Date: Mon, 13 Apr 2015 08:39:15 +0000
Subject: [squid-users] growing number of ntlm_auth helpers (was 100% cpu
 usage with ext auth/Excessive NTLM or Negotiate auth helper annotations)
Message-ID: <1FCF9DA5B29068478ECF15896F19F084015C9A0469@Y011008.bk.fin.local>

hello,

we are using squid version 3.5.3. at the moment we see a growing number of ntlm_auth helpers until they reach the much number configured with


auth_param ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm realm
auth_param ntlm children 192 startup=24 idle=12 concurrency=0
auth_param ntlm keep_alive on
auth_param basic /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic realm Internet-Zugriff
 auth_param basic children 5 startup=2 idle=1 concurrency=0
auth_param basic credentialsttl 7200 seconds
auth_param basic casesensitive off
auth_param basic utf8 off
authenticate_cache_garbage_interval 3600 seconds
authenticate_ttl 3600 seconds
authenticate_ip_ttl 1 seconds



we went to 3.5.1 because this version fixed bug 3997 (100% cpu usage with external auth). but there we saw the growing of ntlm_auth helpers. we then switched to the recent version 3.5.3. no growing number of auth helpers but frequent restarts every 1 or 2 hours with signal 6. so i suppose that the growing of auth helpers still is there but maybe have no effect as squid is restarting before the number gets to high (bug 4190 also is related to auth!)

see related bugs:

http://bugs.squid-cache.org/show_bug.cgi?id=3997
http://bugs.squid-cache.org/show_bug.cgi?id=4190


Mit freundlichen Gr??en

Markus Rietzler
<RIETZLER_SOFTWARE/>
Rechenzentrum der Finanzverwaltung NRW
Ro?str. 131
40476 D?sseldorf

Tel.: 0211 / 4572 - 2130



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150413/26dd4313/attachment.htm>

From markus.rietzler at fv.nrw.de  Mon Apr 13 08:40:01 2015
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, SG 324 / <RIETZLER_SOFTWARE>))
Date: Mon, 13 Apr 2015 08:40:01 +0000
Subject: [squid-users] growing number of ntlm_auth helpers (was 100% cpu
 usage with ext auth/Excessive NTLM or Negotiate auth helper annotations)
Message-ID: <1FCF9DA5B29068478ECF15896F19F084015C9A047B@Y011008.bk.fin.local>

hello,

we are using squid version 3.5.3. at the moment we see a growing number of ntlm_auth helpers until they reach the much number configured with


auth_param ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm realm 
auth_param ntlm children 192 startup=24 idle=12 concurrency=0
auth_param ntlm keep_alive on
auth_param basic /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic realm Internet-Zugriff
 auth_param basic children 5 startup=2 idle=1 concurrency=0
auth_param basic credentialsttl 7200 seconds
auth_param basic casesensitive off
auth_param basic utf8 off
authenticate_cache_garbage_interval 3600 seconds
authenticate_ttl 3600 seconds
authenticate_ip_ttl 1 seconds



we went to 3.5.1 because this version fixed bug 3997 (100% cpu usage with external auth). but there we saw the growing of ntlm_auth helpers. we then switched to the recent version 3.5.3. no growing number of auth helpers but frequent restarts every 1 or 2 hours with signal 6. so i suppose that the growing of auth helpers still is there but maybe have no effect as squid is restarting before the number gets to high (bug 4190 also is related to auth!)

see related bugs:

http://bugs.squid-cache.org/show_bug.cgi?id=3997
http://bugs.squid-cache.org/show_bug.cgi?id=4190


Mit freundlichen Gr??en

Markus Rietzler
<RIETZLER_SOFTWARE/>
Rechenzentrum der Finanzverwaltung NRW



From Richard.Aspley at hammonds-uk.com  Mon Apr 13 08:55:04 2015
From: Richard.Aspley at hammonds-uk.com (Rich549)
Date: Mon, 13 Apr 2015 01:55:04 -0700 (PDT)
Subject: [squid-users] Increase number of ext_ldap_group_acl processes
In-Reply-To: <CAP6yRXiEnpSJ21XLJVdg=VFb7QiOrBCrtk1PNQs9XxFK6zOAvg@mail.gmail.com>
References: <1426689385646-4670484.post@n4.nabble.com>
 <550993B7.5030801@treenet.co.nz> <1426694704086-4670488.post@n4.nabble.com>
 <CAP6yRXiEnpSJ21XLJVdg=VFb7QiOrBCrtk1PNQs9XxFK6zOAvg@mail.gmail.com>
Message-ID: <1428915304555-4670706.post@n4.nabble.com>

Hi,

Thanks for the response, I'd already tried that and got it working. 
Completely forgot to post that I'd sorted it, sorry!

Thanks,

Rich



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Increase-number-of-ext-ldap-group-acl-processes-tp4670484p4670706.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From windj007 at gmail.com  Mon Apr 13 09:58:58 2015
From: windj007 at gmail.com (Roman Suvorov)
Date: Mon, 13 Apr 2015 12:58:58 +0300
Subject: [squid-users] Server FQDN for each request
Message-ID: <CADcN6fPuLM9kj=FP+f8hUiAKQbRP2s9+P01W8jWzgdUCgn5ZHQ@mail.gmail.com>

Dear colleagues,

Is the setup as following possible?

A server 99.99.99.99 hosts a single Squid with ICAP plugged in. There
two DNS records point to this server:
first.example.com -> 99.99.99.99
second.example.com -> 99.99.99.99

If I put first.example.com in browser proxy settings for the first
group of users and second.example.com for the second group, can I
somehow distinguish requests from these two groups of users without
using explicit login-password authentication? Browsers are configured
explicitly (the proxy is not transparent). There are supposed to be
many such groups of users, thus setting up many Squid instances or
using different TCP ports is not appropriate.

I'm looking for a way to identify users without explicit
authentication, maybe there are some other ways?

Best regards,
Roman.


From vdoctor at neuf.fr  Mon Apr 13 10:42:52 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 13 Apr 2015 03:42:52 -0700 (PDT)
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <55272F87.40607@treenet.co.nz>
References: <1428322495420-4670630.post@n4.nabble.com>
 <552281E7.8060405@gmail.com> <1428564569076-4670656.post@n4.nabble.com>
 <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com> <02d601d072d1$390562d0$ab102870$@fr>
 <55272F87.40607@treenet.co.nz>
Message-ID: <1428921772616-4670708.post@n4.nabble.com>

Hi Amos, All,

We have done as you indicate, but the index.tx is still corrupted, have a
look:
*V	250406120057Z		2C564651B40D1F4F6CAFFF06EA8B201580E3B678	unknown
/CN=173.194.65.84+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		71664F2E27C4E321B2A4F59EA3971D70C298DAFB	unknown
/CN=173.194.112.167+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		45205F68B7AD2DEEBE408E55FF8DADE5C88F6A99	unknown
/CN=74.125.136.188+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		6CFF07CBB3BE016022AF2EA75BA56EC99FD8256E	unknown
/CN=173.194.112.177+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		5829213872C31284E8853C079BF51A0E50F89CF8	unknown
/CN=173.194.112.166+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		06BC95EACDEEC116E11B1B6CE66C9179C4251D6E	unknown
/CN=173.194.112.164+Sign=signTrusted+SignHash=SHA256
V	250406120057Z		7ECFE99D51088BD0692A7439EBAFEDA38A29BC	unknown
/CN=173.194.112.185+Sign=signTrusted+SignHash=SHA256
V	150623000000Z		49F18ABCB410F18BE715AF26AEEB0EE4E1D89DC6	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.google.com+Sign=signTrusted+SignHash=SHA256
V	150623000000Z		3A2A74F1431B28F9E268B8762706F69597D11447	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.googleapis.com+Sign=signTrusted+SignHash=SHA256
V	150623000000Z		695CC4B75B9F38E29836BB211432FF8286966313	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=*.google-analytics.com+Sign=signTrusted+SignHash=SHA256
V	150623000000Z		21C204121B238D4B48F0196F4D644B7A5F775574	unknown
/C=US/ST=California/L=Mountain View/O=Google
Inc/CN=www.google.com+Sign=signTrusted+SignHash=SHA256
HA256
*

What's the *HA256* at the end of the file ?

here is the squid.conf (3.5.3):
sslproxy_capath /etc/ssl/certs
acl sslstep1 at_step SslBump1
ssl_bump peek sslstep1
ssl_bump bump all
ssl_bump splice all
sslcrtd_program /usr/local/squid3/lib/ssl_crtd -s /var/lib/ssl_db -M 8MB
sslcrtd_children 16 startup=5 idle=1

The Squid crash every 1-2 hours.
Seems the ssl_crtd fails in writing data tot he index.txt.

Thanks for your help.
Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670708.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Apr 13 13:00:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Apr 2015 01:00:28 +1200
Subject: [squid-users] Server FQDN for each request
In-Reply-To: <CADcN6fPuLM9kj=FP+f8hUiAKQbRP2s9+P01W8jWzgdUCgn5ZHQ@mail.gmail.com>
References: <CADcN6fPuLM9kj=FP+f8hUiAKQbRP2s9+P01W8jWzgdUCgn5ZHQ@mail.gmail.com>
Message-ID: <552BBDEC.4040809@treenet.co.nz>

On 13/04/2015 9:58 p.m., Roman Suvorov wrote:
> Dear colleagues,
> 
> Is the setup as following possible?
> 
> A server 99.99.99.99 hosts a single Squid with ICAP plugged in. There
> two DNS records point to this server:
> first.example.com -> 99.99.99.99
> second.example.com -> 99.99.99.99
> 
> If I put first.example.com in browser proxy settings for the first
> group of users and second.example.com for the second group, can I
> somehow distinguish requests from these two groups of users without
> using explicit login-password authentication?

No you can't. The proxy FQDN is entirely internal to the browser.

> Browsers are configured
> explicitly (the proxy is not transparent). There are supposed to be
> many such groups of users, thus setting up many Squid instances or
> using different TCP ports is not appropriate.
> 
> I'm looking for a way to identify users without explicit
> authentication, maybe there are some other ways?

IP address of the client machine sending the traffic to the proxy is the
usual way. Its not perfect by any means but works better than proxy
listening IP:port combos alone.

NOTE: IPv6 connectivity between the clients and proxy can be used to
avoid many IPv4 NAT issues. Though be aware other admin have forced some
NAT types into IPv6 as well now, so even that is not as reliable as it
used to be if you are dealing with remote/Internet connections.



PS. Why your aversion to authentication? explicitly configured proxy is
*the* use-case where authentication to the proxy actually works very well.

Amos



From ywu at bitglass.com  Mon Apr 13 16:47:48 2015
From: ywu at bitglass.com (Yuhua Wu)
Date: Mon, 13 Apr 2015 09:47:48 -0700
Subject: [squid-users] does http_port sssl-bump work require-proxy-header?
Message-ID: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>

For example, is this configuration supported?

http_port 3129 require-proxy-header ssl-bump ??

By the way, we added acl rules:

acl frontend src 10.0.0.0/8
proxy_protocol_access allow frontend

Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150413/a6567120/attachment.htm>

From szabados0701 at gmail.com  Mon Apr 13 23:26:17 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Tue, 14 Apr 2015 01:26:17 +0200
Subject: [squid-users] Auth conf help
Message-ID: <CADddWko83bPQuvVwW5DNb+yAt2odpb2XBLHuHkGJSFBuMvv9RA@mail.gmail.com>

Hi,

I'd like to ask your help regarding configuring authentication with Squid.

My auth related conf:

auth_param digest program /usr/lib/squid/digest_pw_auth -c /etc/squid/passwd
auth_param digest children 5
auth_param digest realm squid
auth_param digest nonce_garbage_interval 5 minutes
auth_param digest nonce_max_duration 30 minutes
auth_param digest nonce_max_count 50
acl password proxy_auth REQUIRED
http_access allow password

My passwd file:

user1:squid:e43d870426864784e5961742f977a31b

For password, I used "pass".
If I run:
root at OpenWrt:~# echo -n 'user1:squid:pass' |md5sum
e43d870426864784e5961742f977a31b

However, when I want to authenticate, I get access denied all the time.
What did I wrong?

Thanks,
Balazs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/040c8256/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 14 02:56:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Apr 2015 14:56:33 +1200
Subject: [squid-users] does http_port sssl-bump work
	require-proxy-header?
In-Reply-To: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>
References: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>
Message-ID: <552C81E1.3020503@treenet.co.nz>

On 14/04/2015 4:47 a.m., Yuhua Wu wrote:
> For example, is this configuration supported?
> 
> http_port 3129 require-proxy-header ssl-bump ??
> 
> By the way, we added acl rules:
> 
> acl frontend src 10.0.0.0/8
> proxy_protocol_access allow frontend
> 
> Alex
> 

Yes that should work.

<http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.7>

Your above config example decrypts the traffic through the following layers:
  HTTPS over HTTP/1.x over PROXY/TCP ...

As you can see the PROXY and HTTPS layers are separate protocols that
dont interact.

Amos



From squid3 at treenet.co.nz  Tue Apr 14 03:15:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Apr 2015 15:15:33 +1200
Subject: [squid-users] Auth conf help
In-Reply-To: <CADddWko83bPQuvVwW5DNb+yAt2odpb2XBLHuHkGJSFBuMvv9RA@mail.gmail.com>
References: <CADddWko83bPQuvVwW5DNb+yAt2odpb2XBLHuHkGJSFBuMvv9RA@mail.gmail.com>
Message-ID: <552C8655.8030705@treenet.co.nz>

On 14/04/2015 11:26 a.m., Bal?zs Szabados wrote:
> Hi,
> 
> I'd like to ask your help regarding configuring authentication with Squid.
> 
> My auth related conf:
> 
> auth_param digest program /usr/lib/squid/digest_pw_auth -c /etc/squid/passwd

Possile Problem #1:
 digest_pw_auth has not existed in some years. The helpers correct
current name is digest_file_auth. Please upgrade.


> auth_param digest children 5
> auth_param digest realm squid
> auth_param digest nonce_garbage_interval 5 minutes
> auth_param digest nonce_max_duration 30 minutes
> auth_param digest nonce_max_count 50
> acl password proxy_auth REQUIRED
> http_access allow password


Possible Problem #2:
 when wrong credentials are presented the "http_access allow password"
will NOT require new ones. It will just skip to the next line - which is
an implicit "deny all"

Use this instead:
  http_access deny !password
  http_access allow localnet


Possible Problem #3:
 the client you are testing with may not support Digest authentication.
In the current Squid releases use "debug_options 11,2" in squid.conf to
get a cache.log trace of the HTTP headers the client is sending.

Amos



From timothee.christin at gmail.com  Tue Apr 14 08:35:45 2015
From: timothee.christin at gmail.com (tchristin)
Date: Tue, 14 Apr 2015 01:35:45 -0700 (PDT)
Subject: [squid-users] strip Kerberos Realm
In-Reply-To: <6360352D6CE3224194CAD0A4E56FE0BE05A9E108@exch01.numata.co.za>
References: <6360352D6CE3224194CAD0A4E56FE0BE05A9E035@exch01.numata.co.za>
 <4F6C5A97.7050801@treenet.co.nz>
 <6360352D6CE3224194CAD0A4E56FE0BE05A9E108@exch01.numata.co.za>
Message-ID: <1429000545664-4670714.post@n4.nabble.com>

Hi guys,

I need your help for the same problem : I'm not able to strip the realm from
username and the `-r` switch doesn't achieve this...

Please help me !

Tim.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/strip-Kerberos-Realm-tp4498437p4670714.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahaitoute at rinis.nl  Tue Apr 14 11:57:47 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Tue, 14 Apr 2015 13:57:47 +0200
Subject: [squid-users] handling different two way ssl-request via a proxy
	system
Message-ID: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>

Hello,

Currently we?ve got the following situation in our production environment:

Clients ?HTTP?> Apache ?HTTPS TWO-WAY SSL VIA EXTERNAL PROXY ?> HTTPS SERVERS

Just to be clear, the following services are used during this flow:

http client (firefox, chrome, curl, wget, etc.) ?> Apache (which setups two-way ssl) ?> Squid (which simply proxies https connect) ?> HTTPS services of other parties on the internet, supporting two-way ssl

We?ve realized this using the following configuration on the apache service:

LoadModule ssl_module modules/mod_ssl.so

Listen *:3128

<VirtualHost *:3128>

  SSLProxyEngine On
  SSLProxyVerify require
  SSLProxyVerifyDepth 10
  SSLProxyMachineCertificateFile /etc/httpd/certs/client.pem
  SSLProxyCACertificateFile      /etc/httpd/certs/ca.crt

  RewriteEngine On
  RewriteRule ^(.*)$ https://%{HTTP_HOST}$1 <https://%{HTTP_HOST}$1> [NC,P]


  ProxyPreserveHost On
  ProxyPass            /  https://$1/ <https://$1/>
  ProxyPassReverse     /  https://$1/ <https://$1/>

  ProxyRemote https http://192.168.68.102:3128 <http://192.168.68.102:3128/>
</VirtualHost>

We?re trying to replace the apache service by using squid. I?ve installed squid 3.1.10 on CentOS 6.6 and have realized in a development environment a two-way ssl:

http client ?> Squid 3.1.10 ?> https two-way ssl directly ?> https.example.com <http://https.example.com/>

http_port 3128 defaultsite=https.example.com vhost
cache_peer https.example.com parent 443 0 no-query originserver ssl sslcert=/etc/squid/certs/client.crt sslkey=/etc/squid/certs/client.key name=httpToHttps2way sslcafile=/etc/squid/certs/ca.crt
cache_peer_access httpToHttps2way allow all
cache deny all

There are two thing I haven?t realized in the development environment, because I don?t know how:
1. Making the Squid 3.1.10 to use a proxy system, because that?s our policy to communicate to the outside world. In apache we use the following directive: ?ProxyRemote https http://192.168.68.102:3128 <http://192.168.68.102:3128/>"
2. Making the configuration variable as much as possible. So the Squid 3.1.10 handles all different http client requests to different https servers and send them as a https two-way ssl. Currently it only handles request for https.example.com <http://https.example.com/>.

Any help is welcome.

Abdelouahed
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/94507dda/attachment.htm>

From jbaird at follett.com  Tue Apr 14 13:34:09 2015
From: jbaird at follett.com (Baird, Josh)
Date: Tue, 14 Apr 2015 13:34:09 +0000
Subject: [squid-users] Going into hit-only-mode for 5 minutes
Message-ID: <D6C04EC67151214DAD5E55E7EBF5207E42A1A593@WRXXENTEXMB01.na.follett.lan>

Hi,

We recently started having problems where our Squid 2.6 (squid-2.6.STABLE21-6.el5) proxy servers would stop serving requests.  In my cache.log, I see many of these:

2015/04/14 01:13:45| Failure Ratio at 26.15
2015/04/14 01:13:45| Going into hit-only-mode for 5 minutes...
2015/04/14 01:18:46| Failure Ratio at 3.55
2015/04/14 01:18:46| Going into hit-only-mode for 5 minutes...
2015/04/14 01:23:46| Failure Ratio at 1.02
2015/04/14 01:23:46| Going into hit-only-mode for 5 minutes...
...
2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!

I suspect this is the problem - the proxy is running out of DNS sockets.   I have already determined that there are not problems with the DNS servers that these proxies are using (in their /etc/resolv.conf).  Could this be caused by a bad user chewing up DNS sockets/children with invalid URL requests?

The "going into hit-only-mode" errors appear to be ICP related?  In this case, I believe we have ICP completely disabled:

# icp_access allow allowed_src_hosts
# icp_access deny all_src

Could anyone offer any suggestions or advice to help figure out what is causing these problems?

Thanks,

Josh



From squid3 at treenet.co.nz  Tue Apr 14 15:40:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 03:40:36 +1200
Subject: [squid-users] Going into hit-only-mode for 5 minutes
In-Reply-To: <D6C04EC67151214DAD5E55E7EBF5207E42A1A593@WRXXENTEXMB01.na.follett.lan>
References: <D6C04EC67151214DAD5E55E7EBF5207E42A1A593@WRXXENTEXMB01.na.follett.lan>
Message-ID: <552D34F4.3010809@treenet.co.nz>

On 15/04/2015 1:34 a.m., Baird, Josh wrote:
> Hi,
> 
> We recently started having problems where our Squid 2.6 (squid-2.6.STABLE21-6.el5) proxy servers would stop serving requests.  In my cache.log, I see many of these:
> 
> 2015/04/14 01:13:45| Failure Ratio at 26.15
> 2015/04/14 01:13:45| Going into hit-only-mode for 5 minutes...
> 2015/04/14 01:18:46| Failure Ratio at 3.55
> 2015/04/14 01:18:46| Going into hit-only-mode for 5 minutes...
> 2015/04/14 01:23:46| Failure Ratio at 1.02
> 2015/04/14 01:23:46| Going into hit-only-mode for 5 minutes...
> ...
> 2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
> 2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
> 2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
> 2015/04/14 06:50:58| idnsSendQuery: Can't send query, no DNS socket!
> 
> I suspect this is the problem - the proxy is running out of DNS sockets.   I have already determined that there are not problems with the DNS servers that these proxies are using (in their /etc/resolv.conf).  Could this be caused by a bad user chewing up DNS sockets/children with invalid URL requests?
> 


The older the proxy the more ways there are to perform Denial of Service
by consuming all the port and sockets on the *entire* server Squid runs
on. Probably one of those happening to you.


> The "going into hit-only-mode" errors appear to be ICP related?  In this case, I believe we have ICP completely disabled:
> 
> # icp_access allow allowed_src_hosts
> # icp_access deny all_src

You would be wrong. This is how to disable ICP receiving:

 icp_port 0


On the senders you change the cache_peer lines to set the icp-port
parameter (the second port number) to 0.

> 
> Could anyone offer any suggestions or advice to help figure out what is causing these problems?

1) upgrade.

2) seriously, upgrade.

3) try adding "via on" to your squid.conf. If you start to get warnings
about forwarding loops its working. Otherwise you got big problems - see
(2).


Amos


From squid3 at treenet.co.nz  Tue Apr 14 16:20:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 04:20:51 +1200
Subject: [squid-users] handling different two way ssl-request via a
 proxy system
In-Reply-To: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>
References: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>
Message-ID: <552D3E63.7080704@treenet.co.nz>

On 14/04/2015 11:57 p.m., Abdelouahed Haitoute wrote:
> 

> There are two thing I haven?t realized in the development
> environment, because I don?t know how:
> 1. Making the Squid 3.1.10 to use a proxy system, because that?s our
> policy to communicate to the outside world. In apache we use the
> following directive: ?ProxyRemote https http://192.168.68.102:3128
> <http://192.168.68.102:3128/>"

In squid.conf:

 cache_peer 192.168.68.102 parent 3128 0


>
> 2. Making the configuration variable as much as possible. So the
> Squid 3.1.10 handles all different http client requests to different
> https servers and send them as a https two-way ssl. Currently it
> only handles request for https.example.com
> <http://https.example.com/>.


Use the sslproxy_* directives instead of cache_peer.


However you have two problems:

Problem A)
 requirements #1 and #2 above are mutually exclusive.

 Obeying #1 involves sending traffic from Squid to the parent proxy for
action.

 Obeying #2 involves opening direct TLS connections from Squid to the
origin servers.


Problem B)
 Translating between http:// and https:// is explicitly forbidden in
both HTTP and HTTPS protocol security requirements.

 Squid does not permit that highly dangerous action to be taken. However
there are several other possibilities depending on what you actually
need done.


Amos



From arthur at cdne.net  Tue Apr 14 16:25:10 2015
From: arthur at cdne.net (HiP-HiPpO)
Date: Tue, 14 Apr 2015 16:25:10 +0000
Subject: [squid-users] Configuration assistance
Message-ID: <20150414162510.Horde.RocSQmanB7yknBeoxwuL8A1@mail.cdne.net>

Hello-

? I'm using squid version 3.5.3 and I could use some help with
configuration.

? Squid will be installed at AWS, most clients will be within a corporate
network and will not be able to access the service by configuring proxy
settings in a browser.? Instead DNS will be used to resolve to the Squid
service.? The Squid service will authenticate users via SLDAP.? The
service will need to be able to intercept both clear and TLS HTTP traffic
in order to insert an Authorization header.

? The request flow would be something like;

Client browser requests http(s)://proxy-hostX.test.com
(where 0 < X < 50)

Squid will need to intercept the request and add an Authorization header.
Squid will need to rewrite the request to the origin server.? In this
case, the origin server name is hostX.test.com

Squid will then intercept the server response and direct that response to
an ICAP server to modify all embedded links to be the same as the
request.? All links in the response will need to be rewritten to
http(s)://proxy-hostX.test.com

I have official wildcard certificates for the domain.? i.e. *.test.com

Squid will also need to retrieve group memberships from SLDAP and authorize
user access to hostX based on group memberships.? If the user is in the
groups host30 and host43, then access is allowed to only host30.test.com
and host43.test.com

Thank you in advance for all assistance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/d2b5c5b5/attachment.htm>

From ywu at bitglass.com  Tue Apr 14 17:05:39 2015
From: ywu at bitglass.com (Yuhua Wu)
Date: Tue, 14 Apr 2015 10:05:39 -0700
Subject: [squid-users] does http_port sssl-bump work
	require-proxy-header?
In-Reply-To: <552C81E1.3020503@treenet.co.nz>
References: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>
 <552C81E1.3020503@treenet.co.nz>
Message-ID: <CAMjA_BkeMG4wUvuS_BEBdiF=5Xz9KU--oaLFmiapM1+daWPZtw@mail.gmail.com>

I think, in the sslbump mode, if PROXY protocol is enabled, client cannot
set up the SSL tunnel with squid after CONNECT call succeeds. I remember
that HAProxy will send PROXY protocol line during ssl negotiation. If squid
does not parse the PROXY protocol header during SSL negotiation, this will
cause the problem.

Alex

On Mon, Apr 13, 2015 at 7:56 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/04/2015 4:47 a.m., Yuhua Wu wrote:
> > For example, is this configuration supported?
> >
> > http_port 3129 require-proxy-header ssl-bump ??
> >
> > By the way, we added acl rules:
> >
> > acl frontend src 10.0.0.0/8
> > proxy_protocol_access allow frontend
> >
> > Alex
> >
>
> Yes that should work.
>
> <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.7>
>
> Your above config example decrypts the traffic through the following
> layers:
>   HTTPS over HTTP/1.x over PROXY/TCP ...
>
> As you can see the PROXY and HTTPS layers are separate protocols that
> dont interact.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/7e2aac7d/attachment.htm>

From jbaird at follett.com  Tue Apr 14 18:04:08 2015
From: jbaird at follett.com (Baird, Josh)
Date: Tue, 14 Apr 2015 18:04:08 +0000
Subject: [squid-users] Going into hit-only-mode for 5 minutes
In-Reply-To: <552D34F4.3010809@treenet.co.nz>
References: <D6C04EC67151214DAD5E55E7EBF5207E42A1A593@WRXXENTEXMB01.na.follett.lan>
 <552D34F4.3010809@treenet.co.nz>
Message-ID: <D6C04EC67151214DAD5E55E7EBF5207E42A1BF54@WRXXENTEXMB01.na.follett.lan>

> > Could anyone offer any suggestions or advice to help figure out what is
> causing these problems?
> 
> 1) upgrade.
> 
> 2) seriously, upgrade.
> 
> 3) try adding "via on" to your squid.conf. If you start to get warnings about
> forwarding loops its working. Otherwise you got big problems - see (2).

Could the 'Going into hit-only-mode for 5 minute' messages be attributed to spotty/slow DNS resolution, though?  When a proxy is in 'hit-only-mode,' is it able to respond to normal (non ICP) clients?

Josh

From ahaitoute at rinis.nl  Tue Apr 14 19:12:44 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Tue, 14 Apr 2015 21:12:44 +0200
Subject: [squid-users] handling different two way ssl-request via a
	proxy system
In-Reply-To: <552D3E63.7080704@treenet.co.nz>
References: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>
 <552D3E63.7080704@treenet.co.nz>
Message-ID: <206AD888-469E-40DF-9402-2AAC69BFEE3C@rinis.nl>

Hello Amos,

Thank you for your reply.

> Problem A)
> requirements #1 and #2 above are mutually exclusive.
> 
> Obeying #1 involves sending traffic from Squid to the parent proxy for
> action.
> 
> Obeying #2 involves opening direct TLS connections from Squid to the
> origin servers.

Is it possible to send the TLS connection as a HTTP CONNECT tunnel via through the parent proxy? (http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling <http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling>)

Abdelouahed

> Op 14 apr. 2015, om 18:20 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
> On 14/04/2015 11:57 p.m., Abdelouahed Haitoute wrote:
>> 
> 
>> There are two thing I haven?t realized in the development
>> environment, because I don?t know how:
>> 1. Making the Squid 3.1.10 to use a proxy system, because that?s our
>> policy to communicate to the outside world. In apache we use the
>> following directive: ?ProxyRemote https http://192.168.68.102:3128
>> <http://192.168.68.102:3128/>"
> 
> In squid.conf:
> 
> cache_peer 192.168.68.102 parent 3128 0
> 
> 
>> 
>> 2. Making the configuration variable as much as possible. So the
>> Squid 3.1.10 handles all different http client requests to different
>> https servers and send them as a https two-way ssl. Currently it
>> only handles request for https.example.com
>> <http://https.example.com/>.
> 
> 
> Use the sslproxy_* directives instead of cache_peer.
> 
> 
> However you have two problems:
> 
> Problem A)
> requirements #1 and #2 above are mutually exclusive.
> 
> Obeying #1 involves sending traffic from Squid to the parent proxy for
> action.
> 
> Obeying #2 involves opening direct TLS connections from Squid to the
> origin servers.
> 
> 
> Problem B)
> Translating between http:// and https:// is explicitly forbidden in
> both HTTP and HTTPS protocol security requirements.
> 
> Squid does not permit that highly dangerous action to be taken. However
> there are several other possibilities depending on what you actually
> need done.
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/8686a5db/attachment.htm>

From vdoctor at neuf.fr  Tue Apr 14 20:40:03 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 14 Apr 2015 13:40:03 -0700 (PDT)
Subject: [squid-users] ***SPAM*** Re:  Random SSL bump DB corruption
In-Reply-To: <1428921772616-4670708.post@n4.nabble.com>
References: <552281E7.8060405@gmail.com>
 <1428564569076-4670656.post@n4.nabble.com> <552675E1.6090502@gmail.com>
 <029d01d072c5$25e5b800$71b12800$@fr> <552678B9.3090309@gmail.com>
 <02ad01d072c6$70601320$51203960$@fr> <55267B46.2010804@gmail.com>
 <02d601d072d1$390562d0$ab102870$@fr> <55272F87.40607@treenet.co.nz>
 <1428921772616-4670708.post@n4.nabble.com>
Message-ID: <1429044003278-4670723.post@n4.nabble.com>

Hi All,

No reply ?
Do we have to leave with this mega/crazy bug ?
Is there someone in the Squid team able to have a look to this problem or
nobody care ?

Thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670723.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From guy.helmer at gmail.com  Tue Apr 14 21:07:36 2015
From: guy.helmer at gmail.com (Guy Helmer)
Date: Tue, 14 Apr 2015 16:07:36 -0500
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1429044003278-4670723.post@n4.nabble.com>
References: <552281E7.8060405@gmail.com>
 <1428564569076-4670656.post@n4.nabble.com> <552675E1.6090502@gmail.com>
 <029d01d072c5$25e5b800$71b12800$@fr> <552678B9.3090309@gmail.com>
 <02ad01d072c6$70601320$51203960$@fr> <55267B46.2010804@gmail.com>
 <02d601d072d1$390562d0$ab102870$@fr> <55272F87.40607@treenet.co.nz>
 <1428921772616-4670708.post@n4.nabble.com>
 <1429044003278-4670723.post@n4.nabble.com>
Message-ID: <92756812-C713-4B77-84EF-A0CF0395C987@gmail.com>


> On Apr 14, 2015, at 3:40 PM, Stakres <vdoctor at neuf.fr> wrote:
> 
> Hi All,
> 
> No reply ?
> Do we have to leave with this mega/crazy bug ?
> Is there someone in the Squid team able to have a look to this problem or
> nobody care ?
> 
> Thanks in advance.
> 
> Bye Fred
> 

The issue I?m aware of (bug 4212) has been resolved.

Regards,
Guy

From vdoctor at neuf.fr  Tue Apr 14 21:06:57 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 14 Apr 2015 14:06:57 -0700 (PDT)
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <92756812-C713-4B77-84EF-A0CF0395C987@gmail.com>
References: <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com> <02d601d072d1$390562d0$ab102870$@fr>
 <55272F87.40607@treenet.co.nz> <1428921772616-4670708.post@n4.nabble.com>
 <1429044003278-4670723.post@n4.nabble.com>
 <92756812-C713-4B77-84EF-A0CF0395C987@gmail.com>
Message-ID: <1429045617136-4670725.post@n4.nabble.com>

Hi Guy,

Thanks for answering :o)
Based on the bugzilla, it's fixed but not yet available.
Anyway, that's a very good news.
Let's wait the next build.

Thanks for your help.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670725.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Apr 14 21:16:01 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 14 Apr 2015 14:16:01 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
Message-ID: <1429046161591-4670726.post@n4.nabble.com>

[New LWP 4677]
[New LWP 4704]
[New LWP 4687]
[New LWP 4702]
[New LWP 4695]
[New LWP 4725]
[New LWP 4697]
[New LWP 4490]
[New LWP 4493]
[New LWP 4473]
[New LWP 4508]
[New LWP 4495]
[New LWP 4460]
[New LWP 4511]
[New LWP 4516]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f609f45a3e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x000000000063bcd2 in xassert ()
#3  0x00000000007996b7 in comm_read(RefCount<Comm::Connection> const&,
char*, int, RefCount<AsyncCall>&) ()
#4  0x0000000000709f51 in
StoreEntry::delayAwareRead(RefCount<Comm::Connection> const&, char*, int,
RefCount<AsyncCall>) ()
#5  0x0000000000697b72 in HttpStateData::maybeReadVirginBody() ()
#6  0x000000000069a101 in HttpStateData::sendRequest() ()
#7  0x000000000069adea in HttpStateData::start() ()
#8  0x000000000079610f in NullaryMemFunT<AsyncJob>::doDial() ()
#9  0x000000000079615b in JobDialer<AsyncJob>::dial(AsyncCall&) ()
#10 0x0000000000795dbf in AsyncCallT<NullaryMemFunT&lt;AsyncJob> >::fire()
()
#11 0x00000000007933a8 in AsyncCall::make() ()
#12 0x00000000007968d3 in AsyncCallQueue::fireNext() ()
#13 0x000000000079665e in AsyncCallQueue::fire() ()
#14 0x000000000064df2b in EventLoop::dispatchCalls() ()
#15 0x000000000064ddc3 in EventLoop::runOnce() ()
#16 0x000000000064dbd1 in EventLoop::run() ()
#17 0x00000000006c967b in SquidMain(int, char**) ()
#18 0x00000000006c8d52 in SquidMainSafe(int, char**) ()
#19 0x00000000006c8d2f in main ()
(gdb) ^CQuit









./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--with-pthreads  --with-aufs-threads=512 --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
--enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
--enable-http-violations --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-swapdir=/var/spool/squid --enable-ltdl-convenience
--with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
--enable-referer-log --enable-truncate --disable-unlinkd
--enable-useragent-log --enable-eui --enable-large-cache-files
'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From szabados0701 at gmail.com  Tue Apr 14 21:46:30 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Tue, 14 Apr 2015 23:46:30 +0200
Subject: [squid-users] Auth conf help
Message-ID: <CADddWkpJFhRuPv2+6LGAheEaPatCFm7eWHze9bOq-mwZNYqBqw@mail.gmail.com>

Possile Problem #1:
 digest_pw_auth has not existed in some years. The helpers correct
current name is digest_file_auth. Please upgrade.

In the openwrt repository, I can only find Squid 2.7, and I can't upgrade
to newer.
Can I make this work somehow, with the current setup?

Possible Problem #2:
 when wrong credentials are presented the "http_access allow password"
will NOT require new ones. It will just skip to the next line - which is
an implicit "deny all"

Use this instead:
  http_access deny !password
  http_access allow localnet

Tried it, but the issue persists. Actually, I just noticed that the
http_access allow localnet already was present in the config, I forgot to
include in my previous mail.

Possible Problem #3:
 the client you are testing with may not support Digest authentication.
In the current Squid releases use "debug_options 11,2" in squid.conf to
get a cache.log trace of the HTTP headers the client is sending.

I've tried with IExplorer, Chrome and curl, I see this in the access log:

2 192.168.1.177 TCP_DENIED/407 1722 GET http://www.bing.com/news? user1
NONE/- text/html

Balazs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/3676615d/attachment.htm>

From sdevadas at vistaprint.com  Tue Apr 14 21:49:19 2015
From: sdevadas at vistaprint.com (Sriram Devadas)
Date: Tue, 14 Apr 2015 21:49:19 +0000
Subject: [squid-users] cache-control: no-cache="set-cookie" prevents caching
In-Reply-To: <FEB18889A0240C4999CB0826662031103DF6D587@wndmail01.vistaprint.net>
References: <FEB18889A0240C4999CB0826662031103DF6D587@wndmail01.vistaprint.net>
Message-ID: <FEB18889A0240C4999CB0826662031103DF6D5C7@wndmail01.vistaprint.net>

Squid version 3.5.3.
When the http response received by Squid contains a no-cache="set-cookie", the response is not cached. cache.log has the line:
2015/04/14 18:24:38.027 kid1| http.cc(359) cacheableReply: NO because server reply Cache-Control:no-cache has parameters

The relevant source code is in http.cc:
        if (rep->cache_control && rep->cache_control->hasNoCache() && rep->cache_control->noCache().size() > 0) {
            /* TODO: we are allowed to cache when no-cache= has parameters.
             * Provided we strip away any of the listed headers unless they are revalidated
             * successfully (ie, must revalidate AND these headers are prohibited on stale replies).
             * That is a bit tricky for squid right now so we avoid caching entirely.
             */
            debugs(22, 3, HERE << "NO because server reply Cache-Control:no-cache has parameters");
            return 0;
        }

I have read the TODO comment but do not know if this applies to the set-cookie parameter too.
Is it okay to add a condition here not to return if the cache-control header contains only no-cache="set-cookie"?
In client_side_reply.cc the no-cache="set-cookie" seems to be handled correctly, the set-cookie header is removed in the response, which I am guessing removes this header correctly in the response:
    if (is_hit)
        hdr->delById(HDR_SET_COOKIE);

Thanks,
sdevadas



From ywu at bitglass.com  Tue Apr 14 22:14:21 2015
From: ywu at bitglass.com (Yuhua Wu)
Date: Tue, 14 Apr 2015 15:14:21 -0700
Subject: [squid-users] does http_port sssl-bump work
	require-proxy-header?
In-Reply-To: <CAMjA_BkeMG4wUvuS_BEBdiF=5Xz9KU--oaLFmiapM1+daWPZtw@mail.gmail.com>
References: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>
 <552C81E1.3020503@treenet.co.nz>
 <CAMjA_BkeMG4wUvuS_BEBdiF=5Xz9KU--oaLFmiapM1+daWPZtw@mail.gmail.com>
Message-ID: <CAMjA_B=KXV7Qe+wB2wkoyyXqjrs9q_DsCAjRc7Rpfy+Ucg9_Cw@mail.gmail.com>

I found out what is wrong, but I am not familar to squid code, so I post
here to see if someone can show me the next step:

The problem is at this part of code:
void
ClientHttpRequest::sslBumpStart()
{
    debugs(85, 5, HERE << "Confirming " << Ssl::bumpMode(sslBumpNeed_) <<
           "-bumped CONNECT tunnel on FD " << getConn()->clientConnection);
    getConn()->sslBumpMode = sslBumpNeed_;

    AsyncCall::Pointer bumpCall = commCbCall(85, 5,
"ClientSocketContext::sslBumpEstablish",
                                  CommIoCbPtrFun(&SslBumpEstablish, this));

    if (request->flags.interceptTproxy || request->flags.intercepted) {
        CommIoCbParams &params = GetCommParams<CommIoCbParams>(bumpCall);
        params.flag = Comm::OK;
        params.conn = getConn()->clientConnection;
        ScheduleCallHere(bumpCall);
        return;
    }

    // send an HTTP 200 response to kick client SSL negotiation
    // TODO: Unify with tunnel.cc and add a Server(?) header
    static const char *const conn_established = "HTTP/1.1 200 Connection
established\r\n\r\n";
    Comm::Write(getConn()->clientConnection, conn_established,
strlen(conn_established), bumpCall, NULL);
}

if require-proxy-header is not used, then request->flags.interceptTproxy is
0, and when requir-proxy-header is used, the
request->flags.interceptTproxy is 1!

since request->flags.interceptTproxy is 1, the 200 status code for CONNECT
call is not sent. (The last part of code sending 200 status code is
skipped.)

Any kind help?

Alex


On Tue, Apr 14, 2015 at 10:05 AM, Yuhua Wu <ywu at bitglass.com> wrote:

> I think, in the sslbump mode, if PROXY protocol is enabled, client cannot
> set up the SSL tunnel with squid after CONNECT call succeeds. I remember
> that HAProxy will send PROXY protocol line during ssl negotiation. If squid
> does not parse the PROXY protocol header during SSL negotiation, this will
> cause the problem.
>
> Alex
>
> On Mon, Apr 13, 2015 at 7:56 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 14/04/2015 4:47 a.m., Yuhua Wu wrote:
>> > For example, is this configuration supported?
>> >
>> > http_port 3129 require-proxy-header ssl-bump ??
>> >
>> > By the way, we added acl rules:
>> >
>> > acl frontend src 10.0.0.0/8
>> > proxy_protocol_access allow frontend
>> >
>> > Alex
>> >
>>
>> Yes that should work.
>>
>> <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.7>
>>
>> Your above config example decrypts the traffic through the following
>> layers:
>>   HTTPS over HTTP/1.x over PROXY/TCP ...
>>
>> As you can see the PROXY and HTTPS layers are separate protocols that
>> dont interact.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/c4170140/attachment.htm>

From hectorchan at gmail.com  Tue Apr 14 23:33:08 2015
From: hectorchan at gmail.com (Hector Chan)
Date: Tue, 14 Apr 2015 16:33:08 -0700
Subject: [squid-users] Delay pool change
Message-ID: <CAEhCwUyw53YKNE3F=0xQ4r8chvyrTsA-Pc-d=RV1UroA1dRPeQ@mail.gmail.com>

Hi all,

If I configure a new delay pool in the config file or reconfigure an
existing delay pool, do I have to restart squid?  Can I send a SIGHUP to it
to re-read the config file instead ?  If I send a SIGHUP, what would happen
to downloads that are in progress ?

Thanks,
Hector
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/4004c381/attachment.htm>

From nathan at getoffmalawn.com  Wed Apr 15 00:29:35 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Wed, 15 Apr 2015 10:29:35 +1000
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429046161591-4670726.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
Message-ID: <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>

Hello,

I believe you're experiencing bug 3329:
http://bugs.squid-cache.org/show_bug.cgi?id=3329

Please try the patch that I have on there and see if that helps your issue.

Thank you,

Nathan.

On 15 April 2015 at 07:16, HackXBack <hack.back at hotmail.com> wrote:
> [New LWP 4677]
> [New LWP 4704]
> [New LWP 4687]
> [New LWP 4702]
> [New LWP 4695]
> [New LWP 4725]
> [New LWP 4697]
> [New LWP 4490]
> [New LWP 4493]
> [New LWP 4473]
> [New LWP 4508]
> [New LWP 4495]
> [New LWP 4460]
> [New LWP 4511]
> [New LWP 4516]
>
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
> Program terminated with signal 6, Aborted.
> #0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) backtrace
> #0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007f609f45a3e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x000000000063bcd2 in xassert ()
> #3  0x00000000007996b7 in comm_read(RefCount<Comm::Connection> const&,
> char*, int, RefCount<AsyncCall>&) ()
> #4  0x0000000000709f51 in
> StoreEntry::delayAwareRead(RefCount<Comm::Connection> const&, char*, int,
> RefCount<AsyncCall>) ()
> #5  0x0000000000697b72 in HttpStateData::maybeReadVirginBody() ()
> #6  0x000000000069a101 in HttpStateData::sendRequest() ()
> #7  0x000000000069adea in HttpStateData::start() ()
> #8  0x000000000079610f in NullaryMemFunT<AsyncJob>::doDial() ()
> #9  0x000000000079615b in JobDialer<AsyncJob>::dial(AsyncCall&) ()
> #10 0x0000000000795dbf in AsyncCallT<NullaryMemFunT<AsyncJob> >::fire()
> ()
> #11 0x00000000007933a8 in AsyncCall::make() ()
> #12 0x00000000007968d3 in AsyncCallQueue::fireNext() ()
> #13 0x000000000079665e in AsyncCallQueue::fire() ()
> #14 0x000000000064df2b in EventLoop::dispatchCalls() ()
> #15 0x000000000064ddc3 in EventLoop::runOnce() ()
> #16 0x000000000064dbd1 in EventLoop::run() ()
> #17 0x00000000006c967b in SquidMain(int, char**) ()
> #18 0x00000000006c8d52 in SquidMainSafe(int, char**) ()
> #19 0x00000000006c8d2f in main ()
> (gdb) ^CQuit
>
>
>
>
>
>
>
>
>
> ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
> --libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
> --libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
> --infodir=/usr/share/info --mandir=/usr/share/man
> --disable-dependency-tracking --disable-strict-error-checking
> --with-pthreads  --with-aufs-threads=512 --enable-storeio=ufs,aufs
> --enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
> --enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
> --enable-http-violations --enable-linux-netfilter
> --enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
> --with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
> --with-swapdir=/var/spool/squid --enable-ltdl-convenience
> --with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
> --enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
> --enable-referer-log --enable-truncate --disable-unlinkd
> --enable-useragent-log --enable-eui --enable-large-cache-files
> 'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
> 'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
> 'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ywu at bitglass.com  Wed Apr 15 01:23:17 2015
From: ywu at bitglass.com (Yuhua Wu)
Date: Tue, 14 Apr 2015 18:23:17 -0700
Subject: [squid-users] does http_port sssl-bump work
	require-proxy-header?
In-Reply-To: <CAMjA_B=KXV7Qe+wB2wkoyyXqjrs9q_DsCAjRc7Rpfy+Ucg9_Cw@mail.gmail.com>
References: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>
 <552C81E1.3020503@treenet.co.nz>
 <CAMjA_BkeMG4wUvuS_BEBdiF=5Xz9KU--oaLFmiapM1+daWPZtw@mail.gmail.com>
 <CAMjA_B=KXV7Qe+wB2wkoyyXqjrs9q_DsCAjRc7Rpfy+Ucg9_Cw@mail.gmail.com>
Message-ID: <CAMjA_BkbAeRBTrzFv5vRzADWWontOufuCpeMvh9Lx12PvRUx4Q@mail.gmail.com>

I worked a fix:

diff --git a/squid-3.5.1/src/client_side.cc b/squid-3.5.1/src/client_side.cc
index d72e8c4..025316d 100644
--- a/squid-3.5.1/src/client_side.cc
+++ b/squid-3.5.1/src/client_side.cc
@@ -3045,7 +3045,8 @@ ConnStateData::parseProxy1p0()
         debugs(33, 5, "PROXY/1.0 protocol on connection " <<
clientConnection);
         clientConnection->local = originalDest;
         clientConnection->remote = originalClient;
-        clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
+        if (clientConnection->flags & COMM_TRANSPARENT)
+            clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
         debugs(33, 5, "PROXY/1.0 upgrade: " << clientConnection);

         // repeat fetch ensuring the new client FQDN can be logged
@@ -3135,14 +3136,16 @@ ConnStateData::parseProxy2p0()
         clientConnection->local.port(ntohs(ipu.ipv4_addr.dst_port));
         clientConnection->remote = ipu.ipv4_addr.src_addr;
         clientConnection->remote.port(ntohs(ipu.ipv4_addr.src_port));
-        clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
+        if (clientConnection->flags & COMM_TRANSPARENT)
+            clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
         break;
     case 0x2: // IPv6         clientConnection->local =
ipu.ipv6_addr.dst_addr;
         clientConnection->local.port(ntohs(ipu.ipv6_addr.dst_port));
         clientConnection->remote = ipu.ipv6_addr.src_addr;
         clientConnection->remote.port(ntohs(ipu.ipv6_addr.src_port));
-        clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
+        if (clientConnection->flags & COMM_TRANSPARENT)
+            clientConnection->flags ^= COMM_TRANSPARENT; // prevent TPROXY
spoofing of this new IP.
         break;
     default: // do nothing
         break;

I assume the intention of code is to turn off COMM_TRANSPARENT if PROXY
protocol is used.

Is this proper change? At least, it works for me now,

Alex


On Tue, Apr 14, 2015 at 3:14 PM, Yuhua Wu <ywu at bitglass.com> wrote:

> I found out what is wrong, but I am not familar to squid code, so I post
> here to see if someone can show me the next step:
>
> The problem is at this part of code:
> void
> ClientHttpRequest::sslBumpStart()
> {
>     debugs(85, 5, HERE << "Confirming " << Ssl::bumpMode(sslBumpNeed_) <<
>            "-bumped CONNECT tunnel on FD " << getConn()->clientConnection);
>     getConn()->sslBumpMode = sslBumpNeed_;
>
>     AsyncCall::Pointer bumpCall = commCbCall(85, 5,
> "ClientSocketContext::sslBumpEstablish",
>                                   CommIoCbPtrFun(&SslBumpEstablish, this));
>
>     if (request->flags.interceptTproxy || request->flags.intercepted) {
>         CommIoCbParams &params = GetCommParams<CommIoCbParams>(bumpCall);
>         params.flag = Comm::OK;
>         params.conn = getConn()->clientConnection;
>         ScheduleCallHere(bumpCall);
>         return;
>     }
>
>     // send an HTTP 200 response to kick client SSL negotiation
>     // TODO: Unify with tunnel.cc and add a Server(?) header
>     static const char *const conn_established = "HTTP/1.1 200 Connection
> established\r\n\r\n";
>     Comm::Write(getConn()->clientConnection, conn_established,
> strlen(conn_established), bumpCall, NULL);
> }
>
> if require-proxy-header is not used, then request->flags.interceptTproxy
> is 0, and when requir-proxy-header is used, the
> request->flags.interceptTproxy is 1!
>
> since request->flags.interceptTproxy is 1, the 200 status code for CONNECT
> call is not sent. (The last part of code sending 200 status code is
> skipped.)
>
> Any kind help?
>
> Alex
>
>
> On Tue, Apr 14, 2015 at 10:05 AM, Yuhua Wu <ywu at bitglass.com> wrote:
>
>> I think, in the sslbump mode, if PROXY protocol is enabled, client cannot
>> set up the SSL tunnel with squid after CONNECT call succeeds. I remember
>> that HAProxy will send PROXY protocol line during ssl negotiation. If squid
>> does not parse the PROXY protocol header during SSL negotiation, this will
>> cause the problem.
>>
>> Alex
>>
>> On Mon, Apr 13, 2015 at 7:56 PM, Amos Jeffries <squid3 at treenet.co.nz>
>> wrote:
>>
>>> On 14/04/2015 4:47 a.m., Yuhua Wu wrote:
>>> > For example, is this configuration supported?
>>> >
>>> > http_port 3129 require-proxy-header ssl-bump ??
>>> >
>>> > By the way, we added acl rules:
>>> >
>>> > acl frontend src 10.0.0.0/8
>>> > proxy_protocol_access allow frontend
>>> >
>>> > Alex
>>> >
>>>
>>> Yes that should work.
>>>
>>> <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.7>
>>>
>>> Your above config example decrypts the traffic through the following
>>> layers:
>>>   HTTPS over HTTP/1.x over PROXY/TCP ...
>>>
>>> As you can see the PROXY and HTTPS layers are separate protocols that
>>> dont interact.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150414/5a0904f2/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr 15 08:25:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 20:25:06 +1200
Subject: [squid-users] cache-control: no-cache="set-cookie" prevents
	caching
In-Reply-To: <FEB18889A0240C4999CB0826662031103DF6D5C7@wndmail01.vistaprint.net>
References: <FEB18889A0240C4999CB0826662031103DF6D587@wndmail01.vistaprint.net>
 <FEB18889A0240C4999CB0826662031103DF6D5C7@wndmail01.vistaprint.net>
Message-ID: <552E2062.7020800@treenet.co.nz>

On 15/04/2015 9:49 a.m., Sriram Devadas wrote:
> Squid version 3.5.3.
> When the http response received by Squid contains a no-cache="set-cookie", the response is not cached. cache.log has the line:
> 2015/04/14 18:24:38.027 kid1| http.cc(359) cacheableReply: NO because server reply Cache-Control:no-cache has parameters
> 
> The relevant source code is in http.cc:
>         if (rep->cache_control && rep->cache_control->hasNoCache() && rep->cache_control->noCache().size() > 0) {
>             /* TODO: we are allowed to cache when no-cache= has parameters.
>              * Provided we strip away any of the listed headers unless they are revalidated
>              * successfully (ie, must revalidate AND these headers are prohibited on stale replies).
>              * That is a bit tricky for squid right now so we avoid caching entirely.
>              */
>             debugs(22, 3, HERE << "NO because server reply Cache-Control:no-cache has parameters");
>             return 0;
>         }
> 
> I have read the TODO comment but do not know if this applies to the set-cookie parameter too.
> Is it okay to add a condition here not to return if the cache-control header contains only no-cache="set-cookie"?

I dont think so. At least by itself that is not enough.

The existing code is there to enact the generic fail-safe action
permitted of the HTTP specification. To treat any unsupported no-cache
condition from the server as a no-store.

That is currently being done because there has been zero testing of the
code paths your propose change would enable. Please feel free to go
ahead and do that testing, patches that improve the caching are very
welcome.

But be aware there are very likely also some missing code elsewhere that
causes broken behaviour. This change requires that the header stripping
or updating on replies to clients operates correctly in the presence of
all possibly 20x and 30x response codes. More on that below...


> In client_side_reply.cc the no-cache="set-cookie" seems to be handled correctly, the set-cookie header is removed in the response, which I am guessing removes this header correctly in the response:
>     if (is_hit)
>         hdr->delById(HDR_SET_COOKIE);
> 

Well no, this would likely (untested) cause a significant amount of
traffic to have wrongly missing Set-Cookie. Today that traffic is a
MISS, but MiSS guarantees correct headers.

The (is_hit) condition is wrongly positioned in the code sequence (far
too late currently). Its also not handling any other headers that could
be listed by no-cache="...".

There are two options that would do it right, for all headers as easily
as for Set-Cookie alone:

1) going through the no-cache list and stripping away all those headers
from the cached copy before it got stored (and in collapsed-forwarding
code paths before object sharing).

or,

2) The same removal of headers in the code where object is loaded from
cache (and in collapsed-forwarding code paths before object sharing).
The header needs to be erased from the cached copy that could be sent to
clients, AND all same-named headers the origin server provides on its
revalidation 30x response needs to be added in their place later in
those 30x handling code paths.

Amos


From squid3 at treenet.co.nz  Wed Apr 15 08:30:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 20:30:08 +1200
Subject: [squid-users] Going into hit-only-mode for 5 minutes
In-Reply-To: <D6C04EC67151214DAD5E55E7EBF5207E42A1BF54@WRXXENTEXMB01.na.follett.lan>
References: <D6C04EC67151214DAD5E55E7EBF5207E42A1A593@WRXXENTEXMB01.na.follett.lan>
 <552D34F4.3010809@treenet.co.nz>
 <D6C04EC67151214DAD5E55E7EBF5207E42A1BF54@WRXXENTEXMB01.na.follett.lan>
Message-ID: <552E2190.60500@treenet.co.nz>

On 15/04/2015 6:04 a.m., Baird, Josh wrote:
>>> Could anyone offer any suggestions or advice to help figure out what is
>> causing these problems?
>>
>> 1) upgrade.
>>
>> 2) seriously, upgrade.
>>
>> 3) try adding "via on" to your squid.conf. If you start to get warnings about
>> forwarding loops its working. Otherwise you got big problems - see (2).
> 
> Could the 'Going into hit-only-mode for 5 minute' messages be attributed to spotty/slow DNS resolution, though?  When a proxy is in 'hit-only-mode,' is it able to respond to normal (non ICP) clients?
> 

Could be many things. It just means Squid has noticed ICP queries are
taking too long to respond to so its only responding to the HIT ones for
a while.

The problem is usually elsewhere, and you have plenty of other clues
about problems with the socket exhaustion and DNS issues to fix. The ICP
behaviour is very minor compared to those.

Amos



From alex at samad.com.au  Wed Apr 15 08:32:39 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 15 Apr 2015 18:32:39 +1000
Subject: [squid-users] tcp_outgoing_address
Message-ID: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>

Hi

I have squid-3.5.2-2.el6.x86_64 on centos 6.6

I am trying to direct certain destinations from certain ip addresses


acl viaTest dstdomain .abc.com

tcp_outgoing_address 192.168.11.11 viaTEst

This works well for

www.abc.com and test.abc.com when they resolve to ipv4 addresses
but when they resolved to ipv6 it fails :(

so I tried added

dns_v4_first on

but it doesn't seem to help :(

So am i right in presuming that because the name resolution happens
first and because it goes to IPv6 it will not going out with a src of
192.168.11.11

why doesn't the ipv4 first flag work ?


From squid3 at treenet.co.nz  Wed Apr 15 08:34:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 20:34:50 +1200
Subject: [squid-users] strip Kerberos Realm
In-Reply-To: <1429000545664-4670714.post@n4.nabble.com>
References: <6360352D6CE3224194CAD0A4E56FE0BE05A9E035@exch01.numata.co.za>
 <4F6C5A97.7050801@treenet.co.nz>
 <6360352D6CE3224194CAD0A4E56FE0BE05A9E108@exch01.numata.co.za>
 <1429000545664-4670714.post@n4.nabble.com>
Message-ID: <552E22AA.3010909@treenet.co.nz>

On 14/04/2015 8:35 p.m., tchristin wrote:
> Hi guys,
> 
> I need your help for the same problem : I'm not able to strip the realm from
> username and the `-r` switch doesn't achieve this...

Of course. username and realm are very different things.
 What -r does is strip the realm from the user *label* within Negotiate
messages. Separating the user *name* piece out of the *label* syntax.

If -r is not working for you, then you have a different problem.
Possibly your installation needs an upgrade to supprt the -r option. Or
maybe many other things. We need details of *your* problem in order to
provide any better help (please start a new thread though).

Amos



From hack.back at hotmail.com  Wed Apr 15 08:43:23 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 15 Apr 2015 01:43:23 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
Message-ID: <1429087403525-4670737.post@n4.nabble.com>

same issue with your patch , this didnt solve the problem and i still have it
.. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670737.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Apr 15 09:20:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 21:20:49 +1200
Subject: [squid-users] Configuration assistance
In-Reply-To: <20150414162510.Horde.RocSQmanB7yknBeoxwuL8A1@mail.cdne.net>
References: <20150414162510.Horde.RocSQmanB7yknBeoxwuL8A1@mail.cdne.net>
Message-ID: <552E2D71.5060109@treenet.co.nz>

On 15/04/2015 4:25 a.m., HiP-HiPpO wrote:
> Hello-
> 
>   I'm using squid version 3.5.3 and I could use some help with
> configuration.

First up, lets be clear: This is going to be difficult.

The Apache setup you had appears to have been severely violating the
HTTP *and* HTTPS protocols security requirements.

You *will* have different behaviour from one endpoint or the other just
by not using Apache.


> 
>   Squid will be installed at AWS, most clients will be within a corporate
> network and will not be able to access the service by configuring proxy
> settings in a browser.

NOTE: That logic is false.

 Corporate networks / LAN are *the* use-case which is best suited to
explicit browser configuration. You have AD policy, WPAD DHCP, WPAD DNS,
user login scripts ror batch jobs, etc available to push out the
settings. Other use-cases do not have some of those options, or they do
not work as well as in a LAN environment.


>  Instead DNS will be used to resolve to the Squid
> service.  The Squid service will authenticate users via SLDAP.  The
> service will need to be able to intercept both clear and TLS HTTP traffic
> in order to insert an Authorization header.

At first reading this makes no sense. But what you go on to describe is
in fact a reverse-proxy.

SO.. forget the word "intercept", it has nothing to do with what you
have set up.

> 
>   The request flow would be something like;
> 
> Client browser requests http(s)://proxy-hostX.test.com
> (where 0 < X < 50)
> 
> Squid will need to intercept the request and add an Authorization header.

Okay, ignoring the word "intercept", what you have here is just a proxy
that performs login to some backend service(s).

Fairly standard, though you have not said what authentication scheme the
backend is using. That is an important detail.


> Squid will need to rewrite the request to the origin server.  In this
> case, the origin server name is hostX.test.com
> 
> Squid will then intercept the server response and direct that response to
> an ICAP server to modify all embedded links to be the same as the
> request.  All links in the response will need to be rewritten to
> http(s)://proxy-hostX.test.com


This is where you start to have problems. Still ignoring the word
"intercept", you have a proxy that uses ICAP adaptation to re-write both
requests and response, mapping public URI resources to some essentially
completely different private/intenral resoruce URIs.

This ICAP part is very difficult, complex, and has nothing to do with
Squid. So what it (ICAP) actually does to the traffic is not relevant
here. In squid.conf you just need both a REQMOD and RESPMOD service
configured. ICAP service does all the complex bits.

NOTE: be aware that mapping between http:// and https:// scheme is
expressly forbidden by both HTTP and HTTPS specifications. The security
vulnerabilities introduced by doing so are extremely dangerous and have
many unknowable side-effects. You need to be careful that ICAP service
used does not cause it to happen.

> 
> I have official wildcard certificates for the domain.  i.e. *.test.com
> 


> Squid will also need to retrieve group memberships from SLDAP and authorize
> user access to hostX based on group memberships.  If the user is in the
> groups host30 and host43, then access is allowed to only host30.test.com
> and host43.test.com

The SLDAP part for finding group details and adding auth to the backend
is pretty standard. Some limits apply, but pretty easy to do.

BUT ... How do you determine which user made which request?

The usual way is for clients to perform HTTP login to Squid. Anything
else gets much harder, particularly for reverse-proxy.


So, taking note of the fixed terminology I supplied above the wiki
config examples should now be more useful to you:

You have a reverse-proxy doing virtual hosting
(<http://wiki.squid-cache.org/ConfigExamples/Reverse/VirtualHosting>)
also in HTTPS with wildcard certificate
(<http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate>)
to multiple backend servers
(<http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers>).

You have ICAP services performing REQMOD and RESPMOD traffic adaptation:
 <wiki.squid-cache.org/Features/ICAP>

You are performing some form of user identification, with group access
controls
(<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Groups>),
followed by authentication to the backend servers
(<http://www.squid-cache.org/Doc/config/cache_peer/> see login=
authentication options).


HTH
Amos



From squid3 at treenet.co.nz  Wed Apr 15 09:22:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 21:22:50 +1200
Subject: [squid-users] handling different two way ssl-request via a
 proxy system
In-Reply-To: <206AD888-469E-40DF-9402-2AAC69BFEE3C@rinis.nl>
References: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>
 <552D3E63.7080704@treenet.co.nz>
 <206AD888-469E-40DF-9402-2AAC69BFEE3C@rinis.nl>
Message-ID: <552E2DEA.8070102@treenet.co.nz>

On 15/04/2015 7:12 a.m., Abdelouahed Haitoute wrote:
> Hello Amos,
> 
> Thank you for your reply.
> 
>> Problem A)
>> requirements #1 and #2 above are mutually exclusive.
>>
>> Obeying #1 involves sending traffic from Squid to the parent proxy for
>> action.
>>
>> Obeying #2 involves opening direct TLS connections from Squid to the
>> origin servers.
> 
> Is it possible to send the TLS connection as a HTTP CONNECT tunnel via through the parent proxy? (http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling <http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling>)
> 

In theory, but Squid does not yet support that except in the case that
traffic was intercepted and is non-HTTP(S).

Amos



From squid3 at treenet.co.nz  Wed Apr 15 09:33:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 21:33:28 +1200
Subject: [squid-users] Auth conf help
In-Reply-To: <CADddWkpJFhRuPv2+6LGAheEaPatCFm7eWHze9bOq-mwZNYqBqw@mail.gmail.com>
References: <CADddWkpJFhRuPv2+6LGAheEaPatCFm7eWHze9bOq-mwZNYqBqw@mail.gmail.com>
Message-ID: <552E3068.6060706@treenet.co.nz>

On 15/04/2015 9:46 a.m., Bal?zs Szabados wrote:
> Possile Problem #1:
>  digest_pw_auth has not existed in some years. The helpers correct
> current name is digest_file_auth. Please upgrade.
> 
> In the openwrt repository, I can only find Squid 2.7, and I can't upgrade
> to newer.
> Can I make this work somehow, with the current setup?
> 
> Possible Problem #2:
>  when wrong credentials are presented the "http_access allow password"
> will NOT require new ones. It will just skip to the next line - which is
> an implicit "deny all"
> 
> Use this instead:
>   http_access deny !password
>   http_access allow localnet
> 
> Tried it, but the issue persists. Actually, I just noticed that the
> http_access allow localnet already was present in the config, I forgot to
> include in my previous mail.
> 
> Possible Problem #3:
>  the client you are testing with may not support Digest authentication.
> In the current Squid releases use "debug_options 11,2" in squid.conf to
> get a cache.log trace of the HTTP headers the client is sending.
> 
> I've tried with IExplorer, Chrome and curl, I see this in the access log:
> 
> 2 192.168.1.177 TCP_DENIED/407 1722 GET http://www.bing.com/news? user1
> NONE/- text/html

Ah so it found credentials for "user1". But they were not successfully
validated.

Does the helper work if you test it from the command line?
The input format is (including the ':' and '"'s as-is):

"user":"realm"

Your version should supply the HA1 value on accept, ERR on reject.

Amos



From squid3 at treenet.co.nz  Wed Apr 15 09:37:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 21:37:27 +1200
Subject: [squid-users] Delay pool change
In-Reply-To: <CAEhCwUyw53YKNE3F=0xQ4r8chvyrTsA-Pc-d=RV1UroA1dRPeQ@mail.gmail.com>
References: <CAEhCwUyw53YKNE3F=0xQ4r8chvyrTsA-Pc-d=RV1UroA1dRPeQ@mail.gmail.com>
Message-ID: <552E3157.1060809@treenet.co.nz>

On 15/04/2015 11:33 a.m., Hector Chan wrote:
> Hi all,
> 
> If I configure a new delay pool in the config file or reconfigure an
> existing delay pool, do I have to restart squid?  Can I send a SIGHUP to it
> to re-read the config file instead ?  If I send a SIGHUP, what would happen
> to downloads that are in progress ?

It's best to do a full restart if you are adding, removing or changing
the pool parameters or existence.

In a restart they will be given the regular shutdown_timeout to complete
(at their assiged pool speed) before being terminated.


If you are just changing the acess controls a reconfigure should be fine.

In a reconfigure / SIGHUP active requests will be detatched from their
pools, and complete with unlimited speed. Yes, this is an open bug and
needs to be fixed.

Amos



From nathan at getoffmalawn.com  Wed Apr 15 09:47:57 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Wed, 15 Apr 2015 19:47:57 +1000
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429087403525-4670737.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
Message-ID: <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>

What version of Squid are you running, and in what environment?

Based on the fact that the output of the gdb backtrace doesn't show
function parameters (debug symbols don't quite match up to the
binary), this may not work, but if you could run these commands in gdb
and show the output, that would be good...

frame 3
print ccb->conn.p_
print ccb->conn.p_->fd
print conn.p_
print conn.p_->fd

Nathan.

On 15 April 2015 at 18:43, HackXBack <hack.back at hotmail.com> wrote:
> same issue with your patch , this didnt solve the problem and i still have it
> ..
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670737.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Apr 15 09:52:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 21:52:11 +1200
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>
Message-ID: <552E34CB.1080401@treenet.co.nz>

On 15/04/2015 8:32 p.m., Alex Samad wrote:
> Hi
> 
> I have squid-3.5.2-2.el6.x86_64 on centos 6.6
> 
> I am trying to direct certain destinations from certain ip addresses
> 
> 
> acl viaTest dstdomain .abc.com
> 
> tcp_outgoing_address 192.168.11.11 viaTEst
> 
> This works well for
> 
> www.abc.com and test.abc.com when they resolve to ipv4 addresses
> but when they resolved to ipv6 it fails :(

Because 192.168.11.11 is not an IPv6 address. Squid cannot use it to
contact *.abc.com over IPv6.

Configure a tcp_outgoing_address line with an IPv6 address to handle the
IPv6 connections.


> 
> so I tried added
> 
> dns_v4_first on
> 
> but it doesn't seem to help :(
> 
> So am i right in presuming that because the name resolution happens
> first and because it goes to IPv6 it will not going out with a src of
> 192.168.11.11
> 
> why doesn't the ipv4 first flag work ?

All that does is tell Squid to sort the DNS results so that it tries the
IPv4 connections before the IPv6 ones. It will greatly reduce the amount
of IPv6 outbound traffic, but still allow IPv6 to be used for
destinations that need it.

* For any site which is IPv4-only or IPv6-only it has no effect.

* For any site whose IPv4 connectivity is broken or overloaded, the IPv6
routes will be attempted at some point anyway.

Amos


From k1.hedayati93 at gmail.com  Wed Apr 15 10:21:28 2015
From: k1.hedayati93 at gmail.com (Keyvan Hedayati)
Date: Wed, 15 Apr 2015 14:51:28 +0430
Subject: [squid-users] Squid 3.4.8 - Forwarding loop detected - Squid
 doesn't forward request to outside
Message-ID: <CAJdhT7Aosug8aM3XFn8pzTyEWU3hejMp3S4HeJ9_=vRQaDk68w@mail.gmail.com>

Hi
I'm having trouble setting a transparent proxy in our network.
For testing I've asked our net admin to transparently forward all of my
http traffic to squid port but when I try to open a page I get *Access
Denied *error and a warring about Forwarding loop.
As you can see in tcpdump squid sends request to it's machine and not to
outside and I've no idea why this happens.
Can you help my about this? I feel like I'm missing something tiny here.

Squid box: 172.16.1.5
My box: 192.168.10.122

Thanks

-------------------- tcpdump -ntAi any port ! 22
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [S], seq 1494863721, win
29200, options [mss 1460,sackOK,TS val 5421406 ecr 0,nop,wscale 7], length 0
IP 172.16.1.5.3128 > 192.168.10.122.59550: Flags [S.], seq 2174214961, ack
1494863722, win 14480, options [mss 1460,sackOK,TS val 733876 ecr
5421406,nop,wscale 4], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 1, win 229,
options [nop,nop,TS val 5421406 ecr 733876], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [P.], seq 1:75, ack 1, win
229, options [nop,nop,TS val 5421406 ecr 733876], length 74
GET / HTTP/1.1
User-Agent: curl/7.38.0
Host: google.com
Accept: */*

IP 172.16.1.5.3128 > 192.168.10.122.59550: Flags [.], ack 75, win 905,
options [nop,nop,TS val 733876 ecr 5421406], length 0
IP 172.16.1.5.59906 > 172.16.1.5.3128: Flags [S], seq 717864720, win 32792,
options [mss 16396,sackOK,TS val 733876 ecr 0,nop,wscale 4], length 0
IP 172.16.1.5.3128 > 172.16.1.5.59906: Flags [S.], seq 2077204582, ack
717864721, win 32768, options [mss 16396,sackOK,TS val 733876 ecr
733876,nop,wscale 4], length 0
IP 172.16.1.5.59906 > 172.16.1.5.3128: Flags [.], ack 1, win 2050, options
[nop,nop,TS val 733876 ecr 733876], length 0
*IP 172.16.1.5.59906 > 172.16.1.5.3128: Flags [P.], seq 1:198, ack 1, win
2050, options [nop,nop,TS val 733876 ecr 733876], length 197*
E..... at .@.R............8*...{..g....[......
..2...2.GET / HTTP/1.1
User-Agent: curl/7.38.0
Host: google.com
Accept: */*
Via: 1.1 172.16.1.5 (squid/3.4.8)
X-Forwarded-For: 192.168.10.122
Cache-Control: max-age=259200
Connection: keep-alive


IP 172.16.1.5.3128 > 172.16.1.5.59906: Flags [.], ack 198, win 2048,
options [nop,nop,TS val 733876 ecr 733876], length 0
E..4.. at .@.M..........8..{..g*.......ZQ.....
..2...2.
IP 172.16.1.5.3128 > 172.16.1.5.59906: Flags [P.], seq 1:3633, ack 198, win
2048, options [nop,nop,TS val 733876 ecr 733876], length 3632
E..d.. at .@.?..........8..{..g*.......h......
..2...2.HTTP/1.1 403 Forbidden
Server: squid/3.4.8
Mime-Version: 1.0
Date: Wed, 15 Apr 2015 10:02:33 GMT
Content-Type: text/html
Content-Length: 3268
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from 172.16.1.5
X-Cache-Lookup: MISS from 172.16.1.5:8080
Via: 1.1 172.16.1.5 (squid/3.4.8)
Connection: keep-alive

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="http://google.com/">http://google.com/</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

</div>
</body></html>

IP 172.16.1.5.59906 > 172.16.1.5.3128: Flags [.], ack 3633, win 2048,
options [nop,nop,TS val 733876 ecr 733876], length 0
IP 172.16.1.5.3128 > 192.168.10.122.59550: Flags [P.], seq 1:469, ack 75,
win 905, options [nop,nop,TS val 733876 ecr 5421406], length 468
HTTP/1.1 403 Forbidden
Server: squid/3.4.8
Mime-Version: 1.0
Date: Wed, 15 Apr 2015 10:02:33 GMT
Content-Type: text/html
Content-Length: 3268
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from 172.16.1.5
X-Cache-Lookup: MISS from 172.16.1.5:8080
X-Cache: MISS from 172.16.1.5
X-Cache-Lookup: MISS from 172.16.1.5:8080
Via: 1.1 172.16.1.5 (squid/3.4.8), 1.1 172.16.1.5 (squid/3.4.8)
Connection: keep-alive


IP 172.16.1.5.3128 > 192.168.10.122.59550: Flags [.], seq 469:3365, ack 75,
win 905, options [nop,nop,TS val 733876 ecr 5421406], length 2896
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
...
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="http://google.com/">http://google.com/</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

</div>
</body></html>

IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 469, win 237,
options [nop,nop,TS val 5421408 ecr 733876], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 1917, win 260,
options [nop,nop,TS val 5421408 ecr 733876], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 3365, win 282,
options [nop,nop,TS val 5421408 ecr 733876], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 3737, win 305,
options [nop,nop,TS val 5421408 ecr 733876], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [F.], seq 75, ack 3737,
win 305, options [nop,nop,TS val 5421408 ecr 733876], length 0
IP 172.16.1.5.3128 > 192.168.10.122.59550: Flags [F.], seq 3737, ack 76,
win 905, options [nop,nop,TS val 733878 ecr 5421408], length 0
IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [.], ack 3738, win 305,
options [nop,nop,TS val 5421411 ecr 733878], length 0

-------------------------- Squid config
acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8     # RFC 1918 local private network (LAN)
acl localnet src 100.64.0.0/10      # RFC 6598 shared address space (CGN)
acl localhet src 169.254.0.0/16     # RFC 3927 link-local (directly
plugged) machines
acl localnet src 172.16.0.0/12      # RFC 1918 local private network (LAN)
acl localnet src 192.168.0.0/16     # RFC 1918 local private network (LAN)
acl localnet src fc00::/7           # RFC 4193 local private network range
acl localnet src fe80::/10          # RFC 4291 link-local (directly
plugged) machines


acl SSL_ports port 443
acl Safe_ports port 53          # http
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#external_acl_type AclHelper %SRC %DST /usr/bin/python
/etc/squid3/external_acl.py
#acl Acl external AclHelper

http_access deny !Safe_ports        # Deny requests to certain unsafe ports
http_access deny CONNECT !SSL_ports # Deny CONNECT to other than secure SSL
ports
http_access allow localhost manager # Only allow cachemgr access from
localhost
http_access deny manager            # Only allow cachemgr access from
localhost
http_access deny to_localhost
http_access allow localnet
http_access allow localhost
http_access deny all

#ssl_bump server-first all
#always_direct allow all

http_port 3128 intercept
http_port 8080               # avoid ERROR: No forward-proxy ports
configured.
#https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/custom_csr.pem

coredump_dir /var/spool/squid3

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

sslcrtd_program /usr/lib/squid3/ssl_crtd -ds /var/spool/squid3_ssldb -M 4MB
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_adapt setCommonName

shutdown_lifetime 3 seconds
visible_hostname squid

debug_options ALL,1 33,2

------------------- iptables
root at proxy:~# iptables -L -t filter
Chain INPUT (policy ACCEPT)
target     prot opt source               destination
Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

root at proxy:~# iptables -L -t nat
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
Chain INPUT (policy ACCEPT)
target     prot opt source               destination
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination

root at proxy:~# iptables -L -t mangle
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
Chain INPUT (policy ACCEPT)
target     prot opt source               destination
Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination

-------------------- Squid info
root at proxy:~/squid# squid3 -v
Squid Cache: Version 3.4.8
Debian linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3'
'--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests'
'--enable-follow-x-forwarded-for' '--enable-icap-client'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-ssl'
'--enable-ssl-crtd' '--with-openssl' '--enable-build-info=Debian linux'
'--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2
-fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat
-Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Werror=format-security'

-------------------------- Router config
Flags: X - disabled, I - invalid, D - dynamic
 0   ;;; This rule is being used for testing url-redirector service
     chain=prerouting action=mark-routing
new-routing-mark=url-redirector-test-443 passthrough=yes protocol=tcp
src-address=192.168.10.122 dst-address=!172.30.30.1 dst-port=443
 1   ;;; This rule is being used for testing url-redirector service
     chain=prerouting action=mark-routing
new-routing-mark=url-redirector-test-80 passthrough=yes protocol=tcp
src-address=192.168.10.122 dst-address=!172.30.30.1 dst-port=80

Flags: X - disabled, I - invalid, D - dynamic
 2   ;;; url-test
     chain=dstnat action=dst-nat to-addresses=172.16.1.5 to-ports=3128
protocol=tcp routing-mark=url-redirector-test-80
 3   ;;; Url-test
     chain=dstnat action=dst-nat to-addresses=172.16.1.5 to-ports=3129
protocol=tcp routing-mark=url-redirector-test-443

-- 
Live long and prosper,
K1.H <http://k1h.ir/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150415/ab5f4677/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr 15 10:29:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 22:29:56 +1200
Subject: [squid-users] does http_port sssl-bump work
	require-proxy-header?
In-Reply-To: <CAMjA_BkbAeRBTrzFv5vRzADWWontOufuCpeMvh9Lx12PvRUx4Q@mail.gmail.com>
References: <CAMjA_Bmxa0TVUOGEbgF93qinz7J-Hx=ndO-9oxKg3HNMRkgh5w@mail.gmail.com>	<552C81E1.3020503@treenet.co.nz>	<CAMjA_BkeMG4wUvuS_BEBdiF=5Xz9KU--oaLFmiapM1+daWPZtw@mail.gmail.com>	<CAMjA_B=KXV7Qe+wB2wkoyyXqjrs9q_DsCAjRc7Rpfy+Ucg9_Cw@mail.gmail.com>
 <CAMjA_BkbAeRBTrzFv5vRzADWWontOufuCpeMvh9Lx12PvRUx4Q@mail.gmail.com>
Message-ID: <552E3DA4.9020309@treenet.co.nz>

On 15/04/2015 1:23 p.m., Yuhua Wu wrote:
> 
> I assume the intention of code is to turn off COMM_TRANSPARENT if PROXY
> protocol is used.
> 
> Is this proper change? At least, it works for me now,

Yes that was the intention. Your patch looks correct to me. Approved and
applied to Squid-4, it should also make the next 3.5 release.

Amos



From naishal48shah at gmail.com  Wed Apr 15 10:21:15 2015
From: naishal48shah at gmail.com (naishal0748)
Date: Wed, 15 Apr 2015 03:21:15 -0700 (PDT)
Subject: [squid-users] squid tcp_outgoing_address feature not working
In-Reply-To: <1429089494641-4670741.post@n4.nabble.com>
References: <1429089494641-4670741.post@n4.nabble.com>
Message-ID: <1429093275583-4670748.post@n4.nabble.com>

Hello,

I received following reply from Amos.

---------------

Welcome to the world of application layer gateways.

There is no guarantee that IPv4 is being used outbound. You may in fact
be using IPv6 to contact servers.
 All that means is that you need to set a WAN1 IPv6 address in a second
tcp_outgoing_address line for the IPv6.


Also be aware the selection of NIC is entirely up to the kernel routing
logics. Older Linux were well-known for their annoying ability to accept
or send from any NIC using any IP assigned to the machine, depending on
whether you had some voodoo setup in the routing config or not. CentOS
uses ancient enough kernels that it probably does not have the bug fixes
for that.

So, double check that Squid is actually sending from 192.168.3.15 like
you expect. If not we can help you a little further to figure out why
and see if that fixes things for you.


One other effect I've seen in action is that NAT on outbound can take
Squids tcp_outgoing_address and change it so the packets go out the
wrong NIC with different IP entirely.


Otherwise its a kernel routing problem, and we probably cant help with that.

-------------------------------

I am actually checking using traceroute from client system , and it is
always showing me 192.168.5.1 default Gateway IP.

If it is getting difficult with squid configurations, please let me know if
it is possible to implement this setup using iptables, so that iptables
directly routes the traffic from specific source towards specific Gateway /
NIC.

Anyhow, basically I want the specific source traffic to go via specific
Gateway.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tcp-outgoing-address-feature-not-working-tp4670741p4670748.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Apr 15 10:26:29 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 15 Apr 2015 03:26:29 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
Message-ID: <1429093589618-4670749.post@n4.nabble.com>

am running squid 3.4.12 under debian7




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670749.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Apr 15 10:58:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 22:58:04 +1200
Subject: [squid-users] squid tcp_outgoing_address feature not working
In-Reply-To: <1429093275583-4670748.post@n4.nabble.com>
References: <1429089494641-4670741.post@n4.nabble.com>
 <1429093275583-4670748.post@n4.nabble.com>
Message-ID: <552E443C.4080807@treenet.co.nz>

On 15/04/2015 10:21 p.m., naishal0748 wrote:
> Hello,
> 
> I received following reply from Amos.
> 
> ---------------
> 
> Welcome to the world of application layer gateways.
> 
> There is no guarantee that IPv4 is being used outbound. You may in fact
> be using IPv6 to contact servers.
>  All that means is that you need to set a WAN1 IPv6 address in a second
> tcp_outgoing_address line for the IPv6.
> 
> 
> Also be aware the selection of NIC is entirely up to the kernel routing
> logics. Older Linux were well-known for their annoying ability to accept
> or send from any NIC using any IP assigned to the machine, depending on
> whether you had some voodoo setup in the routing config or not. CentOS
> uses ancient enough kernels that it probably does not have the bug fixes
> for that.
> 
> So, double check that Squid is actually sending from 192.168.3.15 like
> you expect. If not we can help you a little further to figure out why
> and see if that fixes things for you.
> 
> 
> One other effect I've seen in action is that NAT on outbound can take
> Squids tcp_outgoing_address and change it so the packets go out the
> wrong NIC with different IP entirely.
> 
> 
> Otherwise its a kernel routing problem, and we probably cant help with that.
> 
> -------------------------------
> 
> I am actually checking using traceroute from client system , and it is
> always showing me 192.168.5.1 default Gateway IP.
> 

>From the client system you will only ever see the IPs on the
client->Squid connection. Not the details of the Squid->origin connection.

Squid has zero control over what TCP connections the *client* opens.

You need to use tcpdump on the Squid machine, or machine(s) at the other
end of the WAN1/2 connections to see what the Squid->origin traffic uses.


> If it is getting difficult with squid configurations, please let me know if
> it is possible to implement this setup using iptables, so that iptables
> directly routes the traffic from specific source towards specific Gateway /
> NIC.

Routing is configured with the "ip route" tool, not the iptables (NAT
and firewall tool).

To see what your current routing does, run:
 ip -4 route show
 ip -6 route show


> 
> Anyhow, basically I want the specific source traffic to go via specific
> Gateway.

Understood.

Amos


From squid3 at treenet.co.nz  Wed Apr 15 11:12:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 23:12:32 +1200
Subject: [squid-users] Squid 3.4.8 - Forwarding loop detected - Squid
 doesn't forward request to outside
In-Reply-To: <CAJdhT7Aosug8aM3XFn8pzTyEWU3hejMp3S4HeJ9_=vRQaDk68w@mail.gmail.com>
References: <CAJdhT7Aosug8aM3XFn8pzTyEWU3hejMp3S4HeJ9_=vRQaDk68w@mail.gmail.com>
Message-ID: <552E47A0.4070300@treenet.co.nz>

On 15/04/2015 10:21 p.m., Keyvan Hedayati wrote:
> Hi
> I'm having trouble setting a transparent proxy in our network.
> For testing I've asked our net admin to transparently forward all of my
> http traffic to squid port but when I try to open a page I get *Access
> Denied *error and a warring about Forwarding loop.
> As you can see in tcpdump squid sends request to it's machine and not to
> outside and I've no idea why this happens.
> Can you help my about this? I feel like I'm missing something tiny here.
> 
> Squid box: 172.16.1.5
> My box: 192.168.10.122
> 
> Thanks
> 
> -------------------- tcpdump -ntAi any port ! 22
> IP 192.168.10.122.59550 > 172.16.1.5.3128: Flags [S], seq 1494863721, win
> 29200, options [mss 1460,sackOK,TS val 5421406 ecr 0,nop,wscale 7], length 0

Wheres the origin server IP?

 google.com:80 != 172.16.1.5:3128

If you are performing NAT on a machine other than the Squid box you are
guaranteed to get this type of forwarding loop.


One of these almost identical configs is the correct Squid box config
for you:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>

DNAT is best if you have static IPs in a high-peformance situation,
REDIRECT if you have DHCP assigned / dynamic proxy IPs or are unsure
what the final machine IP will be (ie plug-n-play proxy device).


You also need the router changed to *route* the packets to the Squid
machine without NAT'ing them in any way. There may be other devices
along the packet path needing updates to handle the new route properly,
your sysadmin should know what to do about all that.

Amos



From squid3 at treenet.co.nz  Wed Apr 15 11:14:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 23:14:12 +1200
Subject: [squid-users] Problem with squid3.3.8 on ubuntu and windows 8.1
 store
In-Reply-To: <CAN0+Xjd2BaW032qkw8pV_PMGWPfRa25jRj8We-ptnYkCCfwqnA@mail.gmail.com>
References: <CAN0+Xjd2BaW032qkw8pV_PMGWPfRa25jRj8We-ptnYkCCfwqnA@mail.gmail.com>
Message-ID: <552E4804.1030506@treenet.co.nz>

On 13/04/2015 3:58 p.m., jimdo x wrote:
> Hi all,
> 
> Anyone knows why with squid3 as proxy, the windows store load contents
> fine, but when comes to download apps, it always saying, you have a slow
> connection? or just can not download.
> Without squid as proxy, like connect directly to internet, it downloads
> fine.

So, do you know *any* other details about the problem?
"it dont work" is not very useful for debugging and you appear to be the
only person (or first?) to notice this.

Amos



From squid3 at treenet.co.nz  Wed Apr 15 11:16:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Apr 2015 23:16:46 +1200
Subject: [squid-users] Client delay pools ...doesn't work
In-Reply-To: <5527D557.8070407@esseweb.eu>
References: <5527D557.8070407@esseweb.eu>
Message-ID: <552E489E.8020505@treenet.co.nz>

On 11/04/2015 1:51 a.m., Fiorenza Meini wrote:
> Hi,
> I'm testing on a 3.4 squid release the client_delay_poolfunctionality.
> It seems that isn't working: on my browser I receive the error that
> proxy isn't reachable, and in log file I can't see nothing useful.
> 
> Has anyone configured this functionality successfully ?
> 

Could be bug <http://bugs.squid-cache.org/show_bug.cgi?id=3696> ?

Amos


From squid3 at treenet.co.nz  Wed Apr 15 13:12:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 01:12:38 +1200
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1429045617136-4670725.post@n4.nabble.com>
References: <552675E1.6090502@gmail.com> <029d01d072c5$25e5b800$71b12800$@fr>
 <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com> <02d601d072d1$390562d0$ab102870$@fr>
 <55272F87.40607@treenet.co.nz> <1428921772616-4670708.post@n4.nabble.com>
 <1429044003278-4670723.post@n4.nabble.com>
 <92756812-C713-4B77-84EF-A0CF0395C987@gmail.com>
 <1429045617136-4670725.post@n4.nabble.com>
Message-ID: <552E63C6.2070409@treenet.co.nz>

On 15/04/2015 9:06 a.m., Stakres wrote:
> Hi Guy,
> 
> Thanks for answering :o)
> Based on the bugzilla, it's fixed but not yet available.
> Anyway, that's a very good news.
> Let's wait the next build.

FYI: The squid-3.5 snapshot r13797 and squid-3.4 snapshot r13219 or
later include it. Those are built now and just awaiting the mirror
updates, which should be complete in about an hour from now.

Amos



From hack.back at hotmail.com  Wed Apr 15 13:06:17 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 15 Apr 2015 06:06:17 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429093589618-4670749.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
Message-ID: <1429103177898-4670756.post@n4.nabble.com>

this bug exist after changing in certificate_db.cc from flock to lockf

=== modified file 'src/ssl/certificate_db.cc'
--- src/ssl/certificate_db.cc	2014-12-03 11:58:37 +0000
+++ src/ssl/certificate_db.cc	2015-01-09 10:27:12 +0000
@@ -55,7 +55,7 @@
 #if _SQUID_WINDOWS_
     if (!LockFile(hFile, 0, 0, 1, 0))
 #else
-    if (flock(fd, LOCK_EX) != 0)
+    if (lockf(fd, F_LOCK, 0) != 0)
 #endif
         throw std::runtime_error("Failed to get a lock of " + filename);
 }
@@ -70,7 +70,7 @@
     }
 #else
     if (fd != -1) {
-        flock(fd, LOCK_UN);
+        lockf(fd, F_ULOCK, 0);
         close(fd);
         fd = -1;
     }



after stopping ssl_crtd corruption i got now assertion failed every 10 min..
!!!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670756.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Wed Apr 15 13:09:59 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Apr 2015 06:09:59 -0700 (PDT)
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <552E63C6.2070409@treenet.co.nz>
References: <552678B9.3090309@gmail.com> <02ad01d072c6$70601320$51203960$@fr>
 <55267B46.2010804@gmail.com> <02d601d072d1$390562d0$ab102870$@fr>
 <55272F87.40607@treenet.co.nz> <1428921772616-4670708.post@n4.nabble.com>
 <1429044003278-4670723.post@n4.nabble.com>
 <92756812-C713-4B77-84EF-A0CF0395C987@gmail.com>
 <1429045617136-4670725.post@n4.nabble.com> <552E63C6.2070409@treenet.co.nz>
Message-ID: <1429103399050-4670757.post@n4.nabble.com>

Hi Amos,

Good news !
Waiting for the new build, we'll test and keep you posted...

Best regards.
Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Random-SSL-bump-DB-corruption-tp4670289p4670757.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From timothee.christin at gmail.com  Wed Apr 15 14:38:02 2015
From: timothee.christin at gmail.com (tchristin)
Date: Wed, 15 Apr 2015 07:38:02 -0700 (PDT)
Subject: [squid-users] Integrate Squid Kerberos auth and Squidguard
	ldapusersearch into AD
Message-ID: <1429108682047-4670758.post@n4.nabble.com>

Hi all,

I'm having trouble with Squid Kerberos auth and the Squidguard
ldapusersearch that I use to apply ACLs by Active Directory groups
membership.

The problem is :

- Squid and Squidguard see my user as : 'user at domain.local' so the '%s'
variable of squidguard is 'user at domain.local'.
- In my ldap query there is no default property that can interpret this
string.

So I would like to strip/hide the realm of the username to let the LDAP
query work with the sAMAccountName attribute.

How can I proceed to have %s = user and not %s = user at domain.local ?

I precise that the UPN is equal to the mail adress so it's not usable.

I'm using version 3.1.20 of Squid Cache and version 1.5 (debian package of
squidGuard).

Thanks for your help !




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Integrate-Squid-Kerberos-auth-and-Squidguard-ldapusersearch-into-AD-tp4670758.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcus.kool at urlfilterdb.com  Wed Apr 15 16:08:17 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 15 Apr 2015 13:08:17 -0300
Subject: [squid-users] Integrate Squid Kerberos auth and Squidguard
 ldapusersearch into AD
In-Reply-To: <1429108682047-4670758.post@n4.nabble.com>
References: <1429108682047-4670758.post@n4.nabble.com>
Message-ID: <552E8CF1.1010304@urlfilterdb.com>


On 04/15/2015 11:38 AM, tchristin wrote:
> Hi all,
>
> I'm having trouble with Squid Kerberos auth and the Squidguard
> ldapusersearch that I use to apply ACLs by Active Directory groups
> membership.
>
> The problem is :
>
> - Squid and Squidguard see my user as : 'user at domain.local' so the '%s'
> variable of squidguard is 'user at domain.local'.
> - In my ldap query there is no default property that can interpret this
> string.
>
> So I would like to strip/hide the realm of the username to let the LDAP
> query work with the sAMAccountName attribute.
>
> How can I proceed to have %s = user and not %s = user at domain.local ?

If you replace squidGuard by ufdbGuard, ufdbGuard does it for you if you set the option
    strip-domain-from-username on

Note that ufdbGuard is not 100% compatible with squidGuard and you need to read section 9.3
of the Reference Manual of ufdbGuard to configure ufdbGuard correctly.

Marcus

> I precise that the UPN is equal to the mail adress so it's not usable.
>
> I'm using version 3.1.20 of Squid Cache and version 1.5 (debian package of
> squidGuard).
>
> Thanks for your help !


From hack.back at hotmail.com  Wed Apr 15 21:09:51 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 15 Apr 2015 14:09:51 -0700 (PDT)
Subject: [squid-users] 206 partial content
Message-ID: <1429132191353-4670760.post@n4.nabble.com>

hello minds,
it seems many files appear in access.log as 206 and not 200,
to make this file hit is by using range_offsit_limit none
but this option cause squid to consume more bandwidth from real internet,
so what is the solution to make 206 cached and hit again without make it
consume this huge bandwidth ?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/206-partial-content-tp4670760.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From naishal48shah at gmail.com  Wed Apr 15 23:08:48 2015
From: naishal48shah at gmail.com (naishal0748)
Date: Wed, 15 Apr 2015 16:08:48 -0700 (PDT)
Subject: [squid-users] squid tcp_outgoing_address feature not working
In-Reply-To: <552E443C.4080807@treenet.co.nz>
References: <1429089494641-4670741.post@n4.nabble.com>
 <1429093275583-4670748.post@n4.nabble.com> <552E443C.4080807@treenet.co.nz>
Message-ID: <1429139328862-4670761.post@n4.nabble.com>


OK I got it, I added "ip route add" commands ,set up routing tables in
/etc/iptables2/rt_tables and everything is working fine now.

Although I could not get it worked by "tcp_outgoing_address" option, this
way it is working perfectly.

Thanks for the help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tcp-outgoing-address-feature-not-working-tp4670741p4670761.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Jason_Haar at trimble.com  Thu Apr 16 00:51:49 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 16 Apr 2015 12:51:49 +1200
Subject: [squid-users] squid tcp_outgoing_address feature not working
In-Reply-To: <552E443C.4080807@treenet.co.nz>
References: <1429089494641-4670741.post@n4.nabble.com>
 <1429093275583-4670748.post@n4.nabble.com> <552E443C.4080807@treenet.co.nz>
Message-ID: <552F07A5.4040104@trimble.com>

On 15/04/15 22:58, Amos Jeffries wrote:
> Squid has zero control over what TCP connections the *client* opens.
> You need to use tcpdump on the Squid machine, or machine(s) at the
> other end of the WAN1/2 connections to see what the Squid->origin
> traffic uses.

Amos is so right. Stop fiddling around with tools like traceroute whose
behaviour *might* mimic that which squid is doing and instead use
tcpdump to actually *see* what squid is doing. Anyone running network
services has got to become proficient in the use of network sniffers -
they are invaluable

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From stan.prescott at gmail.com  Thu Apr 16 01:43:59 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 15 Apr 2015 21:43:59 -0400
Subject: [squid-users] Squid 4 question
Message-ID: <CANLNtGR_ga5iy1xG1LuntRUZqBpOHdo0E0-nJdS_i_uQP7zB0w@mail.gmail.com>

Will Squid 4 be able to be compiled with GCC 4.7.3 or will it require GCC
4.9.x or newer?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150415/5aee7880/attachment.htm>

From alex at samad.com.au  Thu Apr 16 02:33:57 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 16 Apr 2015 12:33:57 +1000
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <552E34CB.1080401@treenet.co.nz>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>
 <552E34CB.1080401@treenet.co.nz>
Message-ID: <CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>

Hi

Is there any way to make some sites to be access only via ipv4 ? So
even if there is a ipv6 address I reject it

Thanks
Alex

On 15 April 2015 at 19:52, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 15/04/2015 8:32 p.m., Alex Samad wrote:
>> Hi
>>
>> I have squid-3.5.2-2.el6.x86_64 on centos 6.6
>>
>> I am trying to direct certain destinations from certain ip addresses
>>
>>
>> acl viaTest dstdomain .abc.com
>>
>> tcp_outgoing_address 192.168.11.11 viaTEst
>>
>> This works well for
>>
>> www.abc.com and test.abc.com when they resolve to ipv4 addresses
>> but when they resolved to ipv6 it fails :(
>
> Because 192.168.11.11 is not an IPv6 address. Squid cannot use it to
> contact *.abc.com over IPv6.
>
> Configure a tcp_outgoing_address line with an IPv6 address to handle the
> IPv6 connections.
>
>
>>
>> so I tried added
>>
>> dns_v4_first on
>>
>> but it doesn't seem to help :(
>>
>> So am i right in presuming that because the name resolution happens
>> first and because it goes to IPv6 it will not going out with a src of
>> 192.168.11.11
>>
>> why doesn't the ipv4 first flag work ?
>
> All that does is tell Squid to sort the DNS results so that it tries the
> IPv4 connections before the IPv6 ones. It will greatly reduce the amount
> of IPv6 outbound traffic, but still allow IPv6 to be used for
> destinations that need it.
>
> * For any site which is IPv4-only or IPv6-only it has no effect.
>
> * For any site whose IPv4 connectivity is broken or overloaded, the IPv6
> routes will be attempted at some point anyway.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jagannath.naidu at fosteringlinux.com  Thu Apr 16 02:35:24 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Thu, 16 Apr 2015 08:05:24 +0530
Subject: [squid-users] Issue using office 2013(microsoft),
	under squid proxy
In-Reply-To: <CA+8bHvwQ4V3zBJUO0MyQ=ujKyo7yosw0XrHTfjHN6zZZAoO0fg@mail.gmail.com>
References: <CA+8bHvwQ4V3zBJUO0MyQ=ujKyo7yosw0XrHTfjHN6zZZAoO0fg@mail.gmail.com>
Message-ID: <CA+8bHvyj55CwtjJEmSGMVr+y-FVNj2Uis0Bb=MFutvHuFvTbxg@mail.gmail.com>

anybody ???
is it even possible ??

On 15/04/2015, Jagannath Naidu <jagannath.naidu at fosteringlinux.com> wrote:
> Dear List,
>
> At a client side, I am facing an issue as follows
>
> OS: Windows 7 : Pro
> App: Office 2013
>
> ISSUE:
>
> By default, whenever the user opens a file to edit or read a office file
> (spreadsheet,doc etc), he continuously gets login prompts as shown in
> attached file.
>
> One way to disable this prompt is by disabling the internet access to the
> office 2013. But we need to do this at server end.
>
> Please reply
>
>
> --
> Thanks & Regards
>
> B Jagannath
> Keen & Able Computers Pvt. Ltd.
> +919871324006
>


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006


From squid3 at treenet.co.nz  Thu Apr 16 02:38:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 14:38:37 +1200
Subject: [squid-users] Squid 4 question
In-Reply-To: <CANLNtGR_ga5iy1xG1LuntRUZqBpOHdo0E0-nJdS_i_uQP7zB0w@mail.gmail.com>
References: <CANLNtGR_ga5iy1xG1LuntRUZqBpOHdo0E0-nJdS_i_uQP7zB0w@mail.gmail.com>
Message-ID: <552F20AD.4020800@treenet.co.nz>

On 16/04/2015 1:43 p.m., Stanford Prescott wrote:
> Will Squid 4 be able to be compiled with GCC 4.7.3 or will it require GCC
> 4.9.x or newer?

Squid-4 will require C++11 support for some of the features only added
later in the language development. Which means 4.9 is the oldest which
will build fully.

YMMV when using 4.8, though expect some problems. Almost all available
4.7 release though will break due to missing C++11 features in the stdlib.

If you need older compilers, I am planning to continue support for 3.5
series a while longer than we normally would so the RHEL family of OS
get a better chance to upgrade. All other OS Squid builds on will have
sufficient compiler support in their stable releases by the time squid-4
comes out (or do already).

Amos



From squid3 at treenet.co.nz  Thu Apr 16 02:58:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 14:58:36 +1200
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>	<552E34CB.1080401@treenet.co.nz>
 <CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>
Message-ID: <552F255C.1000603@treenet.co.nz>

On 16/04/2015 2:33 p.m., Alex Samad wrote:
> Hi
> 
> Is there any way to make some sites to be access only via ipv4 ? So
> even if there is a ipv6 address I reject it

Not in the way you are thinking.

  acl to_ipv6 dst ipv6
  http_access deny to_ipv6

will deny access to any IPv6-enabled website (includes Google, Facebook,
etc.)


Why are you trying to break your connectivity anyway?

By disabling IPv6 you are cutting yourself and your users off from more
than 3% of the Internet today (and growing).

Amos



From squid3 at treenet.co.nz  Thu Apr 16 03:05:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 15:05:39 +1200
Subject: [squid-users] 206 partial content
In-Reply-To: <1429132191353-4670760.post@n4.nabble.com>
References: <1429132191353-4670760.post@n4.nabble.com>
Message-ID: <552F2703.803@treenet.co.nz>

On 16/04/2015 9:09 a.m., HackXBack wrote:
> hello minds,
> it seems many files appear in access.log as 206 and not 200,
> to make this file hit is by using range_offsit_limit none
> but this option cause squid to consume more bandwidth from real internet,
> so what is the solution to make 206 cached and hit again without make it
> consume this huge bandwidth ?

Is that MISS/20 or HIT/206. Big difference.

Also whether the object(s) are cacheable at all also makes a big
difference. No use fetching all that unwanted bytes if the object is
just going to be thrown away.

range_offset_limit none with quick_abort* settings to allow the objects
to finish downloading after the clients are done is how you get the
object to be 200 from the server, and fully available for storage -
assumign it can be cached.


That only works if the 206 are for the same objects repeatedly.

If they are for parts of different objects all the time or the objects
are not cacheable, then dont worry. 206 is the most efficient form of
transfer for that type of traffic.

Amos



From alex at samad.com.au  Thu Apr 16 03:20:22 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 16 Apr 2015 13:20:22 +1000
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <552F255C.1000603@treenet.co.nz>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>
 <552E34CB.1080401@treenet.co.nz>
 <CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>
 <552F255C.1000603@treenet.co.nz>
Message-ID: <CAJ+Q1PXgxSC+Dcp3L6efesporf6mcyXvQdRcyh2sjgnTUJZrpA@mail.gmail.com>

:)  Living in Australia that has just sign into law meta data
recording. So I am sending some of my traffic OS via a vpn service.

But I still want some things to go locally, so I was using src ip
address to help my router determine which path to use.

unfortunately my vpn service doesn't handle IPv6, plus from memory
there is no NAT for ipv6 (last time i looked) so .. I want to allow
ipv6 for somethings but I want to force all traffic for certain site
via the vpn.


Unfortunately I might have to just kill all ipv6 ..


On 16 April 2015 at 12:58, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 16/04/2015 2:33 p.m., Alex Samad wrote:
>> Hi
>>
>> Is there any way to make some sites to be access only via ipv4 ? So
>> even if there is a ipv6 address I reject it
>
> Not in the way you are thinking.
>
>   acl to_ipv6 dst ipv6
>   http_access deny to_ipv6
>
> will deny access to any IPv6-enabled website (includes Google, Facebook,
> etc.)
>
>
> Why are you trying to break your connectivity anyway?
>
> By disabling IPv6 you are cutting yourself and your users off from more
> than 3% of the Internet today (and growing).
>
> Amos
>


From iridium1191 at gmail.com  Thu Apr 16 03:16:55 2015
From: iridium1191 at gmail.com (iridium191)
Date: Wed, 15 Apr 2015 20:16:55 -0700 (PDT)
Subject: [squid-users] Squid downloading huge amounts of un-requested data
Message-ID: <1429154215215-4670770.post@n4.nabble.com>

Hi Forum,

We're running Squid Version 3.3.8 (from repository) on Ubuntu server 14.04
LTS x64 as a caching proxy server for a mixed Linux/Windows network. 
We were attempting to cache apt-get and Windows updates using various
refresh patterns but commented them out a few weeks ago after an issue where
the squid server had downloaded over 100GB of data over 24 hours but in that
time had served only about 6GB to clients. 100GB represents almost our
entire normal monthly traffic. Most of the incoming Squid traffic was from
Windows updates or Akamai servers. Prior to that the server had been running
for months without an issue.
Unfortunately the same thing happened again last night. The only remanent of
the Windows Update cache commands we forgot to uncomment in squid.conf were:
range_offset_limit 200 MB WindowsUpdate 
maximum_object_size 1 GB
quick_abort_min -1

which we have since commented out and restarted the Squid server.

The incoming traffic to Squid is being recorded by the syslog of our ASA5510
firewall and corroborated by our ISP's management console. Clients can only
access the internet through the proxy. The outgoing traffic statistics are
being reported by SARG, SquidAnalyzer and a custom tcpdump script of all
traffic from the server and all three agree.

So our question- is there any way that Squid can download content
repeatedly, and many times in excess of what our clients are actually
requesting? How can we debug this issue further? 

Grateful for any advice you

some squidclient output:
Squid Object Cache: Version 3.3.8
Start Time:     Wed, 15 Apr 2015 23:11:20 GMT
Current Time:   Thu, 16 Apr 2015 03:14:41 GMT
Connection information for squid:
        Number of clients accessing cache:      46
        Number of HTTP requests received:       42126
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   173.1
        Average ICP messages per minute since start:    0.0
        Select loop called: 5688640 times, 2.567 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 17.9%, 60min: 11.3%
        Hits as % of bytes sent:        5min: 4.2%, 60min: 7.2%
        Memory hits as % of hit requests:       5min: 73.5%, 60min: 63.0%
        Disk hits as % of hit requests: 5min: 0.0%, 60min: 17.2%
        Storage Swap size:      11278780 KB
        Storage Swap capacity:  44.1% used, 55.9% free
        Storage Mem size:       153892 KB
        Storage Mem capacity:    7.5% used, 92.5% free
        Mean Object Size:       23.21 KB
        Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.00000  0.03241
        Cache Misses:          0.46965  0.18699
        Cache Hits:            0.00000  0.00091
        Near Hits:             0.04776  0.05331
        Not-Modified Replies:  0.01745  0.00091
        DNS Lookups:           0.01609  0.01609
        ICP Queries:           0.00000  0.00000

 relevant squid.conf

cache_mem 2000 MB
maximum_object_size_in_memory 1 MB

# Log file locations
access_log daemon:/var/log/squid3/access.log squid
cache_store_log none
cache_log /var/log/squid3/cache.log
coredump_dir /var/spool/squid3

# Disk cache directory.
cache_dir aufs /squid_cache/Squid3Cache 25000 16 256refresh_pattern ^ftp:          
1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

dns_nameservers 127.0.0.1

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_req reqmod_precache bypass=0
icap://127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0
icap://127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 20 startup=0 idle=1 concurrency=0







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-downloading-huge-amounts-of-un-requested-data-tp4670770.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From naishal48shah at gmail.com  Thu Apr 16 04:12:08 2015
From: naishal48shah at gmail.com (naishal0748)
Date: Wed, 15 Apr 2015 21:12:08 -0700 (PDT)
Subject: [squid-users] squid tcp_outgoing_address feature not working
In-Reply-To: <552F07A5.4040104@trimble.com>
References: <1429089494641-4670741.post@n4.nabble.com>
 <1429093275583-4670748.post@n4.nabble.com> <552E443C.4080807@treenet.co.nz>
 <552F07A5.4040104@trimble.com>
Message-ID: <1429157528071-4670771.post@n4.nabble.com>

Jason Haar-2 wrote
> On 15/04/15 22:58, Amos Jeffries wrote:
>> Squid has zero control over what TCP connections the *client* opens.
>> You need to use tcpdump on the Squid machine, or machine(s) at the
>> other end of the WAN1/2 connections to see what the Squid->origin
>> traffic uses.
> 
> Amos is so right. Stop fiddling around with tools like traceroute whose
> behaviour *might* mimic that which squid is doing and instead use
> tcpdump to actually *see* what squid is doing. Anyone running network
> services has got to become proficient in the use of network sniffers -
> they are invaluable
> 
> 
> Very True.. I need to learn using tcpdump now. 

> 
> -- 
> Cheers
> 
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tcp-outgoing-address-feature-not-working-tp4670741p4670771.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Apr 16 04:54:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 16:54:38 +1200
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <CAJ+Q1PXgxSC+Dcp3L6efesporf6mcyXvQdRcyh2sjgnTUJZrpA@mail.gmail.com>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>	<552E34CB.1080401@treenet.co.nz>	<CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>	<552F255C.1000603@treenet.co.nz>
 <CAJ+Q1PXgxSC+Dcp3L6efesporf6mcyXvQdRcyh2sjgnTUJZrpA@mail.gmail.com>
Message-ID: <552F408E.6000803@treenet.co.nz>

On 16/04/2015 3:20 p.m., Alex Samad wrote:
> :)  Living in Australia that has just sign into law meta data
> recording. So I am sending some of my traffic OS via a vpn service.
> 
> But I still want some things to go locally, so I was using src ip
> address to help my router determine which path to use.
> 
> unfortunately my vpn service doesn't handle IPv6, plus from memory
> there is no NAT for ipv6 (last time i looked) so .. I want to allow
> ipv6 for somethings but I want to force all traffic for certain site
> via the vpn.
> 
> 
> Unfortunately I might have to just kill all ipv6 ..

Your best hope is actually to do the *opposite*.

Make IPv6 actually work. You can firewall IPv6 traffic going to the
selected set of sites IP ranges, just like you would block them if you
didnt want IPv4 connections to go there.


And yes there is NAT in IPv6 now. And yes it causes just as much trouble
there as it does for IPv4 - but gratuitously since most of the things
NAT is useful for in IPv4 have better alternatives in IPv6.

Amos



From squid3 at treenet.co.nz  Thu Apr 16 05:18:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 17:18:51 +1200
Subject: [squid-users] Squid downloading huge amounts of un-requested
 data
In-Reply-To: <1429154215215-4670770.post@n4.nabble.com>
References: <1429154215215-4670770.post@n4.nabble.com>
Message-ID: <552F463B.2090106@treenet.co.nz>

On 16/04/2015 3:16 p.m., iridium191 wrote:
> Hi Forum,
> 
> We're running Squid Version 3.3.8 (from repository) on Ubuntu server 14.04
> LTS x64 as a caching proxy server for a mixed Linux/Windows network. 
> We were attempting to cache apt-get and Windows updates using various
> refresh patterns but commented them out a few weeks ago after an issue where
> the squid server had downloaded over 100GB of data over 24 hours but in that
> time had served only about 6GB to clients. 100GB represents almost our
> entire normal monthly traffic. Most of the incoming Squid traffic was from
> Windows updates or Akamai servers. Prior to that the server had been running
> for months without an issue.
> Unfortunately the same thing happened again last night. The only remanent of
> the Windows Update cache commands we forgot to uncomment in squid.conf were:
> range_offset_limit 200 MB WindowsUpdate 
> maximum_object_size 1 GB
> quick_abort_min -1
> 
> which we have since commented out and restarted the Squid server.

Leave the maximum_object_size, the default is to only cache 4 MB objects
and anything else becomes a MISS to consume more upstream bandwidth.

The range_offset_limit and quick_abort_min are what causes more download
for Range requests than what clients requested.


> 
> The incoming traffic to Squid is being recorded by the syslog of our ASA5510
> firewall and corroborated by our ISP's management console. Clients can only
> access the internet through the proxy. The outgoing traffic statistics are
> being reported by SARG, SquidAnalyzer and a custom tcpdump script of all
> traffic from the server and all three agree.
> 
> So our question- is there any way that Squid can download content
> repeatedly, and many times in excess of what our clients are actually
> requesting? How can we debug this issue further? 


Several.

* Remote server not supporting HTTP/1.1 304 revalidation properly.
Results in the server delivering 200 response + full object data to
Squid when a 304 would suffice and Squid only delivers 304 to the client.

* Misconfigured / open proxy. When remote users abuse the proxy all
traffic goes over the WAN interfaces. Measurement tools that
differentiate by NIC on the Squid machine will see this as massive
amounts of WAN traffic relative to LAN.

* Long polling connections.

* ICAP. The ICAP service may be doing anything that triggers lots of WAN
traffic use. It can require full objects - forcing the same behaviour as
quick_abort_*, it can do its own downloads - traffic not used by Squid
at all but still used, it can make lots of requests

* squidguard. Same as with ICAP it can be doing its own downloads. Also
URL-rewritten traffic does not relay the client reuqest to the server,
but the re-writer generated request. This is another way server 200
responses can become 200 when only 304 were necessary.



>  relevant squid.conf

Is missing many configuration details that must be present for an
operational proxy. Specifically the access controls to determine if the
open proxy issues are relevant.


With all that logging and traffic measurement are you able to track down
an example transaction where Squid fetches more from the network than it
delivers to the client.

Squids cachemgr utilization report has a breakdown of how much traffic
is used by each I/O type. It can help identify where to look at closer.


Amos


From alex at samad.com.au  Thu Apr 16 05:59:35 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 16 Apr 2015 15:59:35 +1000
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <552F408E.6000803@treenet.co.nz>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>
 <552E34CB.1080401@treenet.co.nz>
 <CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>
 <552F255C.1000603@treenet.co.nz>
 <CAJ+Q1PXgxSC+Dcp3L6efesporf6mcyXvQdRcyh2sjgnTUJZrpA@mail.gmail.com>
 <552F408E.6000803@treenet.co.nz>
Message-ID: <CAJ+Q1PWBOH8x67UKq-DWzH__pGdRhsnRzw2RFvdg6vua6UfubA@mail.gmail.com>

On 16 April 2015 at 14:54, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 16/04/2015 3:20 p.m., Alex Samad wrote:
>> :)  Living in Australia that has just sign into law meta data
>> recording. So I am sending some of my traffic OS via a vpn service.
>>
>> But I still want some things to go locally, so I was using src ip
>> address to help my router determine which path to use.
>>
>> unfortunately my vpn service doesn't handle IPv6, plus from memory
>> there is no NAT for ipv6 (last time i looked) so .. I want to allow
>> ipv6 for somethings but I want to force all traffic for certain site
>> via the vpn.
>>
>>
>> Unfortunately I might have to just kill all ipv6 ..
>
> Your best hope is actually to do the *opposite*.
>
> Make IPv6 actually work. You can firewall IPv6 traffic going to the
> selected set of sites IP ranges, just like you would block them if you
> didnt want IPv4 connections to go there.

Hmm okay so your saying that squid will try the ipv6 address and if it
get a tcp reject (icmp), not available it will fail back to ipv4 ?


>
>
> And yes there is NAT in IPv6 now. And yes it causes just as much trouble
> there as it does for IPv4 - but gratuitously since most of the things
> NAT is useful for in IPv4 have better alternatives in IPv6.
Yeah I heard some rumor, but haven't had a chance to look at it :)
>
> Amos
>


From squid3 at treenet.co.nz  Thu Apr 16 06:15:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 18:15:12 +1200
Subject: [squid-users] tcp_outgoing_address
In-Reply-To: <CAJ+Q1PWBOH8x67UKq-DWzH__pGdRhsnRzw2RFvdg6vua6UfubA@mail.gmail.com>
References: <CAJ+Q1PW9AoPRJo-PbGdEdSsfLJcvCANLc9zH=17MgmFpFWSDkA@mail.gmail.com>	<552E34CB.1080401@treenet.co.nz>	<CAJ+Q1PV3nd8F6OzPN7sHFOmXg+a3pLSiuMuPeYqck6ujkJYAsw@mail.gmail.com>	<552F255C.1000603@treenet.co.nz>	<CAJ+Q1PXgxSC+Dcp3L6efesporf6mcyXvQdRcyh2sjgnTUJZrpA@mail.gmail.com>	<552F408E.6000803@treenet.co.nz>
 <CAJ+Q1PWBOH8x67UKq-DWzH__pGdRhsnRzw2RFvdg6vua6UfubA@mail.gmail.com>
Message-ID: <552F5370.90701@treenet.co.nz>

On 16/04/2015 5:59 p.m., Alex Samad wrote:
> On 16 April 2015 at 14:54, Amos Jeffries wrote:
>> On 16/04/2015 3:20 p.m., Alex Samad wrote:
>>> :)  Living in Australia that has just sign into law meta data
>>> recording. So I am sending some of my traffic OS via a vpn service.
>>>
>>> But I still want some things to go locally, so I was using src ip
>>> address to help my router determine which path to use.
>>>
>>> unfortunately my vpn service doesn't handle IPv6, plus from memory
>>> there is no NAT for ipv6 (last time i looked) so .. I want to allow
>>> ipv6 for somethings but I want to force all traffic for certain site
>>> via the vpn.
>>>
>>>
>>> Unfortunately I might have to just kill all ipv6 ..
>>
>> Your best hope is actually to do the *opposite*.
>>
>> Make IPv6 actually work. You can firewall IPv6 traffic going to the
>> selected set of sites IP ranges, just like you would block them if you
>> didnt want IPv4 connections to go there.
> 
> Hmm okay so your saying that squid will try the ipv6 address and if it
> get a tcp reject (icmp), not available it will fail back to ipv4 ?
> 

Yes. Well, it will fall back to the next route Squid is aware of. There
may be multiple IPv4 and IPv6 routes being tested.

Amos


From ahmed.zaeem at netstream.ps  Thu Apr 16 18:08:53 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 16 Apr 2015 11:08:53 -0700
Subject: [squid-users] squid with outlook and other applications
Message-ID: <001b01d07870$67100dc0$35302940$@netstream.ps>

Hi , just wanted to ask question about squid & email/outlook

Now can squid be proxy for emails ??

 

Assume I have an outlook that works with 25  , http & https

 

S it possible to use squid as proxy for port 25 ? or its just only for  http
& https ??

 

If I have email server and have client

Can the client use the squid as proxy and proxy send requests to email  ?

 

 

As I know , squid is only for http & https and not for any other thing .

 

Help to help

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150416/712c189d/attachment.htm>

From timothee.christin at gmail.com  Thu Apr 16 09:05:49 2015
From: timothee.christin at gmail.com (tchristin)
Date: Thu, 16 Apr 2015 02:05:49 -0700 (PDT)
Subject: [squid-users] Integrate Squid Kerberos auth and Squidguard
 ldapusersearch into AD
In-Reply-To: <552E8CF1.1010304@urlfilterdb.com>
References: <1429108682047-4670758.post@n4.nabble.com>
 <552E8CF1.1010304@urlfilterdb.com>
Message-ID: <1429175149754-4670778.post@n4.nabble.com>

Hi Marcus and thanks for your reply.
I would prefer to keep squidGuard... any other solution ?

Thanks,

Tim.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Integrate-Squid-Kerberos-auth-and-Squidguard-ldapusersearch-into-AD-tp4670758p4670778.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Apr 16 11:44:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2015 23:44:56 +1200
Subject: [squid-users] Integrate Squid Kerberos auth and Squidguard
 ldapusersearch into AD
In-Reply-To: <1429175149754-4670778.post@n4.nabble.com>
References: <1429108682047-4670758.post@n4.nabble.com>
 <552E8CF1.1010304@urlfilterdb.com> <1429175149754-4670778.post@n4.nabble.com>
Message-ID: <552FA0B8.2070505@treenet.co.nz>

On 16/04/2015 9:05 p.m., tchristin wrote:
> Hi Marcus and thanks for your reply.
> I would prefer to keep squidGuard... any other solution ?

Using Squid ACLs for the access controls.

Seriously, squidguard as a project died some time ago. And the whole
concept of performing "security" by mangling the client request messages
handled by Squid to produce a forged server response is a bit twisted to
begin with.

Amos



From timothee.christin at gmail.com  Thu Apr 16 11:36:53 2015
From: timothee.christin at gmail.com (tchristin)
Date: Thu, 16 Apr 2015 04:36:53 -0700 (PDT)
Subject: [squid-users] Integrate Squid Kerberos auth and Squidguard
 ldapusersearch into AD
In-Reply-To: <552FA0B8.2070505@treenet.co.nz>
References: <1429108682047-4670758.post@n4.nabble.com>
 <552E8CF1.1010304@urlfilterdb.com> <1429175149754-4670778.post@n4.nabble.com>
 <552FA0B8.2070505@treenet.co.nz>
Message-ID: <1429184213824-4670780.post@n4.nabble.com>

Thanks Amos for your franchise.
I'm going to try ufdbguard instead of squidGuard.

And thanks for the amazing squid's project !!

Cheers,

Tim.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Integrate-Squid-Kerberos-auth-and-Squidguard-ldapusersearch-into-AD-tp4670758p4670780.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Thu Apr 16 13:12:51 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 16 Apr 2015 06:12:51 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429103177898-4670756.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
Message-ID: <1429189971025-4670781.post@n4.nabble.com>

i changed the squid version to last updated one 
squid-3.4.12-20150414-r13219
and also the same assertion error ...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670781.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Apr 16 13:36:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Apr 2015 01:36:40 +1200
Subject: [squid-users] squid with outlook and other applications
In-Reply-To: <001b01d07870$67100dc0$35302940$@netstream.ps>
References: <001b01d07870$67100dc0$35302940$@netstream.ps>
Message-ID: <552FBAE8.8090909@treenet.co.nz>

On 17/04/2015 6:08 a.m., snakeeyes wrote:
> Hi , just wanted to ask question about squid & email/outlook
> 
> Now can squid be proxy for emails ??
> 

Squid is HTTP software.

> 
> Assume I have an outlook that works with 25  , http & https
> 
>  
> 
> S it possible to use squid as proxy for port 25 ? or its just only for  http
> & https ??
> 

Only for the HTTP and HTTPS.

>  
> 
> If I have email server and have client
> 
> Can the client use the squid as proxy and proxy send requests to email  ?
> 

No.

> 
> As I know , squid is only for http & https and not for any other thing .
> 

Did you see any announcement about Squid becoming an email server?


You want a proxy for port 25? yes they do exist and are called SMTP servers.


Amos



From squid3 at treenet.co.nz  Thu Apr 16 13:39:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Apr 2015 01:39:17 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429189971025-4670781.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com>
Message-ID: <552FBB85.5060009@treenet.co.nz>

On 17/04/2015 1:12 a.m., HackXBack wrote:
> i changed the squid version to last updated one 
> squid-3.4.12-20150414-r13219
> and also the same assertion error ...
> 

Good. Now you can start working on what the assertion is secure in the
knowledge that the ssl_crtd corruption is not getting subtly in the way.

Amos



From ahmed.zaeem at netstream.ps  Fri Apr 17 03:16:05 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 16 Apr 2015 20:16:05 -0700
Subject: [squid-users] squid with outlook and other applications
In-Reply-To: <552FBAE8.8090909@treenet.co.nz>
References: <001b01d07870$67100dc0$35302940$@netstream.ps>
 <552FBAE8.8090909@treenet.co.nz>
Message-ID: <000201d078bd$04658ae0$0d30a0a0$@netstream.ps>

Thankx alot

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, April 16, 2015 6:37 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid with outlook and other applications

On 17/04/2015 6:08 a.m., snakeeyes wrote:
> Hi , just wanted to ask question about squid & email/outlook
> 
> Now can squid be proxy for emails ??
> 

Squid is HTTP software.

> 
> Assume I have an outlook that works with 25  , http & https
> 
>  
> 
> S it possible to use squid as proxy for port 25 ? or its just only for  
> http & https ??
> 

Only for the HTTP and HTTPS.

>  
> 
> If I have email server and have client
> 
> Can the client use the squid as proxy and proxy send requests to email  ?
> 

No.

> 
> As I know , squid is only for http & https and not for any other thing .
> 

Did you see any announcement about Squid becoming an email server?


You want a proxy for port 25? yes they do exist and are called SMTP servers.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From hack.back at hotmail.com  Thu Apr 16 19:18:03 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 16 Apr 2015 12:18:03 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <552FBB85.5060009@treenet.co.nz>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
Message-ID: <1429211883832-4670785.post@n4.nabble.com>

Thank you Amos for answer,
but can you please tell me what may cause this assertion error so i can
solve it ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670785.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From iridium1191 at gmail.com  Fri Apr 17 00:51:39 2015
From: iridium1191 at gmail.com (iridium191)
Date: Thu, 16 Apr 2015 17:51:39 -0700 (PDT)
Subject: [squid-users] Squid downloading huge amounts of un-requested
	data
In-Reply-To: <552F463B.2090106@treenet.co.nz>
References: <1429154215215-4670770.post@n4.nabble.com>
 <552F463B.2090106@treenet.co.nz>
Message-ID: <1429231899613-4670786.post@n4.nabble.com>

Thanks for your response Amos, it is much appreciated. 
The config is below, with comments excluded - we've done tests in the past
to confirm it is not an open proxy and don't believe it is. Any commnts you
may have would also be appreciated.
The past excessive download events correlated with Microsoft patch Tuesdays
or in the most recent case deploying a new Windows server and then manually
updating it, which made us suspect that our refresh rules attempting to
cache Windows updates was the cause of the problem.

In the config squidguard should be bypassed for Windows updates and
squidclamav uses its own whitelist to bypass Windows update sites.

Our traffic monitoring so far has been aggregated, so we could see that
103GB of http traffic was directed to the squid server from the firewall,
and of that 15GB came from Microsoft, 12GB from akamai server 1 etc.. You're
right we didn't consider that something other than squid on the server may
be causing the requests.

The cache utilization report looks interesting in that we may be able to
script it for more real-time notification of excessive traffic rather than
relying on the morning firewall report. Are there any definitions of the
various counters, eg client_http.kbytes_in, client_http.kbytes_in ?

Thanks again,

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280                # http-mgmt
acl Safe_ports port 488                # gss-http
acl Safe_ports port 591                # filemaker
acl Safe_ports port 777                # multiling http

acl CONNECT method CONNECT
acl ftp proto FTP

acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
acl Purge method PURGE

acl Local_Networks src 10.250.111.0/24 10.250.112.0/24
acl BypassCache dst 10.250.111.0/24 10.250.112.0/24
acl BypassCache dst 146.178.211.0/24

acl BypassCacheDomains dstdomain "/etc/squid3/BypassCacheDomains"
acl RestrictedUsers proxy_auth "/etc/squid3/RestrictedUsers"

# ACLs for Windows Updates & other exceptions
acl WindowsUpdate dstdomain "/etc/squid3/WindowsUpdate"
acl Whitelist_Domains dstdomain "/etc/squid3/Whitelist_Domains"

# ACL to allow monitoring of entire proxy chain from 10.250.111.124 without
authentication 
acl MonitorProxy src 10.250.111.124/32

acl Get_Username proxy_auth REQUIRED

# Bypass squidguard for whitelisted domains
redirector_access deny Whitelist_Domains
redirector_access deny WindowsUpdate
# Bypass squidguard for local sites 
redirector_access deny BypassCache
redirector_access deny BypassCacheDomains

# Bypass connections to local network and TLS
always_direct allow BypassCache
cache deny BypassCache
always_direct allow BypassCacheDomains
cache deny BypassCacheDomains

http_access allow manager localhost
http_access allow localhost Purge
http_access deny manager
http_access deny Purge
http_access deny to_localhost
http_access deny !Local_Networks
http_access allow Whitelist_Domains
http_access allow WindowsUpdate
http_access allow MonitorProxy
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
# Allow connection to HTTPS sites from the local network
http_access allow CONNECT SSL_ports Local_Networks
http_access allow ftp
http_access allow !RestrictedUsers

http_access deny all

http_port 8080
visible_hostname Squid3
hierarchy_stoplist cgi-bin ?

# Log file locations
access_log daemon:/var/log/squid3/access.log squid
cache_store_log none
cache_log /var/log/squid3/cache.log

# Disk cache directory.
cache_dir aufs /squid_cache/Squid3Cache 25000 16 256
cache_mem 2000 MB
maximum_object_size_in_memory 1 MB

# Windows Update
#range_offset_limit 200 MB WindowsUpdate 
maximum_object_size 1 GB
#quick_abort_min -1

dns_nameservers 127.0.0.1

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_req reqmod_precache bypass=0
icap://127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0
icap://127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 20 startup=0 idle=1 concurrency=0

#Do not show client IP address
via off
forwarded_for off

#Rules to anonymize http headers
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access Cookie allow all
###request_header_access All deny all




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-downloading-huge-amounts-of-un-requested-data-tp4670770p4670786.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From dan at getbusi.com  Fri Apr 17 01:30:33 2015
From: dan at getbusi.com (djch)
Date: Thu, 16 Apr 2015 18:30:33 -0700 (PDT)
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <201408202220.16861.Antony.Stone@squid.open.source.it>
References: <1408565646927-4667291.post@n4.nabble.com>
 <201408202220.16861.Antony.Stone@squid.open.source.it>
Message-ID: <1429234233466-4670787.post@n4.nabble.com>

I just wanted to revive this thread to note that:

- Delay pools apply just fine to HTTPS requests in Squid 2.7.
- Delay pools in Squid 3.4.x are also applied to HTTPS but the speed is not
correct. 
  - If I apply a 56 Kbps limit the HTTP download tops out at ~7 KB/s. If I
download the same file from the same server via HTTPS it tops out at
~90KB/s. If I download the same file over HTTPS with no delay pools
configured it tops at around 3MB/s.

So I guess that would make this a bug? Which I assume nobody wants to fix
'cause they're going to be deprecated at some point soon?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Individual-delay-pools-and-youtube-tp4667291p4670787.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Fri Apr 17 07:06:23 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 17 Apr 2015 00:06:23 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:557: "F->flags.open"
Message-ID: <1429254383156-4670788.post@n4.nabble.com>

Hi All,

Is anyone with an trick regarding this error message in the cache.log ?
*assertion failed: comm.cc:557: "F->flags.open"*

Squid *3.5.3-20150415-r13798*.
config in diskd, tproxy and ssl_bump.

when the squid faces this error, it reloads itself but it breaks the surf
for a while.

Thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-F-flags-open-tp4670788.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahaitoute at rinis.nl  Fri Apr 17 08:39:16 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Fri, 17 Apr 2015 10:39:16 +0200
Subject: [squid-users] handling different two way ssl-request via a
	proxy system
In-Reply-To: <552E2DEA.8070102@treenet.co.nz>
References: <B893BE06-86C2-492D-A0E9-A80179DA2C15@rinis.nl>
 <552D3E63.7080704@treenet.co.nz>
 <206AD888-469E-40DF-9402-2AAC69BFEE3C@rinis.nl>
 <552E2DEA.8070102@treenet.co.nz>
Message-ID: <52540648-A5E2-4429-BFFE-1CE7C3257E4E@rinis.nl>

Thanks Amos.

I will submit this as a new feature request on http://bugs.squid-cache.org <http://bugs.squid-cache.org/>/

Abdelouahed

> Op 15 apr. 2015, om 11:22 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
> On 15/04/2015 7:12 a.m., Abdelouahed Haitoute wrote:
>> Hello Amos,
>> 
>> Thank you for your reply.
>> 
>>> Problem A)
>>> requirements #1 and #2 above are mutually exclusive.
>>> 
>>> Obeying #1 involves sending traffic from Squid to the parent proxy for
>>> action.
>>> 
>>> Obeying #2 involves opening direct TLS connections from Squid to the
>>> origin servers.
>> 
>> Is it possible to send the TLS connection as a HTTP CONNECT tunnel via through the parent proxy? (http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling <http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling>)
>> 
> 
> In theory, but Squid does not yet support that except in the case that
> traffic was intercepted and is non-HTTP(S).
> 
> Amos
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150417/6b70cdc6/attachment.htm>

From Gary.Woodman at usq.edu.au  Fri Apr 17 07:45:44 2015
From: Gary.Woodman at usq.edu.au (Gary Woodman)
Date: Fri, 17 Apr 2015 07:45:44 +0000
Subject: [squid-users] squid logs not rotating
Message-ID: <81D4BEDF5F61664A9ADB2FF8E4AA61BD24C169D5@EXCH-MBX-PRD-T3.usq.edu.au>

Greetings!

We have a couple of instances of squid to service our students' Internet access.

Squid version is:
Squid Cache: Version 3.4.10
We use the pre-packaged Red Hat binary from http://www1.ngtech.co.il/repo/centos/, as linked to from the squid wiki, as the Red Hat version is rather old.

The two instances are effectively identical; they are not peered, we just use round-robin DNS. We have a script to sync the configs (barring obvious things like tcp_outgoing_address and visible_hostname).

The first system, I'll call T1, functions fine and specifically, the logging behaves as expected and like all our other squid instances. We have "log_rotate 30" to keep 30 days of logs, and "squid -k rotate" at midnight via cron. The rotate renumbers all the logs, and squid starts writing in a new access.log. This is our main interest, as data from access logs feeds our quotaing and other usage reports.

The second system, T2, has the same setup and while it renumbers the logs all right, it doesn't close them or start writing in new ones. Thus we see a succession of zero-length access.logs, eventually access.log.29 reaches over 3 gigabytes, and on the next rotation, the whole lot is thrown away and squid finally starts writing in a fresh access.log, which gradually works its way up to access.log.29 again in the next log_rotate cycle. This is rather disadvantageous, as most of the time there will not be 30 days worth of data in the logs like on T1.

Other logs such as cache.log and store.log suffer equally the same buildup and wipe.

Does anyone have any suggestions? How might we go about debugging this situation?


Regards
Gary

Gary Woodman
Enterprise Systems Engineer

ICT Infrastructure Services | ICT Services | University of Southern Queensland
Toowoomba | Queensland | 4350 | Australia
Phone: +61 7 4631 1688 | Fax: +61 7 4631 2798
Email: gary.woodman at usq.edu.au<mailto:gary.woodman at usq.edu.au>
(CRICOS No: 00244B QLD; 02225M NSW; TEQSA PRV12081)



_____________________________________________________________
This email (including any attached files) is confidential and is for the intended recipient(s) only. If you received this email by mistake, please, as a courtesy, tell the sender, then delete this email.

The views and opinions are the originator's and do not necessarily reflect those of the University of Southern Queensland. Although all reasonable precautions were taken to ensure that this email contained no viruses at the time it was sent we accept no liability for any losses arising from its receipt.

The University of Southern Queensland is a registered provider of education with the Australian Government.
(CRICOS Institution Code QLD 00244B / NSW 02225M, TEQSA PRV12081 )

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150417/7ee473ce/attachment.htm>

From tfransosi at gmail.com  Fri Apr 17 12:57:17 2015
From: tfransosi at gmail.com (Thiago Farina)
Date: Fri, 17 Apr 2015 09:57:17 -0300
Subject: [squid-users] with nginx
Message-ID: <CACnwZYds4vPqqdiyegRSou+PbE+fDTGQKXE-Mb+UFe9eCV0+zQ@mail.gmail.com>

Hi,

Just curious, does people use squid together with nginx?

-- 
Thiago Farina


From tfransosi at gmail.com  Fri Apr 17 12:59:43 2015
From: tfransosi at gmail.com (Thiago Farina)
Date: Fri, 17 Apr 2015 09:59:43 -0300
Subject: [squid-users] Fwd: with nginx
In-Reply-To: <CACnwZYds4vPqqdiyegRSou+PbE+fDTGQKXE-Mb+UFe9eCV0+zQ@mail.gmail.com>
References: <CACnwZYds4vPqqdiyegRSou+PbE+fDTGQKXE-Mb+UFe9eCV0+zQ@mail.gmail.com>
Message-ID: <CACnwZYeekPwyfktmNB_GtcKLAmqL=fi_pL70kpOMhO9Hf9p0xg@mail.gmail.com>

Hi,

Just curious, does people use squid together with nginx?

-- 
Thiago Farina


From michael at hendrie.id.au  Fri Apr 17 13:27:11 2015
From: michael at hendrie.id.au (Michael Hendrie)
Date: Fri, 17 Apr 2015 22:57:11 +0930
Subject: [squid-users] assertion failed: ../src/ipc/AtomicWord.h:88:
	"Enabled()"
In-Reply-To: <2FC66EBD-75C8-4ADD-A8C6-38759120173F@getbusi.com>
References: <CD8F4E6B-5D87-4732-80EE-402D5169B606@getbusi.com>
 <C55F6BE7-274E-488B-AAB5-28B600D655DD@getbusi.com>
 <2FC66EBD-75C8-4ADD-A8C6-38759120173F@getbusi.com>
Message-ID: <AE1DCD02-887C-46D2-890F-9C5783FCC80E@hendrie.id.au>

On 27 Mar 2015, at 9:17 am, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
> 
> Bumping this because I think it might have gone into the black hole the other night.
> 
>> On 23 Mar 2015, at 5:44 pm, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>> 
>> Turns out it?s also shitting the bed whenever I go to an SSL site now that I?ve added --enable-storeio=rock:
>> 
>> 2015/03/23 17:40:13 kid1| assertion failed: ../src/ipc/AtomicWord.h:71: "Enabled()"
>> 2015/03/23 17:42:02 kid1| assertion failed: ../src/ipc/AtomicWord.h:74: "Enabled()"
>> 

Came across this same issue today after building squid-3.5.3 but wasn?t getting the same result in 3.5.2 which I found strange.

In my case I found it was actually caused by the change in r13783 causing the test in configure to fail.  I?m compiling with gcc-4.4.5 and to overcome set CXXFLAGS ?-std=c++0x' or changed the test in configure to use <stdint.h> instead of <cstdint>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150417/175fc993/attachment.htm>

From hack.back at hotmail.com  Fri Apr 17 13:39:50 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 17 Apr 2015 06:39:50 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429211883832-4670785.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
Message-ID: <1429277990509-4670794.post@n4.nabble.com>

why this issue happen i cant find what cause problem ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670794.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From johnzeng2013 at yahoo.com  Fri Apr 17 14:56:34 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 17 Apr 2015 22:56:34 +0800
Subject: [squid-users] how to realize (external_acl_type) logout without
	squid -k reconfigure
Message-ID: <55311F22.40809@yahoo.com>


Hello Dear Sir :

We tested external_acl_type for web authentication via writing check.php ,

and login feature is fine

squid config is

external_acl_type session ipv4 concurrency=10 ttl=3600 negative_ttl=0
cache=1048576 %SRC /opt/check.php

check.php ( for login) is

fwrite(STDOUT, $source_ip." OK\n");


But if we don't squid -k reconfigure , logout feature is bad for me how
to update Ok or ERR cache entry without squid -k reconfigure

check.php ( for logout ) is

fwrite(STDOUT, $source_ip." ERR\n");



if possible , please give me some advisement







From johnzeng2013 at yahoo.com  Fri Apr 17 15:06:20 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 17 Apr 2015 23:06:20 +0800
Subject: [squid-users] how to realize (external_acl_type) logout without
	squid -k reconfigure
In-Reply-To: <55311F22.40809@yahoo.com>
References: <55311F22.40809@yahoo.com>
Message-ID: <5531216C.3030403@yahoo.com>




Hello Dear Sir :






 We tested external_acl_type for web authentication via writing check.php ,

and login feature is fine

squid config is

external_acl_type session ipv4 concurrency=10 ttl=3600 negative_ttl=0
cache=1048576 %SRC /opt/check.php

check.php ( for login) is

fwrite(STDOUT, $stream_id." OK\n");


But if we don't squid -k reconfigure , logout feature is bad for me how
to update Ok or ERR cache entry without squid -k reconfigure

check.php ( for logout ) is

fwrite(STDOUT, $stream_id." ERR\n");



if possible , please give me some advisement



$stream_line = trim(fgets(STDIN));
$stream_array = split("[ ]+", $stream_line);

if(isset($stream_array[1]))
{
$stream_ip = trim($stream_array[1]);
}

if(isset($stream_array[0]))
{
$stream_id = trim($stream_array[0]);
}
 





From sebag at vianetcon.com.ar  Fri Apr 17 16:16:24 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Fri, 17 Apr 2015 13:16:24 -0300
Subject: [squid-users] Very high CPU ussage after migration 2.7 -> 3.5
Message-ID: <553131D8.9030303@vianetcon.com.ar>

Hello,

I have migrated from Squid 2.7 to Squid 3.5 given the fact that I need 
some of the new functions that newer versions have.
The problem I'm having is that squid proccess is using 100% of CPU all 
the time. Cache.log informed squid was low on file descriptors, but 
that's already solved.

Now I see a lot of these in cache.log:

2015/04/17 13:01:07 kid1| Starting new store_id helpers...
2015/04/17 13:01:07 kid1| helperOpenServers: Starting 1/30 'jesred' 
processes
2015/04/17 13:01:08 kid1| Starting new store_id helpers...
2015/04/17 13:01:08 kid1| helperOpenServers: Starting 1/30 'jesred' 
processes

It always says 1/30, is it only using one rewriter? Most of the CPU time 
is idle time, so maybe squid is waiting for something, but I don't know 
what.

squid -v
Squid Cache: Version 3.5.3
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
'--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
'--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools' 
'--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 
'--enable-eui' '--enable-snmp' '--enable-gnuregex' 
'--enable-ltdl-convenience' '--enable-removal-policies=lru heap' 
'--enable-http-violations' '--with-openssl' 
'--with-filedescriptors=24321' '--enable-poll' '--enable-epoll'


Any ideas?


Thanks,
Sebastian


From jkrautter at liveops.com  Fri Apr 17 16:53:10 2015
From: jkrautter at liveops.com (Jonathan Krautter)
Date: Fri, 17 Apr 2015 16:53:10 +0000
Subject: [squid-users] Having Squid listen on another port and forward all
 traffic to a specific address
Message-ID: <1429289589741.19806@liveops.com>

Is there any way to have squid listen on an additional port and then forward any traffic on that port to a specific address?  Example:

proxy listens on port 4456
proxy forwards all traffic received on port 4456 to 72.13.234.15 on port 4456
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150417/f166808d/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Apr 17 17:29:00 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 17 Apr 2015 19:29:00 +0200
Subject: [squid-users] Having Squid listen on another port and forward
	all traffic to a specific address
In-Reply-To: <1429289589741.19806@liveops.com>
References: <1429289589741.19806@liveops.com>
Message-ID: <201504171929.01085.Antony.Stone@squid.open.source.it>

On Friday 17 April 2015 at 18:53:10 (EU time), Jonathan Krautter wrote:

> Is there any way to have squid listen on an additional port and then
> forward any traffic on that port to a specific address?  Example:
> 
> proxy listens on port 4456
> proxy forwards all traffic received on port 4456 to 72.13.234.15 on port
> 4456

No, but this is trivial to implement using IPtables or whatever routing/NAT 
system your OS uses.

Regards,


Antony.

-- 
"I estimate there's a world market for about five computers."

 - Thomas J Watson, Chairman of IBM

                                                   Please reply to the list;
                                                         please *don't* CC me.


From sebag at vianetcon.com.ar  Fri Apr 17 17:29:37 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Fri, 17 Apr 2015 14:29:37 -0300
Subject: [squid-users] Very high CPU ussage after migration 2.7 -> 3.5
In-Reply-To: <553131D8.9030303@vianetcon.com.ar>
References: <553131D8.9030303@vianetcon.com.ar>
Message-ID: <55314301.3070203@vianetcon.com.ar>

Something new:

Taking away all refresh patterns the CPU usage goes down to around 15%
Patterns look like this, maybe I have to change something:

refresh_pattern -i ^http://c2r\.microsoft\.com/ConsumerC2R/(.*) 43200 
100% 129600 override-expire ignore-reload  ignore-private override-lastmod

refresh_pattern -i 
^http:\/\/.*\.avg\.com\/softw\/.*\/update\/(.*\.bin)   43200 100% 
129600  override-expire ignore-reload  ignore-private override-lastmod


Thanks

El 17/04/15 a las 13:16, Sebastian Goicochea escribi?:
> Hello,
>
> I have migrated from Squid 2.7 to Squid 3.5 given the fact that I need 
> some of the new functions that newer versions have.
> The problem I'm having is that squid proccess is using 100% of CPU all 
> the time. Cache.log informed squid was low on file descriptors, but 
> that's already solved.
>
> Now I see a lot of these in cache.log:
>
> 2015/04/17 13:01:07 kid1| Starting new store_id helpers...
> 2015/04/17 13:01:07 kid1| helperOpenServers: Starting 1/30 'jesred' 
> processes
> 2015/04/17 13:01:08 kid1| Starting new store_id helpers...
> 2015/04/17 13:01:08 kid1| helperOpenServers: Starting 1/30 'jesred' 
> processes
>
> It always says 1/30, is it only using one rewriter? Most of the CPU 
> time is idle time, so maybe squid is waiting for something, but I 
> don't know what.
>
> squid -v
> Squid Cache: Version 3.5.3
> Service Name: squid
> configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
> '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
> '--localstatedir=/var' '--sysconfdir=/etc/squid3' 
> '--enable-delay-pools' '--enable-ssl' '--enable-ssl-crtd' 
> '--enable-linux-netfilter' '--enable-eui' '--enable-snmp' 
> '--enable-gnuregex' '--enable-ltdl-convenience' 
> '--enable-removal-policies=lru heap' '--enable-http-violations' 
> '--with-openssl' '--with-filedescriptors=24321' '--enable-poll' 
> '--enable-epoll'
>
>
> Any ideas?
>
>
> Thanks,
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From nathan at getoffmalawn.com  Sat Apr 18 04:16:14 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Sat, 18 Apr 2015 14:16:14 +1000
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429277990509-4670794.post@n4.nabble.com>
References: <1429046161591-4670726.post@n4.nabble.com>
 <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
Message-ID: <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>

As I said earlier, if you can get this information from gdb that would
be helpful.

frame 3
print ccb->conn.p_
print ccb->conn.p_->fd
print conn.p_
print conn.p_->fd

On 17 April 2015 at 23:39, HackXBack <hack.back at hotmail.com> wrote:
> why this issue happen i cant find what cause problem ..
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670794.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From hack.back at hotmail.com  Sat Apr 18 08:46:49 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 18 Apr 2015 01:46:49 -0700 (PDT)
Subject: [squid-users] keep data after format
Message-ID: <1429346809298-4670802.post@n4.nabble.com>

if i want to format my system squid box [debian7]
and install fresh system,
and i have 4 hdd data of old cached files, and i want to keep them,
formating the system and installing fresh one will make loss of data ? 
and how to keep them ...?
Thanks ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/keep-data-after-format-tp4670802.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sat Apr 18 08:49:16 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 18 Apr 2015 01:49:16 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
References: <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
Message-ID: <1429346956458-4670803.post@n4.nabble.com>

[New LWP 20933]
[New LWP 20907]
[New LWP 20918]
[New LWP 20922]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f51ea275165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) frame 3
#3  0x00000000007996a3 in comm_read(RefCount<Comm::Connection> const&,
char*, int, RefCount<AsyncCall>&) ()
(gdb) print ccb->conn.p_
No symbol table is loaded.  Use the "file" command.
(gdb) print ccb->conn.p_->fd
No symbol table is loaded.  Use the "file" command.
(gdb) print conn.p_
No symbol table is loaded.  Use the "file" command.
(gdb) print conn.p_->fd
No symbol table is loaded.  Use the "file" command.
(gdb)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670803.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From nathan at getoffmalawn.com  Sat Apr 18 09:32:14 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Sat, 18 Apr 2015 19:32:14 +1000
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429346956458-4670803.post@n4.nabble.com>
References: <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
Message-ID: <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>

As I mentioned earlier, this hasn't worked the because the debug
symbols you're running gdb against don't quite match the binary in
which the crash occurred. I would check the version of the debug
symbols you're running gdb against versus the version of the squid
binary that was installed. These changes can include patches and
--configure changes, not just version changes of Squid.

I'm not sure how debug symbols are "shipped" with packages in Debian -
under CentOS for example they are shipped separately from the squid
package, as squid-debuginfo-<version>.rpm.

On 18 April 2015 at 18:49, HackXBack <hack.back at hotmail.com> wrote:
> [New LWP 20933]
> [New LWP 20907]
> [New LWP 20918]
> [New LWP 20922]
>
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
> Program terminated with signal 6, Aborted.
> #0  0x00007f51ea275165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) frame 3
> #3  0x00000000007996a3 in comm_read(RefCount<Comm::Connection> const&,
> char*, int, RefCount<AsyncCall>&) ()
> (gdb) print ccb->conn.p_
> No symbol table is loaded.  Use the "file" command.
> (gdb) print ccb->conn.p_->fd
> No symbol table is loaded.  Use the "file" command.
> (gdb) print conn.p_
> No symbol table is loaded.  Use the "file" command.
> (gdb) print conn.p_->fd
> No symbol table is loaded.  Use the "file" command.
> (gdb)
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670803.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From talwandi108 at gmail.com  Sat Apr 18 11:27:36 2015
From: talwandi108 at gmail.com (Ali Raza)
Date: Sat, 18 Apr 2015 15:27:36 +0400
Subject: [squid-users] update an objects in Squid
Message-ID: <CAAhjccFz-0bc56uGXQXubV=S-f6V2pzP4AZbpOwGN3_pNNpTVg@mail.gmail.com>

t there is a problem I can't figure out. I set up squid proxy on my linux
machine and want to work it as dumb proxy except two features.

1- It should allow me to delete objects of my choice (that I can do using
PURGE thing using squid-client)
 2- It should allow me to add objects manually for example, if I want to
add some html file with some information like max-age and other http header
how I can add that to squid cache.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/67bd6d1c/attachment.htm>

From talwandi108 at gmail.com  Sat Apr 18 11:29:05 2015
From: talwandi108 at gmail.com (Ali Raza)
Date: Sat, 18 Apr 2015 15:29:05 +0400
Subject: [squid-users] update an objects in Squid
In-Reply-To: <CAAhjccFz-0bc56uGXQXubV=S-f6V2pzP4AZbpOwGN3_pNNpTVg@mail.gmail.com>
References: <CAAhjccFz-0bc56uGXQXubV=S-f6V2pzP4AZbpOwGN3_pNNpTVg@mail.gmail.com>
Message-ID: <CAAhjccHOGx-Po1iL8UrB4V5w+odE5Nn469T7ZTq_tETcQ+bOgQ@mail.gmail.com>

t there is a problem I can't figure out. I set up squid proxy on my linux
machine and want to work it as dumb proxy except two features.

1- It should allow me to delete objects of my choice (that I can do using
PURGE thing using squid-client)
 2- It should allow me to add objects manually for example, if I want to
add some html file with some information like max-age and other http header
how I can add that to squid cache.

On 18 April 2015 at 15:27, Ali Raza <talwandi108 at gmail.com> wrote:

> t there is a problem I can't figure out. I set up squid proxy on my linux
> machine and want to work it as dumb proxy except two features.
>
> 1- It should allow me to delete objects of my choice (that I can do using
> PURGE thing using squid-client)
>  2- It should allow me to add objects manually for example, if I want to
> add some html file with some information like max-age and other http header
> how I can add that to squid cache.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/c76ec0cd/attachment.htm>

From alberto2perez at gmail.com  Sat Apr 18 12:53:07 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Sat, 18 Apr 2015 08:53:07 -0400
Subject: [squid-users] how to realize (external_acl_type) logout without
 squid -k reconfigure
In-Reply-To: <55311F22.40809@yahoo.com>
References: <55311F22.40809@yahoo.com>
Message-ID: <CAMZauGr_iw91=k=weX2ikGfSaz-1g1JLynVCdm=5CjFPqoXEaw@mail.gmail.com>

Hi, john
To be able to make the logout action "instantly" effective you should
change your external acl ttl to a lower value, for example 3, in which
case logout will be effective after 3 sec (reasonable value I think).

You should also worry about performance of credentials check because
squid will make this check each 3 sec for each client.

Hope it helps

Regards


On 4/17/15, johnzeng <johnzeng2013 at yahoo.com> wrote:
>
> Hello Dear Sir :
>
> We tested external_acl_type for web authentication via writing check.php ,
>
> and login feature is fine
>
> squid config is
>
> external_acl_type session ipv4 concurrency=10 ttl=3600 negative_ttl=0
> cache=1048576 %SRC /opt/check.php
>
> check.php ( for login) is
>
> fwrite(STDOUT, $source_ip." OK\n");
>
>
> But if we don't squid -k reconfigure , logout feature is bad for me how
> to update Ok or ERR cache entry without squid -k reconfigure
>
> check.php ( for logout ) is
>
> fwrite(STDOUT, $source_ip." ERR\n");
>
>
>
> if possible , please give me some advisement
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From yvoinov at gmail.com  Sat Apr 18 13:06:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 18 Apr 2015 19:06:39 +0600
Subject: [squid-users] update an objects in Squid
In-Reply-To: <CAAhjccFz-0bc56uGXQXubV=S-f6V2pzP4AZbpOwGN3_pNNpTVg@mail.gmail.com>
References: <CAAhjccFz-0bc56uGXQXubV=S-f6V2pzP4AZbpOwGN3_pNNpTVg@mail.gmail.com>
Message-ID: <553256DF.6010607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.04.15 17:27, Ali Raza ?????:
> t there is a problem I can't figure out. I set up squid proxy on my linux
> machine and want to work it as dumb proxy except two features.
>
> 1- It should allow me to delete objects of my choice (that I can do using
> PURGE thing using squid-client)
Better to use purge utility - now it included in Squid.

>
>  2- It should allow me to add objects manually for example, if I want to
> add some html file with some information like max-age and other http
header
> how I can add that to squid cache.
It is difficult to set up required refresh_pattern and load one time
with squidclient into cache? :)

>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVMlbeAAoJENNXIZxhPexGLjkH/34dqvz/fy+ObmgPYmXfaQ2E
TaOSxSH6Oj9viZ7pTWRly949aKptf87NFd+HvrJNI0OVNd6J14QHHtqCMbilI1nW
lgqX+/gfnF81gMhZXl7iSyxDFz5JZg9tJdSuElOWFMLtZIjv9AyUprIYQc/slx9K
OWuWzL7vchQLnPvu6OqRj6GASUtjI59N+RJqLSk+op1lK5OS6rAMCSCcA1YIB0yc
j2mwNb8rISC3z0LcLUe4pvV9YbrsWAjJURsz8xPyBqFFmfpHxYEdFzm6mM0TT+W0
OJr5uic9rbqAmUNkNJU122JhTTQYgAiAeVPZNzHHYJVXXQOL9p59oOSTv9GaXkA=
=WB2+
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/06c1b590/attachment.htm>

From talwandi108 at gmail.com  Sat Apr 18 13:34:10 2015
From: talwandi108 at gmail.com (Ali Raza)
Date: Sat, 18 Apr 2015 17:34:10 +0400
Subject: [squid-users] adding objects in squid cache
Message-ID: <CAAhjccE6SbB8idffBLwoYfC-gW-ZUtgDiRFRKVSM3sroTGW-Ng@mail.gmail.com>

Here is the example what i want to do:

say there was a file in cache, index.html
I have an other index.html in my computer and I want to replace the
index.html in cache with my index.html including headers in computer.

Thansk alot :(
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/f1ac016b/attachment.htm>

From yvoinov at gmail.com  Sat Apr 18 13:40:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 18 Apr 2015 19:40:22 +0600
Subject: [squid-users] adding objects in squid cache
In-Reply-To: <CAAhjccE6SbB8idffBLwoYfC-gW-ZUtgDiRFRKVSM3sroTGW-Ng@mail.gmail.com>
References: <CAAhjccE6SbB8idffBLwoYfC-gW-ZUtgDiRFRKVSM3sroTGW-Ng@mail.gmail.com>
Message-ID: <55325EC6.9030501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Don't think so.

Simplified:
Squid cache store objects in disk cache with hash value as an index.
To replace object you need completely re-calculate hash, because of new
object become different.
This can be do with Squid itself with normal request, but I don't know
any mechanism to do it from client.

WBR, Yuri

18.04.15 19:34, Ali Raza ?????:
> Here is the example what i want to do:
>
> say there was a file in cache, index.html
> I have an other index.html in my computer and I want to replace the
> index.html in cache with my index.html including headers in computer.
>
> Thansk alot :(
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVMl7GAAoJENNXIZxhPexG+AIH/3I2XHPdr2vM23STHfzHpdw9
3RN2AMiy4jTvJLG5g0HmNq/+Rs7NVkE44ZdKSJtmcdNYtqgkII+MPCQN6tkt6rcX
Fc3Yv7rpnsxoUu8aWJblNgkwZeaLnFiitegVmcgxUFV+/4rGkPLgUmvRfkKdS5qA
UfgAfHbllYgPIGncN210yLmanGd9Zp0yBz6CtSHcpnysUR4LovXGXCfOzdx2PDkc
+otqmR/GlCIEgLy8RK1nKUE48WIAGl0yBCZtBvFdR0AoTsqhfr6BQNJdBoF0pmD1
hxrhouq+3Tj9sSwZRQIaoOfyO3zxKEAyGOvrXB9zazAj+boitqQTC9XY492XfQo=
=+9Yc
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/6e17cc60/attachment.htm>

From squid3 at treenet.co.nz  Sun Apr 19 12:42:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 00:42:05 +1200
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <1429234233466-4670787.post@n4.nabble.com>
References: <1408565646927-4667291.post@n4.nabble.com>
 <201408202220.16861.Antony.Stone@squid.open.source.it>
 <1429234233466-4670787.post@n4.nabble.com>
Message-ID: <5533A29D.9090009@treenet.co.nz>

On 17/04/2015 1:30 p.m., djch wrote:
> I just wanted to revive this thread to note that:
> 
> - Delay pools apply just fine to HTTPS requests in Squid 2.7.
> - Delay pools in Squid 3.4.x are also applied to HTTPS but the speed is not
> correct. 
>   - If I apply a 56 Kbps limit the HTTP download tops out at ~7 KB/s.

That *is* correct.

   56K *bits* /sec == 7K *Bytes* /sec.

   7x8 = 56


> If I
> download the same file from the same server via HTTPS it tops out at
> ~90KB/s. If I download the same file over HTTPS with no delay pools
> configured it tops at around 3MB/s.

Are you sure thats actually HTTPS ?

CONNECT tunnels these days could contain HTTP/2, SPDY, WebSockets or
data in some other (compressed?) format. Any of which achieve faster
percieved "download" than HTTP using the same basic bytes/sec data rate.

NP: squid-2.7 contains a CONNECT bug that prevents SPDY HTTP/2 and
Websockets working properly.

> 
> So I guess that would make this a bug? Which I assume nobody wants to fix
> 'cause they're going to be deprecated at some point soon?
> 

Its a matter of interest. With FOSS you have to cause someone to be
interested in fixing the bug. Best way to do that is to fix it yourself
and present a patch and get it through review to merge. Second best is
to find someone to pay to do all that. Or you can join the many people
who opted just to wait and pray someone else will give it to them free
one day.

There is another option with delay pools. The pooling system is *just* a
packet rate limiting system. The OS these days have several such QoS
systems built in that work far better than Squid algorithms (admittedly
not on a HTTP per-message basis). If you want total traffic control give
those a try.

Amos




From squid3 at treenet.co.nz  Sun Apr 19 13:07:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 01:07:44 +1200
Subject: [squid-users] squid logs not rotating
In-Reply-To: <81D4BEDF5F61664A9ADB2FF8E4AA61BD24C169D5@EXCH-MBX-PRD-T3.usq.edu.au>
References: <81D4BEDF5F61664A9ADB2FF8E4AA61BD24C169D5@EXCH-MBX-PRD-T3.usq.edu.au>
Message-ID: <5533A8A0.4090507@treenet.co.nz>

On 17/04/2015 7:45 p.m., Gary Woodman wrote:
> Greetings!
> 
> We have a couple of instances of squid to service our students' Internet access.
> 
> Squid version is:
> Squid Cache: Version 3.4.10
> We use the pre-packaged Red Hat binary from http://www1.ngtech.co.il/repo/centos/, as linked to from the squid wiki, as the Red Hat version is rather old.
> 
> The two instances are effectively identical; they are not peered, we just use round-robin DNS. We have a script to sync the configs (barring obvious things like tcp_outgoing_address and visible_hostname).
> 
> The first system, I'll call T1, functions fine and specifically, the logging behaves as expected and like all our other squid instances. We have "log_rotate 30" to keep 30 days of logs, and "squid -k rotate" at midnight via cron. The rotate renumbers all the logs, and squid starts writing in a new access.log. This is our main interest, as data from access logs feeds our quotaing and other usage reports.
> 
> The second system, T2, has the same setup and while it renumbers the logs all right, it doesn't close them or start writing in new ones. Thus we see a succession of zero-length access.logs, eventually access.log.29 reaches over 3 gigabytes, and on the next rotation, the whole lot is thrown away and squid finally starts writing in a fresh access.log, which gradually works its way up to access.log.29 again in the next log_rotate cycle. This is rather disadvantageous, as most of the time there will not be 30 days worth of data in the logs like on T1.
> 
> Other logs such as cache.log and store.log suffer equally the same buildup and wipe.
> 
> Does anyone have any suggestions? How might we go about debugging this situation?

I'd almost guarantee you the log rotation script is different on the two
servers.

Most systems use the OS provided log rotation service, which does the
rotate atomically then runs squid -k rotate. squid.conf has
"logfile_rotate 0" to prevent Squid touching the OS rotation numbering.

What you describe is classic symptoms of the OS rotating logs, but only
doing the "squid -k rotate" action once every month.


HTH
Amos



From squid3 at treenet.co.nz  Sun Apr 19 12:01:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 00:01:35 +1200
Subject: [squid-users] Very high CPU ussage after migration 2.7 -> 3.5
In-Reply-To: <55314301.3070203@vianetcon.com.ar>
References: <553131D8.9030303@vianetcon.com.ar>
 <55314301.3070203@vianetcon.com.ar>
Message-ID: <5533991F.7080109@treenet.co.nz>

On 18/04/2015 5:29 a.m., Sebastian Goicochea wrote:
> Something new:
> 
> Taking away all refresh patterns the CPU usage goes down to around 15%
> Patterns look like this, maybe I have to change something:
> 
> refresh_pattern -i ^http://c2r\.microsoft\.com/ConsumerC2R/(.*) 43200
> 100% 129600 override-expire ignore-reload  ignore-private override-lastmod
> 

Dropping that (.*) off the end would be useful. There is an implicit .*
on the start and end of all regex patterns unless you explicitly add teh
^ and $ anchors.

Removing it allows the regex library to stop trying to match this
pattern when the bit you actually care about has been identified.
Instead of continuing through the other 0-64KB of possible URL length
character...by...character.


> refresh_pattern -i
> ^http:\/\/.*\.avg\.com\/softw\/.*\/update\/(.*\.bin)   43200 100%
> 129600  override-expire ignore-reload  ignore-private override-lastmod
> 

What matters more is how many patterns you have, what library is being
used, and how long the traffic URLs are. Each pattern gets applied 1-2
times per request. Squids provided GNUregex is really very slow, almost
any other available is faster these days.


> 
> Thanks
> 
> El 17/04/15 a las 13:16, Sebastian Goicochea escribi?:
>> Hello,
>>
>> I have migrated from Squid 2.7 to Squid 3.5 given the fact that I need
>> some of the new functions that newer versions have.
>> The problem I'm having is that squid proccess is using 100% of CPU all
>> the time. Cache.log informed squid was low on file descriptors, but
>> that's already solved.
>>
>> Now I see a lot of these in cache.log:
>>
>> 2015/04/17 13:01:07 kid1| Starting new store_id helpers...
>> 2015/04/17 13:01:07 kid1| helperOpenServers: Starting 1/30 'jesred'
>> processes
>> 2015/04/17 13:01:08 kid1| Starting new store_id helpers...
>> 2015/04/17 13:01:08 kid1| helperOpenServers: Starting 1/30 'jesred'
>> processes
>>
>> It always says 1/30, is it only using one rewriter? Most of the CPU
>> time is idle time, so maybe squid is waiting for something, but I
>> don't know what.

That means it is starting 1 *more*, out of a maximum of 30.

If you find these occuring often, increase the idle= helper parameter to
something lke 2,3,5 etc.

Amos


From squid3 at treenet.co.nz  Sun Apr 19 12:16:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 00:16:05 +1200
Subject: [squid-users] Fwd: with nginx
In-Reply-To: <CACnwZYeekPwyfktmNB_GtcKLAmqL=fi_pL70kpOMhO9Hf9p0xg@mail.gmail.com>
References: <CACnwZYds4vPqqdiyegRSou+PbE+fDTGQKXE-Mb+UFe9eCV0+zQ@mail.gmail.com>
 <CACnwZYeekPwyfktmNB_GtcKLAmqL=fi_pL70kpOMhO9Hf9p0xg@mail.gmail.com>
Message-ID: <55339C85.9040606@treenet.co.nz>

On 18/04/2015 12:59 a.m., Thiago Farina wrote:
> Hi,
> 
> Just curious, does people use squid together with nginx?
> 

Squid -> web proxy

NginX -> web origin server


So yes. However ... NGinX has been shown to have some very big HTTP
compliance bugs when serving up dynamic content (missing 30x response
headers, corrupt/invalid ETag values, missing Last-Modified headers,
missing compression type headers on compressed content, missing Vary
headers on negotiated content, etc). Dont expect revalidation (REFRESH)
or dynamic object storage in general to work at all well when objects
are produced by a NGinX server. Squid does what it can to fix these
flaws, but is only capable of small miracles.

Amos



From ahaitoute at rinis.nl  Sun Apr 19 09:58:34 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Sun, 19 Apr 2015 11:58:34 +0200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
	connections?
Message-ID: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>

Hello,

I?ve got the following setup, each application on its own virtual machine:

Client (sends http-requests to proxy)?> Squid (sends http-requests to apache based on destination IP and round robin to multiple apache machines) ?> Apache (setting up a two way ssl to the requested server) ?> HTTPS-server

This setup works great, and I have the Apache and the HTTPS-server its performance tuned. Both can handle 2000 concurrent connections of file sizes up to 10MB.

Unfortunately I haven?t been successful with the Squid-server. After a while I?m getting the following error messages in the log:
1429432828.200  62854 10.10.7.16 TCP_MISS_ABORTED/000 0 GET http://https.example.com/index.html - ROUNDROBIN_PARENT/192.168.0.20 -

The Squid virtual machine contains the following:
CentOS 7.1 with latest updates
Squid Cache: Version 3.3.8
CPU: Intel Xeon E312xx (Sandy Bridge) - 1799.998 MHz (4 cores)
Memory: 4096 MiB
Harddisk: 10 GiB, SCSI, raw, cache none

When I execute a performance test with 2000 concurrent connections handling a file size of 10KB on each request.
# ab -n 10000 -c 2000 -X 10.10.7.15:3128 http://https.example.com/index.html
This is ApacheBench, Version 2.3 <$Revision: 1430300 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking https.rinis.nl [through 10.10.7.15:3128] (be patient)
Completed 1000 requests
Completed 2000 requests
Completed 3000 requests
Completed 4000 requests
Completed 5000 requests
Completed 6000 requests
Completed 7000 requests
Completed 8000 requests
apr_pollset_poll: The timeout specified has expired (70007)
Total of 8610 requests completed

I have the command "vmstat 5? running on the squid server:
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 3823916    764 124992    0    0   519    26  237  503  2  3 92  3  0
 0  0      0 3823744    764 125072    0    0     0     0   44   79  0  0 100  0  0
 0  0      0 3823776    764 125044    0    0     0     2   39   70  0  0 100  0  0
 0  0      0 3729540    764 139116    0    0     1     0 2145  257  1  2 97  0  0
 0  0      0 3728432    764 139888    0    0     0    46 2297  594  1  1 97  0  0
 0  0      0 3726484    764 140892    0    0     0    39 2869  581  2  1 97  0  0
 0  0      0 3725528    764 141376    0    0     0     0 2843  648  2  2 96  0  0
 0  0      0 3724980    764 142008    0    0     0    69 2824  529  2  1 97  0  0
 0  0      0 3724584    764 142540    0    0     0     0 2742  472  2  1 97  0  0
 0  0      0 3723696    764 143004    0    0     0     0 2511  577  2  1 97  0  0
 0  0      0 3722840    764 143200    0    0     0    12  884  228  1  1 99  0  0
 0  0      0 3722704    764 142900    0    0     0     0  136  127  0  0 100  0  0
 0  0      0 3722504    764 142744    0    0     0     0   40   70  0  0 100  0  0
 0  0      0 3722456    764 142784    0    0     0   114   37   68  0  0 100  0  0
 0  0      0 3722208    764 142832    0    0     0     0   41   68  0  0 100  0  0
 0  0      0 3722480    764 142280    0    0     0     0  179   82  0  0 100  0  0
 0  0      0 3722544    764 142140    0    0     0     7   41   75  0  0 100  0  0
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 3722544    764 142136    0    0     0     0   36   67  0  0 100  0  0
 0  0      0 3722996    764 141552    0    0     0     0   42   75  0  0 100  0  0
 0  0      0 3722980    764 141568    0    0     0     0   37   68  0  0 100  0  0
 0  0      0 3723028    764 141524    0    0     0     0   36   66  0  0 100  0  0
 0  0      0 3736816    764 130352    0    0     0     0  809  114  0  0 99  0  0
 0  0      0 3737544    764 130268    0    0     0    41   42   74  0  0 100  0  0

It looks like the hardware has enough resources during the benchmark test.

I?ve got the following squid.conf running:
cache_peer 192.168.0.18 parent 3128 0 round-robin no-query no-digest
cache_peer 192.168.0.20 parent 3128 0 round-robin no-query no-digest

acl development_net dst 192.168.0.0/24
cache_peer_access 192.168.0.18 allow development_net
cache_peer_access 192.168.0.20 allow development_net

never_direct allow all
cache deny all

maximum_object_size_in_memory 16 MB
cache_mem 2048 MB

The squid must not cache at all.

Any help is welcome.

Abdelouahed
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150419/06c8f281/attachment.htm>

From hack.back at hotmail.com  Sun Apr 19 09:45:23 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Apr 2015 02:45:23 -0700 (PDT)
Subject: [squid-users] Fwd: with nginx
In-Reply-To: <CACnwZYeekPwyfktmNB_GtcKLAmqL=fi_pL70kpOMhO9Hf9p0xg@mail.gmail.com>
References: <CACnwZYds4vPqqdiyegRSou+PbE+fDTGQKXE-Mb+UFe9eCV0+zQ@mail.gmail.com>
 <CACnwZYeekPwyfktmNB_GtcKLAmqL=fi_pL70kpOMhO9Hf9p0xg@mail.gmail.com>
Message-ID: <1429436723567-4670811.post@n4.nabble.com>

Not necessary but you can use it for some reason



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/with-nginx-tp4670791p4670811.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Apr 19 14:07:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 02:07:37 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
References: <CAGUJm7ZxDgY+Yt9XZ9GZc4X1nE+dHaSuk5mz-roOFVxGn9auDw@mail.gmail.com>
 <1429087403525-4670737.post@n4.nabble.com>
 <CAGUJm7ZLmsJ0vM56A00xcCt-=e5QpJDfpO3Kk0wuMceiEuRaww@mail.gmail.com>
 <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
Message-ID: <5533B6A9.10209@treenet.co.nz>

On 18/04/2015 9:32 p.m., Nathan Hoad wrote:
> As I mentioned earlier, this hasn't worked the because the debug
> symbols you're running gdb against don't quite match the binary in
> which the crash occurred. I would check the version of the debug
> symbols you're running gdb against versus the version of the squid
> binary that was installed. These changes can include patches and
> --configure changes, not just version changes of Squid.
> 
> I'm not sure how debug symbols are "shipped" with packages in Debian -
> under CentOS for example they are shipped separately from the squid
> package, as squid-debuginfo-<version>.rpm.


aptitude install squid3 squid3-dbg

If its a self-built Squid, caveat emptor.

Though using the source .deb to build the sources of a newer version
should also provide a custom squid3-dbg package that needs to be
installed along with the custom squid3 package.

You load the debug symbols binary on debian like so:
  gdb /usr/lib/debug/usr/lib/squid3 <core-file>

Amos

> 
> On 18 April 2015 at 18:49, HackXBack wrote:
>> [New LWP 20933]
>> [New LWP 20907]
>> [New LWP 20918]
>> [New LWP 20922]
>>
>> warning: Can't read pathname for load map: Input/output error.
>> [Thread debugging using libthread_db enabled]
>> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
>> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
>> Program terminated with signal 6, Aborted.
>> #0  0x00007f51ea275165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
>> (gdb) frame 3
>> #3  0x00000000007996a3 in comm_read(RefCount<Comm::Connection> const&,
>> char*, int, RefCount<AsyncCall>&) ()
>> (gdb) print ccb->conn.p_
>> No symbol table is loaded.  Use the "file" command.
>> (gdb) print ccb->conn.p_->fd
>> No symbol table is loaded.  Use the "file" command.
>> (gdb) print conn.p_
>> No symbol table is loaded.  Use the "file" command.
>> (gdb) print conn.p_->fd
>> No symbol table is loaded.  Use the "file" command.
>> (gdb)
>>
>>
>>
>>
>> --
>> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670803.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From talwandi108 at gmail.com  Sat Apr 18 18:30:00 2015
From: talwandi108 at gmail.com (Ali Raza)
Date: Sat, 18 Apr 2015 22:30:00 +0400
Subject: [squid-users] Adding the Object Manually
Message-ID: <CAAhjccHTT6RXNKE3XKKesVei4z7LiiN4qCQQcmfW_4tO5-vf4Q@mail.gmail.com>

Hi,

  Consider I have an object say some java script file and I want to add
some header information of my own choice(like max-age, url and stuff) and
add it to squid cache.

Is this possible if yes how ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150418/5664cdd9/attachment.htm>

From wrkilu at wp.pl  Sun Apr 19 18:29:58 2015
From: wrkilu at wp.pl (Robert Lasota)
Date: Sun, 19 Apr 2015 20:29:58 +0200
Subject: [squid-users] ACLs work in a half
Message-ID: <5533f4264a2b19.78426459@wp.pl>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150419/3903f385/attachment.htm>

From yvoinov at gmail.com  Sun Apr 19 19:28:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 20 Apr 2015 01:28:25 +0600
Subject: [squid-users] ACLs work in a half
In-Reply-To: <5533f4264a2b19.78426459@wp.pl>
References: <5533f4264a2b19.78426459@wp.pl>
Message-ID: <553401D9.9030509@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


20.04.15 0:29, Robert Lasota ?????:
> Hi,
>
> I have newest Squid (3.5.3). I have ACL with blacklist:
>
> acl blacklist1 dstdomain "/opt/etc/blacklist/porn_domains"
> http_access deny blacklist1
>
> ..and they work on a half. I mean when I type in url e.g. redtube.com
- it
> blocks site. But when I type in some searcher: google or bing, and
then I go to
> the site from search results - I enter to site without problems, so
ACL  doesn't
> work.
>
> What is going on ? Please help me.

Did you see, which URL produces search engine as a result? Problem not
in ACL. Problem in your understanding, how it must work.

You using so primitive scheme to complete blocking.

As a solution - take a look at the redirectors - squidGuard or
ufdbGuard, or some similar.

For example, block page template from squidGuard uses special mechanism
to block inlined URL's with inappropriate contents:

http://i.imgur.com/PnMldpr.png



>
>
> Robert
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNAHYAAoJENNXIZxhPexGStsH/R+Mqr1CXba5u7pd+GDSm9CC
pkz+2aYxn2cr+8sCAxiMYijan9zj3KsaYxxbVnRWV2jt10jCW9qtEZ23OzCgfMnm
iKkgkB1GMwFe7uxX3xCZd/DMridltrsrwFeqgM0+Pw7PiJMRL8oJWCI73rTZ4N6F
D/jeCNgxZuSnO//wSwWf0OBc1DjM68P2m+vC89JGJDZZHF32/LC2L/PRJoRRGDPM
/C+EOs2eJq3VGN/Rb6kiqTXWuaK6s6GOxRVavQ0SqGaOuyXqvqrzXZaTJLkNbDam
ikKA84mdW1DGXzfH6AKaFMLkPTeWM8M5MyqKaMlrGskTv3KysMrSKsagvXEALXk=
=PTAk
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/1e9c08ec/attachment.htm>

From gkinkie at gmail.com  Sun Apr 19 20:00:43 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 19 Apr 2015 22:00:43 +0200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
	connections?
In-Reply-To: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
Message-ID: <CA+Y8hcPum0uCi2KL7yZnkA7_jKvpDOvYchEnN=73KgqyM1yJVg@mail.gmail.com>

What does cachemgr say? In particular what's the contents of the
"general information" and the "filedescriptor allocation" pages?

On Sun, Apr 19, 2015 at 11:58 AM, Abdelouahed Haitoute
<ahaitoute at rinis.nl> wrote:
> Hello,
>
> I've got the following setup, each application on its own virtual machine:
>
> Client (sends http-requests to proxy)--> Squid (sends http-requests to apache
> based on destination IP and round robin to multiple apache machines) -->
> Apache (setting up a two way ssl to the requested server) --> HTTPS-server
>
> This setup works great, and I have the Apache and the HTTPS-server its
> performance tuned. Both can handle 2000 concurrent connections of file sizes
> up to 10MB.
>
> Unfortunately I haven't been successful with the Squid-server. After a while
> I'm getting the following error messages in the log:
> 1429432828.200  62854 10.10.7.16 TCP_MISS_ABORTED/000 0 GET
> http://https.example.com/index.html - ROUNDROBIN_PARENT/192.168.0.20 -
>
> The Squid virtual machine contains the following:
> CentOS 7.1 with latest updates
> Squid Cache: Version 3.3.8
> CPU: Intel Xeon E312xx (Sandy Bridge) - 1799.998 MHz (4 cores)
> Memory: 4096 MiB
> Harddisk: 10 GiB, SCSI, raw, cache none
>
> When I execute a performance test with 2000 concurrent connections handling
> a file size of 10KB on each request.
> # ab -n 10000 -c 2000 -X 10.10.7.15:3128 http://https.example.com/index.html
> This is ApacheBench, Version 2.3 <$Revision: 1430300 $>
> Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
> Licensed to The Apache Software Foundation, http://www.apache.org/
>
> Benchmarking https.rinis.nl [through 10.10.7.15:3128] (be patient)
> Completed 1000 requests
> Completed 2000 requests
> Completed 3000 requests
> Completed 4000 requests
> Completed 5000 requests
> Completed 6000 requests
> Completed 7000 requests
> Completed 8000 requests
> apr_pollset_poll: The timeout specified has expired (70007)
> Total of 8610 requests completed
>
> I have the command "vmstat 5" running on the squid server:
> procs -----------memory---------- ---swap-- -----io---- -system--
> ------cpu-----
>  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id
> wa st
>  2  0      0 3823916    764 124992    0    0   519    26  237  503  2  3 92
> 3  0
>  0  0      0 3823744    764 125072    0    0     0     0   44   79  0  0 100
> 0  0
>  0  0      0 3823776    764 125044    0    0     0     2   39   70  0  0 100
> 0  0
>  0  0      0 3729540    764 139116    0    0     1     0 2145  257  1  2 97
> 0  0
>  0  0      0 3728432    764 139888    0    0     0    46 2297  594  1  1 97
> 0  0
>  0  0      0 3726484    764 140892    0    0     0    39 2869  581  2  1 97
> 0  0
>  0  0      0 3725528    764 141376    0    0     0     0 2843  648  2  2 96
> 0  0
>  0  0      0 3724980    764 142008    0    0     0    69 2824  529  2  1 97
> 0  0
>  0  0      0 3724584    764 142540    0    0     0     0 2742  472  2  1 97
> 0  0
>  0  0      0 3723696    764 143004    0    0     0     0 2511  577  2  1 97
> 0  0
>  0  0      0 3722840    764 143200    0    0     0    12  884  228  1  1 99
> 0  0
>  0  0      0 3722704    764 142900    0    0     0     0  136  127  0  0 100
> 0  0
>  0  0      0 3722504    764 142744    0    0     0     0   40   70  0  0 100
> 0  0
>  0  0      0 3722456    764 142784    0    0     0   114   37   68  0  0 100
> 0  0
>  0  0      0 3722208    764 142832    0    0     0     0   41   68  0  0 100
> 0  0
>  0  0      0 3722480    764 142280    0    0     0     0  179   82  0  0 100
> 0  0
>  0  0      0 3722544    764 142140    0    0     0     7   41   75  0  0 100
> 0  0
> procs -----------memory---------- ---swap-- -----io---- -system--
> ------cpu-----
>  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id
> wa st
>  1  0      0 3722544    764 142136    0    0     0     0   36   67  0  0 100
> 0  0
>  0  0      0 3722996    764 141552    0    0     0     0   42   75  0  0 100
> 0  0
>  0  0      0 3722980    764 141568    0    0     0     0   37   68  0  0 100
> 0  0
>  0  0      0 3723028    764 141524    0    0     0     0   36   66  0  0 100
> 0  0
>  0  0      0 3736816    764 130352    0    0     0     0  809  114  0  0 99
> 0  0
>  0  0      0 3737544    764 130268    0    0     0    41   42   74  0  0 100
> 0  0
>
> It looks like the hardware has enough resources during the benchmark test.
>
> I've got the following squid.conf running:
> cache_peer 192.168.0.18 parent 3128 0 round-robin no-query no-digest
> cache_peer 192.168.0.20 parent 3128 0 round-robin no-query no-digest
>
> acl development_net dst 192.168.0.0/24
> cache_peer_access 192.168.0.18 allow development_net
> cache_peer_access 192.168.0.20 allow development_net
>
> never_direct allow all
> cache deny all
>
> maximum_object_size_in_memory 16 MB
> cache_mem 2048 MB
>
> The squid must not cache at all.
>
> Any help is welcome.
>
> Abdelouahed
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco


From squid3 at treenet.co.nz  Mon Apr 20 00:33:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 12:33:58 +1200
Subject: [squid-users] Adding the Object Manually
In-Reply-To: <CAAhjccHTT6RXNKE3XKKesVei4z7LiiN4qCQQcmfW_4tO5-vf4Q@mail.gmail.com>
References: <CAAhjccHTT6RXNKE3XKKesVei4z7LiiN4qCQQcmfW_4tO5-vf4Q@mail.gmail.com>
Message-ID: <55344976.3020300@treenet.co.nz>

On 19/04/2015 6:30 a.m., Ali Raza wrote:
> Hi,
> 
>   Consider I have an object say some java script file and I want to add
> some header information of my own choice(like max-age, url and stuff) and
> add it to squid cache.
> 
> Is this possible if yes how ?

You do not get to make that choice. Only the owner of the content does,
they do that via the origin (web) server used to publish the contents.
Squid is an HTTP proxy - not a web server.

PS. Those details you mention are also protected and restricted by
copyright law. In particular they are part of the right to
redistribution assigned by the content owner to HTTP caches.


Amos


From squid3 at treenet.co.nz  Mon Apr 20 00:58:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Apr 2015 12:58:28 +1200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
 connections?
In-Reply-To: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
Message-ID: <55344F34.1070607@treenet.co.nz>

On 19/04/2015 9:58 p.m., Abdelouahed Haitoute wrote:
> Hello,
> 
> I?ve got the following setup, each application on its own virtual machine:
> 
> Client (sends http-requests to proxy)?> Squid (sends http-requests to apache based on destination IP and round robin to multiple apache machines) ?> Apache (setting up a two way ssl to the requested server) ?> HTTPS-server
> 
> This setup works great, and I have the Apache and the HTTPS-server its performance tuned. Both can handle 2000 concurrent connections of file sizes up to 10MB.
> 
> Unfortunately I haven?t been successful with the Squid-server. After a while I?m getting the following error messages in the log:
> 1429432828.200  62854 10.10.7.16 TCP_MISS_ABORTED/000 0 GET http://https.example.com/index.html - ROUNDROBIN_PARENT/192.168.0.20 -
> 
> The Squid virtual machine contains the following:
> CentOS 7.1 with latest updates
> Squid Cache: Version 3.3.8
> CPU: Intel Xeon E312xx (Sandy Bridge) - 1799.998 MHz (4 cores)
> Memory: 4096 MiB
> Harddisk: 10 GiB, SCSI, raw, cache none
> 
> When I execute a performance test with 2000 concurrent connections handling a file size of 10KB on each request.
> # ab -n 10000 -c 2000 -X 10.10.7.15:3128 http://https.example.com/index.html

You are wrong. "ab -c 2000" to a non-caching proxy means *4000*
concurrent connections being handled by the proxy. Web server only loads
the file object once.

A non-caching proxy requires +1 connection to server for each inbound
client connection ( 2000 + 2000 = 4K concurrent connections ).


> This is ApacheBench, Version 2.3 <$Revision: 1430300 $>
> Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
> Licensed to The Apache Software Foundation, http://www.apache.org/
> 
> Benchmarking https.rinis.nl [through 10.10.7.15:3128] (be patient)
> Completed 1000 requests
> Completed 2000 requests
> Completed 3000 requests
> Completed 4000 requests
> Completed 5000 requests
> Completed 6000 requests
> Completed 7000 requests
> Completed 8000 requests
> apr_pollset_poll: The timeout specified has expired (70007)

Squid is still responding by the client has given up. As shown by the
_ABORTED in the squid log.


> Total of 8610 requests completed
> 
> I have the command "vmstat 5? running on the squid server:
> procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
>  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
>  2  0      0 3823916    764 124992    0    0   519    26  237  503  2  3 92  3  0
>  0  0      0 3823744    764 125072    0    0     0     0   44   79  0  0 100  0  0
>  0  0      0 3823776    764 125044    0    0     0     2   39   70  0  0 100  0  0
>  0  0      0 3729540    764 139116    0    0     1     0 2145  257  1  2 97  0  0
>  0  0      0 3728432    764 139888    0    0     0    46 2297  594  1  1 97  0  0
>  0  0      0 3726484    764 140892    0    0     0    39 2869  581  2  1 97  0  0
>  0  0      0 3725528    764 141376    0    0     0     0 2843  648  2  2 96  0  0
>  0  0      0 3724980    764 142008    0    0     0    69 2824  529  2  1 97  0  0
>  0  0      0 3724584    764 142540    0    0     0     0 2742  472  2  1 97  0  0
>  0  0      0 3723696    764 143004    0    0     0     0 2511  577  2  1 97  0  0
>  0  0      0 3722840    764 143200    0    0     0    12  884  228  1  1 99  0  0
>  0  0      0 3722704    764 142900    0    0     0     0  136  127  0  0 100  0  0
>  0  0      0 3722504    764 142744    0    0     0     0   40   70  0  0 100  0  0
>  0  0      0 3722456    764 142784    0    0     0   114   37   68  0  0 100  0  0
>  0  0      0 3722208    764 142832    0    0     0     0   41   68  0  0 100  0  0
>  0  0      0 3722480    764 142280    0    0     0     0  179   82  0  0 100  0  0
>  0  0      0 3722544    764 142140    0    0     0     7   41   75  0  0 100  0  0
> procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
>  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
>  1  0      0 3722544    764 142136    0    0     0     0   36   67  0  0 100  0  0
>  0  0      0 3722996    764 141552    0    0     0     0   42   75  0  0 100  0  0
>  0  0      0 3722980    764 141568    0    0     0     0   37   68  0  0 100  0  0
>  0  0      0 3723028    764 141524    0    0     0     0   36   66  0  0 100  0  0
>  0  0      0 3736816    764 130352    0    0     0     0  809  114  0  0 99  0  0
>  0  0      0 3737544    764 130268    0    0     0    41   42   74  0  0 100  0  0
> 
> It looks like the hardware has enough resources during the benchmark test.
> 
> I?ve got the following squid.conf running:
> cache_peer 192.168.0.18 parent 3128 0 round-robin no-query no-digest
> cache_peer 192.168.0.20 parent 3128 0 round-robin no-query no-digest
> 
> acl development_net dst 192.168.0.0/24
> cache_peer_access 192.168.0.18 allow development_net
> cache_peer_access 192.168.0.20 allow development_net
> 
> never_direct allow all
> cache deny all
> 
> maximum_object_size_in_memory 16 MB
> cache_mem 2048 MB
> 
> The squid must not cache at all.

The dont bother setting cache_mem to 2GB. The memory cache wont be used.

Also note that the lack of caching is *worsening* your performance
results. When memory cache is used the FD usage is halved, and the time
to respond is greatly increased (factor of approx 100 in latency reduction).
 Consider removing the "cache deny all" when you get this into
production. The 2GB memory cache you assigned can help a *lot* for quick
short term bursts of high traffic (ie. some DoS situations).


I do not see any SMP configuration in your Squid. Meaning that its
operating all those 4K connections with a single process on a single
1.7GHz core. Thats not much processor to work with.

Try adding this to your config file:
 workers 2


Amos



From dan at getbusi.com  Mon Apr 20 01:44:13 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Sun, 19 Apr 2015 18:44:13 -0700 (PDT)
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <5533A29D.9090009@treenet.co.nz>
References: <5533A29D.9090009@treenet.co.nz>
Message-ID: <1429494253383.2bf365a4@Nodemailer>

Thanks Amos




Sorry if that wasn?t clear, but yeah, 7 KB/s was the desired speed in that test.?




I was testing against an ISO in an S3 bucket of ours. I would start the download using http:// and get 7 KB/s (great). Then cancel it and edit the URL to https:// and get ~90 KB/s.




Oh, and I almost forgot: (I can?t remember whether this has been reported but) there?s also a problem with delay_parameters, such that our software has to x2 any restriction that?s configured when generating the squid config.




So, if I want to configure a 56 kbps restriction our software actually writes:


delay_parameters 1 -1/-1 14336/14336




In order to achieve the correct limit (7 KB/s). Not sure if that could be related to this at all.




P.S. We?re pretty fond of delay_pools and their simplicity?including the fact that it can all be handled by Squid without any lower-level networking concerns.

On Mon, Apr 20, 2015 at 1:25 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 17/04/2015 1:30 p.m., djch wrote:
>> I just wanted to revive this thread to note that:
>> 
>> - Delay pools apply just fine to HTTPS requests in Squid 2.7.
>> - Delay pools in Squid 3.4.x are also applied to HTTPS but the speed is not
>> correct. 
>>   - If I apply a 56 Kbps limit the HTTP download tops out at ~7 KB/s.
> That *is* correct.
>    56K *bits* /sec == 7K *Bytes* /sec.
>    7x8 = 56
>> If I
>> download the same file from the same server via HTTPS it tops out at
>> ~90KB/s. If I download the same file over HTTPS with no delay pools
>> configured it tops at around 3MB/s.
> Are you sure thats actually HTTPS ?
> CONNECT tunnels these days could contain HTTP/2, SPDY, WebSockets or
> data in some other (compressed?) format. Any of which achieve faster
> percieved "download" than HTTP using the same basic bytes/sec data rate.
> NP: squid-2.7 contains a CONNECT bug that prevents SPDY HTTP/2 and
> Websockets working properly.
>> 
>> So I guess that would make this a bug? Which I assume nobody wants to fix
>> 'cause they're going to be deprecated at some point soon?
>> 
> Its a matter of interest. With FOSS you have to cause someone to be
> interested in fixing the bug. Best way to do that is to fix it yourself
> and present a patch and get it through review to merge. Second best is
> to find someone to pay to do all that. Or you can join the many people
> who opted just to wait and pray someone else will give it to them free
> one day.
> There is another option with delay pools. The pooling system is *just* a
> packet rate limiting system. The OS these days have several such QoS
> systems built in that work far better than Squid algorithms (admittedly
> not on a HTTP per-message basis). If you want total traffic control give
> those a try.
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150419/76799cca/attachment.htm>

From jagannath.naidu at fosteringlinux.com  Mon Apr 20 07:31:21 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Mon, 20 Apr 2015 13:01:21 +0530
Subject: [squid-users] [squid ] externalAclLookup: 'wbinfo_group_helper'
	queue overload.
Message-ID: <CA+8bHvw=Ajojam1CMOim-PZNehGxE-puEtdf7NveGc+wc==6PQ@mail.gmail.com>

Hi,

I am having this issue very frequently. Please help on this.

I get these errors randomly, mostly when usage is at very peak. (800 users)


/var/log/squid/cache.log

2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2ce518)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2cf038)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99ada7ce8)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e247b28)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e247b28)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99c25a578)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2daae8)
got NTLMSSP command 3, expected 1
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2eb108)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2f0798)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2f3c58)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e2ff3a8)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99adbba48)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99adbba48)
2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
overload (ch=0x7fc99e323d18)

Then squid stops working. For squid to start work again, I have to dlete
the cache and restart the squid "squid -k reconfigure", and then squid
restart.

squid.conf

max_filedesc 17192
acl manager proto cache_object
acl localhost src 172.16.50.61/24
http_access allow manager localhost
dns_nameservers 172.16.3.34 10.1.2.91
acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
http_access allow allowips
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
acl localnet src 172.16.0.0/24
acl localnet src fc00::/7 # RFC 4193 local private network range
acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
auth_param ntlm children 600
auth_param ntlm keep_alive off
auth_param negotiate children 150
auth_param negotiate keep_alive off
visible_hostname GGNPROXY01.HTMEDIA.NET
external_acl_type wbinfo_group_helper ttl=0 children=40 %LOGIN
/usr/lib64/squid/wbinfo_group.pl -d
auth_param negotiate keep_alive off
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
acl Safe_ports port 8080 #https
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl auth proxy_auth REQUIRED
acl google dstdomain -i "/etc/squid/google_site.com"
http_access allow google
acl sq1 external wbinfo_group_helper "/etc/squid/HT/sq1"
acl sq2 external wbinfo_group_helper "/etc/squid/HT/sq2"
acl sq3 external wbinfo_group_helper "/etc/squid/HT/sq3"
acl sq4 external wbinfo_group_helper "/etc/squid/HT/sq4"
acl sq5 external wbinfo_group_helper "/etc/squid/HT/sq5"
acl pro1 external wbinfo_group_helper "/etc/squid/HT/pro1"
acl pro2 external wbinfo_group_helper "/etc/squid/HT/pro2"
acl pro3 external wbinfo_group_helper "/etc/squid/HT/pro3"
acl pro4 external wbinfo_group_helper "/etc/squid/HT/pro4"
acl pro5 external wbinfo_group_helper "/etc/squid/HT/pro5"
acl pro6 external wbinfo_group_helper "/etc/squid/HT/pro6"
acl webvip external wbinfo_group_helper "/etc/squid/HT/webvip"
acl allgroup external wbinfo_group_helper "/etc/squid/HT/allgreop"
acl restricted external wbinfo_group_helper "/etc/squid/HT/restricted"
acl ad_auth proxy_auth REQUIRE
acl allowwebsites dstdomain -i "/blacklists/allowedwebsite/domains"
acl allowwebsites_url url_regex -i "/blacklists/allowedwebsite/url"
http_access allow allowwebsites
http_access allow allowwebsites_url
acl shopping dstdomain -i "/etc/squid/shopping.txt"
acl social_networking dstdomain -i "/blacklists/social/social.networking"
acl youtube dstdomain -i .youtube.com
http_access allow Safe_ports pro1 pro2 pro3 pro4 pro5 pro6 webvip
http_access allow youtube pro5
http_access allow youtube pro6
http_access allow youtube webvip
http_access deny youtube
http_access allow shopping pro5
http_access allow shopping pro6
http_access allow shopping webvip
http_access deny shopping
http_access allow social_networking pro2
http_access allow social_networking pro4
http_access allow social_networking pro6
http_access allow social_networking webvip
http_access deny social_networking
acl porn_site1   dstdomain "/etc/squid/blacklists/porn/domains.txt"
acl porn_site2   dstdom_regex -i "/etc/squid/blacklists/porn/expressions"
acl porn_site3   dstdom_regex -i "/etc/squid/blacklists/porn/urls.txt"
acl audio_video1   dstdomain "/etc/squid/blacklists/audio-video/urls.txt"
###################### THERE ARE TOO MANY acls and http_access , so not
bothering with vast linux
http_access allow liquorinfo webvip
http_access deny liquorinfo
http_access allow ad_auth
http_access allow auth
http_access allow sq1 sq2
acl NTLMUsers proxy_auth REQUIRED
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_port 8080
hierarchy_stoplist cgi-bin ?
cache_effective_user squid
cache_dir aufs /var/spool/squid 20384 32 512
cache_mem 50 MB
cache_replacement_policy heap LFUDA
cache_swap_low 85
cache_swap_high 95
maximum_object_size 5 MB
maximum_object_size_in_memory 50 KB
ipcache_size 5240
ipcache_low 90
ipcache_high 95
cache_mgr amit
cachemgr_passwd keenable at 123
acl SSL_ports port 443
http_access allow CONNECT SSL_ports
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
url_rewrite_program /usr/local/bin/squidGuard -c
/usr/local/squidGuard/squidGuard.conf



in /var/log/messages,  I get the following errors

pr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.329841,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.333903,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.334474,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.339318,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.339710,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.351705,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.352065,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.358560,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
connections, no idle connection found
Apr 20 12:59:15 GGNPROXY01 winbindd[1910]: [2015/04/20 12:59:15.358913,  0]
winbindd/winbindd.c:975(winbindd_listen_fde_handler)


/var/log/squid/cache.log


2015/04/20 12:59:50| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/04/20 12:59:50|     /var/spool/squid/00/91/000122E6
2015/04/20 12:59:50| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/04/20 12:59:50|     /var/spool/squid/00/C6/00018D9D
2015/04/20 12:59:50|     /var/spool/squid/00/C6/00018D9D
2015/04/20 12:59:50| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/04/20 12:59:50|     /var/spool/squid/00/F7/0001EE64
2015/04/20 12:59:50|     /var/spool/squid/00/F7/0001EE64
2015/04/20 12:59:50| storeSwapOutFileClosed: dirno 0, swapfile 0001EE64,
errflag=FFFFFFFF
2015/04/20 12:59:50| storeSwapOutFileClosed: dirno 0, swapfile 0001EE64,
errflag=FFFFFFFF
2015/04/20 12:59:50|     (2) No such file or directory
2015/04/20 12:59:50| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/04/20 12:59:50|     /var/spool/squid/00/F7/0001EE65
2015/04/20 12:59:50|     /var/spool/squid/00/F7/0001EE65
2015/04/20 12:59:50| storeSwapOutFileClosed: dirno 0, swapfile 0001EE65,
errflag=FFFFFFFF
2015/04/20 12:59:50|     (2) No such file or directory
2015/04/20 12:59:50| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/04/20 12:59:50|     /var/spool/squid/00/F7/0001EE66
2015/04/20 12:59:50| storeSwapOutFileClosed: dirno 0, swapfile 0001EE66,
errflag=FFFFFFFF
2015/04/20 12:59:50|     (2) No such file or directory


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/d833c500/attachment.htm>

From wrkilu at wp.pl  Mon Apr 20 08:01:54 2015
From: wrkilu at wp.pl (Robert Lasota)
Date: Mon, 20 Apr 2015 10:01:54 +0200
Subject: [squid-users] Odp: Re:  ACLs work in a half
Message-ID: <5534b2729250e7.23220392@wp.pl>

Dnia Niedziela, 19 Kwietnia 2015 21:28 Yuri Voinov  napisa?(a) 
>               
>     -----BEGIN PGP SIGNED MESSAGE----- 
>     Hash: SHA256 
>     ?
>     
>     
>     20.04.15 0:29, Robert Lasota &#x43F;&#x438;&#x448;&#x435;&#x442;:
>     > Hi,
>       >
>       > I have newest Squid (3.5.3). I have ACL with blacklist:
>       >
>       > acl blacklist1 dstdomain "/opt/etc/blacklist/porn_domains"
>       > http_access deny blacklist1
>       >
>       > ..and they work on a half. I mean when I type in url e.g.      redtube.com - it 
>       > blocks site. But when I type in some searcher: google or      bing, and then I go to 
>       > the site from search results - I enter to site without      problems, so ACL? doesn't 
>       > work.
>       >
>       > What is going on ? Please help me.
>     
>     Did you see, which URL produces search engine as a result? Problem    not in ACL. Problem in your understanding, how it must work.
>     
>     You using so primitive scheme to complete blocking.
>     
>     As a solution - take a look at the redirectors - squidGuard or    ufdbGuard, or some similar.
>     
>     For example, block page template from squidGuard uses special    mechanism to block inlined URL's with inappropriate contents:
>     
>     http://i.imgur.com/PnMldpr.png
>     
>     

You've wrote " You using so primitive scheme to complete blocking." so is it better way to achieve my goal or only use additional redirector like Squidguard ?

Robert




From wrkilu at wp.pl  Mon Apr 20 09:16:47 2015
From: wrkilu at wp.pl (Robert Lasota)
Date: Mon, 20 Apr 2015 11:16:47 +0200
Subject: [squid-users] Odp: RE: Odp: Re:  ACLs work in a half
Message-ID: <5534c3ffe2c722.80210892@wp.pl>

Dnia Poniedzia?ek, 20 Kwietnia 2015 10:45 <yvoinov at gmail.com> napisa?(a)
> Yep. As I said.
>  

Ok, thanks

Robert

> &#x438;&#x441;&#x445;&#x43E;&#x434;&#x43D;&#x43E;&#x435; &#x441;&#x431;&#x449;
> &#x422;&#x435;&#x43C;&#x430;: Odp: Re: [squid-users] ACLs work in a half
> &#x41E;&#x442;: "Robert Lasota" <wrkilu at wp.pl>
> &#x414;&#x430;&#x442;&#x430;: 20-04-2015 14:01
>  
> Dnia Niedziela, 19 Kwietnia 2015 21:28 Yuri Voinov  napisa?(a)
> >              
> >     -----BEGIN PGP SIGNED MESSAGE-----
> >     Hash: SHA256
> >     
> >    
> >    
> >     20.04.15 0:29, Robert Lasota &#x43F;&#x438;&#x448;&#x435;&#x442;:
> >     > Hi,
> >       >
> >       > I have newest Squid (3.5.3). I have ACL with blacklist:
> >       >
> >       > acl blacklist1 dstdomain "/opt/etc/blacklist/porn_domains"
> >       > http_access deny blacklist1
> >       >
> >       > ..and they work on a half. I mean when I type in url e.g.      redtube.com - it
> >       > blocks site. But when I type in some searcher: google or      bing, and then I go to
> >       > the site from search results - I enter to site without      problems, so ACL  doesn't
> >       > work.
> >       >
> >       > What is going on ? Please help me.
> >    
> >     Did you see, which URL produces search engine as a result? Problem    not in ACL. Problem in your understanding, how it must work.
> >    
> >     You using so primitive scheme to complete blocking.
> >    
> >     As a solution - take a look at the redirectors - squidGuard or    ufdbGuard, or some similar.
> >    
> >     For example, block page template from squidGuard uses special    mechanism to block inlined URL's with inappropriate contents:
> >    
> >     http://i.imgur.com/PnMldpr.png
> >    
> >    
>  
> You've wrote " You using so primitive scheme to complete blocking." so is it better way to achieve my goal or only use additional redirector like Squidguard ?
>  
> Robert


 





From ahmed.zaeem at netstream.ps  Mon Apr 20 23:16:39 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Mon, 20 Apr 2015 16:16:39 -0700
Subject: [squid-users] squid HTTPs as reverse proxy problem
Message-ID: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>

Hi all , I need a help in setting up squid for https reverse proxy 

I mean I want to  authorize the certificate on my pc so that be able to
acces https using http not tunnel method

I have searched a lot and most of docs mention ssl pump , but again im here
don't want ssl pump feature and all I need is just reverse proxy.

 

Here is steps that I did :

cd /etc/squid

 

openssl req -new -newkey rsa:1024 -days 3650 -nodes -x509 -subj
'/C=dsa/ST=asd/L=aaa/O=abcv/CN=abc' -keyout /etc/squid/abc.pem -out 

/etc/squid/abc.pem

 

openssl x509 -in /etc/squid/abc.pem -outform DER -out /etc/squid/abc.der

 

whereis ssl_crtd

 

chown squid:squid /var/lib/ssl_db

 

after that  edited squid.conf with :

 

https_port 443 cert=/etc/squid/abc.pem key=/etc/squid/abc.pem

 

 

 

then went to my browser and added abc.der as authorized certificates

 

when I connect to proxy I have erros logs :

 

2015/04/20 15:44:18 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:44:19 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:44:21 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:44:23 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:45:33 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:45:33 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:47:01 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:53:44 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:53:46 kid1| Error negotiating SSL connection on FD 11: Success
(0)

2015/04/20 15:53:47 kid1| Error negotiating SSL connection on FD 11: Success
(0)

 

 

Where could be the problem ?

 

 

Here is my squid config :

 

 

squid -v

Squid Cache: Version 3.5.1

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=drx' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-esi'
'--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd'
'--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000' '--with-openssl'
'--enable-snmp'

 

 

 

 

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/17180743/attachment.htm>

From yvoinov at gmail.com  Mon Apr 20 13:21:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 20 Apr 2015 19:21:46 +0600
Subject: [squid-users] squid HTTPs as reverse proxy problem
In-Reply-To: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>
References: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>
Message-ID: <5534FD6A.1020200@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Man,

self-signed sertificate required only for SSL Bump (not pump :)).

For SSL reverse proxy you need CA's signed server certificate.

Feel the difference.

21.04.15 5:16, snakeeyes ?????:
> Hi all , I need a help in setting up squid for https reverse proxy
>
> I mean I want to  authorize the certificate on my pc so that be able to
> acces https using http not tunnel method
>
> I have searched a lot and most of docs mention ssl pump , but again im
here
> don't want ssl pump feature and all I need is just reverse proxy.
>
> 
>
> Here is steps that I did :
>
> cd /etc/squid
>
> 
>
> openssl req -new -newkey rsa:1024 -days 3650 -nodes -x509 -subj
> '/C=dsa/ST=asd/L=aaa/O=abcv/CN=abc' -keyout /etc/squid/abc.pem -out
>
> /etc/squid/abc.pem
>
> 
>
> openssl x509 -in /etc/squid/abc.pem -outform DER -out /etc/squid/abc.der
>
> 
>
> whereis ssl_crtd
>
> 
>
> chown squid:squid /var/lib/ssl_db
>
> 
>
> after that  edited squid.conf with :
>
> 
>
> https_port 443 cert=/etc/squid/abc.pem key=/etc/squid/abc.pem
>
> 
>
> 
>
> 
>
> then went to my browser and added abc.der as authorized certificates
>
> 
>
> when I connect to proxy I have erros logs :
>
> 
>
> 2015/04/20 15:44:18 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:44:19 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:44:21 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:44:23 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:47:01 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:53:44 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:53:46 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 2015/04/20 15:53:47 kid1| Error negotiating SSL connection on FD 11:
Success
> (0)
>
> 
>
> 
>
> Where could be the problem ?
>
> 
>
> 
>
> Here is my squid config :
>
> 
>
> 
>
> squid -v
>
> Squid Cache: Version 3.5.1
>
> Service Name: squid
>
> configure options:  '--prefix=/usr' '--includedir=/include'
> '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
> '--enable-cachemgr-hostname=drx' '--localstatedir=/var'
> '--libexecdir=/lib/squid' '--disable-maintainer-mode'
> '--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap'
> '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
> '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
>
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
> ,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
> '--enable-digest-auth-helpers=ldap,password'
> '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-esi'
> '--disable-translation' '--with-logdir=/var/log/squid'
> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
> '--with-large-files' '--with-default-user=squid'
'--enable-linux-netfilter'
> '--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd'
> '--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000' '--with-openssl'
> '--enable-snmp'
>
> 
>
> 
>
> 
>
> 
>
> 
>
> cheers
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNP1qAAoJENNXIZxhPexGA7QIAKGDJIOUiKxo0iemYhT2b+dz
YEVjuOMcjOu643MzUpFNJEezD0spQrGk01Lrj9DLJrlTv6fH5CWEAJJcsy/ieyAV
KN/SVxS6v98N5KitIhNGbeSO3OKMASJVvgaSi/MpTEl2snRUNaSSiJDKvu9oJqje
fo19qw+Ce4tH1QjnvRX+v1IHYlBcqBroGnQAR/kNnW1QdC0kXWy2X/hv0eJ5Lmyd
kSLtiSaOVl6qJ64S1UuQWL9mW8phPI/mYJBOZ3AGe535VO+15pXsFrsxfeIIF8ra
DmV6cEKEtMVDikI8n9DvlRvJV/vFMmrtI2vqWgXE6HEjmr1WNiYDqkQVczYXeQk=
=Pb8X
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/91e60a03/attachment.htm>

From Daniel.Berry at avispl.com  Mon Apr 20 14:34:50 2015
From: Daniel.Berry at avispl.com (Dan Berry)
Date: Mon, 20 Apr 2015 14:34:50 +0000
Subject: [squid-users] Tracking user connection times
Message-ID: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>


I have setup a squid proxy as a POC for user tracking. I am looking for a way to track for close events, most of the customer sites that are accessed are HTTPS so I can't track activity. I might be able to get by with tracking total connect time, so I know the windows of time users were connected to a specific site. Is this possible?

Dan Berry
Data Network Engineer

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/ec951c39/attachment.htm>

From acrow at integrafin.co.uk  Mon Apr 20 14:45:40 2015
From: acrow at integrafin.co.uk (Alex Crow)
Date: Mon, 20 Apr 2015 15:45:40 +0100
Subject: [squid-users] Tracking user connection times
In-Reply-To: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
References: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
Message-ID: <55351114.7030306@integrafin.co.uk>

On 20/04/15 15:34, Dan Berry wrote:
>
> I have setup a squid proxy as a POC for user tracking. I am looking 
> for a way to track for close events, most of the customer sites that 
> are accessed are HTTPS so I can?t track activity. I might be able to 
> get by with tracking total connect time, so I know the windows of time 
> users were connected to a specific site. Is this possible?
>
> Dan Berry
>
> Data Network Engineer
>
>

I doubt it unless you are in control of the sites the users are 
visiting. When a page is loaded, the browser instructs the OS to open a 
TCP connection to download the page. When all the data has been 
downloaded, the TCP connection becomes idle and after a short time the 
OS will close it.

If the sites are yours I suppose you could add some JS that would get 
the browser to repeatedly make a request to the site with a 
page-specific ID so you could track how long they were on that page.

Cheers

Alex

-- 
This message is intended only for the addressee and may contain 
confidential information. Unless you are that person, you may not 
disclose its contents or use it in any way and are requested to delete 
the message along with any attachments and notify us immediately. 
"Transact" is operated by Integrated Financial Arrangements plc. 29 
Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 
5300. (Registered office: as above; Registered in England and Wales 
under number: 3727592). Authorised and regulated by the Financial 
Conduct Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/034112cd/attachment.htm>

From yvoinov at gmail.com  Mon Apr 20 14:48:11 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 20 Apr 2015 20:48:11 +0600
Subject: [squid-users] Tracking user connection times
In-Reply-To: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
References: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
Message-ID: <553511AB.3030609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://sams.perm.ru/new/index.php?option=com_phocadownload&view=category&id=3&Itemid=128

This one?

20.04.15 20:34, Dan Berry ?????:
>
> I have setup a squid proxy as a POC for user tracking. I am looking
for a way to track for close events, most of the customer sites that are
accessed are HTTPS so I can't track activity. I might be able to get by
with tracking total connect time, so I know the windows of time users
were connected to a specific site. Is this possible?
>
> Dan Berry
> Data Network Engineer
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNRGqAAoJENNXIZxhPexGuTYH+wf0m1fJDUc/kk2a62zPnVqG
7Ouah5D2oEsyRwF6hnTaFbamgcfzyj0gBEi305fsygwLkAUKW9Aq8naS/2fYWcBg
Yv9D+vIK4crDX6HoGfXgJlmWQQfJcWtDX/ZgbNNDUe5RpLOZ1/fGlFv1EAQxyK18
dypTPcytF/zq31LfBKeDjrn34MV3aTf4MV5pjGnsXZ+Zje+c8X3+yYw4DVUcOPxm
l0g+0ThWzGYi+BVmJN9MamGwKOh/0DiM0ZrFquamiblUBD16wF4QYK8oQuAm1WKG
qO0de7dJx55gwphBncolh2FCxCgA6wuTgZr4+7Wqtp/fGI22Gxv0C/Lzdf/vmMY=
=ZeKH
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/4a7b67b6/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Apr 21 01:17:11 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Mon, 20 Apr 2015 18:17:11 -0700
Subject: [squid-users] squid HTTPs as reverse proxy problem
In-Reply-To: <5534FD6A.1020200@gmail.com>
References: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>
 <5534FD6A.1020200@gmail.com>
Message-ID: <003c01d07bd0$e53627d0$afa27770$@netstream.ps>

Thankx , I will tell u wt I did so far abd hope u help me in the directive squid needed :

Mkdir /etc/openvpn/
wget https://github.com/OpenVPN/easy-rsa-old/archive/master.zip

unzip master

cd easy-rsa-old-master/

 

cp -R easy-rsa/ /etc/openvpn/

 

cd /etc/openvpn/easy-rsa/2.0

chmod 755 *

source ./vars

./vars

./clean-all

 

./build-ca

 

./build-key-server server

 

./build-dh

 

Now I have the files :

[root at squid keys]# ls -l

total 76

-rw-r--r-- 1 root root 4120 Apr 20 17:51 01.pem

-rw-r--r-- 1 root root 4006 Apr 20 17:52 02.pem

-rw-r--r-- 1 root root 1383 Apr 20 17:51 ca.crt

-rw------- 1 root root  912 Apr 20 17:51 ca.key

-rw-r--r-- 1 root root  245 Apr 20 17:51 dh1024.pem

-rw-r--r-- 1 root root  276 Apr 20 17:52 index.txt

-rw-r--r-- 1 root root   21 Apr 20 17:52 index.txt.attr

-rw-r--r-- 1 root root   21 Apr 20 17:51 index.txt.attr.old

-rw-r--r-- 1 root root  136 Apr 20 17:51 index.txt.old

-rw-r--r-- 1 root root    3 Apr 20 17:52 serial

-rw-r--r-- 1 root root    3 Apr 20 17:51 serial.old

-rw-r--r-- 1 root root 4120 Apr 20 17:51 server.crt

-rw-r--r-- 1 root root  729 Apr 20 17:51 server.csr

-rw------- 1 root root  920 Apr 20 17:51 server.key

 

 

 

 

What do I need for squid directive ?

 

Is what I did above is okay ?

 

 

cheers

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Monday, April 20, 2015 6:22 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid HTTPs as reverse proxy problem

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
Man,

self-signed sertificate required only for SSL Bump (not pump :)).

For SSL reverse proxy you need CA's signed server certificate.

Feel the difference.

21.04.15 5:16, snakeeyes ?????:
> Hi all , I need a help in

      setting up squid for https reverse proxy 



      >



      > I mean I want to  authorize the certificate on my pc so that

      be able to



      > acces https using http not tunnel method



      >



      > I have searched a lot and most of docs mention ssl pump , but

      again im here



      > don't want ssl pump feature and all I need is just reverse

      proxy.



      >



      >  



      >



      > Here is steps that I did :



      >



      > cd /etc/squid



      >



      >  



      >



      > openssl req -new -newkey rsa:1024 -days 3650 -nodes -x509

      -subj



      > '/C=dsa/ST=asd/L=aaa/O=abcv/CN=abc' -keyout

      /etc/squid/abc.pem -out 



      >



      > /etc/squid/abc.pem



      >



      >  



      >



      > openssl x509 -in /etc/squid/abc.pem -outform DER -out

      /etc/squid/abc.der



      >



      >  



      >



      > whereis ssl_crtd



      >



      >  



      >



      > chown squid:squid /var/lib/ssl_db



      >



      >  



      >



      > after that  edited squid.conf with :



      >



      >  



      >



      > https_port 443 cert=/etc/squid/abc.pem key=/etc/squid/abc.pem



      >



      >  



      >



      >  



      >



      >  



      >



      > then went to my browser and added abc.der as authorized

      certificates



      >



      >  



      >



      > when I connect to proxy I have erros logs :



      >



      >  



      >



      > 2015/04/20 15:44:18 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:44:19 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:44:21 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:44:23 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:47:01 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:53:44 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:53:46 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      > 2015/04/20 15:53:47 kid1| Error negotiating SSL connection on

      FD 11: Success



      > (0)



      >



      >  



      >



      >  



      >



      > Where could be the problem ?



      >



      >  



      >



      >  



      >



      > Here is my squid config :



      >



      >  



      >



      >  



      >



      > squid -v



      >



      > Squid Cache: Version 3.5.1



      >



      > Service Name: squid



      >



      > configure options:  '--prefix=/usr' '--includedir=/include'



      > '--mandir=/share/man' '--infodir=/share/info'

      '--sysconfdir=/etc'



      > '--enable-cachemgr-hostname=drx' '--localstatedir=/var'



      > '--libexecdir=/lib/squid' '--disable-maintainer-mode'



      > '--disable-dependency-tracking' '--disable-silent-rules'

      '--srcdir=.'



      > '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'



      > '--mandir=/usr/share/man' '--enable-inline'

      '--enable-async-io=8'



      > '--enable-storeio=ufs,aufs,diskd,rock'

      '--enable-removal-policies=lru,heap'



      > '--enable-delay-pools' '--enable-cache-digests'

      '--enable-underscores'



      > '--enable-icap-client' '--enable-follow-x-forwarded-for'

      '--enable-auth'



      >

'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam



      > ,squid_radius_auth,multi-domain-NTLM'

      '--enable-ntlm-auth-helpers=smb_lm'



      > '--enable-digest-auth-helpers=ldap,password'



      > '--enable-negotiate-auth-helpers=squid_kerb_auth'

      '--enable-esi'



      > '--disable-translation' '--with-logdir=/var/log/squid'



      > '--with-pidfile=/var/run/squid.pid'

      '--with-filedescriptors=131072'



      > '--with-large-files' '--with-default-user=squid'

      '--enable-linux-netfilter'



      > '--enable-ltdl-convenience' '--enable-ssl'

      '--enable-ssl-crtd'



      > '--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000'

      '--with-openssl'



      > '--enable-snmp'



      >



      >  



      >



      >  



      >



      >  



      >



      >  



      >



      >  



      >



      > cheers



      >



      >



      >



      >



      > _______________________________________________



      > squid-users mailing list



      > squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJVNP1qAAoJENNXIZxhPexGA7QIAKGDJIOUiKxo0iemYhT2b+dz 
YEVjuOMcjOu643MzUpFNJEezD0spQrGk01Lrj9DLJrlTv6fH5CWEAJJcsy/ieyAV 
KN/SVxS6v98N5KitIhNGbeSO3OKMASJVvgaSi/MpTEl2snRUNaSSiJDKvu9oJqje 
fo19qw+Ce4tH1QjnvRX+v1IHYlBcqBroGnQAR/kNnW1QdC0kXWy2X/hv0eJ5Lmyd 
kSLtiSaOVl6qJ64S1UuQWL9mW8phPI/mYJBOZ3AGe535VO+15pXsFrsxfeIIF8ra 
DmV6cEKEtMVDikI8n9DvlRvJV/vFMmrtI2vqWgXE6HEjmr1WNiYDqkQVczYXeQk= 
=Pb8X 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/f2b7321a/attachment.htm>

From yvoinov at gmail.com  Mon Apr 20 15:21:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 20 Apr 2015 21:21:23 +0600
Subject: [squid-users] squid HTTPs as reverse proxy problem
In-Reply-To: <003c01d07bd0$e53627d0$afa27770$@netstream.ps>
References: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>
 <5534FD6A.1020200@gmail.com> <003c01d07bd0$e53627d0$afa27770$@netstream.ps>
Message-ID: <55351973.5000601@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
What does OpenVPN to SQUID ?!

21.04.15 7:17, snakeeyes ?????:
> Thankx , I will tell u wt I did so far abd hope u help me in the directive squid needed :
>
> Mkdir /etc/openvpn/
> wget https://github.com/OpenVPN/easy-rsa-old/archive/master.zip
>
> unzip master
>
> cd easy-rsa-old-master/
>
> 
>
> cp -R easy-rsa/ /etc/openvpn/
>
> 
>
> cd /etc/openvpn/easy-rsa/2.0
>
> chmod 755 *
>
> source ./vars
>
> ./vars
>
> ./clean-all
>
> 
>
> ./build-ca
>
> 
>
> ./build-key-server server
>
> 
>
> ./build-dh
>
> 
>
> Now I have the files :
>
> [root at squid keys]# ls -l
>
> total 76
>
> -rw-r--r-- 1 root root 4120 Apr 20 17:51 01.pem
>
> -rw-r--r-- 1 root root 4006 Apr 20 17:52 02.pem
>
> -rw-r--r-- 1 root root 1383 Apr 20 17:51 ca.crt
>
> -rw------- 1 root root  912 Apr 20 17:51 ca.key
>
> -rw-r--r-- 1 root root  245 Apr 20 17:51 dh1024.pem
>
> -rw-r--r-- 1 root root  276 Apr 20 17:52 index.txt
>
> -rw-r--r-- 1 root root   21 Apr 20 17:52 index.txt.attr
>
> -rw-r--r-- 1 root root   21 Apr 20 17:51 index.txt.attr.old
>
> -rw-r--r-- 1 root root  136 Apr 20 17:51 index.txt.old
>
> -rw-r--r-- 1 root root    3 Apr 20 17:52 serial
>
> -rw-r--r-- 1 root root    3 Apr 20 17:51 serial.old
>
> -rw-r--r-- 1 root root 4120 Apr 20 17:51 server.crt
>
> -rw-r--r-- 1 root root  729 Apr 20 17:51 server.csr
>
> -rw------- 1 root root  920 Apr 20 17:51 server.key
>
> 
>
> 
>
> 
>
> 
>
> What do I need for squid directive ?
>
> 
>
> Is what I did above is okay ?
>
> 
>
> 
>
> cheers
>
> 
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Monday, April 20, 2015 6:22 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid HTTPs as reverse proxy problem
>
> 
>
>
> Man,
>
> self-signed sertificate required only for SSL Bump (not pump :)).
>
> For SSL reverse proxy you need CA's signed server certificate.
>
> Feel the difference.
>
> 21.04.15 5:16, snakeeyes ?????:
> > Hi all , I need a help in
>
>       setting up squid for https reverse proxy
>
>
>
>
>
>
>
>       > I mean I want to  authorize the certificate on my pc so that
>
>       be able to
>
>
>
>       > acces https using http not tunnel method
>
>
>
>
>
>
>
>       > I have searched a lot and most of docs mention ssl pump , but
>
>       again im here
>
>
>
>       > don't want ssl pump feature and all I need is just reverse
>
>       proxy.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Here is steps that I did :
>
>
>
>
>
>
>
>       > cd /etc/squid
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > openssl req -new -newkey rsa:1024 -days 3650 -nodes -x509
>
>       -subj
>
>
>
>       > '/C=dsa/ST=asd/L=aaa/O=abcv/CN=abc' -keyout
>
>       /etc/squid/abc.pem -out
>
>
>
>
>
>
>
>       > /etc/squid/abc.pem
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > openssl x509 -in /etc/squid/abc.pem -outform DER -out
>
>       /etc/squid/abc.der
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > whereis ssl_crtd
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > chown squid:squid /var/lib/ssl_db
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > after that  edited squid.conf with :
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > https_port 443 cert=/etc/squid/abc.pem key=/etc/squid/abc.pem
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > then went to my browser and added abc.der as authorized
>
>       certificates
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > when I connect to proxy I have erros logs :
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > 2015/04/20 15:44:18 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:44:19 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:44:21 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:44:23 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:45:33 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:47:01 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:53:44 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:53:46 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>       > 2015/04/20 15:53:47 kid1| Error negotiating SSL connection on
>
>       FD 11: Success
>
>
>
>       > (0)
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Where could be the problem ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Here is my squid config :
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > squid -v
>
>
>
>
>
>
>
>       > Squid Cache: Version 3.5.1
>
>
>
>
>
>
>
>       > Service Name: squid
>
>
>
>
>
>
>
>       > configure options:  '--prefix=/usr' '--includedir=/include'
>
>
>
>       > '--mandir=/share/man' '--infodir=/share/info'
>
>       '--sysconfdir=/etc'
>
>
>
>       > '--enable-cachemgr-hostname=drx' '--localstatedir=/var'
>
>
>
>       > '--libexecdir=/lib/squid' '--disable-maintainer-mode'
>
>
>
>       > '--disable-dependency-tracking' '--disable-silent-rules'
>
>       '--srcdir=.'
>
>
>
>       > '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
>
>
>
>       > '--mandir=/usr/share/man' '--enable-inline'
>
>       '--enable-async-io=8'
>
>
>
>       > '--enable-storeio=ufs,aufs,diskd,rock'
>
>       '--enable-removal-policies=lru,heap'
>
>
>
>       > '--enable-delay-pools' '--enable-cache-digests'
>
>       '--enable-underscores'
>
>
>
>       > '--enable-icap-client' '--enable-follow-x-forwarded-for'
>
>       '--enable-auth'
>
>
>
>
>
>
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
>
>
>
>       > ,squid_radius_auth,multi-domain-NTLM'
>
>       '--enable-ntlm-auth-helpers=smb_lm'
>
>
>
>       > '--enable-digest-auth-helpers=ldap,password'
>
>
>
>       > '--enable-negotiate-auth-helpers=squid_kerb_auth'
>
>       '--enable-esi'
>
>
>
>       > '--disable-translation' '--with-logdir=/var/log/squid'
>
>
>
>       > '--with-pidfile=/var/run/squid.pid'
>
>       '--with-filedescriptors=131072'
>
>
>
>       > '--with-large-files' '--with-default-user=squid'
>
>       '--enable-linux-netfilter'
>
>
>
>       > '--enable-ltdl-convenience' '--enable-ssl'
>
>       '--enable-ssl-crtd'
>
>
>
>       > '--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000'
>
>       '--with-openssl'
>
>
>
>       > '--enable-snmp'
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > cheers
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNRlzAAoJENNXIZxhPexG4/QIAJUl79LIoLOVaFJk2mygK+fd
IZdw9cXaZ3e7nVsgyZ/Ue4PYxQHyHuRXjU36rdVMsOn5xZV8Xltw37WEkMnZZvRF
DheuJ6T2FNVgkRJrMb1PcE4Wz/CjIbWje07l3B49Ou2HRuU5EIXYEerYxv52qXU5
k+T+lRrB1gGwPgH/BveM3JHKq1p2TDj9rR4eYc5VRJenZe7bgRF73ocpgzdkJYzb
Q3VpUhq3IZ+e1JSbiyGV2lD5Uc91Ys7vP8ER9rm4DSjSQC2rO94/jHBwr6mCQbZi
i2ZOA329mtXkfwTbGvWNeyFpNf/AfTxjOIBfY1ZWLfcPzZCm62rA8VIxMA7qaz8=
=264Y
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/31364337/attachment.htm>

From hack.back at hotmail.com  Mon Apr 20 15:32:33 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 20 Apr 2015 08:32:33 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <5533B6A9.10209@treenet.co.nz>
References: <1429093589618-4670749.post@n4.nabble.com>
 <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz>
Message-ID: <1429543953483-4670835.post@n4.nabble.com>

i installed squid from source code
http://www.squid-cache.org/Versions/v3/3.4/squid-3.4.12-20150414-r13219.tar.gz
this is my configure option

./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--with-pthreads  --with-aufs-threads=512 --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
--enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
--enable-http-violations --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-swapdir=/var/spool/squid --enable-ltdl-convenience
--with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
--enable-referer-log --enable-truncate --disable-unlinkd
--enable-useragent-log --enable-eui --enable-large-cache-files
'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'



i didnt install squid from aptitude 
and this is the backtrace report



warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f609f457165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f609f45a3e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x000000000063bcd2 in xassert ()
#3  0x00000000007996b7 in comm_read(RefCount<Comm::Connection> const&,
char*, int, RefCount<AsyncCall>&) ()
#4  0x0000000000709f51 in
StoreEntry::delayAwareRead(RefCount<Comm::Connection> const&, char*, int,
RefCount<AsyncCall>) ()
#5  0x0000000000697b72 in HttpStateData::maybeReadVirginBody() ()
#6  0x000000000069a101 in HttpStateData::sendRequest() ()
#7  0x000000000069adea in HttpStateData::start() ()
#8  0x000000000079610f in NullaryMemFunT<AsyncJob>::doDial() ()
#9  0x000000000079615b in JobDialer<AsyncJob>::dial(AsyncCall&) ()
#10 0x0000000000795dbf in AsyncCallT<NullaryMemFunT&lt;AsyncJob> >::fire()
()
#11 0x00000000007933a8 in AsyncCall::make() ()
#12 0x00000000007968d3 in AsyncCallQueue::fireNext() ()
#13 0x000000000079665e in AsyncCallQueue::fire() ()
#14 0x000000000064df2b in EventLoop::dispatchCalls() ()
#15 0x000000000064ddc3 in EventLoop::runOnce() ()
#16 0x000000000064dbd1 in EventLoop::run() ()
#17 0x00000000006c967b in SquidMain(int, char**) ()
#18 0x00000000006c8d52 in SquidMainSafe(int, char**) ()
#19 0x00000000006c8d2f in main () 





now what i should do to solve this assertion error ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670835.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marc at selectgranitetops.com  Mon Apr 20 19:46:12 2015
From: marc at selectgranitetops.com (Marc Micalizzi)
Date: Mon, 20 Apr 2015 15:46:12 -0400
Subject: [squid-users] assertion failed: client_side.h:364:
 "sslServerBump == srvBump"
Message-ID: <003401d07ba2$a8a7c4f0$f9f74ed0$@selectgranitetops.com>

I'm encountering this same issue in Squid-3.5.3 

 

> Changed to:

> 

>   inline void setServerBump(Ssl::ServerBump *srvBump) {

>       if (!sslServerBump)

>           sslServerBump = srvBump;

>       else

>           assert(sslServerBump = srvBump);

 

Just FYI, from a glance this would leak an Ssl::ServerBump every time it hit
that assert, which at least in my case would be quite often.

 

What I ended up doing, which could be completely wrong for the behaviour,
but it seems to avoid the crashes, was add a function clearServerBump(), and
use that in setServerBump to clean up the old sslServerBump (as in the
header file it is only a forward declaration of the class):

 

client_side.h

 

    void clearServerBump();

    inline void setServerBump(Ssl::ServerBump *srvBump) {

        if (sslServerBump) {

            if (sslServerBump != srvBump) {

              clearServerBump();

            } else {

              return;

            }

        }

        sslServerBump = srvBump;

    }

 

client_side.cc

 

void ConnStateData::clearServerBump() {

  delete sslServerBump;

  sslServerBump = 0;

}

 

This could entirely have unintended side effects as I am not very familiar
with how sslbumping is implemented, and the intended lifetime of the objects
as relates to the client requests, but if you're still using the workaround
you posted I thought you should be aware that you would be leaking memory
and provide you with an equivalent "solution" which does not leak memory in
the same way.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150420/3db123ee/attachment.htm>

From weiguang0314 at 163.com  Tue Apr 21 02:41:56 2015
From: weiguang0314 at 163.com (=?utf-8?B?6a2P5YWJ?=)
Date: Tue, 21 Apr 2015 10:41:56 +0800
Subject: [squid-users] how to add files and url to cache by manual operation?
Message-ID: <tencent_37C5D27C186EE8BB41ADDA6A@qq.com>

I saw this page "Caching YouTube Content"
http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube


Then I have a wonder. 
I have the video file on my machine(not in squid cache) and I know the url, So if I visit the url, the machine would download the same video file. 
How can I add files to squid? nor to download the same files. 
Can squid do that?


------------
Best,
Allen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150421/eb0a2789/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 21 02:53:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 14:53:20 +1200
Subject: [squid-users] Odp: Re:  ACLs work in a half
In-Reply-To: <5534b2729250e7.23220392@wp.pl>
References: <5534b2729250e7.23220392@wp.pl>
Message-ID: <5535BBA0.20806@treenet.co.nz>

On 20/04/2015 8:01 p.m., Robert Lasota wrote:
> Dnia Niedziela, 19 Kwietnia 2015 21:28 Yuri Voinov  napisa?(a) 
>>               
>>     -----BEGIN PGP SIGNED MESSAGE----- 
>>     Hash: SHA256 
>>      
>>     
>>     
>>     20.04.15 0:29, Robert Lasota &#x43F;&#x438;&#x448;&#x435;&#x442;:
>>     > Hi,
>>       >
>>       > I have newest Squid (3.5.3). I have ACL with blacklist:
>>       >
>>       > acl blacklist1 dstdomain "/opt/etc/blacklist/porn_domains"
>>       > http_access deny blacklist1
>>       >
>>       > ..and they work on a half. I mean when I type in url e.g.      redtube.com - it 
>>       > blocks site. But when I type in some searcher: google or      bing, and then I go to 
>>       > the site from search results - I enter to site without      problems, so ACL  doesn't 
>>       > work.
>>       >
>>       > What is going on ? Please help me.
>>     
>>     Did you see, which URL produces search engine as a result? Problem    not in ACL. Problem in your understanding, how it must work.
>>     
>>     You using so primitive scheme to complete blocking.
>>     
>>     As a solution - take a look at the redirectors - squidGuard or    ufdbGuard, or some similar.
>>     
>>     For example, block page template from squidGuard uses special    mechanism to block inlined URL's with inappropriate contents:
>>     
>>     http://i.imgur.com/PnMldpr.png
>>     
>>     
> 
> You've wrote " You using so primitive scheme to complete blocking." so is it better way to achieve my goal or only use additional redirector like Squidguard ?
> 

Some people just seem to like complications. You will face the same
problem no matter whether you configure the access controls in Squid or
in a helper like SG.

You need to understand what traffic message Squid is dealing with to
match and block it appropriately.

* dstdomain only matches the domain name portion of URLs - so it only
works if the domain name is available, eg in GET requests for HTTP.

* google/bing etc the domain name is google.com or bing.com etc (not
redtube), then they shunt you off into a CONNECT tunnel also to google/bing.

There is probably a separate CONNECT tunnel going to redtube, or a GET
to a domain you were not expecting, after the search clicks.

Also, make sure your blocklist entries are in dstdomain wildcard format.
eg ".redtube.com". Or they will not match the sites subdomains. It could
be one of those subdomain the search engine sends you to.

Amos


From squid3 at treenet.co.nz  Tue Apr 21 03:45:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 15:45:57 +1200
Subject: [squid-users] [squid ] externalAclLookup: 'wbinfo_group_helper'
 queue overload.
In-Reply-To: <CA+8bHvw=Ajojam1CMOim-PZNehGxE-puEtdf7NveGc+wc==6PQ@mail.gmail.com>
References: <CA+8bHvw=Ajojam1CMOim-PZNehGxE-puEtdf7NveGc+wc==6PQ@mail.gmail.com>
Message-ID: <5535C7F5.8040102@treenet.co.nz>

On 20/04/2015 7:31 p.m., Jagannath Naidu wrote:
> Hi,
> 
> I am having this issue very frequently. Please help on this.
> 
> I get these errors randomly, mostly when usage is at very peak. (800 users)
> 
> 
> /var/log/squid/cache.log
> 
> 2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
> overload (ch=0x7fc99e2ce518)

What do you think "overload" means?
 The helper is unable to cope with the traffic load being passed to it.

Here is the biggest hint:
>
> in /var/log/messages,  I get the following errors
>
> pr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200 client
> connections, no idle connection found




> Then squid stops working. For squid to start work again, I have to dlete
> the cache and restart the squid "squid -k reconfigure", and then squid
> restart.

What Squid version are you using?

> 
> squid.conf
> 
> max_filedesc 17192
> acl manager proto cache_object
> acl localhost src 172.16.50.61/24

You have an entire /24 (256 IPs) assigned to this machine?

I think you need to remove that "/24" part if the *.61 is the local
machines *public* IP.


> http_access allow manager localhost
> dns_nameservers 172.16.3.34 10.1.2.91
> acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
> 172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
> http_access allow allowips

> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
> children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl

The above two very mangled config lines are useless. Remove them.

> acl localnet src 172.16.0.0/24

Its a bit strange that none of the localhost machine IPs
(172.16.50.0-172.16.50.255) are part of the LAN its plugged into
172.16.0.0-172.16.0.255.


> acl localnet src fc00::/7 # RFC 4193 local private network range
> acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET

Okay you have configured NTLM...

> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET

... but twice. With different settings. Only these last ones will have
any effect.


> auth_param ntlm children 600
> auth_param ntlm keep_alive off

> auth_param negotiate children 150
> auth_param negotiate keep_alive off
> visible_hostname GGNPROXY01.HTMEDIA.NET
> external_acl_type wbinfo_group_helper ttl=0 children=40 %LOGIN
> /usr/lib64/squid/wbinfo_group.pl -d
> auth_param negotiate keep_alive off

You have several useless configuration lines for Negotiate auth which is
not being used in any way. Remove those.


> acl Safe_ports port 8080 #https
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> acl auth proxy_auth REQUIRED
> acl google dstdomain -i "/etc/squid/google_site.com"
> http_access allow google
> acl sq1 external wbinfo_group_helper "/etc/squid/HT/sq1"
> acl sq2 external wbinfo_group_helper "/etc/squid/HT/sq2"
> acl sq3 external wbinfo_group_helper "/etc/squid/HT/sq3"
> acl sq4 external wbinfo_group_helper "/etc/squid/HT/sq4"
> acl sq5 external wbinfo_group_helper "/etc/squid/HT/sq5"
> acl pro1 external wbinfo_group_helper "/etc/squid/HT/pro1"
> acl pro2 external wbinfo_group_helper "/etc/squid/HT/pro2"
> acl pro3 external wbinfo_group_helper "/etc/squid/HT/pro3"
> acl pro4 external wbinfo_group_helper "/etc/squid/HT/pro4"
> acl pro5 external wbinfo_group_helper "/etc/squid/HT/pro5"
> acl pro6 external wbinfo_group_helper "/etc/squid/HT/pro6"
> acl webvip external wbinfo_group_helper "/etc/squid/HT/webvip"
> acl allgroup external wbinfo_group_helper "/etc/squid/HT/allgreop"
> acl restricted external wbinfo_group_helper "/etc/squid/HT/restricted"
> acl ad_auth proxy_auth REQUIRE

You already have an ACL named "auth" which performs authentication.
The above line is not useful. Remove it and replace all uses of
"ad_auth" ACL with "auth" ACL.

> acl allowwebsites dstdomain -i "/blacklists/allowedwebsite/domains"
> acl allowwebsites_url url_regex -i "/blacklists/allowedwebsite/url"
> http_access allow allowwebsites
> http_access allow allowwebsites_url
> acl shopping dstdomain -i "/etc/squid/shopping.txt"
> acl social_networking dstdomain -i "/blacklists/social/social.networking"
> acl youtube dstdomain -i .youtube.com
> http_access allow Safe_ports pro1 pro2 pro3 pro4 pro5 pro6 webvip

Incorrect use of "Safe_ports" security check. Correct usage is to deny
access to all *unsafe* ports. They are unsafe because HTTP can be
smuggled within the ports native protocol to attack your proxy.

Once the correct security protections for Safe_port and CONNECT tunnels
have been moved up the top remove the "Safe_ports" check from this line.

This line is also very odd in another way. ACL tests in a single line
are AND'ed together - so this means the request must be from a user who is:
  authenticated AND a member of group pro1 AND pro2 AND pro3 AND pro4
AND pro5 AND pro6 AND webvip

This hints at what your main helper problem is. The above line requires
7 group helper lookups *per request*. The winbind helper has a maximum
of 200 simultaneous connections. This line alone will limit your proxy
just under 30 new visitors per second (that becomes 60 lookups/sec
before queue overload).
 The helper result caching will help a lot, but you also have a LOT of
other group checks being made and 800 users.


> http_access allow youtube pro5
> http_access allow youtube pro6
> http_access allow youtube webvip
> http_access deny youtube
> http_access allow shopping pro5
> http_access allow shopping pro6
> http_access allow shopping webvip
> http_access deny shopping

Optimization hint:
 "youtube" and "shopping" have the same allow/deny criteria. It would be
worth combining them into one ACL.

> http_access allow social_networking pro2
> http_access allow social_networking pro4
> http_access allow social_networking pro6
> http_access allow social_networking webvip
> http_access deny social_networking
> acl porn_site1   dstdomain "/etc/squid/blacklists/porn/domains.txt"
> acl porn_site2   dstdom_regex -i "/etc/squid/blacklists/porn/expressions"
> acl porn_site3   dstdom_regex -i "/etc/squid/blacklists/porn/urls.txt"
> acl audio_video1   dstdomain "/etc/squid/blacklists/audio-video/urls.txt"
> ###################### THERE ARE TOO MANY acls and http_access , so not
> bothering with vast linux

I will bet a lot of those ACLs are also calling the group helper too yes?

> http_access allow liquorinfo webvip
> http_access deny liquorinfo
> http_access allow ad_auth
> http_access allow auth

Once you have removed ad_auth ACL, this becomes:
 http_access allow auth
 http_access allow auth

I hope you can see how redundant that is.

Also, its very likely that the "allow auth" is a useless operation after
a great many group checks have also performed authentication. That "TOO
MANY acls and https_access" list you omitted will be needed to determine
that.


> http_access allow sq1 sq2
> acl NTLMUsers proxy_auth REQUIRED

You already have an ACL named "auth" which performs authentication.
The above line is not being used in any way. Remove it.

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

These are basic security protection against Denial of Service and other
types of protocol smuggling attacks. They only work when they are used
*above* your custom "allow" rules.

Move these two lines above your "http_access allow google" line.



> http_port 8080
> hierarchy_stoplist cgi-bin ?

The above line is not useful these days. Remove it.

> cache_effective_user squid
> cache_dir aufs /var/spool/squid 20384 32 512
> cache_mem 50 MB
> cache_replacement_policy heap LFUDA
> cache_swap_low 85
> cache_swap_high 95
> maximum_object_size 5 MB
> maximum_object_size_in_memory 50 KB
> ipcache_size 5240
> ipcache_low 90
> ipcache_high 95
> cache_mgr amit
> cachemgr_passwd ****

I hope that was not your real cachemgr password you just published on a
public mailing list.


> acl SSL_ports port 443

The above is a duplicate config line. Remove it.

> http_access allow CONNECT SSL_ports
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> url_rewrite_program /usr/local/bin/squidGuard -c
> /usr/local/squidGuard/squidGuard.conf
> 


Now, as to solving your problem:

1) Clean up your config. Reduce the amount of redundant or unused
things. I've mentioned a few above.

2) Run "squid -k parse" and fix any other problems it highlights.

3) optimize your ACls and http_access rules. I've mentioned a few, such
as moving the main security checks to the top so DoS traffic does not
put load on the helpers and other ACLs.

I believe though that you will probably find Squid works much better
having the following access controls pattern:
"
 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports

 # if they are not authenticated, they will not be in a group
 http_access deny !auth

 # assuming that webvip are the group with full access?
 http_access allow webvip

 # your long list of per-site group check ACLs go here
 ...

 # this is where defining the LAN ranges correctly comes in.
 # note that users have authenticated simply to get near here
 http_access allow localnet
 http_access deny all
"


4) consider an upgrade to Squid 3.4+. The "notes" ACL type offers much
more efficient ACL testing with a custom group lookup helper. The all-of
and any-of ACL types can also much reduce your http_access lines.

HTH
Amos


From squid3 at treenet.co.nz  Tue Apr 21 04:03:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 16:03:46 +1200
Subject: [squid-users] Tracking user connection times
In-Reply-To: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
References: <CY1PR0801MB0892A4E50C1D7BC7A1D6F74BFFE00@CY1PR0801MB0892.namprd08.prod.outlook.com>
Message-ID: <5535CC22.9020006@treenet.co.nz>

On 21/04/2015 2:34 a.m., Dan Berry wrote:
> 
> I have setup a squid proxy as a POC for user tracking. I am looking
> for a way to track for close events, most of the customer sites that
> are accessed are HTTPS so I can't track activity. I might be able to
> get by with tracking total connect time, so I know the windows of
> time users were connected to a specific site. Is this possible?

Nope. Best you will achieve is a guesstimate.

HTTP is stateless protocol. There is no such thing as close (or open).
The only thing anywhere like a duration in it is the time it takes to
deliver the message bytes and request-response latency. If you try to
use that you will find that users spend an *extremely small* amount of
time visiting URLs. The vast majority of time is spent elsewhere doing
things with the response objects.

Semi-reliable user tracking was possible back in the 1980-1990s - though
the coffee problem has always been an issue. But with the invention of
multiple-windows on a screen, and more recently tabbed browsing the
ability to guess offline activity for page A vs page B is much reduced.


I know there are lots of companies etc out there saying they can track
user activity for third-party sites. They are lying-by-omission, at most
they are tracking the activity of some marker which may (or may not)
actually be representing "a user" through some series of message
transactions. It always comes down to somebody making a guess about
whether any two requests are related.

Amos


From squid3 at treenet.co.nz  Tue Apr 21 04:09:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 16:09:13 +1200
Subject: [squid-users] assertion failed: client_side.h:364:
 "sslServerBump == srvBump"
In-Reply-To: <003401d07ba2$a8a7c4f0$f9f74ed0$@selectgranitetops.com>
References: <003401d07ba2$a8a7c4f0$f9f74ed0$@selectgranitetops.com>
Message-ID: <5535CD69.9090708@treenet.co.nz>

On 21/04/2015 7:46 a.m., Marc Micalizzi wrote:
> I'm encountering this same issue in Squid-3.5.3 
> 

Bug 4198 was fixed in the 3.5 snapshots since r13792. If you are
patching for it, please use the official one. Or better yet re-build
with the latest snapshot to get many other SSL-bump related bug fixes as
well.

Amos



From squid3 at treenet.co.nz  Tue Apr 21 04:15:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 16:15:13 +1200
Subject: [squid-users] squid HTTPs as reverse proxy problem
In-Reply-To: <003c01d07bd0$e53627d0$afa27770$@netstream.ps>
References: <001d01d07bc0$0e77e4a0$2b67ade0$@netstream.ps>
 <5534FD6A.1020200@gmail.com> <003c01d07bd0$e53627d0$afa27770$@netstream.ps>
Message-ID: <5535CED1.1050209@treenet.co.nz>

On 21/04/2015 1:17 p.m., snakeeyes wrote:
> Thankx , I will tell u wt I did so far abd hope u help me in the directive squid needed :
> 


Squid does not perform SNI based certificate selection for HTTPS
virtual-hosting. You need an IP address for every top level domain being
served, sub-domains can use wildcard certificates.

<http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate>


For use of self-signed certificates in https:// reverse-proxy it is
worth ensuring that you have DNSSEC and TLS DANE configured in the
website DNS records.

Amos


From squid3 at treenet.co.nz  Tue Apr 21 04:26:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 16:26:17 +1200
Subject: [squid-users] how to add files and url to cache by manual
	operation?
In-Reply-To: <tencent_37C5D27C186EE8BB41ADDA6A@qq.com>
References: <tencent_37C5D27C186EE8BB41ADDA6A@qq.com>
Message-ID: <5535D169.4080903@treenet.co.nz>

On 21/04/2015 2:41 p.m., ?? wrote:
> I saw this page "Caching YouTube Content" 
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube
> 
> 
> Then I have a wonder. I have the video file on my machine(not in
> squid cache) and I know the url, So if I visit the url, the machine
> would download the same video file. How can I add files to squid?

Squid does not store files. It caches HTTP. The way to update that is to
make a GET request for the URL.

> nor
> to download the same files. Can squid do that?

The URL of the file on your machine is file://localhost/... it has
nothing to do with the YouTube website. Even if you downloaded the
"file" from there originally.

The video dispayed by YouTube web page is *not* a single URL. It is a
byte stream negotiated with the YouTube server via a series of messages
flowing back and forwards containing information about the video codec,
playback speed, your network connection speed, the position in the video
you are watching, your YT account details and privacy settings, age and
adult-content applicable settings etc. The URI used changes every time
you load the page, replay the video, pause the video, fast-forward,
rewind etc.

NP: that page about YouTube has not been updated in a while so is very
likely out of date. I know they made at least one message sequence
change we do not publish details about so they wont change it again. And
the HTML5 video and VP9 codec rollouts have also changed some things.

Amos


From squid3 at treenet.co.nz  Tue Apr 21 07:06:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 19:06:02 +1200
Subject: [squid-users] Issue using office 2013(microsoft),
 under squid proxy
In-Reply-To: <CA+8bHvyj55CwtjJEmSGMVr+y-FVNj2Uis0Bb=MFutvHuFvTbxg@mail.gmail.com>
References: <CA+8bHvwQ4V3zBJUO0MyQ=ujKyo7yosw0XrHTfjHN6zZZAoO0fg@mail.gmail.com>
 <CA+8bHvyj55CwtjJEmSGMVr+y-FVNj2Uis0Bb=MFutvHuFvTbxg@mail.gmail.com>
Message-ID: <5535F6DA.4070608@treenet.co.nz>

On 16/04/2015 2:35 p.m., Jagannath Naidu wrote:
> anybody ???
> is it even possible ??
> 

It seems the answer is "unknown".

FWIW: I did have one client see this same issue. It seemed to be NTLM
related (as in Office not doing NTLM properly), but they opted to go
with not passing the affected users through the proxy rather than
investigating further.

Amos



From a9121431 at gmail.com  Tue Apr 21 07:17:29 2015
From: a9121431 at gmail.com (John Mok)
Date: Tue, 21 Apr 2015 15:17:29 +0800
Subject: [squid-users] Issue using office 2013(microsoft),
	under squid proxy
In-Reply-To: <5535F6DA.4070608@treenet.co.nz>
References: <CA+8bHvwQ4V3zBJUO0MyQ=ujKyo7yosw0XrHTfjHN6zZZAoO0fg@mail.gmail.com>
 <CA+8bHvyj55CwtjJEmSGMVr+y-FVNj2Uis0Bb=MFutvHuFvTbxg@mail.gmail.com>
 <5535F6DA.4070608@treenet.co.nz>
Message-ID: <CALYzA8FC8M4GpN2JtNnhF6g5OZJP8zXU60COBqFDuKSrjRV7-w@mail.gmail.com>

Hi,

In my experience, I came across the problem when Windows clients
access files on Windows servers (e.g. IIS, SharePoint, etc.) and the
server kept prompting user to enter user id and password.

Amos is right and my problem was with NTLM authentication. I solved
the problem with Squid version, e.g. 2.6 or 3.1, that supports
connection pinning :-

http://wiki.squid-cache.org/Features/ConnPin

Regards,   John Mok


From jaykbvt at gmail.com  Tue Apr 21 10:44:04 2015
From: jaykbvt at gmail.com (jaykbvt)
Date: Tue, 21 Apr 2015 03:44:04 -0700 (PDT)
Subject: [squid-users] transparent proxy original_dst err
Message-ID: <1429613044753-4670846.post@n4.nabble.com>

Hi,
My squid is configured in interception mode with 

http_port 3130
http_port 3129 intercept

squid is running with single network card. request comes from the Cisco ISG
and internet is also allowed from the same Cisco ISG only.

IPtables has been configured with following 
squidip = 10.58.200.33
squid port = 3129
====================
iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
10.58.200.33:3129
====================

Have also tried setting up config suggested at squid docs

DNAT - http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
Redirect -
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect


But in all three setup I am getting 

I'm getting following entries in my access.log file...

==========================================================
1429610951.208    309 10.210.83.249 TCP_MISS/503 3808 GET
http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
1429611003.025      5 10.210.83.249 TCP_MISS/503 3808 GET
http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
1429611620.888    306 10.210.83.249 TCP_MISS/503 3808 GET
http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
1429611625.952      4 10.210.83.249 TCP_MISS/503 3808 GET
http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
==========================================================

Given bellow are entries in cache.log

+++++++++++++++++++++++++++++++++++
2015/04/21 15:50:20.576 kid1| client_side.cc(3412) httpAccept:
local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: accepted
2015/04/21 15:50:20.576 kid1| client_side.cc(258) readSomeData:
local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: reading
request...
2015/04/21 15:50:20.581 kid1| client_side.cc(2322) parseHttpRequest:
parseHttpRequest: req_hdr = {Host: www.wikipedia.org
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
Firefox/35.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Connection: keep-alive

}
2015/04/21 15:50:20.581 kid1| client_side.cc(2326) parseHttpRequest:
parseHttpRequest: end = {
}
2015/04/21 15:50:20.581 kid1| client_side.cc(2330) parseHttpRequest:
parseHttpRequest: prefix_sz = 284, req_line_sz = 16
2015/04/21 15:50:20.582 kid1| client_side.cc(925) clientSetKeepaliveFlag:
clientSetKeepaliveFlag: http_ver = 1.1
2015/04/21 15:50:20.582 kid1| client_side.cc(927) clientSetKeepaliveFlag:
clientSetKeepaliveFlag: method = GET
2015/04/21 15:50:20.582 kid1| client_side_request.cc(1691) doCallouts: Doing
calloutContext->hostHeaderVerify()
2015/04/21 15:50:20.583 kid1| client_side.cc(258) readSomeData:
local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: reading
request...
2015/04/21 15:50:20.884 kid1| client_side_request.cc(1698) doCallouts: Doing
calloutContext->clientAccessCheck()
2015/04/21 15:50:20.884 kid1| AccessCheck.cc(32) Start: adaptation off,
skipping
2015/04/21 15:50:20.884 kid1| client_side_request.cc(1727) doCallouts: Doing
calloutContext->clientAccessCheck2()
2015/04/21 15:50:20.884 kid1| client_side_request.cc(1746) doCallouts: Doing
clientInterpretRequestHeaders()
2015/04/21 15:50:20.885 kid1| client_side_request.cc(1835) doCallouts:
calling processRequest()
2015/04/21 15:50:20.888 kid1| client_side.cc(1626) keepaliveNextRequest:
ConnnStateData(local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10
flags=33), Context(local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10
flags=33)
2015/04/21 15:50:20.888 kid1| client_side_request.cc(265)
~ClientHttpRequest: httpRequestFree: http://www.wikipedia.org/
2015/04/21 15:50:20.888 kid1| client_side.cc(1696) keepaliveNextRequest:
local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: calling
conn->readNextRequest()
2015/04/21 15:50:23.401 kid1| client_side.cc(2492) connFinishedWithConn:
local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33 closed
2015/04/21 15:50:23.401 kid1| client_side.cc(864) swanSong:
local=10.58.200.33:80 remote=10.210.83.249:3375 flags=33
2015/04/21 15:50:23.401 kid1| client_side.cc(4644) unpinConnection: 
2015/04/21 15:50:23.402 kid1| client_side.cc(895) ~ConnStateData:
local=10.58.200.33:80 remote=10.210.83.249:3375 flags=33
2015/04/21 15:50:25.945 kid1| client_side.cc(3412) httpAccept:
local=10.58.200.33:80 remote=10.210.83.249:3378 FD 10 flags=33: accepted
2015/04/21 15:50:25.946 kid1| client_side.cc(258) readSomeData:
local=10.58.200.33:80 remote=10.210.83.249:3378 FD 10 flags=33: reading
request...
2015/04/21 15:50:25.947 kid1| client_side.cc(2322) parseHttpRequest:
parseHttpRequest: req_hdr = {Host: www.wikipedia.org
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
Firefox/35.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Connection: keep-alive

+++++++++++++++++++++++++++++++++++





any idea how to resolve this.

Thanks & Regards,
Jaykbvt




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-original-dst-err-tp4670846.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Apr 21 10:46:12 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 21 Apr 2015 03:46:12 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429543953483-4670835.post@n4.nabble.com>
References: <1429103177898-4670756.post@n4.nabble.com>
 <1429189971025-4670781.post@n4.nabble.com> <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz> <1429543953483-4670835.post@n4.nabble.com>
Message-ID: <1429613172522-4670847.post@n4.nabble.com>

after updating my second squid box to the latest squid 3.4.12 that have
solved ssl_crtd crashing problem
then i got the same assertion error that i had on my another squid box,
now both are damaged ,
also i uninstall latest 3.4.12 and reinstall the previous version and also
the same problem still here,
now i lost , i cant solve this problem !! i cant find the solution !! i cant
know what is the reason.
and i need help !



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670847.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Apr 21 11:12:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 21 Apr 2015 17:12:52 +0600
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <1429613044753-4670846.post@n4.nabble.com>
References: <1429613044753-4670846.post@n4.nabble.com>
Message-ID: <553630B4.2050007@gmail.com>

So, what?

What's the problem?

21.04.15 16:44, jaykbvt ?????:
> Hi,
> My squid is configured in interception mode with
>
> http_port 3130
> http_port 3129 intercept
>
> squid is running with single network card. request comes from the Cisco ISG
> and internet is also allowed from the same Cisco ISG only.
>
> IPtables has been configured with following
> squidip = 10.58.200.33
> squid port = 3129
> ====================
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
> 10.58.200.33:3129
> ====================
>
> Have also tried setting up config suggested at squid docs
>
> DNAT - http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
> Redirect -
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
>
>
> But in all three setup I am getting
>
> I'm getting following entries in my access.log file...
>
> ==========================================================
> 1429610951.208    309 10.210.83.249 TCP_MISS/503 3808 GET
> http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
> 1429611003.025      5 10.210.83.249 TCP_MISS/503 3808 GET
> http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
> 1429611620.888    306 10.210.83.249 TCP_MISS/503 3808 GET
> http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
> 1429611625.952      4 10.210.83.249 TCP_MISS/503 3808 GET
> http://www.wikipedia.org/ - ORIGINAL_DST/10.58.200.33 text/html
> ==========================================================
>
> Given bellow are entries in cache.log
>
> +++++++++++++++++++++++++++++++++++
> 2015/04/21 15:50:20.576 kid1| client_side.cc(3412) httpAccept:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: accepted
> 2015/04/21 15:50:20.576 kid1| client_side.cc(258) readSomeData:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: reading
> request...
> 2015/04/21 15:50:20.581 kid1| client_side.cc(2322) parseHttpRequest:
> parseHttpRequest: req_hdr = {Host: www.wikipedia.org
> User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
> Firefox/35.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: en-US,en;q=0.5
> Accept-Encoding: gzip, deflate
> Connection: keep-alive
>
> }
> 2015/04/21 15:50:20.581 kid1| client_side.cc(2326) parseHttpRequest:
> parseHttpRequest: end = {
> }
> 2015/04/21 15:50:20.581 kid1| client_side.cc(2330) parseHttpRequest:
> parseHttpRequest: prefix_sz = 284, req_line_sz = 16
> 2015/04/21 15:50:20.582 kid1| client_side.cc(925) clientSetKeepaliveFlag:
> clientSetKeepaliveFlag: http_ver = 1.1
> 2015/04/21 15:50:20.582 kid1| client_side.cc(927) clientSetKeepaliveFlag:
> clientSetKeepaliveFlag: method = GET
> 2015/04/21 15:50:20.582 kid1| client_side_request.cc(1691) doCallouts: Doing
> calloutContext->hostHeaderVerify()
> 2015/04/21 15:50:20.583 kid1| client_side.cc(258) readSomeData:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: reading
> request...
> 2015/04/21 15:50:20.884 kid1| client_side_request.cc(1698) doCallouts: Doing
> calloutContext->clientAccessCheck()
> 2015/04/21 15:50:20.884 kid1| AccessCheck.cc(32) Start: adaptation off,
> skipping
> 2015/04/21 15:50:20.884 kid1| client_side_request.cc(1727) doCallouts: Doing
> calloutContext->clientAccessCheck2()
> 2015/04/21 15:50:20.884 kid1| client_side_request.cc(1746) doCallouts: Doing
> clientInterpretRequestHeaders()
> 2015/04/21 15:50:20.885 kid1| client_side_request.cc(1835) doCallouts:
> calling processRequest()
> 2015/04/21 15:50:20.888 kid1| client_side.cc(1626) keepaliveNextRequest:
> ConnnStateData(local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10
> flags=33), Context(local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10
> flags=33)
> 2015/04/21 15:50:20.888 kid1| client_side_request.cc(265)
> ~ClientHttpRequest: httpRequestFree: http://www.wikipedia.org/
> 2015/04/21 15:50:20.888 kid1| client_side.cc(1696) keepaliveNextRequest:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: calling
> conn->readNextRequest()
> 2015/04/21 15:50:23.401 kid1| client_side.cc(2492) connFinishedWithConn:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33 closed
> 2015/04/21 15:50:23.401 kid1| client_side.cc(864) swanSong:
> local=10.58.200.33:80 remote=10.210.83.249:3375 flags=33
> 2015/04/21 15:50:23.401 kid1| client_side.cc(4644) unpinConnection:
> 2015/04/21 15:50:23.402 kid1| client_side.cc(895) ~ConnStateData:
> local=10.58.200.33:80 remote=10.210.83.249:3375 flags=33
> 2015/04/21 15:50:25.945 kid1| client_side.cc(3412) httpAccept:
> local=10.58.200.33:80 remote=10.210.83.249:3378 FD 10 flags=33: accepted
> 2015/04/21 15:50:25.946 kid1| client_side.cc(258) readSomeData:
> local=10.58.200.33:80 remote=10.210.83.249:3378 FD 10 flags=33: reading
> request...
> 2015/04/21 15:50:25.947 kid1| client_side.cc(2322) parseHttpRequest:
> parseHttpRequest: req_hdr = {Host: www.wikipedia.org
> User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
> Firefox/35.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: en-US,en;q=0.5
> Accept-Encoding: gzip, deflate
> Connection: keep-alive
>
> +++++++++++++++++++++++++++++++++++
>
>
>
>
>
> any idea how to resolve this.
>
> Thanks & Regards,
> Jaykbvt
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-original-dst-err-tp4670846.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Apr 21 11:20:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2015 23:20:48 +1200
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <1429613044753-4670846.post@n4.nabble.com>
References: <1429613044753-4670846.post@n4.nabble.com>
Message-ID: <55363290.4000306@treenet.co.nz>

On 21/04/2015 10:44 p.m., jaykbvt wrote:
> Hi,
> My squid is configured in interception mode with 
> 
> http_port 3130
> http_port 3129 intercept
> 
> squid is running with single network card. request comes from the Cisco ISG
> and internet is also allowed from the same Cisco ISG only.

I think the Cisco is doing NAT and erasing the original dst-IP value
from the client TCP packets. The problem needs to be fixed there (by not
NAT'ing on the Cisco).

> 
> IPtables has been configured with following 
> squidip = 10.58.200.33
> squid port = 3129
> ====================
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
> 10.58.200.33:3129
> ====================
> 

This above iptables NAT is changing something:80 to 10.58.200.33:3129.

When things are configured right the something is the origin web servers
IP the client was contacting. And the NAT un-mangling operation in Squid
converts the 10.58.200.33:3129 back to something:80.

NOTE: there are other iptables rules needed to prevent the from-Squid
traffic being looped back, and attackers contacting the Squid listening
port. But your proxy is not getting that far yet. So this is just a
heads-up for now.


> Given bellow are entries in cache.log
> 
> +++++++++++++++++++++++++++++++++++
> 2015/04/21 15:50:20.576 kid1| client_side.cc(3412) httpAccept:
> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: accepted

This is the connection info *after* the iptables NAT mangling is
un-done. The 10.58.200.33:3129 has succesfully been converted back into
something:80.

Unfortunately that something:80 dst-IP addresc received from the Cisco
was "10.58.200.33:80" as you can see in the local= parameter above.


Amos


From yvoinov at gmail.com  Tue Apr 21 11:37:13 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 21 Apr 2015 17:37:13 +0600
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <55363290.4000306@treenet.co.nz>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz>
Message-ID: <55363669.5060803@gmail.com>


21.04.15 17:20, Amos Jeffries ?????:
> On 21/04/2015 10:44 p.m., jaykbvt wrote:
>> Hi,
>> My squid is configured in interception mode with
>>
>> http_port 3130
>> http_port 3129 intercept
>>
>> squid is running with single network card. request comes from the Cisco ISG
>> and internet is also allowed from the same Cisco ISG only.
> I think the Cisco is doing NAT and erasing the original dst-IP value
> from the client TCP packets. The problem needs to be fixed there (by not
> NAT'ing on the Cisco).

Using NAT onto backoffice Cisco is not good idea. Usually, NAT only 
using on front router.

>
>> IPtables has been configured with following
>> squidip = 10.58.200.33
>> squid port = 3129
>> ====================
>> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
>> 10.58.200.33:3129
>> ====================
>>
> This above iptables NAT is changing something:80 to 10.58.200.33:3129.
>
> When things are configured right the something is the origin web servers
> IP the client was contacting. And the NAT un-mangling operation in Squid
> converts the 10.58.200.33:3129 back to something:80.
>
> NOTE: there are other iptables rules needed to prevent the from-Squid
> traffic being looped back, and attackers contacting the Squid listening
> port. But your proxy is not getting that far yet. So this is just a
> heads-up for now.
>
>
>> Given bellow are entries in cache.log
>>
>> +++++++++++++++++++++++++++++++++++
>> 2015/04/21 15:50:20.576 kid1| client_side.cc(3412) httpAccept:
>> local=10.58.200.33:80 remote=10.210.83.249:3375 FD 10 flags=33: accepted
> This is the connection info *after* the iptables NAT mangling is
> un-done. The 10.58.200.33:3129 has succesfully been converted back into
> something:80.
>
> Unfortunately that something:80 dst-IP addresc received from the Cisco
> was "10.58.200.33:80" as you can see in the local= parameter above.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Apr 21 12:01:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 00:01:12 +1200
Subject: [squid-users] Squid downloading huge amounts of un-requested
 data
In-Reply-To: <1429231899613-4670786.post@n4.nabble.com>
References: <1429154215215-4670770.post@n4.nabble.com>
 <552F463B.2090106@treenet.co.nz> <1429231899613-4670786.post@n4.nabble.com>
Message-ID: <55363C08.5080801@treenet.co.nz>

On 17/04/2015 12:51 p.m., iridium191 wrote:
> Thanks for your response Amos, it is much appreciated. 
> The config is below, with comments excluded - we've done tests in the past
> to confirm it is not an open proxy and don't believe it is. Any commnts you
> may have would also be appreciated.
> The past excessive download events correlated with Microsoft patch Tuesdays
> or in the most recent case deploying a new Windows server and then manually
> updating it, which made us suspect that our refresh rules attempting to
> cache Windows updates was the cause of the problem.
> 
> In the config squidguard should be bypassed for Windows updates and
> squidclamav uses its own whitelist to bypass Windows update sites.

Okay. Noted, and confirmed by the below config.

> 
> Our traffic monitoring so far has been aggregated, so we could see that
> 103GB of http traffic was directed to the squid server from the firewall,
> and of that 15GB came from Microsoft, 12GB from akamai server 1 etc.. You're
> right we didn't consider that something other than squid on the server may
> be causing the requests.
> 

Now that you mention clamav ... I had some issues on my own proxies a
while back where the freshclam auto-update daemon had partially crashed
and on resume was unable to validate the AV updates properly. That
pushed it into a loop of re-downloading the entire virus signatures file
every few minutes - while the file was only a few dozen KB the constant
repeating grew to many GBs over the course of the month before it was
caught.



> The cache utilization report looks interesting in that we may be able to
> script it for more real-time notification of excessive traffic rather than
> relying on the morning firewall report. Are there any definitions of the
> various counters, eg client_http.kbytes_in, client_http.kbytes_in ?

Not that I'm aware of. They should be self-explanatory from the naming
though.

client_http.kbytes_in -> KB received in to Squid from all clients using
HTTP protocol.


The section headers explain how long a time period the counters below it
cover (5min, 60min, totals since last restart, etc).

> 
> Thanks again,
> 
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280                # http-mgmt
> acl Safe_ports port 488                # gss-http
> acl Safe_ports port 591                # filemaker
> acl Safe_ports port 777                # multiling http
> 
> acl CONNECT method CONNECT
> acl ftp proto FTP
> 
> acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
> acl Purge method PURGE
> 
> acl Local_Networks src 10.250.111.0/24 10.250.112.0/24
> acl BypassCache dst 10.250.111.0/24 10.250.112.0/24
> acl BypassCache dst 146.178.211.0/24
> 
> acl BypassCacheDomains dstdomain "/etc/squid3/BypassCacheDomains"
> acl RestrictedUsers proxy_auth "/etc/squid3/RestrictedUsers"
> 
> # ACLs for Windows Updates & other exceptions
> acl WindowsUpdate dstdomain "/etc/squid3/WindowsUpdate"
> acl Whitelist_Domains dstdomain "/etc/squid3/Whitelist_Domains"
> 
> # ACL to allow monitoring of entire proxy chain from 10.250.111.124 without
> authentication 
> acl MonitorProxy src 10.250.111.124/32
> 
> acl Get_Username proxy_auth REQUIRED

The above ACL is unused and does nothing.

> 
> # Bypass squidguard for whitelisted domains
> redirector_access deny Whitelist_Domains
> redirector_access deny WindowsUpdate
> # Bypass squidguard for local sites 
> redirector_access deny BypassCache
> redirector_access deny BypassCacheDomains
> 
> # Bypass connections to local network and TLS
> always_direct allow BypassCache

always_direct does not seem to do what you think it does. All it does is
prevent Squid using a cache_peer to service those requests. They are
still proxied by *this* Squid.

> cache deny BypassCache
> always_direct allow BypassCacheDomains
> cache deny BypassCacheDomains
> 
> http_access allow manager localhost
> http_access allow localhost Purge
> http_access deny manager
> http_access deny Purge
> http_access deny to_localhost
> http_access deny !Local_Networks
> http_access allow Whitelist_Domains
> http_access allow WindowsUpdate
> http_access allow MonitorProxy
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> # Allow connection to HTTPS sites from the local network
> http_access allow CONNECT SSL_ports Local_Networks
> http_access allow ftp
> http_access allow !RestrictedUsers
> 
> http_access deny all
> 
> http_port 8080
> visible_hostname Squid3
> hierarchy_stoplist cgi-bin ?
> 
> # Log file locations
> access_log daemon:/var/log/squid3/access.log squid
> cache_store_log none
> cache_log /var/log/squid3/cache.log
> 
> # Disk cache directory.
> cache_dir aufs /squid_cache/Squid3Cache 25000 16 256
> cache_mem 2000 MB
> maximum_object_size_in_memory 1 MB
> 
> # Windows Update
> #range_offset_limit 200 MB WindowsUpdate 
> maximum_object_size 1 GB
> #quick_abort_min -1
> 
> dns_nameservers 127.0.0.1
> 
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_encode off
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_req reqmod_precache bypass=0
> icap://127.0.0.1:1344/squidclamav
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=0
> icap://127.0.0.1:1344/squidclamav
> adaptation_access service_resp allow all
> 
> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> url_rewrite_children 20 startup=0 idle=1 concurrency=0
> 
> #Do not show client IP address
> via off
> forwarded_for off
> 
> #Rules to anonymize http headers
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all
> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Pragma allow all
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access Content-Language allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Connection allow all
> request_header_access Proxy-Connection allow all
> request_header_access Cookie allow all
> ###request_header_access All deny all


These ones are response-only headers and you can remove from the list:
 Last-Modified, Location, Retry-After, Date, WWW-Authenticate,
Proxy-Authenticate, Expires

I recommend adding the Expect, ETag, TE, Transfer-Encoding, If-Match,
If-None-Match, If-Unmodified-Since, Range and If-Range headers to the
above allow lists. That will allow HTTP/1.1 persistent connections and
revalidations to work a lot better.

Come to think of it the Range/If-Range and ETag not being allowed is
probably related to your problem.

You can remove Proxy-Connection. Its an obsolete header Squid does not
emit. Mime-Version and Title are also pretty useless unless you have
WebDAV clients.

Accept-Charset and Accept-Language are not commonly useful and have a
large impact on anonymity. Removing them from your allow list could be
beneficial.

Amos


From jaykbvt at gmail.com  Tue Apr 21 12:43:36 2015
From: jaykbvt at gmail.com (jaykbvt)
Date: Tue, 21 Apr 2015 05:43:36 -0700 (PDT)
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <55363290.4000306@treenet.co.nz>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz>
Message-ID: <1429620216887-4670852.post@n4.nabble.com>

Hi Amos,

Thanks for reply.

++++++++++++++++++++++++
local=*10.58.200.33:80 remote=10.210.83.249:*3375 FD 10 flags=33: accepted 
++++++++++++++++++++++++

since squid is able to understand which client is requesting and following
lines talks about request..

++++++++++++++++++++++++
parseHttpRequest: parseHttpRequest: req_hdr = {Host: www.wikipedia.org
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
Firefox/35.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Connection: keep-alive

}
++++++++++++++++++++++++

you still feel there could be issue with Cisco erasing original dst-IP
value.??

The thing is Cisoco ISG is not managed by us. They are saying they've
configured any incoming traffic from clients for web its redirected to
squid's IP. I'm no expert on Cisco ISG, yet I've asked them to share the
config pertaining to squid. I am awaiting their response.

Can you help me what should I ask them or point towards to check..and what
type squid/iptables config combination should I do on my squid server given
my network scenario.


Thanks & Regards,
Jaykbvt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-original-dst-err-tp4670846p4670852.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Apr 21 13:24:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 01:24:44 +1200
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <1429620216887-4670852.post@n4.nabble.com>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz> <1429620216887-4670852.post@n4.nabble.com>
Message-ID: <55364F9C.9080908@treenet.co.nz>

On 22/04/2015 12:43 a.m., jaykbvt wrote:
> Hi Amos,
> 
> Thanks for reply.
> 
> ++++++++++++++++++++++++
> local=*10.58.200.33:80 remote=10.210.83.249:*3375 FD 10 flags=33: accepted 
> ++++++++++++++++++++++++
> 
> since squid is able to understand which client is requesting and following
> lines talks about request..
> 
> ++++++++++++++++++++++++
> parseHttpRequest: parseHttpRequest: req_hdr = {Host: www.wikipedia.org
> User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:35.0) Gecko/20100101
> Firefox/35.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: en-US,en;q=0.5
> Accept-Encoding: gzip, deflate
> Connection: keep-alive
> 
> }
> ++++++++++++++++++++++++
> 
> you still feel there could be issue with Cisco erasing original dst-IP
> value.??

Yes. Its receiving the HTTP properly, but the broken TCP details
(10.58.200.33:80) prevent the requests being relayed on to the right server.

pPS. Unless you are working for Wikimedia and the 10.58.200.33:80
actually is the backend server address. In that case we would have gone
completely the wrong way to a fix.


> 
> The thing is Cisoco ISG is not managed by us. They are saying they've
> configured any incoming traffic from clients for web its redirected to
> squid's IP. I'm no expert on Cisco ISG, yet I've asked them to share the
> config pertaining to squid. I am awaiting their response.
> 
> Can you help me what should I ask them or point towards to check..and what
> type squid/iptables config combination should I do on my squid server given
> my network scenario.

As per the DNST page you used already:
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>

Just make sure you have all 4 iptables rules listed on the page. Rather
than just the 1 you mentioned having.

Amos


From ahmed.zaeem at netstream.ps  Wed Apr 22 03:17:36 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Tue, 21 Apr 2015 20:17:36 -0700
Subject: [squid-users] problem in squid certificate installtion
Message-ID: <008601d07caa$e25f3800$a71da800$@netstream.ps>

Hi 

I need to setup squid proxy as reverse proxy with https enabled

I tried  the bash script below and it run ok :

###########################

OPENSSL=/usr/bin/openssl

 

SSLDIR=/etc/mydlp/ssl

 

mkdir -p $SSLDIR || exit 1

 

rm -rf $SSLDIR/*

 

[ -e $SSLDIR/private.pem ] || $OPENSSL genrsa 4096 > $SSLDIR/private.pem

 

[ -e $SSLDIR/public.pem ] || (echo -e
"TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n"| $OPENSSL
req -new -x509 -days 3650 -key $SSLDIR/private.pem -out $SSLDIR/public.pem)

 

[ -e $SSLDIR/user.der ] || $OPENSSL x509 -in $SSLDIR/public.pem -outform DER
-out $SSLDIR/user.der

######################################

 

 

ls -l /etc/mydlp/ssl

total 12

-rw-r--r-- 1 root root 3243 Apr 21 08:26 private.pem

-rw-r--r-- 1 root root 2090 Apr 21 08:26 public.pem

-rw-r--r-- 1 root root 1501 Apr 21 08:27 user.der

 

######################################

 

Added to squid.conf :

 

https_port 443 key=/etc/mydlp/ssl/private.pem cert=/etc/mydlp/ssl/public.pem

 

 

 

And when I start squid , 

 

FATAL: No valid signing SSL certificate configured for HTTPS_port [::]:443

Squid Cache (Version 3.5.1): Terminated abnormally.

CPU Usage: 10.189 seconds = 10.133 user + 0.056 sys

Maximum Resident Size: 271264 KB

Page faults with physical i/o: 44

 

 

 

 

Hope to help

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150421/f88ac714/attachment.htm>

From yvoinov at gmail.com  Tue Apr 21 18:18:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Apr 2015 00:18:58 +0600
Subject: [squid-users] problem in squid certificate installtion
In-Reply-To: <008601d07caa$e25f3800$a71da800$@netstream.ps>
References: <008601d07caa$e25f3800$a71da800$@netstream.ps>
Message-ID: <55369492.4040103@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Self-signed certificate is not suitable for use in a reverse proxy.

22.04.15 9:17, snakeeyes ?????:
> Hi 
>
> I need to setup squid proxy as reverse proxy with https enabled
>
> I tried  the bash script below and it run ok :
>
> ###########################
>
> OPENSSL=/usr/bin/openssl
>
> 
>
> SSLDIR=/etc/mydlp/ssl
>
> 
>
> mkdir -p $SSLDIR || exit 1
>
> 
>
> rm -rf $SSLDIR/*
>
> 
>
> [ -e $SSLDIR/private.pem ] || $OPENSSL genrsa 4096 > $SSLDIR/private.pem
>
> 
>
> [ -e $SSLDIR/public.pem ] || (echo -e
> "TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n"| $OPENSSL
> req -new -x509 -days 3650 -key $SSLDIR/private.pem -out
$SSLDIR/public.pem)
>
> 
>
> [ -e $SSLDIR/user.der ] || $OPENSSL x509 -in $SSLDIR/public.pem
-outform DER
> -out $SSLDIR/user.der
>
> ######################################
>
> 
>
> 
>
> ls -l /etc/mydlp/ssl
>
> total 12
>
> -rw-r--r-- 1 root root 3243 Apr 21 08:26 private.pem
>
> -rw-r--r-- 1 root root 2090 Apr 21 08:26 public.pem
>
> -rw-r--r-- 1 root root 1501 Apr 21 08:27 user.der
>
> 
>
> ######################################
>
> 
>
> Added to squid.conf :
>
> 
>
> https_port 443 key=/etc/mydlp/ssl/private.pem
cert=/etc/mydlp/ssl/public.pem
>
> 
>
> 
>
> 
>
> And when I start squid ,
>
> 
>
> FATAL: No valid signing SSL certificate configured for HTTPS_port [::]:443
>
> Squid Cache (Version 3.5.1): Terminated abnormally.
>
> CPU Usage: 10.189 seconds = 10.133 user + 0.056 sys
>
> Maximum Resident Size: 271264 KB
>
> Page faults with physical i/o: 44
>
> 
>
> 
>
> 
>
> 
>
> Hope to help
>
> 
>
> regards
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNpSSAAoJENNXIZxhPexGq+4H/3KGzflx2iP+/nYH9SITqmun
okbIgNUX31WbNYWy8Na+7fnEqE/e/Sfc5qGP2LhbL3iPz72pspBE0vpvLPvAa8iL
kak/CLDEaFXizPVhfPIi7FI9Vdpvl4D2Pfm3aHHXxoTFjmLvM6htTlNntNCYuG1P
iLm7gFUNC9pltRrEbnKmhxh3CKsc6iUC3L3muLLaH3WX7WJNtCzTxh+8OQKeDIh1
ZWAbvpXnPT6PdXI4rDF6+J16eC6TUo0stiWds2XsYH958AWJRwcHi5UL+Vgq1X6Z
9GWYZVKlXNxBfGR5Zv1anmmaDP2ouJG3DwI5U8Dqe6B6dcGYQWtU+m1Hieuy5Ko=
=BiO/
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/2924348b/attachment.htm>

From jaykbvt at gmail.com  Tue Apr 21 19:31:56 2015
From: jaykbvt at gmail.com (jaykbvt)
Date: Tue, 21 Apr 2015 12:31:56 -0700 (PDT)
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <55364F9C.9080908@treenet.co.nz>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz> <1429620216887-4670852.post@n4.nabble.com>
 <55364F9C.9080908@treenet.co.nz>
Message-ID: <1429644716040-4670856.post@n4.nabble.com>

Hi Amos,

Thanks for reply,

I think I got ur point. If I understood correctly,

if a user makes request for http://www.wikipedia.org then the client request
header should look like:

src: client_IP:random_port
dst: wikipedia.org(ip_address):http
http request: http_request details. (host,url,etc..)

and squid should get the packet like that.

But since Cisco ISG is in between which seems to be changing the client
request header like:

src: client_IP:random_port
dst: squid_IP:http
http request: http_request details. (host,url,etc..)

and eventually squid fails to understand where to send http_request.

And thats why we should look at cisco ISG config.

my iptables config looks like:

iptables -t nat -A PREROUTING -s 10.58.200.33 -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
10.58.200.33:3129
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport 3129 -j DROP

Pls comment.

Thanks & Regards,
Jaykbvt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-original-dst-err-tp4670846p4670856.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Wed Apr 22 07:30:17 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Wed, 22 Apr 2015 00:30:17 -0700
Subject: [squid-users] problem in squid certificate installtion
In-Reply-To: <55369492.4040103@gmail.com>
References: <008601d07caa$e25f3800$a71da800$@netstream.ps>
 <55369492.4040103@gmail.com>
Message-ID: <009a01d07cce$2f451c70$8dcf5550$@netstream.ps>

Hmmm ,  cant u  provide more info??

I followed wiki 

http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate

 

but im still confused with certificates , if possible and don?t mind , could u tell me brief steps ?

 

thanks a lot for ur kind help

 

regards

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Tuesday, April 21, 2015 11:19 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] problem in squid certificate installtion

 


-----BEGIN PGP SIGNED MESSAGE----- 
Hash: SHA256 
 
Self-signed certificate is not suitable for use in a reverse proxy.

22.04.15 9:17, snakeeyes ?????:
> Hi 



      >



      > I need to setup squid proxy as reverse proxy with https

      enabled



      >



      > I tried  the bash script below and it run ok :



      >



      > ###########################



      >



      > OPENSSL=/usr/bin/openssl



      >



      >  



      >



      > SSLDIR=/etc/mydlp/ssl



      >



      >  



      >



      > mkdir -p $SSLDIR || exit 1



      >



      >  



      >



      > rm -rf $SSLDIR/*



      >



      >  



      >



      > [ -e $SSLDIR/private.pem ] || $OPENSSL genrsa 4096 >

      $SSLDIR/private.pem



      >



      >  



      >



      > [ -e $SSLDIR/public.pem ] || (echo -e



      >

       <mailto:TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n> "TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n"|

      $OPENSSL



      > req -new -x509 -days 3650 -key $SSLDIR/private.pem -out

      $SSLDIR/public.pem)



      >



      >  



      >



      > [ -e $SSLDIR/user.der ] || $OPENSSL x509 -in

      $SSLDIR/public.pem -outform DER



      > -out $SSLDIR/user.der



      >



      > ######################################



      >



      >  



      >



      >  



      >



      > ls -l /etc/mydlp/ssl



      >



      > total 12



      >



      > -rw-r--r-- 1 root root 3243 Apr 21 08:26 private.pem



      >



      > -rw-r--r-- 1 root root 2090 Apr 21 08:26 public.pem



      >



      > -rw-r--r-- 1 root root 1501 Apr 21 08:27 user.der



      >



      >  



      >



      > ######################################



      >



      >  



      >



      > Added to squid.conf :



      >



      >  



      >



      > https_port 443 key=/etc/mydlp/ssl/private.pem

      cert=/etc/mydlp/ssl/public.pem



      >



      >  



      >



      >  



      >



      >  



      >



      > And when I start squid , 



      >



      >  



      >



      > FATAL: No valid signing SSL certificate configured for

      HTTPS_port [::]:443



      >



      > Squid Cache (Version 3.5.1): Terminated abnormally.



      >



      > CPU Usage: 10.189 seconds = 10.133 user + 0.056 sys



      >



      > Maximum Resident Size: 271264 KB



      >



      > Page faults with physical i/o: 44



      >



      >  



      >



      >  



      >



      >  



      >



      >  



      >



      > Hope to help



      >



      >  



      >



      > regards



      >



      >



      >



      >



      > _______________________________________________



      > squid-users mailing list



      > squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 



      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE----- 
Version: GnuPG v2 
 
iQEcBAEBCAAGBQJVNpSSAAoJENNXIZxhPexGq+4H/3KGzflx2iP+/nYH9SITqmun 
okbIgNUX31WbNYWy8Na+7fnEqE/e/Sfc5qGP2LhbL3iPz72pspBE0vpvLPvAa8iL 
kak/CLDEaFXizPVhfPIi7FI9Vdpvl4D2Pfm3aHHXxoTFjmLvM6htTlNntNCYuG1P 
iLm7gFUNC9pltRrEbnKmhxh3CKsc6iUC3L3muLLaH3WX7WJNtCzTxh+8OQKeDIh1 
ZWAbvpXnPT6PdXI4rDF6+J16eC6TUo0stiWds2XsYH958AWJRwcHi5UL+Vgq1X6Z 
9GWYZVKlXNxBfGR5Zv1anmmaDP2ouJG3DwI5U8Dqe6B6dcGYQWtU+m1Hieuy5Ko= 
=BiO/ 
-----END PGP SIGNATURE----- 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/5422b956/attachment.htm>

From yvoinov at gmail.com  Tue Apr 21 21:34:49 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Apr 2015 03:34:49 +0600
Subject: [squid-users] problem in squid certificate installtion
In-Reply-To: <009a01d07cce$2f451c70$8dcf5550$@netstream.ps>
References: <008601d07caa$e25f3800$a71da800$@netstream.ps>
 <55369492.4040103@gmail.com> <009a01d07cce$2f451c70$8dcf5550$@netstream.ps>
Message-ID: <5536C279.1020604@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Said it was enough to understand. :)

I strictly recommend to start from here:

http://en.wikipedia.org/wiki/Public_key_infrastructure


22.04.15 13:30, snakeeyes ?????:
> Hmmm ,  cant u  provide more info??
>
> I followed wiki
>
>
http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate
>
> 
>
> but im still confused with certificates , if possible and don?t mind ,
could u tell me brief steps ?
>
> 
>
> thanks a lot for ur kind help
>
> 
>
> regards
>
> 
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Tuesday, April 21, 2015 11:19 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] problem in squid certificate installtion
>
> 
>
>
> Self-signed certificate is not suitable for use in a reverse proxy.
>
> 22.04.15 9:17, snakeeyes ?????:
> > Hi
>
>
>
>
>
>
>
>       > I need to setup squid proxy as reverse proxy with https
>
>       enabled
>
>
>
>
>
>
>
>       > I tried  the bash script below and it run ok :
>
>
>
>
>
>
>
>       > ###########################
>
>
>
>
>
>
>
>       > OPENSSL=/usr/bin/openssl
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > SSLDIR=/etc/mydlp/ssl
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > mkdir -p $SSLDIR || exit 1
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > rm -rf $SSLDIR/*
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > [ -e $SSLDIR/private.pem ] || $OPENSSL genrsa 4096 >
>
>       $SSLDIR/private.pem
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > [ -e $SSLDIR/public.pem ] || (echo -e
>
>
>
>
>
>       
<mailto:TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n>
"TR\nAnkara\nTechnopolis\nMyDLP\nMyDLP\n*\nsupport at mydlp.com\n"|
>
>       $OPENSSL
>
>
>
>       > req -new -x509 -days 3650 -key $SSLDIR/private.pem -out
>
>       $SSLDIR/public.pem)
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > [ -e $SSLDIR/user.der ] || $OPENSSL x509 -in
>
>       $SSLDIR/public.pem -outform DER
>
>
>
>       > -out $SSLDIR/user.der
>
>
>
>
>
>
>
>       > ######################################
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > ls -l /etc/mydlp/ssl
>
>
>
>
>
>
>
>       > total 12
>
>
>
>
>
>
>
>       > -rw-r--r-- 1 root root 3243 Apr 21 08:26 private.pem
>
>
>
>
>
>
>
>       > -rw-r--r-- 1 root root 2090 Apr 21 08:26 public.pem
>
>
>
>
>
>
>
>       > -rw-r--r-- 1 root root 1501 Apr 21 08:27 user.der
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > ######################################
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Added to squid.conf :
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > https_port 443 key=/etc/mydlp/ssl/private.pem
>
>       cert=/etc/mydlp/ssl/public.pem
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > And when I start squid ,
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > FATAL: No valid signing SSL certificate configured for
>
>       HTTPS_port [::]:443
>
>
>
>
>
>
>
>       > Squid Cache (Version 3.5.1): Terminated abnormally.
>
>
>
>
>
>
>
>       > CPU Usage: 10.189 seconds = 10.133 user + 0.056 sys
>
>
>
>
>
>
>
>       > Maximum Resident Size: 271264 KB
>
>
>
>
>
>
>
>       > Page faults with physical i/o: 44
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > Hope to help
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > regards
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>
>
>       > squid-users mailing list
>
>
>
>       > squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
>
>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVNsJ5AAoJENNXIZxhPexGE7IH/3JpGhiwEg2puuCiCxOu81re
GcldpwyP3rdJ5TRF/IxFV1K++a+lNDvppVORQwLCpFX6uY3XeBh2Lsn4lsenpV7n
dGBIcKm4eP34ko8EAyjFjKcpoyF9ocl6ygX7XlVgqEE6PYZZG+GJOz2DOPe2u3kg
RWPQjFLHY0DLKgFTj9h3/uLb+6D+opTYH+5dN3vkuf0jAAuQuGaCz9F5wbnxu8Q9
G2zvWqmRbye2hd3ukHbPY0wRyjHZCiFMBF5Q69ciJJzOqDjPd5+0tkg+o+9AcznL
1Q4gZQADMdf3RcDZ42HhINxoqSeiBiRw8SP67/XATb38giwc1/pppVbgMHGBPOc=
=tltw
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/4cfd54a8/attachment.htm>

From alex at samad.com.au  Wed Apr 22 01:31:33 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 22 Apr 2015 11:31:33 +1000
Subject: [squid-users] peek- splice and client side certs
Message-ID: <CAJ+Q1PXVtpg_q9+VaRgr2zLb+GRQ8ZeWUisbLCXpe8KKmqthWQ@mail.gmail.com>

I presume ssl proxy (peek/splice) doesn't work with client certs ?

Alex


From squid3 at treenet.co.nz  Wed Apr 22 03:01:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 15:01:03 +1200
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <1429644716040-4670856.post@n4.nabble.com>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz> <1429620216887-4670852.post@n4.nabble.com>
 <55364F9C.9080908@treenet.co.nz> <1429644716040-4670856.post@n4.nabble.com>
Message-ID: <55370EEF.3060809@treenet.co.nz>

On 22/04/2015 7:31 a.m., jaykbvt wrote:
> Hi Amos,
> 
> Thanks for reply,
> 
> I think I got ur point. If I understood correctly,
> 
> if a user makes request for http://www.wikipedia.org then the client request
> header should look like:
> 
> src: client_IP:random_port
> dst: wikipedia.org(ip_address):http
> http request: http_request details. (host,url,etc..)
> 
> and squid should get the packet like that.

correct.

> 
> But since Cisco ISG is in between which seems to be changing the client
> request header like:
> 
> src: client_IP:random_port
> dst: squid_IP:http
> http request: http_request details. (host,url,etc..)
> 
> and eventually squid fails to understand where to send http_request.

correct.

> 
> And thats why we should look at cisco ISG config.

yes.

> 
> my iptables config looks like:
> 
> iptables -t nat -A PREROUTING -s 10.58.200.33 -p tcp --dport 80 -j ACCEPT
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
> 10.58.200.33:3129
> iptables -t nat -A POSTROUTING -j MASQUERADE
> iptables -t mangle -A PREROUTING -p tcp --dport 3129 -j DROP
> 

And correct.

Thats all we can help with I'm afraid until at least the Cisco issue is
resolved.

Amos


From squid3 at treenet.co.nz  Wed Apr 22 03:06:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 15:06:34 +1200
Subject: [squid-users] peek- splice and client side certs
In-Reply-To: <CAJ+Q1PXVtpg_q9+VaRgr2zLb+GRQ8ZeWUisbLCXpe8KKmqthWQ@mail.gmail.com>
References: <CAJ+Q1PXVtpg_q9+VaRgr2zLb+GRQ8ZeWUisbLCXpe8KKmqthWQ@mail.gmail.com>
Message-ID: <5537103A.9000907@treenet.co.nz>

On 22/04/2015 1:31 p.m., Alex Samad wrote:
> I presume ssl proxy (peek/splice) doesn't work with client certs ?

peek/splice works with client certs.

peek to get the SNI and other publicly available details from TLS then
splice to pass that client data to the server is precisely the feature
designed to cope with client certs, unknown ciphers and other issues.
 NP: splicing is *not* decrypting the traffic.

*bump* action (or the deprecated client-first/server-first) is the one
that will not work properly with client certs.

Amos




From alberto2perez at gmail.com  Wed Apr 22 03:20:18 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Tue, 21 Apr 2015 23:20:18 -0400
Subject: [squid-users] Help with cachemgr and nginx
Message-ID: <CAMZauGrdH8Ga6tX1t7oEgxXa3myKuASTDzNraYtm+2P2TPuW+w@mail.gmail.com>

Hi everyone,

I am trying to setup cachemgr.cgi in my squid 3.5.2 box with nginx as web
server. I was able to server first page of cachemgr but I can't get further
this page, every credentials I try shows me the same page with no errors.
(attach image)


in my squid.conf file

cache_mgr alberto
cachemgr_passwd managerforsquid40a all

nginx config

location /cgi-bin/ {
                alias /usr/local/squid/libexec;
                index cachemgr.cgi;
                gzip off;
                #include /etc/nginx/fastcgi_params;
                fastcgi_pass unix:/var/run/fcgiwrap.socket;
                fastcgi_param  SCRIPT_FILENAME
/usr/local/squid/libexec/cachemgr.cgi;
}

I successfully run other cgi perl scripts with that same configuration
(squish.cgi, but this one is only one page, so i could be missing something
here)

No access denied from nginx response so I asume this is not the problem
here.


Have anyone experience with a similar setup.

Thanks a lot in advance for the time.

Alberto
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screenshot from 2015-04-21 22:26:28.png
Type: image/png
Size: 23767 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150421/0a4420c2/attachment.png>

From squid3 at treenet.co.nz  Wed Apr 22 03:45:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 15:45:01 +1200
Subject: [squid-users] Help with cachemgr and nginx
In-Reply-To: <CAMZauGrdH8Ga6tX1t7oEgxXa3myKuASTDzNraYtm+2P2TPuW+w@mail.gmail.com>
References: <CAMZauGrdH8Ga6tX1t7oEgxXa3myKuASTDzNraYtm+2P2TPuW+w@mail.gmail.com>
Message-ID: <5537193D.4090100@treenet.co.nz>

On 22/04/2015 3:20 p.m., Alberto Perez wrote:
> Hi everyone,
> 
> I am trying to setup cachemgr.cgi in my squid 3.5.2 box with nginx as web
> server. I was able to server first page of cachemgr but I can't get further
> this page, every credentials I try shows me the same page with no errors.
> (attach image)

Try accessing it directly via squidclient to make sure the Squid part is
working:

  squidclient mgr:menu at managerforsquid40a


> 
> 
> in my squid.conf file
> 
> cache_mgr alberto

Above is supposed to be the email address of the admin. Its listed in
error pages for user help, and gets crash reports emailed to it by Squid.

> cachemgr_passwd managerforsquid40a all
> 

Check that you dont have any other cachemgr_passwd affecting things.
Several passwords can be assigned with different reports named.


Do you also have this?
 http_access allow localhost manager

The username does not matter to the cachemgr its just there for logging
and to allow integration with proxy_auth if you are using that. The
password however must be the one configured in cachemgr_passwd (ie.
managerforsquid40a).



> nginx config
> 
> location /cgi-bin/ {
>                 alias /usr/local/squid/libexec;
>                 index cachemgr.cgi;
>                 gzip off;
>                 #include /etc/nginx/fastcgi_params;
>                 fastcgi_pass unix:/var/run/fcgiwrap.socket;
>                 fastcgi_param  SCRIPT_FILENAME
> /usr/local/squid/libexec/cachemgr.cgi;
> }
> 
> I successfully run other cgi perl scripts with that same configuration
> (squish.cgi, but this one is only one page, so i could be missing something
> here)
> 
> No access denied from nginx response so I asume this is not the problem
> here.

The squidclient test above will confirm or deny that. If squidclient
works then the problem is in how NginX FastCGI is connecting to the proxy.


Amos


From squid3 at treenet.co.nz  Wed Apr 22 03:49:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2015 15:49:36 +1200
Subject: [squid-users] problem in squid certificate installtion
In-Reply-To: <009a01d07cce$2f451c70$8dcf5550$@netstream.ps>
References: <008601d07caa$e25f3800$a71da800$@netstream.ps>
 <55369492.4040103@gmail.com> <009a01d07cce$2f451c70$8dcf5550$@netstream.ps>
Message-ID: <55371A50.5080304@treenet.co.nz>

On 22/04/2015 7:30 p.m., snakeeyes wrote:
> Hmmm ,  cant u  provide more info??
> 
> I followed wiki 
> 
> http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate
> 
>  
> 
> but im still confused with certificates , if possible and don?t mind , could u tell me brief steps ?


1) generate certificates

2) configure Squid

3) watch traffic flow.


Not much help aye? The wiki page is as dumbed-down as we can make it for
newbies without omitting important details needed to actually work.

NP: though I did see it went too far and omitted the https_port "accel"
mode parameter. Adding that your Squid will check for reverse-proxy
capabilitues instead of ssl-bump capabilities in the certs.


As Yuri said learning about how PKI works is somewhat important now
that you are dealing with it as more just than a user. Particularly how
to create the right types of certificate for the task it is supposed to
do is critical.

Amos



From nathan at getoffmalawn.com  Wed Apr 22 04:16:02 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Wed, 22 Apr 2015 14:16:02 +1000
Subject: [squid-users] assertion failed: comm.cc:557: "F->flags.open"
In-Reply-To: <1429254383156-4670788.post@n4.nabble.com>
References: <1429254383156-4670788.post@n4.nabble.com>
Message-ID: <CAGUJm7asdf3K7VPgm6rnfciH+38n3=Oyz2eJNUM1hbd=wLjEgA@mail.gmail.com>

Hi Fred,

I believe you're experiencing bug 3329:
http://bugs.squid-cache.org/show_bug.cgi?id=3329

Please try the patch that I have on there and see if that helps your issue.

Thank you,

Nathan.

On 17 April 2015 at 17:06, Stakres <vdoctor at neuf.fr> wrote:
> Hi All,
>
> Is anyone with an trick regarding this error message in the cache.log ?
> *assertion failed: comm.cc:557: "F->flags.open"*
>
> Squid *3.5.3-20150415-r13798*.
> config in diskd, tproxy and ssl_bump.
>
> when the squid faces this error, it reloads itself but it breaks the surf
> for a while.
>
> Thanks in advance.
>
> Bye Fred
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-F-flags-open-tp4670788.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From hierony_milanisti at yahoo.co.id  Wed Apr 22 04:18:43 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Wed, 22 Apr 2015 04:18:43 +0000 (UTC)
Subject: [squid-users] DNS Server Issued in Squid Proxy
In-Reply-To: <mailman.1397.1429674820.2892.squid-users@lists.squid-cache.org>
References: <mailman.1397.1429674820.2892.squid-users@lists.squid-cache.org>
Message-ID: <187972527.1118886.1429676323410.JavaMail.yahoo@mail.yahoo.com>

?Hierony Manurung
Del Institute of Technology
Network Management

      Pada Rabu, 22 April 2015 10:53, "squid-users-owner at lists.squid-cache.org" <squid-users-owner at lists.squid-cache.org> menulis:
   

 ----- Pesan yang Diteruskan -----

You are not allowed to post to this mailing list, and your message has
been automatically rejected.? If you think that your messages are
being rejected in error, contact the mailing list owner at
squid-users-owner at lists.squid-cache.org.


Hello Fellows,
I have configured a Squid Proxy with using DNS Server from Mikrotik Router.
I am set Mikrotik IP as the DNS Server in the /etc/resolv.conf file.

I tried to Access an url (www.google.com) it works, and when i nslookup the www.google.com it replies too.

But, when i tried to access an url from browser (Before it, i set the proxy in the Browser).
I am found an unable get IP Address Error.

Please help me figured what's wrong here.
For your attention, I say thanks.


?Hierony Manurung
Del Institute of Technology
Network Management

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/a7ce07cd/attachment.htm>
-------------- next part --------------
An embedded message was scrubbed...
From: Hierony Manurung <hierony_milanisti at yahoo.co.id>
Subject: DNS Server Issued in Squid Proxy
Date: Wed, 22 Apr 2015 03:53:33 +0000 (UTC)
Size: 5018
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/a7ce07cd/attachment.eml>

From yvoinov at gmail.com  Wed Apr 22 10:03:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Apr 2015 16:03:28 +0600
Subject: [squid-users] DNS Server Issued in Squid Proxy
In-Reply-To: <187972527.1118886.1429676323410.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.1397.1429674820.2892.squid-users@lists.squid-cache.org>
 <187972527.1118886.1429676323410.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <553771F0.2070605@gmail.com>

Clients must also be pointed to these DNS IP.

22.04.15 10:18, Hierony Manurung ?????:
> Hierony Manurung
> Del Institute of Technology
> Network Management
>
>
> Pada Rabu, 22 April 2015 10:53, 
> "squid-users-owner at lists.squid-cache.org" 
> <squid-users-owner at lists.squid-cache.org> menulis:
>
>
> ----- Pesan yang Diteruskan -----
>
> You are not allowed to post to this mailing list, and your message has
> been automatically rejected.  If you think that your messages are
> being rejected in error, contact the mailing list owner at
> squid-users-owner at lists.squid-cache.org. 
> <mailto:squid-users-owner at lists.squid-cache.org.>
>
>
> Hello Fellows,
> I have configured a Squid Proxy with using DNS Server from Mikrotik 
> Router.
> I am set Mikrotik IP as the DNS Server in the /etc/resolv.conf file.
>
> I tried to Access an url (www.google.com) it works, and when i 
> nslookup the www.google.com <http://www.google.com/> it replies too.
>
> But, when i tried to access an url from browser (Before it, i set the 
> proxy in the Browser).
> I am found an unable get IP Address Error.
>
> Please help me figured what's wrong here.
> For your attention, I say thanks.
>
>
> Hierony Manurung
> Del Institute of Technology
> Network Management
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/7c2a2196/attachment.htm>

From hack.back at hotmail.com  Wed Apr 22 19:57:23 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 22 Apr 2015 12:57:23 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429613172522-4670847.post@n4.nabble.com>
References: <1429189971025-4670781.post@n4.nabble.com>
 <552FBB85.5060009@treenet.co.nz> <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz> <1429543953483-4670835.post@n4.nabble.com>
 <1429613172522-4670847.post@n4.nabble.com>
Message-ID: <1429732643497-4670868.post@n4.nabble.com>

no one can find solution for this issue ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670868.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ncrogers at gmail.com  Wed Apr 22 21:14:44 2015
From: ncrogers at gmail.com (Nick Rogers)
Date: Wed, 22 Apr 2015 14:14:44 -0700
Subject: [squid-users] WARNING: Tos value ... adjusted
Message-ID: <CAKOb=YYbTitQeLLyUUW+DPBNrmFOOhnu3YzHNVxFoWaY4Ge7sA@mail.gmail.com>

After upgrading from 3.4.x to 3.5.x, I've noticed a new error message with
my squid configuration. Apparently squid 3.5 no longer allows setting the
two lower-most ECN bits of the ToS byte. I realize that this is to
encourage people to use the modernized definition of ToS being a 6 bit DSCP
field and a two bit ECN field, however enforcing this in the configuration
parser introduces a big problem for me. I use the ToS byte as a way to tag
outbound requests from squid based on ACLs, to then queue the traffic
differently via PF + ALTQ engine on FreeBSD. This is a way to queue upload
traffic from squid on a per-IP/ACL basis. Having 256 values (8 bit ToS) to
work with is a lot preferable than only 64 (6 bit DSCP field). I'm not sure
if I care if the ECN bits are set, as it is likely scrubbed away by my
upstream ISP, or I can normalize it with PF. The point is I really need to
be able to tag outgoing packets with the full 8 bits of the ToS byte, not
just 6. I've been doing this for nearly a decade since early squid 2.x.

So my question is, is the squid team willing to revert this change
entirely, and leave it up to the admin to decide what ToS values are
appropriate. Or are you guys now pretty adamant about never setting the ECN
field?

It looks like the change got snuck into this commit along with fixing some
other tcp outgoing tos bugs. It really caught me off-guard because there is
no mention of this new behavior in the changelog.

https://github.com/squid-cache/squid/commit/651ba437462fb5fde21f8d3b66d09afa1e069d5c

The enforcement happens in src/cache_cf.cc. Its 5 lines of code that seem
trivial to either remove or make configurable, however I am not sure how to
go about making it configurable. I am debating just patching my squid
builds to remove the check against the ToS field, but I would like to use
unmodified squid 3.5.x. I'm not sure if going from 256 to 64 possible
fields is possible for my deployments.

Thanks.

-Nick
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150422/a3237827/attachment.htm>

From hack.back at hotmail.com  Wed Apr 22 21:28:24 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 22 Apr 2015 14:28:24 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429732643497-4670868.post@n4.nabble.com>
References: <552FBB85.5060009@treenet.co.nz>
 <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz> <1429543953483-4670835.post@n4.nabble.com>
 <1429613172522-4670847.post@n4.nabble.com>
 <1429732643497-4670868.post@n4.nabble.com>
Message-ID: <1429738104097-4670870.post@n4.nabble.com>

after discussing this with Yuri we have thinking is the problem in 
close-wait connections,
i try to put half_closed_clients off
but this didnt solve the problem
some thing here make this assertion error since it not happen immediately ,
some times every 20 min and some times it need 5 hours to exist 
now i set 
/proc/sys/net/ipv4/tcp_keepalive_time = 900
since it is 7200 by default in linux

but after watching netstat to close connection i got this
Every 2.0s: netstat -antop | grep -iE --regexp="close"                                                
Thu Apr 23 00:42:07 2015

tcp    18382      0 192.168.50.4:37692      216.58.210.226:80      
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp        1      0 192.168.70.4:3127       192.168.70.11:60376    
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp        1      0 192.168.70.4:3128       192.168.70.5:1810      
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp        1      0 192.168.70.4:3127       192.168.70.11:60375    
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp        1      0 192.168.50.4:56114      205.185.208.26:80      
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp        1      0 192.168.70.4:3128       192.168.70.11:42139    
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp    27099      0 192.168.50.4:35004      216.58.210.226:80      
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)
tcp    42209      0 192.168.50.4:42235      68.232.34.189:80       
CLOSE_WAIT  2619/(squid-1)   off (0.00/0/0)


and most of these ip's is in none ssl bump since no relation ...
i dont know if this will be more helpfull ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670870.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Apr 22 21:42:15 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 22 Apr 2015 14:42:15 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429738104097-4670870.post@n4.nabble.com>
References: <1429211883832-4670785.post@n4.nabble.com>
 <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz> <1429543953483-4670835.post@n4.nabble.com>
 <1429613172522-4670847.post@n4.nabble.com>
 <1429732643497-4670868.post@n4.nabble.com>
 <1429738104097-4670870.post@n4.nabble.com>
Message-ID: <1429738935318-4670871.post@n4.nabble.com>

root at fibernet:~# netstat -anl | grep 80 | awk '/^tcp/ {t[$NF]++}END{for(state
in t){print state, t[state]} }'
FIN_WAIT2 1
LISTEN 1
CLOSE_WAIT 7
TIME_WAIT 71
ESTABLISHED 125
LAST_ACK 2
FIN_WAIT1 2


Every 2.0s: netstat -antop | grep -iE --regexp="TIME_WAIT"                                                                                                                  
Thu Apr 23 00:55:30 2015

tcp        0      0 192.168.50.4:51215      173.194.19.46:80       
TIME_WAIT   -                timewait (26.22/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.10:50743    
TIME_WAIT   -                timewait (44.26/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:39998    
TIME_WAIT   -                timewait (46.49/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.10:55848    
TIME_WAIT   -                timewait (43.78/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:52532    
TIME_WAIT   -                timewait (56.49/0/0)
tcp        0      0 192.168.50.4:42185      216.58.210.238:80      
TIME_WAIT   -                timewait (7.83/0/0)
tcp        0      0 192.168.50.4:58706      216.58.210.227:443     
TIME_WAIT   -                timewait (42.02/0/0)
tcp        0      0 192.168.50.4:42561      216.58.210.238:80      
TIME_WAIT   -                timewait (38.87/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.5:53853     
TIME_WAIT   -                timewait (23.04/0/0)
tcp        0      0 192.168.50.4:58811      216.58.210.227:443     
TIME_WAIT   -                timewait (46.92/0/0)
tcp        0      0 192.168.70.4:3127       192.168.70.2:62584     
TIME_WAIT   -                timewait (53.05/0/0)
tcp        0      0 192.168.50.4:42527      216.58.210.238:80      
TIME_WAIT   -                timewait (37.18/0/0)
tcp        0      0 192.168.50.4:42314      216.58.210.238:80      
TIME_WAIT   -                timewait (26.43/0/0)
tcp        0      0 192.168.50.4:55684      216.58.210.227:80      
TIME_WAIT   -                timewait (17.38/0/0)
tcp        0      0 192.168.50.4:58818      216.58.210.227:443     
TIME_WAIT   -                timewait (48.92/0/0)
tcp        0      0 192.168.70.4:3127       192.168.70.2:62570     
TIME_WAIT   -                timewait (33.05/0/0)
tcp        0      0 192.168.70.4:3127       192.168.70.2:62583     
TIME_WAIT   -                timewait (53.03/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.5:7455      
TIME_WAIT   -                timewait (8.54/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.5:57256     
TIME_WAIT   -                timewait (29.57/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:34464    
TIME_WAIT   -                timewait (8.40/0/0)
tcp        0      0 192.168.50.4:42526      216.58.210.238:80      
TIME_WAIT   -                timewait (38.41/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.10:55847    
TIME_WAIT   -                timewait (43.78/0/0)
tcp        0      0 192.168.50.4:38569      216.58.210.206:443     
TIME_WAIT   -                timewait (3.68/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:54650    
TIME_WAIT   -                timewait (30.83/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.2:2878      
TIME_WAIT   -                timewait (25.28/0/0)
tcp        0      0 192.168.50.4:42206      216.58.210.238:80      
TIME_WAIT   -                timewait (11.61/0/0)
tcp        0      0 192.168.70.4:3127       192.168.70.2:62580     
TIME_WAIT   -                timewait (43.26/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.10:33557    
TIME_WAIT   -                timewait (23.61/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:1993     
TIME_WAIT   -                timewait (54.87/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.2:59544     
TIME_WAIT   -                timewait (4.64/0/0)
tcp        0      0 192.168.50.4:34515      173.194.19.10:80       
TIME_WAIT   -                timewait (19.42/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.5:53841     
TIME_WAIT   -                timewait (3.04/0/0)
tcp        0      0 192.168.50.4:38739      216.58.210.206:443     
TIME_WAIT   -                timewait (36.13/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.11:41881    
TIME_WAIT   -                timewait (53.06/0/0)
tcp        0      0 192.168.50.4:42516      216.58.210.238:80      
TIME_WAIT   -                timewait (36.41/0/0)
tcp        0      0 192.168.50.4:51155      173.194.19.46:80       
TIME_WAIT   -                timewait (22.97/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.2:49507     
TIME_WAIT   -                timewait (9.66/0/0)
tcp        0      0 192.168.50.4:42239      216.58.210.238:80      
TIME_WAIT   -                timewait (14.34/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.10:55849    
TIME_WAIT   -                timewait (43.78/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:59221    
TIME_WAIT   -                timewait (14.28/0/0)
tcp        0      0 192.168.50.4:40090      37.58.102.37:80        
TIME_WAIT   -                timewait (23.46/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.11:41866    
TIME_WAIT   -                timewait (53.06/0/0)
tcp        0      0 192.168.50.4:50551      173.194.19.78:80       
TIME_WAIT   -                timewait (59.39/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.2:60076     
TIME_WAIT   -                timewait (50.17/0/0)
tcp        0      0 192.168.50.4:42558      216.58.210.238:80      
TIME_WAIT   -                timewait (47.47/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.5:53868     
TIME_WAIT   -                timewait (23.04/0/0)
tcp        0      0 192.168.50.4:49394      173.194.15.210:80      
TIME_WAIT   -                timewait (1.89/0/0)
tcp        0      0 192.168.70.4:3128       192.168.70.12:40497    
TIME_WAIT   -                timewait (8.72/0/0)
tcp        0      0 192.168.50.4:57808      216.58.210.228:443     
TIME_WAIT   -                timewait (44.74/0/0)





also all the ips here in time_wait is the ip's that i put them in none bump
and 192.168.70.x is my servers ip's connected to squid server ...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670871.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Apr 22 21:45:21 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 22 Apr 2015 14:45:21 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1429738935318-4670871.post@n4.nabble.com>
References: <1429277990509-4670794.post@n4.nabble.com>
 <CAGUJm7YPLewDLGZTLNHjSyy8w5kYgm6bFeo=wKB_ifPxtDWrMw@mail.gmail.com>
 <1429346956458-4670803.post@n4.nabble.com>
 <CAGUJm7bJDGnzrMJ08xPq5SfMu--txSAM_L8VW7_=_qsP4h16_A@mail.gmail.com>
 <5533B6A9.10209@treenet.co.nz> <1429543953483-4670835.post@n4.nabble.com>
 <1429613172522-4670847.post@n4.nabble.com>
 <1429732643497-4670868.post@n4.nabble.com>
 <1429738104097-4670870.post@n4.nabble.com>
 <1429738935318-4670871.post@n4.nabble.com>
Message-ID: <1429739121159-4670872.post@n4.nabble.com>

root at issa:~# netstat -anl | grep 80 | awk '/^tcp/ {t[$NF]++}END{for(state in
t){print state, t[state]} }'
FIN_WAIT2 12
LISTEN 1
CLOSE_WAIT 3
TIME_WAIT 809
ESTABLISHED 597
LAST_ACK 9
SYN_SENT 4
FIN_WAIT1 14




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670726p4670872.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Thu Apr 23 04:35:43 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 23 Apr 2015 07:35:43 +0300
Subject: [squid-users] DNS Server Issued in Squid Proxy
In-Reply-To: <187972527.1118886.1429676323410.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.1397.1429674820.2892.squid-users@lists.squid-cache.org>
 <187972527.1118886.1429676323410.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5538769F.9000906@ngtech.co.il>

Here it's working fine in both intercept,tproxy and forward proxy mode.
What version are you using?
What OS?
Did you had the chance to see some access.log output??

Eliezer

* This is one of the days we can say thanks for the first squid 
developers which helped us with this great piece of software.

On 22/04/2015 07:18, Hierony Manurung wrote:
>   Hierony Manurung
> Del Institute of Technology
> Network Management
>
>        Pada Rabu, 22 April 2015 10:53, "squid-users-owner at lists.squid-cache.org" <squid-users-owner at lists.squid-cache.org> menulis:
>
>
>   ----- Pesan yang Diteruskan -----
>
> You are not allowed to post to this mailing list, and your message has
> been automatically rejected.  If you think that your messages are
> being rejected in error, contact the mailing list owner at
> squid-users-owner at lists.squid-cache.org.
>
>
> Hello Fellows,
> I have configured a Squid Proxy with using DNS Server from Mikrotik Router.
> I am set Mikrotik IP as the DNS Server in the /etc/resolv.conf file.
>
> I tried to Access an url (www.google.com) it works, and when i nslookup the www.google.com it replies too.
>
> But, when i tried to access an url from browser (Before it, i set the proxy in the Browser).
> I am found an unable get IP Address Error.
>
> Please help me figured what's wrong here.
> For your attention, I say thanks.
>
>
>   Hierony Manurung
> Del Institute of Technology
> Network Management
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From sudakov at sibptus.tomsk.ru  Thu Apr 23 05:10:50 2015
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Thu, 23 Apr 2015 11:10:50 +0600
Subject: [squid-users] ERR_ONLY_IF_CACHED_MISS and cache digests problem
Message-ID: <20150423051050.GA88687@admin.sibptus.tomsk.ru>

Dear Colleagues,

I have two squid-3.5.3 proxies configured as mutual siblings:

==== proxy:/usr/local/etc/squid/squid.conf

acl internal src 212.73.124.0/28
http_access allow internal
cache_peer proxy2.sibptus.ru sibling 3128 3130

==== proxy2:/usr/local/etc/squid/squid.conf

acl cisa_servers src 212.73.124.0/28
http_access allow cisa_servers
cache_peer proxy.sibptus.ru sibling 3128 3130

I believe this is a correct configuration. The users' browsers are
configured to use proxy2.sibptus.ru, but sometimes users see the
ERR_ONLY_IF_CACHED_MISS error from proxy.sibptus.ru!

Both proxies are compiled with cache digest support. I think it's on
by default. However, the problem seems to disappear when I add the
"no-digest" flag to the cache_peer definitions.

What is wrong with my setup? Is anything wrong with mutual siblings
and cache digests?

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From squid3 at treenet.co.nz  Thu Apr 23 05:35:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2015 17:35:47 +1200
Subject: [squid-users] WARNING: Tos value ... adjusted
In-Reply-To: <CAKOb=YYbTitQeLLyUUW+DPBNrmFOOhnu3YzHNVxFoWaY4Ge7sA@mail.gmail.com>
References: <CAKOb=YYbTitQeLLyUUW+DPBNrmFOOhnu3YzHNVxFoWaY4Ge7sA@mail.gmail.com>
Message-ID: <553884B3.9010809@treenet.co.nz>

On 23/04/2015 9:14 a.m., Nick Rogers wrote:
> After upgrading from 3.4.x to 3.5.x, I've noticed a new error message with
> my squid configuration. Apparently squid 3.5 no longer allows setting the
> two lower-most ECN bits of the ToS byte.

Allowing it and leaving it up to admin was causing too much confusion
over mysteriously dropped traffic.

Attempts to bit-shift values were not backward compatible. So instead we
opted to take a bitmask hex value as the TOS parameter and explicitly
forbid with warning the ECN bits being set in that mask.


> I realize that this is to
> encourage people to use the modernized definition of ToS being a 6 bit DSCP
> field and a two bit ECN field, however enforcing this in the configuration
> parser introduces a big problem for me. I use the ToS byte as a way to tag
> outbound requests from squid based on ACLs, to then queue the traffic
> differently via PF + ALTQ engine on FreeBSD. This is a way to queue upload
> traffic from squid on a per-IP/ACL basis. Having 256 values (8 bit ToS) to
> work with is a lot preferable than only 64 (6 bit DSCP field). I'm not sure
> if I care if the ECN bits are set, as it is likely scrubbed away by my
> upstream ISP,

ECN is an end-to-end property of the packets. It does not get scrubbed
away. Your ISP probably uses MPLS these days, so these packets existing
your network with ECN set cause your network to be marked as a congested
zone - any packets going from another congested network to yours get
*dropped* until the congestion in one of the networks gets resolved. It
can also trigger nasty actions such as TCP throttling on the other end
of the connection.

NP: Do you find your ISP service is "a little shitty" with packet loss
and traffic slowing down occasionally? ECN and/or ICMP congestion
control breakage is often the cause. Could be self-inflicted.


> or I can normalize it with PF. The point is I really need to
> be able to tag outgoing packets with the full 8 bits of the ToS byte, not
> just 6. I've been doing this for nearly a decade since early squid 2.x.
> 
> So my question is, is the squid team willing to revert this change
> entirely, and leave it up to the admin to decide what ToS values are
> appropriate. Or are you guys now pretty adamant about never setting the ECN
> field?

ECN is pretty standard these days. Any casual update or upgrade to the
machine software, hardware, or configuration anywhere in the network(s)
it communicates over could result in unwanted traffic loss or worse
behaviour.

If you are in the practice of using those bits in the TOS value you are
of course free to patch the mask removal yourself, but we wont be
publishing new versions of Squid that touch the ECN bits - ie. not while
IPv4 is still in wide use.

IPv6 handles DSCP+ECN better and is artificially limited by this. But we
feel that is justified temporarily to avoid confusion and problems when
mapping DSCP between IPv4 and IPv6 traffic.


> 
> It looks like the change got snuck into this commit along with fixing some
> other tcp outgoing tos bugs. It really caught me off-guard because there is
> no mention of this new behavior in the changelog.
> 
> https://github.com/squid-cache/squid/commit/651ba437462fb5fde21f8d3b66d09afa1e069d5c
> 

The TOS directives have always been documented as being for the
DiffServ/DSCP values. The only reason they ever allowed those ECN bits
to be set in the first place is that they pre-date ECNs existence.

Sorry that we missed out the documentation update mentioning that
change. A note is going in now.


> The enforcement happens in src/cache_cf.cc. Its 5 lines of code that seem
> trivial to either remove or make configurable, however I am not sure how to
> go about making it configurable. I am debating just patching my squid
> builds to remove the check against the ToS field, but I would like to use
> unmodified squid 3.5.x. I'm not sure if going from 256 to 64 possible
> fields is possible for my deployments.

It better be. Any software not supporting ECN will have problems
operating over VPN, IPSEC or MLPS.

NOTE: We have the NETMASK feature made available on Linux now for
systems that need large numbers of packet flow tag values within the
local machine. I'm not aware of any equivalent on non-Linux systems, but
if they exist in PF (other than setting the TOS bits) then we are open
to supporting that.

Amos




From squid3 at treenet.co.nz  Thu Apr 23 05:48:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2015 17:48:51 +1200
Subject: [squid-users] ERR_ONLY_IF_CACHED_MISS and cache digests problem
In-Reply-To: <20150423051050.GA88687@admin.sibptus.tomsk.ru>
References: <20150423051050.GA88687@admin.sibptus.tomsk.ru>
Message-ID: <553887C3.30106@treenet.co.nz>

On 23/04/2015 5:10 p.m., Victor Sudakov wrote:
> Dear Colleagues,
> 
> I have two squid-3.5.3 proxies configured as mutual siblings:
> 
> ==== proxy:/usr/local/etc/squid/squid.conf
> 
> acl internal src 212.73.124.0/28
> http_access allow internal
> cache_peer proxy2.sibptus.ru sibling 3128 3130
> 
> ==== proxy2:/usr/local/etc/squid/squid.conf
> 
> acl cisa_servers src 212.73.124.0/28
> http_access allow cisa_servers
> cache_peer proxy.sibptus.ru sibling 3128 3130
> 
> I believe this is a correct configuration. The users' browsers are
> configured to use proxy2.sibptus.ru, but sometimes users see the
> ERR_ONLY_IF_CACHED_MISS error from proxy.sibptus.ru!
> 
> Both proxies are compiled with cache digest support. I think it's on
> by default. However, the problem seems to disappear when I add the
> "no-digest" flag to the cache_peer definitions.
> 
> What is wrong with my setup? Is anything wrong with mutual siblings
> and cache digests?
> 

I dont think anything is wrong wth either. Its more a collision in how
the features work vs the protocols.

Cache Digests (CD) are exchanged periodically and updated approx hourly.
Also they are based on just the URL. So there is always a gap where they
may not be accurate for any highly volatile objects, and variant objects
(using Vary headers) will have a high false-positive rate.

only-if-cached requires the *right now* state of the object to be fresh
and in cache. It takes account of both the URL and the entire HTTP
headers. So

The ICP protocol used as a backup to confirm objects existence also
suffers the same URL basis problem as CD. They work fine for HTTP/1.0
but HTTP/1.1 features dont fare quite so well.

My recommendation these days is to use HTCP instead of ICP. Since you
seem to need accuracy HTCP would probably suit your needs better than CD
as well.

Amos



From michael at hendrie.id.au  Thu Apr 23 06:29:01 2015
From: michael at hendrie.id.au (Michael Hendrie)
Date: Thu, 23 Apr 2015 15:59:01 +0930
Subject: [squid-users] ssl_bump peek in squid-3.5.3
Message-ID: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>

Hi All

I?ve been running squid-3.4.x in tproxy mode with ssl_bump server-first for some time and has been working great.

I have just moved to 3.5.3 to use peek to overcome some issues with sites that require SNI to serve up the correct certificate.  In most cases this is work well however I seem to have an issue that (so far) only effects the Safari web browser with certain sites.  As an example, https://twitter.com <https://twitter.com/> and https://www.openssl.org <https://www.openssl.org/> will result in a Safari error page ?can?t establish a secure connection with the server?.  There is also a correlating entry in the cache.log 'Error negotiating SSL connection on FD 45: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)?

Google shows some hits for this SSL error on other products, mostly nginx, but nothing suggesting in those posting seems to have worked for me (settings specific SSL/TLS versions and ciphers)

If use a different browser the above mentioned sites work as expected.  If continue to bump ?server-first? for these problem sites they also load as expected in Safari however I?m hoping to move to peek exclusively to overcome SNI issues.

Anyone experiencing the same thing or have any suggestions?  ssl_bump related config below:

https_port 8090 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl-bump.cer key=/etc/squid/ssl-bump.key
acl p8090 myportname 8090
acl step1 at_step SslBump1
#acl broken_peek dstdomain .twttr.com .twitter.com .facebook.com .openssl.org
#ssl_bump server-first broken_peek
ssl_bump peek step1
ssl_bump bump p8090

Thanks!

Michael


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/d029dd87/attachment.htm>

From squid3 at treenet.co.nz  Thu Apr 23 06:51:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2015 18:51:33 +1200
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
Message-ID: <55389675.3040702@treenet.co.nz>

On 23/04/2015 6:29 p.m., Michael Hendrie wrote:
> Hi All
> 
> I?ve been running squid-3.4.x in tproxy mode with ssl_bump
> server-first for some time and has been working great.
> 
> I have just moved to 3.5.3 to use peek to overcome some issues with
> sites that require SNI to serve up the correct certificate.  In most
> cases this is work well however I seem to have an issue that (so far)
> only effects the Safari web browser with certain sites.  As an
> example, https://twitter.com <https://twitter.com/> and
> https://www.openssl.org <https://www.openssl.org/> will result in a
> Safari error page ?can?t establish a secure connection with the
> server?.  There is also a correlating entry in the cache.log 'Error
> negotiating SSL connection on FD 45: error:140A1175:SSL
> routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)?

Please try the latest snapshot of 3.5 series. There are some TLS session
resume and SNI bug fixes.

Amos


From michael at hendrie.id.au  Thu Apr 23 06:58:13 2015
From: michael at hendrie.id.au (Michael Hendrie)
Date: Thu, 23 Apr 2015 16:28:13 +0930
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <55389675.3040702@treenet.co.nz>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
 <55389675.3040702@treenet.co.nz>
Message-ID: <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>


> On 23 Apr 2015, at 4:21 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 23/04/2015 6:29 p.m., Michael Hendrie wrote:
>> Hi All
>> 
>> I?ve been running squid-3.4.x in tproxy mode with ssl_bump
>> server-first for some time and has been working great.
>> 
>> I have just moved to 3.5.3 to use peek to overcome some issues with
>> sites that require SNI to serve up the correct certificate.  In most
>> cases this is work well however I seem to have an issue that (so far)
>> only effects the Safari web browser with certain sites.  As an
>> example, https://twitter.com <https://twitter.com/> and
>> https://www.openssl.org <https://www.openssl.org/> will result in a
>> Safari error page ?can?t establish a secure connection with the
>> server?.  There is also a correlating entry in the cache.log 'Error
>> negotiating SSL connection on FD 45: error:140A1175:SSL
>> routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)?
> 
> Please try the latest snapshot of 3.5 series. There are some TLS session
> resume and SNI bug fixes.

Thanks Amos, but I did try squid-3.5.3-20150420-r13802 before posting?.any other suggestions?

Michael

From michael at hendrie.id.au  Thu Apr 23 07:48:25 2015
From: michael at hendrie.id.au (Michael Hendrie)
Date: Thu, 23 Apr 2015 17:18:25 +0930
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
 <55389675.3040702@treenet.co.nz>
 <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>
Message-ID: <9890F667-FE7C-49B4-A11F-192F55233BDC@hendrie.id.au>


> On 23 Apr 2015, at 4:28 pm, Michael Hendrie <michael at hendrie.id.au> wrote:
> 
> 
>> On 23 Apr 2015, at 4:21 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> 
>> On 23/04/2015 6:29 p.m., Michael Hendrie wrote:
>>> Hi All
>>> 
>>> I?ve been running squid-3.4.x in tproxy mode with ssl_bump
>>> server-first for some time and has been working great.
>>> 
>>> I have just moved to 3.5.3 to use peek to overcome some issues with
>>> sites that require SNI to serve up the correct certificate.  In most
>>> cases this is work well however I seem to have an issue that (so far)
>>> only effects the Safari web browser with certain sites.  As an
>>> example, https://twitter.com <https://twitter.com/> and
>>> https://www.openssl.org <https://www.openssl.org/> will result in a
>>> Safari error page ?can?t establish a secure connection with the
>>> server?.  There is also a correlating entry in the cache.log 'Error
>>> negotiating SSL connection on FD 45: error:140A1175:SSL
>>> routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)?
>> 
>> Please try the latest snapshot of 3.5 series. There are some TLS session
>> resume and SNI bug fixes.
> 
> Thanks Amos, but I did try squid-3.5.3-20150420-r13802 before posting?.any other suggestions?
> 
> Michael

OK, I seem to have resolved this now, for the benefit of everyone else on the list:

In the above tests the generated certificate was being signed by a RootCA that was installed as trusted in the browser certificate store.  

I had previously noticed in my test environment (and thought completely unrelated) that bumped requests using the new peek/bump in 3.5.x were not sending the entire certificate chain to the browser but since they trusted the RootCA that was fine.  In my production environment however I use an IntermediateCA to sign the bumped requests, this causes a browser error as the clients only trust the RootCA.  As part of investigation to resolve this, I found that adding ?cafile=/path/to/signing_ca_bundle? to the ?https_port' line (which in my config is exactly the same file as ?cert=?) that all certs are sent to the client, and I no longer face the issue with Safari and https://twitter.com <https://twitter.com/> or https://www.openssl.org <https://www.openssl.org/> regardless of using RootCA or InterCA to sign bumped requests.

Not sure why but ?ssl_bump server-first? sends the entire chain without specifying ?cafile=? and ?ssl_bump peek/bump? doesn?t?but anyway my problem is solved!

Michael

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/9ab7a7c3/attachment.htm>

From sudakov at sibptus.tomsk.ru  Thu Apr 23 08:35:24 2015
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Thu, 23 Apr 2015 14:35:24 +0600
Subject: [squid-users] ERR_ONLY_IF_CACHED_MISS and cache digests problem
In-Reply-To: <553887C3.30106@treenet.co.nz>
References: <20150423051050.GA88687@admin.sibptus.tomsk.ru>
 <553887C3.30106@treenet.co.nz>
Message-ID: <20150423083524.GA92752@admin.sibptus.tomsk.ru>

Amos Jeffries wrote:

[dd]

> 
> I dont think anything is wrong wth either. Its more a collision in how
> the features work vs the protocols.
> 
> Cache Digests (CD) are exchanged periodically and updated approx hourly.
> Also they are based on just the URL. So there is always a gap where they
> may not be accurate for any highly volatile objects, and variant objects
> (using Vary headers) will have a high false-positive rate.
> 
> only-if-cached requires the *right now* state of the object to be fresh
> and in cache. It takes account of both the URL and the entire HTTP
> headers. So
> 
> The ICP protocol used as a backup to confirm objects existence also
> suffers the same URL basis problem as CD. They work fine for HTTP/1.0
> but HTTP/1.1 features dont fare quite so well.

Thank you Amos, now I understand the mechanics behind this. However,
I'd prefer that users do not receive this frustrating error in a setup
with has nothing inherently wrong about it (especially frustrating is
the fact that they receive the error from the wrong proxy server, not the
one they have configured in the browser settings).

Do I understand correctly that the only way to avoid this error
message is to switch to HTCP (and ditch both ICP and CD)?

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
sip:sudakov at sibptus.tomsk.ru


From kukuh.amukti at gmail.com  Thu Apr 23 09:40:44 2015
From: kukuh.amukti at gmail.com (kukuh amukti)
Date: Thu, 23 Apr 2015 16:40:44 +0700
Subject: [squid-users] GSSAPI problem when try create keytab using msktutil
Message-ID: <CAKHWrNFg7vUzmDpDJSpQvMRgc4eTCFONYYUnijyNNZRO2U0zTw@mail.gmail.com>

Dear All,
i've building squid in W2K12 and there is no problem but when i try running
in W2K3,
i get problem when try create keytab with msktutil command to win server
2003.
and when i run msktutil :

msktutil -c -b "OU=WSUS - Server,OU=Astragraphia-ITS" -s
HTTP/proxyagit01.ag-it.com -k /etc/squid3/PROXY.keytab --computer-name
PROXYAGIT-01 --upn HTTP/proxyagit01.ag-it.com --server
svr-resdmn22.ag-it.com --verbose

and get some error

 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/udandom = 90
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-F6iL9e
 -- reload: Reloading Kerberos Context
 -- finalize_exec: SAM Account Name is: PROXYAGIT01-K$
 -- try_machine_keytab_princ: Trying to authenticate for PROXYAGIT01-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/
proxyagit01.ag-it.com from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for PROXYAGIT01-K$ with
password.
 -- create_default_machine_password: Default machine password for
PROXYAGIT01-K$ is proxyagit01-k
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets...
 -- finalize_exec: Authenticated using method 4

 -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
try_tls=YES
 -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
try_tls=NO
SASL/GSSAPI authentication started
Error: ldap_sasl_interactive_bind_s failed (Local error)
Error: ldap_connect failed
--> Is your kerberos ticket expired? You might try re-"kinit"ing.
 -- ~KRB5Context: Destroying Kerberos Context


in auth.log  say " msktutil: GSSAPI Error: Unspecified GSS failure.  Minor
code may provide more information (Server not found in Kerberos database)"

what should i do?

thanks,
kukuhga
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/95123d16/attachment.htm>

From jagannath.naidu at fosteringlinux.com  Thu Apr 23 09:41:09 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Thu, 23 Apr 2015 15:11:09 +0530
Subject: [squid-users] [squid ] externalAclLookup: 'wbinfo_group_helper'
 queue overload.
In-Reply-To: <5535C7F5.8040102@treenet.co.nz>
References: <CA+8bHvw=Ajojam1CMOim-PZNehGxE-puEtdf7NveGc+wc==6PQ@mail.gmail.com>
 <5535C7F5.8040102@treenet.co.nz>
Message-ID: <CA+8bHvzhgS=-u5zx1a82uWk0jC62qS1HmaUoawn7eW1W43ZHfA@mail.gmail.com>

Hi Amos,

regrets, I am late.

On 21 April 2015 at 09:15, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/04/2015 7:31 p.m., Jagannath Naidu wrote:
> > Hi,
> >
> > I am having this issue very frequently. Please help on this.
> >
> > I get these errors randomly, mostly when usage is at very peak. (800
> users)
> >
> >
> > /var/log/squid/cache.log
> >
> > 2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
> > overload (ch=0x7fc99e2ce518)
>
> What do you think "overload" means?
>  The helper is unable to cope with the traffic load being passed to it.
>
> Here is the biggest hint:
> >
> > in /var/log/messages,  I get the following errors
> >
> > pr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200
> client
> > connections, no idle connection found
>
>
>
>
> > Then squid stops working. For squid to start work again, I have to dlete
> > the cache and restart the squid "squid -k reconfigure", and then squid
> > restart.
>
> What Squid version are you using?
>
> my squid version  squid-3.1.10-19.el6_4.x86_64



> >
> > squid.conf
> >
> > max_filedesc 17192
> > acl manager proto cache_object
> > acl localhost src 172.16.50.61/24
>
> changed to "acl localhost src 172.16.50.6*1*" already


> You have an entire /24 (256 IPs) assigned to this machine?
>
> I think you need to remove that "/24" part if the *.61 is the local
> machines *public* IP.
>
>
> > http_access allow manager localhost
> > dns_nameservers 172.16.3.34 10.1.2.91
> > acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
> > 172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
> > http_access allow allowips
>
> > auth_param basic realm Squid proxy-caching web server
> > auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
> > children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
>
> The above two very mangled config lines are useless. Remove them.
>
> > acl localnet src 172.16.0.0/24
>


changed


> Its a bit strange that none of the localhost machine IPs
> (172.16.50.0-172.16.50.255) are part of the LAN its plugged into
> 172.16.0.0-172.16.0.255.
>
>
> > acl localnet src fc00::/7 # RFC 4193 local private network range
> > acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged)
> machines
> > auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> > --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>
> Okay you have configured NTLM...
>
> > auth_param ntlm program /usr/bin/ntlm_auth
> > --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>
> ... but twice. With different settings. Only these last ones will have
> any effect.
>
>
> > auth_param ntlm children 600
> > auth_param ntlm keep_alive off
>
> > auth_param negotiate children 150
> > auth_param negotiate keep_alive off
> > visible_hostname GGNPROXY01.HTMEDIA.NET
> > external_acl_type wbinfo_group_helper ttl=0 children=40 %LOGIN
> > /usr/lib64/squid/wbinfo_group.pl -d
> > auth_param negotiate keep_alive off
>
> You have several useless configuration lines for Negotiate auth which is
> not being used in any way. Remove those.
>
>
> > acl Safe_ports port 8080 #https
> > acl SSL_ports port 443
> > acl Safe_ports port 80          # http
> > acl Safe_ports port 21          # ftp
> > acl Safe_ports port 443 # https
> > acl Safe_ports port 70          # gopher
> > acl Safe_ports port 210         # wais
> > acl Safe_ports port 1025-65535  # unregistered ports
> > acl Safe_ports port 280         # http-mgmt
> > acl Safe_ports port 488         # gss-http
> > acl Safe_ports port 591         # filemaker
> > acl Safe_ports port 777         # multiling http
> > acl CONNECT method CONNECT
> > acl auth proxy_auth REQUIRED
> > acl google dstdomain -i "/etc/squid/google_site.com"
> > http_access allow google
> > acl sq1 external wbinfo_group_helper "/etc/squid/HT/sq1"
> > acl sq2 external wbinfo_group_helper "/etc/squid/HT/sq2"
> > acl sq3 external wbinfo_group_helper "/etc/squid/HT/sq3"
> > acl sq4 external wbinfo_group_helper "/etc/squid/HT/sq4"
> > acl sq5 external wbinfo_group_helper "/etc/squid/HT/sq5"
> > acl pro1 external wbinfo_group_helper "/etc/squid/HT/pro1"
> > acl pro2 external wbinfo_group_helper "/etc/squid/HT/pro2"
> > acl pro3 external wbinfo_group_helper "/etc/squid/HT/pro3"
> > acl pro4 external wbinfo_group_helper "/etc/squid/HT/pro4"
> > acl pro5 external wbinfo_group_helper "/etc/squid/HT/pro5"
> > acl pro6 external wbinfo_group_helper "/etc/squid/HT/pro6"
> > acl webvip external wbinfo_group_helper "/etc/squid/HT/webvip"
> > acl allgroup external wbinfo_group_helper "/etc/squid/HT/allgreop"
> > acl restricted external wbinfo_group_helper "/etc/squid/HT/restricted"
> > acl ad_auth proxy_auth REQUIRE
>
> You already have an ACL named "auth" which performs authentication.
> The above line is not useful. Remove it and replace all uses of
> "ad_auth" ACL with "auth" ACL.
>
> > acl allowwebsites dstdomain -i "/blacklists/allowedwebsite/domains"
> > acl allowwebsites_url url_regex -i "/blacklists/allowedwebsite/url"
> > http_access allow allowwebsites
> > http_access allow allowwebsites_url
> > acl shopping dstdomain -i "/etc/squid/shopping.txt"
> > acl social_networking dstdomain -i "/blacklists/social/social.networking"
> > acl youtube dstdomain -i .youtube.com
> > http_access allow Safe_ports pro1 pro2 pro3 pro4 pro5 pro6 webvip
>
> Incorrect use of "Safe_ports" security check. Correct usage is to deny
> access to all *unsafe* ports. They are unsafe because HTTP can be
> smuggled within the ports native protocol to attack your proxy.
>
> Once the correct security protections for Safe_port and CONNECT tunnels
> have been moved up the top remove the "Safe_ports" check from this line.
>
> This line is also very odd in another way. ACL tests in a single line
> are AND'ed together - so this means the request must be from a user who is:
>   authenticated AND a member of group pro1 AND pro2 AND pro3 AND pro4
> AND pro5 AND pro6 AND webvip
>
> This hints at what your main helper problem is. The above line requires
> 7 group helper lookups *per request*. The winbind helper has a maximum
> of 200 simultaneous connections. This line alone will limit your proxy
> just under 30 new visitors per second (that becomes 60 lookups/sec
> before queue overload).
>  The helper result caching will help a lot, but you also have a LOT of
> other group checks being made and 800 users.
>
>
> > http_access allow youtube pro5
> > http_access allow youtube pro6
> > http_access allow youtube webvip
> > http_access deny youtube
> > http_access allow shopping pro5
> > http_access allow shopping pro6
> > http_access allow shopping webvip
> > http_access deny shopping
>
> Optimization hint:
>  "youtube" and "shopping" have the same allow/deny criteria. It would be
> worth combining them into one ACL.
>
> > http_access allow social_networking pro2
> > http_access allow social_networking pro4
> > http_access allow social_networking pro6
> > http_access allow social_networking webvip
> > http_access deny social_networking
> > acl porn_site1   dstdomain "/etc/squid/blacklists/porn/domains.txt"
> > acl porn_site2   dstdom_regex -i "/etc/squid/blacklists/porn/expressions"
> > acl porn_site3   dstdom_regex -i "/etc/squid/blacklists/porn/urls.txt"
> > acl audio_video1   dstdomain "/etc/squid/blacklists/audio-video/urls.txt"
> > ###################### THERE ARE TOO MANY acls and http_access , so not
> > bothering with vast linux
>
> I will bet a lot of those ACLs are also calling the group helper too yes?
>
> > http_access allow liquorinfo webvip
> > http_access deny liquorinfo
> > http_access allow ad_auth
> > http_access allow auth
>
> Once you have removed ad_auth ACL, this becomes:
>  http_access allow auth
>  http_access allow auth
>
> I hope you can see how redundant that is.
>
> Also, its very likely that the "allow auth" is a useless operation after
> a great many group checks have also performed authentication. That "TOO
> MANY acls and https_access" list you omitted will be needed to determine
> that.
>
>
> > http_access allow sq1 sq2
> > acl NTLMUsers proxy_auth REQUIRED
>
> You already have an ACL named "auth" which performs authentication.
> The above line is not being used in any way. Remove it.
>
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
>
> These are basic security protection against Denial of Service and other
> types of protocol smuggling attacks. They only work when they are used
> *above* your custom "allow" rules.
>
> Move these two lines above your "http_access allow google" line.
>
>
>
> > http_port 8080
> > hierarchy_stoplist cgi-bin ?
>
> The above line is not useful these days. Remove it.
>
> > cache_effective_user squid
> > cache_dir aufs /var/spool/squid 20384 32 512
> > cache_mem 50 MB
> > cache_replacement_policy heap LFUDA
> > cache_swap_low 85
> > cache_swap_high 95
> > maximum_object_size 5 MB
> > maximum_object_size_in_memory 50 KB
> > ipcache_size 5240
> > ipcache_low 90
> > ipcache_high 95
> > cache_mgr amit
> > acl SSL_ports port 443
>
> The above is a duplicate config line. Remove it.
>
> > http_access allow CONNECT SSL_ports
> > coredump_dir /var/spool/squid
> > refresh_pattern ^ftp:           1440    20%     10080
> > refresh_pattern ^gopher:        1440    0%      1440
> > refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> > refresh_pattern .               0       20%     4320
> > url_rewrite_program /usr/local/bin/squidGuard -c
> > /usr/local/squidGuard/squidGuard.conf
> >
>
>
> Now, as to solving your problem:
>
> 1) Clean up your config. Reduce the amount of redundant or unused
> things. I've mentioned a few above.
>
> 2) Run "squid -k parse" and fix any other problems it highlights.
>
> 3) optimize your ACls and http_access rules. I've mentioned a few, such
> as moving the main security checks to the top so DoS traffic does not
> put load on the helpers and other ACLs.
>
> I believe though that you will probably find Squid works much better
> having the following access controls pattern:
> "
>  http_access deny !Safe_ports
>  http_access deny CONNECT !SSL_ports
>
>  # if they are not authenticated, they will not be in a group
>  http_access deny !auth
>
>  # assuming that webvip are the group with full access?
>  http_access allow webvip
>
>  # your long list of per-site group check ACLs go here
>  ...
>
>  # this is where defining the LAN ranges correctly comes in.
>  # note that users have authenticated simply to get near here
>  http_access allow localnet
>  http_access deny all
> "
>
>
> 4) consider an upgrade to Squid 3.4+. The "notes" ACL type offers much
> more efficient ACL testing with a custom group lookup helper. The all-of
> and any-of ACL types can also much reduce your http_access lines.
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



Thank you Amos, I will check and will update the list.


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/4e7744c9/attachment.htm>

From jlay at slave-tothe-box.net  Thu Apr 23 11:52:33 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 23 Apr 2015 05:52:33 -0600
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <9890F667-FE7C-49B4-A11F-192F55233BDC@hendrie.id.au>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
 <55389675.3040702@treenet.co.nz>
 <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>
 <9890F667-FE7C-49B4-A11F-192F55233BDC@hendrie.id.au>
Message-ID: <1429789953.3972.1.camel@JamesiMac>

On Thu, 2015-04-23 at 17:18 +0930, Michael Hendrie wrote:

> 
> 
> > On 23 Apr 2015, at 4:28 pm, Michael Hendrie <michael at hendrie.id.au>
> > wrote:
> > 
> > 
> > 
> > 
> > 
> > > On 23 Apr 2015, at 4:21 pm, Amos Jeffries <squid3 at treenet.co.nz>
> > > wrote:
> > > 
> > > On 23/04/2015 6:29 p.m., Michael Hendrie wrote:
> > > 
> > > > Hi All
> > > > 
> > > > I?ve been running squid-3.4.x in tproxy mode with ssl_bump
> > > > server-first for some time and has been working great.
> > > > 
> > > > I have just moved to 3.5.3 to use peek to overcome some issues
> > > > with
> > > > sites that require SNI to serve up the correct certificate.  In
> > > > most
> > > > cases this is work well however I seem to have an issue that (so
> > > > far)
> > > > only effects the Safari web browser with certain sites.  As an
> > > > example, https://twitter.com <https://twitter.com/> and
> > > > https://www.openssl.org <https://www.openssl.org/> will result
> > > > in a
> > > > Safari error page ?can?t establish a secure connection with the
> > > > server?.  There is also a correlating entry in the cache.log
> > > > 'Error
> > > > negotiating SSL connection on FD 45: error:140A1175:SSL
> > > > routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback (1/-1)?
> > > 
> > > 
> > > Please try the latest snapshot of 3.5 series. There are some TLS
> > > session
> > > resume and SNI bug fixes.
> > 
> > 
> > Thanks Amos, but I did try squid-3.5.3-20150420-r13802 before
> > posting?.any other suggestions?
> > 
> > Michael
> 
> 
> 
> OK, I seem to have resolved this now, for the benefit of everyone else
> on the list:
> 
> 
> In the above tests the generated certificate was being signed by a
> RootCA that was installed as trusted in the browser certificate
> store.  
> 
> 
> I had previously noticed in my test environment (and thought
> completely unrelated) that bumped requests using the new peek/bump in
> 3.5.x were not sending the entire certificate chain to the browser but
> since they trusted the RootCA that was fine.  In my production
> environment however I use an IntermediateCA to sign the bumped
> requests, this causes a browser error as the clients only trust the
> RootCA.  As part of investigation to resolve this, I found that adding
> ?cafile=/path/to/signing_ca_bundle? to the ?https_port' line (which in
> my config is exactly the same file as ?cert=?) that all certs are sent
> to the client, and I no longer face the issue with Safari and
> https://twitter.com or https://www.openssl.org regardless of using
> RootCA or InterCA to sign bumped requests.
> 
> 
> Not sure why but ?ssl_bump server-first? sends the entire chain
> without specifying ?cafile=? and ?ssl_bump peek/bump? doesn?t?but
> anyway my problem is solved!
> 
> 
> Michael
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Michael,

Could you post your entire config here if possible?  Many of us continue
to face challenges with ssl_bump and a working config would be great.
Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/4cbe3ba7/attachment.htm>

From dweimer at dweimer.net  Thu Apr 23 14:24:38 2015
From: dweimer at dweimer.net (dweimer)
Date: Thu, 23 Apr 2015 09:24:38 -0500
Subject: [squid-users] Squid Upgrade from 3.4.12 to 3.5.3 on FreeBSD 10.1
 broke Exchange RPC reverse proxy
Message-ID: <e98a29d168bc2680085c23d0b48ac332@dweimer.net>

I upgraded our Reverse proxy from 3.4.12 to 3.5.3 via the FreeBSD ports 
last night. It has broken our Outlook RPC over HTTPS. OWA and Phones are 
still connecting with Active Sync, its just the RPC for Outlook anywhere 
that is broken.

Did anyone else have any issues when upgrading from 3.4 branch to 3.5 
branch with Outlook RPC?

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From mike13 at poczta.fm  Thu Apr 23 17:25:47 2015
From: mike13 at poczta.fm (smaku)
Date: Thu, 23 Apr 2015 10:25:47 -0700 (PDT)
Subject: [squid-users] access_log none acl
Message-ID: <1429809947181-4670886.post@n4.nabble.com>

Hi all,

I dont want to log my traffic to squid in access_log.
That's why I add two lines to default config.
but when I vi or tail access.log I can see my requests:

1429810062.286    142 10.31.6.5 TCP_MISS/302 807 GET http://www.google.pl/ -
HIER_DIRECT/173.194.32.207 text/html


squid.conf:
################################################################

acl myacl1 src 10.31.6.5/32
access_log none myacl1

#LOG - same as default
access_log daemon:/var/squid/logs/access.log squid
cache_log /var/squid/logs/cache.log
#ACL list
acl office src 10.31.6.0/24
#allow http
http_access allow office
http_access deny all

################################################################


any idea?
squid version 3.4.6



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/access-log-none-acl-tp4670886.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Apr 23 18:10:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Apr 2015 00:10:33 +0600
Subject: [squid-users] access_log none acl
In-Reply-To: <1429809947181-4670886.post@n4.nabble.com>
References: <1429809947181-4670886.post@n4.nabble.com>
Message-ID: <55393599.8060406@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://www.squid-cache.org/Doc/config/access_log/

23.04.15 23:25, smaku ?????:
> Hi all,
>
> I dont want to log my traffic to squid in access_log.
> That's why I add two lines to default config.
> but when I vi or tail access.log I can see my requests:
>
> 1429810062.286    142 10.31.6.5 TCP_MISS/302 807 GET
http://www.google.pl/ -
> HIER_DIRECT/173.194.32.207 text/html
>
>
> squid.conf:
> ################################################################
>
> acl myacl1 src 10.31.6.5/32
> access_log none myacl1
>
> #LOG - same as default
> access_log daemon:/var/squid/logs/access.log squid
> cache_log /var/squid/logs/cache.log
> #ACL list
> acl office src 10.31.6.0/24
> #allow http
> http_access allow office
> http_access deny all
>
> ################################################################
>
>
> any idea?
> squid version 3.4.6
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/access-log-none-acl-tp4670886.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVOTWYAAoJENNXIZxhPexGe4AIAKeAoB+lHLjpfcyyIhYf9N9c
HG9UAgOB+ofDhljHlsyd2ihwlQOjQlgjhTLZVXCaLa3u3D+oLqHHb2tvqpnJg6YV
QbrXrfhX39D7WM/Sd5DoyyPKcthI/IFF7hIU8BUkLs7P13avVR9Qp83+BsLlV3N4
R4pWg+dfrK2PvbkPCdTu4gwr59cn9td3SahNnES3Q0RapCshdK0nPqTX2FQO0EMw
Ej7hJu2ZOCYQsNfmUtSYM0UPNvSt3kBGg2J3SYnaWQ/iwy/P2OfcwtKfWdJmgBLU
czNETUCF5b4hcJBTb1ZiJrxwiPLhKJV8KGnQCs8R09Tz98NXuKFB4a+PZ4H/X2I=
=3Jhq
-----END PGP SIGNATURE-----



From ncrogers at gmail.com  Thu Apr 23 18:26:40 2015
From: ncrogers at gmail.com (Nick Rogers)
Date: Thu, 23 Apr 2015 11:26:40 -0700
Subject: [squid-users] WARNING: Tos value ... adjusted
In-Reply-To: <553884B3.9010809@treenet.co.nz>
References: <CAKOb=YYbTitQeLLyUUW+DPBNrmFOOhnu3YzHNVxFoWaY4Ge7sA@mail.gmail.com>
 <553884B3.9010809@treenet.co.nz>
Message-ID: <CAKOb=YYvg3hSmyzV9JcKiREJaiEo=FGpqR4OtarEnFOzhgnKGg@mail.gmail.com>

On Wed, Apr 22, 2015 at 10:35 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 23/04/2015 9:14 a.m., Nick Rogers wrote:
> > After upgrading from 3.4.x to 3.5.x, I've noticed a new error message
> with
> > my squid configuration. Apparently squid 3.5 no longer allows setting the
> > two lower-most ECN bits of the ToS byte.
>
> Allowing it and leaving it up to admin was causing too much confusion
> over mysteriously dropped traffic.
>
> Attempts to bit-shift values were not backward compatible. So instead we
> opted to take a bitmask hex value as the TOS parameter and explicitly
> forbid with warning the ECN bits being set in that mask.
>

Can you elaborate on how you would bit-shift the values to still use the
ECN field and not cause mysterious problems. For example, using ECN bits of
00 and 11, but not 01 or 10?


>
> > I realize that this is to
> > encourage people to use the modernized definition of ToS being a 6 bit
> DSCP
> > field and a two bit ECN field, however enforcing this in the
> configuration
> > parser introduces a big problem for me. I use the ToS byte as a way to
> tag
> > outbound requests from squid based on ACLs, to then queue the traffic
> > differently via PF + ALTQ engine on FreeBSD. This is a way to queue
> upload
> > traffic from squid on a per-IP/ACL basis. Having 256 values (8 bit ToS)
> to
> > work with is a lot preferable than only 64 (6 bit DSCP field). I'm not
> sure
> > if I care if the ECN bits are set, as it is likely scrubbed away by my
> > upstream ISP,
>
> ECN is an end-to-end property of the packets. It does not get scrubbed
> away. Your ISP probably uses MPLS these days, so these packets existing
> your network with ECN set cause your network to be marked as a congested
> zone - any packets going from another congested network to yours get
> *dropped* until the congestion in one of the networks gets resolved. It
> can also trigger nasty actions such as TCP throttling on the other end
> of the connection.
>

My understanding was something else has to happen in my TCP config for ECN
to have an effect, not simply just setting the field in the packet. For
example, in FreeBSD I have the "net.inet.tcp.ecn.enable" sysctl set to 0. I
guess this assumption is wrong? Thanks for clarifying.

>
> NP: Do you find your ISP service is "a little shitty" with packet loss
> and traffic slowing down occasionally? ECN and/or ICMP congestion
> control breakage is often the cause. Could be self-inflicted.
>

I haven't noticed this explicitly, as I have a few hundred deployments, all
with different ISPs, but that is not to say its not happening.


>
> > or I can normalize it with PF. The point is I really need to
> > be able to tag outgoing packets with the full 8 bits of the ToS byte, not
> > just 6. I've been doing this for nearly a decade since early squid 2.x.
> >
> > So my question is, is the squid team willing to revert this change
> > entirely, and leave it up to the admin to decide what ToS values are
> > appropriate. Or are you guys now pretty adamant about never setting the
> ECN
> > field?
>
> ECN is pretty standard these days. Any casual update or upgrade to the
> machine software, hardware, or configuration anywhere in the network(s)
> it communicates over could result in unwanted traffic loss or worse
> behaviour.
>
> If you are in the practice of using those bits in the TOS value you are
> of course free to patch the mask removal yourself, but we wont be
> publishing new versions of Squid that touch the ECN bits - ie. not while
> IPv4 is still in wide use.
>

I understand the reasoning now. Thanks.


>
> IPv6 handles DSCP+ECN better and is artificially limited by this. But we
> feel that is justified temporarily to avoid confusion and problems when
> mapping DSCP between IPv4 and IPv6 traffic.
>
>
> >
> > It looks like the change got snuck into this commit along with fixing
> some
> > other tcp outgoing tos bugs. It really caught me off-guard because there
> is
> > no mention of this new behavior in the changelog.
> >
> >
> https://github.com/squid-cache/squid/commit/651ba437462fb5fde21f8d3b66d09afa1e069d5c
> >
>
> The TOS directives have always been documented as being for the
> DiffServ/DSCP values. The only reason they ever allowed those ECN bits
> to be set in the first place is that they pre-date ECNs existence.
>
> Sorry that we missed out the documentation update mentioning that
> change. A note is going in now.
>
>
> > The enforcement happens in src/cache_cf.cc. Its 5 lines of code that seem
> > trivial to either remove or make configurable, however I am not sure how
> to
> > go about making it configurable. I am debating just patching my squid
> > builds to remove the check against the ToS field, but I would like to use
> > unmodified squid 3.5.x. I'm not sure if going from 256 to 64 possible
> > fields is possible for my deployments.
>
> It better be. Any software not supporting ECN will have problems
> operating over VPN, IPSEC or MLPS.
>

So to clarify, squid is implicitly ECN capable, but manually overriding the
ECN field via tcp_outgoing_tos was causing problems?


> NOTE: We have the NETMASK feature made available on Linux now for
> systems that need large numbers of packet flow tag values within the
> local machine. I'm not aware of any equivalent on non-Linux systems, but
> if they exist in PF (other than setting the TOS bits) then we are open
> to supporting that.
>

Yeah, I've looked into that before. Unfortunately there is nothing like
that in FreeBSD that is supported by PF that I am aware of.

Thanks for your feedback.

Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/225febc3/attachment.htm>

From dweimer at dweimer.net  Thu Apr 23 19:11:16 2015
From: dweimer at dweimer.net (dweimer)
Date: Thu, 23 Apr 2015 14:11:16 -0500
Subject: [squid-users] Squid Upgrade from 3.4.12 to 3.5.3 on FreeBSD
 10.1 broke Exchange RPC reverse proxy
In-Reply-To: <e98a29d168bc2680085c23d0b48ac332@dweimer.net>
References: <e98a29d168bc2680085c23d0b48ac332@dweimer.net>
Message-ID: <d7914300d92c24fabd90ed66aba11702@dweimer.net>

On 04/23/2015 9:24 am, dweimer wrote:
> I upgraded our Reverse proxy from 3.4.12 to 3.5.3 via the FreeBSD
> ports last night. It has broken our Outlook RPC over HTTPS. OWA and
> Phones are still connecting with Active Sync, its just the RPC for
> Outlook anywhere that is broken.
> 
> Did anyone else have any issues when upgrading from 3.4 branch to 3.5
> branch with Outlook RPC?

In case anyone else is having an issue, I found the solution. Which also 
solved a long standing issue with larger file uploads through 
OWA/ActiveSync/RPC, that we were having. I had to force the cache peer 
to use SSLv3 instead of TLSv1.0 by adding sslversion=3 to the cache peer 
line.

cache_peer 1.1.1.1 parent 443 0 ssl no-query proxy-only no-digest 
originserver name=exchange2010_parent sslflags=DONT_VERIFY_PEER 
login=PASSTHRU front-end-https=on connection-auth=on sslversion=3

The HTTPS port line is still enforcing TLSv1.0 or newer, with restricted 
ciphers.

https_port 1.1.1.2:443 accel cert=... key=... 
options=NO_SSLv2:NO_SSLv3:CIPHER_SERVER_PREFERENCE 
cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!SSLv2:!RC4


-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From jonathan_chretien at hotmail.com  Thu Apr 23 20:52:17 2015
From: jonathan_chretien at hotmail.com (Jonathan Chretien)
Date: Thu, 23 Apr 2015 16:52:17 -0400
Subject: [squid-users] HTTPS Filtering and SSL-Bump
Message-ID: <BAY182-W1DCB0628303A8D2F27C2E83ED0@phx.gbl>

Hi all.

I'm trying to implement the filtering of https content for a particular url. The only thing that I'm trying to do it's to unlock corporate video on the Youtube website. I do not want to unlock everything on Youtube but only our corporate stuff.

The url looks like this: https://www.youtube.com/users/MyCompany.

I'm using UFDBGuard as a url filter.

The problem is that SSL-Bump is working well but the URL pass from Squid to UFDBGuard is the non SSL-Bump url. What I means is that the URL that UFDBGuard is receiving is https://www.youtube.com:443 instead of the https://www.youtube.com/users/MyCompany.

So because UFDBGuard is not receiving the complete SSL-Bump URL, UFDBGuard see that it's Youtube.com, so it block the website. If UFDBGuard was receiving the real SSL-Bump url https://www.youtube.com/users/MyCompany, UFDBGuard will see that this url is whitelisted and should allow the access.

Log in the UFDBGuard.log
2015-04-23 16:19:59 [10669] BLOCK MyUser ? 192.168.100.27 ? ? ? Internet ? movies ? ?www.youtube.com:443 CONNECT

Is there something missing in my Squid.conf to pass the correct URL?

http_port 192.168.100.2:3129 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB key=/etc/squid/ssl/mycert.com.private cert=/etc/squid/ssl/mycert.com.cert

# SSL Bump Config
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1

acl sslBumpYoutube dstdomain www.youtube.com

# SSL Bump Config
always_direct allow sslBumpYoutube
ssl_bump server-first sslBumpYoutube
ssl_bump none all

Also all my users using the proxy are authenticated.


Thanks
___________________________________
Jonathan

 		 	   		  

From marcus.kool at urlfilterdb.com  Thu Apr 23 22:39:05 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 23 Apr 2015 19:39:05 -0300
Subject: [squid-users] HTTPS Filtering and SSL-Bump
In-Reply-To: <BAY182-W1DCB0628303A8D2F27C2E83ED0@phx.gbl>
References: <BAY182-W1DCB0628303A8D2F27C2E83ED0@phx.gbl>
Message-ID: <55397489.4010602@urlfilterdb.com>



On 04/23/2015 05:52 PM, Jonathan Chretien wrote:
> Hi all.
>
> I'm trying to implement the filtering of https content for a particular url. The only thing that I'm trying to do it's to unlock corporate video on the Youtube website. I do not want to unlock everything on Youtube but only our corporate stuff.
>
> The url looks like this: https://www.youtube.com/users/MyCompany.
>
> I'm using UFDBGuard as a url filter.
>
> The problem is that SSL-Bump is working well but the URL pass from Squid to UFDBGuard is the non SSL-Bump url. What I means is that the URL that UFDBGuard is receiving is https://www.youtube.com:443 instead of the https://www.youtube.com/users/MyCompany.
>
> So because UFDBGuard is not receiving the complete SSL-Bump URL, UFDBGuard see that it's Youtube.com, so it block the website. If UFDBGuard was receiving the real SSL-Bump url https://www.youtube.com/users/MyCompany, UFDBGuard will see that this url is whitelisted and should allow the access.


This is not the full story.
With SSLbump on Squid sends to ufdbGuard first
    CONNECT www.youtube.com:443
and then
    GET  https://www.youtube.com/users/MyCompany

ufdbGuard has not yet support for this but you could whitelist
    www.youtube.com:443
using a regular expression
and whitelist https://www.youtube.com/users/MyCompany and a bunch of other URLs that used for the markup of the entire page
and blacklist a bunch of other youtube URLs to get the desired behavior.

Whitelisting a subset of a website is usually not so straightforward so one needs to pay much attention to the "bunch of URLs" used in the whitelist and blacklist.
I suggest to not blacklist www.youtube.com but start with blacklisting a few important URLs of youtube such that the effective result is that non-company access to Youtube is blocked.

Marcus
maintainer of ufdbGuard.

PS: after you have done all this, you also need to block all web proxies which can be used to circumvent the intended Youtube block.


> Log in the UFDBGuard.log
> 2015-04-23 16:19:59 [10669] BLOCK MyUser   192.168.100.27       Internet   movies    www.youtube.com:443 CONNECT
>
> Is there something missing in my Squid.conf to pass the correct URL?
>
> http_port 192.168.100.2:3129 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB key=/etc/squid/ssl/mycert.com.private cert=/etc/squid/ssl/mycert.com.cert
>
> # SSL Bump Config
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1
>
> acl sslBumpYoutube dstdomain www.youtube.com
>
> # SSL Bump Config
> always_direct allow sslBumpYoutube
> ssl_bump server-first sslBumpYoutube
> ssl_bump none all
>
> Also all my users using the proxy are authenticated.
>
>
> Thanks
> ___________________________________
> Jonathan
>
>   		 	   		
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Fri Apr 24 02:03:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 14:03:40 +1200
Subject: [squid-users] access_log none acl
In-Reply-To: <55393599.8060406@gmail.com>
References: <1429809947181-4670886.post@n4.nabble.com>
 <55393599.8060406@gmail.com>
Message-ID: <5539A47C.4090307@treenet.co.nz>

On 24/04/2015 6:10 a.m., Yuri Voinov wrote:
> 
> http://www.squid-cache.org/Doc/config/access_log/
> 
> 23.04.15 23:25, smaku ?????:
>> Hi all,
> 
>> I dont want to log my traffic to squid in access_log.
>> That's why I add two lines to default config.
>> but when I vi or tail access.log I can see my requests:
> 
>> 1429810062.286    142 10.31.6.5 TCP_MISS/302 807 GET
> http://www.google.pl/ -
>> HIER_DIRECT/173.194.32.207 text/html
> 
> 
>> squid.conf:
>> ################################################################
> 
>> acl myacl1 src 10.31.6.5/32
>> access_log none myacl1
> 
>> #LOG - same as default
>> access_log daemon:/var/squid/logs/access.log squid
>> cache_log /var/squid/logs/cache.log
>> #ACL list
>> acl office src 10.31.6.0/24
>> #allow http
>> http_access allow office
>> http_access deny all
> 
>> ################################################################
> 
> 
>> any idea?
>> squid version 3.4.6
> 

Thank you, I found a bug in the "none" handling.

As a workaround while this gets fixed you can use this config which is
equivalent to the non-working one:

 access_log daemon:/var/squid/logs/access.log squid !myacl1

Amos


From squid3 at treenet.co.nz  Fri Apr 24 02:11:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 14:11:04 +1200
Subject: [squid-users] Squid Upgrade from 3.4.12 to 3.5.3 on FreeBSD
 10.1 broke Exchange RPC reverse proxy
In-Reply-To: <d7914300d92c24fabd90ed66aba11702@dweimer.net>
References: <e98a29d168bc2680085c23d0b48ac332@dweimer.net>
 <d7914300d92c24fabd90ed66aba11702@dweimer.net>
Message-ID: <5539A638.1010100@treenet.co.nz>

On 24/04/2015 7:11 a.m., dweimer wrote:
> On 04/23/2015 9:24 am, dweimer wrote:
>> I upgraded our Reverse proxy from 3.4.12 to 3.5.3 via the FreeBSD
>> ports last night. It has broken our Outlook RPC over HTTPS. OWA and
>> Phones are still connecting with Active Sync, its just the RPC for
>> Outlook anywhere that is broken.
>>
>> Did anyone else have any issues when upgrading from 3.4 branch to 3.5
>> branch with Outlook RPC?
> 
> In case anyone else is having an issue, I found the solution. Which also
> solved a long standing issue with larger file uploads through
> OWA/ActiveSync/RPC, that we were having. I had to force the cache peer
> to use SSLv3 instead of TLSv1.0 by adding sslversion=3 to the cache peer
> line.
> 
> cache_peer 1.1.1.1 parent 443 0 ssl no-query proxy-only no-digest
> originserver name=exchange2010_parent sslflags=DONT_VERIFY_PEER
> login=PASSTHRU front-end-https=on connection-auth=on sslversion=3
> 
> The HTTPS port line is still enforcing TLSv1.0 or newer, with restricted
> ciphers.
> 
> https_port 1.1.1.2:443 accel cert=... key=...
> options=NO_SSLv2:NO_SSLv3:CIPHER_SERVER_PREFERENCE
> cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!SSLv2:!RC4
> 
> 

Ouch. Good to know thank you.

FYI:
That workaround is one to keep an eye on. You may find the workaround
needs undoing at some point soonish.
 MS are officially in the process of releasing updates that remove and
disable SSLv3 support from their software. It began back in Oct/Nov 2014
and seems to be moving across the product range in a staged rollout with
each of the "Patch Tueday" so far (and probaly some future).

Amos




From squid3 at treenet.co.nz  Fri Apr 24 02:12:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 14:12:49 +1200
Subject: [squid-users] WARNING: Tos value ... adjusted
In-Reply-To: <CAKOb=YYvg3hSmyzV9JcKiREJaiEo=FGpqR4OtarEnFOzhgnKGg@mail.gmail.com>
References: <CAKOb=YYbTitQeLLyUUW+DPBNrmFOOhnu3YzHNVxFoWaY4Ge7sA@mail.gmail.com>	<553884B3.9010809@treenet.co.nz>
 <CAKOb=YYvg3hSmyzV9JcKiREJaiEo=FGpqR4OtarEnFOzhgnKGg@mail.gmail.com>
Message-ID: <5539A6A1.1060002@treenet.co.nz>

On 24/04/2015 6:26 a.m., Nick Rogers wrote:
> On Wed, Apr 22, 2015 at 10:35 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
>> On 23/04/2015 9:14 a.m., Nick Rogers wrote:
>>> After upgrading from 3.4.x to 3.5.x, I've noticed a new error message
>> with
>>> my squid configuration. Apparently squid 3.5 no longer allows setting the
>>> two lower-most ECN bits of the ToS byte.
>>
>> Allowing it and leaving it up to admin was causing too much confusion
>> over mysteriously dropped traffic.
>>
>> Attempts to bit-shift values were not backward compatible. So instead we
>> opted to take a bitmask hex value as the TOS parameter and explicitly
>> forbid with warning the ECN bits being set in that mask.
>>
> 
> Can you elaborate on how you would bit-shift the values to still use the
> ECN field and not cause mysterious problems. For example, using ECN bits of
> 00 and 11, but not 01 or 10?

Sure. We experimented with taking the config value N and using TOS value
(N<<2) for IPv4 and N for IPv6. Didn't work out well.

Amos


From srinath.krishna at gmail.com  Fri Apr 24 02:29:46 2015
From: srinath.krishna at gmail.com (Srinath Krishna)
Date: Thu, 23 Apr 2015 19:29:46 -0700
Subject: [squid-users] Using Squid as a Transparent Proxy
Message-ID: <CAAg1D6BT=YfJk_hZWG+=Yw7ChPxbbRx9pF6AFLBO8mC0KmP=ZQ@mail.gmail.com>

Hello all,

I'm trying my hands with openvswitch and squid. This is what I want to
achieve.

The client tries to connect to the server. This packet is handled through
an openvswitch and it's sent to a machine running squid for proxying. The
machine running squid sees the packet with client to server but iptables
rules help in delivering this packet up the stack. On a cache hit, squid
responds back to the client and also installs iptables rules on the fly and
hence the source IP is that of the server.

This is achieved through the following configuration in squid.conf.

http_port 3128 intercept

With this configuration however, on a cache miss case, squid uses it's IP
address as the source IP to connect to the server. What I expect is squid
to use the client's IP address to establish this new connection to the
server. From the squid.conf, I believe I need to use the tproxy mode with
the http_port directive, but I'm stumped about what iptables rules to
configure.

I'm trying to follow the steps here (
http://wiki.squid-cache.org/Features/Tproxy4#Feature:_TPROXY_version_4.1.2B-_Support)
but no luck yet. And I don't understand why I'd need to use WCCP for
something like this.

I expect squid to use the client's IP address and the reverse traffic from
the server will make it's way to squid's box through openvswitch. All squid
has to do is install an iptable rule on the fly for the outgoing connection
to use the client's IP address and also have a corresponding reverse rule
to translate from the client's IP address to squid's IP address.

The kernel that I'm using is 3.16 and it has the nf_conntrack and xt_TPROXY
modules insmoded. Can someone help me with this?

Thanks,
Srinath
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/e0d15488/attachment.htm>

From alex at samad.com.au  Fri Apr 24 02:43:20 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 24 Apr 2015 12:43:20 +1000
Subject: [squid-users] bandwidth limiting
Message-ID: <CAJ+Q1PXXs4Ui=_qC5NH4KKFmYrirnHn64wyF2Y1wUrkDLsznOw@mail.gmail.com>

Hi

is there any way to limit the bandwidth squid uses to pull stuff from
the internet ?

Can it slow down request, delay acks or ??

A


From kris at linquist.net  Fri Apr 24 02:46:57 2015
From: kris at linquist.net (Kristopher Linquist)
Date: Thu, 23 Apr 2015 19:46:57 -0700 (PDT)
Subject: [squid-users] Bring refresh_pattern down to ~10sec?
Message-ID: <1429843616324.43b5d30b@Nodemailer>

Hi,


I?m using squid to throttle outgoing API calls to various services.


I?ve got Squid working with ssl_bump and currently caching any request more often than 15 minutes with this line:


refresh_pattern .		15	20%	4320 override-expire ignore-reload





I?m interested in making sure I can?t send an identical REST call to twilio(text message service) more often than every 10 seconds. ?


Is there any way to accomplish this?


Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/6048a73e/attachment.htm>

From dklima at gmail.com  Fri Apr 24 02:56:09 2015
From: dklima at gmail.com (Daniel K. Lima)
Date: Fri, 24 Apr 2015 02:56:09 +0000
Subject: [squid-users] Squid Upgrade from 3.4.12 to 3.5.3 on FreeBSD
 10.1 broke Exchange RPC reverse proxy
In-Reply-To: <5539A638.1010100@treenet.co.nz>
References: <e98a29d168bc2680085c23d0b48ac332@dweimer.net>
 <d7914300d92c24fabd90ed66aba11702@dweimer.net>
 <5539A638.1010100@treenet.co.nz>
Message-ID: <CABGWC=PR6d_LQJiNVVorQcsLnVmPnTh6fGWzT9XEWOLrYx6e9w@mail.gmail.com>

At jun, Firefox will drop entirely it support for sslv3.
On Thu, Apr 23, 2015 at 11:11 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 24/04/2015 7:11 a.m., dweimer wrote:
> > On 04/23/2015 9:24 am, dweimer wrote:
> >> I upgraded our Reverse proxy from 3.4.12 to 3.5.3 via the FreeBSD
> >> ports last night. It has broken our Outlook RPC over HTTPS. OWA and
> >> Phones are still connecting with Active Sync, its just the RPC for
> >> Outlook anywhere that is broken.
> >>
> >> Did anyone else have any issues when upgrading from 3.4 branch to 3.5
> >> branch with Outlook RPC?
> >
> > In case anyone else is having an issue, I found the solution. Which also
> > solved a long standing issue with larger file uploads through
> > OWA/ActiveSync/RPC, that we were having. I had to force the cache peer
> > to use SSLv3 instead of TLSv1.0 by adding sslversion=3 to the cache peer
> > line.
> >
> > cache_peer 1.1.1.1 parent 443 0 ssl no-query proxy-only no-digest
> > originserver name=exchange2010_parent sslflags=DONT_VERIFY_PEER
> > login=PASSTHRU front-end-https=on connection-auth=on sslversion=3
> >
> > The HTTPS port line is still enforcing TLSv1.0 or newer, with restricted
> > ciphers.
> >
> > https_port 1.1.1.2:443 accel cert=... key=...
> > options=NO_SSLv2:NO_SSLv3:CIPHER_SERVER_PREFERENCE
> > cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!SSLv2:!RC4
> >
> >
>
> Ouch. Good to know thank you.
>
> FYI:
> That workaround is one to keep an eye on. You may find the workaround
> needs undoing at some point soonish.
>  MS are officially in the process of releasing updates that remove and
> disable SSLv3 support from their software. It began back in Oct/Nov 2014
> and seems to be moving across the product range in a staged rollout with
> each of the "Patch Tueday" so far (and probaly some future).
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/551e3b00/attachment.htm>

From squid3 at treenet.co.nz  Fri Apr 24 03:42:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 15:42:51 +1200
Subject: [squid-users] Using Squid as a Transparent Proxy
In-Reply-To: <CAAg1D6BT=YfJk_hZWG+=Yw7ChPxbbRx9pF6AFLBO8mC0KmP=ZQ@mail.gmail.com>
References: <CAAg1D6BT=YfJk_hZWG+=Yw7ChPxbbRx9pF6AFLBO8mC0KmP=ZQ@mail.gmail.com>
Message-ID: <5539BBBB.6040001@treenet.co.nz>

On 24/04/2015 2:29 p.m., Srinath Krishna wrote:
> Hello all,
> 
> I'm trying my hands with openvswitch and squid. This is what I want to
> achieve.
> 
> The client tries to connect to the server. This packet is handled through
> an openvswitch and it's sent to a machine running squid for proxying. The
> machine running squid sees the packet with client to server but iptables
> rules help in delivering this packet up the stack. On a cache hit, squid
> responds back to the client and also installs iptables rules on the fly and
> hence the source IP is that of the server.

No. Squid has nothing to do with any of that. The kernel TPROXY module
does it all.

>
> This is achieved through the following configuration in squid.conf.
> 
> http_port 3128 intercept
> 

"intercept" means NAT. Which cannot do what you are asking for.

> With this configuration however, on a cache miss case, squid uses it's IP
> address as the source IP to connect to the server. What I expect is squid
> to use the client's IP address to establish this new connection to the
> server. From the squid.conf, I believe I need to use the tproxy mode with

Correct.

> the http_port directive, but I'm stumped about what iptables rules to
> configure.

That would be the lines listed in
<http://wiki.squid-cache.org/Features/Tproxy4#iptables_Configuration>

Also, make sure that any rules in the "iptable -t nat " which you may
have setup for the NAT intercept configuration are removed. They *will*
break TPROXY kernel module operations.

iptables is just one of many networking layers than need configuring
though before TPROXY will work. The rest of the page explains.

> 
> I'm trying to follow the steps here (
> http://wiki.squid-cache.org/Features/Tproxy4#Feature:_TPROXY_version_4.1.2B-_Support)
> but no luck yet. And I don't understand why I'd need to use WCCP for
> something like this.

You dont. Please read the _title_ of the WCCP section again. Slowly.


> 
> I expect squid to use the client's IP address and the reverse traffic from
> the server will make it's way to squid's box through openvswitch.

You expect a lot. Behind the single tproxy flag in squid.conf the entire
routing system of your whole network has to be configured to ensure that
above "expected" flow is what the packets actually do.

If there is any potential path through the network where server packets
can reach the client directly without being diverted back through Squid
there will be hanging connections. The client will drop them because the
TCP connection was not client initiated.



> All squid
> has to do is install an iptable rule on the fly for the outgoing connection
> to use the client's IP address and also have a corresponding reverse rule
> to translate from the client's IP address to squid's IP address.

That is the purpose of the TPROXY kernel module. It does all the
netfilter/firewall bits when instructed to by Squid.

> 
> The kernel that I'm using is 3.16 and it has the nf_conntrack and xt_TPROXY
> modules insmoded. Can someone help me with this?
> 

Provided you also have a recent Squid version, built with the
appropriate components (netfilter and libcap2).

All you should need is the configuration in section 1.3, 1.5, 1.6.1, and
possibly 1.7 of that Tproxy4 wiki page.


Amos



From squid3 at treenet.co.nz  Fri Apr 24 04:05:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 16:05:39 +1200
Subject: [squid-users] Bring refresh_pattern down to ~10sec?
In-Reply-To: <1429843616324.43b5d30b@Nodemailer>
References: <1429843616324.43b5d30b@Nodemailer>
Message-ID: <5539C113.9030802@treenet.co.nz>

On 24/04/2015 2:46 p.m., Kristopher Linquist wrote:
> Hi,
> 
> 
> I?m using squid to throttle outgoing API calls to various services.
> 
> 
> I?ve got Squid working with ssl_bump and currently caching any request more often than 15 minutes with this line:
> 
> 
> refresh_pattern .		15	20%	4320 override-expire ignore-reload
> 
> 
> I?m interested in making sure I can?t send an identical REST call to twilio(text message service) more often than every 10 seconds.  
> 
> 
> Is there any way to accomplish this?

Depends on what you mean by "identical REST call".

There is the miss_access control, but it only allows "fast" ACLs.

Amos



From squid3 at treenet.co.nz  Fri Apr 24 04:06:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2015 16:06:55 +1200
Subject: [squid-users] bandwidth limiting
In-Reply-To: <CAJ+Q1PXXs4Ui=_qC5NH4KKFmYrirnHn64wyF2Y1wUrkDLsznOw@mail.gmail.com>
References: <CAJ+Q1PXXs4Ui=_qC5NH4KKFmYrirnHn64wyF2Y1wUrkDLsznOw@mail.gmail.com>
Message-ID: <5539C15F.4020601@treenet.co.nz>

On 24/04/2015 2:43 p.m., Alex Samad wrote:
> Hi
> 
> is there any way to limit the bandwidth squid uses to pull stuff from
> the internet ?
> 
> Can it slow down request, delay acks or ??

http://wiki.squid-cache.org/Features/DelayPools


Amos


From alex at samad.com.au  Fri Apr 24 04:42:24 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 24 Apr 2015 14:42:24 +1000
Subject: [squid-users] bandwidth limiting
In-Reply-To: <5539C15F.4020601@treenet.co.nz>
References: <CAJ+Q1PXXs4Ui=_qC5NH4KKFmYrirnHn64wyF2Y1wUrkDLsznOw@mail.gmail.com>
 <5539C15F.4020601@treenet.co.nz>
Message-ID: <CAJ+Q1PUqOD0H2Vvc3NBp3csUR6jCoduCpFO4=+O5HKuJ2iAHzA@mail.gmail.com>

Thanks, I had presumed that was from squid -> client not source -> squid.



On 24 April 2015 at 14:06, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 24/04/2015 2:43 p.m., Alex Samad wrote:
>> Hi
>>
>> is there any way to limit the bandwidth squid uses to pull stuff from
>> the internet ?
>>
>> Can it slow down request, delay acks or ??
>
> http://wiki.squid-cache.org/Features/DelayPools
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Fri Apr 24 04:57:21 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 24 Apr 2015 07:57:21 +0300
Subject: [squid-users] Using Squid as a Transparent Proxy
In-Reply-To: <CAAg1D6BT=YfJk_hZWG+=Yw7ChPxbbRx9pF6AFLBO8mC0KmP=ZQ@mail.gmail.com>
References: <CAAg1D6BT=YfJk_hZWG+=Yw7ChPxbbRx9pF6AFLBO8mC0KmP=ZQ@mail.gmail.com>
Message-ID: <5539CD31.6050707@ngtech.co.il>

I have been reading and have seen two things that needs consideration.
1 - An illustration for a WCCP example at:
http://wiki.squid-cache.org/ConfigExamples/UbuntuTproxy4Wccp2#Toplogy

2 - The kernel connection tracking is needed for basic firewall 
functions which TPROXY is not really one of them, but an advanced 
mangling function that helps to run and monitor the network.

The relevant modules are mentioned in the wiki link I attached.

Eliezer

On 24/04/2015 05:29, Srinath Krishna wrote:
> The kernel that I'm using is 3.16 and it has the nf_conntrack and xt_TPROXY
> modules insmoded. Can someone help me with this?
>
> Thanks,
> Srinath



From michael at hendrie.id.au  Fri Apr 24 07:44:13 2015
From: michael at hendrie.id.au (Michael Hendrie)
Date: Fri, 24 Apr 2015 17:14:13 +0930
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <1429789953.3972.1.camel@JamesiMac>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
 <55389675.3040702@treenet.co.nz>
 <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>
 <9890F667-FE7C-49B4-A11F-192F55233BDC@hendrie.id.au>
 <1429789953.3972.1.camel@JamesiMac>
Message-ID: <DB569C7C-080E-454C-BD18-A2DE0E6579B6@hendrie.id.au>


> On 23 Apr 2015, at 9:22 pm, James Lay <jlay at slave-tothe-box.net> wrote:
> 
> Michael,
> 
> Could you post your entire config here if possible?  Many of us continue to face challenges with ssl_bump and a working config would be great.  Thank you.
> 
> James

My ssl_bump configuration is contained in a separate conf file that is ?included? via the main squid.conf file.  There is nothing special about my main squid.conf, here is the contents of the include:

https_port 8090 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl-bump.cer key=/etc/squid/ssl-bump.key cafile=/etc/squid/ssl-bump.cer
acl p8090 myportname 8090
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump p8090

Which was built using information from http://wiki.squid-cache.org/Features/SslPeekAndSplice <http://wiki.squid-cache.org/Features/SslPeekAndSplice>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/90f03be8/attachment.htm>

From mike13 at poczta.fm  Fri Apr 24 07:42:16 2015
From: mike13 at poczta.fm (smaku)
Date: Fri, 24 Apr 2015 00:42:16 -0700 (PDT)
Subject: [squid-users] access_log none acl
In-Reply-To: <5539A47C.4090307@treenet.co.nz>
References: <1429809947181-4670886.post@n4.nabble.com>
 <55393599.8060406@gmail.com> <5539A47C.4090307@treenet.co.nz>
Message-ID: <1429861336073-4670905.post@n4.nabble.com>

Thank you Amos.
 
access_log daemon:/var/squid/logs/access.log squid !myacl1 

working perfect.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/access-log-none-acl-tp4670886p4670905.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Fri Apr 24 12:01:26 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Apr 2015 06:01:26 -0600
Subject: [squid-users] ssl_bump peek in squid-3.5.3
In-Reply-To: <DB569C7C-080E-454C-BD18-A2DE0E6579B6@hendrie.id.au>
References: <D95ED8DB-7C03-49C7-8F87-B5327D719FF0@hendrie.id.au>
 <55389675.3040702@treenet.co.nz>
 <D51B28DC-0779-482A-A48B-0C83FF41442E@hendrie.id.au>
 <9890F667-FE7C-49B4-A11F-192F55233BDC@hendrie.id.au>
 <1429789953.3972.1.camel@JamesiMac>
 <DB569C7C-080E-454C-BD18-A2DE0E6579B6@hendrie.id.au>
Message-ID: <1429876886.3972.8.camel@JamesiMac>

On Fri, 2015-04-24 at 17:14 +0930, Michael Hendrie wrote:

> 
> 
> > On 23 Apr 2015, at 9:22 pm, James Lay <jlay at slave-tothe-box.net>
> > wrote:
> > 
> > Michael,
> > 
> > Could you post your entire config here if possible?  Many of us
> > continue to face challenges with ssl_bump and a working config would
> > be great.  Thank you.
> > 
> > James
> 
> 
> 
> My ssl_bump configuration is contained in a separate conf file that is
> ?included? via the main squid.conf file.  There is nothing special
> about my main squid.conf, here is the contents of the include:
> 
> 
> 
> https_port 8090 tproxy ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl-bump.cer
> key=/etc/squid/ssl-bump.key cafile=/etc/squid/ssl-bump.cer
> acl p8090 myportname 8090
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump p8090
> 
> 
> Which was built using information from
> http://wiki.squid-cache.org/Features/SslPeekAndSplice
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


That helps...thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/55c0ec2e/attachment.htm>

From jonathan_chretien at hotmail.com  Fri Apr 24 12:06:46 2015
From: jonathan_chretien at hotmail.com (Jonathan Chretien)
Date: Fri, 24 Apr 2015 08:06:46 -0400
Subject: [squid-users] HTTPS Filtering and SSL-Bump
In-Reply-To: <55397489.4010602@urlfilterdb.com>
References: <BAY182-W1DCB0628303A8D2F27C2E83ED0@phx.gbl>,
 <55397489.4010602@urlfilterdb.com>
Message-ID: <BAY182-W28B28D1B1396B57C2AA70C83EC0@phx.gbl>

Thanks.

I will give it a try.
___________________________________
Jonathan



----------------------------------------
> Date: Thu, 23 Apr 2015 19:39:05 -0300
> From: marcus.kool at urlfilterdb.com
> To: jonathan_chretien at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] HTTPS Filtering and SSL-Bump
>
>
>
> On 04/23/2015 05:52 PM, Jonathan Chretien wrote:
>> Hi all.
>>
>> I'm trying to implement the filtering of https content for a particular url. The only thing that I'm trying to do it's to unlock corporate video on the Youtube website. I do not want to unlock everything on Youtube but only our corporate stuff.
>>
>> The url looks like this: https://www.youtube.com/users/MyCompany.
>>
>> I'm using UFDBGuard as a url filter.
>>
>> The problem is that SSL-Bump is working well but the URL pass from Squid to UFDBGuard is the non SSL-Bump url. What I means is that the URL that UFDBGuard is receiving is https://www.youtube.com:443 instead of the https://www.youtube.com/users/MyCompany.
>>
>> So because UFDBGuard is not receiving the complete SSL-Bump URL, UFDBGuard see that it's Youtube.com, so it block the website. If UFDBGuard was receiving the real SSL-Bump url https://www.youtube.com/users/MyCompany, UFDBGuard will see that this url is whitelisted and should allow the access.
>
>
> This is not the full story.
> With SSLbump on Squid sends to ufdbGuard first
> CONNECT www.youtube.com:443
> and then
> GET https://www.youtube.com/users/MyCompany
>
> ufdbGuard has not yet support for this but you could whitelist
> www.youtube.com:443
> using a regular expression
> and whitelist https://www.youtube.com/users/MyCompany and a bunch of other URLs that used for the markup of the entire page
> and blacklist a bunch of other youtube URLs to get the desired behavior.
>
> Whitelisting a subset of a website is usually not so straightforward so one needs to pay much attention to the "bunch of URLs" used in the whitelist and blacklist.
> I suggest to not blacklist www.youtube.com but start with blacklisting a few important URLs of youtube such that the effective result is that non-company access to Youtube is blocked.
>
> Marcus
> maintainer of ufdbGuard.
>
> PS: after you have done all this, you also need to block all web proxies which can be used to circumvent the intended Youtube block.
>
>
>> Log in the UFDBGuard.log
>> 2015-04-23 16:19:59 [10669] BLOCK MyUser 192.168.100.27 Internet movies www.youtube.com:443 CONNECT
>>
>> Is there something missing in my Squid.conf to pass the correct URL?
>>
>> http_port 192.168.100.2:3129 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB key=/etc/squid/ssl/mycert.com.private cert=/etc/squid/ssl/mycert.com.cert
>>
>> # SSL Bump Config
>> sslproxy_cert_error deny all
>> sslproxy_flags DONT_VERIFY_PEER
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1
>>
>> acl sslBumpYoutube dstdomain www.youtube.com
>>
>> # SSL Bump Config
>> always_direct allow sslBumpYoutube
>> ssl_bump server-first sslBumpYoutube
>> ssl_bump none all
>>
>> Also all my users using the proxy are authenticated.
>>
>>
>> Thanks
>> ___________________________________
>> Jonathan
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
 		 	   		  

From jlay at slave-tothe-box.net  Fri Apr 24 12:50:46 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Apr 2015 06:50:46 -0600
Subject: [squid-users] Config audit for 3.5.3
Message-ID: <1429879846.3972.15.camel@JamesiMac>

Hey all.

Topic says it....I'm running squid-3.5.3-20150420-r13802 and wanted to
see if there's anything glaring that I'm missing/have misconfigured.  My
setup is squid is running on a router, one nic external, one nic
internal.  This is running as a transparent proxy with iptables doing a
redirect to ports 3128 and 3129.  Config below:

#############################################################
acl localnet src 192.168.1.0/24

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 443		# https

acl CONNECT method CONNECT
acl broken_sites dst 96.16.0.0/15
<others redacted>
acl broken_sites dst 54.160.0.0/12
acl allowed_sites url_regex "/opt/etc/squid/url.txt"
acl all_others dst all
acl SSL method CONNECT


http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow manager localhost
http_access deny manager

http_access allow allowed_sites
http_access allow broken_sites

http_access deny all_others 
http_access allow localnet
http_access allow localhost

http_access deny all
icp_access deny all


sslproxy_cert_error allow broken_sites
sslproxy_cert_error deny all

sslproxy_options ALL
acl p3129 myportname 3129
acl step1 at_step SslBump1
ssl_bump peek step1
#ssl_bump splice broken_sites
ssl_bump bump p3129


http_port 192.168.1.253:3128 intercept 
https_port 192.168.1.253:3129 intercept ssl-bump
cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
cafile=/opt/sslsplit/sslsplitca.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE

always_direct allow all

logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
Sh %ssl::>cert_subject

access_log syslog:daemon.info common

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

icp_port 3130

coredump_dir /opt/var
#############################################################

My goal has been to at least get the domain logged on any https access,
but alas some sites show:

Apr 24 06:39:32 gateway (squid-1): 192.168.1.101 - -
[24/Apr/2015:06:39:32 -0600] "CONNECT 216.58.216.162:443 HTTP/1.1" 200
401 TCP_TUNNEL:ORIGINAL_DST -

Thanks for the look see...trying to keep current.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/9f64c8fe/attachment.htm>

From Andreas.Reschke at mahle.com  Fri Apr 24 13:04:51 2015
From: Andreas.Reschke at mahle.com (Andreas.Reschke at mahle.com)
Date: Fri, 24 Apr 2015 15:04:51 +0200
Subject: [squid-users] squid with sssd instead of winbind
Message-ID: <OFBE3FAF11.0C1D975F-ONC1257E31.004747EC-C1257E31.0047DAF2@mahle.com>

Hi there,
is it posible to use sssd instead of ntlm_auth from samba-winbind? In the 
last days we had trouble with the connectivity from winbind to ADS.


Mit freundlichen Gr??en / Kind regards

Mr. Andreas Reschke
MAHLE International GmbH
Unix/Linux-Administration (FIG5.1)

Pragstr. 26 - 46, 70376 Stuttgart, Germany
Phone: +49 711 501-47598, Fax: +49 711 501-44 47598
Mobile: 0173-3197397
andreas.reschke at mahle.com, http://www.mahle.com

MAHLE International GmbH, Location: Stuttgart, Registered: Amtsgericht 
Stuttgart,  HRB 16413
Management Board: Prof. Dr. Heinz K. Junker, Arnd Franz, Michael Frick, 
Michael Glowatzki
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/26205266/attachment.htm>

From kris at linquist.net  Fri Apr 24 13:57:00 2015
From: kris at linquist.net (Kristopher Linquist)
Date: Fri, 24 Apr 2015 06:57:00 -0700 (PDT)
Subject: [squid-users] Bring refresh_pattern down to ~10sec?
In-Reply-To: <5539C113.9030802@treenet.co.nz>
References: <5539C113.9030802@treenet.co.nz>
Message-ID: <1429883819391.80767a7c@Nodemailer>

Thanks Amos.




By ?identical rest call? I mean a HTTP GET or POST to the same URL with the same data in the POST body of query string.




Looks like dstdomain acl along with miss_access could work - how do I adjust the timing to return a cached object if 2 requests happen within ~10sec?




Thanks!

-Kris

On Thu, Apr 23, 2015 at 9:06 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 24/04/2015 2:46 p.m., Kristopher Linquist wrote:
>> Hi,
>> 
>> 
>> I?m using squid to throttle outgoing API calls to various services.
>> 
>> 
>> I?ve got Squid working with ssl_bump and currently caching any request more often than 15 minutes with this line:
>> 
>> 
>> refresh_pattern .		15	20%	4320 override-expire ignore-reload
>> 
>> 
>> I?m interested in making sure I can?t send an identical REST call to twilio(text message service) more often than every 10 seconds.  
>> 
>> 
>> Is there any way to accomplish this?
> Depends on what you mean by "identical REST call".
> There is the miss_access control, but it only allows "fast" ACLs.
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/308a56e6/attachment.htm>

From hussam.tayeb at gmx.com  Fri Apr 24 23:54:56 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sat, 25 Apr 2015 01:54:56 +0200
Subject: [squid-users] Invalid entries
Message-ID: <trinity-6f0c55fc-842c-455c-81ce-cc15ee4a932d-1429919696070@3capp-mailcom-bs04>

On occasions, when restarting squid after very long uptime, cache.log says "1 invalid entries".
I know for sure that squid was not shut down incorrectly since the last session closed correctly "Squid Cache (Version 3.5.3): Exiting normally."
They usually disappear very quickly when squid swaps a file to disk in the same slot as the invalid object on disk so it's not always an issue. But nevertheless what is the cause of those invalid entries?
Thank you.


From squid3 at treenet.co.nz  Sat Apr 25 01:51:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Apr 2015 13:51:09 +1200
Subject: [squid-users] squid with sssd instead of winbind
In-Reply-To: <OFBE3FAF11.0C1D975F-ONC1257E31.004747EC-C1257E31.0047DAF2@mahle.com>
References: <OFBE3FAF11.0C1D975F-ONC1257E31.004747EC-C1257E31.0047DAF2@mahle.com>
Message-ID: <553AF30D.4090309@treenet.co.nz>

On 25/04/2015 1:04 a.m., Andreas.Reschke wrote:
> Hi there,
> is it posible to use sssd instead of ntlm_auth from samba-winbind? In the 
> last days we had trouble with the connectivity from winbind to ADS.
> 

Short answer: no / maybe.

>From the docs "SSSD does not support the NTLM protocol."

That would kind of prohibit its being used as a direct replacement for
ntlm_auth used for NTLM and Negotiate/NTLM auth protocols.

You could move to Negotiate/Kerberos though. Other helpers are already
provided with Squid for that. To use SSSD someone would have to figure
out how to write a helper integrating it with Squid.

Amos



From squid3 at treenet.co.nz  Sat Apr 25 01:56:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Apr 2015 13:56:20 +1200
Subject: [squid-users] Invalid entries
In-Reply-To: <trinity-6f0c55fc-842c-455c-81ce-cc15ee4a932d-1429919696070@3capp-mailcom-bs04>
References: <trinity-6f0c55fc-842c-455c-81ce-cc15ee4a932d-1429919696070@3capp-mailcom-bs04>
Message-ID: <553AF444.7020807@treenet.co.nz>

On 25/04/2015 11:54 a.m., Hussam Al-Tayeb wrote:
> On occasions, when restarting squid after very long uptime, cache.log
> says "1 invalid entries". I know for sure that squid was not shut
> down incorrectly since the last session closed correctly "Squid Cache
> (Version 3.5.3): Exiting normally." They usually disappear very
> quickly when squid swaps a file to disk in the same slot as the
> invalid object on disk so it's not always an issue. But nevertheless
> what is the cause of those invalid entries? Thank you. 

Could be several things:
* checksums within the disk object dont match the current file checksum.
* cache journal entry does not match the object found on disk at the
journal indicated location
* cache journal entry does not match its own checksum
* cache journal indicates an object exists but file on disk was
erased/missing
* disk file was not completely written out during the last shutdown
(regardless of last shutdown success/fail).


Amos


From squid3 at treenet.co.nz  Sat Apr 25 02:00:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Apr 2015 14:00:13 +1200
Subject: [squid-users] Bring refresh_pattern down to ~10sec?
In-Reply-To: <1429883819391.80767a7c@Nodemailer>
References: <5539C113.9030802@treenet.co.nz>
 <1429883819391.80767a7c@Nodemailer>
Message-ID: <553AF52D.9050008@treenet.co.nz>

On 25/04/2015 1:57 a.m., Kristopher Linquist wrote:
> Thanks Amos.
> 
> 
> 
> 
> By ?identical rest call? I mean a HTTP GET or POST to the same URL with the same data in the POST body of query string.
> 

I was afraid of that. Then the answer is no its not possible.

Squid does not touch the body/payload of messages, meaning it has no
information about what is inside any given POST message payload.

> 
> Looks like dstdomain acl along with miss_access could work - how do I adjust the timing to return a cached object if 2 requests happen within ~10sec?
> 

Thats where the "fast" detail gets in the way. The closest you will get
in Squid is an external ACL helper ("slow" operation - ouch) being
passed the method and URL and returning OK/ERR about whether or not the
pair has been requested in the past N seconds.

Amos



From squid3 at treenet.co.nz  Sat Apr 25 02:25:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Apr 2015 14:25:12 +1200
Subject: [squid-users] Config audit for 3.5.3
In-Reply-To: <1429879846.3972.15.camel@JamesiMac>
References: <1429879846.3972.15.camel@JamesiMac>
Message-ID: <553AFB08.2090900@treenet.co.nz>

On 25/04/2015 12:50 a.m., James Lay wrote:
> Hey all.
> 
> Topic says it....I'm running squid-3.5.3-20150420-r13802 and wanted to
> see if there's anything glaring that I'm missing/have misconfigured.  My
> setup is squid is running on a router, one nic external, one nic
> internal.  This is running as a transparent proxy with iptables doing a
> redirect to ports 3128 and 3129.  Config below:
> 
> #############################################################
> acl localnet src 192.168.1.0/24
> 
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 443		# https
> 
> acl CONNECT method CONNECT
> acl broken_sites dst 96.16.0.0/15
> <others redacted>
> acl broken_sites dst 54.160.0.0/12
> acl allowed_sites url_regex "/opt/etc/squid/url.txt"
> acl all_others dst all

Using "dst all" is very inefficient. It requires Squid to perform DNS
lookups just to answer "yes". Unless there is some unusual reason
requiring that you might as well use the provided "all" ACL for faster
operation.


> acl SSL method CONNECT

This is a bit dangerous. CONNECT does not necessarily mean SSL - even
with the port 443 restriction.  CONNECT could as easily contain a tunnel
to email server and be pumping spam, or literally any other type of
traffic to any other server. Spam emails, FTP, BitTorrent, and Skype are
pretty popular protocols seen with CONNECT.

So you can easily mistake security rules about SSL and create allow
policies that make you vulnerable to some nasty attacks.

Its also a redundant ACL definition with the default CONNECT ACL earlier.

> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow manager localhost
> http_access deny manager
> 
> http_access allow allowed_sites
> http_access allow broken_sites
> 
> http_access deny all_others 

The above being equivalent to "deny all" makes the below rules not do
anything. I dont know yoru policy, maybe you did.

Consider whether that is what you expected/wanted to happen.


> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> icp_access deny all
> 
> 
> sslproxy_cert_error allow broken_sites
> sslproxy_cert_error deny all
> 
> sslproxy_options ALL
> acl p3129 myportname 3129

This name "3129" does not match any listening port name. See below...


> acl step1 at_step SslBump1
> ssl_bump peek step1
> #ssl_bump splice broken_sites
> ssl_bump bump p3129
> 
> 
> http_port 192.168.1.253:3128 intercept 

... in the absence of a name= parameter the default name for tis port is
"192.168.1.253:3128".

> https_port 192.168.1.253:3129 intercept ssl-bump
> cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
> cafile=/opt/sslsplit/sslsplitca.pem generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE

... in the absence of a name= parameter the default name for tis port is
"192.168.1.253:3129".

Do you see the pattern?
 set the name= parameter eplicitly or it becomes teh *string* value of
the host:port field.


> 
> always_direct allow all

Has no use in your config.

> 
> logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
> Sh %ssl::>cert_subject

Bad: do not re-define built in format definitions please.

Either use the provided format, or use a different name if you need the
custom one.

> 
> access_log syslog:daemon.info common
> 
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (cgi-bin|\?)	0	0%	0
> refresh_pattern .		0	20%	4320
> 
> icp_port 3130

You are initializing ICP port, but also configured "icp_access deny all".

To disble ICP leave remove the icp_* directives from your config.

To enable ICP, configure the icp_access to allow some sources to make
queries.

> 
> coredump_dir /opt/var
> #############################################################
> 
> My goal has been to at least get the domain logged on any https access,
> but alas some sites show:
> 
> Apr 24 06:39:32 gateway (squid-1): 192.168.1.101 - -
> [24/Apr/2015:06:39:32 -0600] "CONNECT 216.58.216.162:443 HTTP/1.1" 200
> 401 TCP_TUNNEL:ORIGINAL_DST -
> 

With interception + your custom rule using %ru you should always see
raw-IP:port. If you see a TLS SNI domain in there *that* is a bug. "%ru"
is explicitly asking for the client-presented CONNECT *URL*, not the
server details.


That "TCP_TUNNEL" will always happen whenever the protocol found on port
443 is not HTTPS.

Amos


From dan at getbusi.com  Sat Apr 25 03:09:53 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 24 Apr 2015 20:09:53 -0700 (PDT)
Subject: [squid-users] Config audit for 3.5.3
In-Reply-To: <553AFB08.2090900@treenet.co.nz>
References: <553AFB08.2090900@treenet.co.nz>
Message-ID: <1429931393201.49bd638d@Nodemailer>

This was pretty interesting and informative ?despite the egregious typos ? ? thanks Amos!

On Sat, Apr 25, 2015 at 12:25 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 25/04/2015 12:50 a.m., James Lay wrote:
>> Hey all.
>> 
>> Topic says it....I'm running squid-3.5.3-20150420-r13802 and wanted to
>> see if there's anything glaring that I'm missing/have misconfigured.  My
>> setup is squid is running on a router, one nic external, one nic
>> internal.  This is running as a transparent proxy with iptables doing a
>> redirect to ports 3128 and 3129.  Config below:
>> 
>> #############################################################
>> acl localnet src 192.168.1.0/24
>> 
>> acl SSL_ports port 443
>> acl Safe_ports port 80		# http
>> acl Safe_ports port 443		# https
>> 
>> acl CONNECT method CONNECT
>> acl broken_sites dst 96.16.0.0/15
>> <others redacted>
>> acl broken_sites dst 54.160.0.0/12
>> acl allowed_sites url_regex "/opt/etc/squid/url.txt"
>> acl all_others dst all
> Using "dst all" is very inefficient. It requires Squid to perform DNS
> lookups just to answer "yes". Unless there is some unusual reason
> requiring that you might as well use the provided "all" ACL for faster
> operation.
>> acl SSL method CONNECT
> This is a bit dangerous. CONNECT does not necessarily mean SSL - even
> with the port 443 restriction.  CONNECT could as easily contain a tunnel
> to email server and be pumping spam, or literally any other type of
> traffic to any other server. Spam emails, FTP, BitTorrent, and Skype are
> pretty popular protocols seen with CONNECT.
> So you can easily mistake security rules about SSL and create allow
> policies that make you vulnerable to some nasty attacks.
> Its also a redundant ACL definition with the default CONNECT ACL earlier.
>> 
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> 
>> http_access allow manager localhost
>> http_access deny manager
>> 
>> http_access allow allowed_sites
>> http_access allow broken_sites
>> 
>> http_access deny all_others 
> The above being equivalent to "deny all" makes the below rules not do
> anything. I dont know yoru policy, maybe you did.
> Consider whether that is what you expected/wanted to happen.
>> http_access allow localnet
>> http_access allow localhost
>> 
>> http_access deny all
>> icp_access deny all
>> 
>> 
>> sslproxy_cert_error allow broken_sites
>> sslproxy_cert_error deny all
>> 
>> sslproxy_options ALL
>> acl p3129 myportname 3129
> This name "3129" does not match any listening port name. See below...
>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> #ssl_bump splice broken_sites
>> ssl_bump bump p3129
>> 
>> 
>> http_port 192.168.1.253:3128 intercept 
> ... in the absence of a name= parameter the default name for tis port is
> "192.168.1.253:3128".
>> https_port 192.168.1.253:3129 intercept ssl-bump
>> cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
>> cafile=/opt/sslsplit/sslsplitca.pem generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE
> ... in the absence of a name= parameter the default name for tis port is
> "192.168.1.253:3129".
> Do you see the pattern?
>  set the name= parameter eplicitly or it becomes teh *string* value of
> the host:port field.
>> 
>> always_direct allow all
> Has no use in your config.
>> 
>> logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
>> Sh %ssl::>cert_subject
> Bad: do not re-define built in format definitions please.
> Either use the provided format, or use a different name if you need the
> custom one.
>> 
>> access_log syslog:daemon.info common
>> 
>> refresh_pattern ^ftp:		1440	20%	10080
>> refresh_pattern ^gopher:	1440	0%	1440
>> refresh_pattern -i (cgi-bin|\?)	0	0%	0
>> refresh_pattern .		0	20%	4320
>> 
>> icp_port 3130
> You are initializing ICP port, but also configured "icp_access deny all".
> To disble ICP leave remove the icp_* directives from your config.
> To enable ICP, configure the icp_access to allow some sources to make
> queries.
>> 
>> coredump_dir /opt/var
>> #############################################################
>> 
>> My goal has been to at least get the domain logged on any https access,
>> but alas some sites show:
>> 
>> Apr 24 06:39:32 gateway (squid-1): 192.168.1.101 - -
>> [24/Apr/2015:06:39:32 -0600] "CONNECT 216.58.216.162:443 HTTP/1.1" 200
>> 401 TCP_TUNNEL:ORIGINAL_DST -
>> 
> With interception + your custom rule using %ru you should always see
> raw-IP:port. If you see a TLS SNI domain in there *that* is a bug. "%ru"
> is explicitly asking for the client-presented CONNECT *URL*, not the
> server details.
> That "TCP_TUNNEL" will always happen whenever the protocol found on port
> 443 is not HTTPS.
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/9c0af2b7/attachment.htm>

From hack.back at hotmail.com  Sat Apr 25 10:43:43 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 25 Apr 2015 03:43:43 -0700 (PDT)
Subject: [squid-users] loop 302
Message-ID: <1429958623373-4670917.post@n4.nabble.com>

hello,
after caching dynamic youtube i got loop 302 or some videos,
is there any patch for this issue,?
thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/loop-302-tp4670917.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sat Apr 25 11:10:18 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 25 Apr 2015 04:10:18 -0700 (PDT)
Subject: [squid-users] loop 302
In-Reply-To: <1429958623373-4670917.post@n4.nabble.com>
References: <1429958623373-4670917.post@n4.nabble.com>
Message-ID: <1429960218498-4670918.post@n4.nabble.com>

--- src/Server.cc
+++ src/Server.cc
@@ -31,6 +31,7 @@
  */
 
 #include "squid.h"
+#include "acl/FilledChecklist.h"
 #include "acl/Gadgets.h"
 #include "base/TextException.h"
 #include "comm/Connection.h"
@@ -174,6 +175,8 @@
     // give entry the reply because haveParsedReplyHeaders() expects it
there
     entry->replaceHttpReply(theFinalReply, false); // but do not write yet
     haveParsedReplyHeaders(); // update the entry/reply (e.g., set
timestamps)
+    if (EBIT_TEST(entry->flags, ENTRY_CACHABLE) && blockCaching())
+        entry->release();
     entry->startWriting(); // write the updated entry to store
 
     return theFinalReply;
@@ -533,6 +536,24 @@
     currentOffset = partial ? theFinalReply->content_range->spec.offset :
0;
 }
 
+/// whether to prevent caching of an otherwise cachable response
+bool
+ServerStateData::blockCaching()
+{
+    if (const Acl::Tree *acl = Config.accessList.storeMiss) {
+        // This relatively expensive check is not in
StoreEntry::checkCachable:
+        // That method lacks HttpRequest and may be called too many times.
+        ACLFilledChecklist ch(acl, originalRequest(), NULL);
+        ch.reply = const_cast<HttpReply*>(entry->getReply()); //
ACLFilledChecklist API bug
+        HTTPMSGLOCK(ch.reply);
+        if (ch.fastCheck() != ACCESS_ALLOWED) { // when in doubt, block
+            debugs(20, 3, "store_miss prohibits caching");
+            return true;
+        }
+    }
+    return false;
+}
+
 HttpRequest *
 ServerStateData::originalRequest()
 {
--- src/Server.h
+++ src/Server.h
@@ -131,6 +131,8 @@
     /// Entry-dependent callbacks use this check to quit if the entry went
bad
     bool abortOnBadEntry(const char *abortReason);
 
+    bool blockCaching();
+
 #if USE_ADAPTATION
     void startAdaptation(const Adaptation::ServiceGroupPointer &group,
HttpRequest *cause);
     void adaptVirginReplyBody(const char *buf, ssize_t len);
--- src/SquidConfig.h
+++ src/SquidConfig.h
@@ -375,6 +375,8 @@
         acl_access *AlwaysDirect;
         acl_access *ASlists;
         acl_access *noCache;
+        acl_access *sendHit;
+        acl_access *storeMiss;
         acl_access *stats_collection;
 #if SQUID_SNMP
 
--- src/cf.data.pre
+++ src/cf.data.pre
@@ -4843,18 +4843,97 @@
 NAME: cache no_cache
 TYPE: acl_access
 DEFAULT: none
-DEFAULT_DOC: Allow caching, unless rules exist in squid.conf.
+DEFAULT_DOC: By default, this directive is unused and has no effect.
 LOC: Config.accessList.noCache
 DOC_START
-	A list of ACL elements which, if matched and denied, cause the request to
-	not be satisfied from the cache and the reply to not be cached.
-	In other words, use this to force certain objects to never be cached.
-
-	You must use the words 'allow' or 'deny' to indicate whether items
-	matching the ACL should be allowed or denied into the cache.
+	Requests denied by this directive will not be served from the cache
+	and their responses will not be stored in the cache. This directive
+	has no effect on other transactions and on already cached responses.
 
 	This clause supports both fast and slow acl types.
 	See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	This and the two other similar caching directives listed below are
+	checked at different transaction processing stages, have different
+	access to response information, affect different cache operations,
+	and differ in slow ACLs support:
+
+	* cache: Checked before Squid makes a hit/miss determination.
+		No access to reply information!
+		Denies both serving a hit and storing a miss.
+		Supports both fast and slow ACLs.
+	* send_hit: Checked after a hit was detected.
+		Has access to reply (hit) information.
+		Denies serving a hit only.
+		Supports fast ACLs only.
+	* store_miss: Checked before storing a cachable miss.
+		Has access to reply (miss) information.
+		Denies storing a miss only.
+		Supports fast ACLs only.
+
+	If you are not sure which of the three directives to use, apply the
+	following decision logic:
+
+	* If your ACL(s) are of slow type _and_ need response info, redesign.
+	  Squid does not support that particular combination at this time.
+        Otherwise:
+	* If your directive ACL(s) are of slow type, use "cache"; and/or
+	* if your directive ACL(s) need no response info, use "cache".
+        Otherwise:
+	* If you do not want the response cached, use store_miss; and/or
+	* if you do not want a hit on a cached response, use send_hit.
+DOC_END
+
+NAME: send_hit
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.sendHit
+DOC_START
+	Responses denied by this directive will not be served from the cache
+	(but may still be cached, see store_miss). This directive has no
+	effect on the responses it allows and on the cached objects.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives.
+
+	Unlike the "cache" directive, send_hit only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	For example:
+
+		# apply custom Store ID mapping to some URLs
+		acl MapMe dstdomain .c.example.com
+		store_id_program ...
+		store_id_access allow MapMe
+
+		# but prevent caching of special responses
+		# such as 302 redirects that cause StoreID loops
+		acl Ordinary http_status 200-299
+		store_miss deny MapMe !Ordinary
+
+		# and do not serve any previously stored special responses
+		# from the cache (in case they were already cached before
+		# the above store_miss rule was in effect).
+		send_hit deny MapMe !Ordinary
+DOC_END
+
+NAME: store_miss
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.storeMiss
+DOC_START
+	Responses denied by this directive will not be cached (but may still
+	be served from the cache, see send_hit). This directive has no
+	effect on the responses it allows and on the already cached responses.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives. See the
+	send_hit directive for a usage example.
+
+	Unlike the "cache" directive, store_miss only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
 DOC_END
 
 NAME: max_stale
--- src/client_side_reply.cc
+++ src/client_side_reply.cc
@@ -545,6 +545,11 @@
        ) {
         http->logType = LOG_TCP_NEGATIVE_HIT;
         sendMoreData(result);
+    } else if (blockedHit()) {
+        debugs(88, 5, "send_hit forces a MISS");
+        http->logType = LOG_TCP_MISS;
+        processMiss();
+        return;
     } else if (!http->flags.internal && refreshCheckHTTP(e, r)) {
         debugs(88, 5, "clientCacheHit: in refreshCheck() block");
         /*
@@ -773,6 +778,30 @@
     }
 }
 
+/// whether squid.conf send_hit prevents us from serving this hit
+bool
+clientReplyContext::blockedHit() const
+{
+    if (!Config.accessList.sendHit)
+        return false; // hits are not blocked by default
+
+    if (http->flags.internal)
+        return false; // internal content "hits" cannot be blocked
+
+    if (const HttpReply *rep = http->storeEntry()->getReply()) {
+        std::auto_ptr<ACLFilledChecklist>
chl(clientAclChecklistCreate(Config.accessList.sendHit, http));
+        chl->reply = const_cast<HttpReply*>(rep); // ACLChecklist API bug
+        HTTPMSGLOCK(chl->reply);
+        return chl->fastCheck() != ACCESS_ALLOWED; // when in doubt, block
+    }
+
+    // This does not happen, I hope, because we are called from CacheHit,
which
+    // is called via a storeClientCopy() callback, and store should
initialize
+    // the reply before calling that callback.
+    debugs(88, 3, "Missing reply!");
+    return false;
+}
+
 void
 clientReplyContext::purgeRequestFindObjectToPurge()
 {
--- src/client_side_reply.h
+++ src/client_side_reply.h
@@ -140,6 +140,7 @@
     void triggerInitialStoreRead();
     void sendClientOldEntry();
     void purgeAllCached();
+    bool blockedHit() const;
 
     void sendBodyTooLargeError();
     void sendPreconditionFailedError();




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/loop-302-tp4670917p4670918.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Sat Apr 25 12:36:14 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 25 Apr 2015 06:36:14 -0600
Subject: [squid-users] Config audit for 3.5.3
In-Reply-To: <553AFB08.2090900@treenet.co.nz>
References: <1429879846.3972.15.camel@JamesiMac>
 <553AFB08.2090900@treenet.co.nz>
Message-ID: <1429965374.3969.0.camel@JamesiMac>

On Sat, 2015-04-25 at 14:25 +1200, Amos Jeffries wrote:

> On 25/04/2015 12:50 a.m., James Lay wrote:
> > Hey all.
> > 
> > Topic says it....I'm running squid-3.5.3-20150420-r13802 and wanted to
> > see if there's anything glaring that I'm missing/have misconfigured.  My
> > setup is squid is running on a router, one nic external, one nic
> > internal.  This is running as a transparent proxy with iptables doing a
> > redirect to ports 3128 and 3129.  Config below:
> > 
> > #############################################################
> > acl localnet src 192.168.1.0/24
> > 
> > acl SSL_ports port 443
> > acl Safe_ports port 80		# http
> > acl Safe_ports port 443		# https
> > 
> > acl CONNECT method CONNECT
> > acl broken_sites dst 96.16.0.0/15
> > <others redacted>
> > acl broken_sites dst 54.160.0.0/12
> > acl allowed_sites url_regex "/opt/etc/squid/url.txt"
> > acl all_others dst all
> 
> Using "dst all" is very inefficient. It requires Squid to perform DNS
> lookups just to answer "yes". Unless there is some unusual reason
> requiring that you might as well use the provided "all" ACL for faster
> operation.
> 
> 
> > acl SSL method CONNECT
> 
> This is a bit dangerous. CONNECT does not necessarily mean SSL - even
> with the port 443 restriction.  CONNECT could as easily contain a tunnel
> to email server and be pumping spam, or literally any other type of
> traffic to any other server. Spam emails, FTP, BitTorrent, and Skype are
> pretty popular protocols seen with CONNECT.
> 
> So you can easily mistake security rules about SSL and create allow
> policies that make you vulnerable to some nasty attacks.
> 
> Its also a redundant ACL definition with the default CONNECT ACL earlier.
> 
> > 
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > 
> > http_access allow manager localhost
> > http_access deny manager
> > 
> > http_access allow allowed_sites
> > http_access allow broken_sites
> > 
> > http_access deny all_others 
> 
> The above being equivalent to "deny all" makes the below rules not do
> anything. I dont know yoru policy, maybe you did.
> 
> Consider whether that is what you expected/wanted to happen.
> 
> 
> > http_access allow localnet
> > http_access allow localhost
> > 
> > http_access deny all
> > icp_access deny all
> > 
> > 
> > sslproxy_cert_error allow broken_sites
> > sslproxy_cert_error deny all
> > 
> > sslproxy_options ALL
> > acl p3129 myportname 3129
> 
> This name "3129" does not match any listening port name. See below...
> 
> 
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > #ssl_bump splice broken_sites
> > ssl_bump bump p3129
> > 
> > 
> > http_port 192.168.1.253:3128 intercept 
> 
> ... in the absence of a name= parameter the default name for tis port is
> "192.168.1.253:3128".
> 
> > https_port 192.168.1.253:3129 intercept ssl-bump
> > cert=/opt/sslsplit/sslsplit.crt key=/opt/sslsplit/sslsplitca.key
> > cafile=/opt/sslsplit/sslsplitca.pem generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE
> 
> ... in the absence of a name= parameter the default name for tis port is
> "192.168.1.253:3129".
> 
> Do you see the pattern?
>  set the name= parameter eplicitly or it becomes teh *string* value of
> the host:port field.
> 
> 
> > 
> > always_direct allow all
> 
> Has no use in your config.
> 
> > 
> > logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
> > Sh %ssl::>cert_subject
> 
> Bad: do not re-define built in format definitions please.
> 
> Either use the provided format, or use a different name if you need the
> custom one.
> 
> > 
> > access_log syslog:daemon.info common
> > 
> > refresh_pattern ^ftp:		1440	20%	10080
> > refresh_pattern ^gopher:	1440	0%	1440
> > refresh_pattern -i (cgi-bin|\?)	0	0%	0
> > refresh_pattern .		0	20%	4320
> > 
> > icp_port 3130
> 
> You are initializing ICP port, but also configured "icp_access deny all".
> 
> To disble ICP leave remove the icp_* directives from your config.
> 
> To enable ICP, configure the icp_access to allow some sources to make
> queries.
> 
> > 
> > coredump_dir /opt/var
> > #############################################################
> > 
> > My goal has been to at least get the domain logged on any https access,
> > but alas some sites show:
> > 
> > Apr 24 06:39:32 gateway (squid-1): 192.168.1.101 - -
> > [24/Apr/2015:06:39:32 -0600] "CONNECT 216.58.216.162:443 HTTP/1.1" 200
> > 401 TCP_TUNNEL:ORIGINAL_DST -
> > 
> 
> With interception + your custom rule using %ru you should always see
> raw-IP:port. If you see a TLS SNI domain in there *that* is a bug. "%ru"
> is explicitly asking for the client-presented CONNECT *URL*, not the
> server details.
> 
> 
> That "TCP_TUNNEL" will always happen whenever the protocol found on port
> 443 is not HTTPS.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Thank you Amos...that's extremely helpful.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150425/39a33dfb/attachment.htm>

From hack.back at hotmail.com  Sat Apr 25 20:20:57 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 25 Apr 2015 13:20:57 -0700 (PDT)
Subject: [squid-users] installing squid 3.5.3
Message-ID: <1429993257909-4670920.post@n4.nabble.com>

root at debian:~/squid-3.5.3-20150423-r13808# /etc/init.d/squid restart
[warn] Restarting Squid HTTP Proxy 3.X: squid[....] Creating Squid HTTP
Proxy 3.X cache structure ... (warning).
FATAL: xstrdup: tried to dup a NULL pointer!

Squid Cache (Version 3.5.3-20150423-r13808): Terminated abnormally.
CPU Usage: 0.016 seconds = 0.012 user + 0.004 sys
Maximum Resident Size: 44848 KB
Page faults with physical i/o: 0
Aborted
FATAL: xstrdup: tried to dup a NULL pointer!

Squid Cache (Version 3.5.3-20150423-r13808): Terminated abnormally.
CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
Maximum Resident Size: 44848 KB
Page faults with physical i/o: 0
Aborted
 failed!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/installing-squid-3-5-3-tp4670920.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Apr 26 03:09:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Apr 2015 15:09:35 +1200
Subject: [squid-users] installing squid 3.5.3
In-Reply-To: <1429993257909-4670920.post@n4.nabble.com>
References: <1429993257909-4670920.post@n4.nabble.com>
Message-ID: <553C56EF.9080202@treenet.co.nz>

On 26/04/2015 8:20 a.m., HackXBack wrote:
> root at debian:~/squid-3.5.3-20150423-r13808# /etc/init.d/squid restart
> [warn] Restarting Squid HTTP Proxy 3.X: squid[....] Creating Squid HTTP
> Proxy 3.X cache structure ... (warning).
> FATAL: xstrdup: tried to dup a NULL pointer!
> 
> Squid Cache (Version 3.5.3-20150423-r13808): Terminated abnormally.
> CPU Usage: 0.016 seconds = 0.012 user + 0.004 sys
> Maximum Resident Size: 44848 KB
> Page faults with physical i/o: 0
> Aborted
> FATAL: xstrdup: tried to dup a NULL pointer!
> 
> Squid Cache (Version 3.5.3-20150423-r13808): Terminated abnormally.
> CPU Usage: 0.020 seconds = 0.004 user + 0.016 sys
> Maximum Resident Size: 44848 KB
> Page faults with physical i/o: 0
> Aborted
>  failed!
> 


Thanks, HackXBack

I guess you should know by now the next message is "please locate and
supply a backtrace".


Amos




From hack.back at hotmail.com  Sun Apr 26 12:06:58 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 26 Apr 2015 05:06:58 -0700 (PDT)
Subject: [squid-users] installing squid 3.5.3
In-Reply-To: <553C56EF.9080202@treenet.co.nz>
References: <1429993257909-4670920.post@n4.nabble.com>
 <553C56EF.9080202@treenet.co.nz>
Message-ID: <1430050018006-4670922.post@n4.nabble.com>

Thanks Amos,
I know That, But no coredumps generated , it cant start at the first start
after the installation,
i just finish the installation and want to start it but not success,
configure options :
squid -v
Squid Cache: Version 3.5.3-20150423-r13808
Service Name: squid
configure options:  '--prefix=/usr' '--exec_prefix=/usr'
'--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--sysconfdir=/etc/squid' '--localstatedir=/var/spool/squid'
'--enable-http-violations' '--datadir=/usr/share/squid'
'--enable-async-io=32' '--with-aufs-threads=32' '--with-included-ltdl'
'--enable-ltdl-convenience' '--with-pthreads' '--enable-storeio=aufs'
'--enable-icap-client' '--enable-kill-parent-hack' '--disable-wccp'
'--disable-wccpv2' '--enable-ssl' '--enable-linux-netfilter'
'--disable-ident-lookups' '--disable-select' '--enable-ssl-crtd'
'--enable-zph-qos' '--enable-arp-acl' '--enable-epoll'
'--enable-removal-policies=lru,heap' '--enable-snmp' '--enable-referer-log'
'--disable-unlinkd' '--enable-x-accelerator-vary' '--with-dl'
'--with-openssl' '--enable-truncate' '--enable-useragent-log' '--enable-eui'
'--enable-esi' '--with-aio' '--enable-follow-x-forwarded-for'
'--enable-large-cache-files' '--with-large-files' '--with-maxfd=65536'
'CFLAGS=-Wall -g -O3 -march=native -mtune=native -pipe -DNUMTHREADS=60
-fomit-frame-pointer -fno-strict-aliasing -funroll-loops -ffast-math
-fno-exceptions' 'LDFLAGS=-Wl,-Bsymbolic-functions'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/installing-squid-3-5-3-tp4670920p4670922.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Apr 26 14:51:05 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 26 Apr 2015 07:51:05 -0700 (PDT)
Subject: [squid-users] BUG 3556: FD 1563 is not an open socket.
Message-ID: <1430059865410-4670923.post@n4.nabble.com>

in squid 3.5.3 cache.log
but it works normally , 
what this msg mean ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/BUG-3556-FD-1563-is-not-an-open-socket-tp4670923.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Apr 26 15:07:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 26 Apr 2015 21:07:54 +0600
Subject: [squid-users] BUG 3556: FD 1563 is not an open socket.
In-Reply-To: <1430059865410-4670923.post@n4.nabble.com>
References: <1430059865410-4670923.post@n4.nabble.com>
Message-ID: <553CFF4A.4020606@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Man, you decided to gather all possible errors? :)

26.04.15 20:51, HackXBack ?????:
> in squid 3.5.3 cache.log
> but it works normally ,
> what this msg mean ?
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/BUG-3556-FD-1563-is-not-an-open-socket-tp4670923.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVPP9JAAoJENNXIZxhPexG9e0H/RhLBRFQqa0zEmLLGw/yXNl5
7GRNXyma1HC1ArRUM/6ndHt45RFxYWlOydeqihLTBcNcvYSW91xVa8J4C3N25vAn
1Slt2lSk8Y0vYS7X/VYuRnE/IAuljawNIwbZXKntmTp/gyoagjzcVjIvYPdUkysV
IH3l4rY7cEDKg2M0GUv5WP3Ec3Bin8Gh2X5tvY5Flewig6wvhonzyL2RhQxqs5EN
AW/NtNxIxJ47KyABjUJ4vXunY6hbvBo7YCcVWci3FD5U6rrtRe+XvViv3AlCN3Wr
iDmhRvme/Y7LQd9LgB6lhfnW/nh4ptdyeEgM6eV3ytj7CxWP/vPsyCsoI18ft28=
=YOt0
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Sun Apr 26 14:55:25 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 26 Apr 2015 07:55:25 -0700 (PDT)
Subject: [squid-users] BUG 3556: FD 1563 is not an open socket.
In-Reply-To: <1430059865410-4670923.post@n4.nabble.com>
References: <1430059865410-4670923.post@n4.nabble.com>
Message-ID: <1430060125635-4670925.post@n4.nabble.com>

also this error and squid restart 

2015/04/26 14:07:39 kid1| assertion failed: comm.cc:887: "F->type !=
FD_FILE"


with this backtrace report

root at debian:~# gdb /usr/sbin/squid /coredump/core
GNU gdb (GDB) 7.4.1-debian
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later
<http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/sbin/squid...(no debugging symbols found)...done.
[New LWP 31998]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f6935260165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f6935260165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f69352633e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00000000006ade3e in fatal_dump(char const*) ()
#3  0x0000000000944ea9 in xcalloc ()
#4  0x0000000000636cce in cacheDigestInit(CacheDigest*, int, int) ()
#5  0x0000000000636e27 in cacheDigestCreate(int, int) ()
#6  0x0000000000769b1d in storeDigestInit() ()
#7  0x000000000075e87f in storeInit() ()
#8  0x000000000070c159 in mainInitialize() ()
#9  0x000000000070ce43 in SquidMain(int, char**) ()
#10 0x000000000070c3f9 in SquidMainSafe(int, char**) ()
#11 0x000000000070c3d6 in main ()




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/BUG-3556-FD-1563-is-not-an-open-socket-tp4670923p4670925.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Apr 26 15:03:44 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 26 Apr 2015 08:03:44 -0700 (PDT)
Subject: [squid-users] BUG 3556: FD 1563 is not an open socket.
In-Reply-To: <553CFF4A.4020606@gmail.com>
References: <1430059865410-4670923.post@n4.nabble.com>
 <553CFF4A.4020606@gmail.com>
Message-ID: <1430060624726-4670926.post@n4.nabble.com>

what i should do i upgraded to 3.5.3 because of this error
 	
assertion failed: comm.cc:178: "fd_table[conn->fd].halfClosedReader != NULL"

and now i face this error 

2015/04/26 14:07:39 kid1| assertion failed: comm.cc:887: "F->type !=
FD_FILE" 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/BUG-3556-FD-1563-is-not-an-open-socket-tp4670923p4670926.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From szabados0701 at gmail.com  Sun Apr 26 22:07:19 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Mon, 27 Apr 2015 00:07:19 +0200
Subject: [squid-users] Scaling guide?
Message-ID: <CADddWko1cyUGtsR1hadE6B4qa45yejq=eZPN-Uxo=G_jMW=bJg@mail.gmail.com>

Hi,

I want to use squid for url filtering. Is there a scaling guide for this?
For the first try, I totally overwhelmed it.

Thanks,
Balazs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/5547c9ba/attachment.htm>

From squid3 at treenet.co.nz  Mon Apr 27 01:39:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Apr 2015 13:39:56 +1200
Subject: [squid-users] Scaling guide?
In-Reply-To: <CADddWko1cyUGtsR1hadE6B4qa45yejq=eZPN-Uxo=G_jMW=bJg@mail.gmail.com>
References: <CADddWko1cyUGtsR1hadE6B4qa45yejq=eZPN-Uxo=G_jMW=bJg@mail.gmail.com>
Message-ID: <553D936C.6010706@treenet.co.nz>

On 27/04/2015 10:07 a.m., Bal?zs Szabados wrote:
> Hi,
> 
> I want to use squid for url filtering. Is there a scaling guide for this?
> For the first try, I totally overwhelmed it.

"URL filtering" and scaling are both such a wide areas of possibility,
its hard to tell what you are actually asking for.

Can you expand on what you are trying to achieve?
and whats limits you seem to have hit?

Amos



From squid3 at treenet.co.nz  Mon Apr 27 01:42:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Apr 2015 13:42:54 +1200
Subject: [squid-users] installing squid 3.5.3
In-Reply-To: <1430050018006-4670922.post@n4.nabble.com>
References: <1429993257909-4670920.post@n4.nabble.com>
 <553C56EF.9080202@treenet.co.nz> <1430050018006-4670922.post@n4.nabble.com>
Message-ID: <553D941E.40501@treenet.co.nz>

On 27/04/2015 12:06 a.m., HackXBack wrote:
> Thanks Amos,
> I know That, But no coredumps generated , it cant start at the first start
> after the installation,
> i just finish the installation and want to start it but not success,


That makes it easier to debug :-)

Run this command line to start Squid
  gdb /usr/sbin/squid

Then these gdb commands to get the backtrace:
 # GDB commands
 handle SIGPIPE pass nostop noprint
 handle SIGTERM pass nostop noprint
 handle SIGUSR1 pass nostop noprint
 handle SIGSEGV stop
 handle SIGABRT stop
 run -NYC
 backtrace


Amos



From kukuh.amukti at gmail.com  Mon Apr 27 02:01:32 2015
From: kukuh.amukti at gmail.com (kukuh amukti)
Date: Mon, 27 Apr 2015 09:01:32 +0700
Subject: [squid-users] MSKTUTIL : Error: ldap_sasl_interactive_bind_s failed
	(Local error)
Message-ID: <CAKHWrNGPZw2v44aO5_=F8C53XcJos_Mssc2vG73Owjr8DnPCtA@mail.gmail.com>

Dear Amos,

i have problem when trying to get keytab using msktutil.

SASL/GSSAPI authentication started
Error: ldap_sasl_interactive_bind_s failed (Local error)
Error: ldap_connect failed
--> Is your kerberos ticket expired? You might try re-"kinit"ing.
 -- ~KRB5Context: Destroying Kerberos Context

in auth.log say :  msktutil: GSSAPI Error: Unspecified GSS failure.  Minor
code may provide more information (Ticket expired)


what should i do?

thanks,
kukuhga
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/c7110d07/attachment.htm>

From hierony_milanisti at yahoo.co.id  Mon Apr 27 02:36:29 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Mon, 27 Apr 2015 02:36:29 +0000 (UTC)
Subject: [squid-users] Failover in Squid 3.1.10
Message-ID: <921192801.2884322.1430102189858.JavaMail.yahoo@mail.yahoo.com>

Dear Fellow,
I want to implement Failover in My system, I have 3 Squid proxy servers right now. they are :

- Child (172.30.20.200/16)???????? # receive request, and forward it either to Parent1 or Parent2
- Parent1 (172.31.20.201/16)
- Parent2 (172.31.20.204/16)

I use Mikrotik to connect all the servers, and they have been connected and they can work together (distributed caching). For the caching algorithm, I use round-robin algorithm. 
How can i implement Failover in my system?, so that when Child proxy is down/crash the request move to another Parent proxy.

Thanks in advance.

?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/04a9a12f/attachment.htm>

From squid3 at treenet.co.nz  Mon Apr 27 02:56:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Apr 2015 14:56:09 +1200
Subject: [squid-users] Failover in Squid 3.1.10
In-Reply-To: <921192801.2884322.1430102189858.JavaMail.yahoo@mail.yahoo.com>
References: <921192801.2884322.1430102189858.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <553DA549.2030702@treenet.co.nz>

On 27/04/2015 2:36 p.m., Hierony Manurung wrote:
> Dear Fellow,
> I want to implement Failover in My system, I have 3 Squid proxy servers right now. they are :
> 
> - Child (172.30.20.200/16)         # receive request, and forward it either to Parent1 or Parent2
> - Parent1 (172.31.20.201/16)
> - Parent2 (172.31.20.204/16)
> 
> I use Mikrotik to connect all the servers, and they have been connected and they can work together (distributed caching). For the caching algorithm, I use round-robin algorithm. 
> How can i implement Failover in my system?, so that when Child proxy is down/crash the request move to another Parent proxy.

You can't with this setup. The Child proxy is a bottleneck and the
failover copes with either of the Parents being down but not the Child.

If you are going to have a failover from Child to Parent, then you may
as well have all three proxies operating in a "flat"/horizontal design
as siblings all independently able to connect upstream.

The failover logics in that design is configured into the Mikrotik
somehow (I dont know how exectly though sorry). Or in a PAC file given
to the clients.


The design you have with a single router acting as hub for clients and
three proxies in a 2-tier design sounds like you have traffic travelling
over the same router 2-3 times on its way to the Internet (and same for
the responses). That will be cutting your available router bandwidth by
as much as 60%. You can probably double your bandwidth by using two
routers and two NICs on each proxy - without changing the uplinks
themselves.
Like so:
  clients-> router1 (load balancing)->proxies->router2->Internet

Amos



From nathan at getoffmalawn.com  Mon Apr 27 04:56:22 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 27 Apr 2015 14:56:22 +1000
Subject: [squid-users] bump and splice mode has no context caching
Message-ID: <CAGUJm7aJu7zo7ix3tpJ+fHxVQ5waBk0zwF36DRaDWEHVHS+Brg@mail.gmail.com>

Hello,

This may be a question more for Christos than anyone else (as he did
the original work), but why is SSL Context caching disabled for SSL
bump when set to bump and splice mode?

Thank you,

Nathan.


From hierony_milanisti at yahoo.co.id  Mon Apr 27 05:48:38 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Mon, 27 Apr 2015 05:48:38 +0000 (UTC)
Subject: [squid-users] Bls:  Failover in Squid 3.1.10
In-Reply-To: <553DA549.2030702@treenet.co.nz>
References: <553DA549.2030702@treenet.co.nz>
Message-ID: <338212600.2992727.1430113718197.JavaMail.yahoo@mail.yahoo.com>

Hello,
This may be an answer for Amos than anyone else.

I don't sure if i had time to change the web proxy server design.
because the deadline is 1 week more.

And also for double bandwidth design.
Are you have alternative option so that I can implement Failover by using my existing system.

Thanks in advance.



Hierony Manurung
Del Institute of Technology
Network Management 


     Pada Senin, 27 April 2015 9:56, Amos Jeffries <squid3 at treenet.co.nz> menulis:
   

 On 27/04/2015 2:36 p.m., Hierony Manurung wrote:
> Dear Fellow,
> I want to implement Failover in My system, I have 3 Squid proxy servers right now. they are :
> 
> - Child (172.30.20.200/16)? ? ? ? # receive request, and forward it either to Parent1 or Parent2
> - Parent1 (172.31.20.201/16)
> - Parent2 (172.31.20.204/16)
> 
> I use Mikrotik to connect all the servers, and they have been connected and they can work together (distributed caching). For the caching algorithm, I use round-robin algorithm. 
> How can i implement Failover in my system?, so that when Child proxy is down/crash the request move to another Parent proxy.

You can't with this setup. The Child proxy is a bottleneck and the
failover copes with either of the Parents being down but not the Child.

If you are going to have a failover from Child to Parent, then you may
as well have all three proxies operating in a "flat"/horizontal design
as siblings all independently able to connect upstream.

The failover logics in that design is configured into the Mikrotik
somehow (I dont know how exectly though sorry). Or in a PAC file given
to the clients.


The design you have with a single router acting as hub for clients and
three proxies in a 2-tier design sounds like you have traffic travelling
over the same router 2-3 times on its way to the Internet (and same for
the responses). That will be cutting your available router bandwidth by
as much as 60%. You can probably double your bandwidth by using two
routers and two NICs on each proxy - without changing the uplinks
themselves.
Like so:
? clients-> router1 (load balancing)->proxies->router2->Internet

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/8bb2d297/attachment.htm>

From vdoctor at neuf.fr  Mon Apr 27 06:00:19 2015
From: vdoctor at neuf.fr (Stakres)
Date: Sun, 26 Apr 2015 23:00:19 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:557: "F->flags.open"
In-Reply-To: <CAGUJm7asdf3K7VPgm6rnfciH+38n3=Oyz2eJNUM1hbd=wLjEgA@mail.gmail.com>
References: <1429254383156-4670788.post@n4.nabble.com>
 <CAGUJm7asdf3K7VPgm6rnfciH+38n3=Oyz2eJNUM1hbd=wLjEgA@mail.gmail.com>
Message-ID: <1430114419948-4670935.post@n4.nabble.com>

Hi Nathan,
Thanks for the reply, I applied the latest build (squid.3.5.3-r13808),
waiting for the client to check if it fixed or not 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-F-flags-open-tp4670788p4670935.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Apr 27 12:15:54 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 27 Apr 2015 05:15:54 -0700 (PDT)
Subject: [squid-users] installing squid 3.5.3
In-Reply-To: <553D941E.40501@treenet.co.nz>
References: <1429993257909-4670920.post@n4.nabble.com>
 <553C56EF.9080202@treenet.co.nz> <1430050018006-4670922.post@n4.nabble.com>
 <553D941E.40501@treenet.co.nz>
Message-ID: <1430136954543-4670936.post@n4.nabble.com>

it works and this error gone



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/installing-squid-3-5-3-tp4670920p4670936.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hierony_milanisti at yahoo.co.id  Mon Apr 27 13:05:28 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Mon, 27 Apr 2015 13:05:28 +0000 (UTC)
Subject: [squid-users] Squid cache Monitoring
Message-ID: <1179687925.3237736.1430139928629.JavaMail.yahoo@mail.yahoo.com>

Dear Fellow,

Is there another tools / GUI tools to see Squid caching performance?
I know that when we want to see whether the request is HIT/MISS/DENIED is from access.log file.
But, what that I want is the chart about percentage about cache HIT/MISS/DENIED

Thanks in advance.

?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/c1c0b0cc/attachment.htm>

From hack.back at hotmail.com  Mon Apr 27 14:03:57 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 27 Apr 2015 07:03:57 -0700 (PDT)
Subject: [squid-users] Squid cache Monitoring
In-Reply-To: <1179687925.3237736.1430139928629.JavaMail.yahoo@mail.yahoo.com>
References: <1179687925.3237736.1430139928629.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1430143437475-4670938.post@n4.nabble.com>

you can use monitorix



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-cache-Monitoring-tp4670937p4670938.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Dennis.Wang at bcis.cn  Mon Apr 27 14:36:43 2015
From: Dennis.Wang at bcis.cn (Dennis Wang)
Date: Mon, 27 Apr 2015 14:36:43 +0000
Subject: [squid-users] squid3 doesn't work well
Message-ID: <D1646A79.2342A%dennis.wang@bcis.cn>

Dear all,

Our squid3 server refused to work smoothly. I am not sure if I can post and get help here.

Thanks
Dennis


many are talented, few are driven.
Beijing City International School
?     ?     ?     ?     ?     ?     ?     ?
T   8610. 8771 7171
F   8610. 8771 7778
W  www.bcis.cn<http://www.bcis.cn/>
No.77 Baiziwan Nan Er Road Chaoyang District
Beijing, PR China 100022

An IB World School Accredited by CIS & WASC
YueCheng Education
?       ?       ?      ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/c41fd81e/attachment.htm>

From Hullen at t-online.de  Mon Apr 27 15:12:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 27 Apr 2015 17:12:00 +0200
Subject: [squid-users] squid3 doesn't work well
In-Reply-To: <D1646A79.2342A%dennis.wang@bcis.cn>
Message-ID: <DFfFd22eCXB@helmut.hullen.de>

Hallo, Dennis,

Du meintest am 27.04.15:

> Our squid3 server refused to work smoothly. I am not sure if I can
> post and get help here.

Some details may be helpful.

Viele Gruesse!
Helmut



From yvoinov at gmail.com  Mon Apr 27 15:46:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 27 Apr 2015 21:46:27 +0600
Subject: [squid-users] Squid cache Monitoring
In-Reply-To: <1179687925.3237736.1430139928629.JavaMail.yahoo@mail.yahoo.com>
References: <1179687925.3237736.1430139928629.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <553E59D3.8050202@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
cachemrg.cgi
SARG
squidanalyzer
sqtop
Munin+Squid plugins

27.04.15 19:05, Hierony Manurung ?????:
> Dear Fellow,
>
> Is there another tools / GUI tools to see Squid caching performance?
> I know that when we want to see whether the request is HIT/MISS/DENIED
is from access.log file.
> But, what that I want is the chart about percentage about cache
HIT/MISS/DENIED
>
> Thanks in advance.
>
>  Hierony Manurung
> Del Institute of Technology
> Network Management
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVPlnTAAoJENNXIZxhPexGKNgIAMnpwI3voEBY+ErRIQCfO52U
Mw85lNiB1bt4LQ8MA8J47+ORQFqfbzi2JyGjOu3tzOFr+ph279p3umbdfiOA0Ol3
2owL5ULbmR3YYsC8OHzQkiMi5H/BUb7FpDWZ1Lv96NQrR6V+/dVwVHKY1XkLZVEQ
+flssZdBtLUG/4/AY1w3XX5jP810XArtQPSlyi0CKLEvdTIT1RRkQkXL3TN5wa3X
akvBFbVmrBs9PEdSs4rd64r4dHdkdE00UY87P5dM49V8u+QTcZXO32O9R2HQ0LMh
p91EBuhZuaeI9TdlrAucKw1X/8F0GjvL/O4jlFHywQEGqDkwtqgK8ySIp94WYCc=
=KJwD
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/1aad09e2/attachment.htm>

From brent.newland at casipc.com  Mon Apr 27 16:10:48 2015
From: brent.newland at casipc.com (Brent Newland)
Date: Mon, 27 Apr 2015 10:10:48 -0600
Subject: [squid-users] FW: Trying to setup Squid as a reverse proxy. Works
	fine, but when a client does an HTTP POST,
	Squid claims there's a forwarding error.
In-Reply-To: <016101d08104$528ba480$f7a2ed80$@casipc.com>
References: <016101d08104$528ba480$f7a2ed80$@casipc.com>
Message-ID: <016701d08104$ba5d3fb0$2f17bf10$@casipc.com>

x-post
http://www.reddit.com/r/sysadmin/comments/340f36/trying_to_setup_squid_as_a_
reverse_proxy_works/

At this point, I've trimmed my Squid config down to:

http_access allow all
cache_peer 127.0.0.1 parent 8084 0 no-digest no-query originserver
name=mywebsite
cache_peer_domain mywebsite beta.mywebsite.com
coredump_dir /var/cache/squid
dns_nameservers 8.8.8.8 208.67.222.222

As for my server configuration, I'm on Windows Server 2008 R2 running Squid
3.5 from http://squid.diladele.com/?Squid is a reverse proxy for a PHP
process running the PHP built-in webserver on port 8084. The issue does not
happen when the PHP webserver is on port 80, so I'm 99% sure the problem
isn't coming from there.

It's a wordpress install I'm having the problem in. When logging in at
/wp-login.php, it loads the login form, but then gives me:

ERROR
The requested URL could not be retrieved The following error was encountered
while trying to retrieve the
URL:?http://beta.mywebsite.com/wp-login.php[2]
Access Denied.
Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.
Your cache administrator is webmaster.
Generated Mon, 27 Apr 2015 08:05:24 GMT by SERVER
(squid/3.5.1-20150206-r13746)

In the access log I get:

1430122290.530 1 192.168.10.1 TCP_MISS/403 5050
POST?http://beta.mywebsite.com/wp-login.php[3]?- HIER_NONE/- text/html
1430122290.532 17 69.146.194.21 TCP_MISS/403 5119
POST?http://beta.mywebsite.com/wp-login.php[4]?- HIER_DIRECT/12.34.56.78
text/html

And in the cache log:

2015/04/27 02:11:30 kid1| WARNING: Forwarding loop detected for:
POST /wp-login.php HTTP/1.1
Host: beta.mywebsite.com
Content-Length: 126
Pragma: no-cache
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8
Origin:?http://beta.mywebsite.com[5]
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/41.0.2272.118 Safari/537.36
Content-Type: application/x-www-form-urlencoded
DNT: 1
Referer:?http://beta.mywebsite.com/wp-login.php?loggedout=true[6]
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.8
Cookie:
Via: 1.1 SERVER (squid/3.5.1-20150206-r13746)
Surrogate-Capability: SERVER="Surrogate/1.0 ESI/1.0"
X-Forwarded-For: 12.34.56.78
Cache-Control: no-cache
Connection: keep-alive

Same thing happens on wp-cron.php. Near as I can tell, it's deciding a
perfectly good redirect is a forwarding redirect.

Am I missing something here? It's driving me crazy.

I should mention that viewing PHP files and HTML files and pictures all work
fine, it's just self-referential HTTP POST's that fail.





From netdevel911 at yahoo.it  Mon Apr 27 17:24:52 2015
From: netdevel911 at yahoo.it (Giuseppe)
Date: Mon, 27 Apr 2015 19:24:52 +0200
Subject: [squid-users] Problem with user authentication and SSL
Message-ID: <001f01d0810f$13896b90$3a9c42b0$@yahoo.it>

Hi, I'm successfully using squid with active directory authentication for
HTTP traffic.

Now I've added another http_port for SSL traffic, it works but in the
access.log I can't find the username associated with SSL traffic.

Do you experience the same?

Is there anything special to do to log username in SSL traffic?

Squid version is 3.5

 

Many thanks in advance.

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/48e3aa32/attachment.htm>

From szabados0701 at gmail.com  Mon Apr 27 18:05:34 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Mon, 27 Apr 2015 20:05:34 +0200
Subject: [squid-users] Scaling guide?
Message-ID: <CADddWkpYgnaVRMO--R9Vu3+Wu7htZaGFukr51O09VzMs2CJCyA@mail.gmail.com>

Hi Amos,

I'm running squid on a tp-link tl-wdr4300 router. I've downloaded the MESD
blacklists, and set up deny acl for them, but it seemed to be too much my
router. It took several minutes to start squid, and the whole thing was
unresponsive, sometimes I couldn't even connect to the internet. What I'm
looking for is some reference guide about, how much blacklist I can have
for what hw, or something like that.

Balazs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/868f8f21/attachment.htm>

From yvoinov at gmail.com  Mon Apr 27 18:41:17 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 28 Apr 2015 00:41:17 +0600
Subject: [squid-users] Problem with user authentication and SSL
In-Reply-To: <001f01d0810f$13896b90$3a9c42b0$@yahoo.it>
References: <001f01d0810f$13896b90$3a9c42b0$@yahoo.it>
Message-ID: <553E82CD.4050108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
SSL can be proxying only with bump functionality with interception.
Interception can't use authentification.

27.04.15 23:24, Giuseppe ?????:
> Hi, I'm successfully using squid with active directory authentication for
> HTTP traffic.
>
> Now I've added another http_port for SSL traffic, it works but in the
> access.log I can't find the username associated with SSL traffic.
>
> Do you experience the same?
>
> Is there anything special to do to log username in SSL traffic?
>
> Squid version is 3.5
>
> 
>
> Many thanks in advance.
>
> 
>
> 
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVPoLNAAoJENNXIZxhPexGMwcIAJbcj1/8t7shRV/z3v9DJHFj
9XmZD8KedfnHkColVF+R8MPOpdIsaKRPwG596w/AsnpaXYoLVEdE4sbOUqfPaAkC
Des5fdyg8MrrQ8tqxx39yk/YHcd+n77wt4/+yWxCyABMCcFm5xpBhuoEnA3L4uQB
itxw3EdPqXQZfdz3HDkoQXFDQSxn71hs6zNLlwfMDJ91tzlorem9wrLgEtNd3wGV
jol3LK5/UnMP1EaU3CG/JoMMVxqZCqHaCivU9wetjw2WgoPE4bxhGn/DmY6CgTzc
2LFdWpoc4GTZ8juiyVnMX0ZbTDXrZgqZw7sAF64nwT6afHM3DcPmNXqba5Thhgg=
=kx0q
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/695fb2d7/attachment.htm>

From thom.j.harris at gmail.com  Mon Apr 27 21:08:44 2015
From: thom.j.harris at gmail.com (Tom Harris)
Date: Mon, 27 Apr 2015 14:08:44 -0700
Subject: [squid-users] How are others handling missing intermediate
	certificates?
Message-ID: <CAAme0OYsSfp41UfrCG_M5wL9Ht-Ta-VEJhzW43xwSO167_cq+A@mail.gmail.com>

In SSL bump mode, I find I am hitting sites with incomplete certificate
chains fairly often.   When accessed directly, browsers will work it out -
I guess by downloading the missing CA certs.

I know I can load the intermediate CA certs in my system DB as I encounter
the issues.   But, I'm wondering if others have more proactive solutions.
Is there a list of commonly encountered certs, maybe just a subset like the
top tier CAs?    Or, is this being addressed in code making squid behave
like browsers do?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150427/acf06fd3/attachment.htm>

From Gary.Woodman at usq.edu.au  Tue Apr 28 03:21:28 2015
From: Gary.Woodman at usq.edu.au (Gary Woodman)
Date: Tue, 28 Apr 2015 03:21:28 +0000
Subject: [squid-users] squid logs not rotating
Message-ID: <81D4BEDF5F61664A9ADB2FF8E4AA61BD24C340EC@EXCH-MBX-PRD-T3.usq.edu.au>

> Date: Mon, 20 Apr 2015 01:07:44 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid logs not rotating
> Message-ID: <5533A8A0.4090507 at treenet.co.nz>
>Content-Type: text/plain; charset=utf-8

>> the time there will not be 30 days worth of data in the logs like on T1.
>> 
>> Other logs such as cache.log and store.log suffer equally the same buildup and wipe.
>> 
>> Does anyone have any suggestions? How might we go about debugging this situation?
>
> I'd almost guarantee you the log rotation script is different on the two servers.
>
> Most systems use the OS provided log rotation service, which does the rotate atomically then
> runs squid -k rotate. squid.conf has "logfile_rotate 0" to prevent Squid touching the OS rotation
>  numbering.
>
> What you describe is classic symptoms of the OS rotating logs, but only doing the "squid -k 
> rotate" action once every month.
>
>
> HTH
> Amos

You are absolutely correct. I had moved the default squid config out of /etc/logrotate.d on both of these servers, but somehow, it had crept back on T2.

However, removing it has not affected the behaviour described previously.

I have bowed to the inevitable and followed your advice, to let the OS handle the rotation (with logfile_rotate set to zero), including the following in /etc/logrotate.d/squid:
    daily
    rotate 30


Regards
Gary


_____________________________________________________________
This email (including any attached files) is confidential and is for the intended recipient(s) only. If you received this email by mistake, please, as a courtesy, tell the sender, then delete this email.

The views and opinions are the originator's and do not necessarily reflect those of the University of Southern Queensland. Although all reasonable precautions were taken to ensure that this email contained no viruses at the time it was sent we accept no liability for any losses arising from its receipt.

The University of Southern Queensland is a registered provider of education with the Australian Government.
(CRICOS Institution Code QLD 00244B / NSW 02225M, TEQSA PRV12081 )


From hierony_milanisti at yahoo.co.id  Tue Apr 28 04:42:10 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Tue, 28 Apr 2015 04:42:10 +0000 (UTC)
Subject: [squid-users] Using Cache Digest in Squid Proxy
Message-ID: <1269639970.114350.1430196130290.JavaMail.yahoo@mail.yahoo.com>

Dear Fellow,
I want to implement Cache Digest in my system.

I have 3 Squid proxy servers right now. they are :
- Child (172.30.20.200/16)???????? # receive request, and forward it either to Parent1 or Parent2
- Parent1 (172.31.20.201/16)
- Parent2 (172.31.20.204/16)

How can I implement Cache Digest?
And in what proxy server I implement it (Proxy or Child, or in Both).

Thanks in advance.


?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/412da5e9/attachment.htm>

From lupick at gmail.com  Tue Apr 28 08:46:31 2015
From: lupick at gmail.com (Lupick)
Date: Tue, 28 Apr 2015 01:46:31 -0700 (PDT)
Subject: [squid-users] squid basic ntlm auth error after upgrade to 3.3.8
In-Reply-To: <546E0E96.3010303@treenet.co.nz>
References: <1416483506229-4668477.post@n4.nabble.com>
 <546E0E96.3010303@treenet.co.nz>
Message-ID: <CA+-fCxAoix8yb_Qe+DmTzC7iWPBvLfR2wSSP-MLh7z=g48t_4w@mail.gmail.com>

I've found probably where is the problem but I don't know how to solve it.

If I try to connect with a PC outside the domain squid ntlm auth prompt for
username/password. On the password prompt banner I have the proxy ip
address; so the ntlm auth is used.
I can put my domain\username + password but it keep requesting the
password.

I've tried to comment out all the ntlm auth stuff in squid.conf; and I kept
only the basic.

Now the PC request to me the username\password but this time on the banner
I have " Squid proxy-caching web server"  so basic auth is used.  If I put
my domain\username + pwd all is working well.

So I assume the problem is due to ntlm auth doesn't fall back to basic but
it keeps requesting password.

Do you know how to force squid to fallback to basic auth if ntlm auth fail?
I remember in older version it was automatic.

Thank you for your help

L.

Il giorno gio 20 nov 2014 alle ore 16:54 Amos Jeffries [via Squid Web Proxy
Cache] <ml-node+s1019090n4668485h22 at n4.nabble.com> ha scritto:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 21/11/2014 12:38 a.m., Lupick wrote:
> > Hi I've a problem authenticating users outside my AD domain after
> > the upgrade to squid 3.3.8.
>
> 3.3.8 is far from the latest Squid. There is information about where
> to find updated packages for CentOS at
> <http://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
> >
> > All the domain logged user are able to authenticate without any
> > issue.
> >
> > The local user or user of a non domain computer have a
> > username/password prompt as expected.
> >
> > If I provide the right doamin\username and password the promt
> > appear over and over.
>
> By "right" you mean the Basic or NTLM credentials?
>
> Which popup is the browser selecting to display?
>  - the realm value configured in squid.conf is displayed as part of
> the Basic auth popup, IIRC the proxy hostname or DOMAIN is listed in
> teh NTLM popup. So you should be able to tell which its asking for.
>
> NTLM requires machines to be signed into the domain to get the correct
> credentials crypto tokens from the DC to login with. Any attempt to
> use NTLM credentials without being signed onto the domain will fail.
>
> Basic auth only requires the domain\user:password combo gets delivered.
>
>
> >
> > BUT after the first time if I click cancel qnd I retry i'm able to
> > browse internet.  This happen cause the credential provided  are
> > stored under the windows credentian manager in the control panel.
> >
> > no problem using centos 6 and squid 3.3.1, the problem appears
> > after an upgrade to centos 7 and squid 3.3.8.
> >
> > this is my section on squid.conf:
> >
> > auth_param ntlm program /usr/bin/ntlm_auth
> > --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 45
> > #auth_param ntlm max_challenge_reuses 0 #auth_param ntlm
> > max_challenge_lifetime 2 minutes
> >
> > auth_param basic program /usr/bin/ntlm_auth
> > --helper-protocol=squid-2.5-basic auth_param basic children 5
> > auth_param basic realm Squid proxy-caching web server auth_param
> > basic credentialsttl 5 hours
> >
>
> PS. Have you considered migrating to Kerberos? it has a lot less
> problems than NTLM.
>
> Amos
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2.0.22 (MingW32)
>
> iQEcBAEBAgAGBQJUbg6UAAoJELJo5wb/XPRjHzYIALBvTG3mVsl0QX0I1MzYdM2w
> h9Cz2ShzpYEJWP+JcqeyQsp8xd8eWcxC8jsnibTat60belprPjcG7HLVVKHnKacT
> jwQUQFId5B3KfuIad5MD887CxLwfujT3yoiBB2vFFki+bGWkkEDoOPzkcNY7TsUs
> pSAqlynOpHNWH6UTahzG7L/xvxcHMTv8Wd2n1XxKFSGrdShwkWixLP1x3zA/CB3q
> qckN8H5R/rOnMSBmWNCZ5VDFelPZTItXaxf4HmSbLw4XySxwLkthd8kHO9o/sv4E
> SwiOihvxVMcXD/GPyG+bW9aXDN1p51aPX0SIisUuznuhh6vTTrhCJTqCDU1o9mM=
> =pGgC
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4668485&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-basic-ntlm-auth-error-after-upgrade-to-3-3-8-tp4668477p4668485.html
>  To unsubscribe from squid basic ntlm auth error after upgrade to 3.3.8, click
> here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4668477&code=bHVwaWNrQGdtYWlsLmNvbXw0NjY4NDc3fC0xMjk5NDA0Njcx>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-basic-ntlm-auth-error-after-upgrade-to-3-3-8-tp4668477p4670949.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jborrell at central.aplitec.com  Tue Apr 28 06:45:04 2015
From: jborrell at central.aplitec.com (Josep Borrell)
Date: Tue, 28 Apr 2015 06:45:04 +0000
Subject: [squid-users] How are others handling missing
	intermediate	certificates?
In-Reply-To: <CAAme0OYsSfp41UfrCG_M5wL9Ht-Ta-VEJhzW43xwSO167_cq+A@mail.gmail.com>
References: <CAAme0OYsSfp41UfrCG_M5wL9Ht-Ta-VEJhzW43xwSO167_cq+A@mail.gmail.com>
Message-ID: <42DE25A255671C44A93A7B58470DC09C674E6969@Michelle.aplitec.local>

Hi Tom,

Did you saw the thread ?ssl_bump peek in squid-3.5.3? from James Lay ?
Maybe can help

Josep




De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de Tom Harris
Enviado el: lunes, 27 de abril de 2015 23:09
Para: squid-users at lists.squid-cache.org
Asunto: [squid-users] How are others handling missing intermediate certificates?

In SSL bump mode, I find I am hitting sites with incomplete certificate chains fairly often.   When accessed directly, browsers will work it out - I guess by downloading the missing CA certs.

I know I can load the intermediate CA certs in my system DB as I encounter the issues.   But, I'm wondering if others have more proactive solutions.  Is there a list of commonly encountered certs, maybe just a subset like the top tier CAs?    Or, is this being addressed in code making squid behave like browsers do?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/9d87ecc1/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 28 07:03:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Apr 2015 19:03:23 +1200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
 connections?
In-Reply-To: <867922D2-E927-42D5-8680-AC606432EFEB@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
 <867922D2-E927-42D5-8680-AC606432EFEB@rinis.nl>
Message-ID: <553F30BB.8070504@treenet.co.nz>

On 28/04/2015 6:41 p.m., Abdelouahed Haitoute wrote:
> Hello Amos,
> 
> Thank you for your reply.
> 
>> Consider removing the "cache deny all" when you get this into
>> production.
> 
> 
> We?re expecting all requested files to be unique. So is it still wise to cache these requested files? Are there other advantages of enabling caching?
> 

For truely unique fetches it will not have much use. Disk cache would be
a negative due to latency. RAM cache would be not adding benefit, but a
small (few MB) amount would not be reducing performance either.

For other benefits, you get caching for any unplanned non-unique
requests. eg. DoS events or changes in the applicaton design.

Its also an early indicator of design problems. If your unique files
cannot cope with a cache existing in their pathway something is terribly
wrong with the headers being output by the server.

Amos



From squid3 at treenet.co.nz  Tue Apr 28 06:51:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Apr 2015 18:51:00 +1200
Subject: [squid-users] Using Cache Digest in Squid Proxy
In-Reply-To: <1269639970.114350.1430196130290.JavaMail.yahoo@mail.yahoo.com>
References: <1269639970.114350.1430196130290.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <553F2DD4.50409@treenet.co.nz>

On 28/04/2015 4:42 p.m., Hierony Manurung wrote:
> Dear Fellow,
> I want to implement Cache Digest in my system.
> 
> I have 3 Squid proxy servers right now. they are :
> - Child (172.30.20.200/16)         # receive request, and forward it either to Parent1 or Parent2
> - Parent1 (172.31.20.201/16)
> - Parent2 (172.31.20.204/16)
> 
> How can I implement Cache Digest?
> And in what proxy server I implement it (Proxy or Child, or in Both).

Unless you have Squid built with --disable-cache-digest or configured
with "cache_peer ... no-digest" then it is already enabled and probably
working.

Amos



From squid3 at treenet.co.nz  Tue Apr 28 07:17:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Apr 2015 19:17:11 +1200
Subject: [squid-users] How are others handling missing intermediate
	certificates?
In-Reply-To: <CAAme0OYsSfp41UfrCG_M5wL9Ht-Ta-VEJhzW43xwSO167_cq+A@mail.gmail.com>
References: <CAAme0OYsSfp41UfrCG_M5wL9Ht-Ta-VEJhzW43xwSO167_cq+A@mail.gmail.com>
Message-ID: <553F33F7.7060807@treenet.co.nz>

On 28/04/2015 9:08 a.m., Tom Harris wrote:
> In SSL bump mode, I find I am hitting sites with incomplete certificate
> chains fairly often.   When accessed directly, browsers will work it out -
> I guess by downloading the missing CA certs.
> 
> I know I can load the intermediate CA certs in my system DB as I encounter
> the issues.   But, I'm wondering if others have more proactive solutions.
> Is there a list of commonly encountered certs, maybe just a subset like the
> top tier CAs?

Make sure that your set of trusted-CA used by OpenSSL is up to date. It
changes monthly or so in my experience. On Linux distros it tends to be
the "ca-certificates" software package.

You also have the alternative of building your own list from the ones
you hit. Though this can lead to security problems if you dont take
great care. I suggest at least following the news about what
organisations have been blacklisted from the global Trusted-CA and why
if you take this path.


>    Or, is this being addressed in code making squid behave
> like browsers do?

TLS specification says the sender is responsible for delivering the
entire cert chain except (optionally) those in the global Trusted-CA set.

Do you really think its a good idea to continue talking to broken and
misconfigured HTTPS servers in the modern Internet?

Amos



From squid3 at treenet.co.nz  Tue Apr 28 06:33:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Apr 2015 18:33:38 +1200
Subject: [squid-users] FW: Trying to setup Squid as a reverse proxy.
 Works fine, but when a client does an HTTP POST,
 Squid claims there's a forwarding error.
In-Reply-To: <016701d08104$ba5d3fb0$2f17bf10$@casipc.com>
References: <016101d08104$528ba480$f7a2ed80$@casipc.com>
 <016701d08104$ba5d3fb0$2f17bf10$@casipc.com>
Message-ID: <553F29C2.803@treenet.co.nz>

On 28/04/2015 4:10 a.m., Brent Newland wrote:
> x-post
> http://www.reddit.com/r/sysadmin/comments/340f36/trying_to_setup_squid_as_a_
> reverse_proxy_works/
> 
> At this point, I've trimmed my Squid config down to:
> 
> http_access allow all
> cache_peer 127.0.0.1 parent 8084 0 no-digest no-query originserver
> name=mywebsite
> cache_peer_domain mywebsite beta.mywebsite.com
> coredump_dir /var/cache/squid
> dns_nameservers 8.8.8.8 208.67.222.222

Well I guarantee thats not all, because there is at least a http_port
line to receive traffic.


> 
> As for my server configuration, I'm on Windows Server 2008 R2 running Squid
> 3.5 from http://squid.diladele.com/ Squid is a reverse proxy for a PHP
> process running the PHP built-in webserver on port 8084. The issue does not
> happen when the PHP webserver is on port 80, so I'm 99% sure the problem
> isn't coming from there.
> 

...
> 
> In the access log I get:
> 

This is the loop being rejected.
> 1430122290.530 1 192.168.10.1 TCP_MISS/403 5050
> POST http://beta.mywebsite.com/wp-login.php[3] - HIER_NONE/- text/html

This is the first request.
> 1430122290.532 17 69.146.194.21 TCP_MISS/403 5119
> POST http://beta.mywebsite.com/wp-login.php[4] - HIER_DIRECT/12.34.56.78
> text/html

Notice how its going DIRECT. Reverse-proxy traffic has an automatic
block on going direct as an option because the DNS entries are pointing
at the proxy that is currently serving the request - guaranteeing that
this loop will occur if DIRECT / DNS is used.

That POST request entered your proxy without going through the
reverse-proxy "accel" mode port, OR you have an always_direct line
forcing this traffic to go direct when it must not.


Instead of using the deprecated cache_peer_domain. Try this instead:

 acl mywebsite dstdomain beta.mywebsite.com
 cache_peer_access mywebsite allow mywebsite


Amos


From paul.martin.b787 at gmail.com  Tue Apr 28 10:50:00 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Tue, 28 Apr 2015 12:50:00 +0200
Subject: [squid-users] squid 3.5.3: squid -z => FATAL: memory_cache_shared
 is on, but no support for atomic operations detected
Message-ID: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>

Hello,

I am testing squid 3.5.3 using not the sample squid.conf (this works) but
a  squid.conf containing:
"cache_dir rock ..."
"memory_cache_shared on"

I got problems:
--
problem 1: squid.conf with memory_cache_shared on
#./squid -z
result:
FATAL: memory_cache_shared is on, but no support for atomic operations
detected
---
problem 2: squid.conf with memory_cache_shared off
#./squid -z
2015/04/28 10:28:46| assrtion failed: ../../src/ipc/AtomicWord.h: 88:
"Enabled()"
--

How can I solve problem 1 (I want to keep  "cache_dir rock ..." and
"memory_cache_shared on") ?
(many versions (squid 3.5.3r13abc) tested and same problems)

Why problem 2 ?

Thank you,
Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/bbb92583/attachment.htm>

From dan at getbusi.com  Tue Apr 28 11:11:11 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Tue, 28 Apr 2015 04:11:11 -0700 (PDT)
Subject: [squid-users] squid 3.5.3: squid -z => FATAL:
 memory_cache_shared is on, but no support for atomic operations detected
In-Reply-To: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>
References: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>
Message-ID: <1430219471604.7e41c737@Nodemailer>

Hi Paul




See Michael?s reply to my thread about the same problem, not long ago:


http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-assertion-failed-src-ipc-AtomicWord-h-88-Enabled-tp4670597p4670793.html

On Tue, Apr 28, 2015 at 8:50 PM, Paul Martin <paul.martin.b787 at gmail.com>
wrote:

> Hello,
> I am testing squid 3.5.3 using not the sample squid.conf (this works) but
> a  squid.conf containing:
> "cache_dir rock ..."
> "memory_cache_shared on"
> I got problems:
> --
> problem 1: squid.conf with memory_cache_shared on
> #./squid -z
> result:
> FATAL: memory_cache_shared is on, but no support for atomic operations
> detected
> ---
> problem 2: squid.conf with memory_cache_shared off
> #./squid -z
> 2015/04/28 10:28:46| assrtion failed: ../../src/ipc/AtomicWord.h: 88:
> "Enabled()"
> --
> How can I solve problem 1 (I want to keep  "cache_dir rock ..." and
> "memory_cache_shared on") ?
> (many versions (squid 3.5.3r13abc) tested and same problems)
> Why problem 2 ?
> Thank you,
> Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/79a467ea/attachment.htm>

From paul.martin.b787 at gmail.com  Tue Apr 28 13:25:09 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Tue, 28 Apr 2015 15:25:09 +0200
Subject: [squid-users] squid 3.5.3: squid -z => FATAL:
 memory_cache_shared is on, but no support for atomic operations detected
In-Reply-To: <1430219471604.7e41c737@Nodemailer>
References: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>
 <1430219471604.7e41c737@Nodemailer>
Message-ID: <CAGAgj8Cz8wT3wPvfvBOm9HAKEUpDcxWkVStudYEJhqnCc+WhMg@mail.gmail.com>

Hi Dan,

problem is the same following Michael' idea.
I tried ./configure
with  CXXFLAGS=-std=c++11 => got many warning
with  CXXFLAGS=-std=c++0x => got many warning
with  CXXFLAGS=-matomic-model=strict => got many warning (example:
strings.h present but cannot be compiled)

Any solution ?

Thank you
Paul



2015-04-28 13:11 GMT+02:00 <dan at getbusi.com>:

> Hi Paul
>
> See Michael?s reply to my thread about the same problem, not long ago:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Re-assertion-failed-src-ipc-AtomicWord-h-88-Enabled-tp4670597p4670793.html
>
>
>
>
>
> On Tue, Apr 28, 2015 at 8:50 PM, Paul Martin <paul.martin.b787 at gmail.com>
> wrote:
>
>>  Hello,
>>
>> I am testing squid 3.5.3 using not the sample squid.conf (this works) but
>> a  squid.conf containing:
>> "cache_dir rock ..."
>> "memory_cache_shared on"
>>
>> I got problems:
>> --
>> problem 1: squid.conf with memory_cache_shared on
>> #./squid -z
>> result:
>> FATAL: memory_cache_shared is on, but no support for atomic operations
>> detected
>> ---
>> problem 2: squid.conf with memory_cache_shared off
>> #./squid -z
>> 2015/04/28 10:28:46| assrtion failed: ../../src/ipc/AtomicWord.h: 88:
>> "Enabled()"
>> --
>>
>> How can I solve problem 1 (I want to keep  "cache_dir rock ..." and
>> "memory_cache_shared on") ?
>> (many versions (squid 3.5.3r13abc) tested and same problems)
>>
>> Why problem 2 ?
>>
>> Thank you,
>> Paul
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/1ace49a3/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 28 13:46:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 01:46:25 +1200
Subject: [squid-users] squid basic ntlm auth error after upgrade to 3.3.8
In-Reply-To: <CA+-fCxAoix8yb_Qe+DmTzC7iWPBvLfR2wSSP-MLh7z=g48t_4w@mail.gmail.com>
References: <1416483506229-4668477.post@n4.nabble.com>
 <546E0E96.3010303@treenet.co.nz>
 <CA+-fCxAoix8yb_Qe+DmTzC7iWPBvLfR2wSSP-MLh7z=g48t_4w@mail.gmail.com>
Message-ID: <553F8F31.7010005@treenet.co.nz>

On 28/04/2015 8:46 p.m., Lupick wrote:
> I've found probably where is the problem but I don't know how to solve it.
> 
> If I try to connect with a PC outside the domain squid ntlm auth prompt for
> username/password. On the password prompt banner I have the proxy ip
> address; so the ntlm auth is used.

The HTTP auth label "NTLM" was used by some old client software to
deliver LanMan protocol credentials (DOS 1.0 thru Windows 98 to give you
an idea of scale) - which is essentially 8-bit encrypted username+password.

Naturally a lot of more modern systems are not permitting that type of
downgrade attack anymore. I susect your OS upgrade came with an upgrade
to either CentOS Samba version ntlm_auth helper which dropped supprot
for those 20+ year old insecure protocols.

IIRC the "fix" for this is to turn off MSIE "Windows Integrated
Authentication" on machines which are not part of a domain. That leaves
them with selecting Basic auth which works.

Alternatively upgrading the domain to Kerberos (Negotiate auth) instead
of NTLM has also long been recommended.


> I can put my domain\username + password but it keep requesting the
> password.
> 
> I've tried to comment out all the ntlm auth stuff in squid.conf; and I kept
> only the basic.
> 
> Now the PC request to me the username\password but this time on the banner
> I have " Squid proxy-caching web server"  so basic auth is used.  If I put
> my domain\username + pwd all is working well.
> 
> So I assume the problem is due to ntlm auth doesn't fall back to basic but
> it keeps requesting password.
> 
> Do you know how to force squid to fallback to basic auth if ntlm auth fail?
> I remember in older version it was automatic.

There is no way to force fallback. Squid is merely advertising the set
of HTTP auth schemes it accepts. The client software makes the choice
which to use.

Amos


From squid3 at treenet.co.nz  Tue Apr 28 14:01:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 02:01:31 +1200
Subject: [squid-users] squid 3.5.3: squid -z => FATAL:
 memory_cache_shared is on, but no support for atomic operations detected
In-Reply-To: <CAGAgj8Cz8wT3wPvfvBOm9HAKEUpDcxWkVStudYEJhqnCc+WhMg@mail.gmail.com>
References: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>
 <1430219471604.7e41c737@Nodemailer>
 <CAGAgj8Cz8wT3wPvfvBOm9HAKEUpDcxWkVStudYEJhqnCc+WhMg@mail.gmail.com>
Message-ID: <553F92BB.8040504@treenet.co.nz>

On 29/04/2015 1:25 a.m., Paul Martin wrote:
> Hi Dan,
> 
> problem is the same following Michael' idea.
> I tried ./configure
> with  CXXFLAGS=-std=c++11 => got many warning
> with  CXXFLAGS=-std=c++0x => got many warning
> with  CXXFLAGS=-matomic-model=strict => got many warning (example:
> strings.h present but cannot be compiled)
> 
> Any solution ?

Build with a 3.5 series r13800 or later numbered snapshot.

Amos




From paul.martin.b787 at gmail.com  Tue Apr 28 14:21:36 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Tue, 28 Apr 2015 16:21:36 +0200
Subject: [squid-users] squid 3.5.3: squid -z => FATAL:
 memory_cache_shared is on, but no support for atomic operations detected
In-Reply-To: <553F92BB.8040504@treenet.co.nz>
References: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>
 <1430219471604.7e41c737@Nodemailer>
 <CAGAgj8Cz8wT3wPvfvBOm9HAKEUpDcxWkVStudYEJhqnCc+WhMg@mail.gmail.com>
 <553F92BB.8040504@treenet.co.nz>
Message-ID: <CAGAgj8AkxQ=HEcLQYtNk+afRP85-j-NsbBMAzL4zNFHqwZ_H7A@mail.gmail.com>

Thank you Amos,
but I got the same problem  with squid 3.5.3-20150456-r13812

Any other suggestions ?

Paul

2015-04-28 16:01 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 29/04/2015 1:25 a.m., Paul Martin wrote:
> > Hi Dan,
> >
> > problem is the same following Michael' idea.
> > I tried ./configure
> > with  CXXFLAGS=-std=c++11 => got many warning
> > with  CXXFLAGS=-std=c++0x => got many warning
> > with  CXXFLAGS=-matomic-model=strict => got many warning (example:
> > strings.h present but cannot be compiled)
> >
> > Any solution ?
>
> Build with a 3.5 series r13800 or later numbered snapshot.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/eaf94b50/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 28 14:26:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 02:26:06 +1200
Subject: [squid-users] Scaling guide?
In-Reply-To: <CADddWkpYgnaVRMO--R9Vu3+Wu7htZaGFukr51O09VzMs2CJCyA@mail.gmail.com>
References: <CADddWkpYgnaVRMO--R9Vu3+Wu7htZaGFukr51O09VzMs2CJCyA@mail.gmail.com>
Message-ID: <553F987E.1060802@treenet.co.nz>

On 28/04/2015 6:05 a.m., Bal?zs Szabados wrote:
> Hi Amos,
> 
> I'm running squid on a tp-link tl-wdr4300 router. I've downloaded the MESD
> blacklists, and set up deny acl for them, but it seemed to be too much my
> router. It took several minutes to start squid, and the whole thing was
> unresponsive, sometimes I couldn't even connect to the internet. What I'm
> looking for is some reference guide about, how much blacklist I can have
> for what hw, or something like that.

The list provider can give you best info about how big their list is, or
you should be able to see from the file size(s) and extrapolate.

The rest depends on how you configured Squid. What we have is
<http://wiki.squid-cache.org/SquidFaq/SquidMemory> to help you
understand and troubleshoot.

Amos



From squid3 at treenet.co.nz  Tue Apr 28 14:37:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 02:37:23 +1200
Subject: [squid-users] squid 3.5.3: squid -z => FATAL:
 memory_cache_shared is on, but no support for atomic operations detected
In-Reply-To: <CAGAgj8AkxQ=HEcLQYtNk+afRP85-j-NsbBMAzL4zNFHqwZ_H7A@mail.gmail.com>
References: <CAGAgj8AYXM_p2Kgr2VPM=v==ngTgyeJmm6-5owdkw4MahPh-9Q@mail.gmail.com>	<1430219471604.7e41c737@Nodemailer>	<CAGAgj8Cz8wT3wPvfvBOm9HAKEUpDcxWkVStudYEJhqnCc+WhMg@mail.gmail.com>	<553F92BB.8040504@treenet.co.nz>
 <CAGAgj8AkxQ=HEcLQYtNk+afRP85-j-NsbBMAzL4zNFHqwZ_H7A@mail.gmail.com>
Message-ID: <553F9B23.4070209@treenet.co.nz>

On 29/04/2015 2:21 a.m., Paul Martin wrote:
> Thank you Amos,
> but I got the same problem  with squid 3.5.3-20150456-r13812
> 
> Any other suggestions ?

Unless you are cross-building, no.

It means your libc does not supply the specific GNU atomics __sync_*()
API Squid makes use of. Note that is different from pthreads, general
"atomic support" or even C++11 atomics.


For cross-building it is:
  ./configure squid_cv_gnu_atomics=yes ...

but only use that if you are certain the target machine libc has the
right API available *and working*. Otherwise all you are doing is
building a Squid binary that will corrupt the shared memory it uses.

Amos




From bpk678 at gmail.com  Tue Apr 28 21:18:22 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 28 Apr 2015 17:18:22 -0400
Subject: [squid-users] sharing a tidbit
Message-ID: <553FF91E.1020601@gmail.com>

i have 2 squid instances behind HAProxy, balanced using leastconn.  each 
proxy server has a NFS mount under /etc/squid/acls/ where external acls 
are kept.  because the NFS mount is common to both instances, i only 
need to make an update in one place and both proxies will get the 
update.  when i put this together, i wanted a means of reconfiguring 
squid in some sort of automated fashion, based on if the acl files (or 
their contents) were changed.

below is the script i came up with for that.  i call the script from 
root's crontab once every 5 minutes (mind the wrap):

*/5 * * * * /root/bin/SquidReconfigure #reconfigure squid if ACL files 
have been updated

the script will create a temp file and write the time of last 
modification in seconds since Epoch to the temp file for tracking.  if 
the value changes, the temp file is updated and a flag is set to 
indicate that a reconfigure is warranted.  when the reconfigure is 
performed, it logs via logger/syslog that a refresh was performed.

the logic is tested and running on my boxes and works nicely for my 
needs.  because i am a small environment and can deal with the fact the 
proxies are performing these actions at the same time, i don't need to 
stagger the offset for each server.  if your reconfigure action takes a 
long time, you may want to consider what options you have in order to 
continue providing functionally available services.

#!/bin/bash

aclDir=/etc/squid/acl
statFile=/tmp/squidStats
reconfigure=0

for aclFile in $(ls $aclDir)
do
	previous=$(grep ^$aclFile\  $statFile |awk '{print $2}')
	current=$(stat -t $aclDir/$aclFile -c %Y)

	if [ $current != $previous ]
	then
		#echo -e $aclFile' \t'"change found"
		# mind the wrap on the below line
		sed -i -e "s/$aclFile\ $previous/$aclFile\ $current/" $statFile
		#echo -e $aclFile' \t'"settting marker"
		reconfigure=1
	fi
done

if [ $reconfigure = 1 ]
then
	#echo "reconfiguring squid"
	squid -k reconfigure
	logger -t '(squid-1)' -p 'local4.notice' Squid ACL Refresh
fi


From squid3 at treenet.co.nz  Wed Apr 29 01:21:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 13:21:16 +1200
Subject: [squid-users] sharing a tidbit
In-Reply-To: <553FF91E.1020601@gmail.com>
References: <553FF91E.1020601@gmail.com>
Message-ID: <5540320C.4090005@treenet.co.nz>

On 29/04/2015 9:18 a.m., Brendan Kearney wrote:
> i have 2 squid instances behind HAProxy, balanced using leastconn.  each
> proxy server has a NFS mount under /etc/squid/acls/ where external acls
> are kept.  because the NFS mount is common to both instances, i only
> need to make an update in one place and both proxies will get the
> update.  when i put this together, i wanted a means of reconfiguring
> squid in some sort of automated fashion, based on if the acl files (or
> their contents) were changed.
> 
> below is the script i came up with for that.  i call the script from
> root's crontab once every 5 minutes (mind the wrap):
> 
> */5 * * * * /root/bin/SquidReconfigure #reconfigure squid if ACL files
> have been updated
> 
> the script will create a temp file and write the time of last
> modification in seconds since Epoch to the temp file for tracking.  if
> the value changes, the temp file is updated and a flag is set to
> indicate that a reconfigure is warranted.  when the reconfigure is
> performed, it logs via logger/syslog that a refresh was performed.
> 
> the logic is tested and running on my boxes and works nicely for my
> needs.  because i am a small environment and can deal with the fact the
> proxies are performing these actions at the same time, i don't need to
> stagger the offset for each server.  if your reconfigure action takes a
> long time, you may want to consider what options you have in order to
> continue providing functionally available services.
> 
> #!/bin/bash
> 
> aclDir=/etc/squid/acl
> statFile=/tmp/squidStats
> reconfigure=0
> 
> for aclFile in $(ls $aclDir)
> do
>     previous=$(grep ^$aclFile\  $statFile |awk '{print $2}')
>     current=$(stat -t $aclDir/$aclFile -c %Y)
> 
>     if [ $current != $previous ]
>     then
>         #echo -e $aclFile' \t'"change found"
>         # mind the wrap on the below line
>         sed -i -e "s/$aclFile\ $previous/$aclFile\ $current/" $statFile
>         #echo -e $aclFile' \t'"settting marker"
>         reconfigure=1
>     fi
> done
> 
> if [ $reconfigure = 1 ]
> then
>     #echo "reconfiguring squid"
>     squid -k reconfigure
>     logger -t '(squid-1)' -p 'local4.notice' Squid ACL Refresh
> fi

I'd be using -k check in that setup instead of -k reconfigure. Check
only reconfigures the runnign process if the config is still operational.


Or, you can also script with the squidclient command line tool to send
mgr:reconfigure requests to the proxy. (with a squid -k parse beforehand)

Or, you can set the file(s) on a filesystem trigger so the script only
ever runs when the are changed.


Amos


From dan at getbusi.com  Wed Apr 29 05:44:14 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Tue, 28 Apr 2015 22:44:14 -0700 (PDT)
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <1429494253383.2bf365a4@Nodemailer>
References: <1429494253383.2bf365a4@Nodemailer>
Message-ID: <1430286253928.fcbd2ab7@Nodemailer>

I mentioned last time that we had to x2 all our delay_parameter?s bytes because of a weird bug where squid would apply it at half speed for no reason.




It just occurred to me that (obviously) this is why HTTPS downloads are going too fast; because this bug must only affect HTTP traffic.




So HTTPS downloads are going at the actual speed we?ve specified and HTTP is going at half that.




Therefore, we should be able to work around it by setting different delay_parameters for HTTP and HTTPS requests.




So my question is, how best to target only those requests? By the CONNECT method?




P.S. This bug is still present in the latest Squid 3.5 HEAD.

On Mon, Apr 20, 2015 at 11:44 AM, null <dan at getbusi.com> wrote:

> Thanks Amos
> Sorry if that wasn?t clear, but yeah, 7 KB/s was the desired speed in that test.?
> I was testing against an ISO in an S3 bucket of ours. I would start the download using http:// and get 7 KB/s (great). Then cancel it and edit the URL to https:// and get ~90 KB/s.
> Oh, and I almost forgot: (I can?t remember whether this has been reported but) there?s also a problem with delay_parameters, such that our software has to x2 any restriction that?s configured when generating the squid config.
> So, if I want to configure a 56 kbps restriction our software actually writes:
> delay_parameters 1 -1/-1 14336/14336
> In order to achieve the correct limit (7 KB/s). Not sure if that could be related to this at all.
> P.S. We?re pretty fond of delay_pools and their simplicity?including the fact that it can all be handled by Squid without any lower-level networking concerns.
> On Mon, Apr 20, 2015 at 1:25 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>> On 17/04/2015 1:30 p.m., djch wrote:
>>> I just wanted to revive this thread to note that:
>>> 
>>> - Delay pools apply just fine to HTTPS requests in Squid 2.7.
>>> - Delay pools in Squid 3.4.x are also applied to HTTPS but the speed is not
>>> correct. 
>>>   - If I apply a 56 Kbps limit the HTTP download tops out at ~7 KB/s.
>> That *is* correct.
>>    56K *bits* /sec == 7K *Bytes* /sec.
>>    7x8 = 56
>>> If I
>>> download the same file from the same server via HTTPS it tops out at
>>> ~90KB/s. If I download the same file over HTTPS with no delay pools
>>> configured it tops at around 3MB/s.
>> Are you sure thats actually HTTPS ?
>> CONNECT tunnels these days could contain HTTP/2, SPDY, WebSockets or
>> data in some other (compressed?) format. Any of which achieve faster
>> percieved "download" than HTTP using the same basic bytes/sec data rate.
>> NP: squid-2.7 contains a CONNECT bug that prevents SPDY HTTP/2 and
>> Websockets working properly.
>>> 
>>> So I guess that would make this a bug? Which I assume nobody wants to fix
>>> 'cause they're going to be deprecated at some point soon?
>>> 
>> Its a matter of interest. With FOSS you have to cause someone to be
>> interested in fixing the bug. Best way to do that is to fix it yourself
>> and present a patch and get it through review to merge. Second best is
>> to find someone to pay to do all that. Or you can join the many people
>> who opted just to wait and pray someone else will give it to them free
>> one day.
>> There is another option with delay pools. The pooling system is *just* a
>> packet rate limiting system. The OS these days have several such QoS
>> systems built in that work far better than Squid algorithms (admittedly
>> not on a HTTP per-message basis). If you want total traffic control give
>> those a try.
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150428/ab4471f4/attachment.htm>

From ahaitoute at rinis.nl  Wed Apr 29 06:42:07 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Wed, 29 Apr 2015 08:42:07 +0200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
	connections?
In-Reply-To: <55344F34.1070607@treenet.co.nz>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
Message-ID: <13BF7DA2-AC5C-41BB-9E94-758BF2A06D59@rinis.nl>

Hello Amos,

> Try adding this to your config file:
> workers 2

I?ve added this directive. After a restart I don?t see squid-process listening on the tcp port 3128.
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      921/sshd
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1033/master
tcp6       0      0 :::22                   :::*                    LISTEN      921/sshd
tcp6       0      0 ::1:25                  :::*                    LISTEN      1033/master

/var/log/messages
squid[17362]: Squid Parent: will start 3 kids
squid[17362]: Squid Parent: (squid-coord-3) process 17364 started
squid[17362]: Squid Parent: (squid-2) process 17365 started
squid[17362]: Squid Parent: (squid-1) process 17366 started
systemd: Started Squid caching proxy.

What am I missing?

Abdelouahed

> Op 20 apr. 2015, om 02:58 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
> On 19/04/2015 9:58 p.m., Abdelouahed Haitoute wrote:
>> Hello,
>> 
>> I?ve got the following setup, each application on its own virtual machine:
>> 
>> Client (sends http-requests to proxy)?> Squid (sends http-requests to apache based on destination IP and round robin to multiple apache machines) ?> Apache (setting up a two way ssl to the requested server) ?> HTTPS-server
>> 
>> This setup works great, and I have the Apache and the HTTPS-server its performance tuned. Both can handle 2000 concurrent connections of file sizes up to 10MB.
>> 
>> Unfortunately I haven?t been successful with the Squid-server. After a while I?m getting the following error messages in the log:
>> 1429432828.200  62854 10.10.7.16 TCP_MISS_ABORTED/000 0 GET http://https.example.com/index.html - ROUNDROBIN_PARENT/192.168.0.20 -
>> 
>> The Squid virtual machine contains the following:
>> CentOS 7.1 with latest updates
>> Squid Cache: Version 3.3.8
>> CPU: Intel Xeon E312xx (Sandy Bridge) - 1799.998 MHz (4 cores)
>> Memory: 4096 MiB
>> Harddisk: 10 GiB, SCSI, raw, cache none
>> 
>> When I execute a performance test with 2000 concurrent connections handling a file size of 10KB on each request.
>> # ab -n 10000 -c 2000 -X 10.10.7.15:3128 http://https.example.com/index.html
> 
> You are wrong. "ab -c 2000" to a non-caching proxy means *4000*
> concurrent connections being handled by the proxy. Web server only loads
> the file object once.
> 
> A non-caching proxy requires +1 connection to server for each inbound
> client connection ( 2000 + 2000 = 4K concurrent connections ).
> 
> 
>> This is ApacheBench, Version 2.3 <$Revision: 1430300 $>
>> Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
>> Licensed to The Apache Software Foundation, http://www.apache.org/
>> 
>> Benchmarking https.rinis.nl [through 10.10.7.15:3128] (be patient)
>> Completed 1000 requests
>> Completed 2000 requests
>> Completed 3000 requests
>> Completed 4000 requests
>> Completed 5000 requests
>> Completed 6000 requests
>> Completed 7000 requests
>> Completed 8000 requests
>> apr_pollset_poll: The timeout specified has expired (70007)
> 
> Squid is still responding by the client has given up. As shown by the
> _ABORTED in the squid log.
> 
> 
>> Total of 8610 requests completed
>> 
>> I have the command "vmstat 5? running on the squid server:
>> procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
>> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
>> 2  0      0 3823916    764 124992    0    0   519    26  237  503  2  3 92  3  0
>> 0  0      0 3823744    764 125072    0    0     0     0   44   79  0  0 100  0  0
>> 0  0      0 3823776    764 125044    0    0     0     2   39   70  0  0 100  0  0
>> 0  0      0 3729540    764 139116    0    0     1     0 2145  257  1  2 97  0  0
>> 0  0      0 3728432    764 139888    0    0     0    46 2297  594  1  1 97  0  0
>> 0  0      0 3726484    764 140892    0    0     0    39 2869  581  2  1 97  0  0
>> 0  0      0 3725528    764 141376    0    0     0     0 2843  648  2  2 96  0  0
>> 0  0      0 3724980    764 142008    0    0     0    69 2824  529  2  1 97  0  0
>> 0  0      0 3724584    764 142540    0    0     0     0 2742  472  2  1 97  0  0
>> 0  0      0 3723696    764 143004    0    0     0     0 2511  577  2  1 97  0  0
>> 0  0      0 3722840    764 143200    0    0     0    12  884  228  1  1 99  0  0
>> 0  0      0 3722704    764 142900    0    0     0     0  136  127  0  0 100  0  0
>> 0  0      0 3722504    764 142744    0    0     0     0   40   70  0  0 100  0  0
>> 0  0      0 3722456    764 142784    0    0     0   114   37   68  0  0 100  0  0
>> 0  0      0 3722208    764 142832    0    0     0     0   41   68  0  0 100  0  0
>> 0  0      0 3722480    764 142280    0    0     0     0  179   82  0  0 100  0  0
>> 0  0      0 3722544    764 142140    0    0     0     7   41   75  0  0 100  0  0
>> procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
>> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
>> 1  0      0 3722544    764 142136    0    0     0     0   36   67  0  0 100  0  0
>> 0  0      0 3722996    764 141552    0    0     0     0   42   75  0  0 100  0  0
>> 0  0      0 3722980    764 141568    0    0     0     0   37   68  0  0 100  0  0
>> 0  0      0 3723028    764 141524    0    0     0     0   36   66  0  0 100  0  0
>> 0  0      0 3736816    764 130352    0    0     0     0  809  114  0  0 99  0  0
>> 0  0      0 3737544    764 130268    0    0     0    41   42   74  0  0 100  0  0
>> 
>> It looks like the hardware has enough resources during the benchmark test.
>> 
>> I?ve got the following squid.conf running:
>> cache_peer 192.168.0.18 parent 3128 0 round-robin no-query no-digest
>> cache_peer 192.168.0.20 parent 3128 0 round-robin no-query no-digest
>> 
>> acl development_net dst 192.168.0.0/24
>> cache_peer_access 192.168.0.18 allow development_net
>> cache_peer_access 192.168.0.20 allow development_net
>> 
>> never_direct allow all
>> cache deny all
>> 
>> maximum_object_size_in_memory 16 MB
>> cache_mem 2048 MB
>> 
>> The squid must not cache at all.
> 
> The dont bother setting cache_mem to 2GB. The memory cache wont be used.
> 
> Also note that the lack of caching is *worsening* your performance
> results. When memory cache is used the FD usage is halved, and the time
> to respond is greatly increased (factor of approx 100 in latency reduction).
> Consider removing the "cache deny all" when you get this into
> production. The 2GB memory cache you assigned can help a *lot* for quick
> short term bursts of high traffic (ie. some DoS situations).
> 
> 
> I do not see any SMP configuration in your Squid. Meaning that its
> operating all those 4K connections with a single process on a single
> 1.7GHz core. Thats not much processor to work with.
> 
> Try adding this to your config file:
> workers 2
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150429/aef3b952/attachment.htm>

From jaykbvt at gmail.com  Wed Apr 29 06:39:15 2015
From: jaykbvt at gmail.com (jaykbvt)
Date: Tue, 28 Apr 2015 23:39:15 -0700 (PDT)
Subject: [squid-users] transparent proxy original_dst err
In-Reply-To: <55370EEF.3060809@treenet.co.nz>
References: <1429613044753-4670846.post@n4.nabble.com>
 <55363290.4000306@treenet.co.nz> <1429620216887-4670852.post@n4.nabble.com>
 <55364F9C.9080908@treenet.co.nz> <1429644716040-4670856.post@n4.nabble.com>
 <55370EEF.3060809@treenet.co.nz>
Message-ID: <1430289555867-4670967.post@n4.nabble.com>

Hi Amos,

We've got response from Cisco team and they've agreed that destination IP
gets changed when request passes through Cisco ISG. 

They are taking reference for configuration from this doc

http://www.cisco.com/c/en/us/td/docs/ios/isg/configuration/guide/15_0s/isg_15_0s_book/isg_l4_redirect.html#wp1055689

Now they are very adamant that ISG l4 redirection works only this way. They
are asking us to configure squid in a manner to retrieve destination host
from http request.

I am not aware if its possible or not.

Any suggestions.?

Thanks & Regards,
Jaykbvt 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-original-dst-err-tp4670846p4670967.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tomtux007 at gmail.com  Wed Apr 29 07:38:36 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Wed, 29 Apr 2015 09:38:36 +0200
Subject: [squid-users] Happy Eyeballs and "connect_timeout" in squid 3.4.12
Message-ID: <CACLJR+MvMs-zVV0O=Za8pe6VmQ7v9aWegDunzswWQDfrNr3fjQ@mail.gmail.com>

Hi

I'm running squid (3.4.12) on a IPv6/IPv4-dual-stack system.

While accessing the test-site "http://test.rx.td.h.labs.apnic.net", I
encountered a 60s connection-timeout (configurable with
connect_timeout) while squid is making 5 IPv6-connection-attempts
(SYN), before it tries to connect with IPv4 (which is working on the
test-site). I can decrease the "connect_timeout"-value to 1 second.
This behaves in a better "surf"-experience and results in a 1s-timeout
(also only 1 IPv6-SYN) instead of the default 60s timeout.

Why does squid not tries to connect first IPv6 (based on the host's
address preference-policy) and then - in case of a failure - switch to
IPv4 during a 300ms timeout (like current Browsers are doing)?

Can I enforce a "browser behaviour" (300ms) for squid?

Thank you.

Kind regards,
Tom


From squid3 at treenet.co.nz  Wed Apr 29 10:28:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 22:28:24 +1200
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <1430286253928.fcbd2ab7@Nodemailer>
References: <1429494253383.2bf365a4@Nodemailer>
 <1430286253928.fcbd2ab7@Nodemailer>
Message-ID: <5540B248.9000808@treenet.co.nz>

On 29/04/2015 5:44 p.m., dan wrote:
> I mentioned last time that we had to x2 all our delay_parameter?s
> bytes because of a weird bug where squid would apply it at half speed
> for no reason.
> 
> It just occurred to me that (obviously) this is why HTTPS downloads
> are going too fast; because this bug must only affect HTTP traffic.
> 
> So HTTPS downloads are going at the actual speed we?ve specified and
> HTTP is going at half that.
> 
> Therefore, we should be able to work around it by setting different
> delay_parameters for HTTP and HTTPS requests.
> 
> So my question is, how best to target only those requests? By the
> CONNECT method?

Yes, "CONNECT" ACL matching the method should work.
Or alternatively:
 acl HTTP proto HTTP

Amos


From jagannath.naidu at fosteringlinux.com  Wed Apr 29 11:34:01 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Wed, 29 Apr 2015 17:04:01 +0530
Subject: [squid-users] NTLM AUTH: All redirector processes are busy
Message-ID: <CA+8bHvza9tLGaHk=hCcY8+NzOcKFBDrZJzzpzT__7LTiGkpPgg@mail.gmail.com>

Hi List/Amos,

I am facing an using squid in production.

I get these messages in cache.log, and service stop for a period of time
(like 14 seconds). During this period, users get panic as they get "proxy
server resfusing connections". And automatically the service starts
functioning again. But this happens very frequently whole day.

2015/04/29 10:34:10| WARNING: All redirector processes are busy.
2015/04/29 10:34:10| WARNING: 15 pending requests queued
2015/04/29 10:34:10| storeDirWriteCleanLogs: Starting...
2015/04/29 10:34:10| WARNING: Closing open FD 3327
2015/04/29 10:34:10|     65536 entries written so far.
2015/04/29 10:34:10|    131072 entries written so far.
2015/04/29 10:34:10|    196608 entries written so far.
2015/04/29 10:34:10|    262144 entries written so far.
2015/04/29 10:34:10|    327680 entries written so far.
2015/04/29 10:34:10|    393216 entries written so far.
2015/04/29 10:34:10|    458752 entries written so far.
2015/04/29 10:34:10|    524288 entries written so far.
2015/04/29 10:34:10|    589824 entries written so far.
2015/04/29 10:34:10|    655360 entries written so far.
2015/04/29 10:34:10|   Finished.  Wrote 716101 entries.
2015/04/29 10:34:10|   Took 0.22 seconds (3266168.90 entries/sec).
FATAL: Too many queued redirector requests
Squid Cache (Version 3.1.10): Terminated abnormally.
CPU Usage: 4206.393 seconds = 3778.049 user + 428.344 sys
Maximum Resident Size: 2599760 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
        total space in arena:  750272 KB
        Ordinary blocks:       717419 KB   6620 blks
        Small blocks:               0 KB      1 blks
        Holding blocks:         23020 KB     11 blks
        Free Small blocks:          0 KB
        Free Ordinary blocks:   32852 KB
        Total in use:          740439 KB 99%
        Total free:             32852 KB 4%
fgets() failed! dying..... errno=1 (Operation not permitted)
2015/04/29 10:34:19| Starting Squid Cache version 3.1.10 for
x86_64-redhat-linux-gnu...
2015/04/29 10:34:19| Process ID 4326
2015/04/29 10:34:19| With 100000 file descriptors available
2015/04/29 10:34:19| Initializing IP Cache...
2015/04/29 10:34:19| DNS Socket created at [::], FD 8
2015/04/29 10:34:19| DNS Socket created at 0.0.0.0, FD 9
2015/04/29 10:34:19| Adding nameserver 172.16.3.34 from squid.conf
2015/04/29 10:34:19| Adding nameserver 10.1.2.91 from squid.conf
2015/04/29 10:34:19| helperOpenServers: Starting 5/5 'squidGuard' processes
2015/04/29 10:34:19| helperOpenServers: Starting 1500/1500 'ntlm_auth'
processes
2015/04/29 10:34:24| helperOpenServers: Starting 150/150 'wbinfo_group.pl'
processes


ntlm helpers count is 1500 and external "wbinfo_group.pl" helpers are 150.

squid.conf
###################################################

max_filedesc 100000
acl manager proto cache_object
acl localhost src 172.16.50.61
http_access allow manager localhost
dns_nameservers 172.16.3.34 10.1.2.91
acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
http_access allow allowips
#acl haproxy src 172.16.50.61
#follow_x_forwarded_for allow haproxy
#follow_x_forwarded_for deny all
#acl manager proto cache_object
acl localnet src 172.16.0.0/16
acl manager proto cache_object
acl localhost src 127.0.0.1
acl localnet src fc00::/7 # RFC 4193 local private network range
acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines
acl office dstdomain "/etc/squid/officesites"
http_access allow office
log_ip_on_direct off
#debug_options ALL,3
#logformat squid %9d.%03d %6d %s %s/%03d %d %s %s %s %s%s/%s %s
logformat squid %ts.%03tu %tl %3tr %3dt %3un %>a %Ss/%>Hs %<st %rm %ru
%Sh/%<A %mt
access_log /var/log/squid/access1.log squid
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
#auth_param ntlm program /etc/squid/helper-mux.pl /usr/bin/ntlm_auth
--diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
auth_param ntlm children 1500
#auth_param ntlm children 500
auth_param ntlm keep_alive off
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
external_acl_type wbinfo_group_helper ttl=600 children=150 %LOGIN
/usr/lib64/squid/wbinfo_group.pl -d
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
cl Safe_ports port 8080 #https
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl auth proxy_auth REQUIRED


and rest of acls and http_access rules configured ...............



It seems the helper programs are not closing automatically after serving
and causes this issue. Could anyone help resolving this issue.

[root at GGNPROXY01 squid]# rpm -qa | grep squid
squid-3.1.10-19.el6_4.x86_64

[root at GGNPROXY01 squid]# rpm -qa | grep winbind
samba-winbind-clients-3.6.9-164.el6.x86_64
samba-winbind-3.6.9-164.el6.x86_64

[root at GGNPROXY01 squid]# lsb_release -a
LSB Version:
:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:    RedHatEnterpriseServer
Description:    Red Hat Enterprise Linux Server release 6.5 (Santiago)
Release:    6.5
Codename:    Santiago


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150429/ea918938/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr 29 11:56:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2015 23:56:03 +1200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
 connections?
In-Reply-To: <13BF7DA2-AC5C-41BB-9E94-758BF2A06D59@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
 <13BF7DA2-AC5C-41BB-9E94-758BF2A06D59@rinis.nl>
Message-ID: <5540C6D3.8040900@treenet.co.nz>

On 29/04/2015 6:42 p.m., Abdelouahed Haitoute wrote:
> Hello Amos,
> 
>> Try adding this to your config file:
>> workers 2
> 
> I?ve added this directive. After a restart I don?t see squid-process listening on the tcp port 3128.
> Active Internet connections (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
> tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      921/sshd
> tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1033/master
> tcp6       0      0 :::22                   :::*                    LISTEN      921/sshd
> tcp6       0      0 ::1:25                  :::*                    LISTEN      1033/master
> 
> /var/log/messages
> squid[17362]: Squid Parent: will start 3 kids
> squid[17362]: Squid Parent: (squid-coord-3) process 17364 started
> squid[17362]: Squid Parent: (squid-2) process 17365 started
> squid[17362]: Squid Parent: (squid-1) process 17366 started
> systemd: Started Squid caching proxy.
> 
> What am I missing?

The messages log entries look fine.

SELinux?
 (just a guess, AFAIK with port 3128 the default policy should be working).

What does cache.log say?

Amos



From squid3 at treenet.co.nz  Wed Apr 29 13:02:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2015 01:02:38 +1200
Subject: [squid-users] Happy Eyeballs and "connect_timeout" in squid
	3.4.12
In-Reply-To: <CACLJR+MvMs-zVV0O=Za8pe6VmQ7v9aWegDunzswWQDfrNr3fjQ@mail.gmail.com>
References: <CACLJR+MvMs-zVV0O=Za8pe6VmQ7v9aWegDunzswWQDfrNr3fjQ@mail.gmail.com>
Message-ID: <5540D66E.3000004@treenet.co.nz>

On 29/04/2015 7:38 p.m., Tom Tom wrote:
> Hi
> 
> I'm running squid (3.4.12) on a IPv6/IPv4-dual-stack system.
> 
> While accessing the test-site "http://test.rx.td.h.labs.apnic.net", I
> encountered a 60s connection-timeout (configurable with
> connect_timeout) while squid is making 5 IPv6-connection-attempts
> (SYN), before it tries to connect with IPv4 (which is working on the
> test-site). I can decrease the "connect_timeout"-value to 1 second.
> This behaves in a better "surf"-experience and results in a 1s-timeout
> (also only 1 IPv6-SYN) instead of the default 60s timeout.
> 
> Why does squid not tries to connect first IPv6 (based on the host's
> address preference-policy) and then - in case of a failure - switch to
> IPv4 during a 300ms timeout (like current Browsers are doing)?

Several reasons:

1) The default builds and installs do try IPv6 first in accordance with
RFC 6540. Check your config for a "dns_v4_first" directive which forces
IPv4 to be tried first.


2) Squid is not the OS built-in resolver. Any obeying of that policy by
Squid is purely arbitrary. The host systems DNS resolver policy does not
supposed to affect standalone resolvers such as Squids internal one.
Particularly when there are squid.conf directives overriding the
resolv.conf behaviour (eg. dns_nameservers).

dns_v4_first was a partial implementation added for
<http://bugs.squid-cache.org/show_bug.cgi?id=3086>.


3) when performed by middleware such as Squid the "Happy Eyeballs"
algorithm is heavily destructive.

A browser is consuming at minimum 2 network sockets to perform "Happy
Eyeballs".

At the middlware each of those translates to potentially 3 sockets
(total 6 proxy sockets, 2 outgoing server sockets). If the middleware
were to perform "Happy Eyeballs" itself that would increase to 4 sockets
(total 8 proxy sockets, 4 outgoing server sockets).

A typical network connection travels through at least 2 sometimes 3
proxies when CDN are involved. With each proxy hop doubling the number
of server sockets we easily reach a total of 16 inbound sockets on the
poor backend server to deal with for each single browser connection.

Proxies and servers are already dealing with scales of 10,000 - 100,000
clients at once. Multiplying the network resource load by a factor of 8
would easily push the scale up into millions. Which is a little hard to
deal with when constrained to only 65535 ports per IP address.

Of course the browsers dont care about that because they have only a few
sockets to deal with and plenty of RAM, time, and ports. For them it was
easy.

Instead we *obey HTTP* by caching, multiplexing requests over persistent
connections, and keeping a pool of idle server connections that have
already been setup. Often that works faster than even "Happy Eyeballs" can.

Horror story over, we do have
<http://bugs.squid-cache.org/show_bug.cgi?id=3552> though as I mention
in there we need slightly different ways that work better for proxies
than the prescribed browsers server-unfriendly algorithm.

> 
> Can I enforce a "browser behaviour" (300ms) for squid?

Nope. The big reason for this one is that the config directive is scaled
in whole seconds along with most timeouts in Squid.

Amos



From alex at delgado-lucas.com  Wed Apr 29 13:26:30 2015
From: alex at delgado-lucas.com (Alex Delgado)
Date: Wed, 29 Apr 2015 13:26:30 +0000
Subject: [squid-users] squid-ldap-group not ERR
Message-ID: <DUB130-W389C0D9426B5EB2D1C779A80D70@phx.gbl>

Hello,
 
I'm trying to configure squid to validate Windows users  by group with squid-ldap-group.
 
Server is CENTOS 6.5 . I've installed samba, krb and squid from source.
 
Also, I've configured samba and krb, so centos server is a Windows member.
 
When I type :
 
/usr/lib64/squid/squid_ldap_group -R -b "dc=domain,dc=local" -f "(&(sAMAccountName=%v)(memberOf=cn=%a,dc=domain,dc=local))" -D "cn=user,cn=Users,dc=edvhold,dc=local" -W /dir/dir/ldpass.txt -h pdcserver
user group
 
I got:
 
ERR
 
Does anybody what the erro is?
 
Regards,
 
 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150429/0c077a9c/attachment.htm>

From ulises at vianetcon.com.ar  Wed Apr 29 14:23:50 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Wed, 29 Apr 2015 11:23:50 -0300
Subject: [squid-users] A lot of open rewriter heplers and are hanging! Squid
	3.5
In-Reply-To: <5540DB94.8090307@vianetcon.com.ar>
References: <5540DB94.8090307@vianetcon.com.ar>
Message-ID: <5540E976.3030902@vianetcon.com.ar>

  Hello,

I have migrated from Squid 2.7 to Squid 3.5 in a heavy  traffic 
enviroment (150 http request per second, and more than 1200 clients), 
using trpxy configuration.

squid -v
Squid Cache: Version 3.5.3
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/share' 
'--bindir=/usr/local/sbin' '--libexecdir=/usr/local/lib/squid' 
'--localstatedir=/var' '--sysconfdir=/etc/squid3' '--enable-delay-pools' 
'--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 
'--enable-eui' '--enable-snmp' '--enable-gnuregex' 
'--enable-ltdl-convenience' '--enable-removal-policies=lru heap' 
'--enable-http-violations' '--with-openssl' 
'--with-filedescriptors=24321' '--enable-poll' '--enable-epoll'

I use  jesred rewriter (http://www.linofee.org/~jel/webtools/jesred/). 
This applications was modified to match the output format requested by 
squid 3.5 (example OK 
store-id=http://app.updates.avast.com.SQUIDINTERNAL/jrog2-c40-c3f). This 
is the input and output format that jsred log in file

Input string->1430313641.311 Input String: 
http://photos-g.ak.instagram.com/hphotos-ak-xaf1/t51.2885-15/s150x150/e15/10932037_1549994791925790_574318541_n.jpg 

186.148.250.37/186-148-250-37 - GET myip=- myport=801
1430313641.311 Request: 
http://photos-g.ak.instagram.com/hphotos-ak-xaf1/t51.2885-15/s150x150/e15/10932037_1549994791925790_574318541_n.jpg 
186.148.250.37/186-148-250-37 - GET
1430313641.311 ACL: allowed 186.148.250.37

OutputString ->1430313641.311 Output string: OK 
store-id=http://img.instagram.ak.SQUIDINTERNAL/hphotos-ak-xaf1/t51.2885-15/s150x150/e15/10932037_1549994791925790_574318541_n.jpg



The problem is that after several hours of running squid, the process 
list (ps -aux command)  is filled with hanging jesred !

This is the portion of squid configuration

/store_id_program /sysmgr/squid/jesred//
//store_id_children 20 //
//store_id_bypass on/

And this is my process list after three days!!!

Any ideas?

Thanks!

Ulises


USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root      2618  0.0  0.0  59360   600 ?        Ss   Apr24   0:00 
/usr/local/sbin/squid -f /etc/squid3/squid1.conf
squid     2960  0.0  0.0  10404  1440 ?        S    Apr26   0:00 (jesred)
squid     3135  0.0  0.0  10404  1436 ?        S    Apr26   0:00 (jesred)
squid     3151  0.0  0.0  10540  1700 ?        S    Apr26   0:00 (jesred)
squid      430  0.0  0.0  10672  1704 ?        S    Apr26   0:00 (jesred)
squid     2326  0.0  0.0  10404   836 ?        S    Apr25   0:00 (jesred)
squid     2577  0.0  0.0  10672  1656 ?        S    Apr25   0:00 (jesred)
squid     2578  0.0  0.0  10540  1532 ?        S    Apr25   0:00 (jesred)

... and more than 150 process more
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150429/fac4d982/attachment.htm>

From yvoinov at gmail.com  Wed Apr 29 15:34:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 29 Apr 2015 21:34:56 +0600
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <5540E976.3030902@vianetcon.com.ar>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar>
Message-ID: <5540FA20.9050208@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You think, 1200 clients/150 qps is heavy  traffic enviroment? Absolutely no.

Thit is little typical installation.

//store_id_children 20 //
//store_id_bypass on/

You really sure 20 children is enough for 1200 clients? Also whenever
bypass on?

In any case check your OS settings. OS also need preparation and tuning
- suprise! - and not out-of-the-box. BTW, what is your OS?

You problem seems is not squid issue. But OS-level.

29.04.15 20:23, Ulises Nicolini ?????:
> heavy  traffic enviroment

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVQPogAAoJENNXIZxhPexGDv8H/jlosrvW7Iz4V7RycNZ/ZdMV
KE3oDGJtrQy/z9sQ4QHzQ/V1SbkrljAk5EHCuetaD1t3aZeXfIPGKXr19Acni7ty
RfH0WueOtg1+SCwrKxoJjTF3I5hLjEMdWddXASkAdfz8qx3gY8ZF5vin9XT5OvwO
xmkpl+tgPFRi1wTX+tmium8oqTxmJktSw76x6kCmNBHdd9jAR7F5Gn8aeMEWyd62
3V6FSsxy9RmpVanPsNiH1C18OMxAVMaOFxn8mg49RSQQjsXAgf0myDax4DPXMjKe
O5JIzUAr0bLdeJnEqOYANIyVe0RnjVdSx6dtU31ZEwYtS2RwGESfY8FiUMtlryA=
=fNZy
-----END PGP SIGNATURE-----



From yan at seiner.com  Wed Apr 29 16:24:38 2015
From: yan at seiner.com (Yan Seiner)
Date: Wed, 29 Apr 2015 09:24:38 -0700
Subject: [squid-users] squid + privoxy = Unable to forward request
Message-ID: <554105C6.3070107@seiner.com>

I am migrating a previous installation to new hardware and updated versions.

I use squid + privoxy, with privoxy being the parent.

Privoxy listens on port 8118 and squid on 3128 on the same hardware.

Each proxy works fine on its own.  I can test them individually and both 
proxy correctly.

However, when I add the cache_peer directive, I get the "Unable to 
forward request" from squid.

I have tried this with both localhost (127.0.0.1) and the eth address 
192.168.5.1.  Both result in the same error.

I have disabled the firewall for testing and tried to set up the ACLs to 
allow everyone from everywhere, and still the error persists.  The logs 
show no errors.  The squid logs show the request being received, but the 
privoxy logs don't show any requests hitting it from the squid proxy.

squid.conf:

visible_hostname ap1.seiner.com
cache_effective_user nobody
cache_effective_group nogroup
cache_dir ufs /cache 4096 16 256

cache_peer '127.0.0.1 parent 8118 0 no-query no-digest'
never_direct allow all

acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10

acl ssl_ports port 443

acl safe_ports port 80
acl safe_ports port 21
acl safe_ports port 443
acl safe_ports port 70
acl safe_ports port 210
acl safe_ports port 1025-65535
acl safe_ports port 280
acl safe_ports port 488
acl safe_ports port 591
acl safe_ports port 777
acl connect method connect

http_access deny !safe_ports
http_access deny connect !ssl_ports

http_access allow localhost manager
http_access deny manager

http_access deny to_localhost

http_access allow localnet
http_access allow localhost

http_access deny all

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

debug_options ALL,1
access_log stdio:/var/log/squid.access.log
cache_log stdio:/var/log/squid.cache.log
cache_store_log /dev/null
logfile_rotate 0

logfile_daemon /dev/null
http_port 3128
coredump_dir /tmp/squid
pinger_enable off

privoxy.conf:

confdir    /etc/privoxy
logdir    /var/log
logfile    privoxy.log
filterfile    default.filter
actionsfile    match-all.action
actionsfile    default.action
listen-address    127.0.0.1:8118
toggle    1
enable-remote-toggle    1
enable-remote-http-toggle    0
enable-edit-actions    1
enforce-blocks    0
buffer-limit    4096
forwarded-connect-retries    0
accept-intercepted-requests    0
allow-cgi-request-crunching    0
split-large-forms    0
keep-alive-timeout    300
socket-timeout    300
debug    512
debug    4096
debug    8192



From yan at seiner.com  Wed Apr 29 16:34:23 2015
From: yan at seiner.com (Yan Seiner)
Date: Wed, 29 Apr 2015 09:34:23 -0700
Subject: [squid-users] squid + privoxy = Unable to forward request
In-Reply-To: <554105C6.3070107@seiner.com>
References: <554105C6.3070107@seiner.com>
Message-ID: <5541080F.1010907@seiner.com>

ARGHHH!!!!

Never mind.

Somehow I had managed to insert single quotes around the cache_peer 
arguments.

cache_peer '127.0.0.1 parent 8118 0 no-query no-digest'

I've removed the single quotes and all is well.

On 04/29/2015 09:24 AM, Yan Seiner wrote:
> I am migrating a previous installation to new hardware and updated 
> versions.
>
> I use squid + privoxy, with privoxy being the parent.
>
> Privoxy listens on port 8118 and squid on 3128 on the same hardware.
>
> Each proxy works fine on its own.  I can test them individually and 
> both proxy correctly.
>
> However, when I add the cache_peer directive, I get the "Unable to 
> forward request" from squid.
>
> I have tried this with both localhost (127.0.0.1) and the eth address 
> 192.168.5.1.  Both result in the same error.
>
> I have disabled the firewall for testing and tried to set up the ACLs 
> to allow everyone from everywhere, and still the error persists.  The 
> logs show no errors.  The squid logs show the request being received, 
> but the privoxy logs don't show any requests hitting it from the squid 
> proxy.
>
> squid.conf:
>
> visible_hostname ap1.seiner.com
> cache_effective_user nobody
> cache_effective_group nogroup
> cache_dir ufs /cache 4096 16 256
>
> cache_peer '127.0.0.1 parent 8118 0 no-query no-digest'
> never_direct allow all
>
> acl localnet src 10.0.0.0/8
> acl localnet src 172.16.0.0/12
> acl localnet src 192.168.0.0/16
> acl localnet src fc00::/7
> acl localnet src fe80::/10
>
> acl ssl_ports port 443
>
> acl safe_ports port 80
> acl safe_ports port 21
> acl safe_ports port 443
> acl safe_ports port 70
> acl safe_ports port 210
> acl safe_ports port 1025-65535
> acl safe_ports port 280
> acl safe_ports port 488
> acl safe_ports port 591
> acl safe_ports port 777
> acl connect method connect
>
> http_access deny !safe_ports
> http_access deny connect !ssl_ports
>
> http_access allow localhost manager
> http_access deny manager
>
> http_access deny to_localhost
>
> http_access allow localnet
> http_access allow localhost
>
> http_access deny all
>
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>
> debug_options ALL,1
> access_log stdio:/var/log/squid.access.log
> cache_log stdio:/var/log/squid.cache.log
> cache_store_log /dev/null
> logfile_rotate 0
>
> logfile_daemon /dev/null
> http_port 3128
> coredump_dir /tmp/squid
> pinger_enable off
>
> privoxy.conf:
>
> confdir    /etc/privoxy
> logdir    /var/log
> logfile    privoxy.log
> filterfile    default.filter
> actionsfile    match-all.action
> actionsfile    default.action
> listen-address    127.0.0.1:8118
> toggle    1
> enable-remote-toggle    1
> enable-remote-http-toggle    0
> enable-edit-actions    1
> enforce-blocks    0
> buffer-limit    4096
> forwarded-connect-retries    0
> accept-intercepted-requests    0
> allow-cgi-request-crunching    0
> split-large-forms    0
> keep-alive-timeout    300
> socket-timeout    300
> debug    512
> debug    4096
> debug    8192
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From tomtux007 at gmail.com  Wed Apr 29 19:05:27 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Wed, 29 Apr 2015 21:05:27 +0200
Subject: [squid-users] Happy Eyeballs and "connect_timeout" in squid
	3.4.12
In-Reply-To: <5540D66E.3000004@treenet.co.nz>
References: <CACLJR+MvMs-zVV0O=Za8pe6VmQ7v9aWegDunzswWQDfrNr3fjQ@mail.gmail.com>
 <5540D66E.3000004@treenet.co.nz>
Message-ID: <CACLJR+NVVb2eDjoz_MVG67VijxWq6zdEL_c+i9-va6SScq_xbA@mail.gmail.com>

Thank you Amos, for this explanation.


On Wed, Apr 29, 2015 at 3:02 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 29/04/2015 7:38 p.m., Tom Tom wrote:
>> Hi
>>
>> I'm running squid (3.4.12) on a IPv6/IPv4-dual-stack system.
>>
>> While accessing the test-site "http://test.rx.td.h.labs.apnic.net", I
>> encountered a 60s connection-timeout (configurable with
>> connect_timeout) while squid is making 5 IPv6-connection-attempts
>> (SYN), before it tries to connect with IPv4 (which is working on the
>> test-site). I can decrease the "connect_timeout"-value to 1 second.
>> This behaves in a better "surf"-experience and results in a 1s-timeout
>> (also only 1 IPv6-SYN) instead of the default 60s timeout.
>>
>> Why does squid not tries to connect first IPv6 (based on the host's
>> address preference-policy) and then - in case of a failure - switch to
>> IPv4 during a 300ms timeout (like current Browsers are doing)?
>
> Several reasons:
>
> 1) The default builds and installs do try IPv6 first in accordance with
> RFC 6540. Check your config for a "dns_v4_first" directive which forces
> IPv4 to be tried first.

According to RFC 6555:  "Over time, as most content is available via
IPv6, the amount of IPv4 traffic will decrease.". With forcing this
directive, I reduce the chance for connecting with IPv6 and my
outbound connections are probably a long time with IPv4. This is maybe
not the behaviour we want?

>
>
> 2) Squid is not the OS built-in resolver. Any obeying of that policy by
> Squid is purely arbitrary. The host systems DNS resolver policy does not
> supposed to affect standalone resolvers such as Squids internal one.
> Particularly when there are squid.conf directives overriding the
> resolv.conf behaviour (eg. dns_nameservers).
>
> dns_v4_first was a partial implementation added for
> <http://bugs.squid-cache.org/show_bug.cgi?id=3086>.
>
>
> 3) when performed by middleware such as Squid the "Happy Eyeballs"
> algorithm is heavily destructive.
>
> A browser is consuming at minimum 2 network sockets to perform "Happy
> Eyeballs".
>
> At the middlware each of those translates to potentially 3 sockets
> (total 6 proxy sockets, 2 outgoing server sockets). If the middleware
> were to perform "Happy Eyeballs" itself that would increase to 4 sockets
> (total 8 proxy sockets, 4 outgoing server sockets).

But only in the parallel way (1 x IPv6 and 1 x IPv4)?

Tests with the current curl (I know, curl != squid) behaves not in
doing two similar (parallel) TCP-Connections (1x SYN for IPv6 and 1x
SYN for IPv4). Instead, curl tries IPv6 first and in case of an
connection-error, it tries after a few milliseconds with IPv4. This
way, not a lot of sockets should be consumed. Does this not behave
like a native IPv4-Stack? Squid would behave like curl, if I would be
able to change the connect_timeout to milliseconds.

Is there a well-known restriction (which I don't now, actually), by
setting the connect_timeout to 1 second for all those IPv6-Adresses,
which aren't connectable and for which the IPv4-Stack should be used
after 1s timeout? Is this a practicable way?

>
> A typical network connection travels through at least 2 sometimes 3
> proxies when CDN are involved. With each proxy hop doubling the number
> of server sockets we easily reach a total of 16 inbound sockets on the
> poor backend server to deal with for each single browser connection.
>
> Proxies and servers are already dealing with scales of 10,000 - 100,000
> clients at once. Multiplying the network resource load by a factor of 8
> would easily push the scale up into millions. Which is a little hard to
> deal with when constrained to only 65535 ports per IP address.
>
> Of course the browsers dont care about that because they have only a few
> sockets to deal with and plenty of RAM, time, and ports. For them it was
> easy.
>
> Instead we *obey HTTP* by caching, multiplexing requests over persistent
> connections, and keeping a pool of idle server connections that have
> already been setup. Often that works faster than even "Happy Eyeballs" can.
>
> Horror story over, we do have
> <http://bugs.squid-cache.org/show_bug.cgi?id=3552> though as I mention
> in there we need slightly different ways that work better for proxies
> than the prescribed browsers server-unfriendly algorithm.
>
>>
>> Can I enforce a "browser behaviour" (300ms) for squid?
>
> Nope. The big reason for this one is that the config directive is scaled
> in whole seconds along with most timeouts in Squid.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Thu Apr 30 10:10:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 30 Apr 2015 16:10:32 +0600
Subject: [squid-users] Squid Bugzilla is down
Message-ID: <5541FF98.5010109@gmail.com>

Amos,

what's up with bugzilla? It down and not available.

WBR, Yuri




From steve at opendium.com  Thu Apr 30 16:14:55 2015
From: steve at opendium.com (Steve Hill)
Date: Thu, 30 Apr 2015 17:14:55 +0100
Subject: [squid-users] Assert(call->dialer.handler == callback)
Message-ID: <554254FF.2080603@opendium.com>


I've just migrated a system from Squid 3.4.10 to 3.5.3 and I'm getting 
frequent crashes with an assertion of "call->dialer.handler == callback" 
in Read.cc:comm_read_cancel().

call->dialer.handler == (IOCB *) 0x7ffe1493b2d0 
<TunnelStateData::ReadClient(Comm::ConnectionPointer const&, char*, 
size_t, Comm::Flag, int, void*)>

callback == <IdleConnList::Read(Comm::ConnectionPointer const&, char*, 
size_t, Comm::Flag, int, void*)>


This is quite a busy system doing server-first ssl_bump and I get a lot 
of SSL negotiation errors in cache.log (these were present under 3.4.10 
too).  I think a good chunk of these are Team Viewer, which abuses 
CONNECTs to port 443 of remote servers to do non-SSL traffic, so 
obviously isn't going to work with ssl_bump.  I _suspect_ that the 
assertion may be being triggered by these SSL errors (e.g. connection 
being unexpectedly torn down because SSL negotiation failed?), but I 
can't easily prove that.

I don't quite understand the comm_read_cancel() function though - as far 
as I can see, the callback parameter is only used in the assert() - is 
that correct?


Stack trace:
#0  0x00007ffe1155d625 in raise (sig=6) at 
../nptl/sysdeps/unix/sysv/linux/raise.c:64
#1  0x00007ffe1155ee05 in abort () at abort.c:92
#2  0x00007ffe148210df in xassert (msg=Unhandled dwarf expression opcode 
0xf3
) at debug.cc:544
#3  0x00007ffe14a62787 in comm_read_cancel (fd=600, 
callback=0x7ffe148c8dd0 <IdleConnList::Read(Comm::ConnectionPointer 
const&, char*, size_t, Comm::Flag, int, void*)>,
     data=0x7ffe176c8298) at Read.cc:204
#4  0x00007ffe148c5e62 in IdleConnList::clearHandlers 
(this=0x7ffe176c8298, conn=...) at pconn.cc:157
#5  0x00007ffe148c94ab in IdleConnList::findUseable 
(this=0x7ffe176c8298, key=...) at pconn.cc:269
#6  0x00007ffe148c979d in PconnPool::pop (this=0x7ffe145db010, dest=..., 
domain=Unhandled dwarf expression opcode 0xf3
) at pconn.cc:449
#7  0x00007ffe14852142 in FwdState::pconnPop (this=Unhandled dwarf 
expression opcode 0xf3
) at FwdState.cc:1153
#8  0x00007ffe14855605 in FwdState::connectStart (this=0x7ffe2034c4e8) 
at FwdState.cc:850
#9  0x00007ffe14856a31 in FwdState::startConnectionOrFail 
(this=0x7ffe2034c4e8) at FwdState.cc:398
#10 0x00007ffe148d2fa5 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at 
peer_select.cc:302
#11 0x00007ffe148d6a1d in peerSelectDnsResults (ia=0x7ffe14f0ac20, 
details=Unhandled dwarf expression opcode 0xf3
) at peer_select.cc:383
#12 0x00007ffe148a8e71 in ipcache_nbgethostbyname (name=Unhandled dwarf 
expression opcode 0xf3
) at ipcache.cc:518
#13 0x00007ffe148d23c1 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at 
peer_select.cc:259
#14 0x00007ffe148d6a1d in peerSelectDnsResults (ia=0x7ffe14f0ac20, 
details=Unhandled dwarf expression opcode 0xf3
) at peer_select.cc:383
#15 0x00007ffe148a8e71 in ipcache_nbgethostbyname (name=Unhandled dwarf 
expression opcode 0xf3
) at ipcache.cc:518
#16 0x00007ffe148d23c1 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at 
peer_select.cc:259
#17 0x00007ffe148d382b in peerSelectFoo (ps=0x7ffe1fd0c028) at 
peer_select.cc:522
#18 0x00007ffe149bba6a in ACLChecklist::checkCallback 
(this=0x7ffe2065b9e8, answer=...) at Checklist.cc:167
#19 0x00007ffe148d3f5a in peerSelectFoo (ps=0x7ffe1fd0c028) at 
peer_select.cc:459
#20 0x00007ffe148d5176 in peerSelect (paths=0x7ffe2034c540, 
request=0x7ffe1b660b70, al=Unhandled dwarf expression opcode 0xf3
) at peer_select.cc:163
#21 0x00007ffe14852ae3 in FwdState::Start (clientConn=..., 
entry=0x7ffe1b0da790, request=0x7ffe1b660b70, al=...) at FwdState.cc:366
#22 0x00007ffe14801401 in clientReplyContext::processMiss 
(this=0x7ffe1fcf5838) at client_side_reply.cc:691
#23 0x00007ffe14801eb0 in clientReplyContext::doGetMoreData 
(this=0x7ffe1fcf5838) at client_side_reply.cc:1797
#24 0x00007ffe14805a89 in ClientHttpRequest::httpStart 
(this=0x7ffe1dcda618) at client_side_request.cc:1518
#25 0x00007ffe14808cac in ClientHttpRequest::processRequest 
(this=0x7ffe1dcda618) at client_side_request.cc:1504
#26 0x00007ffe14809013 in ClientHttpRequest::doCallouts 
(this=0x7ffe1dcda618) at client_side_request.cc:1830
#27 0x00007ffe1480b453 in checkNoCacheDoneWrapper (answer=..., 
data=0x7ffe1e5db378) at client_side_request.cc:1400
#28 0x00007ffe149bba6a in ACLChecklist::checkCallback 
(this=0x7ffe1c88b4a8, answer=...) at Checklist.cc:167
#29 0x00007ffe1480b40a in ClientRequestContext::checkNoCache 
(this=0x7ffe1e5db378) at client_side_request.cc:1385
#30 0x00007ffe14809c04 in ClientHttpRequest::doCallouts 
(this=0x7ffe1dcda618) at client_side_request.cc:1748
#31 0x00007ffe1480d109 in ClientRequestContext::clientAccessCheckDone 
(this=0x7ffe1e5db378, answer=Unhandled dwarf expression opcode 0xf3
) at client_side_request.cc:821
#32 0x00007ffe1480d898 in ClientRequestContext::clientAccessCheck2 
(this=0x7ffe1e5db378) at client_side_request.cc:718
#33 0x00007ffe14809767 in ClientHttpRequest::doCallouts 
(this=0x7ffe1dcda618) at client_side_request.cc:1721
#34 0x00007ffe1480afca in ClientHttpRequest::handleAdaptedHeader 
(this=0x7ffe1dcda618, msg=Unhandled dwarf expression opcode 0xf3
) at client_side_request.cc:1935
#35 0x00007ffe14abbcaa in JobDialer<Adaptation::Initiator>::dial 
(this=0x7ffe1ce04990, call=...) at ../../src/base/AsyncJobCalls.h:174
#36 0x00007ffe149bea69 in AsyncCall::make (this=0x7ffe1ce04960) at 
AsyncCall.cc:40
#37 0x00007ffe149c272f in AsyncCallQueue::fireNext (this=Unhandled dwarf 
expression opcode 0xf3
) at AsyncCallQueue.cc:56
#38 0x00007ffe149c2a60 in AsyncCallQueue::fire (this=0x7ffe16f70bf0) at 
AsyncCallQueue.cc:42
#39 0x00007ffe1484110c in EventLoop::runOnce (this=0x7fffcb8c4be0) at 
EventLoop.cc:120
#40 0x00007ffe148412c8 in EventLoop::run (this=0x7fffcb8c4be0) at 
EventLoop.cc:82
#41 0x00007ffe148ae191 in SquidMain (argc=Unhandled dwarf expression 
opcode 0xf3
) at main.cc:1511
#42 0x00007ffe148af2e9 in SquidMainSafe (argc=Unhandled dwarf expression 
opcode 0xf3
) at main.cc:1243
#43 main (argc=Unhandled dwarf expression opcode 0xf3
) at main.cc:1236

(sorry about the DWARF errors - it looks like I've got a version 
mismatch between gcc and gdb)

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150430/b4213adb/attachment.vcf>

From hack.back at hotmail.com  Thu Apr 30 14:11:13 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 30 Apr 2015 07:11:13 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
Message-ID: <1430403073796-4670979.post@n4.nabble.com>

Like i mentioned before i was facing this error and squid restart every few
seconds,
now i found what is causing this error for me
it is :range_offset_limit none partial
when i make range_offset_limit 0
then the error goes out,
now i cant cache 206 contents , if i make 206 contents hit , then i will get
assertion error !
so this is strange !!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gkinkie at gmail.com  Thu Apr 30 17:39:50 2015
From: gkinkie at gmail.com (Kinkie)
Date: Thu, 30 Apr 2015 19:39:50 +0200
Subject: [squid-users] Squid Bugzilla is down
In-Reply-To: <5541FF98.5010109@gmail.com>
References: <5541FF98.5010109@gmail.com>
Message-ID: <CA+Y8hcP4FXVetdCF=RpxSV728bHTikXa1FnwScBkZLh0Dan7WQ@mail.gmail.com>

Hi,
  sorry, we had a severe OOM on the main squid server.
Now rebooted and hopefully better plugged. We will see about upgrading
the server as soon as possible.

On Thu, Apr 30, 2015 at 12:10 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> Amos,
>
> what's up with bugzilla? It down and not available.
>
> WBR, Yuri
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From squid3 at treenet.co.nz  Thu Apr 30 12:28:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 00:28:08 +1200
Subject: [squid-users] Happy Eyeballs and "connect_timeout" in squid
	3.4.12
In-Reply-To: <CACLJR+NVVb2eDjoz_MVG67VijxWq6zdEL_c+i9-va6SScq_xbA@mail.gmail.com>
References: <CACLJR+MvMs-zVV0O=Za8pe6VmQ7v9aWegDunzswWQDfrNr3fjQ@mail.gmail.com>	<5540D66E.3000004@treenet.co.nz>
 <CACLJR+NVVb2eDjoz_MVG67VijxWq6zdEL_c+i9-va6SScq_xbA@mail.gmail.com>
Message-ID: <55421FD8.3050307@treenet.co.nz>

On 30/04/2015 7:05 a.m., Tom Tom wrote:
> Thank you Amos, for this explanation.
> 
> 
> On Wed, Apr 29, 2015 at 3:02 PM, Amos Jeffries wrote:
>> On 29/04/2015 7:38 p.m., Tom Tom wrote:
>>> Hi
>>>
>>> I'm running squid (3.4.12) on a IPv6/IPv4-dual-stack system.
>>>
>>> While accessing the test-site "http://test.rx.td.h.labs.apnic.net", I
>>> encountered a 60s connection-timeout (configurable with
>>> connect_timeout) while squid is making 5 IPv6-connection-attempts
>>> (SYN), before it tries to connect with IPv4 (which is working on the
>>> test-site). I can decrease the "connect_timeout"-value to 1 second.
>>> This behaves in a better "surf"-experience and results in a 1s-timeout
>>> (also only 1 IPv6-SYN) instead of the default 60s timeout.
>>>
>>> Why does squid not tries to connect first IPv6 (based on the host's
>>> address preference-policy) and then - in case of a failure - switch to
>>> IPv4 during a 300ms timeout (like current Browsers are doing)?
>>
>> Several reasons:
>>
>> 1) The default builds and installs do try IPv6 first in accordance with
>> RFC 6540. Check your config for a "dns_v4_first" directive which forces
>> IPv4 to be tried first.
> 
> According to RFC 6555:  "Over time, as most content is available via
> IPv6, the amount of IPv4 traffic will decrease.". With forcing this
> directive, I reduce the chance for connecting with IPv6 and my
> outbound connections are probably a long time with IPv4. This is maybe
> not the behaviour we want?

If you want to encourage IPv6 usage let Squid operate at its default
behaviour and fix the issues in the network which make any given request
go particularly worse than in IPv4. Sadly many of these are caused by
external sysadmins choices nowdays either to run with outdated machinery
or to explicitly break IPv6 in the name of disabling it.

As a proponent of IPv6 adoption myself I have written the IPv6
behaviours into Squid to prefer IPv6 over IPv4 whenever possible. Long
before RFC 6540 required it.


>>
>> 2) Squid is not the OS built-in resolver. Any obeying of that policy by
>> Squid is purely arbitrary. The host systems DNS resolver policy does not
>> supposed to affect standalone resolvers such as Squids internal one.
>> Particularly when there are squid.conf directives overriding the
>> resolv.conf behaviour (eg. dns_nameservers).
>>
>> dns_v4_first was a partial implementation added for
>> <http://bugs.squid-cache.org/show_bug.cgi?id=3086>.
>>
>>
>> 3) when performed by middleware such as Squid the "Happy Eyeballs"
>> algorithm is heavily destructive.
>>
>> A browser is consuming at minimum 2 network sockets to perform "Happy
>> Eyeballs".
>>
>> At the middlware each of those translates to potentially 3 sockets
>> (total 6 proxy sockets, 2 outgoing server sockets). If the middleware
>> were to perform "Happy Eyeballs" itself that would increase to 4 sockets
>> (total 8 proxy sockets, 4 outgoing server sockets).
> 
> But only in the parallel way (1 x IPv6 and 1 x IPv4)?

No, multiplexed. Each hop has both IPv4 and IPv6 outbound possibilities
for each individual packet regardless of the inbound type. "Happy
Eyeballs" worst-case is a straight exponential 2^N socket usage at the
server where N is proxy hop distance from client. Best-case occurs when
an admin chooses to disable IPv4 or IPv6 and cuts the exponential growth
from their hop in half.

So the algorithm happening in middleware would actively encourage bad
network practices by sysadmin. Grr :-(

> 
> Tests with the current curl (I know, curl != squid) behaves not in
> doing two similar (parallel) TCP-Connections (1x SYN for IPv6 and 1x
> SYN for IPv4). Instead, curl tries IPv6 first and in case of an
> connection-error, it tries after a few milliseconds with IPv4. This
> way, not a lot of sockets should be consumed. Does this not behave
> like a native IPv4-Stack? Squid would behave like curl, if I would be
> able to change the connect_timeout to milliseconds.

That sequential operation is already the current behaviour of Squid.
Just with resolution of seconds on the timeout and configurable choice
between ordering of {IPv6,IPv4} or {IPv4,IPv6} as to what gets tried first.

The browser "Happy Eyeballs" algorithm you were talking about /
proposing works quite differenty. All A and AAAA record have DNS queries
generated at once - as those replies happen all listed IPs have TCP SYN
packets generated at once and all of them are sent. It ends when one TCP
SYN packet gets a success response. End of spec.

That performance numbers that come out of it are great ... for a single
point-to-point connection.
It looks less wonderful and more like a DoS attack in all other cases,
especially when taking server resource side-effects into account.

> 
> Is there a well-known restriction (which I don't now, actually), by
> setting the connect_timeout to 1 second for all those IPv6-Adresses,
> which aren't connectable and for which the IPv4-Stack should be used
> after 1s timeout? Is this a practicable way?

There are two restrictions:

 1) the OS unix time (time_t) has a minimum resolution of 1 second.
Anything using more detailed time resolution is paying (in mutiple ms of
extra latency) for expensive syscalls to find out what the nanosecond
clocks in the system are.

 2) the event loop checking for what timeouts have occured cycles once
per second. The guarantee provided by Squid is that a timeout action
will have *at least* the requested N seconds before it triggers.

Also, to a lesser degree translating between timescales for this
directive with all the other timouts it interacts with would be a
performance drag and annoying to code.

Amos



From yvoinov at gmail.com  Thu Apr 30 17:42:20 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 30 Apr 2015 23:42:20 +0600
Subject: [squid-users] Squid Bugzilla is down
In-Reply-To: <CA+Y8hcP4FXVetdCF=RpxSV728bHTikXa1FnwScBkZLh0Dan7WQ@mail.gmail.com>
References: <5541FF98.5010109@gmail.com>
 <CA+Y8hcP4FXVetdCF=RpxSV728bHTikXa1FnwScBkZLh0Dan7WQ@mail.gmail.com>
Message-ID: <5542697C.4040209@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Now server produces 500 error.

30.04.15 23:39, Kinkie ?????:
> Hi,
>   sorry, we had a severe OOM on the main squid server.
> Now rebooted and hopefully better plugged. We will see about upgrading
> the server as soon as possible.
>
> On Thu, Apr 30, 2015 at 12:10 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>> Amos,
>>
>> what's up with bugzilla? It down and not available.
>>
>> WBR, Yuri
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVQml8AAoJENNXIZxhPexGfvEIAKHXVkgDYuOob2YgmFB0AP1h
h3jgjoNkGbxRkV+BCjAYpn/qSDHGHMI54T6d9r0If3oFrDLccWM3Bq+eGQK1smTj
ZbRcvt37QtjYcuRMXqU42m/mQDZ5UvEOireGwn9DR9TKsbHHn0EKynDdsFaLK3A/
8AbSoRIxMLH9vPbhBGd0O5gFsgBit68v/8nt3P+GMbHhS/WIamG0FvlAQDqEnIir
K2avn4C/PL4ZcKErKtCPMRYAl9KyO9HdAhXMKKAq3k0iKCknMd+NTKUtXBmDyH5Z
F+bhdddG81OioGJ1LwMX8xIM4CT6JHEyO+dMa1n5/eydiWg6Fi0qaUYvFZytnLQ=
=iwk9
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Thu Apr 30 09:34:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2015 21:34:07 +1200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
 connections?
In-Reply-To: <012A5BDD-E225-49B5-8352-B6CA28A194FD@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
 <13BF7DA2-AC5C-41BB-9E94-758BF2A06D59@rinis.nl>
 <5540C6D3.8040900@treenet.co.nz>
 <012A5BDD-E225-49B5-8352-B6CA28A194FD@rinis.nl>
Message-ID: <5541F70F.4010603@treenet.co.nz>

On 30/04/2015 2:34 a.m., Abdelouahed Haitoute wrote:
> Selinux is in permissive mode.
> 
> In cache.log I?m getting the following log:
> 2015/04/29 16:31:18 kid3| Starting Squid Cache version 3.3.8 for x86_64-redhat-linux-gnu...
> 2015/04/29 16:31:18 kid3| Process ID 19831
> 2015/04/29 16:31:18 kid3| Process Roles: coordinator
> 2015/04/29 16:31:18 kid3| With 16384 file descriptors available
> 2015/04/29 16:31:18 kid3| Initializing IP Cache...
> 2015/04/29 16:31:18 kid3| DNS Socket created at [::], FD 8
> 2015/04/29 16:31:18 kid3| DNS Socket created at 0.0.0.0, FD 9
> 2015/04/29 16:31:18 kid3| Adding domain rinis.nl from /etc/resolv.conf
> 2015/04/29 16:31:18 kid3| Adding nameserver 10.10.6.250 from /etc/resolv.conf
> 2015/04/29 16:31:18 kid1| Starting Squid Cache version 3.3.8 for x86_64-redhat-linux-gnu...
> 2015/04/29 16:31:18 kid1| Process ID 19833
> 2015/04/29 16:31:18 kid1| Process Roles: worker
> 2015/04/29 16:31:18 kid1| With 16384 file descriptors available
> 2015/04/29 16:31:18 kid1| Initializing IP Cache...
> 2015/04/29 16:31:18 kid1| DNS Socket created at [::], FD 8
> 2015/04/29 16:31:18 kid1| DNS Socket created at 0.0.0.0, FD 9
> 2015/04/29 16:31:18 kid1| Adding domain rinis.nl from /etc/resolv.conf
> 2015/04/29 16:31:18 kid1| Adding nameserver 10.10.6.250 from /etc/resolv.conf
> 2015/04/29 16:31:18 kid3| Logfile: opening log daemon:/var/log/squid/access.log
> 2015/04/29 16:31:18 kid3| Logfile Daemon: opening log /var/log/squid/access.log
> 2015/04/29 16:31:18 kid1| Logfile: opening log daemon:/var/log/squid/access.log
> 2015/04/29 16:31:18 kid1| Logfile Daemon: opening log /var/log/squid/access.log
> 2015/04/29 16:31:18 kid2| Starting Squid Cache version 3.3.8 for x86_64-redhat-linux-gnu...
> 2015/04/29 16:31:18 kid2| Process ID 19832
> 2015/04/29 16:31:18 kid2| Process Roles: worker
> 2015/04/29 16:31:18 kid2| With 16384 file descriptors available
> 2015/04/29 16:31:18 kid2| Initializing IP Cache...
> 2015/04/29 16:31:18 kid2| DNS Socket created at [::], FD 8
> 2015/04/29 16:31:18 kid2| DNS Socket created at 0.0.0.0, FD 9
> 2015/04/29 16:31:18 kid2| Adding domain rinis.nl from /etc/resolv.conf
> 2015/04/29 16:31:18 kid2| Adding nameserver 10.10.6.250 from /etc/resolv.conf
> 2015/04/29 16:31:18 kid2| Logfile: opening log daemon:/var/log/squid/access.log
> 2015/04/29 16:31:18 kid2| Logfile Daemon: opening log /var/log/squid/access.log
> 2015/04/29 16:31:18 kid3| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
> 2015/04/29 16:31:18 kid3| Store logging disabled
> 2015/04/29 16:31:18 kid3| Swap maxSize 0 + 2097152 KB, estimated 161319 objects
> 2015/04/29 16:31:18 kid3| Target number of buckets: 8065
> 2015/04/29 16:31:18 kid3| Using 8192 Store buckets
> 2015/04/29 16:31:18 kid3| Max Mem  size: 2097152 KB [shared]
> 2015/04/29 16:31:18 kid3| Max Swap size: 0 KB
> 2015/04/29 16:31:18 kid3| Using Least Load store dir selection
> 2015/04/29 16:31:18 kid3| Set Current Directory to /var/spool/squid
> 2015/04/29 16:31:18 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
> 2015/04/29 16:31:18 kid1| Store logging disabled
> 2015/04/29 16:31:18 kid1| WARNING: disk-cache maximum object size is unlimited but mem-cache maximum object size is 32.00 KB
> 2015/04/29 16:31:18 kid1| Swap maxSize 0 + 2097152 KB, estimated 161319 objects
> 2015/04/29 16:31:18 kid1| Target number of buckets: 8065
> 2015/04/29 16:31:18 kid1| Using 8192 Store buckets
> 2015/04/29 16:31:18 kid1| Max Mem  size: 2097152 KB [shared]
> 2015/04/29 16:31:18 kid1| Max Swap size: 0 KB
> 2015/04/29 16:31:18 kid1| Using Least Load store dir selection
> 2015/04/29 16:31:18 kid1| Set Current Directory to /var/spool/squid
> 2015/04/29 16:31:18 kid2| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
> 2015/04/29 16:31:18 kid2| Store logging disabled
> 2015/04/29 16:31:18 kid2| WARNING: disk-cache maximum object size is unlimited but mem-cache maximum object size is 32.00 KB
> 2015/04/29 16:31:18 kid2| Swap maxSize 0 + 2097152 KB, estimated 161319 objects
> 2015/04/29 16:31:18 kid2| Target number of buckets: 8065
> 2015/04/29 16:31:18 kid2| Using 8192 Store buckets
> 2015/04/29 16:31:18 kid2| Max Mem  size: 2097152 KB [shared]
> 2015/04/29 16:31:18 kid2| Max Swap size: 0 KB
> 2015/04/29 16:31:18 kid2| Using Least Load store dir selection
> 2015/04/29 16:31:18 kid2| Set Current Directory to /var/spool/squid
> 2015/04/29 16:31:18 kid3| Loaded Icons.
> 2015/04/29 16:31:18 kid3| Configuring Parent 192.168.0.18/3128/0
> 2015/04/29 16:31:18 kid3| Configuring Parent 192.168.0.20/3128/0
> 2015/04/29 16:31:18 kid3| Squid plugin modules loaded: 0
> 2015/04/29 16:31:18 kid3| Adaptation support is off.
> 2015/04/29 16:31:18 kid3| commBind: Cannot bind socket FD 11 to [::]: (2) No such file or directory

This is a bit odd, but it menas the UDS sockets betwen workers and
coordinator are not working.

Does /var/run/squid exist?

Does /dev/shm exist? if not that may be okay depending on OS, AFAIK for
*most* Linux its needed though and for some it needs mounting explicitly.


Amos


From hussam.tayeb at gmx.com  Thu Apr 30 00:47:00 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Thu, 30 Apr 2015 02:47:00 +0200
Subject: [squid-users] How do I no-cache the following url?
Message-ID: <trinity-1cf746ce-df58-46b9-bff3-59b7368ac70c-1430354820884@3capp-mailcom-bs04>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150430/0f13db60/attachment.htm>

From dan at getbusi.com  Wed Apr 29 23:48:49 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 30 Apr 2015 09:48:49 +1000
Subject: [squid-users] Individual delay pools and youtube
In-Reply-To: <5540B248.9000808@treenet.co.nz>
References: <1429494253383.2bf365a4@Nodemailer>
 <1430286253928.fcbd2ab7@Nodemailer>
 <5540B248.9000808@treenet.co.nz>
Message-ID: <CAN8nrKDBJ-GxUo4zyDm1SDm95FtEKbjKUQdGnFXT2Xvx66XOOA@mail.gmail.com>

Thanks Amos. We're using the CONNECT ACL and everything is working as
expected.

On 29 April 2015 at 20:28, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 29/04/2015 5:44 p.m., dan wrote:
> > I mentioned last time that we had to x2 all our delay_parameter?s
> > bytes because of a weird bug where squid would apply it at half speed
> > for no reason.
> >
> > It just occurred to me that (obviously) this is why HTTPS downloads
> > are going too fast; because this bug must only affect HTTP traffic.
> >
> > So HTTPS downloads are going at the actual speed we?ve specified and
> > HTTP is going at half that.
> >
> > Therefore, we should be able to work around it by setting different
> > delay_parameters for HTTP and HTTPS requests.
> >
> > So my question is, how best to target only those requests? By the
> > CONNECT method?
>
> Yes, "CONNECT" ACL matching the method should work.
> Or alternatively:
>  acl HTTP proto HTTP
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150430/c391a629/attachment.htm>

From hussam.tayeb at gmx.com  Thu Apr 30 00:58:18 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Thu, 30 Apr 2015 02:58:18 +0200
Subject: [squid-users] how do I no-cache the following url pattern?
Message-ID: <trinity-c98fadd5-e32c-4728-8f9b-32b8fc383322-1430355498762@3capp-mailcom-bs04>


What rule would I have to add to not cache the following url?
http://images.example.com\imageview.gif?anything
Everything up to the "?" is an exact match.
So I want to not cache
http://images.example.com\imageview.gif?
http://images.example.com\imageview.gif?anything
http://images.example.com\imageview.gif?anything.gif
etc...
Thank you.


From jagannath.naidu at fosteringlinux.com  Thu Apr 30 05:49:50 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Thu, 30 Apr 2015 11:19:50 +0530
Subject: [squid-users] NTLM AUTH: All redirector processes are busy
In-Reply-To: <CA+8bHvza9tLGaHk=hCcY8+NzOcKFBDrZJzzpzT__7LTiGkpPgg@mail.gmail.com>
References: <CA+8bHvza9tLGaHk=hCcY8+NzOcKFBDrZJzzpzT__7LTiGkpPgg@mail.gmail.com>
Message-ID: <CA+8bHvzxBf2WnXNvYz=jRK5RhacPdj840n3ye_Ze6Xu1Lh23GQ@mail.gmail.com>

Is there even any solution for this. ?
Do any one have this working ?

On 29 April 2015 at 17:04, Jagannath Naidu <
jagannath.naidu at fosteringlinux.com> wrote:

> Hi List/Amos,
>
> I am facing an using squid in production.
>
> I get these messages in cache.log, and service stop for a period of time
> (like 14 seconds). During this period, users get panic as they get "proxy
> server resfusing connections". And automatically the service starts
> functioning again. But this happens very frequently whole day.
>
> 2015/04/29 10:34:10| WARNING: All redirector processes are busy.
> 2015/04/29 10:34:10| WARNING: 15 pending requests queued
> 2015/04/29 10:34:10| storeDirWriteCleanLogs: Starting...
> 2015/04/29 10:34:10| WARNING: Closing open FD 3327
> 2015/04/29 10:34:10|     65536 entries written so far.
> 2015/04/29 10:34:10|    131072 entries written so far.
> 2015/04/29 10:34:10|    196608 entries written so far.
> 2015/04/29 10:34:10|    262144 entries written so far.
> 2015/04/29 10:34:10|    327680 entries written so far.
> 2015/04/29 10:34:10|    393216 entries written so far.
> 2015/04/29 10:34:10|    458752 entries written so far.
> 2015/04/29 10:34:10|    524288 entries written so far.
> 2015/04/29 10:34:10|    589824 entries written so far.
> 2015/04/29 10:34:10|    655360 entries written so far.
> 2015/04/29 10:34:10|   Finished.  Wrote 716101 entries.
> 2015/04/29 10:34:10|   Took 0.22 seconds (3266168.90 entries/sec).
> FATAL: Too many queued redirector requests
> Squid Cache (Version 3.1.10): Terminated abnormally.
> CPU Usage: 4206.393 seconds = 3778.049 user + 428.344 sys
> Maximum Resident Size: 2599760 KB
> Page faults with physical i/o: 0
> Memory usage for squid via mallinfo():
>         total space in arena:  750272 KB
>         Ordinary blocks:       717419 KB   6620 blks
>         Small blocks:               0 KB      1 blks
>         Holding blocks:         23020 KB     11 blks
>         Free Small blocks:          0 KB
>         Free Ordinary blocks:   32852 KB
>         Total in use:          740439 KB 99%
>         Total free:             32852 KB 4%
> fgets() failed! dying..... errno=1 (Operation not permitted)
> 2015/04/29 10:34:19| Starting Squid Cache version 3.1.10 for
> x86_64-redhat-linux-gnu...
> 2015/04/29 10:34:19| Process ID 4326
> 2015/04/29 10:34:19| With 100000 file descriptors available
> 2015/04/29 10:34:19| Initializing IP Cache...
> 2015/04/29 10:34:19| DNS Socket created at [::], FD 8
> 2015/04/29 10:34:19| DNS Socket created at 0.0.0.0, FD 9
> 2015/04/29 10:34:19| Adding nameserver 172.16.3.34 from squid.conf
> 2015/04/29 10:34:19| Adding nameserver 10.1.2.91 from squid.conf
> 2015/04/29 10:34:19| helperOpenServers: Starting 5/5 'squidGuard' processes
> 2015/04/29 10:34:19| helperOpenServers: Starting 1500/1500 'ntlm_auth'
> processes
> 2015/04/29 10:34:24| helperOpenServers: Starting 150/150 'wbinfo_group.pl'
> processes
>
>
> ntlm helpers count is 1500 and external "wbinfo_group.pl" helpers are 150.
>
> squid.conf
> ###################################################
>
> max_filedesc 100000
> acl manager proto cache_object
> acl localhost src 172.16.50.61
> http_access allow manager localhost
> dns_nameservers 172.16.3.34 10.1.2.91
> acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
> 172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
> http_access allow allowips
> #acl haproxy src 172.16.50.61
> #follow_x_forwarded_for allow haproxy
> #follow_x_forwarded_for deny all
> #acl manager proto cache_object
> acl localnet src 172.16.0.0/16
> acl manager proto cache_object
> acl localhost src 127.0.0.1
> acl localnet src fc00::/7 # RFC 4193 local private network range
> acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged)
> machines
> acl office dstdomain "/etc/squid/officesites"
> http_access allow office
> log_ip_on_direct off
> #debug_options ALL,3
> #logformat squid %9d.%03d %6d %s %s/%03d %d %s %s %s %s%s/%s %s
> logformat squid %ts.%03tu %tl %3tr %3dt %3un %>a %Ss/%>Hs %<st %rm %ru
> %Sh/%<A %mt
> access_log /var/log/squid/access1.log squid
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
> children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
> #auth_param ntlm program /etc/squid/helper-mux.pl /usr/bin/ntlm_auth
> --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> auth_param ntlm children 1500
> #auth_param ntlm children 500
> auth_param ntlm keep_alive off
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> external_acl_type wbinfo_group_helper ttl=600 children=150 %LOGIN
> /usr/lib64/squid/wbinfo_group.pl -d
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> cl Safe_ports port 8080 #https
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> acl auth proxy_auth REQUIRED
>
>
> and rest of acls and http_access rules configured ...............
>
>
>
> It seems the helper programs are not closing automatically after serving
> and causes this issue. Could anyone help resolving this issue.
>
> [root at GGNPROXY01 squid]# rpm -qa | grep squid
> squid-3.1.10-19.el6_4.x86_64
>
> [root at GGNPROXY01 squid]# rpm -qa | grep winbind
> samba-winbind-clients-3.6.9-164.el6.x86_64
> samba-winbind-3.6.9-164.el6.x86_64
>
> [root at GGNPROXY01 squid]# lsb_release -a
> LSB Version:
> :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
> Distributor ID:    RedHatEnterpriseServer
> Description:    Red Hat Enterprise Linux Server release 6.5 (Santiago)
> Release:    6.5
> Codename:    Santiago
>
>
> --
> Thanks & Regards
>
> B Jagannath
> Keen & Able Computers Pvt. Ltd.
> +919871324006
>



-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150430/d85ea5ef/attachment.htm>

From yvoinov at gmail.com  Thu Apr 30 19:12:26 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 01 May 2015 01:12:26 +0600
Subject: [squid-users] how do I no-cache the following url pattern?
In-Reply-To: <trinity-c98fadd5-e32c-4728-8f9b-32b8fc383322-1430355498762@3capp-mailcom-bs04>
References: <trinity-c98fadd5-e32c-4728-8f9b-32b8fc383322-1430355498762@3capp-mailcom-bs04>
Message-ID: <55427E9A.7060801@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
acl no_cache urlpath_regex imageview\.gif\?

or

acl no_cache urlpath_regex imageview\.gif(\?|$)


30.04.15 6:58, Hussam Al-Tayeb ?????:
>
> What rule would I have to add to not cache the following url?
> http://images.example.com\imageview.gif?anything
> Everything up to the "?" is an exact match.
> So I want to not cache
> http://images.example.com\imageview.gif?
> http://images.example.com\imageview.gif?anything
> http://images.example.com\imageview.gif?anything.gif
> etc...
> Thank you.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVQn6aAAoJENNXIZxhPexG9oQIAJ/zeCm6DOrUnBf+eu0BarGu
BrZOLwRgphQh2RH2vbE2SnmyappPrCUIJGwEsWKnK1kikR27F3xNhWQFrhFLgN8B
GRfTvqp7RsyPM7X2N/4daXnVrsfWCRMhYtoBNl9P0Y1Vm+IUnQRXmzCCZl47CNlD
fgYqnCODKPG4nVRi7FrnxXgCaOk1/f8QOyvzxOluf27F/ZhHZ+Lh/OQxsEUtj3nG
4+dVpxORkFEiwXzoHBqA7QFdr0uk793MEKQiQig5NxYfGi1Jj49oISUsuPdKRyIU
h6QZm5fzdCFUZo+orR9JM7ngzFMwkDzslgX9sT8UED3IkWGLQELEOG+XoptWD4Y=
=E3wO
-----END PGP SIGNATURE-----



From yan at seiner.com  Thu Apr 30 19:13:51 2015
From: yan at seiner.com (Yan Seiner)
Date: Thu, 30 Apr 2015 12:13:51 -0700
Subject: [squid-users] Cache strategy advice
Message-ID: <55427EEF.8030608@seiner.com>

I am building a small embedded squid box.

It has 4GB of ram, dual core CPU, and a 32GB SSD.

Since I'm running a tiny embedded linux distro (openwrt) most of those 
resources are available; I'm only using about 1MB of RAM and about 300MB 
of the SSD.

My incoming internet service is 30 to 60 Mb/sec.

My goals:

Maximize throughput (I don't want squid to slow down the connection)
Minimize wear on the SSD

I am planning to set up two workers but beyond that I'm not really how 
to effectively use what I have.

Any thoughts and advice would be greatly appreciated.




From gkinkie at gmail.com  Thu Apr 30 20:30:00 2015
From: gkinkie at gmail.com (Kinkie)
Date: Thu, 30 Apr 2015 22:30:00 +0200
Subject: [squid-users] Squid Bugzilla is down
In-Reply-To: <5542697C.4040209@gmail.com>
References: <5541FF98.5010109@gmail.com>
 <CA+Y8hcP4FXVetdCF=RpxSV728bHTikXa1FnwScBkZLh0Dan7WQ@mail.gmail.com>
 <5542697C.4040209@gmail.com>
Message-ID: <CA+Y8hcNRBfctHx=j+a+oeurrcKFpmATp0uQiVzRNq9k5FDEKwg@mail.gmail.com>

Should be fine now.
Thanks for notifying of the issue.

On Thu, Apr 30, 2015 at 7:42 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Now server produces 500 error.
>
> 30.04.15 23:39, Kinkie ?????:
>> Hi,
>>   sorry, we had a severe OOM on the main squid server.
>> Now rebooted and hopefully better plugged. We will see about upgrading
>> the server as soon as possible.
>>
>> On Thu, Apr 30, 2015 at 12:10 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>> Amos,
>>>
>>> what's up with bugzilla? It down and not available.
>>>
>>> WBR, Yuri
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVQml8AAoJENNXIZxhPexGfvEIAKHXVkgDYuOob2YgmFB0AP1h
> h3jgjoNkGbxRkV+BCjAYpn/qSDHGHMI54T6d9r0If3oFrDLccWM3Bq+eGQK1smTj
> ZbRcvt37QtjYcuRMXqU42m/mQDZ5UvEOireGwn9DR9TKsbHHn0EKynDdsFaLK3A/
> 8AbSoRIxMLH9vPbhBGd0O5gFsgBit68v/8nt3P+GMbHhS/WIamG0FvlAQDqEnIir
> K2avn4C/PL4ZcKErKtCPMRYAl9KyO9HdAhXMKKAq3k0iKCknMd+NTKUtXBmDyH5Z
> F+bhdddG81OioGJ1LwMX8xIM4CT6JHEyO+dMa1n5/eydiWg6Fi0qaUYvFZytnLQ=
> =iwk9
> -----END PGP SIGNATURE-----
>



-- 
    Francesco


From yvoinov at gmail.com  Thu Apr 30 20:38:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 01 May 2015 02:38:16 +0600
Subject: [squid-users] Squid Bugzilla is down
In-Reply-To: <CA+Y8hcNRBfctHx=j+a+oeurrcKFpmATp0uQiVzRNq9k5FDEKwg@mail.gmail.com>
References: <5541FF98.5010109@gmail.com>
 <CA+Y8hcP4FXVetdCF=RpxSV728bHTikXa1FnwScBkZLh0Dan7WQ@mail.gmail.com>
 <5542697C.4040209@gmail.com>
 <CA+Y8hcNRBfctHx=j+a+oeurrcKFpmATp0uQiVzRNq9k5FDEKwg@mail.gmail.com>
Message-ID: <554292B8.2060402@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yes, it's ok now.

Thank you!

01.05.15 2:30, Kinkie ?????:
> Should be fine now.
> Thanks for notifying of the issue.
>
> On Thu, Apr 30, 2015 at 7:42 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
> Now server produces 500 error.
>
> 30.04.15 23:39, Kinkie ?????:
> >>> Hi,
> >>>   sorry, we had a severe OOM on the main squid server.
> >>> Now rebooted and hopefully better plugged. We will see about upgrading
> >>> the server as soon as possible.
> >>>
> >>> On Thu, Apr 30, 2015 at 12:10 PM, Yuri Voinov <yvoinov at gmail.com>
wrote:
> >>>> Amos,
> >>>>
> >>>> what's up with bugzilla? It down and not available.
> >>>>
> >>>> WBR, Yuri
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>>
> >>>
>
>>
>
>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVQpK4AAoJENNXIZxhPexGIvEH/1j06xYDqK7VjN8JuROdgCoF
sKpjBVnN+zD3WQTnkv+xCSvB0vaz1YXjEcw7i4FFGfHMrqsNqwpHHXSIHPh3YC1V
gpGUPiuNMca+hKtNXuyqRrxlWfGnPJ21VDnEnKpuwivhtBdPbZv/VvlKbsGOyiRq
8cc95k1n1zdkjyC9HNEbjcr6+Wt430SGmGUBE9Q16Xi09vIHSx0vKpUNEeLP5yVo
3tku9+vgJOQz575OXyjBXRkHKreHXl2o/FxvTdj56s8sJg11tP0pBo9h9oARrxpe
YiqeXVi33YiT4cDYGT5GJ8JiQjcRU7jlsWXwo8zAav8YApmoIZRuBQtW7L+T8n0=
=XJXe
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150501/5ea828ff/attachment.htm>

From yan at seiner.com  Thu Apr 30 23:56:38 2015
From: yan at seiner.com (Yan Seiner)
Date: Thu, 30 Apr 2015 16:56:38 -0700
Subject: [squid-users] ACL why does this not work?
Message-ID: <5542C136.50904@seiner.com>

I am trying to prevent squid from proxying to an authorized subnet.

I want to write a set of acl rules that say that if a request does not 
come from the authorized subnet then it should not be allowed to connect 
to the authorized web server.

acl auth_net src 192.168.4.0/24
acl auth dst 192.168.4.1
http_access deny !auth_net auth

AFAICT something like the above should work but it doesn't.  squid 
proxies requests from anywhere on the network to the authorized 
webserver, getting right around the firewall.

Any suggestions on how to make this work?

Thanks.


From kukuh.amukti at gmail.com  Fri Apr 24 02:27:13 2015
From: kukuh.amukti at gmail.com (kukuh amukti)
Date: Fri, 24 Apr 2015 09:27:13 +0700
Subject: [squid-users] squid-users Digest, Vol 8, Issue 52
In-Reply-To: <mailman.2139.1429782073.2892.squid-users@lists.squid-cache.org>
References: <mailman.2139.1429782073.2892.squid-users@lists.squid-cache.org>
Message-ID: <CAKHWrNGCK_+V4o1m0zyzFAcXxPqeJAAHZGD3QExL07P8f66+6Q@mail.gmail.com>

Dear Amos,

i get error :
-- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/udandom = 90
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-F6iL9e
 -- reload: Reloading Kerberos Context
 -- finalize_exec: SAM Account Name is: PROXYAGIT01-K$
 -- try_machine_keytab_princ: Trying to authenticate for PROXYAGIT01-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/
proxyagit01.ag-it.com from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for PROXYAGIT01-K$ with
password.
 -- create_default_machine_password: Default machine password for
PROXYAGIT01-K$ is proxyagit01-k
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets...
 -- finalize_exec: Authenticated using method 4

 -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
try_tls=YES
 -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
try_tls=NO





*SASL/GSSAPI authentication started Error: ldap_sasl_interactive_bind_s
failed (Local error) Error: ldap_connect failed --> Is your kerberos ticket
expired? You might try re-"kinit"ing.  -- ~KRB5Context: Destroying Kerberos
Context*

in auth.log :
" msktutil: GSSAPI Error: Unspecified GSS failure.  Minor
code may provide more information (Server not found in Kerberos database)"

help me

thanks,
kukuhga

On Thu, Apr 23, 2015 at 4:41 PM, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: ERR_ONLY_IF_CACHED_MISS and cache digests problem
>       (Victor Sudakov)
>    2. GSSAPI problem when try create keytab using msktutil
>       (kukuh amukti)
>    3. Re: [squid ] externalAclLookup: 'wbinfo_group_helper' queue
>       overload. (Jagannath Naidu)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Thu, 23 Apr 2015 14:35:24 +0600
> From: Victor Sudakov <sudakov at sibptus.tomsk.ru>
> To: squid-users at lists.squid-cache.org, Amos Jeffries
>         <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] ERR_ONLY_IF_CACHED_MISS and cache digests
>         problem
> Message-ID: <20150423083524.GA92752 at admin.sibptus.tomsk.ru>
> Content-Type: text/plain; charset=us-ascii
>
> Amos Jeffries wrote:
>
> [dd]
>
> >
> > I dont think anything is wrong wth either. Its more a collision in how
> > the features work vs the protocols.
> >
> > Cache Digests (CD) are exchanged periodically and updated approx hourly.
> > Also they are based on just the URL. So there is always a gap where they
> > may not be accurate for any highly volatile objects, and variant objects
> > (using Vary headers) will have a high false-positive rate.
> >
> > only-if-cached requires the *right now* state of the object to be fresh
> > and in cache. It takes account of both the URL and the entire HTTP
> > headers. So
> >
> > The ICP protocol used as a backup to confirm objects existence also
> > suffers the same URL basis problem as CD. They work fine for HTTP/1.0
> > but HTTP/1.1 features dont fare quite so well.
>
> Thank you Amos, now I understand the mechanics behind this. However,
> I'd prefer that users do not receive this frustrating error in a setup
> with has nothing inherently wrong about it (especially frustrating is
> the fact that they receive the error from the wrong proxy server, not the
> one they have configured in the browser settings).
>
> Do I understand correctly that the only way to avoid this error
> message is to switch to HTCP (and ditch both ICP and CD)?
>
> --
> Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
> sip:sudakov at sibptus.tomsk.ru
>
>
> ------------------------------
>
> Message: 2
> Date: Thu, 23 Apr 2015 16:40:44 +0700
> From: kukuh amukti <kukuh.amukti at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] GSSAPI problem when try create keytab using
>         msktutil
> Message-ID:
>         <
> CAKHWrNFg7vUzmDpDJSpQvMRgc4eTCFONYYUnijyNNZRO2U0zTw at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Dear All,
> i've building squid in W2K12 and there is no problem but when i try running
> in W2K3,
> i get problem when try create keytab with msktutil command to win server
> 2003.
> and when i run msktutil :
>
> msktutil -c -b "OU=WSUS - Server,OU=Astragraphia-ITS" -s
> HTTP/proxyagit01.ag-it.com -k /etc/squid3/PROXY.keytab --computer-name
> PROXYAGIT-01 --upn HTTP/proxyagit01.ag-it.com --server
> svr-resdmn22.ag-it.com --verbose
>
> and get some error
>
>  -- init_password: Wiping the computer password structure
>  -- generate_new_password: Generating a new, random password for the
> computer account
>  -- generate_new_password:  Characters read from /dev/udandom = 90
>  -- create_fake_krb5_conf: Created a fake krb5.conf file:
> /tmp/.msktkrb5.conf-F6iL9e
>  -- reload: Reloading Kerberos Context
>  -- finalize_exec: SAM Account Name is: PROXYAGIT01-K$
>  -- try_machine_keytab_princ: Trying to authenticate for PROXYAGIT01-K$
> from local keytab...
>  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
>  -- try_machine_keytab_princ: Authentication with keytab failed
>  -- try_machine_keytab_princ: Trying to authenticate for host/
> proxyagit01.ag-it.com from local keytab...
>  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
>  -- try_machine_keytab_princ: Authentication with keytab failed
>  -- try_machine_password: Trying to authenticate for PROXYAGIT01-K$ with
> password.
>  -- create_default_machine_password: Default machine password for
> PROXYAGIT01-K$ is proxyagit01-k
>  -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
> not found in Kerberos database)
>  -- try_machine_password: Authentication with password failed
>  -- try_user_creds: Checking if default ticket cache has tickets...
>  -- finalize_exec: Authenticated using method 4
>
>  -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
> try_tls=YES
>  -- ldap_connect: Connecting to LDAP server: svr-resdmn22.ag-it.com
> try_tls=NO
> SASL/GSSAPI authentication started
> Error: ldap_sasl_interactive_bind_s failed (Local error)
> Error: ldap_connect failed
> --> Is your kerberos ticket expired? You might try re-"kinit"ing.
>  -- ~KRB5Context: Destroying Kerberos Context
>
>
> in auth.log  say " msktutil: GSSAPI Error: Unspecified GSS failure.  Minor
> code may provide more information (Server not found in Kerberos database)"
>
> what should i do?
>
> thanks,
> kukuhga
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/95123d16/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Thu, 23 Apr 2015 15:11:09 +0530
> From: Jagannath Naidu <jagannath.naidu at fosteringlinux.com>
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] [squid ] externalAclLookup:
>         'wbinfo_group_helper' queue overload.
> Message-ID:
>         <CA+8bHvzhgS=-
> u5zx1a82uWk0jC62qS1HmaUoawn7eW1W43ZHfA at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi Amos,
>
> regrets, I am late.
>
> On 21 April 2015 at 09:15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> > On 20/04/2015 7:31 p.m., Jagannath Naidu wrote:
> > > Hi,
> > >
> > > I am having this issue very frequently. Please help on this.
> > >
> > > I get these errors randomly, mostly when usage is at very peak. (800
> > users)
> > >
> > >
> > > /var/log/squid/cache.log
> > >
> > > 2015/04/20 12:37:40| externalAclLookup: 'wbinfo_group_helper' queue
> > > overload (ch=0x7fc99e2ce518)
> >
> > What do you think "overload" means?
> >  The helper is unable to cope with the traffic load being passed to it.
> >
> > Here is the biggest hint:
> > >
> > > in /var/log/messages,  I get the following errors
> > >
> > > pr 20 12:59:15 GGNPROXY01 winbindd[1910]:   winbindd: Exceeding 200
> > client
> > > connections, no idle connection found
> >
> >
> >
> >
> > > Then squid stops working. For squid to start work again, I have to
> dlete
> > > the cache and restart the squid "squid -k reconfigure", and then squid
> > > restart.
> >
> > What Squid version are you using?
> >
> > my squid version  squid-3.1.10-19.el6_4.x86_64
>
>
>
> > >
> > > squid.conf
> > >
> > > max_filedesc 17192
> > > acl manager proto cache_object
> > > acl localhost src 172.16.50.61/24
> >
> > changed to "acl localhost src 172.16.50.6*1*" already
>
>
> > You have an entire /24 (256 IPs) assigned to this machine?
> >
> > I think you need to remove that "/24" part if the *.61 is the local
> > machines *public* IP.
> >
> >
> > > http_access allow manager localhost
> > > dns_nameservers 172.16.3.34 10.1.2.91
> > > acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
> > > 172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
> > > http_access allow allowips
> >
> > > auth_param basic realm Squid proxy-caching web server
> > > auth_param basic credentialsttl 2 hours external_acl_type nt_group
> ttl=0
> > > children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
> >
> > The above two very mangled config lines are useless. Remove them.
> >
> > > acl localnet src 172.16.0.0/24
> >
>
>
> changed
>
>
> > Its a bit strange that none of the localhost machine IPs
> > (172.16.50.0-172.16.50.255) are part of the LAN its plugged into
> > 172.16.0.0-172.16.0.255.
> >
> >
> > > acl localnet src fc00::/7 # RFC 4193 local private network range
> > > acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged)
> > machines
> > > auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> > > --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> >
> > Okay you have configured NTLM...
> >
> > > auth_param ntlm program /usr/bin/ntlm_auth
> > > --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> >
> > ... but twice. With different settings. Only these last ones will have
> > any effect.
> >
> >
> > > auth_param ntlm children 600
> > > auth_param ntlm keep_alive off
> >
> > > auth_param negotiate children 150
> > > auth_param negotiate keep_alive off
> > > visible_hostname GGNPROXY01.HTMEDIA.NET
> > > external_acl_type wbinfo_group_helper ttl=0 children=40 %LOGIN
> > > /usr/lib64/squid/wbinfo_group.pl -d
> > > auth_param negotiate keep_alive off
> >
> > You have several useless configuration lines for Negotiate auth which is
> > not being used in any way. Remove those.
> >
> >
> > > acl Safe_ports port 8080 #https
> > > acl SSL_ports port 443
> > > acl Safe_ports port 80          # http
> > > acl Safe_ports port 21          # ftp
> > > acl Safe_ports port 443 # https
> > > acl Safe_ports port 70          # gopher
> > > acl Safe_ports port 210         # wais
> > > acl Safe_ports port 1025-65535  # unregistered ports
> > > acl Safe_ports port 280         # http-mgmt
> > > acl Safe_ports port 488         # gss-http
> > > acl Safe_ports port 591         # filemaker
> > > acl Safe_ports port 777         # multiling http
> > > acl CONNECT method CONNECT
> > > acl auth proxy_auth REQUIRED
> > > acl google dstdomain -i "/etc/squid/google_site.com"
> > > http_access allow google
> > > acl sq1 external wbinfo_group_helper "/etc/squid/HT/sq1"
> > > acl sq2 external wbinfo_group_helper "/etc/squid/HT/sq2"
> > > acl sq3 external wbinfo_group_helper "/etc/squid/HT/sq3"
> > > acl sq4 external wbinfo_group_helper "/etc/squid/HT/sq4"
> > > acl sq5 external wbinfo_group_helper "/etc/squid/HT/sq5"
> > > acl pro1 external wbinfo_group_helper "/etc/squid/HT/pro1"
> > > acl pro2 external wbinfo_group_helper "/etc/squid/HT/pro2"
> > > acl pro3 external wbinfo_group_helper "/etc/squid/HT/pro3"
> > > acl pro4 external wbinfo_group_helper "/etc/squid/HT/pro4"
> > > acl pro5 external wbinfo_group_helper "/etc/squid/HT/pro5"
> > > acl pro6 external wbinfo_group_helper "/etc/squid/HT/pro6"
> > > acl webvip external wbinfo_group_helper "/etc/squid/HT/webvip"
> > > acl allgroup external wbinfo_group_helper "/etc/squid/HT/allgreop"
> > > acl restricted external wbinfo_group_helper "/etc/squid/HT/restricted"
> > > acl ad_auth proxy_auth REQUIRE
> >
> > You already have an ACL named "auth" which performs authentication.
> > The above line is not useful. Remove it and replace all uses of
> > "ad_auth" ACL with "auth" ACL.
> >
> > > acl allowwebsites dstdomain -i "/blacklists/allowedwebsite/domains"
> > > acl allowwebsites_url url_regex -i "/blacklists/allowedwebsite/url"
> > > http_access allow allowwebsites
> > > http_access allow allowwebsites_url
> > > acl shopping dstdomain -i "/etc/squid/shopping.txt"
> > > acl social_networking dstdomain -i
> "/blacklists/social/social.networking"
> > > acl youtube dstdomain -i .youtube.com
> > > http_access allow Safe_ports pro1 pro2 pro3 pro4 pro5 pro6 webvip
> >
> > Incorrect use of "Safe_ports" security check. Correct usage is to deny
> > access to all *unsafe* ports. They are unsafe because HTTP can be
> > smuggled within the ports native protocol to attack your proxy.
> >
> > Once the correct security protections for Safe_port and CONNECT tunnels
> > have been moved up the top remove the "Safe_ports" check from this line.
> >
> > This line is also very odd in another way. ACL tests in a single line
> > are AND'ed together - so this means the request must be from a user who
> is:
> >   authenticated AND a member of group pro1 AND pro2 AND pro3 AND pro4
> > AND pro5 AND pro6 AND webvip
> >
> > This hints at what your main helper problem is. The above line requires
> > 7 group helper lookups *per request*. The winbind helper has a maximum
> > of 200 simultaneous connections. This line alone will limit your proxy
> > just under 30 new visitors per second (that becomes 60 lookups/sec
> > before queue overload).
> >  The helper result caching will help a lot, but you also have a LOT of
> > other group checks being made and 800 users.
> >
> >
> > > http_access allow youtube pro5
> > > http_access allow youtube pro6
> > > http_access allow youtube webvip
> > > http_access deny youtube
> > > http_access allow shopping pro5
> > > http_access allow shopping pro6
> > > http_access allow shopping webvip
> > > http_access deny shopping
> >
> > Optimization hint:
> >  "youtube" and "shopping" have the same allow/deny criteria. It would be
> > worth combining them into one ACL.
> >
> > > http_access allow social_networking pro2
> > > http_access allow social_networking pro4
> > > http_access allow social_networking pro6
> > > http_access allow social_networking webvip
> > > http_access deny social_networking
> > > acl porn_site1   dstdomain "/etc/squid/blacklists/porn/domains.txt"
> > > acl porn_site2   dstdom_regex -i
> "/etc/squid/blacklists/porn/expressions"
> > > acl porn_site3   dstdom_regex -i "/etc/squid/blacklists/porn/urls.txt"
> > > acl audio_video1   dstdomain
> "/etc/squid/blacklists/audio-video/urls.txt"
> > > ###################### THERE ARE TOO MANY acls and http_access , so not
> > > bothering with vast linux
> >
> > I will bet a lot of those ACLs are also calling the group helper too yes?
> >
> > > http_access allow liquorinfo webvip
> > > http_access deny liquorinfo
> > > http_access allow ad_auth
> > > http_access allow auth
> >
> > Once you have removed ad_auth ACL, this becomes:
> >  http_access allow auth
> >  http_access allow auth
> >
> > I hope you can see how redundant that is.
> >
> > Also, its very likely that the "allow auth" is a useless operation after
> > a great many group checks have also performed authentication. That "TOO
> > MANY acls and https_access" list you omitted will be needed to determine
> > that.
> >
> >
> > > http_access allow sq1 sq2
> > > acl NTLMUsers proxy_auth REQUIRED
> >
> > You already have an ACL named "auth" which performs authentication.
> > The above line is not being used in any way. Remove it.
> >
> > > http_access deny !Safe_ports
> > > http_access deny CONNECT !SSL_ports
> >
> > These are basic security protection against Denial of Service and other
> > types of protocol smuggling attacks. They only work when they are used
> > *above* your custom "allow" rules.
> >
> > Move these two lines above your "http_access allow google" line.
> >
> >
> >
> > > http_port 8080
> > > hierarchy_stoplist cgi-bin ?
> >
> > The above line is not useful these days. Remove it.
> >
> > > cache_effective_user squid
> > > cache_dir aufs /var/spool/squid 20384 32 512
> > > cache_mem 50 MB
> > > cache_replacement_policy heap LFUDA
> > > cache_swap_low 85
> > > cache_swap_high 95
> > > maximum_object_size 5 MB
> > > maximum_object_size_in_memory 50 KB
> > > ipcache_size 5240
> > > ipcache_low 90
> > > ipcache_high 95
> > > cache_mgr amit
> > > acl SSL_ports port 443
> >
> > The above is a duplicate config line. Remove it.
> >
> > > http_access allow CONNECT SSL_ports
> > > coredump_dir /var/spool/squid
> > > refresh_pattern ^ftp:           1440    20%     10080
> > > refresh_pattern ^gopher:        1440    0%      1440
> > > refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> > > refresh_pattern .               0       20%     4320
> > > url_rewrite_program /usr/local/bin/squidGuard -c
> > > /usr/local/squidGuard/squidGuard.conf
> > >
> >
> >
> > Now, as to solving your problem:
> >
> > 1) Clean up your config. Reduce the amount of redundant or unused
> > things. I've mentioned a few above.
> >
> > 2) Run "squid -k parse" and fix any other problems it highlights.
> >
> > 3) optimize your ACls and http_access rules. I've mentioned a few, such
> > as moving the main security checks to the top so DoS traffic does not
> > put load on the helpers and other ACLs.
> >
> > I believe though that you will probably find Squid works much better
> > having the following access controls pattern:
> > "
> >  http_access deny !Safe_ports
> >  http_access deny CONNECT !SSL_ports
> >
> >  # if they are not authenticated, they will not be in a group
> >  http_access deny !auth
> >
> >  # assuming that webvip are the group with full access?
> >  http_access allow webvip
> >
> >  # your long list of per-site group check ACLs go here
> >  ...
> >
> >  # this is where defining the LAN ranges correctly comes in.
> >  # note that users have authenticated simply to get near here
> >  http_access allow localnet
> >  http_access deny all
> > "
> >
> >
> > 4) consider an upgrade to Squid 3.4+. The "notes" ACL type offers much
> > more efficient ACL testing with a custom group lookup helper. The all-of
> > and any-of ACL types can also much reduce your http_access lines.
> >
> > HTH
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
>
> Thank you Amos, I will check and will update the list.
>
>
> --
> Thanks & Regards
>
> B Jagannath
> Keen & Able Computers Pvt. Ltd.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20150423/4e7744c9/attachment.html
> >
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 8, Issue 52
> ******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150424/38650b84/attachment.htm>

