From squid3 at treenet.co.nz  Sat Aug  1 02:40:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Aug 2015 14:40:19 +1200
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <55B9EC8F.8010009@treenet.co.nz>
References: <1299596190.18572939.1438243698255.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55B9EC8F.8010009@treenet.co.nz>
Message-ID: <55BC3193.2000501@treenet.co.nz>

On 30/07/2015 9:21 p.m., Amos Jeffries wrote:
> On 30/07/2015 8:08 p.m., FredB wrote:
>>
>>>
>>> Well. Yes an 3.4 has a serious CVE that needs releasing. So it will
>>> be a
>>> thing this weekend.
>>>
>>> But no other bug fixes in the past few months qualify as security
>>> issues. So yes you need to be moving on to 3.5. Especially if you are
>>> using the ssl-bump features.
>>>
>>> Amos
>>>
>>
>>
>> So, no chance for http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13867.patch
>> I known, I should migrate  
>>
> 
> Im not sure that even qualifies as a bug really. Just polishing the
> debug output to avoid future confusion like we went through.
> 

Ah, well. Since it is essentially just a documentation fix. I've added
it to 3.4.14. But only because it does not actually change any of the
queue behaviour.

Amos



From squid3 at treenet.co.nz  Sat Aug  1 03:57:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Aug 2015 15:57:09 +1200
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <BAY181-W244B935D970106C8E5264F83830@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
 <55AD47A2.90003@measurement-factory.com>
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>
 <1437508769271-4672366.post@n4.nabble.com>
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>
 <1437523083670-4672368.post@n4.nabble.com>
 <1437524863407-4672369.post@n4.nabble.com>
 <BAY181-W4089B09CA79A17E696C43183830@phx.gbl>
 <55AF8ACB.5040800@treenet.co.nz>
 <BAY181-W244B935D970106C8E5264F83830@phx.gbl>
Message-ID: <55BC4395.5090709@treenet.co.nz>

On 23/07/2015 2:41 a.m., Alex Wu wrote:
> We do not use cache-peer. I thought cache-peer is for connecting another squid-like proxy server.

Historically yes. In Squid-3 it is for connecting to any specific
upstream server.


The correct way to send traffic over TLS/SSL to an intranet server is
like this:

 cache_peer internal.example.com parent 443 0 name=internal \
    originserver ssl forcedomain=www.internal.example.net
 acl example dstdomain .example.com
 cache_peer_access internal allow example

That performs the same outgoing HTTP request as would be generated by
URL-rewriting www.example.com to https://www.internal.example.net. But
far faster and far less processing overheads.


[Sorry for not replying with this earlier. I just re-found the thread
while looking up your name for the bug 4293 commit.]

Amos



From marcus.kool at urlfilterdb.com  Sat Aug  1 11:45:39 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 01 Aug 2015 08:45:39 -0300
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <55BBC4C3.3090801@treenet.co.nz>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
 <55BAA843.5090001@treenet.co.nz>
 <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>
 <55BB6FBB.1060002@urlfilterdb.com>
 <CAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g@mail.gmail.com>
 <55BBC4C3.3090801@treenet.co.nz>
Message-ID: <55BCB163.2050603@urlfilterdb.com>



On 07/31/2015 03:56 PM, Amos Jeffries wrote:
> On 1/08/2015 4:06 a.m., Josip Makarevic wrote:
>> Marcus, tnx for your info.
>> OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
>> Yes, cpu_affinity_map is good and with 6 instances there is load only on
>> first 6 cores and the server is 12 core, 24 HT
>
> Then I suspect that mutex and locking will be the kernel scheduling work
> on the HT cores.
>   In high performance Squid will max out a physical cores worth of
> cycles. HT essentially tries to over-clock physical cores. But trying to
> reach 200% capacity into a physical core with Squid workloads only leads
> to trouble.
>   It is far better to tie Squid with affinity to one instance per
> physical core and let the extra HT capacity be available to the OS and
> other supporting things the Squid instance needs to have happen externally.
>
>
>> each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
>> and so on so that should not be the problem.
>> I've tried with 12 workers but that's even worse.
>
> You do need to be very careful about which core numbers are the HT core
> vs the physical core ID. Last time I saw anyone doing it, every second
> number was a real physical core ID. YMMV.

There are 2 mappings and I have seen them both but I do not recall which I saw where.
You can do the following to find out which CPU# is a sibling (HT core):
cd /sys/devices/system/cpu
for cpu in cpu[0-9]* ; do
    cat $cpu/topology/thread_siblings_list
done

>> Let me try to explain:
>> on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
>> in that case, actual user time is about 10-20% and 70-80% is sys time
>> (osq_lock) and there are no connection timeouts.

The CPU time in osq_lock is not easy to explain but it is not likely caused by Squid itself.
Googling about osq_lock led me to a kernel patch discussion where 500 dd processes on ext4/multipath or a file system repair with 125 threads caused the system to use 70+% CPU in osq_lock.
The general believe was that a lot of outstanding IO caused it.
This brings me to these questions:
- what is your testing method ?
- are there simply too many concurrent connections per instance of Squid ?
- are the bonded 10G interfaces supported by CentOS 6 ?
- can you test with unbonded ethernet? (the bonding driver code uses 2 locks)

You may or may not get better results with CentOS 7 or the custom kernel (try latest or before 3.12 since some issues started with 3.12).

>> If I switch to SMP 6 workers user time goes up but sys time goes up too and
>> there are connection timeouts and the load jumps to ~12.
>> If I give it more workers only load jumps and more connections are being
>> dropped to the point that load goes to 23/24 and the entire server is slow
>> as hell.
>>
>> So, best performance so far are with 6 non-smp workers.

'workers' is a term used by Squid SMP.
To have less confusion, in a non-SMP Squid config, I suggest to use the term 'instance'.

Marcus

>> For now I have 2 options:
>> 1. Install older squid (3.1.10 centos repo) and try it then
>> 2. build custom 64bit kernel with RCU and specific cpu family support (in
>> progress).
>>
>> The end idea is to be able to sustain 1gig of traffic on this server :)
>> Any advice is welcome
>
> I agree with Marcus then. The non-SMP then is the way to go at present.
> The main benefit of SMP support in current Squid is for caching
> de-duplication (ie rock store).
>
>
> Also some things to note:
>
> * a good percentage of the speed of Squid is the 20-40% caching HIT rate
> normal HTTP traffic has. Albeit memory-only caching on highest
> performance boxen. Memory hits are 4-6 orders of magnitude faster than
> network fetches. This has little to do with anything you can control
> (normally). The (relatively) slow speed of origin servers creating the
> content is the bottleneck. Even "static" content may be encoded to the
> clients requested desire on each fetch, which takes time.
>
>
> * Going by out lab tests and real-world results so far I rate Squid
> per-worker at ~50Mbps on 3.1GHz core, and ~70Mbps on 3.7GHz. Your 12
> cores will only get you up around 800 Mbits IMHO (thats after tuning). I
> would gladly be proven wrong though :-)
>
>
> * Squid effectively *polls* all the listening ports every 10ms or once
> every 10 I/O events (whichever is faster). So running with 1024
> listening ports is a bit counter-productive, more time could be spent
> checking those ports than doing work.
>   That said going from one to multiple listening ports does make a speed
> improvement. Finding the sweet spot between those trends is something
> else to tune for.
>   <http://wiki.squid-cache.org/MultipleInstances#Tips>
>
>
>> 2015-07-31 14:53 GMT+02:00 Marcus Kool:
>>
>>> osq_lock is used in the kenel for the implementation of a mutex.
>>> It is not clear which mutex so we can only guess.
>>>
>>> Which version of the kernel and distro do you use?
>>>
>>> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
>>> non-SMP.
>>>
>>> What is the value of cpu_affinity_map in all config files?
>>> You say they are static. But do you allocate each instance on a different
>>> core?
>>> Does 'top' show that all CPUs are used?
>>>
>>> Do you have 24 cores or 12 hyperthreaded cores?
>>> In case you have 12 real cores, you might want to experiment with 12
>>> instances of Squid and then try to upscale.
>>>
>>> Make maximum_object_size large, a max size of 16K will prohibit the
>>> retrieval of objects larger than 16K.
>>> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
>>> infinite and do not worry since
>>> cache_mem is zero.
>>>
>>> Marcus
>>>
>>>
>>>
>>> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
>>>
>>>> Hi Amos,
>>>>
>>>>    cache_mem 0
>>>>    cache deny all
>>>>
>>>> already there.
>>>> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
>>>> interface.
>>>>
>>>> Well, entire config would be way too long but here is the static part:
>>>> via off
>>>> cpu_affinity_map process_numbers=1 cores=2
>>>> forwarded_for delete
>>>> visible_hostname squid1
>>>> pid_filename /var/run/squid1.pid
>
> Remove these...
>
>>>> icp_port 0
>>>> htcp_port 0
>>>> icp_access deny all
>>>> htcp_access deny all
>>>> snmp_port 0
>>>> snmp_access deny all
>
> ... to here. They do nothing but slow Squid-3 down.
>
>>>> dns_nameservers x.x.x.x
>>>> cache_mem 0
>>>> cache deny all
>>>> pipeline_prefetch on
>
> In Squid-3.4 and later this is set to the length of pipeline you want to
> accept.
>
> NP: 'on' traditionally has meant pipeline length of 1 (two parallel
> requests). Longer lengths are not yet well tested but generally it seems
> to work okay.
>
>
>>>> memory_pools off
>>>> maximum_object_size 16 KB
>>>> maximum_object_size_in_memory 16 KB
>
> Like Marcus said. Without even memory caching these two have no useful
> effects.
>
> There is one related setting "read_ahead_gap" which affects performance
> by tuning the amount of undelivered object data Squid will buffer in
> transient memory. Higher value for that mean faster servers can finish
> sending earlier and resources for them released for other uses.
>   Tuning this is a fine art since it modulates how much Squid internal
> buffers (and pipieline prefetching) read off TCP buffers. And all of
> those buffers have limits of their own and may contain multiple requests
> data.
>
>
>>>> ipcache_size 0
>
> Remove this. Without IP cache Squid will be forced to do about 4x remote
> DNS lookup for every single HTTP request - *minimum*. Maybe more if you
> apply any access controls to the traffic.
>   If anything increase the ipcache size to store more results.
>
>
>>>> cache_store_log none
>
> Not needed in Squid-3. You can remove.
>
>>>> half_closed_clients off
>>>> include /etc/squid/rules
>>>> access_log /var/log/squid/squid1-access.log
>
> Logging I/O slows Squid down. I suggest making that a daemon, TCP or UDP
> log output.
>
>
>>>> cache_log /var/log/squid/squid1-cache.log
>>>> coredump_dir /var/spool/squid/squid1
>>>> refresh_pattern ^ftp:           1440    20%     10080
>>>> refresh_pattern ^gopher:        1440    0%      1440
>>>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>>> refresh_pattern .               0       20%     4320
>
> Without caching you can remove these *entirely*.
>
>>>>
>>>> acl port0 myport 30000
>
> Mumble. Less reliable than myportname, but it is infintessimally faster
> when it does work at all.
>
>>>> http_access allow testhost
>>>> tcp_outgoing_address x.x.x.x port0
>>>>
>>>> include is there for basic ACL - safe ports and so on - to minimize
>>>> config file footprint since it's static and same for every worker.
>>>>
>>>> and so on 44 more times in this config file
>
> Only put allow testhost once. Every time you test ACLs Squid slows down.
>
> Some ACLs are worse drag than others. You can probably optimize even the
> default recommended security settings you shuffled into "rules" file to
> operate better.
>
>
>>>>
>>>> Do you know of any good article hot to tune kernel locking or have any
>>>> idea why is it happening?
>>>> I cannot find any good info on it and all I've found are bits and peaces
>>>> of kernel source code.
>
> Sorry no. All I found was the same.
>
> Though I do know that one of the big differences between Linux 2.6 and
> 3.0 was the removal of the "Big Kernel Lock" system that allowed Linux
> to run on multi-core systems properly. It could be CentOS 6 itelf biting
> you with its ancient kernel version.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From stan.prescott at gmail.com  Sat Aug  1 16:16:27 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sat, 1 Aug 2015 11:16:27 -0500
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <CAFLo2QymyReQhzgRh5j3vQADqJaLuShnTZ8sxN1_-2Lmngjmvw@mail.gmail.com>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <55A8837A.1080901@treenet.co.nz>
 <CANLNtGQwEyrXuyEbd+MjJqSLt3EdBqv8F1sbVsi2py36COe9YA@mail.gmail.com>
 <55BBE64E.5070302@treenet.co.nz>
 <CAFLo2QymyReQhzgRh5j3vQADqJaLuShnTZ8sxN1_-2Lmngjmvw@mail.gmail.com>
Message-ID: <CANLNtGRVhbNx9zwQ-F_hkX4s5Et+YaEMGStrvf_UtQ6hQNarxQ@mail.gmail.com>

I added "ssl" to the options


*cache_peer forcesafesearch.google.com <http://forcesafesearch.google.com>
parent 443 0 ssl name=GS originserver no-query no-netdb-exchange no-digest*

but now when I try to access *https://www.google.com
<https://www.google.com> *at all, I get an "SSL certificate error" page.
All other https sites load fine.

@Luis, where can I find information on the icap project? We already use
c-icap for SquidClamAV. It might not be much more of a stretch to add this
to it.

On Fri, Jul 31, 2015 at 6:34 PM, Luis Daniel Lucio Quiroz <
luis.daniel.lucio at gmail.com> wrote:

> There is a project for icap that does exactly what you want. This is like
> a L8 filter, meanwhile dns is L5.
>
> The higher, the better
> On Jul 31, 2015 5:20 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:
>
>> On 1/08/2015 8:49 a.m., Stanford Prescott wrote:
>> > Hi Amos. I wanted to try out the "ssl-bump splice" to send traffic to a
>> > peer found in the recent snapshots for 3.5.6/7 to block Google images. I
>> > compiled configured and installed the latest 3.5 snapshot and added the
>> > directives you listed above to squid.conf but I am not sure I got them
>> > right.
>> >
>> >
>> > acl s1_tls_connect      at_step SslBump1
>> > acl s2_tls_client_hello at_step SslBump2
>> > acl s3_tls_server_hello at_step SslBump3
>> > acl tls_server_name_is_ip ssl::server_name_regex
>> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
>> > acl google ssl::server_name .google.com
>> >
>> > ssl_bump peek s1_tls_connect      all
>> > acl nobumpSites ssl::server_name .wellsfargo.com
>> > ssl_bump splice s2_tls_client_hello nobumpSites
>> > ssl_bump splice s2_tls_client_hello google
>> > ssl_bump stare s2_tls_client_hello all
>> > ssl_bump bump  s3_tls_server_hello all
>> >
>> > cache_peer forcesafesearch.google.com parent 443 0 \
>> > name=GS originserver no-query no-netdb-exchange no-digest
>>
>> Sorry, I missed out the 'ssl' option on the peer.
>>
>> > acl search dstdomain .google.com
>> > cache_peer_access GS allow search
>> >
>> cache_peer_access GS deny all
>> > sslproxy_cert_error allow tls_server_name_is_ip
>> > sslproxy_cert_error deny all
>> > sslproxy_flags DONT_VERIFY_PEER
>> >
>> > When restarting Squid and searching in Google images for "sex" it still
>> > shows images that I want to be able to block with safesearch.
>>
>> Other than the it I missed out mentioning. it looks okay to me. Though I
>> have not tested any of this myself so YMMV.
>>
>> Amos
>>
>> >
>> > On Thu, Jul 16, 2015 at 11:24 PM, Amos Jeffries wrote:
>> >
>> >> On 19/05/2015 5:49 a.m., Andres Granados wrote:
>> >>> hello!I need help on how to block pornographic images of google, I
>> >>> was trying different options and still do not succeed, try:
>> >>> http_reply_access with request_header_add, and even with a
>> >>> configuration dns, I think is to request_header_add the best, though
>> >>> not it has worked for me, I hope your help, is to implement a school,
>> >>> thanks!
>> >>>
>> >>
>> >> FYI; Christos has added a tweak to the "ssl-bump splice" handling that
>> >> permits sending the traffic to a cache_peer configured something like
>> this:
>> >>
>> >>  acl example ssl::server_name .example.com
>> >>  ssl_bump splice example
>> >>  ssl_bump peek all
>> >>
>> >>  cache_peer forcesafesearch.example.com parent 443 0 \
>> >>     name=GS \
>> >>     originserver no-query no-netdb-exchange no-digest
>> >>
>> >>  acl search dstdomain .example.com
>> >>  cache_peer_access GS allow search
>> >>  cache_peer_access GS deny all
>> >>
>> >> The idea being that you can use this on intercepted (or forward-proxy)
>> >> HTTPS traffic instead of hacking about with DNS to direct clients at
>> the
>> >> servers Google use to present "safe" searching.
>> >>
>> >> This should be available in 3.5.7, or the current 3.5 snaphots.
>> >>
>> >> Cheers
>> >> Amos
>> >> _______________________________________________
>> >> squid-users mailing list
>> >> squid-users at lists.squid-cache.org
>> >> http://lists.squid-cache.org/listinfo/squid-users
>> >>
>> >
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150801/2fa4ae11/attachment.htm>

From marciobacci at gmail.com  Sat Aug  1 23:57:44 2015
From: marciobacci at gmail.com (Marcio Demetrio Bacci)
Date: Sat, 1 Aug 2015 20:57:44 -0300
Subject: [squid-users] Problems with Squid 3 Authentication on Samba 4
Message-ID: <CA+0TdypA3ngF=vKPMKh3XSJCYaYfobo3WWZ9cJqaLXfPBczY7Q@mail.gmail.com>

Hi,

I'm trying to authenticate the squid3 in Samba4. The part of Samba 4
authentication is OK, because with the commands wbinfo -i, getent passwd,
klist, kinit is all right and I can  get  the expected results. Also on the
command line can authenticate users (/usr /bin/ntlm_auth --username =
DomainUser), however, when using the squid does not.

I have already followed several tutorials, including to authenticate to AD,
as the Samba 4  is fully compatible with AD. Nothing works.

Follow my configuration file (squid.conf):


http_port 3128
visible_hostname proxy-server.meudominio.com.br
error_directory /usr/share/squid3/errors/Portuguese
error_default_language pt-br
coredump_dir /var/spool/squid3
cache_mem 756 MB
maximum_object_size_in_memory 128 kB
maximum_object_size 512 MB
minimum_object_size 0 KB
cache_swap_low 90
cache_swap_high 95
cache_dir ufs /var/spool/squid3 2048 16 256
cache_access_log /var/log/squid3/access.log
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern . 0 20% 4320
###########################################################################
######### user authentication #########
###########################################################################
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 30
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param ntlm children 5
auth_param basic realm Squid-proxy-caching web server
auth_param basic credentialsttl 2 hours
acl autenticados proxy_auth REQUIRED
# ACLS #
#acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl SSL_ports port 443 563
acl Safe_ports port 80 21 443 70 563 70 210 280 488 59 591 777 901
1025-65535
acl purge method PURGE
acl CONNECT method CONNECT
http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny !autenticados
http_access allow autenticados
acl redelocal src 192.168.0.0/22
http_access allow localhost
http_access allow redelocal
http_access deny all
###########################################################################
My /etc/hosts following below:
127.0.0.1 localhost
192.168.0.35 squid-server.mydomain.com.br squid-server

The  /etc/resolv.conf is:
domain mydomain.com.br
search mydomain.com.br
nameserver 192.168.0.5

I found that the Squid folder there is this file that does not know:
msntauth.conf, with the following contents:

# Sample MSNT authenticator configuration file
# Antonino Iannella, Stellar-X Pty Ltd
# Sun Sep 2 15:52:31 CST 2001
# NT hosts to use. Best to put their IP addresses in /etc/hosts.
server my_PDC my_BDC my_NTdomain
server other_PDC other_BDC otherdomain
# Denied and allowed users. Comment these if not needed.
#denyusers /usr/local/squid/etc/msntauth.denyusers
#allowusers /usr/local/squid/etc/msntauth.allowusers

I'm using Debian 8 and Samba 4.1.17.

Do anybody have an idea?

Regards,

M?rcio
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150801/87393c51/attachment.htm>

From squid3 at treenet.co.nz  Sun Aug  2 01:02:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Aug 2015 13:02:06 +1200
Subject: [squid-users] Problems with Squid 3 Authentication on Samba 4
In-Reply-To: <CA+0TdypA3ngF=vKPMKh3XSJCYaYfobo3WWZ9cJqaLXfPBczY7Q@mail.gmail.com>
References: <CA+0TdypA3ngF=vKPMKh3XSJCYaYfobo3WWZ9cJqaLXfPBczY7Q@mail.gmail.com>
Message-ID: <55BD6C0E.3000109@treenet.co.nz>

On 2/08/2015 11:57 a.m., Marcio Demetrio Bacci wrote:
> Hi,
> 
> I'm trying to authenticate the squid3 in Samba4. The part of Samba 4
> authentication is OK, because with the commands wbinfo -i, getent passwd,
> klist, kinit is all right and I can  get  the expected results. Also on the
> command line can authenticate users (/usr /bin/ntlm_auth --username =
> DomainUser), however, when using the squid does not.
> 

That shows the helper is able to contact AD fine to do lookups in Basic
auth format. What it fails to tell is:

a) whether the browser is trying to use NTLM, Basic or both
b) whether the NTLM token assignment from AD to browser is working
c) whether the browser supports NTLM
d) whether the OS the browser is running on is NTLM-enabled
e) what NTLM v1 or v2 the OS the browser runs on supports
f) whether the OS the browser is running on is attached to the domain

Start by using the ntlm_auth helpers debug option. To see if there is
anything useful there.

Then move on to get debug out of Squid. Configure "debug_options ALL,1
11,2 29,9 84,6" in squid.conf to get a copy of the HTTP messages and the
auth helper activity in cache.log.

With that you can see the answer to (a), and the tokens that are being
passed around can be pasted to command line in for manual testing of the
helper with the same parameters Squid is using.


> I have already followed several tutorials, including to authenticate to AD,
> as the Samba 4  is fully compatible with AD. Nothing works.
> 
> Follow my configuration file (squid.conf):
> 

Looks okay to me. Though it does contain this oddity:

  auth_param ntlm children 30
  auth_param ntlm children 5

I suspect you meant the second one to be for basic auth.


You should also try using the hacky:

  auth_param ntlm keep_alive off

(required by older MSIE, some Firefox, and most Safari versions).


> I found that the Squid folder there is this file that does not know:
> msntauth.conf, with the following contents:

msntauth.conf is the config file for the basic_msnt_auth helper.

> 
> I'm using Debian 8 and Samba 4.1.17.
> 
> Do anybody have an idea?
> 

I suspect the browser<->Squid interaction is not doing what you think it
is. The above debugging will give you a better idea whats going on.

Amos



From dan at getbusi.com  Mon Aug  3 06:06:35 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 3 Aug 2015 16:06:35 +1000
Subject: [squid-users] Detecting clients flooding squid with failed request
Message-ID: <894E9950-0EF2-481E-8AEB-E9033FD5EE32@getbusi.com>

Probably a lot of forward proxy users here have encountered applications which, if they can?t get their web requests through the proxy (because of 407 Proxy Auth Required or whatever), just start aggressively, endlessly spamming requests.

A recent example would be AVG?s ?cloud? features generating around 90 requests per second from one computer. Pretty annoying.

I was wondering if anyone here has any creative ideas for detecting when this is happening programmatically?

It?s obviously easy to spot as a human if you?re looking at the access log, but it would be awesome if we could somehow parse some squidclient manager output and/or the access logs and ?raise the alarm? in some way.

Would love to hear anyone?s ideas about how the logic would work for something like this.

Cheers
Dan

From fredbmail at free.fr  Mon Aug  3 07:09:53 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 3 Aug 2015 09:09:53 +0200 (CEST)
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <55BC3193.2000501@treenet.co.nz>
Message-ID: <1361871316.25955027.1438585793111.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Ah, well. Since it is essentially just a documentation fix. I've
> added
> it to 3.4.14. But only because it does not actually change any of the
> queue behaviour.
> 
> Amos
> 



Yes I know thanks

Not related, I saw a discussion between You and Alex about url parsing 
Do you think it can resolve "my" bug with DIGEST authentication ? http://bugs.squid-cache.org/show_bug.cgi?id=4258 and also the other with the ugly uri - like bing in IE -
I think this not the same part, but ...
 


From Antony.Stone at squid.open.source.it  Mon Aug  3 07:11:30 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 3 Aug 2015 09:11:30 +0200
Subject: [squid-users] Detecting clients flooding squid with failed
	request
In-Reply-To: <894E9950-0EF2-481E-8AEB-E9033FD5EE32@getbusi.com>
References: <894E9950-0EF2-481E-8AEB-E9033FD5EE32@getbusi.com>
Message-ID: <201508030911.30335.Antony.Stone@squid.open.source.it>

On Monday 03 August 2015 at 08:06:35 (EU time), Dan Charlesworth wrote:

> Probably a lot of forward proxy users here have encountered applications
> which, if they can?t get their web requests through the proxy (because of
> 407 Proxy Auth Required or whatever), just start aggressively, endlessly
> spamming requests.
> 
> A recent example would be AVG?s ?cloud? features generating around 90
> requests per second from one computer. Pretty annoying.
> 
> I was wondering if anyone here has any creative ideas for detecting when
> this is happening programmatically?
> 
> It?s obviously easy to spot as a human if you?re looking at the access log,
> but it would be awesome if we could somehow parse some squidclient manager
> output and/or the access logs and ?raise the alarm? in some way.
> 
> Would love to hear anyone?s ideas about how the logic would work for
> something like this.

Depending on what action you want for "raising the alarm", I'm pretty sure 
fail2ban could be configured for this.


Antony.

-- 
Anyone that's normal doesn't really achieve much.

 - Mark Blair, Australian rocket engineer

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dan at getbusi.com  Mon Aug  3 07:25:50 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 3 Aug 2015 17:25:50 +1000
Subject: [squid-users] Detecting clients flooding squid with failed
	request
In-Reply-To: <201508030911.30335.Antony.Stone@squid.open.source.it>
References: <894E9950-0EF2-481E-8AEB-E9033FD5EE32@getbusi.com>
 <201508030911.30335.Antony.Stone@squid.open.source.it>
Message-ID: <97817379-C4B6-40AE-B64D-904AA472C342@getbusi.com>

Thanks Antony. 

Fail2ban looks like a viable option though we would still need to write a regex definition to target this sort of behaviour. Their squid example targets aggressive hosts where my preference would be to target aggressive applications (that could be running on more than one host).

https://github.com/fail2ban/fail2ban/blob/master/config/filter.d/squid.conf

In my case ?raise the alarm? would probably mean send an email to somebody and there are lots of ways to do that programmatically.

Still open to any other ideas anyone has.

> On 3 Aug 2015, at 5:11 pm, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Monday 03 August 2015 at 08:06:35 (EU time), Dan Charlesworth wrote:
> 
>> Probably a lot of forward proxy users here have encountered applications
>> which, if they can?t get their web requests through the proxy (because of
>> 407 Proxy Auth Required or whatever), just start aggressively, endlessly
>> spamming requests.
>> 
>> A recent example would be AVG?s ?cloud? features generating around 90
>> requests per second from one computer. Pretty annoying.
>> 
>> I was wondering if anyone here has any creative ideas for detecting when
>> this is happening programmatically?
>> 
>> It?s obviously easy to spot as a human if you?re looking at the access log,
>> but it would be awesome if we could somehow parse some squidclient manager
>> output and/or the access logs and ?raise the alarm? in some way.
>> 
>> Would love to hear anyone?s ideas about how the logic would work for
>> something like this.
> 
> Depending on what action you want for "raising the alarm", I'm pretty sure 
> fail2ban could be configured for this.
> 
> 
> Antony.
> 
> -- 
> Anyone that's normal doesn't really achieve much.
> 
> - Mark Blair, Australian rocket engineer
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Aug  3 08:49:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 3 Aug 2015 20:49:54 +1200
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <1361871316.25955027.1438585793111.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1361871316.25955027.1438585793111.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55BF2B32.6020306@treenet.co.nz>

On 3/08/2015 7:09 p.m., FredB wrote:
> 
>>
>> Ah, well. Since it is essentially just a documentation fix. I've
>> added
>> it to 3.4.14. But only because it does not actually change any of the
>> queue behaviour.
>>
>> Amos
>>
> 
> 
> 
> Yes I know thanks
> 
> Not related, I saw a discussion between You and Alex about url parsing 
> Do you think it can resolve "my" bug with DIGEST authentication ? http://bugs.squid-cache.org/show_bug.cgi?id=4258 and also the other with the ugly uri - like bing in IE -
> I think this not the same part, but ...

I can't answer that with any certainty. Though I am doubtful it woudl
help. Since your problem is quoted string end detection that goes bad
even before URL parsing gets involved.

Amos



From eliezer at ngtech.co.il  Mon Aug  3 10:03:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 3 Aug 2015 13:03:46 +0300
Subject: [squid-users] Detecting clients flooding squid with failed
 request
In-Reply-To: <97817379-C4B6-40AE-B64D-904AA472C342@getbusi.com>
References: <894E9950-0EF2-481E-8AEB-E9033FD5EE32@getbusi.com>
 <201508030911.30335.Antony.Stone@squid.open.source.it>
 <97817379-C4B6-40AE-B64D-904AA472C342@getbusi.com>
Message-ID: <55BF3C82.9060900@ngtech.co.il>

Hey Dan,

It's pretty simple to write this rule since its a counted+pattern match 
and that's it nothing more.
If it fits your need you can add a send mail target instead of a "ban" one.

Eliezer

On 03/08/2015 10:25, Dan Charlesworth wrote:
> Thanks Antony.
>
> Fail2ban looks like a viable option though we would still need to write a regex definition to target this sort of behaviour. Their squid example targets aggressive hosts where my preference would be to target aggressive applications (that could be running on more than one host).
>
> https://github.com/fail2ban/fail2ban/blob/master/config/filter.d/squid.conf
>
> In my case ?raise the alarm? would probably mean send an email to somebody and there are lots of ways to do that programmatically.



From fredbmail at free.fr  Mon Aug  3 13:38:02 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 3 Aug 2015 15:38:02 +0200 (CEST)
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <55BF2B32.6020306@treenet.co.nz>
Message-ID: <842081060.26842957.1438609082422.JavaMail.root@zimbra4-e1.priv.proxad.net>


> I can't answer that with any certainty. Though I am doubtful it woudl
> help. Since your problem is quoted string end detection that goes bad
> even before URL parsing gets involved.
> 
> Amos


Ok thank you Amos, let me know if I can help with some tests 

Fred


From marke at wadafarms.com  Mon Aug  3 22:13:32 2015
From: marke at wadafarms.com (markme)
Date: Mon, 3 Aug 2015 15:13:32 -0700 (PDT)
Subject: [squid-users] Deny Caching of Video and Audio
Message-ID: <1438640012180-4672593.post@n4.nabble.com>

Is there a simple way to deny caching of video and audio? My manager doesn't
want these to be cached due to the large amount of space they might take up.
Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Deny-Caching-of-Video-and-Audio-tp4672593.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Aug  3 22:29:51 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 4 Aug 2015 00:29:51 +0200
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <1438640012180-4672593.post@n4.nabble.com>
References: <1438640012180-4672593.post@n4.nabble.com>
Message-ID: <201508040029.51378.Antony.Stone@squid.open.source.it>

On Tuesday 04 August 2015 at 00:13:32, markme wrote:

> Is there a simple way to deny caching of video and audio? My manager
> doesn't want these to be cached due to the large amount of space they
> might take up.

Just set the maximum cache object size - then it doesn't matter whether it's 
audio, video, ISO images, windows updates, or whatever - things taking up 
large amounts of space won't be cached.


Antony.

-- 
What do you get when you cross a joke with a rhetorical question?

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jorgeley at gmail.com  Mon Aug  3 23:39:38 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Mon, 3 Aug 2015 20:39:38 -0300
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <201508040029.51378.Antony.Stone@squid.open.source.it>
References: <1438640012180-4672593.post@n4.nabble.com>
 <201508040029.51378.Antony.Stone@squid.open.source.it>
Message-ID: <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>

create an acl:
acl youtube dstdomain .youtube.com
use the directive: cache deny youtube

2015-08-03 19:29 GMT-03:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Tuesday 04 August 2015 at 00:13:32, markme wrote:
>
> > Is there a simple way to deny caching of video and audio? My manager
> > doesn't want these to be cached due to the large amount of space they
> > might take up.
>
> Just set the maximum cache object size - then it doesn't matter whether
> it's
> audio, video, ISO images, windows updates, or whatever - things taking up
> large amounts of space won't be cached.
>
>
> Antony.
>
> --
> What do you get when you cross a joke with a rhetorical question?
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150803/579beb1b/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Aug  4 08:28:32 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 4 Aug 2015 10:28:32 +0200
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
References: <1438640012180-4672593.post@n4.nabble.com>
 <201508040029.51378.Antony.Stone@squid.open.source.it>
 <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
Message-ID: <201508041028.33187.Antony.Stone@squid.open.source.it>

On Tuesday 04 August 2015 at 01:39:38, Jorgeley Junior wrote:

> create an acl:
> acl youtube dstdomain .youtube.com
> use the directive: cache deny youtube

That will prevent caching for all content from a specific domain, no matter 
what size the objects.

It's a bit tedious to have to set this up for all the domains you think you 
might get overly-large content from, and once you have, it's prevents caching 
of anything from those domains, including the acceptably small stuff.

> 2015-08-03 19:29 GMT-03:00 Antony Stone wrote:
> > On Tuesday 04 August 2015 at 00:13:32, markme wrote:
> > > Is there a simple way to deny caching of video and audio? My manager
> > > doesn't want these to be cached due to the large amount of space they
> > > might take up.
> > 
> > Just set the maximum cache object size - then it doesn't matter whether
> > it's audio, video, ISO images, windows updates, or whatever - things
> > taking up large amounts of space won't be cached.
> >
> > Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Tue Aug  4 08:32:17 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 4 Aug 2015 10:32:17 +0200
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <201508041028.33187.Antony.Stone@squid.open.source.it>
References: <1438640012180-4672593.post@n4.nabble.com>
 <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
 <201508041028.33187.Antony.Stone@squid.open.source.it>
Message-ID: <201508041032.18014.Antony.Stone@squid.open.source.it>

On Tuesday 04 August 2015 at 10:28:32, Antony Stone wrote:

> On Tuesday 04 August 2015 at 01:39:38, Jorgeley Junior wrote:
> > create an acl:
> > acl youtube dstdomain .youtube.com
> > use the directive: cache deny youtube
> 
> That will prevent caching for all content from a specific domain, no matter
> what size the objects.

Oh, and I forgot to mention - a lot of people here are struggling to work out 
how to *get* Squid to cache content from youtube, because youtube doesn't send 
content by 'standard HTTP' any more.

Therefore blocking the caching of content from youtube.com in particular will 
very likely make almost zero difference.

> It's a bit tedious to have to set this up for all the domains you think you
> might get overly-large content from, and once you have, it's prevents
> caching of anything from those domains, including the acceptably small
> stuff.
> 
> > 2015-08-03 19:29 GMT-03:00 Antony Stone wrote:
> > > On Tuesday 04 August 2015 at 00:13:32, markme wrote:
> > > > Is there a simple way to deny caching of video and audio? My manager
> > > > doesn't want these to be cached due to the large amount of space they
> > > > might take up.
> > > 
> > > Just set the maximum cache object size - then it doesn't matter whether
> > > it's audio, video, ISO images, windows updates, or whatever - things
> > > taking up large amounts of space won't be cached.
> > > 
> > > Antony.

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From shenan.hawkins at gmail.com  Tue Aug  4 13:04:54 2015
From: shenan.hawkins at gmail.com (Shenan Hawkins)
Date: Tue, 4 Aug 2015 09:04:54 -0400
Subject: [squid-users] Debian Testing libnettle4
Message-ID: <CAFzetiuZVGgVcv+-UsfDFX6FbhgOgW-efUb+t37R3v0SbTsbBQ@mail.gmail.com>

The following packages have unmet dependencies:
 squid3 : Depends: libnettle4 but it is not installable
E: Unable to correct problems, you have held broken packages.

Hello - squid3 got removed this morning in regular update/upgrade
(dist-upgrade actually) this morning.  Should I just wait for resolution?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150804/92a0e3c9/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug  4 13:27:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Aug 2015 01:27:05 +1200
Subject: [squid-users] Debian Testing libnettle4
In-Reply-To: <CAFzetiuZVGgVcv+-UsfDFX6FbhgOgW-efUb+t37R3v0SbTsbBQ@mail.gmail.com>
References: <CAFzetiuZVGgVcv+-UsfDFX6FbhgOgW-efUb+t37R3v0SbTsbBQ@mail.gmail.com>
Message-ID: <55C0BDA9.2010406@treenet.co.nz>

On 5/08/2015 1:04 a.m., Shenan Hawkins wrote:
> The following packages have unmet dependencies:
>  squid3 : Depends: libnettle4 but it is not installable
> E: Unable to correct problems, you have held broken packages.
> 
> Hello - squid3 got removed this morning in regular update/upgrade
> (dist-upgrade actually) this morning.  Should I just wait for resolution?

Ouch. Passing this on to Luigi. The replacement package is stuck in
sid/unstable behind the Debian GCC-5 migration which has a bit of an
indeterminate timeline.


If you have the ability to build the squid3 source package that might be
the best way forward IMHO. The resulting custom build will depend on
whatever nettle is on your system.

That would be "apt-get --build source squid3" then dpkg --install the
appropriate resulting *.deb files you want (starting with the
squid3-common).

Amos



From marke at wadafarms.com  Tue Aug  4 13:29:08 2015
From: marke at wadafarms.com (markme)
Date: Tue, 4 Aug 2015 06:29:08 -0700 (PDT)
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <201508041032.18014.Antony.Stone@squid.open.source.it>
References: <1438640012180-4672593.post@n4.nabble.com>
 <201508040029.51378.Antony.Stone@squid.open.source.it>
 <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
 <201508041028.33187.Antony.Stone@squid.open.source.it>
 <201508041032.18014.Antony.Stone@squid.open.source.it>
Message-ID: <1438694948148-4672601.post@n4.nabble.com>

Well we don't mind caching large objects such as anti-virus updates or other
updates. My manager specifically asked to not cache media such as video and
audio. So is there like a refresh pattern that I can use or an ACL to deny
caching of all video and audio?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Deny-Caching-of-Video-and-Audio-tp4672593p4672601.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Aug  4 13:42:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Aug 2015 01:42:36 +1200
Subject: [squid-users] Debian Testing libnettle4
In-Reply-To: <55C0BDA9.2010406@treenet.co.nz>
References: <CAFzetiuZVGgVcv+-UsfDFX6FbhgOgW-efUb+t37R3v0SbTsbBQ@mail.gmail.com>
 <55C0BDA9.2010406@treenet.co.nz>
Message-ID: <55C0C14C.7000508@treenet.co.nz>

On 5/08/2015 1:27 a.m., Amos Jeffries wrote:
> On 5/08/2015 1:04 a.m., Shenan Hawkins wrote:
>> The following packages have unmet dependencies:
>>  squid3 : Depends: libnettle4 but it is not installable
>> E: Unable to correct problems, you have held broken packages.
>>
>> Hello - squid3 got removed this morning in regular update/upgrade
>> (dist-upgrade actually) this morning.  Should I just wait for resolution?
> 

Oh. Hang on. Something weird going on in your system.

=> squid3 in Testing/Stretch depends on libnettle6

Why is that not installing?

Amos



From squid3 at treenet.co.nz  Tue Aug  4 14:19:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Aug 2015 02:19:39 +1200
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <1438694948148-4672601.post@n4.nabble.com>
References: <1438640012180-4672593.post@n4.nabble.com>
 <201508040029.51378.Antony.Stone@squid.open.source.it>
 <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
 <201508041028.33187.Antony.Stone@squid.open.source.it>
 <201508041032.18014.Antony.Stone@squid.open.source.it>
 <1438694948148-4672601.post@n4.nabble.com>
Message-ID: <55C0C9FB.2030404@treenet.co.nz>

On 5/08/2015 1:29 a.m., markme wrote:
> Well we don't mind caching large objects such as anti-virus updates or other
> updates. My manager specifically asked to not cache media such as video and
> audio. So is there like a refresh pattern that I can use or an ACL to deny
> caching of all video and audio?

Sadly no. It's not even possible to *identify* all audio and video. So
many known and known-weird ways it could be smuggled around.
 Flash (.flv) for example is not video or audio. Its a script (yay^HH
boo for binary blobs).

Best you can hope for is rep_mime_type ACL check for video and audio
mime types. You will also need the Squid-3.5 store_miss feature to use
it for non-caching when it matches ("cache deny ..." wont work on
responses).

Amos



From wiedergold at web.de  Tue Aug  4 18:46:21 2015
From: wiedergold at web.de (Alexander Wiedergold)
Date: Tue, 4 Aug 2015 20:46:21 +0200
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <mailman.3.1438689601.17131.squid-users@lists.squid-cache.org>
References: <mailman.3.1438689601.17131.squid-users@lists.squid-cache.org>
Message-ID: <55C1087D.2070102@web.de>

By default, all headers are allowed
reply_header_access Content-Type allow all
change it's

On 04.08.2015 14:00, squid-users-request at lists.squid-cache.org wrote:
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>     1. Re: Squid 3.4.14 (FredB)
>     2. Deny Caching of Video and Audio (markme)
>     3. Re: Deny Caching of Video and Audio (Antony Stone)
>     4. Re: Deny Caching of Video and Audio (Jorgeley Junior)
>     5. Re: Deny Caching of Video and Audio (Antony Stone)
>     6. Re: Deny Caching of Video and Audio (Antony Stone)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 3 Aug 2015 15:38:02 +0200 (CEST)
> From: FredB <fredbmail at free.fr>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.4.14
> Message-ID:
> 	<842081060.26842957.1438609082422.JavaMail.root at zimbra4-e1.priv.proxad.net>
> 	
> Content-Type: text/plain; charset=utf-8
>
>
>> I can't answer that with any certainty. Though I am doubtful it woudl
>> help. Since your problem is quoted string end detection that goes bad
>> even before URL parsing gets involved.
>>
>> Amos
>
> Ok thank you Amos, let me know if I can help with some tests
>
> Fred
>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 3 Aug 2015 15:13:32 -0700 (PDT)
> From: markme <marke at wadafarms.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Deny Caching of Video and Audio
> Message-ID: <1438640012180-4672593.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Is there a simple way to deny caching of video and audio? My manager doesn't
> want these to be cached due to the large amount of space they might take up.
> Thanks!
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Deny-Caching-of-Video-and-Audio-tp4672593.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 4 Aug 2015 00:29:51 +0200
> From: Antony Stone <Antony.Stone at squid.open.source.it>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Deny Caching of Video and Audio
> Message-ID: <201508040029.51378.Antony.Stone at squid.open.source.it>
> Content-Type: Text/Plain;  charset="utf-8"
>
> On Tuesday 04 August 2015 at 00:13:32, markme wrote:
>
>> Is there a simple way to deny caching of video and audio? My manager
>> doesn't want these to be cached due to the large amount of space they
>> might take up.
> Just set the maximum cache object size - then it doesn't matter whether it's
> audio, video, ISO images, windows updates, or whatever - things taking up
> large amounts of space won't be cached.
>
>
> Antony.
>



From eliezer at ngtech.co.il  Tue Aug  4 20:35:34 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 4 Aug 2015 23:35:34 +0300
Subject: [squid-users] Deny Caching of Video and Audio
In-Reply-To: <55C0C9FB.2030404@treenet.co.nz>
References: <1438640012180-4672593.post@n4.nabble.com>
 <201508040029.51378.Antony.Stone@squid.open.source.it>
 <CAMeoTH=Y50oeMOZGa2VCAv3kFoivOnKpfuHypzxnKHRQB1AZaQ@mail.gmail.com>
 <201508041028.33187.Antony.Stone@squid.open.source.it>
 <201508041032.18014.Antony.Stone@squid.open.source.it>
 <1438694948148-4672601.post@n4.nabble.com> <55C0C9FB.2030404@treenet.co.nz>
Message-ID: <55C12216.4040707@ngtech.co.il>

But in the other hand there is a way to determine some of the domains 
which contains videos.
Specifically for google it's:
.googlevideo.com

For other sites I assume that regular statistics check of the access.log 
can help with finding what your users use the most and minimize the 
specific videos.

It's not an easy thing to do but sometimes needed.

Eliezer

On 04/08/2015 17:19, Amos Jeffries wrote:
> On 5/08/2015 1:29 a.m., markme wrote:
>> Well we don't mind caching large objects such as anti-virus updates or other
>> updates. My manager specifically asked to not cache media such as video and
>> audio. So is there like a refresh pattern that I can use or an ACL to deny
>> caching of all video and audio?
>
> Sadly no. It's not even possible to *identify* all audio and video. So
> many known and known-weird ways it could be smuggled around.
>   Flash (.flv) for example is not video or audio. Its a script (yay^HH
> boo for binary blobs).
>
> Best you can hope for is rep_mime_type ACL check for video and audio
> mime types. You will also need the Squid-3.5 store_miss feature to use
> it for non-caching when it matches ("cache deny ..." wont work on
> responses).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From hack.back at hotmail.com  Tue Aug  4 22:27:45 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 4 Aug 2015 15:27:45 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <559256E0.7070506@treenet.co.nz>
References: <55875970.7060200@treenet.co.nz>
 <1434936992183-4671829.post@n4.nabble.com> <55876E36.4050702@treenet.co.nz>
 <1435549243614-4671937.post@n4.nabble.com> <5590C6B1.1020706@treenet.co.nz>
 <1435575418899-4671942.post@n4.nabble.com> <55913789.7040605@treenet.co.nz>
 <1435650994449-4671958.post@n4.nabble.com>
 <1435651056189-4671959.post@n4.nabble.com> <559256E0.7070506@treenet.co.nz>
Message-ID: <1438727265205-4672606.post@n4.nabble.com>

<< I'm not sure how to fix that. >> then who should i talk to.. you guys
should dig in source and found out its important tks any should i ask
outside ?? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4672606.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From shenan.hawkins at gmail.com  Tue Aug  4 22:42:13 2015
From: shenan.hawkins at gmail.com (Shenan Hawkins)
Date: Tue, 4 Aug 2015 18:42:13 -0400
Subject: [squid-users] Debian Testing libnettle4
In-Reply-To: <55C0C14C.7000508@treenet.co.nz>
References: <CAFzetiuZVGgVcv+-UsfDFX6FbhgOgW-efUb+t37R3v0SbTsbBQ@mail.gmail.com>
 <55C0BDA9.2010406@treenet.co.nz> <55C0C14C.7000508@treenet.co.nz>
Message-ID: <CAFzetitsLFcHQkfEGOW0rXR_O3U5H0AHO_drOL0-mB4pUy3mjA@mail.gmail.com>

Thanks for quick follow-up.  Indeed, a bungled sources.list from
http://debgen.simplylinux.ch/  :-(  Problem corrected.

On 4 August 2015 at 09:42, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/08/2015 1:27 a.m., Amos Jeffries wrote:
> > On 5/08/2015 1:04 a.m., Shenan Hawkins wrote:
> >> The following packages have unmet dependencies:
> >>  squid3 : Depends: libnettle4 but it is not installable
> >> E: Unable to correct problems, you have held broken packages.
> >>
> >> Hello - squid3 got removed this morning in regular update/upgrade
> >> (dist-upgrade actually) this morning.  Should I just wait for
> resolution?
> >
>
> Oh. Hang on. Something weird going on in your system.
>
> => squid3 in Testing/Stretch depends on libnettle6
>
> Why is that not installing?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150804/bb88ee20/attachment.htm>

From squid-user at tlinx.org  Tue Aug  4 23:04:39 2015
From: squid-user at tlinx.org (Linda W)
Date: Tue, 04 Aug 2015 16:04:39 -0700
Subject: [squid-users] linking ltrans get missing xstrerr?
Message-ID: <55C14507.7060909@tlinx.org>

ltrans -- I disabled translation -- should ltrans be getting made?
If so, where can I find xstrerr?

Thanks!
(must be buried in *somefile*!


libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments 
-Wshadow -Werror -pipe -D_REENTRANT -m64 -DOPENSSL_LOAD_CONF -O2 -m64 
-fasynchronous-unwind-tables -fbranch-target-load-optimize 
-fdelete-null-pointer-checks -fgcse-after-reload -fgcse-las -fgcse-sm 
-fgraphite-identity -fipa-pta -fivopts -floop-block -floop-flatten 
-floop-interchange -floop-strip-mine -flto -fmessage-length=0 
-fpredictive-commoning -frename-registers -freorder-blocks-and-partition 
-ftracer -fsched-stalled-insns=1 -fsched-stalled-insns-dep=1 
-ftree-loop-linear -ftree-loop-distribution 
-ftree-loop-distribute-patterns -ftree-loop-im -ftree-loop-ivcanon 
-ftree-vectorize -ftree-slp-vectorize -funswitch-loops -funwind-tables 
-fvariable-expansion-in-unroller -fvect-cost-model -fweb -march=native 
-fpie -pipe -march=native -std=c++11 -m64 -flto=6 -fpie -O2 
-fuse-linker-plugin -o log_file_daemon log_file_daemon.o  -L../../../lib 
../../../compat/.libs/libcompat-squid.a -lm -lnsl -lresolv -lcap -lrt -ldl
/tmp/cc4OQH3C.ltrans0.ltrans.o: In function `rotate(char const*, int)':
cc4OQH3C.ltrans0.o:(.text+0xe2): undefined reference to `xstrerr(int)'
cc4OQH3C.ltrans0.o:(.text+0x150): undefined reference to `xstrerr(int)'
collect2: error: ld returned 1 exit status


From squid-user at tlinx.org  Wed Aug  5 01:49:28 2015
From: squid-user at tlinx.org (Linda W)
Date: Tue, 04 Aug 2015 18:49:28 -0700
Subject: [squid-users] linking ltrans get missing xstrerr?  (SOLVED)
In-Reply-To: <55C14507.7060909@tlinx.org>
References: <55C14507.7060909@tlinx.org>
Message-ID: <55C16BA8.2070506@tlinx.org>

Linda W wrote:
> ltrans -- I disabled translation -- should ltrans be getting made?
> If so, where can I find xstrerr?
---
looks like a windows only thing, so I assumed
my build dir was corrupt.  It is no longer corrupt. ;-/

:


From asakura at ioc.dnp.co.jp  Wed Aug  5 07:16:44 2015
From: asakura at ioc.dnp.co.jp (Kazuhiro Asakura)
Date: Wed, 05 Aug 2015 16:16:44 +0900 (JST)
Subject: [squid-users] I want to display indirect_client_address on error
	page.
Message-ID: <20150805.161644.68539936.asakura@ioc.dnp.co.jp>

Hello,

I have a simple question about squid-3 "follow-x-forwarded-for" feature.

I use "follow-x-forwarded-for" feature, and logged end point IP address.
Also, I created customized error page for end user. but, env %i of
error page does not be translate to end point IP address.

I want to display the address of end point to error page.
If someone could suggest solution, I would really appreciate it.

Sorry my poor English.

regards,
Kazuhiro


From hh.hao at outlook.com  Wed Aug  5 12:38:15 2015
From: hh.hao at outlook.com (tianchao haohan)
Date: Wed, 5 Aug 2015 12:38:15 +0000
Subject: [squid-users] How can I capture post data?
Message-ID: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>



Hi,

Here is the requirement that I need to capture the post data
form the http request. 

It seems that log is not enough for this. Is there any
method to get this data?Appreciate that!


 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150805/f430ef72/attachment.htm>

From rousskov at measurement-factory.com  Wed Aug  5 14:44:40 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 05 Aug 2015 08:44:40 -0600
Subject: [squid-users] How can I capture post data?
In-Reply-To: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
References: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
Message-ID: <55C22158.3090107@measurement-factory.com>

On 08/05/2015 06:38 AM, tianchao haohan wrote:

> I need to capture the post data form the http request.
> 
> It seems that log is not enough for this. Is there any method to get
> this data?


Yes, both ICAP and eCAP interfaces can be used to capture, modify,
and/or block HTTP requests, including requests with bodies:

    http://wiki.squid-cache.org/SquidFaq/ContentAdaptation

Alex.



From squid3 at treenet.co.nz  Wed Aug  5 14:50:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 02:50:34 +1200
Subject: [squid-users] How can I capture post data?
In-Reply-To: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
References: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
Message-ID: <55C222BA.8000104@treenet.co.nz>

On 6/08/2015 12:38 a.m., tianchao haohan wrote:
> 
> Hi,
> 
> Here is the requirement that I need to capture the post data
> form the http request. 
> 
> It seems that log is not enough for this. Is there any
> method to get this data?Appreciate that!


Sigh. Dont, just dont. Its a seriously bad minefield of legal problems
waiting for you down that road.


That said. ICAP or eCAP exist for anything payload related.

Amos



From yvoinov at gmail.com  Wed Aug  5 18:40:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 6 Aug 2015 00:40:44 +0600
Subject: [squid-users] How can I capture post data?
In-Reply-To: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
References: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>
Message-ID: <55C258AC.8090609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I would not do that. It is dangerously close to the illegal hacking.

Even the inclusion of GET query strings in the log is considered a
violation of privacy. And it has done solely for the purpose of
debugging caching.

05.08.15 18:38, tianchao haohan ?????:
>
>
> Hi,
>
> Here is the requirement that I need to capture the post data
> form the http request.
>
> It seems that log is not enough for this. Is there any
> method to get this data?Appreciate that!
>
>
>                           
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVwlisAAoJENNXIZxhPexG6wcIAIL/ebX4RkWbtKmswua3DD9g
abN+x0BhDZGoJQmLiOfABYh8wItOcZ1d6UL66Xe6NmApnez+7QaHwPfiRmyl4vJx
tFSmrZjFN9n6PolBaWHlXNQvPV4sGlAAVvNdRRmhR47FzC+znKoj+sCsuAThhGEN
mmc0kCx/mGGg+TfxwP8GY4u5hW161V5hQSBZsxV4c18gcRmj5oYf2I8EMlndJEms
G5slHtHT8xQjZfdCyplcPP3kCQx1V+YxicMlBdgdqLIyJDXZHtVB20Dtxdf4IJmX
LobaOQPx9UUtGSYgcPPiEoNGR63/trCKaHBd5ywtDeEOEA8P0qL13Ck2cRPMpVY=
=GxW0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150806/5e5b7577/attachment.htm>

From rousskov at measurement-factory.com  Wed Aug  5 19:47:16 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 05 Aug 2015 13:47:16 -0600
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1438727265205-4672606.post@n4.nabble.com>
References: <55875970.7060200@treenet.co.nz>
 <1434936992183-4671829.post@n4.nabble.com> <55876E36.4050702@treenet.co.nz>
 <1435549243614-4671937.post@n4.nabble.com> <5590C6B1.1020706@treenet.co.nz>
 <1435575418899-4671942.post@n4.nabble.com> <55913789.7040605@treenet.co.nz>
 <1435650994449-4671958.post@n4.nabble.com>
 <1435651056189-4671959.post@n4.nabble.com> <559256E0.7070506@treenet.co.nz>
 <1438727265205-4672606.post@n4.nabble.com>
Message-ID: <55C26844.6040403@measurement-factory.com>

On 08/04/2015 04:27 PM, HackXBack wrote:
> << I'm not sure how to fix that. >> then who should i talk to.. 
> you guys should dig in source and found out 


Yes, somebody should "dig in source" and fix the bug. However, Amos (or
anybody else providing free support on this mailing list) is _not_
obligated to do that. You need to find somebody who is both willing and
able to fix the bug you suffer from. When the bug is complex, it is
often difficult to find such a person or company (unfortunately). FWIW,
The Squid Software Foundation is working on streamlining this search
process for Squid admins, but these improvements take time.


> its important tks any should i ask outside ?? 

If you cannot get help here, you may consider other support avenues:

http://www.squid-cache.org/Support/

http://wiki.squid-cache.org/SquidFaq/CompleteFaq#SquidFaq.2FAboutSquid.How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


Good luck,

Alex.



From squid3 at treenet.co.nz  Thu Aug  6 00:38:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 12:38:03 +1200
Subject: [squid-users] I want to display indirect_client_address on
 error page.
In-Reply-To: <20150805.161644.68539936.asakura@ioc.dnp.co.jp>
References: <20150805.161644.68539936.asakura@ioc.dnp.co.jp>
Message-ID: <55C2AC6B.4030401@treenet.co.nz>

On 5/08/2015 7:16 p.m., Kazuhiro Asakura wrote:
> Hello,
> 
> I have a simple question about squid-3 "follow-x-forwarded-for" feature.
> 
> I use "follow-x-forwarded-for" feature, and logged end point IP address.
> Also, I created customized error page for end user. but, env %i of
> error page does not be translate to end point IP address.

More correctly it is the direct endpoint of the client TCP connection.
Not the chain of multiple TCP connections, which may include NAT or
other protocol translations between Squid and a user.

> 
> I want to display the address of end point to error page.
> If someone could suggest solution, I would really appreciate it.


The easiest approach I think would be to add a javascript snippet in
place of the %i code that locates the users IP address and displays it
to them.

This (2) is actually a lot more privacy-friendly for end users. Since
you/Squid dont have to go digging into information about the internal
network structure of their ISP just to display it.


The requirement that Squid error pages be static HTML only applies to
how they are generated as responses at the server (Squid) end. The page
itself can reference external resources, or contains scripts the client
(browser) end runs.


> 
> Sorry my poor English.

Not a problem. Quite good compared to some.

HTH
Amos



From hh.hao at outlook.com  Thu Aug  6 02:25:08 2015
From: hh.hao at outlook.com (tianchao haohan)
Date: Thu, 6 Aug 2015 02:25:08 +0000
Subject: [squid-users] How can I capture post data?
In-Reply-To: <55C258AC.8090609@gmail.com>
References: <SNT147-W8934CD47D7080056003D8794750@phx.gbl>,
 <55C258AC.8090609@gmail.com>
Message-ID: <SNT147-W59FAA1A8BB0750E0BE209B94740@phx.gbl>

Thanks for your reply.
I will use the function in Intranet, which is well trusted.
So, from technical side, is that available to get those post data? 
Can we achieve the goal with plugin?

To: squid-users at lists.squid-cache.org
From: yvoinov at gmail.com
Date: Thu, 6 Aug 2015 00:40:44 +0600
Subject: Re: [squid-users] How can I capture post data?


  
    
  
  
    

    -----BEGIN PGP SIGNED MESSAGE----- 

    Hash: SHA256 

     

    I would not do that. It is dangerously close to the illegal hacking.

    

    Even the inclusion of GET query strings in the log is considered a
    violation of privacy. And it has done solely for the purpose of
    debugging caching.

    

    05.08.15 18:38, tianchao haohan ?????:

    >

      >

      > Hi,

      >

      > Here is the requirement that I need to capture the post data

      > form the http request. 

      >

      > It seems that log is not enough for this. Is there any

      > method to get this data?Appreciate that!

      >

      >

      >                            

      >

      >

      > _______________________________________________

      > squid-users mailing list

      > squid-users at lists.squid-cache.org

      > http://lists.squid-cache.org/listinfo/squid-users

    

    -----BEGIN PGP SIGNATURE-----


    Version: GnuPG v2


     

    iQEcBAEBCAAGBQJVwlisAAoJENNXIZxhPexG6wcIAIL/ebX4RkWbtKmswua3DD9g


    abN+x0BhDZGoJQmLiOfABYh8wItOcZ1d6UL66Xe6NmApnez+7QaHwPfiRmyl4vJx


    tFSmrZjFN9n6PolBaWHlXNQvPV4sGlAAVvNdRRmhR47FzC+znKoj+sCsuAThhGEN


    mmc0kCx/mGGg+TfxwP8GY4u5hW161V5hQSBZsxV4c18gcRmj5oYf2I8EMlndJEms


    G5slHtHT8xQjZfdCyplcPP3kCQx1V+YxicMlBdgdqLIyJDXZHtVB20Dtxdf4IJmX


    LobaOQPx9UUtGSYgcPPiEoNGR63/trCKaHBd5ywtDeEOEA8P0qL13Ck2cRPMpVY=


    =GxW0


    -----END PGP SIGNATURE-----


    

  


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150806/c7f30de3/attachment.htm>

From asakura at ioc.dnp.co.jp  Thu Aug  6 05:27:48 2015
From: asakura at ioc.dnp.co.jp (Kazuhiro Asakura)
Date: Thu, 06 Aug 2015 14:27:48 +0900 (JST)
Subject: [squid-users] I want to display indirect_client_address on
 error page.
In-Reply-To: <55C2AC6B.4030401@treenet.co.nz>
References: <20150805.161644.68539936.asakura@ioc.dnp.co.jp>
 <55C2AC6B.4030401@treenet.co.nz>
Message-ID: <20150806.142748.68559068.asakura@ioc.dnp.co.jp>

Thank you Amos,

From: Amos Jeffries <squid3 at treenet.co.nz>
> > I use "follow-x-forwarded-for" feature, and logged end point IP address.
> > Also, I created customized error page for end user. but, env %i of
> > error page does not be translate to end point IP address.
> 
> More correctly it is the direct endpoint of the client TCP connection.
> Not the chain of multiple TCP connections, which may include NAT or
> other protocol translations between Squid and a user.

Sorry, My description not enough.

In this environment, BIG-IP exist between end point PC and Squid.
BIG-IP do NAT, and add X-FORWARDED-FOR header.
BIG-IP include IP address of endpoint PC for X-FORWARDED-FOR header.

So, I use follow-x-forwarded-for feature, and get endpoint PC IP address .

This time, endpoint PC IP address is written into Squid access.log.

Maybe, source code is below,

AccessLogEntry.cc
----
#if FOLLOW_X_FORWARDED_FOR
    if (Config.onoff.log_uses_indirect_client && request)
        log_ip = request->indirect_client_addr;
    else
#endif
----

But, it is not assigned to environment variable of error page.

If environment variable of error page could support "indirect_client_addr",
I would really appreciate it.

> > I want to display the address of end point to error page.
> > If someone could suggest solution, I would really appreciate it.
> 
> The easiest approach I think would be to add a javascript snippet in
> place of the %i code that locates the users IP address and displays it
> to them.

I feel that way is difficult.
Because, solution of javascript only works JScript, or require STUN server.

> > Sorry my poor English.
>
> Not a problem. Quite good compared to some.

I'm writing using Google translate.
I hope the feelings of my gratitude will reach you.

regards,
Kazuhiro


From squid3 at treenet.co.nz  Thu Aug  6 02:01:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 14:01:42 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2015:2 Improper
	Protection of	Alternate Path
In-Reply-To: <559DECF1.6050403@treenet.co.nz>
References: <559DECF1.6050403@treenet.co.nz>
Message-ID: <55C2C006.2030507@treenet.co.nz>

This is a courtesy announcement for users and distributors of Squid-3.1
that the advisory SQUID-2015:2 also known as CVE-2015-5400 has been
updated to include a patch for Squid-3.1.

The latest advisory document can be downloaded from
<http://www.squid-cache.org/Advisories/SQUID-2015_2.txt>

The patch is available in the 3.1 series patch archive at
<http://www.squid-cache.org/Versions/v3/3.1/changesets/squid-3.1-10494.patch>


Thanks to Raphael Hertzog with sponsorship by Debian LTS sponsors for
this difficult work.


Amos Jeffries
Squid Software Foundation

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Aug  6 02:04:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 14:04:39 +1200
Subject: [squid-users] [squid-announce] Squid 3.4.14 is available
Message-ID: <55C2C0B7.4050709@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.4.14 release!


This release is a security fix release resolving a vulnerability and
some bugs found in the prior releases.

    REMINDER: This and older releases are already deprecated by
              Squid-3.5 availability.



The major changes to be aware of:


* SQUID-2015:2 Improper Protection of Alternate Path

  http://www.squid-cache.org/Advisories/SQUID-2015_2.txt

Squid when passing a CONNECT request to a cache_peer blindly passes the
response back to the client. This can result in further requests on the
connection bypassing all access controls or routing configuration in the
gateway proxy that would otherwise have been applied.

The default settings of Squid protect most sites against this. However
certain known network topologies require the configuration which is
vulnerable.



 All users of older Squid are urged to upgrade as soon as possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.4/RELEASENOTES.html
when you are ready to make the switch to Squid-3.4

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Aug  6 04:47:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 16:47:33 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.7 is available
Message-ID: <55C2E6E5.30207@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.7 release!


This release is a bug fix release resolving several issues found in the
the prior Squid releases.


The major changes to be aware of:


* Regression Bug 4227: assertions in AuthUserHashPointer

This bug showed up as occasional (or not so occasional) crashes when
Squid is cleaning up the username cache that sits behind HTTP Basic and
Digest authentication. It also affected NTLM and Negotiate which
populate the cache entries but do not use them directly.



* Bug 4251: incorrect instance name for memory segments in /dev/shm

This was an omitted part of the named services feature added in 3.5
which rendered it unusable in previous releases.

Now this is resolved Squid-3.5 instances should be fully multi-tenant /
multi-instance capable as documented in the Release Notes.



* Bug 3345: 'any available user name' format code for external ACLs.

This is a long requested feature port from Squid-2.8 (2.HEAD).

The %un format code can be used in place of %LOGIN to provide a user
name from any available source of credentials. However, it does not
trigger HTTP authentication in the absence of credentials.

The resulting user value is generated as documented for the identical
%un logformat code. Exact contents may vary depending on what details
are available at the time the ACL is tested.



* SSL certificate database corruption

The ssl_crtd helper occasionally discovers that its backend disk store
has become corrupted. A number of potential reasons have been identified
for this.

Some of those reasons have been fully solved. Extra validation checks
and automatic recovery procedures are added to resolve others.

The problem may remain for some installations but this release should be
a lot more resilient for most using the ssl_crtd helper.

Work is ongoing with this set of problems. Please stay in touch about
ssl_crtd issues in this or later releases.



* TLS: Splice to origin cache_peer.

When ssl-bump splice action is selected Squid can now relay the traffic
to a cache_peer configured with the 'originserver' option.

SNI and other certificate information received from the client is sent
to the peer exactly as it would have been on a DIRECT origin connection.



* TLS: HTTP error reponses served using invalid certificates when
  dealing with SSL server errors.

When ssl-bump bump action is performed this bug would cause cryptic
certificate errors to be presented to users. A Squid-generated error
"page" to be sent over a secure connection would be sent with an
incorrect Squid-generated server certificate.



* IPv6: improve BCP 177 compliance

Since early 2012 it has been mandatory for new or upgraded Internet
connected machinery and software to support IPv6 ad use it in preference
over IPv4.

Squid IPv6 behaviour has followed these practices since well before the
guidelines became a BCP. Over the years it has also grown into a
well-tested and widely used feature.

The --disable-ipv6 build option is now deprecated. It is long past time
to fix whatever network brokenness you may have that made it look
attractive in past years.

Squid-3.5.7 and later will perform IPv6 availability tests on startup in
all builds.

 - Where IPv6 is unavailable Squid will continue exactly as it would
have had the build option not been used.

   These Squid can have the build option removed now.

 - Where IPv6 is detected but --disable-ipv6 prevents use Squid will log
"WARNING: BCP 177 violation".

    Please test whether you can rebuild with IPv6 enabled.



* Perl pod2man is now optional

Several of the perl based helpers bundled with Squid have previously
been requiring the pod2man documentation generator before they will build.

Since it is only used to create documentation that tool is not optional
and these helpers may be built and installed on any system containing
just a Perl installation.



* basic_smb_auth issues with Samba 4

The basic_smb_auth helper has been identified as having several issues
authenticating with Samba 4 smbclient or any networks containing WINS
servers. Those are now fixed.




 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Aug  6 10:14:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Aug 2015 22:14:02 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.7 is available
In-Reply-To: <55C2E6E5.30207@treenet.co.nz>
References: <55C2E6E5.30207@treenet.co.nz>
Message-ID: <55C3336A.6020604@treenet.co.nz>

[ cc'ing squid-users in case anyone gets confused ]

On 6/08/2015 4:47 p.m., Amos Jeffries wrote:
>
> * Perl pod2man is now optional
> 
> Several of the perl based helpers bundled with Squid have previously
> been requiring the pod2man documentation generator before they will build.
> 
> Since it is only used to create documentation that tool is not optional
> and these helpers may be built and installed on any system containing
> just a Perl installation.
> 

On 6/08/2015 9:16 p.m., Eliezer Croitoru wrote:
>
> Did you mean "that tool is now" optional and not as mentioned about as
> "that tool is not"?
>

Yes, sorry. That was a typo.

The section title is correct:  the tool is *now* optional.


Amos



From eliezer at ngtech.co.il  Thu Aug  6 10:53:35 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 6 Aug 2015 13:53:35 +0300
Subject: [squid-users] LDAP related question.
In-Reply-To: <CAARxGtiURnF_3xv6QX5cWqXUch=NdTgRw4dK4aG7Pr93-DinCA@mail.gmail.com>
References: <CAARxGthe4iUvydQgsDgf_7o8Ai-uf8HuYg2Kcc4Wbb_G+fkLEw@mail.gmail.com>
 <55BB86A7.5030008@ngtech.co.il>
 <CAARxGtiURnF_3xv6QX5cWqXUch=NdTgRw4dK4aG7Pr93-DinCA@mail.gmail.com>
Message-ID: <55C33CAF.8030800@ngtech.co.il>

I also now found that the example for ldap search in squidguard is 
similar to my conclusion.
http://www.squidguard.org/Doc/authentication.html
##START
  ldapbinddn     cn=root, dc=example, dc=com
  ldapbindpass   myultrasecretpassword

  # ldap cache time in seconds
  ldapcachetime  300

  src my_users {
      ldapusersearch 
ldap://ldap.example.com/cn=squidguardusers,ou=groups,dc=example,dc=com?memberUid?sub?(&(objectclass=posixGroup)(memberUid=%s))
   }

  src allowed_ips {
      ldapipsearch 
ldap://ldap.example.com/ou=internetcomputers,dc=example,dc=com?iphostnumber?sub?(&(objectclass=iphost)(iphostnumber=%s))
  }
##END

Eliezer

On 31/07/2015 17:36, brendan kearney wrote:
> Not near my gear and notes, but will get you what I have later.



From squid3 at treenet.co.nz  Thu Aug  6 12:11:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Aug 2015 00:11:43 +1200
Subject: [squid-users] I want to display indirect_client_address on
 error page.
In-Reply-To: <20150806.142748.68559068.asakura@ioc.dnp.co.jp>
References: <20150805.161644.68539936.asakura@ioc.dnp.co.jp>
 <55C2AC6B.4030401@treenet.co.nz>
 <20150806.142748.68559068.asakura@ioc.dnp.co.jp>
Message-ID: <55C34EFF.9050500@treenet.co.nz>

On 6/08/2015 5:27 p.m., Kazuhiro Asakura wrote:
> Thank you Amos,
> 
> From: Amos Jeffries
>>> I use "follow-x-forwarded-for" feature, and logged end point IP address.
>>> Also, I created customized error page for end user. but, env %i of
>>> error page does not be translate to end point IP address.
>>
>> More correctly it is the direct endpoint of the client TCP connection.
>> Not the chain of multiple TCP connections, which may include NAT or
>> other protocol translations between Squid and a user.
> 
> Sorry, My description not enough.
> 
> In this environment, BIG-IP exist between end point PC and Squid.
> BIG-IP do NAT, and add X-FORWARDED-FOR header.
> BIG-IP include IP address of endpoint PC for X-FORWARDED-FOR header.
> 
> So, I use follow-x-forwarded-for feature, and get endpoint PC IP address .

Okay.

> 
> This time, endpoint PC IP address is written into Squid access.log.
> 
> Maybe, source code is below,
> 
> AccessLogEntry.cc
> ----
> #if FOLLOW_X_FORWARDED_FOR
>     if (Config.onoff.log_uses_indirect_client && request)
>         log_ip = request->indirect_client_addr;
>     else
> #endif
> ----
> 

Yes. Correct.

> But, it is not assigned to environment variable of error page.

Correct. That is a bug, but not easily solved.


> 
> If environment variable of error page could support "indirect_client_addr",
> I would really appreciate it.
> 

Understood.


>>> I want to display the address of end point to error page.
>>> If someone could suggest solution, I would really appreciate it.
>>
>> The easiest approach I think would be to add a javascript snippet in
>> place of the %i code that locates the users IP address and displays it
>> to them.
> 
> I feel that way is difficult.
> Because, solution of javascript only works JScript, or require STUN server.
> 

STUN ? I dont understand why that would be relevant.

If the error message is really for client reading then their brower
already is aware of its own IP address and should be able to display
that with javascript/Jscript. All modern browsers have that capability.

If the error details is for your internal use and not the client, then
sending the IP in the body of an error page is the wrong thing to do. A
custom header would be better for most uses. X-Client-IP is used already
by some software.



Amos


From dan at getbusi.com  Thu Aug  6 23:44:07 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 7 Aug 2015 09:44:07 +1000
Subject: [squid-users] Why is overlapping dstdomains a FATAL error now?
Message-ID: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>

This used to just cause a WARNING right? Is this really a good enough reason to stop Squid from starting up?

2015/08/07 09:25:43| ERROR: '.ssl.gstatic.com <http://ssl.gstatic.com/>' is a subdomain of '.gstatic.com <http://gstatic.com/>'
2015/08/07 09:25:43| ERROR: You need to remove '.ssl.gstatic.com <http://ssl.gstatic.com/>' from the ACL named 'cache_bypass_domains'
FATAL: Bungled /etc/squid/squid.conf line 149: acl cache_bypass_domains dstdomain "/acls/lists/8/squid_domains?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150807/43f128d5/attachment.htm>

From webmaster at squidblacklist.org  Thu Aug  6 23:48:36 2015
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Thu, 6 Aug 2015 18:48:36 -0500
Subject: [squid-users] Why is overlapping dstdomains a FATAL error now?
In-Reply-To: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>
References: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>
Message-ID: <55C3F254.2070905@squidblacklist.org>

Agreed, whoever decided it was a wise decision to make this a stop error 
should be fired or at the very least, slapped in the back of the head.

On 8/6/2015 6:44 PM, Dan Charlesworth wrote:
> This used to just cause a WARNING right? Is this really a good enough 
> reason to stop Squid from starting up?
>
> 2015/08/07 09:25:43| ERROR: '.ssl.gstatic.com <http://ssl.gstatic.com/>' is a subdomain of '.gstatic.com <http://gstatic.com/>'
> 2015/08/07 09:25:43| ERROR: You need to remove '.ssl.gstatic.com <http://ssl.gstatic.com/>' from the ACL named 'cache_bypass_domains'
> FATAL: Bungled /etc/squid/squid.conf line 149: acl cache_bypass_domains dstdomain "/acls/lists/8/squid_domains?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Signed,

Benjamin E. Nichols
http://www.squidblacklist.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150806/73502784/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug  7 03:26:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Aug 2015 15:26:14 +1200
Subject: [squid-users] Why is overlapping dstdomains a FATAL error now?
In-Reply-To: <55C3F254.2070905@squidblacklist.org>
References: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>
 <55C3F254.2070905@squidblacklist.org>
Message-ID: <55C42556.4080807@treenet.co.nz>

On 7/08/2015 11:48 a.m., Benjamin E. Nichols wrote:
> Agreed, whoever decided it was a wise decision to make this a stop error
> should be fired or at the very least, slapped in the back of the head.
> 
> On 8/6/2015 6:44 PM, Dan Charlesworth wrote:
>> This used to just cause a WARNING right? Is this really a good enough
>> reason to stop Squid from starting up?
>>
>> 2015/08/07 09:25:43| ERROR: '.ssl.gstatic.com
>> <http://ssl.gstatic.com/>' is a subdomain of '.gstatic.com
>> <http://gstatic.com/>'
>> 2015/08/07 09:25:43| ERROR: You need to remove '.ssl.gstatic.com
>> <http://ssl.gstatic.com/>' from the ACL named 'cache_bypass_domains'
>> FATAL: Bungled /etc/squid/squid.conf line 149: acl
>> cache_bypass_domains dstdomain "/acls/lists/8/squid_domains?
>>

It *seems* very daft. But there actually is a very good reason.

Squid stores these data into a splay tree structure as it goes. Adding
to a splay tree is a one-way operation. There is no remove short of
dumping the entire squid.conf and re-configuring.

[ just a side note: we *are* actively in the process of trying to
eradicate these splay trees and use another type of fast hash we could
remove from. ]

There are several cases for what the loaded file may contain:

A)
 .gstatic.com
 .ssl.gstatic.com

B)
 .gstatic.com
 .gstatic.com

C)
 .gstatic.com
 gstatic.com

D)
 .ssl.gstatic.com
 .gstatic.com

E)
 gstatic.com
 .gstatic.com


Cases (A, B, C) will happily go along and add .gstatic.com to the ACL.
The next one is an overlap with a more narrow matching range than
already in the tree.
 These *are* nicely displayed as WARNING, and just ignored by Squid as
it continues to run. If you put "acl all src 0.0.0.0" into a squid-3
config you can see that happen.


Cases (D, E) are special. The first entry already added to the ACL splay
will match *less* than the second one.
 Now the way using a splay tree works is that for any lookup one of them
will be 'closer' and thus match. But not always the same one.
 So adding both to the splay will cause domains in the non-overlap area
to randomly be blocked or allowed.
 These are what you are seeing displayed as ERROR.

So then we get back to how restarting the entire reconfigure process
from scratch is needed to remove the first entry from the splay tree.

---> Doing a reconfigure will only load the same details in the same
order unless manually fixed first.

Squid must halt and wait for you to fix this. If it is left running the
config *will not* be doing what you intended Squid to do. That is FATAL.


And BTW, finding these issues in manually edited or third-party lists is
one of the reasons one should always run "squid -k parse" before loading
new config.

Amos


From dan at getbusi.com  Fri Aug  7 03:55:50 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 7 Aug 2015 13:55:50 +1000
Subject: [squid-users] Why is overlapping dstdomains a FATAL error now?
In-Reply-To: <55C42556.4080807@treenet.co.nz>
References: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>
 <55C3F254.2070905@squidblacklist.org> <55C42556.4080807@treenet.co.nz>
Message-ID: <15B8C20D-DD46-4715-AC1A-824A736700FE@getbusi.com>

Huh! Thanks for that, Amos. 

Our software actually flags the redundant entries and doesn't write those ones out, just that I realised I didn?t really understand ?why? squid does that and why we need to work around it in the first place.

Doing a `-k parse` before loading any new changes is super good advice though. We?ll be implementing that failsafe for sure.

> On 7 Aug 2015, at 1:26 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 7/08/2015 11:48 a.m., Benjamin E. Nichols wrote:
>> Agreed, whoever decided it was a wise decision to make this a stop error
>> should be fired or at the very least, slapped in the back of the head.
>> 
>> On 8/6/2015 6:44 PM, Dan Charlesworth wrote:
>>> This used to just cause a WARNING right? Is this really a good enough
>>> reason to stop Squid from starting up?
>>> 
>>> 2015/08/07 09:25:43| ERROR: '.ssl.gstatic.com
>>> <http://ssl.gstatic.com/>' is a subdomain of '.gstatic.com
>>> <http://gstatic.com/>'
>>> 2015/08/07 09:25:43| ERROR: You need to remove '.ssl.gstatic.com
>>> <http://ssl.gstatic.com/>' from the ACL named 'cache_bypass_domains'
>>> FATAL: Bungled /etc/squid/squid.conf line 149: acl
>>> cache_bypass_domains dstdomain "/acls/lists/8/squid_domains?
>>> 
> 
> It *seems* very daft. But there actually is a very good reason.
> 
> Squid stores these data into a splay tree structure as it goes. Adding
> to a splay tree is a one-way operation. There is no remove short of
> dumping the entire squid.conf and re-configuring.
> 
> [ just a side note: we *are* actively in the process of trying to
> eradicate these splay trees and use another type of fast hash we could
> remove from. ]
> 
> There are several cases for what the loaded file may contain:
> 
> A)
> .gstatic.com
> .ssl.gstatic.com
> 
> B)
> .gstatic.com
> .gstatic.com
> 
> C)
> .gstatic.com
> gstatic.com
> 
> D)
> .ssl.gstatic.com
> .gstatic.com
> 
> E)
> gstatic.com
> .gstatic.com
> 
> 
> Cases (A, B, C) will happily go along and add .gstatic.com to the ACL.
> The next one is an overlap with a more narrow matching range than
> already in the tree.
> These *are* nicely displayed as WARNING, and just ignored by Squid as
> it continues to run. If you put "acl all src 0.0.0.0" into a squid-3
> config you can see that happen.
> 
> 
> Cases (D, E) are special. The first entry already added to the ACL splay
> will match *less* than the second one.
> Now the way using a splay tree works is that for any lookup one of them
> will be 'closer' and thus match. But not always the same one.
> So adding both to the splay will cause domains in the non-overlap area
> to randomly be blocked or allowed.
> These are what you are seeing displayed as ERROR.
> 
> So then we get back to how restarting the entire reconfigure process
> from scratch is needed to remove the first entry from the splay tree.
> 
> ---> Doing a reconfigure will only load the same details in the same
> order unless manually fixed first.
> 
> Squid must halt and wait for you to fix this. If it is left running the
> config *will not* be doing what you intended Squid to do. That is FATAL.
> 
> 
> And BTW, finding these issues in manually edited or third-party lists is
> one of the reasons one should always run "squid -k parse" before loading
> new config.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From asakura at ioc.dnp.co.jp  Fri Aug  7 09:53:28 2015
From: asakura at ioc.dnp.co.jp (Kazuhiro Asakura)
Date: Fri, 07 Aug 2015 18:53:28 +0900 (JST)
Subject: [squid-users] I want to display indirect_client_address on
 error page.
In-Reply-To: <55C34EFF.9050500@treenet.co.nz>
References: <55C2AC6B.4030401@treenet.co.nz>
 <20150806.142748.68559068.asakura@ioc.dnp.co.jp>
 <55C34EFF.9050500@treenet.co.nz>
Message-ID: <20150807.185328.68556267.asakura@ioc.dnp.co.jp>

Thank you Amos, again.

I will investigate solution of javascript again.

But, next week is holiday, Called "O-bon" in Japanese.
Maybe reply will late. please pardon.

> If the error details is for your internal use and not the client, then
> sending the IP in the body of an error page is the wrong thing to do. A
> custom header would be better for most uses. X-Client-IP is used already
> by some software.

I use custom error page for Helpdesk work.
Our employee are low IT literacy. So, they can not confirm own
IP address by themselves.

regards,
Kazuhiro


From: Amos Jeffries <squid3 at treenet.co.nz>
Subject: Re: [squid-users] I want to display indirect_client_address on error page.
Date: Fri, 7 Aug 2015 00:11:43 +1200

> On 6/08/2015 5:27 p.m., Kazuhiro Asakura wrote:
> > Thank you Amos,
> > 
> > From: Amos Jeffries
> >>> I use "follow-x-forwarded-for" feature, and logged end point IP address.
> >>> Also, I created customized error page for end user. but, env %i of
> >>> error page does not be translate to end point IP address.
> >>
> >> More correctly it is the direct endpoint of the client TCP connection.
> >> Not the chain of multiple TCP connections, which may include NAT or
> >> other protocol translations between Squid and a user.
> > 
> > Sorry, My description not enough.
> > 
> > In this environment, BIG-IP exist between end point PC and Squid.
> > BIG-IP do NAT, and add X-FORWARDED-FOR header.
> > BIG-IP include IP address of endpoint PC for X-FORWARDED-FOR header.
> > 
> > So, I use follow-x-forwarded-for feature, and get endpoint PC IP address .
> 
> Okay.
> 
> > 
> > This time, endpoint PC IP address is written into Squid access.log.
> > 
> > Maybe, source code is below,
> > 
> > AccessLogEntry.cc
> > ----
> > #if FOLLOW_X_FORWARDED_FOR
> >     if (Config.onoff.log_uses_indirect_client && request)
> >         log_ip = request->indirect_client_addr;
> >     else
> > #endif
> > ----
> > 
> 
> Yes. Correct.
> 
> > But, it is not assigned to environment variable of error page.
> 
> Correct. That is a bug, but not easily solved.
> 
> 
> > 
> > If environment variable of error page could support "indirect_client_addr",
> > I would really appreciate it.
> > 
> 
> Understood.
> 
> 
> >>> I want to display the address of end point to error page.
> >>> If someone could suggest solution, I would really appreciate it.
> >>
> >> The easiest approach I think would be to add a javascript snippet in
> >> place of the %i code that locates the users IP address and displays it
> >> to them.
> > 
> > I feel that way is difficult.
> > Because, solution of javascript only works JScript, or require STUN server.
> > 
> 
> STUN ? I dont understand why that would be relevant.
> 
> If the error message is really for client reading then their brower
> already is aware of its own IP address and should be able to display
> that with javascript/Jscript. All modern browsers have that capability.
> 
> If the error details is for your internal use and not the client, then
> sending the IP in the body of an error page is the wrong thing to do. A
> custom header would be better for most uses. X-Client-IP is used already
> by some software.
> 
> 
> 
> Amos



From rousskov at measurement-factory.com  Fri Aug  7 14:54:57 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 07 Aug 2015 08:54:57 -0600
Subject: [squid-users] Why is overlapping dstdomains a FATAL error now?
In-Reply-To: <55C42556.4080807@treenet.co.nz>
References: <46F3DF29-E7B9-4563-930E-8C465DCBA432@getbusi.com>
 <55C3F254.2070905@squidblacklist.org> <55C42556.4080807@treenet.co.nz>
Message-ID: <55C4C6C1.3080405@measurement-factory.com>

On 08/06/2015 09:26 PM, Amos Jeffries wrote:
> On 7/08/2015 11:48 a.m., Benjamin E. Nichols wrote:
>> Agreed, whoever decided it was a wise decision to make this a stop error
>> should be fired or at the very least, slapped in the back of the head.
>>
>> On 8/6/2015 6:44 PM, Dan Charlesworth wrote:
>>> This used to just cause a WARNING right? Is this really a good enough
>>> reason to stop Squid from starting up?
>>>
>>> 2015/08/07 09:25:43| ERROR: '.ssl.gstatic.com
>>> <http://ssl.gstatic.com/>' is a subdomain of '.gstatic.com
>>> <http://gstatic.com/>'
>>> 2015/08/07 09:25:43| ERROR: You need to remove '.ssl.gstatic.com
>>> <http://ssl.gstatic.com/>' from the ACL named 'cache_bypass_domains'
>>> FATAL: Bungled /etc/squid/squid.conf line 149: acl
>>> cache_bypass_domains dstdomain "/acls/lists/8/squid_domains?
>>>
> 
> It *seems* very daft. But there actually is a very good reason.
> 
> Squid stores these data into a splay tree structure as it goes. Adding
> to a splay tree is a one-way operation. There is no remove short of
> dumping the entire squid.conf and re-configuring.


Are you sure "dumping the entire squid.conf and re-configuring" is the
only way?

AFAICT, if I have a splay tree A, I can remove any node from that tree
by creating another splay tree B and inserting all A's nodes except for
the one I want to remove and then replacing A with B. Sure, that kind of
update will take some extra time and extra RAM, but it does not sound
like a bad solution, especially compared to killing Squid when we used
to accept ambiguous configurations before.

Alex.



From eliezer at ngtech.co.il  Fri Aug  7 15:15:30 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 7 Aug 2015 18:15:30 +0300
Subject: [squid-users] I want to display indirect_client_address on
 error page.
In-Reply-To: <20150807.185328.68556267.asakura@ioc.dnp.co.jp>
References: <55C2AC6B.4030401@treenet.co.nz>
 <20150806.142748.68559068.asakura@ioc.dnp.co.jp>
 <55C34EFF.9050500@treenet.co.nz>
 <20150807.185328.68556267.asakura@ioc.dnp.co.jp>
Message-ID: <55C4CB92.5070702@ngtech.co.il>

Does the client has the option to access some internal webserver which 
can reflect the IP address of the client?
If so.. you can redirect from an error page to it.

Eliezer

On 07/08/2015 12:53, Kazuhiro Asakura wrote:
> Thank you Amos, again.
>
> I will investigate solution of javascript again.
>
> But, next week is holiday, Called "O-bon" in Japanese.
> Maybe reply will late. please pardon.
>
>> If the error details is for your internal use and not the client, then
>> sending the IP in the body of an error page is the wrong thing to do. A
>> custom header would be better for most uses. X-Client-IP is used already
>> by some software.
>
> I use custom error page for Helpdesk work.
> Our employee are low IT literacy. So, they can not confirm own
> IP address by themselves.
>
> regards,
> Kazuhiro
>
>
> From: Amos Jeffries <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] I want to display indirect_client_address on error page.
> Date: Fri, 7 Aug 2015 00:11:43 +1200
>
>> On 6/08/2015 5:27 p.m., Kazuhiro Asakura wrote:
>>> Thank you Amos,
>>>
>>> From: Amos Jeffries
>>>>> I use "follow-x-forwarded-for" feature, and logged end point IP address.
>>>>> Also, I created customized error page for end user. but, env %i of
>>>>> error page does not be translate to end point IP address.
>>>>
>>>> More correctly it is the direct endpoint of the client TCP connection.
>>>> Not the chain of multiple TCP connections, which may include NAT or
>>>> other protocol translations between Squid and a user.
>>>
>>> Sorry, My description not enough.
>>>
>>> In this environment, BIG-IP exist between end point PC and Squid.
>>> BIG-IP do NAT, and add X-FORWARDED-FOR header.
>>> BIG-IP include IP address of endpoint PC for X-FORWARDED-FOR header.
>>>
>>> So, I use follow-x-forwarded-for feature, and get endpoint PC IP address .
>>
>> Okay.
>>
>>>
>>> This time, endpoint PC IP address is written into Squid access.log.
>>>
>>> Maybe, source code is below,
>>>
>>> AccessLogEntry.cc
>>> ----
>>> #if FOLLOW_X_FORWARDED_FOR
>>>      if (Config.onoff.log_uses_indirect_client && request)
>>>          log_ip = request->indirect_client_addr;
>>>      else
>>> #endif
>>> ----
>>>
>>
>> Yes. Correct.
>>
>>> But, it is not assigned to environment variable of error page.
>>
>> Correct. That is a bug, but not easily solved.
>>
>>
>>>
>>> If environment variable of error page could support "indirect_client_addr",
>>> I would really appreciate it.
>>>
>>
>> Understood.
>>
>>
>>>>> I want to display the address of end point to error page.
>>>>> If someone could suggest solution, I would really appreciate it.
>>>>
>>>> The easiest approach I think would be to add a javascript snippet in
>>>> place of the %i code that locates the users IP address and displays it
>>>> to them.
>>>
>>> I feel that way is difficult.
>>> Because, solution of javascript only works JScript, or require STUN server.
>>>
>>
>> STUN ? I dont understand why that would be relevant.
>>
>> If the error message is really for client reading then their brower
>> already is aware of its own IP address and should be able to display
>> that with javascript/Jscript. All modern browsers have that capability.
>>
>> If the error details is for your internal use and not the client, then
>> sending the IP in the body of an error page is the wrong thing to do. A
>> custom header would be better for most uses. X-Client-IP is used already
>> by some software.
>>
>>
>>
>> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From chip_pop at hotmail.com  Fri Aug  7 18:31:01 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 7 Aug 2015 11:31:01 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <55ABC4D6.6090307@treenet.co.nz>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <55ABC4D6.6090307@treenet.co.nz>
Message-ID: <1438972261900-4672635.post@n4.nabble.com>

amos  i get as well same alot
TCP_SWAPFAIL_MISS/200  pls guys look at that bug tks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672635.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Fri Aug  7 22:38:36 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 7 Aug 2015 15:38:36 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1438972261900-4672635.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <55ABC4D6.6090307@treenet.co.nz> <1438972261900-4672635.post@n4.nabble.com>
Message-ID: <1438987116031-4672636.post@n4.nabble.com>

yea joe i dont know why ppl dnt give this bug importance while it deduce a
lot of hit ratio



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672636.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Aug  8 07:24:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Aug 2015 19:24:34 +1200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1438987116031-4672636.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <55ABC4D6.6090307@treenet.co.nz> <1438972261900-4672635.post@n4.nabble.com>
 <1438987116031-4672636.post@n4.nabble.com>
Message-ID: <55C5AEB2.7020002@treenet.co.nz>

On 8/08/2015 10:38 a.m., HackXBack wrote:
> yea joe i dont know why ppl dnt give this bug importance while it deduce a
> lot of hit ratio
> 

yea. um ... isn't that your job ?

Answering the question of:
  "Why cant Squid read this file off disk?"

sure sounds like something the admin in charge is responsible for
finding out.


The "ppl" working on Squid are consultants, fixing first the bugs our
customers pay to get fixed ASAP. And/or hobbyists, in our spare time
fixing the ones we want to fix, just for fun (...F ...U ...N).

Anyhow. No less than three of us main developers and a few of the
downstream distro guys have already spent a fair amount of time (paid
and unpaid) fixing the bugs inside Squid we identified as even
_possibly_ leading to SWAPFAIL.

In the end it might simply be your HDD dying. Nothing *we* can do about
that.

You or the init scripts could be sending Squid KILL signals instead of
shutdown (SIGHUP) signals. Also something for you to fix, not us.

IIRC HackXBack is dealing with huge caches and forced short shutdown
times. That is one recipe for SWAPFAIL, for known reasons. And is being
worked on by hobbyists in spare time because despite many complaints
*nobody with money has ever hired anyone to fix that group of failures*.
BUT. It is still unknown if that is actually HackXback's problem,
several bug fixes in the latest Squid so far have apparently made no
dent it.


What can be done by others IS being done by others. In free software
everyone plays a part. Especially you (for all values of 'you' reading
this).


So "we" need you to provide details, lots of details, about what exactly
is happening and answer the question of 'why your Squid can't load the
files off disk?'

Maybe you solve it yourself in the process of investigating. (yay!)

 No joke. This has happened repeatedly in the past already. Bad HDD
firmware, broken FS system drivers, dying HDD, force-kill init scripts,
bad NFS network transfers, even a bad Intranet router firmware once. All
found and fixed by admin looking into SWAPFAIL. Squid is usually just
the messenger with SWAPFAIL.


Whatever happens the answer and research sheds light on the problem, and
only then it can possibly be fixed somehow. Without that answer we are
just stabbing about blindly in the dark trying to swat a flying bug.


Cheers
Amos



From Hullen at t-online.de  Sat Aug  8 09:27:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Sat, 8 Aug 2015 11:27:00 +0200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1438987116031-4672636.post@n4.nabble.com>
Message-ID: <DMU$bFZeCXB@helmut.hullen.de>

Hallo, HackXBack,

Du meintest am 07.08.15:

> yea joe i dont know why ppl dnt give this bug importance while it
> deduce alot of hit ratio

Can you please translate this kind of pidgin english into usual written  
english, to please all foreign readers who have only learned this  
written english? Thank you!

Please excuse my gerlish.

Viele Gruesse!
Helmut



From 1508 at notleb.com  Sat Aug  8 14:20:24 2015
From: 1508 at notleb.com (1508)
Date: Sat, 8 Aug 2015 07:20:24 -0700 (PDT)
Subject: [squid-users] Caching requests to a non standard http port help
	please.
Message-ID: <1439043624158-4672639.post@n4.nabble.com>

Hello,

I would like to configure squid-cache to cache http requests to port 10050
on a certain server such as http://www.example.com:10050/picture.png

I have a working perl script to rewrite requests to the standard port 80 but
not the 10050 port.

Could somebody please help me to get squid-cache to cache GET requests to
http://www.example.com:10050/picture.png

Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Caching-requests-to-a-non-standard-http-port-help-please-tp4672639.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sat Aug  8 14:40:49 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 8 Aug 2015 16:40:49 +0200
Subject: [squid-users] Caching requests to a non standard http port help
	please.
In-Reply-To: <1439043624158-4672639.post@n4.nabble.com>
References: <1439043624158-4672639.post@n4.nabble.com>
Message-ID: <201508081640.49854.Antony.Stone@squid.open.source.it>

On Saturday 08 August 2015 at 16:20:24, 1508 wrote:

> Hello,
> 
> I would like to configure squid-cache to cache http requests to port 10050
> on a certain server such as http://www.example.com:10050/picture.png

So long as 10050 isn't listed as an unsafe port in your Squid config, it'll do 
this by default.

> I have a working perl script to rewrite requests to the standard port 80
> but not the 10050 port.

Why do you have a rewrite script?  What are you rewriting from?

> Could somebody please help me to get squid-cache to cache GET requests to
> http://www.example.com:10050/picture.png

What is it about the standard default Squid configuration which doesn't just do 
this for you?

I suspect I don't really understand what you're trying to do, or what you've 
already tried and what didn't work - perhaps you could give a bit more detail?


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From 1508 at notleb.com  Sun Aug  9 16:27:07 2015
From: 1508 at notleb.com (1508)
Date: Sun, 9 Aug 2015 09:27:07 -0700 (PDT)
Subject: [squid-users] Caching requests to a non standard http port help
	please.
In-Reply-To: <201508081640.49854.Antony.Stone@squid.open.source.it>
References: <1439043624158-4672639.post@n4.nabble.com>
 <201508081640.49854.Antony.Stone@squid.open.source.it>
Message-ID: <1439137627816-4672641.post@n4.nabble.com>

Hello Anthony,

Thank you for your reply.

My quest is to try and cache stuff from a site called Second Life.

My bandwidth is poor and I'm trying to cache large objects that regulary get
re-requested.

The thing is part of the url will change, however the content willl not.

Here are some examples of urls I am trying to cache:

http://sim9077.agni.lindenlab.com:12046/cap/db301865-157f-89d3-a98f-34acc5c9537a
http://sim17097.agni.lindenlab.com:12046/cap/db301865-157f-89d3-a98f-34acc5c9537a

OK, some answers.

Port 10050 was an example. 

acl Safe_ports port 80		# http
acl Safe_ports port 443	# https
acl Safe_ports port 12043 # LL port
acl Safe_ports port 12046 # LL port

I am trying to rewrite part of the URL because the same data gets fetched
from different computers. So
in the RLs above the same item can be sent twice.

I cannot see how the standard configuration can assume part of a URL is a
wildcard.

I have managed to rewrite static URL domain names and just change a ? into a
/ so items get stored OK.

Thanks, Ben.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Caching-requests-to-a-non-standard-http-port-help-please-tp4672639p4672641.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Aug 10 04:08:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Aug 2015 16:08:05 +1200
Subject: [squid-users] Caching requests to a non standard http port help
 please.
In-Reply-To: <1439137627816-4672641.post@n4.nabble.com>
References: <1439043624158-4672639.post@n4.nabble.com>
 <201508081640.49854.Antony.Stone@squid.open.source.it>
 <1439137627816-4672641.post@n4.nabble.com>
Message-ID: <55C823A5.30609@treenet.co.nz>

On 10/08/2015 4:27 a.m., 1508 wrote:
> Hello Anthony,
> 
> Thank you for your reply.
> 
> My quest is to try and cache stuff from a site called Second Life.
> 
> My bandwidth is poor and I'm trying to cache large objects that regulary get
> re-requested.
> 
> The thing is part of the url will change, however the content willl not.
> 
> Here are some examples of urls I am trying to cache:
> 
> http://sim9077.agni.lindenlab.com:12046/cap/db301865-157f-89d3-a98f-34acc5c9537a
> http://sim17097.agni.lindenlab.com:12046/cap/db301865-157f-89d3-a98f-34acc5c9537a
> 
> OK, some answers.
> 
> Port 10050 was an example. 
> 
> acl Safe_ports port 80		# http
> acl Safe_ports port 443	# https
> acl Safe_ports port 12043 # LL port
> acl Safe_ports port 12046 # LL port
> 

The definition for Safe_ports provided with clean installs contains the
entry:
  acl Safe_ports port 1025-65535

The only ports which are unsafe are those used by protocols such as
email where the syntax overlaps with HTTP in dangerous ways. High
numbered ports do not usually have that problem.


> I am trying to rewrite part of the URL because the same data gets fetched
> from different computers. So
> in the RLs above the same item can be sent twice.
> 

DO NOT re-write the URL for that.

Instead use a Store-ID helper to set the internal Squid cache ID for the
object. They work very similar to url-rewriting, so you should be able
to adjust your helper easily.


> I cannot see how the standard configuration can assume part of a URL is a
> wildcard.

Those URLs above are a perfect example of why you should not be
URL-rewriting. Or even assuming that they are the same object coming back.

For me the top URL produces a 404 message saying the thing does not
exist. The bottom one produces something else.

If you were re-writing (and caching) the bad hash to point at the top
server, all visitors would get the 404 message while it was
force-cached. Nobody would be able to get to the working second URL
server. Even if they explicitly asked for that second one.


With Store-ID the helper just tells Squid that both objects are to be
stored in the same cache location (a URI which you provide from the
helper). When anybody asks for one of those the cache location is
checked, and if not found the URL *they asked for* is fetched and cached.

So, visitors who ask for URL #1 will get 404 until somebody asks for URL
#2 and gets the object cached. After which both URLs work using the
stored data from URL #2.

Much nicer.


There is one huge requirement that you need to pay very close attention
to. For any two objects to be collapsed they need to have both semantic
and binary equivalence to each other. That means the use by the client
has to be the same, and the binary form of the object has to be storing
the same details.

For games and REST APIs (things like Second Life), what appears to you
to be the same URL re-fetched constantly may actually be a constantly
changing object with large amounts of game environment updates in it. Or
just different encodings of the object being requested by decoders.
Passing the wrong version of object out could break things in hidden
ways. Be VERY careful what objects you combine.



> 
> I have managed to rewrite static URL domain names and just change a ? into a
> / so items get stored OK.

Not okay. Items after the '?' are variable in what order they can exist
(different order, different cache location). And in what characters are
allowed to be used there.

 Absolutely dont do that unless you are in control of the origin server.
And then you can have the origin tell visitors URLs without the '?' in
the first place.


Hope this helps, and good luck.

Amos


From tomtux007 at gmail.com  Mon Aug 10 06:23:39 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 10 Aug 2015 08:23:39 +0200
Subject: [squid-users] Include XML-File in squid.conf (for ex. allowing
	without authentication...)
Message-ID: <CACLJR+Pvkso-8b2SgsAoQr-e2AOwXJiZOfXUKkhBHynngWB4rA@mail.gmail.com>

Hi

M$ provides a XML-List of IP-Addresses and Domain-Names, which should
be accessible for Office365. Look here:
https://support.content.office.net/en-us/static/O365IPAddresses.xml

Is there a way to include such a XML-File in squid for ex. allow the
mentioned IPs/Domains without authentication? Or is the way to go to
extract the necessary parts of the file with GNU grep/awk/sed... and
include them with an common ACL-directive?

Many thanks.
Kind regards,
Tom


From squid3 at treenet.co.nz  Mon Aug 10 09:35:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Aug 2015 21:35:24 +1200
Subject: [squid-users] Include XML-File in squid.conf (for ex. allowing
 without authentication...)
In-Reply-To: <CACLJR+Pvkso-8b2SgsAoQr-e2AOwXJiZOfXUKkhBHynngWB4rA@mail.gmail.com>
References: <CACLJR+Pvkso-8b2SgsAoQr-e2AOwXJiZOfXUKkhBHynngWB4rA@mail.gmail.com>
Message-ID: <55C8705C.9020005@treenet.co.nz>

On 10/08/2015 6:23 p.m., Tom Tom wrote:
> Hi
> 
> M$ provides a XML-List of IP-Addresses and Domain-Names, which should
> be accessible for Office365. Look here:
> https://support.content.office.net/en-us/static/O365IPAddresses.xml
> 
> Is there a way to include such a XML-File in squid for ex. allow the
> mentioned IPs/Domains without authentication? Or is the way to go to
> extract the necessary parts of the file with GNU grep/awk/sed... and
> include them with an common ACL-directive?

Yes, extract what you need from their cystom format and import as plain
CIDR or dstdomain lists appropriate to what ACL you want.

Anyone have an idea why its not authenticating in the first place?

Amos



From dan at djph.net  Mon Aug 10 10:13:11 2015
From: dan at djph.net (Dan Purgert)
Date: Mon, 10 Aug 2015 06:13:11 -0400
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <DMU$bFZeCXB@helmut.hullen.de>
References: <DMU$bFZeCXB@helmut.hullen.de>
Message-ID: <20150810061311.Horde.Bny3FY_8KdtJsknX0ksZZw5@192.168.10.20>

Quoting Helmut Hullen <Hullen at t-online.de>:

> Hallo, HackXBack,
>
> Du meintest am 07.08.15:
>
>> yea joe i dont know why ppl dnt give this bug importance while it
>> deduce alot of hit ratio
>
> Can you please translate this kind of pidgin english into usual written
> english, to please all foreign readers who have only learned this
> written english? Thank you!
>
> Please excuse my gerlish.
>
> Viele Gruesse!
> Helmut

Hi Helmut,

Native English speaker here -- though I'm not HackXBack, just see that  
he hasn't replied to you yet.


"Yeah Joe, I don't know why people don't give the bug importance while  
it deduce[sic] a lot of hit ratio"


Here's the same phrase worded the way I think that HaxkXBack /meant/ --

"Yeah Joe,

I don't know why people don't give the bug higher priority as it is  
significantly reducing the hit ratio"

HTH :)

-Dan
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4387 bytes
Desc: S/MIME Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150810/e8868e1d/attachment.bin>

From Hullen at t-online.de  Mon Aug 10 10:43:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 10 Aug 2015 12:43:00 +0200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <>
References: <20150810061311.Horde.Bny3FY_8KdtJsknX0ksZZw5@192.168.10.20>
Message-ID: <DMbcdsLuCXB@helmut.hullen.de>

Hallo, Dan,

Du meintest am 10.08.15 zum Thema Re: [squid-users] a lot of TCP_SWAPFAIL_MISS/200:

> Native English speaker here -- though I'm not HackXBack, just see
> that he hasn't replied to you yet.

[...]

> I don't know why people don't give the bug higher priority as it is
> significantly reducing the hit ratio"

> HTH :)

Thank you so much! That longer text I can read, and I can understand it!

Viele Gruesse!
Helmut



From yvoinov at gmail.com  Mon Aug 10 16:58:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 10 Aug 2015 22:58:43 +0600
Subject: [squid-users] Who knows anything about Vimeo caching?
Message-ID: <55C8D843.4010609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Who knows anything about Vimeo caching? Any relevant and _actual_ info
are welcome.

WBR, Yuri
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVyNhDAAoJENNXIZxhPexGGRUH/jKW+F0y+dshSdMPj/f2yDBy
tvNGK/d9Zfqyhq4kkySoyTwJZCYBXr7KfAdap4NMB+1WufhqqMUP99yYhpOgH5jx
YUVrGDsmaAWV4lSm2rIbu4dMxJA+EO+27VA5L9XMaW13BVXcZYTCPZkbjA2a9Qg1
pYeUCA7sIVXvQajI6HDgYgGI6di0muVZDkvj0Gww9TWaZ6YpTI+K+G03CQYR46Hc
BIC59n6VAYOOWh4ZPiDqmsDwh0s0L80RYnHfCghAkJhLLTcVMo9YeRouUV3Dryr3
0Xp5F/562cPLqtZrADkR1Zpffy5TBNTJTVKJ6OP6bjjTRc5kPU1rIYCoAsz6XJY=
=p1J1
-----END PGP SIGNATURE-----



From webmaster at squidblacklist.org  Mon Aug 10 17:01:09 2015
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Mon, 10 Aug 2015 12:01:09 -0500
Subject: [squid-users] Who knows anything about Vimeo caching?
In-Reply-To: <55C8D843.4010609@gmail.com>
References: <55C8D843.4010609@gmail.com>
Message-ID: <55C8D8D5.2000100@squidblacklist.org>

I am also interested in this thread.

On 8/10/2015 11:58 AM, Yuri Voinov wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> Who knows anything about Vimeo caching? Any relevant and _actual_ info
> are welcome.
>
> WBR, Yuri
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJVyNhDAAoJENNXIZxhPexGGRUH/jKW+F0y+dshSdMPj/f2yDBy
> tvNGK/d9Zfqyhq4kkySoyTwJZCYBXr7KfAdap4NMB+1WufhqqMUP99yYhpOgH5jx
> YUVrGDsmaAWV4lSm2rIbu4dMxJA+EO+27VA5L9XMaW13BVXcZYTCPZkbjA2a9Qg1
> pYeUCA7sIVXvQajI6HDgYgGI6di0muVZDkvj0Gww9TWaZ6YpTI+K+G03CQYR46Hc
> BIC59n6VAYOOWh4ZPiDqmsDwh0s0L80RYnHfCghAkJhLLTcVMo9YeRouUV3Dryr3
> 0Xp5F/562cPLqtZrADkR1Zpffy5TBNTJTVKJ6OP6bjjTRc5kPU1rIYCoAsz6XJY=
> =p1J1
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Signed,

Benjamin E. Nichols
http://www.squidblacklist.org



From luis.daniel.lucio at gmail.com  Mon Aug 10 17:20:51 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Mon, 10 Aug 2015 13:20:51 -0400
Subject: [squid-users] Who knows anything about Vimeo caching?
In-Reply-To: <55C8D8D5.2000100@squidblacklist.org>
References: <55C8D843.4010609@gmail.com> <55C8D8D5.2000100@squidblacklist.org>
Message-ID: <CAFLo2QyUqUXPwdfkXg2tSGb4JXf5z4DMcq+VsUAvCMW0u+p3Eg@mail.gmail.com>

Squid wont do caching of 206 answers, which the major part of video uses.

You may force 200 answers and let cache policy to work. But there is a side
efect, video wont play until it is downloaded.

Luis Daniel Lucio Quiroz
CISSP, CISM, CISA
Linux, VoIP and much more fun
www.okay.com.mx

Need LCR? Check out LCR for FusionPBX with FreeSWITCH
Need Billing? Check out Billing for FusionPBX with FreeSWITCH

2015-08-10 13:01 GMT-04:00 Benjamin E. Nichols <webmaster at squidblacklist.org
>:

> I am also interested in this thread.
>
>
> On 8/10/2015 11:58 AM, Yuri Voinov wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>   Who knows anything about Vimeo caching? Any relevant and _actual_ info
>> are welcome.
>>
>> WBR, Yuri
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>   iQEcBAEBCAAGBQJVyNhDAAoJENNXIZxhPexGGRUH/jKW+F0y+dshSdMPj/f2yDBy
>> tvNGK/d9Zfqyhq4kkySoyTwJZCYBXr7KfAdap4NMB+1WufhqqMUP99yYhpOgH5jx
>> YUVrGDsmaAWV4lSm2rIbu4dMxJA+EO+27VA5L9XMaW13BVXcZYTCPZkbjA2a9Qg1
>> pYeUCA7sIVXvQajI6HDgYgGI6di0muVZDkvj0Gww9TWaZ6YpTI+K+G03CQYR46Hc
>> BIC59n6VAYOOWh4ZPiDqmsDwh0s0L80RYnHfCghAkJhLLTcVMo9YeRouUV3Dryr3
>> 0Xp5F/562cPLqtZrADkR1Zpffy5TBNTJTVKJ6OP6bjjTRc5kPU1rIYCoAsz6XJY=
>> =p1J1
>> -----END PGP SIGNATURE-----
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> --
> Signed,
>
> Benjamin E. Nichols
> http://www.squidblacklist.org
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150810/7525ad8c/attachment.htm>

From yvoinov at gmail.com  Mon Aug 10 17:49:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 10 Aug 2015 23:49:12 +0600
Subject: [squid-users] Who knows anything about Vimeo caching?
In-Reply-To: <CAFLo2QyUqUXPwdfkXg2tSGb4JXf5z4DMcq+VsUAvCMW0u+p3Eg@mail.gmail.com>
References: <55C8D843.4010609@gmail.com> <55C8D8D5.2000100@squidblacklist.org>
 <CAFLo2QyUqUXPwdfkXg2tSGb4JXf5z4DMcq+VsUAvCMW0u+p3Eg@mail.gmail.com>
Message-ID: <55C8E418.5070007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The problems is:

1. I don't know what Vimeo URL returns video file ID, mime, etc.
exactly. No info in open access.
2. I can't see Vimeo requests/responses in Squid access.log (like YT).

Any meaningful details are welcome.

206/200 problem will be solved when it becomes clear that in general do.

At the moment, I have no information at all about what's inside Vimeo.

10.08.15 23:20, Luis Daniel Lucio Quiroz ?????:
> Squid wont do caching of 206 answers, which the major part of video uses.
>
> You may force 200 answers and let cache policy to work. But there is a
side
> efect, video wont play until it is downloaded.
>
> Luis Daniel Lucio Quiroz
> CISSP, CISM, CISA
> Linux, VoIP and much more fun
> www.okay.com.mx
>
> Need LCR? Check out LCR for FusionPBX with FreeSWITCH
> Need Billing? Check out Billing for FusionPBX with FreeSWITCH
>
> 2015-08-10 13:01 GMT-04:00 Benjamin E. Nichols
<webmaster at squidblacklist.org
>> :
>
>> I am also interested in this thread.
>>
>>
>> On 8/10/2015 11:58 AM, Yuri Voinov wrote:
>>
> WBR, Yuri
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> --
>> Signed,
>>
>> Benjamin E. Nichols
>> http://www.squidblacklist.org
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVyOQYAAoJENNXIZxhPexGKmUH/1NVYiYYy0qkClM8Q0FFiImk
x63wN8Nx6pobn5sT35/yaeMwwmNHxjj/AGc9Ho3NWeQnfoUj6Cay8bKywTtnkEO1
vohAkUqjKWiSRsbItb+ZDb009vhOarwR9r1amxah16CsorJ6nooJuOg1It08X96s
2SscdsNLF4XxQGnt1thYtWz/uQ534FWULZHdwZFSI4BVxzSgEDqEj7AaZFfdmBUA
qe7V1aaHYulYyJ6DzkF8GP3Ks54KDFEvykYNf0/jJKaigkq42HikiiiMkms5mIxf
nT1BFVOFrOhbt8cGJaJVvpmI4FScwp2PFFAxaFXwztw/BfkuEQL/xNoYVxpBnTo=
=5zIs
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150810/621db69e/attachment.htm>

From eliezer at ngtech.co.il  Mon Aug 10 19:02:01 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 10 Aug 2015 22:02:01 +0300
Subject: [squid-users] Who knows anything about Vimeo caching?
In-Reply-To: <55C8E418.5070007@gmail.com>
References: <55C8D843.4010609@gmail.com> <55C8D8D5.2000100@squidblacklist.org>
 <CAFLo2QyUqUXPwdfkXg2tSGb4JXf5z4DMcq+VsUAvCMW0u+p3Eg@mail.gmail.com>
 <55C8E418.5070007@gmail.com>
Message-ID: <55C8F529.2070201@ngtech.co.il>

I have a suggestion!!
These:
https://addons.mozilla.org/he/firefox/search/?q=video
https://addons.mozilla.org/he/firefox/addon/adblock-plus/?src=search

Should help you to figure out couple things.
If you don't figure it out by your self send another email to this 
thread and I will be able to take a peek at vimeo.
I know they are enabling downloads so it should be very simple to cache 
some of the content using StoreID.
 From my memory I think that they use different ID for the images from 
the videos.

Eliezer

* I know that they are using akamai as their CDN and you can check the 
option for a cache

On 10/08/2015 20:49, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> The problems is:
>
> 1. I don't know what Vimeo URL returns video file ID, mime, etc.
> exactly. No info in open access.
> 2. I can't see Vimeo requests/responses in Squid access.log (like YT).
>
> Any meaningful details are welcome.
>
> 206/200 problem will be solved when it becomes clear that in general do.
>
> At the moment, I have no information at all about what's inside Vimeo.
>
> 10.08.15 23:20, Luis Daniel Lucio Quiroz ?????:
>> Squid wont do caching of 206 answers, which the major part of video uses.
>>
>> You may force 200 answers and let cache policy to work. But there is a
> side
>> efect, video wont play until it is downloaded.
>>
>> Luis Daniel Lucio Quiroz
>> CISSP, CISM, CISA
>> Linux, VoIP and much more fun
>> www.okay.com.mx
>>
>> Need LCR? Check out LCR for FusionPBX with FreeSWITCH
>> Need Billing? Check out Billing for FusionPBX with FreeSWITCH
>>
>> 2015-08-10 13:01 GMT-04:00 Benjamin E. Nichols
> <webmaster at squidblacklist.org
>>> :
>>
>>> I am also interested in this thread.
>>>
>>>
>>> On 8/10/2015 11:58 AM, Yuri Voinov wrote:
>>>
>> WBR, Yuri
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>
>>> --
>>> Signed,
>>>
>>> Benjamin E. Nichols
>>> http://www.squidblacklist.org
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVyOQYAAoJENNXIZxhPexGKmUH/1NVYiYYy0qkClM8Q0FFiImk
> x63wN8Nx6pobn5sT35/yaeMwwmNHxjj/AGc9Ho3NWeQnfoUj6Cay8bKywTtnkEO1
> vohAkUqjKWiSRsbItb+ZDb009vhOarwR9r1amxah16CsorJ6nooJuOg1It08X96s
> 2SscdsNLF4XxQGnt1thYtWz/uQ534FWULZHdwZFSI4BVxzSgEDqEj7AaZFfdmBUA
> qe7V1aaHYulYyJ6DzkF8GP3Ks54KDFEvykYNf0/jJKaigkq42HikiiiMkms5mIxf
> nT1BFVOFrOhbt8cGJaJVvpmI4FScwp2PFFAxaFXwztw/BfkuEQL/xNoYVxpBnTo=
> =5zIs
> -----END PGP SIGNATURE-----
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From yvoinov at gmail.com  Mon Aug 10 19:04:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 11 Aug 2015 01:04:04 +0600
Subject: [squid-users] Who knows anything about Vimeo caching?
In-Reply-To: <55C8F529.2070201@ngtech.co.il>
References: <55C8D843.4010609@gmail.com> <55C8D8D5.2000100@squidblacklist.org>
 <CAFLo2QyUqUXPwdfkXg2tSGb4JXf5z4DMcq+VsUAvCMW0u+p3Eg@mail.gmail.com>
 <55C8E418.5070007@gmail.com> <55C8F529.2070201@ngtech.co.il>
Message-ID: <55C8F5A4.3080306@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I've thougth about it. ;)

But I'm not sure Vimeo downloaded is the same shown online. This is the
question.

11.08.15 1:02, Eliezer Croitoru ?????:
> I have a suggestion!!
> These:
> https://addons.mozilla.org/he/firefox/search/?q=video
> https://addons.mozilla.org/he/firefox/addon/adblock-plus/?src=search
>
> Should help you to figure out couple things.
> If you don't figure it out by your self send another email to this
thread and I will be able to take a peek at vimeo.
> I know they are enabling downloads so it should be very simple to
cache some of the content using StoreID.
> From my memory I think that they use different ID for the images from
the videos.
>
> Eliezer
>
> * I know that they are using akamai as their CDN and you can check the
option for a cache
>
> On 10/08/2015 20:49, Yuri Voinov wrote:
>>
> The problems is:
>
> 1. I don't know what Vimeo URL returns video file ID, mime, etc.
> exactly. No info in open access.
> 2. I can't see Vimeo requests/responses in Squid access.log (like YT).
>
> Any meaningful details are welcome.
>
> 206/200 problem will be solved when it becomes clear that in general do.
>
> At the moment, I have no information at all about what's inside Vimeo.
>
> 10.08.15 23:20, Luis Daniel Lucio Quiroz ?????:
> >>> Squid wont do caching of 206 answers, which the major part of
video uses.
> >>>
> >>> You may force 200 answers and let cache policy to work. But there is a
> side
> >>> efect, video wont play until it is downloaded.
> >>>
> >>> Luis Daniel Lucio Quiroz
> >>> CISSP, CISM, CISA
> >>> Linux, VoIP and much more fun
> >>> www.okay.com.mx
> >>>
> >>> Need LCR? Check out LCR for FusionPBX with FreeSWITCH
> >>> Need Billing? Check out Billing for FusionPBX with FreeSWITCH
> >>>
> >>> 2015-08-10 13:01 GMT-04:00 Benjamin E. Nichols
> <webmaster at squidblacklist.org
> >>>> :
> >>>
> >>>> I am also interested in this thread.
> >>>>
> >>>>
> >>>> On 8/10/2015 11:58 AM, Yuri Voinov wrote:
> >>>>
> >>> WBR, Yuri
> >>>>>
> >>>>> _______________________________________________
> >>>>> squid-users mailing list
> >>>>> squid-users at lists.squid-cache.org
> >>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>
> >>>>
> >>>> --
> >>>> Signed,
> >>>>
> >>>> Benjamin E. Nichols
> >>>> http://www.squidblacklist.org
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVyPWkAAoJENNXIZxhPexGyy4H/A61zD6gY0bFTSTrZuiyMi8u
CkxPLBPdF6uRTTZnEH5psX2a4/MNemuXFANIu1W8JVtm724e9N7s+FCt++4zRdxs
YnivyEyzswVl68CF6TmrDx6my1gn0GM1tN376jyuN6NqVssCjDN2A/210EFGbbc8
pfo2gkJ1eHD/24kjy1kZFAZED7spO8yijq59kuaMJRJ9Y5afdb89SW7m3RMsXBEX
BxHvc9W+F+MTvuC7cX2WmlKbGPd6UvPSJ7JxDEtqWe+Lu99rigF3YacFNMGAPAaU
ihVc1r8/au+UnbLQLuHoZWCxYQpRvF1FWLbTCF9sC5MMFHwysiAQfRtlRoepB4s=
=9FDu
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150811/2450a823/attachment.htm>

From david at articatech.com  Tue Aug 11 10:03:43 2015
From: david at articatech.com (David Touzeau)
Date: Tue, 11 Aug 2015 12:03:43 +0200
Subject: [squid-users] [3.5.7]: NTLM/Kerberos Account contains space
Message-ID: <55C9C87F.7030608@articatech.com>

Hi,

Windows Active Directory server ( such as LDAP too) allow to create 
account using space : "Jhon MacDoo"

When using NTLM/Kerberos and when logged with an account contains space, 
Only the first part of the account is displayed and sent to helpers

If an user is called Jhon[space]MacDoo then in access.log and helper, 
there are only "jhon"

In this case, helpers ( eg find the correct Active Directory group) did 
matches rules.

How to fix it ?


From jn at hz6.de  Tue Aug 11 10:07:15 2015
From: jn at hz6.de (Jan Niggemann)
Date: Tue, 11 Aug 2015 12:07:15 +0200
Subject: [squid-users] websocket connection fails
Message-ID: <20150811120715.Horde.3si7f5ef7wHu_ZOf0BGJpHc@htjn.suhail.uberspace.de>


Hi list,

first of all: Excuse me if this has been brought up before, but the  
link to the search function on page  
http://www.squid-cache.org/Support/mailing-lists.html#squid-users  
returns a 404.

Our company introduced a new proxy solution (called Artica).  
http://www.lagado.com/proxy-test shows that  
"squid/3.5.7-20150801-r13880" is used.

My problem is that chromium 44 on Win7/64 can't establish a  
wss-connection, probably due to incorrect server response headers. I  
know for sure that stackoverflow.com handles websockets correctly, but  
the server response headers lack 'Connection: Upgrade' and show  
'Connection: keep-alive" instead. A screenshot and debug log is  
available in my question on SO:
http://stackoverflow.com/questions/31935536/make-chromium-use-connect-for-websockets

Is there a squid setting that I need to tell our administrators about,  
so websocket connections will work?

Regards
jan


From squid3 at treenet.co.nz  Tue Aug 11 11:46:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Aug 2015 23:46:17 +1200
Subject: [squid-users] websocket connection fails
In-Reply-To: <20150811120715.Horde.3si7f5ef7wHu_ZOf0BGJpHc@htjn.suhail.uberspace.de>
References: <20150811120715.Horde.3si7f5ef7wHu_ZOf0BGJpHc@htjn.suhail.uberspace.de>
Message-ID: <55C9E089.2060709@treenet.co.nz>

On 11/08/2015 10:07 p.m., Jan Niggemann wrote:
> 
> Hi list,
> 
> first of all: Excuse me if this has been brought up before, but the link
> to the search function on page
> http://www.squid-cache.org/Support/mailing-lists.html#squid-users
> returns a 404.
> 
> Our company introduced a new proxy solution (called Artica).
> http://www.lagado.com/proxy-test shows that
> "squid/3.5.7-20150801-r13880" is used.
> 
> My problem is that chromium 44 on Win7/64 can't establish a
> wss-connection, probably due to incorrect server response headers. I
> know for sure that stackoverflow.com handles websockets correctly, but
> the server response headers lack 'Connection: Upgrade' and show
> 'Connection: keep-alive" instead. A screenshot and debug log is
> available in my question on SO:
> http://stackoverflow.com/questions/31935536/make-chromium-use-connect-for-websockets
> 
> 
> Is there a squid setting that I need to tell our administrators about,
> so websocket connections will work?

Squid simply does not support HTTP Upgrade yet. To anything.

Chromium is requesting a GET with _optional_ Upgrade to WebSockets, OR a
fallback of HTTP/1.1.

Since Squid does not Upgrade the HTTP/1.1 fallback is used. The response
headers are correctly signalling that HTTP was used instead of WebSockets.

There is nothing we can do if the client software actively offers to
accept a fallback that it does not support.


Chromium (or anything) should still be able to use HTTP CONNECT tunnels
to perform WebSockets through Squid. Maybe it just needs to be aware of
the proxies existence (non-MITM).

Amos



From vin.krish25 at gmail.com  Wed Aug 12 11:01:34 2015
From: vin.krish25 at gmail.com (vin_krish)
Date: Wed, 12 Aug 2015 04:01:34 -0700 (PDT)
Subject: [squid-users] FATAL: Could not create a DNS socket in squid 3.5.3
	stable
Message-ID: <1439377294506-4672656.post@n4.nabble.com>

Hi,

   I'm running squid 3.5.3 stable version on the linux ARM platform. When I
start to run squid, I get the following errors:

2015/08/12 10:11:54.136| dns_internal.cc(1525) dnsInit: idnsInit: attempt
open DNS socket to: [::]
2015/08/12 10:11:54.137| comm.cc(347) comm_openex: comm_openex: Attempt open
socket for: [::]
2015/08/12 10:11:54.137| comm.cc(360) comm_openex: comm_openex: Attempt
fallback open socket for: 0.0.0.0
2015/08/12 10:11:54.138| comm.cc(362) comm_openex: attempt open DNS Socket
IPv6 socket on: 0.0.0.0
2015/08/12 10:11:54.138| comm_open: socket failure: (13) Permission denied
2015/08/12 10:11:54.139| dns_internal.cc(1534) dnsInit: idnsInit: attempt
open DNS socket to: 0.0.0.0
2015/08/12 10:11:54.139| comm.cc(347) comm_openex: comm_openex: Attempt open
socket for: 0.0.0.0
2015/08/12 10:11:54.139| comm_open: socket failure: (13) Permission denied
2015/08/12 10:11:54.141| tools.cc(543) leave_suid: leave_suid: PID 4305
called
2015/08/12 10:11:54.141| storeDirWriteCleanLogs: Starting...
2015/08/12 10:11:54.141|   Finished.  Wrote 0 entries.
2015/08/12 10:11:54.141|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: Could not create a DNS socket

    I have gone through the forum and searched but none solved the issue
like udp/tcp outgoing_address. I have disabled ipv6 in squid config.
Please help in solving the issue..

Regards,
krish





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-Could-not-create-a-DNS-socket-in-squid-3-5-3-stable-tp4672656.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Aug 12 11:13:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 17:13:44 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object returned by
	Squid?
Message-ID: <55CB2A68.2050609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hi all.

Stupid question:

Which protocol uses when Stored-ID object returned by Squid?

I.e., when I use ssl bump, and use next rules:

squid.conf:

acl store_rewrite_list_web url_regex
^https?:\/\/(khms|mt)[0-9]+\.google\.[a-z\.]+\/.*
^https?:\/\/(kh[0-9]?)+\.google\.[a-z\.]+\/(.*)

store_id_program /usr/local/squid/libexec/storeid_file_rewrite
/usr/local/squid/etc/storeid.conf
store_id_access allow store_rewrite_list_web
store_id_access allow store_rewrite_list_web_CDN
store_id_access allow adobe_java_updates
store_id_access deny all

and

^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)       
http://gmaps.SQUIDINTERNAL/$1/$2
^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)           
http://gearth.SQUIDINTERNAL/$1/$2

in storeid.conf,

and

user goes to Google Maps via HTTPS (and map is already Stored-ID),

how Squid will be output stored object to client? Client shows green
https connection with valid Squid CA. Access.log shows bumped https
connection.

HTTP or HTTPS?

WBR, Yuri
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVyypoAAoJENNXIZxhPexG8VYH/386odEaN6yvgno5sRbrkS60
U+/ayqmHIrkDq/gdxpcRRZS2ZsZWhylg0UQwcx0Ktwimeco4rJFREcMwjaPmlqDf
3qKGsdfDyMv0PPDfDT2zv2AmQpR2TNlqzpAYFd1cCAqfzIiKI6LvWmABC0Cy0Gi9
F1/vevGLur0yGbozvU3OWqDAk2tTeBm8g7r+3hp3vlBYmpJp79OCxkISqalkozYZ
jUcyzCQ13H0l2rFhiglcI4aXrmr0ijVa/Ebx3URCEQWL5nDIBlGdukr5guluW3pC
e+F5AU/8b20/x1GtJWg/wO99fpuLykkvuaiqUCPwKRKIURbpYgfj5G45ZYMcI5Q=
=w+NR
-----END PGP SIGNATURE-----




From squid3 at treenet.co.nz  Wed Aug 12 12:33:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 00:33:27 +1200
Subject: [squid-users] FATAL: Could not create a DNS socket in squid
 3.5.3 stable
In-Reply-To: <1439377294506-4672656.post@n4.nabble.com>
References: <1439377294506-4672656.post@n4.nabble.com>
Message-ID: <55CB3D17.9020109@treenet.co.nz>

On 12/08/2015 11:01 p.m., vin_krish wrote:
> Hi,
> 
>    I'm running squid 3.5.3 stable version on the linux ARM platform. When I
> start to run squid, I get the following errors:
> 
> 2015/08/12 10:11:54.136| dns_internal.cc(1525) dnsInit: idnsInit: attempt
> open DNS socket to: [::]
> 2015/08/12 10:11:54.137| comm.cc(347) comm_openex: comm_openex: Attempt open
> socket for: [::]
> 2015/08/12 10:11:54.137| comm.cc(360) comm_openex: comm_openex: Attempt
> fallback open socket for: 0.0.0.0
> 2015/08/12 10:11:54.138| comm.cc(362) comm_openex: attempt open DNS Socket
> IPv6 socket on: 0.0.0.0
> 2015/08/12 10:11:54.138| comm_open: socket failure: (13) Permission denied
> 2015/08/12 10:11:54.139| dns_internal.cc(1534) dnsInit: idnsInit: attempt
> open DNS socket to: 0.0.0.0
> 2015/08/12 10:11:54.139| comm.cc(347) comm_openex: comm_openex: Attempt open
> socket for: 0.0.0.0
> 2015/08/12 10:11:54.139| comm_open: socket failure: (13) Permission denied
> 2015/08/12 10:11:54.141| tools.cc(543) leave_suid: leave_suid: PID 4305
> called
> 2015/08/12 10:11:54.141| storeDirWriteCleanLogs: Starting...
> 2015/08/12 10:11:54.141|   Finished.  Wrote 0 entries.
> 2015/08/12 10:11:54.141|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: Could not create a DNS socket
> 
>     I have gone through the forum and searched but none solved the issue
> like udp/tcp outgoing_address. I have disabled ipv6 in squid config.
> Please help in solving the issue..

IPv6 has nothing to do with it AFAICS. Apparently the user account
starting Squid just dont have permission to open listening sockets, or
at least ones with randomly OS-assigned port.

To send send receive DNS packets Squid requires a UDP listening port
opened. But for security the OS is left to assign a random port number.

Amos



From joevypana at gmail.com  Wed Aug 12 12:38:55 2015
From: joevypana at gmail.com (joseph jose)
Date: Wed, 12 Aug 2015 18:08:55 +0530
Subject: [squid-users] Configuring squid reverse proxy
Message-ID: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>

Hi,

I have set up squid in reverse proxy mode to cache an apache webserver
hosted in linux vm.

IP of my squid reverse proxy is 10.0.0.1 and 10.0.0.2 is the ip of
webserver which is also a linux vm

my config is as follows

#acl squidallow dstdomain testsquid.com
#
#
#http_port 10.0.0.1:80 accel defaultsite=testsquid.com
#
#
#cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
#cache_peer_access squidtest allow allowsquid


In the squid proxy machine i have edited the host file and set testsquid.com
10.0.0.1(which is the ip of proxy machine itself), as proxy is configured
in reverse mode, it is supposed to serve the static page from
webserver(10.0.0.2).

But when i open browser and search for testsquid.com, squid is logging
request but returning a TCP_DENIED/403 status.

Is there anything additionally required in squid config?

Thanks in advance,
Joseph
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/1b9393ef/attachment.htm>

From joevypana at gmail.com  Wed Aug 12 12:38:55 2015
From: joevypana at gmail.com (joseph jose)
Date: Wed, 12 Aug 2015 18:08:55 +0530
Subject: [squid-users] Configuring squid reverse proxy
Message-ID: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>

Hi,

I have set up squid in reverse proxy mode to cache an apache webserver
hosted in linux vm.

IP of my squid reverse proxy is 10.0.0.1 and 10.0.0.2 is the ip of
webserver which is also a linux vm

my config is as follows

#acl squidallow dstdomain testsquid.com
#
#
#http_port 10.0.0.1:80 accel defaultsite=testsquid.com
#
#
#cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
#cache_peer_access squidtest allow allowsquid


In the squid proxy machine i have edited the host file and set testsquid.com
10.0.0.1(which is the ip of proxy machine itself), as proxy is configured
in reverse mode, it is supposed to serve the static page from
webserver(10.0.0.2).

But when i open browser and search for testsquid.com, squid is logging
request but returning a TCP_DENIED/403 status.

Is there anything additionally required in squid config?

Thanks in advance,
Joseph
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/1b9393ef/attachment-0001.htm>

From squid3 at treenet.co.nz  Wed Aug 12 12:51:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 00:51:08 +1200
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB2A68.2050609@gmail.com>
References: <55CB2A68.2050609@gmail.com>
Message-ID: <55CB413C.2040507@treenet.co.nz>

On 12/08/2015 11:13 p.m., Yuri Voinov wrote:
> 
> Hi all.
> 
> Stupid question:
> 
> Which protocol uses when Stored-ID object returned by Squid?
> 
> I.e., when I use ssl bump, and use next rules:
> 
> squid.conf:
> 
> acl store_rewrite_list_web url_regex
> ^https?:\/\/(khms|mt)[0-9]+\.google\.[a-z\.]+\/.*
> ^https?:\/\/(kh[0-9]?)+\.google\.[a-z\.]+\/(.*)
> 
> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
> /usr/local/squid/etc/storeid.conf
> store_id_access allow store_rewrite_list_web
> store_id_access allow store_rewrite_list_web_CDN
> store_id_access allow adobe_java_updates
> store_id_access deny all
> 
> and
> 
> ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)       
> http://gmaps.SQUIDINTERNAL/$1/$2
> ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)           
> http://gearth.SQUIDINTERNAL/$1/$2
> 
> in storeid.conf,
> 
> and
> 
> user goes to Google Maps via HTTPS (and map is already Stored-ID),
> 
> how Squid will be output stored object to client?

Using the connection to the client. Whatever protocol that uses...

> Client shows green
> https connection with valid Squid CA. Access.log shows bumped https
> connection.
> 
> HTTP or HTTPS?


Store-ID is not involved with any protocols. It is simply a way to
adjust the internal hash key used to store or lookup objects in the
cache index.


The HTTP/1.0 hash key is URI.

The HTTP/1.1 hash key is URI, or for responses with Vary: header the URI
plus list of all request headers named in the Vary.

With Store-ID the URI is swapped with whatever the StoreID helper outputs.


You could use a Store-ID helper to map like this and it would still work
perfectly fine:

 ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)
  bwahahahahah at gmaps.SQUIDINTERNAL/$1/$2

 ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)
 ohnowhatyagonnado::://gearth.SQUIDINTERNAL/$1/$2


This is why we changed the name from Store-URL to Store-ID when
polishing the feature port from 2.7. Its not a URL, its a cache index ID
string.

Amos


From Antony.Stone at squid.open.source.it  Wed Aug 12 12:52:36 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 12 Aug 2015 14:52:36 +0200
Subject: [squid-users] Configuring squid reverse proxy
In-Reply-To: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>
References: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>
Message-ID: <201508121452.36655.Antony.Stone@squid.open.source.it>

On Wednesday 12 August 2015 at 14:38:55, joseph jose wrote:

> Hi,
> 
> I have set up squid in reverse proxy mode to cache an apache webserver
> hosted in linux vm.
> 
> IP of my squid reverse proxy is 10.0.0.1 and 10.0.0.2 is the ip of
> webserver which is also a linux vm

Your squid server has only one interface and IP address?

> my config is as follows
> 
> #acl squidallow dstdomain testsquid.com
> #
> #
> #http_port 10.0.0.1:80 accel defaultsite=testsquid.com
> #
> #
> #cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
> #cache_peer_access squidtest allow allowsquid

I sincerely hope you don't mean that these directives are all commented out, 
thus not having any effect?

Even if they're not commented out, do you see the discrepancy between 
"squidallow" in the first line and "allowsquid" in the last?

> In the squid proxy machine i have edited the host file and set
> testsquid.com 10.0.0.1 (which is the ip of proxy machine itself), as proxy
> is configured in reverse mode, it is supposed to serve the static page
> from webserver (10.0.0.2).

What's more important than /etc/hosts on the squid server is what machine you 
are running the browser on, and what does *that* machine resolve testsquid.com 
to?

> But when i open browser and search for testsquid.com, squid is logging
> request but returning a TCP_DENIED/403 status.

Sounds like the browser is successfully seeing testsquid.com as 10.0.0.1, 
then, however you should be careful about trying to run tests like this on too 
few machines - you should have the browser on one machine, squid on a second, 
and the web server on a third (no matter whether any of these are real 
machines or VMs).


Regards,


Antony.

-- 
Users don't know what they want until they see what they get.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Aug 12 12:58:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 00:58:38 +1200
Subject: [squid-users] Configuring squid reverse proxy
In-Reply-To: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>
References: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>
Message-ID: <55CB42FE.4050709@treenet.co.nz>

On 13/08/2015 12:38 a.m., joseph jose wrote:
> Hi,
> 
> I have set up squid in reverse proxy mode to cache an apache webserver
> hosted in linux vm.
> 
> IP of my squid reverse proxy is 10.0.0.1 and 10.0.0.2 is the ip of
> webserver which is also a linux vm
> 
> my config is as follows
> 
> #acl squidallow dstdomain testsquid.com
> #
> #
> #http_port 10.0.0.1:80 accel defaultsite=testsquid.com
> #
> #
> #cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
> #cache_peer_access squidtest allow allowsquid
> 
> 
> In the squid proxy machine i have edited the host file and set testsquid.com
> 10.0.0.1(which is the ip of proxy machine itself), as proxy is configured
> in reverse mode, it is supposed to serve the static page from
> webserver(10.0.0.2).
> 
> But when i open browser and search for testsquid.com, squid is logging
> request but returning a TCP_DENIED/403 status.
> 
> Is there anything additionally required in squid config?

You also need this:

  http_access allow allowsquid


Amos


From yvoinov at gmail.com  Wed Aug 12 13:12:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 19:12:33 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB413C.2040507@treenet.co.nz>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
Message-ID: <55CB4641.1040404@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Thank you, Amos, for explanation.

It is an exhaustive answer to my doubts. :)

So, finally, I can write Store-ID map rules without any protocol prefix,
or use any, no matter?

I.e., ^https?:\/\/(.*?)\/(.*?)\;(?:.*?)$    anysite$1.SQUIDINTERNAL/$2

?

12.08.15 18:51, Amos Jeffries ?????:
> On 12/08/2015 11:13 p.m., Yuri Voinov wrote:
>>
>> Hi all.
>>
>> Stupid question:
>>
>> Which protocol uses when Stored-ID object returned by Squid?
>>
>> I.e., when I use ssl bump, and use next rules:
>>
>> squid.conf:
>>
>> acl store_rewrite_list_web url_regex
>> ^https?:\/\/(khms|mt)[0-9]+\.google\.[a-z\.]+\/.*
>> ^https?:\/\/(kh[0-9]?)+\.google\.[a-z\.]+\/(.*)
>>
>> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
>> /usr/local/squid/etc/storeid.conf
>> store_id_access allow store_rewrite_list_web
>> store_id_access allow store_rewrite_list_web_CDN
>> store_id_access allow adobe_java_updates
>> store_id_access deny all
>>
>> and
>>
>> ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)      
>> http://gmaps.SQUIDINTERNAL/$1/$2
>> ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)          
>> http://gearth.SQUIDINTERNAL/$1/$2
>>
>> in storeid.conf,
>>
>> and
>>
>> user goes to Google Maps via HTTPS (and map is already Stored-ID),
>>
>> how Squid will be output stored object to client?
>
> Using the connection to the client. Whatever protocol that uses...
>
>> Client shows green
>> https connection with valid Squid CA. Access.log shows bumped https
>> connection.
>>
>> HTTP or HTTPS?
>
>
> Store-ID is not involved with any protocols. It is simply a way to
> adjust the internal hash key used to store or lookup objects in the
> cache index.
>
>
> The HTTP/1.0 hash key is URI.
>
> The HTTP/1.1 hash key is URI, or for responses with Vary: header the URI
> plus list of all request headers named in the Vary.
>
> With Store-ID the URI is swapped with whatever the StoreID helper outputs.
>
>
> You could use a Store-ID helper to map like this and it would still work
> perfectly fine:
>
>  ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)
>   bwahahahahah at gmaps.SQUIDINTERNAL/$1/$2
>
>  ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)
>  ohnowhatyagonnado::://gearth.SQUIDINTERNAL/$1/$2
>
>
> This is why we changed the name from Store-URL to Store-ID when
> polishing the feature port from 2.7. Its not a URL, its a cache index ID
> string.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy0ZBAAoJENNXIZxhPexGgCoIALezJVmzCOSKHlPMmd5oVtWl
E9ODopoyV1V29FnSnFhEJtpdXAac9CkygNMXaIBHSwW/X6O68OYiuhg8w2TRrSyS
fXGhK7NXILwMEwCY/4GR51L/HSRMo8F8oS5hwYLcHiLd38o/0ceOPHHahoMSPoUI
NLatPog1wtjksfR5FxUbFCKL4ATHxHbKRGlLwwbzI7ERH/01kKSGW059sPkV3nfR
vMkv/LWS9mYP2LgTxBy2XMiMtAPSpghS9cg6Ewhn2BnKTrDBJX1fGOUWuPMCA+pz
CQ62WL3MbRrlQhTGBNIZOn/IO2xh3tFjgr/FOyZ5uauwPtj/kv3PH6pZSqoEGjE=
=RThk
-----END PGP SIGNATURE-----



From joevypana at gmail.com  Wed Aug 12 13:16:39 2015
From: joevypana at gmail.com (joseph jose)
Date: Wed, 12 Aug 2015 18:46:39 +0530
Subject: [squid-users] Configuring squid reverse proxy
In-Reply-To: <201508121452.36655.Antony.Stone@squid.open.source.it>
References: <CAPzTSf9UiA8w370gfiiYWL9SMDNkfsi1m7utnccdAJn+mkeTfA@mail.gmail.com>
 <201508121452.36655.Antony.Stone@squid.open.source.it>
Message-ID: <CAPzTSf_7Ath7YUTPtV4bFjrRUdKGGqJSOrBTc9HRPS89pT7nzA@mail.gmail.com>

thanks for the quick reply. Actually those lines are no commented out. and
ACL name is corrected.

The browser is on the proxy machine(10.0.0.1) who host file points
testsquid.com to 10.0.0.1 itself.

Squid which is in reverse mode listen to port 80 in 10.0.0.1 is grabbing
each request. but returning TCP_DENIED/403 for testsquid.com. Instead of
returning the webserver static index file

As you told i have one browser machine win7 machine. in which i edited host
file and set testsquid.com to 10.0.0.1(proxy machine ip)

But behaviour remains same.
below is my actual squid config

acl PURGE method purge
acl SSL_ports port 443 445 448 563 1024-65535
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
acl local_addresses dst "/usr/local/squid/etc/local_addresses.conf"
acl allowsquid dstdomain testsquid.com
httpd_suppress_version_string on
cache allow all
cache_effective_user nobody
cache_effective_group nobody
cache_log /usr/local/squid/var/logs/cache.1.100.log
cache_store_log none
half_closed_clients off
hierarchy_stoplist $ cgi ? & ; .asp .shtml localhost
http_access allow manager localhost
http_access allow allowsquid
http_access allow manager cachemgr
http_access deny manager
http_access deny CONNECT !SSL_ports
http_access deny CONNECT local_addresses
http_access allow purge localhost
http_access allow purge cachemgr
http_access deny purge
http_access allow all
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny all
http_reply_access allow all
log_icp_queries off
maximum_object_size 0 KB
maximum_object_size_in_memory 0 KB
request_header_max_size 64 KB
reply_header_max_size 64 KB
strip_query_terms off
uri_whitespace encode
visible_hostname squidproxy
icp_access allow all
http_port 10.0.0.1:80 accel defaultsite=testsquid.com
cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
cache_peer_access squidtest allow allowsquid
cache_peer_access squidtest deny all
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern . 0 20% 4320

Is there anything faulty in my config?

Regards,

Joseph

On Wed, Aug 12, 2015 at 6:22 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 12 August 2015 at 14:38:55, joseph jose wrote:
>
> > Hi,
> >
> > I have set up squid in reverse proxy mode to cache an apache webserver
> > hosted in linux vm.
> >
> > IP of my squid reverse proxy is 10.0.0.1 and 10.0.0.2 is the ip of
> > webserver which is also a linux vm
>
> Your squid server has only one interface and IP address?
>
> > my config is as follows
> >
> > #acl squidallow dstdomain testsquid.com
> > #
> > #
> > #http_port 10.0.0.1:80 accel defaultsite=testsquid.com
> > #
> > #
> > #cache_peer 10.0.0.2 parent 80 0 no-query originserver name=squidtest
> > #cache_peer_access squidtest allow allowsquid
>
> I sincerely hope you don't mean that these directives are all commented
> out,
> thus not having any effect?
>
> Even if they're not commented out, do you see the discrepancy between
> "squidallow" in the first line and "allowsquid" in the last?
>
> > In the squid proxy machine i have edited the host file and set
> > testsquid.com 10.0.0.1 (which is the ip of proxy machine itself), as
> proxy
> > is configured in reverse mode, it is supposed to serve the static page
> > from webserver (10.0.0.2).
>
> What's more important than /etc/hosts on the squid server is what machine
> you
> are running the browser on, and what does *that* machine resolve
> testsquid.com
> to?
>
> > But when i open browser and search for testsquid.com, squid is logging
> > request but returning a TCP_DENIED/403 status.
>
> Sounds like the browser is successfully seeing testsquid.com as 10.0.0.1,
> then, however you should be careful about trying to run tests like this on
> too
> few machines - you should have the browser on one machine, squid on a
> second,
> and the web server on a third (no matter whether any of these are real
> machines or VMs).
>
>
> Regards,
>
>
> Antony.
>
> --
> Users don't know what they want until they see what they get.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/54ab9220/attachment.htm>

From eliezer at ngtech.co.il  Wed Aug 12 13:39:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 12 Aug 2015 16:39:50 +0300
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB4641.1040404@gmail.com>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com>
Message-ID: <55CB4CA6.8020008@ngtech.co.il>

On 12/08/2015 16:12, Yuri Voinov wrote:
> Thank you, Amos, for explanation.
>
> It is an exhaustive answer to my doubts.:)
>
> So, finally, I can write Store-ID map rules without any protocol prefix,
> or use any, no matter?
>
> I.e., ^https?:\/\/(.*?)\/(.*?)\;(?:.*?)$    anysite$1.SQUIDINTERNAL/$2
>
> ?

Hey Yuri,

Yes indeed you can but it's recommended to use some protocol and if you 
have an https map it to the right one that will de-duplicate for your needs.
For example there are many sites which offers both https and http for 
the same content while redirecting to the https many times instead of http.
In this case you are better to not save the same object for http and https.

All The Bests,
Eliezer



From yvoinov at gmail.com  Wed Aug 12 13:44:24 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 19:44:24 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB4CA6.8020008@ngtech.co.il>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com> <55CB4CA6.8020008@ngtech.co.il>
Message-ID: <55CB4DB8.6060009@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


12.08.15 19:39, Eliezer Croitoru ?????:
> On 12/08/2015 16:12, Yuri Voinov wrote:
>> Thank you, Amos, for explanation.
>>
>> It is an exhaustive answer to my doubts.:)
>>
>> So, finally, I can write Store-ID map rules without any protocol prefix,
>> or use any, no matter?
>>
>> I.e., ^https?:\/\/(.*?)\/(.*?)\;(?:.*?)$    anysite$1.SQUIDINTERNAL/$2
>>
>> ?
>
> Hey Yuri,
>
> Yes indeed you can but it's recommended to use some protocol and if
you have an https map it to the right one that will de-duplicate for
your needs.
> For example there are many sites which offers both https and http for
the same content while redirecting to the https many times instead of http.
> In this case you are better to not save the same object for http and
https.
Hmmmmmm. You want to say will better to have HTTP/HTTPS duplicate rules
for the same content? This can lead problems with YT, for example. And
make storage space bigger...

Now I'm trying to produce some best practice with Store-ID for myself.
This is why this question occurs.
>
> All The Bests,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy024AAoJENNXIZxhPexGHnIIAM4Gn2rE81NHPFvfGeBfrZqj
H1pEXiaMM/1IXi6rT8nDcG6L8+ElnuodfoUYcFvxH2p3XKmwPUcaI1iyGbwIfKKH
DsOGYBIlPYgaa9Ypi1IrBO8BoM3seYYYSZJZQ6HEW3EAiLVMeHR60pWsPMBI3ddV
tfq2vyc9/FR2YiQjlARe/BACG7nsmffQzC/s3M77k1bCFkN75+Bb/RGdJ1D87beS
PoVFAjjb3K1gDMMagRMhx4KoLgLXwriliHVxBs2VhWqdLR+E1brBNl2v77FvqbAX
kYj++JxzDKQRIQgmX23SG6vgZi0i+UJtiPTKgT/m0n2um+H9mUcsdvLcJiGkce4=
=16uO
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Wed Aug 12 14:06:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 12 Aug 2015 17:06:38 +0300
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB4DB8.6060009@gmail.com>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com> <55CB4CA6.8020008@ngtech.co.il>
 <55CB4DB8.6060009@gmail.com>
Message-ID: <55CB52EE.5080009@ngtech.co.il>

On 12/08/2015 16:44, Yuri Voinov wrote:
> Hmmmmmm. You want to say will better to have HTTP/HTTPS duplicate rules
> for the same content? This can lead problems with YT, for example. And
> make storage space bigger...
>
> Now I'm trying to produce some best practice with Store-ID for myself.
> This is why this question occurs.

I think that it's not an issue if you are bumping.
Youtube these days forces https whenever they can so you will probably 
won't have this issue at all.

Youtube is not a basic case but take a look at their smaller content 
such as youtube images which consist of one key for them all using the 
video ID which is different from vimeo and many others.

Are you looking for some sites to analyze for practice?

Eliezer


From yvoinov at gmail.com  Wed Aug 12 14:17:14 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 20:17:14 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB52EE.5080009@ngtech.co.il>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com> <55CB4CA6.8020008@ngtech.co.il>
 <55CB4DB8.6060009@gmail.com> <55CB52EE.5080009@ngtech.co.il>
Message-ID: <55CB556A.5070409@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I still see no problem, if the same content under HTTP/HTTPS will
deduplicated as one record.

12.08.15 20:06, Eliezer Croitoru ?????:
> On 12/08/2015 16:44, Yuri Voinov wrote:
>> Hmmmmmm. You want to say will better to have HTTP/HTTPS duplicate rules
>> for the same content? This can lead problems with YT, for example. And
>> make storage space bigger...
>>
>> Now I'm trying to produce some best practice with Store-ID for myself.
>> This is why this question occurs.
>
> I think that it's not an issue if you are bumping.
> Youtube these days forces https whenever they can so you will probably
won't have this issue at all.
HSTS is not an issue - it's quite simple to disable or force URI to
bump. Issue is still different. When rules for store id is dual - http
and https - this occurs problems with Chrome on PC's and mobile devices.
>
> Youtube is not a basic case but take a look at their smaller content
such as youtube images which consist of one key for them all using the
video ID which is different from vimeo and many others.
In our country YT is basic case. This is over 80% of all video traffic.
All others either blocked, or not know or unused. Vimeo.... Vimeo is
great undocumented problem, like Google YT. But if YT is partially
solved, Vimeo is completely terra incognita. :)
>
> Are you looking for some sites to analyze for practice?
Sure.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy1VqAAoJENNXIZxhPexGe44IAKdrqMTNOy1P7gIn6SU66CBI
Wh6cLMb8A3w/rGlhGmQ5rifpfx9BD6voYpudNhPTpnfhjZgq2ObXuS9OEV9JSwNe
NcZP8KuwW196sPIvrsaK5Elc+xK1Zg52eLk9d14bpNiBu+KgG++h2BbAxX7acoyk
qnYB9yR3V0VDS/HIjKBopCJ3i6COZx9lDRzyTD2IrEg2tnhJdKaPd33wLr1B474m
pJEl1zek5ZSKXm6hSPg3o05K6OJNbFCnYmrz8nEm+ct/SZUbKIoNFvTm9YbyoqjP
k2sqaVINekT4TSJEJAjS3I1TQDr134VkDYblZ7BUvwnsyG6rS4m3kykD/7hoNOY=
=C7Ec
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Aug 12 14:18:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 20:18:48 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB413C.2040507@treenet.co.nz>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
Message-ID: <55CB55C8.6030409@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also, Amos.

What's happens when I normalize or completely suppress Vary header?

12.08.15 18:51, Amos Jeffries ?????:
> On 12/08/2015 11:13 p.m., Yuri Voinov wrote:
>>
>> Hi all.
>>
>> Stupid question:
>>
>> Which protocol uses when Stored-ID object returned by Squid?
>>
>> I.e., when I use ssl bump, and use next rules:
>>
>> squid.conf:
>>
>> acl store_rewrite_list_web url_regex
>> ^https?:\/\/(khms|mt)[0-9]+\.google\.[a-z\.]+\/.*
>> ^https?:\/\/(kh[0-9]?)+\.google\.[a-z\.]+\/(.*)
>>
>> store_id_program /usr/local/squid/libexec/storeid_file_rewrite
>> /usr/local/squid/etc/storeid.conf
>> store_id_access allow store_rewrite_list_web
>> store_id_access allow store_rewrite_list_web_CDN
>> store_id_access allow adobe_java_updates
>> store_id_access deny all
>>
>> and
>>
>> ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)      
>> http://gmaps.SQUIDINTERNAL/$1/$2
>> ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)          
>> http://gearth.SQUIDINTERNAL/$1/$2
>>
>> in storeid.conf,
>>
>> and
>>
>> user goes to Google Maps via HTTPS (and map is already Stored-ID),
>>
>> how Squid will be output stored object to client?
>
> Using the connection to the client. Whatever protocol that uses...
>
>> Client shows green
>> https connection with valid Squid CA. Access.log shows bumped https
>> connection.
>>
>> HTTP or HTTPS?
>
>
> Store-ID is not involved with any protocols. It is simply a way to
> adjust the internal hash key used to store or lookup objects in the
> cache index.
>
>
> The HTTP/1.0 hash key is URI.
>
> The HTTP/1.1 hash key is URI, or for responses with Vary: header the URI
> plus list of all request headers named in the Vary.
>
> With Store-ID the URI is swapped with whatever the StoreID helper outputs.
>
>
> You could use a Store-ID helper to map like this and it would still work
> perfectly fine:
>
>  ^https?:\/\/(khms[\d]|mt[\d])+\.google\.[a-z\.]+\/(.*)
>   bwahahahahah at gmaps.SQUIDINTERNAL/$1/$2
>
>  ^https?:\/\/(kh[\d]?)+\.google\.[a-z\.]+\/(.*)
>  ohnowhatyagonnado::://gearth.SQUIDINTERNAL/$1/$2
>
>
> This is why we changed the name from Store-URL to Store-ID when
> polishing the feature port from 2.7. Its not a URL, its a cache index ID
> string.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy1XIAAoJENNXIZxhPexGo8UIAMrky2Dr6y7dsvl7M/iyBP6s
NQEc0JL32tSvpTVXwVaQadSgrSDV5JQWcyZsKXoZAIwgQLp4Xi6Ds2A4WkT8WkSM
yorGKIT74f/HczROklpxei7RzfpJ1HFswa/kmCJeHYQi8mp1FGNhRw+gw+MX1wG6
SR+4mOhtCKG7ua+IEv2bmRtkD6d5fpvLh82/zyQfTHJOPxsJgXzfJnB5kZRAwpmY
haoB3FiutLppy0VP+anD9S7S7NSxWR2oWKnL5Xu6orzwMo7FrmOJOswdED+tj1zH
tD0O+iWG6Bxpljqkz8wW3Oh9S03Ie9mi9jOrpPcxtnEA62UMLzk4Tj33cxHW2vk=
=J0pn
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Aug 12 14:31:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 02:31:18 +1200
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB55C8.6030409@gmail.com>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB55C8.6030409@gmail.com>
Message-ID: <55CB58B6.7050403@treenet.co.nz>

On 13/08/2015 2:18 a.m., Yuri Voinov wrote:
> 
> Also, Amos.
> 
> What's happens when I normalize or completely suppress Vary header?

The Internet breaks.

* images show up as random colour garbage
* what should be readable text shows up as binary characters
* what should be downloadable objects show up as text pages

lots more. Its not good.

Suppress or normalize the Accept headers instead.

Amos


From yvoinov at gmail.com  Wed Aug 12 14:44:20 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 20:44:20 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB58B6.7050403@treenet.co.nz>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB55C8.6030409@gmail.com> <55CB58B6.7050403@treenet.co.nz>
Message-ID: <55CB5BC4.1080705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ok, but what about User-Agent in Vary? This is seriously decrease hit ratio.

Does exists recommended practice to strip User-Agent from Vary header?

Excluding something like this:

reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding

?

12.08.15 20:31, Amos Jeffries ?????:
> On 13/08/2015 2:18 a.m., Yuri Voinov wrote:
>>
>> Also, Amos.
>>
>> What's happens when I normalize or completely suppress Vary header?
>
> The Internet breaks.
>
> * images show up as random colour garbage
> * what should be readable text shows up as binary characters
> * what should be downloadable objects show up as text pages
>
> lots more. Its not good.
>
> Suppress or normalize the Accept headers instead.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy1vDAAoJENNXIZxhPexGDzwH/AzmsmP5fdcx7fUWvvP/WdC3
CThV9PDwr/3peRv3F67rd44/nYW3Va6Gm6lLhLKiuGs7dowMtj4G9jsl3EOj1hk3
CWf/wHVZoZY6K+qTAAp9Cxlgw27eAoFLxiLnA/ehMmQbc0kFvXD9MyDvyF/OjoNf
kTIVWoifkd/mWPSGtMq3PNckOdWjbQcaAZb4qUbqfVcX+HnqfYeC+8D3m4koueDJ
V/3DZ44paPR9nnOlU8iPtjKImNuf4mJZw+9FxsQ2FkevZ3VCaulMwofoziNGSeFr
KV0Bl3fl740pMQGFTI0gENgRw12Q+l0Wzj24y4MByJl+K/x0lDEObyUuUA7xzME=
=Kd10
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Wed Aug 12 16:04:02 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 12 Aug 2015 19:04:02 +0300
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB556A.5070409@gmail.com>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com> <55CB4CA6.8020008@ngtech.co.il>
 <55CB4DB8.6060009@gmail.com> <55CB52EE.5080009@ngtech.co.il>
 <55CB556A.5070409@gmail.com>
Message-ID: <55CB6E72.5040001@ngtech.co.il>

Vimeo is using akamai and can be cached as far as I understand.

There is always an ID out there in urls(not Vimeo specific if at all in 
Vimeo) but it's either encrypted or is inside the page that needs to be 
parsed.
These are the two options I know about and I have seen couple times that 
there are something like a "cookie" url which can help to identify a 
common ID.

I don't remember if it was to you that I recommended Adblock plus and 
video search at firefox.

There was another software which I think is called "FreeRapid 
Downloader" and they have all bunch of plugins which written in java 
that helped me get started on couple things with video sites analytics.
The software doesn't work very well these days but it gives a lot of 
basics if you have the needed java skills(I am not pushing you to dive 
into it since it's not a simple task).

Adblock plus gives you many details on every page you browse into and 
have a nice filter option.
You can get into the filter and urls list using the "open blockable 
items" option in the Adblock plus firefox menu.

It' much simpler then plain firefox "inspect elements" or firebug but it 
is limited a bit.

Eliezer

* I have read people writing in forums something like "we are not spoon 
feeding got read the man pages" and I do not like the attitude!!
  - Also on this specific case there is not man pages or something 
similar and I encourage to ask.

On 12/08/2015 17:17, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> I still see no problem, if the same content under HTTP/HTTPS will
> deduplicated as one record.
>
> 12.08.15 20:06, Eliezer Croitoru ?????:
>> On 12/08/2015 16:44, Yuri Voinov wrote:
>>> Hmmmmmm. You want to say will better to have HTTP/HTTPS duplicate rules
>>> for the same content? This can lead problems with YT, for example. And
>>> make storage space bigger...
>>>
>>> Now I'm trying to produce some best practice with Store-ID for myself.
>>> This is why this question occurs.
>>
>> I think that it's not an issue if you are bumping.
>> Youtube these days forces https whenever they can so you will probably
> won't have this issue at all.
> HSTS is not an issue - it's quite simple to disable or force URI to
> bump. Issue is still different. When rules for store id is dual - http
> and https - this occurs problems with Chrome on PC's and mobile devices.
>>
>> Youtube is not a basic case but take a look at their smaller content
> such as youtube images which consist of one key for them all using the
> video ID which is different from vimeo and many others.
> In our country YT is basic case. This is over 80% of all video traffic.
> All others either blocked, or not know or unused. Vimeo.... Vimeo is
> great undocumented problem, like Google YT. But if YT is partially
> solved, Vimeo is completely terra incognita. :)
>>
>> Are you looking for some sites to analyze for practice?
> Sure.
>>
>> Eliezer
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVy1VqAAoJENNXIZxhPexGe44IAKdrqMTNOy1P7gIn6SU66CBI
> Wh6cLMb8A3w/rGlhGmQ5rifpfx9BD6voYpudNhPTpnfhjZgq2ObXuS9OEV9JSwNe
> NcZP8KuwW196sPIvrsaK5Elc+xK1Zg52eLk9d14bpNiBu+KgG++h2BbAxX7acoyk
> qnYB9yR3V0VDS/HIjKBopCJ3i6COZx9lDRzyTD2IrEg2tnhJdKaPd33wLr1B474m
> pJEl1zek5ZSKXm6hSPg3o05K6OJNbFCnYmrz8nEm+ct/SZUbKIoNFvTm9YbyoqjP
> k2sqaVINekT4TSJEJAjS3I1TQDr134VkDYblZ7BUvwnsyG6rS4m3kykD/7hoNOY=
> =C7Ec
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From yvoinov at gmail.com  Wed Aug 12 16:06:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 12 Aug 2015 22:06:48 +0600
Subject: [squid-users] Which protocol uses when Stored-ID object
 returned by Squid?
In-Reply-To: <55CB6E72.5040001@ngtech.co.il>
References: <55CB2A68.2050609@gmail.com> <55CB413C.2040507@treenet.co.nz>
 <55CB4641.1040404@gmail.com> <55CB4CA6.8020008@ngtech.co.il>
 <55CB4DB8.6060009@gmail.com> <55CB52EE.5080009@ngtech.co.il>
 <55CB556A.5070409@gmail.com> <55CB6E72.5040001@ngtech.co.il>
Message-ID: <55CB6F18.4010507@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Good info, Eliezer.

I'll research this next week. I've interested in video hostings caching.

Thanks for idea and point to right direction!

12.08.15 22:04, Eliezer Croitoru ?????:
> Vimeo is using akamai and can be cached as far as I understand.
>
> There is always an ID out there in urls(not Vimeo specific if at all
in Vimeo) but it's either encrypted or is inside the page that needs to
be parsed.
> These are the two options I know about and I have seen couple times
that there are something like a "cookie" url which can help to identify
a common ID.
>
> I don't remember if it was to you that I recommended Adblock plus and
video search at firefox.
>
> There was another software which I think is called "FreeRapid
Downloader" and they have all bunch of plugins which written in java
that helped me get started on couple things with video sites analytics.
> The software doesn't work very well these days but it gives a lot of
basics if you have the needed java skills(I am not pushing you to dive
into it since it's not a simple task).
>
> Adblock plus gives you many details on every page you browse into and
have a nice filter option.
> You can get into the filter and urls list using the "open blockable
items" option in the Adblock plus firefox menu.
>
> It' much simpler then plain firefox "inspect elements" or firebug but
it is limited a bit.
>
> Eliezer
>
> * I have read people writing in forums something like "we are not
spoon feeding got read the man pages" and I do not like the attitude!!
>  - Also on this specific case there is not man pages or something
similar and I encourage to ask.
>
> On 12/08/2015 17:17, Yuri Voinov wrote:
>>
> I still see no problem, if the same content under HTTP/HTTPS will
> deduplicated as one record.
>
> 12.08.15 20:06, Eliezer Croitoru ?????:
> >>> On 12/08/2015 16:44, Yuri Voinov wrote:
> >>>> Hmmmmmm. You want to say will better to have HTTP/HTTPS duplicate
rules
> >>>> for the same content? This can lead problems with YT, for
example. And
> >>>> make storage space bigger...
> >>>>
> >>>> Now I'm trying to produce some best practice with Store-ID for
myself.
> >>>> This is why this question occurs.
> >>>
> >>> I think that it's not an issue if you are bumping.
> >>> Youtube these days forces https whenever they can so you will probably
> won't have this issue at all.
> HSTS is not an issue - it's quite simple to disable or force URI to
> bump. Issue is still different. When rules for store id is dual - http
> and https - this occurs problems with Chrome on PC's and mobile devices.
> >>>
> >>> Youtube is not a basic case but take a look at their smaller content
> such as youtube images which consist of one key for them all using the
> video ID which is different from vimeo and many others.
> In our country YT is basic case. This is over 80% of all video traffic.
> All others either blocked, or not know or unused. Vimeo.... Vimeo is
> great undocumented problem, like Google YT. But if YT is partially
> solved, Vimeo is completely terra incognita. :)
> >>>
> >>> Are you looking for some sites to analyze for practice?
> Sure.
> >>>
> >>> Eliezer
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVy28YAAoJENNXIZxhPexGD90H/0OX47ftwV4sMPgyrVDSStN9
PFv6ejEtDkuNTVupP15lxRIBTsFsGODGDgpavA0OG/KHkpwpkaJuZUGUeIrlcyM0
d9KzHkYqA002wGfVhqkA+ZoGkjz0eUuT+xuplxXYnEYUOtxpz2DxMmPEFhtjft2F
I32ahfNQOSjwhDXPaoGt5gwIf5ugIIur8Pa0BdigJG6KMgr3zbFatdgJEi8NCWd6
T7z8248ORAzhvkC6JBQsWyQo6yBbumyEZNOELAlm6445CcbKbBYGOjMFJS6O/zYg
I+RA6rJACuPd/1ZtN0s4PJ9qYEMQm0rFfxbscY7g3a7sbkMMNp1ifDvNVzsCTb0=
=M+Ys
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/aa642e15/attachment.htm>

From jorgeley at gmail.com  Wed Aug 12 17:09:33 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 12 Aug 2015 14:09:33 -0300
Subject: [squid-users] Android
Message-ID: <CAMeoTHnWH+2aUpzq3uw+UnxKLYu6m-8doxbgnfXU6N6FC4mEKQ@mail.gmail.com>

Hi guys.
Is there a way to work around android under squid authentication???
I could make an ACL to a range of address that my wifi router distribute to
my wifi network and deny auth for them, but I'd like to identify the
Android clients and specify that just them do not need authentication.
Any ideas?
Thanks since now

--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/c9a7c867/attachment.htm>

From leolistas at solutti.com.br  Wed Aug 12 17:14:38 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 12 Aug 2015 14:14:38 -0300
Subject: [squid-users] Android
In-Reply-To: <CAMeoTHnWH+2aUpzq3uw+UnxKLYu6m-8doxbgnfXU6N6FC4mEKQ@mail.gmail.com>
References: <CAMeoTHnWH+2aUpzq3uw+UnxKLYu6m-8doxbgnfXU6N6FC4mEKQ@mail.gmail.com>
Message-ID: <55CB7EFE.4070707@solutti.com.br>


     Of course you can always use 'acl aclname browser' to identify some 
specific agents and, using that, try to match android browsers.

     however, that would be basically impossible to guarantee to work 
100% because softwares that calls HTTP requests can always sent 
different identifications and, thus, your rule will not match. And those 
rules would allow, also, other browsers/OSs to fake their agent-id and, 
forcing something that will look like an Android to you, have the access 
allowed without authentication.

     You can try, but i would say you can never have a fully 100% 
working and 100% fake-proof setup on that scenario.


Em 12/08/15 14:09, Jorgeley Junior escreveu:
> Hi guys.
> Is there a way to work around android under squid authentication???
> I could make an ACL to a range of address that my wifi router 
> distribute to my wifi network and deny auth for them, but I'd like to 
> identify the Android clients and specify that just them do not need 
> authentication.
> Any ideas?
> Thanks since now
>

-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From dweimer at dweimer.net  Wed Aug 12 20:22:22 2015
From: dweimer at dweimer.net (dweimer)
Date: Wed, 12 Aug 2015 15:22:22 -0500
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
Message-ID: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>

I am trying to see if I have found another Squid 3.5.x issue with 
FreeBSD 10, or if I just have something set wrong on my https_port 
settings.

The server I am testing with is currently running FreeBSD 10.2-RC3, with 
Squid 3.5.7, and LibreSSL 2.2.2. The Apache 2.4.16 server behind squid 
is using the same cipher list settings, and the same LibreSSL 2.2.2 
library, and the same certificate file.

Here is the squid https_port line.

https_port 443 accel defaultsite=www.dweimer.net \
  cert=/common/GoDaddy.Cert/dweimer.net.gd.bundle.crt \
  key=/common/GoDaddy.Cert/dweimer.net.key \
  options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
  dhparams=dh.params \
  cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4 \
  vhost

Apache SSL Configuration
SSLHonorCipherOrder On
SSLProtocol -ALL +TLSv1.2 +TLSv1.1 +TLSv1
SSLCipherSuite ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
SSLCertificateFile "/common/GoDaddy.Cert/dweimer.net.gd.bundle.crt"
SSLCertificateKeyFile "/common/GoDaddy.Cert/dweimer.net.key"

Apache test:
openssl s_client -tlsv1_2 -connect 192.168.5.2:443
...
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-CHACHA20-POLY1305
Server public key is 4096 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
     Protocol  : TLSv1.2
     Cipher    : ECDHE-RSA-CHACHA20-POLY1305
...

Squid test:
openssl s_client -tlsv1_2 -connect 192.168.5.2:443
...
New, TLSv1/SSLv3, Cipher is AES256-GCM-SHA384
Server public key is 4096 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
     Protocol  : TLSv1.2
     Cipher    : AES256-GCM-SHA384
...

Squid Test with cipher from Apache specified:
openssl s_client -tls1_2 -cipher ECDHE-RSA-CHACHA20-POLY1305 -connect 
192.168.5.3:443
CONNECTED(00000003)
34381405160:error:14094410:SSL routines:SSL3_READ_BYTES:sslv3 alert 
handshake failure:s3_pkt.c:1133:SSL alert number 40
34381405160:error:1409E0E5:SSL routines:SSL3_WRITE_BYTES:ssl handshake 
failure:s3_pkt.c:522:
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 0 bytes
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
     Protocol  : TLSv1.2
     Cipher    : 0000
...

Squid does however use this cipher when connecting to the Apache server, 
even though the client isn't using a forward secrecy capable cipher 
(TLS_RSA_WITH_AES_256_CBC_SHA TLS1.2 reported by Firefox), determined by 
using a script with the PHP $_SERVER predefined variable connected 
through the reverse proxy.
SERVER_PROTOCOL  HTTP/1.1
SERVER_SOFTWARE  Apache/2.4.16 (FreeBSD) LibreSSL/2.2.2 SVN/1.8.14 
PHP/5.6.11
SSL_CIPHER       ECDHE-RSA-CHACHA20-POLY1305

Does anyone see something missing in my https_port configuration that is 
causing it to not use the ECDHE keys?

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From dweimer at dweimer.net  Wed Aug 12 20:38:10 2015
From: dweimer at dweimer.net (dweimer)
Date: Wed, 12 Aug 2015 15:38:10 -0500
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
In-Reply-To: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
References: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
Message-ID: <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>

On 2015-08-12 3:22 pm, dweimer wrote:
> I am trying to see if I have found another Squid 3.5.x issue with
> FreeBSD 10, or if I just have something set wrong on my https_port
> settings.
> 
> The server I am testing with is currently running FreeBSD 10.2-RC3,
> with Squid 3.5.7, and LibreSSL 2.2.2. The Apache 2.4.16 server behind
> squid is using the same cipher list settings, and the same LibreSSL
> 2.2.2 library, and the same certificate file.
> 
> Here is the squid https_port line.
> 
> https_port 443 accel defaultsite=www.dweimer.net \
>  cert=/common/GoDaddy.Cert/dweimer.net.gd.bundle.crt \
>  key=/common/GoDaddy.Cert/dweimer.net.key \
>  options=NO_SSLv2:NO_SSLv3:SINGLE_DH_USE:CIPHER_SERVER_PREFERENCE \
>  dhparams=dh.params \
>  cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4 \
>  vhost

Update, server wasn't finding the dh.params file
dhparams=/usr/local/etc/squid/dh.params

> Apache SSL Configuration
> SSLHonorCipherOrder On
> SSLProtocol -ALL +TLSv1.2 +TLSv1.1 +TLSv1
> SSLCipherSuite ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!RC4
> SSLCertificateFile "/common/GoDaddy.Cert/dweimer.net.gd.bundle.crt"
> SSLCertificateKeyFile "/common/GoDaddy.Cert/dweimer.net.key"
> 
> Apache test:
> openssl s_client -tlsv1_2 -connect 192.168.5.2:443
> ...
> New, TLSv1/SSLv3, Cipher is ECDHE-RSA-CHACHA20-POLY1305
> Server public key is 4096 bit
> Secure Renegotiation IS supported
> Compression: NONE
> Expansion: NONE
> No ALPN negotiated
> SSL-Session:
>     Protocol  : TLSv1.2
>     Cipher    : ECDHE-RSA-CHACHA20-POLY1305
> ...
> 
> Squid test:
> openssl s_client -tlsv1_2 -connect 192.168.5.2:443
> ...

Update: New, TLSv1/SSLv3, Cipher is DHE-RSA-CHACHA20-POLY1305

> Server public key is 4096 bit
> Secure Renegotiation IS supported
> Compression: NONE
> Expansion: NONE
> No ALPN negotiated
> SSL-Session:
>     Protocol  : TLSv1.2

Update:    Cipher    : DHE-RSA-CHACHA20-POLY1305

> ...
> 
> Squid Test with cipher from Apache specified:
> openssl s_client -tls1_2 -cipher ECDHE-RSA-CHACHA20-POLY1305 -connect
> 192.168.5.3:443
> CONNECTED(00000003)
> 34381405160:error:14094410:SSL routines:SSL3_READ_BYTES:sslv3 alert
> handshake failure:s3_pkt.c:1133:SSL alert number 40
> 34381405160:error:1409E0E5:SSL routines:SSL3_WRITE_BYTES:ssl handshake
> failure:s3_pkt.c:522:
> ---
> no peer certificate available
> ---
> No client certificate CA names sent
> ---
> SSL handshake has read 7 bytes and written 0 bytes
> ---
> New, (NONE), Cipher is (NONE)
> Secure Renegotiation IS NOT supported
> Compression: NONE
> Expansion: NONE
> No ALPN negotiated
> SSL-Session:
>     Protocol  : TLSv1.2
>     Cipher    : 0000
> ...
> 
> Squid does however use this cipher when connecting to the Apache
> server, even though the client isn't using a forward secrecy capable
> cipher (TLS_RSA_WITH_AES_256_CBC_SHA TLS1.2 reported by Firefox),
> determined by using a script with the PHP $_SERVER predefined variable
> connected through the reverse proxy.
> SERVER_PROTOCOL  HTTP/1.1
> SERVER_SOFTWARE  Apache/2.4.16 (FreeBSD) LibreSSL/2.2.2 SVN/1.8.14 
> PHP/5.6.11
> SSL_CIPHER       ECDHE-RSA-CHACHA20-POLY1305
> 
> Does anyone see something missing in my https_port configuration that
> is causing it to not use the ECDHE keys?

I made some updates above, the dh.params file wasn't being found, 
changed that line to use full path, and its now use DHE ciphers, but not 
ECDHE ciphers.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From marcus.kool at urlfilterdb.com  Wed Aug 12 21:10:19 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 12 Aug 2015 18:10:19 -0300
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
In-Reply-To: <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>
References: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
 <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>
Message-ID: <55CBB63B.50901@urlfilterdb.com>


>> Does anyone see something missing in my https_port configuration that
>> is causing it to not use the ECDHE keys?
>
> I made some updates above, the dh.params file wasn't being found, changed that line to use full path, and its now use DHE ciphers, but not ECDHE ciphers.

FWIW:
ECDHE is not considered safe by a group of cryptologists since the EC implementation is based on secret parameters that only the author of the algorithm has.
See also http://safecurves.cr.yp.to/rigid.html

Marcus


From jorgeley at gmail.com  Wed Aug 12 21:12:26 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 12 Aug 2015 18:12:26 -0300
Subject: [squid-users] Android
In-Reply-To: <55CB7EFE.4070707@solutti.com.br>
References: <CAMeoTHnWH+2aUpzq3uw+UnxKLYu6m-8doxbgnfXU6N6FC4mEKQ@mail.gmail.com>
 <55CB7EFE.4070707@solutti.com.br>
Message-ID: <CAMeoTHnkkC=f_4Hg=BfNaP2qZdZrkLNEtVVba0fi9X6xk7i=DQ@mail.gmail.com>

thanks, that's what I trying right now.

2015-08-12 14:14 GMT-03:00 Leonardo Rodrigues <leolistas at solutti.com.br>:

>
>     Of course you can always use 'acl aclname browser' to identify some
> specific agents and, using that, try to match android browsers.
>
>     however, that would be basically impossible to guarantee to work 100%
> because softwares that calls HTTP requests can always sent different
> identifications and, thus, your rule will not match. And those rules would
> allow, also, other browsers/OSs to fake their agent-id and, forcing
> something that will look like an Android to you, have the access allowed
> without authentication.
>
>     You can try, but i would say you can never have a fully 100% working
> and 100% fake-proof setup on that scenario.
>
>
> Em 12/08/15 14:09, Jorgeley Junior escreveu:
>
>> Hi guys.
>> Is there a way to work around android under squid authentication???
>> I could make an ACL to a range of address that my wifi router distribute
>> to my wifi network and deny auth for them, but I'd like to identify the
>> Android clients and specify that just them do not need authentication.
>> Any ideas?
>> Thanks since now
>>
>>
> --
>
>
>         Atenciosamente / Sincerily,
>         Leonardo Rodrigues
>         Solutti Tecnologia
>         http://www.solutti.com.br
>
>         Minha armadilha de SPAM, N?O mandem email
>         gertrudes at solutti.com.br
>         My SPAMTRAP, do not email it
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150812/7324b801/attachment.htm>

From rafinjer-squid at yahoo.fr  Wed Aug 12 21:20:39 2015
From: rafinjer-squid at yahoo.fr (Jeremie Rafin)
Date: Wed, 12 Aug 2015 21:20:39 +0000 (UTC)
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
Message-ID: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>

Hello,

If I browse on the internet **without** a proxy like squid, and if I use a browser like firefox, the certificate management of SSL connections looks, as far as I feel it, safe and secure.

One of my favorite web pages to test this is: https://revoked.grc.com/
Going on this site must generate an error such as a "revoked certificate" reject.

But, if I browse with squid "behind", configured with SSL bumping and host certificate generation (in such a way my proxy works well for https), this site (https://revoked.grc.com/) is **not** filtered. Which is, to my eye, a big security hole...

Questions (I am searching for answers for several months, without success):

-while using squid, is it possible to have a SSL/HTTPS level of security at least as high as with a reference like firefox (assuming this is a reference; in my humble opinion, regarding certificate management, it is, as I don't know better)?

-do you know any implementation of NSS library (the security library of firefox, probably safer than openssl) for certificate checking helper (cf. sslcrtvalidator_program)?

-how to manage certificate lists, especially automatic updates of them (e.g. use of OSCP inside squid helpers)? Could we access to tweaks like maximum acceptable age of certificate validity, white and black lists of trust authorities, exclusion of autosigned certificate, etc?

Thanks for any help, any suggestion!
J?r?mie


PS1: some of test web pages, for which, to my mind, security fails with squid:
-https://revoked.grc.com (my "favorite"; must fail browsing)
-https://www.ssllabs.com/ssltest/viewMyClient.html (to get a big picture, especially if OCSP stapling is active)
-https://www.howsmyssl.com/ (not as good as previous; provides another point of view)

PS2: my squid 3.5 works on a debian wheezy 7.6; here is my squid.conf (only my adds in top of the default file content); so far I try to have transparent (implicit) proxy but explicit proxy is not better (only simpler configuration):

# SSL bumping configuration
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
sslcrtd_program /usr/local/squid-3.5/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB

# SSL Options - to mimic firefox; some of keys are weaks but some of my favorite websites need them :(
sslproxy_options NO_SSLv2,No_Compression
sslproxy_cipher ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-ECDSA-RC4-SHA:DHE-RSA-AES256-SHA:AES256-SHA:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA:AES128-SHA:DES-CBC3-SHA:RC4-SHA:RC4-MD5:!aNULL:!eNULL:!EXPORT:!DSS:!DES:!3DES:!PSK
sslproxy_cert_error deny all

# Splice access lists
acl splice_client src 192.168.2.30
acl splice_domain dstdomain .paypal.com
acl splice_dst dst 66.211.169.66 66.211.169.3

# HTTPS access
ssl_bump splice splice_client
ssl_bump splice splice_domain
ssl_bump splice splice_dst
ssl_bump server-first all

# Hide PROXY
via off
forwarded_for delete

# Cache management
cache_dir ufs /var/cache/squid 500 16 256


From squid3 at treenet.co.nz  Thu Aug 13 06:06:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 18:06:10 +1200
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>
References: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55CC33D2.6010004@treenet.co.nz>

On 13/08/2015 9:20 a.m., Jeremie Rafin wrote:
> Hello,
> 
> If I browse on the internet **without** a proxy like squid, and if I
use a browser like firefox, the certificate management of SSL
connections looks, as far as I feel it, safe and secure.
> 

Firstly;

Technically a clean install of Squid with default options is more secure
than any browser you will be able to find.

Because it comes configured for forward-proxy. Which does not touch the
TLS traffic in any way but relays CONNECT tunnels. Inside the tunnel the
browser<->server security is in operating control, which makes the Squid
relay equally secure as whatever the browser and server would agree to
without Squid.

Additionally, Squid enforces that HTTPS tunnels only go to port 443.
Which is something the browsers do not do. Making Squid better in this
one way on top of all the things the browsers do inside their tunnel.


Secondly;

 the feeling of security you have with browser is a lie. Pure "security
theatre", done so well that you and billions of others can't even see it.

What you are doing is trusting a very large set of nearly a thousand CA
entities. Including most of those governments with bad reputations now
for surveillance, and a lot of corporations with agendas of their own.
For all sorts of reasons which you have no knowledge or control over.
Yes, someone has vetted that their published intentions were good to get
them into the list, but it was not you. For you and everyone else it is
almost blind trust.

At any time *one*, just one, of them could sign a faked certificate.
When that happens no user will be able to tell the difference without
detailed digging down into the browser cert information.

The only reason these things come to light at all is when the ability is
abused in obvious user-visible ways for dictatorial censorship or
malware attacks. Or the few vigilant an knowledgable people actively
seeking it out catch it in the act. Its not the CA action that was seen
first, but the abuse of power it allowed.

Thankfully the repercussions of being wiped from the global CA list are
severe enough to prevent power abuse in amost cases. But there have been
some exceptions even so.

So security threatre. Its been working so far, but only just.


> One of my favorite web pages to test this is:
> https://revoked.grc.com/ Going on this site must generate an error
> such as a "revoked
certificate" reject.
> 
> But, if I browse with squid "behind", configured with SSL bumping
> and
host certificate generation (in such a way my proxy works well for
https), this site (https://revoked.grc.com/) is **not** filtered. Which
is, to my eye, a big security hole...


For anything other than blind relay to have happened in Squid with
regards to HTTPS you have to configure it to happen. We try to make the
defaults secure in a reasonable way that works for general use. Just
like the browser people do.


You have configured certificate generation. By definition that means
that a *newly generated* certificate is being used to the browser. Not
the one that was revoked.

You have also configured "sslproxy_cert_error deny all" which forces
Squid to accept and ignore all possible origin server certificate
errors. Including revocation.

I hope you can see/understand the result.


> 
> Questions (I am searching for answers for several months, without
success):
> 
> -while using squid, is it possible to have a SSL/HTTPS level of
security at least as high as with a reference like firefox (assuming
this is a reference; in my humble opinion, regarding certificate
management, it is, as I don't know better)?

Yes.

> 
> -do you know any implementation of NSS library (the security library
of firefox, probably safer than openssl) for certificate checking helper
(cf. sslcrtvalidator_program)?
> 

No. Just the OpenSSL one we provide so far.

I'm still working on getting library-agnostic TLS support rolled into
Squid. But the main effort has been on the squid binary, not the helpers
yet.


> -how to manage certificate lists, especially automatic updates of
> them
(e.g. use of OSCP inside squid helpers)? Could we access to tweaks like
maximum acceptable age of certificate validity, white and black lists of
trust authorities, exclusion of autosigned certificate, etc?

The OSCP specifications and library documentation is the best place to
look for that kind of thing.

How the helper determines valid/invalid is intentionally outside the
squid operating scope.

Within squid itself we use the library API lookup which hides/abstracts
that detail.


> 
> Thanks for any help, any suggestion! J?r?mie
> 
> 
> PS1: some of test web pages, for which, to my mind, security fails with squid:
> -https://revoked.grc.com (my "favorite"; must fail browsing)
> -https://www.ssllabs.com/ssltest/viewMyClient.html (to get a big picture, especially if OCSP stapling is active)
> -https://www.howsmyssl.com/ (not as good as previous; provides another point of view)
> 
> PS2: my squid 3.5 works on a debian wheezy 7.6; here is my
> squid.conf
(only my adds in top of the default file content); so far I try to have
transparent (implicit) proxy but explicit proxy is not better (only
simpler configuration):
> 
> # SSL bumping configuration
> http_port 3126 intercept
> https_port 3127 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
> sslcrtd_program /usr/local/squid-3.5/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
> 
> # SSL Options - to mimic firefox; some of keys are weaks but some of my favorite websites need them :(
> sslproxy_options NO_SSLv2,No_Compression
> sslproxy_cipher ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-ECDSA-RC4-SHA:DHE-RSA-AES256-SHA:AES256-SHA:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA:AES128-SHA:DES-CBC3-SHA:RC4-SHA:RC4-MD5:!aNULL:!eNULL:!EXPORT:!DSS:!DES:!3DES:!PSK

Careful. Squid will do what you tell it to.

In order for this to be more secure than the browser, you will have to
ensure that each of these things you are allowing actually are more
secure than what it does. And that you are not allowing anything that
the browser decides is bad.


> sslproxy_cert_error deny all
> 
> # Splice access lists
> acl splice_client src 192.168.2.30
> acl splice_domain dstdomain .paypal.com
> acl splice_dst dst 66.211.169.66 66.211.169.3
> 
> # HTTPS access

Nope, "TLS access" is better description.

HTTPS is two protocol layers; a HTTP layer over a TLS layer (like
"TCP/IP" is actually TCP over IP layer).

ssl_bump directive controls only the TLS later actions. The http_access
rules later deal with the decrypted HTTP layer - but only if it was
allowed to be decrypted ("bump" action) by these rules.


> ssl_bump splice splice_client

Splice is equivalent to blind tunnelling, but can be done after Squid
has played with the certififcates a bit (using read-only accesses).

Since splice_client is based only on src-IP and nothing TLS layer
related it is better to use "none" instead of splice action on the above
rule. The true secure blind-tunnel will then be done.


> ssl_bump splice splice_domain

This is a good example of how dstdomain fails. You are deciding whether
to splice instead of interpret the HTTP message. Based on details inside
that HTTP message which has not yet been interpreted.

Make sure you are using the latest 3.5 release, and use the
"ssl::server_name" insted of dstdomain in the ACL definition.


> ssl_bump splice splice_dst

> ssl_bump server-first all

DO NOT mix the old and new config styles together. server-first requires
doing things like emitting a fake server cert to the client before
reading soem of the client handshake details the splicing needs. But you
have already spliced a bunch of transactions from the earlier rules.

> 
> # Hide PROXY
> via off
> forwarded_for delete
> 

Does *not* hide the proxy.

Hides the *client* by actively "shouting" the proxy details out to the
world in protocol places where the client details would normally go.


HTH
Amos



From joevypana at gmail.com  Thu Aug 13 07:02:24 2015
From: joevypana at gmail.com (joseph jose)
Date: Thu, 13 Aug 2015 12:32:24 +0530
Subject: [squid-users] Content Adaptation in Reverse proxy mode
Message-ID: <CAPzTSf9UL5PXMJ9K0ZnDAZCo5eqU=P7nmUWQ0vLCMh-CwNNVOg@mail.gmail.com>

Hi,

I have configured squid reverse proxy with eCAP modules.

I have a client machine in which host file is edited and for
www.squid.com(cache
domain in reverse proxy), it will point to squid reverse proxy machine.

What i basically do is like i will append the request url to the cached
domain url and send it to squid.

>From client machine if i request www.wiki.org i will send
www.squid.com/?url=www.wiki.org, so that host file of client will resolve
and send this to squid reverse proxy machine. From eCAP i will trim the
first part and take www.wiki.org.
Say wiki.org is denied, from eCAP  i will send back response
(denied/allowed)back to client.

My problem is eCAP modules are loading and working as expected(checked by
logging).

But squid always sending server response apache/CentOS not my custom
response back to the client machine.

regards,

joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/bb2af035/attachment.htm>

From joevypana at gmail.com  Thu Aug 13 07:02:24 2015
From: joevypana at gmail.com (joseph jose)
Date: Thu, 13 Aug 2015 12:32:24 +0530
Subject: [squid-users] Content Adaptation in Reverse proxy mode
Message-ID: <CAPzTSf9UL5PXMJ9K0ZnDAZCo5eqU=P7nmUWQ0vLCMh-CwNNVOg@mail.gmail.com>

Hi,

I have configured squid reverse proxy with eCAP modules.

I have a client machine in which host file is edited and for
www.squid.com(cache
domain in reverse proxy), it will point to squid reverse proxy machine.

What i basically do is like i will append the request url to the cached
domain url and send it to squid.

>From client machine if i request www.wiki.org i will send
www.squid.com/?url=www.wiki.org, so that host file of client will resolve
and send this to squid reverse proxy machine. From eCAP i will trim the
first part and take www.wiki.org.
Say wiki.org is denied, from eCAP  i will send back response
(denied/allowed)back to client.

My problem is eCAP modules are loading and working as expected(checked by
logging).

But squid always sending server response apache/CentOS not my custom
response back to the client machine.

regards,

joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/bb2af035/attachment-0001.htm>

From squid3 at treenet.co.nz  Thu Aug 13 10:12:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2015 22:12:23 +1200
Subject: [squid-users] Content Adaptation in Reverse proxy mode
In-Reply-To: <CAPzTSf9UL5PXMJ9K0ZnDAZCo5eqU=P7nmUWQ0vLCMh-CwNNVOg@mail.gmail.com>
References: <CAPzTSf9UL5PXMJ9K0ZnDAZCo5eqU=P7nmUWQ0vLCMh-CwNNVOg@mail.gmail.com>
Message-ID: <55CC6D87.2070208@treenet.co.nz>

On 13/08/2015 7:02 p.m., joseph jose wrote:
> Hi,
> 
> I have configured squid reverse proxy with eCAP modules.
> 
> I have a client machine in which host file is edited and for
> www.squid.com(cache
> domain in reverse proxy), it will point to squid reverse proxy machine.

Okay so far.

> 
> What i basically do is like i will append the request url to the cached
> domain url and send it to squid.
> 

No. Just, No.

That is not how HTTP proxying works.


> From client machine if i request www.wiki.org i will send
> www.squid.com/?url=www.wiki.org,

No. No you will not.

If you request www.wiki.org, you will request http://www.wiki.org/....

If you request www.squid.com, you will request http://www.squid.com/...

"www.squid.com/?url=..." is a request for content from www.squid.com. No
matter what follows the "url=" part.


> so that host file of client will resolve
> and send this to squid reverse proxy machine. From eCAP i will trim the
> first part and take www.wiki.org.
> Say wiki.org is denied, from eCAP  i will send back response
> (denied/allowed)back to client.
> 

Take a huge step back and please start by and answering the question:

    WHY ?

It is by far away one of the *worst* designs for proxying possible. HTTP
provides specification compliant proxying *natively*.

 So WHY ?


> My problem is eCAP modules are loading and working as expected(checked by
> logging).

Yes. Things working properly is *the problem*. Right. Only when you are
trying to force sftware to do a role it is explicitly designed *not* to do.

 In this case Squid is not a web server, it is a proxy.


> 
> But squid always sending server response apache/CentOS not my custom
> response back to the client machine.

What response?


Amos


From marko.cupac at mimar.rs  Thu Aug 13 12:47:18 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 13 Aug 2015 14:47:18 +0200
Subject: [squid-users] peek and splice content inspection question
Message-ID: <20150813144718.35a4c9a8@efreet.mimar.rs>

Hi,

a few years ago I had a working setup of squid + dansguardian which was
giving me ability to inspect traffic and filter it according to various
criteria, mainly extensions, mime types and presence of malicious code
(clamav).

Lately most of the web moved to https, and dansguardian isn't maintained
for almost three years, which made my setup obsolete.

Is it possible - by means of squid's peek and splice feature - to
inspect file extensions and mime types of https traffic? Can bumped
https traffic be forwarded to icap (squidclamav) for AV scanning? And
finally, would overly curious and unethical admin be able to easily dump
bumped data and find sensitive information there?

Thank you in advance,
-- 
Marko Cupa?
https://www.mimar.rs/


From arjen at arjenvandermeer.eu  Thu Aug 13 12:55:34 2015
From: arjen at arjenvandermeer.eu (Arjen van der Meer)
Date: Thu, 13 Aug 2015 14:55:34 +0200
Subject: [squid-users] Presenting an internal virtual host externally as
	domain root folder
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAKiYxBMf6OFJlueIezcGalEBAAAAAA==@arjenvandermeer.eu>

Hello all,

 

I'm taking the risk of asking a question a question previously answered,
because after multiple weeks of googling I haven't found a working answer.

 

So let me first introduce myself: I'm an old school software engineer that
turned to IT and operations management 2 decades ago and currently acts as a
pre-sales and account manager. 

 

I still like to "play" around with things at home and run an Ubuntu LTS
setup for zoneminder and postfix with dovecot, a raspberry pi with raspbian
and squid, a Synology diskstation with wordpress.

 

Since a couple of months I have installed and configured squid on my
raspberry pi with raspbian and most things I like to work do! One thing
however I can't get to work. Not even with the help of what I found on the
internet so far. That is that I like to present my internal wordpress on my
diskstation as the domain root on the outside. I want my internal
http://192.168.1.153/wordpress to be available on the outside as
www.arjenvandermeer.eu/ without anything else. I have only one external IP
address so I need to use something like squid to make both my Ubuntu system
and the Synology available on the outside. I could use a rewriter or a
redirect to add the /wordpress part but it is much neater if my internal
/wordpress can present itself as domain root on the outside. My Cisco router
port forwards http and ssl to my pi lan ip from where squid picks it up.

 

Can this be done (with squid) and if so please help me out or point me to
some matching examples.

 

Kind regards and many thanks in advance,

 

 

Arjen van der Meer.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/26ac24e6/attachment.htm>

From bielsk at us.ibm.com  Thu Aug 13 14:40:40 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Thu, 13 Aug 2015 10:40:40 -0400
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
In-Reply-To: <55CBB63B.50901@urlfilterdb.com>
References: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
 <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>
 <55CBB63B.50901@urlfilterdb.com>
Message-ID: <OF269531D0.681C6C55-ON87257EA0.004FA710-85257EA0.00509FE3@us.ibm.com>


But does this mean that ECDHE isn't supported by Squid?

I had a related question as the original poster. Some U.S. federal security
standards (e.g. NSA Suite B) require ECDH and ECDHE adds perfect forward
secrecy.

Can squid bump TLS 1.2 traffic that uses ECDHE and that use certificates
signed using ECDSA?



From:	Marcus Kool <marcus.kool at urlfilterdb.com>
To:	dweimer at dweimer.net, Squid Users <squid-users at squid-cache.org>
Date:	08/12/2015 05:10 PM
Subject:	Re: [squid-users] Squid 3.5 Forward Secrecy on https_port
Sent by:	"squid-users" <squid-users-bounces at lists.squid-cache.org>




>> Does anyone see something missing in my https_port configuration that
>> is causing it to not use the ECDHE keys?
>
> I made some updates above, the dh.params file wasn't being found, changed
that line to use full path, and its now use DHE ciphers, but not ECDHE
ciphers.

FWIW:
ECDHE is not considered safe by a group of cryptologists since the EC
implementation is based on secret parameters that only the author of the
algorithm has.
See also http://safecurves.cr.yp.to/rigid.html

Marcus
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/acbc3655/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/acbc3655/attachment.gif>

From squid3 at treenet.co.nz  Thu Aug 13 15:18:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2015 03:18:21 +1200
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
In-Reply-To: <OF269531D0.681C6C55-ON87257EA0.004FA710-85257EA0.00509FE3@us.ibm.com>
References: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
 <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>
 <55CBB63B.50901@urlfilterdb.com>
 <OF269531D0.681C6C55-ON87257EA0.004FA710-85257EA0.00509FE3@us.ibm.com>
Message-ID: <55CCB53D.9080802@treenet.co.nz>

On 14/08/2015 2:40 a.m., Julianne Bielski wrote:
> 
> But does this mean that ECDHE isn't supported by Squid?
> 

Correct. ECDHE is not supported by 3.5 and older.

EECDHE and ECDHE are coming in Squid-4.

If you really need it you are welcome to download and use Squid-4. Some
of us already are. Just be aware that it is still under development so
anything can change without notice, and there are probably a bunch of
bugs not yet found in those features and other new code.

Amos



From squid3 at treenet.co.nz  Thu Aug 13 15:24:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2015 03:24:31 +1200
Subject: [squid-users] Presenting an internal virtual host externally as
 domain root folder
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAKiYxBMf6OFJlueIezcGalEBAAAAAA==@arjenvandermeer.eu>
References: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAKiYxBMf6OFJlueIezcGalEBAAAAAA==@arjenvandermeer.eu>
Message-ID: <55CCB6AF.3000102@treenet.co.nz>

On 14/08/2015 12:55 a.m., Arjen van der Meer wrote:
>
> Since a couple of months I have installed and configured squid on my
> raspberry pi with raspbian and most things I like to work do! One thing
> however I can't get to work. Not even with the help of what I found on the
> internet so far. That is that I like to present my internal wordpress on my
> diskstation as the domain root on the outside. I want my internal
> http://192.168.1.153/wordpress to be available on the outside as
> www.arjenvandermeer.eu/ without anything else. I have only one external IP
> address so I need to use something like squid to make both my Ubuntu system
> and the Synology available on the outside. I could use a rewriter or a
> redirect to add the /wordpress part but it is much neater if my internal
> /wordpress can present itself as domain root on the outside. My Cisco router
> port forwards http and ssl to my pi lan ip from where squid picks it up.
> 
>  
> 
> Can this be done (with squid) and if so please help me out or point me to
> some matching examples.
> 

What you seek is written at the bottom of this page:
<http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers>


Amos



From squid3 at treenet.co.nz  Thu Aug 13 15:38:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2015 03:38:47 +1200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <20150813144718.35a4c9a8@efreet.mimar.rs>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
Message-ID: <55CCBA07.6030306@treenet.co.nz>

On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
> Hi,
> 
> a few years ago I had a working setup of squid + dansguardian which was
> giving me ability to inspect traffic and filter it according to various
> criteria, mainly extensions, mime types and presence of malicious code
> (clamav).
> 
> Lately most of the web moved to https, and dansguardian isn't maintained
> for almost three years, which made my setup obsolete.
> 
> Is it possible - by means of squid's peek and splice feature - to
> inspect file extensions and mime types of https traffic? Can bumped
> https traffic be forwarded to icap (squidclamav) for AV scanning?

Doing so is the features intended purpose.

> And
> finally, would overly curious and unethical admin be able to easily dump
> bumped data and find sensitive information there?

When correctly used TLS cannot be decrypted.

BUt, most use of HTTPS today is not using TLS correctly.

If it could be bumped at all then it could be dumped as easily as
inspected by an AV.

Like a sharp knife can be as easily used for cutting vegetables as
throats. Ones intent has nothing to do with the tools capability or lack.

Amos



From saravanan.nagarajan87 at gmail.com  Thu Aug 13 18:52:58 2015
From: saravanan.nagarajan87 at gmail.com (SaRaVanAn)
Date: Thu, 13 Aug 2015 13:52:58 -0500
Subject: [squid-users] websockets protocol support via "squid+tproxy"
Message-ID: <CA+86yMgLXvaV7NwFPs73jb_EeeBBQ6w86ZSJGb-PgyenW86fWQ@mail.gmail.com>

Hi All,
We are planning to migrate our client applications from "http" to
"websockets" protocol.
Currently we are using "squid+tproxy" in the router. If request comes from
client application to the router we use "Squid+tproxy" to forward the
traffic to local web-server with the help of cache-peer option.

Topology:


-----------------------------------
                                                         Router
Client Machine ------------
                                                   (Squid + tproxy)
                                                               [image:
Inline image 2]
                                                     Local Websever

-------------------------------------

I want to confirm whether squid supports "WebSocket and a WebSocket Secure
connection" when it acts as a transparent proxy.

Regards,
Saravanan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/4e6c8cdc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 358 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150813/4e6c8cdc/attachment.png>

From marko.cupac at mimar.rs  Thu Aug 13 20:02:43 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 13 Aug 2015 22:02:43 +0200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CCBA07.6030306@treenet.co.nz>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
Message-ID: <20150813220243.132611ff@efreet>

On Fri, 14 Aug 2015 03:38:47 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
> > Hi,
> > 
> > a few years ago I had a working setup of squid + dansguardian which
> > was giving me ability to inspect traffic and filter it according to
> > various criteria, mainly extensions, mime types and presence of
> > malicious code (clamav).
> > 
> > Lately most of the web moved to https, and dansguardian isn't
> > maintained for almost three years, which made my setup obsolete.
> > 
> > Is it possible - by means of squid's peek and splice feature - to
> > inspect file extensions and mime types of https traffic? Can bumped
> > https traffic be forwarded to icap (squidclamav) for AV scanning?
> 
> Doing so is the features intended purpose.
> 
> > And
> > finally, would overly curious and unethical admin be able to easily
> > dump bumped data and find sensitive information there?
> 
> When correctly used TLS cannot be decrypted.
> 
> BUt, most use of HTTPS today is not using TLS correctly.
> 
> If it could be bumped at all then it could be dumped as easily as
> inspected by an AV.
> 
> Like a sharp knife can be as easily used for cutting vegetables as
> throats. Ones intent has nothing to do with the tools capability or
> lack.

I completely agree with you, I shouldn't have mixed intent with
capability which is great and which I intend to put to good use.

So, if I understand well, if I just send traffic to squidclamav on icap
tcp port, then I don't store usernames and passwords or private emails
in cache?

This is important to me in order to explain the complete mechanism to
management and to create understandable policy for end users.
-- 
Marko Cupa?
https://www.mimar.rs/


From rousskov at measurement-factory.com  Thu Aug 13 20:23:44 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Aug 2015 14:23:44 -0600
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>
References: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55CCFCD0.9050503@measurement-factory.com>

On 08/12/2015 03:20 PM, Jeremie Rafin wrote:

> -while using squid, is it possible to have a SSL/HTTPS level of
> security at least as high as with a reference like firefox?


With a custom certificate validation helper, Squid can match and exceed
default browser protections when it comes to certificate validation. As
you probably know already, with that helper, _you_ control which server
certificates are distrusted:

  http://www.squid-cache.org/Doc/config/sslcrtvalidator_program/

http://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator

AFAIK, it is very difficult to write and maintain a good validator. If
you cannot find an existing one that meets your needs and you are not an
SSL expert, then you probably should not try to write one. I am not
aware of any validators or libraries you can reuse, but that does not
mean they do not exist. If nothing like this exists, there is probably
an open source project and/or business opportunity here!


Without a custom validator, Squid validation is pretty much as good as
your OpenSSL installation, which can be better or worse than a specific
browser installation.


Good luck,

Alex.



From rousskov at measurement-factory.com  Thu Aug 13 20:42:19 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Aug 2015 14:42:19 -0600
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <55CC33D2.6010004@treenet.co.nz>
References: <475713687.3811682.1439414439152.JavaMail.yahoo@mail.yahoo.com>
 <55CC33D2.6010004@treenet.co.nz>
Message-ID: <55CD012B.20609@measurement-factory.com>

On 08/13/2015 12:06 AM, Amos Jeffries wrote:

> On 13/08/2015 9:20 a.m., Jeremie Rafin wrote:
>> sslproxy_cert_error deny all


> You have also configured "sslproxy_cert_error deny all" which forces
> Squid to accept and ignore all possible origin server certificate
> errors. Including revocation.


"deny" does not force Squid to "accept and ignore". I think you are
describing the effects of the opposite setting:

  sslproxy_cert_error allow all

The actual "deny all" configuration means "do not allow any error to get
through". We should have used custom ACL verbs if the default allow/deny
are causing confusion among the best of us.

Alex.



From yvoinov at gmail.com  Thu Aug 13 20:54:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 14 Aug 2015 02:54:21 +0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <20150813220243.132611ff@efreet>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <20150813220243.132611ff@efreet>
Message-ID: <55CD03FD.20905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


14.08.15 2:02, Marko Cupa? ?????:
> On Fri, 14 Aug 2015 03:38:47 +1200
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>> On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
>>> Hi,
>>>
>>> a few years ago I had a working setup of squid + dansguardian which
>>> was giving me ability to inspect traffic and filter it according to
>>> various criteria, mainly extensions, mime types and presence of
>>> malicious code (clamav).
>>>
>>> Lately most of the web moved to https, and dansguardian isn't
>>> maintained for almost three years, which made my setup obsolete.
>>>
>>> Is it possible - by means of squid's peek and splice feature - to
>>> inspect file extensions and mime types of https traffic? Can bumped
>>> https traffic be forwarded to icap (squidclamav) for AV scanning?
>>
>> Doing so is the features intended purpose.
>>
>>> And
>>> finally, would overly curious and unethical admin be able to easily
>>> dump bumped data and find sensitive information there?
>>
>> When correctly used TLS cannot be decrypted.
>>
>> BUt, most use of HTTPS today is not using TLS correctly.
>>
>> If it could be bumped at all then it could be dumped as easily as
>> inspected by an AV.
>>
>> Like a sharp knife can be as easily used for cutting vegetables as
>> throats. Ones intent has nothing to do with the tools capability or
>> lack.
>
> I completely agree with you, I shouldn't have mixed intent with
> capability which is great and which I intend to put to good use.
>
> So, if I understand well, if I just send traffic to squidclamav on icap
> tcp port, then I don't store usernames and passwords or private emails
> in cache?
I would not worry about it. No physical access to the cache such data
does not pull out with proper administration. Unless, of course, do not
put a proxy in a phone booth on the street. If it starts to bother me -
I either start using encrypted file system, or build a completely black
box - completely disable logging of user access.
>
>
> This is important to me in order to explain the complete mechanism to
> management and to create understandable policy for end users.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVzQP8AAoJENNXIZxhPexGNDUIAMhXUmakjPIpSBlEcb2CsEZN
gS3b6iTLKo2YnBqr2NU1TV9/fqrDZIqd/lszlIta5phYmkiKcRGLP4bR87+SW7ze
dBGeAZeDehXWv4Ga7/YlmAB6LpWRC3Yd0lm3WTiZ/AnowcaxOHx/Q/H7DhDiIFEN
HRDjRGoTcoIkNP+BC76AnrF+8MErz0cPMXLBqVCXNR+ijNCP9LBza1Y5h88QqX7U
cpRaj88LsW7pQeNHNMDtO7PneNKzho/YUO+M0BTtHXw4Mdwdqt1MBViXhTTh/GP9
C5A1DDLvr384YmoG0eReEt/KVIBliTV80htmn6lYT5dJiX2Fu+TAOEjohz+nkcc=
=T28k
-----END PGP SIGNATURE-----



From rousskov at measurement-factory.com  Thu Aug 13 20:56:47 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Aug 2015 14:56:47 -0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CCBA07.6030306@treenet.co.nz>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
Message-ID: <55CD048F.7090907@measurement-factory.com>

On 08/13/2015 09:38 AM, Amos Jeffries wrote:
> On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
>> Is it possible - by means of squid's peek and splice feature - to
>> inspect file extensions and mime types of https traffic? Can bumped
>> https traffic be forwarded to icap (squidclamav) for AV scanning?

> Doing so is the features intended purpose.


And you may be able to use either Secure ICAP (Squid 4) or the eCAP
ClamAV adapter for AV scanning without transmitting bumped messages over
plain text ICAP connections.


> if I just send traffic to squidclamav on icap
> tcp port, then I don't store usernames and passwords or private emails
> in cache?

Squid caching is not related to AV scanning. If you do not disable
caching, Squid will cache cachable responses. IIRC, the code making the
cachability decision does not check whether the response was bumped.
However, you may configure it to do so using the "cache" directive:

  http://www.squid-cache.org/Doc/config/cache/

Said that, most responses with private information should not be
cachable by default because the server should mark them as such.


The current eCAP ClamAV adapter [temporary] stores message bodies on
disk to pass them to the ClamAV library for analysis. I do not know
about squidclamav.


HTH,

Alex.



From yvoinov at gmail.com  Thu Aug 13 21:15:46 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 14 Aug 2015 03:15:46 +0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CD048F.7090907@measurement-factory.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
Message-ID: <55CD0902.9010703@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


14.08.15 2:56, Alex Rousskov ?????:
> On 08/13/2015 09:38 AM, Amos Jeffries wrote:
>> On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
>>> Is it possible - by means of squid's peek and splice feature - to
>>> inspect file extensions and mime types of https traffic? Can bumped
>>> https traffic be forwarded to icap (squidclamav) for AV scanning?
>
>> Doing so is the features intended purpose.
>
>
> And you may be able to use either Secure ICAP (Squid 4) or the eCAP
> ClamAV adapter for AV scanning without transmitting bumped messages over
> plain text ICAP connections.
Yet another solution is not transmit any over net. Just setup all
services on blade system or one box.
>
>
>
>> if I just send traffic to squidclamav on icap
>> tcp port, then I don't store usernames and passwords or private emails
>> in cache?
>
> Squid caching is not related to AV scanning. If you do not disable
> caching, Squid will cache cachable responses. IIRC, the code making the
> cachability decision does not check whether the response was bumped.
> However, you may configure it to do so using the "cache" directive:
>
>   http://www.squid-cache.org/Doc/config/cache/
>
> Said that, most responses with private information should not be
> cachable by default because the server should mark them as such.
... and we ignore them due to abuse of server owners no-cache directives
when we fight for increase hit-ratio. There is millions cache-unfriendly
web servers, starting from Google...
>
>
>
> The current eCAP ClamAV adapter [temporary] stores message bodies on
> disk to pass them to the ClamAV library for analysis. I do not know
> about squidclamav.
>
>
> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVzQkBAAoJENNXIZxhPexGGDMH/jkrsMwvbDgWADFxcrapPZl8
XCi0fcJTGhO1GPvhBB/T505HMDiwoCeMU5A329i3CWpMXEPJqkllJ0AtPYrcwp7l
gL21HOx50Cqv8rWL4bZR7k9wfb3smLN/NNBZSN6HXZh1chkRhlal+x5qXcfvB7BY
+uJIRnVet0uCQoAHdXuUBH0Qlo+tVaFtlywBRdwNO84uDgW8VaKB4sruV8YO3/Em
wS55QU8nCezaIYaP014LRjh6vpAQfcer5i4FqapGMVe0Qt3VY752ayBl0hN0REN1
kdGoLgvY0263WlWvdbdGB4W1oearfKZzDXUjvwmcTiY0WzpeV+B/XlYFze3w+pg=
=f521
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri Aug 14 04:16:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2015 16:16:53 +1200
Subject: [squid-users] websockets protocol support via "squid+tproxy"
In-Reply-To: <CA+86yMgLXvaV7NwFPs73jb_EeeBBQ6w86ZSJGb-PgyenW86fWQ@mail.gmail.com>
References: <CA+86yMgLXvaV7NwFPs73jb_EeeBBQ6w86ZSJGb-PgyenW86fWQ@mail.gmail.com>
Message-ID: <55CD6BB5.3070002@treenet.co.nz>

On 14/08/2015 6:52 a.m., SaRaVanAn wrote:
> Hi All,
> We are planning to migrate our client applications from "http" to
> "websockets" protocol.

My condolances.

> Currently we are using "squid+tproxy" in the router. If request comes from
> client application to the router we use "Squid+tproxy" to forward the
> traffic to local web-server with the help of cache-peer option.
> 

> I want to confirm whether squid supports "WebSocket and a WebSocket Secure
> connection" when it acts as a transparent proxy.

Depends on your definition of "supports".

In forward-proxy Squid can relay the WebSockets CONNECT requests.

In other modes Squid will correctly handle the Upgrade requests for
Websockets. Gatewaying the traffic back into HTTP.

Squid is an HTTP proxy and gateway. It does not speak native WebSockets.

Amos



From squid3 at treenet.co.nz  Fri Aug 14 04:31:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2015 16:31:49 +1200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CD0902.9010703@gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com>
Message-ID: <55CD6F35.8090005@treenet.co.nz>

On 14/08/2015 9:15 a.m., Yuri Voinov wrote:
> 
> 
> 
> 14.08.15 2:56, Alex Rousskov ?????:
>> On 08/13/2015 09:38 AM, Amos Jeffries wrote:
>>> On 14/08/2015 12:47 a.m., Marko Cupa? wrote:
>>>> Is it possible - by means of squid's peek and splice feature - to
>>>> inspect file extensions and mime types of https traffic? Can bumped
>>>> https traffic be forwarded to icap (squidclamav) for AV scanning?
> 
>>> Doing so is the features intended purpose.
> 
> 
>> And you may be able to use either Secure ICAP (Squid 4) or the eCAP
>> ClamAV adapter for AV scanning without transmitting bumped messages over
>> plain text ICAP connections.
> Yet another solution is not transmit any over net. Just setup all
> services on blade system or one box.
> 

Like Alex said the design of Clam' AV and toolchain is that it uses disk
storage for relaying objects between processes. There are some popular
security policies where disk storage is forbidden.


> 
>>> if I just send traffic to squidclamav on icap
>>> tcp port, then I don't store usernames and passwords or private emails
>>> in cache?
> 
>> Squid caching is not related to AV scanning. If you do not disable
>> caching, Squid will cache cachable responses. IIRC, the code making the
>> cachability decision does not check whether the response was bumped.
>> However, you may configure it to do so using the "cache" directive:
> 
>>   http://www.squid-cache.org/Doc/config/cache/

Or alternatively use a memory-only proxy cache. This allows a large
portion of the caching HIT benefits to still be gained without violating
any security requirements about persistent storage of TLS or HTTPS
message data.
 That only covers the Squid cache storage part of the system though.


> 
>> Said that, most responses with private information should not be
>> cachable by default because the server should mark them as such.
>
> ... and we ignore them due to abuse of server owners no-cache directives
> when we fight for increase hit-ratio. There is millions cache-unfriendly
> web servers, starting from Google...


No Yuri. The confusing "no-cache" control fequently used means only that
the proxy needs to revalidate the cache HIT content and headers before
delivering to the client.

All current Squid releases do that correctly. The squid.conf settings
once available to ignore/override are no longer existing.


Alex was talking of "private" and "no-store" directives. Their meaning
is clear and precise - not easily confused. Overriding those is somewhat
stupid.


> 
>> The current eCAP ClamAV adapter [temporary] stores message bodies on
>> disk to pass them to the ClamAV library for analysis. I do not know
>> about squidclamav.
> 

It seemed to do the same when I checked it a few months ago. AFAICS it
is the backend AV library only scanning disk objects that causes the
whole issue. Otherwise the eCAP could be much, much faster.

Amos


From vin.krish25 at gmail.com  Fri Aug 14 06:05:49 2015
From: vin.krish25 at gmail.com (vin_krish)
Date: Thu, 13 Aug 2015 23:05:49 -0700 (PDT)
Subject: [squid-users] FATAL: Could not create a DNS socket in squid
	3.5.3 stable
In-Reply-To: <55CB3D17.9020109@treenet.co.nz>
References: <1439377294506-4672656.post@n4.nabble.com>
 <55CB3D17.9020109@treenet.co.nz>
Message-ID: <1439532349182-4672699.post@n4.nabble.com>

Hi,

     Thanks for the reply. I'm running squid as root user itself and have
all permissions. But also same error appears. In case of ubuntu OS, using
same configuration the error doesn't appear. Only in linux(ARM) this
happens. Is there something I'm missing ..?? Please help me ..


Regards,
krish



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-Could-not-create-a-DNS-socket-in-squid-3-5-3-stable-tp4672656p4672699.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From arjen at arjenvandermeer.eu  Fri Aug 14 08:57:52 2015
From: arjen at arjenvandermeer.eu (Arjen van der Meer)
Date: Fri, 14 Aug 2015 10:57:52 +0200
Subject: [squid-users] Presenting an internal virtual host externally as
	domain root folder (Amos Jeffries)
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAGb9FoXPdctLjZCLBqoOYbcBAAAAAA==@arjenvandermeer.eu>

Good morning Amos,

And thanks for your reply. But I think this is what I already have configured, unless you notice errors in the following configuration:

acl wordpress urlpath_regex ^/wordpress

cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
cache_peer_access ubuntu deny wordpress

cache_peer 192.168.1.153 parent 80 0 no-query originserver name=diskstation
cache_peer_access diskstation allow wordpress
cache_peer_access diskstation deny all

acl arjenvandermeer_acl dstdomain .arjenvandermeer.eu
http_access allow arjenvandermeer_acl
cache_peer_access ubuntu allow arjenvandermeer_acl
cache_peer_access diskstation allow arjenvandermeer_acl

And this works fine, but it only makes www.arjenvandermeer.eu/wordpress available which is just part of what I want. What I'd like is this website to be externally available as www.arjenvandermeer.eu without rewrite or redirect, while having www.arjenvandermeer.eu/zm and several other sites still available on my Ubuntu system.


Kind regards,

Arjen

------------------------------

Message: 5
Date: Fri, 14 Aug 2015 03:24:31 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Presenting an internal virtual host
	externally as domain root folder
Message-ID: <55CCB6AF.3000102 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 14/08/2015 12:55 a.m., Arjen van der Meer wrote:
>
> Since a couple of months I have installed and configured squid on my
> raspberry pi with raspbian and most things I like to work do! One thing
> however I can't get to work. Not even with the help of what I found on the
> internet so far. That is that I like to present my internal wordpress on my
> diskstation as the domain root on the outside. I want my internal
> http://192.168.1.153/wordpress to be available on the outside as
> www.arjenvandermeer.eu/ without anything else. I have only one external IP
> address so I need to use something like squid to make both my Ubuntu system
> and the Synology available on the outside. I could use a rewriter or a
> redirect to add the /wordpress part but it is much neater if my internal
> /wordpress can present itself as domain root on the outside. My Cisco router
> port forwards http and ssl to my pi lan ip from where squid picks it up.
> 
>  
> 
> Can this be done (with squid) and if so please help me out or point me to
> some matching examples.
> 

What you seek is written at the bottom of this page:
<http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers>


Amos



------------------------------



From Antony.Stone at squid.open.source.it  Fri Aug 14 10:37:49 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 14 Aug 2015 12:37:49 +0200
Subject: [squid-users] Presenting an internal virtual host externally as
	domain root folder (Amos Jeffries)
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAGb9FoXPdctLjZCLBqoOYbcBAAAAAA==@arjenvandermeer.eu>
References: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAGb9FoXPdctLjZCLBqoOYbcBAAAAAA==@arjenvandermeer.eu>
Message-ID: <201508141237.50126.Antony.Stone@squid.open.source.it>

On Friday 14 August 2015 at 10:57:52, Arjen van der Meer wrote:

> Good morning Amos,
> 
> And thanks for your reply. But I think this is what I already have
> configured, unless you notice errors in the following configuration:

> acl wordpress urlpath_regex ^/wordpress

The above matches any URL having "/wordpress" immediately after the hostname

> cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
> cache_peer_access ubuntu deny wordpress

The above sends anything not starting with /wordpress to the Ubuntu machine.

> cache_peer 192.168.1.153 parent 80 0 no-query originserver name=diskstation
> cache_peer_access diskstation allow wordpress
> cache_peer_access diskstation deny all

The above sends the URLs starting with /wordpress to the Wordpress server, but 
only those URLs.

> And this works fine, but it only makes www.arjenvandermeer.eu/wordpress
> available which is just part of what I want. What I'd like is this website
> to be externally available as www.arjenvandermeer.eu without rewrite or
> redirect,

What do you mean by "without rewrite or redirect"?

If you want a URL which does not contain "wordpress" to be served by a server 
which only has content under the directory "wordpress", then you *have* to 
have a rewrite somewhere along the way.

I would suggest creating two ACLs:

acl wordpress urlpath_regex ^/wordpress
acl notroot urlpath_regex ^/..*

That second URL will match any URL with one or more characters after the / 
following the hostname (in other words, the *only* URLs it *won't* match are 
either the hostname itself, or the hostname plus a / and then nothing 
further).

Then you can use these to direct requests for /wordpress to the Wordpress 
server, anything else except the 'null' URL to the Ubuntu server, and the null 
entries to Wordpress (but you'll still have to have a redirect on the 
Wordpress machine so that http://server ends up serving content from 
http://server/wordpress):

cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
cache_peer_access ubuntu deny wordpress
cache_peer_access ubuntu allow notroot

cache_peer 192.168.1.153 parent 80 0 no-query originserver name=diskstation
cache_peer_access diskstation allow wordpress
cache_peer_access diskstation deny notroot
cache_peer_access diskstation allow all

> while having www.arjenvandermeer.eu/zm and several other sites
> still available on my Ubuntu system.

Regards,


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Fri Aug 14 10:41:45 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 14 Aug 2015 12:41:45 +0200
Subject: [squid-users] Presenting an internal virtual host externally as
	domain root folder (Amos Jeffries)
In-Reply-To: <201508141237.50126.Antony.Stone@squid.open.source.it>
References: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAGb9FoXPdctLjZCLBqoOYbcBAAAAAA==@arjenvandermeer.eu>
 <201508141237.50126.Antony.Stone@squid.open.source.it>
Message-ID: <201508141241.45471.Antony.Stone@squid.open.source.it>

On Friday 14 August 2015 at 12:37:49, Antony Stone wrote:

> On Friday 14 August 2015 at 10:57:52, Arjen van der Meer wrote:
> > Good morning Amos,
> > 
> > And thanks for your reply. But I think this is what I already have
> > configured, unless you notice errors in the following configuration:
> > 
> > acl wordpress urlpath_regex ^/wordpress
> 
> The above matches any URL having "/wordpress" immediately after the
> hostname
> 
> > cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
> > cache_peer_access ubuntu deny wordpress
> 
> The above sends anything not starting with /wordpress to the Ubuntu
> machine.
> 
> > cache_peer 192.168.1.153 parent 80 0 no-query originserver
> > name=diskstation cache_peer_access diskstation allow wordpress
> > cache_peer_access diskstation deny all
> 
> The above sends the URLs starting with /wordpress to the Wordpress server,
> but only those URLs.
> 
> > And this works fine, but it only makes www.arjenvandermeer.eu/wordpress
> > available which is just part of what I want. What I'd like is this
> > website to be externally available as www.arjenvandermeer.eu without
> > rewrite or redirect,
> 
> What do you mean by "without rewrite or redirect"?
> 
> If you want a URL which does not contain "wordpress" to be served by a
> server which only has content under the directory "wordpress", then you
> *have* to have a rewrite somewhere along the way.
> 
> I would suggest creating two ACLs:
> 
> acl wordpress urlpath_regex ^/wordpress
> acl notroot urlpath_regex ^/..*

Actually, the .* is not required there - all you need is the ^/. to match a / 
and any other character immediately after the hostname.

> That second URL will match any URL with one or more characters after the /
> following the hostname (in other words, the *only* URLs it *won't* match
> are either the hostname itself, or the hostname plus a / and then nothing
> further).
> 
> Then you can use these to direct requests for /wordpress to the Wordpress
> server, anything else except the 'null' URL to the Ubuntu server, and the
> null entries to Wordpress (but you'll still have to have a redirect on the
> Wordpress machine so that http://server ends up serving content from
> http://server/wordpress):
> 
> cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
> cache_peer_access ubuntu deny wordpress
> cache_peer_access ubuntu allow notroot
> 
> cache_peer 192.168.1.153 parent 80 0 no-query originserver name=diskstation
> cache_peer_access diskstation allow wordpress
> cache_peer_access diskstation deny notroot
> cache_peer_access diskstation allow all
> 
> > while having www.arjenvandermeer.eu/zm and several other sites
> > still available on my Ubuntu system.
> 
> Regards,
> 
> 
> Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From arjen at arjenvandermeer.eu  Fri Aug 14 12:10:54 2015
From: arjen at arjenvandermeer.eu (Arjen van der Meer)
Date: Fri, 14 Aug 2015 14:10:54 +0200
Subject: [squid-users] squid-users Digest, Vol 12, Issue 24
In-Reply-To: <mailman.3.1439553601.7502.squid-users@lists.squid-cache.org>
References: <mailman.3.1439553601.7502.squid-users@lists.squid-cache.org>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAN7vGW3KgedBhC6Y2h5XdqIBAAAAAA==@arjenvandermeer.eu>

Dear Antony (and Amos),

Thanks again for your replies. This answers my question, however I had hoped for a solution that would make /wordpress invisible from the outside. 

I will reinstate the rewriter that did the  before but exposes the /wordpress in the URL.

Kind regards,

Arjen.



Message: 6
Date: Fri, 14 Aug 2015 12:41:45 +0200
From: Antony Stone <Antony.Stone at squid.open.source.it>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Presenting an internal virtual host
	externally as	domain root folder (Amos Jeffries)
Message-ID: <201508141241.45471.Antony.Stone at squid.open.source.it>
Content-Type: Text/Plain;  charset="utf-8"

On Friday 14 August 2015 at 12:37:49, Antony Stone wrote:

> On Friday 14 August 2015 at 10:57:52, Arjen van der Meer wrote:
> > Good morning Amos,
> > 
> > And thanks for your reply. But I think this is what I already have
> > configured, unless you notice errors in the following configuration:
> > 
> > acl wordpress urlpath_regex ^/wordpress
> 
> The above matches any URL having "/wordpress" immediately after the
> hostname
> 
> > cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
> > cache_peer_access ubuntu deny wordpress
> 
> The above sends anything not starting with /wordpress to the Ubuntu
> machine.
> 
> > cache_peer 192.168.1.153 parent 80 0 no-query originserver
> > name=diskstation cache_peer_access diskstation allow wordpress
> > cache_peer_access diskstation deny all
> 
> The above sends the URLs starting with /wordpress to the Wordpress server,
> but only those URLs.
> 
> > And this works fine, but it only makes www.arjenvandermeer.eu/wordpress
> > available which is just part of what I want. What I'd like is this
> > website to be externally available as www.arjenvandermeer.eu without
> > rewrite or redirect,
> 
> What do you mean by "without rewrite or redirect"?
> 
> If you want a URL which does not contain "wordpress" to be served by a
> server which only has content under the directory "wordpress", then you
> *have* to have a rewrite somewhere along the way.
> 
> I would suggest creating two ACLs:
> 
> acl wordpress urlpath_regex ^/wordpress
> acl notroot urlpath_regex ^/..*

Actually, the .* is not required there - all you need is the ^/. to match a / 
and any other character immediately after the hostname.

> That second URL will match any URL with one or more characters after the /
> following the hostname (in other words, the *only* URLs it *won't* match
> are either the hostname itself, or the hostname plus a / and then nothing
> further).
> 
> Then you can use these to direct requests for /wordpress to the Wordpress
> server, anything else except the 'null' URL to the Ubuntu server, and the
> null entries to Wordpress (but you'll still have to have a redirect on the
> Wordpress machine so that http://server ends up serving content from
> http://server/wordpress):
> 
> cache_peer 192.168.1.150 parent 80 0 no-query originserver name=ubuntu
> cache_peer_access ubuntu deny wordpress
> cache_peer_access ubuntu allow notroot
> 
> cache_peer 192.168.1.153 parent 80 0 no-query originserver name=diskstation
> cache_peer_access diskstation allow wordpress
> cache_peer_access diskstation deny notroot
> cache_peer_access diskstation allow all
> 
> > while having www.arjenvandermeer.eu/zm and several other sites
> > still available on my Ubuntu system.
> 
> Regards,
> 
> 
> Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.



------------------------------




From ashish.mukherjee at gmail.com  Fri Aug 14 12:39:14 2015
From: ashish.mukherjee at gmail.com (Ashish Mukherjee)
Date: Fri, 14 Aug 2015 18:09:14 +0530
Subject: [squid-users] Enquiry about C ICAP
Message-ID: <CACgMzfzLSn+ZvWwVmio-Wc_wNu2vV6wZxu=D4rOWLPV26YvfuA@mail.gmail.com>

Hello,

I am trying to write an ICAP module for the C ICAP server. My purpose is to
rewrite HTTP responses which flow through my Squid proxy. However, the
documentation to write such service modules and build them doesn't seem
very good. Does anyone have pointers to some good tutorial online or any
better documentation?

Regards,
Ashish
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150814/b4cd7c0a/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Aug 14 12:50:00 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 14 Aug 2015 14:50:00 +0200
Subject: [squid-users] Presenting an internal virtual host externally as
	domain root folder
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAN7vGW3KgedBhC6Y2h5XdqIBAAAAAA==@arjenvandermeer.eu>
References: <mailman.3.1439553601.7502.squid-users@lists.squid-cache.org>
 <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAN7vGW3KgedBhC6Y2h5XdqIBAAAAAA==@arjenvandermeer.eu>
Message-ID: <201508141450.00271.Antony.Stone@squid.open.source.it>

On Friday 14 August 2015 at 14:10:54, Arjen van der Meer wrote:

> Dear Antony (and Amos),
> 
> Thanks again for your replies. This answers my question, however I had
> hoped for a solution that would make /wordpress invisible from the
> outside.

In that case remove the ^/wordpress ACL from your squid config, and make sure 
wordpress itself is configured to present URLs in links etc without /wordpress 
in them.  How to do that is beyond my expertise and outside the scope of a 
question on the Squid users' list, but it's got to be possible.


Regards,


Antony.

-- 
"Linux is going to be part of the future. It's going to be like Unix was."

 - Peter Moore, Asia-Pacific general manager, Microsoft

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dan at djph.net  Fri Aug 14 13:15:48 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 14 Aug 2015 09:15:48 -0400
Subject: [squid-users] Presenting an internal virtual host externally as
 domain root folder
In-Reply-To: <201508141450.00271.Antony.Stone@squid.open.source.it>
References: <mailman.3.1439553601.7502.squid-users@lists.squid-cache.org>
 <!&!AAAAAAAAAAAYAAAAAAAAAHtV6SDbDL9AhId/75xvyDZigQAAEAAAAN7vGW3KgedBhC6Y2h5XdqIBAAAAAA==@arjenvandermeer.eu>
 <201508141450.00271.Antony.Stone@squid.open.source.it>
Message-ID: <20150814091548.Horde.Z8vALJxfgRoxo-bblZYFYg5@192.168.10.20>

Quoting Antony Stone <Antony.Stone at squid.open.source.it>:

> On Friday 14 August 2015 at 14:10:54, Arjen van der Meer wrote:
>
>> Dear Antony (and Amos),
>>
>> Thanks again for your replies. This answers my question, however I had
>> hoped for a solution that would make /wordpress invisible from the
>> outside.
>
> In that case remove the ^/wordpress ACL from your squid config, and make sure
> wordpress itself is configured to present URLs in links etc without  
> /wordpress
> in them.  How to do that is beyond my expertise and outside the scope of a
> question on the Squid users' list, but it's got to be possible.

As luck would have it, I know how (well, one way anyway) to do it.

You have to point the root folder (i.e. in your apache configuration)  
to the /wordpress folder (i.e. "/var/www/somesite/wordpress" instead  
of "/var/www/somesite").

If your blog is not the main focus of your site (i.e.  
'http://somesite.com' isn't just for your blog), you'll probably do  
better to create a subdomain (blog.somesite.com) so that you don't  
make a mess of things ;)

Regards,
Dan
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4387 bytes
Desc: S/MIME Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150814/5ec9bf80/attachment.bin>

From dweimer at dweimer.net  Fri Aug 14 14:03:14 2015
From: dweimer at dweimer.net (dweimer)
Date: Fri, 14 Aug 2015 09:03:14 -0500
Subject: [squid-users] Squid 3.5 Forward Secrecy on https_port
In-Reply-To: <55CCB53D.9080802@treenet.co.nz>
References: <11a4d70a69ef46a43004a2ba1ed35761@dweimer.net>
 <be1e798e6265a56ac628d5fb9a9243df@dweimer.net>
 <55CBB63B.50901@urlfilterdb.com>
 <OF269531D0.681C6C55-ON87257EA0.004FA710-85257EA0.00509FE3@us.ibm.com>
 <55CCB53D.9080802@treenet.co.nz>
Message-ID: <bd0a5c81a2948a0fcc80ac78b1fc9a2a@dweimer.net>

On 2015-08-13 10:18 am, Amos Jeffries wrote:
> On 14/08/2015 2:40 a.m., Julianne Bielski wrote:
>> 
>> But does this mean that ECDHE isn't supported by Squid?
>> 
> 
> Correct. ECDHE is not supported by 3.5 and older.
> 
> EECDHE and ECDHE are coming in Squid-4.
> 
> If you really need it you are welcome to download and use Squid-4. Some
> of us already are. Just be aware that it is still under development so
> anything can change without notice, and there are probably a bunch of
> bugs not yet found in those features and other new code.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks for all the info on this guys, I have no actual requirement to 
have ECDHE implemented on the servers I maintain. I was just trying to 
improve security by enabling the Forward Secrecy options where possible. 
As some of the browsers support ECDHE and not DHE, IE8-10 for example. I 
will do some more research on the issue mentioned by a previous poster 
between now and when version 4 comes out, then decide if I do want to 
enable it or not at that time.
After some playing around on the test system, testing results using the 
ssllabs test tools with various options and dhparam key sizes, along 
with the input from this thread. I have enabled the DHE ciphers on the 
production reverse proxy server that I maintain at work last night.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From rousskov at measurement-factory.com  Fri Aug 14 14:51:29 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Aug 2015 08:51:29 -0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CD6F35.8090005@treenet.co.nz>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
Message-ID: <55CE0071.7050606@measurement-factory.com>

On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> AFAICS it
> is the backend AV library only scanning disk objects that causes the
> whole issue. Otherwise the eCAP could be much, much faster.

The situation is more nuanced: eCAP supports asynchronous adapters. It
is possible to write a ClamAV adapter that writes messages to disk and
analyses them without blocking Squid. Doing so should eliminate most
overheads between Squid and ClamAV.

Factory ClamAV adapter can run in asynchronous mode, but threads are
only used for _analysis_ of written files. We have not optimized the
file writing part (yet?). Hopefully, using a RAM-based file system can
mitigate a large part of that performance damage (as well as address
some of the security concerns related to disk storage?).

A bigger performance problem, AFAICT, is that ClamAV does not support
incremental analysis. It waits for the entire file to be downloaded
first. This breaks the message delivery pipeline and increases
user-perceived response time. This problem cannot be solved outside the
ClamAV library.


Cheers,

Alex.



From stan.prescott at gmail.com  Sun Aug 16 14:27:03 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 16 Aug 2015 10:27:03 -0400
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55CE0071.7050606@measurement-factory.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
 <55CD048F.7090907@measurement-factory.com> <55CD0902.9010703@gmail.com>
 <55CD6F35.8090005@treenet.co.nz> <55CE0071.7050606@measurement-factory.com>
Message-ID: <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>

I have SquidClamAV implemented with the Smoothwall Express 3.1 firewall. It
works well and fast with ssl-bump, although the majority of our users only
have relatively small networks with smaller loads.

FYI, E2Guardian has replaced the DansGuardian project and is currently well
maintained. E2Guardian can do content filtering for SSL but only in
explicit mode, It currently does not support intercept (transparent) mode
for SSLBump.

On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> > AFAICS it
> > is the backend AV library only scanning disk objects that causes the
> > whole issue. Otherwise the eCAP could be much, much faster.
>
> The situation is more nuanced: eCAP supports asynchronous adapters. It
> is possible to write a ClamAV adapter that writes messages to disk and
> analyses them without blocking Squid. Doing so should eliminate most
> overheads between Squid and ClamAV.
>
> Factory ClamAV adapter can run in asynchronous mode, but threads are
> only used for _analysis_ of written files. We have not optimized the
> file writing part (yet?). Hopefully, using a RAM-based file system can
> mitigate a large part of that performance damage (as well as address
> some of the security concerns related to disk storage?).
>
> A bigger performance problem, AFAICT, is that ClamAV does not support
> incremental analysis. It waits for the entire file to be downloaded
> first. This breaks the message delivery pipeline and increases
> user-perceived response time. This problem cannot be solved outside the
> ClamAV library.
>
>
> Cheers,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150816/4d6c035c/attachment.htm>

From yvoinov at gmail.com  Sun Aug 16 20:07:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 17 Aug 2015 02:07:32 +0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
Message-ID: <55D0ED84.60501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
ufdbguard does.

16.08.15 20:27, Stanford Prescott ?????:
> I have SquidClamAV implemented with the Smoothwall Express 3.1 firewall. It
> works well and fast with ssl-bump, although the majority of our users only
> have relatively small networks with smaller loads.
>
> FYI, E2Guardian has replaced the DansGuardian project and is currently
well
> maintained. E2Guardian can do content filtering for SSL but only in
> explicit mode, It currently does not support intercept (transparent) mode
> for SSLBump.
>
> On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> rousskov at measurement-factory.com> wrote:
>
>> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
>>> AFAICS it
>>> is the backend AV library only scanning disk objects that causes the
>>> whole issue. Otherwise the eCAP could be much, much faster.
>>
>> The situation is more nuanced: eCAP supports asynchronous adapters. It
>> is possible to write a ClamAV adapter that writes messages to disk and
>> analyses them without blocking Squid. Doing so should eliminate most
>> overheads between Squid and ClamAV.
>>
>> Factory ClamAV adapter can run in asynchronous mode, but threads are
>> only used for _analysis_ of written files. We have not optimized the
>> file writing part (yet?). Hopefully, using a RAM-based file system can
>> mitigate a large part of that performance damage (as well as address
>> some of the security concerns related to disk storage?).
>>
>> A bigger performance problem, AFAICT, is that ClamAV does not support
>> incremental analysis. It waits for the entire file to be downloaded
>> first. This breaks the message delivery pipeline and increases
>> user-perceived response time. This problem cannot be solved outside the
>> ClamAV library.
>>
>>
>> Cheers,
>>
>> Alex.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV0O2DAAoJENNXIZxhPexGd40H+wfKHdOmCXaxPQ9TJYvY/qQz
mMLs2qLWae+TnC5Dt89SxOJ8oywodjyZo8BwZWeCnF+oYTjleUoglYxz2d9aJKNi
rONKuAzsOXGekQ4GXv7t7CV2VtUljlppNccw7adWLAPnd4KwrbuRS73hHdoTV3eO
0V951eXnXOttVaW9IOL6kYh6jJtajZBkfoKoEMIR8d+q60vfM+tu7TiLAQHOjxPT
SULJjB3U0swSlG70IwzD5HB/OwdDSlOCHB26A4BkTN9xoUn4gwxD2dRCxlNommtW
MTF0A8s6lJexG4OQ9ci1E909HcfaE7iQJyFonj2st51m0P7aUC5r5DIs9A7/Fbc=
=7hQ0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/be97e141/attachment.htm>

From stan.prescott at gmail.com  Sun Aug 16 22:03:05 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 16 Aug 2015 18:03:05 -0400
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55D0ED84.60501@gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
 <55CD048F.7090907@measurement-factory.com> <55CD0902.9010703@gmail.com>
 <55CD6F35.8090005@treenet.co.nz> <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
Message-ID: <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>

ufdbGuard is not a content filter.

On Sun, Aug 16, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> ufdbguard does.
>
> 16.08.15 20:27, Stanford Prescott ?????:
>
> > I have SquidClamAV implemented with the Smoothwall Express 3.1 firewall.
> It
> > works well and fast with ssl-bump, although the majority of our users
> only
> > have relatively small networks with smaller loads.
> >
> > FYI, E2Guardian has replaced the DansGuardian project and is currently
> well
> > maintained. E2Guardian can do content filtering for SSL but only in
> > explicit mode, It currently does not support intercept (transparent) mode
> > for SSLBump.
> >
> > On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> > rousskov at measurement-factory.com> wrote:
> >
> >> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> >>> AFAICS it
> >>> is the backend AV library only scanning disk objects that causes the
> >>> whole issue. Otherwise the eCAP could be much, much faster.
> >>
> >> The situation is more nuanced: eCAP supports asynchronous adapters. It
> >> is possible to write a ClamAV adapter that writes messages to disk and
> >> analyses them without blocking Squid. Doing so should eliminate most
> >> overheads between Squid and ClamAV.
> >>
> >> Factory ClamAV adapter can run in asynchronous mode, but threads are
> >> only used for _analysis_ of written files. We have not optimized the
> >> file writing part (yet?). Hopefully, using a RAM-based file system can
> >> mitigate a large part of that performance damage (as well as address
> >> some of the security concerns related to disk storage?).
> >>
> >> A bigger performance problem, AFAICT, is that ClamAV does not support
> >> incremental analysis. It waits for the entire file to be downloaded
> >> first. This breaks the message delivery pipeline and increases
> >> user-perceived response time. This problem cannot be solved outside the
> >> ClamAV library.
> >>
> >>
> >> Cheers,
> >>
> >> Alex.
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV0O2DAAoJENNXIZxhPexGd40H+wfKHdOmCXaxPQ9TJYvY/qQz
> mMLs2qLWae+TnC5Dt89SxOJ8oywodjyZo8BwZWeCnF+oYTjleUoglYxz2d9aJKNi
> rONKuAzsOXGekQ4GXv7t7CV2VtUljlppNccw7adWLAPnd4KwrbuRS73hHdoTV3eO
> 0V951eXnXOttVaW9IOL6kYh6jJtajZBkfoKoEMIR8d+q60vfM+tu7TiLAQHOjxPT
> SULJjB3U0swSlG70IwzD5HB/OwdDSlOCHB26A4BkTN9xoUn4gwxD2dRCxlNommtW
> MTF0A8s6lJexG4OQ9ci1E909HcfaE7iQJyFonj2st51m0P7aUC5r5DIs9A7/Fbc=
> =7hQ0
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150816/b7737e4d/attachment.htm>

From yvoinov at gmail.com  Sun Aug 16 22:10:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 17 Aug 2015 04:10:52 +0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
Message-ID: <55D10A6C.3050909@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
O, really?

17.08.15 4:03, Stanford Prescott ?????:
> ufdbGuard is not a content filter.
>
> On Sun, Aug 16, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>
> ufdbguard does.
>
> 16.08.15 20:27, Stanford Prescott ?????:
>
> >>> I have SquidClamAV implemented with the Smoothwall Express 3.1
firewall.
> It
> >>> works well and fast with ssl-bump, although the majority of our users
> only
> >>> have relatively small networks with smaller loads.
> >>>
> >>> FYI, E2Guardian has replaced the DansGuardian project and is currently
> well
> >>> maintained. E2Guardian can do content filtering for SSL but only in
> >>> explicit mode, It currently does not support intercept
(transparent) mode
> >>> for SSLBump.
> >>>
> >>> On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> >>> rousskov at measurement-factory.com> wrote:
> >>>
> >>>> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> >>>>> AFAICS it
> >>>>> is the backend AV library only scanning disk objects that causes the
> >>>>> whole issue. Otherwise the eCAP could be much, much faster.
> >>>>
> >>>> The situation is more nuanced: eCAP supports asynchronous
adapters. It
> >>>> is possible to write a ClamAV adapter that writes messages to
disk and
> >>>> analyses them without blocking Squid. Doing so should eliminate most
> >>>> overheads between Squid and ClamAV.
> >>>>
> >>>> Factory ClamAV adapter can run in asynchronous mode, but threads are
> >>>> only used for _analysis_ of written files. We have not optimized the
> >>>> file writing part (yet?). Hopefully, using a RAM-based file
system can
> >>>> mitigate a large part of that performance damage (as well as address
> >>>> some of the security concerns related to disk storage?).
> >>>>
> >>>> A bigger performance problem, AFAICT, is that ClamAV does not support
> >>>> incremental analysis. It waits for the entire file to be downloaded
> >>>> first. This breaks the message delivery pipeline and increases
> >>>> user-perceived response time. This problem cannot be solved
outside the
> >>>> ClamAV library.
> >>>>
> >>>>
> >>>> Cheers,
> >>>>
> >>>> Alex.
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV0QpsAAoJENNXIZxhPexG24gIAMNuWsyfn/QkXWTXROZEJYL1
0frhC+w22fjV8svGjTrZEtSKY4LTHiHEjp99bPBEpPdoCURifUq20m018qRoIcEA
XZfadD+s47bT7FvZbc2W58BQZUsWvotQRMNDPE+Mf0e38ev6PXsj16SaHmWytdx2
Z9H0y5qlgJwwbUyfps4uQn1wF16Qlf2Fw5TGRUbBrij+rjPYzDSXTXxtfT+4j/3V
4lZ0bN0HSFfvJrbfcpPoMCnSlRyJOm/b6Rxqv7v733OtrY/41EW1+HE1HOmW0em3
rwpAV1KgWrwMZYHcIBE147itXlz1RGQutX01auiiSvm/hO3h78rl6aSawmanOAM=
=GgTR
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/d7c6f6ae/attachment.htm>

From stan.prescott at gmail.com  Sun Aug 16 23:04:34 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 16 Aug 2015 19:04:34 -0400
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55D10A6C.3050909@gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
 <55CD048F.7090907@measurement-factory.com> <55CD0902.9010703@gmail.com>
 <55CD6F35.8090005@treenet.co.nz> <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
Message-ID: <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>

Yes, really. ufdbGuard, like squidGuard before it, is a URL Filter that
filters known unwanted URLs. A content filter, like DansGuardian and
E2Guardian are content filters which examine the content of web pages
looking for unwanted things.

On Sun, Aug 16, 2015 at 6:10 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> O, really?
>
> 17.08.15 4:03, Stanford Prescott ?????:
> > ufdbGuard is not a content filter.
> >
> > On Sun, Aug 16, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com>
> <yvoinov at gmail.com> wrote:
> >
> >>
> > ufdbguard does.
> >
> > 16.08.15 20:27, Stanford Prescott ?????:
> >
> > >>> I have SquidClamAV implemented with the Smoothwall Express 3.1
> firewall.
> > It
> > >>> works well and fast with ssl-bump, although the majority of our users
> > only
> > >>> have relatively small networks with smaller loads.
> > >>>
> > >>> FYI, E2Guardian has replaced the DansGuardian project and is
> currently
> > well
> > >>> maintained. E2Guardian can do content filtering for SSL but only in
> > >>> explicit mode, It currently does not support intercept (transparent)
> mode
> > >>> for SSLBump.
> > >>>
> > >>> On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> > >>> rousskov at measurement-factory.com> wrote:
> > >>>
> > >>>> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> > >>>>> AFAICS it
> > >>>>> is the backend AV library only scanning disk objects that causes
> the
> > >>>>> whole issue. Otherwise the eCAP could be much, much faster.
> > >>>>
> > >>>> The situation is more nuanced: eCAP supports asynchronous adapters.
> It
> > >>>> is possible to write a ClamAV adapter that writes messages to disk
> and
> > >>>> analyses them without blocking Squid. Doing so should eliminate most
> > >>>> overheads between Squid and ClamAV.
> > >>>>
> > >>>> Factory ClamAV adapter can run in asynchronous mode, but threads are
> > >>>> only used for _analysis_ of written files. We have not optimized the
> > >>>> file writing part (yet?). Hopefully, using a RAM-based file system
> can
> > >>>> mitigate a large part of that performance damage (as well as address
> > >>>> some of the security concerns related to disk storage?).
> > >>>>
> > >>>> A bigger performance problem, AFAICT, is that ClamAV does not
> support
> > >>>> incremental analysis. It waits for the entire file to be downloaded
> > >>>> first. This breaks the message delivery pipeline and increases
> > >>>> user-perceived response time. This problem cannot be solved outside
> the
> > >>>> ClamAV library.
> > >>>>
> > >>>>
> > >>>> Cheers,
> > >>>>
> > >>>> Alex.
> > >>>>
> > >>>> _______________________________________________
> > >>>> squid-users mailing list
> > >>>> squid-users at lists.squid-cache.org
> > >>>> http://lists.squid-cache.org/listinfo/squid-users
> > >>>>
> > >>>
> > >>>
> > >>>
> > >>> _______________________________________________
> > >>> squid-users mailing list
> > >>> squid-users at lists.squid-cache.org
> > >>> http://lists.squid-cache.org/listinfo/squid-users
> >
> >>
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >>
> >
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV0QpsAAoJENNXIZxhPexG24gIAMNuWsyfn/QkXWTXROZEJYL1
> 0frhC+w22fjV8svGjTrZEtSKY4LTHiHEjp99bPBEpPdoCURifUq20m018qRoIcEA
> XZfadD+s47bT7FvZbc2W58BQZUsWvotQRMNDPE+Mf0e38ev6PXsj16SaHmWytdx2
> Z9H0y5qlgJwwbUyfps4uQn1wF16Qlf2Fw5TGRUbBrij+rjPYzDSXTXxtfT+4j/3V
> 4lZ0bN0HSFfvJrbfcpPoMCnSlRyJOm/b6Rxqv7v733OtrY/41EW1+HE1HOmW0em3
> rwpAV1KgWrwMZYHcIBE147itXlz1RGQutX01auiiSvm/hO3h78rl6aSawmanOAM=
> =GgTR
> -----END PGP SIGNATURE-----
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150816/b4cb7f9d/attachment.htm>

From marko.cupac at mimar.rs  Mon Aug 17 07:59:42 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Mon, 17 Aug 2015 09:59:42 +0200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz>
 <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
Message-ID: <20150817095942.3d771e8c@efreet>

On Sun, 16 Aug 2015 10:27:03 -0400
Stanford Prescott <stan.prescott at gmail.com> wrote:

> FYI, E2Guardian has replaced the DansGuardian project and is
> currently well maintained. E2Guardian can do content filtering for
> SSL but only in explicit mode, It currently does not support
> intercept (transparent) mode for SSLBump.

Thanks for information about E2Guardian, I was not aware of it.
Dansguardian didn't support kerberos authentication, and didn't have
plans to ever support it, that's why I decided to move away from it to
'pure squid' setup.
-- 
Marko Cupa?
https://www.mimar.rs/


From yvoinov at gmail.com  Mon Aug 17 08:56:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 17 Aug 2015 14:56:32 +0600
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
Message-ID: <55D1A1C0.1050106@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Either SquidGuard, or ufdbGuard has this functional onto blocking page.
Just configure it.

17.08.15 5:04, Stanford Prescott ?????:
> Yes, really. ufdbGuard, like squidGuard before it, is a URL Filter that
> filters known unwanted URLs. A content filter, like DansGuardian and
> E2Guardian are content filters which examine the content of web pages
> looking for unwanted things.
>
> On Sun, Aug 16, 2015 at 6:10 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>
> O, really?
>
> 17.08.15 4:03, Stanford Prescott ?????:
> >>> ufdbGuard is not a content filter.
> >>>
> >>> On Sun, Aug 16, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com>
> <yvoinov at gmail.com> wrote:
> >>>
> >>>>
> >>> ufdbguard does.
> >>>
> >>> 16.08.15 20:27, Stanford Prescott ?????:
> >>>
> >>>>>> I have SquidClamAV implemented with the Smoothwall Express 3.1
> firewall.
> >>> It
> >>>>>> works well and fast with ssl-bump, although the majority of our
users
> >>> only
> >>>>>> have relatively small networks with smaller loads.
> >>>>>>
> >>>>>> FYI, E2Guardian has replaced the DansGuardian project and is
> currently
> >>> well
> >>>>>> maintained. E2Guardian can do content filtering for SSL but only in
> >>>>>> explicit mode, It currently does not support intercept
(transparent)
> mode
> >>>>>> for SSLBump.
> >>>>>>
> >>>>>> On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> >>>>>> rousskov at measurement-factory.com> wrote:
> >>>>>>
> >>>>>>> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> >>>>>>>> AFAICS it
> >>>>>>>> is the backend AV library only scanning disk objects that causes
> the
> >>>>>>>> whole issue. Otherwise the eCAP could be much, much faster.
> >>>>>>>
> >>>>>>> The situation is more nuanced: eCAP supports asynchronous
adapters.
> It
> >>>>>>> is possible to write a ClamAV adapter that writes messages to disk
> and
> >>>>>>> analyses them without blocking Squid. Doing so should
eliminate most
> >>>>>>> overheads between Squid and ClamAV.
> >>>>>>>
> >>>>>>> Factory ClamAV adapter can run in asynchronous mode, but
threads are
> >>>>>>> only used for _analysis_ of written files. We have not
optimized the
> >>>>>>> file writing part (yet?). Hopefully, using a RAM-based file system
> can
> >>>>>>> mitigate a large part of that performance damage (as well as
address
> >>>>>>> some of the security concerns related to disk storage?).
> >>>>>>>
> >>>>>>> A bigger performance problem, AFAICT, is that ClamAV does not
> support
> >>>>>>> incremental analysis. It waits for the entire file to be
downloaded
> >>>>>>> first. This breaks the message delivery pipeline and increases
> >>>>>>> user-perceived response time. This problem cannot be solved
outside
> the
> >>>>>>> ClamAV library.
> >>>>>>>
> >>>>>>>
> >>>>>>> Cheers,
> >>>>>>>
> >>>>>>> Alex.
> >>>>>>>
> >>>>>>> _______________________________________________
> >>>>>>> squid-users mailing list
> >>>>>>> squid-users at lists.squid-cache.org
> >>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> squid-users mailing list
> >>>>>> squid-users at lists.squid-cache.org
> >>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>>
> >>>
>
>
>>
>>
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV0aHAAAoJENNXIZxhPexGQiMIAJAAW6E7JKROzwy0B+KcCLK6
BxfcA1o+lvJYwl6drsTeBH4NzO+Ra4eYmJMC94LwYc17E8Zwj5A+1t25cfQ1orIi
5EFjiVQ+0nseAGidcBnUNM1Nw+b4Xa/WswGo9+ApmslSstO1643uwteVip8o+Blg
FuhYDuodynLNedsvxFq8/098zkZs1yc8d/pDyTAg4rQIGgU3gvxoMj3DLixFAkSy
E0Qx0jEsSBvt0ksJAgxi0dVQh3ybeQqxevLgwDPFI0DuIvDh2Ho+6jwovzv+NyWS
EK2i5CigTk2VguviWSFGmhpEkn3mvdLE5Kdj2XCB+1KFecmRv8ITwjvNNPKuh2w=
=Mh/Z
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/ca2b338a/attachment.htm>

From correiasardinha at gmail.com  Mon Aug 17 12:25:27 2015
From: correiasardinha at gmail.com (Pedro Correia Sardinha)
Date: Mon, 17 Aug 2015 13:25:27 +0100
Subject: [squid-users] Bridge/Tproxy: https dns
Message-ID: <CADLGAbK3wbE-5M_88tWJwYFcb_NdXUuv6cR9czUsNrLHanxfSg@mail.gmail.com>

Hello,

I'm trying to setup a Squid server simple as possible just to review the
web use in office using the last stable version 3.5.7.

I setup the bridge with 2 NIC, br0 with IP 192.168.0.5 and I had disable
IPv6 on boot in my Slackware Current (Fri Aug 14 2015) server.

The script on the boot for Tproxy, routing, IPTables and EBTables:
echo 50 > /proc/sys/vm/swappiness
echo 60 > /proc/sys/net/ipv4/tcp_keepalive_time
echo 10 > /proc/sys/net/ipv4/tcp_keepalive_intvl
echo 6 > /proc/sys/net/ipv4/tcp_keepalive_probes
ip -f inet rule add fwmark 1 lookup 100
ip -f inet route add local default dev lo table 100
echo 1 > /proc/sys/net/ipv4/ip_forward
echo 0 > /proc/sys/net/ipv4/conf/default/rp_filter
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter
iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 1
iptables -t mangle -A DIVERT -j ACCEPT
iptables  -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
iptables  -t mangle -A PREROUTING -p tcp ! --destination 192.168.0.5
--dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129
iptables  -t mangle -A PREROUTING -p tcp ! --destination 192.168.0.5
--dport 443 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3130
CLIENT_IFACE=eth1
INET_IFACE=eth0
ebtables -t broute -A BROUTING \
        -i $CLIENT_IFACE -p ipv4 --ip-proto tcp --ip-dport 80 \
        -j redirect --redirect-target DROP
ebtables -t broute -A BROUTING \
        -i $INET_IFACE -p ipv4 --ip-proto tcp --ip-sport 80 \
        -j redirect --redirect-target DROP
ebtables -t broute -A BROUTING \
        -i $CLIENT_IFACE -p ipv4 --ip-proto tcp --ip-dport 443 \
        -j redirect --redirect-target DROP

I compiled squid from source:
./configure \
  --prefix=/usr \
  --libdir=/usr/lib64 \
  --libexecdir=/usr/lib64/squid \
  --sysconfdir=/etc/squid \
  --localstatedir=/var/log/squid \
  --sharedstatedir=/var/lib \
  --datadir=/usr/share/squid \
  --with-pidfile=/var/run/squid \
  --mandir=/usr/man \
  --with-logdir=/var/log/squid \
  --enable-silent-rules \
  --enable-dependency-tracking \
  --with-openssl=/usr/include/openssl/ \
  --enable-ssl \
  --enable-ssl-crtd \
  --enable-icmp \
  --enable-delay-pools \
  --enable-useragent-log \
  --enable-esi \
  --enable-follow-x-forwarded-for \
  --enable-linux-netfilter \
  --enable-forward-log \
  --enable-poll \
  --enable-referer-log \
  --with-default-user=squid \
  --enable-auto-locale \
  --disable-ipv6 \
  --build=X86_64-slackware-linux

Generating the certificate:
# openssl genrsa -out squid.key 2048
# openssl req -new -key squid.key -out squid.csr
# openssl x509 -req -days 3650 -in squid.csr -signkey squid.key -out
squid.crt
# cat squid.key squid.crt > squid.pem

My squid.conf:
acl localnet src 192.168.0.0/16
acl localhost src 192.168.0.5/32
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443
acl CONNECT method CONNECT
acl ssl-bump_port myportname 3130
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access allow localnet
http_access deny all
http_reply_access allow all
icp_access allow all
tcp_outgoing_address 85.138.204.43
dns_v4_first on
pinger_enable off
http_port 3128
http_port 3129 tproxy
https_port 3130 ssl-bump tproxy generate-host-certificates=off
cert=/etc/squid/ssl/squid.pem cafile=/etc/squid/ssl/squid.pem
always_direct allow ssl-bump_port
ssl_bump none all
dns_nameservers 8.8.8.8 8.8.4.4
access_log daemon:/var/log/squid/access.log squid
cache deny all
pid_filename /var/run/squid/squid.pid
coredump_dir /var/log/squid/cache/squid
visible_hostname myservername.domain.local

In general the configuration (squid.conf) it's working but has some
incomplete behaviors as shows in log files.

access.log (I know this is Facebook but there are no dns resolusion in
https, just IP):
1439811492.625   2377 192.168.0.102 TCP_TUNNEL/200 3574 CONNECT
31.13.90.2:443 - ORIGINAL_DST/31.13.90.2 -

cache.log:
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: max-age=504747
Content-Type: application/ocsp-response
Date: Mon, 17 Aug 2015 11:38:03 GMT
ETag: "55d15943-1d7"
Expires: Sun, 23 Aug 2015 23:38:03 GMT
Last-Modified: Mon, 17 Aug 2015 03:47:15 GMT
Server: ECS (mad/439C)
X-Cache: HIT
Content-Length: 471
X-Cache: MISS from squidhead2.skywalker.local
Via: 1.1 squidhead2.skywalker.local (squid/3.5.7)
Connection: keep-alive
----------
2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(222) doAccept: New connection
on FD 12
2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(297) acceptNext: connection on
local=0.0.0.0:3130 remote=[::] FD 12 flags=25
2015/08/17 12:38:10.248 kid1| client_side.cc(3890)
httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.2:443
remote=192.168.0.102 FD 50 flags=17
2015/08/17 12:38:10.248 kid1| client_side.cc(2337) parseHttpRequest: HTTP
Client local=31.13.90.2:443 remote=192.168.0.102 FD 50 flags=17
2015/08/17 12:38:10.248 kid1| client_side.cc(2338) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 31.13.90.2:443 HTTP/1.1
Host: 31.13.90.2:443
---------
2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED; last
ACL checked: localnet
2015/08/17 12:38:10.248 kid1| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED; last
ACL checked: localnet
2015/08/17 12:38:10.248 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
sources for '31.13.90.2:443'
2015/08/17 12:38:10.248 kid1| peer_select.cc(281) peerSelectDnsPaths:
always_direct = ALLOWED
2015/08/17 12:38:10.248 kid1| peer_select.cc(282) peerSelectDnsPaths:
 never_direct = DENIED
2015/08/17 12:38:10.248 kid1| peer_select.cc(288) peerSelectDnsPaths:
 ORIGINAL_DST = local=192.168.0.102 remote=31.13.90.2:443 flags=25
2015/08/17 12:38:10.248 kid1| peer_select.cc(295) peerSelectDnsPaths:
 timedout = 0
2015/08/17 12:38:12.621 kid1| client_side.cc(815) swanSong: local=
31.13.90.2:443 remote=192.168.0.102 flags=17
2015/08/17 12:38:12.625 kid1| client_side.cc(815) swanSong: local=
31.13.90.2:443 remote=192.168.0.102 flags=17


The logs with http (port 80) has the name resolution of navigation.

I disabled pinger because give some error:
2015/08/17 12:49:55.918 kid1| Pinger socket opened on FD 20
2015/08/17 12:49:55.920 kid1| IcmpSquid.cc(90) SendEcho: to 127.0.0.1,
opcode 1, len 9
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: netdb
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: asndb
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: carp
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: userhash
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: sourcehash
2015/08/17 12:49:55.920 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: server_list
2015/08/17 12:49:55| pinger: Initialising ICMP pinger ...
2015/08/17 12:49:55|  icmp_sock: (1) Operation not permitted
2015/08/17 12:49:55| pinger: Unable to start ICMP pinger.
2015/08/17 12:49:55| FATAL: pinger: Unable to open any ICMP sockets.

This is my cache.log after reload squid:
2015/08/17 12:51:26| Set Current Directory to /var/log/squid/cache/squid
2015/08/17 12:51:27 kid1| Reconfiguring Squid Cache (version 3.5.7)...
2015/08/17 12:51:27 kid1| Closing HTTP port 0.0.0.0:3128
2015/08/17 12:51:27 kid1| Closing HTTP port 0.0.0.0:3129
2015/08/17 12:51:27 kid1| Closing HTTPS port 0.0.0.0:3130
2015/08/17 12:51:27 kid1| Closing Pinger socket on FD 20
2015/08/17 12:51:27.320 kid1| Gadgets.cc(99) authenticateReset: Reset
authentication State.
2015/08/17 12:51:27.320 kid1| Logfile: closing log
daemon:/var/log/squid/access.log
2015/08/17 12:51:27.320 kid1| Logfile Daemon: closing log
daemon:/var/log/squid/access.log
2015/08/17 12:51:27.320 kid1| Startup: Initializing Authentication Schemes
...
2015/08/17 12:51:27.320 kid1| Startup: Initialized Authentication Scheme
'basic'
2015/08/17 12:51:27.320 kid1| Startup: Initialized Authentication Scheme
'digest'
2015/08/17 12:51:27.320 kid1| Startup: Initialized Authentication Scheme
'negotiate'
2015/08/17 12:51:27.320 kid1| Startup: Initialized Authentication Scheme
'ntlm'
2015/08/17 12:51:27.320 kid1| Startup: Initialized Authentication.
2015/08/17 12:51:27.320 kid1| RegexData.cc(303) aclParseRegexList:
aclParseRegexList: new Regex line or file
2015/08/17 12:51:27.320 kid1| RegexData.cc(194) compileOptimisedREs:
compileOptimisedREs: -i
2015/08/17 12:51:27.320 kid1| RegexData.cc(218) compileOptimisedREs:
compileOptimisedREs: adding RE '^cache_object://'
2015/08/17 12:51:27.320 kid1| RegexData.cc(208) compileOptimisedREs:
compileOptimisedREs: +i
2015/08/17 12:51:27.320 kid1| RegexData.cc(153) compileRE: compileRE:
compiled '(^cache_object://)' with flags 11
2015/08/17 12:51:27.320 kid1| RegexData.cc(218) compileOptimisedREs:
compileOptimisedREs: adding RE '^https?://[^/]+/squid-internal-mgr/'
2015/08/17 12:51:27.320 kid1| RegexData.cc(153) compileRE: compileRE:
compiled '(^https?://[^/]+/squid-internal-mgr/)' with flags 9
2015/08/17 12:51:27.320 kid1| RegexData.cc(261) compileOptimisedREs:
compileOptimisedREs: 2 REs are optimised into one RE.
2015/08/17 12:51:27.320 kid1| Processing Configuration File:
/etc/squid/squid.conf (depth 0)
2015/08/17 12:51:27.320 kid1| Disabling Authentication on port 0.0.0.0:3129
(TPROXY enabled)
2015/08/17 12:51:27.320 kid1| Disabling Authentication on port 0.0.0.0:3130
(TPROXY enabled)
2015/08/17 12:51:27.320 kid1| Initializing https proxy context
2015/08/17 12:51:27.320 kid1| Initializing https_port 0.0.0.0:3130 SSL
context
2015/08/17 12:51:27.320 kid1| Using certificate in /etc/squid/ssl/squid.pem
2015/08/17 12:51:27.330 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: config
2015/08/17 12:51:27.330 kid1| main.cc(803) mainReconfigureFinish: running
RegisteredRunner::syncConfig
2015/08/17 12:51:27.331 kid1| errorpage.cc(312) tryLoadTemplate: wildcard
fallback errors not coded yet.
2015/08/17 12:51:27.331 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2015/08/17 12:51:27.331 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2015/08/17 12:51:27.331 kid1| Squid plugin modules loaded: 0
2015/08/17 12:51:27.331 kid1| Adaptation support is off.
2015/08/17 12:51:27.332 kid1| Config.cc(211) FinalizeEach: Initialized 0
message adaptation services
2015/08/17 12:51:27.332 kid1| Config.cc(211) FinalizeEach: Initialized 0
message adaptation service groups
2015/08/17 12:51:27.332 kid1| Config.cc(211) FinalizeEach: Initialized 0
message adaptation access rules
2015/08/17 12:51:27.332 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: store_log_tags
2015/08/17 12:51:27.332 kid1| Store logging disabled
2015/08/17 12:51:27.332 kid1| dns_internal.cc(1534) dnsInit: idnsInit:
attempt open DNS socket to: 0.0.0.0
2015/08/17 12:51:27.332 kid1| DNS Socket created at 0.0.0.0, FD 11
2015/08/17 12:51:27.332 kid1| Adding nameserver 8.8.8.8 from squid.conf
2015/08/17 12:51:27.332 kid1| Adding nameserver 8.8.4.4 from squid.conf
2015/08/17 12:51:27.332 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: idns
2015/08/17 12:51:27.332 kid1| Format.cc(64) parse: got definition '%>a/%>A
%un %>rm myip=%la myport=%lp'
2015/08/17 12:51:27.332 kid1| Format.cc(64) parse: got definition '%>a/%>A
%un %>rm myip=%la myport=%lp'
2015/08/17 12:51:27.332 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: external_acl
2015/08/17 12:51:27.332 kid1| wccp.cc(112) wccpConnectionOpen: WCCPv1
disabled.
2015/08/17 12:51:27.332 kid1| wccp2.cc(960) wccp2ConnectionOpen: WCCPv2
Disabled. No IPv4 Router(s) configured.
2015/08/17 12:51:27.332 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
clientListenerConnectionOpened constructed, this=0x2a0ea40 [call62031]
2015/08/17 12:51:27.333 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call clientListenerConnectionOpened(local=
0.0.0.0:3128 remote=[::] FD 12 flags=9, err=0, HTTP Socket port=0x2a0eaa0)
[call62031]
2015/08/17 12:51:27.333 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
clientListenerConnectionOpened constructed, this=0x2a0eb50 [call62033]
2015/08/17 12:51:27.333 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call clientListenerConnectionOpened(local=
0.0.0.0:3129 remote=[::] FD 16 flags=25, err=0, HTTP Socket port=0x2a0ebb0)
[call62033]
2015/08/17 12:51:27.333 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
clientListenerConnectionOpened constructed, this=0x2a0ec60 [call62035]
2015/08/17 12:51:27.333 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call clientListenerConnectionOpened(local=
0.0.0.0:3130 remote=[::] FD 20 flags=25, err=0, HTTPS Socket
port=0x2a0ecc0) [call62035]
2015/08/17 12:51:27.333 kid1| HTCP Disabled.
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: netdb
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: asndb
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: carp
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: userhash
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: sourcehash
2015/08/17 12:51:27.333 kid1| cache_manager.cc(67) registerProfile: skipped
duplicate profile: server_list
2015/08/17 12:51:27.333 kid1| Finished loading MIME types and icons.
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 12
flags=9, err=0, HTTP Socket port=0x2a0eaa0)
2015/08/17 12:51:27.343 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call62031]
2015/08/17 12:51:27.343 kid1| Accepting HTTP Socket connections at local=
0.0.0.0:3128 remote=[::] FD 12 flags=9
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 12
flags=9, err=0, HTTP Socket port=0x2a0eaa0)
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=0.0.0.0:3129 remote=[::] FD 16
flags=25, err=0, HTTP Socket port=0x2a0ebb0)
2015/08/17 12:51:27.343 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call62033]
2015/08/17 12:51:27.343 kid1| Accepting TPROXY intercepted HTTP Socket
connections at local=0.0.0.0:3129 remote=[::] FD 16 flags=25
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=0.0.0.0:3129 remote=[::] FD 16
flags=25, err=0, HTTP Socket port=0x2a0ebb0)
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=0.0.0.0:3130 remote=[::] FD 20
flags=25, err=0, HTTPS Socket port=0x2a0ecc0)
2015/08/17 12:51:27.343 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call62035]
2015/08/17 12:51:27.343 kid1| Accepting TPROXY intercepted SSL bumped HTTPS
Socket connections at local=0.0.0.0:3130 remote=[::] FD 20 flags=25
2015/08/17 12:51:27.343 kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=0.0.0.0:3130 remote=[::] FD 20
flags=25, err=0, HTTPS Socket port=0x2a0ecc0)

Do I have to setup local DNS server? the internal DNS of squid can't handle
https in Tproxy?
What's missing to have name resolution in https traffic as its showed in
http traffic?

Thanks for your time helping me.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/4da286c6/attachment.htm>

From belle at bazuin.nl  Mon Aug 17 15:06:27 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 17 Aug 2015 17:06:27 +0200
Subject: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic)
 ERROR type NTLM type 3
Message-ID: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>

Hai all, 
?
I have a Debian Jessie setup with squid 3.4 , all debian packages. 
Im using samba 4 AD as domain controllers for my kerberos authentication. 
?
I've a setup as followed here : 
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?
?
I have my kerberos auth working, so i dont type any password with a "domain joined computer"? when i want to internet. 
I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 
?
Now, i need to give?users access to the internet, a non domain joined, windows PC.?
?
Im getting :? ( with markus negotiate_wrapper 1.0.1 ?) 
2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....?? =' from squid (length: 59). 
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......? AA= * 
2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....? 8=' from squid (length: 711).
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
?
?
?
I?know the following : ( and correct me if im thinking wrong here.) 
## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
##??? Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
##??? NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
##??? Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

But i recieve a type 3 NTLM token...? 
?
?
This are the configs have tested and these 2 work. 
For kerberos auth 
auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM??? 
?
for basic auth 
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
??? -b "dc=internal,dc=domain,dc=tld" \
??? -D ldap-bind at internal.domain.tld?-W /etc/squid3/private/ldap-bind \
??? -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
??? -h?addc.internal.domain.tld? 

These dont work. 
?
auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
or 
auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME

tried here the supplied wrapper with squid.:?? ? /usr/lib/squid3/negotiate_wrapper_auth? 
and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says? here
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?? ( Install negotiate_wrapper )? 
?
the kerberos part works but not the ntlm . 
?
when i try with only: 
?
### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
auth_param ntlm children 10
auth_param ntlm keep_alive off
?
im also unable to authenticat on the proxy. 
?
all winbind test work..? 
?
I googled a lot, but i didnt find any solutions so im hoping someone here knows more. 
?
so anyone any hint where to look, i cant figure this out. 
?
?
Greetz, 
?
Louis
?
?
?
?
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/4bb83b41/attachment.htm>

From belle at bazuin.nl  Mon Aug 17 15:08:50 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 17 Aug 2015 17:08:50 +0200
Subject: [squid-users] debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.55d1f902.58cd.1568430e700a4c22@ms249-lin-003.rotterdam.bazuin.nl>

ow forgot to mention. 
I test with a windows 7 64bit pc.???one in and outside the AD Domain 
?
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens L.P.H. van Belle
Verzonden: maandag 17 augustus 2015 17:06
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic) ERROR type NTLM type 3



Hai all, 
?
I have a Debian Jessie setup with squid 3.4 , all debian packages. 
Im using samba 4 AD as domain controllers for my kerberos authentication. 
?
I've a setup as followed here : 
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?
?
I have my kerberos auth working, so i dont type any password with a "domain joined computer"? when i want to internet. 
I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 
?
Now, i need to give?users access to the internet, a non domain joined, windows PC.?
?
Im getting :? ( with markus negotiate_wrapper 1.0.1 ?) 
2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....?? =' from squid (length: 59). 
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......? AA= * 
2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....? 8=' from squid (length: 711).
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
?
?
?
I?know the following : ( and correct me if im thinking wrong here.) 
## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
##??? Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
##??? NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
##??? Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

But i recieve a type 3 NTLM token...? 
?
?
This are the configs have tested and these 2 work. 
For kerberos auth 
auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM??? 
?
for basic auth 
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
??? -b "dc=internal,dc=domain,dc=tld" \
??? -D ldap-bind at internal.domain.tld?-W /etc/squid3/private/ldap-bind \
??? -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
??? -h?addc.internal.domain.tld? 

These dont work. 
?
auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
or 
auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME

tried here the supplied wrapper with squid.:?? ? /usr/lib/squid3/negotiate_wrapper_auth? 
and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says? here
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?? ( Install negotiate_wrapper )? 
?
the kerberos part works but not the ntlm . 
?
when i try with only: 
?
### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
auth_param ntlm children 10
auth_param ntlm keep_alive off
?
im also unable to authenticat on the proxy. 
?
all winbind test work..? 
?
I googled a lot, but i didnt find any solutions so im hoping someone here knows more. 
?
so anyone any hint where to look, i cant figure this out. 
?
?
Greetz, 
?
Louis
?
?
?
?
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/d94c7ff0/attachment.htm>

From Hongfei.Du at InterDigital.com  Mon Aug 17 17:42:45 2015
From: Hongfei.Du at InterDigital.com (Du, Hongfei)
Date: Mon, 17 Aug 2015 17:42:45 +0000
Subject: [squid-users] Question on developing customized Cache Selection
 algorithm from Round Robin, Least Load
Message-ID: <E5924590149B4B46AA1B326C644B6064325C78@NABESITE.InterDigital.com>

Hello
We are in an attempt to extend Squid Cache selection algorithm to make a more sophisticated, let?s say to add WRR or WFQ, a few questions to start with:

-        As we probably has to rewrite new algorithm and recompile it, so does anyone know where(or which file) is the existing Round Robin or Least Load algorithm defined in source codes?

-        Is there straight forward method to tell/instruct squid to store content from network(e.g. an URL) in a predefined specific disk folder rather than using the selection algorithm itself?

Any help would be very much appreciated!

Greetings

Hongfei Du
Staff Engineer (UK Software)
InterDigital UK, Inc.
Shoreditch Business Center
64 Great Eastern Street
London,  EC2A 3QR
T: +44 207.749.9140
Hongfei.Du at InterDigital.com
www.InterDigital.com<http://www.interdigital.com>

[cid:imagef00a56.BMP at 7a20fae9.4f930ca1]

This e-mail is intended only for the use of the individual or entity to which it is addressed, and may contain information that is privileged, confidential and/or otherwise protected from disclosure to anyone other than its intended recipient. Unintended transmission shall not constitute waiver of any privilege or confidentiality obligation. If you received this communication in error, please do not review, copy or distribute it, notify me immediately by email, and delete the original message and any attachments. Unless expressly stated in this e-mail, nothing in this message or any attachment should be construed as a digital or electronic signature.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/6ad92bd9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: imagef00a56.BMP
Type: image/bmp
Size: 13078 bytes
Desc: imagef00a56.BMP
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150817/6ad92bd9/attachment.bin>

From eliezer at ngtech.co.il  Mon Aug 17 17:53:24 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 17 Aug 2015 20:53:24 +0300
Subject: [squid-users] Question on developing customized Cache Selection
 algorithm from Round Robin, Least Load
In-Reply-To: <E5924590149B4B46AA1B326C644B6064325C78@NABESITE.InterDigital.com>
References: <E5924590149B4B46AA1B326C644B6064325C78@NABESITE.InterDigital.com>
Message-ID: <55D21F94.8000403@ngtech.co.il>

Hey,

This topic is probably not for squid-users list but for squid-dev list.
Take a small peek at:
http://www.squid-cache.org/Support/mailing-lists.html#squid-dev

Sorry that I cannot answer on the roundrobin questions but while I was 
working on a simple load balancer script what I did was:
Use an array of numbers and shift from it each time while pushing back 
the object into the array.
In golang I started pesudo that a go routine(something like a forked 
process\function) is managing the shift and push or loop operations.

All The Bests,
Eliezer

On 17/08/2015 20:42, Du, Hongfei wrote:
> Hello
> We are in an attempt to extend Squid Cache selection algorithm to make a more sophisticated, let?s say to add WRR or WFQ, a few questions to start with:
>
> -        As we probably has to rewrite new algorithm and recompile it, so does anyone know where(or which file) is the existing Round Robin or Least Load algorithm defined in source codes?
>
> -        Is there straight forward method to tell/instruct squid to store content from network(e.g. an URL) in a predefined specific disk folder rather than using the selection algorithm itself?
>
> Any help would be very much appreciated!
>
> Greetings
>
> Hongfei Du
> Staff Engineer (UK Software)
> InterDigital UK, Inc.
> Shoreditch Business Center
> 64 Great Eastern Street
> London,  EC2A 3QR
> T: +44 207.749.9140
> Hongfei.Du at InterDigital.com
> www.InterDigital.com<http://www.interdigital.com>
>
> [cid:imagef00a56.BMP at 7a20fae9.4f930ca1]
>
> This e-mail is intended only for the use of the individual or entity to which it is addressed, and may contain information that is privileged, confidential and/or otherwise protected from disclosure to anyone other than its intended recipient. Unintended transmission shall not constitute waiver of any privilege or confidentiality obligation. If you received this communication in error, please do not review, copy or distribute it, notify me immediately by email, and delete the original message and any attachments. Unless expressly stated in this e-mail, nothing in this message or any attachment should be construed as a digital or electronic signature.
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Mon Aug 17 22:03:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 01:03:38 +0300
Subject: [squid-users] Squid 3.5.7 RPMs release for CentOS 32 and 64 bit.
In-Reply-To: <55C2E6E5.30207@treenet.co.nz>
References: <55C2E6E5.30207@treenet.co.nz>
Message-ID: <55D25A3A.2090804@ngtech.co.il>

Published at: http://www1.ngtech.co.il/wpe/?p=127


I am happy to release the new RPMs of squid 3.5.7 for Centos 6.6 64bit, 
32bit and CentOS 7 64bit.

The new release includes couple bug fixes and improvements.
The details about the the RPMs repository are at 
squid-wiki[http://wiki.squid-cache.org/KnowledgeBase/CentOS].

Tradeoffs and poop

When building a big computerized system there is a place for tradeoffs. 
When you pass the couple MB of disk space the information becomes 
sensitive and sometimes if the size is over GB it becomes even more 
sensitive. This is where some tradeoffs are standing.
To get the head of it we assume that a human written information is 
valuable enough to invest couple bucks to preserve it. The information 
can be notes, books, photos or other things such as videos. And as the 
saying states ?a picture is worth a thousand words? and this is the same 
for stored data.
One MB can contain a lot of human readable information. And since 
computers are a mechanical device they tend to break one day. So in the 
case of 1MB, these days there are cheap ways to backup information, but 
when we are talking about 1GB and up it can become a big loss for one 
user. For a company which the data is their employes pay-check sometimes 
there is a need to think about the losses a bit deeper. So the basic 
tradeoffs in IT is integrity and availability when in many cases speed 
is beating these.
So what do we choose? HA or speed?
Since integrity is a must in businesses it is the first step before or 
with HA. In many cases somebody states: ?We want speed!?.
And the answer is that it is impossible these days to max the computing 
systems for both Integrity, Availability and Speed! There is always a 
need to choose in this tradeoff and to prefer one or another.
When you choose one preference the others gets lower priority and 
effects the system design.
Couple MBs are really not a big deal but these days the data sizes are 
pretty big and this is the reality, it can be either slower or faster.

     The above is based on lots of tests with lots of DB systems for a 
url filtering solution DB back-end.

This time I would recommend the next video ?Learn about riak!? which can 
give us a little bit more about the tradeoffs in hands.
Learn about Riak! - Matthew[https://vimeo.com/27438366]
School is in session, Matthew ?Roder? Heitzenroder of Basho is here to 
talk about Riak. Ever wonder how they do map/reduce or why Dynamo is 
cool? Don?t even know what Riak is yet?

Or a local mirror at:
Learn about riak! ? Matthew ?Roder? Heitzenroder of 
Basho[http://ngtech.co.il/squid/videos/Learn_about_riak.mp4]

More details about the release at 
squid-wiki[http://wiki.squid-cache.org/KnowledgeBase/CentOS].

To Each and every one of the RPM files in the repository there is an 
*asc* file which contains  MD5, SHA1, SHA2, SHA256, SHA384, SHA512, 
TIGER hashes.

All The Bests,
Eliezer Croitoru


From squid3 at treenet.co.nz  Tue Aug 18 05:30:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Aug 2015 17:30:01 +1200
Subject: [squid-users] Bridge/Tproxy: https dns
In-Reply-To: <CADLGAbK3wbE-5M_88tWJwYFcb_NdXUuv6cR9czUsNrLHanxfSg@mail.gmail.com>
References: <CADLGAbK3wbE-5M_88tWJwYFcb_NdXUuv6cR9czUsNrLHanxfSg@mail.gmail.com>
Message-ID: <55D2C2D9.5080301@treenet.co.nz>

On 18/08/2015 12:25 a.m., Pedro Correia Sardinha wrote:
> Hello,
> 
> I'm trying to setup a Squid server simple as possible just to review the
> web use in office using the last stable version 3.5.7.
> 

And you chose TPROXY with ssl-bump'ing. The two most complex features to
setup. lol.

> I setup the bridge with 2 NIC, br0 with IP 192.168.0.5 and I had disable
> IPv6 on boot in my Slackware Current (Fri Aug 14 2015) server.

Sigh. Ever heard of IPv6-over-IPv4, 6-in-4, 6to4, etc. ?
All protocols designed to "fix" connectivity going through machines
setup like yours.

And why bother disabling a (BCP 177) mandatory part of the kernel?
The correct way to handle unwanted traffic is to firewall it. Not to
play around with kernel internals.

<snip>
> My squid.conf:

> tcp_outgoing_address 85.138.204.43

This is irrelevant with TPROXY. The client IP address is used instead.

For the regular forward-proxy traffic on port 3128 the machines default
IP will be used.


> dns_v4_first on
> pinger_enable off
> http_port 3128
> http_port 3129 tproxy
> https_port 3130 ssl-bump tproxy generate-host-certificates=off
> cert=/etc/squid/ssl/squid.pem cafile=/etc/squid/ssl/squid.pem
> always_direct allow ssl-bump_port
> ssl_bump none all

You have configued Squid not to even look at the TLS details.


> dns_nameservers 8.8.8.8 8.8.4.4
> access_log daemon:/var/log/squid/access.log squid
> cache deny all
> pid_filename /var/run/squid/squid.pid
> coredump_dir /var/log/squid/cache/squid
> visible_hostname myservername.domain.local
> 
> In general the configuration (squid.conf) it's working but has some
> incomplete behaviors as shows in log files.
> 
> access.log (I know this is Facebook but there are no dns resolusion in
> https, just IP):
> 1439811492.625   2377 192.168.0.102 TCP_TUNNEL/200 3574 CONNECT
> 31.13.90.2:443 - ORIGINAL_DST/31.13.90.2 -


What sort of resolution were you expecting?

* The above log line is recording the TCP connection. TCP packets do not
have any "domain name" fields that need resolving to IP addresses.

* you also configured Squid not to look at the TLS details where it
might have found an SNI entry with server domain name.

The result is that Squid is working purely with TPROXY IP addresses and
setting up a TCP tunnel to relay the traffic through.


> 
> cache.log:
> HTTP/1.1 200 OK
> Accept-Ranges: bytes
> Cache-Control: max-age=504747
> Content-Type: application/ocsp-response
> Date: Mon, 17 Aug 2015 11:38:03 GMT
> ETag: "55d15943-1d7"
> Expires: Sun, 23 Aug 2015 23:38:03 GMT
> Last-Modified: Mon, 17 Aug 2015 03:47:15 GMT
> Server: ECS (mad/439C)
> X-Cache: HIT
> Content-Length: 471
> X-Cache: MISS from squidhead2.skywalker.local
> Via: 1.1 squidhead2.skywalker.local (squid/3.5.7)
> Connection: keep-alive
> ----------
> 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> StoreEntry::checkCachable: NO: not cachable


Above is all the end of some transaction that was started earlier. No
useful details in the provided log snippet about it.



This is where a transaction actually starts:

> 2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(222) doAccept: New connection
> on FD 12
> 2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(297) acceptNext: connection on
> local=0.0.0.0:3130 remote=[::] FD 12 flags=25
> 2015/08/17 12:38:10.248 kid1| client_side.cc(3890)

Notice how the connection local IP:port details change from port 3130 to
port 443. Thats TPROXY working.

> httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.2:443
> remote=192.168.0.102 FD 50 flags=17

This is the "ssl_bump none" action working (by not doing anything TLS
related) exactly as you configured.

Squid is now processing an internally generated CONNECT request
representing the intercepted TPROXY connection in a way that can be
logged and/or relayed to other proxies if it needs to.

> 2015/08/17 12:38:10.248 kid1| client_side.cc(2337) parseHttpRequest: HTTP
> Client local=31.13.90.2:443 remote=192.168.0.102 FD 50 flags=17
> 2015/08/17 12:38:10.248 kid1| client_side.cc(2338) parseHttpRequest: HTTP
> Client REQUEST:
> ---------
> CONNECT 31.13.90.2:443 HTTP/1.1
> Host: 31.13.90.2:443
> ---------
> 2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
> clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED; last
> ACL checked: localnet
> 2015/08/17 12:38:10.248 kid1| client_side_request.cc(717)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
> clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED; last
> ACL checked: localnet
> 2015/08/17 12:38:10.248 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
> sources for '31.13.90.2:443'

The CONNECT request uses a raw-IP address provided by TPROXY. There is
no name to resolve.

> 2015/08/17 12:38:10.248 kid1| peer_select.cc(281) peerSelectDnsPaths:
> always_direct = ALLOWED
> 2015/08/17 12:38:10.248 kid1| peer_select.cc(282) peerSelectDnsPaths:
>  never_direct = DENIED
> 2015/08/17 12:38:10.248 kid1| peer_select.cc(288) peerSelectDnsPaths:
>  ORIGINAL_DST = local=192.168.0.102 remote=31.13.90.2:443 flags=25
> 2015/08/17 12:38:10.248 kid1| peer_select.cc(295) peerSelectDnsPaths:
>  timedout = 0

... stuff happens for 2 seconds...

> 2015/08/17 12:38:12.621 kid1| client_side.cc(815) swanSong: local=
> 31.13.90.2:443 remote=192.168.0.102 flags=17
> 2015/08/17 12:38:12.625 kid1| client_side.cc(815) swanSong: local=
> 31.13.90.2:443 remote=192.168.0.102 flags=17

Then the connection closes.

Looks perfectly normal and expected behaviour to me considering what you
configured.

> 
> 
> The logs with http (port 80) has the name resolution of navigation.
> 
> I disabled pinger because give some error:
<snip>
> 2015/08/17 12:49:55| FATAL: pinger: Unable to open any ICMP sockets.

Okay. Not a big problem. The pinger helper needs its suid bit set, which
is not working on all system installations yet. Disabling it is fine.


> Do I have to setup local DNS server? the internal DNS of squid can't handle
> https in Tproxy?
> What's missing to have name resolution in https traffic as its showed in
> http traffic?

Whats missing is the SSL-bumping part. HTTPS works differently to HTTP.

The URL domain name and all the rest of the HTTP message is encrypted
*in full*. There is simply no client HTTP message involved if you don't
decrypt.

As I mentioned above what you are seeing in the log is a Squid-generated
CONNECT message. Its the HTTP representation of the intercepted TCP SYN
packet and contains purely raw-IP:port details.

TLS does have an SNI record which is sent by browsers un-encrypted that
can be used as domain for some things. BUT, that requires "ssl_bump
peek" action at minimum, and has no guarantee of actually being present.

SNI is also still a new feature, and is not used for these fake CONNECT
requests anyway (since they represent the TCP SYN). So you wont see any
log difference in 3.5 even if you do let your Squid use it for ACLs.

Amos



From belle at bazuin.nl  Tue Aug 18 06:28:25 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 18 Aug 2015 08:28:25 +0200
Subject: [squid-users] debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.55d2d089.2ba7.1a22bdbf5ed74699@ms249-lin-003.rotterdam.bazuin.nl>

Nobody any hint where the NTLM auth is going wrong, or what i can do to fix this. 
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens L.P.H. van Belle
Verzonden: maandag 17 augustus 2015 17:06
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic) ERROR type NTLM type 3



Hai all, 
?
I have a Debian Jessie setup with squid 3.4 , all debian packages. 
Im using samba 4 AD as domain controllers for my kerberos authentication. 
?
I've a setup as followed here : 
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?
?
I have my kerberos auth working, so i dont type any password with a "domain joined computer"? when i want to internet. 
I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 
?
Now, i need to give?users access to the internet, a non domain joined, windows PC.?
?
Im getting :? ( with markus negotiate_wrapper 1.0.1 ?) 
2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....?? =' from squid (length: 59). 
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......? AA= * 
2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....? 8=' from squid (length: 711).
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
?
?
?
I?know the following : ( and correct me if im thinking wrong here.) 
## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
##??? Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
##??? NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
##??? Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

But i recieve a type 3 NTLM token...? 
?
?
This are the configs have tested and these 2 work. 
For kerberos auth 
auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM??? 
?
for basic auth 
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
??? -b "dc=internal,dc=domain,dc=tld" \
??? -D ldap-bind at internal.domain.tld?-W /etc/squid3/private/ldap-bind \
??? -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
??? -h?addc.internal.domain.tld? 

These dont work. 
?
auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
or 
auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME

tried here the supplied wrapper with squid.:?? ? /usr/lib/squid3/negotiate_wrapper_auth? 
and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says? here
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?? ( Install negotiate_wrapper )? 
?
the kerberos part works but not the ntlm . 
?
when i try with only: 
?
### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
auth_param ntlm children 10
auth_param ntlm keep_alive off
?
im also unable to authenticat on the proxy. 
?
all winbind test work..? 
?
I googled a lot, but i didnt find any solutions so im hoping someone here knows more. 
?
so anyone any hint where to look, i cant figure this out. 
?
?
Greetz, 
?
Louis
?
?
?
?
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/52b5ffbd/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 18 06:39:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Aug 2015 18:39:04 +1200
Subject: [squid-users] debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <55D2D308.405@treenet.co.nz>

On 18/08/2015 3:06 a.m., L.P.H. van Belle wrote:
> Hai all, 
>  
> I have a Debian Jessie setup with squid 3.4 , all debian packages. 
> Im using samba 4 AD as domain controllers for my kerberos authentication. 
>  
> I've a setup as followed here : 
> http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory 
>  
> I have my kerberos auth working, so i dont type any password with a "domain joined computer"  when i want to internet. 
> I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 
>  
> Now, i need to give users access to the internet, a non domain joined, windows PC. 
>  
> Im getting :  ( with markus negotiate_wrapper 1.0.1  ) 
> 2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
> 2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....   =' from squid (length: 59). 
> 2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
> 2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token

Type 1 NTLM.

> 2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......  AA= * 
> 2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....  8=' from squid (length: 711).
> 2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
> 2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
> 2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
> 2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
>  
>  
>  
> I know the following : ( and correct me if im thinking wrong here.) 
> ## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
> ##    Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
> ##    NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
> ##    Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
> ## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

Regarding (1):

* "Pure kerberos" aka "Kerberos " auth scheme is not supported in Squid.
Only Negotate/Kerberos. It was accepted by Squid-2 as an alias for
Negotiate, but Squid-3 operates differently and it was dropped for now.

* Rejecting NTLM (ie Negotiate/NTLM) is an artifact of the Squid
kerberos-only helper rejecting NTLM tokens. Nothing more.

You could reject the Negotiate/Kerberos tokens by configuring a
NTLM-only helper in the "auth_param negotiate program".

* off-domain machines only ever worked using Basic authentication or
similar protocols called LanMan which sent passwords inside NTLM or
Negotiate/NTLM tokens. But LanMan are so insecure they are no longer
supported.
 NP: if you have a client that will only authenticate with LanMan (SMB
LM) protocols you are better off security-wise not authenticating it at
all. At least that stops it broadcasting the users password to the world.


Regarding (2):

* The machine still does need to be domain joined, at least recently
enough to have a valid Kerberos token. What can be avoided is being
connected "live" during the handshake itself.

 But that is a feature of the client software not related to Squid. So
some clients support it, most actually dont.


> 
> But i recieve a type 3 NTLM token...  
>  

You also received NTLM type 1 prior to it. I suspect a machine not
joined to the domain is trying to use NTLM, which requires being on the
domain.

There is no problem with this *unless* the client machine is refusing to
fallback to Negotiate/Kerberos or Basic auth after the failure.

There is no reason a popup should occur unless all forms of
Negotiate/Kerberos Negotiate/NTLM, NTLM, and Basic which are offered by
the proxy have failed.


>  
> This are the configs have tested and these 2 work. 
> For kerberos auth 
> auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM    
>  
> for basic auth 
> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
>     -b "dc=internal,dc=domain,dc=tld" \
>     -D ldap-bind at internal.domain.tld -W /etc/squid3/private/ldap-bind \
>     -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
>     -h addc.internal.domain.tld  
> 
> These dont work. 

I assume that by the positioning of your "these" statements you meant
the above work, and the below dont.

>  
> auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
>     --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
>     --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
> or 
> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
>     --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
>     --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
> 
> tried here the supplied wrapper with squid.:     /usr/lib/squid3/negotiate_wrapper_auth  
> and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says  here
> http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory   ( Install negotiate_wrapper )  
>  
> the kerberos part works but not the ntlm . 

One puzzling thing is why Win7 client is trying to use NTLM in the first
place. NTLM is disabled by default in Vista and later due to its lack of
security.

Try adding "auth_param negotiate keep_alive off" to close connections
when Negotiate/NTLM is used and force the client to retry with other
auth credentials on a clean connection.


>  
> when i try with only: 
>  
> ### pure ntlm authentication
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
> auth_param ntlm children 10
> auth_param ntlm keep_alive off
>  
> im also unable to authenticat on the proxy. 

NTLM will only work with current MS software if the client is joined to
the domain, and if NTLM is explicitly re-enabled.

The 1970-80's LanMan protocols are no longer supported since 2006 (WinXP
SP3). The most secure of these can be decrypted in under 50 milliseconds
- ie "live".

Ironically that was exactly how Squid helpers used to work for
off-domain clients all through the 2000's. LanMan passwords being
decrypted in real-time allowed Basic auth APIs in AD to be used. Giving
the appearance that off-domain machines were authenticating securely,
when in fact they were just broadcasting their passwords about. Not a
good situation.

The old 1990's NTLM v1 and v2 are also on the way out since Vista. NTLM
v1 can be decrypted in a few seconds, v2 in a few minutes.


HTH
Amos


From belle at bazuin.nl  Tue Aug 18 08:00:51 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 18 Aug 2015 10:00:51 +0200
Subject: [squid-users] FW: debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
Message-ID: <vmime.55d2e633.57e6.264bd9653dad75e3@ms249-lin-003.rotterdam.bazuin.nl>

Hai Amos, 

Thank you for your very clear responce.. few small questions..

Is there a way to setup the proxy for the following.
1) use negotiate kerberos for auth, ( which is working already for all domain joined machines )
2) use a fall back that works, for now basic ldap works for non windows machines, and domain joined machines.
3) use any other fallback way for authentication users on windows machines, that are not in the domain.
	and without modify-ing anything in windows. as these are often guest machines. 

Is a link to a radius server an option, dont have a radus jet, but can be installed. 
and radius is also comming for my wifi authentication. 
whould that fix my problem (3) above, in a authentication fallback setup. 


>One puzzling thing is why Win7 client is trying to use NTLM in 
>the first
>place. NTLM is disabled by default in Vista and later due to 
>its lack of
>security.
>
>Try adding "auth_param negotiate keep_alive off" to close connections
>when Negotiate/NTLM is used and force the client to retry with other
>auth credentials on a clean connection.

these : 
>> auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM    
and 
>> auth_param negotiate program /usr/local/bin/negotiate_wrapper
These lines, work both for negotiate kerberos.  
The last, when useing : /usr/local/bin/negotiate_wrapper was tested with the parameter 
negotiate keep_alive off. 

Above works fine with the domain joined pc, but not with the "non domain joined" PC. 
the negotiate kerberos works very good, but the fall back not. ( as you explained ) 

I found that if i setup with only basic_ldap_auth, against the AD, then i can use both,
domain joined and not domain joined, but the first time it always gives a popup for authenticating. 
If once authenticated, it keeps it authenticated, aka windows/IE keeps the login and password. 
even if i clear the history. 

Why i dont want this... 
If a user is logging in the domain, and kerberos auth is used, then when going on internet, 
the "correct" aka logged in user, is always used. 
but when i use basic_ldap_auth, then it gives the user to put in an other username/password at popup, 
then it remembers the login and a user now is internetting with an other users name. 

So, when im right, a fallback for all is not possible, due to NTLM auth? 

And a big thank you for your responce. 


Greetz, 

Louis


>-----Oorspronkelijk bericht-----
>Van: squid-users 
>[mailto:squid-users-bounces at lists.squid-cache.org] Namens Amos Jeffries
>Verzonden: dinsdag 18 augustus 2015 8:39
>Aan: squid-users at lists.squid-cache.org
>Onderwerp: Re: [squid-users] debian Jessie squid with auth 
>(kerberos/ntlm/basic) ERROR type NTLM type 3
>
>On 18/08/2015 3:06 a.m., L.P.H. van Belle wrote:
>> Hai all, 
>>  
>> I have a Debian Jessie setup with squid 3.4 , all debian packages. 
>> Im using samba 4 AD as domain controllers for my kerberos 
>authentication. 
>>  
>> I've a setup as followed here : 
>> 
>http://wiki.squid-cache.org/ConfigExamples/Authenticate/Windows
>ActiveDirectory 
>>  
>> I have my kerberos auth working, so i dont type any password 
>with a "domain joined computer"  when i want to internet. 
>> I Have my Ldap auth working, for my "Non windows, non domain 
>joined" Devices. 
>>  
>> Now, i need to give users access to the internet, a non 
>domain joined, windows PC. 
>>  
>> Im getting :  ( with markus negotiate_wrapper 1.0.1  ) 
>> 2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication 
>validating user. Result: {result=BH, notes={message: 
>NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
>> 2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....   =' 
>from squid (length: 59). 
>> 2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' 
>(decoded length: 40).
>> 2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
>
>Type 1 NTLM.


>
>> 2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......  AA= * 
>> 2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....  8=' 
>from squid (length: 711).
>> 2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' 
>(decoded length: 530).
>> 2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
>> 2015/08/17 16:32:03| negotiate_wrapper: Return 'BH 
>NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
>> 2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication 
>validating user. Result: {result=BH, notes={message: 
>NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
>>  
>>  
>>  
>> I know the following : ( and correct me if im thinking wrong here.) 
>> ## 1) Pure Kerberos. Passthrough auth for windows users with 
>windows DOMAIN JOINED pc's.
>> ##    Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
>> ##    NO NTLM. AKA, a windows pc, NOT JOINED in the domain, 
>with end up in always user popup for auth.
>> ##    Which will always fail because of NTLM TYPE 1 and TYPE 
>2, authorisations.
>> ## 2) NEGOTIATE AUTH, which will do all of above, but also 
>authenticated Windows PC's Not domain Joined.
>
>Regarding (1):
>
>* "Pure kerberos" aka "Kerberos " auth scheme is not supported 
>in Squid.
>Only Negotate/Kerberos. It was accepted by Squid-2 as an alias for
>Negotiate, but Squid-3 operates differently and it was dropped for now.
>
>* Rejecting NTLM (ie Negotiate/NTLM) is an artifact of the Squid
>kerberos-only helper rejecting NTLM tokens. Nothing more.
>
>You could reject the Negotiate/Kerberos tokens by configuring a
>NTLM-only helper in the "auth_param negotiate program".
>
>* off-domain machines only ever worked using Basic authentication or
>similar protocols called LanMan which sent passwords inside NTLM or
>Negotiate/NTLM tokens. But LanMan are so insecure they are no longer
>supported.
> NP: if you have a client that will only authenticate with LanMan (SMB
>LM) protocols you are better off security-wise not authenticating it at
>all. At least that stops it broadcasting the users password to 
>the world.
>
>
>Regarding (2):
>
>* The machine still does need to be domain joined, at least recently
>enough to have a valid Kerberos token. What can be avoided is being
>connected "live" during the handshake itself.
>
> But that is a feature of the client software not related to Squid. So
>some clients support it, most actually dont.
>
>
>> 
>> But i recieve a type 3 NTLM token...  
>>  
>
>You also received NTLM type 1 prior to it. I suspect a machine not
>joined to the domain is trying to use NTLM, which requires being on the
>domain.
>
>There is no problem with this *unless* the client machine is 
>refusing to
>fallback to Negotiate/Kerberos or Basic auth after the failure.
>
>There is no reason a popup should occur unless all forms of
>Negotiate/Kerberos Negotiate/NTLM, NTLM, and Basic which are offered by
>the proxy have failed.
>
>
>>  
>> This are the configs have tested and these 2 work. 
>> For kerberos auth 
>> auth_param negotiate program 
>/usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM    
>>  
>> for basic auth 
>> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
>>     -b "dc=internal,dc=domain,dc=tld" \
>>     -D ldap-bind at internal.domain.tld -W 
>/etc/squid3/private/ldap-bind \
>>     -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
>>     -h addc.internal.domain.tld  
>> 
>> These dont work. 
>
>I assume that by the positioning of your "these" statements you meant
>the above work, and the below dont.
>
>>  
>> auth_param negotiate program 
>/usr/lib/squid3/negotiate_wrapper_auth -d \
>>     --ntlm /usr/bin/ntlm_auth --diagnostics 
>--helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
>>     --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s 
>GSS_C_NO_NAME
>> or 
>> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
>>     --ntlm /usr/bin/ntlm_auth --diagnostics 
>--helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
>>     --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s 
>GSS_C_NO_NAME
>> 
>> tried here the supplied wrapper with squid.:     
>/usr/lib/squid3/negotiate_wrapper_auth  
>> and i have tried the negotiate_wrapper of Markus, as the 
>wiki.squid-cache.org also says  here
>> 
>http://wiki.squid-cache.org/ConfigExamples/Authenticate/Windows
>ActiveDirectory   ( Install negotiate_wrapper )  
>>  
>> the kerberos part works but not the ntlm . 
>
>One puzzling thing is why Win7 client is trying to use NTLM in 
>the first
>place. NTLM is disabled by default in Vista and later due to 
>its lack of
>security.
>
>Try adding "auth_param negotiate keep_alive off" to close connections
>when Negotiate/NTLM is used and force the client to retry with other
>auth credentials on a clean connection.
>
>
>>  
>> when i try with only: 
>>  
>> ### pure ntlm authentication
>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics 
>--helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
>> auth_param ntlm children 10
>> auth_param ntlm keep_alive off
>>  
>> im also unable to authenticat on the proxy. 
>
>NTLM will only work with current MS software if the client is joined to
>the domain, and if NTLM is explicitly re-enabled.
>
>The 1970-80's LanMan protocols are no longer supported since 
>2006 (WinXP
>SP3). The most secure of these can be decrypted in under 50 
>milliseconds
>- ie "live".
>
>Ironically that was exactly how Squid helpers used to work for
>off-domain clients all through the 2000's. LanMan passwords being
>decrypted in real-time allowed Basic auth APIs in AD to be used. Giving
>the appearance that off-domain machines were authenticating securely,
>when in fact they were just broadcasting their passwords about. Not a
>good situation.
>
>The old 1990's NTLM v1 and v2 are also on the way out since Vista. NTLM
>v1 can be decrypted in a few seconds, v2 in a few minutes.
>
>
>HTH
>Amos
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Tue Aug 18 08:46:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Aug 2015 20:46:09 +1200
Subject: [squid-users] FW: debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <vmime.55d2e633.57e6.264bd9653dad75e3@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.55d2e633.57e6.264bd9653dad75e3@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <55D2F0D1.2010101@treenet.co.nz>

On 18/08/2015 8:00 p.m., L.P.H. van Belle wrote:
> Hai Amos, 
> 
> Thank you for your very clear responce.. few small questions..
> 
> Is there a way to setup the proxy for the following.
> 1) use negotiate kerberos for auth, ( which is working already for all domain joined machines )
> 2) use a fall back that works, for now basic ldap works for non windows machines, and domain joined machines.
> 3) use any other fallback way for authentication users on windows machines, that are not in the domain.
> 	and without modify-ing anything in windows. as these are often guest machines. 

All Squid can do is offer mechanisms. The client is required to respond
using the most secure that it can use.

Configuring auth schemes in this order:

 auth_param negotiate program ...
 auth_param basic program ...

Should meet your needs almost all the time. But it really depends on the
client following either the specs or your offered order. Not all do. And
some think "Negotiate" means "Negotiate/NTLM", and/or try that before
Negotiate/Kerberos.


> 
> Is a link to a radius server an option, dont have a radus jet, but can be installed. 
> and radius is also comming for my wifi authentication. 
> whould that fix my problem (3) above, in a authentication fallback setup. 

Basic auth RADIUS helper can use RADIUS as an authentication backend.
Other than that I'm not familiar with it.

> 
>> One puzzling thing is why Win7 client is trying to use NTLM in 
>> the first
>> place. NTLM is disabled by default in Vista and later due to 
>> its lack of
>> security.
>>
>> Try adding "auth_param negotiate keep_alive off" to close connections
>> when Negotiate/NTLM is used and force the client to retry with other
>> auth credentials on a clean connection.
> 
> these : 
>>> auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM    
> and 
>>> auth_param negotiate program /usr/local/bin/negotiate_wrapper
> These lines, work both for negotiate kerberos.  
> The last, when useing : /usr/local/bin/negotiate_wrapper was tested with the parameter 
> negotiate keep_alive off. 
> 
> Above works fine with the domain joined pc, but not with the "non domain joined" PC. 
> the negotiate kerberos works very good, but the fall back not. ( as you explained ) 
> 
> I found that if i setup with only basic_ldap_auth, against the AD, then i can use both,
> domain joined and not domain joined, but the first time it always gives a popup for authenticating. 


Take a look and see what its sending for Basic auth credentials on first
try.

You may need to use a Basic auth helper that allows stripping the
@DOMAIN part off the credentials received. I think some systems send the
user at DOMAIN in Basic with the machine name as DOMAIN. That wont work
against any real DC server.



> If once authenticated, it keeps it authenticated, aka windows/IE keeps the login and password. 
> even if i clear the history. 

For NTLM or Negotiate credentials being used. Then credentials once
authenticated are tied permanently to the TCP connection(s) they were
used on. You have to fully close all the affected TCP connections to
"logout". The only reliable way to do that is shutdown the whole browser.

For Basic auth credentials being used. They are only accepted so long as
the auth backend keeps accepting them. HTTP actually requires the
browser/client to send credentials on every single request. Squid checks
these against the ones it seen being valid before, or once every
credentialsttl timeout it re-checks fully against the backend server.


> 
> Why i dont want this... 
> If a user is logging in the domain, and kerberos auth is used, then when going on internet, 
> the "correct" aka logged in user, is always used. 
> but when i use basic_ldap_auth, then it gives the user to put in an other username/password at popup, 
> then it remembers the login and a user now is internetting with an other users name.
> 
> So, when im right, a fallback for all is not possible, due to NTLM auth? 
> 

I dont see where NTLM comes into that.

>From your description fallback authentication *is* working. But with
users able to send credentials that you dont want to allow their use of.

That is a different (authorization) problem to solve. By authenticating
successfully the user is "proving" that they are who they claim to be.
Although Basic auth could possibly be a lie with somebody elses valid
credentials.

It is up to your authorization system to determine if the sener of
credentials X:Y are still allowed to access anything, and what. Squid
ACLs primarily used for that, but also the AD server can reject re-login
verification after Basic auth credentialsttl by Squid helper - that will
probably result in more popups.


Amos



From rafael.akchurin at diladele.com  Tue Aug 18 10:09:18 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 18 Aug 2015 10:09:18 +0000
Subject: [squid-users] Squid 3.5.7 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB13591320B16408695E9BC0468F780@VI1PR04MB1359.eurprd04.prod.outlook.com>

Greetings everyone,


The CygWin based build of Squid proxy for Microsoft Windows version 3.5.7 is now available (amd64 only!).


* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.7-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues


Thanks a lot for Squid developers for making this great software!


Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com.


NB: sorry for the late release - summer time :)


Best regards,

Rafael Akchurin

Diladele B.V.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/1ad7deae/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 18 10:24:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Aug 2015 22:24:45 +1200
Subject: [squid-users] Question on developing customized Cache Selection
 algorithm from Round Robin, Least Load
In-Reply-To: <E5924590149B4B46AA1B326C644B6064325C78@NABESITE.InterDigital.com>
References: <E5924590149B4B46AA1B326C644B6064325C78@NABESITE.InterDigital.com>
Message-ID: <55D307ED.3030500@treenet.co.nz>

On 18/08/2015 5:42 a.m., Du, Hongfei wrote:
> Hello
> We are in an attempt to extend Squid Cache selection algorithm to
> make
a more sophisticated, let?s say to add WRR or WFQ, a few questions to
start with:
> 

Like Eliezer said this is really a question for squid-dev mailing list
where the developers hang out.

WRR (weighted round-robin) is already implemented and exactly how Squid
cache_dir currently operate. The weighting is based on storage area
available size and I/O loading.

WFQ (weighted fair queueing) is a queueing algorthm as the 'Q' says.
Caching != queueing. In fact a cache is so different from a queue that
WFQ would badly affect performance if it were used to decide what
storage an object went into.
 In essence, the problem is that we cannot dictate what objects will be
requested by clients. They want what they ask for. Squids duty is 1) to
answer reliably and 2) fast as possible regardless of objects location.


> - As we probably has to rewrite new algorithm and recompile it, so
does anyone know where(or which file) is the existing Round Robin or
Least Load algorithm defined in source codes?

That depends on whether you mean the algorithm applied for local storage
vs network sources, or the one(s) applied to individual caches for
garbage collection.

> 
> - Is there straight forward method to tell/instruct squid to store
content from network(e.g. an URL) in a predefined specific disk folder
rather than using the selection algorithm itself?

Simply stated:
 The URL and all other relevant details from the transaction are hashed
to lookup an index to find the 32-bit 'sfileno' value which is a UID
encoding the location of indexed objects in Squid local storage.

It _sounds_ simple enough, but those "other relevant details" is a
massive complication. One single URL can potentially contain all
possible objects that ever have or ever will exist on the Internet. Even
considering storing things one file per URL dies a horrible death when
it encounters popular modern websites.


Within Squid we refer to "the HTTP cache" as a single thing. But it is
constructed of many storage areas. The individual cache_dirs and other
places where HTTP objects might be found. Remote network sources are
also accounted for.

There is algorithm(s) applied in layers to decide which type of storage
area is use, then which one within the selected type is most
appropriate. Based on object availability, cacheability, size, storage
area speed, object popularity, and temporal relationships to others.
Then a sfileno is assigned if its local storage.

Then objects get moved between storage areas anyway based on need and
popularity. And objects get removed from invdividual storage areas based
on lack of popularity. Both of which affect future requests for them.

So the particulars of what you want to do matter, a lot.


FWIW, we have known outstanding needs for:

* updated cache_peer selection algorithms. Current Squid outgoing TCP
connection failover works with a list of IPs that get tried until one
succeeds. The old selection algorithms produce only a single IP rather
than a preference-ordered set of peers to try.
 - also none of the algorithms provide byte-base loading.

* ETag based cache index. For better performant If-Match/If-None-Match
revalidation traffic.

* 206 partial object caching. Rock can store them, but no algorithms yet
exist to properly manage the pieces of incomplete objects or aggregation
from different transactions.

* per-area storage indexes, instead of a Big Global Index. Working
towards 64-bit sfileno which are needed for some TB sized caches. Rock
and Transients storage areas are done, but other caches still TODO.

* better HDD load detection. To inform the weighting of cache_dir
seectio algorithms. This is a hardware driver related project.

* Support for ZFS and XFS dynamic inode sizing. This causes lots of
issues with "wrong" disk storage under/over usage. Another hardware
driver related project.


HTH
Amos


From adricustodio at uol.com.br  Tue Aug 18 11:58:17 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 04:58:17 -0700 (PDT)
Subject: [squid-users] Squid + oracle
Message-ID: <1439899097192-4672730.post@n4.nabble.com>

Hello guys. 
I got a few questions.
I need to authenticate my squid on my oracle database, is that possible ?
if its possible, i can do like a captive portal ? like a hotspot for that ?
The last question is... can i drop the connection of a user on squid after X
minutes ?
For example, every 5 minutes connected the user need to authenticate again ?

Btw. Im using Centos 7 + iptables + squid
This is a project that i need to solve.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Tue Aug 18 12:15:28 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 15:15:28 +0300
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439899097192-4672730.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
Message-ID: <55D321E0.5070300@ngtech.co.il>

Hey,

Currently I do not know of such a helper but it is possible to write one.
If you have a DB machine I can run tests against I might be able to 
write a small helper for a basic authentication or session helper based 
on OracleDB.
For a captive portal you would need two separated systems:
- Web
- external_acl helper

The basics of captive portal is that it works in the IP level and can be 
written in php or any other web scripts.

You can start by looking at:
https://mohammadthalif.wordpress.com/2010/12/14/list-of-open-source-captive-portal-software-and-network-access-control-nac/

The external_acl helper supports a cache for the authentication and can 
be used to allow the access from the first time surfing onwards for a 
period of time.
Take a look at the session helper of squid for more details.

I would recommend to utilize iptables for the captive portal.

All The Bests,
Eliezer

On 18/08/2015 14:58, adricustodio wrote:
> Hello guys.
> I got a few questions.
> I need to authenticate my squid on my oracle database, is that possible ?
> if its possible, i can do like a captive portal ? like a hotspot for that ?
> The last question is... can i drop the connection of a user on squid after X
> minutes ?
> For example, every 5 minutes connected the user need to authenticate again ?
>
> Btw. Im using Centos 7 + iptables + squid
> This is a project that i need to solve.
>
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From adricustodio at uol.com.br  Tue Aug 18 12:20:04 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 05:20:04 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D321E0.5070300@ngtech.co.il>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il>
Message-ID: <1439900404769-4672732.post@n4.nabble.com>

Well the captive portal is not the importante here... 
First i need to authenticate my users on oracle DB.
I was thinking if its possible to create a mysql DB and replicate my oracle
on mysql DB and authenticate on the mysql localhost... what do you think ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672732.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Tue Aug 18 12:31:59 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 18 Aug 2015 14:31:59 +0200
Subject: [squid-users] peek and splice content inspection question
Message-ID: <mailman.4.1736411263.1101126.squid-users@lists.squid-cache.org>

At least with squidguard you can't check the content (cookies, keywords in html, bad words, etc) 
It's "just" an URL/domains filter, but it can also block some objets contain in the request, eg http://foo.com/test.mp3, but it can't deny some kind of browsers or header informations.

Maybe ufdbguard can, I don't know

Sorry, I'm writing with my phone 
@Stanford a new release is coming ...

From rafael.akchurin at diladele.com  Tue Aug 18 12:44:43 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 18 Aug 2015 12:44:43 +0000
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>,
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
Message-ID: <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Stanford and the list,


Sorry to jump on a late thread - it is also possible to use ICAP/eCAP server to filter the actual contents of the stream.

C-ICAP comes to mind first, then eCap samples from http://www.e-cap.org/Downloads


Best regards,

Rafael

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Stanford Prescott <stan.prescott at gmail.com>
Sent: Monday, August 17, 2015 1:04 AM
To: Yuri Voinov
Cc: squid-users
Subject: Re: [squid-users] peek and splice content inspection question

Yes, really. ufdbGuard, like squidGuard before it, is a URL Filter that filters known unwanted URLs. A content filter, like DansGuardian and E2Guardian are content filters which examine the content of web pages looking for unwanted things.

On Sun, Aug 16, 2015 at 6:10 PM, Yuri Voinov <yvoinov at gmail.com<mailto:yvoinov at gmail.com>> wrote:

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

O, really?

17.08.15 4:03, Stanford Prescott ?????:
> ufdbGuard is not a content filter.
>
> On Sun, Aug 16, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com><mailto:yvoinov at gmail.com> wrote:
>
>>
> ufdbguard does.
>
> 16.08.15 20:27, Stanford Prescott ?????:
>
> >>> I have SquidClamAV implemented with the Smoothwall Express 3.1 firewall.
> It
> >>> works well and fast with ssl-bump, although the majority of our users
> only
> >>> have relatively small networks with smaller loads.
> >>>
> >>> FYI, E2Guardian has replaced the DansGuardian project and is currently
> well
> >>> maintained. E2Guardian can do content filtering for SSL but only in
> >>> explicit mode, It currently does not support intercept (transparent) mode
> >>> for SSLBump.
> >>>
> >>> On Fri, Aug 14, 2015 at 10:51 AM, Alex Rousskov <
> >>> rousskov at measurement-factory.com<mailto:rousskov at measurement-factory.com>> wrote:
> >>>
> >>>> On 08/13/2015 10:31 PM, Amos Jeffries wrote:
> >>>>> AFAICS it
> >>>>> is the backend AV library only scanning disk objects that causes the
> >>>>> whole issue. Otherwise the eCAP could be much, much faster.
> >>>>
> >>>> The situation is more nuanced: eCAP supports asynchronous adapters. It
> >>>> is possible to write a ClamAV adapter that writes messages to disk and
> >>>> analyses them without blocking Squid. Doing so should eliminate most
> >>>> overheads between Squid and ClamAV.
> >>>>
> >>>> Factory ClamAV adapter can run in asynchronous mode, but threads are
> >>>> only used for _analysis_ of written files. We have not optimized the
> >>>> file writing part (yet?). Hopefully, using a RAM-based file system can
> >>>> mitigate a large part of that performance damage (as well as address
> >>>> some of the security concerns related to disk storage?).
> >>>>
> >>>> A bigger performance problem, AFAICT, is that ClamAV does not support
> >>>> incremental analysis. It waits for the entire file to be downloaded
> >>>> first. This breaks the message delivery pipeline and increases
> >>>> user-perceived response time. This problem cannot be solved outside the
> >>>> ClamAV library.
> >>>>
> >>>>
> >>>> Cheers,
> >>>>
> >>>> Alex.
> >>>>
> >>>> _______________________________________________
> >>>> squid-users mailing list
> >>>> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJV0QpsAAoJENNXIZxhPexG24gIAMNuWsyfn/QkXWTXROZEJYL1
0frhC+w22fjV8svGjTrZEtSKY4LTHiHEjp99bPBEpPdoCURifUq20m018qRoIcEA
XZfadD+s47bT7FvZbc2W58BQZUsWvotQRMNDPE+Mf0e38ev6PXsj16SaHmWytdx2
Z9H0y5qlgJwwbUyfps4uQn1wF16Qlf2Fw5TGRUbBrij+rjPYzDSXTXxtfT+4j/3V
4lZ0bN0HSFfvJrbfcpPoMCnSlRyJOm/b6Rxqv7v733OtrY/41EW1+HE1HOmW0em3
rwpAV1KgWrwMZYHcIBE147itXlz1RGQutX01auiiSvm/hO3h78rl6aSawmanOAM=
=GgTR
-----END PGP SIGNATURE-----


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/cd9aae4d/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 18 13:08:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 01:08:08 +1200
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D321E0.5070300@ngtech.co.il>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il>
Message-ID: <55D32E38.7020905@treenet.co.nz>

On 19/08/2015 12:15 a.m., Eliezer Croitoru wrote:
> Hey,
> 
> Currently I do not know of such a helper but it is possible to write one.

You surprise me Eliezer. Thought you had been around long enough to be
aware of these SQL database helpers:

<http://www.squid-cache.org/Versions/v3/3.5/manuals/basic_db_auth.html> and
<http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_sql_session_acl.html>

(and for completeness)
<http://www.squid-cache.org/Versions/v3/3.5/manuals/log_db_daemon.html>

Any database that the Perl DBI module can connect to can be used as a
backend to these helpers.


> 
> On 18/08/2015 14:58, adricustodio wrote:
>> Hello guys.
>> I got a few questions.
>> I need to authenticate my squid on my oracle database, is that possible ?

Assuming you mean an actual SQL database such as Oracles' MySQL product
then the answer is yes.

Some other types of database such as BerkleyDB, ActiveDirectory,
eDirectory, RADIUS, or some flat file formats are also supported in
various ways.

Then there is always the possibility of writing your own helper like
Eliezer mentioned. Thats why we call them add-on helpers.


>> if its possible, i can do like a captive portal ? like a hotspot for
>> that ?

No.

The problem is simple: Would you hand your credit card and pin number to
any stranger that walked up and asked you for them ?
Similarly browsers refuse to hand over authentication credentials to any
MITM proxy that suddenly appears and demands credentials.


Like Eliezer mentioned, captive portals must use only _authorization_.

You redirect non-authorized traffic to a "login" page and have a DB
record created for clients that successfully completion of some task
there such as filing out a "login" form. Be aware that has nothing to do
with HTTP authentication though.

Squid comes with two helpers designed for captive portals :

* ext_sql_session_acl uses a SQL database to check some detail like
client IP, MAC address, or some combo with user-agent against records
stored there (ie by your portals "login" form).

* ext_session_acl in "ACTIVE" mode uses clicking on a link to access a
particular activation URL to create its database records. But that
helper maintains its own built-in database.


IMPORTANT: Be careful that the traffic you redirect to your captive
portal page is an HTML GET request by a browser; not a background tool
update, PUT / POST / CONNECT method, browser images/video fetches, etc.


>> The last question is... can i drop the connection of a user on squid
>> after X
>> minutes ?

No. That is not possible. A) there is no "user", B) only idle
connections can be closed arbitrarily, and C) HTTP is message-based not
connection-based.

(C) means that its entirely reasonable for a client to be redirected to
your portal login page for one request. Then make another request on the
same connection after portal login has succeeded. The second request
should be accepted.


>> For example, every 5 minutes connected the user need to authenticate
>> again ?

That is completely unrelated to TCP connections being open or closed.

In captive potal scenarios it is a session timeout. How you configure it
depends on the session helper being used. But make sure the
external_acl_type ttl= parameter is small enough for the timeout to
happen properly, since the timeout can ony reach the helepr on a
multiple of the ttl=N seconds.

With ext_sql_session_acl helper the DB records are maintained separately
in any way you like. You can set a job going that removed DB records
after a timeout. Or have the helper SQL query check a session creation
timestamp in the record is no more than X.

With ext_session_acl you set a timeout (-T) value and sessions are
closed at that time since being opened. You can also set an idle timout.

With a self-written custom helper you do it however you like.

Amos


From squid3 at treenet.co.nz  Tue Aug 18 13:13:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 01:13:27 +1200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
 <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <55D32F77.1070906@treenet.co.nz>

On 19/08/2015 12:44 a.m., Rafael Akchurin wrote:
> Hello Stanford and the list,
> 
> 
> Sorry to jump on a late thread - it is also possible to use ICAP/eCAP server to filter the actual contents of the stream.
> 
> C-ICAP comes to mind first, then eCap samples from http://www.e-cap.org/Downloads
> 

And the *CAP services is a better solution than either URL-rewriting or
chaining proxies. Since the HTTPS only gets MITM'd once, not twice or more.

Amos


From squid3 at treenet.co.nz  Tue Aug 18 13:20:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 01:20:44 +1200
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439900404769-4672732.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
Message-ID: <55D3312C.50701@treenet.co.nz>

On 19/08/2015 12:20 a.m., adricustodio wrote:
> Well the captive portal is not the importante here... 

It is the most critical part of the system. Its very existence
determines whether the rest of your plans are even possible.

Its method of session management determins how your database can
integrate (or not).


> First i need to authenticate my users on oracle DB.
> I was thinking if its possible to create a mysql DB and replicate my oracle
> on mysql DB and authenticate on the mysql localhost... what do you think ?

What do you mean by "oracle DB" ?

Does it have an SQL interface?

Amos



From adricustodio at uol.com.br  Tue Aug 18 13:22:26 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 06:22:26 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D32E38.7020905@treenet.co.nz>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <55D32E38.7020905@treenet.co.nz>
Message-ID: <1439904146021-4672738.post@n4.nabble.com>

Well, thanks for the help.
The captive portal isnt importante, and the connection timeout either.

My DB is an oracle/oracle literally... isnt oracle/mysql.
So, is there any way ?
Im thinking to autheticate on localhost (ncsa) but create a job on my oracle
to write the users and passwords on a file and my squid gets from there or a
script that write the users and passwords on my squid.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672738.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Aug 18 13:39:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 18 Aug 2015 19:39:58 +0600
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D3312C.50701@treenet.co.nz>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz>
Message-ID: <55D335AE.9090308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.08.15 19:20, Amos Jeffries ?????:
> On 19/08/2015 12:20 a.m., adricustodio wrote:
>> Well the captive portal is not the importante here...
>
> It is the most critical part of the system. Its very existence
> determines whether the rest of your plans are even possible.
>
> Its method of session management determins how your database can
> integrate (or not).
>
>
>> First i need to authenticate my users on oracle DB.
>> I was thinking if its possible to create a mysql DB and replicate my
oracle
>> on mysql DB and authenticate on the mysql localhost... what do you
think ?
>
> What do you mean by "oracle DB" ?
>
> Does it have an SQL interface?
Sure. This is ANSI/99 SQL-compliant DB. ;) It has own proprietary
SQL*Net API, Pro*C API, ODBC API.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV0zWuAAoJENNXIZxhPexGjGIH/jxhQXRpVwvlBnt08KFxFXKF
wnvkiG1VI8L+xRE+71e8vqY0zvwbQCtql96tBwSb7l1Wxx5e30ynJcFVY/G4wJtm
0YTInXlGs/Sz6EnDpquPLhGxBZ2D02iAea948Hnbmgh0v5FbQMXCBVvJTjezw4Dv
kBftVJQVb3Clwn4t9+fVFiyK5GcALLtwNlSqNnag3yDWzxQL6BDDa08yYJg71Cle
Pzll2dEe6S8HgfZ5cGydzYWy1QI7DMPz3Jn2iQsyIkhH6oxbzt/Cl1jjI55+PdCK
HKU5dxVxFHVEkf9TBC9IrrnsGtbgEQbDwXHxkuQfOUmdYQzAPCwwG8wKiFwmUj8=
=vHpT
-----END PGP SIGNATURE-----



From bernih75 at gmail.com  Tue Aug 18 13:43:02 2015
From: bernih75 at gmail.com (Hicham Berni)
Date: Tue, 18 Aug 2015 15:43:02 +0200
Subject: [squid-users] Change url_rewrite_program problem
Message-ID: <CANFg0+zNUCT60P8+GEaNB+Frdojj_RZShi1=dbUjPeQD5_QWZg@mail.gmail.com>

Hi,

We have a squid reverse configuration, and we need to change  backend
webserver  with a new webserver with new IP and port (80 --> 8000) .

Old squid configuration used url_rewrite_program. I 'not familar with this
configuration.
With port changing on new web server ( 80 --> 8000) , i find that i had to
add new production web on url_rerite_program.

I tried to change program , but i met a issue.

on part 1 --> current rewrite program configuration  running today and
correctly  for testurl --> running fine
on part 2 --> configuration with change  for testurl and productionurl ->
OK for productionurl and KO for testurl

An idea if something is wrong on change ?

Part 1)  old configuration :

#!/usr/bin/perl

$INTERNALIP="15.40.40.40";
$PRODUCTIONURL="add.ptr.lu";
$TESTURL="test.add.ptr.lu";
$TESTPORT="8001";

# turn off write buffering
$| = 1;
while (<>) {

  # get the URL from the request
  chomp($url = $_);

  if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
  {
    # fix up the cname and port
    $url =~ s^:$TESTPORT^^;
    $url =~ s^$INTERNALIP^$TESTURL^;

    # fix the protocol
    $url =~ s^https://^http://^;
  }
  else
  {
    # fix up the name
    $url =~ s^$INTERNALIP^$PRODUCTIONURL^;

    # fix the protocol
    $url =~ s^http://^https://^;
  }

  # return the fixed URL to squid
  print "$url\n";
}


Part 2) configuration with changes

#!/usr/bin/perl

$INTERNALIP="15.40.40.40";
$PRODUCTIONURL="add.ptr.lu";
$TESTURL="test.add.ptr.lu";
$TESTPORT="8001";
$PRODPORT="8000";

# turn off write buffering
$| = 1;
while (<>) {

  # get the URL from the request
  chomp($url = $_);

  if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
  {
    # fix up the cname and port
    $url =~ s^:$TESTPORT^^;
    $url =~ s^$INTERNALIP^$TESTURL^;

    # fix the protocol
    $url =~ s^https://^http://^;
  }

elsif ($url =~ m/($INTERNALIP|$PRODUCTIONURL):$PRODPORT/)
  {
    # fix up the cname and port
    $url =~ s^:$PRODPORT^^;
    $url =~ s^$INTERNALIP^$PRODURL^;

    # fix the protocol
    $url =~ s^https://^http://^;
  }

  else
  {
    # fix up the name
    $url =~ s^$INTERNALIP^$PRODUCTIONURL^;

    # fix the protocol
    $url =~ s^http://^https://^;
  }

  # return the fixed URL to squid
  print "$url\n";
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/20b48f81/attachment.htm>

From yvoinov at gmail.com  Tue Aug 18 13:44:17 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 18 Aug 2015 19:44:17 +0600
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D3312C.50701@treenet.co.nz>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz>
Message-ID: <55D336B1.2090508@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also, AFAIK, exists Perl DBI interface to Oracle:

https://www.google.com/search?q=Perl+Oracle+RBDMS+interface

And this is real possibility to write own (or modify existing) auth helper.

18.08.15 19:20, Amos Jeffries ?????:
> On 19/08/2015 12:20 a.m., adricustodio wrote:
>> Well the captive portal is not the importante here...
>
> It is the most critical part of the system. Its very existence
> determines whether the rest of your plans are even possible.
>
> Its method of session management determins how your database can
> integrate (or not).
>
>
>> First i need to authenticate my users on oracle DB.
>> I was thinking if its possible to create a mysql DB and replicate my
oracle
>> on mysql DB and authenticate on the mysql localhost... what do you
think ?
>
> What do you mean by "oracle DB" ?
>
> Does it have an SQL interface?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV0zaxAAoJENNXIZxhPexG0XoH/AmYdFe4UH48idN8EkrcbAoF
bgTjDMisDpcscNYqHBGOIiYXix6Fu8qx/Hec+HfxzKA5E65p0oekQJA08hriSRav
5O2GRERHD9JUAZ69xnXUX4T4bwdi4a8Uvyrll1H2uzg0w9QQ1igzs8TW8RndORY8
8rfQ2kNn/lO6WEyYSfLzLFNqvaFXprKpW0XM+V5NKN4jSZngwO7BeNCwiywQucVr
rOOruEvPoB6lfrZOJ+MTaxUTxmlc/W2KlzPGWZ4byqDAmHESqYX2O1ffNTaA5hKh
mGLentMDEGbIcgQssvsVbU13gNFAyIA5rZFel0V8Bo6oQ7Wa9ax4Cha585K4oZI=
=lkJp
-----END PGP SIGNATURE-----



From adricustodio at uol.com.br  Tue Aug 18 14:06:47 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 07:06:47 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D336B1.2090508@gmail.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
Message-ID: <1439906807136-4672742.post@n4.nabble.com>

I was thinking... isnt it possible to make squid authenticate on a radius
server and the radius goes into the oracle db ?

Sorry if it sounds stupid, im new on DB. I usually only work on proxy with
ncsa authentication.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672742.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Tue Aug 18 14:41:19 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 17:41:19 +0300
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D32E38.7020905@treenet.co.nz>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <55D32E38.7020905@treenet.co.nz>
Message-ID: <55D3440F.1040109@ngtech.co.il>

Sometimes I even surprise myself.

I was talking about an OracleDB supported one.
I know that there are other helpers but since Oracle uses another ways 
to run things, the current helpers, as far as I could understood from 
him was not of help to him.

For a sec I forgot that the script is written in perl :\
* This is what happens when working for an hour or two and diving into 
the other world of snmp :D

Eliezer

On 18/08/2015 16:08, Amos Jeffries wrote:
> On 19/08/2015 12:15 a.m., Eliezer Croitoru wrote:
>> Hey,
>>
>> Currently I do not know of such a helper but it is possible to write one.
>
> You surprise me Eliezer. Thought you had been around long enough to be
> aware of these SQL database helpers:
>
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/basic_db_auth.html> and
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_sql_session_acl.html>
>
> (and for completeness)
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/log_db_daemon.html>
>
> Any database that the Perl DBI module can connect to can be used as a
> backend to these helpers.
>
>
>>
>> On 18/08/2015 14:58, adricustodio wrote:
>>> Hello guys.
>>> I got a few questions.
>>> I need to authenticate my squid on my oracle database, is that possible ?
>
> Assuming you mean an actual SQL database such as Oracles' MySQL product
> then the answer is yes.
>
> Some other types of database such as BerkleyDB, ActiveDirectory,
> eDirectory, RADIUS, or some flat file formats are also supported in
> various ways.
>
> Then there is always the possibility of writing your own helper like
> Eliezer mentioned. Thats why we call them add-on helpers.
>
>
>>> if its possible, i can do like a captive portal ? like a hotspot for
>>> that ?
>
> No.
>
> The problem is simple: Would you hand your credit card and pin number to
> any stranger that walked up and asked you for them ?
> Similarly browsers refuse to hand over authentication credentials to any
> MITM proxy that suddenly appears and demands credentials.
>
>
> Like Eliezer mentioned, captive portals must use only _authorization_.
>
> You redirect non-authorized traffic to a "login" page and have a DB
> record created for clients that successfully completion of some task
> there such as filing out a "login" form. Be aware that has nothing to do
> with HTTP authentication though.
>
> Squid comes with two helpers designed for captive portals :
>
> * ext_sql_session_acl uses a SQL database to check some detail like
> client IP, MAC address, or some combo with user-agent against records
> stored there (ie by your portals "login" form).
>
> * ext_session_acl in "ACTIVE" mode uses clicking on a link to access a
> particular activation URL to create its database records. But that
> helper maintains its own built-in database.
>
>
> IMPORTANT: Be careful that the traffic you redirect to your captive
> portal page is an HTML GET request by a browser; not a background tool
> update, PUT / POST / CONNECT method, browser images/video fetches, etc.
>
>
>>> The last question is... can i drop the connection of a user on squid
>>> after X
>>> minutes ?
>
> No. That is not possible. A) there is no "user", B) only idle
> connections can be closed arbitrarily, and C) HTTP is message-based not
> connection-based.
>
> (C) means that its entirely reasonable for a client to be redirected to
> your portal login page for one request. Then make another request on the
> same connection after portal login has succeeded. The second request
> should be accepted.
>
>
>>> For example, every 5 minutes connected the user need to authenticate
>>> again ?
>
> That is completely unrelated to TCP connections being open or closed.
>
> In captive potal scenarios it is a session timeout. How you configure it
> depends on the session helper being used. But make sure the
> external_acl_type ttl= parameter is small enough for the timeout to
> happen properly, since the timeout can ony reach the helepr on a
> multiple of the ttl=N seconds.
>
> With ext_sql_session_acl helper the DB records are maintained separately
> in any way you like. You can set a job going that removed DB records
> after a timeout. Or have the helper SQL query check a session creation
> timestamp in the record is no more than X.
>
> With ext_session_acl you set a timeout (-T) value and sessions are
> closed at that time since being opened. You can also set an idle timout.
>
> With a self-written custom helper you do it however you like.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Tue Aug 18 14:46:10 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 17:46:10 +0300
Subject: [squid-users] Change url_rewrite_program problem
In-Reply-To: <CANFg0+zNUCT60P8+GEaNB+Frdojj_RZShi1=dbUjPeQD5_QWZg@mail.gmail.com>
References: <CANFg0+zNUCT60P8+GEaNB+Frdojj_RZShi1=dbUjPeQD5_QWZg@mail.gmail.com>
Message-ID: <55D34532.5060801@ngtech.co.il>

Hey Berni,

I was wondering to myself, why do you need to rewrite the url?
Can't you just use a cache_peer and couple acls?

Eliezer

On 18/08/2015 16:43, Hicham Berni wrote:
> Hi,
>
> We have a squid reverse configuration, and we need to change  backend
> webserver  with a new webserver with new IP and port (80 --> 8000) .
>
> Old squid configuration used url_rewrite_program. I 'not familar with this
> configuration.
> With port changing on new web server ( 80 --> 8000) , i find that i had to
> add new production web on url_rerite_program.
>
> I tried to change program , but i met a issue.
>
> on part 1 --> current rewrite program configuration  running today and
> correctly  for testurl --> running fine
> on part 2 --> configuration with change  for testurl and productionurl ->
> OK for productionurl and KO for testurl
>
> An idea if something is wrong on change ?
>
> Part 1)  old configuration :
>
> #!/usr/bin/perl
>
> $INTERNALIP="15.40.40.40";
> $PRODUCTIONURL="add.ptr.lu";
> $TESTURL="test.add.ptr.lu";
> $TESTPORT="8001";
>
> # turn off write buffering
> $| = 1;
> while (<>) {
>
>    # get the URL from the request
>    chomp($url = $_);
>
>    if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
>    {
>      # fix up the cname and port
>      $url =~ s^:$TESTPORT^^;
>      $url =~ s^$INTERNALIP^$TESTURL^;
>
>      # fix the protocol
>      $url =~ s^https://^http://^;
>    }
>    else
>    {
>      # fix up the name
>      $url =~ s^$INTERNALIP^$PRODUCTIONURL^;
>
>      # fix the protocol
>      $url =~ s^http://^https://^;
>    }
>
>    # return the fixed URL to squid
>    print "$url\n";
> }
>
>
> Part 2) configuration with changes
>
> #!/usr/bin/perl
>
> $INTERNALIP="15.40.40.40";
> $PRODUCTIONURL="add.ptr.lu";
> $TESTURL="test.add.ptr.lu";
> $TESTPORT="8001";
> $PRODPORT="8000";
>
> # turn off write buffering
> $| = 1;
> while (<>) {
>
>    # get the URL from the request
>    chomp($url = $_);
>
>    if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
>    {
>      # fix up the cname and port
>      $url =~ s^:$TESTPORT^^;
>      $url =~ s^$INTERNALIP^$TESTURL^;
>
>      # fix the protocol
>      $url =~ s^https://^http://^;
>    }
>
> elsif ($url =~ m/($INTERNALIP|$PRODUCTIONURL):$PRODPORT/)
>    {
>      # fix up the cname and port
>      $url =~ s^:$PRODPORT^^;
>      $url =~ s^$INTERNALIP^$PRODURL^;
>
>      # fix the protocol
>      $url =~ s^https://^http://^;
>    }
>
>    else
>    {
>      # fix up the name
>      $url =~ s^$INTERNALIP^$PRODUCTIONURL^;
>
>      # fix the protocol
>      $url =~ s^http://^https://^;
>    }
>
>    # return the fixed URL to squid
>    print "$url\n";
> }
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From yvoinov at gmail.com  Tue Aug 18 14:55:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 18 Aug 2015 20:55:41 +0600
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439906807136-4672742.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com>
Message-ID: <55D3476D.1040203@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Oracle has it's own LDAP server, named Oracle Internet Directory. With
Oracle RDBMS at backend.

Just go http://oracle.com.

Squid supports LDAP auth.

PS. BTW, you know how much does Oracle license's cost? Per CPU core? OID
and RDBMS licenses separately.

18.08.15 20:06, adricustodio ?????:
> I was thinking... isnt it possible to make squid authenticate on a radius
> server and the radius goes into the oracle db ?
>
> Sorry if it sounds stupid, im new on DB. I usually only work on proxy with
> ncsa authentication.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672742.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV00dtAAoJENNXIZxhPexGXfMH/A9jCqJ38p9y9HnGaJJ4TBil
HmHbhrNQUTCOOCxa3+QHyHUpnydpOoT2yWpHf85rf9A1pqNMUNPftFikfTeLRxJC
X4RF8Ej4VS04xg4iNpbH4WDjWdg/Dj/l9zEKoVGeIZmv12fLQUMac5F1meBOlM6C
FzhEWAOCyG+a4SAc5c6OdQOArHl13bNLormZvl9PY/VoZI1z9YNSpkJ9HNrF0ovl
l6A9DdGNsAiJw2NkJBvYx26momg3kdsCnaYiwbyvs/PFAdY2bvEK0qlnyO0iglIS
32e+pAMpTaAgpkWjOs423VM3FfDxN7UIDaf71Pz+QsiU9uRmxoPdzIRVwtxZCsk=
=5p1U
-----END PGP SIGNATURE-----



From vkukk at xvidservices.com  Tue Aug 18 15:18:13 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Tue, 18 Aug 2015 18:18:13 +0300
Subject: [squid-users] Squid reverse proxy in http > https mode
Message-ID: <55D34CB5.7060900@xvidservices.com>

Hi

I'm trying to get most optimized solution for caching objects of cloud 
storage.

The data flow I'd like to achieve is: http client <http> squid reverse 
mode cache <https> remote https storage server

Common scenarios/examples of accel mode http(s) port include opposite 
direction of traffic encryption/decryption, client connecting via https 
and backend servers (cache_peer) being http. In my case client connects 
via localhost and to reduce cpu load, I'd like to have this connection 
without encryption and start encryption only when data leaves local 
squid to parent cache_peer, because this remote server speaks https only.


Is this possible with squid?

Another related question is: when cache_peer hostname is dns name that 
resolves into multiple ip addresses, then how does squid act? Does it 
resolve it once and use first ip addres or all addresses in round robin, 
or does it resolve every time cache_peer is accessed? What happens if 
one of the addresses is not accessible? I have checked documentation but 
did not find any explanation on that topic.

Best regards
Veiko


From eliezer at ngtech.co.il  Tue Aug 18 16:23:58 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 19:23:58 +0300
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D3476D.1040203@gmail.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D3476D.1040203@gmail.com>
Message-ID: <55D35C1E.3050703@ngtech.co.il>

On 18/08/2015 17:55, Yuri Voinov wrote:
> PS. BTW, you know how much does Oracle license's cost? Per CPU core? OID
> and RDBMS licenses separately.

And for some it's worth.
This is a bit different but if you get into a pants shop and buy a pair 
in the price of 400$ just because it's custom made for you then it might 
worth.

If there is a currently implemented OracleDB based system then there is 
no reason to not use it.

Eliezer


From adricustodio at uol.com.br  Tue Aug 18 16:20:19 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 09:20:19 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D3476D.1040203@gmail.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D3476D.1040203@gmail.com>
Message-ID: <1439914819502-4672747.post@n4.nabble.com>

Ohh i see... 

So i can use OID, but basically i will need to buy more licenses right ?

I got another idea... i can export the table i need from oracle as .sql...
how about i import that on mysql server on linux and autheticate with mysql
?

Would that be possible ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672747.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Tue Aug 18 16:32:30 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 18 Aug 2015 19:32:30 +0300
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439914819502-4672747.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D3476D.1040203@gmail.com>
 <1439914819502-4672747.post@n4.nabble.com>
Message-ID: <55D35E1E.40802@ngtech.co.il>

Hey,

It is possible to export and import but you need to ask the question if 
you need to always update the mysql DB you should run it every minute or 
so.(it is being done in production systems in many places, dump and update)
The other question is how to do it.. I do not know yet how.

Eliezer

On 18/08/2015 19:20, adricustodio wrote:
> Ohh i see...
>
> So i can use OID, but basically i will need to buy more licenses right ?
>
> I got another idea... i can export the table i need from oracle as .sql...
> how about i import that on mysql server on linux and autheticate with mysql
> ?
>
> Would that be possible ?
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672747.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From vdoctor at neuf.fr  Tue Aug 18 16:39:01 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 18 Aug 2015 09:39:01 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
Message-ID: <1439915941519-4672750.post@n4.nabble.com>

Hi All,

I'm facing a weird situation with a squid *3.5.7*, have a look:
Store Directory #0 (aufs): /cachesdg/spool
FS Block Size 4096 Bytes
First level subdirectories: 16
Second level subdirectories: 256
Maximum Size: 67108864 KB
Current Size: 66288408.00 KB
*Percent Used: 98.78%*
Filemap bits in use: 7858031 of 8388608 (94%)
Filesystem Space in use: 63582908/575775620 KB (11%)
Filesystem Inodes in use: 7428375/36569088 (20%)
Flags: SELECTED
Removal policy: lru
LRU reference age: 1.06 days

In the squid.conf:
...
cache_swap_low 75
cache_swap_high *80*
...

Is that normal ?
I should see the Squid cleaning objects to respect the 80% swap high, am I
wrong ?


Thanks in advance for your comments...

Bye fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From adricustodio at uol.com.br  Tue Aug 18 16:43:05 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 09:43:05 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D35E1E.40802@ngtech.co.il>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D3476D.1040203@gmail.com>
 <1439914819502-4672747.post@n4.nabble.com> <55D35E1E.40802@ngtech.co.il>
Message-ID: <1439916185403-4672751.post@n4.nabble.com>

our DB guy said its possible... we dont need to update that much, since we
will only need to update when we got a new user, and sometimes this user
wont connect on squid.. so it will be updated every 2 days... maybe 1
week... 

So im thinking about trying that, export my oracle DB as a .mysql and import
that on a mysql base on squid server and try to autheticate there... 

Like i said, im a sysadmin, but i dont do databases, all i know is reading
on internet.
Our DB guy dont know firewalls/proxy, so we need to work together on this!
hahahahha

thanks for the helps dudes!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672751.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From correiasardinha at gmail.com  Tue Aug 18 16:54:11 2015
From: correiasardinha at gmail.com (Pedro Correia Sardinha)
Date: Tue, 18 Aug 2015 17:54:11 +0100
Subject: [squid-users] Bridge/Tproxy: https dns
In-Reply-To: <55D2C2D9.5080301@treenet.co.nz>
References: <CADLGAbK3wbE-5M_88tWJwYFcb_NdXUuv6cR9czUsNrLHanxfSg@mail.gmail.com>
 <55D2C2D9.5080301@treenet.co.nz>
Message-ID: <CADLGAbK2PcSbSMehhXyN7sJtCWNKubPRkcwDKr+yeZBj=jNiDA@mail.gmail.com>

Thank you Amos for the review.

Following your advice... trying to use the pure transparent proxy (Tproxy)
but getting different behaviours with google domains in google chrome and
after adding peek there aren't still no names in https addresses.

First create ssl certs directory, just to check later.
/usr/lib64/squid/ssl_crtd -c -s /var/log/squid/lib/ssl_db

remove from squid.conf:
#tcp_outgoing_address
#always_direct allow ssl-bump_port
#ssl_bump none all

add to squid.conf:
ssl_bump splice localnet
ssl_bump peek all
ssl_bump splice all

I removed IPv6 restriction in kernel and in new squid compile without
--disable-ipv6 option. And I added some similiar IPv6 rules to Tproxy boot
script.

I continue to not generate-host-certificates(=off), I can check it in
ssl_db directory. But, just for testing, if I change
generate-host-certificates to on then there are certificates changes with
my self CA Autority notice in Google Chrome related to google sites, some
other sites tested get the certificate in right way. In IE and Firefox
there are no certificates issues.
A workaround for this issue in Google Chrome with websites in google
domains was create a specific acl for IP from google domains and splice
first like did with localnet:
acl google dst "/etc/squid/google.txt"
...
ssl_bump splice google
...

After those changes and setup ssl-bump with peek for most of sites (e.g.
facebook) but still no names in logs, just the IP in https navigation.

access.log:
1439907457.958     49 192.168.0.102 TCP_MISS/200 910 POST
http://ocsp.digicert.com/ - ORIGINAL_DST/93.184.220.29
application/ocsp-response
1439907466.798  59053 192.168.0.102 TCP_TUNNEL/200 3944 CONNECT
212.113.184.216:443 - ORIGINAL_DST/212.113.184.216 -
1439907472.813  58798 192.168.0.102 TCP_TUNNEL/200 8320 CONNECT
212.113.185.35:443 - ORIGINAL_DST/212.113.185.35 -
1439907472.817  59014 192.168.0.102 TCP_TUNNEL/200 5234 CONNECT
212.113.184.221:443 - ORIGINAL_DST/212.113.184.221 -
1439907490.935  65674 192.168.0.102 TCP_TUNNEL/200 4207 CONNECT
54.171.32.174:443 - ORIGINAL_DST/54.171.32.174 -
1439907527.998 115712 192.168.0.102 TCP_TUNNEL/200 5485 CONNECT
54.192.61.197:443 - ORIGINAL_DST/54.192.61.197 -
1439907545.804 122741 192.168.0.102 TCP_TUNNEL/200 5817 CONNECT
132.245.50.66:443 - ORIGINAL_DST/132.245.50.66 -


cache.log:
---------
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: max-age=499906
Content-Type: application/ocsp-response
Date: Tue, 18 Aug 2015 14:17:31 GMT
ETag: "55d2d1da-1d7"
Expires: Tue, 25 Aug 2015 02:17:31 GMT
Last-Modified: Tue, 18 Aug 2015 06:34:02 GMT
Server: ECS (mad/42F2)
X-Cache: HIT
Content-Length: 471
X-Cache: MISS from squidhead2.skywalker.local
Via: 1.1 squidhead2.skywalker.local (squid/3.5.7)
Connection: keep-alive
----------
2015/08/18 15:17:37.958 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/18 15:17:37.958 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/18 15:17:37.958 kid1| store.cc(955) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2015/08/18 15:17:38.152 kid1| TcpAcceptor.cc(222) doAccept: New connection
on FD 15
2015/08/18 15:17:38.152 kid1| TcpAcceptor.cc(297) acceptNext: connection on
local=[::]:3130 remote=[::] FD 15 flags=25
2015/08/18 15:17:38.153 kid1| client_side.cc(3890)
httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.6:443
remote=192.168.0.102 FD 40 flags=17
2015/08/18 15:17:38.153 kid1| client_side.cc(2337) parseHttpRequest: HTTP
Client local=31.13.90.6:443 remote=192.168.0.102 FD 40 flags=17
2015/08/18 15:17:38.153 kid1| client_side.cc(2338) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 31.13.90.6:443 HTTP/1.1
Host: 31.13.90.6:443
----------
2015/08/18 15:17:38.153 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.6:443 is ALLOWED; last
ACL checked: ssl-bump_port
2015/08/18 15:17:38.153 kid1| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/08/18 15:17:38.153 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.6:443 is ALLOWED; last
ACL checked: ssl-bump_port
2015/08/18 15:17:38.153 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
sources for '31.13.90.6:443'
2015/08/18 15:17:38.153 kid1| peer_select.cc(281) peerSelectDnsPaths:
always_direct = DENIED
2015/08/18 15:17:38.153 kid1| peer_select.cc(282) peerSelectDnsPaths:
 never_direct = DENIED
2015/08/18 15:17:38.153 kid1| peer_select.cc(288) peerSelectDnsPaths:
 ORIGINAL_DST = local=192.168.0.102 remote=31.13.90.6:443 flags=25
2015/08/18 15:17:38.153 kid1| peer_select.cc(295) peerSelectDnsPaths:
 timedout = 0
2015/08/18 15:17:38.156 kid1| TcpAcceptor.cc(222) doAccept: New connection
on FD 15
2015/08/18 15:17:38.156 kid1| TcpAcceptor.cc(297) acceptNext: connection on
local=[::]:3130 remote=[::] FD 15 flags=25
2015/08/18 15:17:38.156 kid1| client_side.cc(3890)
httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.6:443
remote=192.168.0.102 FD 42 flags=17
2015/08/18 15:17:38.156 kid1| client_side.cc(2337) parseHttpRequest: HTTP
Client local=31.13.90.6:443 remote=192.168.0.102 FD 42 flags=17
2015/08/18 15:17:38.156 kid1| client_side.cc(2338) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 31.13.90.6:443 HTTP/1.1
Host: 31.13.90.6:443
----------
2015/08/18 15:17:38.157 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.6:443 is ALLOWED; last
ACL checked: ssl-bump_port
2015/08/18 15:17:38.157 kid1| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/08/18 15:17:38.157 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 31.13.90.6:443 is ALLOWED; last
ACL checked: ssl-bump_port
2015/08/18 15:17:38.157 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
sources for '31.13.90.6:443'
2015/08/18 15:17:38.157 kid1| peer_select.cc(281) peerSelectDnsPaths:
always_direct = DENIED
2015/08/18 15:17:38.157 kid1| peer_select.cc(282) peerSelectDnsPaths:
 never_direct = DENIED
2015/08/18 15:17:38.157 kid1| peer_select.cc(288) peerSelectDnsPaths:
 ORIGINAL_DST = local=192.168.0.102 remote=31.13.90.6:443 flags=25
2015/08/18 15:17:38.157 kid1| peer_select.cc(295) peerSelectDnsPaths:
 timedout = 0
2015/08/18 15:17:45.351 kid1| TcpAcceptor.cc(222) doAccept: New connection
on FD 15
2015/08/18 15:17:45.351 kid1| TcpAcceptor.cc(297) acceptNext: connection on
local=[::]:3130 remote=[::] FD 15 flags=25
2015/08/18 15:17:45.351 kid1| client_side.cc(3890)
httpsSslBumpAccessCheckDone: sslBump not needed for local=212.113.185.24:443
remote=192.168.0.102 FD 46 flags=17
2015/08/18 15:17:45.351 kid1| client_side.cc(2337) parseHttpRequest: HTTP
Client local=212.113.185.24:443 remote=192.168.0.102 FD 46 flags=17
2015/08/18 15:17:45.351 kid1| client_side.cc(2338) parseHttpRequest: HTTP
Client REQUEST:
---------
CONNECT 212.113.185.24:443 HTTP/1.1
Host: 212.113.185.24:443
----------
2015/08/18 15:17:45.351 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 212.113.185.24:443 is ALLOWED;
last ACL checked: ssl-bump_port
2015/08/18 15:17:45.351 kid1| client_side_request.cc(717)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/08/18 15:17:45.351 kid1| client_side_request.cc(741)
clientAccessCheckDone: The request CONNECT 212.113.185.24:443 is ALLOWED;
last ACL checked: ssl-bump_port
2015/08/18 15:17:45.351 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
sources for '212.113.185.24:443'
2015/08/18 15:17:45.351 kid1| peer_select.cc(281) peerSelectDnsPaths:
always_direct = DENIED
2015/08/18 15:17:45.351 kid1| peer_select.cc(282) peerSelectDnsPaths:
 never_direct = DENIED
2015/08/18 15:17:45.351 kid1| peer_select.cc(288) peerSelectDnsPaths:
 ORIGINAL_DST = local=192.168.0.102 remote=212.113.185.24:443 flags=25
2015/08/18 15:17:45.351 kid1| peer_select.cc(295) peerSelectDnsPaths:
 timedout = 0

Can I ask what I missed?
Thank you for your time.



On Tue, Aug 18, 2015 at 6:30 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/08/2015 12:25 a.m., Pedro Correia Sardinha wrote:
> > Hello,
> >
> > I'm trying to setup a Squid server simple as possible just to review the
> > web use in office using the last stable version 3.5.7.
> >
>
> And you chose TPROXY with ssl-bump'ing. The two most complex features to
> setup. lol.
>
> > I setup the bridge with 2 NIC, br0 with IP 192.168.0.5 and I had disable
> > IPv6 on boot in my Slackware Current (Fri Aug 14 2015) server.
>
> Sigh. Ever heard of IPv6-over-IPv4, 6-in-4, 6to4, etc. ?
> All protocols designed to "fix" connectivity going through machines
> setup like yours.
>
> And why bother disabling a (BCP 177) mandatory part of the kernel?
> The correct way to handle unwanted traffic is to firewall it. Not to
> play around with kernel internals.
>
> <snip>
> > My squid.conf:
>
> > tcp_outgoing_address 85.138.204.43
>
> This is irrelevant with TPROXY. The client IP address is used instead.
>
> For the regular forward-proxy traffic on port 3128 the machines default
> IP will be used.
>
>
> > dns_v4_first on
> > pinger_enable off
> > http_port 3128
> > http_port 3129 tproxy
> > https_port 3130 ssl-bump tproxy generate-host-certificates=off
> > cert=/etc/squid/ssl/squid.pem cafile=/etc/squid/ssl/squid.pem
> > always_direct allow ssl-bump_port
> > ssl_bump none all
>
> You have configued Squid not to even look at the TLS details.
>
>
> > dns_nameservers 8.8.8.8 8.8.4.4
> > access_log daemon:/var/log/squid/access.log squid
> > cache deny all
> > pid_filename /var/run/squid/squid.pid
> > coredump_dir /var/log/squid/cache/squid
> > visible_hostname myservername.domain.local
> >
> > In general the configuration (squid.conf) it's working but has some
> > incomplete behaviors as shows in log files.
> >
> > access.log (I know this is Facebook but there are no dns resolusion in
> > https, just IP):
> > 1439811492.625   2377 192.168.0.102 TCP_TUNNEL/200 3574 CONNECT
> > 31.13.90.2:443 - ORIGINAL_DST/31.13.90.2 -
>
>
> What sort of resolution were you expecting?
>
> * The above log line is recording the TCP connection. TCP packets do not
> have any "domain name" fields that need resolving to IP addresses.
>
> * you also configured Squid not to look at the TLS details where it
> might have found an SNI entry with server domain name.
>
> The result is that Squid is working purely with TPROXY IP addresses and
> setting up a TCP tunnel to relay the traffic through.
>
>
> >
> > cache.log:
> > HTTP/1.1 200 OK
> > Accept-Ranges: bytes
> > Cache-Control: max-age=504747
> > Content-Type: application/ocsp-response
> > Date: Mon, 17 Aug 2015 11:38:03 GMT
> > ETag: "55d15943-1d7"
> > Expires: Sun, 23 Aug 2015 23:38:03 GMT
> > Last-Modified: Mon, 17 Aug 2015 03:47:15 GMT
> > Server: ECS (mad/439C)
> > X-Cache: HIT
> > Content-Length: 471
> > X-Cache: MISS from squidhead2.skywalker.local
> > Via: 1.1 squidhead2.skywalker.local (squid/3.5.7)
> > Connection: keep-alive
> > ----------
> > 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2015/08/17 12:38:09.067 kid1| store.cc(955) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
>
>
> Above is all the end of some transaction that was started earlier. No
> useful details in the provided log snippet about it.
>
>
>
> This is where a transaction actually starts:
>
> > 2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(222) doAccept: New
> connection
> > on FD 12
> > 2015/08/17 12:38:10.248 kid1| TcpAcceptor.cc(297) acceptNext: connection
> on
> > local=0.0.0.0:3130 remote=[::] FD 12 flags=25
> > 2015/08/17 12:38:10.248 kid1| client_side.cc(3890)
>
> Notice how the connection local IP:port details change from port 3130 to
> port 443. Thats TPROXY working.
>
> > httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.2:443
> > remote=192.168.0.102 FD 50 flags=17
>
> This is the "ssl_bump none" action working (by not doing anything TLS
> related) exactly as you configured.
>
> Squid is now processing an internally generated CONNECT request
> representing the intercepted TPROXY connection in a way that can be
> logged and/or relayed to other proxies if it needs to.
>
> > 2015/08/17 12:38:10.248 kid1| client_side.cc(2337) parseHttpRequest: HTTP
> > Client local=31.13.90.2:443 remote=192.168.0.102 FD 50 flags=17
> > 2015/08/17 12:38:10.248 kid1| client_side.cc(2338) parseHttpRequest: HTTP
> > Client REQUEST:
> > ---------
> > CONNECT 31.13.90.2:443 HTTP/1.1
> > Host: 31.13.90.2:443
> > ---------
> > 2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
> > clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED;
> last
> > ACL checked: localnet
> > 2015/08/17 12:38:10.248 kid1| client_side_request.cc(717)
> > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> > 2015/08/17 12:38:10.248 kid1| client_side_request.cc(741)
> > clientAccessCheckDone: The request CONNECT 31.13.90.2:443 is ALLOWED;
> last
> > ACL checked: localnet
> > 2015/08/17 12:38:10.248 kid1| peer_select.cc(280) peerSelectDnsPaths:
> Found
> > sources for '31.13.90.2:443'
>
> The CONNECT request uses a raw-IP address provided by TPROXY. There is
> no name to resolve.
>
> > 2015/08/17 12:38:10.248 kid1| peer_select.cc(281) peerSelectDnsPaths:
> > always_direct = ALLOWED
> > 2015/08/17 12:38:10.248 kid1| peer_select.cc(282) peerSelectDnsPaths:
> >  never_direct = DENIED
> > 2015/08/17 12:38:10.248 kid1| peer_select.cc(288) peerSelectDnsPaths:
> >  ORIGINAL_DST = local=192.168.0.102 remote=31.13.90.2:443 flags=25
> > 2015/08/17 12:38:10.248 kid1| peer_select.cc(295) peerSelectDnsPaths:
> >  timedout = 0
>
> ... stuff happens for 2 seconds...
>
> > 2015/08/17 12:38:12.621 kid1| client_side.cc(815) swanSong: local=
> > 31.13.90.2:443 remote=192.168.0.102 flags=17
> > 2015/08/17 12:38:12.625 kid1| client_side.cc(815) swanSong: local=
> > 31.13.90.2:443 remote=192.168.0.102 flags=17
>
> Then the connection closes.
>
> Looks perfectly normal and expected behaviour to me considering what you
> configured.
>
> >
> >
> > The logs with http (port 80) has the name resolution of navigation.
> >
> > I disabled pinger because give some error:
> <snip>
> > 2015/08/17 12:49:55| FATAL: pinger: Unable to open any ICMP sockets.
>
> Okay. Not a big problem. The pinger helper needs its suid bit set, which
> is not working on all system installations yet. Disabling it is fine.
>
>
> > Do I have to setup local DNS server? the internal DNS of squid can't
> handle
> > https in Tproxy?
> > What's missing to have name resolution in https traffic as its showed in
> > http traffic?
>
> Whats missing is the SSL-bumping part. HTTPS works differently to HTTP.
>
> The URL domain name and all the rest of the HTTP message is encrypted
> *in full*. There is simply no client HTTP message involved if you don't
> decrypt.
>
> As I mentioned above what you are seeing in the log is a Squid-generated
> CONNECT message. Its the HTTP representation of the intercepted TCP SYN
> packet and contains purely raw-IP:port details.
>
> TLS does have an SNI record which is sent by browsers un-encrypted that
> can be used as domain for some things. BUT, that requires "ssl_bump
> peek" action at minimum, and has no guarantee of actually being present.
>
> SNI is also still a new feature, and is not used for these fake CONNECT
> requests anyway (since they represent the TCP SYN). So you wont see any
> log difference in 3.5 even if you do let your Squid use it for ACLs.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/2caa9cdf/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 18 17:10:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 05:10:14 +1200
Subject: [squid-users] Squid reverse proxy in http > https mode
In-Reply-To: <55D34CB5.7060900@xvidservices.com>
References: <55D34CB5.7060900@xvidservices.com>
Message-ID: <55D366F6.4010503@treenet.co.nz>

On 19/08/2015 3:18 a.m., Veiko Kukk wrote:
> Hi
> 
> I'm trying to get most optimized solution for caching objects of cloud
> storage.
> 
> The data flow I'd like to achieve is: http client <http> squid reverse
> mode cache <https> remote https storage server
> 

Squid does support gatewaying HTTP to servers over TLS/SSL. That part is
just:
  cache_peer example.com parent 443 0 originserver ssl

That will effectively take http:// or https:// traffic and convert it to
solely https:// for the origin server.


https:// is forbidden to be sent around un-encrypted. So getting that
traffic to Squid un-encrypted is near impossible.


> Common scenarios/examples of accel mode http(s) port include opposite
> direction of traffic encryption/decryption, client connecting via https
> and backend servers (cache_peer) being http. In my case client connects
> via localhost and to reduce cpu load, I'd like to have this connection
> without encryption and start encryption only when data leaves local
> squid to parent cache_peer, because this remote server speaks https only.
> 
> 
> Is this possible with squid?
> 

That is up to the client software, but in almost all cases no. You have
to have an HTTPS listening port.



> Another related question is: when cache_peer hostname is dns name that
> resolves into multiple ip addresses, then how does squid act? Does it
> resolve it once and use first ip addres or all addresses in round robin,
> or does it resolve every time cache_peer is accessed?

It resolves AAAA and A once per DNS TTL as is proper to ensure it has an
up to date and accurate IP list.

IPv6 results are sorted for preference over IPv4 results. Unless
dns_v4_first is configured to reverse the preference order. In
accordance with BCP 177.

That cache_peer's IP list is appended to the full list of potential
destination IPs along with any other sources which may be used.

Then TCP connection setup is tried sequentially down the full list of
destinations. Whichever IP is listed first and responds is used.


> What happens if
> one of the addresses is not accessible?

It gets marked as bad/unavailable and the next one on the available list
is tried.


> I have checked documentation but
> did not find any explanation on that topic.

The DNS parts are simply how DNS is supposed to work. Nothing special
about Squid.

Amos


From squid3 at treenet.co.nz  Tue Aug 18 17:10:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 05:10:48 +1200
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439906807136-4672742.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com>
Message-ID: <55D36718.2060703@treenet.co.nz>

On 19/08/2015 2:06 a.m., adricustodio wrote:
> I was thinking... isnt it possible to make squid authenticate on a radius
> server and the radius goes into the oracle db ?

Depends on whether you are using the Captive Portal installation you
mentioned, a forward-proxy or interception proxy.

Of those options only the forward-proxy can do HTTP authentication.

I'm not familiar enough with RADIUS to know how (if at all) you coudl
get data between them.

> 
> Sorry if it sounds stupid, im new on DB. I usually only work on proxy with
> ncsa authentication.

Operationally there is no difference. NCSA is one of the flat-file
database formats I mentioned. Both even use crypt() function in the
background for secure password hashing.

The only visible difference between basic_db_auth and basic_ncsa_auth is
the parameters on the helper line in squid.conf.

Here is an example config for the DB auth helper:

auth_param basic program /usr/bin/perl /usr/lib/squid/basic_db_auth \
  --persist --md5 --salt "pepper" \
  --dsn DBI:Oracle:database=users:db.example.com:1234 \
  --user proxy --password blah --table accounts \
  --usercol UserName --passwdcol Passwd \
  --cond Status='ACTIVE'


As Yuri pointed the basic_ldap_auth helper is also usable if you prefer
LDAP query syntax to SQL.

Amos



From squid3 at treenet.co.nz  Tue Aug 18 17:38:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 05:38:05 +1200
Subject: [squid-users] Bridge/Tproxy: https dns
In-Reply-To: <CADLGAbK2PcSbSMehhXyN7sJtCWNKubPRkcwDKr+yeZBj=jNiDA@mail.gmail.com>
References: <CADLGAbK3wbE-5M_88tWJwYFcb_NdXUuv6cR9czUsNrLHanxfSg@mail.gmail.com>
 <55D2C2D9.5080301@treenet.co.nz>
 <CADLGAbK2PcSbSMehhXyN7sJtCWNKubPRkcwDKr+yeZBj=jNiDA@mail.gmail.com>
Message-ID: <55D36D7D.6040901@treenet.co.nz>

On 19/08/2015 4:54 a.m., Pedro Correia Sardinha wrote:
> Thank you Amos for the review.
> 
> Following your advice... trying to use the pure transparent proxy (Tproxy)
> but getting different behaviours with google domains in google chrome and
> after adding peek there aren't still no names in https addresses.

Like I said. peek allows you to do ACL stuff with a server name. But it
wont show up in the log because the log is showing  the TCP SYN
equivalent CONNECT request. *TCP* uses just IP:port details.

> 
> First create ssl certs directory, just to check later.
> /usr/lib64/squid/ssl_crtd -c -s /var/log/squid/lib/ssl_db
> 
> remove from squid.conf:
> #tcp_outgoing_address
> #always_direct allow ssl-bump_port
> #ssl_bump none all
> 
> add to squid.conf:
> ssl_bump splice localnet

"splice" is the same as just doing a tunnel. This should just do the
same thing as your earlier "none" configuration for all traffic arriving
from "localnet" machines. Which should be about 100% of your traffic yes?

So it does not even reach the peek...

> ssl_bump peek all
> ssl_bump splice all
> 
> I removed IPv6 restriction in kernel and in new squid compile without
> --disable-ipv6 option. And I added some similiar IPv6 rules to Tproxy boot
> script.
> 
> I continue to not generate-host-certificates(=off), I can check it in

I dont quite undertand that sentence. Missing words?

> ssl_db directory. But, just for testing, if I change
> generate-host-certificates to on then there are certificates changes with
> my self CA Autority notice in Google Chrome related to google sites, some
> other sites tested get the certificate in right way. In IE and Firefox
> there are no certificates issues.
> A workaround for this issue in Google Chrome with websites in google
> domains was create a specific acl for IP from google domains and splice
> first like did with localnet:
> acl google dst "/etc/squid/google.txt"

No. Use an ACL type of "ssl::server_name".

 acl google ssl::server_name "/etc/squid/google.txt"
 acl step1 at_step SslBump1

 ssl_bump peek step1
 ssl_bump splice google
 ssl_bump bump all

That will use the SNI details from the peek to match against the google
domain names the browser is requesting. You can splice based on that
before any decryption or cert generation happens.

Then bump everything else.

> ...
> ssl_bump splice google
> ...

Yes Google Chrome was created by Google (surprise). They embed their
legitimate CA certificate for their web servers directly into the
browser itself in a feature they call "cert pinning".

Whatever your Squid generates cannot match that certificate because you
dont have Googles private credential key.

> 
> After those changes and setup ssl-bump with peek for most of sites (e.g.
> facebook) but still no names in logs, just the IP in https navigation.
> 
> access.log:
> 1439907457.958     49 192.168.0.102 TCP_MISS/200 910 POST
> http://ocsp.digicert.com/ - ORIGINAL_DST/93.184.220.29
> application/ocsp-response
> 1439907466.798  59053 192.168.0.102 TCP_TUNNEL/200 3944 CONNECT
> 212.113.184.216:443 - ORIGINAL_DST/212.113.184.216 -
> 1439907472.813  58798 192.168.0.102 TCP_TUNNEL/200 8320 CONNECT
> 212.113.185.35:443 - ORIGINAL_DST/212.113.185.35 -
> 1439907472.817  59014 192.168.0.102 TCP_TUNNEL/200 5234 CONNECT
> 212.113.184.221:443 - ORIGINAL_DST/212.113.184.221 -
> 1439907490.935  65674 192.168.0.102 TCP_TUNNEL/200 4207 CONNECT
> 54.171.32.174:443 - ORIGINAL_DST/54.171.32.174 -
> 1439907527.998 115712 192.168.0.102 TCP_TUNNEL/200 5485 CONNECT
> 54.192.61.197:443 - ORIGINAL_DST/54.192.61.197 -
> 1439907545.804 122741 192.168.0.102 TCP_TUNNEL/200 5817 CONNECT
> 132.245.50.66:443 - ORIGINAL_DST/132.245.50.66 -
> 

Still TUNNEL'ing. Probably that first "splice" this time. Two ways to
reach the same result.


> 
> cache.log:
<snip>

> 2015/08/18 15:17:38.152 kid1| TcpAcceptor.cc(222) doAccept: New connection
> on FD 15
> 2015/08/18 15:17:38.152 kid1| TcpAcceptor.cc(297) acceptNext: connection on
> local=[::]:3130 remote=[::] FD 15 flags=25
> 2015/08/18 15:17:38.153 kid1| client_side.cc(3890)
> httpsSslBumpAccessCheckDone: sslBump not needed for local=31.13.90.6:443
> remote=192.168.0.102 FD 40 flags=17

"not needed" still because of splice implying TUNNEL.

Amos


From squid3 at treenet.co.nz  Tue Aug 18 17:51:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 05:51:28 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1439915941519-4672750.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
Message-ID: <55D370A0.4060809@treenet.co.nz>

On 19/08/2015 4:39 a.m., Stakres wrote:
> Hi All,
> 
> I'm facing a weird situation with a squid *3.5.7*, have a look:
> Store Directory #0 (aufs): /cachesdg/spool
> FS Block Size 4096 Bytes
> First level subdirectories: 16
> Second level subdirectories: 256
> Maximum Size: 67108864 KB
> Current Size: 66288408.00 KB
> *Percent Used: 98.78%*
> Filemap bits in use: 7858031 of 8388608 (94%)
> Filesystem Space in use: 63582908/575775620 KB (11%)
> Filesystem Inodes in use: 7428375/36569088 (20%)
> Flags: SELECTED
> Removal policy: lru
> LRU reference age: 1.06 days
> 
> In the squid.conf:
> ...
> cache_swap_low 75
> cache_swap_high *80*
> ...
> 
> Is that normal ?

Maybe. Depends on your traffic speed and cache_dir capacity. If its
arriving faster than the garbage colection can purge objects then it
could get very high.


> I should see the Squid cleaning objects to respect the 80% swap high, am I
> wrong ?

Its been a very long time since I looked at teh code in this area and
cant check it right now. But IIRC this is the intended behaviour:

* Below swap_high you should see only turnover from objects being updated.

* Above swap_high you should see hourly purges down to swap_low.

But there is also <http://bugs.squid-cache.org/show_bug.cgi?id=3553>
nobody has followed up on yet.

Amos


From adricustodio at uol.com.br  Tue Aug 18 17:57:29 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 10:57:29 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D36718.2060703@treenet.co.nz>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D36718.2060703@treenet.co.nz>
Message-ID: <1439920649963-4672757.post@n4.nabble.com>

Ok, last question....

Squid is able to authenticate on mysql right ?

If i create a mysql base and import all my oracle data there ? will that be
possible ?
Will squid authenticate there ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672757.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Tue Aug 18 18:53:39 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 18 Aug 2015 11:53:39 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55D370A0.4060809@treenet.co.nz>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz>
Message-ID: <1439924019637-4672758.post@n4.nabble.com>

Hi Amos,

New check, 30 sec ago, same server:
Store Directory #0 (aufs): /cachesdg/spool
FS Block Size 4096 Bytes
First level subdirectories: 16
Second level subdirectories: 256
Maximum Size: 67108864 KB
Current Size: 67045256.00 KB
*Percent Used: 99.91%*
Filemap bits in use: 7962720 of 8388608 (95%)
Filesystem Space in use: 65048176/575775620 KB (11%)
Filesystem Inodes in use: 7595323/36569088 (21%)
Flags: SELECTED
Removal policy: lru
LRU reference age: 1.08 days

squid.conf:
...
cache_dir aufs /cachesdg/spool 65536 16 256 min-size=0 max-size=16384
...

Traffic is 700Mbps, the server is a 8 real cores (no virtual here), 64GB
memory.
Load is 3, CPU is 40%

So, the cache will be full in the 30 min... then, what ?
Do I must be ready to see the squid crashing ?

Please, check the squid code asap, thanks in advance.

Bye fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672758.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Aug 18 20:09:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 19 Aug 2015 02:09:34 +0600
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439920649963-4672757.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D36718.2060703@treenet.co.nz>
 <1439920649963-4672757.post@n4.nabble.com>
Message-ID: <55D390FE.7080307@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


18.08.15 23:57, adricustodio ?????:
> Ok, last question....
>
> Squid is able to authenticate on mysql right ?
Yes.
>
>
> If i create a mysql base and import all my oracle data there ? will
that be
> possible ?
Yes. The only point is how you will synchronyze Oracle and MySQL data.
AFAIK you need to write data replication procedure yourself.
>
> Will squid authenticate there ?
Sure.
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672757.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV05D+AAoJENNXIZxhPexGCrIH/jKnJLRzECzG/b1XGAAwOFIu
d851cX4L28eUXp4FNF1PTakzcfVtm41Kc+YOt+nLMpJ/sK/Y/4uOoO/CjbxCHDaA
2eICSE3JyYs4IC7CwbAedGxoi/WmZHtJJJmqySoaj85/Y4ooxBhUBRFWoSkubykV
776s7aQ7tPCDtHxEjIYty1aJabK2+g8ygTDgUEfgNgcIoLuPUnshTJBecj7LQWZs
oFWsLLvlU8xKXlaXtUC2lbbv4BpZrX1MXzlmqQwgbUsSiQ271MTt9IbI+QYTHVWS
FmivVLBnYPWrrZR74grrjIFiwUSNZeUcHxbIA6IYcYO9YC0Au2do1VvCHd4q+KU=
=dgW3
-----END PGP SIGNATURE-----



From adricustodio at uol.com.br  Tue Aug 18 20:09:27 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Tue, 18 Aug 2015 13:09:27 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D390FE.7080307@gmail.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D36718.2060703@treenet.co.nz>
 <1439920649963-4672757.post@n4.nabble.com> <55D390FE.7080307@gmail.com>
Message-ID: <1439928567425-4672760.post@n4.nabble.com>

Oh well, thats not a problem, i think we will make once a week.
I can do that manually i think... or when people create a new user create on
mysql aswell....
Thanks for the helps guys




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672760.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From huaraz at moeller.plus.com  Tue Aug 18 22:02:33 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Tue, 18 Aug 2015 23:02:33 +0100
Subject: [squid-users] debian Jessie squid with auth
	(kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <vmime.55d2d089.2ba7.1a22bdbf5ed74699@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.55d1f873.5825.7867d92569e10c23@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.55d2d089.2ba7.1a22bdbf5ed74699@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <mr0a1t$rce$1@ger.gmane.org>

Hi Louis,

   When you have an offline PC do you use DHCP to give an IP ?   If so can you also provide the PC with a WINS server via DHCP ?  If that is possible and you run WINS you can authenticate the user with user at DOMAIN.COM when you get the authentication popup. The WINS server will point the PC to the AD server of the domain DOMAIN.COM ( I assume you have given out some AD guest accounts to the none domain PC )  

Regards
Markus


"L.P.H. van Belle" <belle at bazuin.nl> wrote in message news:vmime.55d2d089.2ba7.1a22bdbf5ed74699 at ms249-lin-003.rotterdam.bazuin.nl...
Nobody any hint where the NTLM auth is going wrong, or what i can do to fix this. 




------------------------------------------------------------------------------
  Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens L.P.H. van Belle
  Verzonden: maandag 17 augustus 2015 17:06
  Aan: squid-users at lists.squid-cache.org
  Onderwerp: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic) ERROR type NTLM type 3


  Hai all, 

  I have a Debian Jessie setup with squid 3.4 , all debian packages. 
  Im using samba 4 AD as domain controllers for my kerberos authentication. 

  I've a setup as followed here : 
  http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory 

  I have my kerberos auth working, so i dont type any password with a "domain joined computer"  when i want to internet. 
  I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 

  Now, i need to give users access to the internet, a non domain joined, windows PC. 

  Im getting :  ( with markus negotiate_wrapper 1.0.1  ) 
  2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
  2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....   =' from squid (length: 59). 
  2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
  2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
  2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......  AA= * 
  2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....  8=' from squid (length: 711).
  2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
  2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
  2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
  2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 



  I know the following : ( and correct me if im thinking wrong here.) 
  ## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
  ##    Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
  ##    NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
  ##    Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
  ## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

  But i recieve a type 3 NTLM token...  


  This are the configs have tested and these 2 work. 
  For kerberos auth 
  auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM    

  for basic auth 
  auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
      -b "dc=internal,dc=domain,dc=tld" \
      -D ldap-bind at internal.domain.tld -W /etc/squid3/private/ldap-bind \
      -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
      -h addc.internal.domain.tld  

  These dont work. 

  auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
      --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
      --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
  or 
  auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
      --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
      --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME

  tried here the supplied wrapper with squid.:     /usr/lib/squid3/negotiate_wrapper_auth  
  and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says  here
  http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory   ( Install negotiate_wrapper )  

  the kerberos part works but not the ntlm . 

  when i try with only: 

  ### pure ntlm authentication
  auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
  auth_param ntlm children 10
  auth_param ntlm keep_alive off

  im also unable to authenticat on the proxy. 

  all winbind test work..  

  I googled a lot, but i didnt find any solutions so im hoping someone here knows more. 

  so anyone any hint where to look, i cant figure this out. 


  Greetz, 

  Louis







--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150818/67bd81f7/attachment.htm>

From Jason_Haar at trimble.com  Wed Aug 19 02:20:04 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 19 Aug 2015 14:20:04 +1200
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
Message-ID: <55D3E7D4.4030508@trimble.com>

Hi there

I've had bump working before (testing), but went off to different things
for a while, but now I'm back and can't get it to work anymore. I've
upgraded to 3.5.7 (from some previous release - maybe 3.5.4?), so it may
be something that happened in there

I've stripped back my config in order to maximize getting bumping to
work, and is probably best described by:

root]# egrep -i 'crtd|bump|ssl:' squid.conf ssl-bump.inc|grep -v '#'
squid.conf:http_port 3128 ssl-bump cert=/etc/squid/squidCA.cert 
generate-host-certificates=on dynamic_cert_mem_cache_size=256MB options=ALL
squid.conf:https_port 3129 intercept ssl-bump
cert=/etc/squid/squidCA.cert  generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
squid.conf:include /etc/squid/ssl-bump.inc
squid.conf:logformat logdetailed %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm
%ru %[un %Sh/%<a %mt %ssl::>sni %ssl::>cert_subject
ssl-bump.inc:sslcrtd_program /usr/lib64/squid/ssl_crtd -s
/var/lib/squid/ssl_db -M 256MB
ssl-bump.inc:sslcrtd_children 32 startup=15 idle=5
ssl-bump.inc:ssl_bump peek all
ssl-bump.inc:ssl_bump bump all

I interpret that as peek at all traffic, then bump all. And that bumping
will involve create new certs signed by squidCA.cert and stored under
/var/lib/squid/ssl_db

However, on an empty system, "curl -vi -xlocalhost:3128
https://facebook.com/" shows a SSL session that *doesn't* involve
squidCA - and indeed there are no changes made under 
/var/lib/squid/ssl_db (yes the files/dirs exists and perms are correct).

ie no matter what https website I go to, they are all spliced -
exclusively "TCP_TUNNEL/200" in the logs

I cranked up debug_options and saw this


2015/08/19 14:13:16.493 kid1| bio.cc(1065) parseV3Hello: Found server
name: facebook.com
2015/08/19 14:13:16.493 kid1| bio.cc(1050) parseV3Hello: TLS Extension:
ff01 of size:1
2015/08/19 14:13:16.493 kid1| bio.cc(1050) parseV3Hello: TLS Extension:
d of size:16
2015/08/19 14:13:16.493 kid1| bio.cc(260) read: Hold flag is set, retry
latter. (Hold 11bytes)
2015/08/19 14:13:16.493 kid1| bio.cc(170) stateChanged: FD 24 now:
0x2002 23RCHA (SSLv2/v3 read client hello A)
2015/08/19 14:13:16.493 kid1| ModEpoll.cc(116) SetSelect: FD 24, type=1,
handler=1, client_data=0x3d9b8f8, timeout=0
2015/08/19 14:13:16.493 kid1| client_side.cc(4240)
clientPeekAndSpliceSSL: SSL_accept failed.

I recall hearing that some new code has been introduced that helps squid
"magically" figure out whether to even bother bumping some traffic
types? Is this related? It smells like squid has already decided to not
bump: based on it's own logic more than the config? (ie is my config
correct - but irrelevant)

This is squid-3.5.7 on Fedora-22

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From rousskov at measurement-factory.com  Wed Aug 19 04:07:20 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 18 Aug 2015 22:07:20 -0600
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D3E7D4.4030508@trimble.com>
References: <55D3E7D4.4030508@trimble.com>
Message-ID: <55D400F8.5050403@measurement-factory.com>

On 08/18/2015 08:20 PM, Jason Haar wrote:
> ssl_bump peek all
> ssl_bump bump all
> 
> I interpret that as peek at all traffic, then bump all.

Your interpretation is correct: Your configuration tells Squid to peek
at steps #1 and #2 and then try to bump at step #3. Unfortunately, the
last two actions (peeking at step #2 and then bumping at step #3) are
usually not compatible. Please see the Limitations section at
http://wiki.squid-cache.org/Features/SslPeekAndSplice

The same wiki page section suggests some workarounds.


HTH,

Alex.



From squid3 at treenet.co.nz  Wed Aug 19 06:06:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Aug 2015 18:06:21 +1200
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439928567425-4672760.post@n4.nabble.com>
References: <1439899097192-4672730.post@n4.nabble.com>
 <55D321E0.5070300@ngtech.co.il> <1439900404769-4672732.post@n4.nabble.com>
 <55D3312C.50701@treenet.co.nz> <55D336B1.2090508@gmail.com>
 <1439906807136-4672742.post@n4.nabble.com> <55D36718.2060703@treenet.co.nz>
 <1439920649963-4672757.post@n4.nabble.com> <55D390FE.7080307@gmail.com>
 <1439928567425-4672760.post@n4.nabble.com>
Message-ID: <55D41CDD.5020605@treenet.co.nz>

On 19/08/2015 8:09 a.m., adricustodio wrote:
> Oh well, thats not a problem, i think we will make once a week.
> I can do that manually i think... or when people create a new user create on
> mysql aswell....
> Thanks for the helps guys


I'm just wondering why you are so set on exporting the data to something
outside of the existing Oracle DB ?

We have identified that Squid related tools can integrate directly with
the main DB.

Amos



From belle at bazuin.nl  Wed Aug 19 08:09:53 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 19 Aug 2015 10:09:53 +0200
Subject: [squid-users] debian Jessie squid with auth
 (kerberos/ntlm/basic) ERROR type NTLM type 3
In-Reply-To: <mr0a1t$rce$1@ger.gmane.org>
References: <vmime.55d2d089.2ba7.1a22bdbf5ed74699@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.55d439d1.83f.7d06eeb752223061@ms249-lin-003.rotterdam.bazuin.nl>

Hello Markus, 
?
This a hard one , but ill explian a bit first, because this depends on the pc im testing with. 
?
I?have 2 networks within one ip range atm, and with the 2 networks i mean 2 samba (windows) domains. 
Im migrating the old to new and im testing in the new domain, but old 2 new is complete rebuild, setup clean. 
?
Old. samba 3 ldap, with dhcp. own dns servers and wins through dhcp. wins is assinged by dhcp here. 
new, samba 4 kerberos, the DCs are the DNS servers and static ips for the pc's. 
?
>? I assume you have given out some AD guest accounts to the none domain PC? 
no, this isnt done, is this needed? i was testing with a AD user. 
for example myself, i must be able to?auth on the proxy with any device, domain joined or not. 
?
What i will do, use the kerberos and ldap fall back first, this works. 
Migrate the netwerk first?and then redo my tests on my proxy server. 
setup the DHCP for the new AD servers, and take-ing notice of the wins setting your pointed me to. 
?
When i'll test, you say : user at DOMAIN.COM?for user. 
Do you mean, user at UPN? or user at REALM? just to be sure. 
?
If you can confirm that the use setup below is correct, thats a nice to know. 
then i can put?these auth files in the "working" backup-setup folder..? ;-) 
?
And thank you for your reply, very appriciated. 
?
Greetz, 
?
Louis
?
?
Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Markus Moeller
Verzonden: woensdag 19 augustus 2015 0:03
Aan: squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic) ERROR type NTLM type 3



Hi Louis,
?
?? When you have an offline PC do you use DHCP to give an IP ??? If so can you also provide the PC with a WINS server via DHCP ?? If that is possible and you run WINS you can authenticate the user with user at DOMAIN.COM when you get the authentication popup. The WINS server will point the PC to the AD server of the domain DOMAIN.COM ( I assume you have given out some AD guest accounts to the none domain PC )? 
?
Regards
Markus
?
?
"L.P.H. van Belle" <belle at bazuin.nl> wrote in message news:vmime.55d2d089.2ba7.1a22bdbf5ed74699 at ms249-lin-003.rotterdam.bazuin.nl...


Nobody any hint where the NTLM auth is going wrong, or what i can do to fix this. 
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens L.P.H. van Belle
Verzonden: maandag 17 augustus 2015 17:06
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] debian Jessie squid with auth (kerberos/ntlm/basic) ERROR type NTLM type 3



Hai all, 
?
I have a Debian Jessie setup with squid 3.4 , all debian packages. 
Im using samba 4 AD as domain controllers for my kerberos authentication. 
?
I've a setup as followed here : 
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?
?
I have my kerberos auth working, so i dont type any password with a "domain joined computer"? when i want to internet. 
I Have my Ldap auth working, for my "Non windows, non domain joined" Devices. 
?
Now, i need to give users access to the internet, a non domain joined, windows PC. 
?
Im getting :? ( with markus negotiate_wrapper 1.0.1? ) 
2015/08/17 16:31:51 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }
2015/08/17 16:32:03| negotiate_wrapper: Got 'YR TlR....?? =' from squid (length: 59). 
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR... =' (decoded length: 40).
2015/08/17 16:32:03| negotiate_wrapper: received type 1 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'TT TlR......? AA= * 
2015/08/17 16:32:03| negotiate_wrapper: Got 'KK TlR....? 8=' from squid (length: 711).
2015/08/17 16:32:03| negotiate_wrapper: Decode 'TlR.....8=' (decoded length: 530).
2015/08/17 16:32:03| negotiate_wrapper: received type 3 NTLM token
2015/08/17 16:32:03| negotiate_wrapper: Return 'BH NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL
2015/08/17 16:32:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: NT_STATUS_UNSUCCESSFUL * NT_STATUS_UNSUCCESSFUL; }} 
?
?
?
I know the following : ( and correct me if im thinking wrong here.) 
## 1) Pure Kerberos. Passthrough auth for windows users with windows DOMAIN JOINED pc's.
##??? Fallback to Ldap for NON WINDOWS NON DOMAIN JOINED Devices.
##??? NO NTLM. AKA, a windows pc, NOT JOINED in the domain, with end up in always user popup for auth.
##??? Which will always fail because of NTLM TYPE 1 and TYPE 2, authorisations.
## 2) NEGOTIATE AUTH, which will do all of above, but also authenticated Windows PC's Not domain Joined.

But i recieve a type 3 NTLM token...? 
?
?
This are the configs have tested and these 2 work. 
For kerberos auth 
auth_param negotiate program /usr/lib/squid3/negotiate_kerberos_auth -s HTTP/hostname.fqdn at REALM??? 
?
for basic auth 
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R \
??? -b "dc=internal,dc=domain,dc=tld" \
??? -D ldap-bind at internal.domain.tld -W /etc/squid3/private/ldap-bind \
??? -f (|(userPrincipalName=%s)(sAMAccountName=%s)) \
??? -h addc.internal.domain.tld? 

These dont work. 
?
auth_param negotiate program /usr/lib/squid3/negotiate_wrapper_auth -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
or 
auth_param negotiate program /usr/local/bin/negotiate_wrapper -d \
??? --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=BAZRTD \
??? --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME

tried here the supplied wrapper with squid.:???? /usr/lib/squid3/negotiate_wrapper_auth? 
and i have tried the negotiate_wrapper of Markus, as the wiki.squid-cache.org also says? here
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory?? ( Install negotiate_wrapper )? 
?
the kerberos part works but not the ntlm . 
?
when i try with only: 
?
### pure ntlm authentication
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=EXAMPLE
auth_param ntlm children 10
auth_param ntlm keep_alive off
?
im also unable to authenticat on the proxy. 
?
all winbind test work..? 
?
I googled a lot, but i didnt find any solutions so im hoping someone here knows more. 
?
so anyone any hint where to look, i cant figure this out. 
?
?
Greetz, 
?
Louis
?
?
?
?
?

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150819/623a87d2/attachment.htm>

From Jason_Haar at trimble.com  Wed Aug 19 10:09:19 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 19 Aug 2015 22:09:19 +1200
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D400F8.5050403@measurement-factory.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com>
Message-ID: <55D455CF.9030907@trimble.com>

On 19/08/15 16:07, Alex Rousskov wrote:
> Your interpretation is correct: Your configuration tells Squid to peek
> at steps #1 and #2 and then try to bump at step #3. Unfortunately, the
> last two actions (peeking at step #2 and then bumping at step #3) are
> usually not compatible. Please see the Limitations section at
> http://wiki.squid-cache.org/Features/SslPeekAndSplice


Ah! I used to use "external_acl_type" to run a script that would check
the SSL status of the host:port and that would allow squid to decide
whether to bump or splice. I'd turned it off for whatever reason - I
guess that's why it was working before. (in all of this I am speaking in
the context of transparent TLS - I realize for the formal proxy scenario
you typically have the SNI name/hostname via the CONNECT method). Sure
enough, once I deleted the peeks, it started bumping

So is there no way to get the SNI field from the client without breaking
the opportunity for bump? It's just that my testing has already shown
everyone using CloudFlare for HTTPS is now protected by their WAF
technology - which rejects SSL sessions that don't contain SNI. So if
you are wanting to (transparently) bump HTTPS, you can't use peek - but
you need peek in order to discover the SNI hostname, because if you
don't have that then  acls using ssl::server_name_regex and/or
"external_acl_type" will basically get rejected talking to vast numbers
of https servers in the world. This is a bit of a catch-22

My packet sniffer implies the SNI details is in the first TLS packet
sent from the client (ie pre-encryption). So couldn't squid just make a
note of that detail? Sort of a "pre-peek" I guess? I read about how
there are so many SSL extensions/etc that squid will always be running
afoul of issues if it tries to be too smart, but can't we look at this as

1. client->server (3-way TCP handshake)
2. client sends first TLS packet (contains extensions data - including
SNI). Gets to squid server
3. squid can extract that data, and make decisions immediately just on
that one packet. It can compare with acls and decide to bump or splice
4. if bump, squid forms the client->squid TLS channel and a separate
squid->server TLS channel
5. if splice, squid now forms a TCP channel to the server, then forwards
that first TLS packet, then "joins the two ends"
6. waits for next client packet


If all HTTPS transactions contained the hostname (because of CONNECT or
SNI), then squid could be set to default to bump, but to splice known
"un-bumpable" sites  - due to pinning or because they are not actually
HTTPS sites (eg Skype). It just seems like it's currently limited to
default splice, with bumping explicit things? (which I can't believe is
useful)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From adricustodio at uol.com.br  Wed Aug 19 10:23:50 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Wed, 19 Aug 2015 03:23:50 -0700 (PDT)
Subject: [squid-users] Squid + oracle
In-Reply-To: <55D41CDD.5020605@treenet.co.nz>
References: <55D321E0.5070300@ngtech.co.il>
 <1439900404769-4672732.post@n4.nabble.com> <55D3312C.50701@treenet.co.nz>
 <55D336B1.2090508@gmail.com> <1439906807136-4672742.post@n4.nabble.com>
 <55D36718.2060703@treenet.co.nz> <1439920649963-4672757.post@n4.nabble.com>
 <55D390FE.7080307@gmail.com> <1439928567425-4672760.post@n4.nabble.com>
 <55D41CDD.5020605@treenet.co.nz>
Message-ID: <1439979830892-4672767.post@n4.nabble.com>

Because its easyer i guess....

I have no idea how to start with those tools.
As long as i noticed you guys said i need the oracle OID, but that is paid,
and i dont think we have paid that.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672767.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Hongfei.Du at InterDigital.com  Wed Aug 19 11:45:28 2015
From: Hongfei.Du at InterDigital.com (Du, Hongfei)
Date: Wed, 19 Aug 2015 11:45:28 +0000
Subject: [squid-users] squid-users Digest, Vol 12, Issue 33
In-Reply-To: <mailman.3.1439899201.21888.squid-users@lists.squid-cache.org>
References: <mailman.3.1439899201.21888.squid-users@lists.squid-cache.org>
Message-ID: <E5924590149B4B46AA1B326C644B6064326195@NABESITE.InterDigital.com>

Hi Eliezer and Amos
Sure, I have posted this to the squid-dev list, which is more relevant to this issue.

Many thanks for Amos' comments, really helpful information. For clarification, here we look at the HTTP caching, and only look into the cache-dir selection algorithm rather than the peer selection algorithm at this stage, namely, we create three separate folders in /var/spool/cache1, /var/spool/cache2, /var/spool/cache3, and we intend to make squid intelligent enough to strictly put content(e.g. all elements from a single URL as defined from one of our subscriber user) into a specified folder, rather than following any build-in RR/LL rules which is based on the status(e.g., residual capacity) of the cache server itself. For the RR/LL source codes we are looking for, it is really for the cache-dir selection algorithm applied for local storage(the three folders as mentioned above.) Besides, refers to: " There is algorithm(s) applied in layers to decide which type of storage area is use, then which one within the selected type is most appropriate. Based on object availability, cacheability, size, storage area speed, object popularity, and temporal relationships to others." , can you elaborate more on where we can look into this algorithm(s)?

Best Regards,

IDCC

Humphrey(Hongfei)


Hongfei Du
Staff Engineer (UK Software)
InterDigital UK, Inc.
Shoreditch Business Center
64 Great Eastern Street
London,  EC2A 3QR
T: +44 207.749.9140
Hongfei.Du at InterDigital.com
www.InterDigital.com<http://www.interdigital.com>

[cid:image3d6f6e.BMP at 467ad0ef.48a6cd90]

This e-mail is intended only for the use of the individual or entity to which it is addressed, and may contain information that is privileged, confidential and/or otherwise protected from disclosure to anyone other than its intended recipient. Unintended transmission shall not constitute waiver of any privilege or confidentiality obligation. If you received this communication in error, please do not review, copy or distribute it, notify me immediately by email, and delete the original message and any attachments. Unless expressly stated in this e-mail, nothing in this message or any attachment should be construed as a digital or electronic signature.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of squid-users-request at lists.squid-cache.org
Sent: Tuesday, August 18, 2015 1:00 PM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 12, Issue 33

Send squid-users mailing list submissions to
       squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
       http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
       squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
       squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of squid-users digest..."


Today's Topics:

  1. Re: Question on developing customized Cache Selection
     algorithm from Round Robin, Least Load (Amos Jeffries)


----------------------------------------------------------------------

Message: 1
Date: Tue, 18 Aug 2015 22:24:45 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Question on developing customized Cache
       Selection algorithm from Round Robin, Least Load
Message-ID: <55D307ED.3030500 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 18/08/2015 5:42 a.m., Du, Hongfei wrote:
> Hello
> We are in an attempt to extend Squid Cache selection algorithm to make
a more sophisticated, let?s say to add WRR or WFQ, a few questions to start with:
>

Like Eliezer said this is really a question for squid-dev mailing list where the developers hang out.

WRR (weighted round-robin) is already implemented and exactly how Squid cache_dir currently operate. The weighting is based on storage area available size and I/O loading.

WFQ (weighted fair queueing) is a queueing algorthm as the 'Q' says.
Caching != queueing. In fact a cache is so different from a queue that WFQ would badly affect performance if it were used to decide what storage an object went into.
In essence, the problem is that we cannot dictate what objects will be requested by clients. They want what they ask for. Squids duty is 1) to answer reliably and 2) fast as possible regardless of objects location.


> - As we probably has to rewrite new algorithm and recompile it, so
does anyone know where(or which file) is the existing Round Robin or Least Load algorithm defined in source codes?

That depends on whether you mean the algorithm applied for local storage vs network sources, or the one(s) applied to individual caches for garbage collection.

>
> - Is there straight forward method to tell/instruct squid to store
content from network(e.g. an URL) in a predefined specific disk folder rather than using the selection algorithm itself?

Simply stated:
The URL and all other relevant details from the transaction are hashed to lookup an index to find the 32-bit 'sfileno' value which is a UID encoding the location of indexed objects in Squid local storage.

It _sounds_ simple enough, but those "other relevant details" is a massive complication. One single URL can potentially contain all possible objects that ever have or ever will exist on the Internet. Even considering storing things one file per URL dies a horrible death when it encounters popular modern websites.


Within Squid we refer to "the HTTP cache" as a single thing. But it is constructed of many storage areas. The individual cache_dirs and other places where HTTP objects might be found. Remote network sources are also accounted for.

There is algorithm(s) applied in layers to decide which type of storage area is use, then which one within the selected type is most appropriate. Based on object availability, cacheability, size, storage area speed, object popularity, and temporal relationships to others.
Then a sfileno is assigned if its local storage.

Then objects get moved between storage areas anyway based on need and popularity. And objects get removed from invdividual storage areas based on lack of popularity. Both of which affect future requests for them.

So the particulars of what you want to do matter, a lot.


FWIW, we have known outstanding needs for:

* updated cache_peer selection algorithms. Current Squid outgoing TCP connection failover works with a list of IPs that get tried until one succeeds. The old selection algorithms produce only a single IP rather than a preference-ordered set of peers to try.
- also none of the algorithms provide byte-base loading.

* ETag based cache index. For better performant If-Match/If-None-Match revalidation traffic.

* 206 partial object caching. Rock can store them, but no algorithms yet exist to properly manage the pieces of incomplete objects or aggregation from different transactions.

* per-area storage indexes, instead of a Big Global Index. Working towards 64-bit sfileno which are needed for some TB sized caches. Rock and Transients storage areas are done, but other caches still TODO.

* better HDD load detection. To inform the weighting of cache_dir seectio algorithms. This is a hardware driver related project.

* Support for ZFS and XFS dynamic inode sizing. This causes lots of issues with "wrong" disk storage under/over usage. Another hardware driver related project.


HTH
Amos


------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 12, Issue 33
*******************************************
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image3d6f6e.BMP
Type: image/bmp
Size: 13078 bytes
Desc: image3d6f6e.BMP
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150819/cf86889a/attachment.bin>

From adricustodio at uol.com.br  Wed Aug 19 12:17:38 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Wed, 19 Aug 2015 05:17:38 -0700 (PDT)
Subject: [squid-users] Squid + Mysql
Message-ID: <1439986658974-4672769.post@n4.nabble.com>

Hi guys, i got a question... again....

Im runing here Centos7 + squid 3.3.8
Im trying to set up a squid with mysql auth, but im kinda lost here...

For now squid is running fine on basic_ncsa_auth
I've created a mysql db called "squid" and a table called "users" with name,
pass (varchar) and status.

My squid did not come with mysql_auth, so i tried to create a new one and
tried to use basic_db_auth
mysql_auth is this

#!/usr/bin/php
<?
$link = mysqli_connect("localhost", "usuario_do_banco", "senha_do_banco");

if (!$link) {
   printf("Erro ao Conectar com o Banco de Dados: %s\n",
mysqli_connect_error());
   die();
}

$selectdb = mysqli_select_db($link, "squid");

if (!$selectdb) {
   printf("Erro ao Abrir o Banco de Dados: %s\n", mysqli_error($link));
   die();
}

while ( fscanf(STDIN, "%s %s", $nome, $senha) ) {
   $select = "SELECT nome, senha FROM usuarios WHERE nome = '".$nome."' AND
status = 1";
   $Query = mysqli_query($link, $select);
   $nrRegistros = mysqli_num_rows($Query);
   $erro = true;

   while ( $Registro = mysqli_fetch_array($Query) ) {
      $erro = false;

      if ( crypt($senha, $Registro[senha]) ==    $Registro[senha] )
printf("OK\n");
      else printf("ERR\n");
   }
      if ($erro) printf("ERR\n");
}
?> 

My squid asks for user and password but do not authenticate...
on the basic_db_auth i didnt change anything, i dont know how to configure
squid to authenticate on mysql.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Mysql-tp4672769.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jorgeley at gmail.com  Wed Aug 19 12:28:13 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 19 Aug 2015 09:28:13 -0300
Subject: [squid-users] Virtual Memory
Message-ID: <CAMeoTH=AZY9TP8dPqYkv5cNHBfWo0fAbooFKwJ=aE0+ERkGkVw@mail.gmail.com>

Hi guys, sorry if I'm doing a dumb question...
My squid is using so much virtual memory how you can see in the print, is
that normal?
I have 8GB physic memory and my squid is set to 4GB, my cache is like this:
cache_dir diskd /cache 4096 16 256 Q1=64 Q2=72
cache_dir diskd /cache 4096 16 256 Q1=64 Q2=72
cache_dir diskd /cache 4096 16 256 Q1=64 Q2=72
cache_dir diskd /cache 4096 16 256 Q1=64 Q2=72
cache_dir diskd /cache 4096 16 256 Q1=64 Q2=72

Thanks!!!

?

--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150819/88f86bf3/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.jpg
Type: image/jpeg
Size: 23132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150819/88f86bf3/attachment.jpg>

From eliezer at ngtech.co.il  Wed Aug 19 13:31:17 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 19 Aug 2015 16:31:17 +0300
Subject: [squid-users] Squid + oracle
In-Reply-To: <1439979830892-4672767.post@n4.nabble.com>
References: <55D321E0.5070300@ngtech.co.il>
 <1439900404769-4672732.post@n4.nabble.com> <55D3312C.50701@treenet.co.nz>
 <55D336B1.2090508@gmail.com> <1439906807136-4672742.post@n4.nabble.com>
 <55D36718.2060703@treenet.co.nz> <1439920649963-4672757.post@n4.nabble.com>
 <55D390FE.7080307@gmail.com> <1439928567425-4672760.post@n4.nabble.com>
 <55D41CDD.5020605@treenet.co.nz> <1439979830892-4672767.post@n4.nabble.com>
Message-ID: <55D48525.1010103@ngtech.co.il>

How is the OID related exactly?
If you have a paid DB I assume you can use it..
Leave aside licensing for the DB guy.. he probably knows whether you 
need to pay more for something.

Eliezer

On 19/08/2015 13:23, adricustodio wrote:
> Because its easyer i guess....
>
> I have no idea how to start with those tools.
> As long as i noticed you guys said i need the oracle OID, but that is paid,
> and i dont think we have paid that.
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-oracle-tp4672730p4672767.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Wed Aug 19 14:49:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2015 02:49:49 +1200
Subject: [squid-users] Squid + Mysql
In-Reply-To: <1439986658974-4672769.post@n4.nabble.com>
References: <1439986658974-4672769.post@n4.nabble.com>
Message-ID: <55D4978D.1090407@treenet.co.nz>

On 20/08/2015 12:17 a.m., adricustodio wrote:
> Hi guys, i got a question... again....
> 
> Im runing here Centos7 + squid 3.3.8
> Im trying to set up a squid with mysql auth, but im kinda lost here...
> 
> For now squid is running fine on basic_ncsa_auth
> I've created a mysql db called "squid" and a table called "users" with name,
> pass (varchar) and status.
> 
> My squid did not come with mysql_auth, so i tried to create a new one and
> tried to use basic_db_auth

basic_db_auth is what I use to authenticate my clients against a MySQL
database. Whats the problem?


> mysql_auth is this
> 
> #!/usr/bin/php

I seriously advise against PHP. Spent most of a year trying to get it
interoperating well as a helper language, but had to give up. It has a
history of bugs and weirdness with timeouts rooted in its primary
use-case as a web CGI interface that has not yet gone away AFAICT.


> <?
> $link = mysqli_connect("localhost", "usuario_do_banco", "senha_do_banco");
> 
> if (!$link) {
>    printf("Erro ao Conectar com o Banco de Dados: %s\n",
> mysqli_connect_error());
>    die();
> }
> 
> $selectdb = mysqli_select_db($link, "squid");
> 
> if (!$selectdb) {
>    printf("Erro ao Abrir o Banco de Dados: %s\n", mysqli_error($link));
>    die();
> }
> 
> while ( fscanf(STDIN, "%s %s", $nome, $senha) ) {
>    $select = "SELECT nome, senha FROM usuarios WHERE nome = '".$nome."' AND
> status = 1";
>    $Query = mysqli_query($link, $select);
>    $nrRegistros = mysqli_num_rows($Query);
>    $erro = true;
> 
>    while ( $Registro = mysqli_fetch_array($Query) ) {
>       $erro = false;
> 
>       if ( crypt($senha, $Registro[senha]) ==    $Registro[senha] )

Replace " [senha] " with " ['senha'] " (notice the single-quotes).

> printf("OK\n");
>       else printf("ERR\n");
>    }
>       if ($erro) printf("ERR\n");

On helper errors output "BH"


> }
> ?> 
> 
> My squid asks for user and password but do not authenticate...
> on the basic_db_auth i didnt change anything, i dont know how to configure
> squid to authenticate on mysql.

Same as the demo I gave for using it to authenticate directly to
OracleDB but with "mysql" instead of "Oracle" and 3306 instead of 1234
in the --dsn value.

With your described DB setup you also want --plaintext instead of --md5
and --salt.


Amos


From squid3 at treenet.co.nz  Wed Aug 19 14:58:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2015 02:58:25 +1200
Subject: [squid-users] Virtual Memory
In-Reply-To: <CAMeoTH=AZY9TP8dPqYkv5cNHBfWo0fAbooFKwJ=aE0+ERkGkVw@mail.gmail.com>
References: <CAMeoTH=AZY9TP8dPqYkv5cNHBfWo0fAbooFKwJ=aE0+ERkGkVw@mail.gmail.com>
Message-ID: <55D49991.7090003@treenet.co.nz>

On 20/08/2015 12:28 a.m., Jorgeley Junior wrote:
> Hi guys, sorry if I'm doing a dumb question...
> My squid is using so much virtual memory how you can see in the print, is
> that normal?

Yes.

Squid worker processes use a lot of memory. Then fork() many helper
processes. Each of which is allocated the same amount of virtual memory
that the worker process was using for its much larger needs.

The helper does not actually use that memory so it should not be a major
problem, or even accessed. But the system records it anyway.

Some OS do have problems with it. The dynamic helpers feature and
concurrency channels workaround it by reducing the number of needed
helpers and thus "virtual memory" used.

Amos



From rousskov at measurement-factory.com  Wed Aug 19 15:36:38 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 19 Aug 2015 09:36:38 -0600
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D455CF.9030907@trimble.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com> <55D455CF.9030907@trimble.com>
Message-ID: <55D4A286.4060603@measurement-factory.com>

On 08/19/2015 04:09 AM, Jason Haar wrote:

> So is there no way to get the SNI field from the client without breaking
> the opportunity for bump?

SNI is obtained during step #1. Peeking during step #1 does _not_
preclude future bumping.

If you want to get SNI and bump, then peek at step #1 and bump at the
next step (i.e., step #2):

  acl step1 at_step SslBump1
  ssl_bump peek step1
  ssl_bump bump !bank


Please note that doing so will give you no knowledge about the SSL
server point of view. All your decisions will be based on what the
client has told you. This is often not a problem because, in most cases,
if the client lied, the [bumped or spliced] connection to the SSL server
will not work anyway. However, if the client supplied no SNI
information, then your "bank" ACL (or equivalent) may not have enough
information to go on, especially for intercepted connections.

If you also peek at step #2, you will know the server certificate, but
you will no longer be able to bump the connection in most cases.


HTH,

Alex.



From rafinjer-squid at yahoo.fr  Wed Aug 19 15:43:19 2015
From: rafinjer-squid at yahoo.fr (Jeremie Rafin)
Date: Wed, 19 Aug 2015 15:43:19 +0000 (UTC)
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <55CD012B.20609@measurement-factory.com>
References: <55CD012B.20609@measurement-factory.com>
Message-ID: <830832878.7684678.1439998999365.JavaMail.yahoo@mail.yahoo.com>

Amos and Alex,

Thanks a lot for all your advises. I appreciate your helpful comments! :))

Nevertheless, all is not cristal clear for me. I have setup a sandbox (virtual box with squid 3.5.7 on debian 7.6; for transparent proxy, I have setup NAT iptables and IP route to run client through squid; my client browser is configured with squid certificate) and have tried many configurations. I am still a little bit lost...

Let me sum-up my needs, first. In a family context, I would like:
-a) to black/white list accesses (with e.g. squidguard);
-b) to check for content (with e.g. diladele or e2guardian);
-c) to do that for https (because more and more sites cipher links, like google);
-d) not to check for content for a given (spliced) sites (like banks);
-e) not to degrade security; for instance, revoked CA must not succeed in access, even if bumped;
-f) [nice to have]: to do this in a transparent proxy way; but explicit proxy would also be ok, if required.

Second, as per your advises, and some searches, I have setup this configuration (from the default one, unchanged; no third party, yet):

#

# Black list: meteofrance (http) and google (https)
acl blacklist dstdomain .meteofrance.com
acl sslblacklist ssl::server_name .google.fr
http_access deny blacklist
http_access deny sslblacklist

# Non bumped list (only spliced): wellsfargo
acl splicelist ssl::server_name .wellsfargo.com

# SSL configuration
acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1 all
ssl_bump splice step2 splicelist
ssl_bump bump all

# Web access
http_port 3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
sslcrtd_program /usr/local/squid-3.5/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB

#

With this config file, I am able to satisfy my requirements a, b (ready for), c, d and f (I mean a, b, c and d are ok in transparent and explicit modes). But e fails: https://revoked.grc.com/ is not rejected.

So, even if I think my configuration is much cleaner thanks to you (you will probably comment), I have still the same feeling of degrading the security when bumping. Hence, my main question remains (title of the thread): how to get (at least) revocation working (requirement "e")?



During my investigation, I have met some other difficulties (these points are less important than above main question, but are still obscur for me):

-"peek"/"splice"/"bump": nothing is logged in access.log (I tried a "debug_options ALL,9"; no success); I have read in some post that nothing is logged yet, but I double check: is it planed (or done) to get log for SSL decision?

-with this SSL configuration:
#
...

# SSL configuration
ssl_bump splice splicelist
ssl_bump bump all

...
#
 -all works but **only in explicit proxy**; in transparent, the HTTPS are failing (no certification???); if I add a "ssl_bump peek all" before the "bump" rule, all https accesses are peeked (poken?) or spliced; nothing is bumped (in explicit and transparent); is it normal?
 -furthermore, google is not blacklisted **only** in transparent mode! Why?
 -the wiki (http://wiki.squid-cache.org/Features/SslPeekAndSplice) does not mention this need of step1/step2 (like first configuration) for having splice/bump decision working well in transparent mode, does it?

-I noticed a quite similar unexpected behavior for "ssl_bump terminate" without "step1/step2"; for instance, a simple "ssl_bump terminate all" gives:
 -in transparent mode: no effet (no bump, no black list), all is spliced (or poken);
 -in explicit mode: google is rejected but every other https works (in a non bumped way);
Why?
Note that "terminate" behaves logically (i.e. as I would expect) with a preceeding "acl step1 at_step SslBump1/acl step2 at_step SslBump2/ssl_bump peek step1 all". With "all" or, for instance, with "sslblacklist".



Thanks for any answer/help!
J?r?mie





P.S.: my comments for your answers below.
>
>Technically a clean install of Squid with default options is more secure
>than any browser you will be able to find.
>
>Because it comes configured for forward-proxy. Which does not touch the
>TLS traffic in any way but relays CONNECT tunnels. Inside the tunnel the
>browser<->server security is in operating control, which makes the Squid
>relay equally secure as whatever the browser and server would agree to
>without Squid.
>
>Additionally, Squid enforces that HTTPS tunnels only go to port 443.
>Which is something the browsers do not do. Making Squid better in this
>one way on top of all the things the browsers do inside their tunnel.
>


For "splicing", OK; but I still have a doubt with bumping since I fail in revoking the test page.


>
>Secondly;
>
>the feeling of security you have with browser is a lie. Pure "security
>theatre", done so well that you and billions of others can't even see it.
>
>What you are doing is trusting a very large set of nearly a thousand CA
>entities. Including most of those governments with bad reputations now
>for surveillance, and a lot of corporations with agendas of their own.
>For all sorts of reasons which you have no knowledge or control over.
>Yes, someone has vetted that their published intentions were good to get
>them into the list, but it was not you. For you and everyone else it is
>almost blind trust.
>


This is my next goal: be able to manage the CA by myself in order to increase security for my clients surfing on internet. But this is another story: in a first time, I would like to reach same level of security as without squid. No more, no less. Even if I agree this is not perfect, this is a must, as I can not give better, so far.


>
>At any time *one*, just one, of them could sign a faked certificate.
>When that happens no user will be able to tell the difference without
>detailed digging down into the browser cert information.
>
>The only reason these things come to light at all is when the ability is
>abused in obvious user-visible ways for dictatorial censorship or
>malware attacks. Or the few vigilant an knowledgable people actively
>seeking it out catch it in the act. Its not the CA action that was seen
>first, but the abuse of power it allowed.
>
>Thankfully the repercussions of being wiped from the global CA list are
>severe enough to prevent power abuse in amost cases. But there have been
>some exceptions even so.
>
>So security threatre. Its been working so far, but only just.
>


I may have the same feeling than you; but maybe this discussion is out of scope (even if I would appreciate to discuss longer)? Again, before improving my (our) condition of simple browser people, I would like not to degrade it...


>You have also configured "sslproxy_cert_error deny all" which forces
>Squid to accept and ignore all possible origin server certificate
>errors. Including revocation.
>
>I hope you can see/understand the result.
>


According to Alex, I am not wrong. Anyway, with or without, with deny or allow, the revoked test page still fails, as soon as it is bumped...


>
>>
>> -do you know any implementation of NSS library (the security library
>of firefox, probably safer than openssl) for certificate checking helper
>(cf. sslcrtvalidator_program)?
>>
>
>No. Just the OpenSSL one we provide so far.
>
>I'm still working on getting library-agnostic TLS support rolled into
>Squid. But the main effort has been on the squid binary, not the helpers
>yet.
>


OK. As Alex says it, and I definitly agree with him, maintening such a tool is out of my capability and/or time availability. I have not been able to find any project on that topic, so I give this idea up.


>> # SSL Options - to mimic firefox; some of keys are weaks but some of my favorite websites need them :(
>> sslproxy_options NO_SSLv2,No_Compression
>> sslproxy_cipher ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-ECDSA-RC4-SHA:DHE-RSA-AES256-SHA:AES256-SHA:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA:AES128-SHA:DES-CBC3-SHA:RC4-SHA:RC4-MD5:!aNULL:!eNULL:!EXPORT:!DSS:!DES:!3DES:!PSK
>
>Careful. Squid will do what you tell it to.
>
>In order for this to be more secure than the browser, you will have to
>ensure that each of these things you are allowing actually are more
>secure than what it does. And that you are not allowing anything that
>the browser decides is bad.
>
>


Thanks for the advise. Improving this is another next step for me.


>> sslproxy_cert_error deny all
>>
>> # Splice access lists
>> acl splice_client src 192.168.2.30
>> acl splice_domain dstdomain .paypal.com
>> acl splice_dst dst 66.211.169.66 66.211.169.3
>>
>> # HTTPS access
>
>Nope, "TLS access" is better description.
>
>HTTPS is two protocol layers; a HTTP layer over a TLS layer (like
>"TCP/IP" is actually TCP over IP layer).
>
>ssl_bump directive controls only the TLS later actions. The http_access
>rules later deal with the decrypted HTTP layer - but only if it was
>allowed to be decrypted ("bump" action) by these rules.
>


Thanks for this reminder :)


>
>> ssl_bump splice splice_client
>
>Splice is equivalent to blind tunnelling, but can be done after Squid
>has played with the certififcates a bit (using read-only accesses).
>
>Since splice_client is based only on src-IP and nothing TLS layer
>related it is better to use "none" instead of splice action on the above
>rule. The true secure blind-tunnel will then be done.
>


This makes sens; thanks.


>
>> ssl_bump splice splice_domain
>
>This is a good example of how dstdomain fails. You are deciding whether
>to splice instead of interpret the HTTP message. Based on details inside
>that HTTP message which has not yet been interpreted.
>
>Make sure you are using the latest 3.5 release, and use the
>"ssl::server_name" insted of dstdomain in the ACL definition.
>


Your comment is really helpful! Maybe the wiki (http://wiki.squid-cache.org/Features/SslPeekAndSplice) should insist on this basis. I am not the only one doing this mistake...


>
>> ssl_bump splice splice_dst
>
>> ssl_bump server-first all
>
>DO NOT mix the old and new config styles together. server-first requires
>doing things like emitting a fake server cert to the client before
>reading soem of the client handshake details the splicing needs. But you
>have already spliced a bunch of transactions from the earlier rules.
>


Once again, maybe this basis should be explained in the wiki.
If I remember correclty, I was motivated by the effect in log (cache). I have not checked, but I remember that "server-first" gives more logs than "bump", hence I was more confident. Definitly, ssl decisions should be logged.


>>
>> # Hide PROXY
>> via off
>> forwarded_for delete
>>
>
>Does *not* hide the proxy.
>
>Hides the *client* by actively "shouting" the proxy details out to the
>world in protocol places where the client details would normally go.
>


This is another interesting topic -maybe not the right place here.
The goal is to get video streaming: some web sites refuse to provide video if a proxy is detected (because of broadcast laws); to debug I use these sites (streaming web site is very slow): 
 http://whatismyipaddress.com/proxy-check
 http://proxydetect.com/


The only configuration I have found to hide the proxy (and to get the video streaming) was the above one. If you have a better configuration, I would be curious to know it: all of the configurations I found on internet was failing (and I was not able to find something else that works).
Thanks :)


From adricustodio at uol.com.br  Wed Aug 19 15:46:58 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Wed, 19 Aug 2015 08:46:58 -0700 (PDT)
Subject: [squid-users] Squid + Mysql
In-Reply-To: <55D4978D.1090407@treenet.co.nz>
References: <1439986658974-4672769.post@n4.nabble.com>
 <55D4978D.1090407@treenet.co.nz>
Message-ID: <1439999218314-4672776.post@n4.nabble.com>

Amos, thanks for answering...

So here is where i am.

i configured my basic_db_auth and tested with 
/usr/lib64/squid/basic_db_auth --user root --password password --plaintext
-- persist

But my Centos does nothing, it stays blinking... so i need to crtl + c to
stop....

I think something is blocking the connection with mysql, but i dont know
what.
Already disabled selinux.

On cache.log it says 
2015/08/18 21:16:22 kid1| helperOpenServers: Starting 0/5 'basic_db_auth'
processes
2015/08/18 21:16:22 kid1| HTCP Disabled.

Any idea if Centos 7 have anything that could block mysql connection ? or
something like that ?
It dont even show "wrong password" , just blinking... literally




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Mysql-tp4672769p4672776.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From adricustodio at uol.com.br  Wed Aug 19 16:26:22 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Wed, 19 Aug 2015 09:26:22 -0700 (PDT)
Subject: [squid-users] Squid + Mysql
In-Reply-To: <1439999218314-4672776.post@n4.nabble.com>
References: <1439986658974-4672769.post@n4.nabble.com>
 <55D4978D.1090407@treenet.co.nz> <1439999218314-4672776.post@n4.nabble.com>
Message-ID: <1440001582864-4672777.post@n4.nabble.com>

thanks dude!

I fixed!
I was looking the logs and found that i needed a pearl_DBD_mysql 
Installed and all worked fine!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Mysql-tp4672769p4672777.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Aug 19 17:08:49 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 19 Aug 2015 20:08:49 +0300
Subject: [squid-users] Squid + Mysql
In-Reply-To: <1440001582864-4672777.post@n4.nabble.com>
References: <1439986658974-4672769.post@n4.nabble.com>
 <55D4978D.1090407@treenet.co.nz> <1439999218314-4672776.post@n4.nabble.com>
 <1440001582864-4672777.post@n4.nabble.com>
Message-ID: <55D4B821.8010502@ngtech.co.il>

Great to hear it works!

And since you are using CentOS I would just say, "take a look at the 
wiki" at:
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5

In a case you would want a squid 3.5 version.

Eliezer

On 19/08/2015 19:26, adricustodio wrote:
> thanks dude!
>
> I fixed!
> I was looking the logs and found that i needed a pearl_DBD_mysql
> Installed and all worked fine!



From squid3 at treenet.co.nz  Wed Aug 19 17:09:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2015 05:09:40 +1200
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <830832878.7684678.1439998999365.JavaMail.yahoo@mail.yahoo.com>
References: <55CD012B.20609@measurement-factory.com>
 <830832878.7684678.1439998999365.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55D4B854.3080506@treenet.co.nz>

On 20/08/2015 3:43 a.m., Jeremie Rafin wrote:
> Amos and Alex,
> 
> Thanks a lot for all your advises. I appreciate your helpful comments! :))
> 
> Nevertheless, all is not cristal clear for me. I have setup a sandbox (virtual box with squid 3.5.7 on debian 7.6; for transparent proxy, I have setup NAT iptables and IP route to run client through squid; my client browser is configured with squid certificate) and have tried many configurations. I am still a little bit lost...
> 
> Let me sum-up my needs, first. In a family context, I would like:
> -a) to black/white list accesses (with e.g. squidguard);

Use ufdbguard if you actually have to have one of these. SG is a dead
project.

Or just use an ACL blacklist/whitelist in squid.conf. Far simpler.


> -b) to check for content (with e.g. diladele or e2guardian);

Use an ICAP / eCAP service instead. It will greatly simplify your next
requirements implementation ...

> -c) to do that for https (because more and more sites cipher links, like google);
> -d) not to check for content for a given (spliced) sites (like banks);
> -e) not to degrade security; for instance, revoked CA must not succeed in access, even if bumped;
> -f) [nice to have]: to do this in a transparent proxy way; but explicit proxy would also be ok, if required.
> 
> Second, as per your advises, and some searches, I have setup this configuration (from the default one, unchanged; no third party, yet):
> 
> #
> 
> # Black list: meteofrance (http) and google (https)
> acl blacklist dstdomain .meteofrance.com
> acl sslblacklist ssl::server_name .google.fr
> http_access deny blacklist
> http_access deny sslblacklist


Is that "sslblacklist" test in http_access actually matching?

I didn't think we had ssl_server_name working outside the ssl_bump rules
yet. Good news if we do.


> 
> # Non bumped list (only spliced): wellsfargo
> acl splicelist ssl::server_name .wellsfargo.com
> 
> # SSL configuration
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1 all
> ssl_bump splice step2 splicelist
> ssl_bump bump all
> 
> # Web access
> http_port 3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
> http_port 3126 intercept
> https_port 3127 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
> sslcrtd_program /usr/local/squid-3.5/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
> 
> #
> 
> With this config file, I am able to satisfy my requirements a, b
(ready for), c, d and f (I mean a, b, c and d are ok in transparent and
explicit modes). But e fails: https://revoked.grc.com/ is not rejected.
> 
> So, even if I think my configuration is much cleaner thanks to you
(you will probably comment), I have still the same feeling of degrading
the security when bumping. Hence, my main question remains (title of the
thread): how to get (at least) revocation working (requirement "e")?
> 


I was about to suggest the CRL file loading option. But I see that
Squid-3 did not actually expose that option in squid.conf, even though
the code behind it was present. I guess that is another new config
feature coming in Squid-4 then, since its now possible.

For now in 3.5 there is still the cert validator helper. Should be
possible through that I think.


> 
> During my investigation, I have met some other difficulties (these
points are less important than above main question, but are still obscur
for me):
> 
> -"peek"/"splice"/"bump": nothing is logged in access.log (I tried a
"debug_options ALL,9"; no success); I have read in some post that
nothing is logged yet, but I double check: is it planed (or done) to get
log for SSL decision?

I hoping it will be eventually. But Christos appears to be concentrating
on getting the underlying security operations working well and finalized.

> 
> -with this SSL configuration:
> #
> ...
> 
> # SSL configuration
> ssl_bump splice splicelist
> ssl_bump bump all
> 
> ...
> #
>  -all works but **only in explicit proxy**; in transparent, the HTTPS are failing (no certification???); if I add a "ssl_bump peek all" before the "bump" rule, all https accesses are peeked (poken?) or spliced; nothing is bumped (in explicit and transparent); is it normal?
>  -furthermore, google is not blacklisted **only** in transparent mode! Why?
>  -the wiki (http://wiki.squid-cache.org/Features/SslPeekAndSplice) does not mention this need of step1/step2 (like first configuration) for having splice/bump decision working well in transparent mode, does it?
> 
> -I noticed a quite similar unexpected behavior for "ssl_bump terminate" without "step1/step2"; for instance, a simple "ssl_bump terminate all" gives:
>  -in transparent mode: no effet (no bump, no black list), all is spliced (or poken);
>  -in explicit mode: google is rejected but every other https works (in a non bumped way);
> Why?
> Note that "terminate" behaves logically (i.e. as I would expect) with a preceeding "acl step1 at_step SslBump1/acl step2 at_step SslBump2/ssl_bump peek step1 all". With "all" or, for instance, with "sslblacklist".
> 


The page is outdated, essentially unchanged from when the whole feature
was a wishlist plan and no actual code. So it does not reflect much of
what the latest code actually does when bugs and workaround hacks are
accounted for.

There is another patch coming down to 3.5 in the next few days which
will resolve a lot of the little oddities in the actions selected. It
should make the behaviour more accurately follow that documentation.



<snip, but remembering the feedback for later>
>>>
>>> # Hide PROXY
>>> via off
>>> forwarded_for delete
>>>
>>
>> Does *not* hide the proxy.
>>
>> Hides the *client* by actively "shouting" the proxy details out to the
>> world in protocol places where the client details would normally go.
>>
> 
> 
> This is another interesting topic -maybe not the right place here.
> The goal is to get video streaming: some web sites refuse to provide video if a proxy is detected (because of broadcast laws); to debug I use these sites (streaming web site is very slow): 
>  http://whatismyipaddress.com/proxy-check
>  http://proxydetect.com/
> 
> 
> The only configuration I have found to hide the proxy (and to get the video streaming) was the above one. If you have a better configuration, I would be curious to know it: all of the configurations I found on internet was failing (and I was not able to find something else that works).
> Thanks :)

 "forwarded_for transparent" (rather than delete. A tool can test for a
proxy deleting XFF).

Plus TPROXY. But that is another complex feature almost as tricky as
ssl_bump for some OS. So get one sorted out before mixing the other in.

Each time you bump a TLS certificate though, you will expose the proxy
via it too. Which can be tested for if the origin gets desperate to
prevent HTTP working properly (proxy support is a non-optional part of
the standards).

Amos


From johnpearson555 at gmail.com  Wed Aug 19 19:20:41 2015
From: johnpearson555 at gmail.com (John Pearson)
Date: Wed, 19 Aug 2015 12:20:41 -0700
Subject: [squid-users] Mac OS X Updates
Message-ID: <CAKNtY_yxPfFJhuP__sbWi_p6UNkyCuz=eFSkUQKeB_7=9wg+0w@mail.gmail.com>

Anyone have Mac OS X update caching working ? Without doing a SSL bump. I
think they are hosted through https (
https://support.apple.com/en-us/HT202943 )

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150819/3637dc3d/attachment.htm>

From rousskov at measurement-factory.com  Thu Aug 20 00:02:54 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 19 Aug 2015 18:02:54 -0600
Subject: [squid-users] How to have squid as safe as (e.g.) firefox?
In-Reply-To: <830832878.7684678.1439998999365.JavaMail.yahoo@mail.yahoo.com>
References: <55CD012B.20609@measurement-factory.com>
 <830832878.7684678.1439998999365.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55D5192E.3080608@measurement-factory.com>

On 08/19/2015 09:43 AM, Jeremie Rafin wrote:

> # Non bumped list (only spliced): wellsfargo
> acl splicelist ssl::server_name .wellsfargo.com
> 
> # SSL configuration
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1 all
> ssl_bump splice step2 splicelist
> ssl_bump bump all


> With this config file, https://revoked.grc.com/ is not rejected.

On my test machine, "openssl verify -crl_check ..." does not reject that
site's certificate either unless I manually download and set up the
corresponding CRL. You should not expect much more vigilance from a
stock Squid installation than you get from OpenSSL on the same box:
Squid uses OpenSSL for certificate validation.

FireFox does reject that URL with sec_error_revoked_certificate. This
means that FireFox CRL lists maintenance is "better" than that of stock
OpenSSL installation [on Ubuntu 14.04.3 LTS].

You might also find Squid's http_port crlfile option and the following
answer useful:
http://askubuntu.com/questions/448876/how-do-i-install-an-openssl-crl-file


HTH,

Alex.



From Jason_Haar at trimble.com  Thu Aug 20 00:42:17 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 20 Aug 2015 12:42:17 +1200
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D4A286.4060603@measurement-factory.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com> <55D455CF.9030907@trimble.com>
 <55D4A286.4060603@measurement-factory.com>
Message-ID: <55D52269.4090805@trimble.com>

On 20/08/15 03:36, Alex Rousskov wrote:
> SNI is obtained during step #1. Peeking during step #1 does _not_
> preclude future bumping.

thanks for persisting with me Alex - I got there in the end! :-)

That looks a lot better, my config is now

root# egrep -i 'crtd|bump|ssl:|checkIfHTTPS' squid.conf
ssl-bump.inc|grep -v '#'
squid.conf:http_port 3128 ssl-bump cert=/etc/squid/squidCA.cert 
generate-host-certificates=on dynamic_cert_mem_cache_size=256MB options=ALL
squid.conf:https_port 3129 intercept ssl-bump
cert=/etc/squid/squidCA.cert  generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
squid.conf:include /etc/squid/ssl-bump.inc
squid.conf:logformat logdetails %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm
%ru %[un %Sh/%<a %mt %ssl::>sni %ssl::>cert_subject
ssl-bump.inc:sslcrtd_program /usr/lib64/squid/ssl_crtd -s
/var/lib/squid/ssl_db -M 256MB
ssl-bump.inc:sslcrtd_children 32 startup=15 idle=5
ssl-bump.inc:acl DiscoverSNIHost at_step SslBump1
ssl-bump.inc:ssl_bump peek DiscoverSNIHost
ssl-bump.inc:acl NoSNIpresent ssl::server_name_regex ".*"
ssl-bump.inc:acl NoSSLIntercept ssl::server_name_regex -i
"/etc/squid/acl-NoSSLIntercept.txt"
ssl-bump.inc:external_acl_type checkIfHTTPS children-max=20
concurrency=20 negative_ttl=3600 ttl=3600 grace=90  %SRC %DST %PORT
%ssl::>sni /usr/local/bin/confirm_https.pl
ssl-bump.inc:acl is_ssl external checkIfHTTPS
####ssl-bump.inc:ssl_bump splice !NoSNIpresent
ssl-bump.inc:ssl_bump splice NoSSLIntercept
ssl-bump.inc:ssl_bump bump is_ssl

So now I can:

1.  ###dynamically whitelist/splice non-SNI traffic via it's existence
(commented because it didn't work - ended up splicing everything)
2.  statically whitelist/splice cert pinning apps via acl "NoSSLIntercept"
3.  dynamically whitelist/splice some classes of websites (eg banks) by
external process checkIfHTTPS
4.  bump the rest

Can't get that "###" one to work. How do I create an acl that will match
when there's any SNI - so that I can splice anything that hasn't got it?

The only remaining question I have is about SSL session resumption. If a
*bumped* session uses resumption - that's purely a squid issue  - so I
suspect that would always work? (including intercept mode?). And if it's
a spliced session, then all squid can do is allow it anyway (because in
my config, I want to splice anything that hasn't got SNI) - so that
would also work?


> Please note that doing so will give you no knowledge about the SSL
> server point of view. All your decisions will be based on what the
> client has told you. This is often not a problem because, in most cases,
> if the client lied, the [bumped or spliced] connection to the SSL server
> will not work anyway. However, if the client supplied no SNI
> information, then your "bank" ACL (or equivalent) may not have enough
> information to go on, especially for intercepted connections.

My only desire for doing TLS intercept is to introduce content filtering
(ie AV). So I  am quite happy throw away (ie splice) old SSL plus
non-HTTPS sessions - as the primary target I'm after is people in web
browsers downloading viruses from https://dropbox.com, etc (which aren't
old SSL: a hacker who deliberately brings up a SSLv2 system in order to
subvert my assumption is welcome to - try finding a web browser that
will talk to it :-). People who bash their way through multiple layers
of browser warning popups/etc in order to get infected are out of scope ;-)


Thanks again for your help Alex. Hopefully this conversation will be
useful for others. TLS intercept is a bit of a step up in complexity
over standard TCP ;-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From Jason_Haar at trimble.com  Thu Aug 20 07:16:51 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 20 Aug 2015 19:16:51 +1200
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D52269.4090805@trimble.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com> <55D455CF.9030907@trimble.com>
 <55D4A286.4060603@measurement-factory.com> <55D52269.4090805@trimble.com>
Message-ID: <55D57EE3.2080604@trimble.com>

On 20/08/15 12:42, Jason Haar wrote:
> So now I can:
>
> 1.  ###dynamically whitelist/splice non-SNI traffic via it's existence
> (commented because it didn't work - ended up splicing everything)
>
Figured that one out: ".*" is a file - .* is a regex :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From list-squid-users at jyborn.se  Thu Aug 20 07:59:18 2015
From: list-squid-users at jyborn.se (Peter)
Date: Thu, 20 Aug 2015 09:59:18 +0200
Subject: [squid-users] Has anyone a working config for windows update
	through squid?
Message-ID: <20150820075918.GQ18051@pol-server.leissner.se>

We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
Squid is the only way out, there is no transparency at all.
We have problems with windows update through squid.

I have looked at this:
https://www.mail-archive.com/squid-users at squid-cache.org/msg94710.html
and this:
http://wiki.squid-cache.org/SquidFaq/WindowsUpdate

But they are both more than a year old.

I have entered the config recommendations from the Faq page above.
But reload-into-ims seems to be removed, I get syntax error when
I try to add that option. Even though this page still lists
reload-into-ims as a valid option:
http://www.squid-cache.org/Versions/v3/3.5/cfgman/refresh_pattern.html

Anyway, I wonder if anyone has a working config for
windows update through squid?

Thanks!

Peter


From squid3 at treenet.co.nz  Thu Aug 20 08:32:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2015 20:32:13 +1200
Subject: [squid-users] Has anyone a working config for windows update
 through squid?
In-Reply-To: <20150820075918.GQ18051@pol-server.leissner.se>
References: <20150820075918.GQ18051@pol-server.leissner.se>
Message-ID: <55D5908D.7080802@treenet.co.nz>

On 20/08/2015 7:59 p.m., Peter wrote:
> We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
> Squid is the only way out, there is no transparency at all.
> We have problems with windows update through squid.

What "problems" ?

> 
> I have looked at this:
> https://www.mail-archive.com/squid-users at squid-cache.org/msg94710.html
> and this:
> http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> 
> But they are both more than a year old.

Nothing has really changed until Win10. And that only to use P2P
protocols not relevant to proxying.


> 
> I have entered the config recommendations from the Faq page above.
> But reload-into-ims seems to be removed, I get syntax error when
> I try to add that option. Even though this page still lists
> reload-into-ims as a valid option:
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/refresh_pattern.html

HTTP violations require Squid to be built with --enable-http-violations.


> 
> Anyway, I wonder if anyone has a working config for
> windows update through squid?

Question is "which" windows?

 XP and older have problems needing a specific command-line to be run on
the client. Thats mentioned in the wiki.

 Vista thru 8 just use HTTP and the proxy like any other updater system.
Nothing special there apart from occasional GB big service packs needing
sysadmin care.

 8.1 and 10 sometimes use HTTPS, but not in a way that would cause
proxying problems.

 10 uses P2P protocols or HTTPS. But still not in ways that would cause
proxy problems (beyond the normal HTTPS bandwidth increase).

Amos


From belle at bazuin.nl  Thu Aug 20 09:17:33 2015
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 20 Aug 2015 11:17:33 +0200
Subject: [squid-users] Has anyone a working config for windows update
 through squid?
In-Reply-To: <55D5908D.7080802@treenet.co.nz>
References: <20150820075918.GQ18051@pol-server.leissner.se>
Message-ID: <vmime.55d59b2d.3a4.449f703b3f81317d@ms249-lin-003.rotterdam.bazuin.nl>

Stock windows with wpad setup in dns, nothing needed to do on command line. 
Well, at least thats what im have-ing here. 

few xp, win 7 and 10. 
same config as on the squid wiki. 

and a download of windows 10 update: 
http://fg.v4.download.windowsupdate.com/d/msdownload/update/software/secu/2015/08/windows10.0-kb3087916-x64-express_3f5b7379e3f0255b2a45b75fed3ce294f778a306.cab username at DOMAIN HERE_DIRECT/88.221.254.160 application/octet-stream

setup:  squid 3.4.8 debian with kerberos auth. 

Greetz, 

Louis



From list-squid-users at jyborn.se  Thu Aug 20 09:46:13 2015
From: list-squid-users at jyborn.se (Peter)
Date: Thu, 20 Aug 2015 11:46:13 +0200
Subject: [squid-users] Has anyone a working config for windows update
 through squid?
In-Reply-To: <55D5908D.7080802@treenet.co.nz>
References: <20150820075918.GQ18051@pol-server.leissner.se>
 <55D5908D.7080802@treenet.co.nz>
Message-ID: <20150820094613.GH18051@pol-server.leissner.se>

On Thu, Aug 20, 2015 at 08:32:13PM +1200, Amos Jeffries wrote:
> On 20/08/2015 7:59 p.m., Peter wrote:
> > We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
> > Squid is the only way out, there is no transparency at all.
> > We have problems with windows update through squid.
> 
> What "problems" ?

Just general failures I'm afraid.
 
> > I have looked at this:
> > https://www.mail-archive.com/squid-users at squid-cache.org/msg94710.html
> > and this:
> > http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> > 
> > But they are both more than a year old.
> 
> Nothing has really changed until Win10. And that only to use P2P
> protocols not relevant to proxying.
> 
> 
> > 
> > I have entered the config recommendations from the Faq page above.
> > But reload-into-ims seems to be removed, I get syntax error when
> > I try to add that option. Even though this page still lists
> > reload-into-ims as a valid option:
> > http://www.squid-cache.org/Versions/v3/3.5/cfgman/refresh_pattern.html
> 
> HTTP violations require Squid to be built with --enable-http-violations.

Ok, I see. Should I add reload-into-ims or should I avoid it?
 
> > Anyway, I wonder if anyone has a working config for
> > windows update through squid?
> 
> Question is "which" windows?

Sorry, forgot to write that. Windows 7 and 8.

But I just found a possible cause for error that I didn't
see earlier. We had an old line
refresh_pattern -i xxx 0 0% 0
with update.microsoft.com among others in the xxx pattern.
I removed that update.microsoft.com and now it seems to work.
We will let it be like this for a week and check status later.

Thanks!

Peter
 
>  XP and older have problems needing a specific command-line to be run on
> the client. Thats mentioned in the wiki.
> 
>  Vista thru 8 just use HTTP and the proxy like any other updater system.
> Nothing special there apart from occasional GB big service packs needing
> sysadmin care.
> 
>  8.1 and 10 sometimes use HTTPS, but not in a way that would cause
> proxying problems.
> 
>  10 uses P2P protocols or HTTPS. But still not in ways that would cause
> proxy problems (beyond the normal HTTPS bandwidth increase).
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu Aug 20 11:24:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2015 23:24:56 +1200
Subject: [squid-users] Has anyone a working config for windows update
 through squid?
In-Reply-To: <20150820094613.GH18051@pol-server.leissner.se>
References: <20150820075918.GQ18051@pol-server.leissner.se>
 <55D5908D.7080802@treenet.co.nz>
 <20150820094613.GH18051@pol-server.leissner.se>
Message-ID: <55D5B908.7090006@treenet.co.nz>

On 20/08/2015 9:46 p.m., Peter wrote:
> On Thu, Aug 20, 2015 at 08:32:13PM +1200, Amos Jeffries wrote:
>> On 20/08/2015 7:59 p.m., Peter wrote:
>>> We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
>>> Squid is the only way out, there is no transparency at all.
>>> We have problems with windows update through squid.
>>
>> What "problems" ?
> 
> Just general failures I'm afraid.
>  
>>> I have looked at this:
>>> https://www.mail-archive.com/squid-users at squid-cache.org/msg94710.html
>>> and this:
>>> http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
>>>
>>> But they are both more than a year old.
>>
>> Nothing has really changed until Win10. And that only to use P2P
>> protocols not relevant to proxying.
>>
>>
>>>
>>> I have entered the config recommendations from the Faq page above.
>>> But reload-into-ims seems to be removed, I get syntax error when
>>> I try to add that option. Even though this page still lists
>>> reload-into-ims as a valid option:
>>> http://www.squid-cache.org/Versions/v3/3.5/cfgman/refresh_pattern.html
>>
>> HTTP violations require Squid to be built with --enable-http-violations.
> 
> Ok, I see. Should I add reload-into-ims or should I avoid it?

Up to you. That one makes it impossible for the end-user to force a
re-fetch of any truncated or broken-Vary-header content your Squid may
be providing.

The popular browsers dont send a reload request without a specific
3-button magic sequence by the user anyway. So its not as much needed
now as with IE5/6 which seemed to do one on every other fetch.


>  
>>> Anyway, I wonder if anyone has a working config for
>>> windows update through squid?
>>
>> Question is "which" windows?
> 
> Sorry, forgot to write that. Windows 7 and 8.
> 
> But I just found a possible cause for error that I didn't
> see earlier. We had an old line
> refresh_pattern -i xxx 0 0% 0
> with update.microsoft.com among others in the xxx pattern.
> I removed that update.microsoft.com and now it seems to work.

Strange. That rule says *not* to cache unless the server explicitly
gives a Expires or Cache-Control settings.

> We will let it be like this for a week and check status later.
> 
> Thanks!
> 

Welcome.

Amos



From james at ejbdigital.com.au  Thu Aug 20 11:58:37 2015
From: james at ejbdigital.com.au (James Harper)
Date: Thu, 20 Aug 2015 11:58:37 +0000
Subject: [squid-users] Has anyone a working config for windows
	update	through squid?
In-Reply-To: <20150820075918.GQ18051@pol-server.leissner.se>
References: <20150820075918.GQ18051@pol-server.leissner.se>
Message-ID: <SG2PR04MB0839B80E20EE47E48E737C30E8660@SG2PR04MB0839.apcprd04.prod.outlook.com>

> 
> We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
> Squid is the only way out, there is no transparency at all.
> We have problems with windows update through squid.
> 

Problems without doing anything with Squid, or problems trying to get Squid to actually cache windows updates?

At home I divert all my updates through apt-cacher, which seems to work well.

The things I had to do were:
. write the rewriter
. add some rules to divert the required requests into the rewriter (and then not divert apt-cacher requests)
. modify apt-cacher to cache cab, exe, and psf files in the right way

I seem to get an appropriate number of hits. apt-cacher handles the partial file requests without problems. The only problem I can see is I don't think apt-cacher knows how to clean up the windows update related files, so it will either be expiring files when it thinks they are old enough, expiring them instantly, or not expiring them all. Right now I have files dating back to 31/01/2015, so I suspect probably the latter (clean cache option is off)

The apt-cacher report says I'm getting a 85% hit rate (by GB - 63% by request).

James


From list-squid-users at jyborn.se  Thu Aug 20 13:05:15 2015
From: list-squid-users at jyborn.se (Peter)
Date: Thu, 20 Aug 2015 15:05:15 +0200
Subject: [squid-users] Has anyone a working config for windows update
 through squid?
In-Reply-To: <SG2PR04MB0839B80E20EE47E48E737C30E8660@SG2PR04MB0839.apcprd04.prod.outlook.com>
References: <20150820075918.GQ18051@pol-server.leissner.se>
 <SG2PR04MB0839B80E20EE47E48E737C30E8660@SG2PR04MB0839.apcprd04.prod.outlook.com>
Message-ID: <20150820130515.GR18051@pol-server.leissner.se>

On Thu, Aug 20, 2015 at 11:58:37AM +0000, James Harper wrote:
> > 
> > We run squid 3.5.6 in a proxy server with FreeBSD 9.3.
> > Squid is the only way out, there is no transparency at all.
> > We have problems with windows update through squid.
> > 
> 
> Problems without doing anything with Squid, or problems trying to get Squid to actually cache windows updates?

Our main problem is getting windows update working through
squid at all, without ending in "fail" on the internal PC:s.
Caching of windows update files would be an added bonus, but
it's not a critical requirement for us. It's just that the
squid server is the only way out for the internal clients,
so all their web traffic must pass through squid, caching
or not.

At the moment it seems that windows update is working for us.
We will check again in about a week and see if the failures
have disappeared.

Thanks!

Peter
 
> At home I divert all my updates through apt-cacher, which seems to work well.
> 
> The things I had to do were:
> . write the rewriter
> . add some rules to divert the required requests into the rewriter (and then not divert apt-cacher requests)
> . modify apt-cacher to cache cab, exe, and psf files in the right way
> 
> I seem to get an appropriate number of hits. apt-cacher handles the partial file requests without problems. The only problem I can see is I don't think apt-cacher knows how to clean up the windows update related files, so it will either be expiring files when it thinks they are old enough, expiring them instantly, or not expiring them all. Right now I have files dating back to 31/01/2015, so I suspect probably the latter (clean cache option is off)
> 
> The apt-cacher report says I'm getting a 85% hit rate (by GB - 63% by request).
> 
> James
> 


From vdoctor at neuf.fr  Thu Aug 20 14:26:26 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 20 Aug 2015 07:26:26 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1439924019637-4672758.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
Message-ID: <1440080786735-4672791.post@n4.nabble.com>

Hi Amos,

Any update ?

This morning, it was crazy:
*Percent used: 100.76%*
How is it possible ?

Then the squid has crashed and it's now cleaning objects.
I can understand new objects could be added faster than the squid is able to
clean older objects, but it seems there is something wrong in the cleaning
process.
The wiki says "/As swap utilization gets close to high-water mark object
eviction becomes more aggressive./". From my point of view the "aggressive"
is not aggressive enough... 

Possible to have a patch/workaround soon ?
thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672791.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Thu Aug 20 14:38:42 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 20 Aug 2015 07:38:42 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
Message-ID: <1440081521999-4672792.post@n4.nabble.com>

Hi All,

Maybe someone gets the info already...
A refresh_pattern with 1 week maxi, if the same object is "visited" (coming
from the squid cache) every day, will the object be deleted 1 week after the
first cache action or will the squid add +1 week each time the object is
used from the cache ?

My issue is if we cache a big object (windowsupdate, chrome, etc...) for 1
week or 6 months, do we have to download it again once the initial time is
over ?
Or can we expect the same big object will be available from the cache for a
very long time if it's visited at least 1 time before the end of the time
limit...

Windowsupdate 10 is about 2.6GB objects, if the max time is 1 month, I don't
want to re-download such size monthly, if it's used daily...

See what I mean ? 

Bye fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Thu Aug 20 14:56:41 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 20 Aug 2015 07:56:41 -0700 (PDT)
Subject: [squid-users] refresh_pattern by type mime
Message-ID: <1440082601318-4672793.post@n4.nabble.com>

Hi All,

There is an existing case in the bugzilla
(http://bugs.squid-cache.org/show_bug.cgi?id=1913) speaking about this
request and it seems a good idea:
refresh_pattern by type mime

It would be very nice and cool to have this feature in squid to define
different min/max time per mime.
We could define script/html/css/etc... with a short time,
images/videos/audio/application/etc... with a long time...

Squid team, what is your opinion about that ?
Maybe already in the roadmap for the next 3.5.x build or the 4.x ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-by-type-mime-tp4672793.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Aug 20 22:10:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2015 10:10:42 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1440081521999-4672792.post@n4.nabble.com>
References: <1440081521999-4672792.post@n4.nabble.com>
Message-ID: <55D65062.9090206@treenet.co.nz>

On 21/08/2015 2:38 a.m., Stakres wrote:
> Hi All,
> 
> Maybe someone gets the info already...
> A refresh_pattern with 1 week maxi, if the same object is "visited" (coming
> from the squid cache) every day, will the object be deleted 1 week after the
> first cache action or will the squid add +1 week each time the object is
> used from the cache ?
> 
> My issue is if we cache a big object (windowsupdate, chrome, etc...) for 1
> week or 6 months, do we have to download it again once the initial time is
> over ?
> Or can we expect the same big object will be available from the cache for a
> very long time if it's visited at least 1 time before the end of the time
> limit...
> 
> Windowsupdate 10 is about 2.6GB objects, if the max time is 1 month, I don't
> want to re-download such size monthly, if it's used daily...
> 
> See what I mean ? 

It should revalidate after expiry. Only getting a full copy of the
server insists on sending one in response to the revalidate.

Older Squid used to keep revalidating per-request, I'm not sure if that
got fixed yet. But that is still better than another huge download in
this case.

Worst case you can download a local copy and redirect to it. Like a
self-made WSUS.

Amos



From spider at smoothnet.org  Fri Aug 21 01:14:05 2015
From: spider at smoothnet.org (Nicolaas Hyatt)
Date: Thu, 20 Aug 2015 20:14:05 -0500
Subject: [squid-users] ETA for Bug 3775
Message-ID: <001901d0dbae$acc0e6e0$0642b4a0$@smoothnet.org>

Hey guys,

I have been paying close attention to the list for a while and am just
beginning to realize the scale of things the squid team has in front of
them. So please understand that I'm _NOT_ begging here. I understand other
priority issues take precedence, and my little issue is way on down the
line. I was wondering if there was some sort of schedule as to when this may
be examined in case I need to provide any more dumps.

 

http://bugs.squid-cache.org/show_bug.cgi?id=3775

 

Thanks in advance,

The_Spider

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150820/389ddcf9/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 21 04:04:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2015 16:04:59 +1200
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <001901d0dbae$acc0e6e0$0642b4a0$@smoothnet.org>
References: <001901d0dbae$acc0e6e0$0642b4a0$@smoothnet.org>
Message-ID: <55D6A36B.5050808@treenet.co.nz>

On 21/08/2015 1:14 p.m., Nicolaas Hyatt wrote:
> Hey guys,
> 
> I have been paying close attention to the list for a while and am just
> beginning to realize the scale of things the squid team has in front of
> them. So please understand that I'm _NOT_ begging here. I understand other
> priority issues take precedence, and my little issue is way on down the
> line. I was wondering if there was some sort of schedule as to when this may
> be examined in case I need to provide any more dumps.
> 
>  
> 
> http://bugs.squid-cache.org/show_bug.cgi?id=3775
> 

Unknown again I'm afraid. That latest feedback was a painfully
disappointing result from a very hopeful looking approach.

If you have a dump that provides any new backtrace sequence to what we
have already they may be useful.

So far it all still seems to link back to a DeferredReader pausing a
transaction read, then something else playing around with its I/O
socket/FD/connection, then defer resumes and boom. But there is
potentially some minutes of delay between those actions.

PS. you may want to help with the testing of todays snapshot when its
ready, it contains some patches to HTTPS and FTP connection handling
that may have an impact on this bug (or not, grasping at straws here).

Amos



From spider at smoothnet.org  Fri Aug 21 06:48:35 2015
From: spider at smoothnet.org (The_Spider)
Date: Fri, 21 Aug 2015 01:48:35 -0500
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <55D6A36B.5050808@treenet.co.nz>
References: <001901d0dbae$acc0e6e0$0642b4a0$@smoothnet.org>
 <55D6A36B.5050808@treenet.co.nz>
Message-ID: <CAJfE2f6btE82qNoCO2tXHCZ_2A2fCNDhZP2j=-jNku3eSYunWA@mail.gmail.com>

I'll compile it and report as soon as I get a crash, luckily the
changes will have resolved the issue!
Thanks for all the hard work!

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Amos Jeffries
Sent: Thursday, August 20, 2015 11:05 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ETA for Bug 3775

On 21/08/2015 1:14 p.m., Nicolaas Hyatt wrote:
> Hey guys,
>
> I have been paying close attention to the list for a while and am just
> beginning to realize the scale of things the squid team has in front
> of them. So please understand that I'm _NOT_ begging here. I
> understand other priority issues take precedence, and my little issue
> is way on down the line. I was wondering if there was some sort of
> schedule as to when this may be examined in case I need to provide any
> more dumps.
>
>
>
> http://bugs.squid-cache.org/show_bug.cgi?id=3775
>

Unknown again I'm afraid. That latest feedback was a painfully disappointing
result from a very hopeful looking approach.

If you have a dump that provides any new backtrace sequence to what we have
already they may be useful.

So far it all still seems to link back to a DeferredReader pausing a
transaction read, then something else playing around with its I/O
socket/FD/connection, then defer resumes and boom. But there is potentially
some minutes of delay between those actions.

PS. you may want to help with the testing of todays snapshot when its ready,
it contains some patches to HTTPS and FTP connection handling that may have
an impact on this bug (or not, grasping at straws here).

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Aug 21 07:20:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2015 19:20:34 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440080786735-4672791.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com>
Message-ID: <55D6D142.4030803@treenet.co.nz>

On 21/08/2015 2:26 a.m., Stakres wrote:
> Hi Amos,
> 
> Any update ?
> 

Spent the afternoon looking into it. There is now a patch in the bug
report with some experimental changes.

> This morning, it was crazy:
> *Percent used: 100.76%*
> How is it possible ?

It seems all that documentation about the watermarks I was reading was
b*sh*t. :-(

The actual code is purging no more than 80 objects per second. And like
the bug said, only gets that many if its actually over 100% filled already.

> 
> Then the squid has crashed and it's now cleaning objects.
> I can understand new objects could be added faster than the squid is able to
> clean older objects, but it seems there is something wrong in the cleaning
> process.
> The wiki says "/As swap utilization gets close to high-water mark object
> eviction becomes more aggressive./". From my point of view the "aggressive"
> is not aggressive enough... 
> 
> Possible to have a patch/workaround soon ?

Yeah 80 remove/sec is *way* to small for a proxy that can and probably
is handling ~2K add/sec.


In that experimental patch I've done a few things that you can help test
out:

* tripled the basic removal rate to 200/sec

* set the rate of purging to reach maximum speed at high watermark (as
previously documented).

- between those two adjustments is it fast enough? all it has to do is
counter the completely new additions rate. general traffic req/sec doing
replace on content does not matter.


* removed the limit on purges when cache is >100% full, it just keeps
going until under-100% is reached or it cant go further. Squid service
is effectively paused until the cache is back within its size limit.
That may not be the best option, but its doable without a lot of change
all over the store code. Better tuning on the base removal rate should
avoid it happening in the first place.

- is that pause acceptibly small?


- do all these changes actually help in your situation any?

BTW, "debug_options 47,3" to get the purging event debug traces.

Amos



From squid3 at treenet.co.nz  Fri Aug 21 07:28:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2015 19:28:38 +1200
Subject: [squid-users] ssl_bump updates coming in 3.5.8
Message-ID: <55D6D326.4070305@treenet.co.nz>

Hi all,

 Christos has managed (we think) to resolve a fairly major design issue
that has been plaguing the 3.5 series peek-and-splice feature so far.
 (<http://wiki.squid-cache.org/Features/SslPeekAndSplice>)

The problem was that Squid was not actually following the intended and
documented logic of skipping the impossible bumping actions. The patch
for that will be in 3.5 snaphots labelled r13895 or later (still waiting
on mirror updates as I write this 1-2hrs more maybe).
(<http://www.squid-cache.org/Versions/v3/3.5/>)


Since it is affecting the visible behaviour of squid.conf settings I
would like some volunteers to help test it out. Find what problems
remain, and let me know what to alert others to in the next formal release.


We need testing both from those having issues currently, and those who
managed to get a trial-and-error config going with older 3.5.

Hopefully, if you are using the at_step workarounds there should not be
any visible difference. But some of the at_step tests may be needless now.

Thank you in advance for any assistance.

Amos


From vdoctor at neuf.fr  Fri Aug 21 08:11:09 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 21 Aug 2015 01:11:09 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55D6D142.4030803@treenet.co.nz>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
Message-ID: <1440144669633-4672800.post@n4.nabble.com>

Hi Amos,
Thanks for the explaination.
We'll try to apply the patch and will test with the customer, keep you
posted asap... 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672800.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Fri Aug 21 08:36:43 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 21 Aug 2015 01:36:43 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55D65062.9090206@treenet.co.nz>
References: <1440081521999-4672792.post@n4.nabble.com>
 <55D65062.9090206@treenet.co.nz>
Message-ID: <1440146203116-4672802.post@n4.nabble.com>

Hi Amos,
Is that possible to have a dedicated option with the Squid to keep objects
in the cache if they're regulary used even if the time is expired ?
Cleaning small expired files (<16kb) is not a problem but we must keep big
files into the cache if often used.
There are many "small" ISPs with 2, 4 or 8Mbps bandwidth and big files are a
problem if they've to download a fresh copy every month (if max-day 1
month).
Doing a local copy is not a right way here because too many possible big
objects and issues to manage them, we should have Squid able to do that.

I'm thinking something like that:
save_big_file on/off
save_big_file_min_size 128 MB
save_big_file_max_time 1 years

Would it be something you could implement with Squid ?
I'm sure it would work so fine 

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4672802.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Fri Aug 21 08:46:42 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 21 Aug 2015 01:46:42 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440144669633-4672800.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
Message-ID: <1440146802401-4672803.post@n4.nabble.com>

Amos,

in the meantime, I was thinking another point:
We know there are at least 2 limit with cache_dirs, the max size and the 16+
million entries (Filemap bits).
the cache_swap_high should take care both.
example: if the used cache is 95% of the space or if the Filemap bits is 95%
of the 16+ million entries.

Because if the used cache is 100%, the squid could crash a few later, but if
the squid is 16777216 entries it does crash immetiatly and will be looping
in crashes.

See what I mean ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672803.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Aug 21 09:14:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2015 21:14:07 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1440146203116-4672802.post@n4.nabble.com>
References: <1440081521999-4672792.post@n4.nabble.com>
 <55D65062.9090206@treenet.co.nz> <1440146203116-4672802.post@n4.nabble.com>
Message-ID: <55D6EBDF.90201@treenet.co.nz>

On 21/08/2015 8:36 p.m., Stakres wrote:
> Hi Amos,
> Is that possible to have a dedicated option with the Squid to keep objects
> in the cache if they're regulary used even if the time is expired ?
> Cleaning small expired files (<16kb) is not a problem but we must keep big
> files into the cache if often used.
> There are many "small" ISPs with 2, 4 or 8Mbps bandwidth and big files are a
> problem if they've to download a fresh copy every month (if max-day 1
> month).
> Doing a local copy is not a right way here because too many possible big
> objects and issues to manage them, we should have Squid able to do that.
> 
> I'm thinking something like that:
> save_big_file on/off
> save_big_file_min_size 128 MB
> save_big_file_max_time 1 years
> 
> Would it be something you could implement with Squid ?
> I'm sure it would work so fine 


Stale objects *do* stay in cache. I think that behaviour is pretty much
what happens with LRU in practice. To be removed it has to be *the*
least recently used object in cache when that watermark algorithm we are
fixing in the other thread runs.

Its just that if you are talking an object used literally once a month
and the smaller objects have a high churn rate resulting in a 1-week LRU
age. Then it would be stored for only that week before it met the
criteria for removal. If its touched at any time in that week its
LRU-age becomes 0 again and starts re-counting back towards a week while
the churn goes on with other objects.

The current policy/algorithm options are here:
 <http://www.squid-cache.org/Doc/config/cache_replacement_policy/>

new ones are possible of course with the usual wait/sponsor/DIY options
and disclaimers about feature requests.


Note that this is all per-cache_dir. What I recommend is a cache_dir
with some TB of HDD behind it for approx. GB sized objects.

That way they are *not* competeing with many small objects at high churn
rate. The cache_dir LRU age could be weeks/months/years if all the big
objects are reasonably low popularity.

Also, you can size the cache_dir capacities to avoid wastage in the
smaller cache_dir from the 2^25 object limitations vs the size of
objects stored there.

Amos



From vdoctor at neuf.fr  Fri Aug 21 09:40:55 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 21 Aug 2015 02:40:55 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55D6EBDF.90201@treenet.co.nz>
References: <1440081521999-4672792.post@n4.nabble.com>
 <55D65062.9090206@treenet.co.nz> <1440146203116-4672802.post@n4.nabble.com>
 <55D6EBDF.90201@treenet.co.nz>
Message-ID: <1440150055225-4672805.post@n4.nabble.com>

Amos,

We do use "cache_replacement_policy heap LFUDA", so it should do the job as
you explain, right ?
If i understand you correctly, we should also use something like that
"max_stale 1 year", correct ?

Thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4672805.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Fri Aug 21 10:06:30 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 21 Aug 2015 03:06:30 -0700 (PDT)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1440150055225-4672805.post@n4.nabble.com>
References: <1440081521999-4672792.post@n4.nabble.com>
 <55D65062.9090206@treenet.co.nz> <1440146203116-4672802.post@n4.nabble.com>
 <55D6EBDF.90201@treenet.co.nz> <1440150055225-4672805.post@n4.nabble.com>
Message-ID: <1440151590944-4672806.post@n4.nabble.com>

Amos,

With this type of config, we'll keep in cache all stale and popular objects,
I think we need special options:
save_big_file on/off
save_big_file_min_size 128 MB
save_big_file_max_time 1 years

It'll be more clear and precise, can we count of these options soon ?

Bye fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/refresh-pattern-and-same-objects-tp4672792p4672806.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcus.kool at urlfilterdb.com  Fri Aug 21 11:04:38 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 21 Aug 2015 08:04:38 -0300
Subject: [squid-users] ssl_bump updates coming in 3.5.8
In-Reply-To: <55D6D326.4070305@treenet.co.nz>
References: <55D6D326.4070305@treenet.co.nz>
Message-ID: <55D705C6.1040506@urlfilterdb.com>

I do not want to spoil things, but did you already read my latest addition to bug 4303 ?

Marcus


On 08/21/2015 04:28 AM, Amos Jeffries wrote:
> Hi all,
>
>   Christos has managed (we think) to resolve a fairly major design issue
> that has been plaguing the 3.5 series peek-and-splice feature so far.
>   (<http://wiki.squid-cache.org/Features/SslPeekAndSplice>)
>
> The problem was that Squid was not actually following the intended and
> documented logic of skipping the impossible bumping actions. The patch
> for that will be in 3.5 snaphots labelled r13895 or later (still waiting
> on mirror updates as I write this 1-2hrs more maybe).
> (<http://www.squid-cache.org/Versions/v3/3.5/>)
>
>
> Since it is affecting the visible behaviour of squid.conf settings I
> would like some volunteers to help test it out. Find what problems
> remain, and let me know what to alert others to in the next formal release.
>
>
> We need testing both from those having issues currently, and those who
> managed to get a trial-and-error config going with older 3.5.
>
> Hopefully, if you are using the at_step workarounds there should not be
> any visible difference. But some of the at_step tests may be needless now.
>
> Thank you in advance for any assistance.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From jlay at slave-tothe-box.net  Fri Aug 21 11:26:31 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 21 Aug 2015 05:26:31 -0600
Subject: [squid-users] ssl_bump updates coming in 3.5.8
In-Reply-To: <55D6D326.4070305@treenet.co.nz>
References: <55D6D326.4070305@treenet.co.nz>
Message-ID: <1440156391.3658.2.camel@JamesiMac>

On Fri, 2015-08-21 at 19:28 +1200, Amos Jeffries wrote:

> Hi all,
> 
>  Christos has managed (we think) to resolve a fairly major design issue
> that has been plaguing the 3.5 series peek-and-splice feature so far.
>  (<http://wiki.squid-cache.org/Features/SslPeekAndSplice>)
> 
> The problem was that Squid was not actually following the intended and
> documented logic of skipping the impossible bumping actions. The patch
> for that will be in 3.5 snaphots labelled r13895 or later (still waiting
> on mirror updates as I write this 1-2hrs more maybe).
> (<http://www.squid-cache.org/Versions/v3/3.5/>)
> 
> 
> Since it is affecting the visible behaviour of squid.conf settings I
> would like some volunteers to help test it out. Find what problems
> remain, and let me know what to alert others to in the next formal release.
> 
> 
> We need testing both from those having issues currently, and those who
> managed to get a trial-and-error config going with older 3.5.
> 
> Hopefully, if you are using the at_step workarounds there should not be
> any visible difference. But some of the at_step tests may be needless now.
> 
> Thank you in advance for any assistance.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Count me in....I'll let you know my results...my config is in this
list...it hasn't changed.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150821/e2213350/attachment.htm>

From fredbmail at free.fr  Fri Aug 21 11:39:33 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 21 Aug 2015 13:39:33 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1440146203116-4672802.post@n4.nabble.com>
Message-ID: <1945943350.61734920.1440157173877.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hi all,

I think I misunderstand something but why refresh pattern is not useless ?
I mean the objects are supposed to be delivered with instructions from the web server, lifetime, creation time, etc 

I thought, and it seem I'm wrong ?, that squid check the HTTP header when the object seems expired (HEAD request), if yes the object should be refreshed, or not, when is life is ended ?

Force a long refresh pattern is not dangerous ? For example with some security files. 
 
Regards 

Fred


From jlay at slave-tothe-box.net  Fri Aug 21 12:24:33 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 21 Aug 2015 06:24:33 -0600
Subject: [squid-users] ssl_bump updates coming in 3.5.8
In-Reply-To: <1440156391.3658.2.camel@JamesiMac>
References: <55D6D326.4070305@treenet.co.nz>
 <1440156391.3658.2.camel@JamesiMac>
Message-ID: <1440159873.3658.4.camel@JamesiMac>

On Fri, 2015-08-21 at 05:26 -0600, James Lay wrote:

> On Fri, 2015-08-21 at 19:28 +1200, Amos Jeffries wrote: 
> 
> > Hi all,
> > 
> >  Christos has managed (we think) to resolve a fairly major design issue
> > that has been plaguing the 3.5 series peek-and-splice feature so far.
> >  (<http://wiki.squid-cache.org/Features/SslPeekAndSplice>)
> > 
> > The problem was that Squid was not actually following the intended and
> > documented logic of skipping the impossible bumping actions. The patch
> > for that will be in 3.5 snaphots labelled r13895 or later (still waiting
> > on mirror updates as I write this 1-2hrs more maybe).
> > (<http://www.squid-cache.org/Versions/v3/3.5/>)
> > 
> > 
> > Since it is affecting the visible behaviour of squid.conf settings I
> > would like some volunteers to help test it out. Find what problems
> > remain, and let me know what to alert others to in the next formal release.
> > 
> > 
> > We need testing both from those having issues currently, and those who
> > managed to get a trial-and-error config going with older 3.5.
> > 
> > Hopefully, if you are using the at_step workarounds there should not be
> > any visible difference. But some of the at_step tests may be needless now.
> > 
> > Thank you in advance for any assistance.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> Count me in....I'll let you know my results...my config is in this
> list...it hasn't changed.
> 
> James 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Appears to work fine here:

Squid Cache: Version 3.5.7-20150821-r13895
Service Name: squid
configure options:  '--prefix=/opt' '--with-openssl' '--enable-ssl'
'--enable-ssl-crtd' '--enable-linux-netfilter'
'--enable-follow-x-forwarded-for' '--with-large-files'
'--sysconfdir=/opt/etc/squid' '--enable-external-acl-helpers=none'


Aug 21 06:21:11 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:11 -0600] "CONNECT 69.192.193.247:443 HTTP/1.1"
configuration.apple.com - 200 9 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:29 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:29 -0600] "CONNECT 17.173.66.95:443 HTTP/1.1"
pd-st.itunes.apple.com - 200 532 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:30 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:30 -0600] "CONNECT 69.192.207.154:443 HTTP/1.1"
init.itunes.apple.com - 200 31123 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:30 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:30 -0600] "CONNECT 17.173.66.135:443 HTTP/1.1"
xp.apple.com - 200 657 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:30 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:30 -0600] "CONNECT 17.173.66.95:443 HTTP/1.1"
pd-st.itunes.apple.com - 200 2059 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:31 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:31 -0600] "CONNECT 17.173.66.73:443 HTTP/1.1"
partiality.itunes.apple.com - 200 679 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:32 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:32 -0600] "CONNECT 69.192.193.29:443 HTTP/1.1"
iadsdk.apple.com - 200 409 TCP_TUNNEL:ORIGINAL_DST peek
Aug 21 06:21:32 gateway (squid-1): 192.168.1.100 - -
[21/Aug/2015:06:21:32 -0600] "CONNECT 69.192.193.29:443 HTTP/1.1"
iadsdk.apple.com - 200 409 TCP_TUNNEL:ORIGINAL_DST peek

I still see only peek instead of the final splice/bump in the
logs...hoping that gets resolved soon.  Thanks Alex.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150821/ac4084c9/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 21 12:37:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Aug 2015 00:37:22 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440146802401-4672803.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com>
Message-ID: <55D71B82.7020203@treenet.co.nz>

On 21/08/2015 8:46 p.m., Stakres wrote:
> Amos,
> 
> in the meantime, I was thinking another point:
> We know there are at least 2 limit with cache_dirs, the max size and the 16+
> million entries (Filemap bits).
> the cache_swap_high should take care both.
> example: if the used cache is 95% of the space or if the Filemap bits is 95%
> of the 16+ million entries.
> 
> Because if the used cache is 100%, the squid could crash a few later, but if
> the squid is 16777216 entries it does crash immetiatly and will be looping
> in crashes.
> 
> See what I mean ?
> 

Yes. Good point.

Though actually reaching that sfileno limit of 2^25 objects is not the
problem in the crash issue. Just like memory and rock stores happily sit
at 100% full in capacity with no need to purge. So can any other cache
happily sit with 2^25 objects stored.

The crash part is in whatever specific bit of code is allowing stores to
go out-of-bounds on the sfilno value range. Anything we do here would be
a workaround at most, and at the cost of cache HITS for anyone who runs
with full caches but not crashing with that bug.

Amos



From a.alii85 at gmail.com  Fri Aug 21 13:39:21 2015
From: a.alii85 at gmail.com (asad)
Date: Fri, 21 Aug 2015 18:39:21 +0500
Subject: [squid-users] Using Squid as forward http proxy failing to complete
	request?
Message-ID: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>

I'm using Squid as local proxy.

I have the config file as below:-#

    # Recommended minimum configuration:
    #

    # Example rule allowing access from your local networks.
    # Adapt to list your (internal) IP networks from where browsing
    # should be allowed

    acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
    acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
    acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
    acl localnet src fc00::/7       # RFC 4193 local private network range
    acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

    acl SSL_ports port 443
    acl Safe_ports port 80        # http
    acl Safe_ports port 21        # ftp
    acl Safe_ports port 443        # https
    acl Safe_ports port 70        # gopher
    acl Safe_ports port 210        # wais
    acl Safe_ports port 1025-65535    # unregistered ports
    acl Safe_ports port 280        # http-mgmt
    acl Safe_ports port 488        # gss-http
    acl Safe_ports port 591        # filemaker
    acl Safe_ports port 777        # multiling http
    acl CONNECT method CONNECT

    #
    # Recommended minimum Access Permission configuration:
    #

    # Only allow cachemgr access from localhost
    http_access allow localhost manager
    http_access deny manager

    # Deny requests to certain unsafe ports
    http_access deny !Safe_ports

    # Deny CONNECT to other than secure SSL ports
    http_access deny CONNECT !SSL_ports

    # We strongly recommend the following be uncommented to protect innocent
    # web applications running on the proxy server who think the only
    # one who can access services on "localhost" is a local user
    #http_access deny to_localhost

    #
    # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
    #

    # Example rule allowing access from your local networks.
    # Adapt localnet in the ACL section to list your (internal) IP networks
    # from where browsing should be allowed
    http_access allow localnet
    http_access allow localhost

    # And finally deny all other access to this proxy
    http_access deny all

    # Squid normally listens to port 3128
    http_port 3128

    # Uncomment the line below to enable disk caching - path format is
/cygdrive/<full path to cache folder>, i.e.
    #cache_dir aufs /cygdrive/d/squid/cache 3000 16 256



*  cache_peer 10.1.2.3 parent 8080 0 no-query default
login=my_username:my_password    never_direct allow all*

    # Leave coredumps in the first cache dir
    coredump_dir /var/cache/squid

    # Add any of your own refresh_pattern entries above these.
    refresh_pattern ^ftp:        1440    20%    10080
    refresh_pattern ^gopher:    1440    0%    1440
    refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
    refresh_pattern .        0    20%    4320

    dns_nameservers 8.8.8.8 208.67.222.222

    max_filedescriptors 3200

Now, I want to use it as forward proxy For that the configuration is shown
in the config file in ***bold-text***.

I have browsed tons of web-pages and all have said to include a line
similar to this. Beyond this configuration, I don't know what else to add
in order to make it work.


One, thing under safe_port should I be changing the http port to "8080"
since my local machine is already behind another proxy.

Also, I'm using domain authentication (NTLM) to connect to other proxy. Is
authentication configuration required beside what already done in config
file.

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150821/ce795c86/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 21 14:41:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Aug 2015 02:41:05 +1200
Subject: [squid-users] ssl_bump updates coming in 3.5.8
In-Reply-To: <55D705C6.1040506@urlfilterdb.com>
References: <55D6D326.4070305@treenet.co.nz> <55D705C6.1040506@urlfilterdb.com>
Message-ID: <55D73881.6090708@treenet.co.nz>

On 21/08/2015 11:04 p.m., Marcus Kool wrote:
> I do not want to spoil things, but did you already read my latest
> addition to bug 4303 ?
> 
> Marcus
> 

Haven't had a chance to read the logs yet, but got the main text. Thank you.

The main emphasis of the patch was getting the action ignore/skip
actually doing as intended and documented. And simpler set of states to
track as a bonus. We are both confident it goes that far. This testing
request is to see if anyone is burned by the change and what can be done
before formal release to reduce any pain.

Even though its not a fix for your issue, and others issues too, it does
make the whole situation a bit simpler/deterministic for tracking them down.

Amos



From zimurgy at gmail.com  Fri Aug 21 15:41:48 2015
From: zimurgy at gmail.com (JL)
Date: Fri, 21 Aug 2015 11:41:48 -0400
Subject: [squid-users] Cache Drive Permission Issues
Message-ID: <CAPPTbuoPU8MJrs_MnghfyR_xzpR_vP4ESccWLeObz6L__39oDA@mail.gmail.com>

Hi,

I am having an issue with my caching drives that I can't seem to pinpoint
the problem. I have 4 drives to be used for caching, they are ext3
filesystems mounted like so. I am running CentOS 7.

/dev/sde1 on /var/spool/squid4 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdd1 on /var/spool/squid3 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdc1 on /var/spool/squid2 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdb1 on /var/spool/squid1 type ext3 (rw,noatime,seclabel,data=ordered)

I set the cache user to be squid in the squid.conf and I reference the
drives for caching.

cache_dir aufs /var/spool/squid1 460800 32 512
cache_dir aufs /var/spool/squid2 460800 32 512
cache_dir aufs /var/spool/squid3 460800 32 512
cache_dir aufs /var/spool/squid4 460800 32 512

ls -l of the /var/spool shows proper perms, they are propagated.

drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid1
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid2
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid3
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid4

The cache dirs were successfully created using squid -z and all of the
subfolders were generated. Squid starts and everything appears to be great.
Until I monitor the cache.log, then I see the errors begin.

Some like this.

2015/08/16 03:41:55 kid1| /var/spool/squid1/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:10 kid1| /var/spool/squid2/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:25 kid1| /var/spool/squid3/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:40 kid1| /var/spool/squid4/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:55 kid1| /var/spool/squid1/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:10 kid1| /var/spool/squid2/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:25 kid1| /var/spool/squid3/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:40 kid1| /var/spool/squid4/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:55 kid1| /var/spool/squid1/cache/1B/8B: (13) Permission
denied

Some like this.

2015/08/21 10:02:13 kid1| /var/spool/squid2/17/1FF: (13) Permission denied
2015/08/21 10:02:13 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:13 kid1| /var/spool/squid4/00/00/00000000
2015/08/21 10:02:28 kid1| /var/spool/squid3/17/1FF: (13) Permission denied
2015/08/21 10:02:32 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:32 kid1| /var/spool/squid4/00/00/00000001
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000002
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000003
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000004
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000005
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000006
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000007

The folders remain empty of course, so no caching is taking place.

I am truly stumped as to what I may be doing wrong since squid has enough
perms to create the dirs but not enough to save to them.

Any help would be great.

Thanks!
Zim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150821/1cecb1ee/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 21 15:57:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Aug 2015 03:57:27 +1200
Subject: [squid-users] Using Squid as forward http proxy failing to
 complete request?
In-Reply-To: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
References: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
Message-ID: <55D74A67.3000600@treenet.co.nz>

On 22/08/2015 1:39 a.m., asad wrote:
> I'm using Squid as local proxy.
> 
> I have the config file as below:-#
> 
<snip>
> 
> 
> *  cache_peer 10.1.2.3 parent 8080 0 no-query default
> login=my_username:my_password    never_direct allow all*
> 
<snip>
> Now, I want to use it as forward proxy For that the configuration is shown
> in the config file in ***bold-text***.


I'm not sure you are using the term "forward proxy" right.
"forward-proxy" is nowdays commonly also stated "explicit proxy".


See this bit here:

   # Squid normally listens to port 3128
   http_port 3128

That bit alone, is what makes your Squid an explicit/forward-proxy.


Using cache_peer like shown above just relays traffic through a parent
proxy. Which in this case is also happens to be a forward/explicit proxy.

Using "never_direct allow all" forces only that parent proxy to be used.
No traffic may go anywhere but through a cache_peer.


> 
> I have browsed tons of web-pages and all have said to include a line
> similar to this. Beyond this configuration, I don't know what else to add
> in order to make it work.
> 
> 
> One, thing under safe_port should I be changing the http port to "8080"
> since my local machine is already behind another proxy.

No. The cache_peer details is all private between the two proxies. It
has no part in the traffic messages themselves.

> 
> Also, I'm using domain authentication (NTLM) to connect to other proxy. Is
> authentication configuration required beside what already done in config
> file.
> 

What those tutorials are not explaining is that the
"login=my_username:my_password" is sending Basic auth credentials to the
parent proxy.

Squid does not support using NTLM to authenticate its cache_peer TCP
connection.

The options available for cache_peer directive and what they do is
documented at <http://www.squid-cache.org/Doc/config/cache_peer/>


My suggestions for a fix:

You could use "login=PASSTHRU connection-auth=on" and not do anything
else with auth in your proxy. That way the client software can (try to)
authenticate to the parent proxy directly with NTLM.


Or, if the parent proxy accepts Negotiate/Kerberos auth you can use
"login=NEGOTIATE" and setup the necessary keytab for your proxy to
authenticate with.


Amos



From sebag at vianetcon.com.ar  Fri Aug 21 16:20:20 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Fri, 21 Aug 2015 13:20:20 -0300
Subject: [squid-users] Lots of "Vary object loop!"
Message-ID: <55D74FC4.7050203@vianetcon.com.ar>

Hello everyone, I'm having a strange problem:

Several servers, same hardware, using same version of squid (3.5.4) 
compiled using the same configure options, same configuration files. But 
in two of them I get LOTS of these Vary object loop! lines in cache.log

2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://resources.mlstatic.com/frontend/vip-fend-webserver/assets/bundles/photoswipe-6301b943e5586fe729e5d6480120a893.js' 
'accept-encoding="gzip"'
2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 'http://www.google.com/afs/ads/i/iframe.html' 
'accept-encoding="gzip,%20deflate"'
2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
2015/08/21 13:08:01 kid1| varyEvaluateMatch: Oops. Not a Vary match on 
second attempt, 
'http://minicuotas.ribeiro.com.ar/images/products/large/035039335000.jpg' 'accept-encoding="gzip,%20deflate"'
2015/08/21 13:08:01 kid1| clientProcessHit: Vary object loop!

I've read what I could find on forums but could not solve it. Is this 
something to worry about? If that is not the case, how can I disable the 
excessive logging?
Which is the condition that generates this?


Thanks,
Sebasti?n


From squid3 at treenet.co.nz  Fri Aug 21 16:32:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Aug 2015 04:32:52 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1945943350.61734920.1440157173877.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1945943350.61734920.1440157173877.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55D752B4.3050204@treenet.co.nz>

On 21/08/2015 11:39 p.m., FredB wrote:
> Hi all,
> 
> I think I misunderstand something but why refresh pattern is not useless ?
> I mean the objects are supposed to be delivered with instructions from the web server, lifetime, creation time, etc 
> 

Well, we like it when they do. Since that makes things easy and
predictable. Especially for the third-party debugging. But no they are
not actually required to.

Still the HTTP freshness algorithm does have a certain fixed set of
parameters that are needed to caculate freshness for every response object.

In the absence of header values from the server the refresh_pattern
whose regex matches the URL provides the default values for the
parameters min/max-storage-time and lifetime-expectency-percent.



> I thought, and it seem I'm wrong ?, that squid check the HTTP header when the object seems expired (HEAD request), if yes the object should be refreshed, or not, when is life is ended ?
> 

Sending a HEAD request would use an RTT of latency, plus server CPU
cycles just to figure out if another fetch was needed.

HTTP/1.0 has to work that way because it has no revalidation mechanism.

HTTP/1.1 revaidation collapses all that cost down into one with a
conditional-GET request (aka, IMS or INM). Which either updates just
headers, or headers+payload in one go.

Still, by not sending explicit values the server is offering cache admin
the privilege of deciding for themselves what type of storage profile to
use. refresh_pattern is the squid.conf mechanism for doing that.


> Force a long refresh pattern is not dangerous ? For example with some security files. 
>  

HTTP specs permit up to 68 years storage for any given item. That is
imposed only by a need for age values to fit within a 32-bit integer.

Objects with security sentitive details MUST be labelled appropriately
as Expires:-1 and Cache-Control: private, no-store, no-transform -
whichever applies to that objects data.

Caches MUST obey those controls.

 "MUST" being the spec emphasis and meaning for mandatory requirement.

And the key reason why I go on so much about not using the override-*
and ignore-* options on refresh_pattern. Or at least targetting those
patterns VERY specifically at sites that are so broken there is no
choice. Every time they are used on an object it "breaks the Internet"
for someone, usually the admins own users/customers.

Amos



From fredbmail at free.fr  Fri Aug 21 17:06:43 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 21 Aug 2015 19:06:43 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55D752B4.3050204@treenet.co.nz>
Message-ID: <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>

Thank Amos, very interesting as usual 

So, my vision was old school (HTTP 1.0), I should read the recent documentations to find something optimal for my caches without side effect, in the past (squid 2.x I guess) I saw some objects changed in website who were never delivered by Squid (always the cached version, without a browser/squidclient force refresh of course) and it's a lot of pain with many proxies and many users behind a load balancer ...

Actually with the default values I have 20/30 % bytes cached (40/50 % HIT ratio) without any phone call :) Interesting score without sslbump, for youtube and others, squid makes a great job  

 


From squid3 at treenet.co.nz  Fri Aug 21 17:34:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Aug 2015 05:34:41 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <211039338.62276974.1440176803569.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55D76131.2070409@treenet.co.nz>

On 22/08/2015 5:06 a.m., FredB wrote:
> Thank Amos, very interesting as usual
> 
> So, my vision was old school (HTTP 1.0), I should read the recent
> documentations to find something optimal for my caches without side
> effect, in the past (squid 2.x I guess) I saw some objects changed in
> website who were never delivered by Squid (always the cached version,
> without a browser/squidclient force refresh of course) and it's a lot
> of pain with many proxies and many users behind a load balancer ...

Yes.

<http://wiki.squid-cache.org/Features/HTTP11> may be a good starting
point to upskill or just learn about squid abilities within the
protocol. The current RFC 723x are pretty easy reading compared to most
(intentionally).

The checklist .ODS document compares several Squid 2.7+ versions
compliance per-feature. Though the co-advisor still does not seem to
check the 723x updated requirements fully.

Its all a little bit stale, but mostly accurate still.


Amos


From hack.back at hotmail.com  Sat Aug 22 01:38:18 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 21 Aug 2015 18:38:18 -0700 (PDT)
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <55D57EE3.2080604@trimble.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com> <55D455CF.9030907@trimble.com>
 <55D4A286.4060603@measurement-factory.com> <55D52269.4090805@trimble.com>
 <55D57EE3.2080604@trimble.com>
Message-ID: <1440207498365-4672821.post@n4.nabble.com>

can you share your perl file 
/usr/local/bin/confirm_https.pl 
Thanks ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/can-t-get-bump-to-work-anymore-on-3-5-7-tp4672762p4672821.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From spider at smoothnet.org  Sat Aug 22 04:15:16 2015
From: spider at smoothnet.org (Nicolaas Hyatt)
Date: Fri, 21 Aug 2015 23:15:16 -0500
Subject: [squid-users] ETA for Bug 3775
In-Reply-To: <55D6A36B.5050808@treenet.co.nz>
References: <001901d0dbae$acc0e6e0$0642b4a0$@smoothnet.org>
 <55D6A36B.5050808@treenet.co.nz>
Message-ID: <009801d0dc91$268b81f0$73a285d0$@smoothnet.org>

Squid v3.5.7-20150821-r13895

Backtrace:
Program received signal SIGSEGV, Segmentation fault.
DiskdFile::writeDone (this=<optimized out>, M=0x7fffffffe140) at DiskIO/DiskDaemon/DiskdFile.cc:389
389         ioRequestor->writeCompleted (DISK_OK,M->status, writeRequest);
Missing separate debuginfos, use: debuginfo-install glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7.9.x86_64 pcre-8.32-14.el7.x86_64 sssd-client-1.12.2-58.el7_1.6.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64
 (gdb) backtrace
#0  DiskdFile::writeDone (this=<optimized out>, M=0x7fffffffe140) at DiskIO/DiskDaemon/DiskdFile.cc:389
#1  0x000000000070a0e4 in DiskdIOStrategy::handle (this=this at entry=0x1748950, M=M at entry=0x7fffffffe140) at DiskIO/DiskDaemon/DiskdIOStrategy.cc:303
#2  0x000000000070a1c5 in DiskdIOStrategy::callback (this=0x1748950) at DiskIO/DiskDaemon/DiskdIOStrategy.cc:556
#3  0x0000000000676a56 in StoreHashIndex::callback (this=<optimized out>) at store_dir.cc:1036
#4  0x0000000000616016 in StoreRootEngine::checkEvents (this=<optimized out>, timeout=<optimized out>) at main.cc:189
#5  0x00000000005a69bf in EventLoop::checkEngine (this=this at entry=0x7fffffffe350, engine=0x7fffffffe2d0, primary=primary at entry=false) at EventLoop.cc:33
#6  0x00000000005a6be2 in EventLoop::runOnce (this=this at entry=0x7fffffffe350) at EventLoop.cc:104
#7  0x00000000005a6e30 in EventLoop::run (this=this at entry=0x7fffffffe350) at EventLoop.cc:82
#8  0x000000000061487a in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1533
#9  0x000000000050533d in SquidMainSafe (argv=<optimized out>, argc=<optimized out>) at main.cc:1258
#10 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1251

If you want me to continue with more backtraces let me know. Also I can provide any configuration should you need them.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, August 20, 2015 11:05 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ETA for Bug 3775

On 21/08/2015 1:14 p.m., Nicolaas Hyatt wrote:
> Hey guys,
> 
> I have been paying close attention to the list for a while and am just 
> beginning to realize the scale of things the squid team has in front 
> of them. So please understand that I'm _NOT_ begging here. I 
> understand other priority issues take precedence, and my little issue 
> is way on down the line. I was wondering if there was some sort of 
> schedule as to when this may be examined in case I need to provide any more dumps.
> 
>  
> 
> http://bugs.squid-cache.org/show_bug.cgi?id=3775
> 

Unknown again I'm afraid. That latest feedback was a painfully disappointing result from a very hopeful looking approach.

If you have a dump that provides any new backtrace sequence to what we have already they may be useful.

So far it all still seems to link back to a DeferredReader pausing a transaction read, then something else playing around with its I/O socket/FD/connection, then defer resumes and boom. But there is potentially some minutes of delay between those actions.

PS. you may want to help with the testing of todays snapshot when its ready, it contains some patches to HTTPS and FTP connection handling that may have an impact on this bug (or not, grasping at straws here).

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Jason_Haar at trimble.com  Sat Aug 22 23:54:29 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Sun, 23 Aug 2015 11:54:29 +1200
Subject: [squid-users] can't get bump to work anymore on 3.5.7?
In-Reply-To: <1440207498365-4672821.post@n4.nabble.com>
References: <55D3E7D4.4030508@trimble.com>
 <55D400F8.5050403@measurement-factory.com> <55D455CF.9030907@trimble.com>
 <55D4A286.4060603@measurement-factory.com> <55D52269.4090805@trimble.com>
 <55D57EE3.2080604@trimble.com> <1440207498365-4672821.post@n4.nabble.com>
Message-ID: <55D90BB5.7070304@trimble.com>

On 22/08/15 13:38, HackXBack wrote:
> can you share your perl file 
> /usr/local/bin/confirm_https.pl 
> Thanks ..
>
It's not really useful... I hacked it together in order to be able to
differentiate a lot of the ways that I discovered bumping could fail
(client certs, lack of SNI, non-SSL). It's awful: bunches of calls to
openssl and curl - all sorts of kruft.

Very useful for me to learn about how all this works - but not designed
for production. Probably full of security risks too (eg I don't bother
sanitizing the hostnames - which are dropped into shell calls - not a
good look)

 

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From tech at neosys.net  Sun Aug 23 03:55:41 2015
From: tech at neosys.net (Brandon Elliott)
Date: Sat, 22 Aug 2015 23:55:41 -0400
Subject: [squid-users] Getting Invalid ACL type 'all-of' ... Please help
Message-ID: <CALknM8_gw_wWQMxEDQtw4tc2m-q2uu2QYwh5=wfeFzn6WKO_DA@mail.gmail.com>

Hello all,

I believe I have followed the fairly self-explanatory steps to create an
AND of two ACL rules using the 'all-of' type of ACL, but I keep getting an
error as if the type is not recognized. I have searched google for hours
and tried a number of things but nothing works.

My rules are:

acl user1-ip myportname xxx.xxx.xxx.xxx:3128
acl user1-pw proxy_auth user1
acl user1-acl all-of user1-ip user1-pw

The authentication is working fine if I don't use the 'all-of' ACL, but I
really need to be able to combine these two rules to lock down each proxy
to the incoming IP AND the user/pw.

The error I get is:

aclParseAclLine: Invalid ACL type 'all-of'
FATAL: Bungled squid.conf line 100: acl user1-acl all-of user1-ip user1-pw
Squid Cache (Version 3.1.23): Terminated abnormally.

Any help or direction would be greatly appreciated!!

Thanks,

Brandon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150822/a82ecf0c/attachment.htm>

From squid3 at treenet.co.nz  Sun Aug 23 04:43:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2015 16:43:01 +1200
Subject: [squid-users] Getting Invalid ACL type 'all-of' ... Please help
In-Reply-To: <CALknM8_gw_wWQMxEDQtw4tc2m-q2uu2QYwh5=wfeFzn6WKO_DA@mail.gmail.com>
References: <CALknM8_gw_wWQMxEDQtw4tc2m-q2uu2QYwh5=wfeFzn6WKO_DA@mail.gmail.com>
Message-ID: <55D94F55.6030308@treenet.co.nz>

On 23/08/2015 3:55 p.m., Brandon Elliott wrote:
> Hello all,
> 
> I believe I have followed the fairly self-explanatory steps to create an
> AND of two ACL rules using the 'all-of' type of ACL, but I keep getting an
> error as if the type is not recognized. I have searched google for hours
> and tried a number of things but nothing works.
> 
> My rules are:
> 
> acl user1-ip myportname xxx.xxx.xxx.xxx:3128
> acl user1-pw proxy_auth user1
> acl user1-acl all-of user1-ip user1-pw
> 

For syntax that is correct. BUT ...

> Squid Cache (Version 3.1.23): Terminated abnormally.

all-of is an ACL type added in Squid-3.4.

The web docs should be stating that but seem not to be adding the
history texts for 3.3+ to the generic copy. Sorry for that. I'm looking
into it now.


PS. Please upgrade. Current production/stable release is 3.5.7 (with 3.4
still a solid fallback if you really need to).

Amos



From wmunny at mail.com  Sun Aug 23 10:36:52 2015
From: wmunny at mail.com (wmunny william)
Date: Sun, 23 Aug 2015 12:36:52 +0200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55D32F77.1070906@treenet.co.nz>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
 <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>,
 <55D32F77.1070906@treenet.co.nz>
Message-ID: <trinity-797a4b47-09dd-446a-bd28-dbede84a1a01-1440326211869@3capp-mailcom-lxa04>


> > Sorry to jump on a late thread - it is also possible to use ICAP/eCAP server to filter the actual contents of the stream.
> > 
> > C-ICAP comes to mind first, then eCap samples from http://www.e-cap.org/Downloads
> > 
> 
> And the *CAP services is a better solution than either URL-rewriting or
> chaining proxies. Since the HTTPS only gets MITM'd once, not twice or more.
> 
> Amos
> _______________________________________________


Hello All,

I know DansGuardian, e2guardian, squidguard but no free solution with ICAP
Do you have an advice for this ?

Maybe there are some "hubs", Squid > icap > DansGuardian ?

William 


From squid3 at treenet.co.nz  Sun Aug 23 11:14:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2015 23:14:27 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55D74FC4.7050203@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
Message-ID: <55D9AB13.2000702@treenet.co.nz>

On 22/08/2015 4:20 a.m., Sebastian Goicochea wrote:
> Hello everyone, I'm having a strange problem:
> 
> Several servers, same hardware, using same version of squid (3.5.4)
> compiled using the same configure options, same configuration files. But
> in two of them I get LOTS of these Vary object loop! lines in cache.log
> 
> 2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://resources.mlstatic.com/frontend/vip-fend-webserver/assets/bundles/photoswipe-6301b943e5586fe729e5d6480120a893.js'
> 'accept-encoding="gzip"'
> 2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
> 2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt, 'http://www.google.com/afs/ads/i/iframe.html'
> 'accept-encoding="gzip,%20deflate"'
> 2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
> 2015/08/21 13:08:01 kid1| varyEvaluateMatch: Oops. Not a Vary match on
> second attempt,
> 'http://minicuotas.ribeiro.com.ar/images/products/large/035039335000.jpg' 'accept-encoding="gzip,%20deflate"'
> 
> 2015/08/21 13:08:01 kid1| clientProcessHit: Vary object loop!
> 
> I've read what I could find on forums but could not solve it. Is this
> something to worry about?

The short answer:

Yes and no. Squid is signalling that it is completely unable to perform
its caching duty for these URLs. The proxying duty continues with only
high latency visible to the client.

It is up to you whether that latency cost is urgent or not. It is
certainy high enough importance that you need to be told each time (no
rate limiting) when you have asked to receive important notices.


> If that is not the case, how can I disable the
> excessive logging?

You can reduce your logging level to show only critical problems,
instead of showing all details rated 'important'.

  debug_options ALL,0

NOTE: important (ALL,1) includes a lot of things like this that do
really need to be fixed to get better service out of either your proxy
or the underlying network. But can be put on your todo list if you dont
have time right now.


> Which is the condition that generates this?


In long;


The "whats happening" is:

Your cache contains an object which was delivered by the server along
with headers stating that behind the URL is a large set of porssible
responses. *all* requests for that URL use a certain set of headers
(listed in Vary) to determine which binary-level object is applicable
(or not) on a per-client / per-reqeust basis.
 In order to cache the object Squid has to follow that same selection
criteria *exactly*.

Most common example is gzip vs non-gzip encoded copies of things. Which
you can see those messages relate to.

Squid stores this information in a "Vary object" associated with only
the URL. That vary object is used to perform a secondary cache index
lookup to see if the partiular variant needed is stored.

The expectation is that there would be 3+ objects stored for this URL; a
gzip data object, various non-gzip data objects, and a metadata object
("Vary object") telling Squid that it needs to look at the
accept-encoding header to find which of the those data objects to send
the client.


The messages themselves mean:

"Oops. Not a Vary match on second attempt"

 - that the Vary object saying look at headers X+Y+X is pointing at
itself or another Vary metadata object saying look at some other
headers. A URL cannot have two different Vary header values
simultaneously (Vary is a single list "value").
Something really weird is going on in your cache. Squid should handle
this by abandoning the cache lookups and go to the origin for fresh copies.

You could be causing it by using url-rewrite or store-id helpers wrongly
to pass requests for a URL to servers which produce different responses.
So that is well worth looking into.

IMPORTANT: It is mandatory that any re-writing only be done to
'collapse' URLs that are *actually* producing identical objects and
producing them in (outwardly) identical ways. This Vary looping is just
the tip of an iceberg of truly horrible failures that occur "silently"
with re-writing.



There is another similar message that can be mixed in the long list:

"Oops. Not a Vary object on second attempt," (note the 1-word difference)
 - this is almost but not quite so bad, and is usually seen with broken
origin servers. All you can do about the problem itself then is fire off
bug reports to people and hope it gets fixed by the sysadmin in charge.


Both situations are very bad for HTTP performance, and bad for churning
your cache as well. But Squid can cope easily enough by just fetching a
new object and dropping what is in the cache. That "Vary object loop!"
message is telling you Squid is doing exactly that.


A quick test with the tool at redbot.org shows that the
resources.mlstatic.com server is utterly borked. Not even sending
correct ETag ids for the objects its outputting. Thats a sign to me that
the admin is trying to be smart with headers, and getting it very badly
wrong.

minicuotas.ribeiro.com.ar is also a Nginx server. But only showing the
signs of normal default nginx brokenness
(<http://trac.nginx.org/nginx/ticket/118>).

The only thing you can do about that nginx bug is add pressure to get it
fixed, or cut away all the Accept-Encoding headers on all requests sent
to those servers. (request_header_access Accept-Encoding deny ...) Squid
is already doing everything it reasonably can to correct the traffic
output to your clients.


The google related messages ... they are usually pretty good at this
type of thing and my tests do show their server to be working correctly.
 So that points me back at suspecting your config does something bad
with url-rewriter or store-id helpers.

Amos



From squid3 at treenet.co.nz  Sun Aug 23 11:39:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2015 23:39:09 +1200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <trinity-797a4b47-09dd-446a-bd28-dbede84a1a01-1440326211869@3capp-mailcom-lxa04>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
 <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <55D32F77.1070906@treenet.co.nz>
 <trinity-797a4b47-09dd-446a-bd28-dbede84a1a01-1440326211869@3capp-mailcom-lxa04>
Message-ID: <55D9B0DD.70301@treenet.co.nz>

On 23/08/2015 10:36 p.m., wmunny william wrote:
> 
>>> Sorry to jump on a late thread - it is also possible to use ICAP/eCAP server to filter the actual contents of the stream.
>>>
>>> C-ICAP comes to mind first, then eCap samples from http://www.e-cap.org/Downloads
>>>
>>
>> And the *CAP services is a better solution than either URL-rewriting or
>> chaining proxies. Since the HTTPS only gets MITM'd once, not twice or more.
>>
>> Amos
>> _______________________________________________
> 
> 
> Hello All,
> 
> I know DansGuardian, e2guardian, squidguard but no free solution with ICAP
> Do you have an advice for this ?
> 
> Maybe there are some "hubs", Squid > icap > DansGuardian ?

Looking for one single thing that does everything DG or e2guardian do,
or wraps them completely is the wrong approach. They are almost
full-blown proxies like Squid.

The *CAP design is to leave all the transfer proxying and caching duties
to software like Squid and only perform the actual content adaptation
policies in the service/module.

You need to look at what DG is doing for you now and how much of that is
available from squid.conf capabilities. Most of it usually is. Only the
remaining "fiddling with payloads" bit actually needs a third-party service.


GreasySpoon or qlproxy seems to be the high profile ones. c-icap and
Traffic Spicer seem to offer frameworks rather than pre-made filters
<http://www.squid-cache.org/Misc/icap.html>

The interfaces GreasySpoon or qlproxy describe say easily scriptable to
do filtering. Though filtering is a huge topic, so "easily" is up for
interpretation.

And fiddling around with your customers content is very site-specific
about what can or can't be done. Thus the frameworks and script engines
being most high profile.

HTH
Amos



From wmunny at mail.com  Sun Aug 23 13:43:10 2015
From: wmunny at mail.com (wmunny william)
Date: Sun, 23 Aug 2015 15:43:10 +0200
Subject: [squid-users] peek and splice content inspection question
In-Reply-To: <55D9B0DD.70301@treenet.co.nz>
References: <20150813144718.35a4c9a8@efreet.mimar.rs>
 <55CCBA07.6030306@treenet.co.nz> <55CD048F.7090907@measurement-factory.com>
 <55CD0902.9010703@gmail.com> <55CD6F35.8090005@treenet.co.nz>
 <55CE0071.7050606@measurement-factory.com>
 <CANLNtGS2wyieNzLV_n43fbaSka=q9HD-5WcbicOMXtJN1R-xhQ@mail.gmail.com>
 <55D0ED84.60501@gmail.com>
 <CANLNtGRVPgfS4fDr3LFPLjeNiohWVSEkeoLoyjnq=1RmLAb6OA@mail.gmail.com>
 <55D10A6C.3050909@gmail.com>
 <CANLNtGRsrb-C-ABMS_Dkw151XvxPcLsTY=pm66jQznwQRRY7nw@mail.gmail.com>
 <VI1PR04MB13596D12E5E2AAC65BC1996E8F780@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <55D32F77.1070906@treenet.co.nz>
 <trinity-797a4b47-09dd-446a-bd28-dbede84a1a01-1440326211869@3capp-mailcom-lxa04>,
 <55D9B0DD.70301@treenet.co.nz>
Message-ID: <trinity-b57d0ff7-3476-480c-8910-6cc441a3051a-1440337390073@3capp-mailcom-lxa04>


> Looking for one single thing that does everything DG or e2guardian do,
> or wraps them completely is the wrong approach. They are almost
> full-blown proxies like Squid.
> 
> The *CAP design is to leave all the transfer proxying and caching duties
> to software like Squid and only perform the actual content adaptation
> policies in the service/module.
> 
> You need to look at what DG is doing for you now and how much of that is
> available from squid.conf capabilities. Most of it usually is. Only the
> remaining "fiddling with payloads" bit actually needs a third-party service.
> 
> 

Yes I know that Squid is very powerful, but (DG or E2) seems - to me - more easier with complex rules
I'm working with multi users groups, regex in HTML, rules with exceptions (site allowed if some conditions), etc

I guess if I reproduce my configuration in squid.conf it will be more hard to maintain after ..
 
Also these soft are massively multi-threaded, in my usage squid + dg use less CPU than Squid alone I mean the load is shared by the cores. 
I also tried squid with SMP but there are some restrictions (delay pool, identification - if I remember right -)  


> GreasySpoon or qlproxy seems to be the high profile ones. c-icap and
> Traffic Spicer seem to offer frameworks rather than pre-made filters
> <http://www.squid-cache.org/Misc/icap.html>
> 
> The interfaces GreasySpoon or qlproxy describe say easily scriptable to
> do filtering. Though filtering is a huge topic, so "easily" is up for
> interpretation.

Thanks, I will take a look 

> 
> And fiddling around with your customers content is very site-specific
> about what can or can't be done. Thus the frameworks and script engines
> being most high profile.
> 

Yes you are right, I wish that there is a change in E2, for me the ideal situation is SQUID (cache and identification) and a pool of E2 with ICAP
I guess that there is no hope for SquidGuard because it's very different, a redirector related with Squid


From tech at neosys.net  Sun Aug 23 21:27:05 2015
From: tech at neosys.net (Brandon Elliott)
Date: Sun, 23 Aug 2015 17:27:05 -0400
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
Message-ID: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>

Hello all,

I ran into another issue which I could not find any answers for online. In
testing the proxies, I found that bandwidth limiting was properly limited
for download using delay pools but upload bandwidth was allowed unlimited
access. So a single user could potentially use up all the bandwidth with a
large, fast upload.

How can I prevent this?

Thanks,

Brandon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150823/6e0d8bf9/attachment.htm>

From spider at smoothnet.org  Sun Aug 23 21:35:14 2015
From: spider at smoothnet.org (Nicolaas Hyatt)
Date: Sun, 23 Aug 2015 16:35:14 -0500
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
Message-ID: <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>

I don?t really know if squid supports this or not, but I can?t imagine why it would need to, with traffic control built into the linux kernel in a number of ways. For example the Linux Advanced Routing & Traffic Control (Chapter 9) (http://www.lartc.org/howto/lartc.qdisc.html) can easily help you acieve what you are wanting.

 

 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Brandon Elliott
Sent: Sunday, August 23, 2015 4:27 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] How to limit upload bandwidth in squid proxy?

 

Hello all,

I ran into another issue which I could not find any answers for online. In testing the proxies, I found that bandwidth limiting was properly limited for download using delay pools but upload bandwidth was allowed unlimited access. So a single user could potentially use up all the bandwidth with a large, fast upload.

How can I prevent this?

Thanks,

Brandon

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150823/b92ab665/attachment.htm>

From rousskov at measurement-factory.com  Sun Aug 23 23:33:17 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 23 Aug 2015 17:33:17 -0600
Subject: [squid-users] ssl_bump updates coming in 3.5.8
In-Reply-To: <55D6D326.4070305@treenet.co.nz>
References: <55D6D326.4070305@treenet.co.nz>
Message-ID: <55DA583D.5030405@measurement-factory.com>

On 08/21/2015 01:28 AM, Amos Jeffries wrote:

> Christos has managed (we think) to resolve a fairly major design issue
> that has been plaguing the 3.5 series peek-and-splice feature so far.
> (<http://wiki.squid-cache.org/Features/SslPeekAndSplice>)


Clarification: No major design issue has been resolved. The design has
not changed. We fixed the implementation to match the documented design.

I cannot come up with a specific previously-working configuration
example that our fix would break, but that does not mean such
configurations do not exist. If your ssl_bump peek or stare rule could
match at step #3, then you were in a danger zone: Our buggy code used to
incorrectly splice or bump (depending on various complex factors) when
such a match happens at step3. After the fix, such a match can never
happen: peek and stare rules are now correctly ignored during step3.

Here is an example of a configuration that was _not_ working reliably
before the fix (under certain atypical but realistic conditions such as
IE on Windows XP):

  ssl_bump peek all
  ssl_bump splice all

The above configuration should work as expected after the fix.


The change is not meant to resolve any assertions. However, since it
affects when/whether Squid splices or bumps, the change may affect the
asserting code as well.


Hope this clarifies,

Alex.



From haishitan at gmail.com  Mon Aug 24 00:17:06 2015
From: haishitan at gmail.com (hs tan)
Date: Mon, 24 Aug 2015 08:17:06 +0800
Subject: [squid-users] external_acl_type not working on Squid Cache: Version
	3.5.5
Message-ID: <CAM-w1tWMHiaM3_B1j+djB62QrxbYJi44XgPkCZdLZ8Rnw0jkTA@mail.gmail.com>

I have been trying to test squid but it doesn't seems to be working. The
closest example I studied are:

http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+12.+Authentication+Helpers/12.5+External+ACLs/
http://www.stress-free.co.nz/transparent_squid_authentication_to_edirectory
but none of it works.

>From the simple test, I did on the following:

The print "ERR" supposed to have an out put at the cache.log, but I din't
see anything appearing
Neither I change the "ERR" nor "OK", there is no effect on the access.
I just want a simple test, if set to print "ERR" then stop user to proceed,
if "OK" then proceed.

The error message in cache.log
2015/07/28 11:45:56 kid1| helperHandleRead: unexpected reply on channel 0
from mysql_log #Hlpr17 ''

squid.conf is:

auth_param basic program /usr/lib64/squid/basic_ldap_auth -v 3 -b
"dc=xxx,dc=edu.xx" -D "cn=Manager,dc=xxx,dc=edu.xx"  -w passwd -f uid=%s
ldap.xxx.edu.xx:389

acl ldap-auth proxy_auth REQUIRED
auth_param basic children 5
auth_param basic realm Web Proxy Server
auth_param basic credentialsttl 1 minute

external_acl_type mysql_log %SRC %LOGIN %{Host} /home/squid/quota_helper.pl
acl ex_log external mysql_log
http_access allow ex_log
....
http_access allow ldap-auth
http_access allow localnet
http_access allow localhost
http_access deny all
quota_helper.pl is:

#!/usr/bin/perl -wl

$|=1;
while(<STDIN>){
print "ERR";
}
[root at localhost ~]# squid -v shows:

Squid Cache: Version 3.5.5
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
'--enable-ssl-crtd' '--enable-icmp' '--with-aio'
'--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl'
'--with-openssl' '--with-pthreads' '--with-included-ltdl'
'--disable-arch-native' '--without-nettle'
'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fPIC'
'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
--enable-ltdl-convenience
[root at localhost ~]#
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/06e14b0f/attachment.htm>

From tech at neosys.net  Mon Aug 24 01:03:13 2015
From: tech at neosys.net (Brandon Elliott)
Date: Sun, 23 Aug 2015 21:03:13 -0400
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
 <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
Message-ID: <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>

Hi Nicolaas,

I read the article you pointed to, but it isn't applicable. I need to be
able to limit upload bandwidth PER USER just like squid does for download
bandwidth, per user.

QUOTE >> I don?t really know if squid supports this or not, but I can?t
imagine why it would need to, with traffic control built into the linux
kernel in a number of ways.

Somebody saw fit to put download bandwidth limiting into squid, so it makes
sense to support upload bandwidth limiting as well.

I am already using ncsa_auth to manage squid users. I need a solution that
doesn't involve using a second authentication just to limit upload
bandwidth per user.

Any ideas?

On Sun, Aug 23, 2015 at 5:35 PM, Nicolaas Hyatt <spider at smoothnet.org>
wrote:

> I don?t really know if squid supports this or not, but I can?t imagine why
> it would need to, with traffic control built into the linux kernel in a
> number of ways. For example the Linux Advanced Routing & Traffic Control
> (Chapter 9) (http://www.lartc.org/howto/lartc.qdisc.html) can easily help
> you acieve what you are wanting.
>
>
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Brandon Elliott
> *Sent:* Sunday, August 23, 2015 4:27 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] How to limit upload bandwidth in squid proxy?
>
>
>
> Hello all,
>
> I ran into another issue which I could not find any answers for online. In
> testing the proxies, I found that bandwidth limiting was properly limited
> for download using delay pools but upload bandwidth was allowed unlimited
> access. So a single user could potentially use up all the bandwidth with a
> large, fast upload.
>
> How can I prevent this?
>
> Thanks,
>
> Brandon
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
*Brandon Elliott*
CEO
---------------------

Office:  888.763.6797
http://www.neosys.net
2774 N Cobb Pkwy NW #244
Suite 109
Kennesaw, GA 30152
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150823/2d7140f6/attachment.htm>

From vdoctor at neuf.fr  Mon Aug 24 07:11:15 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 24 Aug 2015 00:11:15 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55D71B82.7020203@treenet.co.nz>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
Message-ID: <1440400275614-4672835.post@n4.nabble.com>

Hi Amos,
The patch is running since 3 days and seems working fine 
Can we expect the next squid build including the patch ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672835.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Aug 24 07:20:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 19:20:44 +1200
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
 <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
 <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>
Message-ID: <55DAC5CC.9050706@treenet.co.nz>

On 24/08/2015 1:03 p.m., Brandon Elliott wrote:
> Hi Nicolaas,
> 
> I read the article you pointed to, but it isn't applicable. I need to be
> able to limit upload bandwidth PER USER just like squid does for download
> bandwidth, per user.
> 
> QUOTE >> I don?t really know if squid supports this or not, but I can?t
> imagine why it would need to, with traffic control built into the linux
> kernel in a number of ways.
> 
> Somebody saw fit to put download bandwidth limiting into squid, so it makes
> sense to support upload bandwidth limiting as well.

Squid is *very* old software. Some of its features were literally part
of the founding days of the Internet.

Delay Pools is one that predates all modern QoS functionality. And is
quite limited in ways like the one you are hitting. If you can use the
system QoS do so, it is far more efficient.

> 
> I am already using ncsa_auth to manage squid users. I need a solution that
> doesn't involve using a second authentication just to limit upload
> bandwidth per user.
> 
> Any ideas?


client_delay_pools was added to meet this need. But be aware there is a
crash in there somewhere that has not been resolved.


Alternatively, Squid does have integration with the system QoS. The note
ACL type allows these to be set based on auth helper provided kv-pair
details; eg. user=Fred vs user=Bob.

You setup the system with all the QoS funtionality and Squid informs the
kernel what TOS/MASK classification to apply on the connection packets.
 <http://www.squid-cache.org/Doc/config/clientside_tos/>
 <http://www.squid-cache.org/Doc/config/clientside_mark/>

With client side there will always be inaccuracy on that auth based flow
control because the transaction is already underway by the time auth
gets invoked.

Amos



From squid3 at treenet.co.nz  Mon Aug 24 08:49:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 20:49:02 +1200
Subject: [squid-users] Using Squid as forward http proxy failing to
 complete request?
In-Reply-To: <CAP3=H7uOYss84FM_ce+zOry90T6qeCpOs5kWTfgNDmfV_Azcsw@mail.gmail.com>
References: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
 <55D74A67.3000600@treenet.co.nz>
 <CAP3=H7sS7bVM3CEWVfF6pRkeeNbFhXp8O_HDv0XmE==A4RZdQw@mail.gmail.com>
 <55D758B1.1090309@treenet.co.nz>
 <CAP3=H7uOYss84FM_ce+zOry90T6qeCpOs5kWTfgNDmfV_Azcsw@mail.gmail.com>
Message-ID: <55DADA7E.5030105@treenet.co.nz>

On 24/08/2015 6:41 p.m., asad wrote:
> Amos, sorry for late reply. I wanted to test the config in office
> environment, good news is that it worked:). Only two issues:-
> 
> 1) the client is repeatedly prompted for credentials when even in
> background the website is loading.
> 

Thats the way HTTP authentication is designed. Nothing can happen
without authentication credentials.

And HTTP is not aware what is displayed on the users screen (or not).
There is not even a real concept of "user" or "screen". Just clients and
servers messaging each other.


> 2) It doesn't work for ssl websites. Does squid do MITM?
> 

Yes, but MITM will not help you here.

Because the only case where HTTP authentication is possible with MITM is
when intercepting traffic destined to a forward-proxy. Your setup is
very close to that edge case, but the HTTPS CONNECT requests will still
be having the same user-end behaviour in either format.

There are three suspects for CONNECT not authentiation. In order of
likelihood they are;

1) on recent Squid you are probably running into one of the edge cases
where our fix for CVE-2015-5400 causes unwanted blocking. That makes two
that I've now found since the security fix went out.

But its worth double-checking that suspicion. Please check if your proxy
is actually relaying the CONNECT to its parent. And when the parent
replies 407 if proxy is sending the client a 502 or 407.

2) the client software just being broken when its asked to authenticate
for a CONNECT.

3) Needing to configure "nonheirarchical_direct off" so the CONNECT
actually get sent to the parent. This shows up as clients receiving 503
status responses generated by your proxy after CONNECT.

You can see the details clients are getting with this:
 squidclient -m CONNECT example.com:443


[pPS. *ssl* sites not working is good because SSLv2 and SSLv3 are
terribly broken security. *HTTPS* on the other hand ...]

Amos



From squid3 at treenet.co.nz  Mon Aug 24 08:51:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 20:51:11 +1200
Subject: [squid-users] external_acl_type not working on Squid Cache:
 Version 3.5.5
In-Reply-To: <CAM-w1tWMHiaM3_B1j+djB62QrxbYJi44XgPkCZdLZ8Rnw0jkTA@mail.gmail.com>
References: <CAM-w1tWMHiaM3_B1j+djB62QrxbYJi44XgPkCZdLZ8Rnw0jkTA@mail.gmail.com>
Message-ID: <55DADAFF.70501@treenet.co.nz>

On 24/08/2015 12:17 p.m., hs tan wrote:
> I have been trying to test squid but it doesn't seems to be working. The
> closest example I studied are:
> 
> http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+12.+Authentication+Helpers/12.5+External+ACLs/
> http://www.stress-free.co.nz/transparent_squid_authentication_to_edirectory
> but none of it works.
> 
> From the simple test, I did on the following:
> 
> The print "ERR" supposed to have an out put at the cache.log, but I din't
> see anything appearing
> Neither I change the "ERR" nor "OK", there is no effect on the access.
> I just want a simple test, if set to print "ERR" then stop user to proceed,
> if "OK" then proceed.
> 
> The error message in cache.log
> 2015/07/28 11:45:56 kid1| helperHandleRead: unexpected reply on channel 0
> from mysql_log #Hlpr17 ''
> 

"on channel 0" means your Squid is using concurrency channels when
talking to this helper.

The helper protocol syntax is documented here:
<http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29>

** Be careful about emitting unnecessarys newlines. **

Amos



From Markus.Preis at berge-meer.de  Mon Aug 24 08:51:50 2015
From: Markus.Preis at berge-meer.de (Markus.Preis at berge-meer.de)
Date: Mon, 24 Aug 2015 10:51:50 +0200
Subject: [squid-users] TCP_MISS/429
Message-ID: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>

Hi everyone,

this is the first time i use a mailinglist, so i hope i don't mess things
up.

I encountered a very strange TPC_MISS/429 in my squid access.log.
1440405573.871    285 10.2.0.5 TCP_MISS/429 255 GET http://crushftp.com/
Preism HIER_DIRECT/104.236.78.254 -

I've searched the web, but nothing fount yet. The Webpage is blank. When i
try from home, everything works fine.
http://wiki.squid-cache.org/SquidFaq/SquidLogs#access.log does not list
the status code.

I'm not sure if this error is maybe: "Too Many Requests. The user has sent
too many requests in a given amount of time"
If it is, i dunno why. I got this error right when i tried to open the
webpage for the very first time.
Hope someone can help me.

best regards
Markus

___________________________________________________________________________

Markus Preis


Berge & Meer Touristik GmbH
Andr?estrasse 27
56578 Rengsdorf

Tel: +49 2634 960 1077

Fax: +49 2634 967 5555
Mailto:Markus.Preis at berge-meer.de
http://www.berge-meer.de

Melden Sie sich unter https://www.berge-meer.de/newsletter?f=1762 f?r unseren Newsletter an und erhalten Sie jede Woche kostenlos die aktuellsten Reiseangebote

Berge & Meer Touristik GmbH
Handelsregistergericht: Montabaur/HRB 13067
Sitz: Rengsdorf
Gesch?ftsf?hrer: Thomas Klein (Vorsitzender), Tim Dunker, Marcel Mayer

Vertraulichkeitshinweis: 
Diese E-Mail enthaelt vertrauliche und/oder rechtlich geschuetzte Informationen.
Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrtuemlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail.
Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser Mail ist nicht gestattet.

Confidential Note: 
This e-mail may contain confidential and/or privileged information.
If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and destroy this e-mail. 
Any unauthorised copying, disclosure or distribution of the material in this e-mail is strictly forbidden.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/cb552318/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 24 09:05:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 21:05:09 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440400275614-4672835.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com>
Message-ID: <55DADE45.2010905@treenet.co.nz>

On 24/08/2015 7:11 p.m., Stakres wrote:
> Hi Amos,
> The patch is running since 3 days and seems working fine 
> Can we expect the next squid build including the patch ?

Yes, or a close variant.

Some details though please (if you can):

Has it had to deal with an over-100% filled event yet?
 if so was there any noticible effects such as traffic slowdown or HDD
load peaks?
 if not, how close to or far past high-water is it getting as compared
to before?

The rate-of-purge tuning is arbitrary number changes in the new patch,
and I'm not aware of any actual measurements for the previous numbers
either. So I'm hoping its not just bumped up out of your speed range and
waiting quietly for somebody else at higher speeds.

Amos



From vdoctor at neuf.fr  Mon Aug 24 09:23:46 2015
From: vdoctor at neuf.fr (FredT)
Date: Mon, 24 Aug 2015 02:23:46 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55DADE45.2010905@treenet.co.nz>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
Message-ID: <1440408226749-4672841.post@n4.nabble.com>

Hi Amos,
Since the patch, the Percent Used is decreasing correctly, no new 100% at
the moment.

Why not using the Req/min as the value to use in the cleaning when the
cache_swap_low/high are reached ?
ex: Average HTTP requests per minute since start:   19232.8
Btw, when the cache_swap_low is reached, the squid could use 25% of the
req/min for the cleaning and with the cache_swap_high the squid could use
100%.
This is an example, there are tons of data in the squid we could use 

Or, we could have special options to define by ourselves the number of
objects to clean, example:
- cache_swap_low_del_object 256
- cache_swap_high_del_object 1024

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672841.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Aug 24 10:45:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 22:45:49 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440408226749-4672841.post@n4.nabble.com>
References: <1439915941519-4672750.post@n4.nabble.com>
 <55D370A0.4060809@treenet.co.nz> <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
 <1440408226749-4672841.post@n4.nabble.com>
Message-ID: <55DAF5DD.7060406@treenet.co.nz>

On 24/08/2015 9:23 p.m., FredT wrote:
> Hi Amos,
> Since the patch, the Percent Used is decreasing correctly, no new 100% at
> the moment.

Oh well. Good signs at least for current needs.

If you are open to the experiment
<http://bugs.squid-cache.org/show_bug.cgi?id=2448> indicates a good way
to force the over-100% issue to happen on demand. Just cut a GB off the
current cache-used size and reconfigure. (I do fully understand not
wanting to see the effects of that in a production machine. So your choice.)

This patch will make that bugs issue more visible since it will outright
pause all service while the resize happens. Rather than just slowing
things down randomly as it grabs CPU cycles between traffic I/O.
 But clear predictable situation is better anyway. So is acceptable if
pause is the only bad side effect.


> 
> Why not using the Req/min as the value to use in the cleaning when the
> cache_swap_low/high are reached ?
> ex: Average HTTP requests per minute since start:   19232.8
> Btw, when the cache_swap_low is reached, the squid could use 25% of the
> req/min for the cleaning and with the cache_swap_high the squid could use
> 100%.
> This is an example, there are tons of data in the squid we could use 

That is what I hope for long-term. But the value you see in those
reports is not a narrow point value. It is an average over the entire
uptime of the Squid. Calculating it requires clock lookups and a
histogram of past data. A bit too complex for this loop which runs every
single second.

We simply dont have any nice metric yet that says "done N requests in
the past 1 second". There are a few things in Squid that could make good
use of it :-).


> 
> Or, we could have special options to define by ourselves the number of
> objects to clean, example:
> - cache_swap_low_del_object 256
> - cache_swap_high_del_object 1024
> 

I have been considering both, and some others and the TODO is getting a
bit longer.

But its unlikely to happen in the coming weekends release. So far just
the two bug fixes, and rate increase.

Amos



From squid3 at treenet.co.nz  Mon Aug 24 11:18:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2015 23:18:21 +1200
Subject: [squid-users] TCP_MISS/429
In-Reply-To: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>
References: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>
Message-ID: <55DAFD7D.8020701@treenet.co.nz>

On 24/08/2015 8:51 p.m., Markus.Preis wrote:
> Hi everyone,
> 
> this is the first time i use a mailinglist, so i hope i don't mess things
> up.

No problem. Welcome.

> 
> I encountered a very strange TPC_MISS/429 in my squid access.log.
> 1440405573.871    285 10.2.0.5 TCP_MISS/429 255 GET http://crushftp.com/
> Preism HIER_DIRECT/104.236.78.254 -
> 
> I've searched the web, but nothing fount yet. The Webpage is blank. When i
> try from home, everything works fine.
> http://wiki.squid-cache.org/SquidFaq/SquidLogs#access.log does not list
> the status code.


see <http://tools.ietf.org/html/rfc6585#section-4> for what it means.

Squid does not generate that code itself. Although it is possible for an
admin to use "deny_info 429:ERR_BLANK someAcl" and have setup a custom
rate-limiting ACL helper.

In this case I think its just the normal traffic event with the origin
server 104.236.78.254 generating it.

Amos



From gkinkie at gmail.com  Mon Aug 24 11:52:23 2015
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 24 Aug 2015 13:52:23 +0200
Subject: [squid-users] Mac OS X Updates
In-Reply-To: <CAKNtY_yxPfFJhuP__sbWi_p6UNkyCuz=eFSkUQKeB_7=9wg+0w@mail.gmail.com>
References: <CAKNtY_yxPfFJhuP__sbWi_p6UNkyCuz=eFSkUQKeB_7=9wg+0w@mail.gmail.com>
Message-ID: <CA+Y8hcO_D8EF323UC8aTYxjfnD3O1AAJY30t5oWP2xhKO27tZQ@mail.gmail.com>

Hi John,
  according to the article you link to, it's not possible to cache these
updates: Apple puts some effort as a conscious choice to make it so.

  Updates for older versions of MacOS may be over HTTP, newer ones are over
HTTPs over port 443 and and dynamically-generated ports. HTTP could be
cached, https cannot without ssl-bump/peek-n-splice (SSL man-in-the-middle).
  The wording of the article seems to suggest that the list of trusted
issuers of certificates for the https service is not the same as the
system's CA root certificate store but is probably locked to Apple's. This
means that also SSL MITM is not possible, by design.


On Wed, Aug 19, 2015 at 9:20 PM, John Pearson <johnpearson555 at gmail.com>
wrote:

> Anyone have Mac OS X update caching working ? Without doing a SSL bump. I
> think they are hosted through https (
> https://support.apple.com/en-us/HT202943 )
>
> Thanks!
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/ab4f5ae4/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 24 12:29:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Aug 2015 00:29:14 +1200
Subject: [squid-users] refresh_pattern by type mime
In-Reply-To: <1440082601318-4672793.post@n4.nabble.com>
References: <1440082601318-4672793.post@n4.nabble.com>
Message-ID: <55DB0E1A.1050400@treenet.co.nz>

On 21/08/2015 2:56 a.m., Stakres wrote:
> Hi All,
> 
> There is an existing case in the bugzilla
> (http://bugs.squid-cache.org/show_bug.cgi?id=1913) speaking about this
> request and it seems a good idea:
> refresh_pattern by type mime
> 
> It would be very nice and cool to have this feature in squid to define
> different min/max time per mime.
> We could define script/html/css/etc... with a short time,
> images/videos/audio/application/etc... with a long time...
> 
> Squid team, what is your opinion about that ?
> Maybe already in the roadmap for the next 3.5.x build or the 4.x ?
> 

Being tracked as <http://bugs.squid-cache.org/show_bug.cgi?id=1913>. But
not on the roadmap with any priority AFAIK.

My opinion (cant speak for anyone else on this);

It has some useful merit, but the bugs making the algorithm
non-compliant with RFC7234 have higher priority when I look at that area
of the code.

Amos



From vdoctor at neuf.fr  Mon Aug 24 13:10:59 2015
From: vdoctor at neuf.fr (FredT)
Date: Mon, 24 Aug 2015 06:10:59 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55DAF5DD.7060406@treenet.co.nz>
References: <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
 <1440408226749-4672841.post@n4.nabble.com> <55DAF5DD.7060406@treenet.co.nz>
Message-ID: <1440421859169-4672846.post@n4.nabble.com>

Amos,

Uploading the patched squid took time to be agreed by the client, sorry but
the server is in production and we cannot take the risk to see if the action
will crash or not the squid, i don't want to lose this client...

If you fix in the next release the cache_swap_low/high taking care the
Percent Used and the Filemap, it would be a good solution at the moment 

Keep us posted...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672846.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From zimurgy at gmail.com  Mon Aug 24 14:05:13 2015
From: zimurgy at gmail.com (JL)
Date: Mon, 24 Aug 2015 10:05:13 -0400
Subject: [squid-users] Cache Permission Errors
Message-ID: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>

Hi,

Sorry if this is a duplicate message, I am not sure it went through
initially. I am having an issue with my caching drives that I can't seem to
pinpoint the problem. I have 4 drives to be used for caching, they are ext3
filesystems mounted like so. I am running CentOS 7.

/dev/sde1 on /var/spool/squid4 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdd1 on /var/spool/squid3 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdc1 on /var/spool/squid2 type ext3 (rw,noatime,seclabel,data=ordered)
/dev/sdb1 on /var/spool/squid1 type ext3 (rw,noatime,seclabel,data=ordered)

I set the cache user to be squid in the squid.conf and I reference the
drives for caching.

cache_dir aufs /var/spool/squid1 460800 32 512
cache_dir aufs /var/spool/squid2 460800 32 512
cache_dir aufs /var/spool/squid3 460800 32 512
cache_dir aufs /var/spool/squid4 460800 32 512

ls -l of the /var/spool shows proper perms, they are propagated.

drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid1
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid2
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid3
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid4

The cache dirs were successfully created using squid -z and all of the
subfolders were generated. Squid starts and everything appears to be great.
Until I monitor the cache.log, then I see the errors begin.

Some like this.

2015/08/16 03:41:55 kid1| /var/spool/squid1/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:10 kid1| /var/spool/squid2/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:25 kid1| /var/spool/squid3/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:40 kid1| /var/spool/squid4/cache/19/8B: (13) Permission
denied
2015/08/16 03:42:55 kid1| /var/spool/squid1/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:10 kid1| /var/spool/squid2/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:25 kid1| /var/spool/squid3/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:40 kid1| /var/spool/squid4/cache/1A/8B: (13) Permission
denied
2015/08/16 03:43:55 kid1| /var/spool/squid1/cache/1B/8B: (13) Permission
denied

Some like this.

2015/08/21 10:02:13 kid1| /var/spool/squid2/17/1FF: (13) Permission denied
2015/08/21 10:02:13 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:13 kid1| /var/spool/squid4/00/00/00000000
2015/08/21 10:02:28 kid1| /var/spool/squid3/17/1FF: (13) Permission denied
2015/08/21 10:02:32 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:32 kid1| /var/spool/squid4/00/00/00000001
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000002
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000003
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000004
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000005
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000006
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000007

The folders remain empty of course, so no caching is taking place.

I am truly stumped as to what I may be doing wrong since squid has enough
perms to create the dirs but not enough to save to them.

Any help would be great.

Thanks!
Zim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/10d0964d/attachment.htm>

From yvoinov at gmail.com  Mon Aug 24 15:46:14 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 24 Aug 2015 21:46:14 +0600
Subject: [squid-users] refresh_pattern by type mime
In-Reply-To: <55DB0E1A.1050400@treenet.co.nz>
References: <1440082601318-4672793.post@n4.nabble.com>
 <55DB0E1A.1050400@treenet.co.nz>
Message-ID: <55DB3C46.8060005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Join to the wish. It would be very cool functionality.

24.08.15 18:29, Amos Jeffries ?????:
> On 21/08/2015 2:56 a.m., Stakres wrote:
>> Hi All,
>>
>> There is an existing case in the bugzilla
>> (http://bugs.squid-cache.org/show_bug.cgi?id=1913) speaking about this
>> request and it seems a good idea:
>> refresh_pattern by type mime
>>
>> It would be very nice and cool to have this feature in squid to define
>> different min/max time per mime.
>> We could define script/html/css/etc... with a short time,
>> images/videos/audio/application/etc... with a long time...
>>
>> Squid team, what is your opinion about that ?
>> Maybe already in the roadmap for the next 3.5.x build or the 4.x ?
>>
>
> Being tracked as <http://bugs.squid-cache.org/show_bug.cgi?id=1913>. But
> not on the roadmap with any priority AFAIK.
>
> My opinion (cant speak for anyone else on this);
>
> It has some useful merit, but the bugs making the algorithm
> non-compliant with RFC7234 have higher priority when I look at that area
> of the code.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV2zxGAAoJENNXIZxhPexGyEYH/38HM2YATTE962q07+XIKSCG
66nTA1vK+Jv7S/obzMuZ0N9GBzjGtoszQ7jGXczHPLnd6y75a/aDJfXEAgQJ35rd
KuRbCde8zcmstuJGluzBh9ySXBWPUV2fmrJO33tqP9ZikFwj2F/emTYtdbwSxDfS
oQXfqF4JyWFubOinha3pI3ZXilpl7Q9KO7ZahqCCqNA4CMmdRDJl6VSaIhg64rkk
+iZkqpfXzdG4tY4iFudaEPykqtvzTqmX5nCRlA8BbCAAv3N8IaWEbB047USMyq3L
o+ps/zXYwukL1ij+vKqI6DJcxZG/JeOyok0484XHbZUtdiZg8XOol31jIL9P+Bo=
=16EM
-----END PGP SIGNATURE-----



From johnpearson555 at gmail.com  Mon Aug 24 16:04:47 2015
From: johnpearson555 at gmail.com (John Pearson)
Date: Mon, 24 Aug 2015 09:04:47 -0700
Subject: [squid-users] Mac OS X Updates
In-Reply-To: <CA+Y8hcO_D8EF323UC8aTYxjfnD3O1AAJY30t5oWP2xhKO27tZQ@mail.gmail.com>
References: <CAKNtY_yxPfFJhuP__sbWi_p6UNkyCuz=eFSkUQKeB_7=9wg+0w@mail.gmail.com>
 <CA+Y8hcO_D8EF323UC8aTYxjfnD3O1AAJY30t5oWP2xhKO27tZQ@mail.gmail.com>
Message-ID: <CAKNtY_wJs5x-KnfipsLvoXPniPYmVCcPeAbeBzr3ep8NeHq4PQ@mail.gmail.com>

Thanks! That's what I figured. I wanted to see if anyone in the community
had better ideas or another way.

On Mon, Aug 24, 2015 at 4:52 AM, Kinkie <gkinkie at gmail.com> wrote:

> Hi John,
>   according to the article you link to, it's not possible to cache these
> updates: Apple puts some effort as a conscious choice to make it so.
>
>   Updates for older versions of MacOS may be over HTTP, newer ones are
> over HTTPs over port 443 and and dynamically-generated ports. HTTP could be
> cached, https cannot without ssl-bump/peek-n-splice (SSL man-in-the-middle).
>   The wording of the article seems to suggest that the list of trusted
> issuers of certificates for the https service is not the same as the
> system's CA root certificate store but is probably locked to Apple's. This
> means that also SSL MITM is not possible, by design.
>
>
> On Wed, Aug 19, 2015 at 9:20 PM, John Pearson <johnpearson555 at gmail.com>
> wrote:
>
>> Anyone have Mac OS X update caching working ? Without doing a SSL bump. I
>> think they are hosted through https (
>> https://support.apple.com/en-us/HT202943 )
>>
>> Thanks!
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
> --
>     Francesco
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/50a547dd/attachment.htm>

From eliezer at ngtech.co.il  Mon Aug 24 16:10:27 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 24 Aug 2015 19:10:27 +0300
Subject: [squid-users] external_acl_type not working on Squid Cache:
 Version 3.5.5
In-Reply-To: <CAM-w1tWMHiaM3_B1j+djB62QrxbYJi44XgPkCZdLZ8Rnw0jkTA@mail.gmail.com>
References: <CAM-w1tWMHiaM3_B1j+djB62QrxbYJi44XgPkCZdLZ8Rnw0jkTA@mail.gmail.com>
Message-ID: <55DB41F3.9020401@ngtech.co.il>

Two things:
  - take a look at this helper to see something that works: 
http://bazaar.launchpad.net/~squid/squid/trunk/view/head:/helpers/storeid_rewrite/file/storeid_file_rewrite.pl.in
  - newlines are important in the communication between squid and the 
helper.
perl's "print" is not sending a new line character even if needed, you 
need to put it there.

Since squid 3.5.X there is a string "%un" which sends the authenticated 
user name to the helper without triggering a authentication if not needed.

All The Bests,
Eliezer

On 24/08/2015 03:17, hs tan wrote:
> I have been trying to test squid but it doesn't seems to be working. The
> closest example I studied are:
>
> http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+12.+Authentication+Helpers/12.5+External+ACLs/
> http://www.stress-free.co.nz/transparent_squid_authentication_to_edirectory
> but none of it works.
>
>  From the simple test, I did on the following:
>
> The print "ERR" supposed to have an out put at the cache.log, but I din't
> see anything appearing
> Neither I change the "ERR" nor "OK", there is no effect on the access.
> I just want a simple test, if set to print "ERR" then stop user to proceed,
> if "OK" then proceed.
>
> The error message in cache.log
> 2015/07/28 11:45:56 kid1| helperHandleRead: unexpected reply on channel 0
> from mysql_log #Hlpr17 ''
>
> squid.conf is:
>
> auth_param basic program /usr/lib64/squid/basic_ldap_auth -v 3 -b
> "dc=xxx,dc=edu.xx" -D "cn=Manager,dc=xxx,dc=edu.xx"  -w passwd -f uid=%s
> ldap.xxx.edu.xx:389
>
> acl ldap-auth proxy_auth REQUIRED
> auth_param basic children 5
> auth_param basic realm Web Proxy Server
> auth_param basic credentialsttl 1 minute
>
> external_acl_type mysql_log %SRC %LOGIN %{Host} /home/squid/quota_helper.pl
> acl ex_log external mysql_log
> http_access allow ex_log
> ....
> http_access allow ldap-auth
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> quota_helper.pl is:
>
> #!/usr/bin/perl -wl
>
> $|=1;
> while(<STDIN>){
> print "ERR";
> }
> [root at localhost ~]# squid -v shows:
>
> Squid Cache: Version 3.5.5
> Service Name: squid
> configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--exec_prefix=/usr'
> '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=$(localstatedir)/log/squid'
> '--with-pidfile=$(localstatedir)/run/squid.pid'
> '--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
> '--enable-auth'
> '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter'
> '--enable-removal-policies=heap,lru' '--enable-snmp'
> '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
> '--enable-ssl-crtd' '--enable-icmp' '--with-aio'
> '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl'
> '--with-openssl' '--with-pthreads' '--with-included-ltdl'
> '--disable-arch-native' '--without-nettle'
> 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
> 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
> -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> -fPIC'
> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> --enable-ltdl-convenience
> [root at localhost ~]#
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From xen at dds.nl  Mon Aug 24 16:47:09 2015
From: xen at dds.nl (Xen)
Date: Mon, 24 Aug 2015 18:47:09 +0200
Subject: [squid-users] Cache Permission Errors
In-Reply-To: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>
References: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>
Message-ID: <55DB4A8D.7090204@dds.nl>

Are you terribly certain the owner:group of the generated log files is 
the same as those of the cache dirs?.

That is a way to check for sure what user it is running under. I was 
just compiling the latest 3.5.x when it would complain, and I had 
forgotten to compile it with --with-default-user=squid.


On 08/24/2015 04:05 PM, JL wrote:
> Hi,
>
> Sorry if this is a duplicate message, I am not sure it went through 
> initially. I am having an issue with my caching drives that I can't 
> seem to pinpoint the problem. I have 4 drives to be used for caching, 
> they are ext3 filesystems mounted like so. I am running CentOS 7.
>
> /dev/sde1 on /var/spool/squid4 type ext3 
> (rw,noatime,seclabel,data=ordered)
> /dev/sdd1 on /var/spool/squid3 type ext3 
> (rw,noatime,seclabel,data=ordered)
> /dev/sdc1 on /var/spool/squid2 type ext3 
> (rw,noatime,seclabel,data=ordered)
> /dev/sdb1 on /var/spool/squid1 type ext3 
> (rw,noatime,seclabel,data=ordered)
>
> I set the cache user to be squid in the squid.conf and I reference the 
> drives for caching.
>
> cache_dir aufs /var/spool/squid1 460800 32 512
> cache_dir aufs /var/spool/squid2 460800 32 512
> cache_dir aufs /var/spool/squid3 460800 32 512
> cache_dir aufs /var/spool/squid4 460800 32 512
>
> ls -l of the /var/spool shows proper perms, they are propagated.
>
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid1
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid2
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid3
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid4
>
> The cache dirs were successfully created using squid -z and all of the 
> subfolders were generated. Squid starts and everything appears to be 
> great. Until I monitor the cache.log, then I see the errors begin.
>
> Some like this.
>
> 2015/08/16 03:41:55 kid1| /var/spool/squid1/cache/19/8B: (13) 
> Permission denied
> 2015/08/16 03:42:10 kid1| /var/spool/squid2/cache/19/8B: (13) 
> Permission denied
> 2015/08/16 03:42:25 kid1| /var/spool/squid3/cache/19/8B: (13) 
> Permission denied
> 2015/08/16 03:42:40 kid1| /var/spool/squid4/cache/19/8B: (13) 
> Permission denied
> 2015/08/16 03:42:55 kid1| /var/spool/squid1/cache/1A/8B: (13) 
> Permission denied
> 2015/08/16 03:43:10 kid1| /var/spool/squid2/cache/1A/8B: (13) 
> Permission denied
> 2015/08/16 03:43:25 kid1| /var/spool/squid3/cache/1A/8B: (13) 
> Permission denied
> 2015/08/16 03:43:40 kid1| /var/spool/squid4/cache/1A/8B: (13) 
> Permission denied
> 2015/08/16 03:43:55 kid1| /var/spool/squid1/cache/1B/8B: (13) 
> Permission denied
>
> Some like this.
>
> 2015/08/21 10:02:13 kid1| /var/spool/squid2/17/1FF: (13) Permission denied
> 2015/08/21 10:02:13 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:13 kid1| /var/spool/squid4/00/00/00000000
> 2015/08/21 10:02:28 kid1| /var/spool/squid3/17/1FF: (13) Permission denied
> 2015/08/21 10:02:32 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:32 kid1| /var/spool/squid4/00/00/00000001
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000002
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000003
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000004
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000005
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000006
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) 
> Permission denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000007
>
> The folders remain empty of course, so no caching is taking place.
>
> I am truly stumped as to what I may be doing wrong since squid has 
> enough perms to create the dirs but not enough to save to them.
>
> Any help would be great.
>
> Thanks!
> Zim
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/6a0217d6/attachment.htm>

From zimurgy at gmail.com  Mon Aug 24 17:00:54 2015
From: zimurgy at gmail.com (JL)
Date: Mon, 24 Aug 2015 13:00:54 -0400
Subject: [squid-users] Cache Permission Errors
In-Reply-To: <55DB4A8D.7090204@dds.nl>
References: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>
 <55DB4A8D.7090204@dds.nl>
Message-ID: <CAPPTbuo4rJNEaDrrM6Cy98LPj2qTGPkHWDiVGsgomkygyXys5w@mail.gmail.com>

Hi Xen,

Thanks for the feedback. I can confirm both the --with-default-user=squid
is present in squid -v and the perms of the logs are indeed squid user.

Regards,
Zim

On Mon, Aug 24, 2015 at 12:47 PM, Xen <xen at dds.nl> wrote:

> Are you terribly certain the owner:group of the generated log files is the
> same as those of the cache dirs?.
>
> That is a way to check for sure what user it is running under. I was just
> compiling the latest 3.5.x when it would complain, and I had forgotten to
> compile it with --with-default-user=squid.
>
>
>
> On 08/24/2015 04:05 PM, JL wrote:
>
> Hi,
>
> Sorry if this is a duplicate message, I am not sure it went through
> initially. I am having an issue with my caching drives that I can't seem to
> pinpoint the problem. I have 4 drives to be used for caching, they are ext3
> filesystems mounted like so. I am running CentOS 7.
>
> /dev/sde1 on /var/spool/squid4 type ext3 (rw,noatime,seclabel,data=ordered)
> /dev/sdd1 on /var/spool/squid3 type ext3 (rw,noatime,seclabel,data=ordered)
> /dev/sdc1 on /var/spool/squid2 type ext3 (rw,noatime,seclabel,data=ordered)
> /dev/sdb1 on /var/spool/squid1 type ext3 (rw,noatime,seclabel,data=ordered)
>
> I set the cache user to be squid in the squid.conf and I reference the
> drives for caching.
>
> cache_dir aufs /var/spool/squid1 460800 32 512
> cache_dir aufs /var/spool/squid2 460800 32 512
> cache_dir aufs /var/spool/squid3 460800 32 512
> cache_dir aufs /var/spool/squid4 460800 32 512
>
> ls -l of the /var/spool shows proper perms, they are propagated.
>
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid1
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid2
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid3
> drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid4
>
> The cache dirs were successfully created using squid -z and all of the
> subfolders were generated. Squid starts and everything appears to be great.
> Until I monitor the cache.log, then I see the errors begin.
>
> Some like this.
>
> 2015/08/16 03:41:55 kid1| /var/spool/squid1/cache/19/8B: (13) Permission
> denied
> 2015/08/16 03:42:10 kid1| /var/spool/squid2/cache/19/8B: (13) Permission
> denied
> 2015/08/16 03:42:25 kid1| /var/spool/squid3/cache/19/8B: (13) Permission
> denied
> 2015/08/16 03:42:40 kid1| /var/spool/squid4/cache/19/8B: (13) Permission
> denied
> 2015/08/16 03:42:55 kid1| /var/spool/squid1/cache/1A/8B: (13) Permission
> denied
> 2015/08/16 03:43:10 kid1| /var/spool/squid2/cache/1A/8B: (13) Permission
> denied
> 2015/08/16 03:43:25 kid1| /var/spool/squid3/cache/1A/8B: (13) Permission
> denied
> 2015/08/16 03:43:40 kid1| /var/spool/squid4/cache/1A/8B: (13) Permission
> denied
> 2015/08/16 03:43:55 kid1| /var/spool/squid1/cache/1B/8B: (13) Permission
> denied
>
> Some like this.
>
> 2015/08/21 10:02:13 kid1| /var/spool/squid2/17/1FF: (13) Permission denied
> 2015/08/21 10:02:13 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:13 kid1| /var/spool/squid4/00/00/00000000
> 2015/08/21 10:02:28 kid1| /var/spool/squid3/17/1FF: (13) Permission denied
> 2015/08/21 10:02:32 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:32 kid1| /var/spool/squid4/00/00/00000001
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000002
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000003
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000004
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000005
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000006
> 2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission
> denied
> 2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000007
>
> The folders remain empty of course, so no caching is taking place.
>
> I am truly stumped as to what I may be doing wrong since squid has enough
> perms to create the dirs but not enough to save to them.
>
> Any help would be great.
>
> Thanks!
> Zim
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150824/68ac489d/attachment.htm>

From Kuntal_Basak at bnz.co.nz  Mon Aug 24 21:41:49 2015
From: Kuntal_Basak at bnz.co.nz (Kuntal_Basak at bnz.co.nz)
Date: Tue, 25 Aug 2015 09:41:49 +1200
Subject: [squid-users] Cache Permission Errors
In-Reply-To: <CAPPTbuo4rJNEaDrrM6Cy98LPj2qTGPkHWDiVGsgomkygyXys5w@mail.gmail.com>
References: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>
 <55DB4A8D.7090204@dds.nl>
 <CAPPTbuo4rJNEaDrrM6Cy98LPj2qTGPkHWDiVGsgomkygyXys5w@mail.gmail.com>
Message-ID: <OF59591ABA.FA2C0544-ONCC257EAB.007720DA-CC257EAB.00772F59@nab.com.au>

Could you please un-subscribe me ?

Cheers,

Kuntal
Senior Infrastructure Architecture and Design Specialist
Infrastructure Architecture and Design
Bank of New Zealand

DDI: 04-474 6722
Mobile: 021-2408034 

?Success is not final, failure is not fatal: it is the courage to continue 
that counts.? - Winston Churchill 



From:   JL <zimurgy at gmail.com>
To:     Xen <xen at dds.nl>, 
Cc:     squid-users at lists.squid-cache.org
Date:   25/08/2015 05:02 a.m.
Subject:        Re: [squid-users] Cache Permission Errors
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org>



Hi Xen,

Thanks for the feedback. I can confirm both the --with-default-user=squid 
is present in squid -v and the perms of the logs are indeed squid user.

Regards,
Zim

On Mon, Aug 24, 2015 at 12:47 PM, Xen <xen at dds.nl> wrote:
Are you terribly certain the owner:group of the generated log files is the 
same as those of the cache dirs?.

That is a way to check for sure what user it is running under. I was just 
compiling the latest 3.5.x when it would complain, and I had forgotten to 
compile it with --with-default-user=squid.



On 08/24/2015 04:05 PM, JL wrote:
Hi, 

Sorry if this is a duplicate message, I am not sure it went through 
initially. I am having an issue with my caching drives that I can't seem 
to pinpoint the problem. I have 4 drives to be used for caching, they are 
ext3 filesystems mounted like so. I am running CentOS 7.

/dev/sde1 on /var/spool/squid4 type ext3 
(rw,noatime,seclabel,data=ordered)
/dev/sdd1 on /var/spool/squid3 type ext3 
(rw,noatime,seclabel,data=ordered)
/dev/sdc1 on /var/spool/squid2 type ext3 
(rw,noatime,seclabel,data=ordered)
/dev/sdb1 on /var/spool/squid1 type ext3 
(rw,noatime,seclabel,data=ordered)

I set the cache user to be squid in the squid.conf and I reference the 
drives for caching.

cache_dir aufs /var/spool/squid1 460800 32 512
cache_dir aufs /var/spool/squid2 460800 32 512
cache_dir aufs /var/spool/squid3 460800 32 512
cache_dir aufs /var/spool/squid4 460800 32 512

ls -l of the /var/spool shows proper perms, they are propagated.

drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid1
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid2
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid3
drwxr-x---. 34 squid squid 4096 Aug 21 10:01 squid4

The cache dirs were successfully created using squid -z and all of the 
subfolders were generated. Squid starts and everything appears to be 
great. Until I monitor the cache.log, then I see the errors begin.

Some like this.

2015/08/16 03:41:55 kid1| /var/spool/squid1/cache/19/8B: (13) Permission 
denied
2015/08/16 03:42:10 kid1| /var/spool/squid2/cache/19/8B: (13) Permission 
denied
2015/08/16 03:42:25 kid1| /var/spool/squid3/cache/19/8B: (13) Permission 
denied
2015/08/16 03:42:40 kid1| /var/spool/squid4/cache/19/8B: (13) Permission 
denied
2015/08/16 03:42:55 kid1| /var/spool/squid1/cache/1A/8B: (13) Permission 
denied
2015/08/16 03:43:10 kid1| /var/spool/squid2/cache/1A/8B: (13) Permission 
denied
2015/08/16 03:43:25 kid1| /var/spool/squid3/cache/1A/8B: (13) Permission 
denied
2015/08/16 03:43:40 kid1| /var/spool/squid4/cache/1A/8B: (13) Permission 
denied
2015/08/16 03:43:55 kid1| /var/spool/squid1/cache/1B/8B: (13) Permission 
denied

Some like this.

2015/08/21 10:02:13 kid1| /var/spool/squid2/17/1FF: (13) Permission denied
2015/08/21 10:02:13 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:13 kid1| /var/spool/squid4/00/00/00000000
2015/08/21 10:02:28 kid1| /var/spool/squid3/17/1FF: (13) Permission denied
2015/08/21 10:02:32 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:32 kid1| /var/spool/squid4/00/00/00000001
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000002
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000003
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000004
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000005
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000006
2015/08/21 10:02:33 kid1| DiskThreadsDiskFile::openDone: (13) Permission 
denied
2015/08/21 10:02:33 kid1| /var/spool/squid4/00/00/00000007

The folders remain empty of course, so no caching is taking place.

I am truly stumped as to what I may be doing wrong since squid has enough 
perms to create the dirs but not enough to save to them.

Any help would be great.

Thanks!
Zim


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


CAUTION - This message may contain privileged and confidential information
intended only for the use of the addressee named above. If you are not the
intended recipient of this message you are hereby notified that any use,
dissemination, distribution or reproduction of this message is prohibited.
This email was sent by the Bank of New Zealand. You can contact us on
0800 ASK BNZ (0800 275 269). Any views expressed in this message are those
of the individual sender and may not necessarily reflect the views of Bank
of New Zealand.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150825/23aa26a8/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Aug 24 22:08:19 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 25 Aug 2015 00:08:19 +0200
Subject: [squid-users] Cache Permission Errors
In-Reply-To: <OF59591ABA.FA2C0544-ONCC257EAB.007720DA-CC257EAB.00772F59@nab.com.au>
References: <CAPPTbuqqJdW+qYp0YOo=eUP=3M+Lcb8LNzgWu+w4C=9Hh4p3Qw@mail.gmail.com>
 <CAPPTbuo4rJNEaDrrM6Cy98LPj2qTGPkHWDiVGsgomkygyXys5w@mail.gmail.com>
 <OF59591ABA.FA2C0544-ONCC257EAB.007720DA-CC257EAB.00772F59@nab.com.au>
Message-ID: <201508250008.19242.Antony.Stone@squid.open.source.it>

On Monday 24 August 2015 at 23:41:49, Kuntal_Basak at bnz.co.nz wrote:

> Could you please un-subscribe me ?

I've sent you an unsubscribe request confirmation.

Reply to it and you'll be removed from the list.

> Senior Infrastructure Architecture and Design Specialist
> Infrastructure Architecture and Design
> Bank of New Zealand

Hm.


Regards,


Antony.


From eliezer at ngtech.co.il  Mon Aug 24 23:32:48 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Aug 2015 02:32:48 +0300
Subject: [squid-users] FreeBSD pf route-to and linux tproxy
Message-ID: <55DBA9A0.9060702@ngtech.co.il>

After remembering this thread: 
http://www.squid-cache.org/mail-archive/squid-users/201102/0236.html

I had some time to run tests here and there, I am testing now FreeBSD 
traffic diverting with PF and seems to not understand something.
The topology is:
client(192.168.12.150/24) --> R1(FBSD-PF)-------->R2(VYOS+NAT)
		      (192.168.11.254/24)
         			|
				|
                        PROXY(192.168.11.1/24)

R2 and R1 are at net 192.168.15.0/24 R1 -192.168.15.1, R2 - 192.168.15.254

Now I am watching something weird on both the PROXY and both R2.
I am trying to divert traffic using PF to the proxy using the "route-to" 
method.
Example PF rules:
##START pf.conf
int_if = "vtnet2"
ext_if = "vtnet0"
proxy_if = "vtnet1"
lan_net = "192.168.12.0/24"
proxy1 = "192.168.11.1"

pass in quick on $proxy_if
pass in quick on $int_if proto tcp from $lan_net to any port 80 rtable 1
pass in quick on $ext_if proto tcp from any port 80 to $lan_net rtable 1

pass in all
pass out all
##END pf.conf

In this scenario the tproxy is diverting the SYN packet and the squid do 
not reply with a syn-ack.
When I am disabling the pf and using the FreeBSD machine as a router I 
am getting a weird result: The tcp packet gets to the origin server 
without being masqurading(snat) on the VYOS machine.

So two weird scenarios with FreeBSD.
If I replace the R1 with a drop in replacement with a VYOS or CENTOS 
machine it all suddenly works magically, both TPROXY and TCP nat.
The only packets I see that are being snatted are ICMP but not tcp.

* The R1 FreeBSD is a clone of the VYOS so the networks are the same but 
with different nic mac addresses.

I do not look for a resolution to the OS level since with LINUX boxes 
all works magically fine.
But if someone have seen this I will be happy to hear about that I am 
not lonely on that.

Eliezer


From markus.preis at berge-meer.de  Tue Aug 25 12:30:10 2015
From: markus.preis at berge-meer.de (T3h vICE)
Date: Tue, 25 Aug 2015 05:30:10 -0700 (PDT)
Subject: [squid-users] TCP_MISS/429
In-Reply-To: <55DAFD7D.8020701@treenet.co.nz>
References: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>
 <55DAFD7D.8020701@treenet.co.nz>
Message-ID: <1440505810111-4672856.post@n4.nabble.com>

Thank you Amos for your thoughts.

I've tried some stuff to filter the error.
When i'm on my squidsever i can wget http://crushftp/index.html .
Trying to download the index.html on my Windowsclient gives me 429-Error
instead.
Has this maybe something to do with proxy transparency?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-429-tp4672839p4672856.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bernih75 at gmail.com  Tue Aug 25 13:48:50 2015
From: bernih75 at gmail.com (Hicham Berni)
Date: Tue, 25 Aug 2015 15:48:50 +0200
Subject: [squid-users] Change url_rewrite_program problem
In-Reply-To: <55D35BDC.9020203@ngtech.co.il>
References: <CANFg0+zNUCT60P8+GEaNB+Frdojj_RZShi1=dbUjPeQD5_QWZg@mail.gmail.com>
 <55D34532.5060801@ngtech.co.il>
 <CANFg0+z4XRef9=kiJq49BCmqb5co9OzDvs0G+MqpXo8=pZDHhA@mail.gmail.com>
 <55D35BDC.9020203@ngtech.co.il>
Message-ID: <CANFg0+yXHdZY6HR0M1YFZfpwtp=sWoDx4iXmAbhNc8wCvUfnZA@mail.gmail.com>

Hi Eliezer,

Thanks you for feedback,

    ? Squid version installed is2.6.STABLE21  and may be this version not
handling these requests correctly ?

setting for cache peer :
http_port 80 accel defaultsite=add.ptr.lu  vhost
https_port 443 cert=/root/add.ptr.lu.pem key=/root/add.ptr.lu.key accel
defaultsite=add.ptr.lu vhost
cache_peer 15.40.40.47 parent 8000 0 no-query originserver
front-end-https=on name=ProdAccel
cache_peer 15.40.40.47 parent 8001 0 no-query originserver name=TestAccel

    ? From access logs, we do not see any TCP_DENIED log event for the '
add.ptr.lu' request.

> 2015/06/12 17:11:36| Starting Squid Cache version 2.6.STABLE21 for
i686-redhat-linux-gnu... > 2015/06/12 17:11:36| Process ID 3897 > ... >
2015/06/12 17:11:36| Accepting accelerated HTTP connections at 0.0.0.0,
port 8000, FD 18. > 2015/06/12 17:11:36| Accepting proxy HTTP connections
at 0.0.0.0, port 3128, FD 19. > 2015/06/12 17:11:36| Accepting HTTPS
connections at 0.0.0.0, port 443, FD 20. > 2015/06/12 17:11:36| Accepting
ICP messages at 0.0.0.0, port 3130, FD 21. > 2015/06/12 17:11:36| WCCP
Disabled. > 2015/06/12 17:11:36| Configuring Parent 158.64.50.47/8000/0 >
2015/06/12 17:11:36| Configuring Parent 158.64.50.47/8001/0

    ? Port 8000 is the port using for backend  webserver
    ? So I try to access from external , I try https://add.ptr.lu , I
receive a issue with new url https://add.ptr.lu:8000/register

    So I supposed that it cannot return with this url

2015-08-18 18:22 GMT+02:00 Eliezer Croitoru <eliezer at ngtech.co.il>:

> Well it's a bit weird since squid is capable of handling these requests
> and it should not pass a port as a part of the HTTP protocol.
>
> What settings have you tried for a cache peer?
> Have you tried matching with a domain name matching?
> Did you had the chance to get the access.log with or without the
> url_rewrite?
>
> How did you got a request with port 8000? You can always use the "port"
> acl to match it.
> If you are receiving url on port 80 you should not have any issues.
>
> If you can give me couple examples of urls which you get I might be able
> to understand a bit more.
>
> Eliezer
>
>
> On 18/08/2015 18:20, Hicham Berni wrote:
>
>> Hi Eliezer,
>>
>> I try first with cache_peer , but it was not running for this specific
>> web-server port configuration ( 8000 and error was that we received url
>> with port number),
>>
>> I noted that current configuration was using rewrite url program ( for
>> test
>> with port number 8001) and I supposed that it could be same problem by
>> using port 8000 instead 80 ( webdesign don't want to change port number)
>>
>> When i tried  with rewrite url program , it ran but just for production
>> url  and not test url.
>>
>> 2015-08-18 16:46 GMT+02:00 Eliezer Croitoru <eliezer at ngtech.co.il>:
>>
>> Hey Berni,
>>>
>>> I was wondering to myself, why do you need to rewrite the url?
>>> Can't you just use a cache_peer and couple acls?
>>>
>>> Eliezer
>>>
>>>
>>> On 18/08/2015 16:43, Hicham Berni wrote:
>>>
>>> Hi,
>>>>
>>>> We have a squid reverse configuration, and we need to change  backend
>>>> webserver  with a new webserver with new IP and port (80 --> 8000) .
>>>>
>>>> Old squid configuration used url_rewrite_program. I 'not familar with
>>>> this
>>>> configuration.
>>>> With port changing on new web server ( 80 --> 8000) , i find that i had
>>>> to
>>>> add new production web on url_rerite_program.
>>>>
>>>> I tried to change program , but i met a issue.
>>>>
>>>> on part 1 --> current rewrite program configuration  running today and
>>>> correctly  for testurl --> running fine
>>>> on part 2 --> configuration with change  for testurl and productionurl
>>>> ->
>>>> OK for productionurl and KO for testurl
>>>>
>>>> An idea if something is wrong on change ?
>>>>
>>>> Part 1)  old configuration :
>>>>
>>>> #!/usr/bin/perl
>>>>
>>>> $INTERNALIP="15.40.40.40";
>>>> $PRODUCTIONURL="add.ptr.lu";
>>>> $TESTURL="test.add.ptr.lu";
>>>> $TESTPORT="8001";
>>>>
>>>> # turn off write buffering
>>>> $| = 1;
>>>> while (<>) {
>>>>
>>>>     # get the URL from the request
>>>>     chomp($url = $_);
>>>>
>>>>     if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
>>>>     {
>>>>       # fix up the cname and port
>>>>       $url =~ s^:$TESTPORT^^;
>>>>       $url =~ s^$INTERNALIP^$TESTURL^;
>>>>
>>>>       # fix the protocol
>>>>       $url =~ s^https://^http://^;
>>>>     }
>>>>     else
>>>>     {
>>>>       # fix up the name
>>>>       $url =~ s^$INTERNALIP^$PRODUCTIONURL^;
>>>>
>>>>       # fix the protocol
>>>>       $url =~ s^http://^https://^;
>>>>     }
>>>>
>>>>     # return the fixed URL to squid
>>>>     print "$url\n";
>>>> }
>>>>
>>>>
>>>> Part 2) configuration with changes
>>>>
>>>> #!/usr/bin/perl
>>>>
>>>> $INTERNALIP="15.40.40.40";
>>>> $PRODUCTIONURL="add.ptr.lu";
>>>> $TESTURL="test.add.ptr.lu";
>>>> $TESTPORT="8001";
>>>> $PRODPORT="8000";
>>>>
>>>> # turn off write buffering
>>>> $| = 1;
>>>> while (<>) {
>>>>
>>>>     # get the URL from the request
>>>>     chomp($url = $_);
>>>>
>>>>     if ($url =~ m/($INTERNALIP|$TESTURL):$TESTPORT/)
>>>>     {
>>>>       # fix up the cname and port
>>>>       $url =~ s^:$TESTPORT^^;
>>>>       $url =~ s^$INTERNALIP^$TESTURL^;
>>>>
>>>>       # fix the protocol
>>>>       $url =~ s^https://^http://^;
>>>>     }
>>>>
>>>> elsif ($url =~ m/($INTERNALIP|$PRODUCTIONURL):$PRODPORT/)
>>>>     {
>>>>       # fix up the cname and port
>>>>       $url =~ s^:$PRODPORT^^;
>>>>       $url =~ s^$INTERNALIP^$PRODURL^;
>>>>
>>>>       # fix the protocol
>>>>       $url =~ s^https://^http://^;
>>>>     }
>>>>
>>>>     else
>>>>     {
>>>>       # fix up the name
>>>>       $url =~ s^$INTERNALIP^$PRODUCTIONURL^;
>>>>
>>>>       # fix the protocol
>>>>       $url =~ s^http://^https://^;
>>>>     }
>>>>
>>>>     # return the fixed URL to squid
>>>>     print "$url\n";
>>>> }
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150825/3169235c/attachment.htm>

From eliezer at ngtech.co.il  Tue Aug 25 14:25:16 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Aug 2015 17:25:16 +0300
Subject: [squid-users] SquidBlocker stable release 0.3.10 RPM
Message-ID: <55DC7ACC.1090805@ngtech.co.il>

I am pleased to publicly release the first version of SquidBlocker which 
considered by me stable enough for production use.

SquidBlocker can replace squidguard blacklist functionality but with a 
whole another concept of blacklisting in it. The two main things I 
considered are fast updates and very very high up time.
An example implementation topology\scenario with SquidBlocker can be 
seen here:[http://ngtech.co.il/squidblocker/topology1.png]

Information about SquidBlocker you can find 
here[http://ngtech.co.il/squidblocker/]

I have been working for quite some time on this version which was beta 
tested for weeks now.  It doesn't contains all that I want but it does 
give more then many other tools.

I am not opening the sources for this tool yet and it will probably 
happen later in the future.

All The Bests,
Eliezer Croitoru


From yvoinov at gmail.com  Tue Aug 25 15:14:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 25 Aug 2015 21:14:29 +0600
Subject: [squid-users] SquidBlocker stable release 0.3.10 RPM
In-Reply-To: <55DC7ACC.1090805@ngtech.co.il>
References: <55DC7ACC.1090805@ngtech.co.il>
Message-ID: <55DC8655.8080700@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Eliezer,

how to take a look on sources?

25.08.15 20:25, Eliezer Croitoru ?????:
> I am pleased to publicly release the first version of SquidBlocker which considered by me stable
enough for production use.
>
> SquidBlocker can replace squidguard blacklist functionality but with a
whole another concept of blacklisting in it. The two main things I
considered are fast updates and very very high up time.
> An example implementation topology\scenario with SquidBlocker can be
seen here:[http://ngtech.co.il/squidblocker/topology1.png]
>
> Information about SquidBlocker you can find
here[http://ngtech.co.il/squidblocker/]
>
> I have been working for quite some time on this version which was beta
tested for weeks now.  It doesn't contains all that I want but it does
give more then many other tools.
>
> I am not opening the sources for this tool yet and it will probably
happen later in the future.
>
> All The Bests,
> Eliezer Croitoru
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEbBAEBCAAGBQJV3IZUAAoJENNXIZxhPexG5zMH+Ow7cmX5+TRztj6aFKfoejRJ
cc9sLkTITlkvYEE+bWd6z3+WoZ9mDPhOgBCDCnDj2y+cHwV0++iokvR/fAGM3zdB
dVshK27TDlhJh0Tdg8ZDFayA6G7FFoxwL/Tx5MRShz9u8Yfp6Fs+uLH0+xYdYxwq
CzGegzsWnNrzzpVY2G2DtIrtbK/GvWl4TmIRRHiWZ/Frkk1FOUgySqXq3CRWBuZC
GsTDnynhbSGullt7lCg5Tgl8jgPHCcDzdNLMuY87JDQPraik00eM+dEYY57TeOA3
PBGpbqH8UHVVO1zD3i0tYPD3jEVYdagAdL0i0hMsFXBT6GG9rrduE9R+vySoeQ==
=R4I9
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Tue Aug 25 16:31:11 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 25 Aug 2015 19:31:11 +0300
Subject: [squid-users] SquidBlocker stable release 0.3.10 RPM
In-Reply-To: <55DC8655.8080700@gmail.com>
References: <55DC7ACC.1090805@ngtech.co.il> <55DC8655.8080700@gmail.com>
Message-ID: <55DC984F.9040809@ngtech.co.il>

On 25/08/2015 18:14, Yuri Voinov wrote:
> Eliezer,
>
> how to take a look on sources?

The sources are not publicly available for now.
It is however written in GoLang and the algorithms are described in the 
software page.
It should not be very hard to write a similar application just by 
understanding the lookup path.

For now I can write some pesudo codes if you need something specific.

Eliezer


From yvoinov at gmail.com  Tue Aug 25 16:53:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 25 Aug 2015 22:53:12 +0600
Subject: [squid-users] SquidBlocker stable release 0.3.10 RPM
In-Reply-To: <55DC984F.9040809@ngtech.co.il>
References: <55DC7ACC.1090805@ngtech.co.il> <55DC8655.8080700@gmail.com>
 <55DC984F.9040809@ngtech.co.il>
Message-ID: <55DC9D78.9030705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'll be interested in test redirector on my platform (this is Solaris),
this is why I asked about sources....

I have databases. :) Need only code.

25.08.15 22:31, Eliezer Croitoru ?????:
> On 25/08/2015 18:14, Yuri Voinov wrote:
>> Eliezer,
>>
>> how to take a look on sources?
>
> The sources are not publicly available for now.
> It is however written in GoLang and the algorithms are described in
the software page.
> It should not be very hard to write a similar application just by
understanding the lookup path.
>
> For now I can write some pesudo codes if you need something specific.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3J14AAoJENNXIZxhPexGAH0H/2ZSrA2EkRTsTPTY05//K1jo
qzgNGSZGKk4hOleg+NsW3RMTLcJ/1Rry9NR34VM/D6NE5yKCzQ+eDAAwIVA3/xF9
eGX2gn/mZZZ0CkVFzOaV/gkCKihvyyqyDwpfShjlVwBl1VirbPCKSO2E98IPq31D
2VBRcLdvXwFGdcPQyq0vS38uQSEbMdA9TdacMrU250xtnqKLh9Dd9TaHwQi5kZXo
u9D0f6Bq6jjOKGQvPMXJ32qLSBWcFGSIUE8xPjb8omk0XosPPMiiVWUVfb4R4EdM
m8PHW3ceCKtZ+XNPvs1pab7yT0SGfzugGmhDXrlwXwOxCsEjqaBpd5rjxCKNAeg=
=jnBj
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Tue Aug 25 18:17:03 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 26 Aug 2015 00:17:03 +0600
Subject: [squid-users] Squid HIT ratio
In-Reply-To: <55DC984F.9040809@ngtech.co.il>
References: <55DC7ACC.1090805@ngtech.co.il> <55DC8655.8080700@gmail.com>
 <55DC984F.9040809@ngtech.co.il>
Message-ID: <55DCB11F.2070006@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Fred,

look ;)

http://i.imgur.com/UBu13g0.png

Store-ID rulez! :)
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3LEfAAoJENNXIZxhPexGvhAH/2XZARm3G1ZA73ikAZAGo5h3
/EYrU+ZdZc0E4GxLhO8a49jD8gSQ4H/Wc8MMkbXT/+Dflhcpy70N0CQ8M8IBAL54
tItHMraCnmcs6/sDBb3UMk/J8P0J9u8H849s08b7RaLwaPO9rMUY5qaU98Mt/e8s
mRxL8mFYuaUdSviEr2Tq9O3XvHyj/F7Tl1wmcmxg+FtuLoR36VJATKBI6asgSQpz
VHabMt+fdMMPsbgZC6YQdL6Xes4/yPLRf3qcDylgHg0TsHoDqxrUDuk3kVfnP0dZ
5r87MdFGwxBc0TbYwQHrD/BbjU/TnUFXRHPVuDDwj2bhbDDZjas61uBFRrP3a7Y=
=u1AI
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Tue Aug 25 18:43:25 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 25 Aug 2015 20:43:25 +0200 (CEST)
Subject: [squid-users] Squid HIT ratio
In-Reply-To: <55DCB11F.2070006@gmail.com>
Message-ID: <1744899328.70836712.1440528205803.JavaMail.root@zimbra4-e1.priv.proxad.net>


> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Fred,
> 
> look ;)
> 
> http://i.imgur.com/UBu13g0.png
> 
> Store-ID rulez! :)


Yes very interesting, can you share your bytes ratio please ? I will take a look to increase my cache as I discussed with Amos but I can't touch the SSL part (no bump for me)
http://wiki.squid-cache.org/Features/StoreID Squid configuration example seems wrong - for YT - , no ? Google's increased use of HTTPS and now we can't access youtube without SSL ?  

Thanks, I take any advice I can get, specially for delicate users :) 


From yvoinov at gmail.com  Tue Aug 25 18:50:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 26 Aug 2015 00:50:28 +0600
Subject: [squid-users] Squid HIT ratio
In-Reply-To: <1744899328.70836712.1440528205803.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1744899328.70836712.1440528205803.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55DCB8F4.6060405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://i.imgur.com/3jwftYC.png

Bytes ratio is a less, of course. But not so dramatically.

YT seems not cacheable now. I made some research and AFAIK we can't
cache YT now without VERY special store-ID rewriter.

Also, of course, I use SSL-bump. SSL consists over 60% in my traffic.
Without bump I can't cache them. 40% hit ratio (and lower) is issue.

26.08.15 0:43, FredB ?????:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>> 
>> Fred,
>>
>> look ;)
>>
>> http://i.imgur.com/UBu13g0.png
>>
>> Store-ID rulez! :)
>
>
> Yes very interesting, can you share your bytes ratio please ? I will
take a look to increase my cache as I discussed with Amos but I can't
touch the SSL part (no bump for me)
> http://wiki.squid-cache.org/Features/StoreID Squid configuration
example seems wrong - for YT - , no ? Google's increased use of HTTPS
and now we can't access youtube without SSL ? 
>
> Thanks, I take any advice I can get, specially for delicate users :)
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3Lj0AAoJENNXIZxhPexGpy8H/0JgG6MpmPHshyawW0cvTQDn
Fc90hVTQitOsdYN+GqZSYRw9PKsrlmtXMuxtZyqKTKU6nLMtOAN0fmO1M6a1rZ3o
XxeyYsjlsHh8MtFwyP/8HYKZzqJfnUeKuSb5Hbm267P4Zy/CWLQG3Bv5mp1C5R2M
uPQv0Jw7BFnBojxc70ryvPyrjdNbiGgAXGHwh5M3Z65ueV2B1mX1WRQa2Hn1mOpJ
1PEL0ZRYYw29xvU+N7XI3vHenU6uuJrejoGgtUWMQBI5kNqeDiEVh7Pqr1vJuWWZ
u756P820IHuzIRDqpsf13mCm8qj3oe1JiFR15fXhF1p6Iop9LTaNAtqZcwOBiw4=
=Hk+b
-----END PGP SIGNATURE-----



From ow97 at outlook.com  Tue Aug 25 18:51:19 2015
From: ow97 at outlook.com (Oliver Webb)
Date: Tue, 25 Aug 2015 19:51:19 +0100
Subject: [squid-users] FATAL: Unable to open HTTPS Socket
Message-ID: <SNT147-W1473772EA94E8B3D488F87CA610@phx.gbl>

TLDR Skip to ----------

I have squid 3.5.7 installed on linux with the following configure options:

?'--build=arm-linux-gnueabihf' '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-ssl' '--with-openssl' '--enable-ssl-crtd' '--enable-delay-pools' '--enable-external-acl-helpers=session' 'build_alias=arm-linux-gnueabihf'

I have the following ports assigned in squid.conf:

http_port 3129
http_port 3128 intercept
https_port 3130 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem

I also have IPTables redirecting port 443 traffic to port 3130 and port 80 traffic to 3128

For port 80 HTTP traffic the proxy works fine pages load except blocked ones which the proxy successfully replaces which blocked message

Port 443 HTTPS traffic is successfully bumped by squid and the certificate is replaced with the dynamically generated one.
----------
HOWEVER
The page squid serves over the browser-squid tunnel is the ERR_DNS_FAIL error page with the %H hostname template code evaluated to 'http' (without quotes)

Also in the cache.log the following message appears after every HTTPS request
FATAL: Unable to open HTTPS Socket
Nothing else is using the 3130 port as when I stop squid I am able to run a web server off this port (no it *definitely* isn't the web server using the port I just started it as a test)

I have no clue what is wrong. Please Help!! Thank you! 		 	   		  

From squid3 at treenet.co.nz  Wed Aug 26 00:24:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 12:24:36 +1200
Subject: [squid-users] Change url_rewrite_program problem
In-Reply-To: <CANFg0+yXHdZY6HR0M1YFZfpwtp=sWoDx4iXmAbhNc8wCvUfnZA@mail.gmail.com>
References: <CANFg0+zNUCT60P8+GEaNB+Frdojj_RZShi1=dbUjPeQD5_QWZg@mail.gmail.com>
 <55D34532.5060801@ngtech.co.il>
 <CANFg0+z4XRef9=kiJq49BCmqb5co9OzDvs0G+MqpXo8=pZDHhA@mail.gmail.com>
 <55D35BDC.9020203@ngtech.co.il>
 <CANFg0+yXHdZY6HR0M1YFZfpwtp=sWoDx4iXmAbhNc8wCvUfnZA@mail.gmail.com>
Message-ID: <55DD0744.6030509@treenet.co.nz>

On 26/08/2015 1:48 a.m., Hicham Berni wrote:
> Hi Eliezer,
> 
> Thanks you for feedback,
> 
>     ? Squid version installed is2.6.STABLE21  and may be this version not
> handling these requests correctly ?


This setup is very normal and these are teh simple its. So it should
work with your Squid. BUT, 2.6 is the very first version of Squid that
did virtual hosting in the modern way. Some parts have been improved in
the years since the feature was first added. So YMMV a little.


On the other hand 2.6 went EOL way, way, back in May 2008.

It only speaks HTTP/1.0 and a little bit of HTTP/1.1. Most of what
happens in The Internet these days will either break or bypass it.

The TLS/SSL parts of HTTPS are entirely offloaded to your system OpenSSL
library. So any failures there are outside Squid. It could simply be
that the library is trying to speak SSLv2 or some other ancient thing
that the browsers reject.

> 
> setting for cache peer :
> http_port 80 accel defaultsite=add.ptr.lu  vhost
> https_port 443 cert=/root/add.ptr.lu.pem key=/root/add.ptr.lu.key accel
> defaultsite=add.ptr.lu vhost
> cache_peer 15.40.40.47 parent 8000 0 no-query originserver
> front-end-https=on name=ProdAccel
> cache_peer 15.40.40.47 parent 8001 0 no-query originserver name=TestAccel

There is some missing config:

 acl HTTPS proto HTTPS

 cache_peer_access ProdAccel allow HTTPS
 cache_peer_access ProdAccel deny all

 cache_peer_access TestAccel allow !HTTPS
 cache_peer_access TestAccel deny all

That will make the requests go to the right backend server. :-)

> 
>     ? From access logs, we do not see any TCP_DENIED log event for the '
> add.ptr.lu' request.
> 
>> 2015/06/12 17:11:36| Starting Squid Cache version 2.6.STABLE21 for
> i686-redhat-linux-gnu... > 2015/06/12 17:11:36| Process ID 3897 > ... >
> 2015/06/12 17:11:36| Accepting accelerated HTTP connections at 0.0.0.0,
> port 8000, FD 18. > 2015/06/12 17:11:36| Accepting proxy HTTP connections
> at 0.0.0.0, port 3128, FD 19. > 2015/06/12 17:11:36| Accepting HTTPS
> connections at 0.0.0.0, port 443, FD 20. > 2015/06/12 17:11:36| Accepting
> ICP messages at 0.0.0.0, port 3130, FD 21. > 2015/06/12 17:11:36| WCCP
> Disabled. > 2015/06/12 17:11:36| Configuring Parent 158.64.50.47/8000/0 >
> 2015/06/12 17:11:36| Configuring Parent 158.64.50.47/8001/0

That is not the access.log transaction log. That is the cache.log debug
log for critical/important service failures. All that says is that your
Squid is running okay *as a program*.


> 
>     ? Port 8000 is the port using for backend  webserver
>     ? So I try to access from external , I try https://add.ptr.lu , I
> receive a issue with new url https://add.ptr.lu:8000/register
> 
>     So I supposed that it cannot return with this url
> 

Your Squid is not listening on port 8000. You cannot make requests for
that URL and expect them to go through Squid.

The port 8000/8001 detail is just for the private TCP connection between
Squid and the peers. It is not part of the HTTP(S) messages or URLs.


You need the backend servers to accept and service
http://add.ptr.lu/register. And generate relative-URLs.


Amos


From squid3 at treenet.co.nz  Wed Aug 26 00:50:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 12:50:14 +1200
Subject: [squid-users] TCP_MISS/429
In-Reply-To: <1440505810111-4672856.post@n4.nabble.com>
References: <OF90243C87.4B64FF54-ONC1257EAB.002F5C97-C1257EAB.0030B12C@berge-meer.de>
 <55DAFD7D.8020701@treenet.co.nz> <1440505810111-4672856.post@n4.nabble.com>
Message-ID: <55DD0D46.1040101@treenet.co.nz>

On 26/08/2015 12:30 a.m., T3h vICE wrote:
> Thank you Amos for your thoughts.
> 
> I've tried some stuff to filter the error.
> When i'm on my squidsever i can wget http://crushftp/index.html .
> Trying to download the index.html on my Windowsclient gives me 429-Error
> instead.
> Has this maybe something to do with proxy transparency?
> 

Might be. Might not be.

Squid is always properly transparent (in the HTTP meaning) by default.
If the TCP and IP and anonymity definitions of "transparent" are needed
or configured that is something only the admin of the proxy knows and
can change.


Might be that you are not the only user visiting that site through that
proxy, or one its traffic goes through further upstream.

Might be an old server with custom status code predating the specifications.

Might be a lot of other things. Only the admin of the server generating
it actually knows why and when the code is used.

Keep digging. Good luck.
Amos



From alex at samad.com.au  Wed Aug 26 01:56:55 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 26 Aug 2015 11:56:55 +1000
Subject: [squid-users] caching question
Message-ID: <CAJ+Q1PWFeerWuCTGrf3K8NrV10O2acsCem7q7F2phGWr+URQ9g@mail.gmail.com>

Hi

I want to get squid to not cache urls that terminate like this

updates/x86_64/repodata/repomd.xml
os/x86_64/repodata/repomd.xml

How do I organize that.

Having problems with old repmod.xml files making my yum updates fail..

Alex


From alex at samad.com.au  Wed Aug 26 01:59:52 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 26 Aug 2015 11:59:52 +1000
Subject: [squid-users] caching question
In-Reply-To: <CAJ+Q1PWFeerWuCTGrf3K8NrV10O2acsCem7q7F2phGWr+URQ9g@mail.gmail.com>
References: <CAJ+Q1PWFeerWuCTGrf3K8NrV10O2acsCem7q7F2phGWr+URQ9g@mail.gmail.com>
Message-ID: <CAJ+Q1PXUzThx6-P9EXwhoXGwKRYtYK8P_TvAGXRBB4MTbwTL-Q@mail.gmail.com>

Hi

Sorry add more info

I have this already in my squid.conf
acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
cache deny nonCacheDom


I presume i can add something similiar but with urlpath_regex

acl nonCacheURL urlpath_regex ".*/x86_64/repodata/repomd.xml"
cache deny nonCacheURL


A

On 26 August 2015 at 11:56, Alex Samad <alex at samad.com.au> wrote:
> Hi
>
> I want to get squid to not cache urls that terminate like this
>
> updates/x86_64/repodata/repomd.xml
> os/x86_64/repodata/repomd.xml
>
> How do I organize that.
>
> Having problems with old repmod.xml files making my yum updates fail..
>
> Alex


From alex at samad.com.au  Wed Aug 26 02:10:42 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 26 Aug 2015 12:10:42 +1000
Subject: [squid-users] caching question
In-Reply-To: <CAJ+Q1PXUzThx6-P9EXwhoXGwKRYtYK8P_TvAGXRBB4MTbwTL-Q@mail.gmail.com>
References: <CAJ+Q1PWFeerWuCTGrf3K8NrV10O2acsCem7q7F2phGWr+URQ9g@mail.gmail.com>
 <CAJ+Q1PXUzThx6-P9EXwhoXGwKRYtYK8P_TvAGXRBB4MTbwTL-Q@mail.gmail.com>
Message-ID: <CAJ+Q1PWeneSgHXRZ=0KJytRnAec00DPCiKOq_fXe-OaevwhppA@mail.gmail.com>

Hi

Sorry, answered my own question.

acl nonCacheURL urlpath_regex .*/x86_64/repodata/repomd.xml$
cache deny nonCacheURL

seems like "" makes it look for a file ?

On 26 August 2015 at 11:59, Alex Samad <alex at samad.com.au> wrote:
> Hi
>
> Sorry add more info
>
> I have this already in my squid.conf
> acl nonCacheDom dstdomain -i "/etc/squid/lists/nonCacheDom.lst"
> cache deny nonCacheDom
>
>
> I presume i can add something similiar but with urlpath_regex
>
> acl nonCacheURL urlpath_regex ".*/x86_64/repodata/repomd.xml"
> cache deny nonCacheURL
>
>
> A
>
> On 26 August 2015 at 11:56, Alex Samad <alex at samad.com.au> wrote:
>> Hi
>>
>> I want to get squid to not cache urls that terminate like this
>>
>> updates/x86_64/repodata/repomd.xml
>> os/x86_64/repodata/repomd.xml
>>
>> How do I organize that.
>>
>> Having problems with old repmod.xml files making my yum updates fail..
>>
>> Alex


From saravanan.nagarajan87 at gmail.com  Wed Aug 26 02:33:04 2015
From: saravanan.nagarajan87 at gmail.com (SaRaVanAn)
Date: Tue, 25 Aug 2015 21:33:04 -0500
Subject: [squid-users] Client <-> Squid <-> WebServer packet flow
Message-ID: <CA+86yMipWKemsn_qapPAT6GxUVxrGx2LSAOm2gR-AndEGLjTiQ@mail.gmail.com>

Hi All,
I have a basic question on Client - web server communication through Squid
using transparent proxy.
In case of transparent proxy Whenever a client tries to access a webpage,
squid spoofs the packet and acts like a web server. In turn Squid initiates
a new TCP connection with the web server.
My clarification is like whether squid initiates a new TCP connection with
webserver as soon as it receives a SYN packet from the client or after a
TCP session has been established between the client and squid proxy server.
I both the

Client                Squid                Webserver

SYN
------------------------->
SYN+ACK
<------------------------
ACK
--------------------------->

HTTP GET
------------------------------>
                                                                SYN
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150825/75428c14/attachment.htm>

From sumodirjo at gmail.com  Wed Aug 26 02:52:09 2015
From: sumodirjo at gmail.com (Muhammad Panji)
Date: Wed, 26 Aug 2015 09:52:09 +0700
Subject: [squid-users] Mac OS X Updates
In-Reply-To: <CAKNtY_wJs5x-KnfipsLvoXPniPYmVCcPeAbeBzr3ep8NeHq4PQ@mail.gmail.com>
References: <CAKNtY_yxPfFJhuP__sbWi_p6UNkyCuz=eFSkUQKeB_7=9wg+0w@mail.gmail.com>
 <CA+Y8hcO_D8EF323UC8aTYxjfnD3O1AAJY30t5oWP2xhKO27tZQ@mail.gmail.com>
 <CAKNtY_wJs5x-KnfipsLvoXPniPYmVCcPeAbeBzr3ep8NeHq4PQ@mail.gmail.com>
Message-ID: <CANbzdHk5dkR3FxNwsw0Mkb7nT_OLm40K2+BTVmthmTLafPFMqg@mail.gmail.com>

Hi,
Not squid but if you want to cache mac os x and ios update you can install
mac os x server and use caching service provided by mac os x server. You
can read more details here :
https://help.apple.com/advancedserveradmin/mac/4.0/#/apd74DDE89F-08D2-4E0A-A5CD-155E345EFB83

Thanks.
Regards,



Panji

On Mon, Aug 24, 2015 at 11:04 PM, John Pearson <johnpearson555 at gmail.com>
wrote:

> Thanks! That's what I figured. I wanted to see if anyone in the community
> had better ideas or another way.
>
> On Mon, Aug 24, 2015 at 4:52 AM, Kinkie <gkinkie at gmail.com> wrote:
>
>> Hi John,
>>   according to the article you link to, it's not possible to cache these
>> updates: Apple puts some effort as a conscious choice to make it so.
>>
>>   Updates for older versions of MacOS may be over HTTP, newer ones are
>> over HTTPs over port 443 and and dynamically-generated ports. HTTP could be
>> cached, https cannot without ssl-bump/peek-n-splice (SSL man-in-the-middle).
>>   The wording of the article seems to suggest that the list of trusted
>> issuers of certificates for the https service is not the same as the
>> system's CA root certificate store but is probably locked to Apple's. This
>> means that also SSL MITM is not possible, by design.
>>
>>
>> On Wed, Aug 19, 2015 at 9:20 PM, John Pearson <johnpearson555 at gmail.com>
>> wrote:
>>
>>> Anyone have Mac OS X update caching working ? Without doing a SSL bump.
>>> I think they are hosted through https (
>>> https://support.apple.com/en-us/HT202943 )
>>>
>>> Thanks!
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>>
>> --
>>     Francesco
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
Muhammad Panji
http://www.panji.web.id
http://www.kurungsiku.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150826/1a5df060/attachment.htm>

From ciindyrox at gmail.com  Wed Aug 26 02:54:35 2015
From: ciindyrox at gmail.com (Cindy Yang)
Date: Tue, 25 Aug 2015 19:54:35 -0700
Subject: [squid-users] Remove squid cache?
Message-ID: <438E23D6-C6F0-47A4-B033-1969F0EE1D67@gmail.com>

Hi all,

I hope this is the right place to ask for help. A few weeks ago I updated my laptop to windows 10. It worked great for a few weeks and then I noticed that certain websites were not able to load. I paid no attention to it since I figured it was my shitty wifi. Well turns out I've somehow installed squid cache on my computer and its preventing me from visiting certain websites. I get the error "The remote host or network may be down. Please try the request again". 

I don't have the time or patience really to be dealing with this. Can someone help me in removing or righting whatever I did?

Thanks!

From squid3 at treenet.co.nz  Wed Aug 26 05:03:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 17:03:32 +1200
Subject: [squid-users] Remove squid cache?
In-Reply-To: <438E23D6-C6F0-47A4-B033-1969F0EE1D67@gmail.com>
References: <438E23D6-C6F0-47A4-B033-1969F0EE1D67@gmail.com>
Message-ID: <55DD48A4.3030108@treenet.co.nz>

On 26/08/2015 2:54 p.m., Cindy Yang wrote:
> Hi all,
> 
> I hope this is the right place to ask for help. A few weeks ago I
> updated my laptop to windows 10. It worked great for a few weeks and
> then I noticed that certain websites were not able to load. I paid no
> attention to it since I figured it was my shitty wifi. Well turns out
> I've somehow installed squid cache on my computer and its preventing
> me from visiting certain websites. I get the error "The remote host
> or network may be down. Please try the request again".
> 
> I don't have the time or patience really to be dealing with this. Can
> someone help me in removing or righting whatever I did?

Hi Cindy,

Good news is I doubt its anthing particular you did. Bad news is that
might mean there is nothing you can do about it.

The modern popular browsers all have some 'developer tools' or such that
you can use to view the page traffic. In particular where the browser is
connecting to and what HTTP messages are happening. If you can check the
HTTP message headers on the reply the browser is getting back it should
show a Via: or Server: headers with domain name(s) hinting at where
exacty the Squid is.


The rest of this is all maybe and guesswork. So see if you can do the
above to get some solid info.

...

Firstly, Squid is server software not generally useful for PC or
workstations. But *is* commonly used by ISPs or website CDNs. Thats
behind the bad-news part.

 If your ISP is using Squid you need to talk with them about your problems.

 If its the website owners themselves the same deal, but with somewhat
more difficulty finding who to contact.

...

On the other hand if Squid is actually on your machine you are going to
have to locate how it was installed and see if you can remove the thing
it was bundled with. A bit of work but with a solution at the end.


Start with your browser settings to see if there is a proxy host:IP or
PAC file configured. And also the system "Internet Options" contol panel
(was under the Network Sharing last I saw). If there is a localhost,
127.0.0.1, or ::1 configured as proxy address that might just be your AV
vendor - so go carefully with changing, but its worth a try.

The legitimate Squid for Windows are installed via the normal progm
install so should show up in "Programs and Features" control panel.
Other ways are a bit nasty.


In the far past I've seen some virus using squid and other proxies to
grab web traffic on the machine. But that practice seems to have died
away, so you would be very unlucky if it was. Well worth a good AV scan
anyway though.

It may also actually be a part of your AV vendors product - I'm aware of
Squid being used somehow by Symantec AV suite / security utilities (for
servers), Trend Micro AV security suite (for servers), or any "cloud"
based security product.
 Note that the big names there are server/ISP products not designed for
end users devices anyway. So checking the product suitability to your
device might be in order.


Since Squid is usually network software and you mention wifi, you could
be seeing signs of someone hacking your wifi access point. Using only
securely encrypted wifi/wireless security is the only way to avoid that.
Maybe changing the password/phrase there if it has not changed in a while.

Hope this helps.


Amos


From squid3 at treenet.co.nz  Wed Aug 26 05:21:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 17:21:12 +1200
Subject: [squid-users] Client <-> Squid <-> WebServer packet flow
In-Reply-To: <CA+86yMipWKemsn_qapPAT6GxUVxrGx2LSAOm2gR-AndEGLjTiQ@mail.gmail.com>
References: <CA+86yMipWKemsn_qapPAT6GxUVxrGx2LSAOm2gR-AndEGLjTiQ@mail.gmail.com>
Message-ID: <55DD4CC8.1000409@treenet.co.nz>

On 26/08/2015 2:33 p.m., SaRaVanAn wrote:
> Hi All,
> I have a basic question on Client - web server communication through Squid
> using transparent proxy.
> In case of transparent proxy Whenever a client tries to access a webpage,
> squid spoofs the packet and acts like a web server. In turn Squid initiates
> a new TCP connection with the web server.
> My clarification is like whether squid initiates a new TCP connection with
> webserver as soon as it receives a SYN packet from the client or after a
> TCP session has been established between the client and squid proxy server.
> I both the
> 
> Client                Squid                Webserver
> 
> SYN
> ------------------------->
> SYN+ACK
> <------------------------
> ACK
> --------------------------->
> 
> HTTP GET
> ------------------------------>
>                                                                 SYN
> 

Not quite. Squid doesn't act like a web server exactly. It just avoids
using the proxy-only HTTP features in messages visible to the client.
For all other intents and purposes it is still a proxy and doing
proxying for this traffic.

Which partially answers your question. Since Squid is proxying these
messages - no, there is no server connection opened until one is needed.
After the caching logics have checked for HITs, adaptation and
re-writing checked for early responses/errors etc, and routing logics
decided what server(s) to try going to.

The spoofing of the client IP on TPROXY only affects the routing logics
by adding a restriction: only server accessible to the client are
usable. Original dst-IP of the server the client was trying to reach is
also preferred over others for a more seamless/invisible/transparent
service and is the most likely to succeed on first-try. But other
destinations selected by the routing logics could easily be used if that
fails.

Amos




From squid3 at treenet.co.nz  Wed Aug 26 05:30:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 17:30:14 +1200
Subject: [squid-users] caching question
In-Reply-To: <CAJ+Q1PWeneSgHXRZ=0KJytRnAec00DPCiKOq_fXe-OaevwhppA@mail.gmail.com>
References: <CAJ+Q1PWFeerWuCTGrf3K8NrV10O2acsCem7q7F2phGWr+URQ9g@mail.gmail.com>
 <CAJ+Q1PXUzThx6-P9EXwhoXGwKRYtYK8P_TvAGXRBB4MTbwTL-Q@mail.gmail.com>
 <CAJ+Q1PWeneSgHXRZ=0KJytRnAec00DPCiKOq_fXe-OaevwhppA@mail.gmail.com>
Message-ID: <55DD4EE6.8000103@treenet.co.nz>

On 26/08/2015 2:10 p.m., Alex Samad wrote:
> Hi
> 
> Sorry, answered my own question.
> 
> acl nonCacheURL urlpath_regex .*/x86_64/repodata/repomd.xml$
> cache deny nonCacheURL

Almost exactly like that. Just without the ".* at the start of the pattern.

All regex patterns are assumed to start and end with .* by default
unless you specify the ^ or $ start/end anchors explicitly.

> 
> seems like "" makes it look for a file ?

Yes.


If those old files were delivered without Expires/Cache-Control headers
(or if you used override-expires on *.xml) then your refresh_pattern are
the cause of the old files being cached too long in the first place.

This pattern will prevent them being stored heuristically, but otherwise
let the repos cache controls work as designed by the repo operators.

 refresh_pattern /repodata/repomd.xml$ 0 0% 0


Amos



From squid3 at treenet.co.nz  Wed Aug 26 05:36:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 17:36:06 +1200
Subject: [squid-users] FATAL: Unable to open HTTPS Socket
In-Reply-To: <SNT147-W1473772EA94E8B3D488F87CA610@phx.gbl>
References: <SNT147-W1473772EA94E8B3D488F87CA610@phx.gbl>
Message-ID: <55DD5046.5000101@treenet.co.nz>

On 26/08/2015 6:51 a.m., Oliver Webb wrote:
> TLDR Skip to ----------
> 
> I have squid 3.5.7 installed on linux with the following configure options:
> 
>  '--build=arm-linux-gnueabihf' '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid' '--enable-ssl' '--with-openssl' '--enable-ssl-crtd' '--enable-delay-pools' '--enable-external-acl-helpers=session' 'build_alias=arm-linux-gnueabihf'
> 
> I have the following ports assigned in squid.conf:
> 
> http_port 3129
> http_port 3128 intercept
> https_port 3130 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
> 
> I also have IPTables redirecting port 443 traffic to port 3130 and port 80 traffic to 3128
> 
> For port 80 HTTP traffic the proxy works fine pages load except blocked ones which the proxy successfully replaces which blocked message
> 
> Port 443 HTTPS traffic is successfully bumped by squid and the certificate is replaced with the dynamically generated one.
> ----------
> HOWEVER
> The page squid serves over the browser-squid tunnel is the ERR_DNS_FAIL error page with the %H hostname template code evaluated to 'http' (without quotes)
> 

That means one or more of these is most likely:

A) the SNI value sent by the client was "http".
 - that is invalid TLS protocol

B) the Host header in the HTTP message was "Host: http:443" or "Host:
http://blah"
 - both are invalid FQDN

C) the reverse-DNS for the IP address Squid is dealing with says "http"
 - that is an invalid DNS record


Maybe be more I've overlooked right now.


> Also in the cache.log the following message appears after every HTTPS request
> FATAL: Unable to open HTTPS Socket


For (B) and some of (A), try adding "debug_options 11,2 83,5" to your
config file and see what the messages are doing in Squid.
 If there are no HTTP messages, then the issue is in the TLS or DNS layers.

For the rest of (A), you will likely need to use packet captures to see
what is happening on the connections both in and out of Squid
(from-client and to-server). The TLS/SSL library command line test tools
may also be useful there to track the TLS protocol details Squis is not
showing well yet.


For (C) try resolving the domain name "http" from the command line of
your Squid machine.

If you configured dns_nameservers directive in squid.conf repeat that
test using each of the listed servers.

If the machine normal lookup works but the Squid NS fail, you may need
to add dns_defnames and dns_appenddomain directives to your squid.conf
with details that match what /etc/resolv.conf tells the machine to do.


PS. The best setup IMHO, is to remove the dns_* directives entirely and
let Squid use the normal /etc/resolv.conf settings.


Amos



From yvoinov at gmail.com  Wed Aug 26 09:31:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 26 Aug 2015 15:31:43 +0600
Subject: [squid-users] FATAL: Unable to open HTTPS Socket
In-Reply-To: <55DD5046.5000101@treenet.co.nz>
References: <SNT147-W1473772EA94E8B3D488F87CA610@phx.gbl>
 <55DD5046.5000101@treenet.co.nz>
Message-ID: <55DD877F.7010105@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Amos,

this issue looks like very similar to bug 4188, isn't it?

WBR, Yuri

26.08.15 11:36, Amos Jeffries ?????:
> On 26/08/2015 6:51 a.m., Oliver Webb wrote:
>> TLDR Skip to ----------
>>
>> I have squid 3.5.7 installed on linux with the following configure
options:
>>
>>  '--build=arm-linux-gnueabihf' '--prefix=/usr' '--localstatedir=/var'
'--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid'
'--enable-ssl' '--with-openssl' '--enable-ssl-crtd'
'--enable-delay-pools' '--enable-external-acl-helpers=session'
'build_alias=arm-linux-gnueabihf'
>>
>> I have the following ports assigned in squid.conf:
>>
>> http_port 3129
>> http_port 3128 intercept
>> https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
>>
>> I also have IPTables redirecting port 443 traffic to port 3130 and
port 80 traffic to 3128
>>
>> For port 80 HTTP traffic the proxy works fine pages load except
blocked ones which the proxy successfully replaces which blocked message
>>
>> Port 443 HTTPS traffic is successfully bumped by squid and the
certificate is replaced with the dynamically generated one.
>> ----------
>> HOWEVER
>> The page squid serves over the browser-squid tunnel is the
ERR_DNS_FAIL error page with the %H hostname template code evaluated to
'http' (without quotes)
>>
>
> That means one or more of these is most likely:
>
> A) the SNI value sent by the client was "http".
>  - that is invalid TLS protocol
>
> B) the Host header in the HTTP message was "Host: http:443" or "Host:
> http://blah"
>  - both are invalid FQDN
>
> C) the reverse-DNS for the IP address Squid is dealing with says "http"
>  - that is an invalid DNS record
>
>
> Maybe be more I've overlooked right now.
>
>
>> Also in the cache.log the following message appears after every HTTPS
request
>> FATAL: Unable to open HTTPS Socket
>
>
> For (B) and some of (A), try adding "debug_options 11,2 83,5" to your
> config file and see what the messages are doing in Squid.
>  If there are no HTTP messages, then the issue is in the TLS or DNS
layers.
>
> For the rest of (A), you will likely need to use packet captures to see
> what is happening on the connections both in and out of Squid
> (from-client and to-server). The TLS/SSL library command line test tools
> may also be useful there to track the TLS protocol details Squis is not
> showing well yet.
>
>
> For (C) try resolving the domain name "http" from the command line of
> your Squid machine.
>
> If you configured dns_nameservers directive in squid.conf repeat that
> test using each of the listed servers.
>
> If the machine normal lookup works but the Squid NS fail, you may need
> to add dns_defnames and dns_appenddomain directives to your squid.conf
> with details that match what /etc/resolv.conf tells the machine to do.
>
>
> PS. The best setup IMHO, is to remove the dns_* directives entirely and
> let Squid use the normal /etc/resolv.conf settings.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3Yd/AAoJENNXIZxhPexGGd8H/1NfOxLblRGsy9qCaPY2yM1J
WnF2f5smag9uk+BW08OTz+Px+GePMl6lvdauUXp5NFj4YDNh1q94/tG7sKF0zyOG
qt3OafVSdbLuUooE80RjMRPTxaEM2ibgcEj7lAvsDdsQVOFlBJeaysyvsgi+jql5
zijJXcGFfy2y38nhSQAlt8WTgDwLEBxVT77twbSp64l3GMsknugF6X6z97Brwffg
IA01dKhUrXZl3ElJokp62XTMs+luBzopwuK77exEvxJSgn1chK6/F6V1GlDaxZYt
YL3LGiV57TToluMKxmPkmp9UjCTc+kMU23k8lSzOAYp7KXOhgDXJ0fdWafWhtSg=
=JXK6
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Aug 26 11:09:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2015 23:09:27 +1200
Subject: [squid-users] FATAL: Unable to open HTTPS Socket
In-Reply-To: <55DD877F.7010105@gmail.com>
References: <SNT147-W1473772EA94E8B3D488F87CA610@phx.gbl>
 <55DD5046.5000101@treenet.co.nz> <55DD877F.7010105@gmail.com>
Message-ID: <55DD9E67.4020703@treenet.co.nz>

On 26/08/2015 9:31 p.m., Yuri Voinov wrote:
> 
> Amos,
> 
> this issue looks like very similar to bug 4188, isn't it?
> 


Not even close. This one is a DNS failure after being given something bad.

The unresolved issue in your bug is ERR_CONNECTION_REFUSED after an
unexplained timeout. If you suspect the timeout is your DNS servers not
responding you should look into that though.

Amos


From tyyeul at gmail.com  Wed Aug 26 13:00:19 2015
From: tyyeul at gmail.com (Paul LINDEN)
Date: Wed, 26 Aug 2015 15:00:19 +0200
Subject: [squid-users] Logfile analysis
Message-ID: <55DDB863.5030007@gmail.com>

Hello,

I'm working with SQUID 3.5.6 win64 version et i'm very happy with.
I'm looking for a loganalysis software running under Windows... Does 
anybody know one ? I'm running Windows 8.1 64 bits.
Thanks in advance... Regards,
Paul


From yvoinov at gmail.com  Wed Aug 26 13:15:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 26 Aug 2015 19:15:15 +0600
Subject: [squid-users] Logfile analysis
In-Reply-To: <55DDB863.5030007@gmail.com>
References: <55DDB863.5030007@gmail.com>
Message-ID: <55DDBBE3.5080503@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
SARG has Windows version now.

26.08.15 19:00, Paul LINDEN ?????:
> Hello,
>
> I'm working with SQUID 3.5.6 win64 version et i'm very happy with.
> I'm looking for a loganalysis software running under Windows... Does
anybody know one ? I'm running Windows 8.1 64 bits.
> Thanks in advance... Regards,
> Paul
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3bviAAoJENNXIZxhPexGX3AIAMb9E6zZgkxsjX9HgIiRbCQT
6FGYnNAUgSqGYLd3o9e+LvsEb3FdPC8LJYP+0ctYHgQpGgAqDVbDuuUZmDZ2PwFa
mVyvY6yKS7aSOPf4D0798HlEfG74QxXVR3so4/oYhRKPtPkxBW/kyuuNQ1oEgVYF
cLMycXtA7OVv/o8cxq91MaMIVeB20AoDIVXQ9bE/c6Bl5icP2KwuWXkx8YAFdyiC
Azhcck7/jdsWBfqon/4ksHpJDEbmT09E6KaAV29GmRUaaelEJhSV1A+PFNhA/q1o
YZjMfYzsStTFljBnP99K0cxT7n6BlBxD5OFQa82+DNWd+AoZ1OEQGif1oufrspA=
=n/UD
-----END PGP SIGNATURE-----



From sebag at vianetcon.com.ar  Wed Aug 26 16:11:19 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Wed, 26 Aug 2015 13:11:19 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55D9AB13.2000702@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz>
Message-ID: <55DDE527.2060800@vianetcon.com.ar>

Hello Amos, thanks for your help. I've disabled our rewriter helper but 
the errors remain the same. So I think that's not the reason.
I've been reading some older mails from the list and a guy named Hussam 
Al-Tayeb exchanged some interesting mails with you a couple of months 
ago .. That got me thinking, can I completely disable Vary checking? I 
know is an http violation and not recommended, but if I could disable it 
using an ACL for certain sites that are missconfigured and I have the 
certainty that the content is exactly the same no matter what .. I could 
get better performance. (It's ok if I have to patch something and 
recompile squid)
If this is not possible, what about "bypassing" content that has the 
Vary in its response header so squid does not make this 2 lookups only 
to find that it has to retrieve it from the original server anyway?



Thanks,
Sebastian


El 23/08/15 a las 08:14, Amos Jeffries escribi?:
> On 22/08/2015 4:20 a.m., Sebastian Goicochea wrote:
>> Hello everyone, I'm having a strange problem:
>>
>> Several servers, same hardware, using same version of squid (3.5.4)
>> compiled using the same configure options, same configuration files. But
>> in two of them I get LOTS of these Vary object loop! lines in cache.log
>>
>> 2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://resources.mlstatic.com/frontend/vip-fend-webserver/assets/bundles/photoswipe-6301b943e5586fe729e5d6480120a893.js'
>> 'accept-encoding="gzip"'
>> 2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
>> 2015/08/21 13:07:52 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt, 'http://www.google.com/afs/ads/i/iframe.html'
>> 'accept-encoding="gzip,%20deflate"'
>> 2015/08/21 13:07:52 kid1| clientProcessHit: Vary object loop!
>> 2015/08/21 13:08:01 kid1| varyEvaluateMatch: Oops. Not a Vary match on
>> second attempt,
>> 'http://minicuotas.ribeiro.com.ar/images/products/large/035039335000.jpg' 'accept-encoding="gzip,%20deflate"'
>>
>> 2015/08/21 13:08:01 kid1| clientProcessHit: Vary object loop!
>>
>> I've read what I could find on forums but could not solve it. Is this
>> something to worry about?
> The short answer:
>
> Yes and no. Squid is signalling that it is completely unable to perform
> its caching duty for these URLs. The proxying duty continues with only
> high latency visible to the client.
>
> It is up to you whether that latency cost is urgent or not. It is
> certainy high enough importance that you need to be told each time (no
> rate limiting) when you have asked to receive important notices.
>
>
>> If that is not the case, how can I disable the
>> excessive logging?
> You can reduce your logging level to show only critical problems,
> instead of showing all details rated 'important'.
>
>    debug_options ALL,0
>
> NOTE: important (ALL,1) includes a lot of things like this that do
> really need to be fixed to get better service out of either your proxy
> or the underlying network. But can be put on your todo list if you dont
> have time right now.
>
>
>> Which is the condition that generates this?
> In long;
>
>
> The "whats happening" is:
>
> Your cache contains an object which was delivered by the server along
> with headers stating that behind the URL is a large set of porssible
> responses. *all* requests for that URL use a certain set of headers
> (listed in Vary) to determine which binary-level object is applicable
> (or not) on a per-client / per-reqeust basis.
>   In order to cache the object Squid has to follow that same selection
> criteria *exactly*.
>
> Most common example is gzip vs non-gzip encoded copies of things. Which
> you can see those messages relate to.
>
> Squid stores this information in a "Vary object" associated with only
> the URL. That vary object is used to perform a secondary cache index
> lookup to see if the partiular variant needed is stored.
>
> The expectation is that there would be 3+ objects stored for this URL; a
> gzip data object, various non-gzip data objects, and a metadata object
> ("Vary object") telling Squid that it needs to look at the
> accept-encoding header to find which of the those data objects to send
> the client.
>
>
> The messages themselves mean:
>
> "Oops. Not a Vary match on second attempt"
>
>   - that the Vary object saying look at headers X+Y+X is pointing at
> itself or another Vary metadata object saying look at some other
> headers. A URL cannot have two different Vary header values
> simultaneously (Vary is a single list "value").
> Something really weird is going on in your cache. Squid should handle
> this by abandoning the cache lookups and go to the origin for fresh copies.
>
> You could be causing it by using url-rewrite or store-id helpers wrongly
> to pass requests for a URL to servers which produce different responses.
> So that is well worth looking into.
>
> IMPORTANT: It is mandatory that any re-writing only be done to
> 'collapse' URLs that are *actually* producing identical objects and
> producing them in (outwardly) identical ways. This Vary looping is just
> the tip of an iceberg of truly horrible failures that occur "silently"
> with re-writing.
>
>
>
> There is another similar message that can be mixed in the long list:
>
> "Oops. Not a Vary object on second attempt," (note the 1-word difference)
>   - this is almost but not quite so bad, and is usually seen with broken
> origin servers. All you can do about the problem itself then is fire off
> bug reports to people and hope it gets fixed by the sysadmin in charge.
>
>
> Both situations are very bad for HTTP performance, and bad for churning
> your cache as well. But Squid can cope easily enough by just fetching a
> new object and dropping what is in the cache. That "Vary object loop!"
> message is telling you Squid is doing exactly that.
>
>
> A quick test with the tool at redbot.org shows that the
> resources.mlstatic.com server is utterly borked. Not even sending
> correct ETag ids for the objects its outputting. Thats a sign to me that
> the admin is trying to be smart with headers, and getting it very badly
> wrong.
>
> minicuotas.ribeiro.com.ar is also a Nginx server. But only showing the
> signs of normal default nginx brokenness
> (<http://trac.nginx.org/nginx/ticket/118>).
>
> The only thing you can do about that nginx bug is add pressure to get it
> fixed, or cut away all the Accept-Encoding headers on all requests sent
> to those servers. (request_header_access Accept-Encoding deny ...) Squid
> is already doing everything it reasonably can to correct the traffic
> output to your clients.
>
>
> The google related messages ... they are usually pretty good at this
> type of thing and my tests do show their server to be working correctly.
>   So that points me back at suspecting your config does something bad
> with url-rewriter or store-id helpers.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150826/7191e87e/attachment.htm>

From rafael.akchurin at diladele.com  Wed Aug 26 17:28:37 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 26 Aug 2015 17:28:37 +0000
Subject: [squid-users] Logfile analysis
In-Reply-To: <55DDB863.5030007@gmail.com>
References: <55DDB863.5030007@gmail.com>
Message-ID: <VI1PR04MB135912DACAF2625AC3B839028F600@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Paul,

We are trying to add Squid log file analysis and traffic reporting over ICAP stream coming from Squid to the latest build of qlproxy (ICAP server for Squid) but this work is far from over.

May I ask what reports you find useful and why SARG reports are not good enough?

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Paul LINDEN
Sent: Wednesday, August 26, 2015 3:00 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Logfile analysis

Hello,

I'm working with SQUID 3.5.6 win64 version et i'm very happy with.
I'm looking for a loganalysis software running under Windows... Does anybody know one ? I'm running Windows 8.1 64 bits.
Thanks in advance... Regards,
Paul
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Wed Aug 26 18:46:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 06:46:55 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440421859169-4672846.post@n4.nabble.com>
References: <1439924019637-4672758.post@n4.nabble.com>
 <1440080786735-4672791.post@n4.nabble.com> <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
 <1440408226749-4672841.post@n4.nabble.com> <55DAF5DD.7060406@treenet.co.nz>
 <1440421859169-4672846.post@n4.nabble.com>
Message-ID: <55DE099F.1000507@treenet.co.nz>

On 25/08/2015 1:10 a.m., FredT wrote:
> Amos,
> 
> Uploading the patched squid took time to be agreed by the client, sorry but
> the server is in production and we cannot take the risk to see if the action
> will crash or not the squid, i don't want to lose this client...
> 
> If you fix in the next release the cache_swap_low/high taking care the
> Percent Used and the Filemap, it would be a good solution at the moment 
> 
> Keep us posted...
> 
> Bye Fred
> 

I've spoken the change over with Alex who knows the overall store
behaviours a bit better than we decided to remove the over-100% part of
the patch for now. At least until someone can test it properly. That
would be the fix for bug 2448, with test-case provided in there if
anyone is keen to take it on.


Instead I have slightly increased the rate to a full 200 objects/sec for
easier calculations and just let the multiplier for the removal rate
increase indefinitely.

What this means in practice is the low-water mark is still the signal
for eviction to begin (or stop if utilization drops under that) but the
relative position of the high-water mark to it determines the agressiveness.

What "aggressive" means is a linear scale of evictions/sec. The 'gap'
percentage between the two marks represents 200 objects to be evicted,
and each multiple of that gap above low-water is another 200 objects
removed from the cache that second.
 If high-water is set equal or lower than low-water mark the rate is a
flat 220 objects per second evicted.


So in a way we have the squid.conf controlled agressiveness. With the
defaut 90%/95% watermarks Squid will be attempting to remove 420
objects/sec by the time it reached 100% cache_dir capacity. Which should
be plenty for just about every installation.
But if you were say, needing Squid to evict 800 objects/sec you could
set them to 92% and 94%. Closer for smaller 2% gap, and 200 x4 of that
gap between low-water and 100% capacity.

But also keep in mind thats purges/sec. The HIT and REFRESH requests
serviced do not count here at all. So the rate needed is almost always
much lower than the per-second equivalent of request per minute rate
shown in mgr:info report.

There is also a feature request to turn these watermarks from whole
percentages to taking decimals. Which would make them even nicer for
this. But that is a much bigger change for later. If anyone wants to
sponsor that work I'm available.

Amos



From vdoctor at neuf.fr  Wed Aug 26 18:48:57 2015
From: vdoctor at neuf.fr (FredT)
Date: Wed, 26 Aug 2015 11:48:57 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55DE099F.1000507@treenet.co.nz>
References: <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
 <1440408226749-4672841.post@n4.nabble.com> <55DAF5DD.7060406@treenet.co.nz>
 <1440421859169-4672846.post@n4.nabble.com> <55DE099F.1000507@treenet.co.nz>
Message-ID: <1440614937789-4672888.post@n4.nabble.com>

Hi Amos,

OK, clear :o)

Did you think to apply the low/high to the "Filemap Used" too ?
>From my point of view the Filemap Used is much more important in the objects
cleaning because it reaches the 16 million entries faster than the cache
space, especialy with small objects (<64kb).

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672888.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Aug 26 19:07:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 07:07:10 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440614937789-4672888.post@n4.nabble.com>
References: <55D6D142.4030803@treenet.co.nz>
 <1440144669633-4672800.post@n4.nabble.com>
 <1440146802401-4672803.post@n4.nabble.com> <55D71B82.7020203@treenet.co.nz>
 <1440400275614-4672835.post@n4.nabble.com> <55DADE45.2010905@treenet.co.nz>
 <1440408226749-4672841.post@n4.nabble.com> <55DAF5DD.7060406@treenet.co.nz>
 <1440421859169-4672846.post@n4.nabble.com> <55DE099F.1000507@treenet.co.nz>
 <1440614937789-4672888.post@n4.nabble.com>
Message-ID: <55DE0E5E.5050203@treenet.co.nz>

On 27/08/2015 6:48 a.m., FredT wrote:
> Hi Amos,
> 
> OK, clear :o)
> 
> Did you think to apply the low/high to the "Filemap Used" too ?
> From my point of view the Filemap Used is much more important in the objects
> cleaning because it reaches the 16 million entries faster than the cache
> space, especialy with small objects (<64kb).
> 

In those circumstances this algorithm clears out fileno faster than it
clears space. Since theagrressiveness is measured in pure count of how
many fileno/sec it tries to free up. At high-water it will always try to
free 220 fileno/sec but that may be 0.0001% of the cache distance
towards low-water - forcing another 220 etc the next second. Only when
caching bigger objects will the fileno free-ups be low - when it doesn't
matter as much.

I've put a "TODO possible improvements" list into the code where someone
will see it anyway. With that and the other things to experiment with.

Amos



From squid3 at treenet.co.nz  Wed Aug 26 19:11:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 07:11:23 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55DDE527.2060800@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
Message-ID: <55DE0F5B.2070703@treenet.co.nz>

On 27/08/2015 4:11 a.m., Sebasti?n Goicochea wrote:
> Hello Amos, thanks for your help. I've disabled our rewriter helper but
> the errors remain the same. So I think that's not the reason.

You emptied the cache, or at least altered the cache_dir line to point
at a new empty cache_dir during the test after disabling the helper?
anything it might have done to the cache contents is already done to the
data stored there by the time you disable it.


> I've been reading some older mails from the list and a guy named Hussam
> Al-Tayeb exchanged some interesting mails with you a couple of months
> ago .. That got me thinking, can I completely disable Vary checking? I
> know is an http violation and not recommended, but if I could disable it
> using an ACL for certain sites that are missconfigured and I have the
> certainty that the content is exactly the same no matter what .. I could
> get better performance. (It's ok if I have to patch something and
> recompile squid)
> If this is not possible, what about "bypassing" content that has the
> Vary in its response header so squid does not make this 2 lookups only
> to find that it has to retrieve it from the original server anyway?
> 

Its not a violation of HTTP. It is a critical internal validity check
for the cache index itself.

Preventing the contents of say your bank account display page being sent
to someone else fetching http://google.com/. That kind of critical.

If the Vary meta object is not pointing at the object its supposed to
be. Then the object it was supposed to be pointing at could be anything
at all.


For your other question. Yes, 3.5 has the store_miss directive now.
<http://master.squid-cache.org/Doc/config/store_miss/>.
You can use ACLs in there to check for either the known URLs or the Vary
header existence on replies and prevent caching of those objects. I'm
not sure how that will interact with the vary objects in your case but
none I know of using it has mentioned any issues.

Amos



From jorgeley at gmail.com  Wed Aug 26 19:48:25 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Wed, 26 Aug 2015 16:48:25 -0300
Subject: [squid-users] Zero Sized Reply
Message-ID: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>

Hi guys.
I'm having a weird problem, my squid is doing "ZERO SIZED REPLY" when I try
to connect with some addresses, like this on log above:
2015/08/26 13:50:31.335 kid1| http.cc(1300) continueAfterParsingHeader:
WARNING: HTTP: Invalid Response: No object data received for
http://www.grupoatuall.com.br/ AKA www.grupoatuall.com.br/
2015/08/26 13:50:31.335 kid1| store.cc(1755) reset: StoreEntry::reset:
http://www.grupoatuall.com.br/
2015/08/26 13:50:31.335 kid1| FwdState.cc(412) fail: ERR_ZERO_SIZE_OBJECT
"Bad Gateway"
        http://www.grupoatuall.com.br/
Any ideas???
--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150826/805ac2b0/attachment.htm>

From sebag at vianetcon.com.ar  Wed Aug 26 19:53:52 2015
From: sebag at vianetcon.com.ar (=?UTF-8?Q?Sebasti=c3=a1n_Goicochea?=)
Date: Wed, 26 Aug 2015 16:53:52 -0300
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55DE0F5B.2070703@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz>
Message-ID: <55DE1950.2010306@vianetcon.com.ar>

After I sent you my previous email, I continued investigating the 
subject .. I made a change in the source code as follows:

File: /src/http.cc

HttpStateData::haveParsedReplyHeaders()
{
     .
     .
##### THIS IS NEW STUFF ###########
     if (rep->header.has(HDR_VARY)) {
     rep->header.delById(HDR_VARY);
     debugs(11,3, "Vary detected. Hack Cleaning it up");
     }
##### END OF NEW STUFF ###########

#if X_ACCELERATOR_VARY
     if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
     rep->header.delById(HDR_X_ACCELERATOR_VARY);
     debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning 
it up");
     }
#endif
     .
     .


Deleting Vary from the header at this point gives me hits in every 
object I test (that previously didn't hit) .. web browser never receives 
the Vary in the response header.
Now I read your answer and you say that this is a critical validity 
check and that worries me. Taking away the vary altogether at this point 
could lead to the problems that you described? If that is the case .. I 
have to investigate other alternatives.


Thanks,
Sebastian




El 26/08/15 a las 16:11, Amos Jeffries escribi?:
> On 27/08/2015 4:11 a.m., Sebasti?n Goicochea wrote:
>> Hello Amos, thanks for your help. I've disabled our rewriter helper but
>> the errors remain the same. So I think that's not the reason.
> You emptied the cache, or at least altered the cache_dir line to point
> at a new empty cache_dir during the test after disabling the helper?
> anything it might have done to the cache contents is already done to the
> data stored there by the time you disable it.
>
>
>> I've been reading some older mails from the list and a guy named Hussam
>> Al-Tayeb exchanged some interesting mails with you a couple of months
>> ago .. That got me thinking, can I completely disable Vary checking? I
>> know is an http violation and not recommended, but if I could disable it
>> using an ACL for certain sites that are missconfigured and I have the
>> certainty that the content is exactly the same no matter what .. I could
>> get better performance. (It's ok if I have to patch something and
>> recompile squid)
>> If this is not possible, what about "bypassing" content that has the
>> Vary in its response header so squid does not make this 2 lookups only
>> to find that it has to retrieve it from the original server anyway?
>>
> Its not a violation of HTTP. It is a critical internal validity check
> for the cache index itself.
>
> Preventing the contents of say your bank account display page being sent
> to someone else fetching http://google.com/. That kind of critical.
>
> If the Vary meta object is not pointing at the object its supposed to
> be. Then the object it was supposed to be pointing at could be anything
> at all.
>
>
> For your other question. Yes, 3.5 has the store_miss directive now.
> <http://master.squid-cache.org/Doc/config/store_miss/>.
> You can use ACLs in there to check for either the known URLs or the Vary
> header existence on replies and prevent caching of those objects. I'm
> not sure how that will interact with the vary objects in your case but
> none I know of using it has mentioned any issues.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150826/58be9445/attachment.htm>

From squid3 at treenet.co.nz  Wed Aug 26 20:15:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 08:15:04 +1200
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55DE1950.2010306@vianetcon.com.ar>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
Message-ID: <55DE1E48.1090900@treenet.co.nz>

On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
> After I sent you my previous email, I continued investigating the
> subject .. I made a change in the source code as follows:
> 
> File: /src/http.cc
> 
> HttpStateData::haveParsedReplyHeaders()
> {
>     .
>     .
> ##### THIS IS NEW STUFF ###########
>     if (rep->header.has(HDR_VARY)) {
>     rep->header.delById(HDR_VARY);
>     debugs(11,3, "Vary detected. Hack Cleaning it up");
>     }
> ##### END OF NEW STUFF ###########
> 
> #if X_ACCELERATOR_VARY
>     if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>     rep->header.delById(HDR_X_ACCELERATOR_VARY);
>     debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
> up");
>     }
> #endif
>     .
>     .
> 
> 
> Deleting Vary from the header at this point gives me hits in every
> object I test (that previously didn't hit) .. web browser never receives
> the Vary in the response header.
> Now I read your answer and you say that this is a critical validity
> check and that worries me. Taking away the vary altogether at this point
> could lead to the problems that you described? If that is the case .. I
> have to investigate other alternatives.
> 

I'll have to look into that function when I'm back at the code later to
confirm this. But IIRC that function is acting directly on a freshly
received reply message. You are not removing the validity check, you are
removing Squids ability to see that it is a Vary object at all. So it is
never even cached as one.

The side effect of that is that clients asking for non-gzip can get the
cached gzip copy, etc. but at least its the same URL. So the security
risks are gone. But the user experience is not always good either way.

Amos



From yvoinov at gmail.com  Wed Aug 26 20:50:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 27 Aug 2015 02:50:23 +0600
Subject: [squid-users] Lots of "Vary object loop!"
In-Reply-To: <55DE1E48.1090900@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz>
Message-ID: <55DE268F.60601@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Btw,

when Squid will directly support gzip, inflate compression itself?

27.08.15 2:15, Amos Jeffries ?????:
> On 27/08/2015 7:53 a.m., Sebasti?n Goicochea wrote:
>> After I sent you my previous email, I continued investigating the
>> subject .. I made a change in the source code as follows:
>>
>> File: /src/http.cc
>>
>> HttpStateData::haveParsedReplyHeaders()
>> {
>>     .
>>     .
>> ##### THIS IS NEW STUFF ###########
>>     if (rep->header.has(HDR_VARY)) {
>>     rep->header.delById(HDR_VARY);
>>     debugs(11,3, "Vary detected. Hack Cleaning it up");
>>     }
>> ##### END OF NEW STUFF ###########
>>
>> #if X_ACCELERATOR_VARY
>>     if (rep->header.has(HDR_X_ACCELERATOR_VARY)) {
>>     rep->header.delById(HDR_X_ACCELERATOR_VARY);
>>     debugs(11,3, "HDR_X_ACCELERATOR_VARY Vary detected. Hack Cleaning it
>> up");
>>     }
>> #endif
>>     .
>>     .
>>
>>
>> Deleting Vary from the header at this point gives me hits in every
>> object I test (that previously didn't hit) .. web browser never receives
>> the Vary in the response header.
>> Now I read your answer and you say that this is a critical validity
>> check and that worries me. Taking away the vary altogether at this point
>> could lead to the problems that you described? If that is the case .. I
>> have to investigate other alternatives.
>>
>
> I'll have to look into that function when I'm back at the code later to
> confirm this. But IIRC that function is acting directly on a freshly
> received reply message. You are not removing the validity check, you are
> removing Squids ability to see that it is a Vary object at all. So it is
> never even cached as one.
>
> The side effect of that is that clients asking for non-gzip can get the
> cached gzip copy, etc. but at least its the same URL. So the security
> risks are gone. But the user experience is not always good either way.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3iaOAAoJENNXIZxhPexG5/gIALjD6Xg9DMlW+Bten9ZvElsh
3/XHNy5OUAn6WvFULldkZSEnF5jvgUk0vGaGluVjnfriCsoCTnpaxwZmAclG3ZTd
ug/QRYvQCNB2fDXRFxUPJl+kzE53WSA6gpistDK2xmQJxEF1er7cMZVzGeMbR73/
32/wlo7WgZQ/pRmM1EhYqwCiXf8MRhufCx4AILNXkiN5O/CgUqsSHwl+jnOXWzqH
IakxTbtLyxgdN/nLhph0rscTQAMcYIX8aRTbmYjoXJoOYdDL49I4CoMegM7sX8Qi
lhctMLFjl7y2Qo7ofowJzo4NQPCIjx7268J80k6mZcRCROTor/VERFwp72UxZ6o=
=Vv6C
-----END PGP SIGNATURE-----



From eliezer at ngtech.co.il  Thu Aug 27 00:05:47 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 27 Aug 2015 03:05:47 +0300
Subject: [squid-users] FreeBSD pf route-to and linux tproxy
In-Reply-To: <55DBA9A0.9060702@ngtech.co.il>
References: <55DBA9A0.9060702@ngtech.co.il>
Message-ID: <55DE545B.1030901@ngtech.co.il>

After a research I have found out that there is something wrong with the 
virtio driver of FreeBSD.
Then I wrote a basic wiki example on how to Policy routing traffic flows 
throw a FreeBSD box to a squid box in DMZ:
http://wiki.squid-cache.org/ConfigExamples/Intercept/PfPolicyRoute

So in a case someone reads it, e1000 virtual adapters works fine on 
FreeBSD while the virtio(for kvm) do some weird things to routed traffic.
I do not know yet what is the issue and this is a FreeBSD related issue 
rather then squid.

Now squid works fine with VYOS+other linux+FreeBSD

Eliezer

On 25/08/2015 02:32, Eliezer Croitoru wrote:
> After remembering this thread:
> http://www.squid-cache.org/mail-archive/squid-users/201102/0236.html
>
> I had some time to run tests here and there, I am testing now FreeBSD
> traffic diverting with PF and seems to not understand something.
> The topology is:
> client(192.168.12.150/24) --> R1(FBSD-PF)-------->R2(VYOS+NAT)
>                (192.168.11.254/24)
>                      |
>                  |
>                         PROXY(192.168.11.1/24)
>
> R2 and R1 are at net 192.168.15.0/24 R1 -192.168.15.1, R2 - 192.168.15.254
>
> Now I am watching something weird on both the PROXY and both R2.
> I am trying to divert traffic using PF to the proxy using the "route-to"
> method.
> Example PF rules:
> ##START pf.conf
> int_if = "vtnet2"
> ext_if = "vtnet0"
> proxy_if = "vtnet1"
> lan_net = "192.168.12.0/24"
> proxy1 = "192.168.11.1"
>
> pass in quick on $proxy_if
> pass in quick on $int_if proto tcp from $lan_net to any port 80 rtable 1
> pass in quick on $ext_if proto tcp from any port 80 to $lan_net rtable 1
>
> pass in all
> pass out all
> ##END pf.conf
>
> In this scenario the tproxy is diverting the SYN packet and the squid do
> not reply with a syn-ack.
> When I am disabling the pf and using the FreeBSD machine as a router I
> am getting a weird result: The tcp packet gets to the origin server
> without being masqurading(snat) on the VYOS machine.
>
> So two weird scenarios with FreeBSD.
> If I replace the R1 with a drop in replacement with a VYOS or CENTOS
> machine it all suddenly works magically, both TPROXY and TCP nat.
> The only packets I see that are being snatted are ICMP but not tcp.
>
> * The R1 FreeBSD is a clone of the VYOS so the networks are the same but
> with different nic mac addresses.
>
> I do not look for a resolution to the OS level since with LINUX boxes
> all works magically fine.
> But if someone have seen this I will be happy to hear about that I am
> not lonely on that.
>
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Aug 27 03:49:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 15:49:04 +1200
Subject: [squid-users] Squid and compression
In-Reply-To: <55DE268F.60601@gmail.com>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55DE268F.60601@gmail.com>
Message-ID: <55DE88B0.5010407@treenet.co.nz>

On 27/08/2015 8:50 a.m., Yuri Voinov wrote:
> 
> Btw,
> 
> when Squid will directly support gzip, inflate compression itself?

Thats a tough question. "When someone does it." is the sadly true cliche.

Transfer-Encoding with gzip is what Squid as a proxy is actually
expected to do by the protocol. But neither Squid nor most other
software implement it so its not got much demand. I'm working on it as a
hobby task and a favour for a customer who cant get signoff on big costs
for such a low-gain feature. So small steps at a time and still a ways
off at this rate.
 (exactly *when* is sponsor dependent. Anyone want to front up a few
weeks or months of full-time developer paycheck to see it happen by Jan
2016 or some deadline like that?)


But what most of you are talking about with this gzip question is
actually Content-Encoding:gzip as used by browser and origin servers.
Recoding that on the fly is content adaptation. An eCAP service plugin
already exists and was the right way to go IMHO. I'm not sure what
happend to the plugins author. Theres still lots of optimization stuff
to be done in that area.

Normalizing the Accept headers before Vary processing is simpler change.
But again needs someone interested in doing it, and is possibly better
suited to the eCAP adaper doing in prep for its reply transformations.
This Vary stuff does have a fair few bugs and missing bits so its not
quite clean sailing or would have happened years ago.


So lots to be done. But nobody with money has enough interest right now
in seeing it happen. Whats the phrase, "free as in freedom, not beer".

Amos


From squid3 at treenet.co.nz  Thu Aug 27 06:04:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2015 18:04:14 +1200
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
Message-ID: <55DEA85E.3010305@treenet.co.nz>

On 27/08/2015 7:48 a.m., Jorgeley Junior wrote:
> Hi guys.
> I'm having a weird problem, my squid is doing "ZERO SIZED REPLY" when I try
> to connect with some addresses, like this on log above:
> 2015/08/26 13:50:31.335 kid1| http.cc(1300) continueAfterParsingHeader:
> WARNING: HTTP: Invalid Response: No object data received for
> http://www.grupoatuall.com.br/ AKA www.grupoatuall.com.br/
> 2015/08/26 13:50:31.335 kid1| store.cc(1755) reset: StoreEntry::reset:
> http://www.grupoatuall.com.br/
> 2015/08/26 13:50:31.335 kid1| FwdState.cc(412) fail: ERR_ZERO_SIZE_OBJECT
> "Bad Gateway"
>         http://www.grupoatuall.com.br/
> Any ideas???

The server connection got disconnected between sending Squid the reply
headers and the message payload they were attached to.

If you have a Squid between 3.2.0 and 3.5.5 (inclusive) please upgrade.
Which is processing SSL-bump, NTLM or Negotiate auth (even just relaying
those in www-auth form). It is probably bug 3329 related.

If you have a more current Squid "debug_options 11,2" should show you
the HTTP headers going through to eyeball if they had any kind of fatal
syntax problem that would make Squid abandon the connection.

Otherwise it would seem to be the server disconnecting. There can be a
lot of reasons for that.

Amos



From yvoinov at gmail.com  Thu Aug 27 09:48:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 27 Aug 2015 15:48:25 +0600
Subject: [squid-users] Squid and compression
In-Reply-To: <55DE88B0.5010407@treenet.co.nz>
References: <55D74FC4.7050203@vianetcon.com.ar>
 <55D9AB13.2000702@treenet.co.nz> <55DDE527.2060800@vianetcon.com.ar>
 <55DE0F5B.2070703@treenet.co.nz> <55DE1950.2010306@vianetcon.com.ar>
 <55DE1E48.1090900@treenet.co.nz> <55DE268F.60601@gmail.com>
 <55DE88B0.5010407@treenet.co.nz>
Message-ID: <55DEDCE9.5000003@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I agree in general, but there are some considerations.

1. Compressed content is almost always becomes unique. Which leads to a
dramatic decrease in the cache hit ratio.

2. Compression actually became a de facto standard on the Internet.

3. Project ecap-gzip looks dead for over three years as squidguard.
There is no active maintainers and we have to dig in sources myself.
Solution compatibility do not actually confirm or not the Squid team.
While independent tests confirm the compatibility and show good results.

4. The structure of Squid-3 actually included a third-party utility
purge. What prevents the same way include ecap-gzip directly in code,
without ecap usage?

It is not about exotic functionality demanded a bunch of geeks. It is
about having the widest distribution of functionality, more than 90% of
servers on the Internet are using compression. This is not a whim of the
user. This is servers default behaviour. And I believe that the caching
proxy default is to use all that as part of the standard, to obtain the
highest possible degree of caching.

WBR, Yuri

PS. Amos, I generally agree that we talk functionality, which "will not
be implemented neve because we do not want to do that." But you will
agree that my arguments are essential.

27.08.15 9:49, Amos Jeffries ?????:
> On 27/08/2015 8:50 a.m., Yuri Voinov wrote:
>>
>> Btw,
>>
>> when Squid will directly support gzip, inflate compression itself?
>
> Thats a tough question. "When someone does it." is the sadly true cliche.
>
> Transfer-Encoding with gzip is what Squid as a proxy is actually
> expected to do by the protocol. But neither Squid nor most other
> software implement it so its not got much demand. I'm working on it as a
> hobby task and a favour for a customer who cant get signoff on big costs
> for such a low-gain feature. So small steps at a time and still a ways
> off at this rate.
>  (exactly *when* is sponsor dependent. Anyone want to front up a few
> weeks or months of full-time developer paycheck to see it happen by Jan
> 2016 or some deadline like that?)
>
>
> But what most of you are talking about with this gzip question is
> actually Content-Encoding:gzip as used by browser and origin servers.
> Recoding that on the fly is content adaptation. An eCAP service plugin
> already exists and was the right way to go IMHO. I'm not sure what
> happend to the plugins author. Theres still lots of optimization stuff
> to be done in that area.
>
> Normalizing the Accept headers before Vary processing is simpler change.
> But again needs someone interested in doing it, and is possibly better
> suited to the eCAP adaper doing in prep for its reply transformations.
> This Vary stuff does have a fair few bugs and missing bits so its not
> quite clean sailing or would have happened years ago.
>
>
> So lots to be done. But nobody with money has enough interest right now
> in seeing it happen. Whats the phrase, "free as in freedom, not beer".
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV3tzpAAoJENNXIZxhPexGofcIAMUA+huUxU4q+x0Q4zWQu6pf
Oy/5+xzAyYIpTwV4yptmuvn9sjn6dkeF5jfNnkERdvNvS/jWU3dVxCs5CkpWnmV8
guKCycyh+dDLsDisp2+xi46UnZQNNcpvpJgxnjNK84Mft44SNtHPX4+upXi9B276
UjXwjkOjhcBll+kRiNJKlYMyd9p/5parOG01SWPXhUBiI0ON0QS5mQ8cJdyA6Dsa
quxv8iL/3BCfT71rEgOYSHYb5JmZIBGtIIb4oVAtytBgPOTCJCJZOf8CSLd8x33t
GlfhV9nocAFjndQ1N2tSPyQ4EKQEmmLsHlHlnyGgSPMdx23et4CdMoaRh0tp49c=
=8Dx/
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/128a3320/attachment.htm>

From mo at stellarise.com  Thu Aug 27 10:10:24 2015
From: mo at stellarise.com (Imaginovskiy)
Date: Thu, 27 Aug 2015 03:10:24 -0700 (PDT)
Subject: [squid-users] High-Availability in Squid
Message-ID: <1440670224046-4672899.post@n4.nabble.com>

Hi all, 

Bit of a strange one but I'm wondering if it's possible to have squid
redirect a site to a secondary backend server if the primary is down. Have
been looking into this but haven't seen much similar to this. Currently I
have my setup along the lines of this;

Client -> Squid -> Backend1

but in the event that Backend1 is down, the following should be done;

Client -> Squid -> Backend2 

Is squid capable of monitoring connections to peer or redirecting based on
an ACL looking for some HTTP error code?

Thanks.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/High-Availability-in-Squid-tp4672899.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From john.kj1984 at gmail.com  Thu Aug 27 10:50:59 2015
From: john.kj1984 at gmail.com (john jacob)
Date: Thu, 27 Aug 2015 16:20:59 +0530
Subject: [squid-users] peek all step with bump instance of proxy
Message-ID: <CAOoFbVfiUV6oL_woB0WTSfdr_DnXobpmr6k4drs_opEMgOMUpQ@mail.gmail.com>

Hi All,

I am trying to configure a squid filtering instance which serves both proxy
and intercepted (transparent) connections. Filtering is accomplished by a
Request eCAP adapter which have something like

if(IsDenied() && RequestMethod=="CONNECT")
{
                          // Gives TAG_NONE/403 in the access log
 hostx->blockVirgin();
 return;
}

I also have a requirement to bump a particular domain and peek other https
connections for intercepted mode. So there are 3 possible
outcomes/filtering decision for any https connections hitting this server.
They are

1) Bump and allow the access
2) Non bumped and allowed access
3) Non bumped and denied access, by the code given above in eCAP adapter

My squid (tried with v3.5.6 and v3.5.7-20150823-r13895, same outcome)
config looks like below
.
.
.
#  TAG: ssl_bump
ssl_bump server-first <ip of the domain to be bumped>
ssl_bump peek all
ssl_bump splice all
.
.
.
http_port <proxy ip>:<port> ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=<path>


https_port <intercept/transparent ip>:<port> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=<path>

Things are fine with the intercepted connections (for all the 3 scenarios).
But with the proxy connections I am encountering some peculiar behavior
with scenario 3 (ie when a non bumped https CONNECT is denied by eCAP).
Instead of terminating the connection, it is logged as TAG_NONE/200 in the
access log and getting bumped (a dynamic certificate is generated) and then
getting terminated. The behavior disappears and works if I comment the
"peek all" line.

I am not sure if this is a bug or an expected behavior.

Of course the proxy bumped connection works fine if I selectively peek for
intercepted connections (ssl_bump peek <if only in intercepted mode>), but
in this case I am getting duplicate entries in the access log file (ie 2
CONNECT log messages for each https CONNECT) for intercepted mode https
connections.The same goes for other ACL combinations like the below
resulting in duplicated log messages

ssl_bump server-first <ip of the domain to be bumped>
ssl_bump splice <only if the request hit the proxy ip:port and not the
intercept/transparent ip :port>
ssl_bump peek all
ssl_bump splice all

Regards,
John
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/0dc19653/attachment.htm>

From jorgeley at gmail.com  Thu Aug 27 14:42:47 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 11:42:47 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <55DEA85E.3010305@treenet.co.nz>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
Message-ID: <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>

Thanks Amos.
my squid is 3.5.6, so i can disconsider the bug, right?
I'm very lost about this problem, any suggestion will be appreciated

2015-08-27 3:04 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 27/08/2015 7:48 a.m., Jorgeley Junior wrote:
> > Hi guys.
> > I'm having a weird problem, my squid is doing "ZERO SIZED REPLY" when I
> try
> > to connect with some addresses, like this on log above:
> > 2015/08/26 13:50:31.335 kid1| http.cc(1300) continueAfterParsingHeader:
> > WARNING: HTTP: Invalid Response: No object data received for
> > http://www.grupoatuall.com.br/ AKA www.grupoatuall.com.br/
> > 2015/08/26 13:50:31.335 kid1| store.cc(1755) reset: StoreEntry::reset:
> > http://www.grupoatuall.com.br/
> > 2015/08/26 13:50:31.335 kid1| FwdState.cc(412) fail: ERR_ZERO_SIZE_OBJECT
> > "Bad Gateway"
> >         http://www.grupoatuall.com.br/
> > Any ideas???
>
> The server connection got disconnected between sending Squid the reply
> headers and the message payload they were attached to.
>
> If you have a Squid between 3.2.0 and 3.5.5 (inclusive) please upgrade.
> Which is processing SSL-bump, NTLM or Negotiate auth (even just relaying
> those in www-auth form). It is probably bug 3329 related.
>
> If you have a more current Squid "debug_options 11,2" should show you
> the HTTP headers going through to eyeball if they had any kind of fatal
> syntax problem that would make Squid abandon the connection.
>
> Otherwise it would seem to be the server disconnecting. There can be a
> lot of reasons for that.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/b66717a1/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 16:01:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 04:01:13 +1200
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
Message-ID: <55DF3449.2080904@treenet.co.nz>

On 28/08/2015 2:42 a.m., Jorgeley Junior wrote:
> Thanks Amos.
> my squid is 3.5.6, so i can disconsider the bug, right?

Yes I believe so.

Amos




From jorgeley at gmail.com  Thu Aug 27 16:25:03 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 13:25:03 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <55DF3449.2080904@treenet.co.nz>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
Message-ID: <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>

increasing the log leve i got this:
2015/08/27 11:43:30.966 kid1| ipcache.cc(501) ipcache_nbgethostbyname:
ipcache_nbgethostbyname: Name 'www.grupoatuall.com.br'.
2015/08/27 11:43:30.966 kid1| Address.cc(389) lookupHostIP: Given Non-IP '
www.grupoatuall.com.br': Name or service not known
2015/08/27 11:43:30.966 kid1| ipcache.cc(549) ipcache_nbgethostbyname:
ipcache_nbgethostbyname: MISS for 'www.grupoatuall.com.br'
Any other ideas???

2015-08-27 13:01 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 28/08/2015 2:42 a.m., Jorgeley Junior wrote:
> > Thanks Amos.
> > my squid is 3.5.6, so i can disconsider the bug, right?
>
> Yes I believe so.
>
> Amos
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/1ca4e46e/attachment.htm>

From 221184 at gmail.com  Thu Aug 27 16:21:04 2015
From: 221184 at gmail.com (Arkantos)
Date: Thu, 27 Aug 2015 09:21:04 -0700 (PDT)
Subject: [squid-users] completely transparent Squid
Message-ID: <1440692464604-4672904.post@n4.nabble.com>

hello everybody,

my friend and I, happen to run the neighborhood cable and wifi network.
Costs are picked up by the users' community, and we get a salary for running
around.

we have about 35 users

we get around 60 mb from the ISP and then we have deployed Inventum Unify
Cloud MSC for user management (only for login, B/w control and URL logging).

the community is now wanting a caching server.

i have zeroed in on CentOS+Squid+Webmin
but we are unable to configure it as a "completely transparent cache"

can anybody help us to configure?
we can pay if wanted. but fees should not be astronomical.


please help.
Arkantos.

221184 at gmail



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/completely-transparent-Squid-tp4672904.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Aug 27 16:36:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 04:36:56 +1200
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
Message-ID: <55DF3CA8.2000306@treenet.co.nz>

On 28/08/2015 4:25 a.m., Jorgeley Junior wrote:
> increasing the log leve i got this:
> 2015/08/27 11:43:30.966 kid1| ipcache.cc(501) ipcache_nbgethostbyname:
> ipcache_nbgethostbyname: Name 'www.grupoatuall.com.br'.
> 2015/08/27 11:43:30.966 kid1| Address.cc(389) lookupHostIP: Given Non-IP '
> www.grupoatuall.com.br': Name or service not known
> 2015/08/27 11:43:30.966 kid1| ipcache.cc(549) ipcache_nbgethostbyname:
> ipcache_nbgethostbyname: MISS for 'www.grupoatuall.com.br'
> Any other ideas???

Thats not 11,2. The 11,2 level will output full HTTP protocol message
headers in current Squid.

Amos



From jorgeley at gmail.com  Thu Aug 27 16:43:20 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 13:43:20 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <55DF3CA8.2000306@treenet.co.nz>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
Message-ID: <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>

Just put the directive debug_options 11,2 ???
more logs:
2015/08/27 11:43:31.291 kid1| FwdState.cc(383) startConnectionOrFail:
http://www.grupoatuall.com.br/
2015/08/27 11:43:31.291 kid1| HttpRequest.cc(486) clearError: old error
details: 0/0
2015/08/27 11:43:31.291 kid1| FwdState.cc(785) connectStart:
fwdConnectStart: http://www.grupoatuall.com.br/
2015/08/27 11:43:31.291 kid1| pconn.cc(329) key:
PconnPool::key(local=0.0.0.0 remote=200.98.190.9:80 flags=1,
www.grupoatuall.com.br) is {200.98.190.9:80/www.grupoatuall.com.br}
2015/08/27 11:43:31.291 kid1| pconn.cc(439) pop: lookup for key {
200.98.190.9:80/www.grupoatuall.com.br} failed.


2015-08-27 13:36 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 28/08/2015 4:25 a.m., Jorgeley Junior wrote:
> > increasing the log leve i got this:
> > 2015/08/27 11:43:30.966 kid1| ipcache.cc(501) ipcache_nbgethostbyname:
> > ipcache_nbgethostbyname: Name 'www.grupoatuall.com.br'.
> > 2015/08/27 11:43:30.966 kid1| Address.cc(389) lookupHostIP: Given Non-IP
> '
> > www.grupoatuall.com.br': Name or service not known
> > 2015/08/27 11:43:30.966 kid1| ipcache.cc(549) ipcache_nbgethostbyname:
> > ipcache_nbgethostbyname: MISS for 'www.grupoatuall.com.br'
> > Any other ideas???
>
> Thats not 11,2. The 11,2 level will output full HTTP protocol message
> headers in current Squid.
>
> Amos
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/492f9b4d/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Aug 27 16:52:46 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 27 Aug 2015 17:52:46 +0100
Subject: [squid-users] completely transparent Squid
In-Reply-To: <1440692464604-4672904.post@n4.nabble.com>
References: <1440692464604-4672904.post@n4.nabble.com>
Message-ID: <201508271752.46406.Antony.Stone@squid.open.source.it>

On Thursday 27 Aug 2015 at 17:21, Arkantos wrote:

> the community is now wanting a caching server.
> 
> i have zeroed in on CentOS+Squid+Webmin
> but we are unable to configure it as a "completely transparent cache"

If your community of users wants a caching proxy server, why make it 
transparent?

I'm assuming that you're in charge of both a DHCP server giving the users 
their IP addresses, and local caching DNS server doing name resolution, so why 
not just implement PAC https://en.wikipedia.org/wiki/Proxy_auto-config and let 
the clients discover the caching proxy and then use it?

Even if you can't do this, why not set up a proxy (not in transparent mode), 
tell the users how to connect to it, and let them use it because they want to?

Configuring a proxy such as squid in transparent mode never works quite as 
well as configuring it for explicit mode (where the clients know they're 
talking to a proxy), so this would give you a better end result as well as 
being easier to set up.


Hope that helps,


Antony.

PS: If you feel you really do want to set up a transparent proxy, but have 
failed to get Squid to do what you need, please at least let us know what you 
have tried so far so we can help you deal with the problem.  This means 
telling us:

 - the network configuration, being especially clear about where the Squid 
machine is in the network setup
 - the squid.conf you are trying to use (without comments or blank lines)
 - how you are testing it and finding that it doesn't work
 - what you are seeing (browser errors, log file messages, etc) to indicate 
that it doesn't work.


-- 
Atheism is a non-prophet-making organisation.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jorgeley at gmail.com  Thu Aug 27 17:04:38 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 14:04:38 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
Message-ID: <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>

more logs:
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1590) HttpHeaderEntry: created
HttpHeaderEntry 0x10541e8c0: 'Cache-Control : max-age=259200
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(972) addEntry: 0x7ffeda4c9eb0
adding entry: 10 at 7
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1192) has: 0x7ffeda4c9eb0
lookup for 9
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(762) packInto: 0x7ffeda4c9eb0
into 0x7ffeda4c9ee0
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(479) clean: cleaning hdr:
0x7ffeda4c9eb0 owner: 2
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e800: 'Host: www.grupoatuall.com.br'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e1c0: 'User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux
x86_64; rv:40.0) Gecko/20100101 Firefox/40.0'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e280: 'Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e3c0: 'Accept-Language:
pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e480: 'Accept-Encoding: gzip, deflate'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e540: 'Via: 1.1 firewall (squid)'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e5c0: 'X-Forwarded-For: 192.168.1.11'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(1596) ~HttpHeaderEntry:
destroying entry 0x10541e8c0: 'Cache-Control: max-age=259200'
2015/08/27 11:43:31.301 kid1| HttpHeader.cc(479) clean: cleaning hdr:
0x7ffeda4c9eb0 owner: 2
2015/08/27 11:43:31.301 kid1| http.cc(2217) sendRequest: HTTP Server local=
192.168.25.2:43127 remote=200.98.190.9:80 FD 66 flags=1
2015/08/27 11:43:31.301 kid1| http.cc(2218) sendRequest: HTTP Server
REQUEST:
---------
GET / HTTP/1.1^M
Host: www.grupoatuall.com.br^M
User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:40.0) Gecko/20100101
Firefox/40.0^M
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8^M
Accept-Language: pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3^M
Accept-Encoding: gzip, deflate^M
Via: 1.1 firewall (squid)^M
X-Forwarded-For: 192.168.1.11^M
Cache-Control: max-age=259200^M

2015-08-27 13:43 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> Just put the directive debug_options 11,2 ???
> more logs:
> 2015/08/27 11:43:31.291 kid1| FwdState.cc(383) startConnectionOrFail:
> http://www.grupoatuall.com.br/
> 2015/08/27 11:43:31.291 kid1| HttpRequest.cc(486) clearError: old error
> details: 0/0
> 2015/08/27 11:43:31.291 kid1| FwdState.cc(785) connectStart:
> fwdConnectStart: http://www.grupoatuall.com.br/
> 2015/08/27 11:43:31.291 kid1| pconn.cc(329) key:
> PconnPool::key(local=0.0.0.0 remote=200.98.190.9:80 flags=1,
> www.grupoatuall.com.br) is {200.98.190.9:80/www.grupoatuall.com.br}
> 2015/08/27 11:43:31.291 kid1| pconn.cc(439) pop: lookup for key {
> 200.98.190.9:80/www.grupoatuall.com.br} failed.
>
>
> 2015-08-27 13:36 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 28/08/2015 4:25 a.m., Jorgeley Junior wrote:
>> > increasing the log leve i got this:
>> > 2015/08/27 11:43:30.966 kid1| ipcache.cc(501) ipcache_nbgethostbyname:
>> > ipcache_nbgethostbyname: Name 'www.grupoatuall.com.br'.
>> > 2015/08/27 11:43:30.966 kid1| Address.cc(389) lookupHostIP: Given
>> Non-IP '
>> > www.grupoatuall.com.br': Name or service not known
>> > 2015/08/27 11:43:30.966 kid1| ipcache.cc(549) ipcache_nbgethostbyname:
>> > ipcache_nbgethostbyname: MISS for 'www.grupoatuall.com.br'
>> > Any other ideas???
>>
>> Thats not 11,2. The 11,2 level will output full HTTP protocol message
>> headers in current Squid.
>>
>> Amos
>>
>>
>
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/5a4525e4/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 17:21:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 05:21:19 +1200
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
Message-ID: <55DF470F.7070700@treenet.co.nz>

On 28/08/2015 5:04 a.m., Jorgeley Junior wrote:
> more logs:

Aha! the first bit. What Squid sent to the server:

> 2015/08/27 11:43:31.301 kid1| http.cc(2217) sendRequest: HTTP Server local=
> 192.168.25.2:43127 remote=200.98.190.9:80 FD 66 flags=1
> 2015/08/27 11:43:31.301 kid1| http.cc(2218) sendRequest: HTTP Server
> REQUEST:
> ---------
> GET / HTTP/1.1^M
> Host: www.grupoatuall.com.br^M
> User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:40.0) Gecko/20100101
> Firefox/40.0^M
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8^M
> Accept-Language: pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3^M
> Accept-Encoding: gzip, deflate^M
> Via: 1.1 firewall (squid)^M
> X-Forwarded-For: 192.168.1.11^M
> Cache-Control: max-age=259200^M
> 


Now. Notice the FD number (66) on the line at the top there. And look
for the very next set of headers with matching set of local=,remote=,FD
values but titled "HTTP Server RESPONSE".

That is apparently the reply that failed.

* Compare the timestamps of the request/reply lines to see if it reminds
you of any kind of timeout value you are aware of. Multiples of 30sec or
5min are common for timeout settings.

* anything visibly wrong about the reply headers?

Amos


From eliezer at ngtech.co.il  Thu Aug 27 17:48:06 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 27 Aug 2015 20:48:06 +0300
Subject: [squid-users] Does anyone have a working Juniper SRX with tproxy
	squid?
Message-ID: <55DF4D56.2030903@ngtech.co.il>

I am gathering information on different routing options for squid tproxy 
mode for quite some time.
I have a working settings for:
  - Cisco
  - Linux
  - FreeBSD
  - OpenBSD
  - Mikrotik

The topology I have tested it until now is at:
http://ngtech.co.il/squidblocker/topology1.png

The Edge router divert traffic to the squid instances using routing policy.

I have been reading about ways to make squid work with Juniper but they 
all use intercept mode and not tproxy.
A list of sources until now:
http://kb.juniper.net/InfoCenter/index?page=content&id=KB23300

https://andymillett.co.uk/2013/09/14/load-balancing-transparent-redirect-junos/

http://kb.juniper.net/InfoCenter/index?page=content&id=KB21046

http://forums.juniper.net/t5/SRX-Services-Gateway/SRX650-routing-instance-not-working/m-p/54130

http://forums.juniper.net/t5/SRX-Services-Gateway/port-80-redirection-on-srx650-cluster/m-p/53010

http://serverfault.com/questions/442385/how-to-route-all-network-traffic-for-vlan-through-a-proxy-server-on-srx

https://forum.ivorde.com/squid-http-s-transparent-proxy-with-juniper-srx-part-3-t14191.html

http://kb.juniper.net/InfoCenter/index?page=content&id=KB23895
###END SOURCES

I know that on FreeBSD and Linux I must refer to route each packet by 
itself or to mark the connection.
On juniper SRX devices I do not know what to do exactly.
I have seen an option to disable the flowd which follows the tcp\udp 
flows and I am not sure it is a requirement.

My current vSRX settings are at:
http://paste.ngtech.co.il/pdsltlobf

And the connection is being redirected from the client to the proxy and 
back from the proxy to the client.
The issue is that the traffic which flows from the internet back which 
suppose to be redirected into the proxy are flowing back to the client.

The issue as I identify it is that there is a routing decision based on 
some routing table.
The option I have seen here and there mentioned are to use a virtual router.

I am pretty sure there is some network admin here on the list which 
might have a clue about how to solve the reverse path traffic flow 
routing issue.

Thanks,
Eliezer



From jorgeley at gmail.com  Thu Aug 27 17:49:13 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 14:49:13 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <55DF470F.7070700@treenet.co.nz>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
 <55DF470F.7070700@treenet.co.nz>
Message-ID: <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>

Amos, thank you so much for attention, but sorry, I didn't understand what
you said.
So, I tried to change the http for https and it showed the website and i
added the security exception for no trusted certificate, but I really would
like that the squid didn't show the error.
Why http show de Zero Sized Reply and https no?

2015-08-27 14:21 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 28/08/2015 5:04 a.m., Jorgeley Junior wrote:
> > more logs:
>
> Aha! the first bit. What Squid sent to the server:
>
> > 2015/08/27 11:43:31.301 kid1| http.cc(2217) sendRequest: HTTP Server
> local=
> > 192.168.25.2:43127 remote=200.98.190.9:80 FD 66 flags=1
> > 2015/08/27 11:43:31.301 kid1| http.cc(2218) sendRequest: HTTP Server
> > REQUEST:
> > ---------
> > GET / HTTP/1.1^M
> > Host: www.grupoatuall.com.br^M
> > User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:40.0)
> Gecko/20100101
> > Firefox/40.0^M
> > Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8^M
> > Accept-Language: pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3^M
> > Accept-Encoding: gzip, deflate^M
> > Via: 1.1 firewall (squid)^M
> > X-Forwarded-For: 192.168.1.11^M
> > Cache-Control: max-age=259200^M
> >
>
>
> Now. Notice the FD number (66) on the line at the top there. And look
> for the very next set of headers with matching set of local=,remote=,FD
> values but titled "HTTP Server RESPONSE".
>
> That is apparently the reply that failed.
>
> * Compare the timestamps of the request/reply lines to see if it reminds
> you of any kind of timeout value you are aware of. Multiples of 30sec or
> 5min are common for timeout settings.
>
> * anything visibly wrong about the reply headers?
>
> Amos
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/05aa7649/attachment.htm>

From jorgeley at gmail.com  Thu Aug 27 18:13:59 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 15:13:59 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
 <55DF470F.7070700@treenet.co.nz>
 <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
Message-ID: <CAMeoTHnq9mD9T6wt03PNaV+t9ijVOJShgvS7PrFzeHzFqoxf+Q@mail.gmail.com>

seens like the web server is a ISS, look at that:
2015/08/27 11:44:11.087 kid1| ctx: enter level  0: '
http://www.grupoatuall.com.br/favicon.ico'
2015/08/27 11:44:11.087 kid1| http.cc(695) processReplyHeader:
processReplyHeader: key 'B8D92AE82DF4B8ED3202042600CED02D'
2015/08/27 11:44:11.087 kid1| HttpHeader.cc(446) HttpHeader: init-ing hdr:
0x20b0218 owner: 3
2015/08/27 11:44:11.087 kid1| cbdata.cc(299) cbdataInternalAlloc:
Allocating 0x20ada98
2015/08/27 11:44:11.087 kid1| HttpHeader.cc(1192) has: 0x20b0218 lookup for
49
2015/08/27 11:44:11.087 kid1| HttpHeader.cc(1192) has: 0x20b0218 lookup for
11
2015/08/27 11:44:11.087 kid1| HttpHeader.cc(1192) has: 0x20b0218 lookup for
26
2015/08/27 11:44:11.087 kid1| HttpHeader.cc(595) parse: parsing hdr:
(0x20b0218)
Content-Type: text/html^M
Server: Microsoft-IIS/7.5^M
X-Powered-By: ASP.NET^M
X-Powered-By-Plesk: PleskWin^M
Date: Thu, 27 Aug 2015 14:48:18 GMT^M
Content-Length: 1080^M
...(more logs)
2015/08/27 11:44:11.087 kid1| HttpMsg.cc(176) parse: HttpMsg::parse success
(190 bytes) near 'HTTP/1.1 404 Not Found^M
Content-Type: text/html^M
Server: Microsoft-IIS/7.5^M
X-Powered-By: ASP.NET^M
X-Powered-By-Plesk: PleskWin^M
Date: Thu, 27 Aug 2015 14:48:18 GMT^M
Content-Length: 1080^M
^M
<HTML>^M
<HEAD>^M
<TITLE>404 Not Found</TITLE>^M
</HEAD>^M
<BODY>^M
<H1>Not Found</H1>^M
The requested document was not found on this server.^M
<P>^M
<HR>^M
<ADDRESS>^M
Web Server at
&#103;&#114;&#117;&#112;&#111;&#97;&#116;&#117;&#97;&#108;&#108;&#46;&#99;&#111;&#109;&#46;&#98;&#114;^M
</ADDRESS>^M
</BODY>^M
</HTML>^M
^M
<!--^M
   - Unfortunately, Microsoft has added a clever new^M
   - "feature" to Internet Explorer. If the text of^M
   - an error's message is "too small", specifically^M
   - less than 512 bytes, Internet Explorer returns^M
   - its own error message. You can turn that off,^M
   - but it's pretty tricky to find switch called^M
   - "smart error messages". That means, of course,^M
   - that short error messages are censored by default.^M
   - IIS always returns error messages that are long^M
   - enough to make Internet Explorer happy. The^M
   - workaround is pretty simple: pad the error^M
   - message with a big comment like this to push it^M
   - over the five hundred and twelve bytes minimum.^M
   - Of course, that's exactly what you're reading^M
   - right now.^M
   -->^M
'
2015/08/27 11:44:11.087 kid1| http.cc(739) processReplyHeader: HTTP Server
local=192.168.25.2:43147 remote=200.98.190.9:80 FD 98 flags=1
2015/08/27 11:44:11.087 kid1| http.cc(740) processReplyHeader: HTTP Server
REPLY:
---------
HTTP/1.1 404 Not Found^M

572544,4      69%


2015-08-27 14:49 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> Amos, thank you so much for attention, but sorry, I didn't understand what
> you said.
> So, I tried to change the http for https and it showed the website and i
> added the security exception for no trusted certificate, but I really would
> like that the squid didn't show the error.
> Why http show de Zero Sized Reply and https no?
>
> 2015-08-27 14:21 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 28/08/2015 5:04 a.m., Jorgeley Junior wrote:
>> > more logs:
>>
>> Aha! the first bit. What Squid sent to the server:
>>
>> > 2015/08/27 11:43:31.301 kid1| http.cc(2217) sendRequest: HTTP Server
>> local=
>> > 192.168.25.2:43127 remote=200.98.190.9:80 FD 66 flags=1
>> > 2015/08/27 11:43:31.301 kid1| http.cc(2218) sendRequest: HTTP Server
>> > REQUEST:
>> > ---------
>> > GET / HTTP/1.1^M
>> > Host: www.grupoatuall.com.br^M
>> > User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:40.0)
>> Gecko/20100101
>> > Firefox/40.0^M
>> > Accept:
>> text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8^M
>> > Accept-Language: pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3^M
>> > Accept-Encoding: gzip, deflate^M
>> > Via: 1.1 firewall (squid)^M
>> > X-Forwarded-For: 192.168.1.11^M
>> > Cache-Control: max-age=259200^M
>> >
>>
>>
>> Now. Notice the FD number (66) on the line at the top there. And look
>> for the very next set of headers with matching set of local=,remote=,FD
>> values but titled "HTTP Server RESPONSE".
>>
>> That is apparently the reply that failed.
>>
>> * Compare the timestamps of the request/reply lines to see if it reminds
>> you of any kind of timeout value you are aware of. Multiples of 30sec or
>> 5min are common for timeout settings.
>>
>> * anything visibly wrong about the reply headers?
>>
>> Amos
>>
>
>
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/b52c73cc/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 19:03:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 07:03:58 +1200
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
 <55DF470F.7070700@treenet.co.nz>
 <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
Message-ID: <55DF5F1E.4050905@treenet.co.nz>

On 28/08/2015 5:49 a.m., Jorgeley Junior wrote:
> Amos, thank you so much for attention, but sorry, I didn't understand what
> you said.

Nevermind. The website code is broken.

I have been looking into it from here using those request headers from
your log.

What I see happening is that the server starts responding. Then the PHP
code it is running hangs for a very long time. If you wait long enough
it will pop out part of a page and a PHP error message about its
database connection script and some timeout.

The best I could get was over a minute (78 seconds) delay before
anything at all happened. Usually a bit longer.

I think something in your network is terminating the server connection
after it takes too long. NAT and high speed router systems tend to have
a 30 second maximum wait between TCP packets before they close the
connection.

Either way the website server itself is very broken.


> So, I tried to change the http for https and it showed the website and i
> added the security exception for no trusted certificate, but I really would
> like that the squid didn't show the error.
> Why http show de Zero Sized Reply and https no?

Different protocols and ports.

I still see the same delays, partial page and database errors when
connecting with HTTPS. But kept digging to see why you might be getting
a page...

It seems to be an IPv6 server sitting behind some form of gateway access
network and only pretending to be IPv4-only. When sending it a
X-Forwarded-For header claiming to be an IPv6-enabled browser it seems
to operate just fine.

So, try adding this to your squid.conf:

 acl magicXff dstdomain .grupoatuall.com.br
 request_header_access X-Forwarded-For deny magicXff
 request_header_replace X-Forwarded-For ::1


Amos



From jorgeley at gmail.com  Thu Aug 27 19:51:39 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 27 Aug 2015 16:51:39 -0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <55DF5F1E.4050905@treenet.co.nz>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
 <55DF470F.7070700@treenet.co.nz>
 <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
 <55DF5F1E.4050905@treenet.co.nz>
Message-ID: <CAMeoTHm_nzRi9ypmrHz4kkxAxTe6dp-env_CVNF_fdXSHo2ZRQ@mail.gmail.com>

You're the man Amos!!! You're the man!!! Thanks!!! Thanks so so much!!!
that's solved the problem, but I'm thinking if it solved just for this
domain, so can it happen again with another domains, ok? No way to solve
for future errors of this same type?

2015-08-27 16:03 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 28/08/2015 5:49 a.m., Jorgeley Junior wrote:
> > Amos, thank you so much for attention, but sorry, I didn't understand
> what
> > you said.
>
> Nevermind. The website code is broken.
>
> I have been looking into it from here using those request headers from
> your log.
>
> What I see happening is that the server starts responding. Then the PHP
> code it is running hangs for a very long time. If you wait long enough
> it will pop out part of a page and a PHP error message about its
> database connection script and some timeout.
>
> The best I could get was over a minute (78 seconds) delay before
> anything at all happened. Usually a bit longer.
>
> I think something in your network is terminating the server connection
> after it takes too long. NAT and high speed router systems tend to have
> a 30 second maximum wait between TCP packets before they close the
> connection.
>
> Either way the website server itself is very broken.
>
>
> > So, I tried to change the http for https and it showed the website and i
> > added the security exception for no trusted certificate, but I really
> would
> > like that the squid didn't show the error.
> > Why http show de Zero Sized Reply and https no?
>
> Different protocols and ports.
>
> I still see the same delays, partial page and database errors when
> connecting with HTTPS. But kept digging to see why you might be getting
> a page...
>
> It seems to be an IPv6 server sitting behind some form of gateway access
> network and only pretending to be IPv4-only. When sending it a
> X-Forwarded-For header claiming to be an IPv6-enabled browser it seems
> to operate just fine.
>
> So, try adding this to your squid.conf:
>
>  acl magicXff dstdomain .grupoatuall.com.br
>  request_header_access X-Forwarded-For deny magicXff
>  request_header_replace X-Forwarded-For ::1
>
>
> Amos
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150827/551969ea/attachment.htm>

From eliezer at ngtech.co.il  Thu Aug 27 19:56:30 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 27 Aug 2015 22:56:30 +0300
Subject: [squid-users] Zero Sized Reply
In-Reply-To: <CAMeoTHm_nzRi9ypmrHz4kkxAxTe6dp-env_CVNF_fdXSHo2ZRQ@mail.gmail.com>
References: <CAMeoTHmMvgUVH343pW2-QWOJbz+e7ueg1ifjxfNvOsJhReN-cA@mail.gmail.com>
 <55DEA85E.3010305@treenet.co.nz>
 <CAMeoTHnP+JbxT-roaBwi1cbhRTnE=K_B8KSwPC2yaFkMOMWHKQ@mail.gmail.com>
 <55DF3449.2080904@treenet.co.nz>
 <CAMeoTHk5zn-GPWK0SKJndhGkyEXvcFL-arEseRz1C9qSqBBFOQ@mail.gmail.com>
 <55DF3CA8.2000306@treenet.co.nz>
 <CAMeoTHnK3b73xjoP-vLhLh_gk4RW=zHCgOreKe3Be6bc2v8Ekg@mail.gmail.com>
 <CAMeoTH=w78AXkDeKjHH7OFKEnqVpXVKqUhioFQrWCN5Kt7KeLw@mail.gmail.com>
 <55DF470F.7070700@treenet.co.nz>
 <CAMeoTHkJOU3dRezsFyAubdhf_=bXiDmAy2Hs=8tqgCRy5aodyg@mail.gmail.com>
 <55DF5F1E.4050905@treenet.co.nz>
 <CAMeoTHm_nzRi9ypmrHz4kkxAxTe6dp-env_CVNF_fdXSHo2ZRQ@mail.gmail.com>
Message-ID: <55DF6B6E.5090207@ngtech.co.il>

Solve what exactly??
If the site is broken I think that the only solution is for the site 
admin to fix the issue...

Eliezer

On 27/08/2015 22:51, Jorgeley Junior wrote:
> You're the man Amos!!! You're the man!!! Thanks!!! Thanks so so much!!!
> that's solved the problem, but I'm thinking if it solved just for this
> domain, so can it happen again with another domains, ok? No way to solve
> for future errors of this same type?
>
> 2015-08-27 16:03 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 28/08/2015 5:49 a.m., Jorgeley Junior wrote:
>>> Amos, thank you so much for attention, but sorry, I didn't understand
>> what
>>> you said.
>>
>> Nevermind. The website code is broken.
>>
>> I have been looking into it from here using those request headers from
>> your log.
>>
>> What I see happening is that the server starts responding. Then the PHP
>> code it is running hangs for a very long time. If you wait long enough
>> it will pop out part of a page and a PHP error message about its
>> database connection script and some timeout.
>>
>> The best I could get was over a minute (78 seconds) delay before
>> anything at all happened. Usually a bit longer.
>>
>> I think something in your network is terminating the server connection
>> after it takes too long. NAT and high speed router systems tend to have
>> a 30 second maximum wait between TCP packets before they close the
>> connection.
>>
>> Either way the website server itself is very broken.
>>
>>
>>> So, I tried to change the http for https and it showed the website and i
>>> added the security exception for no trusted certificate, but I really
>> would
>>> like that the squid didn't show the error.
>>> Why http show de Zero Sized Reply and https no?
>>
>> Different protocols and ports.
>>
>> I still see the same delays, partial page and database errors when
>> connecting with HTTPS. But kept digging to see why you might be getting
>> a page...
>>
>> It seems to be an IPv6 server sitting behind some form of gateway access
>> network and only pretending to be IPv4-only. When sending it a
>> X-Forwarded-For header claiming to be an IPv6-enabled browser it seems
>> to operate just fine.
>>
>> So, try adding this to your squid.conf:
>>
>>   acl magicXff dstdomain .grupoatuall.com.br
>>   request_header_access X-Forwarded-For deny magicXff
>>   request_header_replace X-Forwarded-For ::1
>>
>>
>> Amos
>>
>>
>
>
> --
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From Kuntal_Basak at bnz.co.nz  Thu Aug 27 21:43:23 2015
From: Kuntal_Basak at bnz.co.nz (Kuntal_Basak at bnz.co.nz)
Date: Fri, 28 Aug 2015 09:43:23 +1200
Subject: [squid-users] completely transparent Squid
In-Reply-To: <1440692464604-4672904.post@n4.nabble.com>
References: <1440692464604-4672904.post@n4.nabble.com>
Message-ID: <OF0757628F.262C65F9-ONCC257EAE.00771A80-CC257EAE.00775448@nab.com.au>

Could you please un-subscribe me ?


Cheers,

Kuntal
Senior Infrastructure Architecture and Design Specialist
Infrastructure Architecture and Design
Bank of New Zealand

DDI: 04-474 6722
Mobile: 021-2408034

 ?Success is not final, failure is not fatal: it is the courage to 
continue that counts.? - Winston Churchill 



From:   Arkantos <221184 at gmail.com>
To:     squid-users at lists.squid-cache.org, 
Date:   28/08/2015 04:26 a.m.
Subject:        [squid-users] completely transparent Squid
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org>



hello everybody,

my friend and I, happen to run the neighborhood cable and wifi network.
Costs are picked up by the users' community, and we get a salary for 
running
around.

we have about 35 users

we get around 60 mb from the ISP and then we have deployed Inventum Unify
Cloud MSC for user management (only for login, B/w control and URL 
logging).

the community is now wanting a caching server.

i have zeroed in on CentOS+Squid+Webmin
but we are unable to configure it as a "completely transparent cache"

can anybody help us to configure?
we can pay if wanted. but fees should not be astronomical.


please help.
Arkantos.

221184 at gmail



--
View this message in context: 
http://squid-web-proxy-cache.1019090.n4.nabble.com/completely-transparent-Squid-tp4672904.html

Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


CAUTION - This message may contain privileged and confidential information
intended only for the use of the addressee named above. If you are not the
intended recipient of this message you are hereby notified that any use,
dissemination, distribution or reproduction of this message is prohibited.
This email was sent by the Bank of New Zealand. You can contact us on
0800 ASK BNZ (0800 275 269). Any views expressed in this message are those
of the individual sender and may not necessarily reflect the views of Bank
of New Zealand.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/c77caaac/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 22:19:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 10:19:04 +1200
Subject: [squid-users] completely transparent Squid
In-Reply-To: <OF0757628F.262C65F9-ONCC257EAE.00771A80-CC257EAE.00775448@nab.com.au>
References: <1440692464604-4672904.post@n4.nabble.com>
 <OF0757628F.262C65F9-ONCC257EAE.00771A80-CC257EAE.00775448@nab.com.au>
Message-ID: <55DF8CD8.9010000@treenet.co.nz>

On 28/08/2015 9:43 a.m., Kuntal_Basak at bnz.co.nz wrote:
> Could you please un-subscribe me ?
> 
> 

Hi Kuntal,

You need to begin the process by entering the email you want
unsubscribed into the unsubscribe form at the bottom of the listinfo
page whose link is here:

> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

(I have just done this step for you).

Then you need to check for the confirmation email that should arrive and
follow the instructions there. If you do not do that step yourself the
email will remain subscribed.


If you have email forwarded from another address in your company than
the one you are posting from. Then you need to arrange for that address
to be no longer delivering to you, or unsubscribed using the above
process for it too.

>
> --
> View this message in context:
>
http://squid-web-proxy-cache.1019090.n4.nabble.com/completely-transparent-Squid-tp4672904.html
>
> Sent from the Squid - Users mailing list archive at Nabble.com.

If that was you receiving this through Nabble, gmane or another email
relay service then you may need to unsubscribe from there separately.
They are independent services only relaying our list messages.


Hope this helps.

Amos Jeffries
Squid Software Foundation



From chip_pop at hotmail.com  Thu Aug 27 22:35:12 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 27 Aug 2015 15:35:12 -0700 (PDT)
Subject: [squid-users] cpu high 100%
Message-ID: <1440714912575-4672918.post@n4.nabble.com>

hi i don't know if someone facing hi cpu after 6 or more hoer 
my server is xeon quad 2 cpu 2.2geg 
ubuntu 14 latest i never had problem with v 2.7
on normal use with peek time max cpu get up to 20% no more
then up to 40 50 %
Squid Object Cache: Version 3.5.6
Build Info: 
Service Name: squid
Start Time:     Thu, 27 Aug 2015 08:15:34 GMT
Current Time:   Thu, 27 Aug 2015 22:08:38 GMT
Connection information for squid:
        Number of clients accessing cache:      (client_db off)
        Number of HTTP requests received:       632823
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   759.6
        Average ICP messages per minute since start:    0.0
        Select loop called: 27134859 times, 1.842 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 16.5%, 60min: 14.1%
        Hits as % of bytes sent:        5min: -6.1%, 60min: -52.5%
        Memory hits as % of hit requests:       5min: 42.8%, 60min: 34.1%
        Disk hits as % of hit requests: 5min: 29.4%, 60min: 42.5%
        Storage Swap size:      1005260424 KB
        Storage Swap capacity:  65.4% used, 34.6% free
        Storage Mem size:       8953296 KB
        Storage Mem capacity:   71.2% used, 28.8% free
        Mean Object Size:       131.49 KB
        Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.27332  0.42149
        Cache Misses:          0.32154  0.46965
        Cache Hits:            0.01469  0.02451
        Near Hits:             0.24524  0.37825
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.08717  0.07968
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        49984.156 seconds
        CPU Time:       6753.848 seconds
        CPU Usage:      13.51%
        CPU Usage, 5 minute avg:        60.63%
        CPU Usage, 60 minute avg:       51.62%
        Maximum Resident Size: 46576384 KB
        Page faults with physical i/o: 1
Memory accounted for:
        Total accounted:       -1789337 KB
        memPoolAlloc calls:       455
        memPoolFree calls:  223754685
File descriptor usage for squid:
        Maximum number of file descriptors:   32768
        Largest file desc currently in use:    428
        Number of file desc currently in use:  182
        Files queued for open:                   0
        Available number of file descriptors: 32586
        Reserved number of file descriptors:   100
        Store Disk files open:                   5
Internal Data Structures:
        7645004 StoreEntries
        199880 StoreEntries with MemObjects
        199854 Hot Object Cache Items
        7644926 on-disk objects

then it go up higher


Start Time:     Thu, 27 Aug 2015 08:15:34 GMT
Current Time:   Thu, 27 Aug 2015 22:27:53 GMT
Connection information for squid:
        Number of clients accessing cache:      (client_db off)
        Number of HTTP requests received:       644780
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   756.5
        Average ICP messages per minute since start:    0.0
        Select loop called: 27407388 times, 1.866 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 22.6%, 60min: 16.3%
        Hits as % of bytes sent:        5min: -19.7%, 60min: -26.7%
        Memory hits as % of hit requests:       5min: 53.7%, 60min: 39.0%
        Disk hits as % of hit requests: 5min: 28.9%, 60min: 40.2%
        Storage Swap size:      1005560000 KB
        Storage Swap capacity:  65.5% used, 34.5% free
        Storage Mem size:       9147656 KB
        Storage Mem capacity:   72.7% used, 27.3% free
        Mean Object Size:       131.47 KB
        Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.33943  0.32154
        Cache Misses:          0.39928  0.37825
        Cache Hits:            0.03829  0.02899
        Near Hits:             0.27332  0.30459
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.09535  0.08334
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        51139.181 seconds
        CPU Time:       7501.588 seconds
        CPU Usage:      14.67%
        CPU Usage, 5 minute avg:        67.96%
        CPU Usage, 60 minute avg:       58.95%
        Maximum Resident Size: 47446256 KB
        Page faults with physical i/o: 1
Memory accounted for:
        Total accounted:       -1578094 KB
        memPoolAlloc calls:       455
        memPoolFree calls:  227574133
File descriptor usage for squid:
        Maximum number of file descriptors:   32768
        Largest file desc currently in use:    428
        Number of file desc currently in use:  314
        Files queued for open:                   0
        Available number of file descriptors: 32454
        Reserved number of file descriptors:   100
        Store Disk files open:                  12
Internal Data Structures:
        7648546 StoreEntries
        204499 StoreEntries with MemObjects
        204466 Hot Object Cache Items
        7648462 on-disk objects

wen it reach 100% it stay at 100% cpu  never goes down untill i restart it 
and then again the same propaganda
   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                        
 14655 proxy     20   0 11.433g 0.011t  10284 R  74.6 36.2 126:54.60 squid                                                          
    10 root      20   0       0      0      0 S   0.3  0.0   5:36.92 rcuos/1                                                        
  1949 root      20   0   24184   3392   2372 R   0.3  0.0  14:46.89 top                                                            
     1 root      20   0   33388   3572   2280 S   0.0  0.0   0:03.57 init                                                           
     2 root      20   0       0      0      0 S   0.0  0.0   0:00.11
kthreadd                                                       
     3 root      20   0       0      0      0 S   0.0  0.0   0:01.12
ksoftirqd/0                                                    
     5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00
kworker/0:0H                                                   
     6 root      20   0       0      0      0 S   0.0  0.0   0:00.00
kworker/u480:0                                                 
     8 root      20   0       0      0      0 S   0.0  0.0   4:17.68
rcu_sched                                                      
     9 root      20   0       0      0      0 S   0.0  0.0   0:12.32 rcuos/0                                                        
    11 root      20   0       0      0      0 S   0.0  0.0   0:13.10 rcuos/2     
here at 74%
i stop it caus it crache somtime
even i stop  all the traf.. going to squid and squid stay ant hi cpu usage
lol
it look like internal loop by some code pls  help
no error in my logg
just  a lots of 
2015/08/28 01:24:28 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/08/28 01:24:28 kid1| 	/mnt/sda/02/58/001C587D
2015/08/28 01:24:29 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/08/28 01:24:29 kid1| 	/mnt/sda/02/58/001C5882
2015/08/28 01:25:18 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/08/28 01:25:18 kid1| 	/mnt/sdc/03/AB/0003AB48

and  lots of
 WARNING: HTTP: Invalid Response: No object data received for http
that all
processor       : 7
vendor_id       : GenuineIntel
cpu family      : 6
model           : 45
model name      : Intel(R) Xeon(R) CPU E5-2407 0 @ 2.20GHz
stepping        : 7
microcode       : 0x710
cpu MHz         : 1224.265
cache size      : 10240 KB
physical id     : 1
siblings        : 4
core id         : 3
cpu cores       : 4




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cpu-high-100-tp4672918.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Thu Aug 27 23:50:02 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 27 Aug 2015 17:50:02 -0600
Subject: [squid-users] peek all step with bump instance of proxy
In-Reply-To: <CAOoFbVfiUV6oL_woB0WTSfdr_DnXobpmr6k4drs_opEMgOMUpQ@mail.gmail.com>
References: <CAOoFbVfiUV6oL_woB0WTSfdr_DnXobpmr6k4drs_opEMgOMUpQ@mail.gmail.com>
Message-ID: <55DFA22A.4040603@measurement-factory.com>

On 08/27/2015 04:50 AM, john jacob wrote:

> with the proxy connections I am encountering some
> peculiar behavior with scenario 3 (ie when a non bumped https CONNECT is
> denied by eCAP). Instead of terminating the connection, it is logged as
> TAG_NONE/200 in the access log and getting bumped (a dynamic certificate
> is generated) and then getting terminated. The behavior disappears and
> works if I comment the "peek all" line.
> 
> I am not sure if this is a bug or an expected behavior.

Most of the stuff you are describing matches my expectations, but there
is not enough information in a couple of cases:

* "getting bumped and then getting terminated". Terminated without
serving an error page to the bumped client? Is there a request after
CONNECT? Is the client happy about the bumped connection? How many
CONNECT requests does your eCAP adapter see (and deny) in this case?

* "works if I comment the 'peek all' line". Works how? I would expect
Squid to bump and then serve an error page to the user on a bumped
connection. How many CONNECT requests does your eCAP adapter see (and
deny) in this case?

Here is some background so that you can debug this further:

Denying a CONNECT request implies serving an error page to the user.
However, the user will not see the error page if Squid sends it as a
response to the CONNECT request (due to lazy browsers security policies;
nothing to do with Squid). The only way to show that error page to the
user is to bump the client connection and then serve the error page over
that bumped connection in response to the first bumped request.

If your eCAP adapter denies CONNECT during an SslBump step, Squid should
bump the client connection (200 OK) and serve an error page to the user
in response to the first request on that bumped connection, if any
(TAG_NONE/403?).

You can find more information about this behaviour at
http://bazaar.launchpad.net/~squid/squid/trunk/revision/13759
but some of that old info is probably out of date by now.



> Of course the proxy bumped connection works fine if I selectively peek
> for intercepted connections (ssl_bump peek <if only in intercepted
> mode>), but in this case I am getting duplicate entries in the access
> log file (ie 2 CONNECT log messages for each https CONNECT) for
> intercepted mode https connections.

This is expected if you peek at step1. The second CONNECT should have
SNI information (but it often does not -- it is complicated and there
are bugs/missing features in that area of the code). If you do not need
SNI, you can make all decisions during the first CONNECT.


> The same goes for other ACL
> combinations like the below resulting in duplicated log messages
> 
> ssl_bump server-first <ip of the domain to be bumped>
> ssl_bump splice <only if the request hit the proxy ip:port and not the
> intercept/transparent ip :port>
> ssl_bump peek all
> ssl_bump splice all

In general, it is a bad idea to combine legacy (server-first) and modern
(peek/stare/splice/bump/terminate) rules. If at all possible, use modern
rules.

Alex.



From squid3 at treenet.co.nz  Fri Aug 28 01:35:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 13:35:09 +1200
Subject: [squid-users] peek all step with bump instance of proxy
In-Reply-To: <CAOoFbVfiUV6oL_woB0WTSfdr_DnXobpmr6k4drs_opEMgOMUpQ@mail.gmail.com>
References: <CAOoFbVfiUV6oL_woB0WTSfdr_DnXobpmr6k4drs_opEMgOMUpQ@mail.gmail.com>
Message-ID: <55DFBACD.3060507@treenet.co.nz>

On 27/08/2015 10:50 p.m., john jacob wrote:
> Hi All,
> 
> I am trying to configure a squid filtering instance which serves both proxy
> and intercepted (transparent) connections. Filtering is accomplished by a
> Request eCAP adapter which have something like
> 
> if(IsDenied() && RequestMethod=="CONNECT")
> {
>                           // Gives TAG_NONE/403 in the access log
>  hostx->blockVirgin();
>  return;
> }
> 
> I also have a requirement to bump a particular domain and peek other https
> connections for intercepted mode. So there are 3 possible
> outcomes/filtering decision for any https connections hitting this server.
> They are
> 
> 1) Bump and allow the access
> 2) Non bumped and allowed access
> 3) Non bumped and denied access, by the code given above in eCAP adapter
> 
> My squid (tried with v3.5.6 and v3.5.7-20150823-r13895, same outcome)
> config looks like below
> .
> .
> .
> #  TAG: ssl_bump
> ssl_bump server-first <ip of the domain to be bumped>
> ssl_bump peek all
> ssl_bump splice all
> .

Dont use those three options together. The "server-first" option is a
backward compatibility option only.

The "bump" action is usualy best to use in its place. This config looks
like a case where that is true.

> .
> .
> http_port <proxy ip>:<port> ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=<path>
> 
> 
> https_port <intercept/transparent ip>:<port> intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=<path>
> 
> Things are fine with the intercepted connections (for all the 3 scenarios).
> But with the proxy connections I am encountering some peculiar behavior
> with scenario 3 (ie when a non bumped https CONNECT is denied by eCAP).
> Instead of terminating the connection, it is logged as TAG_NONE/200 in the
> access log and getting bumped (a dynamic certificate is generated) and then
> getting terminated. The behavior disappears and works if I comment the
> "peek all" line.
> 
> I am not sure if this is a bug or an expected behavior.

Certain very popular browsers refuse to show the user any error messages
output by a proxy in response to a CONNECT message. Just a bland
connection failed message of their own.

To get around that we have a nasty hack that delays emitting error pages
until after the bumping has been done. I think that is what you are
seeing happen. There may well be bugs in the hack itself, but the
behaviours you describe all seem like logical side effects given what it
does.
 - delays the 403 output -> logs 200 for the 'CONNECT', then
 - bumping starts and does peek, then
 - splice starts, and detects 403 to be sent back.
 - but thats impossible in splice, so terminate.

Theres also a good possibility Squid is sending the 403 as plain-text
instead of terminating or delivering TLS handshake data. But the
client/browser simply not showing it to you because well, browsers.


> 
> Of course the proxy bumped connection works fine if I selectively peek for
> intercepted connections (ssl_bump peek <if only in intercepted mode>), but
> in this case I am getting duplicate entries in the access log file (ie 2
> CONNECT log messages for each https CONNECT) for intercepted mode https
> connections.The same goes for other ACL combinations like the below
> resulting in duplicated log messages
> 
> ssl_bump server-first <ip of the domain to be bumped>
> ssl_bump splice <only if the request hit the proxy ip:port and not the
> intercept/transparent ip :port>
> ssl_bump peek all
> ssl_bump splice all
> 
> Regards,
> John


Amos



From dan at getbusi.com  Fri Aug 28 05:27:31 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 28 Aug 2015 15:27:31 +1000
Subject: [squid-users] Any plan for an SSL bump mode ACL?
Message-ID: <82083353-7550-4121-A9D2-45A2368E9FE4@getbusi.com>

I?m trying to figure out if there?s a way to avoid those 0 byte ?peeked? requests being processed by the rest of our external ACLs etc. by allowing them early on in the transaction.

Unfortunately there doesn?t seem to be a way to target just those ones with http_access?the TAG_NONE isn?t an actual method and and there?s no ACL for the bump mode?without also targeting the spliced ones.

Any ideas, denizens of the mailing list?

From marko.cupac at mimar.rs  Fri Aug 28 08:36:56 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Fri, 28 Aug 2015 10:36:56 +0200
Subject: [squid-users] completely transparent Squid
In-Reply-To: <1440692464604-4672904.post@n4.nabble.com>
References: <1440692464604-4672904.post@n4.nabble.com>
Message-ID: <20150828103656.7e761012@efreet>

On Thu, 27 Aug 2015 09:21:04 -0700 (PDT)
Arkantos <221184 at gmail.com> wrote:

> hello everybody,
> 
> my friend and I, happen to run the neighborhood cable and wifi
> network. Costs are picked up by the users' community, and we get a
> salary for running around.
> 
> we have about 35 users
> 
> we get around 60 mb from the ISP and then we have deployed Inventum
> Unify Cloud MSC for user management (only for login, B/w control and
> URL logging).
> 
> the community is now wanting a caching server.
> 
> i have zeroed in on CentOS+Squid+Webmin
> but we are unable to configure it as a "completely transparent cache"
> 
> can anybody help us to configure?
> we can pay if wanted. but fees should not be astronomical.

If I was to implement transparent proxy for 35 users, I would put it on
edge firewall running OpenBSD + PF + Squid.

There's (quite old but still relevant) howto:
http://www.kernel-panic.it/openbsd/proxy/index.html

Regards,
-- 
Marko Cupa?
https://www.mimar.rs/


From ow97 at outlook.com  Fri Aug 28 09:58:36 2015
From: ow97 at outlook.com (Oliver Webb)
Date: Fri, 28 Aug 2015 10:58:36 +0100
Subject: [squid-users] Internet Explorer error with SSL bumping
In-Reply-To: <20150828103656.7e761012@efreet>
References: <1440692464604-4672904.post@n4.nabble.com>,
 <20150828103656.7e761012@efreet>
Message-ID: <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>

I have transparent SSL bumping working perfectly in Chrome and Safari (iOS and Windows 7) and Internet Explorer *on Windows Phone*, and by perfectly I mean no certificate warnings of any description for any site everything just behaves normally (apart from the sites certificate being signed by me.) However in Internet Explorer 11 on Windows 7 I get the following message for all secure bumped sites (secure sites like ebay for example load fine because I have configured not to be bumped and also unsecure sites load fine as well)

     This page can?t be displayed

     Turn on TLS 1.0, TLS 1.1, and TLS 1.2 in Advanced settings and try connecting to 
     https://google.co.uk again. If this error persists, contact your site administrator.

I just wondered if anyone had any bright ideas as to what might be up.

Thanks 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/23d29819/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 28 11:08:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 23:08:46 +1200
Subject: [squid-users] Any plan for an SSL bump mode ACL?
In-Reply-To: <82083353-7550-4121-A9D2-45A2368E9FE4@getbusi.com>
References: <82083353-7550-4121-A9D2-45A2368E9FE4@getbusi.com>
Message-ID: <55E0413E.6000906@treenet.co.nz>

On 28/08/2015 5:27 p.m., Dan Charlesworth wrote:
> I?m trying to figure out if there?s a way to avoid those 0 byte
> ?peeked? requests being processed by the rest of our external ACLs
> etc. by allowing them early on in the transaction.
> 
> Unfortunately there doesn?t seem to be a way to target just those
> ones with http_access?the TAG_NONE isn?t an actual method and and
> there?s no ACL for the bump mode?without also targeting the spliced
> ones.


If your helpers logic cant handle CONNECT method requests then you
should be using the default configs CONNECT acl definition to skip them.

The synthetic ones are just what regular explicit-proxy HTTP would
actually have at that point had HTTP been the used properly instead of
interception or SSL-Bump.



For intercepted port 443 traffic the synthetic/fake CONNECT requests
should match something like this:

 acl portA myportname the-https_port-name
 acl hasUA req_header User-Agent .+
 acl syntheticCONNECT all-of portA CONNECT hasUA

 http_access allow syntheticCONNECT
 ...

I have not tested this. All the synthetic CONNECT used by squid are
generated the same right now, and can be emulated by a client.
 So you should use with care with teh above. And definitely try to avoid
eliding logs in case one comes from outside that matches.

I plan to have the squid User-Agent string added in there when it more
convenient. So the above will not work long-term. And a Forwarded header
with something to indicate intercept is also an eventual possibility.

Amos



From squid3 at treenet.co.nz  Fri Aug 28 11:28:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2015 23:28:53 +1200
Subject: [squid-users] Internet Explorer error with SSL bumping
In-Reply-To: <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>
References: <1440692464604-4672904.post@n4.nabble.com>
 <20150828103656.7e761012@efreet>
 <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>
Message-ID: <55E045F5.4090103@treenet.co.nz>

On 28/08/2015 9:58 p.m., Oliver Webb wrote:
> I have transparent SSL bumping working perfectly in Chrome and
> Safari
(iOS and Windows 7) and Internet Explorer *on Windows Phone*, and by
perfectly I mean no certificate warnings of any description for any site
everything just behaves normally (apart from the sites certificate being
signed by me.) However in Internet Explorer 11 on Windows 7 I get the
following message for all secure bumped sites (secure sites like ebay
for example load fine because I have configured not to be bumped and
also unsecure sites load fine as well)

>      This page can?t be displayed
> 
>      Turn on TLS 1.0, TLS 1.1, and TLS 1.2 in Advanced settings and try connecting to 
>      https://google.co.uk again. If this error persists, contact your site administrator.
> 
> I just wondered if anyone had any bright ideas as to what might be up.

The complete lack of warnings is a BAD sign. It means the certificate
mimic feature is probably is not working at all.

Mimic is supposed to pass certificate flaws in the server certs through
to the client/browser so all the security go/die decisions can be made
by the end-users own preference confg.


The error message you show implies that you have configured your proxy
for SSLv3-only or SSLv2-only. At least on the listening ports the
browser is connecting to. Though since it was displayed by a browser we
can't be 100% sure it contains truth (SSL-bump is feeding some bold lies
to it).


PS. If not 3.5.7 or a later snapshot please try an upgrade.

PPS. I'm told people are having pain from OpenSSL 0.9.8 apparently
trying to do TLS/1.0 in a way Squid does not handle properly right now.
If that library version is installed on the client you may need to wait
for a fix the guys are working on as I type this (ETA unknown). Though
if you can get the client to upgrade to a more current and secure
OpenSSL that would be even better.

Amos



From ow97 at outlook.com  Fri Aug 28 12:17:20 2015
From: ow97 at outlook.com (Oliver Webb)
Date: Fri, 28 Aug 2015 13:17:20 +0100
Subject: [squid-users] Internet Explorer error with SSL bumping
In-Reply-To: <55E045F5.4090103@treenet.co.nz>
References: <1440692464604-4672904.post@n4.nabble.com>,
 <20150828103656.7e761012@efreet>,
 <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>,
 <55E045F5.4090103@treenet.co.nz>
Message-ID: <SNT147-W7CA7DF76C058092BF14EECA6E0@phx.gbl>

Thanks for your reply Amos. I will explain a bit more of my setup in the hope it clarifies a few of the issues.

I have installed the certificate portion of squids key/cert into the trusted root store of all the devices concerned all clients see the "server's" certificate as being signed by squid's private key not the origin servers.
I have the following line in squid.conf to specifically stop the use of SSL
? ??sslproxy_options NO_SSLv2 NO_SSLv3 SINGLE_DH_USE
If I navigate to the internal test site I have just created that has a self signed certificate the self signed cert gets passed through to the client for them to make their own decision

If there is no easy solution I will just avoid IE, which I won't be too upset about.

Thanks,
? ? ?Oliver

----------------------------------------
> To:?squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Fri, 28 Aug 2015 23:28:53 +1200
> Subject: Re: [squid-users] Internet Explorer error with SSL bumping
>
> On 28/08/2015 9:58 p.m., Oliver Webb wrote:
>> I have transparent SSL bumping working perfectly in Chrome and
>> Safari
> (iOS and Windows 7) and Internet Explorer *on Windows Phone*, and by
> perfectly I mean no certificate warnings of any description for any site
> everything just behaves normally (apart from the sites certificate being
> signed by me.) However in Internet Explorer 11 on Windows 7 I get the
> following message for all secure bumped sites (secure sites like ebay
> for example load fine because I have configured not to be bumped and
> also unsecure sites load fine as well)
>
>> This page can?t be displayed
>>
>> Turn on TLS 1.0, TLS 1.1, and TLS 1.2 in Advanced settings and try connecting to
>> https://google.co.uk again. If this error persists, contact your site administrator.
>>
>> I just wondered if anyone had any bright ideas as to what might be up.
>
> The complete lack of warnings is a BAD sign. It means the certificate
> mimic feature is probably is not working at all.
>
> Mimic is supposed to pass certificate flaws in the server certs through
> to the client/browser so all the security go/die decisions can be made
> by the end-users own preference confg.
>
>
> The error message you show implies that you have configured your proxy
> for SSLv3-only or SSLv2-only. At least on the listening ports the
> browser is connecting to. Though since it was displayed by a browser we
> can't be 100% sure it contains truth (SSL-bump is feeding some bold lies
> to it).
>
>
> PS. If not 3.5.7 or a later snapshot please try an upgrade.
>
> PPS. I'm told people are having pain from OpenSSL 0.9.8 apparently
> trying to do TLS/1.0 in a way Squid does not handle properly right now.
> If that library version is installed on the client you may need to wait
> for a fix the guys are working on as I type this (ETA unknown). Though
> if you can get the client to upgrade to a more current and secure
> OpenSSL that would be even better.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  

From paul.martin.b787 at gmail.com  Fri Aug 28 13:28:12 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Fri, 28 Aug 2015 15:28:12 +0200
Subject: [squid-users] squid: workers on multicore
Message-ID: <CAGAgj8CD+J-+Xt-4zkZHEuZzmaSeUBJYGwT7gMbYe4pot61PXw@mail.gmail.com>

Hello

I have 2 questions. Could you help me ?

1) my squid is working with 4 workers (1 to 4). My processor is 8 cores but
only 4 cores (5 to 8) are used by squid. How make the 8 cores working
(cpu_affinity_map
process_numbers ...)?

2) my squidguard blocks some https site but the client does not receive a
redirection page to let him know he should not surf on this site. How to
enable a redirection for a https forbidden site ?

Thank you

Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/c2227eaa/attachment.htm>

From jorgeley at gmail.com  Fri Aug 28 14:32:22 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 28 Aug 2015 11:32:22 -0300
Subject: [squid-users] 16G Virtual Mem
Message-ID: <CAMeoTHkwD=VzuiqGeCyw2s17qcnfsQhjh_S5hV5xn99-1kt7GQ@mail.gmail.com>

Guys, is this really normal???

?

--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/570a4cd9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.jpg
Type: image/jpeg
Size: 54453 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/570a4cd9/attachment.jpg>

From gkinkie at gmail.com  Fri Aug 28 14:41:53 2015
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 28 Aug 2015 16:41:53 +0200
Subject: [squid-users] 16G Virtual Mem
In-Reply-To: <CAMeoTHkwD=VzuiqGeCyw2s17qcnfsQhjh_S5hV5xn99-1kt7GQ@mail.gmail.com>
References: <CAMeoTHkwD=VzuiqGeCyw2s17qcnfsQhjh_S5hV5xn99-1kt7GQ@mail.gmail.com>
Message-ID: <CA+Y8hcNOx8btcR_zHmY4euk5RvrT69yqRMchocJymbA1SCVZPQ@mail.gmail.com>

Hi,
   yes, it could be, depending on your configuration.
Please see http://wiki.squid-cache.org/SquidFaq/SquidMemory


On Fri, Aug 28, 2015 at 4:32 PM, Jorgeley Junior <jorgeley at gmail.com> wrote:

> Guys, is this really normal???
>
> ?
>
> --
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/bc97f8b7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.jpg
Type: image/jpeg
Size: 54453 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/bc97f8b7/attachment.jpg>

From hwaterfall at gmail.com  Fri Aug 28 15:31:27 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Fri, 28 Aug 2015 08:31:27 -0700
Subject: [squid-users] logfileHandleWrite: daemon:/var/logs/access.log:
 error writing ((32) Broken pipe)
Message-ID: <CAP6bC05Ke1Ex5R6G7KXgNr-ocm9GsPt_MMD8nW6pqkxQpdpX=A@mail.gmail.com>

Ubuntu 14.04
Squid 3.5.5

Hi -
I'm having pretty much exactly the same problem as Priya. I didn't see a
resolution so I'm picking up the thread.

I've installed and run squid 3.5.5 previously w/o a problem using:

sudo apt-get install squid


I want to filter by mac address though, so I purged that install and
re-built squid with the *--enable-arp-acl* flag. This is when the problems
begin. I've provided what was requested from Priya below. I'm grateful for
any help.

Thanks,
Deiter

Output from *sudo /usr/local/squid/sbin/squid -NCd1:*

WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
/usr/local/squid/var/logs/cache.log: Permission denied
         messages will be sent to 'stderr'.
2015/08/28 14:47:27| cannot change current directory to /var/spool/squid3:
(2) No such file or directory
2015/08/28 14:47:27| Current Directory is /home/ubuntu
WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
/usr/local/squid/var/logs/cache.log: Permission denied
         messages will be sent to 'stderr'.
2015/08/28 14:47:27| WARNING: Closing open FD    2
2015/08/28 14:47:27| Starting Squid Cache version 3.5.5 for
armv7l-unknown-linux-gnueabihf...
2015/08/28 14:47:27| Service Name: squid
2015/08/28 14:47:27| Process ID 2118
2015/08/28 14:47:27| Process Roles: master worker
2015/08/28 14:47:27| With 1024 file descriptors available
2015/08/28 14:47:27| Initializing IP Cache...
2015/08/28 14:47:27| DNS Socket created at [::], FD 7
2015/08/28 14:47:27| DNS Socket created at 0.0.0.0, FD 8
2015/08/28 14:47:27| Adding nameserver 75.75.75.75 from /etc/resolv.conf
2015/08/28 14:47:27| Adding nameserver 75.75.76.76 from /etc/resolv.conf
2015/08/28 14:47:27| Adding domain hsd1.ca.comcast.net from /etc/resolv.conf
2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
2015/08/28 14:47:27| Adding nameserver 192.168.1.1 from /etc/resolv.conf
2015/08/28 14:47:27| Logfile: opening log
daemon:/usr/local/squid/var/logs/access.log
2015/08/28 14:47:27| Logfile Daemon: opening log
/usr/local/squid/var/logs/access.log
fopen: Permission denied
2015/08/28 14:47:27| Store logging disabled
2015/08/28 14:47:27| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2015/08/28 14:47:27| Target number of buckets: 1008
2015/08/28 14:47:27| Using 8192 Store buckets
2015/08/28 14:47:27| Max Mem  size: 262144 KB
2015/08/28 14:47:27| Max Swap size: 0 KB
2015/08/28 14:47:27| Using Least Load store dir selection
2015/08/28 14:47:27| cannot change current directory to /var/spool/squid3:
(2) No such file or directory
2015/08/28 14:47:27| Current Directory is /home/ubuntu
2015/08/28 14:47:27| Finished loading MIME types and icons.
2015/08/28 14:47:27| HTCP Disabled.
2015/08/28 14:47:27| Squid plugin modules loaded: 0
2015/08/28 14:47:27| Adaptation support is off.
2015/08/28 14:47:27| Accepting HTTP Socket connections at local=[::]:8080
remote=[::] FD 11 flags=9
2015/08/28 14:47:28| logfileHandleWrite:
daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
pipe)
2015/08/28 14:47:28| Closing HTTP port [::]:8080
2015/08/28 14:47:28| storeDirWriteCleanLogs: Starting...
2015/08/28 14:47:28|   Finished.  Wrote 0 entries.
2015/08/28 14:47:28|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: I don't handle this error well!
Squid Cache (Version 3.5.5): Terminated abnormally.
CPU Usage: 0.290 seconds = 0.220 user + 0.070 sys
Maximum Resident Size: 34480 KB
Page faults with physical i/o: 0


Output from *squid -v:*

Squid Cache: Version 3.5.5
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid'
'--enable-arp-acl' --enable-ltdl-convenience

Output from *ls -al /usr/local/squid/var/logs/access.log:*

-rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47
/usr/local/squid/var/logs/access.log

Output from *ls -al /usr/local/squid/var/logs:*

total 5224
drwxrwxrwx 2 squid squid_admin    4096 Aug 28 09:35 .
drwxr-xr-x 5 root  root           4096 Jun 12 06:57 ..
-rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47 access.log
-rw-rw---- 1 squid squid_admin   67925 Jun 27 02:47 cache.log

Output from *ls -al /usr/local/squid/var:*

total 20
drwxr-xr-x 5 root  root        4096 Jun 12 06:57 .
drwxr-xr-x 8 root  root        4096 Jun 12 06:57 ..
drwxrwxrwx 3 root  root        4096 Jun 12 06:57 cache
drwxrwxrwx 2 squid squid_admin 4096 Aug 28 09:35 logs
drwxrwxrwx 3 root  root        4096 Jun 21 12:17 run

I adjusted the paths above from what was asked of Priya to jibe with my
set-up, but just in case:

Output from *ls -al /var/logs/access.log:*

ls: cannot access /var/logs/access.log: No such file or directory


Output from *ls -al /var/logs:*

ls: cannot access /var/logs: No such file or directory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/18775dd1/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 28 15:39:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Aug 2015 03:39:34 +1200
Subject: [squid-users] squid: workers on multicore
In-Reply-To: <CAGAgj8CD+J-+Xt-4zkZHEuZzmaSeUBJYGwT7gMbYe4pot61PXw@mail.gmail.com>
References: <CAGAgj8CD+J-+Xt-4zkZHEuZzmaSeUBJYGwT7gMbYe4pot61PXw@mail.gmail.com>
Message-ID: <55E080B6.4060809@treenet.co.nz>

On 29/08/2015 1:28 a.m., Paul Martin wrote:
> Hello
> 
> I have 2 questions. Could you help me ?
> 
> 1) my squid is working with 4 workers (1 to 4). My processor is 8 cores but
> only 4 cores (5 to 8) are used by squid. How make the 8 cores working
> (cpu_affinity_map
> process_numbers ...)?

In the way the documentation displays:
 <http://www.squid-cache.org/Doc/config/cpu_affinity_map/>

> 
> 2) my squidguard blocks some https site but the client does not receive a
> redirection page to let him know he should not surf on this site. How to
> enable a redirection for a https forbidden site ?

If it is a CONNECT request you are trying to redirect Squid is
outputting the page. Then well, browser bugs happen. We have been on at
them about it for more than a decade now. But it seems nobody wants to
touch the issue since there is also security vulnerabilities to navigate
around in the fix.

Anyway. The latest versions of Squid with SSL-Bump configured should be
able to get something to reach the user screen by decrypting the traffic
and injecting a deny_info "page" with 511 status. However it does
require that Squid be generating an error message, so SG probably still
can't do it as a redirect.

Amos



From steve at opendium.com  Fri Aug 28 16:17:14 2015
From: steve at opendium.com (Steve Hill)
Date: Fri, 28 Aug 2015 17:17:14 +0100
Subject: [squid-users] %un format code doesn't work for external ssl_bump
	ACLs
Message-ID: <55E0898A.2050704@opendium.com>


Squid 3.5.7

I'm using an external ACL to decide whether to bump traffic during SSL 
bump step 2.  The external ACL needs to know the user's username for 
requests that have authenticated, but not all requests are authenticated 
so I can't use %LOGIN and I'm therefore using %un instead.  However, %un 
is never being filled in with a user name.


The relevant parts of the config are:

http_access allow proxy_auth
http_access deny all
external_acl_type sslpeek children-max=10 concurrency=100 ttl=0 
negative_ttl=0 %SRC %un %URI %ssl::>sni %>ha{User-Agent} 
/usr/sbin/check_bump.sh
acl sslpeek external sslpeek
acl ssl_bump_step_1 at_step SslBump1
acl ssl_bump_step_2 at_step SslBump2
acl ssl_bump_step_3 at_step SslBump3
ssl_bump peek ssl_bump_step_1 #icap_says_peek
ssl_bump bump ssl_bump_step_2 sslpeek
ssl_bump splice all
sslproxy_cert_error allow all


The debug log shows that the request is successfully authenticated:

Acl.cc(138) matches: checking proxy_auth
UserData.cc(22) match: user is steve, case_insensitive is 0
UserData.cc(28) match: aclMatchUser: user REQUIRED and auth-info present.
Acl.cc(340) cacheMatchAcl: ACL::cacheMatchAcl: miss for 'proxy_auth'. 
Adding result 1
Acl.cc(158) matches: checked: proxy_auth = 1

But then later in the log I see:

external_acl.cc(1416) Start: fg lookup in 'sslpeek' for 
'2a00:1940:1:8:468a:5bff:fe9a:cd7f - www.hsbc.co.uk:443 www.hsbc.co.uk 
Mozilla/5.0%20(X11;%20Fedora;%20Linux%20x86_64;%20rv:39.0)%20Gecko/20100101%20Firefox/39.0'


The user name given to the external ACL is "-" even though the request 
has been authenticated.  Setting a->require_auth in 
parse_externalAclHelper() makes it work, but obviously just makes %un 
behave like %LOGIN, so isn't a solution.

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 283 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/e425f1bf/attachment.vcf>

From rousskov at measurement-factory.com  Fri Aug 28 17:00:43 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 28 Aug 2015 11:00:43 -0600
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <55DAC5CC.9050706@treenet.co.nz>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
 <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
 <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>
 <55DAC5CC.9050706@treenet.co.nz>
Message-ID: <55E093BB.5080108@measurement-factory.com>

On 08/24/2015 01:20 AM, Amos Jeffries wrote:

> On 24/08/2015 1:03 p.m., Brandon Elliott wrote:
>> I need a solution that
>> doesn't involve using a second authentication just to limit upload
>> bandwidth per user.


> client_delay_pools was added to meet this need. 


client_delay_pools limit Squid-to-client download bandwidth, not
client-to-Squid upload bandwidth.


Alex.



From jorgeley at gmail.com  Fri Aug 28 17:06:35 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 28 Aug 2015 14:06:35 -0300
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <55E093BB.5080108@measurement-factory.com>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
 <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
 <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>
 <55DAC5CC.9050706@treenet.co.nz> <55E093BB.5080108@measurement-factory.com>
Message-ID: <CAMeoTH=ci6wZwwQ5UgdXqM2e9N7GwxshzDNnwHKwMvoyPUzCCA@mail.gmail.com>

I think the directive: client_request_buffer_max_size do a limit to upload,
but it will stop the request, is that it?

2015-08-28 14:00 GMT-03:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 08/24/2015 01:20 AM, Amos Jeffries wrote:
>
> > On 24/08/2015 1:03 p.m., Brandon Elliott wrote:
> >> I need a solution that
> >> doesn't involve using a second authentication just to limit upload
> >> bandwidth per user.
>
>
> > client_delay_pools was added to meet this need.
>
>
> client_delay_pools limit Squid-to-client download bandwidth, not
> client-to-Squid upload bandwidth.
>
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/7dc4e0e8/attachment.htm>

From rentorbuy at yahoo.com  Fri Aug 28 17:27:13 2015
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 28 Aug 2015 17:27:13 +0000 (UTC)
Subject: [squid-users] squid tproxy
Message-ID: <1654832879.2197498.1440782833488.JavaMail.yahoo@mail.yahoo.com>

Hi,


[reposting a trimmed-down message]

My goal is to allow lan users to access a greater number of sites if they explicitly configure the squid proxy server in their browsers and authenticate. If they don't then traffic to port 80 and 443 will be transparently redirected to a squid proxy server by the corporate firewall (in my case, firewall and squid are on the same machine).

Since I noticed that I cannot REQUIRE proxy_auth and create an additional http_port for tproxy without authentication, I merely created two instances of squid.

The first instance requires authentication and listens on port 3128. All works fine when setting up the proxy address and port 3128 (or via wpad.dat) on the client.

The second instance does not require authentication and listens on port 3129 in tproxy mode and on port 3130 in forward proxy mode.

The firewall on the same machine as squid (iptables) redirects port 80 to 3129

I tried connecting from a Firefox client browser (lan IP addr. 10.215.144.48) without proxy manually configured to internet host 89.16.167.134:80.

The second squid proxy instance handles the connection but fails with a connection timeout (see log below).

squid.tproxy.conf (of second instance):

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 901 # SWAT
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/squid.custom.rules.tproxy
http_access allow localhost
http_access deny all
coredump_dir /var/cache/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
pid_filename /run/squid.tproxy.pid
cache_dir ufs /var/cache/squid.tproxy 100 16 256

squid.custom.rules.tproxy:

access_log daemon:/var/log/squid/access.tproxy.log squid
cache_log /var/log/squid/cache.tproxy.log
http_access allow all
email_err_data on
error_directory /usr/share/squid/errors/HMAN
debug_options rotate=1 ALL,5
append_domain .mydomain.org
http_port 3130
http_port 3129 tproxy
dns_v4_first on

squid 3.5.6
kernel 4.1.4

CONFIG_NETFILTER_XT_TARGET_TPROXY=m
CONFIG_NETFILTER_XT_MATCH_SOCKET=m
CONFIG_NF_CONNTRACK=m

lsmod shows xt_TPROXY, nf_conntrack, xt_socket

Here's the log (connecting from client browser at 10.215.144.48 to internet host at 89.16.167.134):

http://pastebin.com/W2e8csZT

What is causing the timeout?

Is there something wrong with my squid configuration or should I look elsewhere?

Thanks,

Vieri


From rousskov at measurement-factory.com  Fri Aug 28 17:37:02 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 28 Aug 2015 11:37:02 -0600
Subject: [squid-users] How to limit upload bandwidth in squid proxy?
In-Reply-To: <CAMeoTH=ci6wZwwQ5UgdXqM2e9N7GwxshzDNnwHKwMvoyPUzCCA@mail.gmail.com>
References: <CALknM89nHythjFAFLr3PxFdUp_8ygQET5ULjoS32fFC4egEK=g@mail.gmail.com>
 <005601d0ddeb$995daa90$cc18ffb0$@smoothnet.org>
 <CALknM8-B6k79K0x9ExV3h7gkZahkyGTkC0bkg57UVNxJU+fdSg@mail.gmail.com>
 <55DAC5CC.9050706@treenet.co.nz> <55E093BB.5080108@measurement-factory.com>
 <CAMeoTH=ci6wZwwQ5UgdXqM2e9N7GwxshzDNnwHKwMvoyPUzCCA@mail.gmail.com>
Message-ID: <55E09C3E.5050200@measurement-factory.com>

On 08/28/2015 11:06 AM, Jorgeley Junior wrote:
> I think the directive: client_request_buffer_max_size do a limit to
> upload, but it will stop the request, is that it?

In general, it is not possible to limit _bandwidth_ usage by limiting
buffer sizes. With enough processing speed, one can [theoretically]
consume all available bandwidth using a 1-byte buffer!

However, if you just want to "slow down" all uploading connections, then
decreasing client_request_buffer_max_size is better than nothing. It
will not stop requests; it will just not let them _buffer_ more than
configured at any given time.


HTH,

Alex.


> 2015-08-28 14:00 GMT-03:00 Alex Rousskov:
> 
>     On 08/24/2015 01:20 AM, Amos Jeffries wrote:
> 
>     > On 24/08/2015 1:03 p.m., Brandon Elliott wrote:
>     >> I need a solution that
>     >> doesn't involve using a second authentication just to limit upload
>     >> bandwidth per user.
> 
> 
>     > client_delay_pools was added to meet this need.
> 
> 
>     client_delay_pools limit Squid-to-client download bandwidth, not
>     client-to-Squid upload bandwidth.
> 
> 
>     Alex.
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> -- 
> *_
> _*
> *_
> _*



From rousskov at measurement-factory.com  Fri Aug 28 17:54:52 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 28 Aug 2015 11:54:52 -0600
Subject: [squid-users] squid: workers on multicore
In-Reply-To: <55E080B6.4060809@treenet.co.nz>
References: <CAGAgj8CD+J-+Xt-4zkZHEuZzmaSeUBJYGwT7gMbYe4pot61PXw@mail.gmail.com>
 <55E080B6.4060809@treenet.co.nz>
Message-ID: <55E0A06C.5090906@measurement-factory.com>

On 08/28/2015 09:39 AM, Amos Jeffries wrote:
> On 29/08/2015 1:28 a.m., Paul Martin wrote:
>> 1) my squid is working with 4 workers (1 to 4). My processor is 8 cores but
>> only 4 cores (5 to 8) are used by squid. How make the 8 cores working
>> (cpu_affinity_map process_numbers ...)?


> In the way the documentation displays:
>  <http://www.squid-cache.org/Doc/config/cpu_affinity_map/>


Squid only supports 1:1 mapping right now, so it is not possible to use
cpu_affinity_map to map 4 workers to 8 cores. Fortunately, this
limitation is a _good_ thing in most cases as discussed below.

If you have 8 virtual/hyper cores and only 4 *physical* ones (which is a
typical case for many (most?) modern CPUs), then you probably do _not_
want a single busy Squid worker to use multiple cores. In that case, you
may want to make sure Squid workers are not sharing physical cores. The
documentation that Amos pointed to will help you after you figure out
where your physical cores are.


If you are sure you want 4:8 mapping, you can still achieve it using
system tools such as taskset(1) on Linux.


HTH,

Alex.



From informaticagabriel at gmail.com  Fri Aug 28 22:31:38 2015
From: informaticagabriel at gmail.com (=?UTF-8?Q?Gabriel_Ordo=C3=B1ez?=)
Date: Fri, 28 Aug 2015 15:31:38 -0700 (PDT)
Subject: [squid-users] You can use squid on site like facebook or youtube?
Message-ID: <1440801098630-4672939.post@n4.nabble.com>

Hello, first of all this it is my first time here.
I'm trying to use squid for content sites like facebook, youtube and others,
but because these sites use SSL or TLS for the connection to their servers
do not have it made.
I read everything there is much here about it but I am not very clear yet.
The specific question is: someone has accomplished? You can store content
facebook or youtube?

I would be grateful if my clarify this.

PS: My native language is Spanish so I'm using google translator to
communicate with you. Apologies for that.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/You-can-use-squid-on-site-like-facebook-or-youtube-tp4672939.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Fri Aug 28 23:53:58 2015
From: vdoctor at neuf.fr (FredT)
Date: Fri, 28 Aug 2015 16:53:58 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55DE0E5E.5050203@treenet.co.nz>
References: <1440146802401-4672803.post@n4.nabble.com>
 <55D71B82.7020203@treenet.co.nz> <1440400275614-4672835.post@n4.nabble.com>
 <55DADE45.2010905@treenet.co.nz> <1440408226749-4672841.post@n4.nabble.com>
 <55DAF5DD.7060406@treenet.co.nz> <1440421859169-4672846.post@n4.nabble.com>
 <55DE099F.1000507@treenet.co.nz> <1440614937789-4672888.post@n4.nabble.com>
 <55DE0E5E.5050203@treenet.co.nz>
Message-ID: <1440806038633-4672940.post@n4.nabble.com>

Hi Amos,
We have applied the patch with the client on the squid in prod a coule of
hours ago...
We can see now a real aggressive objects cleaning 

I can confirm a 200 obj/sec is a minimal number with huge traffic, you could
fix the value a bit higher (250-300) that would be perfect.

Let's see this patch in the next official build... 

Bye Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672940.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hwaterfall at gmail.com  Sat Aug 29 00:43:09 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Fri, 28 Aug 2015 17:43:09 -0700
Subject: [squid-users] logfileHandleWrite: daemon:/var/logs/access.log:
 error writing ((32) Broken pipe)
In-Reply-To: <CAP6bC05Ke1Ex5R6G7KXgNr-ocm9GsPt_MMD8nW6pqkxQpdpX=A@mail.gmail.com>
References: <CAP6bC05Ke1Ex5R6G7KXgNr-ocm9GsPt_MMD8nW6pqkxQpdpX=A@mail.gmail.com>
Message-ID: <CAP6bC05yhXF-7jE_JQjKJP0EXEzvqsV_MKSeO23+H3T1mwu4jw@mail.gmail.com>

Sorry, I thought that by copying the subject line of Priya's message
thread, I would have continued the thread. Here's the link for context:
http://lists.squid-cache.org/pipermail/squid-users/2015-February/002131.html

Cheers,
Deiter

On Fri, Aug 28, 2015 at 8:31 AM, Howard Waterfall <hwaterfall at gmail.com>
wrote:

> Ubuntu 14.04
> Squid 3.5.5
>
> Hi -
> I'm having pretty much exactly the same problem as Priya. I didn't see a
> resolution so I'm picking up the thread.
>
> I've installed and run squid 3.5.5 previously w/o a problem using:
>
> sudo apt-get install squid
>
>
> I want to filter by mac address though, so I purged that install and
> re-built squid with the *--enable-arp-acl* flag. This is when the
> problems begin. I've provided what was requested from Priya below. I'm
> grateful for any help.
>
> Thanks,
> Deiter
>
> Output from *sudo /usr/local/squid/sbin/squid -NCd1:*
>
> WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
> /usr/local/squid/var/logs/cache.log: Permission denied
>          messages will be sent to 'stderr'.
> 2015/08/28 14:47:27| cannot change current directory to /var/spool/squid3:
> (2) No such file or directory
> 2015/08/28 14:47:27| Current Directory is /home/ubuntu
> WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
> /usr/local/squid/var/logs/cache.log: Permission denied
>          messages will be sent to 'stderr'.
> 2015/08/28 14:47:27| WARNING: Closing open FD    2
> 2015/08/28 14:47:27| Starting Squid Cache version 3.5.5 for
> armv7l-unknown-linux-gnueabihf...
> 2015/08/28 14:47:27| Service Name: squid
> 2015/08/28 14:47:27| Process ID 2118
> 2015/08/28 14:47:27| Process Roles: master worker
> 2015/08/28 14:47:27| With 1024 file descriptors available
> 2015/08/28 14:47:27| Initializing IP Cache...
> 2015/08/28 14:47:27| DNS Socket created at [::], FD 7
> 2015/08/28 14:47:27| DNS Socket created at 0.0.0.0, FD 8
> 2015/08/28 14:47:27| Adding nameserver 75.75.75.75 from /etc/resolv.conf
> 2015/08/28 14:47:27| Adding nameserver 75.75.76.76 from /etc/resolv.conf
> 2015/08/28 14:47:27| Adding domain hsd1.ca.comcast.net from
> /etc/resolv.conf
> 2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
> 2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
> 2015/08/28 14:47:27| Adding nameserver 192.168.1.1 from /etc/resolv.conf
> 2015/08/28 14:47:27| Logfile: opening log
> daemon:/usr/local/squid/var/logs/access.log
> 2015/08/28 14:47:27| Logfile Daemon: opening log
> /usr/local/squid/var/logs/access.log
> fopen: Permission denied
> 2015/08/28 14:47:27| Store logging disabled
> 2015/08/28 14:47:27| Swap maxSize 0 + 262144 KB, estimated 20164 objects
> 2015/08/28 14:47:27| Target number of buckets: 1008
> 2015/08/28 14:47:27| Using 8192 Store buckets
> 2015/08/28 14:47:27| Max Mem  size: 262144 KB
> 2015/08/28 14:47:27| Max Swap size: 0 KB
> 2015/08/28 14:47:27| Using Least Load store dir selection
> 2015/08/28 14:47:27| cannot change current directory to /var/spool/squid3:
> (2) No such file or directory
> 2015/08/28 14:47:27| Current Directory is /home/ubuntu
> 2015/08/28 14:47:27| Finished loading MIME types and icons.
> 2015/08/28 14:47:27| HTCP Disabled.
> 2015/08/28 14:47:27| Squid plugin modules loaded: 0
> 2015/08/28 14:47:27| Adaptation support is off.
> 2015/08/28 14:47:27| Accepting HTTP Socket connections at local=[::]:8080
> remote=[::] FD 11 flags=9
> 2015/08/28 14:47:28| logfileHandleWrite:
> daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
> pipe)
> 2015/08/28 14:47:28| Closing HTTP port [::]:8080
> 2015/08/28 14:47:28| storeDirWriteCleanLogs: Starting...
> 2015/08/28 14:47:28|   Finished.  Wrote 0 entries.
> 2015/08/28 14:47:28|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: I don't handle this error well!
> Squid Cache (Version 3.5.5): Terminated abnormally.
> CPU Usage: 0.290 seconds = 0.220 user + 0.070 sys
> Maximum Resident Size: 34480 KB
> Page faults with physical i/o: 0
>
>
> Output from *squid -v:*
>
> Squid Cache: Version 3.5.5
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var'
> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid'
> '--enable-arp-acl' --enable-ltdl-convenience
>
> Output from *ls -al /usr/local/squid/var/logs/access.log:*
>
> -rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47
> /usr/local/squid/var/logs/access.log
>
> Output from *ls -al /usr/local/squid/var/logs:*
>
> total 5224
> drwxrwxrwx 2 squid squid_admin    4096 Aug 28 09:35 .
> drwxr-xr-x 5 root  root           4096 Jun 12 06:57 ..
> -rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47 access.log
> -rw-rw---- 1 squid squid_admin   67925 Jun 27 02:47 cache.log
>
> Output from *ls -al /usr/local/squid/var:*
>
> total 20
> drwxr-xr-x 5 root  root        4096 Jun 12 06:57 .
> drwxr-xr-x 8 root  root        4096 Jun 12 06:57 ..
> drwxrwxrwx 3 root  root        4096 Jun 12 06:57 cache
> drwxrwxrwx 2 squid squid_admin 4096 Aug 28 09:35 logs
> drwxrwxrwx 3 root  root        4096 Jun 21 12:17 run
>
> I adjusted the paths above from what was asked of Priya to jibe with my
> set-up, but just in case:
>
> Output from *ls -al /var/logs/access.log:*
>
> ls: cannot access /var/logs/access.log: No such file or directory
>
>
> Output from *ls -al /var/logs:*
>
> ls: cannot access /var/logs: No such file or directory
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/3a7119b3/attachment.htm>

From hwaterfall at gmail.com  Sat Aug 29 00:52:32 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Fri, 28 Aug 2015 17:52:32 -0700
Subject: [squid-users] Building squid | Best Practices?
Message-ID: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>

Ubuntu 14.04

Squid 3.5.5

Hi -

I've successfully installed, configured and run squid 3.5.5 w/o a problem
using:

sudo apt-get install squid

I want to filter by mac address though, so I purged that install and
re-built squid with the *--enable-arp-acl* flag. I installed using make
install.

This is when the problems begin. I've summarized that in a separate thread:

http://lists.squid-cache.org/pipermail/squid-users/2015-August/005232.html

I wonder if there's a way to simply *avoid* all those problems.

A couple of questions:


   1. My build configuration is below. After building and installing, squid
   ended up here: */usr/local/squid/sbin*. I think that's the same place *sudo
   apt-get install squid *puts it right?
   2. Given I had no problems with *sudo apt-get install squid*, is the
   best practice to install squid using that method and arrange for my builds
   to install squid to a different location and simply copy it over?

Thanks,

Deiter

./configure --prefix=/usr --localstatedir=/var
--libexecdir=${prefix}/lib/squid3 --srcdir=.
--datadir=${prefix}/share/squid3 --sysconfdir=/etc/squid3
--with-default-user=proxy --with-logdir=/var/log
--with-pidfile=/var/run/squid.pid --enable-arp-acl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150828/f408efb7/attachment.htm>

From yvoinov at gmail.com  Sat Aug 29 09:24:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 29 Aug 2015 15:24:52 +0600
Subject: [squid-users] You can use squid on site like facebook or
	youtube?
In-Reply-To: <1440801098630-4672939.post@n4.nabble.com>
References: <1440801098630-4672939.post@n4.nabble.com>
Message-ID: <55E17A64.9040804@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sure. This is a bit difficult, but possible. Excluding YT videos
(googlevideo), of course. Due to YT construction.

29.08.15 4:31, Gabriel Ordo?ez ?????:
> Hello, first of all this it is my first time here.
> I'm trying to use squid for content sites like facebook, youtube and
others,
> but because these sites use SSL or TLS for the connection to their servers
> do not have it made.
> I read everything there is much here about it but I am not very clear yet.
> The specific question is: someone has accomplished? You can store content
> facebook or youtube?
>
> I would be grateful if my clarify this.
>
> PS: My native language is Spanish so I'm using google translator to
> communicate with you. Apologies for that.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/You-can-use-squid-on-site-like-facebook-or-youtube-tp4672939.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV4XpjAAoJENNXIZxhPexG1K8H/0lC8zseWqS7bDMvPn+o3QQZ
4j1s8hcbTksw0zh+FoUmjWxD4GTfHJhcYaesyOGPHr+szW4umBKw0+27zoALTa4L
mFQqgHd6L+W7Mm3Hu9XVRse4cxi2qAeWhgFqFzRwe+szanE0LBLzzXZvXTb8e1tM
s/fIp1186ZaiGXdUbTP5gFYzB+1kTRidNdl3uo5QEQWARCt5dt0Y8TqgveaMhy84
bvUSr1y14o6hLUrtVQkH+21VDjTI7Umne8Grf9tUrcMNX1U6I3NWZBL9aeD5QuhN
WvFgePKDsjMBP7J9NEU2KYt+4lCSEZrQwNrqQ0KZjxe6xKgcwku+hwpUHEltMAA=
=9HIP
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sat Aug 29 09:35:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 29 Aug 2015 15:35:56 +0600
Subject: [squid-users] You can use squid on site like facebook or
	youtube?
In-Reply-To: <1440801098630-4672939.post@n4.nabble.com>
References: <1440801098630-4672939.post@n4.nabble.com>
Message-ID: <55E17CFC.3050104@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Here is my squidview screenshots:

http://i.imgur.com/svyWY6i.png
http://i.imgur.com/0ChSDql.png

H means TCP_HIT. :)

29.08.15 4:31, Gabriel Ordo?ez ?????:
> Hello, first of all this it is my first time here.
> I'm trying to use squid for content sites like facebook, youtube and
others,
> but because these sites use SSL or TLS for the connection to their servers
> do not have it made.
> I read everything there is much here about it but I am not very clear yet.
> The specific question is: someone has accomplished? You can store content
> facebook or youtube?
>
> I would be grateful if my clarify this.
>
> PS: My native language is Spanish so I'm using google translator to
> communicate with you. Apologies for that.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/You-can-use-squid-on-site-like-facebook-or-youtube-tp4672939.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV4Xz8AAoJENNXIZxhPexG8dkH/3EkCNiaxkuNBNeN4eKUxwkN
C28dxSNWEaf4ULtFU6/7CLswUAQDS+8urskaxkDtuRPmyWe10WqJwVlQrhD3/x/3
m6O8Juzz7263R9GRlv/Ypcr+VNb/KqRHsNfkbp5evgcGPA68XbrV9MkAV7MlAckd
THDz+qtZnSz8X96q0KJXWZeJC3Z7v+cEqZGKZxUPDKDqapDtxUmNxFuX6sCJgG08
R/T1RsTEm63DE4PsaLMNwBL4lhsXXk/CpOnkqMLAvzRBpEq39mL63quvtZFaaEnc
P+rTSB0mrye6aRTk7p/IqSLdCyxDZzY8kscQo6fIjrqIctnngRE5BlsdxdX8zk4=
=VFvY
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Sat Aug 29 09:31:06 2015
From: chip_pop at hotmail.com (joe)
Date: Sat, 29 Aug 2015 02:31:06 -0700 (PDT)
Subject: [squid-users] cpu high 100%
In-Reply-To: <1440714912575-4672918.post@n4.nabble.com>
References: <1440714912575-4672918.post@n4.nabble.com>
Message-ID: <1440840666984-4672945.post@n4.nabble.com>

ok guys i solve it it was one of the command future inside the conf its a bug
but next cpl day i re test them again cause my client upset 
i took off cpl of the option and my CPU peek at 15% now on traffic time 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cpu-high-100-tp4672918p4672945.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From informaticagabriel at gmail.com  Sat Aug 29 11:50:25 2015
From: informaticagabriel at gmail.com (=?UTF-8?Q?Gabriel_Ordo=C3=B1ez?=)
Date: Sat, 29 Aug 2015 04:50:25 -0700 (PDT)
Subject: [squid-users] You can use squid on site like facebook or
	youtube?
In-Reply-To: <55E17CFC.3050104@gmail.com>
References: <1440801098630-4672939.post@n4.nabble.com>
 <55E17CFC.3050104@gmail.com>
Message-ID: <1440849025359-4672946.post@n4.nabble.com>

yuri thanks for the clarification. I will read more about it and you are any
questions if necessary.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/You-can-use-squid-on-site-like-facebook-or-youtube-tp4672939p4672946.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcus.kool at urlfilterdb.com  Sat Aug 29 15:14:35 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 29 Aug 2015 12:14:35 -0300
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440806038633-4672940.post@n4.nabble.com>
References: <1440146802401-4672803.post@n4.nabble.com>
 <55D71B82.7020203@treenet.co.nz> <1440400275614-4672835.post@n4.nabble.com>
 <55DADE45.2010905@treenet.co.nz> <1440408226749-4672841.post@n4.nabble.com>
 <55DAF5DD.7060406@treenet.co.nz> <1440421859169-4672846.post@n4.nabble.com>
 <55DE099F.1000507@treenet.co.nz> <1440614937789-4672888.post@n4.nabble.com>
 <55DE0E5E.5050203@treenet.co.nz> <1440806038633-4672940.post@n4.nabble.com>
Message-ID: <55E1CC5B.20604@urlfilterdb.com>



On 08/28/2015 08:53 PM, FredT wrote:
> Hi Amos,
> We have applied the patch with the client on the squid in prod a coule of
> hours ago...
> We can see now a real aggressive objects cleaning
>
> I can confirm a 200 obj/sec is a minimal number with huge traffic, you could
> fix the value a bit higher (250-300) that would be perfect.

Yeah, everybody needs a different cleaning rate depending on number of physical disks and
use of faster devices like advanced disk arrays or SSDs.

I first thought that the cleanup rate could be a formula like 100 + #disks * 80
but SSDs and disk arrays can do more than that so probably the best thing to do is to have
a parameter for the cleanup rate and use 200 as the default value.

Marcus

> Let's see this patch in the next official build...
>
> Bye Fred
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672940.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From 221184 at gmail.com  Sat Aug 29 15:32:47 2015
From: 221184 at gmail.com (Arkantos)
Date: Sat, 29 Aug 2015 08:32:47 -0700 (PDT)
Subject: [squid-users] completely transparent Squid
In-Reply-To: <201508271752.46406.Antony.Stone@squid.open.source.it>
References: <1440692464604-4672904.post@n4.nabble.com>
 <201508271752.46406.Antony.Stone@squid.open.source.it>
Message-ID: <1440862367021-4672949.post@n4.nabble.com>

hello everybody,

thanks for the show of support. we really appreciate it. god bless.


as the network stands presently, we take delivery from our ISP via fiber. it
feeds into eth0 of the Unify MSC. eth1 is our LAN which is distributed to
all our users.

IP is assigned to each user. eth1 is 172.16.0.1 and users start from
172.16.0.11

right now it is 35 users, but we are expecting an influx of another 120-130
users as our competing service provider is shutting shop. there will be an
influx of even more users as we are actively expanding our geographical
coverage. we are expecting to be with 600 users within a months time.

user IPs are sometimes directly connected to computers, and sometimes to
users' routers.

DHCP is completely disabled.

user authentication is done from Unify MSC by matching user/pass/IP/MAC
using strict method - this is the reason we need completely transparent
proxy.

in India (where i am from) the rules regarding cyber crimes is very very
strict. so we have to maintain complete access logs (which Unify is capable
of doing very cheaply and easily)


the rig on which Squid is installed 2 NICs

currently our topology is like this:
delivery at MUX of ISP --- WAN port of Unify --- LAN port of Unify ---
Distribution network to user's home


we were thinking of this kind of a topology:
delivery at MUX of ISP --- WAN port of Unify --- LAN port of Unify --- WAN
port of Squid --- LAN port of Squid --- Distribution network to user's home

at the moment, i have installed CentOS 7.1.1503 on which Squid 3.3.8 and
Webmin 1.760 are installed.
i have just initialised the cache. and thats that.


on the Squid rig, i can access the internet through eth0
but i am finding it difficult to make the necessary config changes to allow
eth1 to give out connectivity to other computers.

i have tried bridging from this site, but it is not working:
https://sites.google.com/site/ghidit/how-to-2/configure-bridging-on-centos

i dont know how to move forward.


What would you suggest?
please help.

Arkantos.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/completely-transparent-Squid-tp4672904p4672949.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marko.cupac at mimar.rs  Sat Aug 29 18:34:53 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Sat, 29 Aug 2015 20:34:53 +0200
Subject: [squid-users] completely transparent Squid
In-Reply-To: <1440862367021-4672949.post@n4.nabble.com>
References: <1440692464604-4672904.post@n4.nabble.com>
 <201508271752.46406.Antony.Stone@squid.open.source.it>
 <1440862367021-4672949.post@n4.nabble.com>
Message-ID: <20150829203453.2e72a9f2@efreet>

On Sat, 29 Aug 2015 08:32:47 -0700 (PDT)
Arkantos <221184 at gmail.com> wrote:

> at the moment, i have installed CentOS 7.1.1503 on which Squid 3.3.8
> and Webmin 1.760 are installed.
> i have just initialised the cache. and thats that.
> 
> 
> on the Squid rig, i can access the internet through eth0
> but i am finding it difficult to make the necessary config changes to
> allow eth1 to give out connectivity to other computers.
> 
> i have tried bridging from this site, but it is not working:
> https://sites.google.com/site/ghidit/how-to-2/configure-bridging-on-centos
> 
> i dont know how to move forward.
> 
> 
> What would you suggest?
> please help.

I already suggested, but I'll try to suggest some more. Try to
concentrate on relevant stuff, which your version of Webmin surely
isn't. Actually I'd suggest you get rid of Webmin completely.

You should understand how data flows in transparent proxy. User's
browser generates packet destined to tcp port 80 or 443, but when it
arrives at the router, it redirects it to proxy instead of forwarding
it to the destination web server.

You will need to redirect web requests from your users' ip pool to
squid. In PF syntax this is something like:

users = "{ 172.16.0.11 - 172.16.0.253 }"
squid = "{ 172.6.0.10 }"
unifi_lan = "{ 172.6.0.254 }"

pass in on $unifi_lan proto tcp from $users to any port { 80 443 } \
	rdr-to $squid

Regards,
-- 
Marko Cupa?
https://www.mimar.rs/


From eliezer at ngtech.co.il  Sat Aug 29 19:21:25 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 29 Aug 2015 22:21:25 +0300
Subject: [squid-users] cpu high 100%
In-Reply-To: <1440840666984-4672945.post@n4.nabble.com>
References: <1440714912575-4672918.post@n4.nabble.com>
 <1440840666984-4672945.post@n4.nabble.com>
Message-ID: <55E20635.2060307@ngtech.co.il>

Hey Joe,

Can you give more details? I didn't understood what was the issue.

Eliezer

On 29/08/2015 12:31, joe wrote:
> ok guys i solve it it was one of the command future inside the conf its a bug
> but next cpl day i re test them again cause my client upset
> i took off cpl of the option and my CPU peek at 15% now on traffic time



From Antony.Stone at squid.open.source.it  Sat Aug 29 19:58:11 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 29 Aug 2015 20:58:11 +0100
Subject: [squid-users] completely transparent Squid
In-Reply-To: <1440862367021-4672949.post@n4.nabble.com>
References: <1440692464604-4672904.post@n4.nabble.com>
 <201508271752.46406.Antony.Stone@squid.open.source.it>
 <1440862367021-4672949.post@n4.nabble.com>
Message-ID: <201508292058.11944.Antony.Stone@squid.open.source.it>

On Saturday 29 Aug 2015 at 16:32, Arkantos wrote:

> user authentication is done from Unify MSC by matching user/pass/IP/MAC
> using strict method - this is the reason we need completely transparent
> proxy.

MAC address?  Really?

You won't get user MAC addresses to pass through routers, or through a 
transparent Squid proxy.  The packets seen by the upstream "authentication 
machine", whatever that is, will carry the MAC address of the last device they 
went through which isn't a switch.

I just wanted to check you're not trying to build something that will break 
authentication, and therefore be unusable...


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chip_pop at hotmail.com  Sat Aug 29 22:04:52 2015
From: chip_pop at hotmail.com (joe)
Date: Sat, 29 Aug 2015 15:04:52 -0700 (PDT)
Subject: [squid-users] cpu high 100%
In-Reply-To: <55E20635.2060307@ngtech.co.il>
References: <1440714912575-4672918.post@n4.nabble.com>
 <1440840666984-4672945.post@n4.nabble.com> <55E20635.2060307@ngtech.co.il>
Message-ID: <1440885892047-4672953.post@n4.nabble.com>

hi Eliezer one of the option in squid.conf has bug or duno wat to say going
to see wish one i took of cpl of it and i solve the issu   i did not know
witsh one but i have backup for the squid.conf going to re use it to make
shuur wish option did the caus tomorow i will do for shurr  now im letting
my client to enjoy a bit off that craching so they dont bug me and i will
post the result now its ben since i post betwen my first 2 post untill now
up max cpu % is 16 so tomorow i test again the bug dose not fireup fast it
take several hr to happen and it dose go up in stage 30%   50% then 80 90
100 and it stay hi untill crache happen  or i stop squid then re run



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cpu-high-100-tp4672918p4672953.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gkinkie at gmail.com  Sat Aug 29 22:26:42 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 30 Aug 2015 00:26:42 +0200
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <1440670224046-4672899.post@n4.nabble.com>
References: <1440670224046-4672899.post@n4.nabble.com>
Message-ID: <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>

Hi,
  please see http://wiki.squid-cache.org/Features/MonitorUrl.
It's available in squid 2.6 , and is one of the last few features who
haven't yet made it to Squid 3.X. If anyone is interested, code and
sponsorships are always welcome :)

On Thu, Aug 27, 2015 at 12:10 PM, Imaginovskiy <mo at stellarise.com> wrote:

> Hi all,
>
> Bit of a strange one but I'm wondering if it's possible to have squid
> redirect a site to a secondary backend server if the primary is down. Have
> been looking into this but haven't seen much similar to this. Currently I
> have my setup along the lines of this;
>
> Client -> Squid -> Backend1
>
> but in the event that Backend1 is down, the following should be done;
>
> Client -> Squid -> Backend2
>
> Is squid capable of monitoring connections to peer or redirecting based on
> an ACL looking for some HTTP error code?
>
> Thanks.
>
>
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/High-Availability-in-Squid-tp4672899.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/cd3c71a6/attachment.htm>

From luis.daniel.lucio at gmail.com  Sat Aug 29 22:44:44 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Sat, 29 Aug 2015 18:44:44 -0400
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
Message-ID: <CAFLo2Qzu=1PeSwOHp=EywEhb=4dnY0vp_N+CUqsqBhSomRUxqw@mail.gmail.com>

You may want to play with PEN
On Aug 29, 2015 6:27 PM, "Kinkie" <gkinkie at gmail.com> wrote:

> Hi,
>   please see http://wiki.squid-cache.org/Features/MonitorUrl.
> It's available in squid 2.6 , and is one of the last few features who
> haven't yet made it to Squid 3.X. If anyone is interested, code and
> sponsorships are always welcome :)
>
> On Thu, Aug 27, 2015 at 12:10 PM, Imaginovskiy <mo at stellarise.com> wrote:
>
>> Hi all,
>>
>> Bit of a strange one but I'm wondering if it's possible to have squid
>> redirect a site to a secondary backend server if the primary is down. Have
>> been looking into this but haven't seen much similar to this. Currently I
>> have my setup along the lines of this;
>>
>> Client -> Squid -> Backend1
>>
>> but in the event that Backend1 is down, the following should be done;
>>
>> Client -> Squid -> Backend2
>>
>> Is squid capable of monitoring connections to peer or redirecting based on
>> an ACL looking for some HTTP error code?
>>
>> Thanks.
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/High-Availability-in-Squid-tp4672899.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
>     Francesco
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150829/ee9a41f8/attachment.htm>

From eliezer at ngtech.co.il  Sun Aug 30 00:00:38 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 30 Aug 2015 03:00:38 +0300
Subject: [squid-users] cpu high 100%
In-Reply-To: <1440885892047-4672953.post@n4.nabble.com>
References: <1440714912575-4672918.post@n4.nabble.com>
 <1440840666984-4672945.post@n4.nabble.com> <55E20635.2060307@ngtech.co.il>
 <1440885892047-4672953.post@n4.nabble.com>
Message-ID: <55E247A6.6040803@ngtech.co.il>

What speeds are you talking about?

Eliezer

On 30/08/2015 01:04, joe wrote:
> hi Eliezer one of the option in squid.conf has bug or duno wat to say going
> to see wish one i took of cpl of it and i solve the issu   i did not know
> witsh one but i have backup for the squid.conf going to re use it to make
> shuur wish option did the caus tomorow i will do for shurr  now im letting
> my client to enjoy a bit off that craching so they dont bug me and i will
> post the result now its ben since i post betwen my first 2 post untill now
> up max cpu % is 16 so tomorow i test again the bug dose not fireup fast it
> take several hr to happen and it dose go up in stage 30%   50% then 80 90
> 100 and it stay hi untill crache happen  or i stop squid then re run
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cpu-high-100-tp4672918p4672953.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Sun Aug 30 00:30:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Aug 2015 12:30:58 +1200
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
Message-ID: <55E24EC2.30704@treenet.co.nz>

On 29/08/2015 12:52 p.m., Howard Waterfall wrote:
> Ubuntu 14.04
> 
> Squid 3.5.5
> 
> Hi -
> 
> I've successfully installed, configured and run squid 3.5.5 w/o a problem
> using:
> 
> sudo apt-get install squid
> 
> I want to filter by mac address though, so I purged that install and
> re-built squid with the *--enable-arp-acl* flag. I installed using make
> install.
> 

It seems the documentation is outdated. EUI support is built into
current Squid by default.

Note however that ARP and MAC/EUI-48 is an IPv4-only feature and does
not work on complex networks anyway due to how ARP operates. IPv6 uses
other mechanisms to route packets.


> This is when the problems begin. I've summarized that in a separate thread:
> 
> http://lists.squid-cache.org/pipermail/squid-users/2015-August/005232.html
> 
> I wonder if there's a way to simply *avoid* all those problems.

There is. At least its "simple" for someone used to custom building.


step 1) Purge your custom build and reinstall the distro package.

If this is just for the ARP. That should be enough.


If you want other custom features continue with ...

step 2) run "apt-get build-dep squid" to install the packages needed to
build squid yourself.

step 3) Use "squid -v" to identify the system-specific build options.


If you build Squd now with the options straight from -v the "make
install" step will replace the existing package files with updated copies.

Note that Upgrading the system package later will in turn replace your
custom build in the same way. So this is mostly useful for custom builds
to fix/patch temporary bugs you expect the system package to fix later.
Or if you use aptitudes 'hold' feature.

But, there are more steps below..

> 
> A couple of questions:
> 
> 
>    1. My build configuration is below. After building and installing, squid
>    ended up here: */usr/local/squid/sbin*. I think that's the same place *sudo
>    apt-get install squid *puts it right?

That /usr/local is the default location for user-built programs in *NIX
and *BSD operating systems. System provided packages go elsewhere.

Debian/Ubuntu has a different package layout as well. The squid -v step
above will provide you with the build options needed to setup for that
layout properly.



>    2. Given I had no problems with *sudo apt-get install squid*, is the
>    best practice to install squid using that method and arrange for my builds
>    to install squid to a different location and simply copy it over?


If you just want to keep the Squid installation separate from the system
package. Then the FS layout does not matter. Just...


> 
> Thanks,
> 
> Deiter
> 
> ./configure --prefix=/usr --localstatedir=/var
> --libexecdir=${prefix}/lib/squid3 --srcdir=.
> --datadir=${prefix}/share/squid3 --sysconfdir=/etc/squid3
> --with-default-user=proxy --with-logdir=/var/log
> --with-pidfile=/var/run/squid.pid --enable-arp-acl
> 


step 4) run "mkdir /proxy" (or whatever you want to call it)

step 5) change that --prefix option to /proxy

The "make install" should then create everything which makes up "Squid"
inside /proxy.

step 6) edit the /etc/init.d/squid script DAEMON= and CONFIG= variables
to point at your custom /proxy/sbin/squid and /proxy/etc/squid/squid.conf


The system package upgrades will then not affect your customizations.


Thats how I do it anyway :-)

Amos


From squid3 at treenet.co.nz  Sun Aug 30 00:52:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Aug 2015 12:52:49 +1200
Subject: [squid-users] squid tproxy
In-Reply-To: <1654832879.2197498.1440782833488.JavaMail.yahoo@mail.yahoo.com>
References: <1654832879.2197498.1440782833488.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55E253E1.4080009@treenet.co.nz>

On 29/08/2015 5:27 a.m., Vieri wrote:
> Hi,
> 
> 
> [reposting a trimmed-down message]
> 

> My goal is to allow lan users to access a greater number of sites if
they explicitly configure the squid proxy server in their browsers and
authenticate. If they don't then traffic to port 80 and 443 will be
transparently redirected to a squid proxy server by the corporate
firewall (in my case, firewall and squid are on the same machine).
> 
> Since I noticed that I cannot REQUIRE proxy_auth and create an
additional http_port for tproxy without authentication, I merely created
two instances of squid.

Yes you cannot require authn from an ACL test that gets checked on
intercepted traffic.

Jumping to the conclusion that you needed two proxies was extreme.

This would do it:

 http_port 3128
 http_port 3129 tproxy

 acl login proxy_auth REQUIRED
 acl explicit myportname 3128
 acl interceted myportname 3129

 http_access deny explicit !login
 http_access deny intercepted !localnet

Your regular rules then follow.

But if there is anything like a group check of external ACL with %LOGIN
place the 'explicit' ACL as the first one on the line, like the login
above. You will then have to figure out what (if anything) to do with
the intercepted traffic to check the same thing(s).


> 
> The first instance requires authentication and listens on port 3128.
All works fine when setting up the proxy address and port 3128 (or via
wpad.dat) on the client.
> 
> The second instance does not require authentication and listens on
port 3129 in tproxy mode and on port 3130 in forward proxy mode.
> 
> The firewall on the same machine as squid (iptables) redirects port
> 80
to 3129
> 
> I tried connecting from a Firefox client browser (lan IP addr.
10.215.144.48) without proxy manually configured to internet host
89.16.167.134:80.
> 
> The second squid proxy instance handles the connection but fails
> with
a connection timeout (see log below).

> squid.tproxy.conf (of second instance):
> 
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl Safe_ports port 901 # SWAT
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> include /etc/squid/squid.custom.rules.tproxy
> http_access allow localhost
> http_access deny all
> coredump_dir /var/cache/squid
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
> pid_filename /run/squid.tproxy.pid
> cache_dir ufs /var/cache/squid.tproxy 100 16 256
> 
> squid.custom.rules.tproxy:
> 
> access_log daemon:/var/log/squid/access.tproxy.log squid
> cache_log /var/log/squid/cache.tproxy.log
> http_access allow all
> email_err_data on
> error_directory /usr/share/squid/errors/HMAN
> debug_options rotate=1 ALL,5
> append_domain .mydomain.org
> http_port 3130
> http_port 3129 tproxy
> dns_v4_first on
> 
> squid 3.5.6
> kernel 4.1.4
> 
> CONFIG_NETFILTER_XT_TARGET_TPROXY=m
> CONFIG_NETFILTER_XT_MATCH_SOCKET=m
> CONFIG_NF_CONNTRACK=m
> 
> lsmod shows xt_TPROXY, nf_conntrack, xt_socket
> 
> Here's the log (connecting from client browser at 10.215.144.48 to internet host at 89.16.167.134):
> 
> http://pastebin.com/W2e8csZT
> 
> What is causing the timeout?

TCP SYN+ACK packet never gets back to Squid on the server connection.

> 
> Is there something wrong with my squid configuration or should I look elsewhere?
> 

The Squid operation seems to be perfectly fine.


The above SYN+ACK packet disappearance is a common sign that you have
triangular routing going on. Where the server response gets sent
straight to the client not to Squid.

You dont mention what routing or iptables configuration you have. It
could be there on the same machine, or it could be any of the network
routers elsewhere the traffic is going over.

Amos


From hwaterfall at gmail.com  Sun Aug 30 01:07:18 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Sun, 30 Aug 2015 01:07:18 +0000
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55E24EC2.30704@treenet.co.nz>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
Message-ID: <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>

Great stuff thanks a lot! Excellent timing too just getting ready to do the
make. I'll follow your suggestions. A couple of follow ups:

When doing the make (including ./configure), do I have to be running as
root or some particular user? I set up a squid user and user group and also
a squidadm user group...

Finally I noticed that configure didn't have -x permissions with 3.5.7. I
guess I'll have to change that out of the gate...

Cheers,
Deiter

On Sat, Aug 29, 2015, 5:32 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 29/08/2015 12:52 p.m., Howard Waterfall wrote:
> > Ubuntu 14.04
> >
> > Squid 3.5.5
> >
> > Hi -
> >
> > I've successfully installed, configured and run squid 3.5.5 w/o a problem
> > using:
> >
> > sudo apt-get install squid
> >
> > I want to filter by mac address though, so I purged that install and
> > re-built squid with the *--enable-arp-acl* flag. I installed using make
> > install.
> >
>
> It seems the documentation is outdated. EUI support is built into
> current Squid by default.
>
> Note however that ARP and MAC/EUI-48 is an IPv4-only feature and does
> not work on complex networks anyway due to how ARP operates. IPv6 uses
> other mechanisms to route packets.
>
>
> > This is when the problems begin. I've summarized that in a separate
> thread:
> >
> >
> http://lists.squid-cache.org/pipermail/squid-users/2015-August/005232.html
> >
> > I wonder if there's a way to simply *avoid* all those problems.
>
> There is. At least its "simple" for someone used to custom building.
>
>
> step 1) Purge your custom build and reinstall the distro package.
>
> If this is just for the ARP. That should be enough.
>
>
> If you want other custom features continue with ...
>
> step 2) run "apt-get build-dep squid" to install the packages needed to
> build squid yourself.
>
> step 3) Use "squid -v" to identify the system-specific build options.
>
>
> If you build Squd now with the options straight from -v the "make
> install" step will replace the existing package files with updated copies.
>
> Note that Upgrading the system package later will in turn replace your
> custom build in the same way. So this is mostly useful for custom builds
> to fix/patch temporary bugs you expect the system package to fix later.
> Or if you use aptitudes 'hold' feature.
>
> But, there are more steps below..
>
> >
> > A couple of questions:
> >
> >
> >    1. My build configuration is below. After building and installing,
> squid
> >    ended up here: */usr/local/squid/sbin*. I think that's the same place
> *sudo
> >    apt-get install squid *puts it right?
>
> That /usr/local is the default location for user-built programs in *NIX
> and *BSD operating systems. System provided packages go elsewhere.
>
> Debian/Ubuntu has a different package layout as well. The squid -v step
> above will provide you with the build options needed to setup for that
> layout properly.
>
>
>
> >    2. Given I had no problems with *sudo apt-get install squid*, is the
> >    best practice to install squid using that method and arrange for my
> builds
> >    to install squid to a different location and simply copy it over?
>
>
> If you just want to keep the Squid installation separate from the system
> package. Then the FS layout does not matter. Just...
>
>
> >
> > Thanks,
> >
> > Deiter
> >
> > ./configure --prefix=/usr --localstatedir=/var
> > --libexecdir=${prefix}/lib/squid3 --srcdir=.
> > --datadir=${prefix}/share/squid3 --sysconfdir=/etc/squid3
> > --with-default-user=proxy --with-logdir=/var/log
> > --with-pidfile=/var/run/squid.pid --enable-arp-acl
> >
>
>
> step 4) run "mkdir /proxy" (or whatever you want to call it)
>
> step 5) change that --prefix option to /proxy
>
> The "make install" should then create everything which makes up "Squid"
> inside /proxy.
>
> step 6) edit the /etc/init.d/squid script DAEMON= and CONFIG= variables
> to point at your custom /proxy/sbin/squid and /proxy/etc/squid/squid.conf
>
>
> The system package upgrades will then not affect your customizations.
>
>
> Thats how I do it anyway :-)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/449816b1/attachment.htm>

From stanley at wo.cn  Sun Aug 30 01:10:33 2015
From: stanley at wo.cn (=?UTF-8?b?6LS+5Yeh?=)
Date: Sun, 30 Aug 2015 09:10:33 +0800 (CST)
Subject: [squid-users] =?utf-8?q?Please_help_me_=2C_about_none_HTTP_protoc?=
	=?utf-8?q?ol_=28DPORT_80=29?=
Message-ID: <Zw849169908966.31965@ZB-MAIL-WAP03>

Please help me , about none HTTP protocol (DPORT 80) . 

I installed a SQUID (3.5) on Centos 6.5  as a web & video caching system .(not reverse proxy or cache )
I use the TPROXY method following here: http://wiki.squid-cache.org/Features/Tproxy4'>http://wiki.squid-cache.org/Features/Tproxy4

the section in this tourial :
iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129

it will set all the data incoming from LAN erea (condition: destination port is 80) as the MARK , and then 
squid process the data .

here , I want to ask a question :
not all the DPORT 80 data is HTTP protocol (such as : tencent's IM service port is 80 , but it is not HTTP protocol ) 

in this condition , source client will appear problem because the SQUID box could not rightly process the data.

then how Squid process the data that dport is 80 but which is not http protocol  ?

enter in a cache process ? or do nothing and let it passthru ?

please help me . thank you .

sorry for my poor English . if you could read Chinese , I will resent my native language mail again .

From ashish.mukherjee at gmail.com  Sun Aug 30 03:58:06 2015
From: ashish.mukherjee at gmail.com (Ashish Mukherjee)
Date: Sun, 30 Aug 2015 09:28:06 +0530
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
Message-ID: <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>

Why should Squid take care of HA though? Isn't it the responsibility of the
Website to ensure its uptime? Even if used as a reverse proxy, Apache may
do a better job at this, since one can do more sophisticated configurations
with mod_proxy.

On Sun, Aug 30, 2015 at 3:56 AM, Kinkie <gkinkie at gmail.com> wrote:

> Hi,
>   please see http://wiki.squid-cache.org/Features/MonitorUrl.
> It's available in squid 2.6 , and is one of the last few features who
> haven't yet made it to Squid 3.X. If anyone is interested, code and
> sponsorships are always welcome :)
>
> On Thu, Aug 27, 2015 at 12:10 PM, Imaginovskiy <mo at stellarise.com> wrote:
>
>> Hi all,
>>
>> Bit of a strange one but I'm wondering if it's possible to have squid
>> redirect a site to a secondary backend server if the primary is down. Have
>> been looking into this but haven't seen much similar to this. Currently I
>> have my setup along the lines of this;
>>
>> Client -> Squid -> Backend1
>>
>> but in the event that Backend1 is down, the following should be done;
>>
>> Client -> Squid -> Backend2
>>
>> Is squid capable of monitoring connections to peer or redirecting based on
>> an ACL looking for some HTTP error code?
>>
>> Thanks.
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/High-Availability-in-Squid-tp4672899.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
>     Francesco
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/ed249ee7/attachment.htm>

From chip_pop at hotmail.com  Sun Aug 30 08:23:51 2015
From: chip_pop at hotmail.com (joe)
Date: Sun, 30 Aug 2015 01:23:51 -0700 (PDT)
Subject: [squid-users] cpu high 100%
In-Reply-To: <55E247A6.6040803@ngtech.co.il>
References: <1440714912575-4672918.post@n4.nabble.com>
 <1440840666984-4672945.post@n4.nabble.com> <55E20635.2060307@ngtech.co.il>
 <1440885892047-4672953.post@n4.nabble.com> <55E247A6.6040803@ngtech.co.il>
Message-ID: <1440923031746-4672961.post@n4.nabble.com>

<What speeds are you talking about?>
speeds of ? i did not mention speeds  bandwith ? is 25Meg   or cpu speed i
post that on previus msg detail of my server on clients side is full open
speed no Queues

<Eliezer>





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cpu-high-100-tp4672918p4672961.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Sun Aug 30 09:02:24 2015
From: vdoctor at neuf.fr (FredT)
Date: Sun, 30 Aug 2015 02:02:24 -0700 (PDT)
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <55E1CC5B.20604@urlfilterdb.com>
References: <1440400275614-4672835.post@n4.nabble.com>
 <55DADE45.2010905@treenet.co.nz> <1440408226749-4672841.post@n4.nabble.com>
 <55DAF5DD.7060406@treenet.co.nz> <1440421859169-4672846.post@n4.nabble.com>
 <55DE099F.1000507@treenet.co.nz> <1440614937789-4672888.post@n4.nabble.com>
 <55DE0E5E.5050203@treenet.co.nz> <1440806038633-4672940.post@n4.nabble.com>
 <55E1CC5B.20604@urlfilterdb.com>
Message-ID: <1440925344035-4672962.post@n4.nabble.com>

Hi Amos,
I see the new build squid-3.5.7-20150829-r13899 released taking care the
cache_swap_high, does this build apply the limit with the "Filemap bits in
use" or is it on the cache "Percent Used" only ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-7-cache-swap-high-bug-or-not-bug-tp4672750p4672962.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Aug 30 12:04:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2015 00:04:03 +1200
Subject: [squid-users] Squid 3.5.7, cache_swap_high, bug or not bug ?
In-Reply-To: <1440925344035-4672962.post@n4.nabble.com>
References: <1440400275614-4672835.post@n4.nabble.com>
 <55DADE45.2010905@treenet.co.nz> <1440408226749-4672841.post@n4.nabble.com>
 <55DAF5DD.7060406@treenet.co.nz> <1440421859169-4672846.post@n4.nabble.com>
 <55DE099F.1000507@treenet.co.nz> <1440614937789-4672888.post@n4.nabble.com>
 <55DE0E5E.5050203@treenet.co.nz> <1440806038633-4672940.post@n4.nabble.com>
 <55E1CC5B.20604@urlfilterdb.com> <1440925344035-4672962.post@n4.nabble.com>
Message-ID: <55E2F133.1040306@treenet.co.nz>

On 30/08/2015 9:02 p.m., FredT wrote:
> Hi Amos,
> I see the new build squid-3.5.7-20150829-r13899 released taking care the
> cache_swap_high, does this build apply the limit with the "Filemap bits in
> use" or is it on the cache "Percent Used" only ?

Just the bug fix, which was about the volume based watermark. The
directives still mean the same thing they have always been documented as
meaning. Just the adjusted algorithm now defines different
aggressiveness level and the docs give you some numbers to figure out
good settings from.

The only issue sfileno has is a crash bug. Which is un-found but clearly
elsewhere in the code and being tracked under its own bug number 3566.

Amos



From squid3 at treenet.co.nz  Sun Aug 30 12:54:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2015 00:54:07 +1200
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
Message-ID: <55E2FCEF.7020308@treenet.co.nz>

On 30/08/2015 1:07 p.m., Howard Waterfall wrote:
> Great stuff thanks a lot! Excellent timing too just getting ready to do the
> make. I'll follow your suggestions. A couple of follow ups:
> 
> When doing the make (including ./configure), do I have to be running as
> root or some particular user? I set up a squid user and user group and also
> a squidadm user group...

You dont for the ./configure part.

Make might need it to overwrite system files during the install part if
you choose to replace the system files in-place. But that is just
another reason to use a custom /proxy folder for the installation, which
wont need root.


> 
> Finally I noticed that configure didn't have -x permissions with 3.5.7. I
> guess I'll have to change that out of the gate...
> 

Hmm. Yes it should have -x permissions. Thanks for that I will have to
double-check the 3.5.8 release.

Amos



From squid3 at treenet.co.nz  Sun Aug 30 14:32:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2015 02:32:56 +1200
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
 <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
Message-ID: <55E31418.7040409@treenet.co.nz>

On 30/08/2015 3:58 p.m., Ashish Mukherjee wrote:
> Why should Squid take care of HA though? Isn't it the responsibility of the
> Website to ensure its uptime? Even if used as a reverse proxy, Apache may
> do a better job at this, since one can do more sophisticated configurations
> with mod_proxy.

The hint is in the name of that module: mod_*proxy*.

HA is an outcome of using any one or more of several _proxy features_.

Squid is a proxy. The purpose of mod_proxy is to hack/squash/ proxy
gateway capabilities (ie Squid functionality) into a piece of origin
server software (Apache).

Would you expect a dedicated/designed piece of proxy software or an
'other' software design with add-on tacked into it to actually be better
at the proxying functionality?

Sure you can use the other softwares add-on *if* it performs the
function correctly and sufficiently for your needs. But that does not
make it the function any less a proxy mechanism.

On the other end Browsers also have HA functionality they call "Happy
Eyeballs". Varuous downloaders and app update serices try to do IP-based
filovers and recovery. And it call causes no end of trouble to the real
proxies in the middle just doing their job properly.

Amos

> 
> On Sun, Aug 30, 2015 at 3:56 AM, Kinkie wrote:
> 
>> Hi,
>>   please see http://wiki.squid-cache.org/Features/MonitorUrl.
>> It's available in squid 2.6 , and is one of the last few features who
>> haven't yet made it to Squid 3.X. If anyone is interested, code and
>> sponsorships are always welcome :)
>>
>> On Thu, Aug 27, 2015 at 12:10 PM, Imaginovskiy wrote:
>>
>>> Hi all,
>>>
>>> Bit of a strange one but I'm wondering if it's possible to have squid
>>> redirect a site to a secondary backend server if the primary is down.

Yes it is. This is one of the core functionalities of proxying and so
well integrated there is *no* special configuration needed. In fact you
have to explicitly configure alternative if you want to avoid it.

>>> Have
>>> been looking into this but haven't seen much similar to this. Currently I
>>> have my setup along the lines of this;
>>>
>>> Client -> Squid -> Backend1
>>>
>>> but in the event that Backend1 is down, the following should be done;
>>>
>>> Client -> Squid -> Backend2
>>>
>>> Is squid capable of monitoring connections to peer or redirecting based on
>>> an ACL looking for some HTTP error code?

I would almost bet you already have it setup in your squid.conf:

 cache_peer Backend1.example.com ...
 cache_peer Backend2.example.com ...


Backend2 will be used only if Backend1 is detected as unavailable, or
starts getting overloaded/slow (yes the HA includes latency optimization
too), or starts returning some retriable status responses (most common
is 502).


Yes, simply the line order itself. Everything else is just tuning of the
HA parameters.

To prevent flapping there is a default of 10 requests that need to have
failed (in a row) before the 'dead' detection kicks in to send 100% to
Backend2. Those 10 will make TCP connect attempts to Backend1 before
retrying with Backend2 (if enough time remains in the forward_timeout).
That is configurable, see cache_peer documentation for timeouts and
failure options. background-ping may also be of interest to you.


You can improve further by using DNS-based HA availability. Pointing
each Backend1 and Backend2 domain names at multiple IPs. But be wary of
those Browsers and their (overly) "Happy eyeballs".

Amos



From squid3 at treenet.co.nz  Sun Aug 30 15:33:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2015 03:33:52 +1200
Subject: [squid-users] Please help me ,
 about none HTTP protocol (DPORT 80)
In-Reply-To: <Zw849169908966.31965@ZB-MAIL-WAP03>
References: <Zw849169908966.31965@ZB-MAIL-WAP03>
Message-ID: <55E32260.3030109@treenet.co.nz>

On 30/08/2015 1:10 p.m., ?? wrote:
> Please help me , about none HTTP protocol (DPORT 80) . 
> 
> I installed a SQUID (3.5) on Centos 6.5  as a web & video caching system .(not reverse proxy or cache )
> I use the TPROXY method following here: <http://wiki.squid-cache.org/Features/Tproxy4>
> 
> the section in this tourial :
> iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129
> 
> it will set all the data incoming from LAN erea (condition: destination port is 80) as the MARK , and then 
> squid process the data .

Yes. Amongst many other things, and only with support of the other rules
documented around it.

> 
> here , I want to ask a question :
> not all the DPORT 80 data is HTTP protocol (such as : tencent's IM service port is 80 , but it is not HTTP protocol ) 
> 
> in this condition , source client will appear problem because the SQUID box could not rightly process the data.
> 
> then how Squid process the data that dport is 80 but which is not http protocol  ?

By rejecting. Port 80 is a *reserved* port for HTTP protocol use *only*.

Software that uses port 80 for non-HTTP protocols and messaging does so
for the explicit purpose of bypassing the network administrative
policies. Squid does not support that abuse of the port.

IM protocols in particular share most of their behaviour with REST HTTP
messaging design. So there is zero excuse for those IM services not to
be using HTTP syntax for their messages on port 80.


Squid-4 contains a directive on_unsupported_protocol that lets you
configure what happens.

Amos



From squid3 at treenet.co.nz  Sun Aug 30 15:52:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2015 03:52:35 +1200
Subject: [squid-users] Internet Explorer error with SSL bumping
In-Reply-To: <SNT147-W7CA7DF76C058092BF14EECA6E0@phx.gbl>
References: <1440692464604-4672904.post@n4.nabble.com>
 <20150828103656.7e761012@efreet>
 <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>
 <55E045F5.4090103@treenet.co.nz> <SNT147-W7CA7DF76C058092BF14EECA6E0@phx.gbl>
Message-ID: <55E326C3.1000409@treenet.co.nz>

On 29/08/2015 12:17 a.m., Oliver Webb wrote:
> Thanks for your reply Amos. I will explain a bit more of my setup in the hope it clarifies a few of the issues.
> 
> I have installed the certificate portion of squids key/cert into the trusted root store of all the devices concerned all clients see the "server's" certificate as being signed by squid's private key not the origin servers.
> I have the following line in squid.conf to specifically stop the use of SSL
>     sslproxy_options NO_SSLv2 NO_SSLv3 SINGLE_DH_USE

The OpenSSL options list is ':' or ',' separated. Not spaces.

What you have there is actually just:

 sslproxy_options NO_SSLv2



> If I navigate to the internal test site I have just created that has a self signed certificate the self signed cert gets passed through to the client for them to make their own decision
> 
> If there is no easy solution I will just avoid IE, which I won't be too upset about.
> 

The problem would seem to be the OpenSSL support at the client end not
overlapping with the support in the Squid library. Which is always a
problem when dealing with very old vs very new library versions.

There are combinations like IE only supporting TLS 1.0 (the default
until last year IIRC) and the latest most modern library behind the
proxy only supporting TLS/1.1 or later.

I assumed that you did already try following IEs error page instruction
("Turn on TLS 1.0, TLS 1.1, and TLS 1.2 in Advanced settings"). Is that
correct?

Amos




From hwaterfall at gmail.com  Sun Aug 30 16:50:55 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Sun, 30 Aug 2015 09:50:55 -0700
Subject: [squid-users] logfileHandleWrite: daemon:/var/logs/access.log:
 error writing ((32) Broken pipe)
In-Reply-To: <CAP6bC05yhXF-7jE_JQjKJP0EXEzvqsV_MKSeO23+H3T1mwu4jw@mail.gmail.com>
References: <CAP6bC05Ke1Ex5R6G7KXgNr-ocm9GsPt_MMD8nW6pqkxQpdpX=A@mail.gmail.com>
 <CAP6bC05yhXF-7jE_JQjKJP0EXEzvqsV_MKSeO23+H3T1mwu4jw@mail.gmail.com>
Message-ID: <CAP6bC04mtePby2AJhhoJD+Y0RaT-i7M3R2ek9hUboy=tScLhLg@mail.gmail.com>

Resolved; well sort of!

I've been setting up log file permissions to jibe with the
*cache_effective_user* tag in /etc/squid3/squid.conf, which I had set to
squid. As I was troubleshooting, I also changed the *http_port* tag from
8080, but I noticed the output when I tried starting squid remained 8080:


*Accepting HTTP Socket connections at local=[::]:8080 remote=[::] FD 11
flags=9*

Obviously my squid.conf was not being used, that's when I noticed *squid -v*
indicated *sysconfdir=/etc/squid* (i.e. not */etc/squid**3)*.

I discovered that I had ended up with two config locations:
*/etc/squid/squid.conf*
*/etc/squid3/squid.conf*

This was likely due to issues with me using a binary install:

*sudo apt-get install squid*

and also making custom builds, with outdated ./configure options. I was
using the configure options listed here:
http://wiki.squid-cache.org/SquidFaq/CompilingSquid#Debian.2C_Ubuntu

It indicates using *--sysconfdir=/etc/squid*, but that doesn't jibe with *sudo
apt-get install squid*, which puts squid.conf in */etc/squid3*

I've opened this bug:
http://bugs.squid-cache.org/show_bug.cgi?id=4313

I'll request wiki update access too.

I indicated 'sort of' at the outset because while I'm pretty certain having
two locations for *squid.conf* was the source of all my problems, I wasn't
able to confirm 100%. I abandoned my troubleshooting activities because I
had to re-install the OS anyway. For example, I can't reconcile why I
had */etc/squid3/
*given I had purged my original squid install...

Interestingly, it turns out that I didn't need to rebuild at all. Filtering
by MAC address is now provided by default, as indicated in this thread
which also provides some excellent suggestions for configuring your build
environment:
http://lists.squid-cache.org/pipermail/squid-users/2015-August/005257.html


On Fri, Aug 28, 2015 at 5:43 PM, Howard Waterfall <hwaterfall at gmail.com>
wrote:

> Sorry, I thought that by copying the subject line of Priya's message
> thread, I would have continued the thread. Here's the link for context:
>
> http://lists.squid-cache.org/pipermail/squid-users/2015-February/002131.html
>
> Cheers,
> Deiter
>
> On Fri, Aug 28, 2015 at 8:31 AM, Howard Waterfall <hwaterfall at gmail.com>
> wrote:
>
>> Ubuntu 14.04
>> Squid 3.5.5
>>
>> Hi -
>> I'm having pretty much exactly the same problem as Priya. I didn't see a
>> resolution so I'm picking up the thread.
>>
>> I've installed and run squid 3.5.5 previously w/o a problem using:
>>
>> sudo apt-get install squid
>>
>>
>> I want to filter by mac address though, so I purged that install and
>> re-built squid with the *--enable-arp-acl* flag. This is when the
>> problems begin. I've provided what was requested from Priya below. I'm
>> grateful for any help.
>>
>> Thanks,
>> Deiter
>>
>> Output from *sudo /usr/local/squid/sbin/squid -NCd1:*
>>
>> WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
>> /usr/local/squid/var/logs/cache.log: Permission denied
>>          messages will be sent to 'stderr'.
>> 2015/08/28 14:47:27| cannot change current directory to
>> /var/spool/squid3: (2) No such file or directory
>> 2015/08/28 14:47:27| Current Directory is /home/ubuntu
>> WARNING: Cannot write log file: /usr/local/squid/var/logs/cache.log
>> /usr/local/squid/var/logs/cache.log: Permission denied
>>          messages will be sent to 'stderr'.
>> 2015/08/28 14:47:27| WARNING: Closing open FD    2
>> 2015/08/28 14:47:27| Starting Squid Cache version 3.5.5 for
>> armv7l-unknown-linux-gnueabihf...
>> 2015/08/28 14:47:27| Service Name: squid
>> 2015/08/28 14:47:27| Process ID 2118
>> 2015/08/28 14:47:27| Process Roles: master worker
>> 2015/08/28 14:47:27| With 1024 file descriptors available
>> 2015/08/28 14:47:27| Initializing IP Cache...
>> 2015/08/28 14:47:27| DNS Socket created at [::], FD 7
>> 2015/08/28 14:47:27| DNS Socket created at 0.0.0.0, FD 8
>> 2015/08/28 14:47:27| Adding nameserver 75.75.75.75 from /etc/resolv.conf
>> 2015/08/28 14:47:27| Adding nameserver 75.75.76.76 from /etc/resolv.conf
>> 2015/08/28 14:47:27| Adding domain hsd1.ca.comcast.net from
>> /etc/resolv.conf
>> 2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
>> 2015/08/28 14:47:27| Adding domain localdomain from /etc/resolv.conf
>> 2015/08/28 14:47:27| Adding nameserver 192.168.1.1 from /etc/resolv.conf
>> 2015/08/28 14:47:27| Logfile: opening log
>> daemon:/usr/local/squid/var/logs/access.log
>> 2015/08/28 14:47:27| Logfile Daemon: opening log
>> /usr/local/squid/var/logs/access.log
>> fopen: Permission denied
>> 2015/08/28 14:47:27| Store logging disabled
>> 2015/08/28 14:47:27| Swap maxSize 0 + 262144 KB, estimated 20164 objects
>> 2015/08/28 14:47:27| Target number of buckets: 1008
>> 2015/08/28 14:47:27| Using 8192 Store buckets
>> 2015/08/28 14:47:27| Max Mem  size: 262144 KB
>> 2015/08/28 14:47:27| Max Swap size: 0 KB
>> 2015/08/28 14:47:27| Using Least Load store dir selection
>> 2015/08/28 14:47:27| cannot change current directory to
>> /var/spool/squid3: (2) No such file or directory
>> 2015/08/28 14:47:27| Current Directory is /home/ubuntu
>> 2015/08/28 14:47:27| Finished loading MIME types and icons.
>> 2015/08/28 14:47:27| HTCP Disabled.
>> 2015/08/28 14:47:27| Squid plugin modules loaded: 0
>> 2015/08/28 14:47:27| Adaptation support is off.
>> 2015/08/28 14:47:27| Accepting HTTP Socket connections at local=[::]:8080
>> remote=[::] FD 11 flags=9
>> 2015/08/28 14:47:28| logfileHandleWrite:
>> daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
>> pipe)
>> 2015/08/28 14:47:28| Closing HTTP port [::]:8080
>> 2015/08/28 14:47:28| storeDirWriteCleanLogs: Starting...
>> 2015/08/28 14:47:28|   Finished.  Wrote 0 entries.
>> 2015/08/28 14:47:28|   Took 0.00 seconds (  0.00 entries/sec).
>> FATAL: I don't handle this error well!
>> Squid Cache (Version 3.5.5): Terminated abnormally.
>> CPU Usage: 0.290 seconds = 0.220 user + 0.070 sys
>> Maximum Resident Size: 34480 KB
>> Page faults with physical i/o: 0
>>
>>
>> Output from *squid -v:*
>>
>> Squid Cache: Version 3.5.5
>> Service Name: squid
>> configure options:  '--prefix=/usr' '--localstatedir=/var'
>> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
>> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
>> '--with-logdir=/var/log' '--with-pidfile=/var/run/squid.pid'
>> '--enable-arp-acl' --enable-ltdl-convenience
>>
>> Output from *ls -al /usr/local/squid/var/logs/access.log:*
>>
>> -rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47
>> /usr/local/squid/var/logs/access.log
>>
>> Output from *ls -al /usr/local/squid/var/logs:*
>>
>> total 5224
>> drwxrwxrwx 2 squid squid_admin    4096 Aug 28 09:35 .
>> drwxr-xr-x 5 root  root           4096 Jun 12 06:57 ..
>> -rw-rw---- 1 squid squid_admin 5262306 Jun 29 21:47 access.log
>> -rw-rw---- 1 squid squid_admin   67925 Jun 27 02:47 cache.log
>>
>> Output from *ls -al /usr/local/squid/var:*
>>
>> total 20
>> drwxr-xr-x 5 root  root        4096 Jun 12 06:57 .
>> drwxr-xr-x 8 root  root        4096 Jun 12 06:57 ..
>> drwxrwxrwx 3 root  root        4096 Jun 12 06:57 cache
>> drwxrwxrwx 2 squid squid_admin 4096 Aug 28 09:35 logs
>> drwxrwxrwx 3 root  root        4096 Jun 21 12:17 run
>>
>> I adjusted the paths above from what was asked of Priya to jibe with my
>> set-up, but just in case:
>>
>> Output from *ls -al /var/logs/access.log:*
>>
>> ls: cannot access /var/logs/access.log: No such file or directory
>>
>>
>> Output from *ls -al /var/logs:*
>>
>> ls: cannot access /var/logs: No such file or directory
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/2527f73c/attachment.htm>

From hwaterfall at gmail.com  Sun Aug 30 17:42:03 2015
From: hwaterfall at gmail.com (Howard Waterfall)
Date: Sun, 30 Aug 2015 10:42:03 -0700
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <55E2FCEF.7020308@treenet.co.nz>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
Message-ID: <CAP6bC07e8tQboWUj44eSyLDR3-MEow7wDH_VNYNYSAv8no=V=w@mail.gmail.com>

+squid-users

Thanks again, this is valuable information!

As you may have guessed, I'm asking about the user that should do builds to
ensure that the build outputs are created with the appropriate permissions
- I get a little concerned about security. It sounds like you are
suggesting that I simply create a directory for my custom builds:

I assign the --prefix option to the folder I create, so my build output
goes there, and then I make sure the permissions for that folder (and it's
sub-directories) are set for the user defined by *cache_effective_user* (and
the user defined by the ./configure option --*with-default-user*). Could
you confirm?

Finally (I hope), I've re-installed Ubuntu (various reasons, not just squid
issues) and I successfully installed squid using:
*sudo apt-get install squid3*

Squid wasn't found the first time:
*E: Unable to locate package squid3*

I had to run this first:
*sudo apt-get update*

However, when I try *apt-get build-dep squid,* I get:
*You must put some 'source' uris in your sources.list*

I can't seem to get over this problem. I've un-commented every line in
*/etc/apt/sources.list* that starts with deb-src.

Could you suggest a repository that I can add to */etc/apt/sources.list*?

Thanks,
Deiter

On Sun, Aug 30, 2015 at 5:54 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 30/08/2015 1:07 p.m., Howard Waterfall wrote:
> > Great stuff thanks a lot! Excellent timing too just getting ready to do
> the
> > make. I'll follow your suggestions. A couple of follow ups:
> >
> > When doing the make (including ./configure), do I have to be running as
> > root or some particular user? I set up a squid user and user group and
> also
> > a squidadm user group...
>
> You dont for the ./configure part.
>
> Make might need it to overwrite system files during the install part if
> you choose to replace the system files in-place. But that is just
> another reason to use a custom /proxy folder for the installation, which
> wont need root.
>
>
> >
> > Finally I noticed that configure didn't have -x permissions with 3.5.7. I
> > guess I'll have to change that out of the gate...
> >
>
> Hmm. Yes it should have -x permissions. Thanks for that I will have to
> double-check the 3.5.8 release.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/efac705e/attachment.htm>

From yvoinov at gmail.com  Sun Aug 30 17:53:43 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 30 Aug 2015 23:53:43 +0600
Subject: [squid-users] Internet Explorer error with SSL bumping
In-Reply-To: <55E326C3.1000409@treenet.co.nz>
References: <1440692464604-4672904.post@n4.nabble.com>
 <20150828103656.7e761012@efreet>
 <SNT147-W60A67D51D31928FE97E465CA6E0@phx.gbl>
 <55E045F5.4090103@treenet.co.nz> <SNT147-W7CA7DF76C058092BF14EECA6E0@phx.gbl>
 <55E326C3.1000409@treenet.co.nz>
Message-ID: <55E34327.9020605@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


30.08.15 21:52, Amos Jeffries ?????:
> On 29/08/2015 12:17 a.m., Oliver Webb wrote:
>> Thanks for your reply Amos. I will explain a bit more of my setup in
the hope it clarifies a few of the issues.
>>
>> I have installed the certificate portion of squids key/cert into the
trusted root store of all the devices concerned all clients see the
"server's" certificate as being signed by squid's private key not the
origin servers.
>> I have the following line in squid.conf to specifically stop the use
of SSL
>>     sslproxy_options NO_SSLv2 NO_SSLv3 SINGLE_DH_USE
>
> The OpenSSL options list is ':' or ',' separated. Not spaces.
>
> What you have there is actually just:
>
>  sslproxy_options NO_SSLv2
Amos, this is not obvious. And nowhere clear documented, AFAIK. In Squid
documentation.

Read any and all sources is no substitute for good documentation.

Am I right or I am absolutely right?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV40MnAAoJENNXIZxhPexG2mcH/0HLNazE3sR8v7D4BUL6pwDS
otlGC3hxY2O23BCo4Lnd6PdiV/biPs5ceoezqjwBzL65157jkq3Uvk+YjQXwYLdU
svN2BR2i9ZNns8wTpsyT3fDj6ILOFW+NbhnUbsAVfdQ3YsZM97Q8BmM92Idibzik
/O5aDqjWrhbCbB033warzTXyn4ag6pQ1Y03OC6Zs4kJsr8gdG9rRW16zCKIzCnrk
sAZtTT2w4IuKi5qffn+rQ9b04Qp31snFDYPnTb8oN0jWQnT9VvjvhCNjjNKCmBZU
zsqUyZ9w6C5hnOT5ShGgSH8DVz7dLqPFlijNCmxdtoafC529WWkj+faXxwP2vGc=
=UOmz
-----END PGP SIGNATURE-----



From rafael.akchurin at diladele.com  Sun Aug 30 18:58:13 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 30 Aug 2015 18:58:13 +0000
Subject: [squid-users] Building squid | Best Practices?
In-Reply-To: <CAP6bC07e8tQboWUj44eSyLDR3-MEow7wDH_VNYNYSAv8no=V=w@mail.gmail.com>
References: <CAP6bC04LMFEMqrpRJr6aSzC-cZDNUi9NGHwovL_g2RU2nXm_xw@mail.gmail.com>
 <55E24EC2.30704@treenet.co.nz>
 <CAP6bC07t6AKCSpc-5ub7JaHuN329oN2njwUXb8btT9kUziJiGQ@mail.gmail.com>
 <55E2FCEF.7020308@treenet.co.nz>
 <CAP6bC07e8tQboWUj44eSyLDR3-MEow7wDH_VNYNYSAv8no=V=w@mail.gmail.com>
Message-ID: <VI1PR04MB1359E379C6845822B90FF3B28F6C0@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Howard,

This is how we rebuild default Squid 3.3.8 in Ubuntu 14.01 to enable HTTPS filtering - http://docs.diladele.com/administrator_guide_4_2/installation_and_removal/ubuntu14/squid.html
Hope the scripts can be easily adapted to build the latest Squid instead of the stock one.

Best regards,
Rafael Akchurin
Diladele B.V.

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Howard Waterfall
Sent: Sunday, August 30, 2015 7:42 PM
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Building squid | Best Practices?

+squid-users

Thanks again, this is valuable information!

As you may have guessed, I'm asking about the user that should do builds to ensure that the build outputs are created with the appropriate permissions - I get a little concerned about security. It sounds like you are suggesting that I simply create a directory for my custom builds:

I assign the --prefix option to the folder I create, so my build output goes there, and then I make sure the permissions for that folder (and it's sub-directories) are set for the user defined by cache_effective_user (and the user defined by the ./configure option --with-default-user). Could you confirm?

Finally (I hope), I've re-installed Ubuntu (various reasons, not just squid issues) and I successfully installed squid using:
sudo apt-get install squid3

Squid wasn't found the first time:
E: Unable to locate package squid3

I had to run this first:
sudo apt-get update

However, when I try apt-get build-dep squid, I get:
You must put some 'source' uris in your sources.list

I can't seem to get over this problem. I've un-commented every line in /etc/apt/sources.list that starts with deb-src.

Could you suggest a repository that I can add to /etc/apt/sources.list?

Thanks,
Deiter

On Sun, Aug 30, 2015 at 5:54 AM, Amos Jeffries <squid3 at treenet.co.nz<mailto:squid3 at treenet.co.nz>> wrote:
On 30/08/2015 1:07 p.m., Howard Waterfall wrote:
> Great stuff thanks a lot! Excellent timing too just getting ready to do the
> make. I'll follow your suggestions. A couple of follow ups:
>
> When doing the make (including ./configure), do I have to be running as
> root or some particular user? I set up a squid user and user group and also
> a squidadm user group...

You dont for the ./configure part.

Make might need it to overwrite system files during the install part if
you choose to replace the system files in-place. But that is just
another reason to use a custom /proxy folder for the installation, which
wont need root.


>
> Finally I noticed that configure didn't have -x permissions with 3.5.7. I
> guess I'll have to change that out of the gate...
>

Hmm. Yes it should have -x permissions. Thanks for that I will have to
double-check the 3.5.8 release.

Amos

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150830/987ae9c0/attachment.htm>

From a.alii85 at gmail.com  Mon Aug 31 06:04:17 2015
From: a.alii85 at gmail.com (asad)
Date: Mon, 31 Aug 2015 11:04:17 +0500
Subject: [squid-users] Using Squid as forward http proxy failing to
 complete request?
In-Reply-To: <55E07610.9080300@treenet.co.nz>
References: <CAP3=H7s77gS1cYZdtyhVu9qmuDdRM3G9wHteSk8cL=y4Z0xWNQ@mail.gmail.com>
 <55D74A67.3000600@treenet.co.nz>
 <CAP3=H7sS7bVM3CEWVfF6pRkeeNbFhXp8O_HDv0XmE==A4RZdQw@mail.gmail.com>
 <55D758B1.1090309@treenet.co.nz>
 <CAP3=H7uOYss84FM_ce+zOry90T6qeCpOs5kWTfgNDmfV_Azcsw@mail.gmail.com>
 <55DADA7E.5030105@treenet.co.nz>
 <CAP3=H7tZMsSoCFZP-nd33m9GBwBKuQeG6OO_0D4_hDcnK1+M+Q@mail.gmail.com>
 <55DB13CC.6010709@treenet.co.nz>
 <CAP3=H7tMiqHLjxMEZ6O98iqOiyEzwawO9EQDO1c8At9_NKfNjA@mail.gmail.com>
 <CAP3=H7v0MGZACBZVeDUSj7qXtsXYn-sF7=p2tVO8fKrxCWOqug@mail.gmail.com>
 <55DC0141.9000203@treenet.co.nz>
 <CAP3=H7sPXGbKeh+3vtL9uZ+76BJH_KuTkfFbihRhjw2Ji6D6kw@mail.gmail.com>
 <55E07610.9080300@treenet.co.nz>
Message-ID: <CAP3=H7sxZm0wNVAzV1koUz-Fmdr+G+Zwhhify2eGL3cMq8YhxQ@mail.gmail.com>

Amos thanks. I was sick over the weekend thus the late reply

Sorry by mistake I left out the mailing-list email on previously mail.
I would look into the donation link and see how I can tribute :).

Bug:) that was unexpected. I thought it was routine debugging work.

Here is output of squid -v


"
Squid Cache: Version 3.5.7-20150808-r13884
Service Name: squid
configure options:  '--bindir=/bin/squid' '--sbindir=/usr/sbin/squid'
'--sysconfdir=/etc/squid' '--datadir=/usr/share/squid'
'--libexecdir=/usr/lib/squid' '--disable-strict-error-checking'
'--with-logdir=/var/log/squid' '--with-swapdir=/var/cache/squid'
'--with-pidfile=/var/run/squid.pid' '--enable-ssl'
'--enable-delay-pools' '--enable-ssl-crtd' '--enable-icap-client'
'--enable-esi' '--disable-eui' '--localstatedir=/var/run/squid'
'--sharedstatedir=/var/run/squid' '--datarootdir=/usr/share/squid'
'--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped'
'--enable-auth-basic=DB,LDAP,NCSA,POP3,RADIUS,SASL,SMB,fake,getpwnam'
'--enable-auth-ntlm=fake' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=LDAP_group,SQL_session,eDirectory_userip,file_userip,kerberos_ldap_group,session,time_quota,unix_group,wbinfo_group'
'--with-openssl' '--with-filedescriptors=65536'
'--enable-removal-policies=lru,heap'
"

The last output of http-headers was taken from cache.log itself.

regards
Aasad


On 8/28/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 28/08/2015 10:50 p.m., asad wrote:
>> Sorry I complete missed this reply from you.
>>
>
> We'd probably get back to cc'ig the list if you dont mind.
> I try to restrict private help to my paying clients.
>
> Though if you would care to donate what you think a fair value I happy
> to continue in private; <http://treenet.co.nz/projects/squid/>
>
>
>> I enabled debugging , and here is the results from cache.log file.
>>
>>
>> Here is one for https://www.google.com
>>
>> "2015/08/28 15:45:00.749 kid1| client_side.cc(2337) parseHttpRequest:
>> HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:64062 FD 16 flags=1
>> 2015/08/28 15:45:00.749 kid1| client_side.cc(2338) parseHttpRequest:
>> HTTP Client REQUEST:
>> ---------
>> CONNECT www.google.com.pk:443 HTTP/1.1
>> User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0)
>> Gecko/20100101 Firefox/40.0
>> Proxy-Connection: keep-alive
>> Connection: keep-alive
>> Host: www.google.com.pk:443
>>
>>
>> ----------
>> 2015/08/28 15:45:00.751 kid1| tunnel.cc(1103)
>> tunnelRelayConnectRequest: Tunnel Server REQUEST:
>> local=10.10.131.13:64063 remote=10.10.32.4:8080 FD 19 flags=1:
>> ----------
>>  tunnelRelayConnectRequest[267]"
>
> Drat. Well you have found a bug in the debugging code. :-(
>
> That should be listing what was attempting to leave. But
> tunnelRelayConnectRequest is just where the 267 byte string was stored.
>
> What is your squid -v output? I'm probably going to have to give you a
> patch for this.
>
>>
>>
>> Interestingly, in both the case there is no response the request never
>> left my machine running squid proxy. I hope you can shed some light on
>> this.
>
> Yes that is interesting. I'm seeing something similar here now with
> Squid-4 trying to resolve the above bug. Not sure if its a stupid
> mistake in my rough test setup or something else.
>
> Did you get that from packet traces or something independent of Squid?
>
>
> Cheers
> Amos
>


From ashish.mukherjee at gmail.com  Mon Aug 31 06:23:58 2015
From: ashish.mukherjee at gmail.com (Ashish Mukherjee)
Date: Mon, 31 Aug 2015 11:53:58 +0530
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <55E31418.7040409@treenet.co.nz>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
 <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
 <55E31418.7040409@treenet.co.nz>
Message-ID: <CACgMzfw=9MhNzEftOWMstPzkSe4mN9hL9nO-648dkUdP0g2=NQ@mail.gmail.com>

Hi,

Agree that Squid is a specialized proxy and more optimal architecture for
the purpose and trying to achieve HA on the Browser side is certainly a bad
idea.

Talking specifically of a reverse proxy scenario, whether one uses Squid or
Apache mod_proxy or something else may well depend upon what other features
are needed and the traffic volumes. In many reverse proxy environments
where more complex control is needed, mod_proxy seems to be often used with
modules like mod_rewrite. My understanding of Squid is that it does great
as a proxy but does not provide these features, as that is not its
purpose.  Does Squid have its own production level extensions for some
scenarios which may be typically addressed by Apache modules?

Thanks!

- Ashish

On Sun, Aug 30, 2015 at 8:02 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 30/08/2015 3:58 p.m., Ashish Mukherjee wrote:
> > Why should Squid take care of HA though? Isn't it the responsibility of
> the
> > Website to ensure its uptime? Even if used as a reverse proxy, Apache may
> > do a better job at this, since one can do more sophisticated
> configurations
> > with mod_proxy.
>
> The hint is in the name of that module: mod_*proxy*.
>
> HA is an outcome of using any one or more of several _proxy features_.
>
> Squid is a proxy. The purpose of mod_proxy is to hack/squash/ proxy
> gateway capabilities (ie Squid functionality) into a piece of origin
> server software (Apache).
>
> Would you expect a dedicated/designed piece of proxy software or an
> 'other' software design with add-on tacked into it to actually be better
> at the proxying functionality?
>
> Sure you can use the other softwares add-on *if* it performs the
> function correctly and sufficiently for your needs. But that does not
> make it the function any less a proxy mechanism.
>
> On the other end Browsers also have HA functionality they call "Happy
> Eyeballs". Varuous downloaders and app update serices try to do IP-based
> filovers and recovery. And it call causes no end of trouble to the real
> proxies in the middle just doing their job properly.
>
> Amos
>
> >
> > On Sun, Aug 30, 2015 at 3:56 AM, Kinkie wrote:
> >
> >> Hi,
> >>   please see http://wiki.squid-cache.org/Features/MonitorUrl.
> >> It's available in squid 2.6 , and is one of the last few features who
> >> haven't yet made it to Squid 3.X. If anyone is interested, code and
> >> sponsorships are always welcome :)
> >>
> >> On Thu, Aug 27, 2015 at 12:10 PM, Imaginovskiy wrote:
> >>
> >>> Hi all,
> >>>
> >>> Bit of a strange one but I'm wondering if it's possible to have squid
> >>> redirect a site to a secondary backend server if the primary is down.
>
> Yes it is. This is one of the core functionalities of proxying and so
> well integrated there is *no* special configuration needed. In fact you
> have to explicitly configure alternative if you want to avoid it.
>
> >>> Have
> >>> been looking into this but haven't seen much similar to this.
> Currently I
> >>> have my setup along the lines of this;
> >>>
> >>> Client -> Squid -> Backend1
> >>>
> >>> but in the event that Backend1 is down, the following should be done;
> >>>
> >>> Client -> Squid -> Backend2
> >>>
> >>> Is squid capable of monitoring connections to peer or redirecting
> based on
> >>> an ACL looking for some HTTP error code?
>
> I would almost bet you already have it setup in your squid.conf:
>
>  cache_peer Backend1.example.com ...
>  cache_peer Backend2.example.com ...
>
>
> Backend2 will be used only if Backend1 is detected as unavailable, or
> starts getting overloaded/slow (yes the HA includes latency optimization
> too), or starts returning some retriable status responses (most common
> is 502).
>
>
> Yes, simply the line order itself. Everything else is just tuning of the
> HA parameters.
>
> To prevent flapping there is a default of 10 requests that need to have
> failed (in a row) before the 'dead' detection kicks in to send 100% to
> Backend2. Those 10 will make TCP connect attempts to Backend1 before
> retrying with Backend2 (if enough time remains in the forward_timeout).
> That is configurable, see cache_peer documentation for timeouts and
> failure options. background-ping may also be of interest to you.
>
>
> You can improve further by using DNS-based HA availability. Pointing
> each Backend1 and Backend2 domain names at multiple IPs. But be wary of
> those Browsers and their (overly) "Happy eyeballs".
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150831/818b0c6c/attachment.htm>

From nelson.corbo at teleflex.com  Mon Aug 31 13:08:01 2015
From: nelson.corbo at teleflex.com (Corbo, Nelson)
Date: Mon, 31 Aug 2015 13:08:01 +0000
Subject: [squid-users] Software caused connection abort
Message-ID: <BLUPR0801MB5803663A1CA15379755A2A6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>

Hello,

              I'm getting failure messages in squid 3.57 for Windows, I don't know the reason. I'll appreciate your support with this issue.

Access.log:

2015/08/31 10:03:41 kid1| local=IP_SQUID_SERVER:3128 remote=IP_CLIENT:CLIENT_PORT FD 77 flags=1: read/write failure: (113) Software caused connection abort


Nelson Corbo
IT Responsible
Teleflex Medical Uruguay
Camino Carrasco 7365
Montevideo 12100 - Uruguay
Office Phone (598) 2601 9835
Fax Number  (598) 2604 3716
nelson.corbo at teleflex.com<mailto:ncorbo at teleflexmedical.com>
www.teleflex.com<http://www.teleflex.com>


This e-mail message is for the sole use of the intended recipient(s) and may contain confidential and/or privileged information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply e-mail and destroy all copies of the original message.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150831/80c7b1a7/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Aug 31 13:14:54 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 31 Aug 2015 14:14:54 +0100
Subject: [squid-users] Software caused connection abort
In-Reply-To: <BLUPR0801MB5803663A1CA15379755A2A6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
References: <BLUPR0801MB5803663A1CA15379755A2A6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
Message-ID: <201508311414.54222.Antony.Stone@squid.open.source.it>

On Monday 31 Aug 2015 at 14:08, Corbo, Nelson wrote:

>               I'm getting failure messages in squid 3.57 for Windows, I
> don't know the reason. I'll appreciate your support with this issue.

We'd appreciate in return some information about:

 - what was the request which got aborted?

 - what other information about this connection appeared in the log file 
leading up to this message?

 - what is your squid.conf (without comments or blank lines)?

In other words, please give us as much information as you can (rather than as 
little as you can) to help us work out what might have happened...

> Access.log:
> 
> 2015/08/31 10:03:41 kid1| local=IP_SQUID_SERVER:3128
> remote=IP_CLIENT:CLIENT_PORT FD 77 flags=1: read/write failure: (113)
> Software caused connection abort

Regards,


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From nelson.corbo at teleflex.com  Mon Aug 31 13:52:24 2015
From: nelson.corbo at teleflex.com (Corbo, Nelson)
Date: Mon, 31 Aug 2015 13:52:24 +0000
Subject: [squid-users] Software caused connection abort
In-Reply-To: <201508311414.54222.Antony.Stone@squid.open.source.it>
References: <BLUPR0801MB5803663A1CA15379755A2A6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
 <201508311414.54222.Antony.Stone@squid.open.source.it>
Message-ID: <BLUPR0801MB58091DC1D983532C2EC37F6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>

Antony thanks for your quick reply

About request it looks to be this one:
Acces.log
		1441029005.619 2159098 10.183.2.33 TCP_TUNNEL/200 23389 CONNECT outlook.office365.com:443 - HIER_DIRECT/132.245.44.226 -
Cache.log
		2015/08/31 10:50:05 kid1| local=10.183.10.61:3128 remote=10.183.2.33:50206 FD 93 flags=1: read/write failure: (113) Software caused connection abort

The squid installation is new and with the default squid.conf, the unique change is dns server because I'm trying to start with a clean installation (previously I'd installed 2.7 and 3.56)
********************************************squid.conf******************************************************
acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access allow localhost manager
http_access deny manager

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localnet
http_access allow localhost

http_access deny all

http_port 3128

coredump_dir /var/cache/squid

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

dns_nameservers 8.8.8.8 208.67.222.222

max_filedescriptors 3200


--
Nelson Corbo
IT Responsible

-----Mensaje original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de Antony Stone
Enviado el: lunes, 31 de agosto de 2015 10:15
Para: squid-users at lists.squid-cache.org
Asunto: Re: [squid-users] Software caused connection abort

On Monday 31 Aug 2015 at 14:08, Corbo, Nelson wrote:

>               I'm getting failure messages in squid 3.57 for Windows, 
> I don't know the reason. I'll appreciate your support with this issue.

We'd appreciate in return some information about:

 - what was the request which got aborted?

 - what other information about this connection appeared in the log file leading up to this message?

 - what is your squid.conf (without comments or blank lines)?

In other words, please give us as much information as you can (rather than as little as you can) to help us work out what might have happened...

> Access.log:
> 
> 2015/08/31 10:03:41 kid1| local=IP_SQUID_SERVER:3128 
> remote=IP_CLIENT:CLIENT_PORT FD 77 flags=1: read/write failure: (113) 
> Software caused connection abort

Regards,


Antony.

--
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From informaticagabriel at gmail.com  Mon Aug 31 14:12:09 2015
From: informaticagabriel at gmail.com (=?UTF-8?Q?Gabriel_Ordo=C3=B1ez?=)
Date: Mon, 31 Aug 2015 07:12:09 -0700 (PDT)
Subject: [squid-users] You can use squid on site like facebook or
	youtube?
In-Reply-To: <1440851042950-4672947.post@n4.nabble.com>
References: <1440801098630-4672939.post@n4.nabble.com>
 <55E17CFC.3050104@gmail.com> <1440849025359-4672946.post@n4.nabble.com>
 <1440851042950-4672947.post@n4.nabble.com>
Message-ID: <1441030329267-4672977.post@n4.nabble.com>

many thanks .... Anas is very encouraging to know that someone is working for
you to continue with my goal.

greetings and thank you very much.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/You-can-use-squid-on-site-like-facebook-or-youtube-tp4672939p4672977.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Aug 31 14:28:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 02:28:59 +1200
Subject: [squid-users] High-Availability in Squid
In-Reply-To: <CACgMzfw=9MhNzEftOWMstPzkSe4mN9hL9nO-648dkUdP0g2=NQ@mail.gmail.com>
References: <1440670224046-4672899.post@n4.nabble.com>
 <CA+Y8hcN2o9VE9FTQkyprpV+Dd8Pc21CqTbicLiqCf=SzExNWtA@mail.gmail.com>
 <CACgMzfz99OgBWCCbnCA=Hg00wZHPoFj=46ijqS8qXUV+DJddYw@mail.gmail.com>
 <55E31418.7040409@treenet.co.nz>
 <CACgMzfw=9MhNzEftOWMstPzkSe4mN9hL9nO-648dkUdP0g2=NQ@mail.gmail.com>
Message-ID: <55E464AB.1020903@treenet.co.nz>

On 31/08/2015 6:23 p.m., Ashish Mukherjee wrote:
> Hi,
> 
> Agree that Squid is a specialized proxy and more optimal architecture for
> the purpose and trying to achieve HA on the Browser side is certainly a bad
> idea.
> 
> Talking specifically of a reverse proxy scenario, whether one uses Squid or
> Apache mod_proxy or something else may well depend upon what other features
> are needed and the traffic volumes. In many reverse proxy environments
> where more complex control is needed, mod_proxy seems to be often used with
> modules like mod_rewrite.

Does it not strike you as somehow wrong that "flexibility" is gained by
mangling the original request URL in ways such that the script engines
do not see what the original actually is?

A very large portion of that "complexity" in the backend applications
and CGI is having to deal with the way the URL was (or might have been)
mangled by the server itself. Then guessing what URIs to output that the
client would understand in the public context.


> My understanding of Squid is that it does great
> as a proxy but does not provide these features, as that is not its
> purpose.  Does Squid have its own production level extensions for some
> scenarios which may be typically addressed by Apache modules?


The key is again in the middle word of the phrase "reverse proxy
scenario". If it is *proxy* related Squid does it.


extensions? everything relevant is core functionality to a proxy. But
yes, there are addons and extensions for integrating to particular
network situations. We call them "helpers".

"production level"? Squid is the de-facto benchmark all the other
proxying software is compared against. Including mod_proxy. We usually
see them crowing about how fast they are at one particular little
targeted feature while glossing over the things they traded away to get
that speed. Squid goes somewhat slower overall, but fast enough and
"does everything".


virtual hosting?
 http_port 80 accel vhost
 https_port 80 accel vhost

mod_rewrite?
 url_rewrite_program (a helper interface, script your own poison)

mod_proxy?
 cache_peer

authentication?
 auth_param (a helper interface)

security policies?
 acl (including a helper interface ACL)
 many *_access directives

message payload transcoding?
 ESI
 icap_service
 ecap_service


Okay that last one is not internal to Squid exactly (except ESI is), but
that is because of the line between proxy and origin: touching the
message content is not a proxy functionality.


The one scenario where using Apache modules makes complete sense is when
dealing with FastCGI and/or a mix of FastCGI and static content on the
same server. Thats where Apache came from, and it does it well.

Once you start getting into using HTTP to pull from other servers and/or
ports on one server you are moving well into territory where a proper
proxy is the better tool (not just Squid, there are others). Apache
simply wont scale. Squid scales both horizontally and vertically. Our
poster child installations are Wikimedia (~200 Squid serving up
Wikipedia on a scale of TB/sec), and FrontierNET at CERN (a mesh layout
pumping Petabytes of science data around, where the small files are
measured in GB).

Amos



From stan.prescott at gmail.com  Mon Aug 31 14:34:25 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Mon, 31 Aug 2015 09:34:25 -0500
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
	SSLBump enabled
Message-ID: <CANLNtGR4WEK_BLWKBsw8zj3zNQucCmV_nfnB+257CkDha2BZ=A@mail.gmail.com>

We have users of Squid 3.5.x with SSLBump enabled complaining about their
DropBox and GoogleDrive apps not connecting. We are assuming this is
related to the fact that these apps use HTTPS but they are not part of any
of the browsers, therefor these apps do not have the sefl-signed
certificate installed needed for SSLBump.

Has anyone successfully gotten these apps to connect with Squid's SSLBump
enabled?

Regards,

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150831/c6c06a89/attachment.htm>

From phearing at gmail.com  Mon Aug 31 14:59:42 2015
From: phearing at gmail.com (Shane King)
Date: Mon, 31 Aug 2015 07:59:42 -0700
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
Message-ID: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>

Accessing via the browser may work but the sync clients that sit in the system tray use certificate pinning I believe. So if certificate pinning is being used, ssl bumping will not work. You will see an alert message in the pcap followed by a connection termination.
-Shane-------- Original message --------From: Stanford Prescott <stan.prescott at gmail.com> Date: 8/31/2015  07:34  (GMT-07:00) To: squid-users <squid-users at lists.squid-cache.org> Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with SSLBump enabled 
We have users of Squid 3.5.x with SSLBump enabled complaining about their DropBox and GoogleDrive apps not connecting. We are assuming this is related to the fact that these apps use HTTPS but they are not part of any of the browsers, therefor these apps do not have the sefl-signed certificate installed needed for SSLBump. 

Has anyone successfully gotten these apps to connect with Squid's SSLBump enabled?

Regards,

Stan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150831/7c276bc9/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 31 15:11:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 03:11:57 +1200
Subject: [squid-users] Software caused connection abort
In-Reply-To: <BLUPR0801MB58091DC1D983532C2EC37F6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
References: <BLUPR0801MB5803663A1CA15379755A2A6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
 <201508311414.54222.Antony.Stone@squid.open.source.it>
 <BLUPR0801MB58091DC1D983532C2EC37F6F26B0@BLUPR0801MB580.namprd08.prod.outlook.com>
Message-ID: <55E46EBC.7070802@treenet.co.nz>

On 1/09/2015 1:52 a.m., Corbo, Nelson wrote:
> Antony thanks for your quick reply
> 
> About request it looks to be this one:
> Acces.log
> 		1441029005.619 2159098 10.183.2.33 TCP_TUNNEL/200 23389 CONNECT outlook.office365.com:443 - HIER_DIRECT/132.245.44.226 -
> Cache.log
> 		2015/08/31 10:50:05 kid1| local=10.183.10.61:3128 remote=10.183.2.33:50206 FD 93 flags=1: read/write failure: (113) Software caused connection abort
> 

The error is not coming from Squid itself, but from the system
(specifically the TCP stack) underneath Squid.

Whats going on there is that Squid is asked to setup a tunnel, it does
so. ~23KB of data transits within 36sec, then the TCP connection between
Squid and the client is terminated by "something" happening in the middle.

An educated guess is that all that 23K was transmitted in ~6sec. then
nothing happened until a 30sec TCP or NAT timeout triggered in a router
or switch along the path.

Amos



From fredbmail at free.fr  Mon Aug 31 15:47:57 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 31 Aug 2015 17:47:57 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <55D76131.2070409@treenet.co.nz>
Message-ID: <1581965756.83618121.1441036077918.JavaMail.root@zimbra4-e1.priv.proxad.net>

I'm thinking about something like this 



From fredbmail at free.fr  Mon Aug 31 16:01:43 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 31 Aug 2015 18:01:43 +0200 (CEST)
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1581965756.83618121.1441036077918.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> I'm thinking about something like this
> 
> 


Sorry wrong move :)

So, What I meant was 

I'm thinking about something like this

# HTTP 1/1
# The refresh_pattern rules applied only to responses without an explicit expiration time
# min 1440 minutes
# Max 10080 minutes

# http 10080 / 60 / 24 H = 7 days
refresh_pattern ^http 1440 10% 10080

# All Files 30 days max
refresh_pattern -i \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)       43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)  43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll)         43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob) 43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
refresh_pattern -i \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv) 43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale

Maybe my values are too high ? 

About reload-into-ims if I understand right it translates "Pragma: no-cache" into "If-Modified-Since" if the object is cached and if there is no explicit expiration time, in real life this should be dangerous or annoying for users ?

The context is many simultaneous users (thousands) with very different kind of profiles

Regards

FredB


From adricustodio at uol.com.br  Mon Aug 31 16:16:08 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Mon, 31 Aug 2015 09:16:08 -0700 (PDT)
Subject: [squid-users] Splash page ?
Message-ID: <1441037768835-4672984.post@n4.nabble.com>

Hello guys...

I need to make on my squid the following.

When the user put its login and password it redirects the user to a default
page (lets say www.example.com)
And then after that the user can browse normally.
Some people told me i need to make a splash page, is that right ?
how do i do that ?

I added this to my .conf 

external_acl_type splash_page ttl=60 concurrency=100 %SRC
/usr/lib64/squid/ext_session_acl -t 7200 -b /tmp/squidcache/sessions.db

acl existing_users external splash_page

http_access deny !existing_users

deny_info 511:/etc/squid/splash.html existing_users

but im not sure how this works... i tested on a PC here and nothing changed.






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Splash-page-tp4672984.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From adricustodio at uol.com.br  Mon Aug 31 16:38:43 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Mon, 31 Aug 2015 09:38:43 -0700 (PDT)
Subject: [squid-users] Splash page ?
In-Reply-To: <1441037768835-4672984.post@n4.nabble.com>
References: <1441037768835-4672984.post@n4.nabble.com>
Message-ID: <1441039123583-4672985.post@n4.nabble.com>

Well... now its appearing the following when i try to access...

Internal Error: Missing Template /etc/squid/splash.html

Ideas ?




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Splash-page-tp4672984p4672985.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Aug 31 16:53:29 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 31 Aug 2015 17:53:29 +0100
Subject: [squid-users] Splash page ?
In-Reply-To: <1441039123583-4672985.post@n4.nabble.com>
References: <1441037768835-4672984.post@n4.nabble.com>
 <1441039123583-4672985.post@n4.nabble.com>
Message-ID: <201508311753.29405.Antony.Stone@squid.open.source.it>

On Monday 31 Aug 2015 at 17:38, adricustodio wrote:

> Well... now its appearing the following when i try to access...
> 
> Internal Error: Missing Template /etc/squid/splash.html
> 
> Ideas ?

Well, sorry to be a bit obvious about this, but did you create that file when 
you added this line to squid.conf:

deny_info 511:/etc/squid/splash.html existing_users

After all, that line says that users who are denied access should see the 
contents of /etc/squid/splash.html - did you create that?


Regards,


Antony.

-- 
"It would appear we have reached the limits of what it is possible to achieve 
with computer technology, although one should be careful with such statements; 
they tend to sound pretty silly in five years."

 - John von Neumann (1949)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From adricustodio at uol.com.br  Mon Aug 31 17:02:01 2015
From: adricustodio at uol.com.br (adricustodio)
Date: Mon, 31 Aug 2015 10:02:01 -0700 (PDT)
Subject: [squid-users] Splash page ?
In-Reply-To: <201508311753.29405.Antony.Stone@squid.open.source.it>
References: <1441037768835-4672984.post@n4.nabble.com>
 <1441039123583-4672985.post@n4.nabble.com>
 <201508311753.29405.Antony.Stone@squid.open.source.it>
Message-ID: <1441040521742-4672987.post@n4.nabble.com>

yes.

I found the error just now...
I saw here on this forum the answer.

I need to add the splash.html on the template folder
(/usr/share/squid/errors/template)
And on the squid.conf file just did deny_info 511:splash.html existing_users
Now it worked!

=D




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Splash-page-tp4672984p4672987.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Aug 31 17:52:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2015 05:52:28 +1200
Subject: [squid-users] refresh_pattern and same objects
In-Reply-To: <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1390026260.83651733.1441036903296.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55E4945C.8030209@treenet.co.nz>

On 1/09/2015 4:01 a.m., FredB wrote:
> 
>>
>> I'm thinking about something like this
>>
>>
> 
> 
> Sorry wrong move :)
> 
> So, What I meant was 
> 
> I'm thinking about something like this
> 
> # HTTP 1/1
> # The refresh_pattern rules applied only to responses without an explicit expiration time
> # min 1440 minutes
> # Max 10080 minutes
> 
> # http 10080 / 60 / 24 H = 7 days
> refresh_pattern ^http 1440 10% 10080

That will match almost everything and the rules below will remain unused.

> 
> # All Files 30 days max
> refresh_pattern -i \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt)       43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar)  43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll)         43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob) 43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
> refresh_pattern -i \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv) 43200 99% 43200 ignore-no-cache ignore-no-store reload-into-ims store-stale
> 
> Maybe my values are too high ? 

I dont think so. The 99% is probably too low if anything. Try 1000%. It
will be capped by the min/max values anyway.


The problems I see there are more related to the overrides and the file
types listed;

* ignore-no-cache does absolutely nothing since 3.2. So remove it.

* ignore-no-store is dangerous. Particularly when used like this for
blanket "file extension" patterns.

The cases I have personally seen that you might run into serious trouble
with are .tiff files, TFF is a "high quality" format. At least its very
high in detail, and I've seen it used with only no-store protection to
send medical, mapping and hi-res photographic data around by software
where it is supposed to be one-use transmission. Caching that would be
both legally risky, and sometimes just waste space (app dependent).
 And with the .wm* formats, which are pretty much awash with DRM
encryption, maybe others in that set too. By caching them all you would
do is prevent users from being able to view the media.

The patterns will not match only file extensions. Any domain name that
starts with the character sequence will also match query-string segments
listing file types etc.


> 
> About reload-into-ims if I understand right it translates "Pragma: no-cache" into "If-Modified-Since" if the object is cached and if there is no explicit expiration time, in real life this should be dangerous or annoying for users ?
> 

Yes, relative to the fact its Cache-Contro: no-cache and/or max-age=0
from the client. Rather than Pragma.

> The context is many simultaneous users (thousands) with very different kind of profiles

I would use the above patterns but without the extra options, then add
them in one by one to see how the behaviour changes. Starting with the
ones on the right hand-side.

Trying to avoid override-no-store as long as possible, and target it to
problem sites when it is used.

And after placing this at the end of the patterns:

  (\?.*)?$


Amos



From Jason_Haar at trimble.com  Mon Aug 31 20:21:38 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 1 Sep 2015 08:21:38 +1200
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
Message-ID: <55E4B752.4040804@trimble.com>

On 01/09/15 02:59, Shane King wrote:
> Accessing via the browser may work but the sync clients that sit in
> the system tray use certificate pinning I believe. So if certificate
> pinning is being used, ssl bumping will not work. You will see an
> alert message in the pcap followed by a connection termination.

This stopped working for me last week - I suspect there was an update or
something

Really frustrating: one of the primary reasons I want to do TLS
intercept is to AV all the viruses published on dropbox!!!

If the Cloud providers go full pinning, the future of TLS Intercept is bleak


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/d40626a8/attachment.htm>

From yvoinov at gmail.com  Mon Aug 31 20:24:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 02:24:42 +0600
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <55E4B752.4040804@trimble.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com>
Message-ID: <55E4B80A.6070006@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha. And future of caching software too. With total HTTPS migration.

01.09.15 2:21, Jason Haar ?????:
> On 01/09/15 02:59, Shane King wrote:
>> Accessing via the browser may work but the sync clients that sit in
>> the system tray use certificate pinning I believe. So if certificate
>> pinning is being used, ssl bumping will not work. You will see an
>> alert message in the pcap followed by a connection termination.
>
> This stopped working for me last week - I suspect there was an update or
> something
>
> Really frustrating: one of the primary reasons I want to do TLS
> intercept is to AV all the viruses published on dropbox!!!
>
> If the Cloud providers go full pinning, the future of TLS Intercept is
bleak
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5LgKAAoJENNXIZxhPexGixMIAKbDAOmalxc2fF5tN/hJfodl
hsm3rrRTIZNIMqk2q91zAEMjgiD2GVebyfYDrrkYwsqe1s+062YUHVB483KnAWcS
3W4guzGzDc1t3E5xrzmryFyVyLtAp8FFMZfoZqckN/OQxk23MbY9LQHSennO9Aw/
xeuZtXizYaK33um1TB96O4YKaWr2e2NG1jPnaeHGmO/SUu01lnzXbgRwvTWfHbl6
3A04aNbCpFUlpeS+mI62sfKfX0r7bO8DqKZPIywU2xmwbBim3lzpxzv0UD+qeLAw
JFmREBkos4iQuggGbJsCtVHYp8oo/jJTM7ylL6IN+bSgoHozUVbJ4ne3Kl5v75Q=
=Wded
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/b60db177/attachment.htm>

From yvoinov at gmail.com  Mon Aug 31 20:28:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 02:28:40 +0600
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <55E4B752.4040804@trimble.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com>
Message-ID: <55E4B8F8.6060007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
But everything will very secure, is it? :)))))))))

01.09.15 2:21, Jason Haar ?????:
> On 01/09/15 02:59, Shane King wrote:
>> Accessing via the browser may work but the sync clients that sit in
>> the system tray use certificate pinning I believe. So if certificate
>> pinning is being used, ssl bumping will not work. You will see an
>> alert message in the pcap followed by a connection termination.
>
> This stopped working for me last week - I suspect there was an update or
> something
>
> Really frustrating: one of the primary reasons I want to do TLS
> intercept is to AV all the viruses published on dropbox!!!
>
> If the Cloud providers go full pinning, the future of TLS Intercept is
bleak
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5Lj4AAoJENNXIZxhPexGE9sH/1CijpL52XT3m3OE91JdoLVI
A4/Ah92tuYa1Nq/vF9b/dIyrDT4J4AuA3Cj3xYPcFKTNFIWdsqdEDXiUi3nrWK9u
w+veH1wnwE/eu77EhH/eamJxPLrUr/AN6UtRMFYW+3Pq1SGsLtBFkjXUJuVPTOh9
eryezIE0VQ6SJ6pdxUdjbUg3vhNbgB8QIxFsAkrjFNu+sD1b3zciBfrdu3tqd/ZM
Y4v/0B9vJVbMULYS5xg/t+BHGtDYTY7TCCjbkpq6hdZ0blJCcOQwqnJ05Vbo97vi
f/lsb51TtWS6rZSCQnLKaqDtVt16GXwoQRltxt1bfNZspomBkiIUNKeKjtTS7fs=
=cLnJ
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/159ae1f4/attachment.htm>

From yvoinov at gmail.com  Mon Aug 31 20:29:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 02:29:32 +0600
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <55E4B752.4040804@trimble.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com>
Message-ID: <55E4B92C.6020501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
BTW, GoogleDrive web application still works with bump. Use it, Luke ;)

01.09.15 2:21, Jason Haar ?????:
> On 01/09/15 02:59, Shane King wrote:
>> Accessing via the browser may work but the sync clients that sit in
>> the system tray use certificate pinning I believe. So if certificate
>> pinning is being used, ssl bumping will not work. You will see an
>> alert message in the pcap followed by a connection termination.
>
> This stopped working for me last week - I suspect there was an update or
> something
>
> Really frustrating: one of the primary reasons I want to do TLS
> intercept is to AV all the viruses published on dropbox!!!
>
> If the Cloud providers go full pinning, the future of TLS Intercept is
bleak
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5LkrAAoJENNXIZxhPexGH9oH/AyK089Jek7yb/YPB16jAKPJ
LnKgKPQ4r8lu3wm5o4JuOXF6mun79fGVW9dymB5rasTJlHiCHrvXEK4G2KqyRg3B
57TdvHuLhHr+IE0jcpMpk6n/pbdHzYJwkbplTd9HNApw+/LJpfxXVzQZsspJJC58
e12pMXL+i5Dv2vEYLEeySVnDN0mtuBdxD7lxDWFDFDbfBZvoGHEptOQYR3lelEet
xEIds+sNYrjYPK8a9BuiKSK0IqQ5mxhsbUIg4Z7LxyKv3+sTV+aW3HMdKkMoc5t8
bPCHec1eIxU7p9lgyKGn2HXtV1WQ5MAeOuI9YHGqdeSfgCPfT1wYF2imiHC9ez8=
=2wPb
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150901/ef0b2f3d/attachment.htm>

From yvoinov at gmail.com  Mon Aug 31 20:52:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 1 Sep 2015 02:52:34 +0600
Subject: [squid-users] wiki.squid-cache.org is broken
Message-ID: <55E4BE92.5070100@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I see this one?

1441054231.642  21243 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
http://wiki.squid-cache.org/wiki/squidtheme/js/kutils.js -
HIER_DIRECT/2001:4b78:2003::1 -
1441054231.642  21245 127.0.0.1 TCP_SWAPFAIL_MISS_ABORTED/000 0 GET
http://wiki.squid-cache.org/wiki/squidtheme/css/screen.css -
HIER_DIRECT/2001:4b78:2003::1 -
1441054231.642  21240 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
http://wiki.squid-cache.org/wiki/common/js/common.js -
HIER_DIRECT/2001:4b78:2003::1 -
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJV5L6QAAoJENNXIZxhPexGnMcIAITyKGx0jTpTtvNuQ47coCsw
SHA77muHxcYXdgDteDigSk/F8rTIMbPx156EeVbRaWuRp8kpFkdrFuzydHmu9SG5
2jcFMj3zVIcvlJBIGBd4rqgONvW5DgxnRvWkoD6nMxu5yW4rGMAZCPDTFIWTTRQd
jj+HUCmTeBaY2HibEPGlSY8HW/GHBtE+P0Ww4+53FXWfdiC9UnytDc3pI7s49bxS
OkJd+zQ1xaZurLOF7f56A5dXNcQrpcKgvXtDn5Sp6i1o7c+kvBGSdtav1UtyAHLG
/3F2Ax/x5jN15phRhi9tSvTNK3QxsqjQQi16csB+rUmYjIe7iQ5gok6qsrVYM9M=
=3OzJ
-----END PGP SIGNATURE-----



From stan.prescott at gmail.com  Mon Aug 31 22:54:28 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Mon, 31 Aug 2015 17:54:28 -0500
Subject: [squid-users] Dropbox and GoogleDrive apps won't connect with
 SSLBump enabled
In-Reply-To: <55E4B92C.6020501@gmail.com>
References: <7t20l68jwlvywwg5ho8xnoi6.1441033182933@email.android.com>
 <55E4B752.4040804@trimble.com> <55E4B92C.6020501@gmail.com>
Message-ID: <CANLNtGSet4OZALRkygjTZhQiPsYV4o3tkEHDtHDyGw2_+QM7Sg@mail.gmail.com>

Yes, SSLBump still works with the web apps, but it would be a lot more
convenient if the mobile apps would also work.

Does anyone know how to pin Squid's self-signed certificate's public key to
Googledrive and Dropbox so that it would work with SSLBump enabled?

Stan

On Mon, Aug 31, 2015 at 3:29 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> BTW, GoogleDrive web application still works with bump. Use it, Luke ;)
>
> 01.09.15 2:21, Jason Haar ?????:
> > On 01/09/15 02:59, Shane King wrote:
> >> Accessing via the browser may work but the sync clients that sit in
> >> the system tray use certificate pinning I believe. So if certificate
> >> pinning is being used, ssl bumping will not work. You will see an
> >> alert message in the pcap followed by a connection termination.
> >
> > This stopped working for me last week - I suspect there was an update or
> > something
> >
> > Really frustrating: one of the primary reasons I want to do TLS
> > intercept is to AV all the viruses published on dropbox!!!
> >
> > If the Cloud providers go full pinning, the future of TLS Intercept is
> bleak
> >
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV5LkrAAoJENNXIZxhPexGH9oH/AyK089Jek7yb/YPB16jAKPJ
> LnKgKPQ4r8lu3wm5o4JuOXF6mun79fGVW9dymB5rasTJlHiCHrvXEK4G2KqyRg3B
> 57TdvHuLhHr+IE0jcpMpk6n/pbdHzYJwkbplTd9HNApw+/LJpfxXVzQZsspJJC58
> e12pMXL+i5Dv2vEYLEeySVnDN0mtuBdxD7lxDWFDFDbfBZvoGHEptOQYR3lelEet
> xEIds+sNYrjYPK8a9BuiKSK0IqQ5mxhsbUIg4Z7LxyKv3+sTV+aW3HMdKkMoc5t8
> bPCHec1eIxU7p9lgyKGn2HXtV1WQ5MAeOuI9YHGqdeSfgCPfT1wYF2imiHC9ez8=
> =2wPb
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150831/dc835565/attachment.htm>

From eliezer at ngtech.co.il  Mon Aug 31 23:22:08 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 1 Sep 2015 02:22:08 +0300
Subject: [squid-users] wiki.squid-cache.org is broken
In-Reply-To: <55E4BE92.5070100@gmail.com>
References: <55E4BE92.5070100@gmail.com>
Message-ID: <55E4E1A0.4050201@ngtech.co.il>

Works for me:
#curl -Iv wiki.squid-cache.org
* Rebuilt URL to: wiki.squid-cache.org/
* Hostname was NOT found in DNS cache
*   Trying 2001:4b78:2003::1...
* Connected to wiki.squid-cache.org (2001:4b78:2003::1) port 80 (#0)
 > HEAD / HTTP/1.1
 > User-Agent: curl/7.35.0
 > Host: wiki.squid-cache.org
 > Accept: */*
 >
< HTTP/1.1 200 OK
HTTP/1.1 200 OK
< Date: Mon, 31 Aug 2015 23:21:00 GMT
Date: Mon, 31 Aug 2015 23:21:00 GMT
* Server Apache/2.4.10 (Debian) is not blacklisted
< Server: Apache/2.4.10 (Debian)
Server: Apache/2.4.10 (Debian)
< Vary: Cookie,User-Agent
Vary: Cookie,User-Agent
< Content-Length: 16273
Content-Length: 16273
< Cache-Control: max-age=3600
Cache-Control: max-age=3600
< Expires: Tue, 01 Sep 2015 00:21:00 GMT
Expires: Tue, 01 Sep 2015 00:21:00 GMT
< Content-Type: text/html; charset=utf-8
Content-Type: text/html; charset=utf-8

<
* Connection #0 to host wiki.squid-cache.org left intact

But from an ABORT it seems like a client side issue.. Chrome?

Eliezer

On 31/08/2015 23:52, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> I see this one?
>
> 1441054231.642  21243 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/js/kutils.js -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21245 127.0.0.1 TCP_SWAPFAIL_MISS_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/squidtheme/css/screen.css -
> HIER_DIRECT/2001:4b78:2003::1 -
> 1441054231.642  21240 127.0.0.1 TCP_HIT_ABORTED/000 0 GET
> http://wiki.squid-cache.org/wiki/common/js/common.js -
> HIER_DIRECT/2001:4b78:2003::1 -
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJV5L6QAAoJENNXIZxhPexGnMcIAITyKGx0jTpTtvNuQ47coCsw
> SHA77muHxcYXdgDteDigSk/F8rTIMbPx156EeVbRaWuRp8kpFkdrFuzydHmu9SG5
> 2jcFMj3zVIcvlJBIGBd4rqgONvW5DgxnRvWkoD6nMxu5yW4rGMAZCPDTFIWTTRQd
> jj+HUCmTeBaY2HibEPGlSY8HW/GHBtE+P0Ww4+53FXWfdiC9UnytDc3pI7s49bxS
> OkJd+zQ1xaZurLOF7f56A5dXNcQrpcKgvXtDn5Sp6i1o7c+kvBGSdtav1UtyAHLG
> /3F2Ax/x5jN15phRhi9tSvTNK3QxsqjQQi16csB+rUmYjIe7iQ5gok6qsrVYM9M=
> =3OzJ
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



