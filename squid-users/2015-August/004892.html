<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] squid centos and osq_lock
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3C55BCB163.2050603%40urlfilterdb.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="004891.html">
   <LINK REL="Next"  HREF="004893.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] squid centos and osq_lock</H1>
    <B>Marcus Kool</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3C55BCB163.2050603%40urlfilterdb.com%3E"
       TITLE="[squid-users] squid centos and osq_lock">marcus.kool at urlfilterdb.com
       </A><BR>
    <I>Sat Aug  1 11:45:39 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="004891.html">[squid-users] SSL connction failed due to SNI after content redirection
</A></li>
        <LI>Next message (by thread): <A HREF="004893.html">[squid-users] block inappropriate images of google
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4892">[ date ]</a>
              <a href="thread.html#4892">[ thread ]</a>
              <a href="subject.html#4892">[ subject ]</a>
              <a href="author.html#4892">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>

On 07/31/2015 03:56 PM, Amos Jeffries wrote:
&gt;<i> On 1/08/2015 4:06 a.m., Josip Makarevic wrote:
</I>&gt;&gt;<i> Marcus, tnx for your info.
</I>&gt;&gt;<i> OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
</I>&gt;&gt;<i> Yes, cpu_affinity_map is good and with 6 instances there is load only on
</I>&gt;&gt;<i> first 6 cores and the server is 12 core, 24 HT
</I>&gt;<i>
</I>&gt;<i> Then I suspect that mutex and locking will be the kernel scheduling work
</I>&gt;<i> on the HT cores.
</I>&gt;<i>   In high performance Squid will max out a physical cores worth of
</I>&gt;<i> cycles. HT essentially tries to over-clock physical cores. But trying to
</I>&gt;<i> reach 200% capacity into a physical core with Squid workloads only leads
</I>&gt;<i> to trouble.
</I>&gt;<i>   It is far better to tie Squid with affinity to one instance per
</I>&gt;<i> physical core and let the extra HT capacity be available to the OS and
</I>&gt;<i> other supporting things the Squid instance needs to have happen externally.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
</I>&gt;&gt;<i> and so on so that should not be the problem.
</I>&gt;&gt;<i> I've tried with 12 workers but that's even worse.
</I>&gt;<i>
</I>&gt;<i> You do need to be very careful about which core numbers are the HT core
</I>&gt;<i> vs the physical core ID. Last time I saw anyone doing it, every second
</I>&gt;<i> number was a real physical core ID. YMMV.
</I>
There are 2 mappings and I have seen them both but I do not recall which I saw where.
You can do the following to find out which CPU# is a sibling (HT core):
cd /sys/devices/system/cpu
for cpu in cpu[0-9]* ; do
    cat $cpu/topology/thread_siblings_list
done

&gt;&gt;<i> Let me try to explain:
</I>&gt;&gt;<i> on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
</I>&gt;&gt;<i> in that case, actual user time is about 10-20% and 70-80% is sys time
</I>&gt;&gt;<i> (osq_lock) and there are no connection timeouts.
</I>
The CPU time in osq_lock is not easy to explain but it is not likely caused by Squid itself.
Googling about osq_lock led me to a kernel patch discussion where 500 dd processes on ext4/multipath or a file system repair with 125 threads caused the system to use 70+% CPU in osq_lock.
The general believe was that a lot of outstanding IO caused it.
This brings me to these questions:
- what is your testing method ?
- are there simply too many concurrent connections per instance of Squid ?
- are the bonded 10G interfaces supported by CentOS 6 ?
- can you test with unbonded ethernet? (the bonding driver code uses 2 locks)

You may or may not get better results with CentOS 7 or the custom kernel (try latest or before 3.12 since some issues started with 3.12).

&gt;&gt;<i> If I switch to SMP 6 workers user time goes up but sys time goes up too and
</I>&gt;&gt;<i> there are connection timeouts and the load jumps to ~12.
</I>&gt;&gt;<i> If I give it more workers only load jumps and more connections are being
</I>&gt;&gt;<i> dropped to the point that load goes to 23/24 and the entire server is slow
</I>&gt;&gt;<i> as hell.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> So, best performance so far are with 6 non-smp workers.
</I>
'workers' is a term used by Squid SMP.
To have less confusion, in a non-SMP Squid config, I suggest to use the term 'instance'.

Marcus

&gt;&gt;<i> For now I have 2 options:
</I>&gt;&gt;<i> 1. Install older squid (3.1.10 centos repo) and try it then
</I>&gt;&gt;<i> 2. build custom 64bit kernel with RCU and specific cpu family support (in
</I>&gt;&gt;<i> progress).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The end idea is to be able to sustain 1gig of traffic on this server :)
</I>&gt;&gt;<i> Any advice is welcome
</I>&gt;<i>
</I>&gt;<i> I agree with Marcus then. The non-SMP then is the way to go at present.
</I>&gt;<i> The main benefit of SMP support in current Squid is for caching
</I>&gt;<i> de-duplication (ie rock store).
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Also some things to note:
</I>&gt;<i>
</I>&gt;<i> * a good percentage of the speed of Squid is the 20-40% caching HIT rate
</I>&gt;<i> normal HTTP traffic has. Albeit memory-only caching on highest
</I>&gt;<i> performance boxen. Memory hits are 4-6 orders of magnitude faster than
</I>&gt;<i> network fetches. This has little to do with anything you can control
</I>&gt;<i> (normally). The (relatively) slow speed of origin servers creating the
</I>&gt;<i> content is the bottleneck. Even &quot;static&quot; content may be encoded to the
</I>&gt;<i> clients requested desire on each fetch, which takes time.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> * Going by out lab tests and real-world results so far I rate Squid
</I>&gt;<i> per-worker at ~50Mbps on 3.1GHz core, and ~70Mbps on 3.7GHz. Your 12
</I>&gt;<i> cores will only get you up around 800 Mbits IMHO (thats after tuning). I
</I>&gt;<i> would gladly be proven wrong though :-)
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> * Squid effectively *polls* all the listening ports every 10ms or once
</I>&gt;<i> every 10 I/O events (whichever is faster). So running with 1024
</I>&gt;<i> listening ports is a bit counter-productive, more time could be spent
</I>&gt;<i> checking those ports than doing work.
</I>&gt;<i>   That said going from one to multiple listening ports does make a speed
</I>&gt;<i> improvement. Finding the sweet spot between those trends is something
</I>&gt;<i> else to tune for.
</I>&gt;<i>   &lt;<A HREF="http://wiki.squid-cache.org/MultipleInstances#Tips">http://wiki.squid-cache.org/MultipleInstances#Tips</A>&gt;
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> 2015-07-31 14:53 GMT+02:00 Marcus Kool:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> osq_lock is used in the kenel for the implementation of a mutex.
</I>&gt;&gt;&gt;<i> It is not clear which mutex so we can only guess.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Which version of the kernel and distro do you use?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
</I>&gt;&gt;&gt;<i> non-SMP.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> What is the value of cpu_affinity_map in all config files?
</I>&gt;&gt;&gt;<i> You say they are static. But do you allocate each instance on a different
</I>&gt;&gt;&gt;<i> core?
</I>&gt;&gt;&gt;<i> Does 'top' show that all CPUs are used?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Do you have 24 cores or 12 hyperthreaded cores?
</I>&gt;&gt;&gt;<i> In case you have 12 real cores, you might want to experiment with 12
</I>&gt;&gt;&gt;<i> instances of Squid and then try to upscale.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Make maximum_object_size large, a max size of 16K will prohibit the
</I>&gt;&gt;&gt;<i> retrieval of objects larger than 16K.
</I>&gt;&gt;&gt;<i> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
</I>&gt;&gt;&gt;<i> infinite and do not worry since
</I>&gt;&gt;&gt;<i> cache_mem is zero.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Marcus
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Hi Amos,
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>    cache_mem 0
</I>&gt;&gt;&gt;&gt;<i>    cache deny all
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> already there.
</I>&gt;&gt;&gt;&gt;<i> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
</I>&gt;&gt;&gt;&gt;<i> interface.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Well, entire config would be way too long but here is the static part:
</I>&gt;&gt;&gt;&gt;<i> via off
</I>&gt;&gt;&gt;&gt;<i> cpu_affinity_map process_numbers=1 cores=2
</I>&gt;&gt;&gt;&gt;<i> forwarded_for delete
</I>&gt;&gt;&gt;&gt;<i> visible_hostname squid1
</I>&gt;&gt;&gt;&gt;<i> pid_filename /var/run/squid1.pid
</I>&gt;<i>
</I>&gt;<i> Remove these...
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> icp_port 0
</I>&gt;&gt;&gt;&gt;<i> htcp_port 0
</I>&gt;&gt;&gt;&gt;<i> icp_access deny all
</I>&gt;&gt;&gt;&gt;<i> htcp_access deny all
</I>&gt;&gt;&gt;&gt;<i> snmp_port 0
</I>&gt;&gt;&gt;&gt;<i> snmp_access deny all
</I>&gt;<i>
</I>&gt;<i> ... to here. They do nothing but slow Squid-3 down.
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> dns_nameservers x.x.x.x
</I>&gt;&gt;&gt;&gt;<i> cache_mem 0
</I>&gt;&gt;&gt;&gt;<i> cache deny all
</I>&gt;&gt;&gt;&gt;<i> pipeline_prefetch on
</I>&gt;<i>
</I>&gt;<i> In Squid-3.4 and later this is set to the length of pipeline you want to
</I>&gt;<i> accept.
</I>&gt;<i>
</I>&gt;<i> NP: 'on' traditionally has meant pipeline length of 1 (two parallel
</I>&gt;<i> requests). Longer lengths are not yet well tested but generally it seems
</I>&gt;<i> to work okay.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> memory_pools off
</I>&gt;&gt;&gt;&gt;<i> maximum_object_size 16 KB
</I>&gt;&gt;&gt;&gt;<i> maximum_object_size_in_memory 16 KB
</I>&gt;<i>
</I>&gt;<i> Like Marcus said. Without even memory caching these two have no useful
</I>&gt;<i> effects.
</I>&gt;<i>
</I>&gt;<i> There is one related setting &quot;read_ahead_gap&quot; which affects performance
</I>&gt;<i> by tuning the amount of undelivered object data Squid will buffer in
</I>&gt;<i> transient memory. Higher value for that mean faster servers can finish
</I>&gt;<i> sending earlier and resources for them released for other uses.
</I>&gt;<i>   Tuning this is a fine art since it modulates how much Squid internal
</I>&gt;<i> buffers (and pipieline prefetching) read off TCP buffers. And all of
</I>&gt;<i> those buffers have limits of their own and may contain multiple requests
</I>&gt;<i> data.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> ipcache_size 0
</I>&gt;<i>
</I>&gt;<i> Remove this. Without IP cache Squid will be forced to do about 4x remote
</I>&gt;<i> DNS lookup for every single HTTP request - *minimum*. Maybe more if you
</I>&gt;<i> apply any access controls to the traffic.
</I>&gt;<i>   If anything increase the ipcache size to store more results.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> cache_store_log none
</I>&gt;<i>
</I>&gt;<i> Not needed in Squid-3. You can remove.
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> half_closed_clients off
</I>&gt;&gt;&gt;&gt;<i> include /etc/squid/rules
</I>&gt;&gt;&gt;&gt;<i> access_log /var/log/squid/squid1-access.log
</I>&gt;<i>
</I>&gt;<i> Logging I/O slows Squid down. I suggest making that a daemon, TCP or UDP
</I>&gt;<i> log output.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> cache_log /var/log/squid/squid1-cache.log
</I>&gt;&gt;&gt;&gt;<i> coredump_dir /var/spool/squid/squid1
</I>&gt;&gt;&gt;&gt;<i> refresh_pattern ^ftp:           1440    20%     10080
</I>&gt;&gt;&gt;&gt;<i> refresh_pattern ^gopher:        1440    0%      1440
</I>&gt;&gt;&gt;&gt;<i> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
</I>&gt;&gt;&gt;&gt;<i> refresh_pattern .               0       20%     4320
</I>&gt;<i>
</I>&gt;<i> Without caching you can remove these *entirely*.
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> acl port0 myport 30000
</I>&gt;<i>
</I>&gt;<i> Mumble. Less reliable than myportname, but it is infintessimally faster
</I>&gt;<i> when it does work at all.
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> http_access allow testhost
</I>&gt;&gt;&gt;&gt;<i> tcp_outgoing_address x.x.x.x port0
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> include is there for basic ACL - safe ports and so on - to minimize
</I>&gt;&gt;&gt;&gt;<i> config file footprint since it's static and same for every worker.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> and so on 44 more times in this config file
</I>&gt;<i>
</I>&gt;<i> Only put allow testhost once. Every time you test ACLs Squid slows down.
</I>&gt;<i>
</I>&gt;<i> Some ACLs are worse drag than others. You can probably optimize even the
</I>&gt;<i> default recommended security settings you shuffled into &quot;rules&quot; file to
</I>&gt;<i> operate better.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Do you know of any good article hot to tune kernel locking or have any
</I>&gt;&gt;&gt;&gt;<i> idea why is it happening?
</I>&gt;&gt;&gt;&gt;<i> I cannot find any good info on it and all I've found are bits and peaces
</I>&gt;&gt;&gt;&gt;<i> of kernel source code.
</I>&gt;<i>
</I>&gt;<i> Sorry no. All I found was the same.
</I>&gt;<i>
</I>&gt;<i> Though I do know that one of the big differences between Linux 2.6 and
</I>&gt;<i> 3.0 was the removal of the &quot;Big Kernel Lock&quot; system that allowed Linux
</I>&gt;<i> to run on multi-core systems properly. It could be CentOS 6 itelf biting
</I>&gt;<i> you with its ancient kernel version.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Amos
</I>&gt;<i> _______________________________________________
</I>&gt;<i> squid-users mailing list
</I>&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;<i> <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>&gt;<i>
</I>
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="004891.html">[squid-users] SSL connction failed due to SNI after content redirection
</A></li>
	<LI>Next message (by thread): <A HREF="004893.html">[squid-users] block inappropriate images of google
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4892">[ date ]</a>
              <a href="thread.html#4892">[ thread ]</a>
              <a href="subject.html#4892">[ subject ]</a>
              <a href="author.html#4892">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
