<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] CVE-2009-0801
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20CVE-2009-0801&In-Reply-To=%3C5674970F.5060600%40ntcomputer.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="008392.html">
   <LINK REL="Next"  HREF="008395.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] CVE-2009-0801</H1>
    <B>dc</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20CVE-2009-0801&In-Reply-To=%3C5674970F.5060600%40ntcomputer.de%3E"
       TITLE="[squid-users] CVE-2009-0801">dc.sqml at ntcomputer.de
       </A><BR>
    <I>Fri Dec 18 23:30:23 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="008392.html">[squid-users] CVE-2009-0801
</A></li>
        <LI>Next message (by thread): <A HREF="008395.html">[squid-users] CVE-2009-0801
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#8394">[ date ]</a>
              <a href="thread.html#8394">[ thread ]</a>
              <a href="subject.html#8394">[ subject ]</a>
              <a href="author.html#8394">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Thank you very much for this detailed explanation!

I have a setup where squid doesn't know about the original destination
IP address, so I tried to enforce using DNS responses as destination
addresses for any request, without success. Looking at the relevant code
I found the limitation (and CVE) to be quite obscure, but now I know why
it's there.
Since the vulnerability can't be exploited in my case anyway, I have
altered the code to allow the replacement of the destination address
regardless of a mismatch of the original dst.

Best regards
Nikolaus

Am 18.12.2015 um 23:14 schrieb Amos Jeffries:
&gt;<i>
</I>&gt;<i> The problem(s):
</I>&gt;<i>
</I>&gt;<i> With CVE-2009-0801 the ORIGINAL_DST signals arriving at Squid look like
</I>&gt;<i> TCP IP:port for some ServerA.example.net domain. But the HTTP message
</I>&gt;<i> contains something different like:
</I>&gt;<i>
</I>&gt;<i>  GET /logo.jpg HTTP/1.1
</I>&gt;<i>  Host: attacker.example.com
</I>&gt;<i>
</I>&gt;<i> The browser Same-Origin and sandbox Origin security protections ensure
</I>&gt;<i> that sandboxed scripts can only open TCP connections to the same origin
</I>&gt;<i> server(s) they are scoped for. But scripts can send any header values
</I>&gt;<i> they like, including fake Host on that connection once it is open.
</I>&gt;<i>
</I>&gt;<i> If Squid were to use the Host header to route the message in any
</I>&gt;<i> situation where the ORIGINAL_DST details do not match the Host DNS
</I>&gt;<i> records. Then Squid would be fetching and delivering some content into a
</I>&gt;<i> browser sandbox from a server that sandbox did not permit.
</I>&gt;<i>
</I>&gt;<i> The fix for that is to simply fetch form the ORIGINAL_DST server
</I>&gt;<i> IP:port. Acting as if the proxy were not there.
</I>&gt;<i>
</I>&gt;<i> BUT ... the proxy actually is there, so the cache has to be accounted
</I>&gt;<i> for. That stores things by URL. This causes Vuln #2 below if we use the
</I>&gt;<i> Host value as domain name like it is supposed to be. And if we don't the
</I>&gt;<i> proxy outgoing Host header is mandatory to re-write to the URL host
</I>&gt;<i> portion. Meaning the outbound traffic would have raw-IP:port for the
</I>&gt;<i> domain name.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Vuln #2:  the attacker script can cause hijacking of popular content
</I>&gt;<i> URLs simply by fetching the above request from its own malicious server
</I>&gt;<i> with Host:google.com.
</I>&gt;<i> [no CVE for this bit since no published Squid was ever vulnerable].
</I>&gt;<i>
</I>&gt;<i> This is not just bypassing the its own sandbox protection, but causing
</I>&gt;<i> its attack payload to be delivered in future to another sandbox (on its
</I>&gt;<i> own machine OR any other machine using the proxy) in the followup proxy
</I>&gt;<i> cache HITs. That payload has escaped its own origin sandbox and now runs
</I>&gt;<i> with whatever permissions and data access the victims domain normally
</I>&gt;<i> has access to (plus the ability to jump again buy hijacking any of that
</I>&gt;<i> sandboxes valid servers/URLs).
</I>&gt;<i>
</I>&gt;<i> The fix we chose to use is not to cache anything where the Host vs DNS
</I>&gt;<i> is not matching.
</I>&gt;<i>
</I>&gt;<i> BUT ... PAIN ... it turns out rather a *lot* of content have been using
</I>&gt;<i> systems where the Host and origin server DNS do not match all the time.
</I>&gt;<i> Everything using Akamai CDN, Google web hosting, Geo-based DNS services,
</I>&gt;<i> so called &quot;client-based HTTP routing&quot; as done by some popular AV
</I>&gt;<i> vendors, and many other smaller sites for odd reasons. Or at least they
</I>&gt;<i> dont match when the DNS is looked up through two different DNS routing
</I>&gt;<i> paths.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> The alternative would be to use raw-IP port on the URL and outgoing Host
</I>&gt;<i> value. That latter is mandatory but breaks Virtual Hosting on these
</I>&gt;<i> messages. Given the particular providers whose actions cause the pain;
</I>&gt;<i> breaking virtual hosting would be far worse than not caching. The common
</I>&gt;<i> cases are anyway low-HIT content or major providers with high speed
</I>&gt;<i> networks (very low MISS latency).
</I>&gt;<i>
</I>&gt;<i> So the fix we use is to verify the DNS and Host details match. Only
</I>&gt;<i> allowing cache storage and/or Host message routing when they do.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> This still leaves us with pain in all those situations where non-match
</I>&gt;<i> happens. There are no fixes for that, just workarounds to iprove the
</I>&gt;<i> chances of matching.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> PS. The two vulnerabilities have been known about since at least 1990's.
</I>&gt;<i> The original choice was to leave the CVE-2009-0801 behaviour happening
</I>&gt;<i> to improve caching (and same-origin, sandbox etc did not exist back
</I>&gt;<i> then). But nowdays the browser protections do exist, and in 2009 active
</I>&gt;<i> malware was found using the proxy behaviour to escape it. So the CVE got
</I>&gt;<i> allocated and we had to fix. Opening the second vulnerability was never
</I>&gt;<i> an option, so we are where we are now.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> PPS. ideas or attempts at resolving the HIT side effect is welcome. Just
</I>&gt;<i> be aware that every attempt so far has lead to one or other
</I>&gt;<i> vulnerability being re-opened and neither is acceptible behaviour. So
</I>&gt;<i> having what appears to be a good idea shot down and rejected is normal
</I>&gt;<i> when attacking this problem (it took a good year to get the fix as far
</I>&gt;<i> as it is).
</I>&gt;<i>
</I>&gt;<i> Amos
</I>&gt;<i>
</I>

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="008392.html">[squid-users] CVE-2009-0801
</A></li>
	<LI>Next message (by thread): <A HREF="008395.html">[squid-users] CVE-2009-0801
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#8394">[ date ]</a>
              <a href="thread.html#8394">[ thread ]</a>
              <a href="subject.html#8394">[ subject ]</a>
              <a href="author.html#8394">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
