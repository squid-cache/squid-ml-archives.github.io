From squid3 at treenet.co.nz  Wed Jul  1 02:37:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Jul 2015 14:37:03 +1200
Subject: [squid-users] Transparent Proxy Configuration
In-Reply-To: <SNT146-DS26AC8FBFCF64D2697ECB76DEA90@phx.gbl>
References: <SNT146-DS26AC8FBFCF64D2697ECB76DEA90@phx.gbl>
Message-ID: <5593524F.2060003@treenet.co.nz>

On 1/07/2015 6:21 a.m., Chris Greene wrote:
> I?ve had Squid running on Ubuntu for a few weeks.  I?d configured the
> proxy settings in the browsers.  Everything has been working well and
> I've been pleased with the results.  But now I need to make this a
> transparent proxy and I?m running into trouble & need some help.
> 
> I?ve got a Destination NAT rule set up on my router to forward TCP port
> 80 traffic to my proxy.

There is the problem. The NAT must be done on the Squid box, not the router.

Amos


From squid3 at treenet.co.nz  Wed Jul  1 02:56:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Jul 2015 14:56:46 +1200
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>,
 <55922067.2000401@treenet.co.nz>
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>
Message-ID: <559356EE.2060209@treenet.co.nz>

On 1/07/2015 5:08 a.m., Alex Wu wrote:
> /*
> You could assign two workers, each with a different http_port and
> ssl_crtd helper using different cert databases.
> 
> */
> 
> How to do this? It sounds it might meet our need. 
> 

at the top of squid.conf place:

 workers 2

 if ${process_number} = 1
   http_port 10045 ...
   sslcrtd_program ...

 else
   http_port 10046 ...
   sslcrtd_program ...

 endif

The list of other directives which also need separate per-worker
configuration can be found at
<http://wiki.squid-cache.org/MultipleInstances#Relevant_squid.conf_directives>.


> The reason is that we assign a port for internal, 
> so we can use cheap CA (self-generated CA), for the collaboration, we use a diffrent port, 
> may need to set up a different CA.

That dont make sense to me. There should be no need for internal traffic
to use a different CA from what external has. Costs are already paid to
get the public CA, there is no incremental increase for internal traffic
to use it as well.

You can do simpler things like using a private LAN-specific IP on the
listening http_port for internal traffic and myportname ACL for internal
vs external access controls (that work regardless of whether the request
has been bumped or not).

Amos



From squid3 at treenet.co.nz  Wed Jul  1 03:24:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Jul 2015 15:24:29 +1200
Subject: [squid-users] TFD-CONNECT (501 errors)
In-Reply-To: <CAHObhNUw-_Us0gAxyag-_xB7iTsV3Kb9SL06DY9SVMf2otKUOw@mail.gmail.com>
References: <CAHObhNUw-_Us0gAxyag-_xB7iTsV3Kb9SL06DY9SVMf2otKUOw@mail.gmail.com>
Message-ID: <55935D6D.4090306@treenet.co.nz>

On 1/07/2015 8:52 a.m., Randal Cowen wrote:
> For years I've been successfully running a squid. Last Wednesday the 17th
> magically only HTTPS requests fail over only AT&T's cellular network....
> 
> Everything still works great on any other land-line provider I've tested
> including AT&T's DSL service. Typically my logs show
> 
> 1435691713.787 240084 <Source IP> TCP_TUNNEL/200 381 CONNECT
> www.google.com:443 - HIER_DIRECT/216.239.32.20 -
> 
> but now magically over the AT&T cell network they come in..
> 
> 1435692019.503      0 <Source IP> TAG_NONE/501 4175 TFD-CONNECT
> https://iecvlist.microsoft.com/ - HIER_NONE/- text/html
> 
> Notice the odd "TFD-CONNECT" which I assume is 501 "Not Implemented" along
> with the URL now containing the https:// prefix...

"TFD-CONNECT" is being sent in as the HTTP request method. It is a
custom method unknown to Squid. It will therefore *not* trigger the
CONNECT method handling code, but be treated as a GET/POST request.

Please double-check this with a "debug_options 11,2" trace to see the
client request message. But I assume the URL also came in either to a
https_port or with "https://" already in the URL.

Your Squid can handle those URLs only if it has been built with OpenSSL
support. Otherwise they get rejected as HTTPS protocol is not implemented.


> 
> I'm not finding much on the TFD-CONNECT, what I am finding is leading me to
> believe AT&T has possibly enabled their "Toll Free Data" in my area and is
> messing with my headers/proxy tunnel.
> 
> Has anyone else been experiencing this? Or have any helpful clues?

I'm getting the nasty suspicion that they are decrypting the traffic and
passing it around un-encrypted. But you will need to investigate closer
to know for sure.

> 
> I have even downloaded and recompiled a completely new box for testing with
> the same behavior. The new box is
> 
> Squid Cache: Version 3.5.5-20150624-r13848
> Service Name: squid
> configure options:  '--prefix=/usr' '--includedir=/usr/include'
> '--datadir=/usr/share' '--bindir=/usr/sbin'
> '--libexecdir=/usr/lib64/squid--localstatedir=/var'
> '--sysconfdir=/etc/squid' --enable-ltdl-convenience

Please try adding --with-openssl (may need to install the openssl-dev
dependency). At the very least it will enable your Squid to service the
https:// URLs. No guarantees about what happens after that though.

Amos


From truniger at bluewin.ch  Wed Jul  1 06:43:41 2015
From: truniger at bluewin.ch (truniger at bluewin.ch)
Date: Wed, 1 Jul 2015 06:43:41 +0000 (GMT+00:00)
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>
Message-ID: <21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>

Not much reaction on my statements so far.
So let me ask: is anyone actually receiving a valid 404-http-response from squid 3.5.x when trying to download a file which does not exist on the FTP-server?
I don't and I think it's a bug with 3.5.x and I wonder why I seem to be the only one facing it.

Regards, Othmar

>The problem: when trying to FTP-download a non-existing file the 
>client-http-connection just hangs forever. The external FTP 
>communication works fine but squid misses to send back a http-response with code 404.
>Instead the connection just stays open.
>I have several jobs in production polling for files on external FTP 
>servers which work fine with 3.4.x but cannot be used anymore on 3.5.x
>
>Would be nice if someone could confirm this problem and even better if 
>someone has a valid 3.5.x workaround for this.



From ignazio.raia at eutelia.com  Wed Jul  1 07:30:57 2015
From: ignazio.raia at eutelia.com (Ignazio Raia)
Date: Wed, 01 Jul 2015 09:30:57 +0200
Subject: [squid-users] Personal Request
In-Reply-To: <559272F5.1080102@treenet.co.nz>
References: <1435582226950-4671946.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com>
 <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com>
 <55925BD5.10108@treenet.co.nz>
 <1435654549375-4671966.post@n4.nabble.com>
 <559264F1.6020106@treenet.co.nz>
 <1435657372098-4671970.post@n4.nabble.com>
 <559272F5.1080102@treenet.co.nz>
Message-ID: <20150701073057.5775440.6454.4055@eutelia.com>

?Good morning Amos, 
I would unsuscribe from the mailing list. I have unsuscribed from the forum yet, but i receive mail again.
I found the forum very useful, but at the moment i have a different mission ?in my firm.
With reguards
Ignazio Raia
Invio?eseguito?dallo smartphone?BlackBerry?10.
? Messaggio originale ?
Da: Amos Jeffries
Inviato: marted? 30 giugno 2015 12:46
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] TCP_MISS/504 in cache_peer

On 30/06/2015 9:42 p.m., Stakres wrote:
> Amos,
> Yes, similar case here on the 4223.
> By reading the case 4223, we can see that part "Non-cacheable objects should
> never be added to the digest." from you.
> In my squid, there is no restriction, ICP is fully open, squid server
> (3.5.5) are compiled with the digest option, so all is done to allow
> ICP/digest connection and exchange.
> So, why the servers think they got the objects, especially when they are not
> cacheable and not cached ?

They have *an* object cached for the URL. But not the one the client is
requesting from that same URL. Or its got some revalidation requirements
attached. Thats what Chudy found. As a result the sibling is unable to
supply its cached object as the answer.

> 
> To me, it seems the servers think they have the object but they don't, so
> they reply with a 404 translated by a 504 to the squid client because
> sibling archi.
> I could understand it could be a bug but the squid client should see the 504
> and should request the object from internet, no ?

For example;
the client wants object no more than 60 seconds old with unique label
"ETag:blahblah". But the sibling has a 90 second old object labeled
"ETag:oops".

It would have to contact the origin to get a new copy, which is
forbidden by the sibling relationship Cache-Control:only-if-cached criteria.


It's a common problem when using ICP or cache digests which consider
only the URL to identify objects. HTCP was created to resolve that
problem by considering the whole HTTP header when testing the peer for
objects. Though I'm not sure myself if it would resolve the
only-if-cached issue.


> 
> At the moment, i use a "retry_on_error on" as a workaround but not sure it's
> fixing all 504.
> 

Strange if that is working.

The reforwarding happens on 502 and 504 status without any special
configuration. That directive just adds 403, 500, 501 and 503 to the
list of status that can be retried.


> Then, having a dedicated cable between squid servers is not a realistic
> solution, my ISP will not see that as a serious solution 
> 
> Last point, "no-tproxy rule on parent type cache_peer", it does not work, we
> tried that.
> We applied that option 1 month ago and the internet sees the squid ip, not
> the original ip address, so maybe another bug here... 

Nod. Without the dedicated link between proxies and TPROXY the parent
(or sibling) wont be aware of what IP the original client was using.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From tomtux007 at gmail.com  Wed Jul  1 08:17:21 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Wed, 1 Jul 2015 10:17:21 +0200
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>
 <21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>
Message-ID: <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>

Hi Othmar

The same behaviour here with squid 3.5.5:

# Catching an existing file results in a correct 200:
$ curl -x proxy:3128 -I ftp://mirror.switch.ch/README
HTTP/1.1 200 OK
Server: squid
Mime-Version: 1.0
Date: Wed, 01 Jul 2015 07:58:28 GMT
Content-Type: text/plain
Last-Modified: Wed, 05 Dec 2007 14:13:16 GMT
X-Cache: MISS from proxy
Connection: keep-alive


# Catching an non-existing file hangs for a unknown time
$ curl -x proxy:3128 -I ftp://mirror.switch.ch/README123


I can confirm, that with older squids (< 3.5) catching a
non-existing-file with ftp gives back a correct 404.

Kind regards,
Tom

On Wed, Jul 1, 2015 at 8:43 AM, truniger at bluewin.ch <truniger at bluewin.ch> wrote:
> Not much reaction on my statements so far.
> So let me ask: is anyone actually receiving a valid 404-http-response from squid 3.5.x when trying to download a file which does not exist on the FTP-server?
> I don't and I think it's a bug with 3.5.x and I wonder why I seem to be the only one facing it.
>
> Regards, Othmar
>
>>The problem: when trying to FTP-download a non-existing file the
>>client-http-connection just hangs forever. The external FTP
>>communication works fine but squid misses to send back a http-response with code 404.
>>Instead the connection just stays open.
>>I have several jobs in production polling for files on external FTP
>>servers which work fine with 3.4.x but cannot be used anymore on 3.5.x
>>
>>Would be nice if someone could confirm this problem and even better if
>>someone has a valid 3.5.x workaround for this.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From alexandre.magnat at mecaprotec.fr  Wed Jul  1 08:55:38 2015
From: alexandre.magnat at mecaprotec.fr (Alexandre Magnat)
Date: Wed, 01 Jul 2015 10:55:38 +0200
Subject: [squid-users] Squid3 authentification proxy and method CONNECT SSL
Message-ID: <5593AB0A.9060405@mecaprotec.fr>

Hello,

I use Squid3 (3.1.20) with Squidguard filtering linked with an user 's 
authentication  on a OpenLDAP.
But, recurrently, Firefox, Thunderbird, Chrome (certainly IE) ask again 
frequently the login and password in a popup.

It seem, the popup authentication appear when the browser try a request 
on a CONNECT method like this:
172.16.1.215 - - [01/Jul/2015:10:40:18 +0200] "CONNECT 
fhr.data.mozilla.com:443 HTTP/1.1" 407 3812 TCP_DENIED:NONE
or like this:
172.16.1.207 - - [01/Jul/2015:10:39:40 +0200] "CONNECT 
safebrowsing.google.com:443 HTTP/1.1" 407 3824 TCP_DENIED:NONE


But, I think, I have configured correctly Squid3 for accept this kind of 
request:

acl SSL_ports port 443
acl CONNECT method CONNECT
http_access deny CONNECT !SSL_ports


It's a boring problem for my user to have 4 or 5 times per day this kind 
of popup :-(
Anybody have an idea for helping me to resolve this ?



-- 
Cordialement,
Alexandre Magnat
Ing?nieur Syst?mes

MECAPROTEC Industries
34, Boulevard de Joffrery
BP 30204 ? 31605 MURET Cedex
T?l: +33(0)5.61.51.85.92



From truniger at bluewin.ch  Wed Jul  1 09:24:44 2015
From: truniger at bluewin.ch (truniger at bluewin.ch)
Date: Wed, 1 Jul 2015 09:24:44 +0000 (GMT+00:00)
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>\r\n\t<21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>
 <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>
Message-ID: <21842932.20345.1435742684665.JavaMail.webmail@bluewin.ch>

Thanks Tom,
as I described in Bug-Id 4279, the whole external FTP communication is working and completed perfectly, as with older releases. It's just that the FTP result status is not put into any http-response. Instead the client-proxy connection stays open 'forever'.

Othmar

># Catching an non-existing file hangs for a unknown time
>$ curl -x proxy:3128 -I ftp://mirror.switch.ch/README123
>
>
>I can confirm, that with older squids (< 3.5) catching a
>non-existing-file with ftp gives back a correct 404.
>
>Kind regards,
>Tom


From paul.martin.b787 at gmail.com  Wed Jul  1 11:49:00 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Wed, 1 Jul 2015 13:49:00 +0200
Subject: [squid-users] squid version 3.5.5
Message-ID: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>

Hello,

I am using 2 machines with 2 squids versions,same squid.conf and both with
700 Http requests/sec.

-squid version 3.3.8: I have 40k squid cache objects

-squid version 3.5.5: I have 2k squid cache objects

Do you have an idea why squid cache objects is so different with squid
version 3.5.5 compare with 3.3.8 ?

Thanks.
Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150701/bb31eb65/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Jul  1 11:57:19 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 1 Jul 2015 13:57:19 +0200
Subject: [squid-users] squid version 3.5.5
In-Reply-To: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>
References: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>
Message-ID: <201507011357.19294.Antony.Stone@squid.open.source.it>

On Wednesday 01 July 2015 at 13:49:00 (EU time), Paul Martin wrote:

> Hello,
> 
> I am using 2 machines with 2 squid versions, same squid.conf and both with
> 700 Http requests/sec.
> 
> -squid version 3.3.8: I have 40k squid cache objects
> 
> -squid version 3.5.5: I have 2k squid cache objects
> 
> Do you have an idea why squid cache objects is so different with squid
> version 3.5.5 compare with 3.3.8 ?

Well, firstly, how long have the two machines been running (did you start both 
with an empty cache at the same time)?

Also, how confident are you that the two machines are getting the same sort of 
requests - how is your user base split across the two proxies?

I also have a nagging suspicion that if you really do mean "same squid.conf" 
on both machines, that could be a source of a problem, but I'll let someone 
more familiar with the specific differences between 3.3.8 and 3.5.5, and the 
config options they each support, respond to that.


Regards,


Antony.

-- 
"640 kilobytes (of RAM) should be enough for anybody."

 - Bill Gates

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Jul  1 11:58:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Jul 2015 23:58:54 +1200
Subject: [squid-users] Squid3 authentification proxy and method CONNECT
 SSL
In-Reply-To: <5593AB0A.9060405@mecaprotec.fr>
References: <5593AB0A.9060405@mecaprotec.fr>
Message-ID: <5593D5FE.7030100@treenet.co.nz>

On 1/07/2015 8:55 p.m., Alexandre Magnat wrote:
> Hello,
> 
> I use Squid3 (3.1.20) 

Please upgrade.

> with Squidguard filtering linked with an user 's
> authentication  on a OpenLDAP.
> But, recurrently, Firefox, Thunderbird, Chrome (certainly IE) ask again
> frequently the login and password in a popup.
> 
> It seem, the popup authentication appear when the browser try a request
> on a CONNECT method like this:
> 172.16.1.215 - - [01/Jul/2015:10:40:18 +0200] "CONNECT
> fhr.data.mozilla.com:443 HTTP/1.1" 407 3812 TCP_DENIED:NONE
> or like this:
> 172.16.1.207 - - [01/Jul/2015:10:39:40 +0200] "CONNECT
> safebrowsing.google.com:443 HTTP/1.1" 407 3824 TCP_DENIED:NONE
> 

1) no credentials were presented. Thus 407 - Auth required.

OR

2) credentials presented were rejected by the auth system. Thus 407 -
Auth requires different credentials (or scheme).

OR

3) NTLM or Negotiate handshake underway. Thus 407 - Auth requires
handshake completion.


> 
> But, I think, I have configured correctly Squid3 for accept this kind of
> request:
> 
> acl SSL_ports port 443
> acl CONNECT method CONNECT
> http_access deny CONNECT !SSL_ports
> 

Those lines have nothing to do with auth. They are for rejecting non-
port 443 connection attempts.

> 
> It's a boring problem for my user to have 4 or 5 times per day this kind
> of popup :-(
> Anybody have an idea for helping me to resolve this ?
> 

Firefox and Thunderbird it may be
<https://bugzilla.mozilla.org/show_bug.cgi?id=318253>. I'm not sure how
long it will take Mozilla to get a fixed version of their software out.
At least they have now finally found the problem.

Chrome and IE may have similar issues. They all tend to copy each others
behaviour with things like this.

Meanwhile there is a workaround that should work - add whichever is
relavant to your config:
 auth_param ntlm keep_alive off
 auth_param negotiate keep_alive off

Amos



From squid3 at treenet.co.nz  Wed Jul  1 12:15:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 02 Jul 2015 00:15:59 +1200
Subject: [squid-users] squid version 3.5.5
In-Reply-To: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>
References: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>
Message-ID: <5593D9FF.5020203@treenet.co.nz>

On 1/07/2015 11:49 p.m., Paul Martin wrote:
> Hello,
> 
> I am using 2 machines with 2 squids versions,same squid.conf and both with
> 700 Http requests/sec.
> 
> -squid version 3.3.8: I have 40k squid cache objects
> 
> -squid version 3.5.5: I have 2k squid cache objects
> 
> Do you have an idea why squid cache objects is so different with squid
> version 3.5.5 compare with 3.3.8 ?

Insufficient data. (sorry bad pun, but its true).

Unless the two proxies are receiving exacty the same data at the same
time for the whole of their uptime its unlikely they will have the same
content cached.

A lot of change has also happened over the 2 years between those versions.

At the very least "Bug 3806: Caching responses with Vary header" will
cause many fewer cache entries since Vary objects are no longer added to
cache then index entries 'lost'.

Also, various fixes in CONNECT handling allow some modern non-HTTP
protocols to work better over proxy CONNECT tunnels. That could be
allowing more of teh traffic through the 3.5 proxy to be using those
protocols instead of HTTP the proxy can cache.

There have also been several revalidation and Range handling fixes that
may reduce the number of objects needed in cache.

Amos



From squid3 at treenet.co.nz  Wed Jul  1 12:22:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 02 Jul 2015 00:22:10 +1200
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <21842932.20345.1435742684665.JavaMail.webmail@bluewin.ch>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>\r\n\t<21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>
 <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>
 <21842932.20345.1435742684665.JavaMail.webmail@bluewin.ch>
Message-ID: <5593DB72.1080401@treenet.co.nz>

On 1/07/2015 9:24 p.m., truniger at bluewin.ch wrote:
> Thanks Tom,
> as I described in Bug-Id 4279, the whole external FTP communication is working and completed perfectly, as with older releases. It's just that the FTP result status is not put into any http-response. Instead the client-proxy connection stays open 'forever'.
> 
> Othmar
> 

The FTP handling code was re-written and shuffled around a lot to get
FTP native support working. I've 'pinged' the author who did that about
your bug report. Hopefully he will have some time to look at it.

Amos



From truniger at bluewin.ch  Wed Jul  1 12:49:46 2015
From: truniger at bluewin.ch (Othmar Truniger)
Date: Wed, 1 Jul 2015 12:49:46 +0000 (GMT+00:00)
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <5593DB72.1080401@treenet.co.nz>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>\r\n\t<21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>\r\n
 <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>\r\n
 <21842932.20345.1435742684665.JavaMail.webmail@bluewin.ch>
 <5593DB72.1080401@treenet.co.nz>
Message-ID: <27680022.28371.1435754986747.JavaMail.webmail@bluewin.ch>

Thank you Amos,

I'm glad I got your attention on this. I realized that when I did a quick code comparison. I also hope we get a quick fix for this because I would like to upgrade to a supported version soon.

Othmar

>The FTP handling code was re-written and shuffled around a lot to get
>FTP native support working. I've 'pinged' the author who did that about
>your bug report. Hopefully he will have some time to look at it.
>
>Amos
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
>


From alexandre.magnat at mecaprotec.fr  Wed Jul  1 13:42:55 2015
From: alexandre.magnat at mecaprotec.fr (Alexandre Magnat)
Date: Wed, 01 Jul 2015 15:42:55 +0200
Subject: [squid-users] Squid3 authentification proxy and method CONNECT
 SSL
In-Reply-To: <5593D5FE.7030100@treenet.co.nz>
References: <5593AB0A.9060405@mecaprotec.fr> <5593D5FE.7030100@treenet.co.nz>
Message-ID: <5593EE5F.5070606@mecaprotec.fr>

Hi Amos;

Thanks you for this complete response.
You're true, I have to upgrade my Debian :) soon !

For the conf, I have put this value in my squid.conf

  auth_param ntlm keep_alive off
  auth_param negotiate keep_alive off

but it seems it's not working (one user have call me).... to be sure, 
i'm waiting more user's return.

Alex



Le 01/07/2015 13:58, Amos Jeffries a ?crit :
> On 1/07/2015 8:55 p.m., Alexandre Magnat wrote:
>> Hello,
>>
>> I use Squid3 (3.1.20)
> Please upgrade.
>
>> with Squidguard filtering linked with an user 's
>> authentication  on a OpenLDAP.
>> But, recurrently, Firefox, Thunderbird, Chrome (certainly IE) ask again
>> frequently the login and password in a popup.
>>
>> It seem, the popup authentication appear when the browser try a request
>> on a CONNECT method like this:
>> 172.16.1.215 - - [01/Jul/2015:10:40:18 +0200] "CONNECT
>> fhr.data.mozilla.com:443 HTTP/1.1" 407 3812 TCP_DENIED:NONE
>> or like this:
>> 172.16.1.207 - - [01/Jul/2015:10:39:40 +0200] "CONNECT
>> safebrowsing.google.com:443 HTTP/1.1" 407 3824 TCP_DENIED:NONE
>>
> 1) no credentials were presented. Thus 407 - Auth required.
>
> OR
>
> 2) credentials presented were rejected by the auth system. Thus 407 -
> Auth requires different credentials (or scheme).
>
> OR
>
> 3) NTLM or Negotiate handshake underway. Thus 407 - Auth requires
> handshake completion.
>
>
>> But, I think, I have configured correctly Squid3 for accept this kind of
>> request:
>>
>> acl SSL_ports port 443
>> acl CONNECT method CONNECT
>> http_access deny CONNECT !SSL_ports
>>
> Those lines have nothing to do with auth. They are for rejecting non-
> port 443 connection attempts.
>
>> It's a boring problem for my user to have 4 or 5 times per day this kind
>> of popup :-(
>> Anybody have an idea for helping me to resolve this ?
>>
> Firefox and Thunderbird it may be
> <https://bugzilla.mozilla.org/show_bug.cgi?id=318253>. I'm not sure how
> long it will take Mozilla to get a fixed version of their software out.
> At least they have now finally found the problem.
>
> Chrome and IE may have similar issues. They all tend to copy each others
> behaviour with things like this.
>
> Meanwhile there is a workaround that should work - add whichever is
> relavant to your config:
>   auth_param ntlm keep_alive off
>   auth_param negotiate keep_alive off
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From mcsnv96 at afo.net  Wed Jul  1 15:11:57 2015
From: mcsnv96 at afo.net (Mike)
Date: Wed, 01 Jul 2015 10:11:57 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <5594033D.6030307@afo.net>

Rafael, We're trying to keep the setups lean, and primarily just deal 
with google and youtube, not all websites. ICAP processes deal with a 
whole new layer of complexity and usually cover all websites, no just 
the few.

On 6/30/2015 16:17 PM, Rafael Akchurin wrote:
> Hello Mike,
>
> May be it is time to take a look at ICAP/eCAP protocol implementations which target specifically this problem - filtering within the *contents* of the stream on Squid?
>
> Best regards,
> Rafael

Marcus,

This is multiple servers used for thousands of customers across North 
America, not an office, so changing from a proxy to a DNS server is not 
an option, since we would also be required to change all several 
thousand of our customers DNS settings.

On 6/30/2015 17:30 PM, Marcus Kool wrote:
> I suggest to read this:
> https://support.google.com/websearch/answer/186669
>
> and look at option 3 of section 'Keep SafeSearch turned on for your 
> network'
>
> Marcus 

Such a pain, there is no reason for our every day searches should be 
encrypted.


Mike

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike
> Sent: Tuesday, June 30, 2015 10:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl for redirect
>
> Scratch that (my previous email to this list), google disabled their insecure sites when used as part of a redirect. We as individual users can use that url directly in the browser (
> http://www.google.com/webhp?nord=1 ) but any google page load starts with secure page causing that redirect to fail... The newer 3.1.2 e2guardian SSL MITM requires options (like a der certificate file) that cannot be used with thousands of existing users on our system, so squid may be our only option.
>
> Another issue right now is google is using a "VPN-style" internal redirect on their server, so e2guardian (shown in log) sees
> https://www.google.com:443  CONNECT, passes along TCP_TUNNEL/200
> www.google.com:443 to squid (shown in squid log), and after that it is directly between google and the browser, not allowing e2guardian nor squid to see further urls from google (such as search terms) for the rest of that specific session. Can click news, maps, images, videos, and NONE of these are passed along to the proxy.
>
> So my original question still stands, how to set an acl for google urls that references a file with blocked terms/words/phrases, and denies it if those terms are found (like a black list)?
>
> Another option I thought of is since the meta content in the code including title is passed along, so is there a way to have it can the header or title content as part of the acl "content scan" process?
>
>
> Thanks
> Mike
>
>
> On 6/26/2015 13:29 PM, Mike wrote:
>> Nevermind... I found another fix within e2guardian:
>>
>> etc/e2guardian/lists/urlregexplist
>>
>> Added this entry:
>> # Disable Google SSL Search
>> # allows e2g to filter searches properly
>> "^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1"
>>
>>
>> This means whenever google.com or www.google.com is typed in the
>> address bar, it loads the insecure page and allows e2guardian to
>> properly filter whatever search terms they type in. This does break
>> other aspects such as google toolbars, using the search bar at upper
>> right of many browsers with google as the set search engine, and other
>> ways, but that is an issue we can live with.
>>
>> On 26/06/2015 2:36 a.m., Mike wrote:
>>> Amos, thanks for info.
>>>
>>> The primary settings being used in squid.conf:
>>>
>>> http_port 8080
>>> # this port is what will be used for SSL Proxy on client browser
>>> http_port 8081 intercept
>>>
>>> https_port 8082 intercept ssl-bump connection-auth=off
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
>>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-
>>> RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>>
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M
>>> 16MB sslcrtd_children 50 startup=5 idle=1 ssl_bump server-first all
>>> ssl_bump none localhost
>>>
>>>
>>> Then e2guardian uses 10101 for the browsers, and uses 8080 for
>>> connecting to squid on the same server.
>> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
>> connection in re-encrypted on outgoing.
>>
>>
>> I am doubtful eth nord works anymore since Googles own documentation
>> for schools states that one must install a MITM proxy that does the
>> traffic filtering - e2guardian is not one of those. IMO you should
>> convert your e2guardian config into Squid ACL rules that can be
>> applied to the bumped traffic without forcinghttp://
>>
>> But if nord does work, so should the deny_info in Squid. Something
>> like this probably:
>>
>>    acl google dstdomain .google.com
>>    deny_info 301:http://%H%R?nord=1  google
>>
>>    acl GwithQuery urlpath_regex ?
>>    deny_info 301:http://%H%R&nord=1  GwithQuery
>>
>>    http_access deny google Gquery
>>    http_access deny google
>>
>>
>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From marcus.kool at urlfilterdb.com  Wed Jul  1 16:11:15 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 01 Jul 2015 13:11:15 -0300
Subject: [squid-users] acl for redirect
In-Reply-To: <5594033D.6030307@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <5594033D.6030307@afo.net>
Message-ID: <55941123.4080404@urlfilterdb.com>

The article does not say to change from a proxy to a DNS server.
Instead, it says to add an entry for google to your own DNS server (the one that Squid uses) and continue to use your proxy.

Marcus

> Marcus,
>
> This is multiple servers used for thousands of customers across North America, not an office, so changing from a proxy to a DNS server is not an option, since we would also be required to change all
> several thousand of our customers DNS settings.
>
> On 6/30/2015 17:30 PM, Marcus Kool wrote:
>> I suggest to read this:
>> https://support.google.com/websearch/answer/186669
>>
>> and look at option 3 of section 'Keep SafeSearch turned on for your network'
>>
>> Marcus


From ashish_behl at yahoo.com  Wed Jul  1 16:54:07 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Wed, 1 Jul 2015 09:54:07 -0700 (PDT)
Subject: [squid-users] Squid kerberos_ldap_group ACL dependencies on SUSE12.
Message-ID: <1435769647028-4672008.post@n4.nabble.com>

Hello All,
I have been trying to compile squid on SUSE12 enterprise, but the only
option that fails is external ACL kerberos_ldap_group.

Can someone please let me know the dependencies for this external helper?
I have tried my best but cannot find a reason why this does not build..
Please note that I have been able to build squid with this helper enabled on
Suse 11.

My configure options:

%configure \
    --disable-strict-error-checking \
    --with-dl \
    --enable-disk-io \
    --enable-storeio \
    --enable-removal-policies=heap,lru \
    --enable-icmp \
    --enable-delay-pools \
    --enable-esi \
    --enable-icap-client \
    --enable-useragent-log \
    --enable-referer-log \
    --enable-kill-parent-hack \
    --enable-arp-acl \
    --enable-ssl-crtd \
    --enable-ssl \
    --with-openssl \
    --enable-forw-via-db \
    --enable-cache-digests \
    --enable-linux-netfilter \
    --with-large-files \
    --enable-underscores \
    --enable-auth \
    --enable-auth-basic \
    --enable-auth-ntlm \
    --enable-auth-negotiate \
    --enable-auth-digest \
   
--enable-external-acl-helpers=LDAP_group,eDirectory_userip,file_userip,kerberos_ldap_group,session,unix_group,wbinfo_group
\
    --enable-ntlm-fail-open \
    --enable-stacktraces \
    --enable-x-accelerator-vary \
    --disable-ident-lookups \
    --enable-follow-x-forwarded-for \
    --disable-arch-native

Installed packages, that I think are relevant:
krb5-devel
openldap2-devel
libbz2-devel
libexpat-devel
pam
pam-devel
cyrus-sasl
krb5
cyrus-sasl-devel
keyutils-devel
libdb-4_8-devel
perl-Module-Build


Request you to please help! Or guide me to relevant documentation section/
user forum





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-kerberos-ldap-group-ACL-dependencies-on-SUSE12-tp4672008.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tomtux007 at gmail.com  Wed Jul  1 19:14:47 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Wed, 1 Jul 2015 21:14:47 +0200
Subject: [squid-users] Squid kerberos_ldap_group ACL dependencies on
	SUSE12.
In-Reply-To: <1435769647028-4672008.post@n4.nabble.com>
References: <1435769647028-4672008.post@n4.nabble.com>
Message-ID: <CACLJR+O5+jbejCY2Ztf6ZVjNMh-Ti0HUGXKBgigjO2AADvKGSg@mail.gmail.com>

Whats the error-message?
I also run a SLES12-Box with kerberos-auth. I had to ensure, that in
the users-path, from which you try to configure && make && make
install, the following directories are present:
export PATH=$PATH:/usr/lib/mit/bin:/usr/lib/mit/sbin

Regards,
Tom

On Wed, Jul 1, 2015 at 6:54 PM, Ashish Behl <ashish_behl at yahoo.com> wrote:
> Hello All,
> I have been trying to compile squid on SUSE12 enterprise, but the only
> option that fails is external ACL kerberos_ldap_group.
>
> Can someone please let me know the dependencies for this external helper?
> I have tried my best but cannot find a reason why this does not build..
> Please note that I have been able to build squid with this helper enabled on
> Suse 11.
>
> My configure options:
>
> %configure \
>     --disable-strict-error-checking \
>     --with-dl \
>     --enable-disk-io \
>     --enable-storeio \
>     --enable-removal-policies=heap,lru \
>     --enable-icmp \
>     --enable-delay-pools \
>     --enable-esi \
>     --enable-icap-client \
>     --enable-useragent-log \
>     --enable-referer-log \
>     --enable-kill-parent-hack \
>     --enable-arp-acl \
>     --enable-ssl-crtd \
>     --enable-ssl \
>     --with-openssl \
>     --enable-forw-via-db \
>     --enable-cache-digests \
>     --enable-linux-netfilter \
>     --with-large-files \
>     --enable-underscores \
>     --enable-auth \
>     --enable-auth-basic \
>     --enable-auth-ntlm \
>     --enable-auth-negotiate \
>     --enable-auth-digest \
>
> --enable-external-acl-helpers=LDAP_group,eDirectory_userip,file_userip,kerberos_ldap_group,session,unix_group,wbinfo_group
> \
>     --enable-ntlm-fail-open \
>     --enable-stacktraces \
>     --enable-x-accelerator-vary \
>     --disable-ident-lookups \
>     --enable-follow-x-forwarded-for \
>     --disable-arch-native
>
> Installed packages, that I think are relevant:
> krb5-devel
> openldap2-devel
> libbz2-devel
> libexpat-devel
> pam
> pam-devel
> cyrus-sasl
> krb5
> cyrus-sasl-devel
> keyutils-devel
> libdb-4_8-devel
> perl-Module-Build
>
>
> Request you to please help! Or guide me to relevant documentation section/
> user forum
>
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-kerberos-ldap-group-ACL-dependencies-on-SUSE12-tp4672008.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From mcsnv96 at afo.net  Wed Jul  1 19:54:43 2015
From: mcsnv96 at afo.net (Mike)
Date: Wed, 01 Jul 2015 14:54:43 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <55941123.4080404@urlfilterdb.com>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <5594033D.6030307@afo.net> <55941123.4080404@urlfilterdb.com>
Message-ID: <55944583.6030309@afo.net>

This is a proxy server, not a DNS server, and does not connect to a DNS 
server that we have any control over... The primary/secondary DNS is 
handled through the primary host (Cox) for all of our servers so we do 
not want to alter it for all several hundred servers, just these 4 
(maybe 6).
I was originally thinking of modifying the resolv.conf but again that is 
internal DNS used by the server itself. The users will have their own 
DNS settings causing it to either ignore our settings, or right back to 
the "Website cannot be displayed" errors due to the DNS loop.

So finding a way to redirect in squid should be the better route for us 
since DNS is not an option....
Essentially www.google.com --> forcesafesearch.google.com

Mike

On 7/1/2015 11:11 AM, Marcus Kool wrote:
> The article does not say to change from a proxy to a DNS server.
> Instead, it says to add an entry for google to your own DNS server 
> (the one that Squid uses) and continue to use your proxy.
>
> Marcus
>
>> Marcus,
>>
>> This is multiple servers used for thousands of customers across North 
>> America, not an office, so changing from a proxy to a DNS server is 
>> not an option, since we would also be required to change all
>> several thousand of our customers DNS settings.
>>
>> On 6/30/2015 17:30 PM, Marcus Kool wrote:
>>> I suggest to read this:
>>> https://support.google.com/websearch/answer/186669
>>>
>>> and look at option 3 of section 'Keep SafeSearch turned on for your 
>>> network'
>>>
>>> Marcus
>



From hack.back at hotmail.com  Wed Jul  1 23:31:23 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 1 Jul 2015 16:31:23 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
Message-ID: <1435793483490-4672011.post@n4.nabble.com>

after upgrading to 3.5.5
i see in cache.log
2015/07/02 01:51:51 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/02 01:51:51 kid1|       /cache01/2/16/AA/0016AA3B
 - ORIGINAL_DST/203.77.186.75 video/mp4
access.log
TCP_SWAPFAIL_MISS/200






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From paul.martin.b787 at gmail.com  Thu Jul  2 05:55:38 2015
From: paul.martin.b787 at gmail.com (Paul Martin)
Date: Thu, 2 Jul 2015 07:55:38 +0200
Subject: [squid-users] squid version 3.5.5
In-Reply-To: <5593D9FF.5020203@treenet.co.nz>
References: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>
 <5593D9FF.5020203@treenet.co.nz>
Message-ID: <CAGAgj8BDWcYoKOJzCwwqVOAmRZqs2FzBA+zRe9LE5F1jkwJDRA@mail.gmail.com>

Hello

Thanks and just another question about internet bandwith

I notice
> -squid version 3.3.8: I have 40k squid cache objects
> -squid version 3.5.5: I have 2k squid cache objects

Does it mean I need x10 to x20 times more "internet bandwith network" with
squid 3.5.5 compare to 3.3.8 ?
Because if I upgrade squid to version 3.5.5,  I should also need to upgrade
my network capacity, what do you think ?

Thank you
Paul




2015-07-01 14:15 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 1/07/2015 11:49 p.m., Paul Martin wrote:
> > Hello,
> >
> > I am using 2 machines with 2 squids versions,same squid.conf and both
> with
> > 700 Http requests/sec.
> >
> > -squid version 3.3.8: I have 40k squid cache objects
> >
> > -squid version 3.5.5: I have 2k squid cache objects
> >
> > Do you have an idea why squid cache objects is so different with squid
> > version 3.5.5 compare with 3.3.8 ?
>
> Insufficient data. (sorry bad pun, but its true).
>
> Unless the two proxies are receiving exacty the same data at the same
> time for the whole of their uptime its unlikely they will have the same
> content cached.
>
> A lot of change has also happened over the 2 years between those versions.
>
> At the very least "Bug 3806: Caching responses with Vary header" will
> cause many fewer cache entries since Vary objects are no longer added to
> cache then index entries 'lost'.
>
> Also, various fixes in CONNECT handling allow some modern non-HTTP
> protocols to work better over proxy CONNECT tunnels. That could be
> allowing more of teh traffic through the 3.5 proxy to be using those
> protocols instead of HTTP the proxy can cache.
>
> There have also been several revalidation and Range handling fixes that
> may reduce the number of objects needed in cache.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150702/285efe88/attachment.htm>

From vdoctor at neuf.fr  Thu Jul  2 06:32:15 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 1 Jul 2015 23:32:15 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1428307801410-4670629.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
Message-ID: <1435818735040-4672013.post@n4.nabble.com>

Hi,

I'm back to this post because it still does not work.
You explain "OFF - Squid selects a (possibly new, or not) IP to be used as
the
server (logs DIRECT).", sorry to say this is not the reality in the Squid.
We have set the pass-thru directive to OFF and here is the result:
TCP_MISS/206 72540 GET
http://www.google.com/dl/chrome/win/B6585D9F8CF5DBD2/43.0.2357.130_chrome_installer.exe
- ORIGINAL_DST/216.58.220.36

Is there a way to totaly disable the DNS control done by Squid ?

Thanks 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672013.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stu at spacehopper.org  Thu Jul  2 07:06:53 2015
From: stu at spacehopper.org (Stuart Henderson)
Date: Thu, 2 Jul 2015 07:06:53 +0000 (UTC)
Subject: [squid-users] acl for redirect
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <5594033D.6030307@afo.net> <55941123.4080404@urlfilterdb.com>
 <55944583.6030309@afo.net>
Message-ID: <slrnmp9ood.qvn.stu@naiad.spacehopper.org>

On 2015-07-01, Mike <mcsnv96 at afo.net> wrote:
> This is a proxy server, not a DNS server, and does not connect to a DNS 
> server that we have any control over... The primary/secondary DNS is 
> handled through the primary host (Cox) for all of our servers so we do 
> not want to alter it for all several hundred servers, just these 4 
> (maybe 6).
> I was originally thinking of modifying the resolv.conf but again that is 
> internal DNS used by the server itself. The users will have their own 
> DNS settings causing it to either ignore our settings, or right back to 
> the "Website cannot be displayed" errors due to the DNS loop.

resolv.conf would work, or you can use dns_nameservers in squid.conf and
point just squid (if you want) to a private resolver configured to hand
out the forcesafesearch address.

When a proxy is used, the client defers name resolution to the proxy, you
don't need to change DNS on client machines to do this.



From ashish_behl at yahoo.com  Thu Jul  2 07:00:54 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Thu, 2 Jul 2015 00:00:54 -0700 (PDT)
Subject: [squid-users] Squid kerberos_ldap_group ACL dependencies on
	SUSE12.
In-Reply-To: <CACLJR+O5+jbejCY2Ztf6ZVjNMh-Ti0HUGXKBgigjO2AADvKGSg@mail.gmail.com>
References: <1435769647028-4672008.post@n4.nabble.com>
 <CACLJR+O5+jbejCY2Ztf6ZVjNMh-Ti0HUGXKBgigjO2AADvKGSg@mail.gmail.com>
Message-ID: <1435820454468-4672015.post@n4.nabble.com>

Thanks a lot Tom, I was able to compile after setting this PATH.

thanks again for your help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-kerberos-ldap-group-ACL-dependencies-on-SUSE12-tp4672008p4672015.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Thu Jul  2 07:32:41 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 2 Jul 2015 07:32:41 +0000
Subject: [squid-users] acl for redirect
In-Reply-To: <5594033D.6030307@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>,
 <5594033D.6030307@afo.net>
Message-ID: <VI1PR04MB13596E8542D535161683D4358F970@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Mike,

Access to ICAP is controlled with same looking acls as access to anything else. Something like:

icap_enable on
icap_service qlproxy1 reqmod_precache bypass=0 icap://127.0.0.1:1344/reqmod
icap_service qlproxy2 respmod_precache bypass=0 icap://127.0.0.1:1344/respmod
acl target_domains dstdomain "/path/to/target/domains/list"
adaptation_access qlproxy1 allow target_domains
adaptation_access qlproxy2 allow target_domains
adaptation_access qlproxy1 deny all
adaptation_access qlproxy2 deny all

will forward *only* requests/responses to those domain names specified in /path/to/target/domains/list to ICAP REQMOD and RESPMOD services.
All other connections are not forwarded to ICAP.

Raf

________________________________________
From: Mike <mcsnv96 at afo.net>
Sent: Wednesday, July 1, 2015 5:11 PM
To: Rafael Akchurin; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] acl for redirect

Rafael, We're trying to keep the setups lean, and primarily just deal
with google and youtube, not all websites. ICAP processes deal with a
whole new layer of complexity and usually cover all websites, no just
the few.

On 6/30/2015 16:17 PM, Rafael Akchurin wrote:
> Hello Mike,
>
> May be it is time to take a look at ICAP/eCAP protocol implementations which target specifically this problem - filtering within the *contents* of the stream on Squid?
>
> Best regards,
> Rafael

Marcus,

This is multiple servers used for thousands of customers across North
America, not an office, so changing from a proxy to a DNS server is not
an option, since we would also be required to change all several
thousand of our customers DNS settings.

On 6/30/2015 17:30 PM, Marcus Kool wrote:
> I suggest to read this:
> https://support.google.com/websearch/answer/186669
>
> and look at option 3 of section 'Keep SafeSearch turned on for your
> network'
>
> Marcus

Such a pain, there is no reason for our every day searches should be
encrypted.


Mike

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike
> Sent: Tuesday, June 30, 2015 10:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl for redirect
>
> Scratch that (my previous email to this list), google disabled their insecure sites when used as part of a redirect. We as individual users can use that url directly in the browser (
> http://www.google.com/webhp?nord=1 ) but any google page load starts with secure page causing that redirect to fail... The newer 3.1.2 e2guardian SSL MITM requires options (like a der certificate file) that cannot be used with thousands of existing users on our system, so squid may be our only option.
>
> Another issue right now is google is using a "VPN-style" internal redirect on their server, so e2guardian (shown in log) sees
> https://www.google.com:443  CONNECT, passes along TCP_TUNNEL/200
> www.google.com:443 to squid (shown in squid log), and after that it is directly between google and the browser, not allowing e2guardian nor squid to see further urls from google (such as search terms) for the rest of that specific session. Can click news, maps, images, videos, and NONE of these are passed along to the proxy.
>
> So my original question still stands, how to set an acl for google urls that references a file with blocked terms/words/phrases, and denies it if those terms are found (like a black list)?
>
> Another option I thought of is since the meta content in the code including title is passed along, so is there a way to have it can the header or title content as part of the acl "content scan" process?
>
>
> Thanks
> Mike
>
>
> On 6/26/2015 13:29 PM, Mike wrote:
>> Nevermind... I found another fix within e2guardian:
>>
>> etc/e2guardian/lists/urlregexplist
>>
>> Added this entry:
>> # Disable Google SSL Search
>> # allows e2g to filter searches properly
>> "^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1"
>>
>>
>> This means whenever google.com or www.google.com is typed in the
>> address bar, it loads the insecure page and allows e2guardian to
>> properly filter whatever search terms they type in. This does break
>> other aspects such as google toolbars, using the search bar at upper
>> right of many browsers with google as the set search engine, and other
>> ways, but that is an issue we can live with.
>>
>> On 26/06/2015 2:36 a.m., Mike wrote:
>>> Amos, thanks for info.
>>>
>>> The primary settings being used in squid.conf:
>>>
>>> http_port 8080
>>> # this port is what will be used for SSL Proxy on client browser
>>> http_port 8081 intercept
>>>
>>> https_port 8082 intercept ssl-bump connection-auth=off
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
>>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-
>>> RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>>
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M
>>> 16MB sslcrtd_children 50 startup=5 idle=1 ssl_bump server-first all
>>> ssl_bump none localhost
>>>
>>>
>>> Then e2guardian uses 10101 for the browsers, and uses 8080 for
>>> connecting to squid on the same server.
>> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
>> connection in re-encrypted on outgoing.
>>
>>
>> I am doubtful eth nord works anymore since Googles own documentation
>> for schools states that one must install a MITM proxy that does the
>> traffic filtering - e2guardian is not one of those. IMO you should
>> convert your e2guardian config into Squid ACL rules that can be
>> applied to the bumped traffic without forcinghttp://
>>
>> But if nord does work, so should the deny_info in Squid. Something
>> like this probably:
>>
>>    acl google dstdomain .google.com
>>    deny_info 301:http://%H%R?nord=1  google
>>
>>    acl GwithQuery urlpath_regex ?
>>    deny_info 301:http://%H%R&nord=1  GwithQuery
>>
>>    http_access deny google Gquery
>>    http_access deny google
>>
>>
>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Thu Jul  2 10:46:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 02 Jul 2015 22:46:37 +1200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1435793483490-4672011.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
Message-ID: <5595168D.5030504@treenet.co.nz>

On 2/07/2015 11:31 a.m., HackXBack wrote:
> after upgrading to 3.5.5
> i see in cache.log
> 2015/07/02 01:51:51 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
> 2015/07/02 01:51:51 kid1|       /cache01/2/16/AA/0016AA3B
>  - ORIGINAL_DST/203.77.186.75 video/mp4
> access.log
> TCP_SWAPFAIL_MISS/200
> 

Your cache index (from swap.state) does not match what objects actually
exist on disk. This is Squid auto-recovery fetching a new copy from the
network.

Amos



From squid3 at treenet.co.nz  Thu Jul  2 10:53:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 02 Jul 2015 22:53:49 +1200
Subject: [squid-users] squid version 3.5.5
In-Reply-To: <CAGAgj8BDWcYoKOJzCwwqVOAmRZqs2FzBA+zRe9LE5F1jkwJDRA@mail.gmail.com>
References: <CAGAgj8CtwD2+PfnCXstqGBR8i6gJABdSTgSzs+ghM3jj+3tW_Q@mail.gmail.com>	<5593D9FF.5020203@treenet.co.nz>
 <CAGAgj8BDWcYoKOJzCwwqVOAmRZqs2FzBA+zRe9LE5F1jkwJDRA@mail.gmail.com>
Message-ID: <5595183D.8040004@treenet.co.nz>

On 2/07/2015 5:55 p.m., Paul Martin wrote:
> Hello
> 
> Thanks and just another question about internet bandwith
> 
> I notice
>> -squid version 3.3.8: I have 40k squid cache objects
>> -squid version 3.5.5: I have 2k squid cache objects
> 
> Does it mean I need x10 to x20 times more "internet bandwith network" with
> squid 3.5.5 compare to 3.3.8 ?

No. They are not related like that. You could have a full cache and not
HIT at all, or a single object that HITs for 100% of all traffic.

Amos



From squid3 at treenet.co.nz  Thu Jul  2 11:11:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 02 Jul 2015 23:11:38 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435818735040-4672013.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com>
Message-ID: <55951C6A.4030204@treenet.co.nz>

On 2/07/2015 6:32 p.m., Stakres wrote:
> Hi,
> 
> I'm back to this post because it still does not work.
> You explain "OFF - Squid selects a (possibly new, or not) IP to be used as
> the
> server (logs DIRECT).", sorry to say this is not the reality in the Squid.
> We have set the pass-thru directive to OFF and here is the result:
> TCP_MISS/206 72540 GET
> http://www.google.com/dl/chrome/win/B6585D9F8CF5DBD2/43.0.2357.130_chrome_installer.exe
> - ORIGINAL_DST/216.58.220.36
> 
> Is there a way to totaly disable the DNS control done by Squid ?

No. The requests where ORIGINAL_DST is mandatory it is so because the
client Host header contains an identifiable problem. The URL cannot be
cached without allowing other clients to be affected by that problem.
Specifically that 216.58.220.36 != www.google.com.

Amos



From vdoctor at neuf.fr  Thu Jul  2 11:05:31 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 2 Jul 2015 04:05:31 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <55951C6A.4030204@treenet.co.nz>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
Message-ID: <1435835131864-4672020.post@n4.nabble.com>

Hi Amos,

216.58.220.36 != www.google.com ??? 
Have a look: http://www.ip-adress.com/whois/216.58.220.36, this is google.

Depending the DNS server used, the IP can change, we know that especialy due
to BGP.

In the case the client is an ISP providing internet to smaller ISPs with
different DNS with their end users, here I understand that due to the
ORIGINAL_DST squid will check the headers and if the dns records do not
match so squid will not cache, even with a storeid engine, because too many
different DNS servers in the loop (users -> small ISP -> big ISP -> squid ->
internet), am I right ?

So, the result is a very poor 9% saving where we could expect around 50%
saving. 

Can you plan, for a next build, a workaround to accept the original dns
record from the headers and check dns if and only if the headers do not
contain any dns record ?
I understand Squid should provide some securities but here we should have
the possibility to ON/OFF these securities.
Or do we need to downgrade to Squid 2.7/3.0 ?

ISPs need to cache a lot, security is not their main issue.

Thanks in advance.
Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672020.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jul  2 12:04:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 00:04:29 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435835131864-4672020.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com>
Message-ID: <559528CD.5020600@treenet.co.nz>

On 2/07/2015 11:05 p.m., Stakres wrote:
> Hi Amos,
> 
> 216.58.220.36 != www.google.com ??? 
> Have a look: http://www.ip-adress.com/whois/216.58.220.36, this is google.


www.google.com did not resolve to 216.58.220.36 when Squid checked.
Otherwise caching would have been allowed and ORIGINAL_DST not necessary.

Although that said the response was a 206 which does not cache anyway.


> 
> Depending the DNS server used, the IP can change, we know that especialy due
> to BGP.

The Google IPs given also change every 60-90 seconds no matter what DNS
server is used. Which is why its very important to have your clients
using the same DNS resolvers as Squid. So the IPs found by the client
are cached in the DNS resolver when Squid goes to check.

> 
> In the case the client is an ISP providing internet to smaller ISPs with
> different DNS with their end users, here I understand that due to the
> ORIGINAL_DST squid will check the headers and if the dns records do not
> match so squid will not cache, even with a storeid engine, because too many
> different DNS servers in the loop (users -> small ISP -> big ISP -> squid ->
> internet), am I right ?

Very likely yes.

> 
> So, the result is a very poor 9% saving where we could expect around 50%
> saving. 

Unfortunately that is the price for preventing any client script from
replacing the in-cache content with arbitrary content. A very nasty
vulenerability.

You can get around it somewhat by having the ISP resolvers use each
other same as proxy chains do.

> 
> Can you plan, for a next build, a workaround to accept the original dns
> record from the headers and check dns if and only if the headers do not
> contain any dns record ?

That is exactly what ORIGINAL_DST is.

The client DNS lookup results in an IP being used by the client. Squid
takes that IP from the TCP packet and relays the traffic there. It just
cannot be cached because Squid cannot know if the reconstructed URL is a
client lie or not.

Consider some malicious server at 192.168.0.2 responding with an
infected JPG to all requests. An infected server contains a script that
fetches the Google icon from 192.168.0.2 using Host:www.google.com.

Now cache that as http://www.google.com/... and what happens?


> I understand Squid should provide some securities but here we should have
> the possibility to ON/OFF these securities.
> Or do we need to downgrade to Squid 2.7/3.0 ?

You will hit the above vulnerability in older versions. And there *is*
malware out there actively abusing it since other proxies still contain it.

> 
> ISPs need to cache a lot, security is not their main issue.

Understood. You can divert all traffic to a single kitten picture. That
will have a 100% HIT rate.

Then again, you need reliability of service as much as caching. This
ORIGINAL_DST mechanism it there to guarantee reliability in the face of
that vulnerability.

Amos



From vdoctor at neuf.fr  Thu Jul  2 12:43:08 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 2 Jul 2015 05:43:08 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <559528CD.5020600@treenet.co.nz>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
Message-ID: <1435840988272-4672022.post@n4.nabble.com>

Hi Amos,

"/You can get around it somewhat by having the ISP resolvers use each other
same as proxy chains do./"
This is impossible to do in a multi-level ISPs archi, because each ISP could
use any DNS servers (google, level3, etc...). From the original end user to
the latest ISP step the dns header could be using an ip address that the
Squid could not know.

"/Consider some malicious server at 192.168.0.2 responding with an infected
JPG to all requests. An infected server contains a script that fetches the
Google icon from 192.168.0.2 using Host:www.google.com. /"
Totaly agree with you but what we/you could do is to replace the original
dns records from the headers by the records squid will find and allow the
cache hit.
Here, squid only applies the correct dns but deny the object to be cached.
if squid corrects the dns it means the object should be safe (normaly) so it
should accept to see the object saved into the cache (partial object or
not), right ?

So, fixing a wrong dns record is a good thing I agree, but why do you deny
the cache action if the request was corrected ?

What about if the end user has fixed to a special dns server (home made,
exotic server, etc...), here the ISP cannot increase the % saving and this
point (% saving) is the top priority for the ISP that's why he needs
solutions like Squid products.

Do you think we could have a workaround for fixing the wrong dns record from
headers (Squid action) and having the object cached ? or it does not make
sense because other security issues ?

I read many forums where admins are requesting this behaviour, I 'm sure
we/you can find a nice solution for all of us .

Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672022.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Jul  2 13:08:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Jul 2015 19:08:51 +0600
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435840988272-4672022.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
Message-ID: <559537E3.4090809@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
In my installation I use caching DNS (unbound) in conjunction with Squid.

This cachind DNS directly on squid box and solves many problem with DNS.

Unbound cache itself uses custom TTL setting (maximal) for DNS records,
which is overrides provider/original DNS settings.

02.07.15 18:43, Stakres ?????:
> Hi Amos,
>
> "/You can get around it somewhat by having the ISP resolvers use each
other
> same as proxy chains do./"
> This is impossible to do in a multi-level ISPs archi, because each ISP
could
> use any DNS servers (google, level3, etc...). From the original end
user to
> the latest ISP step the dns header could be using an ip address that the
> Squid could not know.
>
> "/Consider some malicious server at 192.168.0.2 responding with an
infected
> JPG to all requests. An infected server contains a script that fetches the
> Google icon from 192.168.0.2 using Host:www.google.com. /"
> Totaly agree with you but what we/you could do is to replace the original
> dns records from the headers by the records squid will find and allow the
> cache hit.
> Here, squid only applies the correct dns but deny the object to be cached.
> if squid corrects the dns it means the object should be safe (normaly)
so it
> should accept to see the object saved into the cache (partial object or
> not), right ?
>
> So, fixing a wrong dns record is a good thing I agree, but why do you deny
> the cache action if the request was corrected ?
>
> What about if the end user has fixed to a special dns server (home made,
> exotic server, etc...), here the ISP cannot increase the % saving and this
> point (% saving) is the top priority for the ISP that's why he needs
> solutions like Squid products.
>
> Do you think we could have a workaround for fixing the wrong dns
record from
> headers (Squid action) and having the object cached ? or it does not make
> sense because other security issues ?
>
> I read many forums where admins are requesting this behaviour, I 'm sure
> we/you can find a nice solution for all of us .
>
> Fred
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672022.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVlTfiAAoJENNXIZxhPexGzE0H/igDMTH/YQQa5LxjuxS7kz5B
tCiGhynWB4LDl8rs9bWKUrvL+vSTU3P5CiiZgSSdSJL3HmpOr/C2tLlepAh3zkql
pKTjlb/Lw6X5q8HpGRc6hRHoR2qaFpXM28H01358UQpZPIEIxG4ivgPz9hSkZZ72
6RnZR1uVlgGBRqh1mPDdPFUq8WCGi49InqjkZwd+cxOVeoujJDdKwI1mKYU/QDoy
UkPQRLfPDnqJBo4Y4Fowj/aQTTreQSLGtjhIIuGlOllomIdU9d8JruUHKUPfEnZe
mIWk9oMAL4e0hhZf1l+1Du6w4g1XCQ750a6DOjaYZaTmyBuSdyeaER3y/cc566M=
=9HYZ
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Thu Jul  2 12:59:27 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 2 Jul 2015 05:59:27 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <559537E3.4090809@gmail.com>
References: <54F65CA3.5070708@ngtech.co.il>
 <1425453570201-4670194.post@n4.nabble.com> <54F6D2AC.6050203@treenet.co.nz>
 <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com> <559537E3.4090809@gmail.com>
Message-ID: <1435841967543-4672024.post@n4.nabble.com>

Hi Yury,

In your installation, with your devices... At home, I do the same like you,
but I'm not an ISP.

Here the issue is that end users could use different dns the ISPs cannot
control.
Home/Entreprise, the admin can control the used DNS servers with devices. In
an ISP environment, we cannot control/manage, end users do what they want.
2 different worlds, not the same rules, sorry 

Fred






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672024.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Jul  2 13:35:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Jul 2015 19:35:33 +0600
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435841967543-4672024.post@n4.nabble.com>
References: <54F65CA3.5070708@ngtech.co.il>
 <1425453570201-4670194.post@n4.nabble.com> <54F6D2AC.6050203@treenet.co.nz>
 <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com> <559537E3.4090809@gmail.com>
 <1435841967543-4672024.post@n4.nabble.com>
Message-ID: <55953E25.5080705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Fred,

I'm talkin not about localhost installation.

My squid serves business-center. With hundreds of users.

In this environment, we use also transparent DNS interception onto DNS
cache. DNS cache itself uses clean sources for resolving, using dnscrypt.

This permit me almost full control above DNS. ;)

Sorry, but you can build your own world. :)))))))))))))) Or can't
:)))))))) As you wish.

WNR, Yuri

02.07.15 18:59, Stakres ?????:
> Hi Yury,
>
> In your installation, with your devices... At home, I do the same like
you,
> but I'm not an ISP.
>
> Here the issue is that end users could use different dns the ISPs cannot
> control.
> Home/Entreprise, the admin can control the used DNS servers with
devices. In
> an ISP environment, we cannot control/manage, end users do what they want.
> 2 different worlds, not the same rules, sorry
>
> Fred
>
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672024.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVlT4lAAoJENNXIZxhPexG90EH/0YI+7ERqjv32GDz564YupeF
Cu0y2oCdclt5zNBQMVzXfKOwYpePk6XDk9coSCMiTPOq8gjagB4sx5nm+da3tCd/
+vJvF17ht4f0Ue1CPblv7h2McX+ui6+92V3/saaDMMHr59XjAqfycg3Iev8wnH56
uWL35hYfm+djZVse0roKUdB4E43fAFH5NelMEnFOdWRXuJn8WFlWPTNMly1mYOzz
5KwQR0mWhb9QyKgQc/rWmsEoby2SxqulkbpkHfu5cT+F1G0CtcNvjcaseEZ7S9ku
WSaex0XNQtBX/WDEDla/pagPc45yMUBpQXm10k5B4V6RUO8R/67/EZmUXrQ+8EE=
=aBUc
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Thu Jul  2 13:35:47 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Jul 2015 19:35:47 +0600
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435841967543-4672024.post@n4.nabble.com>
References: <54F65CA3.5070708@ngtech.co.il>
 <1425453570201-4670194.post@n4.nabble.com> <54F6D2AC.6050203@treenet.co.nz>
 <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com> <559537E3.4090809@gmail.com>
 <1435841967543-4672024.post@n4.nabble.com>
Message-ID: <55953E33.7040409@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Fred,

I'm talkin not about localhost installation.

My squid serves business-center. With hundreds of users.

In this environment, we use also transparent DNS interception onto DNS
cache. DNS cache itself uses clean sources for resolving, using dnscrypt.

This permit me almost full control above DNS. ;)

Sorry, but you can build your own world. :)))))))))))))) Or can't
:)))))))) As you wish.

WBR, Yuri

02.07.15 18:59, Stakres ?????:
> Hi Yury,
>
> In your installation, with your devices... At home, I do the same like
you,
> but I'm not an ISP.
>
> Here the issue is that end users could use different dns the ISPs cannot
> control.
> Home/Entreprise, the admin can control the used DNS servers with
devices. In
> an ISP environment, we cannot control/manage, end users do what they want.
> 2 different worlds, not the same rules, sorry
>
> Fred
>
>
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672024.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVlT4zAAoJENNXIZxhPexGgIUIAKxf4R9KsRmCAsQPOMysX/LO
EhMyv5FgGzVCWg2aSLfPX1QwPJJS0FAg7VUxEXuKVk8biRWGDpgHIlJEMGThSkRh
bp7GH6CLesvv5fs+jG9uumWtS/bS7Kogvr8dZso784qo1fU6bxEp1imol1JnIW8i
I45E8+3JBuniIrxY62wY5jgbKoa+JxAEyGRcptLGaBpTofivg5b7Lkoe8s9+zRSy
YoJl8N/KoTk0bP4BTTjsC+YKKvqMhzv1iFEoebqd/Tpk2t+9pPoek26gosfmbZyw
iZE6FKtH2Hx5YROHYnY0lJTRZS7Av2NO8ZwtEEOORfJM5nnzGWMaXlLer/w7KwA=
=yLQQ
-----END PGP SIGNATURE-----



From mynixmail at gmail.com  Thu Jul  2 14:41:58 2015
From: mynixmail at gmail.com (Danny)
Date: Thu, 2 Jul 2015 16:41:58 +0200
Subject: [squid-users] reply_body_max_size question
Message-ID: <20150702144158.GB28280@fever.havannah.local>

I am running Debian 8 with Squid3 installed (transparent). However, I would like to know a little more about the "reply_body_max_size" directive. I have read quite a bit about it but none of the discussions on the net fits my criteria ... 
(Oh yes, squidGuard is also running around my server somewhere doing what it is supposed to do ... I hope ... )

It is a home setup with the Debian box serving DHCP IP's over wlan0 (which all devices in the house connect to for internet access). 9 laptops, 4 PC's, 7 tablets and 9 SmartPhones (and that is only the kid's stuff fighting for bandwidth supremacy ... ;) ) ... We are all on the same subnet ...

The problem I have (as with most parents) is to limit the kid's download sizes from all over the net. Where I am we have capped internet and have to pay for more cap. 
Currently I get 20GB of data every month and by the end of the month I have purchased in excess of 100GB throughout the month which gets very expensive. 
My son plays games on his PS3 and some of the games (Call of Duty, I think) one player can download another player's in-game recorded video (or something like that) and that eats up the cap.

Currently my "reply_body_max_size" is set to 20 MB in my efforts to curb downloads and save some bandwidth. 
However, whenever myself or the wife wants to download or visit youtube I have to change the 20MB limit, restart Squid3, watch youtube, change limit back to 20MB and reload Squid3 again ... which is a pain in the butt ...

Currently my ACL's look like this:

acl localnet src 10.0.0.0/24
acl localnet_dad_laptop 10.0.0.10
acl localnet_dad_smartphone 10.0.0.11
acl localnet_mom_laptop 10.0.0.12
acl localnet_mom_smartphone 10.0.0.13
acl localnet_son_laptop 10.0.0.14
acl localnet_son_smartphone 10.0.0.15
acl localnet_son_tablet 10.0.0.16

---and so it goes on for all the other devices---

http_access allow localnet
http_access allow localnet_dad_laptop
http_access allow localnet_dad_smartphone
http_access allow localnet_mom_laptop
http_access allow localnet_mom_smartphone
http_access allow localnet_son_laptop
http_access allow localnet_son_smartphone
http_access allow localnet_son_tablet

---and so it goes on for all the other devices---

How can I allow mom and dad unlimited download sizes but limit download sizes for my kids (son, daughter and daughter) and all the kid's friends that visit and sleep over?

Thank You

Danny


From shakirgil at yahoo.com  Thu Jul  2 15:09:35 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Thu, 2 Jul 2015 15:09:35 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <1467191445.2188429.1435640703348.JavaMail.yahoo@mail.yahoo.com>
References: <55921D3C.30808@treenet.co.nz>
 <1467191445.2188429.1435640703348.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <423657939.1025678.1435849775055.JavaMail.yahoo@mail.yahoo.com>

We are running single instance of squid. after 2 days running squid we got the same error.

[root at cache ~]# cat /var/log/squid/cache.log 

2015/07/02 20:01:28 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory 
2015/07/02 20:01:28 kid1|       /cache01/cache02/0C/F4/000CF437 
2015/07/02 20:01:29 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory 
2015/07/02 20:01:29 kid1|       /cache01/cache02/13/00/00130051 
2015/07/02 20:01:29 kid1| WARNING: 1 swapin MD5 mismatches 
2015/07/02 20:01:29 kid1| Could not parse headers from on disk object 
2015/07/02 20:01:29 kid1| BUG 3279: HTTP reply without Date: 
2015/07/02 20:01:29 kid1| StoreEntry->key: 388240BC608374033E480EABC453432A 
2015/07/02 20:01:29 kid1| StoreEntry->next: 0x20becba8 
2015/07/02 20:01:29 kid1| StoreEntry->mem_obj: 0x238c2140 
2015/07/02 20:01:29 kid1| StoreEntry->timestamp: -1 
2015/07/02 20:01:29 kid1| StoreEntry->lastref: 1435849289 
2015/07/02 20:01:29 kid1| StoreEntry->expires: -1 
2015/07/02 20:01:29 kid1| StoreEntry->lastmod: -1 
2015/07/02 20:01:29 kid1| StoreEntry->swap_file_sz: 0 
2015/07/02 20:01:29 kid1| StoreEntry->refcount: 1 
2015/07/02 20:01:29 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED 
2015/07/02 20:01:29 kid1| StoreEntry->swap_dirn: -1 
2015/07/02 20:01:29 kid1| StoreEntry->swap_filen: -1 
2015/07/02 20:01:29 kid1| StoreEntry->lock_count: 2 
2015/07/02 20:01:29 kid1| StoreEntry->mem_status: 0 
2015/07/02 20:01:29 kid1| StoreEntry->ping_status: 2 
2015/07/02 20:01:29 kid1| StoreEntry->store_status: 1 
2015/07/02 20:01:29 kid1| StoreEntry->swap_status: 0 
2015/07/02 20:01:29 kid1| assertion failed: store.cc:1885: "isEmpty()" 
2015/07/02 20:01:41 kid1| Current Directory is /root 
2015/07/02 20:01:41 kid1| Starting Squid Cache version 3.5.5 for x86_64-redhat-linux-gnu... 
2015/07/02 20:01:41 kid1| Service Name: squid 
2015/07/02 20:01:41 kid1| Process ID 12717 
2015/07/02 20:01:41 kid1| Process Roles: worker 
2015/07/02 20:01:41 kid1| With 65536 file descriptors available 
2015/07/02 20:01:41 kid1| Initializing IP Cache... 
2015/07/02 20:01:41 kid1| DNS Socket created at [::], FD 6 
2015/07/02 20:01:41 kid1| DNS Socket created at 0.0.0.0, FD 8 
2015/07/02 20:01:41 kid1| Adding nameserver 192.167.1.1 from squid.conf 
2015/07/02 20:01:41 kid1| Adding nameserver 192.167.1.1 from squid.conf 
2015/07/02 20:01:41 kid1| helperOpenServers: Starting 10/40 'storeid.pl' processes 
2015/07/02 20:01:41 kid1| Logfile: opening log /var/log/squid/access.log 
2015/07/02 20:01:41 kid1| WARNING: log name now starts with a module name. Use 'stdio:/var/log/squid/access.log' 
2015/07/02 20:01:41 kid1| Store logging disabled 
2015/07/02 20:01:41 kid1| Swap maxSize 1433600000 + 131072 KB, estimated 4779103 objects 
2015/07/02 20:01:41 kid1| Target number of buckets: 238955 
2015/07/02 20:01:41 kid1| Using 262144 Store buckets 
2015/07/02 20:01:41 kid1| Max Mem  size: 131072 KB 
2015/07/02 20:01:41 kid1| Max Swap size: 1433600000 KB 
2015/07/02 20:01:41 kid1| Rebuilding storage in /cache01/cache01 (dirty log) 
2015/07/02 20:01:41 kid1| Rebuilding storage in /cache01/cache02 (dirty log) 
2015/07/02 20:01:41 kid1| Rebuilding storage in /cache02/cache01 (dirty log) 
2015/07/02 20:01:41 kid1| Rebuilding storage in /cache02/cache02 (dirty log) 
2015/07/02 20:01:41 kid1| Using Least Load store dir selection 
2015/07/02 20:01:41 kid1| Current Directory is /root 
2015/07/02 20:01:41 kid1| Finished loading MIME types and icons. 
2015/07/02 20:01:41 kid1| Sending SNMP messages from [::]:3401 
2015/07/02 20:01:41 kid1| Squid plugin modules loaded: 0 
2015/07/02 20:01:41 kid1| Adaptation support is off. 
2015/07/02 20:01:41 kid1| Accepting HTTP Socket connections at local=[::]:3129 remote=[::] FD 38 flags=9 
2015/07/02 20:01:41 kid1| Accepting NAT intercepted HTTP Socket connections at local=0.0.0.0:8080 remote=[::] FD 39 flags=41 
2015/07/02 20:01:41 kid1| Accepting SNMP messages on [::]:3401 
2015/07/02 20:01:41 kid1| Store rebuilding is 3.79% complete 
2015/07/02 20:01:43 kid1| Done reading /cache02/cache01 swaplog (105665 entries) 
2015/07/02 20:01:43 kid1| Done reading /cache01/cache01 swaplog (105579 entries) 
2015/07/02 20:01:56 kid1| Store rebuilding is 77.09% complete 
2015/07/02 20:02:01 kid1| Done reading /cache02/cache02 swaplog (1463112 entries) 
2015/07/02 20:02:01 kid1| Done reading /cache01/cache02 swaplog (1465014 entries) 
2015/07/02 20:02:01 kid1| Finished rebuilding storage from disk. 
2015/07/02 20:02:01 kid1|   3139360 Entries scanned 
2015/07/02 20:02:01 kid1|         0 Invalid entries. 
2015/07/02 20:02:01 kid1|         0 With invalid flags. 
2015/07/02 20:02:01 kid1|   3139356 Objects loaded. 
2015/07/02 20:02:01 kid1|         0 Objects expired. 
2015/07/02 20:02:01 kid1|         0 Objects cancelled. 
2015/07/02 20:02:01 kid1|         2 Duplicate URLs purged. 
2015/07/02 20:02:01 kid1|         2 Swapfile clashes avoided. 
2015/07/02 20:02:01 kid1|   Took 19.76 seconds (158911.29 objects/sec). 
2015/07/02 20:02:01 kid1| Beginning Validation Procedure 
2015/07/02 20:02:01 kid1|   262144 Entries Validated so far. 
2015/07/02 20:02:01 kid1|   524288 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   786432 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   1048576 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   1310720 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   1572864 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   1835008 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   2097152 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   2359296 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   2621440 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   2883584 Entries Validated so far. 
2015/07/02 20:02:02 kid1|   Completed Validation Procedure 
2015/07/02 20:02:02 kid1|   Validated 3139344 Entries 
2015/07/02 20:02:02 kid1|   store_swap_size = 313596844.00 KB 
2015/07/02 20:02:02 kid1| storeLateRelease: released 0 objects 
2015/07/02 20:02:02 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory 
2015/07/02 20:02:02 kid1|       /cache02/cache02/0B/AC/000BAC73 





On Tuesday, June 30, 2015 10:05 AM, Mohammad Shakir <shakirgil at yahoo.com> wrote:
I checked my email which show me that my email address has been blocked by squid-user list so I could not check it.

Now should I upgrade to higher version ?






On Tuesday, June 30, 2015 9:38 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 30/06/2015 4:24 p.m., Mohammad Shakir wrote:

> We are using squid on centon 6.6 64 bit with this option.
> 

<http://bugs.squid-cache.org/show_bug.cgi?id=3483>

Did you see the responses to your post from yesterday?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu Jul  2 15:19:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 03:19:19 +1200
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <423657939.1025678.1435849775055.JavaMail.yahoo@mail.yahoo.com>
References: <55921D3C.30808@treenet.co.nz>
 <1467191445.2188429.1435640703348.JavaMail.yahoo@mail.yahoo.com>
 <423657939.1025678.1435849775055.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55955677.2040208@treenet.co.nz>

On 3/07/2015 3:09 a.m., Mohammad Shakir wrote:
> We are running single instance of squid. after 2 days running squid we got the same error.


Please try the latest snapshot of 3.5. r13857 or later.

Amos



From augusto.gabanzo at ole.com.do  Thu Jul  2 15:29:18 2015
From: augusto.gabanzo at ole.com.do (Augusto Gabanzo)
Date: Thu, 2 Jul 2015 11:29:18 -0400
Subject: [squid-users] New to Squid,
	Foward proxy problems with domain blocks.
Message-ID: <001b01d0b4db$deb49b00$9c1dd100$@gabanzo@ole.com.do>

Hello, as the subject says im new. 

 

Been reading a lot and some examples and i do have a weird problem where i
can't block some domains. First and foremost im using the squid proxy for
windows version 2.7.8 

as thats the only one for windows that works for me the 3.x versions always
deny requests from clients even with the default conf. I've been testing all
this in a production enviroment so ... help me!! please of i will get killed
soon :D.

 

my conf for 2.7.8 is(I modifying one that comes with proxy 3-1):

 

#Modified by Kyi Thar 15 March 2010

http_port 8080

cache_mgr helpdesk at ole.com.do

visible_hostname lotus.hidden

hierarchy_stoplist cgi-bin ?

cache_mem 64 MB

cache_replacement_policy heap LFUDA

cache_dir aufs c:/Squid/cache01 2000 16 256

cache_dir aufs c:/Squid/cache02 2000 16 256

cache_dir aufs c:/Squid/cache03 2000 16 256

cache_access_log c:/Squid/var/logs/access.log

cache_log c:/Squid/var/logs/cache.log

cache_store_log c:/Squid/var/logs/store.log

mime_table c:/Squid/etc/mime.conf

pid_filename c:/Squid/var/logs/squid.pid (this part here i dont know whats
its use as i cant find info about it on the net)

diskd_program c:/Squid/libexec/diskd.exe

unlinkd_program c:/Squid/libexec/unlinkd.exe

logfile_daemon c:/squid/libexec/logfile-daemon.exe

forwarded_for off

via off

httpd_suppress_version_string on

uri_whitespace strip

 

maximum_object_size 524288 KB

maximum_object_size_in_memory 1024 KB

 

#redirect_program c:/usr/local/squidGuard/squidGuard.exe

 

#authenication with Windows server (commented this part as i dont want users
to have to log on once more in the web pages I wasnt able to stop them from
doing so and my boss didnt like the extra hassle)

#auth_param basic program c:/squid/libexec/mswin_auth.exe -O HIDDEN

#auth_param ntlm program c:/squid/libexec/mswin_ntlm_auth.exe

#auth_param ntlm children 5

#auth_param ntlm keep_alive on

 

acl all src all

acl manager proto cache_object

acl localhost src 127.0.0.1/32

acl to_localhost dst 127.0.0.0/8 0.0.0.0/32

acl localnet src 10.0.0.0/8             # RFC1918 possible internal network
(some of my computers are in this range)

acl localnet src 172.16.0.0/12      # RFC1918 possible internal network
(Dont use this range but i will make a DMZ for the servers with it)

acl localnet src 192.168.0.0/16   # RFC1918 possible internal network
(NORMAL range for users)

 

#             catch certain bugs (for example with persistent connections)
and possibly

#             buffer-overflow or denial-of-service attacks.

request_header_max_size 20 KB

reply_header_max_size 20 KB

 

#Limit upload to 2M and download to 10M (trying to stop users from uploading
big files to email sites and fb and download big files  as i only have 6mbps
and 1mbps down/up bandwidth)

request_body_max_size 2048 KB

reply_body_max_size 10485760 deny localnet

 

# compressed (i moddief this part as instead of 0 they had 10080 and instead
of 10080 they had 999999 those times are too big files could stay forever
fresh! inside the cache)

 

refresh_pattern -i \.gz$ 0 90% 10080 

refresh_pattern -i \.cab$ 0 90% 10080 

refresh_pattern -i \.bzip2$ 0 90% 10080 

refresh_pattern -i \.bz2$ 0 90% 10080 

refresh_pattern -i \.gz2$ 0 90% 10080 

refresh_pattern -i \.tgz$ 0 90% 10080 

refresh_pattern -i \.tar.gz$ 0 90% 10080 

refresh_pattern -i \.zip$ 0 90% 10080 

refresh_pattern -i \.rar$ 000 90% 10080 

refresh_pattern -i \.tar$ 0 90% 10080 

refresh_pattern -i \.ace$ 0 90% 10080 

refresh_pattern -i \.7z$ 0 90% 10080 

 

# documents

refresh_pattern -i \.xls$ 0 90% 10080 

refresh_pattern -i \.doc$ 0 90% 10080 

refresh_pattern -i \.xlsx$ 0 90% 10080 

refresh_pattern -i \.docx$ 0 90% 10080 

refresh_pattern -i \.pdf$ 0 90% 10080 

refresh_pattern -i \.ppt$ 0 90% 10080 

refresh_pattern -i \.pptx$ 0 90% 10080 

refresh_pattern -i \.rtf\?$ 0 90% 10080 

 

# multimedia

refresh_pattern -i \.mid$ 0 90% 10080 

refresh_pattern -i \.wav$ 0 90% 10080 

refresh_pattern -i \.viv$ 0 90% 10080 

refresh_pattern -i \.mpg$ 0 90% 10080 

refresh_pattern -i \.mov$ 0 90% 10080 

refresh_pattern -i \.avi$ 0 90% 10080 

refresh_pattern -i \.asf$ 0 90% 10080 

refresh_pattern -i \.qt$ 0 90% 10080 

refresh_pattern -i \.rm$ 0 90% 10080 

refresh_pattern -i \.rmvb$ 0 90% 10080 

refresh_pattern -i \.mpeg$ 0 90% 10080 

refresh_pattern -i \.wmp$ 0 90% 10080 

refresh_pattern -i \.3gp$ 0 90% 10080 

refresh_pattern -i \.mp3$ 0 90% 10080 

refresh_pattern -i \.mp4$ 0 90% 10080 

 

# images

refresh_pattern -i \.gif$ 0 90% 10080 

refresh_pattern -i \.jpg$ 0 90% 10080 

refresh_pattern -i \.png$ 0 90% 10080 

refresh_pattern -i \.jpeg$ 0 90% 10080 

refresh_pattern -i \.bmp$ 0 90% 10080 

refresh_pattern -i \.psd$ 0 90% 10080 

refresh_pattern -i \.ad$ 0 90% 10080 

refresh_pattern -i \.gif\?$ 0 90% 10080 

refresh_pattern -i \.jpg\?$ 0 90% 10080 

refresh_pattern -i \.png\?$ 0 90% 10080 

refresh_pattern -i \.jpeg\?$ 0 90% 10080 

refresh_pattern -i \.psd\?$ 0 90% 10080 

 

# application

refresh_pattern -i \.deb$ 0 90% 10080 

refresh_pattern -i \.rpm$ 0 90% 10080 

refresh_pattern -i \.msi$ 0 90% 10080 

refresh_pattern -i \.exe$ 0 90% 10080 

refresh_pattern -i \.dmg$ 0 90% 10080 

 

# default refresh patterns

refresh_pattern ^ftp: 1440 20% 0 

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0 

 

# if a file ends before finishing sends the quick abort if those parameters
comply ( i kinda forgot why i copied this from tha web )

quick_abort_min 16 KB

quick_abort_max 16 KB

quick_abort_pct 95

 

#ACL to define ports allowed to passthrough Squid

acl SSL_ports port 443

acl Safe_ports port 80                   # http

acl Safe_ports port 84                   # laboratorios cortina

acl Safe_ports port 21                   # ftp

acl Safe_ports port 443                # https

acl Safe_ports port 1025-65535 # unregistered ports

acl Safe_ports port 280                # http-mgmt

acl Safe_ports port 488                # gss-http

acl CONNECT method CONNECT

 

http_access deny manager

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

 

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

 

http_access deny to_localhost

 

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

 

acl fullvideo src "c:/squid/etc/ipfullvideo.sq"  # here is a file with ips
allowed to see youtube and facebook videos , media streaming 

acl bad_url url_regex -i "c:/squid/etc/bad-sites.sq" # .facebook.com
.twitter.com rule to block those sites for users inside ipbloqueada

acl ipbloqueada src 192.168.1.117/32 192.168.1.179/32 192.168.1.170/32
192.168.1.15/32  # ips of 3 users that shouldnt be accessing fb and twitter.

acl bad_ext urlpath_regex -i "c:/squid/etc/extensiones.sq" # rule to block
some file extesions like .avi$, .mpg$ etc stop downloads from them even if
they are smaller than 10MB (this doesn't WORK!)

 

#Media Streams   i try to block streaming here downloaded this from your
site

## MediaPlayer MMS Protocol

acl media rep_mime_type mms

acl mediapr url_regex dvrplayer mediastream ^mms://

## (Squid does not yet handle the URI as a known proto type.)

 

## Active Stream Format (Windows Media Player)

acl media rep_mime_type x-ms-asf

##acl mediapr urlpath_regex \.(afx|asf)(\?.*)?$             #(regex make
squid 2.7.8 to blow up had to comment them)

 

## Flash Video Format

acl media rep_mime_type video/flv video/x-flv

##acl mediapr urlpath_regex \.flv(\?.*)?$                         #(regex
make squid 2.7.8 to blow up had to comment them)

 

## Flash General Media Scripts (Animation)

acl media rep_mime_type application/x-shockwave-flash

##acl mediapr urlpath_regex \.swf(\?.*)?$                       #(regex make
squid 2.7.8 to blow up had to comment them)

 

## Others currently unknown

acl media rep_mime_type ms-hdr

acl media rep_mime_type x-fcs

 

# now we do the reall blocking here

 

http_access allow localnet                                         #let the
network use the proxy

http_access allow localhost                                       #let the
proxy server use itself ??( O_o i dont quite get this part.)

http_access allow manager localhost

 

http_access deny bad_url ipbloqueada               #here i want all the urls
in BAD_URL from the ips IPBLOQUEADA to be denied used to work ... when i
started but now it doesnt i will show a sample of the file at the end

http_access deny bad_ext                                        #block
reading of files with those extensions.

deny_info TCP_RESET bad_ext                                #send a tcp_reset
so they dont know proxy blocked them

http_reply_access deny media !fullvideo           # here i try to deny
access to media to all but those inside fullvideo (doesnt quite work either
youtube loads and works :D) some other streaming are blocked well

##http_access deny mediapr

 

# And finally deny all other access to this proxy

http_access deny all

 

#always_direct allow all                                              # i
feel this part is to let squidguard work, i removed it cuz it blocked
youtube  and many other sites i bet that was because the ads.

 

icon_directory c:/Squid/share/icons

error_directory c:/Squid/share/errors/Spanish

coredump_dir c:/Squid

 

 

##This is bad_sites.sq

.fanfiction.net

.meebo.com

.playboy.com

.myspace.com

.sexo.com

.facebook.com

.twitter.com

.hi5.com

plus.google.com

.identi.li

 

 

## this is extensiones.sq 

 

.mp3$

.exe$

.com$

.bat$

.pif$

.avi$

.mpg$

.zip$

.rar$

.z7$

 

##this is ipfullvideo.sq

 

192.168.1.36

192.168.1.51

192.168.1.67

192.168.1.170

192.168.1.171

192.168.1.185

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150702/d6414e97/attachment.htm>

From shakirgil at yahoo.com  Thu Jul  2 15:29:31 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Thu, 2 Jul 2015 15:29:31 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <55955677.2040208@treenet.co.nz>
References: <55955677.2040208@treenet.co.nz>
Message-ID: <39431952.1086563.1435850971990.JavaMail.yahoo@mail.yahoo.com>

Ok, we will try it today and post our results.



On Thursday, July 2, 2015 8:19 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 3/07/2015 3:09 a.m., Mohammad Shakir wrote:
> We are running single instance of squid. after 2 days running squid we got the same error.


Please try the latest snapshot of 3.5. r13857 or later.


Amos


From squid3 at treenet.co.nz  Thu Jul  2 15:30:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 03:30:32 +1200
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <20150702144158.GB28280@fever.havannah.local>
References: <20150702144158.GB28280@fever.havannah.local>
Message-ID: <55955918.2080204@treenet.co.nz>

On 3/07/2015 2:41 a.m., Danny wrote:
> I am running Debian 8 with Squid3 installed (transparent). However, I would like to know a little more about the "reply_body_max_size" directive. I have read quite a bit about it but none of the discussions on the net fits my criteria ... 

It works as documented at
<http://www.squid-cache.org/Doc/config/reply_body_max_size/>.  If that
does not fit your criteria then its not what you need.


> (Oh yes, squidGuard is also running around my server somewhere doing what it is supposed to do ... I hope ... )
> 
> It is a home setup with the Debian box serving DHCP IP's over wlan0 (which all devices in the house connect to for internet access). 9 laptops, 4 PC's, 7 tablets and 9 SmartPhones (and that is only the kid's stuff fighting for bandwidth supremacy ... ;) ) ... We are all on the same subnet ...
> 
> The problem I have (as with most parents) is to limit the kid's download sizes from all over the net. Where I am we have capped internet and have to pay for more cap. 
> Currently I get 20GB of data every month and by the end of the month I have purchased in excess of 100GB throughout the month which gets very expensive. 
> My son plays games on his PS3 and some of the games (Call of Duty, I think) one player can download another player's in-game recorded video (or something like that) and that eats up the cap.
> 
> Currently my "reply_body_max_size" is set to 20 MB in my efforts to curb downloads and save some bandwidth. 
> However, whenever myself or the wife wants to download or visit youtube I have to change the 20MB limit, restart Squid3, watch youtube, change limit back to 20MB and reload Squid3 again ... which is a pain in the butt ...
> 
> Currently my ACL's look like this:
> 
> acl localnet src 10.0.0.0/24
> acl localnet_dad_laptop 10.0.0.10
> acl localnet_dad_smartphone 10.0.0.11
> acl localnet_mom_laptop 10.0.0.12
> acl localnet_mom_smartphone 10.0.0.13
> acl localnet_son_laptop 10.0.0.14
> acl localnet_son_smartphone 10.0.0.15
> acl localnet_son_tablet 10.0.0.16
> 
> ---and so it goes on for all the other devices---
> 
> http_access allow localnet

NOTE: No http_access ACLs controlling 10.0.0.0/24 have any effect below
this one that allows them all access to use the proxy.

> http_access allow localnet_dad_laptop
> http_access allow localnet_dad_smartphone
> http_access allow localnet_mom_laptop
> http_access allow localnet_mom_smartphone
> http_access allow localnet_son_laptop
> http_access allow localnet_son_smartphone
> http_access allow localnet_son_tablet
> 
> ---and so it goes on for all the other devices---
> 
> How can I allow mom and dad unlimited download sizes but limit download sizes for my kids (son, daughter and daughter) and all the kid's friends that visit and sleep over?

By applying ACLs for the kids on the reply_body_max_size directive lines
setting the sizes to use for them. Like so:
  reply_body_max_size 50 KB localnet_son_smartphone

Amos



From augusto.gabanzo at ole.com.do  Thu Jul  2 15:37:12 2015
From: augusto.gabanzo at ole.com.do (Augusto Gabanzo)
Date: Thu, 2 Jul 2015 11:37:12 -0400
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <20150702144158.GB28280@fever.havannah.local>
References: <20150702144158.GB28280@fever.havannah.local>
Message-ID: <002601d0b4dc$f97f03c0$ec7d0b40$@gabanzo@ole.com.do>

i belive you can do something like this:

reply_body_max_size X deny localnet !localnet_dad_laptop !localnet_mom_laptop 

that should if im not wrong deny all ips but those ending in 10 and 12

-----Mensaje original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de Danny
Enviado el: jueves, 02 de julio de 2015 10:42 a. m.
Para: squid-users at lists.squid-cache.org
Asunto: [squid-users] reply_body_max_size question

I am running Debian 8 with Squid3 installed (transparent). However, I would like to know a little more about the "reply_body_max_size" directive. I have read quite a bit about it but none of the discussions on the net fits my criteria ... 
(Oh yes, squidGuard is also running around my server somewhere doing what it is supposed to do ... I hope ... )

It is a home setup with the Debian box serving DHCP IP's over wlan0 (which all devices in the house connect to for internet access). 9 laptops, 4 PC's, 7 tablets and 9 SmartPhones (and that is only the kid's stuff fighting for bandwidth supremacy ... ;) ) ... We are all on the same subnet ...

The problem I have (as with most parents) is to limit the kid's download sizes from all over the net. Where I am we have capped internet and have to pay for more cap. 
Currently I get 20GB of data every month and by the end of the month I have purchased in excess of 100GB throughout the month which gets very expensive. 
My son plays games on his PS3 and some of the games (Call of Duty, I think) one player can download another player's in-game recorded video (or something like that) and that eats up the cap.

Currently my "reply_body_max_size" is set to 20 MB in my efforts to curb downloads and save some bandwidth. 
However, whenever myself or the wife wants to download or visit youtube I have to change the 20MB limit, restart Squid3, watch youtube, change limit back to 20MB and reload Squid3 again ... which is a pain in the butt ...

Currently my ACL's look like this:

acl localnet src 10.0.0.0/24
acl localnet_dad_laptop 10.0.0.10
acl localnet_dad_smartphone 10.0.0.11
acl localnet_mom_laptop 10.0.0.12
acl localnet_mom_smartphone 10.0.0.13
acl localnet_son_laptop 10.0.0.14
acl localnet_son_smartphone 10.0.0.15
acl localnet_son_tablet 10.0.0.16

---and so it goes on for all the other devices---

http_access allow localnet
http_access allow localnet_dad_laptop
http_access allow localnet_dad_smartphone http_access allow localnet_mom_laptop http_access allow localnet_mom_smartphone http_access allow localnet_son_laptop http_access allow localnet_son_smartphone http_access allow localnet_son_tablet

---and so it goes on for all the other devices---

How can I allow mom and dad unlimited download sizes but limit download sizes for my kids (son, daughter and daughter) and all the kid's friends that visit and sleep over?

Thank You

Danny
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jul  2 16:18:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 04:18:21 +1200
Subject: [squid-users] New to Squid,
 Foward proxy problems with domain blocks.
In-Reply-To: <001b01d0b4db$deb49b00$9c1dd100$@gabanzo@ole.com.do>
References: <001b01d0b4db$deb49b00$9c1dd100$@gabanzo@ole.com.do>
Message-ID: <5595644D.3080005@treenet.co.nz>

On 3/07/2015 3:29 a.m., Augusto Gabanzo wrote:
> Hello, as the subject says im new. 
> 
>  
> 
> Been reading a lot and some examples and i do have a weird problem where i
> can't block some domains. First and foremost im using the squid proxy for
> windows version 2.7.8 
> 
> as thats the only one for windows that works for me the 3.x versions always
> deny requests from clients even with the default conf. I've been testing all
> this in a production enviroment so ... help me!! please of i will get killed
> soon :D.
> 
>  
> 
> my conf for 2.7.8 is(I modifying one that comes with proxy 3-1):

Don't. 2.7 contains no built-in defaults where 3.x does. The .conf file
contents need to be very different.


> pid_filename c:/Squid/var/logs/squid.pid (this part here i dont know whats
> its use as i cant find info about it on the net)

<http://www.squid-cache.org/Doc/config/pid_filename/>

The PID is used for sending signals to the Squid process/service.

> 
> #Limit upload to 2M and download to 10M (trying to stop users from uploading
> big files to email sites and fb and download big files  as i only have 6mbps
> and 1mbps down/up bandwidth)
> 
> request_body_max_size 2048 KB
> 
> reply_body_max_size 10485760 deny localnet
> 
>  
> 
> # compressed (i moddief this part as instead of 0 they had 10080 and instead
> of 10080 they had 999999 those times are too big files could stay forever
> fresh! inside the cache)

"forever" in HTTP is "no more than 68 years". In 2.7 thats 1 year.

And no, these lines only affect objects with are completely lacking
Cache-Control values. Most traffic has such controls and Squid obeys them.

Also, each refresh_pattern line has to be matched against a request
individually. Repeating many lines causes a lot of work to be done for
each request. Better to combine the patterns manually.


> 
> acl fullvideo src "c:/squid/etc/ipfullvideo.sq"  # here is a file with ips
> allowed to see youtube and facebook videos , media streaming 
> 
> acl bad_url url_regex -i "c:/squid/etc/bad-sites.sq" # .facebook.com
> .twitter.com rule to block those sites for users inside ipbloqueada

So why is it a slow regex and not a fast dstdomain ?

> 
> acl ipbloqueada src 192.168.1.117/32 192.168.1.179/32 192.168.1.170/32
> 192.168.1.15/32  # ips of 3 users that shouldnt be accessing fb and twitter.
> 
> acl bad_ext urlpath_regex -i "c:/squid/etc/extensiones.sq" # rule to block
> some file extesions like .avi$, .mpg$ etc stop downloads from them even if
> they are smaller than 10MB (this doesn't WORK!)
> 

The regex syntax mentioned assumes the URL ends with the file extension.
That is fairly uncommon. Most of the download sites these days the ext
is some dynamic script like .php or .asp. Using the content-type and
content-disposition headers to deliver the filename details.


> 
> http_access allow localnet                                         #let the
> network use the proxy
> 
> http_access allow localhost                                       #let the
> proxy server use itself ??( O_o i dont quite get this part.)


Lets other software on the Squid server us it. Yes that includes the
proxy looping traffic back at tself, but the Via header protects against
that begin a problem.


> 
> http_access allow manager localhost
> 
>  
> 
> http_access deny bad_url ipbloqueada               #here i want all the urls
> in BAD_URL from the ips IPBLOQUEADA to be denied used to work ... when i
> started but now it doesnt i will show a sample of the file at the end

If I'm reding that comment on the ipbloqueada definition you are
assuming that Facebook, Twitter etc are still using plaintext HTTP
through the proxy. They dont. These days they use TLS with SPDY or
HTTP/2 or QUIC or HTTPS.


> 
> http_access deny bad_ext                                        #block
> reading of files with those extensions.
> 
> deny_info TCP_RESET bad_ext                                #send a tcp_reset
> so they dont know proxy blocked them
> 
> http_reply_access deny media !fullvideo           # here i try to deny
> access to media to all but those inside fullvideo (doesnt quite work either
> youtube loads and works :D) some other streaming are blocked well
> 

YT is HTTPS not HTTP now.


> 
> # And finally deny all other access to this proxy
> 
> http_access deny all
> 
>  
> 
> #always_direct allow all                                              # i
> feel this part is to let squidguard work, i removed it cuz it blocked
> youtube  and many other sites i bet that was because the ads.
> 

always_direct has no effect unless cache_peer directive is used. In
which case it makes the cache_peer not be used for traffic.

Amos



From mcsnv96 at afo.net  Thu Jul  2 17:39:14 2015
From: mcsnv96 at afo.net (Mike)
Date: Thu, 02 Jul 2015 12:39:14 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <slrnmp9ood.qvn.stu@naiad.spacehopper.org>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
 <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <5594033D.6030307@afo.net> <55941123.4080404@urlfilterdb.com>
 <55944583.6030309@afo.net> <slrnmp9ood.qvn.stu@naiad.spacehopper.org>
Message-ID: <55957742.7080404@afo.net>

We have a DNS guru on staff and editing the resolv.conf in this manner 
does not work (we tested it to make sure). Looks like we are using an 
older desktop to setup a basic DNS server and then point squid to redirect.



Mike


On 7/2/2015 2:06 AM, Stuart Henderson wrote:
> On 2015-07-01, Mike <mcsnv96 at afo.net> wrote:
>> This is a proxy server, not a DNS server, and does not connect to a DNS
>> server that we have any control over... The primary/secondary DNS is
>> handled through the primary host (Cox) for all of our servers so we do
>> not want to alter it for all several hundred servers, just these 4
>> (maybe 6).
>> I was originally thinking of modifying the resolv.conf but again that is
>> internal DNS used by the server itself. The users will have their own
>> DNS settings causing it to either ignore our settings, or right back to
>> the "Website cannot be displayed" errors due to the DNS loop.
> resolv.conf would work, or you can use dns_nameservers in squid.conf and
> point just squid (if you want) to a private resolver configured to hand
> out the forcesafesearch address.
>
> When a proxy is used, the client defers name resolution to the proxy, you
> don't need to change DNS on client machines to do this.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From mynixmail at gmail.com  Thu Jul  2 17:46:15 2015
From: mynixmail at gmail.com (Danny)
Date: Thu, 2 Jul 2015 19:46:15 +0200
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <55955918.2080204@treenet.co.nz>
References: <20150702144158.GB28280@fever.havannah.local>
 <55955918.2080204@treenet.co.nz>
Message-ID: <20150702174615.GA18066@fever.havannah.local>

> It works as documented at
> <http://www.squid-cache.org/Doc/config/reply_body_max_size/>.  If that
> does not fit your criteria then its not what you need.

I am aware of that, I was just a little unsure how to split the different dowload
sizes amongst all the different users.
 
> > http_access allow localnet
> 
> NOTE: No http_access ACLs controlling 10.0.0.0/24 have any effect below
> this one that allows them all access to use the proxy.
> 
> > http_access allow localnet_dad_laptop
> > http_access allow localnet_dad_smartphone
> > http_access allow localnet_mom_laptop
> > http_access allow localnet_mom_smartphone
> > http_access allow localnet_son_laptop
> > http_access allow localnet_son_smartphone
> > http_access allow localnet_son_tablet

Thank you ... did not know that ... I was under the impression every user i.e
device needed to be granted http_access ...

> By applying ACLs for the kids on the reply_body_max_size directive lines
> setting the sizes to use for them. Like so:
>   reply_body_max_size 50 KB localnet_son_smartphone

O.k ... so currently I have:
reply_body_max_size 20 MB

If I combine your suggestion and Augusto Gabanzo's (who suggested something a little different) can I then do something like this:
##########
reply_body_max_size 0 MB !localnet_son_laptop !localnet_son_smartphone !localnet_son_tablet
reply_body_max_size 5 MB localnet_son_laptop localnet_son_smartphone localnet_son_tablet (// Or must each device get it's own limit?)


From bqmackay at gmail.com  Thu Jul  2 21:07:36 2015
From: bqmackay at gmail.com (Byron Mackay)
Date: Thu, 2 Jul 2015 15:07:36 -0600
Subject: [squid-users] Setting logfile_daemon
Message-ID: <CADfO2BDPx0Jz65AOPnhumJ2Kz=jX0Ey12VduQCKwH-JJJcUx4g@mail.gmail.com>

I'm running Squid 3.3.8 on Ubuntu inside a Docker container and I want to
add a custom logger. I want to keep what the current logger is doing and
append a few things, so I simply copied the default logger's source (
http://www.squid-cache.org/Doc/code/log__file__daemon_8cc_source.html) and
put it into another file. Then in my config I set that file to be the
daemon using logfile_daemon and setting the path to the file. When I spin
up the server, I get the following:

2015/07/02 20:24:06| logfileHandleWrite: daemon:/var/log/squid3/access.log:
error writing ((32) Broken pipe) 2015/07/02 20:24:06| Closing HTTP port
[::]:3128 2015/07/02 20:24:06| storeDirWriteCleanLogs: Starting...
2015/07/02 20:24:06| Finished. Wrote 0 entries. 2015/07/02 20:24:06| Took
0.00 seconds ( 0.00 entries/sec). FATAL: I don't handle this error well!
2015/07/02 20:24:06| Closing Pinger socket on FD 18

I've tried a number of things (including making a skeleton version with
just the methods and no content) and looked all over online and in the
Squid 3.1 beginner book (I know, out of date, but still a good reference),
but I haven't found a thing. Am I missing something obvious here? I feel
like it doesn't like something inside my logger, but I can't be sure based
on the error. Attached is my config file. The line of interest is
"logfile_daemon /usr/lib/squid3/my_logger".
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150702/84133b5d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 2797 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150702/84133b5d/attachment.obj>

From squid3 at treenet.co.nz  Fri Jul  3 06:00:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 18:00:07 +1200
Subject: [squid-users] Setting logfile_daemon
In-Reply-To: <CADfO2BDPx0Jz65AOPnhumJ2Kz=jX0Ey12VduQCKwH-JJJcUx4g@mail.gmail.com>
References: <CADfO2BDPx0Jz65AOPnhumJ2Kz=jX0Ey12VduQCKwH-JJJcUx4g@mail.gmail.com>
Message-ID: <559624E7.1050306@treenet.co.nz>

On 3/07/2015 9:07 a.m., Byron Mackay wrote:
> I'm running Squid 3.3.8 on Ubuntu inside a Docker container and I want to
> add a custom logger. I want to keep what the current logger is doing and
> append a few things, so I simply copied the default logger's source (
> http://www.squid-cache.org/Doc/code/log__file__daemon_8cc_source.html) and
> put it into another file. Then in my config I set that file to be the
> daemon using logfile_daemon and setting the path to the file. When I spin
> up the server, I get the following:


Did you compile the new helper?

What happens if you run it manually?

Amos



From Jiri.Wetter at DixonsCarphone.com  Fri Jul  3 08:36:18 2015
From: Jiri.Wetter at DixonsCarphone.com (Jiri Wetter)
Date: Fri, 3 Jul 2015 08:36:18 +0000
Subject: [squid-users] Dynamic content
Message-ID: <VI1PR06MB1456567C50C6A92D15D2DBC18D960@VI1PR06MB1456.eurprd06.prod.outlook.com>

Hello,

I have problem with caching of dynamic content.

I verified conditions which have to be met in order to cache dynamic content here: http://wiki.squid-cache.org/SquidFaq/InnerWorkings#How_come_some_objects_do_not_get_cached.3F
and also check by online tool  https://redbot.org/
Result is that my page can be cached but it doesn't work with my current settings. Therefore I would like to ask you for help with my configuration.

OS: Ubuntu 12.04.5 LTS
Squid Cache: Version 3.1.19
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--with-cppunit-basedir=/usr' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth=basic,digest,ntlm,negotiate' '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm,' '--enable-digest-auth-helpers=ldap,password' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group' '--enable-arp-acl' '--enable-esi' '--enable-zph-qos' '--enable-wccpv2' '--disable-translation' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Wformat-security -Werror=format-security' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Wformat-security -Werror=format-security' --with-squid=/build/buildd/squid3-3.1.19

Works: http://wiki.squid-cache.org/ConfigExamples/DynamicContent?action=fullsearch&context=180&value=test&titlesearch=Titles
Works: http://pechal.net/?q=apple&sp_staged=1&sp_cs=UTF-8
Doesn't work: http://sony_playstation_dev.guided.lon5.atomz.com/?q=sony&sp_staged=1&sp_cs=UTF-8 - Why?


http_port 3128
coredump_dir /var/spool/squid3
httpd_suppress_version_string on
visible_hostname squid
max_open_disk_fds 960
log_mime_hdrs on
log_fqdn on
buffered_logs on
logformat combined %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh
access_log /var/log/squid3/access.log combined
cache allow all
cache_store_log /var/log/squid3/store.log
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern .            0 20% 4320

acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl SSL_ports port 443
acl Safe_ports port 80 21 443 70 210 1025-65535 280 488 591 777
acl CONNECT method CONNECT

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost


acl WEB_FRONT src 10.10.40.0/24
acl TEST .pechal.net

http_access allow WEB_FRONT TEST

http_access deny all



Thank you for help!

[cid:image001.png at 01CFAB51.9D803700]
Ji?? Wetter
IT Infrastructure System Administrator/Technical Leader
Dixons Retail SSC s.r.o. part of Dixons Carphone plc.

Trinity Office Center
Trnita 491/5, 602 00, Brno, Czech Republic
T:   +420 731 518 233
W: dixonscarphonegroup.com

::DISCLAIMER::
________________________________________________________________________________________________________________
Confidentiality Notice from Dixons Carphone plc (registered in England & Wales No.07105905) of 1 Portal Way, London, W3 6RS ("Dixons Carphone"). The information contained in this e-mail and any attachments may be legally privileged, proprietary and/or confidential. If you received this e-mail in error, please notify the sender by return, permanently delete the e-mail and destroy all hard copies immediately. No warranty is made as to the completeness or accuracy of the information contained in this e-mail. Opinions, conclusions and statements of intent in this e-mail are those of the sender and will not bind any Dixons Carphone group company (Dixons Carphone Group) unless confirmed by an authorised representative independently of this e-mail. We do not accept responsibility for viruses; you must scan for these. E-mails sent to and from Dixons Carphone Group are routinely monitored for record keeping, quality control, training purposes, to ensure regulatory compliance and to prevent viruses and unauthorised use of our computer systems. The Carphone Warehouse Limited (registered in England & Wales No.02142673) is a member of the Dixons Carphone Group and is authorised and regulated by the Financial Conduct Authority.
________________________________________________________________________________________________________________
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150703/de01dafc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11513 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150703/de01dafc/attachment.png>

From squid3 at treenet.co.nz  Fri Jul  3 10:01:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 03 Jul 2015 22:01:50 +1200
Subject: [squid-users] Dynamic content
In-Reply-To: <VI1PR06MB1456567C50C6A92D15D2DBC18D960@VI1PR06MB1456.eurprd06.prod.outlook.com>
References: <VI1PR06MB1456567C50C6A92D15D2DBC18D960@VI1PR06MB1456.eurprd06.prod.outlook.com>
Message-ID: <55965D8E.70706@treenet.co.nz>

On 3/07/2015 8:36 p.m., Jiri Wetter wrote:
> Hello,
> 
> I have problem with caching of dynamic content.
> 
> I verified conditions which have to be met in order to cache dynamic content here: http://wiki.squid-cache.org/SquidFaq/InnerWorkings#How_come_some_objects_do_not_get_cached.3F
> and also check by online tool  https://redbot.org/
> Result is that my page can be cached but it doesn't work with my current settings. Therefore I would like to ask you for help with my configuration.
> 
> OS: Ubuntu 12.04.5 LTS
> Squid Cache: Version 3.1.19
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--with-cppunit-basedir=/usr' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth=basic,digest,ntlm,negotiate' '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm,' '--enable-digest-auth-helpers=ldap,password' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-extern
al-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group' '--enable-arp-acl' '--enable-esi' '--enable-zph-qos' '--enable-wccpv2' '--disable-translation' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Wformat-security -Werror=format-security' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Wformat-security -Werror=format-security' --with-squid=/build/buildd/squid3-3.1.19
> 
> Works: http://wiki.squid-cache.org/ConfigExamples/DynamicContent?action=fullsearch&context=180&value=test&titlesearch=Titles

Cache-Control:max-age and Expires place explicit storage time on the object.

> Works: http://pechal.net/?q=apple&sp_staged=1&sp_cs=UTF-8

Date + Last-Modified allow heuristic age estimation to provide an
estimate storage time for the object.


> Doesn't work: http://sony_playstation_dev.guided.lon5.atomz.com/?q=sony&sp_staged=1&sp_cs=UTF-8 - Why?

Lack of all headers (Cache-Control, Expires, Last-Modified, Age)
extending/placing bounds on object freshness beyond current time point
(Date) makes it stale immediately.

This is a special case for dynamic content due to its nature of being
re-generated on demand / dynamically each request.

Static content with the same headers Squid can safely assume up to 1
year freshness (but default is 1 week to avoid problems).

Amos



From vdoctor at neuf.fr  Fri Jul  3 12:05:26 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 3 Jul 2015 05:05:26 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435840988272-4672022.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
Message-ID: <1435925126078-4672041.post@n4.nabble.com>

Hi Amos,
Can we expect a workaround to allow the object to the cache if the dns
record is corrected by Squid instead that having an ORIGINAL_DST ?
If Squid corrects the request, it mean the URL will be good, so we should be
able to cache the object 

Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672041.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul  3 12:59:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 00:59:22 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435925126078-4672041.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com>
Message-ID: <5596872A.7050508@treenet.co.nz>

On 4/07/2015 12:05 a.m., Stakres wrote:
> Hi Amos,
> Can we expect a workaround to allow the object to the cache if the dns
> record is corrected by Squid instead that having an ORIGINAL_DST ?
> If Squid corrects the request, it mean the URL will be good, so we should be
> able to cache the object 

Any ideas on how Squid could "correct" the request when all data comes
from the untrusted client?

I am still looking for ways to get per-client caching to operate
cleanly. For these (and Cache-Control:private objects) to be stored for
re-use by the one client.

Amos



From bqmackay at gmail.com  Fri Jul  3 13:20:13 2015
From: bqmackay at gmail.com (Byron Mackay)
Date: Fri, 3 Jul 2015 07:20:13 -0600
Subject: [squid-users] Setting logfile_daemon
Message-ID: <CADfO2BCUMrjOAxhK6YN8cg+o=jK_YSgxMP4Ex6b+q0AcYFuYnQ@mail.gmail.com>

> Did you compile the new helper?
> What happens if you run it manually?

That was it! I admit that I'm new to the space, but I still should have
tried to understand basic c++ first. I got it to run manually on my Mac,
but it looks like I need to compile it on Ubuntu instead of just throwing
it up there from my Mac. I can figure that one out though.

Thank you!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150703/dd83cb1d/attachment.htm>

From vdoctor at neuf.fr  Fri Jul  3 13:21:42 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 3 Jul 2015 06:21:42 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <5596872A.7050508@treenet.co.nz>
References: <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
Message-ID: <1435929702099-4672044.post@n4.nabble.com>

Amos,
You told the Squid will check the original dns from the headers, then it'll
do its own dns resolution to verify they both match.
So, if no match, Squid does the request to internet based on the dns it
found.
If I'm right, that the current way, correct ?

What we could do is the same way but as Squid has downloaded the object
based on its dns records, it means the object is correct, the right one. So,
keep all details from Squid job and push the object to the cache (if
cacheable).

user request -> squid checks the dns is ok (corrects it if needed) -> squid
download the right object and cache.
user request -> squid checks the dns is ok (corrects it if needed) -> squid
pushs from its cache.

Again, if Squid requests the right object based on its dns requests, it'll
deliver to clients the good one.
So, we should not see ORIGINAL_DST anymore...

And, when I see the archi Yuri must to do to avoid ORIGINAL_DST, I'm sure
all Squid users will be happy 

Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672044.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul  3 15:25:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 03:25:41 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435929702099-4672044.post@n4.nabble.com>
References: <1425453570201-4670194.post@n4.nabble.com>
 <54F6D2AC.6050203@treenet.co.nz> <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
 <1435929702099-4672044.post@n4.nabble.com>
Message-ID: <5596A975.2060903@treenet.co.nz>

On 4/07/2015 1:21 a.m., Stakres wrote:
> Amos,
> You told the Squid will check the original dns from the headers, then it'll
> do its own dns resolution to verify they both match.
> So, if no match, Squid does the request to internet based on the dns it
> found.
> If I'm right, that the current way, correct ?

Depends on what you mean by "it found".

ORIGINAL_DST comes from TCP packet headers, which cannot be forged
without the packets going astray. Squid trusts it when in doubt.

Squid own DNS lookup is for the HTTP Host header value. To compare
against the TCP value. Host can be trivially forged. So neither Host nor
the DNS resulting from it can be trusted when in doubt.

> 
> What we could do is the same way but as Squid has downloaded the object
> based on its dns records, it means the object is correct, the right one. So,
> keep all details from Squid job and push the object to the cache (if
> cacheable).

When there is doubt about what server is correct there is no "right"
object. Squid relays the request to the place the client would have
reached had the proxy not been intercepting the traffic (ORIGINAL_DST).
Then prevents the unreliable object being given to other clients (cached).


There does seem to be one bug in that Squid will not always HIT on
existing cache content for the requested URL. Any help finding and
fixing that.


> 
> user request -> squid checks the dns is ok (corrects it if needed) -> squid
> download the right object and cache.
> user request -> squid checks the dns is ok (corrects it if needed) -> squid
> pushs from its cache.
> 
> Again, if Squid requests the right object based on its dns requests, it'll
> deliver to clients the good one.
> So, we should not see ORIGINAL_DST anymore...

Thats the CVE-2009-0801 problem. Whenever the Host header DNS is used
the proxy and all other clients fetching the cached URL from it, are
subject to malicious alterations made to that header.
Thus its only near-trustworthy when the DNS results contain the TCP dst-IP.

We let the request through to the ORIGINAL_DST to reduce penalty on the
client. But caching without the trust is going a bit too far.


Amos


From squid3 at treenet.co.nz  Fri Jul  3 15:27:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 03:27:51 +1200
Subject: [squid-users] Setting logfile_daemon
In-Reply-To: <CADfO2BCUMrjOAxhK6YN8cg+o=jK_YSgxMP4Ex6b+q0AcYFuYnQ@mail.gmail.com>
References: <CADfO2BCUMrjOAxhK6YN8cg+o=jK_YSgxMP4Ex6b+q0AcYFuYnQ@mail.gmail.com>
Message-ID: <5596A9F7.9040809@treenet.co.nz>

On 4/07/2015 1:20 a.m., Byron Mackay wrote:
>> Did you compile the new helper?
>> What happens if you run it manually?
> 
> That was it! I admit that I'm new to the space, but I still should have
> tried to understand basic c++ first. I got it to run manually on my Mac,
> but it looks like I need to compile it on Ubuntu instead of just throwing
> it up there from my Mac. I can figure that one out though.

Right. Any compiled code will only run on machines with the same CPU
type and software environment. Mac vs Ubuntu are just too different even
if running on the same machine/CPUs.

Amos



From squid3 at treenet.co.nz  Fri Jul  3 15:35:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 03:35:23 +1200
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <20150702174615.GA18066@fever.havannah.local>
References: <20150702144158.GB28280@fever.havannah.local>
 <55955918.2080204@treenet.co.nz>
 <20150702174615.GA18066@fever.havannah.local>
Message-ID: <5596ABBB.4080700@treenet.co.nz>

On 3/07/2015 5:46 a.m., Danny wrote:
>> It works as documented at
>> <http://www.squid-cache.org/Doc/config/reply_body_max_size/>.  If that
>> does not fit your criteria then its not what you need.
> 
> I am aware of that, I was just a little unsure how to split the different dowload
> sizes amongst all the different users.
>  
>>> http_access allow localnet
>>
>> NOTE: No http_access ACLs controlling 10.0.0.0/24 have any effect below
>> this one that allows them all access to use the proxy.
>>
>>> http_access allow localnet_dad_laptop
>>> http_access allow localnet_dad_smartphone
>>> http_access allow localnet_mom_laptop
>>> http_access allow localnet_mom_smartphone
>>> http_access allow localnet_son_laptop
>>> http_access allow localnet_son_smartphone
>>> http_access allow localnet_son_tablet
> 
> Thank you ... did not know that ... I was under the impression every user i.e
> device needed to be granted http_access ...

They do. But not necessarily individually. The /24 does all IPs in the
subnetwork as a group.

You can also list multiple IPs and/or subnets in one ACL name. That
helps fixing the below...


> 
>> By applying ACLs for the kids on the reply_body_max_size directive lines
>> setting the sizes to use for them. Like so:
>>   reply_body_max_size 50 KB localnet_son_smartphone
> 
> O.k ... so currently I have:
> reply_body_max_size 20 MB
> 
> If I combine your suggestion and Augusto Gabanzo's (who suggested something a little different) can I then do something like this:
> ##########
> reply_body_max_size 0 MB !localnet_son_laptop !localnet_son_smartphone !localnet_son_tablet
> reply_body_max_size 5 MB localnet_son_laptop localnet_son_smartphone localnet_son_tablet (// Or must each device get it's own limit?)

The ACLs on a line are AND'd together. Better to make one ACL that
matches all the IPs for the user you want to limit.

Amos


From vdoctor at neuf.fr  Fri Jul  3 15:21:48 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 3 Jul 2015 08:21:48 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <5596A975.2060903@treenet.co.nz>
References: <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
 <1435929702099-4672044.post@n4.nabble.com> <5596A975.2060903@treenet.co.nz>
Message-ID: <1435936908218-4672048.post@n4.nabble.com>

Amos,
OK, got your points.

What I don't understand is:
- The dns records do not match. Squid does the dns request by itself,
downloads the object, delivers it to the client and flags with an
ORIGINAL_DST, right ?
- Same request from another client, same way, it'll be the same object and
flagged ORIGINAL_DST too.
- Again and again... each time the same fresh object...
- Why do we repeat the same action if we deliver the same object each time ?
it makes me crazy... 

Here, I mean by using "*client_dst_passthru off*" and "*host_verify_strict
off*".
I understand the "host_verify_strict on" must act as you explain, no
problem.


Squid re-checks the DNS as there is an issue, downloads and delivers the
object. If Squid delivers the object it should be able to cache it with the
"*client_dst_passthru off*" and "*host_verify_strict off*".

I agree Squid must respect the CVE-2009-0801 but you/we should deal nicely
with and not just applying it...

The right way should be:
Squid think the object is OK to be delivered ?
Yes: deliver it and cache it.
No: block it and don't cache it.

See what I mean ?
(Sorry to be boring with this topic but it's highly important...).

Fred.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672048.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul  3 16:38:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 04:38:50 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435936908218-4672048.post@n4.nabble.com>
References: <1428307801410-4670629.post@n4.nabble.com>
 <1435818735040-4672013.post@n4.nabble.com> <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
 <1435929702099-4672044.post@n4.nabble.com> <5596A975.2060903@treenet.co.nz>
 <1435936908218-4672048.post@n4.nabble.com>
Message-ID: <5596BA9A.9070401@treenet.co.nz>

On 4/07/2015 3:21 a.m., Stakres wrote:
> Amos,
> OK, got your points.
> 
> What I don't understand is:
> - The dns records do not match. Squid does the dns request by itself,

Reverse those two.

> downloads the object,

... from the ORIGINAL_DST (*not* its own DNS lookup)

> delivers it to the client and flags with an
> ORIGINAL_DST, right ?
> - Same request from another client, same way, it'll be the same object and
> flagged ORIGINAL_DST too.
> - Again and again... each time the same fresh object...
> - Why do we repeat the same action if we deliver the same object each time ?
> it makes me crazy... 

Not the same object. Different ORIGINAL_DST. Thats what ORIGINAL_DST
means ... original client dst-IP.


> 
> Here, I mean by using "*client_dst_passthru off*" and "*host_verify_strict
> off*".
> I understand the "host_verify_strict on" must act as you explain, no
> problem.

host_verify_strict applies the DNS lookup tests on more traffic than
just intercepted. Also rejecting client requests instead of allowing
through to the original dst-IP.

> 
> Squid re-checks the DNS as there is an issue, downloads and delivers the
> object. If Squid delivers the object it should be able to cache it with the
> "*client_dst_passthru off*" and "*host_verify_strict off*".
> 
> I agree Squid must respect the CVE-2009-0801 but you/we should deal nicely
> with and not just applying it...
> 
> The right way should be:
> Squid think the object is OK to be delivered ?
> Yes: deliver it and cache it.
> No: block it and don't cache it.

That is "host_verify_strict on" behaviour.

Tried exactly that for most of the 3.2 beta series. With many complaints
about rejecting traffic. As you noted a good 20-30% of traffic fails the
verify. A lot of Google and Akamai hosted sites, some Facebook traffic.

So now we have the third alternative...
  No: allow through and dont cache it.

Which is "host_verify_strict off".


Amos


From dan at djph.net  Fri Jul  3 18:08:49 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 3 Jul 2015 18:08:49 +0000 (UTC)
Subject: [squid-users] Force LDAP groups to de-authenticate?
Message-ID: <mn6j3h$12d$1@ger.gmane.org>

I'm setting up a squid proxy with LDAP user/group authentication, and so 
far have been able to sort out the problems I've run into with a little 
help from google and caches of the various squid mailing lists. 

Currently, it's in a mostly working state for nearly everything (i.e. 
user authentication, allowed/blocked based on what group a user belongs 
to, client pc auto-updates, etc.).  However, I can't figure out how to 
force a user to re-authenticate after a set interval of time (say 30 
mintues).


Essentially, the idea is that the "less-privileged" users (i.e. the 
students) can get to the sites that they need for their day-to-day school 
work, but that their permissions should be able to be elevated for a set 
amount of time in the event the teacher deems it OK.  

Right or wrong, the administration doesn't want to go with one of the 
"big boys" in web filters, so I need to kick the users and force a re-
auth, as this is for a school environment. It's small (only 10-15 
students at one time), but the students have already figured their way 
around the previous filter that was installed before my time.


I know closing the browser clears out all the authentication tokens ... 
but hoping there's a way I can do this from the backend so there's no 
need to play those "okay, now close all your browsers" type games if a 
student gets the elevated permissions.


Leads have pointed me to 

 - auth_param basic credentials_ttl <N> minutes

 - authenticate_ttl <N> minutes

 - authenticate_cache_garbage_interval <N> minutes

Though I don't seem to be able to grasp the concept of getting them to do 
what I want (if it's possible)


Thanks!




From shakirgil at yahoo.com  Fri Jul  3 19:31:09 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Fri, 3 Jul 2015 19:31:09 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <39431952.1086563.1435850971990.JavaMail.yahoo@mail.yahoo.com>
References: <55955677.2040208@treenet.co.nz>
 <39431952.1086563.1435850971990.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1922642738.1824616.1435951869931.JavaMail.yahoo@mail.yahoo.com>

After 24hours my squid performance which is not as much as good. should I change to get better performance.

Squid Object Cache: Version 3.5.5-20150701-r13857 
Build Info: 
Service Name: squid 
Start Time:     Thu, 02 Jul 2015 15:54:20 GMT 
Current Time:   Fri, 03 Jul 2015 19:12:30 GMT 
Connection information for squid: 
Number of clients accessing cache:      2797 
Number of HTTP requests received:       11750500 
Number of ICP messages received:        0 
Number of ICP messages sent:    0 
Number of queued ICP replies:   0 
Request failure ratio:   0.00 
Average HTTP requests per minute since start:   7173.0 
Average ICP messages per minute since start:    0.0 
Select loop called: 300258675 times, 0.327 ms avg 
Cache information for squid: 
Hits as % of all requests:      5min: 15.1%, 60min: 15.0% 
Hits as % of bytes sent:        5min: 9.1%, 60min: 10.3% 
Memory hits as % of hit requests:       5min: 25.0%, 60min: 21.8% 
Disk hits as % of hit requests: 5min: 58.2%, 60min: 55.8% 
Storage Swap size:      132639936 KB 
Storage Swap capacity:   3.4% used, 96.6% free 
Storage Mem size:       128416 KB 
Storage Mem capacity:   98.0% used,  2.0% free 
Mean Object Size:       60.20 KB 
Requests given to unlinkd:      0 
Median Service Times (seconds)  5 min    60 min: 
HTTP Requests (All):   0.42149  0.42149 
Cache Misses:          0.46965  0.46965 
Cache Hits:            0.00286  0.00286 
Near Hits:             0.28853  0.30459 
Not-Modified Replies:  0.00091  0.00091 
DNS Lookups:           0.16304  0.14912 
ICP Queries:           0.00000  0.00000 
Resource usage for squid: 
UP Time:        98290.098 seconds 
CPU Time:       42139.837 seconds 
CPU Usage:      42.87% 
CPU Usage, 5 minute avg:        62.41% 
CPU Usage, 60 minute avg:       59.00% 
Maximum Resident Size: 2635680 KB 
Page faults with physical i/o: 2 
Memory accounted for: 
Total accounted:       441310 KB 
memPoolAlloc calls:       501 
memPoolFree calls:  3310926049 
File descriptor usage for squid: 
Maximum number of file descriptors:   65536 
Largest file desc currently in use:   1990 
Number of file desc currently in use: 1750 
Files queued for open:                   0 
Available number of file descriptors: 63786 
Reserved number of file descriptors:   100 
Store Disk files open:                 200 
Internal Data Structures: 
2203893 StoreEntries 
6002 StoreEntries with MemObjects 
5499 Hot Object Cache Items 
2203404 on-disk objects 





On Thursday, July 2, 2015 8:29 PM, Mohammad Shakir <shakirgil at yahoo.com> wrote:
Ok, we will try it today and post our results.




On Thursday, July 2, 2015 8:19 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 3/07/2015 3:09 a.m., Mohammad Shakir wrote:
> We are running single instance of squid. after 2 days running squid we got the same error.


Please try the latest snapshot of 3.5. r13857 or later.


Amos


From Jason_Haar at trimble.com  Sat Jul  4 05:07:27 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Sat, 4 Jul 2015 17:07:27 +1200
Subject: [squid-users] Force LDAP groups to de-authenticate?
In-Reply-To: <mn6j3h$12d$1@ger.gmane.org>
References: <mn6j3h$12d$1@ger.gmane.org>
Message-ID: <55976A0F.80306@trimble.com>

On 04/07/15 06:08, Dan Purgert wrote:
> I need to kick the users and force a re-
> auth, as this is for a school environment. 

You can't really do that with proxy authentication methods. Once a
browser has successfully authenticated, it remembers that - so even if
you flush the server cache, all that happens is the browser sends the
cached credentials it has and the server revalidates: the user doesn't
even know it's happened

The only way I can think of that will serve your purposes is to move to
a "portal" solution instead. ie don't use proxy authentication - instead
block Internet and redirect port 80 requests to a captive portal, force
people to login there, then that action whitelists their Internet access
for the next 'n' minutes, after that time expires, they are pushed back
to the portal page again

...but that will require a different product - something like pfsense
comes to mind

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Sat Jul  4 06:57:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 04 Jul 2015 18:57:20 +1200
Subject: [squid-users] Force LDAP groups to de-authenticate?
In-Reply-To: <mn6j3h$12d$1@ger.gmane.org>
References: <mn6j3h$12d$1@ger.gmane.org>
Message-ID: <559783D0.9070005@treenet.co.nz>

On 4/07/2015 6:08 a.m., Dan Purgert wrote:
> I'm setting up a squid proxy with LDAP user/group authentication, and so 
> far have been able to sort out the problems I've run into with a little 
> help from google and caches of the various squid mailing lists. 
> 
> Currently, it's in a mostly working state for nearly everything (i.e. 
> user authentication, allowed/blocked based on what group a user belongs 
> to, client pc auto-updates, etc.).  However, I can't figure out how to 
> force a user to re-authenticate after a set interval of time (say 30 
> mintues).


What exact use-case is this for?
 students logged in only for a class period?
 access differences between class and break times?
 something else?

As Dan mentioned HTTP authentication alone will not do this. Since HTTP
is stateless the browser is *already* re-authenticating on every single
request. The user has no interaction. The auth TTLS are just to ensure
Squid has accurate info about the credentials in its auth cache for the
backend part.

What you can do is use an external ACL helper to allow/reject based on
any criteria you code/script it for.

Amos



From vdoctor at neuf.fr  Sat Jul  4 08:02:54 2015
From: vdoctor at neuf.fr (Stakres)
Date: Sat, 4 Jul 2015 01:02:54 -0700 (PDT)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <5596BA9A.9070401@treenet.co.nz>
References: <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
 <1435929702099-4672044.post@n4.nabble.com> <5596A975.2060903@treenet.co.nz>
 <1435936908218-4672048.post@n4.nabble.com> <5596BA9A.9070401@treenet.co.nz>
Message-ID: <1435996974697-4672054.post@n4.nabble.com>

Hi Amos,

We did tons of tests with the latest Squid versions and this is not the
behaviour with the "host_verify_strict off" and "client_dst_passthru off".
With those 2 options OFF, we see a lot of ORIGINAL_DST that we should not
see if we follow your explainations, so it seems there is a bug somewhere ?

Can you check from your side (tproxy or not, same behaviour), thanks in
advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4672054.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Jul  4 15:04:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 05 Jul 2015 03:04:26 +1200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1435996974697-4672054.post@n4.nabble.com>
References: <55951C6A.4030204@treenet.co.nz>
 <1435835131864-4672020.post@n4.nabble.com> <559528CD.5020600@treenet.co.nz>
 <1435840988272-4672022.post@n4.nabble.com>
 <1435925126078-4672041.post@n4.nabble.com> <5596872A.7050508@treenet.co.nz>
 <1435929702099-4672044.post@n4.nabble.com> <5596A975.2060903@treenet.co.nz>
 <1435936908218-4672048.post@n4.nabble.com> <5596BA9A.9070401@treenet.co.nz>
 <1435996974697-4672054.post@n4.nabble.com>
Message-ID: <5597F5FA.3020808@treenet.co.nz>

On 4/07/2015 8:02 p.m., Stakres wrote:
> Hi Amos,
> 
> We did tons of tests with the latest Squid versions and this is not the
> behaviour with the "host_verify_strict off" and "client_dst_passthru off".
> With those 2 options OFF, we see a lot of ORIGINAL_DST that we should not
> see if we follow your explainations, so it seems there is a bug somewhere ?
> 

Such as?
 Enable debug_options 85,3 to see host verify checks and results in action.


> Can you check from your side (tproxy or not, same behaviour), thanks in
> advance.

The tests I have all work as expected, including malware PoC...

When verify passes Squid goes DIRECT (client_dst_passthru off) or
ORIGINAL_DST (client_dst_passthru on). With caching allowed.

When verify fails Squid goes ORIGINAL_DST or NONE (409 rejection). With
caching blocked.

Non-intercepted traffic does not get verified by default
(host_verfy_strict off).

Verified non-intercepted traffic (host_verify_strict on) with URL and
Host header containing identical content is treated normally. 409
rejection for all other.

Amos



From dan at djph.net  Sat Jul  4 17:43:14 2015
From: dan at djph.net (Dan Purgert)
Date: Sat, 04 Jul 2015 13:43:14 -0400
Subject: [squid-users] Force LDAP groups to de-authenticate?
In-Reply-To: <559783D0.9070005@treenet.co.nz>
References: <mn6j3h$12d$1@ger.gmane.org> <559783D0.9070005@treenet.co.nz>
Message-ID: <ED252FE3-A466-48AC-B4A3-B755992F2D60@djph.net>

On July 4, 2015 2:57:20 AM EDT, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>On 4/07/2015 6:08 a.m., Dan Purgert wrote:
>> I'm setting up a squid proxy with LDAP user/group authentication, and
>so 
>> far have been able to sort out the problems I've run into with a
>little 
>> help from google and caches of the various squid mailing lists. 
>> 
>> Currently, it's in a mostly working state for nearly everything (i.e.
>
>> user authentication, allowed/blocked based on what group a user
>belongs 
>> to, client pc auto-updates, etc.).  However, I can't figure out how
>to 
>> force a user to re-authenticate after a set interval of time (say 30 
>> mintues).
>
>
>What exact use-case is this for?
> students logged in only for a class period?
> access differences between class and break times?
> something else?
>
>As Dan mentioned HTTP authentication alone will not do this. Since HTTP
>is stateless the browser is *already* re-authenticating on every single
>request. The user has no interaction. The auth TTLS are just to ensure
>Squid has accurate info about the credentials in its auth cache for the
>backend part.
>
>What you can do is use an external ACL helper to allow/reject based on
>any criteria you code/script it for.
>
>Amos
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

Yes,  it's a "allow sites ABC for class time" and "allow xyz for break".

The acls work already for "class"  ... am looking for a way to give "on the fly" breaks.  If that's not possible,  I can work out something else (e.g. define a time based acl from say 2-3 pm or something).  I was just hoping to be able to be less heavy-handed than that. 
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From san2roy at gmail.com  Sun Jul  5 11:27:00 2015
From: san2roy at gmail.com (san2roy)
Date: Sun, 5 Jul 2015 04:27:00 -0700 (PDT)
Subject: [squid-users] Squid mikrotik public IP
Message-ID: <1436095620905-4672057.post@n4.nabble.com>

<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4672057/Network.png> 

This is my setup which need to work. as i am very new in squid need your
help. my setup are following

Squid 3.5.5
centos 6.6
Mikrotik Router os 6
WAN IP : 125.20.xx.32/30
Client IP : 103.16.xx.0/24 (public IP)

My ISP recently assign a public ip pool /24 in my router... now i need to
setup the squid over public ip.. i have tried tproxy but didn't get get to
work.. please tell me how can i configure so that public IP work without
changing the client host ip.

thank you




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-mikrotik-public-IP-tp4672057.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Sun Jul  5 12:34:11 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 5 Jul 2015 14:34:11 +0200
Subject: [squid-users] Squid mikrotik public IP
In-Reply-To: <1436095620905-4672057.post@n4.nabble.com>
References: <1436095620905-4672057.post@n4.nabble.com>
Message-ID: <201507051434.12166.Antony.Stone@squid.open.source.it>

On Sunday 05 July 2015 at 13:27:00 (EU time), san2roy wrote:

> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4672057/Network.p
> ng>
> 
> This is my setup which need to work. as i am very new in squid need your
> help. my setup are following
> 
> Squid 3.5.5
> centos 6.6
> Mikrotik Router os 6
> WAN IP : 125.20.xx.32/30
> Client IP : 103.16.xx.0/24 (public IP)

Why are you using public IPs on your internal client machines?

Are you running any services on those machines which need to be accessed from 
the Internet (ie: they're not just clients, they're servers too)?

I'm rather amazed that any ISP these days would assign you a public /24 
without a very good reason.

I would say it's a bad idea, both from a security point of view (yours), and 
also from a net friendliness point of view (you're using up a /24 which other 
people are desperately tryng to get for good reasons).


However, getting back to the Squid setup, Squid doesn't care whether the IPs 
assigned to your "client" machines are public or private.

You define the local network range which Squid is supposed to accept requests 
from, and that's that - there's no difference between public and private IPs.


So, tell us what setup you had working with private IPs (ie: show us your 
squid.conf without comments or blank lines), and then tell us what changes you 
made to accommodate the public address range, and tell us what's not now 
working.


Regards,


Antony.


-- 
Ramdisk is not an installation procedure.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From san2roy at gmail.com  Sun Jul  5 13:19:30 2015
From: san2roy at gmail.com (san2roy)
Date: Sun, 5 Jul 2015 06:19:30 -0700 (PDT)
Subject: [squid-users] Squid mikrotik public IP
In-Reply-To: <201507051434.12166.Antony.Stone@squid.open.source.it>
References: <1436095620905-4672057.post@n4.nabble.com>
 <201507051434.12166.Antony.Stone@squid.open.source.it>
Message-ID: <1436102370451-4672059.post@n4.nabble.com>

Hello sir,

The public IPs are our own provided by APNIC.. ISP asign them to our
Mikrotik Router.. 

"they're not just clients, they're servers too" yes you are right there are
some server also running on it... we provide leased line service so public
ip need to provide to clients..

there is no problem in squid config... problem is that i don't understand
how to configure router to send the traffic to squid and back to to client..
in my previous setup there was no public ip got help from bellow url

https://aacable.wordpress.com/2011/07/21/mikrotik-howto-redirect-http-traffic-to-squid-with-original-source-client-ip/
and
http://forum.mikrotik.com/viewtopic.php?t=77635

i want a setup in a way so that client can not understand that traffic
coming from squid... and when they search whatismyip it should show the
public ip not the squid box ip...

thanks for your help







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-mikrotik-public-IP-tp4672057p4672059.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcel at guineanet.net  Sun Jul  5 14:13:33 2015
From: marcel at guineanet.net (Marcel)
Date: Sun, 05 Jul 2015 15:13:33 +0100
Subject: [squid-users] Squid mikrotik public IP
In-Reply-To: <1436102370451-4672059.post@n4.nabble.com>
References: <1436095620905-4672057.post@n4.nabble.com>
 <201507051434.12166.Antony.Stone@squid.open.source.it>
 <1436102370451-4672059.post@n4.nabble.com>
Message-ID: <55993B8D.5030104@guineanet.net>

HI san2roy
since your WAN is /30 I guess 125.20.xx.31/30 is you ISP GW while 
125.20.xx.32/30 is your WAN right?

can your ISP change your /30 to let say /29
to allow your Squid Box Wan be on the same subnet with your Mikrotik WAN?

or split your /30 to (2) /31 subnet
then we can use private IP on your Squid box LAN to route your clients public IP
but be sure you have the appropriate kernel and iptables on your centos 6.6

Kernel >=Linux Kernel 2.6.37
iptables >= iptables 1.4.10
http://wiki.squid-cache.org/Features/Tproxy4

Rgds


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150705/fd0e25da/attachment.htm>

From Walter.H at mathemainzel.info  Sun Jul  5 14:01:52 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 05 Jul 2015 16:01:52 +0200
Subject: [squid-users] SSL-bump and Public Key Piinning (HPKP)
Message-ID: <559938D0.1060901@mathemainzel.info>

Hello,

I'm using squid with ssl-bump, after updating (I update only in bigger 
steps and not this often) my browser I realize,
that this supports HPKP; I didn't find how to deactivate this - Chrome 43

so I thought, I could prevent squid of replying this header field with this:

reply_header_access Public-Key-Pins deny all

but this doesn't really work; is there another way?

the squid is running in a VM on my own computer and only used by me;

Thanks,
Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4312 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150705/e71d1658/attachment.bin>

From san2roy at gmail.com  Sun Jul  5 14:47:08 2015
From: san2roy at gmail.com (san2roy)
Date: Sun, 5 Jul 2015 07:47:08 -0700 (PDT)
Subject: [squid-users] Squid mikrotik public IP
In-Reply-To: <55993B8D.5030104@guineanet.net>
References: <1436095620905-4672057.post@n4.nabble.com>
 <201507051434.12166.Antony.Stone@squid.open.source.it>
 <1436102370451-4672059.post@n4.nabble.com> <55993B8D.5030104@guineanet.net>
Message-ID: <1436107628109-4672062.post@n4.nabble.com>

Hello Sir,

"since your WAN is /30 I guess 125.20.xx.31/30 is you ISP GW while
125.20.xx.32/30 is your WAN 

right?" yes you are right


"can your ISP change your /30 to let say /29
to allow your Squid Box Wan be on the same subnet with your Mikrotik WAN?"

Yes we can arrange /29 WAN IP from ISP

[root at cache ~]# uname -r
2.6.32-504.23.4.el6.x86_64
[root at cache ~]#  rpm -q iptables
iptables-1.4.7-14.el6.x86_64

Should I upgrade the version?

but question to configure the routing and what will be the network topology

are you saying to connect squid box from wan IP range?

like this

        Internet
         |    |
         |    |
 Mikrotik  Squid
         |    |
         |    |
        Switch
            |
        Clients

Please note we have /24 IPs in our LAN in Mikrotik router. and /30 wan ip
which is static route.









--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-mikrotik-public-IP-tp4672057p4672062.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Jason_Haar at trimble.com  Sun Jul  5 23:34:30 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 6 Jul 2015 11:34:30 +1200
Subject: [squid-users] SSL-bump and Public Key Piinning (HPKP)
In-Reply-To: <559938D0.1060901@mathemainzel.info>
References: <559938D0.1060901@mathemainzel.info>
Message-ID: <5599BF06.7020308@trimble.com>

On 6/07/15 2:01 am, Walter H. wrote:
> reply_header_access Public-Key-Pins deny all
>
> but this doesn't really work; is there another way?
If you think you can override all pinning options, then I'm afraid 
you're mistaken. Well written security apps should do their darndest to 
stop TLS intercept from working: eg hardwiring the CA cert into the 
application itself and barfing if it ever starts a HTTPS connection that 
isn't signed by their "one" CA

You have to accept that and configure for it: simply create a 
"noSSLintercept" acl and in there place the ones that can't be fiddled 
with. I'm still only testing TLS intercept myself, but so far I've only 
whitelisted the following

.preyproject.com
accounts.google.com
.push.hello.firefox.com

BTW, even though Chrome/Firefox support key pinning, as a general rule 
they actually support TLS intercept as well - in that if they detect the 
CA involved in a cert-chain is trusted by the *user* and is not a 
"commercial" CA, then they assume TLS Intercept must be involved and 
allow it to work (at least that's how it seems to work to me). Not a bad 
idea as it allows companies to do TLS intercept, but still guards 
against governments forcing commercial CAs to create "fake" server certs 
(let's be honest - all of this is about stopping government snooping - 
not about normal criminal behavior)

Jason


From adam900710 at gmail.com  Mon Jul  6 09:22:20 2015
From: adam900710 at gmail.com (adam900710)
Date: Mon, 6 Jul 2015 17:22:20 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
	after Client Hello.
Message-ID: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>

Hi all,

I tried to build a ssl bumping proxy with up level proxy, but client
failed to connect like the following.

The error:
---
$ curl https://www.google.co.jp -vvvv -k
* Rebuilt URL to: https://www.google.co.jp/
* Trying ::1...
* Connected to localhost (::1) port 3128 (#0)
* Establish HTTP proxy tunnel to www.google.co.jp:443
> CONNECT www.google.co.jp:443 HTTP/1.1
> Host: www.google.co.jp:443
> User-Agent: curl/7.43.0
> Proxy-Connection: Keep-Alive
>
< HTTP/1.1 200 Connection established
<
* Proxy replied OK to CONNECT request
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
* CAfile: /etc/ssl/certs/ca-certificates.crt
CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* Unknown SSL protocol error in connection to www.google.co.jp:443
* Closing connection 0
curl: (35) Unknown SSL protocol error in connection to www.google.co.jp:443
---

My squid.conf:
---
# default acls/configs are ignored
cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
never_direct allow all
ssl_bump peek all
ssl_bump bump all
http_port 3128 ssl-bump \
cert=/etc/squid/ssl/ca.crt \
key=/etc/squid/ssl/ca.key \
generate-host-certificates=on \
dynamic_cert_mem_cache_size=4MB
---

>From the cache_peer port, someone may notice that I'm using privoxy.
That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
or I can't ever access some sites.

Here is some of my experiments:
1) Remove "never_direct"
Then ssl_bump works as expected, but all traffic doesn't goes through
the SOCKS5 proxy. So a lot of sites I can't access.

2) Use local 8118 proxy
That works fine without any problem, but SSL_dump is needed...
So just prove privoxy are working.

Any clue?

Thanks


From adam900710 at gmail.com  Mon Jul  6 09:30:33 2015
From: adam900710 at gmail.com (adam900710)
Date: Mon, 6 Jul 2015 17:30:33 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
	after Client Hello.
In-Reply-To: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
Message-ID: <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>

Forgot some extra infomation:
squid build info:
---
Squid Cache: Version 3.5.5
Service Name: squid
configure options:  '--prefix=/usr' '--sbindir=/usr/bin'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--localstatedir=/var'
'--with-logdir=/var/log/squid' '--with-pidfile=/run/squid.pid'
'--enable-auth' '--enable-auth-basic' '--enable-auth-ntlm'
'--enable-auth-digest' '--enable-auth-negotiate'
'--enable-removal-policies=lru,heap' '--enable-storeio=aufs,ufs,diskd'
'--enable-delay-pools' '--with-openssl=/usr' '--enable-snmp'
'--enable-linux-netfilter' '--enable-ident-lookups'
'--enable-useragent-log' '--enable-cache-digests'
'--enable-referer-log' '--enable-htcp' '--enable-carp'
'--enable-epoll' '--with-large-files' '--enable-arp-acl'
'--with-default-user=proxy' '--enable-async-io' '--enable-truncate'
'--enable-icap-client' '--enable-ssl-crtd' '--disable-arch-native'
'--disable-strict-error-checking' '--enable-wccpv2'
'CFLAGS=-march=x86-64 -mtune=generic -O2 -pipe
-fstack-protector-strong --param=ssp-buffer-size=4'
'LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-march=x86-64 -mtune=generic
-O2 -pipe -fstack-protector-strong --param=ssp-buffer-size=4'
---

Also, If I disable "ssl_bump" at http_port line, squid works without
any problem just as a forwarder.
But that makes no sense anyway.

Thanks

2015-07-06 17:22 GMT+08:00 adam900710 <adam900710 at gmail.com>:
> Hi all,
>
> I tried to build a ssl bumping proxy with up level proxy, but client
> failed to connect like the following.
>
> The error:
> ---
> $ curl https://www.google.co.jp -vvvv -k
> * Rebuilt URL to: https://www.google.co.jp/
> * Trying ::1...
> * Connected to localhost (::1) port 3128 (#0)
> * Establish HTTP proxy tunnel to www.google.co.jp:443
>> CONNECT www.google.co.jp:443 HTTP/1.1
>> Host: www.google.co.jp:443
>> User-Agent: curl/7.43.0
>> Proxy-Connection: Keep-Alive
>>
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * ALPN, offering http/1.1
> * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> * successfully set certificate verify locations:
> * CAfile: /etc/ssl/certs/ca-certificates.crt
> CApath: none
> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> * Unknown SSL protocol error in connection to www.google.co.jp:443
> * Closing connection 0
> curl: (35) Unknown SSL protocol error in connection to www.google.co.jp:443
> ---
>
> My squid.conf:
> ---
> # default acls/configs are ignored
> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
> never_direct allow all
> ssl_bump peek all
> ssl_bump bump all
> http_port 3128 ssl-bump \
> cert=/etc/squid/ssl/ca.crt \
> key=/etc/squid/ssl/ca.key \
> generate-host-certificates=on \
> dynamic_cert_mem_cache_size=4MB
> ---
>
> From the cache_peer port, someone may notice that I'm using privoxy.
> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
> or I can't ever access some sites.
>
> Here is some of my experiments:
> 1) Remove "never_direct"
> Then ssl_bump works as expected, but all traffic doesn't goes through
> the SOCKS5 proxy. So a lot of sites I can't access.
>
> 2) Use local 8118 proxy
> That works fine without any problem, but SSL_dump is needed...
> So just prove privoxy are working.
>
> Any clue?
>
> Thanks


From squid3 at treenet.co.nz  Mon Jul  6 12:06:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 07 Jul 2015 00:06:06 +1200
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
Message-ID: <559A6F2E.20801@treenet.co.nz>

On 6/07/2015 9:30 p.m., adam900710 wrote:
> 
> Here is some of my experiments:
> 1) Remove "never_direct"
> Then ssl_bump works as expected, but all traffic doesn't goes through
> the SOCKS5 proxy. So a lot of sites I can't access.
> 
> 2) Use local 8118 proxy
> That works fine without any problem, but SSL_dump is needed...
> So just prove privoxy are working.
> 
> Any clue?

> Also, If I disable "ssl_bump" at http_port line, squid works without
> any problem just as a forwarder.
> But that makes no sense anyway.

Makes perfect sense. Would you like anybody to be able to decrypt your
HTTPS traffic and send it as plain-text wherever they want?

Squid does not permit that. All inbound encrypted traffic must one way
or another leave upstream only by encrypted channels.

Amos



From yvoinov at gmail.com  Mon Jul  6 12:41:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 18:41:34 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A6F2E.20801@treenet.co.nz>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
 <559A6F2E.20801@treenet.co.nz>
Message-ID: <559A777E.3030302@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


06.07.15 18:06, Amos Jeffries ?????:
> On 6/07/2015 9:30 p.m., adam900710 wrote:
>>
>> Here is some of my experiments:
>> 1) Remove "never_direct"
>> Then ssl_bump works as expected, but all traffic doesn't goes through
>> the SOCKS5 proxy. So a lot of sites I can't access.
>>
>> 2) Use local 8118 proxy
>> That works fine without any problem, but SSL_dump is needed...
>> So just prove privoxy are working.
>>
>> Any clue?
>
>> Also, If I disable "ssl_bump" at http_port line, squid works without
>> any problem just as a forwarder.
>> But that makes no sense anyway.
>
> Makes perfect sense. Would you like anybody to be able to decrypt your
> HTTPS traffic and send it as plain-text wherever they want?
Disagree. Not anybody, but anything. Like SpamAsassin reads my mail. And
so what?
>
>
> Squid does not permit that. All inbound encrypted traffic must one way
> or another leave upstream only by encrypted channels.
Aha, without caching. And will we need caching proxy this way?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmnd+AAoJENNXIZxhPexGAQ0IAKdNK16UITyVODfIdpXeE0J7
XqSuzGNVF29zbZFKF77+0YURoTvP+9QAz0etQqxw/B5vXMSuUmeRABkmzeFmnSJp
aYevXI59j1I+a/1Y+cxR5r99vk+hiEEjNECEq+y1xR7/W3xL1RwaJNlzT9IsbUsU
8lmyZ7WRNIudRmH7DaGiiGdfUC0/hXiKEcEZBhjVe8okYCDKyloG7i4GQisaHZqG
hp2hSckPLp4URhu/qj20i3mrEcplpf1XUvfBnr9FngZni4IiyiclxSAcgCAw3ukE
0dWYtE382Q1WVOLDLsFNYTwBlBuVSko2ddUk246GtBIVhnF6/2D0fmsw2SL3GYc=
=FXay
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Mon Jul  6 13:20:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 19:20:44 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A6F2E.20801@treenet.co.nz>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
 <559A6F2E.20801@treenet.co.nz>
Message-ID: <559A80AC.8080204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


06.07.15 18:06, Amos Jeffries ?????:
> On 6/07/2015 9:30 p.m., adam900710 wrote:
>>
>> Here is some of my experiments:
>> 1) Remove "never_direct"
>> Then ssl_bump works as expected, but all traffic doesn't goes through
>> the SOCKS5 proxy. So a lot of sites I can't access.
>>
>> 2) Use local 8118 proxy
>> That works fine without any problem, but SSL_dump is needed...
>> So just prove privoxy are working.
>>
>> Any clue?
>
>> Also, If I disable "ssl_bump" at http_port line, squid works without
>> any problem just as a forwarder.
>> But that makes no sense anyway.
>
> Makes perfect sense. Would you like anybody to be able to decrypt your
> HTTPS traffic and send it as plain-text wherever they want?
Anybody already have the ability to decrypt our HTTPS traffic. This
named "government".
>
>
> Squid does not permit that. All inbound encrypted traffic must one way
> or another leave upstream only by encrypted channels.
Websense does. Another commercial products and solutions does. So?
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmoCsAAoJENNXIZxhPexGXo8H/2vYEU7Xt+T85477hZYN5nr8
TMdRMfQIudoJcZWDyqqcf6JRw6SRwcgZDaRHUnH3CrUejcf9AMYH1MxX+Knwrsd+
IwXs0LEO45hTfKo005NV/1BVcXu1s7OPEvgRa85WUCfexBALqVT+JAA+ZG6BO3XW
QNSSgsdxCgPl9ITFetBkTOQTZJaXTkGy+JSPhaTl+eOp+DFKWzcTLOhKybOP4VlF
qW/srBT1VDdBSy1/BIGGUGzFMboRW0hV7izfhbq3A38pxaEKrA9IraamOPwVl/8k
0tDZ5bfFlvzknO4xqu73hHjIu1aByGSfSwablImeV9eCDJ1dzAJTbFR1YXq0/XM=
=+H6m
-----END PGP SIGNATURE-----



From adam900710 at gmail.com  Mon Jul  6 13:34:19 2015
From: adam900710 at gmail.com (adam900710)
Date: Mon, 6 Jul 2015 21:34:19 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A6F2E.20801@treenet.co.nz>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
 <559A6F2E.20801@treenet.co.nz>
Message-ID: <CAFy8SQUm+uNahwEXcpLpVM_7wfy+sy3Mjcx-S0rcxtoPqNSXZg@mail.gmail.com>

2015-07-06 20:06 GMT+08:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 6/07/2015 9:30 p.m., adam900710 wrote:
>>
>> Here is some of my experiments:
>> 1) Remove "never_direct"
>> Then ssl_bump works as expected, but all traffic doesn't goes through
>> the SOCKS5 proxy. So a lot of sites I can't access.
>>
>> 2) Use local 8118 proxy
>> That works fine without any problem, but SSL_dump is needed...
>> So just prove privoxy are working.
>>
>> Any clue?
>
>> Also, If I disable "ssl_bump" at http_port line, squid works without
>> any problem just as a forwarder.
>> But that makes no sense anyway.
>
> Makes perfect sense. Would you like anybody to be able to decrypt your
> HTTPS traffic and send it as plain-text wherever they want?
>
> Squid does not permit that. All inbound encrypted traffic must one way
> or another leave upstream only by encrypted channels.
Agree with Yuri, I hate the government (Yeah, especially the f**king
China gov!) and
the evil Chinese one has alreayd tried this trick on gmail some month ago.

That's who forces me to pass the traffic to privoxy, as the Great
Firewall is already
blocking me to reach most sites in the open world.

Also you get a little confused with ssl dump and encryption/authentication.

SSL bump in fact doesn't do the black magic to magically decrypt
everything without cost.
PKI things still makes you know that some one is bump your SSL communication.

So normally with SSL bump, you will see a big browser warning about
the unknown issuer of
the faked certificates.
And normal routine like curl will just abort the connection when it
found the certificate is not valid.

Although the communication lost the encryption, you can still know you
are under monitoring.
And this implement needs you to trust the fake CA.
If one doesn't trust it, just blacklist the fake CA and use tor or
whatever to really hide the trace.

So although the ssl bump destory encryption, but it doesn't destory
authentication.
And the combination of ssl bump and cache peer should be allowed if no
bugs or my configuration error.

Thanks.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Mon Jul  6 13:48:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 19:48:53 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQUm+uNahwEXcpLpVM_7wfy+sy3Mjcx-S0rcxtoPqNSXZg@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
 <559A6F2E.20801@treenet.co.nz>
 <CAFy8SQUm+uNahwEXcpLpVM_7wfy+sy3Mjcx-S0rcxtoPqNSXZg@mail.gmail.com>
Message-ID: <559A8745.7010606@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And also:

As long as you stay in the white robes, the whole world supports the
illusion of security HTTPS. The world has changed in the eyes of the
past three years. And by the way, your branch 3.4 has long been used in
commercial solutions. Doing the bump. The illusion of security is much
worse insecurity. Is not it time to admit it?

06.07.15 19:34, adam900710 ?????:
> 2015-07-06 20:06 GMT+08:00 Amos Jeffries <squid3 at treenet.co.nz>:
>> On 6/07/2015 9:30 p.m., adam900710 wrote:
>>>
>>> Here is some of my experiments:
>>> 1) Remove "never_direct"
>>> Then ssl_bump works as expected, but all traffic doesn't goes through
>>> the SOCKS5 proxy. So a lot of sites I can't access.
>>>
>>> 2) Use local 8118 proxy
>>> That works fine without any problem, but SSL_dump is needed...
>>> So just prove privoxy are working.
>>>
>>> Any clue?
>>
>>> Also, If I disable "ssl_bump" at http_port line, squid works without
>>> any problem just as a forwarder.
>>> But that makes no sense anyway.
>>
>> Makes perfect sense. Would you like anybody to be able to decrypt your
>> HTTPS traffic and send it as plain-text wherever they want?
>>
>> Squid does not permit that. All inbound encrypted traffic must one way
>> or another leave upstream only by encrypted channels.
> Agree with Yuri, I hate the government (Yeah, especially the f**king
> China gov!) and
> the evil Chinese one has alreayd tried this trick on gmail some month ago.
>
> That's who forces me to pass the traffic to privoxy, as the Great
> Firewall is already
> blocking me to reach most sites in the open world.
>
> Also you get a little confused with ssl dump and
encryption/authentication.
>
> SSL bump in fact doesn't do the black magic to magically decrypt
> everything without cost.
> PKI things still makes you know that some one is bump your SSL
communication.
>
> So normally with SSL bump, you will see a big browser warning about
> the unknown issuer of
> the faked certificates.
> And normal routine like curl will just abort the connection when it
> found the certificate is not valid.
>
> Although the communication lost the encryption, you can still know you
> are under monitoring.
> And this implement needs you to trust the fake CA.
> If one doesn't trust it, just blacklist the fake CA and use tor or
> whatever to really hide the trace.
>
> So although the ssl bump destory encryption, but it doesn't destory
> authentication.
> And the combination of ssl bump and cache peer should be allowed if no
> bugs or my configuration error.
>
> Thanks.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmodEAAoJENNXIZxhPexGK10IAImDjOVFy+W+v1IFKg8KVZzW
dbdQu00RnpOxKyEf9mQHb27DX674mr7LxxOHmXEpttPd2EdRERVveViJNOw0Hs1B
LeSeqp9D9ZvP4lqyVLdvJTqCzvF1TbFKF7Xc8S5olUrI4yOsvDIdpLqZ3emFqIQd
rXgdM8FJtxTMf/qgPfkJMfVS8zyo1CMeAxlMayTzwdvk6E7IGUk2CyEG7XKDjzrd
Lp89qUk6vpuzHoirVefFKq4M/TPLtSeL1647MiIP5L5Do6nREYXNlYn5IywZTEQC
6rn81G+g+vIbRdASBPtVQ1tWI6HD3oD9j2965DNdgIkmjwfG47Kotam6tHftBwA=
=qyX/
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Mon Jul  6 13:57:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 19:57:55 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQUm+uNahwEXcpLpVM_7wfy+sy3Mjcx-S0rcxtoPqNSXZg@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <CAFy8SQVCx_n3RK2Z4zy-r8bLnk5fPLq-4aFGMYG8ZtYY2vQXwg@mail.gmail.com>
 <559A6F2E.20801@treenet.co.nz>
 <CAFy8SQUm+uNahwEXcpLpVM_7wfy+sy3Mjcx-S0rcxtoPqNSXZg@mail.gmail.com>
Message-ID: <559A8963.20606@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And finally:

HTTPS is used for malware transmission - and we can't scan it!, for porn
viewing, for illegal P2P traffic and others.

And we are the paladines in white robes.

06.07.15 19:34, adam900710 ?????:
> 2015-07-06 20:06 GMT+08:00 Amos Jeffries <squid3 at treenet.co.nz>:
>> On 6/07/2015 9:30 p.m., adam900710 wrote:
>>>
>>> Here is some of my experiments:
>>> 1) Remove "never_direct"
>>> Then ssl_bump works as expected, but all traffic doesn't goes through
>>> the SOCKS5 proxy. So a lot of sites I can't access.
>>>
>>> 2) Use local 8118 proxy
>>> That works fine without any problem, but SSL_dump is needed...
>>> So just prove privoxy are working.
>>>
>>> Any clue?
>>
>>> Also, If I disable "ssl_bump" at http_port line, squid works without
>>> any problem just as a forwarder.
>>> But that makes no sense anyway.
>>
>> Makes perfect sense. Would you like anybody to be able to decrypt your
>> HTTPS traffic and send it as plain-text wherever they want?
>>
>> Squid does not permit that. All inbound encrypted traffic must one way
>> or another leave upstream only by encrypted channels.
> Agree with Yuri, I hate the government (Yeah, especially the f**king
> China gov!) and
> the evil Chinese one has alreayd tried this trick on gmail some month ago.
>
> That's who forces me to pass the traffic to privoxy, as the Great
> Firewall is already
> blocking me to reach most sites in the open world.
>
> Also you get a little confused with ssl dump and
encryption/authentication.
>
> SSL bump in fact doesn't do the black magic to magically decrypt
> everything without cost.
> PKI things still makes you know that some one is bump your SSL
communication.
>
> So normally with SSL bump, you will see a big browser warning about
> the unknown issuer of
> the faked certificates.
> And normal routine like curl will just abort the connection when it
> found the certificate is not valid.
>
> Although the communication lost the encryption, you can still know you
> are under monitoring.
> And this implement needs you to trust the fake CA.
> If one doesn't trust it, just blacklist the fake CA and use tor or
> whatever to really hide the trace.
>
> So although the ssl bump destory encryption, but it doesn't destory
> authentication.
> And the combination of ssl bump and cache peer should be allowed if no
> bugs or my configuration error.
>
> Thanks.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmoliAAoJENNXIZxhPexGtL4H/3/Q6A7Rg4UzN1o/PGJc1rb/
WKzolOZ6Hj810108EQ19okSsoShrkzA1mXeaGOktCcTUfFMwYBnIdt+WV7V8LiZT
4AyrwdBrxREu+hPn0NQWRex4nzobG47aOqVF81npYLp+mioM4J4FWCv0Y9hbglSt
w+IvZhhcyswYR5LP2BiS4dUZMY52O8y0S4HpOe85f3/24/l/pswUoVgSdcHW1Dck
Nq34i0fZ560QiJjJZzAGc9a2Akbq5ppx414bKaCCxG9DyKLO1As793bPIxvIQuQ7
KpiD5bkaKYkyA2XhZ/BJIB2dUSJa7HI4GXOrUjCgXN0XnH8aDLlsgZ8XhXlxJ4o=
=2Fvz
-----END PGP SIGNATURE-----



From mynixmail at gmail.com  Mon Jul  6 14:01:22 2015
From: mynixmail at gmail.com (Danny)
Date: Mon, 6 Jul 2015 16:01:22 +0200
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <5596ABBB.4080700@treenet.co.nz>
References: <20150702144158.GB28280@fever.havannah.local>
 <55955918.2080204@treenet.co.nz>
 <20150702174615.GA18066@fever.havannah.local>
 <5596ABBB.4080700@treenet.co.nz>
Message-ID: <20150706140122.GA5260@fever.havannah.local>

Thank You Amos ... with a  little trial and error I got it right.

Danny

On Jul 04 15, Amos Jeffries :
> To: squid-users at lists.squid-cache.org
> Date: Sat, 04 Jul 2015 03:35:23 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] reply_body_max_size question
> User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101
>  Thunderbird/31.7.0
> X-BeenThere: squid-users at lists.squid-cache.org
> 
> On 3/07/2015 5:46 a.m., Danny wrote:
> >> It works as documented at
> >> <http://www.squid-cache.org/Doc/config/reply_body_max_size/>.  If that
> >> does not fit your criteria then its not what you need.
> > 
> > I am aware of that, I was just a little unsure how to split the different dowload
> > sizes amongst all the different users.
> >  
> >>> http_access allow localnet
> >>
> >> NOTE: No http_access ACLs controlling 10.0.0.0/24 have any effect below
> >> this one that allows them all access to use the proxy.
> >>
> >>> http_access allow localnet_dad_laptop
> >>> http_access allow localnet_dad_smartphone
> >>> http_access allow localnet_mom_laptop
> >>> http_access allow localnet_mom_smartphone
> >>> http_access allow localnet_son_laptop
> >>> http_access allow localnet_son_smartphone
> >>> http_access allow localnet_son_tablet
> > 
> > Thank you ... did not know that ... I was under the impression every user i.e
> > device needed to be granted http_access ...
> 
> They do. But not necessarily individually. The /24 does all IPs in the
> subnetwork as a group.
> 
> You can also list multiple IPs and/or subnets in one ACL name. That
> helps fixing the below...
> 
> 
> > 
> >> By applying ACLs for the kids on the reply_body_max_size directive lines
> >> setting the sizes to use for them. Like so:
> >>   reply_body_max_size 50 KB localnet_son_smartphone
> > 
> > O.k ... so currently I have:
> > reply_body_max_size 20 MB
> > 
> > If I combine your suggestion and Augusto Gabanzo's (who suggested something a little different) can I then do something like this:
> > ##########
> > reply_body_max_size 0 MB !localnet_son_laptop !localnet_son_smartphone !localnet_son_tablet
> > reply_body_max_size 5 MB localnet_son_laptop localnet_son_smartphone localnet_son_tablet (// Or must each device get it's own limit?)
> 
> The ACLs on a line are AND'd together. Better to make one ACL that
> matches all the IPs for the user you want to limit.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Mon Jul  6 14:05:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 20:05:53 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
Message-ID: <559A8B41.8060600@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
My own solution in conjunction with Tor + Privoxy looks like this (Note:
for Squid 3.4.13):

# Tor acl
acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"

# SSL bump rules
sslproxy_cert_error allow all
ssl_bump none localhost
ssl_bump none url_nobump
ssl_bump none dst_nobump
ssl_bump server-first net_bump

# Privoxy+Tor access rules
never_direct allow tor_url
always_direct deny tor_url
always_direct allow all

# And finally deny all other access to this proxy
http_access deny all

# Local Privoxy is cache parent
cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default

cache_peer_access 127.0.0.1 allow tor_url
cache_peer_access 127.0.0.1 deny all

http_port 3127
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key
sslproxy_capath /etc/opt/csw/ssl/certs
sslproxy_options NO_SSLv2 NO_SSLv3
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

Generally,

works like charm.

06.07.15 15:22, adam900710 ?????:
> Hi all,
>
> I tried to build a ssl bumping proxy with up level proxy, but client
> failed to connect like the following.
>
> The error:
> ---
> $ curl https://www.google.co.jp -vvvv -k
> * Rebuilt URL to: https://www.google.co.jp/
> * Trying ::1...
> * Connected to localhost (::1) port 3128 (#0)
> * Establish HTTP proxy tunnel to www.google.co.jp:443
>> CONNECT www.google.co.jp:443 HTTP/1.1
>> Host: www.google.co.jp:443
>> User-Agent: curl/7.43.0
>> Proxy-Connection: Keep-Alive
>>
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * ALPN, offering http/1.1
> * Cipher selection:
ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> * successfully set certificate verify locations:
> * CAfile: /etc/ssl/certs/ca-certificates.crt
> CApath: none
> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> * Unknown SSL protocol error in connection to www.google.co.jp:443
> * Closing connection 0
> curl: (35) Unknown SSL protocol error in connection to
www.google.co.jp:443
> ---
>
> My squid.conf:
> ---
> # default acls/configs are ignored
> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
> never_direct allow all
> ssl_bump peek all
> ssl_bump bump all
> http_port 3128 ssl-bump \
> cert=/etc/squid/ssl/ca.crt \
> key=/etc/squid/ssl/ca.key \
> generate-host-certificates=on \
> dynamic_cert_mem_cache_size=4MB
> ---
>
> From the cache_peer port, someone may notice that I'm using privoxy.
> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
> or I can't ever access some sites.
>
> Here is some of my experiments:
> 1) Remove "never_direct"
> Then ssl_bump works as expected, but all traffic doesn't goes through
> the SOCKS5 proxy. So a lot of sites I can't access.
>
> 2) Use local 8118 proxy
> That works fine without any problem, but SSL_dump is needed...
> So just prove privoxy are working.
>
> Any clue?
>
> Thanks
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmotBAAoJENNXIZxhPexG0PQIAJ0Cy3o/diVtZsCYPTZ5At8K
RuP3wHjahKhXj3xZjLiE+QKWvfr1ehZNWSj4wHF616ciX2w23QbghqNIBbV7Awpl
7JrTIv3L2nR/19uWgmr2FnhCKf2gSeC9j9Za0aBPAv3PoPwkMNmLbdlwq3mG8pey
6Tk8Tsh8+BlfUYXNgO+x/05eyLx6k4ZRV7009E7U3akt5ye+d8vcYXSfwL8+O+ni
JReTJ2CwXSakb+Olti+ZTJvJWxI49Szdc3FrAyh7cTe2Bgo8hDTyW9Pj5WNvINYG
+LQZUqOBF/YWtvpXbVVWAcJxYyzTGJJE/1+TtfIFEDsULTe4G74wCqsPu5VanM0=
=TEp1
-----END PGP SIGNATURE-----



From adam900710 at gmail.com  Mon Jul  6 14:11:13 2015
From: adam900710 at gmail.com (adam900710)
Date: Mon, 6 Jul 2015 22:11:13 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A8B41.8060600@gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
Message-ID: <CAFy8SQWpNribW3mnC22sfKXfzNSm1YfrH1xDR0Dzw6MC+XkyBQ@mail.gmail.com>

Great thanks,I'll try it later.

Thanks
2015?7?6? 22:06? "Yuri Voinov" <yvoinov at gmail.com>???

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> My own solution in conjunction with Tor + Privoxy looks like this (Note:
> for Squid 3.4.13):
>
> # Tor acl
> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> ssl_bump none localhost
> ssl_bump none url_nobump
> ssl_bump none dst_nobump
> ssl_bump server-first net_bump
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
> always_direct deny tor_url
> always_direct allow all
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> http_port 3127
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key
> sslproxy_capath /etc/opt/csw/ssl/certs
> sslproxy_options NO_SSLv2 NO_SSLv3
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> Generally,
>
> works like charm.
>
> 06.07.15 15:22, adam900710 ?????:
> > Hi all,
> >
> > I tried to build a ssl bumping proxy with up level proxy, but client
> > failed to connect like the following.
> >
> > The error:
> > ---
> > $ curl https://www.google.co.jp -vvvv -k
> > * Rebuilt URL to: https://www.google.co.jp/
> > * Trying ::1...
> > * Connected to localhost (::1) port 3128 (#0)
> > * Establish HTTP proxy tunnel to www.google.co.jp:443
> >> CONNECT www.google.co.jp:443 HTTP/1.1
> >> Host: www.google.co.jp:443
> >> User-Agent: curl/7.43.0
> >> Proxy-Connection: Keep-Alive
> >>
> > < HTTP/1.1 200 Connection established
> > <
> > * Proxy replied OK to CONNECT request
> > * ALPN, offering http/1.1
> > * Cipher selection:
> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> > * successfully set certificate verify locations:
> > * CAfile: /etc/ssl/certs/ca-certificates.crt
> > CApath: none
> > * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> > * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> > * Unknown SSL protocol error in connection to www.google.co.jp:443
> > * Closing connection 0
> > curl: (35) Unknown SSL protocol error in connection to
> www.google.co.jp:443
> > ---
> >
> > My squid.conf:
> > ---
> > # default acls/configs are ignored
> > cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
> > never_direct allow all
> > ssl_bump peek all
> > ssl_bump bump all
> > http_port 3128 ssl-bump \
> > cert=/etc/squid/ssl/ca.crt \
> > key=/etc/squid/ssl/ca.key \
> > generate-host-certificates=on \
> > dynamic_cert_mem_cache_size=4MB
> > ---
> >
> > From the cache_peer port, someone may notice that I'm using privoxy.
> > That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
> > or I can't ever access some sites.
> >
> > Here is some of my experiments:
> > 1) Remove "never_direct"
> > Then ssl_bump works as expected, but all traffic doesn't goes through
> > the SOCKS5 proxy. So a lot of sites I can't access.
> >
> > 2) Use local 8118 proxy
> > That works fine without any problem, but SSL_dump is needed...
> > So just prove privoxy are working.
> >
> > Any clue?
> >
> > Thanks
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVmotBAAoJENNXIZxhPexG0PQIAJ0Cy3o/diVtZsCYPTZ5At8K
> RuP3wHjahKhXj3xZjLiE+QKWvfr1ehZNWSj4wHF616ciX2w23QbghqNIBbV7Awpl
> 7JrTIv3L2nR/19uWgmr2FnhCKf2gSeC9j9Za0aBPAv3PoPwkMNmLbdlwq3mG8pey
> 6Tk8Tsh8+BlfUYXNgO+x/05eyLx6k4ZRV7009E7U3akt5ye+d8vcYXSfwL8+O+ni
> JReTJ2CwXSakb+Olti+ZTJvJWxI49Szdc3FrAyh7cTe2Bgo8hDTyW9Pj5WNvINYG
> +LQZUqOBF/YWtvpXbVVWAcJxYyzTGJJE/1+TtfIFEDsULTe4G74wCqsPu5VanM0=
> =TEp1
> -----END PGP SIGNATURE-----
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150706/c0b9acfd/attachment.htm>

From adam900710 at gmail.com  Mon Jul  6 14:21:45 2015
From: adam900710 at gmail.com (adam900710)
Date: Mon, 6 Jul 2015 22:21:45 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A8B41.8060600@gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
Message-ID: <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>

2015-07-06 22:05 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> My own solution in conjunction with Tor + Privoxy looks like this (Note:
> for Squid 3.4.13):
>
> # Tor acl
> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> ssl_bump none localhost
> ssl_bump none url_nobump
> ssl_bump none dst_nobump
> ssl_bump server-first net_bump
This seems to be old config directive.
New corresponding one shoud be "ssl_bump bump net_bump"

And, no "peek" one? Or that's the problem?

Thanks.
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
> always_direct deny tor_url
> always_direct allow all
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> http_port 3127
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key
I also tried such config.
With such "http_port" and "http_port intercept" with ssl-bump at last.
Although curl works under test, the certificate is not the fake one.
(Issuer is not my fake one)
So I consider the ssl-bump not working in that case.

I'd like to reply when I set it up later to test.

Thanks

> sslproxy_capath /etc/opt/csw/ssl/certs
> sslproxy_options NO_SSLv2 NO_SSLv3
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> Generally,
>
> works like charm.
>
> 06.07.15 15:22, adam900710 ?????:
>> Hi all,
>>
>> I tried to build a ssl bumping proxy with up level proxy, but client
>> failed to connect like the following.
>>
>> The error:
>> ---
>> $ curl https://www.google.co.jp -vvvv -k
>> * Rebuilt URL to: https://www.google.co.jp/
>> * Trying ::1...
>> * Connected to localhost (::1) port 3128 (#0)
>> * Establish HTTP proxy tunnel to www.google.co.jp:443
>>> CONNECT www.google.co.jp:443 HTTP/1.1
>>> Host: www.google.co.jp:443
>>> User-Agent: curl/7.43.0
>>> Proxy-Connection: Keep-Alive
>>>
>> < HTTP/1.1 200 Connection established
>> <
>> * Proxy replied OK to CONNECT request
>> * ALPN, offering http/1.1
>> * Cipher selection:
> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
>> * successfully set certificate verify locations:
>> * CAfile: /etc/ssl/certs/ca-certificates.crt
>> CApath: none
>> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
>> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
>> * Unknown SSL protocol error in connection to www.google.co.jp:443
>> * Closing connection 0
>> curl: (35) Unknown SSL protocol error in connection to
> www.google.co.jp:443
>> ---
>>
>> My squid.conf:
>> ---
>> # default acls/configs are ignored
>> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
>> never_direct allow all
>> ssl_bump peek all
>> ssl_bump bump all
>> http_port 3128 ssl-bump \
>> cert=/etc/squid/ssl/ca.crt \
>> key=/etc/squid/ssl/ca.key \
>> generate-host-certificates=on \
>> dynamic_cert_mem_cache_size=4MB
>> ---
>>
>> From the cache_peer port, someone may notice that I'm using privoxy.
>> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
>> or I can't ever access some sites.
>>
>> Here is some of my experiments:
>> 1) Remove "never_direct"
>> Then ssl_bump works as expected, but all traffic doesn't goes through
>> the SOCKS5 proxy. So a lot of sites I can't access.
>>
>> 2) Use local 8118 proxy
>> That works fine without any problem, but SSL_dump is needed...
>> So just prove privoxy are working.
>>
>> Any clue?
>>
>> Thanks
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVmotBAAoJENNXIZxhPexG0PQIAJ0Cy3o/diVtZsCYPTZ5At8K
> RuP3wHjahKhXj3xZjLiE+QKWvfr1ehZNWSj4wHF616ciX2w23QbghqNIBbV7Awpl
> 7JrTIv3L2nR/19uWgmr2FnhCKf2gSeC9j9Za0aBPAv3PoPwkMNmLbdlwq3mG8pey
> 6Tk8Tsh8+BlfUYXNgO+x/05eyLx6k4ZRV7009E7U3akt5ye+d8vcYXSfwL8+O+ni
> JReTJ2CwXSakb+Olti+ZTJvJWxI49Szdc3FrAyh7cTe2Bgo8hDTyW9Pj5WNvINYG
> +LQZUqOBF/YWtvpXbVVWAcJxYyzTGJJE/1+TtfIFEDsULTe4G74wCqsPu5VanM0=
> =TEp1
> -----END PGP SIGNATURE-----
>


From yvoinov at gmail.com  Mon Jul  6 14:23:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 6 Jul 2015 20:23:21 +0600
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
Message-ID: <559A8F59.90606@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I use 3.4 version. Yes, this is old directives.

3.5.x, on my opinion, don't do SSL Bump in NAT transparent interception
environment.

06.07.15 20:21, adam900710 ?????:
> 2015-07-06 22:05 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>
> My own solution in conjunction with Tor + Privoxy looks like this (Note:
> for Squid 3.4.13):
>
> # Tor acl
> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>
> # SSL bump rules
> sslproxy_cert_error allow all
> ssl_bump none localhost
> ssl_bump none url_nobump
> ssl_bump none dst_nobump
> ssl_bump server-first net_bump
> > This seems to be old config directive.
> > New corresponding one shoud be "ssl_bump bump net_bump"
>
> > And, no "peek" one? Or that's the problem?
>
> > Thanks.
>
> # Privoxy+Tor access rules
> never_direct allow tor_url
> always_direct deny tor_url
> always_direct allow all
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Local Privoxy is cache parent
> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>
> cache_peer_access 127.0.0.1 allow tor_url
> cache_peer_access 127.0.0.1 deny all
>
> http_port 3127
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key
> > I also tried such config.
> > With such "http_port" and "http_port intercept" with ssl-bump at last.
> > Although curl works under test, the certificate is not the fake one.
> > (Issuer is not my fake one)
> > So I consider the ssl-bump not working in that case.
>
> > I'd like to reply when I set it up later to test.
>
> > Thanks
>
> sslproxy_capath /etc/opt/csw/ssl/certs
> sslproxy_options NO_SSLv2 NO_SSLv3
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
-M 4MB
>
> Generally,
>
> works like charm.
>
> 06.07.15 15:22, adam900710 ?????:
> >>> Hi all,
> >>>
> >>> I tried to build a ssl bumping proxy with up level proxy, but client
> >>> failed to connect like the following.
> >>>
> >>> The error:
> >>> ---
> >>> $ curl https://www.google.co.jp -vvvv -k
> >>> * Rebuilt URL to: https://www.google.co.jp/
> >>> * Trying ::1...
> >>> * Connected to localhost (::1) port 3128 (#0)
> >>> * Establish HTTP proxy tunnel to www.google.co.jp:443
> >>>> CONNECT www.google.co.jp:443 HTTP/1.1
> >>>> Host: www.google.co.jp:443
> >>>> User-Agent: curl/7.43.0
> >>>> Proxy-Connection: Keep-Alive
> >>>>
> >>> < HTTP/1.1 200 Connection established
> >>> <
> >>> * Proxy replied OK to CONNECT request
> >>> * ALPN, offering http/1.1
> >>> * Cipher selection:
> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> >>> * successfully set certificate verify locations:
> >>> * CAfile: /etc/ssl/certs/ca-certificates.crt
> >>> CApath: none
> >>> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> >>> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> >>> * Unknown SSL protocol error in connection to www.google.co.jp:443
> >>> * Closing connection 0
> >>> curl: (35) Unknown SSL protocol error in connection to
> www.google.co.jp:443
> >>> ---
> >>>
> >>> My squid.conf:
> >>> ---
> >>> # default acls/configs are ignored
> >>> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
> >>> never_direct allow all
> >>> ssl_bump peek all
> >>> ssl_bump bump all
> >>> http_port 3128 ssl-bump \
> >>> cert=/etc/squid/ssl/ca.crt \
> >>> key=/etc/squid/ssl/ca.key \
> >>> generate-host-certificates=on \
> >>> dynamic_cert_mem_cache_size=4MB
> >>> ---
> >>>
> >>> From the cache_peer port, someone may notice that I'm using privoxy.
> >>> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
> >>> or I can't ever access some sites.
> >>>
> >>> Here is some of my experiments:
> >>> 1) Remove "never_direct"
> >>> Then ssl_bump works as expected, but all traffic doesn't goes through
> >>> the SOCKS5 proxy. So a lot of sites I can't access.
> >>>
> >>> 2) Use local 8118 proxy
> >>> That works fine without any problem, but SSL_dump is needed...
> >>> So just prove privoxy are working.
> >>>
> >>> Any clue?
> >>>
> >>> Thanks
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>
>>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVmo9ZAAoJENNXIZxhPexGjzsIALCunLEQOJGKkcm0V0wr3QTQ
xdfkLvJTh9i5sJNaMGbfuE2SCYIERf7HOTu9vNFpFwZBZoQTiMqud1v8KQkzGXTC
xXCjlLAu937DJ+cJoeWNw+wacCB5wBFp4GoonoF3zf2HdIu76u5BQn2WeFZEfnN0
G1WNMh2j7BlCOgRzI7cPnFZPzomcwlCRm7VqfgmadBMU9NpP3w+iVlngGTbt0WOu
Apf6ktZpumfvu68hu0I1Vtn746Dz/U+mmU8Ue+FBga5wyYW6JSMMAQOdsZTeXLnh
Iyu56A47ouNkugcueeuLOXbVlE9N44KpBc96QkXdOvKyx+VemRzaCrMYlvaFO1U=
=Mt1T
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150706/60c588c6/attachment.htm>

From david at articatech.com  Mon Jul  6 16:49:52 2015
From: david at articatech.com (David Touzeau)
Date: Mon, 06 Jul 2015 18:49:52 +0200
Subject: [squid-users] [3.5.5]: BUG 3279: HTTP reply without Date
Message-ID: <559AB1B0.6090903@articatech.com>

Dear

I'm using 3.5.5-20150528-r13841
After this error, the kid crash
How can fix this issue ?


2015/06/12 08:37:22 kid1| BUG 3279: HTTP reply without Date:
2015/06/12 08:37:22 kid1| StoreEntry->key: 9A3B8E1EFB517CD386A1CBF13E477C5B
2015/06/12 08:37:22 kid1| StoreEntry->next: 0
2015/06/12 08:37:22 kid1| StoreEntry->mem_obj: 0x35c74e0
2015/06/12 08:37:22 kid1| StoreEntry->timestamp: -1
2015/06/12 08:37:22 kid1| StoreEntry->lastref: 1434091042
2015/06/12 08:37:22 kid1| StoreEntry->expires: -1
2015/06/12 08:37:22 kid1| StoreEntry->lastmod: -1
2015/06/12 08:37:22 kid1| StoreEntry->swap_file_sz: 0
2015/06/12 08:37:22 kid1| StoreEntry->refcount: 1
2015/06/12 08:37:22 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/06/12 08:37:22 kid1| StoreEntry->swap_dirn: -1
2015/06/12 08:37:22 kid1| StoreEntry->swap_filen: -1
2015/06/12 08:37:22 kid1| StoreEntry->lock_count: 2
2015/06/12 08:37:22 kid1| StoreEntry->mem_status: 0
2015/06/12 08:37:22 kid1| StoreEntry->ping_status: 2
2015/06/12 08:37:22 kid1| StoreEntry->store_status: 1
2015/06/12 08:37:22 kid1| StoreEntry->swap_status: 0
2015/06/12 08:37:22 kid1| WARNING: swapfile header inconsistent with 
available data
FATAL: Received Segment Violation...dying.
2015/06/12 08:37:22 kid1| Closing HTTP port 0.0.0.0:63375
2015/06/12 08:37:22 kid1| Closing HTTP port 0.0.0.0:3128
2015/06/12 08:37:22 kid1| storeDirWriteCleanLogs: Starting...
2015/06/12 08:37:22 kid1|   Finished.  Wrote 45782 entries.
2015/06/12 08:37:22 kid1|   Took 0.02 seconds (2097109.61 entries/sec).
CPU Usage: 0.536 seconds = 0.288 user + 0.248 sys
Maximum Resident Size: 104224 KB
Page faults with physical i/o: 0
2015/06/12 08:37:25 kid1| Set Current Directory to /var/squid/cache
2015/06/12 08:37:25 kid1| Starting Squid Cache version 
3.5.5-20150528-r13841 for x86_64-pc-linux-gnu...
2015/06/12 08:37:25 kid1| Service Name: squid
2015/06/12 08:37:25 kid1| Process ID 17530
2015/06/12 08:37:25 kid1| Process Roles: worker
2015/06/12 08:37:25 kid1| With 8192 file descriptors available
2015/06/12 08:37:25 kid1| Initializing IP Cache...
2015/06/12 08:37:25 kid1| DNS Socket created at [::], FD 6
2015/06/12 08:37:25 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/06/12 08:37:25 kid1| Adding nameserver 10.0.0.2 from squid.conf
2015/06/12 08:37:25 kid1| Adding nameserver 10.0.0.15 from squid.conf
2015/06/12 08:37:25 kid1| Adding nameserver 10.0.0.19 from squid.conf
2015/06/12 08:37:25 kid1| helperOpenServers: Starting 5/30 'ntlm_auth' 
processes
2015/06/12 08:37:25 kid1| helperOpenServers: Starting 5/5 'ntlm_auth' 
processes
2015/06/12 08:37:25 kid1| helperOpenServers: Starting 5/30 
'external_acl_squid_ldap.php' processes
2015/06/12 08:37:25 kid1| Logfile: opening log 
stdio:/var/log/squid/squidtail.log
2015/06/12 08:37:25 kid1| Logfile: opening log 
stdio:/var/log/squid/access.log
2015/06/12 08:37:25 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2015/06/12 08:37:25 kid1| Store logging disabled
2015/06/12 08:37:25 kid1| Swap maxSize 32768000 + 262144 KB, estimated 
2540780 objects
2015/06/12 08:37:25 kid1| Target number of buckets: 127039
2015/06/12 08:37:25 kid1| Using 131072 Store buckets
2015/06/12 08:37:25 kid1| Max Mem  size: 262144 KB
2015/06/12 08:37:25 kid1| Max Swap size: 32768000 KB
2015/06/12 08:37:25 kid1| Rebuilding storage in 
/home/squid/cache-default/proxy-caches/big-cpu1 (clean log)
2015/06/12 08:37:25 kid1| Rebuilding storage in 
/home/squid/cache-default/proxy-caches/small-cpu1 (clean log)
2015/06/12 08:37:25 kid1| Rebuilding storage in 
/home/squid/cache-default (clean log)
2015/06/12 08:37:25 kid1| Using Least Load store dir selection
2015/06/12 08:37:25 kid1| Set Current Directory to /var/squid/cache
2015/06/12 08:37:26 kid1| Finished loading MIME types and icons.
2015/06/12 08:37:26 kid1| HTCP Disabled.
2015/06/12 08:37:26 kid1| Squid plugin modules loaded: 0
2015/06/12 08:37:26 kid1| Adaptation support is off.
2015/06/12 08:37:26 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:63375 remote=[::] FD 47 flags=9
2015/06/12 08:37:26 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:3128 remote=[::] FD 48 flags=9
2015/06/12 08:37:26 kid1| Done reading 
/home/squid/cache-default/proxy-caches/big-cpu1 swaplog (428 entries)
2015/06/12 08:37:26 kid1| Done reading /home/squid/cache-default swaplog 
(0 entries)
2015/06/12 08:37:26 kid1| Store rebuilding is 0.00% complete
2015/06/12 08:37:26 kid1| Done reading 
/home/squid/cache-default/proxy-caches/small-cpu1 swaplog (45354 entries)
2015/06/12 08:37:26 kid1| Finished rebuilding storage from disk.
2015/06/12 08:37:26 kid1|     45782 Entries scanned
2015/06/12 08:37:26 kid1|         0 Invalid entries.
2015/06/12 08:37:26 kid1|         0 With invalid flags.
2015/06/12 08:37:26 kid1|     45782 Objects loaded.
2015/06/12 08:37:26 kid1|         0 Objects expired.
2015/06/12 08:37:26 kid1|         0 Objects cancelled.
2015/06/12 08:37:26 kid1|         0 Duplicate URLs purged.
2015/06/12 08:37:26 kid1|         0 Swapfile clashes avoided.
2015/06/12 08:37:26 kid1|   Took 0.65 seconds (70812.97 objects/sec).
2015/06/12 08:37:26 kid1| Beginning Validation Procedure
2015/06/12 08:37:26 kid1|   Completed Validation Procedure
2015/06/12 08:37:26 kid1|   Validated 45781 Entries
2015/06/12 08:37:26 kid1|   store_swap_size = 1924652.00 KB
2015/06/12 08:37:26 kid1| storeLateRelease: released 0 objects
2015/06/12 08:37:30 kid1| Starting new ntlmauthenticator helpers...
2015/06/12 08:37:30 kid1| helperOpenServers: Starting 3/30 'ntlm_auth' 
processes
2015/06/12 08:37:31 kid1| WARNING: 1 swapin MD5 mismatches
2015/06/12 08:37:31 kid1| Could not parse headers from on disk object
2015/06/12 08:37:31 kid1| BUG 3279: HTTP reply without Date:
2015/06/12 08:37:31 kid1| StoreEntry->key: C956AABDFA564FC6E09DCFD827A0EF1F
2015/06/12 08:37:31 kid1| StoreEntry->next: 0
2015/06/12 08:37:31 kid1| StoreEntry->mem_obj: 0x20a8350
2015/06/12 08:37:31 kid1| StoreEntry->timestamp: -1
2015/06/12 08:37:31 kid1| StoreEntry->lastref: 1434091051
2015/06/12 08:37:31 kid1| StoreEntry->expires: -1
2015/06/12 08:37:31 kid1| StoreEntry->lastmod: -1
2015/06/12 08:37:31 kid1| StoreEntry->swap_file_sz: 0
2015/06/12 08:37:31 kid1| StoreEntry->refcount: 1
2015/06/12 08:37:31 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/06/12 08:37:31 kid1| StoreEntry->swap_dirn: -1
2015/06/12 08:37:31 kid1| StoreEntry->swap_filen: -1
2015/06/12 08:37:31 kid1| StoreEntry->lock_count: 2
2015/06/12 08:37:31 kid1| StoreEntry->mem_status: 0
2015/06/12 08:37:31 kid1| StoreEntry->ping_status: 2
2015/06/12 08:37:31 kid1| StoreEntry->store_status: 1
2015/06/12 08:37:31 kid1| StoreEntry->swap_status: 0
2015/06/12 08:37:31 kid1| WARNING: swapfile header inconsistent with 
available data
FATAL: Received Segment Violation...dying.
2015/06/12 08:37:31 kid1| Closing HTTP port 0.0.0.0:63375
2015/06/12 08:37:31 kid1| Closing HTTP port 0.0.0.0:3128
2015/06/12 08:37:31 kid1| storeDirWriteCleanLogs: Starting...
2015/06/12 08:37:31 kid1|   Finished.  Wrote 45784 entries.
2015/06/12 08:37:31 kid1|   Took 0.01 seconds (4493473.35 entries/sec).
CPU Usage: 0.632 seconds = 0.320 user + 0.312 sys
Maximum Resident Size: 107936 KB
Page faults with physical i/o: 0
2015/06/12 08:37:34 kid1| Set Current Directory to /var/squid/cache
2015/06/12 08:37:34 kid1| Starting Squid Cache version 
3.5.5-20150528-r13841 for x86_64-pc-linux-gnu...
2015/06/12 08:37:34 kid1| Service Name: squid
2015/06/12 08:37:34 kid1| Process ID 18517
2015/06/12 08:37:34 kid1| Process Roles: worker
2015/06/12 08:37:34 kid1| With 8192 file descriptors available
2015/06/12 08:37:34 kid1| Initializing IP Cache...
2015/06/12 08:37:34 kid1| DNS Socket created at [::], FD 6
2015/06/12 08:37:34 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/06/12 08:37:34 kid1| Adding nameserver 10.0.0.2 from squid.conf
2015/06/12 08:37:34 kid1| Adding nameserver 10.0.0.15 from squid.conf
2015/06/12 08:37:34 kid1| Adding nameserver 10.0.0.19 from squid.conf
2015/06/12 08:37:34 kid1| helperOpenServers: Starting 5/30 'ntlm_auth' 
processes
2015/06/12 08:37:34 kid1| helperOpenServers: Starting 5/5 'ntlm_auth' 
processes
2015/06/12 08:37:34 kid1| helperOpenServers: Starting 5/30 
'external_acl_squid_ldap.php' processes
2015/06/12 08:37:34 kid1| Logfile: opening log 
stdio:/var/log/squid/squidtail.log
2015/06/12 08:37:34 kid1| Logfile: opening log 
stdio:/var/log/squid/access.log
2015/06/12 08:37:34 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2015/06/12 08:37:34 kid1| Store logging disabled
2015/06/12 08:37:34 kid1| Swap maxSize 32768000 + 262144 KB, estimated 
2540780 objects
2015/06/12 08:37:34 kid1| Target number of buckets: 127039
2015/06/12 08:37:34 kid1| Using 131072 Store buckets
2015/06/12 08:37:34 kid1| Max Mem  size: 262144 KB
2015/06/12 08:37:34 kid1| Max Swap size: 32768000 KB
2015/06/12 08:37:34 kid1| Rebuilding storage in 
/home/squid/cache-default/proxy-caches/big-cpu1 (clean log)
2015/06/12 08:37:34 kid1| Rebuilding storage in 
/home/squid/cache-default/proxy-caches/small-cpu1 (clean log)
2015/06/12 08:37:34 kid1| Rebuilding storage in 
/home/squid/cache-default (clean log)
2015/06/12 08:37:34 kid1| Using Least Load store dir selection
2015/06/12 08:37:34 kid1| Set Current Directory to /var/squid/cache
2015/06/12 08:37:34 kid1| Finished loading MIME types and icons.
2015/06/12 08:37:34 kid1| HTCP Disabled.
2015/06/12 08:37:34 kid1| Squid plugin modules loaded: 0
2015/06/12 08:37:34 kid1| Adaptation support is off.
2015/06/12 08:37:34 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:63375 remote=[::] FD 47 flags=9
2015/06/12 08:37:34 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:3128 remote=[::] FD 48 flags=9
2015/06/12 08:37:34 kid1| Done reading 
/home/squid/cache-default/proxy-caches/big-cpu1 swaplog (428 entries)
2015/06/12 08:37:34 kid1| Store rebuilding is 8.82% complete
2015/06/12 08:37:34 kid1| Done reading /home/squid/cache-default swaplog 
(0 entries)
2015/06/12 08:37:34 kid1| Done reading 
/home/squid/cache-default/proxy-caches/small-cpu1 swaplog (45356 entries)
2015/06/12 08:37:34 kid1| Finished rebuilding storage from disk.
2015/06/12 08:37:34 kid1|     45784 Entries scanned
2015/06/12 08:37:34 kid1|         0 Invalid entries.
2015/06/12 08:37:34 kid1|         0 With invalid flags.
2015/06/12 08:37:34 kid1|     45784 Objects loaded.
2015/06/12 08:37:34 kid1|         0 Objects expired.
2015/06/12 08:37:34 kid1|         0 Objects cancelled.
2015/06/12 08:37:34 kid1|         0 Duplicate URLs purged.
2015/06/12 08:37:34 kid1|         0 Swapfile clashes avoided.
2015/06/12 08:37:34 kid1|   Took 0.64 seconds (71185.58 objects/sec).
2015/06/12 08:37:34 kid1| Beginning Validation Procedure
2015/06/12 08:37:34 kid1|   Completed Validation Procedure
2015/06/12 08:37:34 kid1|   Validated 45783 Entries
2015/06/12 08:37:34 kid1|   store_swap_size = 1924660.00 KB
2015/06/12 08:37:35 kid1| storeLateRelease: released 0 objects
2015/06/12 08:37:53 kid1| WARNING: 1 swapin MD5 mismatches
2015/06/12 08:37:53 kid1| Could not parse headers from on disk object
2015/06/12 08:37:53 kid1| BUG 3279: HTTP reply without Date:
2015/06/12 08:37:53 kid1| StoreEntry->key: 3D4194B57CC4380C6551A1D2D86CC9D4
2015/06/12 08:37:53 kid1| StoreEntry->next: 0
2015/06/12 08:37:53 kid1| StoreEntry->mem_obj: 0x3156320
2015/06/12 08:37:53 kid1| StoreEntry->timestamp: -1
2015/06/12 08:37:53 kid1| StoreEntry->lastref: 1434091073
2015/06/12 08:37:53 kid1| StoreEntry->expires: -1
2015/06/12 08:37:53 kid1| StoreEntry->lastmod: -1
2015/06/12 08:37:53 kid1| StoreEntry->swap_file_sz: 0
2015/06/12 08:37:53 kid1| StoreEntry->refcount: 1
2015/06/12 08:37:53 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/06/12 08:37:53 kid1| StoreEntry->swap_dirn: -1
2015/06/12 08:37:53 kid1| StoreEntry->swap_filen: -1
2015/06/12 08:37:53 kid1| StoreEntry->lock_count: 2
2015/06/12 08:37:53 kid1| StoreEntry->mem_status: 0
2015/06/12 08:37:53 kid1| StoreEntry->ping_status: 2
2015/06/12 08:37:53 kid1| StoreEntry->store_status: 1
2015/06/12 08:37:53 kid1| StoreEntry->swap_status: 0
2015/06/12 08:37:53 kid1| WARNING: swapfile header inconsistent with 
available data
FATAL: Received Segment Violation...dying.


From squid3 at treenet.co.nz  Mon Jul  6 17:09:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 07 Jul 2015 05:09:03 +1200
Subject: [squid-users] [3.5.5]: BUG 3279: HTTP reply without Date
In-Reply-To: <559AB1B0.6090903@articatech.com>
References: <559AB1B0.6090903@articatech.com>
Message-ID: <559AB62F.8010001@treenet.co.nz>

On 7/07/2015 4:49 a.m., David Touzeau wrote:
> Dear
> 
> I'm using 3.5.5-20150528-r13841
> After this error, the kid crash
> How can fix this issue ?

Please try 3.5.6. If the problem persists you will need to run Squid
under gdb and obtain a backtrace.


Amos



From david at articatech.com  Mon Jul  6 20:04:31 2015
From: david at articatech.com (David Touzeau)
Date: Mon, 06 Jul 2015 22:04:31 +0200
Subject: [squid-users] [3.5.5]: BUG 3279: HTTP reply without Date
In-Reply-To: <559AB62F.8010001@treenet.co.nz>
References: <559AB1B0.6090903@articatech.com> <559AB62F.8010001@treenet.co.nz>
Message-ID: <559ADF4F.7080400@articatech.com>

Thanks Amos,

i will test it!!

Le 06/07/2015 19:09, Amos Jeffries a ?crit :
> On 7/07/2015 4:49 a.m., David Touzeau wrote:
>> Dear
>>
>> I'm using 3.5.5-20150528-r13841
>> After this error, the kid crash
>> How can fix this issue ?
> Please try 3.5.6. If the problem persists you will need to run Squid
> under gdb and obtain a backtrace.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From adam900710 at gmail.com  Tue Jul  7 00:54:31 2015
From: adam900710 at gmail.com (adam900710)
Date: Tue, 7 Jul 2015 08:54:31 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <559A8F59.90606@gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
Message-ID: <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>

Tried your config in my environment.
Although curl can get to the sites through privoxy, just like the log says:
------
1436230195.213    432 ::1 TCP_TUNNEL/200 4146 CONNECT
www.google.com:443 - FIRSTUP_PARENT/127.0.0.1 -
------

But the certificate got is still the original one, not the fake one:
------
* Server certificate:
*      subject: C=US; ST=California; L=Mountain View; O=Google Inc;
CN=www.google.com
*      start date: 2015-06-18 08:52:56 GMT
*      expire date: 2015-09-16 00:00:00 GMT
*      issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
*      SSL certificate verify ok.
------

Does it works only in 3.4?
Anyway, I'll try to downgrade squid and try it again.

Thanks

2015-07-06 22:23 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> I use 3.4 version. Yes, this is old directives.
>
> 3.5.x, on my opinion, don't do SSL Bump in NAT transparent interception
> environment.
>
> 06.07.15 20:21, adam900710 ?????:
>> 2015-07-06 22:05 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>>
>> My own solution in conjunction with Tor + Privoxy looks like this (Note:
>> for Squid 3.4.13):
>>
>> # Tor acl
>> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>>
>> # SSL bump rules
>> sslproxy_cert_error allow all
>> ssl_bump none localhost
>> ssl_bump none url_nobump
>> ssl_bump none dst_nobump
>> ssl_bump server-first net_bump
>> > This seems to be old config directive.
>> > New corresponding one shoud be "ssl_bump bump net_bump"
>>
>> > And, no "peek" one? Or that's the problem?
>>
>> > Thanks.
>>
>> # Privoxy+Tor access rules
>> never_direct allow tor_url
>> always_direct deny tor_url
>> always_direct allow all
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # Local Privoxy is cache parent
>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>
>> cache_peer_access 127.0.0.1 allow tor_url
>> cache_peer_access 127.0.0.1 deny all
>>
>> http_port 3127
>> http_port 3128 intercept
>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
>> key=/usr/local/squid/etc/rootCA.key
>> > I also tried such config.
>> > With such "http_port" and "http_port intercept" with ssl-bump at last.
>> > Although curl works under test, the certificate is not the fake one.
>> > (Issuer is not my fake one)
>> > So I consider the ssl-bump not working in that case.
>>
>> > I'd like to reply when I set it up later to test.
>>
>> > Thanks
>>
>> sslproxy_capath /etc/opt/csw/ssl/certs
>> sslproxy_options NO_SSLv2 NO_SSLv3
>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>> 4MB
>>
>> Generally,
>>
>> works like charm.
>>
>> 06.07.15 15:22, adam900710 ?????:
>> >>> Hi all,
>> >>>
>> >>> I tried to build a ssl bumping proxy with up level proxy, but client
>> >>> failed to connect like the following.
>> >>>
>> >>> The error:
>> >>> ---
>> >>> $ curl https://www.google.co.jp -vvvv -k
>> >>> * Rebuilt URL to: https://www.google.co.jp/
>> >>> * Trying ::1...
>> >>> * Connected to localhost (::1) port 3128 (#0)
>> >>> * Establish HTTP proxy tunnel to www.google.co.jp:443
>> >>>> CONNECT www.google.co.jp:443 HTTP/1.1
>> >>>> Host: www.google.co.jp:443
>> >>>> User-Agent: curl/7.43.0
>> >>>> Proxy-Connection: Keep-Alive
>> >>>>
>> >>> < HTTP/1.1 200 Connection established
>> >>> <
>> >>> * Proxy replied OK to CONNECT request
>> >>> * ALPN, offering http/1.1
>> >>> * Cipher selection:
>> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
>> >>> * successfully set certificate verify locations:
>> >>> * CAfile: /etc/ssl/certs/ca-certificates.crt
>> >>> CApath: none
>> >>> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
>> >>> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
>> >>> * Unknown SSL protocol error in connection to www.google.co.jp:443
>> >>> * Closing connection 0
>> >>> curl: (35) Unknown SSL protocol error in connection to
>> www.google.co.jp:443
>> >>> ---
>> >>>
>> >>> My squid.conf:
>> >>> ---
>> >>> # default acls/configs are ignored
>> >>> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
>> >>> never_direct allow all
>> >>> ssl_bump peek all
>> >>> ssl_bump bump all
>> >>> http_port 3128 ssl-bump \
>> >>> cert=/etc/squid/ssl/ca.crt \
>> >>> key=/etc/squid/ssl/ca.key \
>> >>> generate-host-certificates=on \
>> >>> dynamic_cert_mem_cache_size=4MB
>> >>> ---
>> >>>
>> >>> From the cache_peer port, someone may notice that I'm using privoxy.
>> >>> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
>> >>> or I can't ever access some sites.
>> >>>
>> >>> Here is some of my experiments:
>> >>> 1) Remove "never_direct"
>> >>> Then ssl_bump works as expected, but all traffic doesn't goes through
>> >>> the SOCKS5 proxy. So a lot of sites I can't access.
>> >>>
>> >>> 2) Use local 8118 proxy
>> >>> That works fine without any problem, but SSL_dump is needed...
>> >>> So just prove privoxy are working.
>> >>>
>> >>> Any clue?
>> >>>
>> >>> Thanks
>> >>> _______________________________________________
>> >>> squid-users mailing list
>> >>> squid-users at lists.squid-cache.org
>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>>
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVmo9ZAAoJENNXIZxhPexGjzsIALCunLEQOJGKkcm0V0wr3QTQ
> xdfkLvJTh9i5sJNaMGbfuE2SCYIERf7HOTu9vNFpFwZBZoQTiMqud1v8KQkzGXTC
> xXCjlLAu937DJ+cJoeWNw+wacCB5wBFp4GoonoF3zf2HdIu76u5BQn2WeFZEfnN0
> G1WNMh2j7BlCOgRzI7cPnFZPzomcwlCRm7VqfgmadBMU9NpP3w+iVlngGTbt0WOu
> Apf6ktZpumfvu68hu0I1Vtn746Dz/U+mmU8Ue+FBga5wyYW6JSMMAQOdsZTeXLnh
> Iyu56A47ouNkugcueeuLOXbVlE9N44KpBc96QkXdOvKyx+VemRzaCrMYlvaFO1U=
> =Mt1T
> -----END PGP SIGNATURE-----
>


From adam900710 at gmail.com  Tue Jul  7 01:12:55 2015
From: adam900710 at gmail.com (adam900710)
Date: Tue, 7 Jul 2015 09:12:55 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
Message-ID: <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>

Some extra clue:

Cache log says:
------
2015/07/07 08:55:54 kid1| Accepting SSL bumped HTTP Socket connections
at local=[::]:3128 remote=[::] FD 23 flags=9
2015/07/07 08:55:55 kid1| storeLateRelease: released 0 objects
2015/07/07 08:55:57 kid1| assertion failed: PeerConnector.cc:116:
"peer->use_ssl"
------

So I tried adding "ssl" at the end of "cache_peer" directive.
And it still fails but with different error, squid error page now.

Google also found some mail archive from Amos, which implies that,
squid doesn't yet support
CONNECT + SSL/TLS cache_peer.
http://squid-web-proxy-cache.1019090.n4.nabble.com/Behind-enemy-lines-squid-behind-proxy-td4668223.html

If so, I think I'd better seek other solutions like use direct_allow
with tsocks/proxychains...

Thanks.

2015-07-07 8:54 GMT+08:00 adam900710 <adam900710 at gmail.com>:
> Tried your config in my environment.
> Although curl can get to the sites through privoxy, just like the log says:
> ------
> 1436230195.213    432 ::1 TCP_TUNNEL/200 4146 CONNECT
> www.google.com:443 - FIRSTUP_PARENT/127.0.0.1 -
> ------
>
> But the certificate got is still the original one, not the fake one:
> ------
> * Server certificate:
> *      subject: C=US; ST=California; L=Mountain View; O=Google Inc;
> CN=www.google.com
> *      start date: 2015-06-18 08:52:56 GMT
> *      expire date: 2015-09-16 00:00:00 GMT
> *      issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
> *      SSL certificate verify ok.
> ------
>
> Does it works only in 3.4?
> Anyway, I'll try to downgrade squid and try it again.
>
> Thanks
>
> 2015-07-06 22:23 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>> I use 3.4 version. Yes, this is old directives.
>>
>> 3.5.x, on my opinion, don't do SSL Bump in NAT transparent interception
>> environment.
>>
>> 06.07.15 20:21, adam900710 ?????:
>>> 2015-07-06 22:05 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>>>
>>> My own solution in conjunction with Tor + Privoxy looks like this (Note:
>>> for Squid 3.4.13):
>>>
>>> # Tor acl
>>> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>>>
>>> # SSL bump rules
>>> sslproxy_cert_error allow all
>>> ssl_bump none localhost
>>> ssl_bump none url_nobump
>>> ssl_bump none dst_nobump
>>> ssl_bump server-first net_bump
>>> > This seems to be old config directive.
>>> > New corresponding one shoud be "ssl_bump bump net_bump"
>>>
>>> > And, no "peek" one? Or that's the problem?
>>>
>>> > Thanks.
>>>
>>> # Privoxy+Tor access rules
>>> never_direct allow tor_url
>>> always_direct deny tor_url
>>> always_direct allow all
>>>
>>> # And finally deny all other access to this proxy
>>> http_access deny all
>>>
>>> # Local Privoxy is cache parent
>>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>>
>>> cache_peer_access 127.0.0.1 allow tor_url
>>> cache_peer_access 127.0.0.1 deny all
>>>
>>> http_port 3127
>>> http_port 3128 intercept
>>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
>>> key=/usr/local/squid/etc/rootCA.key
>>> > I also tried such config.
>>> > With such "http_port" and "http_port intercept" with ssl-bump at last.
>>> > Although curl works under test, the certificate is not the fake one.
>>> > (Issuer is not my fake one)
>>> > So I consider the ssl-bump not working in that case.
>>>
>>> > I'd like to reply when I set it up later to test.
>>>
>>> > Thanks
>>>
>>> sslproxy_capath /etc/opt/csw/ssl/certs
>>> sslproxy_options NO_SSLv2 NO_SSLv3
>>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>>> 4MB
>>>
>>> Generally,
>>>
>>> works like charm.
>>>
>>> 06.07.15 15:22, adam900710 ?????:
>>> >>> Hi all,
>>> >>>
>>> >>> I tried to build a ssl bumping proxy with up level proxy, but client
>>> >>> failed to connect like the following.
>>> >>>
>>> >>> The error:
>>> >>> ---
>>> >>> $ curl https://www.google.co.jp -vvvv -k
>>> >>> * Rebuilt URL to: https://www.google.co.jp/
>>> >>> * Trying ::1...
>>> >>> * Connected to localhost (::1) port 3128 (#0)
>>> >>> * Establish HTTP proxy tunnel to www.google.co.jp:443
>>> >>>> CONNECT www.google.co.jp:443 HTTP/1.1
>>> >>>> Host: www.google.co.jp:443
>>> >>>> User-Agent: curl/7.43.0
>>> >>>> Proxy-Connection: Keep-Alive
>>> >>>>
>>> >>> < HTTP/1.1 200 Connection established
>>> >>> <
>>> >>> * Proxy replied OK to CONNECT request
>>> >>> * ALPN, offering http/1.1
>>> >>> * Cipher selection:
>>> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
>>> >>> * successfully set certificate verify locations:
>>> >>> * CAfile: /etc/ssl/certs/ca-certificates.crt
>>> >>> CApath: none
>>> >>> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
>>> >>> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
>>> >>> * Unknown SSL protocol error in connection to www.google.co.jp:443
>>> >>> * Closing connection 0
>>> >>> curl: (35) Unknown SSL protocol error in connection to
>>> www.google.co.jp:443
>>> >>> ---
>>> >>>
>>> >>> My squid.conf:
>>> >>> ---
>>> >>> # default acls/configs are ignored
>>> >>> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
>>> >>> never_direct allow all
>>> >>> ssl_bump peek all
>>> >>> ssl_bump bump all
>>> >>> http_port 3128 ssl-bump \
>>> >>> cert=/etc/squid/ssl/ca.crt \
>>> >>> key=/etc/squid/ssl/ca.key \
>>> >>> generate-host-certificates=on \
>>> >>> dynamic_cert_mem_cache_size=4MB
>>> >>> ---
>>> >>>
>>> >>> From the cache_peer port, someone may notice that I'm using privoxy.
>>> >>> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
>>> >>> or I can't ever access some sites.
>>> >>>
>>> >>> Here is some of my experiments:
>>> >>> 1) Remove "never_direct"
>>> >>> Then ssl_bump works as expected, but all traffic doesn't goes through
>>> >>> the SOCKS5 proxy. So a lot of sites I can't access.
>>> >>>
>>> >>> 2) Use local 8118 proxy
>>> >>> That works fine without any problem, but SSL_dump is needed...
>>> >>> So just prove privoxy are working.
>>> >>>
>>> >>> Any clue?
>>> >>>
>>> >>> Thanks
>>> >>> _______________________________________________
>>> >>> squid-users mailing list
>>> >>> squid-users at lists.squid-cache.org
>>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEcBAEBCAAGBQJVmo9ZAAoJENNXIZxhPexGjzsIALCunLEQOJGKkcm0V0wr3QTQ
>> xdfkLvJTh9i5sJNaMGbfuE2SCYIERf7HOTu9vNFpFwZBZoQTiMqud1v8KQkzGXTC
>> xXCjlLAu937DJ+cJoeWNw+wacCB5wBFp4GoonoF3zf2HdIu76u5BQn2WeFZEfnN0
>> G1WNMh2j7BlCOgRzI7cPnFZPzomcwlCRm7VqfgmadBMU9NpP3w+iVlngGTbt0WOu
>> Apf6ktZpumfvu68hu0I1Vtn746Dz/U+mmU8Ue+FBga5wyYW6JSMMAQOdsZTeXLnh
>> Iyu56A47ouNkugcueeuLOXbVlE9N44KpBc96QkXdOvKyx+VemRzaCrMYlvaFO1U=
>> =Mt1T
>> -----END PGP SIGNATURE-----
>>


From adam900710 at gmail.com  Tue Jul  7 01:24:34 2015
From: adam900710 at gmail.com (adam900710)
Date: Tue, 7 Jul 2015 09:24:34 +0800
Subject: [squid-users] ssl_bump with cache_peer problem: Handshake fail
 after Client Hello.
In-Reply-To: <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
References: <CAFy8SQXgDUn=Gd-2uWwyVUrJ8imsodLgnPXxB90=yUwpbj9LaA@mail.gmail.com>
 <559A8B41.8060600@gmail.com>
 <CAFy8SQUu3wjMdakm09PSpm-2FjYidyQ9=f2N1AFDt1+sJ2VS-Q@mail.gmail.com>
 <559A8F59.90606@gmail.com>
 <CAFy8SQXT=Y0spM2DwBGvRhSbSAhWpMMH9jg_VSMghhxXeaVrvQ@mail.gmail.com>
 <CAFy8SQWRG0i7sEQNBQ1f6nhRrutgpnG0vjEnmdT8K8oytp6iMQ@mail.gmail.com>
Message-ID: <CAFy8SQWwUmqWe1acL8+id_rgw2LkBuxgVgqBUcFzttr1dGxSQQ@mail.gmail.com>

OK, it seems that CONNECT+SSL/TLS is really not supported yet...

So I use proxychains and allow_direct without cache_peer.
And things works:
------
* ALPN, server did not agree to a protocol
* Server certificate:
*      subject: CN=www.google.com
*      start date: 2015-07-06 07:17:41 GMT
*      expire date: 2018-04-25 07:17:41 GMT
*      issuer: C=XX; ST=XXXXX; L=XXXXX; O=XXXXX; OU=Linux; CN=Splice
SSL; emailAddress=XXXXX at XXXXX
*      SSL certificate verify result: self signed certificate in
certificate chain (19), continuing anyway.
------

Thanks everyone for the help.

2015-07-07 9:12 GMT+08:00 adam900710 <adam900710 at gmail.com>:
> Some extra clue:
>
> Cache log says:
> ------
> 2015/07/07 08:55:54 kid1| Accepting SSL bumped HTTP Socket connections
> at local=[::]:3128 remote=[::] FD 23 flags=9
> 2015/07/07 08:55:55 kid1| storeLateRelease: released 0 objects
> 2015/07/07 08:55:57 kid1| assertion failed: PeerConnector.cc:116:
> "peer->use_ssl"
> ------
>
> So I tried adding "ssl" at the end of "cache_peer" directive.
> And it still fails but with different error, squid error page now.
>
> Google also found some mail archive from Amos, which implies that,
> squid doesn't yet support
> CONNECT + SSL/TLS cache_peer.
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Behind-enemy-lines-squid-behind-proxy-td4668223.html
>
> If so, I think I'd better seek other solutions like use direct_allow
> with tsocks/proxychains...
>
> Thanks.
>
> 2015-07-07 8:54 GMT+08:00 adam900710 <adam900710 at gmail.com>:
>> Tried your config in my environment.
>> Although curl can get to the sites through privoxy, just like the log says:
>> ------
>> 1436230195.213    432 ::1 TCP_TUNNEL/200 4146 CONNECT
>> www.google.com:443 - FIRSTUP_PARENT/127.0.0.1 -
>> ------
>>
>> But the certificate got is still the original one, not the fake one:
>> ------
>> * Server certificate:
>> *      subject: C=US; ST=California; L=Mountain View; O=Google Inc;
>> CN=www.google.com
>> *      start date: 2015-06-18 08:52:56 GMT
>> *      expire date: 2015-09-16 00:00:00 GMT
>> *      issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
>> *      SSL certificate verify ok.
>> ------
>>
>> Does it works only in 3.4?
>> Anyway, I'll try to downgrade squid and try it again.
>>
>> Thanks
>>
>> 2015-07-06 22:23 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA256
>>>
>>> I use 3.4 version. Yes, this is old directives.
>>>
>>> 3.5.x, on my opinion, don't do SSL Bump in NAT transparent interception
>>> environment.
>>>
>>> 06.07.15 20:21, adam900710 ?????:
>>>> 2015-07-06 22:05 GMT+08:00 Yuri Voinov <yvoinov at gmail.com>:
>>>>>
>>>> My own solution in conjunction with Tor + Privoxy looks like this (Note:
>>>> for Squid 3.4.13):
>>>>
>>>> # Tor acl
>>>> acl tor_url url_regex -i "/usr/local/squid/etc/url.tor"
>>>>
>>>> # SSL bump rules
>>>> sslproxy_cert_error allow all
>>>> ssl_bump none localhost
>>>> ssl_bump none url_nobump
>>>> ssl_bump none dst_nobump
>>>> ssl_bump server-first net_bump
>>>> > This seems to be old config directive.
>>>> > New corresponding one shoud be "ssl_bump bump net_bump"
>>>>
>>>> > And, no "peek" one? Or that's the problem?
>>>>
>>>> > Thanks.
>>>>
>>>> # Privoxy+Tor access rules
>>>> never_direct allow tor_url
>>>> always_direct deny tor_url
>>>> always_direct allow all
>>>>
>>>> # And finally deny all other access to this proxy
>>>> http_access deny all
>>>>
>>>> # Local Privoxy is cache parent
>>>> cache_peer 127.0.0.1 parent 8118 0 no-query no-digest default
>>>>
>>>> cache_peer_access 127.0.0.1 allow tor_url
>>>> cache_peer_access 127.0.0.1 deny all
>>>>
>>>> http_port 3127
>>>> http_port 3128 intercept
>>>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>>>> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
>>>> key=/usr/local/squid/etc/rootCA.key
>>>> > I also tried such config.
>>>> > With such "http_port" and "http_port intercept" with ssl-bump at last.
>>>> > Although curl works under test, the certificate is not the fake one.
>>>> > (Issuer is not my fake one)
>>>> > So I consider the ssl-bump not working in that case.
>>>>
>>>> > I'd like to reply when I set it up later to test.
>>>>
>>>> > Thanks
>>>>
>>>> sslproxy_capath /etc/opt/csw/ssl/certs
>>>> sslproxy_options NO_SSLv2 NO_SSLv3
>>>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>>>> 4MB
>>>>
>>>> Generally,
>>>>
>>>> works like charm.
>>>>
>>>> 06.07.15 15:22, adam900710 ?????:
>>>> >>> Hi all,
>>>> >>>
>>>> >>> I tried to build a ssl bumping proxy with up level proxy, but client
>>>> >>> failed to connect like the following.
>>>> >>>
>>>> >>> The error:
>>>> >>> ---
>>>> >>> $ curl https://www.google.co.jp -vvvv -k
>>>> >>> * Rebuilt URL to: https://www.google.co.jp/
>>>> >>> * Trying ::1...
>>>> >>> * Connected to localhost (::1) port 3128 (#0)
>>>> >>> * Establish HTTP proxy tunnel to www.google.co.jp:443
>>>> >>>> CONNECT www.google.co.jp:443 HTTP/1.1
>>>> >>>> Host: www.google.co.jp:443
>>>> >>>> User-Agent: curl/7.43.0
>>>> >>>> Proxy-Connection: Keep-Alive
>>>> >>>>
>>>> >>> < HTTP/1.1 200 Connection established
>>>> >>> <
>>>> >>> * Proxy replied OK to CONNECT request
>>>> >>> * ALPN, offering http/1.1
>>>> >>> * Cipher selection:
>>>> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
>>>> >>> * successfully set certificate verify locations:
>>>> >>> * CAfile: /etc/ssl/certs/ca-certificates.crt
>>>> >>> CApath: none
>>>> >>> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
>>>> >>> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
>>>> >>> * Unknown SSL protocol error in connection to www.google.co.jp:443
>>>> >>> * Closing connection 0
>>>> >>> curl: (35) Unknown SSL protocol error in connection to
>>>> www.google.co.jp:443
>>>> >>> ---
>>>> >>>
>>>> >>> My squid.conf:
>>>> >>> ---
>>>> >>> # default acls/configs are ignored
>>>> >>> cache_peer 127.0.0.1 parent 8118 0 default no-digest proxy-only
>>>> >>> never_direct allow all
>>>> >>> ssl_bump peek all
>>>> >>> ssl_bump bump all
>>>> >>> http_port 3128 ssl-bump \
>>>> >>> cert=/etc/squid/ssl/ca.crt \
>>>> >>> key=/etc/squid/ssl/ca.key \
>>>> >>> generate-host-certificates=on \
>>>> >>> dynamic_cert_mem_cache_size=4MB
>>>> >>> ---
>>>> >>>
>>>> >>> From the cache_peer port, someone may notice that I'm using privoxy.
>>>> >>> That's right, as I need to redirect the ssl traffic to SOCKS5 proxy,
>>>> >>> or I can't ever access some sites.
>>>> >>>
>>>> >>> Here is some of my experiments:
>>>> >>> 1) Remove "never_direct"
>>>> >>> Then ssl_bump works as expected, but all traffic doesn't goes through
>>>> >>> the SOCKS5 proxy. So a lot of sites I can't access.
>>>> >>>
>>>> >>> 2) Use local 8118 proxy
>>>> >>> That works fine without any problem, but SSL_dump is needed...
>>>> >>> So just prove privoxy are working.
>>>> >>>
>>>> >>> Any clue?
>>>> >>>
>>>> >>> Thanks
>>>> >>> _______________________________________________
>>>> >>> squid-users mailing list
>>>> >>> squid-users at lists.squid-cache.org
>>>> >>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>>
>>>
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>
>>> iQEcBAEBCAAGBQJVmo9ZAAoJENNXIZxhPexGjzsIALCunLEQOJGKkcm0V0wr3QTQ
>>> xdfkLvJTh9i5sJNaMGbfuE2SCYIERf7HOTu9vNFpFwZBZoQTiMqud1v8KQkzGXTC
>>> xXCjlLAu937DJ+cJoeWNw+wacCB5wBFp4GoonoF3zf2HdIu76u5BQn2WeFZEfnN0
>>> G1WNMh2j7BlCOgRzI7cPnFZPzomcwlCRm7VqfgmadBMU9NpP3w+iVlngGTbt0WOu
>>> Apf6ktZpumfvu68hu0I1Vtn746Dz/U+mmU8Ue+FBga5wyYW6JSMMAQOdsZTeXLnh
>>> Iyu56A47ouNkugcueeuLOXbVlE9N44KpBc96QkXdOvKyx+VemRzaCrMYlvaFO1U=
>>> =Mt1T
>>> -----END PGP SIGNATURE-----
>>>


From tomtux007 at gmail.com  Tue Jul  7 04:27:10 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Tue, 7 Jul 2015 06:27:10 +0200
Subject: [squid-users] Fwd: Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <CACLJR+PDcZgheQKM+Og4gyxo_XWm1hGppHdQJPwkmtq3CMsAjA@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
 <CACLJR+PDcZgheQKM+Og4gyxo_XWm1hGppHdQJPwkmtq3CMsAjA@mail.gmail.com>
Message-ID: <CACLJR+OkFmHn=mtqmt132rxaEdyD4K01=mW237MsLCf5ZE2kyw@mail.gmail.com>

Hi

Opened a while ago, but no answer, if this problem is a (known) bug or
it's already solved with 3.5.6......?

Thanks for a answer.

Kind regards,
Tom


---------- Forwarded message ----------
From: Tom Tom <tomtux007 at gmail.com>
Date: Tue, Jun 30, 2015 at 1:09 PM
Subject: Re: [squid-users] Squid 3.5.5 automatically reload itself in 2h rhythm
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users <squid-users at lists.squid-cache.org>


Hi Amos

On Fri, Jun 19, 2015 at 12:06 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 19/06/2015 5:23 a.m., Tom Tom wrote:
>> Hi
>>
>> gdb shows the following:
>>
>>
>
>> #3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
>> file=0x8114cb "hash.cc", line=240,
>>     function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
>> hash_remove_link(hash_table*, hash_link*)") at assert.c:101
>> #4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
>> hl=0x204e060) at hash.cc:240
>> #5  0x0000000000685296 in Auth::User::cacheCleanup
>> (datanotused=<optimized out>) at User.cc:208
>
> Ah. The auth username cache again.

Seems like a known issue.....? Is there a patch available? Do you need
more information?

Many thanks.
Tom

>
> Amos
>


From rafael.akchurin at diladele.com  Tue Jul  7 08:39:23 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 7 Jul 2015 08:39:23 +0000
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is available
Message-ID: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>


Greetings everyone,


The CygWin based build of Squid proxy for Microsoft Windows version 3.5.6 is now available (amd64 only!).


* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.

* Ready to use MSI package can be downloaded from http://squid.diladele.com.

* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues


Thanks a lot for Squid developers for making this great software!


Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -

https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com.


NB: fixed crash of the tray app on Windows 2012, enabled delay pools in compilation flags.


Best regards,

Rafael Akchurin

Diladele B.V.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/b64096a4/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul  7 08:48:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 07 Jul 2015 20:48:31 +1200
Subject: [squid-users] Fwd: Squid 3.5.5 automatically reload itself in
 2h rhythm
In-Reply-To: <CACLJR+OkFmHn=mtqmt132rxaEdyD4K01=mW237MsLCf5ZE2kyw@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
 <CACLJR+PDcZgheQKM+Og4gyxo_XWm1hGppHdQJPwkmtq3CMsAjA@mail.gmail.com>
 <CACLJR+OkFmHn=mtqmt132rxaEdyD4K01=mW237MsLCf5ZE2kyw@mail.gmail.com>
Message-ID: <559B925F.8070601@treenet.co.nz>

On 7/07/2015 4:27 p.m., Tom Tom wrote:
> Hi
> 
> Opened a while ago, but no answer, if this problem is a (known) bug or
> it's already solved with 3.5.6......?

Its bug 4190. A workaround patch was developed a while ago, but it has
some problems of its own and I forgot to apply it as a temporary
workaround for the 3.5.6 cyle.
see <http://bugs.squid-cache.org/show_bug.cgi?id=4190#c28> for details.

New snapshot containing the workaround is building now and should be
available in a few hours.

Amos



From tarotapprentice at yahoo.com  Tue Jul  7 11:14:39 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 7 Jul 2015 11:14:39 +0000 (UTC)
Subject: [squid-users] Trying the SmpCarpCluster example
Message-ID: <1583805174.279677.1436267679196.JavaMail.yahoo@mail.yahoo.com>

I've setup a test machine with Debian Jessie and got squid from the repo (3.4.8). I'm trying the SmpCarpCluster example given in the wiki. Unfortunately its not working too well.

I've used the 3 conf files shown in the example, adjusted memory sizes and the directories. I had to add "http_access allow localnet" to frontend.conf just after the manager lines. I am getting the following errors. I'm not too sure what to look for now or maybe its a bug in the 3.4.8 version.

It looks like its started up 5 kids, one coordinator, one disker, one frontend  and two workers. I am also wondering why its creating missing swap directories when I did a "squid3 -z" before starting it up.

MarkJ

---------------------------------------------------------

frontend.cache.log
2015/07/07 20:44:12 kid1| TCP connection to localhost/4002 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4002 failed
2015/07/07 20:44:12 kid1| Detected DEAD Parent: backend-kid2
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| Detected DEAD Parent: backend-kid3
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed
2015/07/07 20:44:12 kid1| TCP connection to localhost/4003 failed


backend.cache.log
2015/07/07 20:43:52 kid2| Creating missing swap directories
2015/07/07 20:43:52 kid2| /var/squid3/cache2 exists
2015/07/07 20:43:52 kid2| /var/squid3/cache2/00 exists
...
2015/07/07 20:43:52 kid4| Creating missing swap directories
2015/07/07 20:43:52 kid4| Skipping existing Rock db: /var/squid3/cacheRock/rock
2015/07/07 20:43:52 kid5| Creating missing swap directories
...
2015/07/07 20:43:53 kid4| Starting Squid Cache version 3.4.8 for i586-pc-linux-gnu...
2015/07/07 20:43:53 kid4| Process ID 1063
2015/07/07 20:43:53 kid4| Process Roles: disker
...
2015/07/07 20:43:53 kid3| Starting Squid Cache version 3.4.8 for i586-pc-linux-gnu...
2015/07/07 20:43:53 kid3| Process ID 1064
2015/07/07 20:43:53 kid3| Process Roles: worker
...
2015/07/07 20:43:53 kid5| Starting Squid Cache version 3.4.8 for i586-pc-linux-gnu...
2015/07/07 20:43:53 kid5| Process ID 1062
2015/07/07 20:43:53 kid5| Process Roles: coordinator
...
2015/07/07 20:43:53 kid2| Starting Squid Cache version 3.4.8 for i586-pc-linux-gnu...
2015/07/07 20:43:53 kid2| Process ID 1065
2015/07/07 20:43:53 kid2| Process Roles: worker



Squid3 -v reports
Squid Cache: Version 3.4.8
 linux
configure options:  '--build=i586-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-build-info= linux' '--enable-linux-netfilter' 'build_alias=i586-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'


From squid3 at treenet.co.nz  Tue Jul  7 11:50:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 07 Jul 2015 23:50:06 +1200
Subject: [squid-users] Trying the SmpCarpCluster example
In-Reply-To: <1583805174.279677.1436267679196.JavaMail.yahoo@mail.yahoo.com>
References: <1583805174.279677.1436267679196.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <559BBCEE.9040005@treenet.co.nz>

On 7/07/2015 11:14 p.m., TarotApprentice wrote:
> I've setup a test machine with Debian Jessie and got squid from the repo (3.4.8). I'm trying the SmpCarpCluster example given in the wiki. Unfortunately its not working too well.
> 
> I've used the 3 conf files shown in the example, adjusted memory sizes and the directories. I had to add "http_access allow localnet" to frontend.conf just after the manager lines. I am getting the following errors. I'm not too sure what to look for now or maybe its a bug in the 3.4.8 version.
> 
> It looks like its started up 5 kids, one coordinator, one disker, one frontend  and two workers. I am also wondering why its creating missing swap directories when I did a "squid3 -z" before starting it up.
> 

Please try replacing backend.conf http_port line with:
  http_port localhost:400${process_number}

The latest Debian use IPv6 by default, so I think the frontend is trying
to contact [::1] instead of 127.0.0.1.

Amos



From s.kirschner at afa-finanz.de  Tue Jul  7 11:45:35 2015
From: s.kirschner at afa-finanz.de (S.Kirschner)
Date: Tue, 7 Jul 2015 04:45:35 -0700 (PDT)
Subject: [squid-users] transparent proxy splice using dstdomain issue
Message-ID: <1436269535080-4672088.post@n4.nabble.com>

Hi I?m using squid version 3.5.3 as transparent proxy in pfsense and got an
issue with my configuration.

I would like to bump ssl connections and some should be spliced(for the
example I used "sparkasse.de"), in my case banking sites should be spliced.

Its working fine when i use IP?s for the acl?s or insert the hostname in the
/etc/hosts,
but I think both cant be the solution.

I think the issues exist because the reverse lookup dont got the anwser
"sparkasse.de", but why it does not use the hostname from the dns request to
the dns-server ?

Also got errors that the ssl accept failed.

Below you could see my squid.conf and the entries from the cache.log for
both cases.

*Without hostname in etc/hosts*


*With hostname in etc/hosts*


*SSL accept log entries*


*Squid.conf*




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-splice-using-dstdomain-issue-tp4672088.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jul  7 12:55:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Jul 2015 00:55:53 +1200
Subject: [squid-users] transparent proxy splice using dstdomain issue
In-Reply-To: <1436269535080-4672088.post@n4.nabble.com>
References: <1436269535080-4672088.post@n4.nabble.com>
Message-ID: <559BCC59.1090403@treenet.co.nz>

On 7/07/2015 11:45 p.m., S.Kirschner wrote:
> Hi I?m using squid version 3.5.3 as transparent proxy in pfsense and got an
> issue with my configuration.
> 
> I would like to bump ssl connections and some should be spliced(for the
> example I used "sparkasse.de"), in my case banking sites should be spliced.
> 
> Its working fine when i use IP?s for the acl?s or insert the hostname in the
> /etc/hosts,
> but I think both cant be the solution.
> 
> I think the issues exist because the reverse lookup dont got the anwser
> "sparkasse.de", but why it does not use the hostname from the dns request to
> the dns-server ?

Because Squid is not a DNS server.

The HTTP message details including URL where dstdomain comes from are
encrypted at the time you are trying to use the dstdomain ACL.

Please upgrade to the latest 3.5 release (today that is 3.5.6) and use
the "ssl::server_name" ACL instead of dstdomain for ssl_bump access
controls. It grabs the domain name (if any) from TLS directly without
needing decryption first.

> 
> Also got errors that the ssl accept failed.
> 
> Below you could see my squid.conf and the entries from the cache.log for
> both cases.
> 
> *Without hostname in etc/hosts*
> 
> 
> *With hostname in etc/hosts*
> 
> 
> *SSL accept log entries*
> 
> 
> *Squid.conf*
> 

Please be aware when using Nabble interface that fancy embeded
quotations are not sent to the mailing list. Only the plain message text.

Amos



From Bodo.Teichmann at brandad-systems.de  Tue Jul  7 13:26:29 2015
From: Bodo.Teichmann at brandad-systems.de (Bodo Teichmann)
Date: Tue, 7 Jul 2015 13:26:29 +0000
Subject: [squid-users] how to use client_delay_access without a named ACL ?
Message-ID: <DCCF608DB376BD4FAF176332BC0F089F065AFCF6@ba-server-004>

Hi,
Since using "client_delay_parameters" in the "normal" way, using 
client_delay_access 1 allow <acl> 
is prevented by 
http://bugs.squid-cache.org/show_bug.cgi?id=3696

therefore  Amos Jeffries wrote on Apr 02, 2013

>client_delay_access is tested as soon as the TCP SYN packet has been 
>accepted. All Squid has for ACLs to work with at that point is the 
>IP:port of each end of the client TCP connection. 
Which I don't understand 
And he wrote further: 
>client_delay_access can be used with: src, arp, localip / myip, 
>localport / myport. 
>???"myportname" ACL should in theory work as well, but looking at the 
>code I see the required details are not yet passed to the ACL code 
>properly so that is broken. 

Therefore I tried to use client_delay_access without a named ACL, an using "src" directly,  but was not able to find a valid syntax for it.
e.g. I tried in /etc/squid3/squid.conf (using squid 3.4.8) : 

     client_delay_access 1 allow src 10.41.1.205/32

but just get an syntax error : 

      ACL not found: src 10.41.1.205/32

Any idea/example on how to actually use client_delay_access 1 allow ..... ?

Bodo 



From dweimer at dweimer.net  Tue Jul  7 13:37:36 2015
From: dweimer at dweimer.net (dweimer)
Date: Tue, 07 Jul 2015 08:37:36 -0500
Subject: [squid-users] Question about squid-3.5-13849.patch
Message-ID: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>

I just updated to Squid 3.5.6 and after running QualSYS SSL Labs test it 
still lists my server as supporting Secure Client-Initiated 
Renegotiation and potentially being vulnerable to CVE-2009-3555 which 
the patch 
<http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13849.patch> 
included in the 3.5.6 change list, is described as hardening against. Is 
there an option I need to add to the https_port setting in my squid.conf 
file to correctly make use of this?

Currently running with the following options specified.

   options=NO_SSLv2:NO_SSLv3:CIPHER_SERVER_PREFERENCE \
   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!SSLv2:!RC4 \

System is Running on FreeBSD 10.1-RELEASE-p14, using OpenSSL included in 
base FreeBSD.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From jvdwesthuiz at shoprite.co.za  Tue Jul  7 13:57:21 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Tue, 7 Jul 2015 13:57:21 +0000
Subject: [squid-users] Windows 10 Updates
Message-ID: <1436277443.2605.137.camel@shoprite.co.za>

Hi list

I have a problem with Windows 10 updates. It seems that Microsoft will do updates via https now.

--cut--
1436268325.765   5294 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 9899569 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5? - HIER_DIRECT/165.165.47.19 application/octet-stream
1436268333.267   7484 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 21564261 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5? - HIER_DIRECT/165.165.47.19 application/octet-stream
1436268430.871 147280 xxx.xxx.xxx.xxx TCP_TUNNEL/200 4267 CONNECT cp201-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/23.214.151.174 -
1436268478.259  96621 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
1436268786.878  78517 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
--cut--

To my knowledge there is no way to cache this. How would one handle this? Is it even possible to cache the updates?


--
Kind Regards
Jasper






Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/3c6135f9/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul  7 14:05:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Jul 2015 02:05:45 +1200
Subject: [squid-users] Question about squid-3.5-13849.patch
In-Reply-To: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>
References: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>
Message-ID: <559BDCB9.2070005@treenet.co.nz>

On 8/07/2015 1:37 a.m., dweimer wrote:
> I just updated to Squid 3.5.6 and after running QualSYS SSL Labs test it
> still lists my server as supporting Secure Client-Initiated
> Renegotiation and potentially being vulnerable to CVE-2009-3555 which
> the patch
> <http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13849.patch>
> included in the 3.5.6 change list, is described as hardening against. Is
> there an option I need to add to the https_port setting in my squid.conf
> file to correctly make use of this?
> 
> Currently running with the following options specified.
> 
>   options=NO_SSLv2:NO_SSLv3:CIPHER_SERVER_PREFERENCE \
>   cipher=ALL:!aNULL:!eNULL:!LOW:!EXP:!ADH:+HIGH:+MEDIUM:!SSLv2:!RC4 \
> 
> System is Running on FreeBSD 10.1-RELEASE-p14, using OpenSSL included in
> base FreeBSD.
> 

No, the change is automatic for all Squid built against an OpenSSL
library that supports the library API option. If it is not working, then
the library you are using probably does not support that option.

AFAIK you need at least OpenSSL 0.9.8m for anything related to that
vulnerability to be fixable. The latest 1.x libraries do not support the
flag we use because they do the rejection internally without needing any
help from Squid.

Amos



From squid3 at treenet.co.nz  Tue Jul  7 14:10:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Jul 2015 02:10:27 +1200
Subject: [squid-users] how to use client_delay_access without a named
 ACL ?
In-Reply-To: <DCCF608DB376BD4FAF176332BC0F089F065AFCF6@ba-server-004>
References: <DCCF608DB376BD4FAF176332BC0F089F065AFCF6@ba-server-004>
Message-ID: <559BDDD3.9050106@treenet.co.nz>

On 8/07/2015 1:26 a.m., Bodo Teichmann wrote:
> Hi,
> Since using "client_delay_parameters" in the "normal" way, using 
> client_delay_access 1 allow <acl> 
> is prevented by 
> http://bugs.squid-cache.org/show_bug.cgi?id=3696
> 
> therefore  Amos Jeffries wrote on Apr 02, 2013
> 
>> client_delay_access is tested as soon as the TCP SYN packet has been 
>> accepted. All Squid has for ACLs to work with at that point is the 
>> IP:port of each end of the client TCP connection. 
> Which I don't understand 
> And he wrote further: 
>> client_delay_access can be used with: src, arp, localip / myip, 
>> localport / myport. 
>>    "myportname" ACL should in theory work as well, but looking at the 
>> code I see the required details are not yet passed to the ACL code 
>> properly so that is broken. 
> 
> Therefore I tried to use client_delay_access without a named ACL, an using "src" directly,  but was not able to find a valid syntax for it.
> e.g. I tried in /etc/squid3/squid.conf (using squid 3.4.8) : 
> 
>      client_delay_access 1 allow src 10.41.1.205/32
> 
> but just get an syntax error : 
> 
>       ACL not found: src 10.41.1.205/32
> 
> Any idea/example on how to actually use client_delay_access 1 allow ..... ?

Squid always requires ACls to be named.

"myportname" is the *type* of a certain ACL, which is not working.


PS. AFAIK the bug is unrelated to the ACL naming business. It happens
with a plain src type ACL as well. So if you are hitting it at all, you
wont be able to use the feature until its fixed by someone.

Amos


From squid3 at treenet.co.nz  Tue Jul  7 14:16:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Jul 2015 02:16:27 +1200
Subject: [squid-users] Windows 10 Updates
In-Reply-To: <1436277443.2605.137.camel@shoprite.co.za>
References: <1436277443.2605.137.camel@shoprite.co.za>
Message-ID: <559BDF3B.3090208@treenet.co.nz>

On 8/07/2015 1:57 a.m., Jasper Van Der Westhuizen wrote:
> Hi list
> 
> I have a problem with Windows 10 updates. It seems that Microsoft will do updates via https now.
> 
> --cut--
> 1436268325.765   5294 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 9899569 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5? - HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268333.267   7484 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 21564261 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5? - HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268430.871 147280 xxx.xxx.xxx.xxx TCP_TUNNEL/200 4267 CONNECT cp201-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/23.214.151.174 -
> 1436268478.259  96621 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> 1436268786.878  78517 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> --cut--
> 
> To my knowledge there is no way to cache this.

Technically yes, there is no way to cache it without breaking into the
HTTPS.

> How would one handle this? Is it even possible to cache the updates?
> 

SSL-Bump is the Squid feature for accessing HTTPS data in decrypted form
for filtering and/or caching.

However, that will depend on;
a) being able to "bump" the crypto (if the WU app is validating server
cert against a known signature its not),
b) the content inside actually being HTTPS (they do updates via P2P now
too), and
c) the HTTP content inside being cacheable (no guarantees, but a good
chance its about as cacheable as non-encrypted updates).

You are the first to mention it, so there is no existing info on those
requirements.

Amos



From s.kirschner at afa-finanz.de  Tue Jul  7 13:54:16 2015
From: s.kirschner at afa-finanz.de (S.Kirschner)
Date: Tue, 7 Jul 2015 06:54:16 -0700 (PDT)
Subject: [squid-users] transparent proxy splice using dstdomain issue
In-Reply-To: <559BCC59.1090403@treenet.co.nz>
References: <1436269535080-4672088.post@n4.nabble.com>
 <559BCC59.1090403@treenet.co.nz>
Message-ID: <1436277256782-4672095.post@n4.nabble.com>

Amos Jeffries wrote
> On 7/07/2015 11:45 p.m., S.Kirschner wrote:
>> I think the issues exist because the reverse lookup dont got the anwser
>> "sparkasse.de", but why it does not use the hostname from the dns request
>> to
>> the dns-server ?
> 
> Because Squid is not a DNS server.
> 
> The HTTP message details including URL where dstdomain comes from are
> encrypted at the time you are trying to use the dstdomain ACL.

Yes but, in pfsense a dns server is installed, so on these host a dns server
is running. Also i tried to use the google DNS 

Here now the entries from the cache.log

With sparkasse.de in /etc/hosts
#2015/06/19 14:03:03.907 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking '212.34.69.3'
#2015/06/19 14:03:03.907 kid1| DomainData.cc(113) match: aclMatchDomainList:
'212.34.69.3' NOT found
#2015/06/19 14:03:03.908 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking 'sparkasse.de'
#2015/06/19 14:03:03.908 kid1| DomainData.cc(113) match: aclMatchDomainList:
'sparkasse.de' found
#2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: bypass = 1
#2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: (ssl_bump rule)
= 1
#2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: (ssl_bump
rules) = 1

Without sparkasse.de in /etc/hosts
#2015/06/19 14:05:19.842 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking '212.34.69.3'
#2015/06/19 14:05:19.842 kid1| DomainData.cc(113) match: aclMatchDomainList:
'212.34.69.3' NOT found
#2015/06/19 14:05:19.842 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking 'rev-212.34.69.3.rev.izb.net'
#2015/06/19 14:05:19.842 kid1| DomainData.cc(113) match: aclMatchDomainList:
'rev-212.34.69.3.rev.izb.net' NOT found
#2015/06/19 14:05:19.842 kid1| Acl.cc(158) matches: checked: bypass = 0
#2015/06/19 14:05:19.842 kid1| Acl.cc(158) matches: checked: (ssl_bump rule)
= 0

The ssl accept error in cache.log
#2015/06/19 14:05:19.825 kid1| Checklist.cc(61) markFinished: 0x8041b7798
answer ALLOWED for match
#2015/06/19 14:05:19.825 kid1| Checklist.cc(161) checkCallback:
ACLChecklist::checkCallback: 0x8041b7798 answer=ALLOWED
#2015/06/19 14:05:19.825 kid1| client_side_request.cc(1527) sslBumpNeed:
sslBump required: peek
#2015/06/19 14:05:19.825 kid1| client_side_request.cc(115)
~ClientRequestContext: 0x807468098 ClientRequestContext destructed
#2015/06/19 14:05:19.825 kid1| client_side_request.cc(1829) doCallouts:
calling processRequest()
#2015/06/19 14:05:19.825 kid1| store.cc(780) storeCreatePureEntry:
storeCreateEntry: '212.34.69.3:443'
#2015/06/19 14:05:19.825 kid1| MemObject.cc(97) MemObject: new MemObject
0x807567f40
#2015/06/19 14:05:19.825 kid1| store.cc(485) lock: storeCreateEntry locked
key [null_store_key] e:=V/0x80755ada0*1
#2015/06/19 14:05:19.825 kid1| store_key_md5.cc(89) storeKeyPrivate:
storeKeyPrivate: CONNECT 212.34.69.3:443
#2015/06/19 14:05:19.825 kid1| store.cc(449) hashInsert:
StoreEntry::hashInsert: Inserting Entry e:=IV/0x80755ada0*1 key
'04808DEC55BF24579C431922F1A83DE0'
#2015/06/19 14:05:19.840 kid1| client_side.cc(4245) clientPeekAndSpliceSSL:
SSL_accept failed.
#2015/06/19 14:05:19.840 kid1| client_side.cc(4245) clientPeekAndSpliceSSL:
SSL_accept failed.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/transparent-proxy-splice-using-dstdomain-issue-tp4672088p4672095.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ulises at vianetcon.com.ar  Tue Jul  7 15:03:21 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Tue, 07 Jul 2015 12:03:21 -0300
Subject: [squid-users] Use of conditionals in config file
Message-ID: <559BEA39.50401@vianetcon.com.ar>

Hello,

Is there any way to do something like this?

if ${process_number} = 2 && url_regex facebook.com.*

	cache_dir aufs /cache_face   590000  32 256 min-size=500 max-size=576000
endif


Of course this is not valid syntax config , but the idea is to store in a certain cache_dir only the content from a given domain (i.e facebook.com.*)

Thanks


Ulises

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/ca882522/attachment.htm>

From yvoinov at gmail.com  Tue Jul  7 15:04:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 7 Jul 2015 21:04:15 +0600
Subject: [squid-users] Windows 10 Updates
In-Reply-To: <1436277443.2605.137.camel@shoprite.co.za>
References: <1436277443.2605.137.camel@shoprite.co.za>
Message-ID: <559BEA6F.7050700@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I'm afraid, you can't. WU uses TLS pinning, so, as Amos said, "Correct
TLS cannot be bumped".

That's all.

07.07.15 19:57, Jasper Van Der Westhuizen ?????:
> Hi list
>
> I have a problem with Windows 10 updates. It seems that Microsoft will
do updates via https now.
>
> --cut--
> 1436268325.765   5294 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206
9899569 GET
http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?
- HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268333.267   7484 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206
21564261 GET
http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?
- HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268430.871 147280 xxx.xxx.xxx.xxx TCP_TUNNEL/200 4267 CONNECT
cp201-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/23.214.151.174 -
> 1436268478.259  96621 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT
array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> 1436268786.878  78517 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT
array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> --cut--
>
> To my knowledge there is no way to cache this. How would one handle
this? Is it even possible to cache the updates?
>
>
> --
> Kind Regards
> Jasper
>
>
>
>
>
>
> Disclaimer:
> http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVm+puAAoJENNXIZxhPexGxkwIALR757Ao4BN2dqYU3yxcSboy
cLIKsZeldYu2X8eaziVWqvCVWtm14h9B5vpfkZA52yXhy6A+OavQj5ah9rUAx0Pv
z6x/W0G071+CfKM7BdHJwsFjt5sw4MJyHbnbkdDeJEdEPlKfMXKTmrtWW3GlRZAb
fzi1jXhE1hXhzq7QlEpunIOm4VVkKomksxDT/dXWK6WcsD4yzh2HSUkzs+qbSeEW
QZ9Z7IZR18rOSAhaPpFwJxoOhfIxJs409ibKe14aAuVwyAqX50Je6OHtEoVkF1qm
5/kjjs2gOzmZ9SmsuXamQQEg9K2ZaGQ9bRqawKov9BqDddnE4VhxqJzVmg8mD4A=
=uhUo
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/39b86cbf/attachment.htm>

From yvoinov at gmail.com  Tue Jul  7 15:05:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 7 Jul 2015 21:05:55 +0600
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
 available
In-Reply-To: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
References: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
Message-ID: <559BEAD3.6080400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hi Raf,

does you team close issue with cache_dir already?

07.07.15 14:39, Rafael Akchurin ?????:
>
> Greetings everyone,
>
>
> The CygWin based build of Squid proxy for Microsoft Windows version
3.5.6 is now available (amd64 only!).
>
>
> * Original release notes are at
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.
>
> * Ready to use MSI package can be downloaded from
http://squid.diladele.com.
>
> * List of open issues for the installer -
https://github.com/diladele/squid3-windows/issues
>
>
> Thanks a lot for Squid developers for making this great software!
>
>
> Please join our humble efforts to provide ready to run MSI installer
for Squid on Microsoft Windows with all required dependencies at GitHub -
>
> https://github.com/diladele/squid3-windows. Please report all
issues/bugs/feature requests at GitHub project. Issues about the *MSI
installer only* can also be reported to support at diladele.com.
>
>
> NB: fixed crash of the tray app on Windows 2012, enabled delay pools
in compilation flags.
>
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVm+rSAAoJENNXIZxhPexGWgEIAL8lI7fWdL4T38XxUCvFakkf
ScifD/9yg5Y9RqtEZ4/SHP5P5grPdgqG7raFN1bM2cpWS3g4lVm7iYeahCKvkJn6
EZGktHf2JCCD48/LGBorCnhz6+pA1Y44SSs2H9y4IcMtPuJA9A0v0rX5OMVFAKl4
t5VRc8OJYVnvTVE1BFJr15kIy68WMYiFsPP+QlWYNf80ZXGy7/AJxCxP6+HqEO8v
prXajkPp79kXLm9lzvNUPlNmsOKCMfMyCl/qVVsoGpZOFHvj8QD71ed1wrjjAOkH
RzYpp3ox11HH56Hp0IV+BfioFbp4GtRads/MT6CnGCnjHc8yjSZhBO6qF/VYd3c=
=k+Ci
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/4ca5eb7c/attachment.htm>

From yvoinov at gmail.com  Tue Jul  7 15:09:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 7 Jul 2015 21:09:52 +0600
Subject: [squid-users] Windows 10 Updates
In-Reply-To: <1436277443.2605.137.camel@shoprite.co.za>
References: <1436277443.2605.137.camel@shoprite.co.za>
Message-ID: <559BEBC0.8000507@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think,

we must forgot about SSL Bump as a feature and caching HTTPS. Due to all
world;s developer position.

Sad, but true.

This feature dead now.

WBR, Yuri

07.07.15 19:57, Jasper Van Der Westhuizen ?????:
> Hi list
>
> I have a problem with Windows 10 updates. It seems that Microsoft will
do updates via https now.
>
> --cut--
> 1436268325.765   5294 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206
9899569 GET
http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?
- HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268333.267   7484 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206
21564261 GET
http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?
- HIER_DIRECT/165.165.47.19 application/octet-stream
> 1436268430.871 147280 xxx.xxx.xxx.xxx TCP_TUNNEL/200 4267 CONNECT
cp201-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/23.214.151.174 -
> 1436268478.259  96621 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT
array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> 1436268786.878  78517 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT
array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117 -
> --cut--
>
> To my knowledge there is no way to cache this. How would one handle
this? Is it even possible to cache the updates?
>
>
> --
> Kind Regards
> Jasper
>
>
>
>
>
>
> Disclaimer:
> http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVm+vAAAoJENNXIZxhPexG7fMIAM3rsXl+x5jM8abxsUADnKlx
1y8WmVwh3k3hxe/Mds8Mp4GEkEee+ulCGmobw7u+88hQGt/WzViZBc/PC10q/wm2
sIHZbHfMe4DPP8WKGUyfYvwQx92O9V1y5a61Ukxw5PSrMDHqcviD5wxxPDz0pK1f
cp18v9SB9uZAMK/T1fevU3diJ7OAZC5ghW2nuhwHCxQXys0uvwd9Ac3YJSTEKegf
sc7reQl7SkatzWBKpjl3AqYzSy1udkEJ4VVLp9DcjdjOyYTBgRpiC9cxf/oCqhkf
EjV/Wp7WLMItuE+BUwnEhjmZMEgnEKUrJKgNedaEkapYLQ3QAhIg0Us9nkgVPE8=
=H1fp
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/c5dd0073/attachment.htm>

From rafael.akchurin at diladele.com  Tue Jul  7 15:23:30 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 7 Jul 2015 15:23:30 +0000
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
 available
In-Reply-To: <559BEAD3.6080400@gmail.com>
References: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <559BEAD3.6080400@gmail.com>
Message-ID: <HE1PR04MB1353B0EC9E246C1F972DCD058F920@HE1PR04MB1353.eurprd04.prod.outlook.com>

Hello Yuri,

Is it - https://github.com/diladele/squid3-windows/issues/40?

Raf

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Tuesday, July 7, 2015 5:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is available


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Hi Raf,

does you team close issue with cache_dir already?

07.07.15 14:39, Rafael Akchurin ?????:
>

      > Greetings everyone,

      >

      >

      > The CygWin based build of Squid proxy for Microsoft Windows
      version 3.5.6 is now available (amd64 only!).

      >

      >

      > * Original release notes are at
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.

      >

      > * Ready to use MSI package can be downloaded from
      http://squid.diladele.com.

      >

      > * List of open issues for the installer -
      https://github.com/diladele/squid3-windows/issues

      >

      >

      > Thanks a lot for Squid developers for making this great
      software!

      >

      >

      > Please join our humble efforts to provide ready to run MSI
      installer for Squid on Microsoft Windows with all required
      dependencies at GitHub -

      >

      > https://github.com/diladele/squid3-windows. Please report all
      issues/bugs/feature requests at GitHub project. Issues about the
      *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com>.

      >

      >

      > NB: fixed crash of the tray app on Windows 2012, enabled
      delay pools in compilation flags.

      >

      >

      > Best regards,

      >

      > Rafael Akchurin

      >

      > Diladele B.V.

      >

      >

      >

      >

     >

      > _______________________________________________

      > squid-users mailing list

      > squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

      > http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVm+rSAAoJENNXIZxhPexGWgEIAL8lI7fWdL4T38XxUCvFakkf
ScifD/9yg5Y9RqtEZ4/SHP5P5grPdgqG7raFN1bM2cpWS3g4lVm7iYeahCKvkJn6
EZGktHf2JCCD48/LGBorCnhz6+pA1Y44SSs2H9y4IcMtPuJA9A0v0rX5OMVFAKl4
t5VRc8OJYVnvTVE1BFJr15kIy68WMYiFsPP+QlWYNf80ZXGy7/AJxCxP6+HqEO8v
prXajkPp79kXLm9lzvNUPlNmsOKCMfMyCl/qVVsoGpZOFHvj8QD71ed1wrjjAOkH
RzYpp3ox11HH56Hp0IV+BfioFbp4GtRads/MT6CnGCnjHc8yjSZhBO6qF/VYd3c=
=k+Ci
-----END PGP SIGNATURE-----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/c277c28c/attachment.htm>

From yvoinov at gmail.com  Tue Jul  7 15:27:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 7 Jul 2015 21:27:58 +0600
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
 available
In-Reply-To: <HE1PR04MB1353B0EC9E246C1F972DCD058F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
References: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <559BEAD3.6080400@gmail.com>
 <HE1PR04MB1353B0EC9E246C1F972DCD058F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
Message-ID: <559BEFFE.8080902@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think so.

07.07.15 21:23, Rafael Akchurin ?????:
> Hello Yuri,
>
> Is it - https://github.com/diladele/squid3-windows/issues/40?
>
> Raf
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
On Behalf Of Yuri Voinov
> Sent: Tuesday, July 7, 2015 5:06 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
available
>
>
> Hi Raf,
>
> does you team close issue with cache_dir already?
>
> 07.07.15 14:39, Rafael Akchurin ?????:
>
>
>       > Greetings everyone,
>
>
>
>
>
>       > The CygWin based build of Squid proxy for Microsoft Windows
>       version 3.5.6 is now available (amd64 only!).
>
>
>
>
>
>       > * Original release notes are at
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.
>
>
>
>       > * Ready to use MSI package can be downloaded from
>       http://squid.diladele.com.
>
>
>
>       > * List of open issues for the installer -
>       https://github.com/diladele/squid3-windows/issues
>
>
>
>
>
>       > Thanks a lot for Squid developers for making this great
>       software!
>
>
>
>
>
>       > Please join our humble efforts to provide ready to run MSI
>       installer for Squid on Microsoft Windows with all required
>       dependencies at GitHub -
>
>
>
>       > https://github.com/diladele/squid3-windows. Please report all
>       issues/bugs/feature requests at GitHub project. Issues about the
>       *MSI installer only* can also be reported to
support at diladele.com<mailto:support at diladele.com>.
>
>
>
>
>
>       > NB: fixed crash of the tray app on Windows 2012, enabled
>       delay pools in compilation flags.
>
>
>
>
>
>       > Best regards,
>
>
>
>       > Rafael Akchurin
>
>
>
>       > Diladele B.V.
>
>
>
>
>
>
>
>
>
>
>
>       > _______________________________________________
>
>       > squid-users mailing list
>
>       >
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>
>       > http://lists.squid-cache.org/listinfo/squid-users
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVm+/+AAoJENNXIZxhPexGQWUH/A8ou3kQdn5w6CORadXlWRAT
34evpPLNy0UBuABBNntkGBp4hXIcGcBuoMOsmetsTR8QSDQhmgbEZ9ASieMQDF7Y
NiEjOFlBlEjwEJaI7pxiM12T/V50w5J4DYpweIaQ1waMD1pen5MEZjhrieL6dNfL
jd+ieUfMHab8zyycte0jrAi+d5zCTkmuKdWtyWbPJ+txmKPU24FXLJjA/omASpTP
TeLXpvQKANylVucbg8W5+zEZ7lf+RnEks/XDYQaGSw/pqq5gJf2A+M6HznVVppTB
RtvMraHJrbBS2rtJFbidqE+WDJAZVE+AMuAH4LAxKtCSQ2Soy7LUVIdFBSyIuvA=
=0DGh
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/b2386105/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul  7 15:40:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 08 Jul 2015 03:40:07 +1200
Subject: [squid-users] Use of conditionals in config file
In-Reply-To: <559BEA39.50401@vianetcon.com.ar>
References: <559BEA39.50401@vianetcon.com.ar>
Message-ID: <559BF2D7.9050109@treenet.co.nz>

On 8/07/2015 3:03 a.m., Ulises Nicolini wrote:
> Hello,
> 
> Is there any way to do something like this?
> 
> if ${process_number} = 2 && url_regex facebook.com.*
> 
>     cache_dir aufs /cache_face   590000  32 256 min-size=500
> max-size=576000
> endif
> 
> 
> Of course this is not valid syntax config , but the idea is to store in
> a certain cache_dir only the content from a given domain (i.e
> facebook.com.*)

What you are thinking is this:

 if ${process_number} = 2
  acl FB dstdomain facebook.com
  cache deny !FB
  cache_dir ...
 endif


However it makes no sense to do.

Firstly, an HTTP proxy cache is just a temporary buffer in the network
path between client and origin server. Segregating temporary storage
location based on arbitrary origins gains no benefit over non-segregated
data, and costs extra processing time to figure out the special vs
non-special locations for every request.

Secondly, caches do not store content by domain name. Content is stored
by an efficient hash of the URL plus various other details specific to
the object type. One of which is how closely related in time any two
given requests occured, such that sites built as a mashup of references
to other domains content will render as quickly. Segregation by domain
defeats that algorithm. NP: Facebook is one of those types of site.

Thirdly, disk is only one layer of a multi-layered caching system.
Objects have been through both the in-transit storage and memory cache
and remained cacheable before they get considered for saving to disk
locations.

Amos



From rafael.akchurin at diladele.com  Tue Jul  7 15:47:59 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 7 Jul 2015 15:47:59 +0000
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
 available
In-Reply-To: <559BEFFE.8080902@gmail.com>
References: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <559BEAD3.6080400@gmail.com>
 <HE1PR04MB1353B0EC9E246C1F972DCD058F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
 <559BEFFE.8080902@gmail.com>
Message-ID: <HE1PR04MB1353945A8287F8BF722EC24E8F920@HE1PR04MB1353.eurprd04.prod.outlook.com>

Then it is still open ?

From: Yuri Voinov [mailto:yvoinov at gmail.com]
Sent: Tuesday, July 7, 2015 5:28 PM
To: Rafael Akchurin
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is available


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

I think so.

07.07.15 21:23, Rafael Akchurin ?????:
> Hello Yuri,

      >

      > Is it - https://github.com/diladele/squid3-windows/issues/40?

      >

      > Raf

      >

      > From: squid-users
      [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
      Yuri Voinov

      > Sent: Tuesday, July 7, 2015 5:06 PM

      > To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

      > Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows
      64-bit is available

      >

      >

     > Hi Raf,

      >

      > does you team close issue with cache_dir already?

      >

      > 07.07.15 14:39, Rafael Akchurin ?????:

      >

      >

      >       > Greetings everyone,

      >

      >

      >

      >

      >

      >       > The CygWin based build of Squid proxy for
      Microsoft Windows

      >       version 3.5.6 is now available (amd64 only!).

      >

      >

      >

      >

      >

      >       > * Original release notes are at

      >
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.

      >

      >

      >

      >       > * Ready to use MSI package can be downloaded from

      >       http://squid.diladele.com.

      >

      >

      >

      >       > * List of open issues for the installer -

      >       https://github.com/diladele/squid3-windows/issues

      >

      >

      >

      >

      >

      >       > Thanks a lot for Squid developers for making this
      great

      >       software!

      >

      >

      >

      >

      >

      >       > Please join our humble efforts to provide ready to
      run MSI

      >       installer for Squid on Microsoft Windows with all
      required

      >       dependencies at GitHub -

      >

      >

      >

      >       > https://github.com/diladele/squid3-windows. Please
      report all

      >       issues/bugs/feature requests at GitHub project. Issues
      about the

      >       *MSI installer only* can also be reported to
      support at diladele.com<mailto:support at diladele.com><mailto:support at diladele.com><mailto:support at diladele.com>.

      >

      >

      >

      >

      >

      >       > NB: fixed crash of the tray app on Windows 2012,
      enabled

      >       delay pools in compilation flags.

      >

      >

      >

      >

      >

      >       > Best regards,

      >

      >

      >

      >       > Rafael Akchurin

      >

      >

      >

      >       > Diladele B.V.

      >

      >

      >

      >

      >

      >

      >

      >

      >

      >

      >

      >       > _______________________________________________

      >

      >       > squid-users mailing list

      >

      >       >
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org><mailto:squid-users at lists.squid-cache.org><mailto:squid-users at lists.squid-cache.org>

      >

      >       > http://lists.squid-cache.org/listinfo/squid-users

      >
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVm+/+AAoJENNXIZxhPexGQWUH/A8ou3kQdn5w6CORadXlWRAT
34evpPLNy0UBuABBNntkGBp4hXIcGcBuoMOsmetsTR8QSDQhmgbEZ9ASieMQDF7Y
NiEjOFlBlEjwEJaI7pxiM12T/V50w5J4DYpweIaQ1waMD1pen5MEZjhrieL6dNfL
jd+ieUfMHab8zyycte0jrAi+d5zCTkmuKdWtyWbPJ+txmKPU24FXLJjA/omASpTP
TeLXpvQKANylVucbg8W5+zEZ7lf+RnEks/XDYQaGSw/pqq5gJf2A+M6HznVVppTB
RtvMraHJrbBS2rtJFbidqE+WDJAZVE+AMuAH4LAxKtCSQ2Soy7LUVIdFBSyIuvA=
=0DGh
-----END PGP SIGNATURE-----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/3249db89/attachment.htm>

From yvoinov at gmail.com  Tue Jul  7 15:52:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 7 Jul 2015 21:52:31 +0600
Subject: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
 available
In-Reply-To: <HE1PR04MB1353945A8287F8BF722EC24E8F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
References: <VI1PR04MB135953A166B54D245F6640F98F920@VI1PR04MB1359.eurprd04.prod.outlook.com>
 <559BEAD3.6080400@gmail.com>
 <HE1PR04MB1353B0EC9E246C1F972DCD058F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
 <559BEFFE.8080902@gmail.com>
 <HE1PR04MB1353945A8287F8BF722EC24E8F920@HE1PR04MB1353.eurprd04.prod.outlook.com>
Message-ID: <559BF5BF.5010803@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Zzzzzzzzzzz....... Still using 3.5.1 on my Win.......

07.07.15 21:47, Rafael Akchurin ?????:
> Then it is still open ?
>
> From: Yuri Voinov [mailto:yvoinov at gmail.com]
> Sent: Tuesday, July 7, 2015 5:28 PM
> To: Rafael Akchurin
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows 64-bit is
available
>
>
> I think so.
>
> 07.07.15 21:23, Rafael Akchurin ?????:
> > Hello Yuri,
>
>
>
>       > Is it - https://github.com/diladele/squid3-windows/issues/40?
>
>
>
>       > Raf
>
>
>
>       > From: squid-users
>       [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
>       Yuri Voinov
>
>       > Sent: Tuesday, July 7, 2015 5:06 PM
>
>       > To:
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
>
>       > Subject: Re: [squid-users] Squid 3.5.6 for Microsoft Windows
>       64-bit is available
>
>
>
>
>
>      > Hi Raf,
>
>
>
>       > does you team close issue with cache_dir already?
>
>
>
>       > 07.07.15 14:39, Rafael Akchurin ?????:
>
>
>
>
>
>       >       > Greetings everyone,
>
>
>
>
>
>
>
>
>
>
>
>       >       > The CygWin based build of Squid proxy for
>       Microsoft Windows
>
>       >       version 3.5.6 is now available (amd64 only!).
>
>
>
>
>
>
>
>
>
>
>
>       >       > * Original release notes are at
>
>
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.6-RELEASENOTES.html.
>
>
>
>
>
>
>
>       >       > * Ready to use MSI package can be downloaded from
>
>       >       http://squid.diladele.com.
>
>
>
>
>
>
>
>       >       > * List of open issues for the installer -
>
>       >       https://github.com/diladele/squid3-windows/issues
>
>
>
>
>
>
>
>
>
>
>
>       >       > Thanks a lot for Squid developers for making this
>       great
>
>       >       software!
>
>
>
>
>
>
>
>
>
>
>
>       >       > Please join our humble efforts to provide ready to
>       run MSI
>
>       >       installer for Squid on Microsoft Windows with all
>       required
>
>       >       dependencies at GitHub -
>
>
>
>
>
>
>
>       >       > https://github.com/diladele/squid3-windows. Please
>       report all
>
>       >       issues/bugs/feature requests at GitHub project. Issues
>       about the
>
>       >       *MSI installer only* can also be reported to
>      
support at diladele.com<mailto:support at diladele.com><mailto:support at diladele.com><mailto:support at diladele.com>.
>
>
>
>
>
>
>
>
>
>
>
>       >       > NB: fixed crash of the tray app on Windows 2012,
>       enabled
>
>       >       delay pools in compilation flags.
>
>
>
>
>
>
>
>
>
>
>
>       >       > Best regards,
>
>
>
>
>
>
>
>       >       > Rafael Akchurin
>
>
>
>
>
>
>
>       >       > Diladele B.V.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>       >       > _______________________________________________
>
>
>
>       >       > squid-users mailing list
>
>
>
>       >       >
>
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org><mailto:squid-users at lists.squid-cache.org><mailto:squid-users at lists.squid-cache.org>
>
>
>
>       >       > http://lists.squid-cache.org/listinfo/squid-users
>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVm/W/AAoJENNXIZxhPexGk2YH/AzBwQFBoOg6/tUPZBY8YzjN
F/umYDWz0l8Z30+y8im8oQDppalxhESQFTtGW80FwOu0lDx4IilKQDgIMcvJ8Bz6
ZMEvJZiMAsC5DX6g/FURIrvANb+oJOiv4/tDATgRxT2t/mJsopyWj6rVAXBT42Eo
wSIzVWQMmb1gbvfO9TTOaLXlSjnoJrcf2YrjYp5xB2MbhbId/Xj3CTmQn9xcJ2B7
f87lmwuOxiANkGwjNK3Fk4FMgqBAyctyBivkN9bqHI7knMgpfTpq9FOIHHwtIggW
gq3jvKt3bsSRvK2ESQSilo/WqwlSct9hFYGJPSNoIceps6c3SrC/822LJJRshJQ=
=33Su
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150707/f90f73b6/attachment.htm>

From jvdwesthuiz at shoprite.co.za  Wed Jul  8 07:20:08 2015
From: jvdwesthuiz at shoprite.co.za (Jasper Van Der Westhuizen)
Date: Wed, 8 Jul 2015 07:20:08 +0000
Subject: [squid-users] Windows 10 Updates
In-Reply-To: <559BDF3B.3090208@treenet.co.nz>
References: <1436277443.2605.137.camel@shoprite.co.za>
 <559BDF3B.3090208@treenet.co.nz>
Message-ID: <1436340007.2605.161.camel@shoprite.co.za>


On 8/07/2015 1:57 a.m., Jasper Van Der Westhuizen wrote:
> Hi list
>
> I have a problem with Windows 10 updates. It seems that Microsoft will do updates via https now.
>
> --cut--
> 1436268325.765 5294 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 9899569 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?<http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?> - HIER_DIRECT/165.165.47.19<http://DIRECT/165.165.47.19> application/octet-stream
> 1436268333.267 7484 xxx.xxx.xxx.xxx TCP_REFRESH_UNMODIFIED/206 21564261 GET http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?<http://tlu.dl.delivery.mp.microsoft.com/filestreamingservice/files/0cbda2af-bf7d-4408-8a17-d305e378c8e5?> - HIER_DIRECT/165.165.47.19<http://DIRECT/165.165.47.19> application/octet-stream
> 1436268430.871 147280 xxx.xxx.xxx.xxx TCP_TUNNEL/200 4267 CONNECT cp201-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/23.214.151.174<http://DIRECT/23.214.151.174> -
> 1436268478.259 96621 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117<http://DIRECT/64.4.54.117> -
> 1436268786.878 78517 xxx.xxx.xxx.xxx TCP_TUNNEL/200 5705 CONNECT array204-prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/64.4.54.117<http://DIRECT/64.4.54.117> -
> --cut--
>
> To my knowledge there is no way to cache this.

Technically yes, there is no way to cache it without breaking into the
HTTPS.

> How would one handle this? Is it even possible to cache the updates?
>

SSL-Bump is the Squid feature for accessing HTTPS data in decrypted form
for filtering and/or caching.

However, that will depend on;
a) being able to "bump" the crypto (if the WU app is validating server
cert against a known signature its not),
b) the content inside actually being HTTPS (they do updates via P2P now
too), and
c) the HTTP content inside being cacheable (no guarantees, but a good
chance its about as cacheable as non-encrypted updates).

You are the first to mention it, so there is no existing info on those
requirements.

Amos

_______________________________
Thank you Amos.

Like in Windows 8.1, these updates are HUGE. I will keep an eye on developments. Microsoft really makes things difficult. For now we will be shaping the bandwidth on the network layer.

Kind Regards
Jasper






Disclaimer:
http://www.shopriteholdings.co.za/Pages/ShopriteE-mailDisclaimer.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150708/5d90bc62/attachment.htm>

From vdoctor at neuf.fr  Wed Jul  8 07:19:34 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 8 Jul 2015 00:19:34 -0700 (PDT)
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraries/CDNs Booster
In-Reply-To: <1433141073516-4671470.post@n4.nabble.com>
References: <54AA918C.10300@gmail.com>
 <1420465642362-4668933.post@n4.nabble.com> <54AA979B.7060803@gmail.com>
 <1420470567538-4668941.post@n4.nabble.com>
 <1421655817441-4669159.post@n4.nabble.com>
 <1422466278065-4669395.post@n4.nabble.com>
 <1423553249913-4669653.post@n4.nabble.com>
 <1424604199892-4670015.post@n4.nabble.com>
 <1426261064321-4670396.post@n4.nabble.com>
 <1433141073516-4671470.post@n4.nabble.com>
Message-ID: <1436339974331-4672107.post@n4.nabble.com>

Hi All,

Advanced Caching Add-On for Linux Squid Proxy Cache v2.7, v3.4 and v3.5 with
Videos, Music, Images, Libraries and CDNs.

New  version 2.545 <https://sourceforge.net/projects/squidvideosbooster/>  
- July 8th 2015.
- Apple Music - new!
- Google Music - new!
- and more ...
More details on https://svb.unveiltech.com

Enjoy

Bye Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-2-7-3-4-and-3-5-Videos-Music-Images-Libraries-CDNs-Booster-tp4668683p4672107.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tomtux007 at gmail.com  Wed Jul  8 09:20:22 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Wed, 8 Jul 2015 11:20:22 +0200
Subject: [squid-users] Fwd: Squid 3.5.5 automatically reload itself in
	2h rhythm
In-Reply-To: <559B925F.8070601@treenet.co.nz>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
 <CACLJR+PDcZgheQKM+Og4gyxo_XWm1hGppHdQJPwkmtq3CMsAjA@mail.gmail.com>
 <CACLJR+OkFmHn=mtqmt132rxaEdyD4K01=mW237MsLCf5ZE2kyw@mail.gmail.com>
 <559B925F.8070601@treenet.co.nz>
Message-ID: <CACLJR+M20-58EL5fAizg--kah_5G-tym_way-d=AHhKvcKzeig@mail.gmail.com>

The workaround in the mentioned 3.5.6-snapshot seems to solve these
periodically restarts.

Many thanks.
Tom

On Tue, Jul 7, 2015 at 10:48 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 7/07/2015 4:27 p.m., Tom Tom wrote:
>> Hi
>>
>> Opened a while ago, but no answer, if this problem is a (known) bug or
>> it's already solved with 3.5.6......?
>
> Its bug 4190. A workaround patch was developed a while ago, but it has
> some problems of its own and I forgot to apply it as a temporary
> workaround for the 3.5.6 cyle.
> see <http://bugs.squid-cache.org/show_bug.cgi?id=4190#c28> for details.
>
> New snapshot containing the workaround is building now and should be
> available in a few hours.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From tarotapprentice at yahoo.com  Wed Jul  8 12:19:20 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Wed, 8 Jul 2015 12:19:20 +0000 (UTC)
Subject: [squid-users] Squid 3.5.6 Windows SquidTray crash
Message-ID: <1463859080.1051090.1436357960591.JavaMail.yahoo@mail.yahoo.com>

Unfortunately SquidTray still crashes with 3.5.6. This is on Server 2008 R2 x64 (as before). The mini dump is shown below.

MarkJ

-----------------------------------------------------------------
Description:
  Stopped working
Problem signature:
  Problem Event Name: CLR20r3
  Problem Signature 01: diladele.squid.tray.exe
  Problem Signature 02: 1.0.0.0
  Problem Signature 03: 559b843a
  Problem Signature 04: mscorlib
  Problem Signature 05: 2.0.0.0
  Problem Signature 06: 53a11de1
  Problem Signature 07: 123f
  Problem Signature 08: 5f
  Problem Signature 09: System.IO.FileNotFoundException
  OS Version: 6.1.7601.2.1.0.272.7
  Locale ID: 3081


From squid3 at treenet.co.nz  Wed Jul  8 12:48:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 00:48:50 +1200
Subject: [squid-users] transparent proxy splice using dstdomain issue
In-Reply-To: <1436277256782-4672095.post@n4.nabble.com>
References: <1436269535080-4672088.post@n4.nabble.com>
 <559BCC59.1090403@treenet.co.nz> <1436277256782-4672095.post@n4.nabble.com>
Message-ID: <559D1C32.4010507@treenet.co.nz>

On 8/07/2015 1:54 a.m., S.Kirschner wrote:
> Amos Jeffries wrote
>> On 7/07/2015 11:45 p.m., S.Kirschner wrote:
>>> I think the issues exist because the reverse lookup dont got the anwser
>>> "sparkasse.de", but why it does not use the hostname from the dns request
>>> to
>>> the dns-server ?
>>
>> Because Squid is not a DNS server.
>>
>> The HTTP message details including URL where dstdomain comes from are
>> encrypted at the time you are trying to use the dstdomain ACL.
> 
> Yes but, in pfsense a dns server is installed, so on these host a dns server
> is running. Also i tried to use the google DNS 

Location of the DNS resolver has nothing to do with how Squid (or DNS)
operates.


> 
> Here now the entries from the cache.log
> 
> With sparkasse.de in /etc/hosts
> #2015/06/19 14:03:03.907 kid1| DomainData.cc(108) match: aclMatchDomainList:
> checking '212.34.69.3'
> #2015/06/19 14:03:03.907 kid1| DomainData.cc(113) match: aclMatchDomainList:
> '212.34.69.3' NOT found
> #2015/06/19 14:03:03.908 kid1| DomainData.cc(108) match: aclMatchDomainList:
> checking 'sparkasse.de'
> #2015/06/19 14:03:03.908 kid1| DomainData.cc(113) match: aclMatchDomainList:
> 'sparkasse.de' found

These are the rDNS host name in your hosts file for that IP.

In DNS hosts file entries are authoritative and override any gobal
registrations.


> #2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: bypass = 1
> #2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: (ssl_bump rule)
> = 1
> #2015/06/19 14:03:03.908 kid1| Acl.cc(158) matches: checked: (ssl_bump
> rules) = 1
> 
> Without sparkasse.de in /etc/hosts
> #2015/06/19 14:05:19.842 kid1| DomainData.cc(108) match: aclMatchDomainList:
> checking '212.34.69.3'
> #2015/06/19 14:05:19.842 kid1| DomainData.cc(113) match: aclMatchDomainList:
> '212.34.69.3' NOT found
> #2015/06/19 14:05:19.842 kid1| DomainData.cc(108) match: aclMatchDomainList:
> checking 'rev-212.34.69.3.rev.izb.net'
> #2015/06/19 14:05:19.842 kid1| DomainData.cc(113) match: aclMatchDomainList:
> 'rev-212.34.69.3.rev.izb.net' NOT found

The real host name registered in global rDNS for that IP.

If I assume you configured Squid to use the pfsense DNS resolver. That
is the hostname it presents Squid with for that IP.

Note that domain name and host name are different concepts...
* one domain name DNS entry possibly points at multiple IPs, and
* multiple domain names can possibly point at one IP, but
* each IP rDNS points at exactly one host name.

So,
 212.34.69.3 is one of possibly many IPs for sparkasse.de.
 sparkasse.de is one of may names pointing at 212.34.69.3.


But, rev-212.34.69.3.rev.izb.net is the host name for 212.34.69.3.
 (which also means rev-212.34.69.3.rev.izb.net is the primary one of may
names pointing at 212.34.69.3).


Problem: since that IP has many domain names pointing at it. Which one
did the user lookup in *forward* DNS to get to that IP address?

 They could have as easily gone to https://rev-212.34.69.3.rev.izb.net/
as to https://sparkasse.de/ and the TCP connection would look identical
to Squid.


When dealing with HTTP (not encrypted) the answer is to look at the HTTP
message headers and see what they are requesting. dstdomain does that.

 BUT ... in HTTPS those headers are encrypted. And you are currently
deciding whether or not its appropriate to try and decrypt at all.

 Meaning the HTTP URL domain used by dstdomain is unavailable, and thus
dstdomain will not work properly.


> #2015/06/19 14:05:19.842 kid1| Acl.cc(158) matches: checked: bypass = 0
> #2015/06/19 14:05:19.842 kid1| Acl.cc(158) matches: checked: (ssl_bump rule)
> = 0
> 

The proper solution for HTTPS is to use the correct ACL type
("ssl::server_name") designed for use in your situation. That uses the
non-encrypted TLS metadata, which provides the server hostname.


Despite popular myths TLS is not end-to-end (user-to-origin). It is
point-to-point (client-to-server) encryption, with maybe multiple hops
along the way.

The TLS server name metadata will only give you the hostname of the
server the client was contacting. With SNI it is usually (but no
guarantee) the domain name. When SNI is not available it's down to TLS
certificate SubjectName that could as easily be a TLS proxy or CDN
service in front of the real server(s) and in fact its usually states a
whole list of alternative names, and regex patterns!! of domain names,
which the cert might be used for.

The definitions for Site, Domain name, host name (note the space),
hostname, and X.509 SubjectName are different for good reasons. So are
the ACL definitions.

HTH
Amos


From matias at ufscar.br  Wed Jul  8 14:33:22 2015
From: matias at ufscar.br (Paulo Matias)
Date: Wed, 08 Jul 2015 11:33:22 -0300
Subject: [squid-users] Question about squid-3.5-13849.patch
In-Reply-To: <559BDCB9.2070005@treenet.co.nz>
References: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>
 <559BDCB9.2070005@treenet.co.nz>
Message-ID: <559D34B2.5010408@ufscar.br>

Hi,

On 07-07-2015 11:05, Amos Jeffries wrote:
> On 8/07/2015 1:37 a.m., dweimer wrote:
>> System is Running on FreeBSD 10.1-RELEASE-p14, using OpenSSL included in
>> base FreeBSD.
> 
> No, the change is automatic for all Squid built against an OpenSSL
> library that supports the library API option. If it is not working, then
> the library you are using probably does not support that option.
> 
> AFAIK you need at least OpenSSL 0.9.8m for anything related to that
> vulnerability to be fixable. The latest 1.x libraries do not support the
> flag we use because they do the rejection internally without needing any
> help from Squid.

Unfortunately this seems not to be the case. I have installed
FreeBSD 10.1-RELEASE-p14 in a VM for testing. Running "openssl version"
reports "OpenSSL 1.0.1l-freebsd 15 Jan 2015". I was able to reproduce
Dean's issue (renegotiation does not get disabled), but I was not able
to fix it so far.

For OpenSSL version comparison purposes, Debian wheezy (which the patch
was able to harden) ships 1.0.1e. Debian jessie (which was already hardened
out-of-the-box, without the patch) ships 1.0.1k. It is strange that FreeBSD's
more recent OpenSSL version (1.0.1l) presents the issue.

The SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS define exists in FreeBSD OpenSSL headers,
the relevant code gets compiled in squid executable, SSL_CTX_set_info_callback
runs, but *the ssl_info_cb callback is never called* (I tested by inserting
a debug message inside the "#if defined", just after SSL_CTX_set_info_callback,
and another one at the beginning of the callback).

Maybe we could try to adapt nginx's solution, but it does not seem to be
trivial to do that in the current codebase
https://github.com/nginx/nginx/commit/70bd187c4c386d82d6e4d180e0db84f361d1be02


Best regards,
Paulo Matias


From dweimer at dweimer.net  Wed Jul  8 16:24:03 2015
From: dweimer at dweimer.net (dweimer)
Date: Wed, 08 Jul 2015 11:24:03 -0500
Subject: [squid-users] Question about squid-3.5-13849.patch
In-Reply-To: <559D34B2.5010408@ufscar.br>
References: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>
 <559BDCB9.2070005@treenet.co.nz> <559D34B2.5010408@ufscar.br>
Message-ID: <46c03ddac901efb616606f9b70119c08@dweimer.net>

On 07/08/2015 9:33 am, Paulo Matias wrote:
> Hi,
> 
> On 07-07-2015 11:05, Amos Jeffries wrote:
>> On 8/07/2015 1:37 a.m., dweimer wrote:
>>> System is Running on FreeBSD 10.1-RELEASE-p14, using OpenSSL included 
>>> in
>>> base FreeBSD.
>> 
>> No, the change is automatic for all Squid built against an OpenSSL
>> library that supports the library API option. If it is not working, 
>> then
>> the library you are using probably does not support that option.
>> 
>> AFAIK you need at least OpenSSL 0.9.8m for anything related to that
>> vulnerability to be fixable. The latest 1.x libraries do not support 
>> the
>> flag we use because they do the rejection internally without needing 
>> any
>> help from Squid.
> 
> Unfortunately this seems not to be the case. I have installed
> FreeBSD 10.1-RELEASE-p14 in a VM for testing. Running "openssl version"
> reports "OpenSSL 1.0.1l-freebsd 15 Jan 2015". I was able to reproduce
> Dean's issue (renegotiation does not get disabled), but I was not able
> to fix it so far.
> 
> For OpenSSL version comparison purposes, Debian wheezy (which the patch
> was able to harden) ships 1.0.1e. Debian jessie (which was already 
> hardened
> out-of-the-box, without the patch) ships 1.0.1k. It is strange that 
> FreeBSD's
> more recent OpenSSL version (1.0.1l) presents the issue.
> 
> The SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS define exists in FreeBSD OpenSSL 
> headers,
> the relevant code gets compiled in squid executable, 
> SSL_CTX_set_info_callback
> runs, but *the ssl_info_cb callback is never called* (I tested by 
> inserting
> a debug message inside the "#if defined", just after 
> SSL_CTX_set_info_callback,
> and another one at the beginning of the callback).
> 
> Maybe we could try to adapt nginx's solution, but it does not seem to 
> be
> trivial to do that in the current codebase
> https://github.com/nginx/nginx/commit/70bd187c4c386d82d6e4d180e0db84f361d1be02
> 
> 

I also tried building against OpenSSL (1.0.2c 12 Jun 2015) from FreeBSD 
ports instead of from base.

Still same result.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From sebag at vianetcon.com.ar  Wed Jul  8 17:17:22 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 08 Jul 2015 14:17:22 -0300
Subject: [squid-users] Two questions about stored objects
Message-ID: <559D5B22.1090201@vianetcon.com.ar>

Hello everyone, I have been making some modifications (size, object max 
size) in some cache dirs and I have a couple of questions:

1) If I lower de maximum object size for a certain cache_dir and 
reconfigure (I did a squid -z without squid running), what happens to 
the files that are no longer in the cache_dir size limits but are 
already stored?

2) If I change the min and max times in refresh_patterns, what happens 
to the objects that are already stored? Where they stored with the "old" 
times or are they going to be re-evaluated the next time they are 
requested by a user?


Thanks a lot
Sebastian


From david at articatech.com  Wed Jul  8 18:26:52 2015
From: david at articatech.com (David Touzeau)
Date: Wed, 08 Jul 2015 20:26:52 +0200
Subject: [squid-users] Issue with Citrix sessions and squid
Message-ID: <559D6B6C.5050306@articatech.com>

Dear

I would like  to share a strange behavior.

We have servers that stores Citrix application.
Each Citrix server run about 10 users/session
Each session execute browsers connected to squid 3.5.6 or 3.3.13.

After opening 10 tabs, browsers generates error about Connections broken 
or connection unavailable from Proxy.
So next tabs cannot be opened correctly.
Both HTTP/HTTPS destination websites meet this behavior.
if we wait several seconds , refresh the browser tab that generate the 
error, website can be opened.
No Squid error page can be seen on the browser error.

Using the same test without Squid ( in direct mode ) and inside a Citrix 
Session did *not* reproduce this issue
Using the same test on a physical machine did *not* reproduce this issue.
Using the same test on a server "Without Citrix" inside a TSE session 
did *not* reproduce this issue.

Using the same test on  with Chrome and Internet Explorer and FireFox 
*reproduce* the issue
Using Squid without any ACL, without any caching system, with only one 
worker *reproduce* this issue


This issue can only be reproduced on a server with Citrix installed or 
inside a Citrix Session with Squid as proxy.

It seems/like that Squid refuse connections from a Citrix Server.




Can anybody have already reproduced/fixed this issue ?

Is there a squid limitation of number of opened browsers from one single 
IP ( the Citrix server) ?


















From yvoinov at gmail.com  Wed Jul  8 18:48:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 9 Jul 2015 00:48:53 +0600
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D6B6C.5050306@articatech.com>
References: <559D6B6C.5050306@articatech.com>
Message-ID: <559D7095.3030209@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Looks like TCP/IP stack level issue.

09.07.15 0:26, David Touzeau ?????:
> Dear
>
> I would like  to share a strange behavior.
>
> We have servers that stores Citrix application.
> Each Citrix server run about 10 users/session
> Each session execute browsers connected to squid 3.5.6 or 3.3.13.
>
> After opening 10 tabs, browsers generates error about Connections
broken or connection unavailable from Proxy.
> So next tabs cannot be opened correctly.
> Both HTTP/HTTPS destination websites meet this behavior.
> if we wait several seconds , refresh the browser tab that generate the
error, website can be opened.
> No Squid error page can be seen on the browser error.
>
> Using the same test without Squid ( in direct mode ) and inside a
Citrix Session did *not* reproduce this issue
> Using the same test on a physical machine did *not* reproduce this issue.
> Using the same test on a server "Without Citrix" inside a TSE session
did *not* reproduce this issue.
>
> Using the same test on  with Chrome and Internet Explorer and FireFox
*reproduce* the issue
> Using Squid without any ACL, without any caching system, with only one
worker *reproduce* this issue
>
>
> This issue can only be reproduced on a server with Citrix installed or
inside a Citrix Session with Squid as proxy.
>
> It seems/like that Squid refuse connections from a Citrix Server.
>
>
>
>
> Can anybody have already reproduced/fixed this issue ?
>
> Is there a squid limitation of number of opened browsers from one
single IP ( the Citrix server) ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVnXCVAAoJENNXIZxhPexGjmsIAMmY4fHGysvNCENf9LlM8fNQ
i3lLCHls3JDSP/tcaCWOuFkuGn/2SejEs7kZx5iw/JGcE6WNwTZg/gCkK/aMmBCb
8LaBs7tixcpfYT1zqT/xeax7Sz6gjB5aNcVKLL6jNTuZX2Q3bJZx/UhZbNCM99cV
XN9VlrvErM4P45KvlBeZFkdOOPgCK49uMfEZmPN15RxbqRj8WeuBRiX5QFXwZHTO
/IcjAAorVcRgrdCC8DmwS9MwPZwDfdWvX6PYTLmcUoexJMlAIikwFW6GJ+dloUMW
OVsFol29IzKUQc1oBqc4CPwN2UURnhTFKMTsp+NQh3702hY49rHeaPPoyTqipN0=
=SK/q
-----END PGP SIGNATURE-----



From david at articatech.com  Wed Jul  8 19:01:25 2015
From: david at articatech.com (David Touzeau)
Date: Wed, 08 Jul 2015 21:01:25 +0200
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D7095.3030209@gmail.com>
References: <559D6B6C.5050306@articatech.com> <559D7095.3030209@gmail.com>
Message-ID: <559D7385.3040000@articatech.com>

Thanks Yuri,

Any tips how to increase TCP/IP stack ?
Did you means TCP/IP stack on the Citrix Server side or on the squid 
box  or both ?

Because , all computers that did not use Citrix can surf trough squid 
and open unlimited tabs without any issue.
And Citrix sessions that *did not use Squid* can surf trough Internet 
and open unlimited tabs without any issue.



Le 08/07/2015 20:48, Yuri Voinov a ?crit :
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> Looks like TCP/IP stack level issue.
>
> 09.07.15 0:26, David Touzeau ?????:
>> Dear
>>
>> I would like  to share a strange behavior.
>>
>> We have servers that stores Citrix application.
>> Each Citrix server run about 10 users/session
>> Each session execute browsers connected to squid 3.5.6 or 3.3.13.
>>
>> After opening 10 tabs, browsers generates error about Connections
> broken or connection unavailable from Proxy.
>> So next tabs cannot be opened correctly.
>> Both HTTP/HTTPS destination websites meet this behavior.
>> if we wait several seconds , refresh the browser tab that generate the
> error, website can be opened.
>> No Squid error page can be seen on the browser error.
>>
>> Using the same test without Squid ( in direct mode ) and inside a
> Citrix Session did *not* reproduce this issue
>> Using the same test on a physical machine did *not* reproduce this issue.
>> Using the same test on a server "Without Citrix" inside a TSE session
> did *not* reproduce this issue.
>> Using the same test on  with Chrome and Internet Explorer and FireFox
> *reproduce* the issue
>> Using Squid without any ACL, without any caching system, with only one
> worker *reproduce* this issue
>>
>> This issue can only be reproduced on a server with Citrix installed or
> inside a Citrix Session with Squid as proxy.
>> It seems/like that Squid refuse connections from a Citrix Server.
>>
>>
>>
>>
>> Can anybody have already reproduced/fixed this issue ?
>>
>> Is there a squid limitation of number of opened browsers from one
> single IP ( the Citrix server) ?
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJVnXCVAAoJENNXIZxhPexGjmsIAMmY4fHGysvNCENf9LlM8fNQ
> i3lLCHls3JDSP/tcaCWOuFkuGn/2SejEs7kZx5iw/JGcE6WNwTZg/gCkK/aMmBCb
> 8LaBs7tixcpfYT1zqT/xeax7Sz6gjB5aNcVKLL6jNTuZX2Q3bJZx/UhZbNCM99cV
> XN9VlrvErM4P45KvlBeZFkdOOPgCK49uMfEZmPN15RxbqRj8WeuBRiX5QFXwZHTO
> /IcjAAorVcRgrdCC8DmwS9MwPZwDfdWvX6PYTLmcUoexJMlAIikwFW6GJ+dloUMW
> OVsFol29IzKUQc1oBqc4CPwN2UURnhTFKMTsp+NQh3702hY49rHeaPPoyTqipN0=
> =SK/q
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul  8 21:26:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 09:26:37 +1200
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D7385.3040000@articatech.com>
References: <559D6B6C.5050306@articatech.com> <559D7095.3030209@gmail.com>
 <559D7385.3040000@articatech.com>
Message-ID: <559D958D.9050708@treenet.co.nz>

On 9/07/2015 7:01 a.m., David Touzeau wrote:
> Thanks Yuri,
> 
> Any tips how to increase TCP/IP stack ?
> Did you means TCP/IP stack on the Citrix Server side or on the squid
> box  or both ?

I'm thinking its a problem related to TCP sockets.
A rough estimate calculatino of:
 10 users x10 tabs x20 avg domains per page x 2 for happy eyeballs
makes it somewhere up to 4k sockets in active use at any time. if the
users are accessing domains with larger numbers ofdomains per page (ie
Facebook has up to 100) that could be 20k concurrent sockets just from
the browser.
 By the time that goes through Squid it becomes 40k, and if you have
ICAP it becomes "up to 80K" (out of an available 64k sockets).

Then there is all the OS background services that use HTTP through the
proxy, etc.

Without Citrix the users internal src-IPs vary. Making available a 64k
sockets per-user. Which is harder to reach, and the browser silently
limits itself when socekts start to run out.

Without Squid the Citrix connections are going to N different domains
with varying dst-IP. Which again raises the available port numbers per-user.


If the assumptino behind the above is right you should be able to
alleviate the problem by having Squid listen on multiple ports and/or
IPs. Then spreading the client connections out across those Squid ports.

Amos


From squid3 at treenet.co.nz  Wed Jul  8 21:33:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 09:33:45 +1200
Subject: [squid-users] Two questions about stored objects
In-Reply-To: <559D5B22.1090201@vianetcon.com.ar>
References: <559D5B22.1090201@vianetcon.com.ar>
Message-ID: <559D9739.6030504@treenet.co.nz>

On 9/07/2015 5:17 a.m., Sebastian Goicochea wrote:
> Hello everyone, I have been making some modifications (size, object max
> size) in some cache dirs and I have a couple of questions:
> 
> 1) If I lower de maximum object size for a certain cache_dir and
> reconfigure (I did a squid -z without squid running), what happens to
> the files that are no longer in the cache_dir size limits but are
> already stored?

No. It only affects new objects being stored to disk.

There is one caveat however. Sometimes objects get "promoted" from disk
to memory caching. When those cycle back to disk they will not go to the
original cache_dir. Which may leave you with an un-deleted but nolonger
indexed file on disk.

> 
> 2) If I change the min and max times in refresh_patterns, what happens
> to the objects that are already stored? Where they stored with the "old"
> times or are they going to be re-evaluated the next time they are
> requested by a user?

Have no effect except on active traffic. That does include "active" in
the sense of being saved to disk. But not objects just sitting there
already.


Overall, if you change settings like these the state does slowly
migrates to the new values as cached content expires. But that is not
fast, could take minutes or weeks depending on your initial state.

To make immediate administrative changes to on-disk AUFS/UFS/diskd cache
content use the squid-purge tool which is bundled with recent Squid
versions.

Amos



From squid3 at treenet.co.nz  Wed Jul  8 22:14:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 10:14:50 +1200
Subject: [squid-users] Question about squid-3.5-13849.patch
In-Reply-To: <559D34B2.5010408@ufscar.br>
References: <95d0b429511b3ca23bd5fba24d4ca9f0@dweimer.net>
 <559BDCB9.2070005@treenet.co.nz> <559D34B2.5010408@ufscar.br>
Message-ID: <559DA0DA.5030004@treenet.co.nz>

On 9/07/2015 2:33 a.m., Paulo Matias wrote:
> Hi,
> 
> On 07-07-2015 11:05, Amos Jeffries wrote:
>> On 8/07/2015 1:37 a.m., dweimer wrote:
>>> System is Running on FreeBSD 10.1-RELEASE-p14, using OpenSSL included in
>>> base FreeBSD.
>>
>> No, the change is automatic for all Squid built against an OpenSSL
>> library that supports the library API option. If it is not working, then
>> the library you are using probably does not support that option.
>>
>> AFAIK you need at least OpenSSL 0.9.8m for anything related to that
>> vulnerability to be fixable. The latest 1.x libraries do not support the
>> flag we use because they do the rejection internally without needing any
>> help from Squid.
> 
> Unfortunately this seems not to be the case. I have installed
> FreeBSD 10.1-RELEASE-p14 in a VM for testing. Running "openssl version"
> reports "OpenSSL 1.0.1l-freebsd 15 Jan 2015". I was able to reproduce
> Dean's issue (renegotiation does not get disabled), but I was not able
> to fix it so far.
> 
> For OpenSSL version comparison purposes, Debian wheezy (which the patch
> was able to harden) ships 1.0.1e. Debian jessie (which was already hardened
> out-of-the-box, without the patch) ships 1.0.1k. It is strange that FreeBSD's
> more recent OpenSSL version (1.0.1l) presents the issue.
> 
> The SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS define exists in FreeBSD OpenSSL headers,
> the relevant code gets compiled in squid executable, SSL_CTX_set_info_callback
> runs, but *the ssl_info_cb callback is never called* (I tested by inserting
> a debug message inside the "#if defined", just after SSL_CTX_set_info_callback,
> and another one at the beginning of the callback).

That would be a nasty bug in the FreeBSD OpenSSL then.

(FreeBSD 10 is growing an annoying set of bugs; libpthreads not working,
OS signals not working, now OpenSSL not working...)

> 
> Maybe we could try to adapt nginx's solution, but it does not seem to be
> trivial to do that in the current codebase
> https://github.com/nginx/nginx/commit/70bd187c4c386d82d6e4d180e0db84f361d1be02
> 

They are using the same SSL_CTX_set_info_callback() mechanism we are to
set the initial flag which triggers errors. If the callback itself is
not being run their fix will not work either.

Amos



From squid at borrill.org.uk  Thu Jul  9 07:33:48 2015
From: squid at borrill.org.uk (Stephen Borrill)
Date: Thu, 09 Jul 2015 08:33:48 +0100
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D6B6C.5050306@articatech.com>
References: <559D6B6C.5050306@articatech.com>
Message-ID: <559E23DC.9060402@borrill.org.uk>

On 08/07/2015 19:26, David Touzeau wrote:
> Dear
>
> I would like  to share a strange behavior.
>
> We have servers that stores Citrix application.
> Each Citrix server run about 10 users/session
> Each session execute browsers connected to squid 3.5.6 or 3.3.13.
>
> After opening 10 tabs, browsers generates error about Connections broken
> or connection unavailable from Proxy.
> So next tabs cannot be opened correctly.
> Both HTTP/HTTPS destination websites meet this behavior.
> if we wait several seconds , refresh the browser tab that generate the
> error, website can be opened.
> No Squid error page can be seen on the browser error.
>
> Using the same test without Squid ( in direct mode ) and inside a Citrix
> Session did *not* reproduce this issue
> Using the same test on a physical machine did *not* reproduce this issue.
> Using the same test on a server "Without Citrix" inside a TSE session
> did *not* reproduce this issue.
>
> Using the same test on  with Chrome and Internet Explorer and FireFox
> *reproduce* the issue
> Using Squid without any ACL, without any caching system, with only one
> worker *reproduce* this issue
>
> This issue can only be reproduced on a server with Citrix installed or
> inside a Citrix Session with Squid as proxy.
>
> It seems/like that Squid refuse connections from a Citrix Server.
>
> Can anybody have already reproduced/fixed this issue ?

I've been running squid + Citrix in a number of large installations 
since the 1990's. I've never seen a problem like you describe.

Are you using virtual IP addressing in your session?

-- 
Stephen


From 2005now at mail.ru  Thu Jul  9 09:54:08 2015
From: 2005now at mail.ru (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0KDRg9C60LDQstGG0L7Qsg==?=)
Date: Thu, 09 Jul 2015 12:54:08 +0300
Subject: [squid-users] =?utf-8?q?Squid_+_kerberos=2C_all_childrens_are_bus?=
	=?utf-8?q?y?=
Message-ID: <1436435648.897981882@f301.i.mail.ru>

 Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.

Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:

2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy. 
2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued

If i put 100 childrens at config it won't help too.

TTL are fine:

??????? auth_param negotiate program /usr/local/libexec/squid/negotiate_kerberos_auth -r -s HTTP/comp.domain.com at DOMAIN.COM
??????? auth_param negotiate children 60 startup=15 idle=1
??????? auth_param negotiate keep_alive on
??????? auth_param basic program /usr/local/libexec/squid/basic_ldap_auth -R -D user at domain.com -w "pass" -b "DC=domain,DC=com" -f "sAMAccountName=%s" -h domain.com
??????? auth_param basic credentialsttl 8 hours
??????? auth_param basic children 10

?????? authenticate_ttl 8 hour

????? external_acl_type nt_group ttl=1200 %LOGIN?????? /usr/local/libexec/squid/ext_ldap_group_acl -R -b "DC=domain,DC=com" -f "(&(sAMAccountName=%v)(memberOf=CN=%a,OU=squid,DC=domain,DC=com))" -D user at domain.com -w "pass" -h domain.com

KRB5.CONF

[libdefaults] 
? ? ? ? default_realm = DOMAIN.COM
? ? ? ? dns_lookup_realm = no 
? ? ? ? dns_lookup_kdc = no 
? ? ? ? ticket_lifetime = 24h 
? ? ? ? default_keytab_name = /usr/local/etc/squid/comp.domain.com.keytab 
? ? ? ? default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5 
? ? ? ? default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5 
? ? ? ? permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5 

[realms] 
? ? ? ? DOMAIN.COM = { 
? ? ? ? ? ? kdc = kd1.domain.com
? ? ? ? ? ? kdc = kd2.domain.com
? ? ? ? ? ? admin_server = kd1.domain.com
? ? ? ? ? ? default_domain = domain.com
? ? ? ? } 

[domain_realm] 
? ? ? ? .domain.com? = DOMAIN.COM
? ? ? ? domain.com = DOMAIN.COM


Server shutting down in like 7 mins, i can't even restart squid(system endless trying to kill squid PID), can't even make kill -9, not working (but system load is very low)

Can you please help me to find out what is wrong? Is there any way to monitor what happens with negotiate_kerberos_auth  processes ?



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150709/9863ae65/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  9 10:06:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 22:06:40 +1200
Subject: [squid-users] Squid + kerberos, all childrens are busy
In-Reply-To: <1436435648.897981882@f301.i.mail.ru>
References: <1436435648.897981882@f301.i.mail.ru>
Message-ID: <559E47B0.2030205@treenet.co.nz>

On 9/07/2015 9:54 p.m., ??????? ???????? wrote:
>  Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
> 
> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
> 
> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy. 
> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
> 

So 50 users / 60 helpers ... how many requests per second? and how
fast/slow is the helper responding?


> If i put 100 childrens at config it won't help too.
> 

Please define "wont help".

> 
> Server shutting down in like 7 mins, i can't even restart squid(system endless trying to kill squid PID), can't even make kill -9, not working (but system load is very low)
> 
> Can you please help me to find out what is wrong? Is there any way to monitor what happens with negotiate_kerberos_auth  processes ?

-d helper parameter should log to cache.log what its doing.

Amos



From 2005now at mail.ru  Thu Jul  9 10:32:12 2015
From: 2005now at mail.ru (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0KDRg9C60LDQstGG0L7Qsg==?=)
Date: Thu, 09 Jul 2015 13:32:12 +0300
Subject: [squid-users]
 =?utf-8?q?Squid_+_kerberos=2C_all_childrens_are_bus?= =?utf-8?q?y?=
In-Reply-To: <559E47B0.2030205@treenet.co.nz>
References: <1436435648.897981882@f301.i.mail.ru>
 <559E47B0.2030205@treenet.co.nz>
Message-ID: <1436437932.137673239@f40.i.mail.ru>




>>???????,  9 ???? 2015, 22:06 +12:00 ?? Amos Jeffries <squid3 at treenet.co.nz>:
>>
>>On 9/07/2015 9:54 p.m., ??????? ???????? wrote:
>>>  Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
>>> 
>>> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
>>> 
>>> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy. 
>>> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
>>> 
>>
>>So 50 users / 60 helpers ... how many requests per second? and how
>>fast/slow is the helper responding?
Could you clarify how I can get?value of requests per second and respond?
>
>>> If i put 100 childrens at config it won't help too.
>>> 
>>
>>Please define "wont help".
It will take more time, but all 100 will be busy too.
>
>>> 
>>> Server shutting down in like 7 mins, i can't even restart squid(system endless trying to kill squid PID), can't even make kill -9, not working (but system load is very low)
>>> 
>>> Can you please help me to find out what is wrong? Is there any way to monitor what happens with negotiate_kerberos_auth  processes ?
>>
>>-d helper parameter should log to cache.log what its doing.
>
Debugs show like 3-4 message per second like:

negotiate_kerberos_pac.cc(376): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got PAC data of lengh 624
negotiate_kerberos_pac.cc(180): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 5 rids
negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 513
negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 3692
negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4268
negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4622
negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4623
negotiate_kerberos_pac.cc(256): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got DomainLogonId S-1-5-21-1708537768-1580818891-2146958067
negotiate_kerberos_pac.cc(278): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 1 ExtraSIDs
negotiate_kerberos_pac.cc(327): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got ExtraSid S-1-5-21-1708537768-1580818891-2146958067-4107
negotiate_kerberos_pac.cc(456): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Read 620 of 624 bytes
negotiate_kerberos_auth.cc(778): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: Groups group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/AQIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/bA4AAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/rBAAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DhIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DxIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/CxAAAA==
negotiate_kerberos_auth.cc(783): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: AF oYGyMIGvoAMKAQChCwYJKoZIgvcSAQICooGaBIGXYIGUBgkqhkiG9xIBAgICAG+BhDCBgaADAgEFoQMCAQ+idTBzoAMCAReibARqY4fSYtg+X4HhiH8dFmWxdn3wxtoKKZzEfUjLYibMoy0XAAWgkSYVXgC7gxO7cgCkOofEqZQhi/GKa4NZqn2dQqOJU/3y4zkPqBP9Ialh//BL5ov03L5BqjgthrbYbrcxJTo57EJIdO8O1g== avialex

And errors like:
2015/07/09 13:28:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
All my friends get the same error, but their squid is working fine.

Don't see anything else
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150709/d0ebc698/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  9 12:42:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jul 2015 00:42:38 +1200
Subject: [squid-users] Squid + kerberos, all childrens are busy
In-Reply-To: <1436437932.137673239@f40.i.mail.ru>
References: <1436435648.897981882@f301.i.mail.ru>
 <559E47B0.2030205@treenet.co.nz> <1436437932.137673239@f40.i.mail.ru>
Message-ID: <559E6C3E.5030004@treenet.co.nz>

On 9/07/2015 10:32 p.m., ??????? ???????? wrote:
> 
> 
> 
>>> ???????,  9 ???? 2015, 22:06 +12:00 ?? Amos Jeffries <squid3 at treenet.co.nz>:
>>>
>>> On 9/07/2015 9:54 p.m., ??????? ???????? wrote:
>>>>  Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
>>>>
>>>> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
>>>>
>>>> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy. 
>>>> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
>>>>
>>>
>>> So 50 users / 60 helpers ... how many requests per second? and how
>>> fast/slow is the helper responding?
> Could you clarify how I can get value of requests per second and respond?

The cachemgr "info" report. From the cachemgr.cgi tool, or "squidclient
mgr:info" command line, or
http://$visible_hostname:3128/squid-internal-mgr/info

 Or calculated from a quick count of the access.log lines over a few mins.



>>
>>>> If i put 100 childrens at config it won't help too.
>>>>
>>>
>>> Please define "wont help".
> It will take more time, but all 100 will be busy too.
>>
>>>>
>>>> Server shutting down in like 7 mins, i can't even restart squid(system endless trying to kill squid PID), can't even make kill -9, not working (but system load is very low)
>>>>
>>>> Can you please help me to find out what is wrong? Is there any way to monitor what happens with negotiate_kerberos_auth  processes ?
>>>
>>> -d helper parameter should log to cache.log what its doing.
>>
> Debugs show like 3-4 message per second like:
> 
> negotiate_kerberos_pac.cc(376): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got PAC data of lengh 624
> negotiate_kerberos_pac.cc(180): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 5 rids
> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 513
> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 3692
> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4268
> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4622
> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4623
> negotiate_kerberos_pac.cc(256): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got DomainLogonId S-1-5-21-1708537768-1580818891-2146958067
> negotiate_kerberos_pac.cc(278): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 1 ExtraSIDs
> negotiate_kerberos_pac.cc(327): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got ExtraSid S-1-5-21-1708537768-1580818891-2146958067-4107
> negotiate_kerberos_pac.cc(456): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Read 620 of 624 bytes
> negotiate_kerberos_auth.cc(778): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: Groups group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/AQIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/bA4AAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/rBAAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DhIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DxIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/CxAAAA==
> negotiate_kerberos_auth.cc(783): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: AF oYGyMIGvoAMKAQChCwYJKoZIgvcSAQICooGaBIGXYIGUBgkqhkiG9xIBAgICAG+BhDCBgaADAgEFoQMCAQ+idTBzoAMCAReibARqY4fSYtg+X4HhiH8dFmWxdn3wxtoKKZzEfUjLYibMoy0XAAWgkSYVXgC7gxO7cgCkOofEqZQhi/GKa4NZqn2dQqOJU/3y4zkPqBP9Ialh//BL5ov03L5BqjgthrbYbrcxJTo57EJIdO8O1g== avialex
> 
> And errors like:
> 2015/07/09 13:28:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
> All my friends get the same error, but their squid is working fine.
> 
> Don't see anything else
> 

Aha. So your users browsers are sending NTLM auth instead of Kerberos.
That is at least one part of the problem. NTLM handshake can take whole
seconds and places a lot of extra load on the helpers. To resolve these
the users software needs fixing to use Kerberos properly when Negotiate
is offered.

The other part is figuring out what amount of helpers is needed to meet
the load requirements. With NTLM it is usually several hundred.

Amos



From andrew at perpetualmotion.co.uk  Thu Jul  9 13:05:09 2015
From: andrew at perpetualmotion.co.uk (Andrew Wood)
Date: Thu, 09 Jul 2015 14:05:09 +0100
Subject: [squid-users] Difference between Squid 3.1 & 3.4 regarding HTTPS
	CONNECT handling
Message-ID: <559E7185.9040709@perpetualmotion.co.uk>

Ive had Squid 3.1 running on a Debian 7 server for a couple of years 
working in intercept mode for HTTP traffic and using WPAD to force HTTPS 
to be CONNECT tunneled through Squid and it works fine.

Im now trying to set up a similar system for someone else using Squid 
3.4 which is the version in Debian 8. Everything is configured 
identically between the two as I copied the iptables rule file and the 
squid.conf file over, the only thing changed was the IP address ranges 
for the LAN

HTTP intercept is working OK but HTTPS connect requests are being 
refused. Despite the fact that the WPAD file had been updated with the 
new IP address and is being served correctly from lighttpd.

I can only conclude therefore that there is some difference between 
Squid 3.1 and 3.4 which means it no longer likes my configuration.

Below is the Squid.conf file

VLAN1 is the staff LAN with IP addresses 192.168.10 and VLAN2 is the 
public Wifi LAN with IP addresses 192.168.100

Is there anything here in the squid file which Im doing wrong?

Many thanks

#squid.conf:

#set which port to accept clients on & which interfaces to accept clients on
http_port 192.168.10.254:3128 intercept
http_port 192.168.100.254:3128 intercept


#set delay pool to do bandwidth throttling on VLAN2
delay_pools 1
delay_class 1 2
delay_parameters 1 250000/500000 125000/500000

#ORd
acl AllUsers src all
acl ToSentryBoxVL1 dstdom_regex ^192.168.10.254$
acl ToWPADServer dstdom_regex ^wpad.commsmuseum.local$
acl ToPublicWiFiGateway dstdom_regex ^192.168.100.254$
acl PublicWiFiLAN src 192.168.100.0/24
acl PrivateLAN src 192.168.10.0/24
acl DestinedForPrivateLAN dst 192.168.10.0/24
acl DestinedForPublicWiFiLAN dst 192.168.100.0/24
acl ProhibitedSitesDomains dstdomain "/var/squidblacklist.org/domains.squid"
acl IPAddressForHostname dstdom_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$
acl SafePorts port 80 443
acl SSLPorts port 443

#block users tunneling
acl CONNECT method CONNECT
http_access deny CONNECT !SSLPorts


#disable caching
cache deny all

#add VLAN2 to delay pool 1
delay_access 1 allow PublicWiFiLAN

#force traffic coming in on VLAN2 to go out on VLAN2
tcp_outgoing_address 192.168.100.254 PublicWiFiLAN
tcp_outgoing_address 192.168.10.254 PrivateLAN

#block traffic between VLAN1 & VLAN2
#iptables does this for everything EXCEPT stuff coming through Squid
#because iptables sees stuff coming out of squid as originating from the 
localhost
#hence iptables FORWARD rules dont apply
http_access deny PublicWiFiLAN DestinedForPrivateLAN
http_access deny PrivateLAN DestinedForPublicWiFiLAN

#show splash screen to new users on public wifi to show t&c etc
#pass session length as arg to perl script cache +ve & -ve responses for 
0 secs so
#perl script is always called. Script is responsible for deciding how 
long a session is valid for
#%SRC passes client ip on stdin
external_acl_type currentsessiontype ttl=60 negative_ttl=0 %SRC 
/var/publicwifisessions/checksession.pl
acl currentsession external currentsessiontype

#will stop on first of these which matches so watch order!
http_access deny ProhibitedSitesDomains
http_access allow ToPublicWiFiGateway
http_access allow ToSentryBoxVL1
http_access allow ToWPADServer
http_access deny CONNECT PublicWiFiLAN !currentsession
http_access deny PublicWiFiLAN !currentsession
http_access deny IPAddressForHostname
http_access deny !SafePorts
http_access allow PrivateLAN
http_access allow PublicWiFiLAN
http_access deny AllUsers




From tmblue at gmail.com  Thu Jul  9 18:43:22 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 9 Jul 2015 11:43:22 -0700
Subject: [squid-users] Squid 3.5.6 build switches seem to not work.
Message-ID: <CAEaSS0bRCCCYpFLSdZuhN89gUEv4Cpxuy+nm-QjGN5LyNdwsoQ@mail.gmail.com>

I had my build scripts working with the 3.5 betas, but i've switched over
to the mainline and I'm getting a ton of invalid switches.

So far I've had to comment out the following, as I'm getting errors such as

config.status: executing libtool commands

+ --enable-snmp --enable-storeio=aufs,diskd,ufs,rock --enable-wccpv2
--enable-esi --enable-ssl --enable-ssl-crtd --enable-icmp --with-aio
--with-default-user=squid --with-filedescriptors=16384 --with-dl
--with-openssl --with-pthreads --with-included-ltdl --disable-arch-native
--without-nettle

/var/tmp/rpm-tmp.Gsh6Y6: line 80: --enable-snmp: command not found

error: Bad exit status from /var/tmp/rpm-tmp.Gsh6Y6 (%build)

   #--enable-cache-digests \

   #--enable-cachemgr-hostname=localhost \

   #--enable-delay-pools \

   #--enable-epoll \

   #--enable-icap-client \

   #--enable-ident-lookups \

   %ifnarch ppc64 ia64 x86_64 s390x

   --with-large-files \

   %endif

   #--enable-linux-netfilter \

   #--enable-removal-policies="heap,lru" \

   #--enable-snmp \



Here is my configure statements (again the comments are all new as I was
trying to figure out what was going on..)


%configure \

   --exec_prefix=/usr \

   --libexecdir=%{_libdir}/squid \

   --localstatedir=/var \

   --datadir=%{_datadir}/squid \

   --sysconfdir=%{_sysconfdir}/squid \

   --with-logdir='$(localstatedir)/log/squid' \

   --with-pidfile='$(localstatedir)/run/squid.pid' \

   --disable-dependency-tracking \

   --enable-follow-x-forwarded-for \

   --enable-auth \

   --enable-auth-basic="DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam"
\

   --enable-auth-ntlm="smb_lm,fake" \

   --enable-auth-digest="file,LDAP,eDirectory" \

   --enable-auth-negotiate="kerberos,wrapper" \

   --enable-external-acl-helpers="wbinfo_group,kerberos_ldap_group" \


#--enable-external-acl-helpers="wbinfo_group,kerberos_ldap_group,AD_group" \

   #--enable-cache-digests \

   #--enable-cachemgr-hostname=localhost \

   #--enable-delay-pools \

   #--enable-epoll \

   #--enable-icap-client \

   #--enable-ident-lookups \

   %ifnarch ppc64 ia64 x86_64 s390x

   --with-large-files \

   %endif

   #--enable-linux-netfilter \

   #--enable-removal-policies="heap,lru" \

   --enable-snmp \

   --enable-storeio="aufs,diskd,ufs,rock" \

   --enable-wccpv2 \

   --enable-esi \

   --enable-ssl \

   --enable-ssl-crtd  \

   --enable-icmp \

   --with-aio \

   --with-default-user="squid" \

   --with-filedescriptors=16384 \

   --with-dl \

   --with-openssl \

   --with-pthreads \

   --with-included-ltdl \

   --disable-arch-native \

   --without-nettle
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150709/3f2c0a02/attachment.htm>

From alex_wu2012 at hotmail.com  Thu Jul  9 19:03:21 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Thu, 9 Jul 2015 12:03:21 -0700
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <559356EE.2060209@treenet.co.nz>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>,
 <55922067.2000401@treenet.co.nz>
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>, <559356EE.2060209@treenet.co.nz>
Message-ID: <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>

It seems the option http_port cannot be put under each process ID. If using workers, http_port cannot bind to ports specified from http_port.

Alex
> Date: Wed, 1 Jul 2015 14:56:46 +1200
> From: squid3 at treenet.co.nz
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] sslbump and caching of generated cert
> 
> On 1/07/2015 5:08 a.m., Alex Wu wrote:
> > /*
> > You could assign two workers, each with a different http_port and
> > ssl_crtd helper using different cert databases.
> > 
> > */
> > 
> > How to do this? It sounds it might meet our need. 
> > 
> 
> at the top of squid.conf place:
> 
>  workers 2
> 
>  if ${process_number} = 1
>    http_port 10045 ...
>    sslcrtd_program ...
> 
>  else
>    http_port 10046 ...
>    sslcrtd_program ...
> 
>  endif
> 
> The list of other directives which also need separate per-worker
> configuration can be found at
> <http://wiki.squid-cache.org/MultipleInstances#Relevant_squid.conf_directives>.
> 
> 
> > The reason is that we assign a port for internal, 
> > so we can use cheap CA (self-generated CA), for the collaboration, we use a diffrent port, 
> > may need to set up a different CA.
> 
> That dont make sense to me. There should be no need for internal traffic
> to use a different CA from what external has. Costs are already paid to
> get the public CA, there is no incremental increase for internal traffic
> to use it as well.
> 
> You can do simpler things like using a private LAN-specific IP on the
> listening http_port for internal traffic and myportname ACL for internal
> vs external access controls (that work regardless of whether the request
> has been bumped or not).
> 
> Amos
> 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150709/07bef2a1/attachment.htm>

From iknano at outlook.com  Thu Jul  9 20:19:21 2015
From: iknano at outlook.com (Ikna Nou)
Date: Thu, 9 Jul 2015 16:19:21 -0400
Subject: [squid-users] Squid and ufdbGuard,
 display blocked URL on client browser address bar
Message-ID: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>

Dear all,?
We recently migrated from Squid3.4.13/squidGuard to Squid3.5/ufdbGuard

With Squid3.4+squidguard, we were able to display on clients browser a customized error page showing ONLY the original URL request on the address bar.

But, now: what we display on clients browser is:
http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com

>From command line:
:~#echo "http://www.blocked_site.com 10.10.0.1/ - - GET" | /usr/local/ufdbguard/bin/ufdbgclient -d
OK status=302 url="http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com"

squid access log:
10.10.2.2 GET http://www.blocked_site.com/ HTTP/1.1 - 287 - "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36" TCP_REDIRECT:HIER_NONE - www.blocked_site.com / - - -


Is it possible to achieve the prevoius behavior? (display the blocked URL on clients browser address bar?)
Thank you

 		 	   		  

From andrew at perpetualmotion.co.uk  Thu Jul  9 20:30:24 2015
From: andrew at perpetualmotion.co.uk (Andrew Wood)
Date: Thu, 09 Jul 2015 21:30:24 +0100
Subject: [squid-users] Difference between Squid 3.1 & 3.4 regarding
 HTTPS CONNECT handling
In-Reply-To: <559E7185.9040709@perpetualmotion.co.uk>
References: <559E7185.9040709@perpetualmotion.co.uk>
Message-ID: <559ED9E0.4030004@perpetualmotion.co.uk>

OK heres the difference

http_port 192.168.10.254:3128 intercept
http_port 192.168.10.254:3129

I had to setup squid on a second port not in intercept mode and set the 
WPAD file to send HTTPS requests there.

Why will the intercept port accept HTTPS CONNECT requests in 3.1 but not 
3.4?

Regards
Andrew

On 09/07/15 14:05, Andrew Wood wrote:
> Ive had Squid 3.1 running on a Debian 7 server for a couple of years 
> working in intercept mode for HTTP traffic and using WPAD to force 
> HTTPS to be CONNECT tunneled through Squid and it works fine.
>
> Im now trying to set up a similar system for someone else using Squid 
> 3.4 which is the version in Debian 8. Everything is configured 
> identically between the two as I copied the iptables rule file and the 
> squid.conf file over, the only thing changed was the IP address ranges 
> for the LAN
>
> HTTP intercept is working OK but HTTPS connect requests are being 
> refused. Despite the fact that the WPAD file had been updated with the 
> new IP address and is being served correctly from lighttpd.
>
> I can only conclude therefore that there is some difference between 
> Squid 3.1 and 3.4 which means it no longer likes my configuration.
>
> Below is the Squid.conf file
>
> VLAN1 is the staff LAN with IP addresses 192.168.10 and VLAN2 is the 
> public Wifi LAN with IP addresses 192.168.100
>
> Is there anything here in the squid file which Im doing wrong?
>
> Many thanks
>
> #squid.conf:
>
> #set which port to accept clients on & which interfaces to accept 
> clients on
> http_port 192.168.10.254:3128 intercept
> http_port 192.168.100.254:3128 intercept
>
>
> #set delay pool to do bandwidth throttling on VLAN2
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 250000/500000 125000/500000
>
> #ORd
> acl AllUsers src all
> acl ToSentryBoxVL1 dstdom_regex ^192.168.10.254$
> acl ToWPADServer dstdom_regex ^wpad.commsmuseum.local$
> acl ToPublicWiFiGateway dstdom_regex ^192.168.100.254$
> acl PublicWiFiLAN src 192.168.100.0/24
> acl PrivateLAN src 192.168.10.0/24
> acl DestinedForPrivateLAN dst 192.168.10.0/24
> acl DestinedForPublicWiFiLAN dst 192.168.100.0/24
> acl ProhibitedSitesDomains dstdomain 
> "/var/squidblacklist.org/domains.squid"
> acl IPAddressForHostname dstdom_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$
> acl SafePorts port 80 443
> acl SSLPorts port 443
>
> #block users tunneling
> acl CONNECT method CONNECT
> http_access deny CONNECT !SSLPorts
>
>
> #disable caching
> cache deny all
>
> #add VLAN2 to delay pool 1
> delay_access 1 allow PublicWiFiLAN
>
> #force traffic coming in on VLAN2 to go out on VLAN2
> tcp_outgoing_address 192.168.100.254 PublicWiFiLAN
> tcp_outgoing_address 192.168.10.254 PrivateLAN
>
> #block traffic between VLAN1 & VLAN2
> #iptables does this for everything EXCEPT stuff coming through Squid
> #because iptables sees stuff coming out of squid as originating from 
> the localhost
> #hence iptables FORWARD rules dont apply
> http_access deny PublicWiFiLAN DestinedForPrivateLAN
> http_access deny PrivateLAN DestinedForPublicWiFiLAN
>
> #show splash screen to new users on public wifi to show t&c etc
> #pass session length as arg to perl script cache +ve & -ve responses 
> for 0 secs so
> #perl script is always called. Script is responsible for deciding how 
> long a session is valid for
> #%SRC passes client ip on stdin
> external_acl_type currentsessiontype ttl=60 negative_ttl=0 %SRC 
> /var/publicwifisessions/checksession.pl
> acl currentsession external currentsessiontype
>
> #will stop on first of these which matches so watch order!
> http_access deny ProhibitedSitesDomains
> http_access allow ToPublicWiFiGateway
> http_access allow ToSentryBoxVL1
> http_access allow ToWPADServer
> http_access deny CONNECT PublicWiFiLAN !currentsession
> http_access deny PublicWiFiLAN !currentsession
> http_access deny IPAddressForHostname
> http_access deny !SafePorts
> http_access allow PrivateLAN
> http_access allow PublicWiFiLAN
> http_access deny AllUsers
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Thu Jul  9 21:42:44 2015
From: david at articatech.com (David Touzeau)
Date: Thu, 09 Jul 2015 23:42:44 +0200
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D958D.9050708@treenet.co.nz>
References: <559D6B6C.5050306@articatech.com> <559D7095.3030209@gmail.com>
 <559D7385.3040000@articatech.com> <559D958D.9050708@treenet.co.nz>
Message-ID: <559EEAD4.7080403@articatech.com>

Hi Amos, Yuri


We have created a dedicated port and added a dedicated network card.
change all browsers settings of only one Citrix server to this dedicated 
network card/port.

Issue is the same.

Connection are broken...


Le 08/07/2015 23:26, Amos Jeffries a ?crit :
> On 9/07/2015 7:01 a.m., David Touzeau wrote:
>> Thanks Yuri,
>>
>> Any tips how to increase TCP/IP stack ?
>> Did you means TCP/IP stack on the Citrix Server side or on the squid
>> box  or both ?
> I'm thinking its a problem related to TCP sockets.
> A rough estimate calculatino of:
>   10 users x10 tabs x20 avg domains per page x 2 for happy eyeballs
> makes it somewhere up to 4k sockets in active use at any time. if the
> users are accessing domains with larger numbers ofdomains per page (ie
> Facebook has up to 100) that could be 20k concurrent sockets just from
> the browser.
>   By the time that goes through Squid it becomes 40k, and if you have
> ICAP it becomes "up to 80K" (out of an available 64k sockets).
>
> Then there is all the OS background services that use HTTP through the
> proxy, etc.
>
> Without Citrix the users internal src-IPs vary. Making available a 64k
> sockets per-user. Which is harder to reach, and the browser silently
> limits itself when socekts start to run out.
>
> Without Squid the Citrix connections are going to N different domains
> with varying dst-IP. Which again raises the available port numbers per-user.
>
>
> If the assumptino behind the above is right you should be able to
> alleviate the problem by having Squid listen on multiple ports and/or
> IPs. Then spreading the client connections out across those Squid ports.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Thu Jul  9 21:51:13 2015
From: david at articatech.com (David Touzeau)
Date: Thu, 09 Jul 2015 23:51:13 +0200
Subject: [squid-users] Squid and ufdbGuard,
 display blocked URL on client browser address bar
In-Reply-To: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>
References: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>
Message-ID: <559EECD1.9020001@articatech.com>

Hi ikna

This can be done, but you need to forget the ufdbgclient and create 
yourself a new one that is able to connect to the ufdbguard server in 
order to get ufdbguard results.
In this case, you have with your code to replace the  OK status=302 
url="" sent by ufdbguard server by OK rewrite-url=""

Then the address bar will be not changed.

If you need an example, you will find it after installing this open 
source software :
http://sourceforge.net/projects/artica-squid/files/ISO/proxy-appliances/






Le 09/07/2015 22:19, Ikna Nou a ?crit :
> Dear all,
> We recently migrated from Squid3.4.13/squidGuard to Squid3.5/ufdbGuard
>
> With Squid3.4+squidguard, we were able to display on clients browser a customized error page showing ONLY the original URL request on the address bar.
>
> But, now: what we display on clients browser is:
> http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com
>
>  From command line:
> :~#echo "http://www.blocked_site.com 10.10.0.1/ - - GET" | /usr/local/ufdbguard/bin/ufdbgclient -d
> OK status=302 url="http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com"
>
> squid access log:
> 10.10.2.2 GET http://www.blocked_site.com/ HTTP/1.1 - 287 - "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36" TCP_REDIRECT:HIER_NONE - www.blocked_site.com / - - -
>
>
> Is it possible to achieve the prevoius behavior? (display the blocked URL on clients browser address bar?)
> Thank you
>
>   		 	   		
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jul 10 03:42:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jul 2015 15:42:23 +1200
Subject: [squid-users] Squid 3.5.6 build switches seem to not work.
In-Reply-To: <CAEaSS0bRCCCYpFLSdZuhN89gUEv4Cpxuy+nm-QjGN5LyNdwsoQ@mail.gmail.com>
References: <CAEaSS0bRCCCYpFLSdZuhN89gUEv4Cpxuy+nm-QjGN5LyNdwsoQ@mail.gmail.com>
Message-ID: <559F3F1F.1020703@treenet.co.nz>

On 10/07/2015 6:43 a.m., Tory M Blue wrote:
> I had my build scripts working with the 3.5 betas, but i've switched over
> to the mainline and I'm getting a ton of invalid switches.
> 
> So far I've had to comment out the following, as I'm getting errors such as
> 
> config.status: executing libtool commands
> 
> + --enable-snmp --enable-storeio=aufs,diskd,ufs,rock --enable-wccpv2
> --enable-esi --enable-ssl --enable-ssl-crtd --enable-icmp --with-aio
> --with-default-user=squid --with-filedescriptors=16384 --with-dl
> --with-openssl --with-pthreads --with-included-ltdl --disable-arch-native
> --without-nettle
> 
> /var/tmp/rpm-tmp.Gsh6Y6: line 80: --enable-snmp: command not found

Looks to me like you are missing an '\' character at the end of what is
supposed to be a wrapped line.

> 
> error: Bad exit status from /var/tmp/rpm-tmp.Gsh6Y6 (%build)
> 
>    #--enable-cache-digests \

Lines ike this will cause the above problem. Sine comments are not
naturally wrapped in shell code.

To remove an option from your list properly delete it.

In cases where I need a record of the previous, I use version control,
or make a long comment of the whole original shell code line(s).


> Here is my configure statements (again the comments are all new as I was
> trying to figure out what was going on..)
> 

...
> 
>    --enable-auth \

--enable-auth is not useful anymore. Its auto-enabled by default, and
using any of the --enable-auth-* sub-options is equivalent to forcing it
fully on like this anyway.

...
> 
>    --enable-ssl \
> 

--enable-ssl is no longer existing in 3.5. --with-openssl replaced it.


Amos


From squid3 at treenet.co.nz  Fri Jul 10 03:44:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jul 2015 15:44:49 +1200
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>,
 <55922067.2000401@treenet.co.nz>
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>,
 <559356EE.2060209@treenet.co.nz> <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>
Message-ID: <559F3FB1.9060703@treenet.co.nz>

On 10/07/2015 7:03 a.m., Alex Wu wrote:
> It seems the option http_port cannot be put under each process ID. If using workers, http_port cannot bind to ports specified from http_port.
> 

?? Works for me and many others.

What I dont expect to work is the *same* port line in two specific
workers unless the coordinator process also is assigned to mediate its use.

The coordinator always needs a generic port for its own use. 3128 is
officially registered for that. But that is a separate problem to what
you are describing.

Amos



From squid3 at treenet.co.nz  Fri Jul 10 03:54:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jul 2015 15:54:54 +1200
Subject: [squid-users] Squid and ufdbGuard,
 display blocked URL on client browser address bar
In-Reply-To: <559EECD1.9020001@articatech.com>
References: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>
 <559EECD1.9020001@articatech.com>
Message-ID: <559F420E.2000001@treenet.co.nz>

On 10/07/2015 9:51 a.m., David Touzeau wrote:
> Hi ikna
> 
> This can be done, but you need to forget the ufdbgclient and create
> yourself a new one that is able to connect to the ufdbguard server in
> order to get ufdbguard results.
> In this case, you have with your code to replace the  OK status=302
> url="" sent by ufdbguard server by OK rewrite-url=""
> 
> Then the address bar will be not changed.
> 
> If you need an example, you will find it after installing this open
> source software :
> http://sourceforge.net/projects/artica-squid/files/ISO/proxy-appliances/
> 

Thats not what he is asking for though.

He is asking to change the page *content*. By only altering the URL
request-line on the received message.

The answer is to change what the http://10.1.1.142/sgerror.php script
does. So it displays the url= parameter instead of its own full URL.


IMPORTANT:  doing that was a XSS / open-proxy vulnerability that
squidguard had. Its not a good idea to just dump out query-string data
delivered by the client as body content. It needs to be sanitized
properly first.

Amos

> 
> Le 09/07/2015 22:19, Ikna Nou a ?crit :
>> Dear all,
>> We recently migrated from Squid3.4.13/squidGuard to Squid3.5/ufdbGuard
>>
>> With Squid3.4+squidguard, we were able to display on clients browser a
>> customized error page showing ONLY the original URL request on the
>> address bar.
>>
>> But, now: what we display on clients browser is:
>> http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com
>>
>>  From command line:
>> :~#echo "http://www.blocked_site.com 10.10.0.1/ - - GET" |
>> /usr/local/ufdbguard/bin/ufdbgclient -d
>> OK status=302
>> url="http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com"
>>
>> squid access log:
>> 10.10.2.2 GET http://www.blocked_site.com/ HTTP/1.1 - 287 -
>> "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML,
>> like Gecko) Chrome/43.0.2357.124 Safari/537.36" TCP_REDIRECT:HIER_NONE
>> - www.blocked_site.com / - - -
>>
>>
>> Is it possible to achieve the prevoius behavior? (display the blocked
>> URL on clients browser address bar?)
>> Thank you
>>
>>                          
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From marcus.kool at urlfilterdb.com  Fri Jul 10 10:31:21 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 10 Jul 2015 07:31:21 -0300
Subject: [squid-users] Squid and ufdbGuard,
 display blocked URL on client browser address bar
In-Reply-To: <559F420E.2000001@treenet.co.nz>
References: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>
 <559EECD1.9020001@articatech.com> <559F420E.2000001@treenet.co.nz>
Message-ID: <559F9EF9.5020809@urlfilterdb.com>



On 07/10/2015 12:54 AM, Amos Jeffries wrote:
> On 10/07/2015 9:51 a.m., David Touzeau wrote:
>> Hi ikna
>>
>> This can be done, but you need to forget the ufdbgclient and create
>> yourself a new one that is able to connect to the ufdbguard server in
>> order to get ufdbguard results.
>> In this case, you have with your code to replace the  OK status=302
>> url="" sent by ufdbguard server by OK rewrite-url=""
>>
>> Then the address bar will be not changed.
>>
>> If you need an example, you will find it after installing this open
>> source software :
>> http://sourceforge.net/projects/artica-squid/files/ISO/proxy-appliances/
>>
>
> Thats not what he is asking for though.
>
> He is asking to change the page *content*. By only altering the URL
> request-line on the received message.
>
> The answer is to change what the http://10.1.1.142/sgerror.php script
> does. So it displays the url= parameter instead of its own full URL.

Ikna contacted me yesterday and I have sent the same answer yesterday
directly without notifying the list.

The issue is basically that URL redirectors usually send an HTML 302
redirection code to redirect a blocked URL to an error page.
squidGuard and ufdbGuard use by default the 302 code.

Ikna has, however, a 404 code which behaves differrent and likes to know
how to configure ufdbGuard to send a 404 code.  This is the same as with
squidGuard:
    redirect "404:<some-URL>"

> IMPORTANT:  doing that was a XSS / open-proxy vulnerability that
> squidguard had. Its not a good idea to just dump out query-string data
> delivered by the client as body content. It needs to be sanitized
> properly first.

ufdbGuard sends a sanitised URL so in this case dumping out the value of the
url= parameter is safe.

Marcus

> Amos
>
>>
>> Le 09/07/2015 22:19, Ikna Nou a ?crit :
>>> Dear all,
>>> We recently migrated from Squid3.4.13/squidGuard to Squid3.5/ufdbGuard
>>>
>>> With Squid3.4+squidguard, we were able to display on clients browser a
>>> customized error page showing ONLY the original URL request on the
>>> address bar.
>>>
>>> But, now: what we display on clients browser is:
>>> http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com
>>>
>>>   From command line:
>>> :~#echo "http://www.blocked_site.com 10.10.0.1/ - - GET" |
>>> /usr/local/ufdbguard/bin/ufdbgclient -d
>>> OK status=302
>>> url="http://10.1.1.142/sgerror.php?url=http%3A%2F%2Fwww.blocked_site.com"
>>>
>>> squid access log:
>>> 10.10.2.2 GET http://www.blocked_site.com/ HTTP/1.1 - 287 -
>>> "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML,
>>> like Gecko) Chrome/43.0.2357.124 Safari/537.36" TCP_REDIRECT:HIER_NONE
>>> - www.blocked_site.com / - - -
>>>
>>>
>>> Is it possible to achieve the prevoius behavior? (display the blocked
>>> URL on clients browser address bar?)
>>> Thank you
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From philipp.wehling at megatel.de  Fri Jul 10 12:21:21 2015
From: philipp.wehling at megatel.de (Philipp Wehling)
Date: Fri, 10 Jul 2015 14:21:21 +0200 (CEST)
Subject: [squid-users] accessing google.com
In-Reply-To: <349816787.774.1436530804494.JavaMail.zimbra@megatel.de>
Message-ID: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>

Hello,

from time to time we have trouble accessing google.com.

After many many troubleshooting and trying to understand debug_options 28,x I wanted to go another way:

All I want is to display, which ACL is blocking the access to the website.

I already found the directive deny_info but this doesnt give me the right parameters.


Any ideas would be appreciated.


kind regards,
pwe


From squid3 at treenet.co.nz  Fri Jul 10 12:51:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jul 2015 00:51:05 +1200
Subject: [squid-users] Squid and ufdbGuard,
 display blocked URL on client browser address bar
In-Reply-To: <559F9EF9.5020809@urlfilterdb.com>
References: <COL130-W59B4443642015FAA34195AD0900@phx.gbl>
 <559EECD1.9020001@articatech.com> <559F420E.2000001@treenet.co.nz>
 <559F9EF9.5020809@urlfilterdb.com>
Message-ID: <559FBFB9.8080206@treenet.co.nz>

On 10/07/2015 10:31 p.m., Marcus Kool wrote:
> 
> 
> On 07/10/2015 12:54 AM, Amos Jeffries wrote:
>> On 10/07/2015 9:51 a.m., David Touzeau wrote:
>>> Hi ikna
>>>
>>> This can be done, but you need to forget the ufdbgclient and create
>>> yourself a new one that is able to connect to the ufdbguard server in
>>> order to get ufdbguard results.
>>> In this case, you have with your code to replace the  OK status=302
>>> url="" sent by ufdbguard server by OK rewrite-url=""
>>>
>>> Then the address bar will be not changed.
>>>
>>> If you need an example, you will find it after installing this open
>>> source software :
>>> http://sourceforge.net/projects/artica-squid/files/ISO/proxy-appliances/
>>>
>>
>> Thats not what he is asking for though.
>>
>> He is asking to change the page *content*. By only altering the URL
>> request-line on the received message.
>>
>> The answer is to change what the http://10.1.1.142/sgerror.php script
>> does. So it displays the url= parameter instead of its own full URL.
> 
> Ikna contacted me yesterday and I have sent the same answer yesterday
> directly without notifying the list.
> 
> The issue is basically that URL redirectors usually send an HTML 302
> redirection code to redirect a blocked URL to an error page.
> squidGuard and ufdbGuard use by default the 302 code.
> 
> Ikna has, however, a 404 code which behaves differrent and likes to know
> how to configure ufdbGuard to send a 404 code.  This is the same as with
> squidGuard:
>    redirect "404:<some-URL>"


Squid only accepts RFC defined redirection status codes for redirection
(301, 302, 303, 307, 308). Any other status gets passed through as if
the helper indicated no-change.

In HTTP 4xx messages have an optional body/payload. A Location:<url>
header has no defined meaning and is ignored by browsers.

So the equivalent deny_info uses 4xx:file where the file will be sent as
error message payload.

> 
>> IMPORTANT:  doing that was a XSS / open-proxy vulnerability that
>> squidguard had. Its not a good idea to just dump out query-string data
>> delivered by the client as body content. It needs to be sanitized
>> properly first.
> 
> ufdbGuard sends a sanitised URL so in this case dumping out the value of
> the
> url= parameter is safe.
> 

I'm speaking about the URL that comes back from the client after the
redirect 3xx has happened. The script should not assume that it is the
same as the sanitized URL the helper produced.

Amos



From david at articatech.com  Fri Jul 10 13:34:49 2015
From: david at articatech.com (David Touzeau)
Date: Fri, 10 Jul 2015 15:34:49 +0200
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559D958D.9050708@treenet.co.nz>
References: <559D6B6C.5050306@articatech.com> <559D7095.3030209@gmail.com>
 <559D7385.3040000@articatech.com> <559D958D.9050708@treenet.co.nz>
Message-ID: <559FC9F9.4070203@articatech.com>

Many thanks Amos

With your suggests, we have found that the issue is generated by Palo 
Alto Client for Citrix
https://live.paloaltonetworks.com/docs/DOC-1321
And not from SQUID...



Le 08/07/2015 23:26, Amos Jeffries a ?crit :
> On 9/07/2015 7:01 a.m., David Touzeau wrote:
>> Thanks Yuri,
>>
>> Any tips how to increase TCP/IP stack ?
>> Did you means TCP/IP stack on the Citrix Server side or on the squid
>> box  or both ?
> I'm thinking its a problem related to TCP sockets.
> A rough estimate calculatino of:
>   10 users x10 tabs x20 avg domains per page x 2 for happy eyeballs
> makes it somewhere up to 4k sockets in active use at any time. if the
> users are accessing domains with larger numbers ofdomains per page (ie
> Facebook has up to 100) that could be 20k concurrent sockets just from
> the browser.
>   By the time that goes through Squid it becomes 40k, and if you have
> ICAP it becomes "up to 80K" (out of an available 64k sockets).
>
> Then there is all the OS background services that use HTTP through the
> proxy, etc.
>
> Without Citrix the users internal src-IPs vary. Making available a 64k
> sockets per-user. Which is harder to reach, and the browser silently
> limits itself when socekts start to run out.
>
> Without Squid the Citrix connections are going to N different domains
> with varying dst-IP. Which again raises the available port numbers per-user.
>
>
> If the assumptino behind the above is right you should be able to
> alleviate the problem by having Squid listen on multiple ports and/or
> IPs. Then spreading the client connections out across those Squid ports.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid at borrill.org.uk  Fri Jul 10 13:40:32 2015
From: squid at borrill.org.uk (Stephen Borrill)
Date: Fri, 10 Jul 2015 14:40:32 +0100
Subject: [squid-users] Issue with Citrix sessions and squid
In-Reply-To: <559FC9F9.4070203@articatech.com>
References: <559D6B6C.5050306@articatech.com> <559D7095.3030209@gmail.com>
 <559D7385.3040000@articatech.com> <559D958D.9050708@treenet.co.nz>
 <559FC9F9.4070203@articatech.com>
Message-ID: <559FCB50.4080403@borrill.org.uk>

On 10/07/2015 14:34, David Touzeau wrote:
> Many thanks Amos
>
> With your suggests, we have found that the issue is generated by Palo
> Alto Client for Citrix
> https://live.paloaltonetworks.com/docs/DOC-1321
> And not from SQUID...

Or Citrix...

-- 
Stephen




From sebag at vianetcon.com.ar  Fri Jul 10 17:14:29 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Fri, 10 Jul 2015 14:14:29 -0300
Subject: [squid-users] Two questions about stored objects
In-Reply-To: <559D9739.6030504@treenet.co.nz>
References: <559D5B22.1090201@vianetcon.com.ar>
 <559D9739.6030504@treenet.co.nz>
Message-ID: <559FFD75.2030609@vianetcon.com.ar>

Amos, your answer was very helpful. Purge looks exactly like the 
solution I'm looking for, but I might be not using it correctly. I'm 
getting loads of 404s like these ones:

purge  -p localhost:3080 -c /etc/squid/squid.conf -e 
'.*video\.squidinternal.*' -P 1

/cache/2/00/E9/0000E95B 404   321477 
http://video.squidinternal/140/6584817-6905732/1CSDuPCBf10
/cache/1/00/E9/0000E93E 404  1790312 
http://video.squidinternal/135/89497600-91287551/o01E_GZ1_mM
/cache/2/00/E9/0000E9A3 404   123367 
http://video.squidinternal/251/377034-499838/dl_yFFX8xk0


Is that 404 code ok? Is squid deleting the object? I don't know how to 
interpretate the output


Thanks again
Sebastian


El 08/07/15 a las 18:33, Amos Jeffries escribi?:
> On 9/07/2015 5:17 a.m., Sebastian Goicochea wrote:
>> Hello everyone, I have been making some modifications (size, object max
>> size) in some cache dirs and I have a couple of questions:
>>
>> 1) If I lower de maximum object size for a certain cache_dir and
>> reconfigure (I did a squid -z without squid running), what happens to
>> the files that are no longer in the cache_dir size limits but are
>> already stored?
> No. It only affects new objects being stored to disk.
>
> There is one caveat however. Sometimes objects get "promoted" from disk
> to memory caching. When those cycle back to disk they will not go to the
> original cache_dir. Which may leave you with an un-deleted but nolonger
> indexed file on disk.
>
>> 2) If I change the min and max times in refresh_patterns, what happens
>> to the objects that are already stored? Where they stored with the "old"
>> times or are they going to be re-evaluated the next time they are
>> requested by a user?
> Have no effect except on active traffic. That does include "active" in
> the sense of being saved to disk. But not objects just sitting there
> already.
>
>
> Overall, if you change settings like these the state does slowly
> migrates to the new values as cached content expires. But that is not
> fast, could take minutes or weeks depending on your initial state.
>
> To make immediate administrative changes to on-disk AUFS/UFS/diskd cache
> content use the squid-purge tool which is bundled with recent Squid
> versions.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dan at djph.net  Fri Jul 10 17:57:58 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 10 Jul 2015 17:57:58 +0000 (UTC)
Subject: [squid-users] [SOLVED] Force LDAP groups to de-authenticate?
References: <mn6j3h$12d$1@ger.gmane.org>
Message-ID: <mnp136$4sn$1@ger.gmane.org>

On Fri, 03 Jul 2015 18:08:49 +0000, Dan Purgert wrote:

> I'm setting up a squid proxy with LDAP user/group authentication, and so
> far have been able to sort out the problems I've run into with a little
> help from google and caches of the various squid mailing lists.
> 
> Currently, it's in a mostly working state for nearly everything (i.e.
> user authentication, allowed/blocked based on what group a user belongs
> to, client pc auto-updates, etc.).  However, I can't figure out how to
> force a user to re-authenticate after a set interval of time (say 30
> mintues).
> 
> 
> Essentially, the idea is that the "less-privileged" users (i.e. the
> students) can get to the sites that they need for their day-to-day
> school work, but that their permissions should be able to be elevated
> for a set amount of time in the event the teacher deems it OK.
> 
> Right or wrong, the administration doesn't want to go with one of the
> "big boys" in web filters, so I need to kick the users and force a re-
> auth, as this is for a school environment. It's small (only 10-15
> students at one time), but the students have already figured their way
> around the previous filter that was installed before my time.
> 
> 
> I know closing the browser clears out all the authentication tokens ...
> but hoping there's a way I can do this from the backend so there's no
> need to play those "okay, now close all your browsers" type games if a
> student gets the elevated permissions.
> 
> 
> Leads have pointed me to
> 
>  - auth_param basic credentials_ttl <N> minutes
> 
>  - authenticate_ttl <N> minutes
> 
>  - authenticate_cache_garbage_interval <N> minutes
> 
> Though I don't seem to be able to grasp the concept of getting them to
> do what I want (if it's possible)
> 
> 
> Thanks!


Thanks to everyone who sorted my incorrect understanding with the ttls / 
garbage intervals.  

Have finally gotten a response from the decision makers, and they're OK 
with the explicit time limits for allowing "not school" type websites, so 
that's the route we're going to pursue.



From alex_wu2012 at hotmail.com  Fri Jul 10 21:21:09 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Fri, 10 Jul 2015 14:21:09 -0700
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <559F3FB1.9060703@treenet.co.nz>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>,
 <55922067.2000401@treenet.co.nz>
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>, <559356EE.2060209@treenet.co.nz>
 <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>, <559F3FB1.9060703@treenet.co.nz>
Message-ID: <BAY181-W82C26BB63AE1C3E7D21E16839F0@phx.gbl>

figured it out. It needs all helpers under the process id. I have content redirect helpers which are not under the process ids.

Alex
> Date: Fri, 10 Jul 2015 15:44:49 +1200
> From: squid3 at treenet.co.nz
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] sslbump and caching of generated cert
> 
> On 10/07/2015 7:03 a.m., Alex Wu wrote:
> > It seems the option http_port cannot be put under each process ID. If using workers, http_port cannot bind to ports specified from http_port.
> > 
> 
> ?? Works for me and many others.
> 
> What I dont expect to work is the *same* port line in two specific
> workers unless the coordinator process also is assigned to mediate its use.
> 
> The coordinator always needs a generic port for its own use. 3128 is
> officially registered for that. But that is a separate problem to what
> you are describing.
> 
> Amos
> 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150710/41c32cb8/attachment.htm>

From alex_wu2012 at hotmail.com  Fri Jul 10 23:02:27 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Fri, 10 Jul 2015 16:02:27 -0700
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <BAY181-W82C26BB63AE1C3E7D21E16839F0@phx.gbl>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>, ,
 <55922067.2000401@treenet.co.nz>,
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>, 
 <559356EE.2060209@treenet.co.nz>, <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>,
 <559F3FB1.9060703@treenet.co.nz>, <BAY181-W82C26BB63AE1C3E7D21E16839F0@phx.gbl>
Message-ID: <BAY181-W89DAEBBD78D1E75BBE4242839F0@phx.gbl>

actually, the major problems are:

Once workers > 1, squid looks for /var/run/squid. pidfile for workers=1 is done in squid.conf, but for workers > 1, this will be ignored.

once configuring localstatedir=/opt/deploy/squid/var using ./configure, for workers > 1, the squid is looking for /opt/deploy/squid/var/runsquid. The directory has to be created properly before starting squid.

all helpers must be defined in process_number > 1 respectively, otherwise, there are extra helpers launched by process_number = 0 like crtd.

The document is not clear on this, but we figure it out now.

Alex
From: alex_wu2012 at hotmail.com
To: squid3 at treenet.co.nz; squid-users at lists.squid-cache.org
Date: Fri, 10 Jul 2015 14:21:09 -0700
Subject: Re: [squid-users] sslbump and caching of generated cert




figured it out. It needs all helpers under the process id. I have content redirect helpers which are not under the process ids.

Alex


> Date: Fri, 10 Jul 2015 15:44:49 +1200
> From: squid3 at treenet.co.nz
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] sslbump and caching of generated cert
> 
> On 10/07/2015 7:03 a.m., Alex Wu wrote:
> > It seems the option http_port cannot be put under each process ID. If using workers, http_port cannot bind to ports specified from http_port.
> > 
> 
> ?? Works for me and many others.
> 
> What I dont expect to work is the *same* port line in two specific
> workers unless the coordinator process also is assigned to mediate its use.
> 
> The coordinator always needs a generic port for its own use. 3128 is
> officially registered for that. But that is a separate problem to what
> you are describing.
> 
> Amos
> 
 		 	   		  

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150710/b45d2c19/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul 11 08:38:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jul 2015 20:38:42 +1200
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <BAY181-W89DAEBBD78D1E75BBE4242839F0@phx.gbl>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>, ,
 <55922067.2000401@treenet.co.nz>,
 <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>,
 <559356EE.2060209@treenet.co.nz>,
 <BAY181-W9418A089E3BBD6AE10365E83900@phx.gbl>,
 <559F3FB1.9060703@treenet.co.nz>,
 <BAY181-W82C26BB63AE1C3E7D21E16839F0@phx.gbl>
 <BAY181-W89DAEBBD78D1E75BBE4242839F0@phx.gbl>
Message-ID: <55A0D612.8060900@treenet.co.nz>

On 11/07/2015 11:02 a.m., Alex Wu wrote:
> actually, the major problems are:
> 
> Once workers > 1, squid looks for /var/run/squid. pidfile for workers=1 is done in squid.conf, but for workers > 1, this will be ignored.

PID stands for "Process ID". The pidfile contains the process ID of the
Squid process which is responsible for handling signals form the OS
(kill etc). There should only ever be one PID file per Squid (not
per-worker).


> 
> once configuring localstatedir=/opt/deploy/squid/var using ./configure, for workers > 1, the squid is looking for /opt/deploy/squid/var/runsquid. The directory has to be created properly before starting squid.
> 

The "runsquid" part looks broken.

Probably because localstatedir=/opt/deploy/squid/var is naming a file
"var" instead of a directory "/opt/deploy/squid/var/".


> all helpers must be defined in process_number > 1 respectively, otherwise, there are extra helpers launched by process_number = 0 like crtd.
> 

That is not necessary. The extra helpers will not be used. We are fixing
that as the wrongly started ones are identified. Latest Squid no longer
start crtd helpers unless cert-generation is actually used by the worker
starting them.

Which means "Please ugrade to 3.5.6".


Amos


From david at articatech.com  Sat Jul 11 09:23:45 2015
From: david at articatech.com (David Touzeau)
Date: Sat, 11 Jul 2015 11:23:45 +0200
Subject: [squid-users] TAG_NONE/xxxx
Message-ID: <55A0E0A1.2080002@articatech.com>

Hi all


We using Squid 3.5.6 in transparent mode with SSL


With the following settings:

acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all


We have many entries "TAG_NONE/XXXX" in access.log when accessing to SSL 
websites.

What does it means ?

1436605387.980    400 192.168.1.164 TAG_NONE_ABORTED/000 0 GET 
http://conn.skype.com/ - HIER_NONE/- -
1436606075.433   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.55:443 - HIER_NONE/- -
1436606079.027   9194 192.168.1.159 TAG_NONE/200 0 CONNECT 
199.16.156.72:443 KLFRL0051 HIER_NONE/- -
1436606079.816   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.35:443 - HIER_NONE/- -
1436606082.694    274 192.168.1.159 TAG_NONE/200 0 CONNECT 
77.74.178.24:443 KLFRL0051 HIER_NONE/- -
1436606084.200   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.33:443 - HIER_NONE/- -
1436606088.583   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.41:443 - HIER_NONE/- -
1436606088.987  68883 192.168.1.159 TAG_NONE/200 0 CONNECT 
216.239.38.120:443 KLFRL0051 HIER_NONE/- -
1436606092.967   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.161:443 - HIER_NONE/- -
1436606097.350   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.57:443 - HIER_NONE/- -
1436606101.614  59657 192.168.1.159 TAG_NONE/200 0 CONNECT 
173.194.40.129:443 KLFRL0051 HIER_NONE/- -
1436606101.735   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.221:443 - HIER_NONE/- -
1436606104.965    350 192.168.1.159 TAG_NONE/200 0 CONNECT 
77.74.178.24:443 KLFRL0051 HIER_NONE/- -
1436606106.117   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.163:443 - HIER_NONE/- -
1436606110.501   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.54:443 - HIER_NONE/- -
1436606114.884   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.223:443 - HIER_NONE/- -
1436606119.268   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.47:443 - HIER_NONE/- -
1436606123.651   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.102:443 - HIER_NONE/- -
1436606128.035   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
27.111.185.138:443 - HIER_NONE/- -
1436606132.418   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.219:443 - HIER_NONE/- -
1436606136.802   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.30:443 - HIER_NONE/- -
1436606141.185   4384 192.168.1.182 TAG_NONE/200 0 CONNECT 
38.113.165.83:443 - HIER_NONE/- -
1436606145.568   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
38.113.165.145:443 - HIER_NONE/- -
1436606149.952   4383 192.168.1.182 TAG_NONE/200 0 CONNECT 
81.19.104.120:443 - HIER_NONE/- -
1436606154.336   4384 192.168.1.182 TAG_NONE/200 0 CONNECT 
27.111.185.141:443 - HIER_NONE/- -
1436606158.719   4382 192.168.1.182 TAG_NONE/200 0 CONNECT 
62.128.100.37:443 - HIER_NONE/- -



From squid3 at treenet.co.nz  Sat Jul 11 09:33:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jul 2015 21:33:50 +1200
Subject: [squid-users] TAG_NONE/xxxx
In-Reply-To: <55A0E0A1.2080002@articatech.com>
References: <55A0E0A1.2080002@articatech.com>
Message-ID: <55A0E2FE.9060507@treenet.co.nz>

On 11/07/2015 9:23 p.m., David Touzeau wrote:
> Hi all
> 
> 
> We using Squid 3.5.6 in transparent mode with SSL
> 
> 
> With the following settings:
> 
> acl ssl_step1 at_step SslBump1
> acl ssl_step2 at_step SslBump2
> acl ssl_step3 at_step SslBump3
> ssl_bump peek ssl_step1
> ssl_bump splice all
> 
> 
> We have many entries "TAG_NONE/XXXX" in access.log when accessing to SSL
> websites.
> 
> What does it means ?

Spliced connection. No HTTP handling occured.

The TLS logics do not yet have proper processing tags defined yet. That
is still on the todo list.

Amos



From david at articatech.com  Sat Jul 11 10:13:45 2015
From: david at articatech.com (David Touzeau)
Date: Sat, 11 Jul 2015 12:13:45 +0200
Subject: [squid-users] TAG_NONE/xxxx
In-Reply-To: <55A0E2FE.9060507@treenet.co.nz>
References: <55A0E0A1.2080002@articatech.com> <55A0E2FE.9060507@treenet.co.nz>
Message-ID: <55A0EC59.2040903@articatech.com>

To understand what you says:

Means that squid try to understand the TLS protocol in order to retrieve 
certificate information but some TAGS in certificate are not properly 
understood.
So Squid still accept/forward the connection without decoding TLS ?

Le 11/07/2015 11:33, Amos Jeffries a ?crit :
> On 11/07/2015 9:23 p.m., David Touzeau wrote:
>> Hi all
>>
>>
>> We using Squid 3.5.6 in transparent mode with SSL
>>
>>
>> With the following settings:
>>
>> acl ssl_step1 at_step SslBump1
>> acl ssl_step2 at_step SslBump2
>> acl ssl_step3 at_step SslBump3
>> ssl_bump peek ssl_step1
>> ssl_bump splice all
>>
>>
>> We have many entries "TAG_NONE/XXXX" in access.log when accessing to SSL
>> websites.
>>
>> What does it means ?
> Spliced connection. No HTTP handling occured.
>
> The TLS logics do not yet have proper processing tags defined yet. That
> is still on the todo list.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Jul 11 20:16:58 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 12 Jul 2015 02:16:58 +0600
Subject: [squid-users] TAG_NONE/xxxx
In-Reply-To: <55A0EC59.2040903@articatech.com>
References: <55A0E0A1.2080002@articatech.com> <55A0E2FE.9060507@treenet.co.nz>
 <55A0EC59.2040903@articatech.com>
Message-ID: <55A179BA.9020107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Yep, man.

Sad, but true.

11.07.15 16:13, David Touzeau ?????:
> To understand what you says:
>
> Means that squid try to understand the TLS protocol in order to
retrieve certificate information but some TAGS in certificate are not
properly understood.
> So Squid still accept/forward the connection without decoding TLS ?
>
> Le 11/07/2015 11:33, Amos Jeffries a ?crit :
>> On 11/07/2015 9:23 p.m., David Touzeau wrote:
>>> Hi all
>>>
>>>
>>> We using Squid 3.5.6 in transparent mode with SSL
>>>
>>>
>>> With the following settings:
>>>
>>> acl ssl_step1 at_step SslBump1
>>> acl ssl_step2 at_step SslBump2
>>> acl ssl_step3 at_step SslBump3
>>> ssl_bump peek ssl_step1
>>> ssl_bump splice all
>>>
>>>
>>> We have many entries "TAG_NONE/XXXX" in access.log when accessing to SSL
>>> websites.
>>>
>>> What does it means ?
>> Spliced connection. No HTTP handling occured.
>>
>> The TLS logics do not yet have proper processing tags defined yet. That
>> is still on the todo list.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVoXm5AAoJENNXIZxhPexG6TUH/2H02D3FXynZ1Y2lngkEhDD3
ov8I4uujWwAEW9cmaoNqWCcewO/8gOzxa46dTntxnFi8Zun6/C88bWSVedBmcGN5
4hjjdQjnIO7D1aT+ehp0ozW2TrXJcy2IYx/9S5tZuWLByz77YTjyau+e+4Eym3/H
rjEzX8yQjBJiWhl4ihMn9Xl3LLBBRsidDmaTNPpNAWhUBJcR5SYQ54LitNdWJjTe
I3eIyzU7UlInjhCD4VFhyuuT2lwXSsD8HrLPSaLalenZNeeFMofw6h3NHibyYENa
zyNgPMp8pHXcok2+ipY5I0wGYoXTpbncRry45G4ae9wQJhuPZsULI2pB2ToHWUQ=
=EQ9v
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150712/c4acb5a7/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 12 00:43:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Jul 2015 12:43:25 +1200
Subject: [squid-users] TAG_NONE/xxxx
In-Reply-To: <55A179BA.9020107@gmail.com>
References: <55A0E0A1.2080002@articatech.com> <55A0E2FE.9060507@treenet.co.nz>
 <55A0EC59.2040903@articatech.com> <55A179BA.9020107@gmail.com>
Message-ID: <55A1B82D.5080805@treenet.co.nz>

On 12/07/2015 8:16 a.m., Yuri Voinov wrote:
> 
> Yep, man.
> 
> Sad, but true.

No man.

[sorry, sent the followup in private by mistake. Repeating now for
everyone ... with a bit more info]

The TAG_* is a representation of what Squid has done in processing
the transaction.

We dont yet have tags/codes labelling the different SSL-Bump logic
paths. So the fake-CONNECT request processing shows up as "NONE" at the
moment when splicing or bumping was done. TUNNEL when SSL-bump action
"none" is done for intercepted traffic. And "terminate" action is not
logged at all for some reason.

Details on the existing tag meanings can be found here:
<http://wiki.squid-cache.org/SquidFaq/SquidLogs#Squid_result_codes>


Since David configurd splice as has bumping action the TAG_NONE/200
means the intercepted transation (fake-CONNET) was spliced. No further
details will be logged, because they are inside the encryption flowing
through the splice.

Amos



> 
> 11.07.15 16:13, David Touzeau ?????:
>> To understand what you says:
> 
>> Means that squid try to understand the TLS protocol in order to
> retrieve certificate information but some TAGS in certificate are not
> properly understood.
>> So Squid still accept/forward the connection without decoding TLS ?
> 
>> Le 11/07/2015 11:33, Amos Jeffries a ?crit :
>>> On 11/07/2015 9:23 p.m., David Touzeau wrote:
>>>> Hi all
>>>>
>>>>
>>>> We using Squid 3.5.6 in transparent mode with SSL
>>>>
>>>>
>>>> With the following settings:
>>>>
>>>> acl ssl_step1 at_step SslBump1
>>>> acl ssl_step2 at_step SslBump2
>>>> acl ssl_step3 at_step SslBump3
>>>> ssl_bump peek ssl_step1
>>>> ssl_bump splice all
>>>>
>>>>
>>>> We have many entries "TAG_NONE/XXXX" in access.log when accessing to SSL
>>>> websites.
>>>>
>>>> What does it means ?
>>> Spliced connection. No HTTP handling occured.
>>>
>>> The TLS logics do not yet have proper processing tags defined yet. That
>>> is still on the todo list.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Jul  9 03:39:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 15:39:29 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2015:2 Improper
	Protection of	Alternate Path
Message-ID: <559DECF1.6050403@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2015:2
__________________________________________________________________

Advisory ID:            SQUID-2015:2
Date:                   July 06, 2015
Summary:                Improper Protection of Alternate Path
Affected versions:      Squid 0.x -> 3.5.5
Fixed in version:       Squid 3.5.6
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2015_2.txt
__________________________________________________________________

Problem Description:

 Squid configured with cache_peer and operating on explicit proxy
 traffic does not correctly handle CONNECT method peer responses.

__________________________________________________________________

Severity:

 The bug is important because it allows remote clients to bypass
 security in an explicit gateway proxy.

 However, the bug is exploitable only if you have configured
 cache_peer to receive CONNECT requests.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 3.5.6.

 In addition, patches addressing this problem for stable releases
 can be found in our patch archives:

Squid 3.4:
http://www.squid-cache.org/Versions/v3/3.4/changesets/squid-3.4-13225.patch

Squid 3.5:
http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13856.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid versions with cache_peer omitted from squid.conf are
 not vulnerable to the problem.

 All Squid versions with squid.conf containing
 "nonhierarchical_direct on" are not vulnerable to the problem.

 All Squid-3.1 and later with nonhierarchical_direct omitted from
 squid.conf are not vulnerable to the problem.

 All other unpatched Squid configured to use a cache_peer without
 the "originserver" option are vulnerable to the problem.

__________________________________________________________________

Workaround:

 For Squid-3.0 and older ensure squid.conf contains
 "nonhierarchical_direct on".

 For Squid-3.1 and newer remove nonhierarchical_direct from
 squid.conf.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 http://www.squid-cache.org/Support/mailing-lists.html.

 For reporting of non-security bugs in the latest release
 the squid bugzilla database should be used
 http://bugs.squid-cache.org/.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The vulnerability was reported and fixed by Alex Rousskov, The
 Measurement Factory.

__________________________________________________________________

Revision history:

 2015-06-16 16:54 GMT Initial Report and Patches Released
 2015-05-03 15:37 GMT Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Jul  9 03:40:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 09 Jul 2015 15:40:56 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.6 is available
Message-ID: <559DED48.1060704@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.6 release!


This release is a security and bug fix release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:


* SQUID-2015:2 Improper Protection of Alternate Path

  http://www.squid-cache.org/Advisories/SQUID-2015_2.txt

Squid when passing a CONNECT request to a cache_peer blindly passes the
response back to the client. This can result in further requests on the
connection bypassing all access controls or routing configuration in the
gateway proxy that would otherwise have been applied.

The default settings of Squid protect most sites against this. However
certain known network topologies require the configuration which is
vulnerable.


* Regression Bug 4193: Memory leak on FTP listings

Recent releases have been leaking a small amount of memory on every
successful FTP directory listing. That has now been resolved.


* Bug 3329: The server side pinned connection is not closed properly

Squid internal state for remotely closed server connections was not
updated correctly. Which may result in pinned client connections hanging
until a timout, then abort being applied unexpectedly to an unrelated
connection.


* Bug 3875: bad mimeLoadIconFile error handling

This bug represented a small collection of errors possible when loading
icon files during startup. They may have resulted in various secondary
errors later as the icons were used. Squid will now log such failures on
startup and respond to requests with 204 (No Content) when the icon is
requested.


* Bug 4183: segfault when freeing https_port clientca on reconfigure or
exit.

This bug would appear on reconfigure when squid.conf contained the
http(s)_port clientca= parameter.


* Bug 3483: assertion failed store.cc:1866: 'isEmpty()'

This bug appeared randomly after Squid crashed, was shutdown with short
timeouts, or encountered various cache access issues (including bug 3875
above). While some of these causes still exist, this release treats the
resulting error properly as a SWAPFAIL and continues operation instead
of aborting with assertion.


* TLS: Disable client-initiated renegotiation

Current OpenSSL libraries protect against renegotiation already. Squid
does not renegotiate which avoids the specific CVE-2009-3555 issue. Use
of only the latest TLS protocol (as per Best Current Practice) also
protects against these effects.

However, Client-initiated TLS/SSL renegotiation could still result in
Denial of Service vulnerability for some libraries and configurations.
This further hardens against the SSL protocol flaw by rejecting client
attempts to renegotiate security protocol after initial TLS/SSL client
handshake has completed.

This change only has effect when Squid is built against libraries which
allow vulnerable forms of renegotiation. Or when Squid is configured to
allow SSLv3 downgrade renegotiation. Note that SSLv3 downgrade from TLS
is still permitted, but only before initial client handshake has completed.


* Fix CONNECT failover to IPv4 after trying broken IPv6 servers

This bug affects Squid attempting to open a TCP connection to a server
over broken IP connectivity. When the initial attempt times out Squid
would respond to the client with an error instead of attempting further IPs.

Note that only broken IP connectivity is required to trigger this bug.
That break may exist connecting to an IPv4 server or cache_peer. It is
currently more common in IPv6 connections due to explicit sysadmin
breakage "disabling" IPv6.


* Use relative-URL in errorpage.css for SN.png

The errorpage.css default file has previously been required due to
technical problems to use an absolute-URL to reference the default error
message Squid icon. With the current generation of browsers CSS3
behaviour and bug 4132 fixed in the prevous 3.5 release this requirement
is lifted.

As of the current release Squid default error page icon uses a
relative-URL relating to the stored icon file published and installed
with the Squid generating the error page (or any intervening Squid proxy
closer to the client). Resolving privacy information leak worries that
have been presented by some sysadmin.



 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Jul 12 00:59:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Jul 2015 12:59:23 +1200
Subject: [squid-users] IMPORTANT: mailing list changes
Message-ID: <55A1BBEB.7070800@treenet.co.nz>

As most of you will know by now with the mailing list server update last
year we allowed attachments to be mailed to the list. There are a large
number of participants receiving each message so the size and file types
accepted are strictly limited.

Due to the number of people needing to submit config files and debug
traces attachments in the 50-90KB range I am reluctantly increasing the
allowed message size to 100KB effective immediately. Please ensure that
you do not exceed this size, moderation will be more eager to reject
than before.

Amos Jeffries
Squid Software Foundation


From nathan at getoffmalawn.com  Sun Jul 12 01:02:35 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Sun, 12 Jul 2015 11:02:35 +1000
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <558A7EBD.3090309@gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
 <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
 <55705009.9080200@treenet.co.nz>
 <CAA8ViV8==pthmX4X4r3RZ9JTjSCyaTv1ZJUp0adr4cJnaR6kDA@mail.gmail.com>
 <CAA8ViV_oHsmmtCO_5hogrsHtZJQSTZ4w6+e0LwCHz2LyaPeqhQ@mail.gmail.com>
 <558A7EBD.3090309@gmail.com>
Message-ID: <CAGUJm7b3YsSefZBPkEVRPOKXT3U4Ht0fpE4GLM5fdvKmFioEsw@mail.gmail.com>

I'm using 3.5 with transparent server first bumping in ~100 deployments so
far, it works just fine, excluding with SNI and everything.
On 12/07/2015 10:58 am, "Yuri Voinov" <yvoinov at gmail.com> wrote:

>  Man,
>
> 3.5.x don't work with server-first. It must be for backward compatibility
> - but don't be.
>
> Also, AFAIK, 3.5.x series don't work with transparent NAT interception in
> bump mode. Fake certs are generated, but with IP against hostnames (in all
> my test installations).
>
> So, if you strictly need working bump with transparent interception,
> rollback to 3.4.
>
> WBR, Yuri.
>
> 24.06.15 12:04, Reet Vyas ?????:
>
>  Hi
>   Below is my squid file , I have configured squid 3.5.3 with ssl, but I
> cant filter https traffic and also in access log I cant see https in access
> logs.
>
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 116.72.152.37 192.168.0.0/24 # Sesuaikan dengan ip
> client/local
>
> acl SSL_ports port 443
> acl Safe_ports port 80  # http
> acl Safe_ports port 21  # ftp
> acl Safe_ports port 443  # https
> acl Safe_ports port 70  # gopher
> acl Safe_ports port 210  # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280  # http-mgmt
> acl Safe_ports port 488  # gss-http
> acl Safe_ports port 591  # filemaker
> acl Safe_ports port 777  # multiling http
> # storeid *test*
> acl urlrewrite dstdomain .fbcdn.net .akamaihd.net
> acl speedtest url_regex -i speedtest\/.*\.(jpg|txt)\?.*
> acl reverbnation url_regex -i reverbnation.*audio_player.*ec_stream_song.*$
> acl utmgif url_regex -i utm.gif.*
> acl playstoreandroid url_regex -i
> c.android.clients.google.com.market.GetBinary.GetBinary.*
> acl idyoutube url_regex -i
> youtube.*(ptracking|stream_204|player_204).*(v\=|docid\=|video_id\=).*$
> acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
> acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
> acl CONNECT method CONNECT
> acl getmethod method GET
> acl loop_302 http_status 302
> acl step1 at_step SslBump1
> acl youtube dstdomain .youtube.com
> acl blocksites dstdomain "/etc/squid/restricted-sites.squid"
> # TAG: QUERY
> #
> -----------------------------------------------------------------------------
> acl QUERY urlpath_regex -i
> (hackshield|blank.html|infinity.js|hshield.da|renew_session_token.php|recaptcha.js|dat.asp|notice.swf|patchlist.txt|hackshield|captcha|reset.css|update.ver|notice.html|updates.txt|gamenotice|images.kom|patchinfo.xml|noupdate.ui|\.Xtp|\.htc|\.txt)
> acl QUERY urlpath_regex -i
> (patch.conf|uiimageset.xml.iop|gashaponwnd.xml.iop|loading.swf|download.swf|version.list|version.ini|launch.jnlp|server_patch.cfg.iop|core.swf|Loading.swf|resouececheck.sq|mainloading.swf|config.xml|gemmaze.swf|xml.png|size.xml|resourcesbar.swf|version.xml|version.list|delete.ini)
> acl QUERY urlpath_regex -i \.(jsp|asp|aspx|cfg|iop|zip|php|xml|html)(\?|$)
> cache deny QUERY
> cache deny youtube
>
> #
> acl dontstore url_regex ^http:\/\/((
> [\d\w-]*(\.[^\.\-]*?\..*?))(\/\mosalsal\/[\d]{4}\/.*\/)(.*\.flv))\?start.*
> acl dontstore url_regex redbot\.org \.php
> acl dontstore url_regex -i ^http:\/\/.*gemscool\.com\/.*
> acl dontstore url_regex \.(aspx|php)\?
> acl dontstore url_regex goldprice\.org\/NewCharts\/gold\/images\/.*\.png
> acl dontstore url_regex google\.co(m|\.[a-z]{2})\/complete\/search\?
> acl dontstore url_regex
> redirector\.([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id|get_video_info\?|ptracking\?|player_204\?|stream_204\?).*
>
> acl store_yt_id url_regex -i
> youtube.*(ptracking|stream_204|playback|player_204|watchtime|set_awesome|s\?|ads).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*$
> acl store_id_list_yt url_regex -i (youtube|googlevideo).*videoplayback.*$
> acl store_id_list_yt url_regex
> ^https?\:\/\/([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id).*
>
> acl store-id_list urlpath_regex -i dl\.sourceforge\.net
> acl store-id_list urlpath_regex -i \.ytimg\.com
> acl store-id_list urlpath_regex -i \.(akamaihd|fbcdn)\.net
> acl store_id_list urlpath_regex -i
> [a-zA-Z]{2}[0-9]*\.4shared\.com\/download\/
>
> acl store_id_list_url url_regex ^http:\/\/
> [0-9]\.bp\.blogspot\.com.*\.(jpeg|jpg|png|gif|ico)
> acl store_id_list_url url_regex
> ^http[s]?:\/\/.*\.twimg\.com\/(.*)\.(gif|jpeg|jpg|png|js|css)
> acl store_id_list_url url_regex
> ^http[s]?:\/\/(media|static)\.licdn\.com\/.*\.(png|jpg|gif|woff)
> acl store_id_list_url url_regex ^https:\/\/fb(static|cdn)\-.*\-
> a.akamaihd.net\/(.*)\.(gif|jpeg|jpg|png|js|css|mp4)
> acl store_id_list_url url_regex ^http:\/\/.*\.ak\.fbcdn\.net\/.*\.(gif
> |jpg|png|js|mp4)
>
> # pass requests
> url_rewrite_program /etc/squid/phpredir.php
> url_rewrite_access allow youtube
>
> request_header_access Range deny store_id_list_yt
> range_offset_limit 10 KB store_id_list_yt
>
>
>
> ###############################################################################
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
>
> ###############################################################################
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny blocksites
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
>
>
> ###############################################################################
> # squid ssl_bump option
>
> ###############################################################################
> always_direct allow all
> ssl_bump server-first all
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> #ssl_bump peek step1
> #ssl_bump bump all
>
> ###############################################################################
> # Squid normally listens to port 3128
>
> ###############################################################################
> https_port 3130 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt
> key=/etc/squid/ssl_certs/squid.key
> http_port 3129 intercept
> http_port 3128
>
> # TAG: Store-id Program
> #
> -----------------------------------------------------------------------------
> store_id_program /usr/bin/perl /etc/squid/store-id.pl
> store_id_children 100 startup=0 idle=1 concurrency=1000
>
> # TAG: Store-id Access
> #
> -----------------------------------------------------------------------------
> store_id_access allow urlrewrite
> store_id_access allow speedtest
> store_id_access allow reverbnation
> store_id_access allow utmgif
> store_id_access allow playstoreandroid
> store_id_access allow idyoutube
> store_id_access allow videoyoutube
> store_id_access deny dontstore
> store_id_access deny !getmethod
> store_id_access allow store_id_list_yt
> store_id_access allow store_yt_id
> store_id_access allow store-id_list
> store_id_access deny all
> store_id_bypass on
>
> # TAG: Youtube 302
> #
> -----------------------------------------------------------------------------
> store_miss deny store_id_list_yt loop_302
> send_hit deny store_id_list_yt loop_302
>
>
> ###############################################################################
> ## MEMORY CACHE OPTIONS
>
> ###############################################################################
> client_dst_passthru on
> cache_mem 1024 MB
> maximum_object_size_in_memory 1024 KB
> memory_cache_shared off
> memory_cache_mode disk
> memory_replacement_policy heap GDSF
>
>
> ###############################################################################
> ## DISK CACHE OPTIONS
>
> ###############################################################################
> cache_replacement_policy heap LFUDA
> minimum_object_size 1 bytes
> maximum_object_size 10 GB
>
>
> ###############################################################################
> # Uncomment and adjust the following to add a disk cache directory.
>
> ###############################################################################
> cache_dir aufs /usr/local/cache_proxy 25000 16 256 # sesuaikan dengan
> drive penyimpanan cache
> store_dir_select_algorithm round-robin
> cache_swap_low 90
> cache_swap_high 95
>
>
> ###############################################################################
> # Leave coredumps in the first cache dir
>
> ###############################################################################
> coredump_dir /var/spool/squid
>
>
> ###############################################################################
> ## LOGFILE OPTIONS
>
> ###############################################################################
> #access_log daemon:/tmp/access.log !log
> #logfile_daemon /usr/lib/squid/log_file_daemon
> cache_store_log none
> logfile_rotate 1
> mime_table /etc/squid/mime.conf
> pid_filename /var/run/squid.pid
> strip_query_terms off
> buffered_logs off
>
>
> ###############################################################################
> ## OPTIONS FOR TROUBLESHOOTING
>
> ###############################################################################
> #cache_log /tmp/cache.log
> cache_log /dev/null
> #debug_options ALL,1 22,3
> coredump_dir /var/spool/squid
>
>
> ###############################################################################
> ## OPTIONS FOR TUNING THE CACHE
>
> ###############################################################################
> max_stale 1 years
> vary_ignore_expire on
> shutdown_lifetime 10 seconds
>
>
> ###############################################################################
> # Add any of your own refresh_pattern entries above these.
>
> ###############################################################################
> refresh_pattern ^ftp:  1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> # Youtube Video
> refresh_pattern -i
> (get_video\?|videoplayback\?|videodownload\?|\.mp4|\.webm|\.flv|((audio|video)\/(webm|mp4)))
> 241920 100% 241920 override-expire ignore-reload ignore-private
> ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
> store-stale
> refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*
> 10080 99% 43200 override-lastmod override-expire ignore-reload
> reload-into-ims ignore-private reload-into-ims ignore-auth store-stale
> refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*$
> 241920 100% 241920 override-expire ignore-reload ignore-private
> ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
> store-stale
>
> refresh_pattern (akamaihd|fbcdn)\.net 14400 99% 518400  ignore-no-store
> ignore-private ignore-reload ignore-must-revalidate store-stale
> refresh_pattern -i squid\.internal 14400 99% 518400  ignore-no-store
> ignore-private ignore-reload ignore-must-revalidate store-stale
> refresh_pattern \.(jpg|png|gif|css|ico)($|\?) 14400 99% 518400
> ignore-no-store ignore-private reload-into-ims ignore-must-revalidate
> store-stale
> refresh_pattern . 0 99% 518400  ignore-no-store ignore-private
> reload-into-ims store-stale
> # Image Youtube
> refresh_pattern -i (yimg|twimg)\.com\.*         1440 100% 129600
> override-expire ignore-reload reload-into-ims
> refresh_pattern -i (ytimg|ggpht)\.com\.*        1440 80% 129600
> override-expire override-lastmod ignore-auth ignore-reload reload-into-ims
>
> #images facebook
> refresh_pattern -i
> fbcdn.*net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
> 241920 99% 241920 ignore-no-store ignore-private override-expire
> override-lastmod reload-into-ims ignore-auth
> refresh_pattern -i pixel\.facebook\.com.*\.(jpg|png|gif|ico|css|js) 241920
> 80% 241920 override-expire ignore-reload reload-into-ims ignore-auth
> refresh_pattern -i \.akamaihd\.net.*\.(jpg|png|gif|ico|css|js) 241920 80%
> 241920 override-expire ignore-reload reload-into-ims ignore-auth
> refresh_pattern -i ((facebook.com)|(85.131.151.39))\.(jpg|png|gif) 241920
> 99% 241920 ignore-reload override-expire ignore-no-store store-stale
> refresh_pattern -i
> fbcdn\.net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
> 241920 99% 241920 ignore-no-store ignore-private override-expire
> override-lastmod reload-into-ims ignore-auth
> refresh_pattern static\.(xx|ak)\.fbcdn\.net*\.(jpg|gif|png) 241920 99%
> 241920 ignore-reload override-expire ignore-no-store
> refresh_pattern ^https?\:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png)
> 241920 99% 241920 ignore-reload override-expire ignore-no-store
>
> # Video Facebook
> refresh_pattern -i
> \.video.ak.fbcdn.net.*\.(mp4|flv|mp3|amf)                    10080 80%
> 43200 override-expire ignore-reload reload-into-ims ignore-private
> ignore-no-store ignore-must-revalidate
> refresh_pattern (audio|video)\/(webm|mp4) 129600 99% 129600 ignore-reload
> override-expire override-lastmod ignore-must-revalidate  ignore-private
> ignore-no-store ignore-auth store-stale
> refresh_pattern -i ^http://.*squid\.internal.*  241920 100% 241920
> override-lastmod override-expire ignore-reload ignore-must-revalidate
> ignore-private ignore-no-store ignore-auth store-stale
>
> # All File
> refresh_pattern -i
> \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt) 10080 80%
> 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern -i
> \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar|iop|nzp|pak|mar|msp)
> 10080 80% 10080 override-expire override-lastmod reload-into-ims
> ignore-reload
> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll) 10080 80% 10080
> override-expire override-lastmod reload-into-ims
> refresh_pattern -i
> \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob|webm)
> 10080 80% 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern -i
> \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)
> 10080 80% 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern .  0 20% 4320
>
>
> ###############################################################################
> ## ADMINISTRATIVE PARAMETERS
>
> ###############################################################################
> cache_mgr reetika at foxymoron.org
> cache_effective_user proxy
> cache_effective_group proxy
> visible_hostname foxysquid.foxymoron.tv
> unique_hostname foxysquid.foxymoron.tv
>
>
> ###############################################################################
> ## PERSISTENT CONNECTION HANDLING
>
> ###############################################################################
> detect_broken_pconn on
> client_persistent_connections off
> server_persistent_connections on
>
>
> ###############################################################################
> ## ERROR PAGE OPTIONS
>
> ###############################################################################
> error_directory /usr/share/squid/errors/en
> error_log_languages off
>
>
> ###############################################################################
> ## DNS OPTIONS
>
> ###############################################################################
> check_hostnames off
> hosts_file /etc/hosts
> connect_retries 2
> ipcache_low 90
> ipcache_high 95
> ipcache_size 84024                        # 2x Besar RAM
> fqdncache_size 64024                        # real RAM Hardware
> pipeline_prefetch 100
>
>
> ###############################################################################
> ## MISCELLANEOUS
>
> ###############################################################################
> memory_pools off
> reload_into_ims on
> uri_whitespace strip
> max_filedescriptors 65536
>
>  IPtable rules :
>
> ................................................
>
>  My IPtables Rules
>
> Chain PREROUTING (policy ACCEPT 27405 packets, 1872K bytes)
>  pkts bytes target     prot opt in     out     source
> destination
> 76873 4457K DNAT       tcp  --  eth1   *       0.0.0.0/0
> 0.0.0.0/0            tcp dpt:80 to:192.168.0.200:3129
>    26  1184 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
> 0.0.0.0/0            tcp dpt:80 redir ports 3129
>     0     0 DNAT       tcp  --  eth0   *       0.0.0.0/0
> 0.0.0.0/0            tcp dpt:443 to:192.168.0.200:3130
>
> Chain INPUT (policy ACCEPT 9321 packets, 543K bytes)
>  pkts bytes target     prot opt in     out     source
> destination
>
> Chain OUTPUT (policy ACCEPT 1426 packets, 85560 bytes)
>  pkts bytes target     prot opt in     out     source
> destination
>
> Chain POSTROUTING (policy ACCEPT 1426 packets, 85560 bytes)
>  pkts bytes target     prot opt in     out     source
> destination
> 81432   14M MASQUERADE  all  --  *      eth0    192.168.0.0/24
> 0.0.0.0/0
>
> On Fri, Jun 5, 2015 at 1:43 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:
>
>>  Hi
>>
>>  Thanks for reply. I am trying to cache youtube using this wiki
>> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube but I
>> cant cache youtube.
>>
>>  I want to cache facebook and youtube. SSl certificate installation that
>> I have to do . Please suggest some links.
>>
>> On Thu, Jun 4, 2015 at 6:48 PM, Amos Jeffries <squid3 at treenet.co.nz>
>> wrote:
>>
>>> On 5/06/2015 12:55 a.m., Reet Vyas wrote:
>>> > Thank you everyone for helping me to setup squid , Now its working but
>>> in
>>> > access.logs  I only see tcp_miss if m using same website. I mean squid
>>> is
>>> > not caching
>>>
>>> You will get MISS a fair bit more with intercepted traffic than with
>>> normal proxied traffic. Particularly on certain major CDN who play
>>> tricks with DNS.
>>>
>>> The reasons and some workarounds to need to be doing are explained in
>>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150712/7ff761e0/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 12 01:05:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Jul 2015 13:05:30 +1200
Subject: [squid-users] accessing google.com
In-Reply-To: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>
References: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>
Message-ID: <55A1BD5A.5030400@treenet.co.nz>

On 11/07/2015 12:21 a.m., Philipp Wehling wrote:
> Hello,
> 
> from time to time we have trouble accessing google.com.
> 
> After many many troubleshooting and trying to understand debug_options 28,x I wanted to go another way:
> 
> All I want is to display, which ACL is blocking the access to the website.
> 
> I already found the directive deny_info but this doesnt give me the right parameters.
> 
> 
> Any ideas would be appreciated.

This is where you really do need to understand the 28,3 debug output.
There is no one ACL that is causing it. A whole set of ACLs and
particular sequence order of test leads up to any denial.

That is of course following your assumption that Squid is actively
rejecting the traffic with ACLs (you provide no evidence of that).

Chances are rather high these days that it has nothing to do with Squid
since Google prefer to use HTTPS, QUIC, SPDY, or HTTP/2 protocols. All
of which normally bypass Squid processing entirely.

Amos



From squid3 at treenet.co.nz  Sun Jul 12 02:49:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Jul 2015 14:49:04 +1200
Subject: [squid-users] Difference between Squid 3.1 & 3.4 regarding
 HTTPS CONNECT handling
In-Reply-To: <559ED9E0.4030004@perpetualmotion.co.uk>
References: <559E7185.9040709@perpetualmotion.co.uk>
 <559ED9E0.4030004@perpetualmotion.co.uk>
Message-ID: <55A1D5A0.4010801@treenet.co.nz>

On 10/07/2015 8:30 a.m., Andrew Wood wrote:
> OK heres the difference
> 
> http_port 192.168.10.254:3128 intercept
> http_port 192.168.10.254:3129

You should have port 3128 for the non-interept traffic.

> 
> I had to setup squid on a second port not in intercept mode and set the
> WPAD file to send HTTPS requests there.
> 
> Why will the intercept port accept HTTPS CONNECT requests in 3.1 but not
> 3.4?

3.2+ enforce the requirement that NAT is performed on the same machine
as Squid runs.

Explicit proxy traffic (from WPAD), naturally does not have NAT records
so will fail that requirement check. Thus separate ports are needed for
each of the different HTTP traffic types.

Please also check your intercept rules match
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect> (or
the equivalent DNAT example). Specifically the *mangle* table rule
preventing external connections directly to the intercept port is very
important to prevent certain attacks that happen nowdays.


PS. Some hints on optimizing your config are inline below...

> On 09/07/15 14:05, Andrew Wood wrote:
>> #squid.conf:
>>
>> #set which port to accept clients on & which interfaces to accept
>> clients on
>> http_port 192.168.10.254:3128 intercept
>> http_port 192.168.100.254:3128 intercept
>>

No need for two ports. Its up to you, but I would do this:
  http_port 3129 intercept
  http_port 3128

Also, note the number of the intercept port should be random. It must
only ever receive traffic for the NAT sub-system so everything you can
do to obfusctae and prevent external connections getting directly to it
is good.


>>
>> #set delay pool to do bandwidth throttling on VLAN2
>> delay_pools 1
>> delay_class 1 2
>> delay_parameters 1 250000/500000 125000/500000
>>
>> #ORd
>> acl AllUsers src all

Just use the built-in "all" and remove this definition entirely.

The only reason to custom-define an 'all' ACL is to attach a custom
deny_info page to it. You are not doing that.


>> acl ToSentryBoxVL1 dstdom_regex ^192.168.10.254$
>> acl ToWPADServer dstdom_regex ^wpad.commsmuseum.local$
>> acl ToPublicWiFiGateway dstdom_regex ^192.168.100.254$

The above ACL are all used sequentially to allow access. You can
simplify (and speed up Squid a bit) by merging those three into one ACL
with three patterns.

Also, there is no need to regex them. Just use raw-IP address in the
dstdomain ACL type.

NP: since you have multicast .local DNS in use, you may want to turn on
Squid 3.4s multicast DNS support:
  dns_multicast_local on
 <http://www.squid-cache.org/Doc/config/dns_multicast_local/>


>> acl PublicWiFiLAN src 192.168.100.0/24
>> acl PrivateLAN src 192.168.10.0/24
>> acl DestinedForPrivateLAN dst 192.168.10.0/24
>> acl DestinedForPublicWiFiLAN dst 192.168.100.0/24
>> acl ProhibitedSitesDomains dstdomain
>> "/var/squidblacklist.org/domains.squid"
>> acl IPAddressForHostname dstdom_regex ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$

raw-IP can also come in IPv6 syntax, and sometimes may have a port
listed. *Even in IPv4-only traffic*

This pattern is better:
^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\]))(:[0-9]+)?$



>> acl SSLPorts port 443
>>
>> #block users tunneling
>> acl CONNECT method CONNECT
>> http_access deny CONNECT !SSLPorts
>>
>>
>> #disable caching
>> cache deny all
>>
>> #add VLAN2 to delay pool 1
>> delay_access 1 allow PublicWiFiLAN
>>
>> #force traffic coming in on VLAN2 to go out on VLAN2
>> tcp_outgoing_address 192.168.100.254 PublicWiFiLAN
>> tcp_outgoing_address 192.168.10.254 PrivateLAN
>>

Those do not force anything. The iptables outgoing MASQUERADE rule will
replace those addresses with whatever iptables is configured with and is
a far better way to do what you seem to want anyway.

Also note that Debian packages of Squid are IPv6-enabled. So link-local
IPv6 connections may also exist on outbound. If you dont already have
ip6tables rules setup to manage that traffic properly to/from the VPN
its getting urgent.

 And ...

>> #block traffic between VLAN1 & VLAN2
>> #iptables does this for everything EXCEPT stuff coming through Squid
>> #because iptables sees stuff coming out of squid as originating from
>> the localhost
>> #hence iptables FORWARD rules dont apply
>> http_access deny PublicWiFiLAN DestinedForPrivateLAN
>> http_access deny PrivateLAN DestinedForPublicWiFiLAN

... THESE are the rules that actually prevent traffic crossover between
the subnets by Squid.

>>
>> #show splash screen to new users on public wifi to show t&c etc
>> #pass session length as arg to perl script cache +ve & -ve responses
>> for 0 secs so
>> #perl script is always called. Script is responsible for deciding how
>> long a session is valid for
>> #%SRC passes client ip on stdin
>> external_acl_type currentsessiontype ttl=60 negative_ttl=0 %SRC
>> /var/publicwifisessions/checksession.pl
>> acl currentsession external currentsessiontype
>>
>> #will stop on first of these which matches so watch order!
>> http_access deny ProhibitedSitesDomains

>> http_access allow ToPublicWiFiGateway
>> http_access allow ToSentryBoxVL1
>> http_access allow ToWPADServer

See above about these ACLs.

>> http_access deny CONNECT PublicWiFiLAN !currentsession
>> http_access deny PublicWiFiLAN !currentsession

The above two lines overlap each other. You can remove the one with
"CONNECT" in it.


>> http_access deny IPAddressForHostname
>> http_access deny !SafePorts
>> http_access allow PrivateLAN
>> http_access allow PublicWiFiLAN
>> http_access deny AllUsers

See above about this "deny all" customization.


Amos



From enzerj at gmail.com  Sun Jul 12 18:13:02 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Sun, 12 Jul 2015 11:13:02 -0700
Subject: [squid-users] issue with multiple outgoing addresses for same
	source address
Message-ID: <CAOC8e38wpJGLFVv8uvchW0TugfMPH24A5BEtQc6xUXpDfiOUzA@mail.gmail.com>

T have a single source address I want access to 10 ips.
I also have a single source address I want to access 1 ip.

The 1:1 works fine

The 1:10 acl is off.. I can connect to the ip:port fine and it works
fine, but the outgoing address is always the one from the 1st acl.
Can someone please help? I have been going over the documentation, and
looking for examples for a day now...

Looks like this:

acl tasty3171 src 104.175.56.145/32
acl ip1 myip 172.86.226.4
tcp_outgoing_address 172.86.226.4 tasty3171
http_access allow tasty3171 ip1
http_access deny ip1 tasty3171

acl inc3172 src 52.2.xxx.204/32
tcp_outgoing_address 172.xx.xxx.5 inc3172
acl ip2 myip 172.xx.xxx.5
http_access allow inc3172 ip2
http_access deny inc3172 ip2

acl inc3173 src 52.2.xxx.204/32
acl ip3 myip 172.xx.xxx.6
tcp_outgoing_address 172.xx.xxx.6 inc3173
http_access allow inc3173 ip3
http_access deny inc3173 ip3



Thanks!

-Jason


From dan at djph.net  Sun Jul 12 18:29:12 2015
From: dan at djph.net (Dan Purgert)
Date: Sun, 12 Jul 2015 18:29:12 +0000 (UTC)
Subject: [squid-users] issue with multiple outgoing addresses for
	same	source address
References: <CAOC8e38wpJGLFVv8uvchW0TugfMPH24A5BEtQc6xUXpDfiOUzA@mail.gmail.com>
Message-ID: <mnublo$e9a$1@ger.gmane.org>

On Sun, 12 Jul 2015 11:13:02 -0700, Jason Enzer wrote:

> [...]
> Looks like this:
> 
> [snip] 
> http_access allow tasty3171 ip1
> http_access deny ip1 tasty3171
>
> [snip]
> http_access allow inc3172 ip2
> http_access deny *inc3172 ip2*
> 
> [snip]
> http_access allow inc3173 ip3
> http_access deny *inc3173 ip3*

I'm hardly a novice in squid (more of an initiate, actually) ... but it 
looks like you've got the deny rules backwards in examples 2 & 3. With 
they assumption that the first rule works fine, they should read:

2. http_access deny *ip2 inc3172*

3. http_access deny *ip3 inc3173*



From enzerj at gmail.com  Sun Jul 12 22:50:53 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Sun, 12 Jul 2015 15:50:53 -0700
Subject: [squid-users] issue with multiple outgoing addresses for same
 source address
In-Reply-To: <mnublo$e9a$1@ger.gmane.org>
References: <CAOC8e38wpJGLFVv8uvchW0TugfMPH24A5BEtQc6xUXpDfiOUzA@mail.gmail.com>
 <mnublo$e9a$1@ger.gmane.org>
Message-ID: <CAOC8e3_00kpuyBmcAQffaQqXyADm1m6M+Mo=Uw-C9AL8LSCbQQ@mail.gmail.com>

'm hardly a novice in squid (more of an initiate, actually) ... but it
looks like you've got the deny rules backwards in examples 2 & 3. With
they assumption that the first rule works fine, they should read:

2. http_access deny *ip2 inc3172*

3. http_access deny *ip3 inc3173*

client still shows outgoing address from first acl statement... if i
comment out the first acl the 2nd acl works and the outgoing address
is what is expected.

stumped!
-jason

On Sun, Jul 12, 2015 at 11:29 AM, Dan Purgert <dan at djph.net> wrote:
> On Sun, 12 Jul 2015 11:13:02 -0700, Jason Enzer wrote:
>
>> [...]
>> Looks like this:
>>
>> [snip]
>> http_access allow tasty3171 ip1
>> http_access deny ip1 tasty3171
>>
>> [snip]
>> http_access allow inc3172 ip2
>> http_access deny *inc3172 ip2*
>>
>> [snip]
>> http_access allow inc3173 ip3
>> http_access deny *inc3173 ip3*
>
> I'm hardly a novice in squid (more of an initiate, actually) ... but it
> looks like you've got the deny rules backwards in examples 2 & 3. With
> they assumption that the first rule works fine, they should read:
>
> 2. http_access deny *ip2 inc3172*
>
> 3. http_access deny *ip3 inc3173*
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From enzerj at gmail.com  Mon Jul 13 01:21:24 2015
From: enzerj at gmail.com (Jason Enzer)
Date: Sun, 12 Jul 2015 18:21:24 -0700
Subject: [squid-users] issue with multiple outgoing addresses for same
 source address
In-Reply-To: <CAOC8e3_00kpuyBmcAQffaQqXyADm1m6M+Mo=Uw-C9AL8LSCbQQ@mail.gmail.com>
References: <CAOC8e38wpJGLFVv8uvchW0TugfMPH24A5BEtQc6xUXpDfiOUzA@mail.gmail.com>
 <mnublo$e9a$1@ger.gmane.org>
 <CAOC8e3_00kpuyBmcAQffaQqXyADm1m6M+Mo=Uw-C9AL8LSCbQQ@mail.gmail.com>
Message-ID: <CAOC8e39-Oweqv73p8dP69bsMpEt0k8bDSd5DMQ8=ow183z93cA@mail.gmail.com>

sorry for the repost. putting the whole config up. maybe someone can
spot the issue.

I have a single source address I want access to 10 ips.
I also have a single source address I want to access 1 ip.

The 1:1 works fine

The 1:10 acl is off.. I can connect to the ip:port fine and it works
fine, but the outgoing address is always the one from the 1st acl.
Can someone please help? I have been going over the documentation, and
looking for examples for a day now...

Looks like this:

#
# Recommended minimum configuration:
#
#acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

http_access allow localhost
http_port 172.86.226.4:3171 name=3171
http_port 172.86.226.5:3172 name=3172
http_port 172.86.226.6:3173 name=3173


#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
#http_access allow manager localhost
#http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#



acl tasty3171 src 104.175.56.145/32
acl ip1 myip 172.86.226.4
tcp_outgoing_address 172.86.226.4 tasty3171
http_access allow ip1 tasty3171
http_access deny ip1 tasty3171

acl inc3172 src 52.2.100.204/32
acl ip2 myip 172.86.226.5
tcp_outgoing_address 172.86.226.5 inc3172
http_access allow ip2 inc3172
http_access deny ip2 inc3172

acl inc3173 src 52.2.100.204/32
acl ip3 myip 172.86.226.6
tcp_outgoing_address 172.86.226.6 inc3173
http_access allow ip3 inc3173
http_access deny ip3 inc3173



# And finally deny all other access to this proxy
http_access deny all



# Uncomment the line below to enable disk caching - path format is
/cygdrive/<full path to cache folder>, i.e.
#cache_dir aufs /cygdrive/d/squid/cache 3000 16 256


# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid




dns_nameservers 4.2.2.1 8.8.8.8

request_header_access Allow allow all
request_header_access User-Agent allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Cookie allow all
request_header_access Proxy-Connection allow all
header_replace Accept */*
header_replace Accept-Encoding *
header_replace Accept-Language en-us
header_replace User-Agent Mozilla/6.0 (Windows NT 6.2; WOW64;
rv:16.0.1) Gecko/20121011 Firefox/16.0.1
forwarded_for off


#Privacy Things
via off
request_header_access X-Forwarded-For deny all

follow_x_forwarded_for deny all

cache_replacement_policy heap LRU
memory_replacement_policy heap LRU

max_filedescriptors 3200

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/spool/squid 256 16 256
cache_mem 128 MB
# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
cache_effective_user squid
cache_effective_group squid

thanks!

-jason


From tmblue at gmail.com  Mon Jul 13 01:36:18 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Sun, 12 Jul 2015 18:36:18 -0700
Subject: [squid-users] RPM for 3.5.6 CentOS 6.x
Message-ID: <CAEaSS0YLD0rZx+KdVZuPwjWFJEMNL+tQyDVibKarUpNx2uRJ9A@mail.gmail.com>

Wondering when a 3.5.6 RPM will be available. I can build the beta's no
issue, but I've spent a couple of days with trying to get 3.5.6 packaged up
and am failing. So it would be nice to get a 3.5.6 spun up as the 3.5.x was
provided and that was clean, but for some reason I can't get the 3.5.6 to
build in my environments and after 2 days I'm about to throw in the towel.

Thanks
tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150712/ee349845/attachment.htm>

From squid3 at treenet.co.nz  Mon Jul 13 06:38:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Jul 2015 18:38:16 +1200
Subject: [squid-users] RPM for 3.5.6 CentOS 6.x
In-Reply-To: <CAEaSS0YLD0rZx+KdVZuPwjWFJEMNL+tQyDVibKarUpNx2uRJ9A@mail.gmail.com>
References: <CAEaSS0YLD0rZx+KdVZuPwjWFJEMNL+tQyDVibKarUpNx2uRJ9A@mail.gmail.com>
Message-ID: <55A35CD8.1050007@treenet.co.nz>

On 13/07/2015 1:36 p.m., Tory M Blue wrote:
> Wondering when a 3.5.6 RPM will be available. I can build the beta's no
> issue, but I've spent a couple of days with trying to get 3.5.6 packaged up
> and am failing. So it would be nice to get a 3.5.6 spun up as the 3.5.x was
> provided and that was clean, but for some reason I can't get the 3.5.6 to
> build in my environments and after 2 days I'm about to throw in the towel.

Eliezer mentioned having 3.5.6 RPMs available last night. I'm not sure
if CentOS 6 was included in that first bunch, but it wouldn'y hirt to
check with him.

Amos



From squid3 at treenet.co.nz  Mon Jul 13 06:50:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Jul 2015 18:50:59 +1200
Subject: [squid-users] issue with multiple outgoing addresses for same
 source address
In-Reply-To: <CAOC8e3_00kpuyBmcAQffaQqXyADm1m6M+Mo=Uw-C9AL8LSCbQQ@mail.gmail.com>
References: <CAOC8e38wpJGLFVv8uvchW0TugfMPH24A5BEtQc6xUXpDfiOUzA@mail.gmail.com>
 <mnublo$e9a$1@ger.gmane.org>
 <CAOC8e3_00kpuyBmcAQffaQqXyADm1m6M+Mo=Uw-C9AL8LSCbQQ@mail.gmail.com>
Message-ID: <55A35FD3.3000902@treenet.co.nz>

On 13/07/2015 10:50 a.m., Jason Enzer wrote:
> 'm hardly a novice in squid (more of an initiate, actually) ... but it
> looks like you've got the deny rules backwards in examples 2 & 3. With
> they assumption that the first rule works fine, they should read:
> 
> 2. http_access deny *ip2 inc3172*
> 
> 3. http_access deny *ip3 inc3173*
> 
> client still shows outgoing address from first acl statement... if i
> comment out the first acl the 2nd acl works and the outgoing address
> is what is expected.
> 

The "myip" and "myport" ACLs were deprecated years ago due to unreliability.

Use "myportname" ACL type instead. That matches the actual listening
port Squid received on, not a lookup of its host IP(s).

Or you could use "localip" ACL type, which uses the IP from the TCP
connection arriving at Squid.

Amos



From philipp.wehling at megatel.de  Mon Jul 13 10:39:33 2015
From: philipp.wehling at megatel.de (Philipp Wehling)
Date: Mon, 13 Jul 2015 12:39:33 +0200 (CEST)
Subject: [squid-users] accessing google.com
In-Reply-To: <55A1BD5A.5030400@treenet.co.nz>
References: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>
 <55A1BD5A.5030400@treenet.co.nz>
Message-ID: <386647485.4484.1436783973377.JavaMail.zimbra@megatel.de>

Hello,

thank you for your answer.

> This is where you really do need to understand the 28,3 debug output.

I thought squid works in a "first-match"-manner... Do you have any documentation, I can read about?

> That is of course following your assumption that Squid is actively rejecting the traffic with ACLs (you provide no evidence of that).

How can I proof that? When the error comes up, I get the default ACL-Block-Page.

> Chances are rather high these days that it has nothing to do with Squid since Google prefer to use HTTPS, QUIC, SPDY, or HTTP/2 protocols. All of which normally bypass Squid processing entirely.

That answer is interesting, because we have some company (blocking) policies regarding drive.google.com and everything works fine. How is it possible, that google.com bypasses these rules? Please give me more hints for my investigation.

Thanks!


kind regards,
pwe

----- Urspr?ngliche Mail -----
Von: "Amos Jeffries" <squid3 at treenet.co.nz>
An: squid-users at lists.squid-cache.org
Gesendet: Sonntag, 12. Juli 2015 03:05:30
Betreff: Re: [squid-users] accessing google.com

On 11/07/2015 12:21 a.m., Philipp Wehling wrote:
> Hello,
> 
> from time to time we have trouble accessing google.com.
> 
> After many many troubleshooting and trying to understand debug_options 28,x I wanted to go another way:
> 
> All I want is to display, which ACL is blocking the access to the website.
> 
> I already found the directive deny_info but this doesnt give me the right parameters.
> 
> 
> Any ideas would be appreciated.

This is where you really do need to understand the 28,3 debug output.
There is no one ACL that is causing it. A whole set of ACLs and
particular sequence order of test leads up to any denial.

That is of course following your assumption that Squid is actively
rejecting the traffic with ACLs (you provide no evidence of that).

Chances are rather high these days that it has nothing to do with Squid
since Google prefer to use HTTPS, QUIC, SPDY, or HTTP/2 protocols. All
of which normally bypass Squid processing entirely.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Jul 13 11:14:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Jul 2015 23:14:25 +1200
Subject: [squid-users] accessing google.com
In-Reply-To: <386647485.4484.1436783973377.JavaMail.zimbra@megatel.de>
References: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>
 <55A1BD5A.5030400@treenet.co.nz>
 <386647485.4484.1436783973377.JavaMail.zimbra@megatel.de>
Message-ID: <55A39D91.6070303@treenet.co.nz>

On 13/07/2015 10:39 p.m., Philipp Wehling wrote:
> Hello,
> 
> thank you for your answer.
> 
>> This is where you really do need to understand the 28,3 debug output.
> 
> I thought squid works in a "first-match"-manner... Do you have any documentation, I can read about?

Nothing that woudl be helpful Im afraid. Cleaning it up and writing a
how-to blog has been on my todo list for years now :-(

Its currently a matter of following the trace and eyeballing what it
says its doing vs what your squid.conf contains.

The debugs output should show you the squid.conf line its processing,
then the 0/1/-1 (aka fail, pass, async-delay) results of each individual
test run for that line.
Then the line result action 0/1/-1 (aka allow, deny, dunno/skip)


> 
>> That is of course following your assumption that Squid is actively rejecting the traffic with ACLs (you provide no evidence of that).
> 
> How can I proof that? When the error comes up, I get the default ACL-Block-Page.

The debugs trace will result in a deny action in one of the access
control lines. Which then logs something like "Request for ... DENIED
due to ...".

There are other things forbidden by the protocol, or errors that can
happen which Squid just responds with a DENIED page to. No ACL
processing required for that to happen.

> 
>> Chances are rather high these days that it has nothing to do with Squid since Google prefer to use HTTPS, QUIC, SPDY, or HTTP/2 protocols. All of which normally bypass Squid processing entirely.
> 
> That answer is interesting, because we have some company (blocking) policies regarding drive.google.com and everything works fine. How is it possible, that google.com bypasses these rules? Please give me more hints for my investigation.
> 

They hard-code the Chrome web browser and other apps or scripts they
produce to try the other protocols first when connnecting to their own
servers. HTTP messages coming out of their servers also contain headers
such as Alternative-Protocol or Alt-Svc requesting the cleint software
uses non-HTTP protocols.

The latest release of Squid automatically strip the HTTP headers away on
related responses. You still have to also firewall block ports UDP/443
and UDP/80 to avoid QUICK protocol. Only HTTPS decryption will prevent
HTTP/2, SPDY, and WebSockets usage since they are either inside TLS or
CONNECT tunnels.

Amos


From 2005now at mail.ru  Mon Jul 13 17:54:13 2015
From: 2005now at mail.ru (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0KDRg9C60LDQstGG0L7Qsg==?=)
Date: Mon, 13 Jul 2015 20:54:13 +0300
Subject: [squid-users]
 =?utf-8?q?Squid_+_kerberos=2C_all_childrens_are_bus?= =?utf-8?q?y?=
In-Reply-To: <559E6C3E.5030004@treenet.co.nz>
References: <1436435648.897981882@f301.i.mail.ru>
 <1436437932.137673239@f40.i.mail.ru>
 <559E6C3E.5030004@treenet.co.nz>
Message-ID: <1436810053.1887534@f178.i.mail.ru>






??? >>>>> Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
??? >>>>>
??? >>>>> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
??? >>>>>
??? >>>>> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy.
??? >>>>> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
??? >>>>>
??? >>>>
??? >>>> So 50 users / 60 helpers ... how many requests per second? and how
??? >>>> fast/slow is the helper responding?
??? >> Could you clarify how I can get value of requests per second and respond?
??? >
??? >The cachemgr "info" report. From the cachemgr.cgi tool, or "squidclient
??? >mgr:info" command line, or
??? >http://$visible_hostname:3128/squid-internal-mgr/info
??? >
??? > Or calculated from a quick count of the access.log lines over a few mins.

??? ~600 lines per minute,



??? >> Debugs show like 3-4 message per second like:
??? >>
??? >> negotiate_kerberos_pac.cc(376): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got PAC data of lengh 624
??? >> negotiate_kerberos_pac.cc(180): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 5 rids
??? >> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 513
??? >> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 3692
??? >> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4268
??? >> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4622
??? >> negotiate_kerberos_pac.cc(188): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: Info: Got rid: 4623
??? >> negotiate_kerberos_pac.cc(256): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got DomainLogonId S-1-5-21-1708537768-1580818891-2146958067
??? >> negotiate_kerberos_pac.cc(278): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Found 1 ExtraSIDs
??? >> negotiate_kerberos_pac.cc(327): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Got ExtraSid S-1-5-21-1708537768-1580818891-2146958067-4107
??? >> negotiate_kerberos_pac.cc(456): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: INFO: Read 620 of 624 bytes
??? >> negotiate_kerberos_auth.cc(778): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: Groups group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/AQIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/bA4AAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/rBAAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DhIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/DxIAAA== group=AQUAAAAAAAUVAAAAqDfWZcthOV7z+vd/CxAAAA==
??? >> negotiate_kerberos_auth.cc(783): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: AF oYGyMIGvoAMKAQChCwYJKoZIgvcSAQICooGaBIGXYIGUBgkqhkiG9xIBAgICAG+BhDCBgaADAgEFoQMCAQ+idTBzoAMCAReibARqY4fSYtg+X4HhiH8dFmWxdn3wxtoKKZzEfUjLYibMoy0XAAWgkSYVXgC7gxO7cgCkOofEqZQhi/GKa4NZqn2dQqOJU/3y4zkPqBP9Ialh//BL5ov03L5BqjgthrbYbrcxJTo57EJIdO8O1g== avialex
??? >>
??? >> And errors like:
??? >> 2015/07/09 13:28:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
??? >> All my friends get the same error, but their squid is working fine.
??? >>
??? >> Don't see anything else
??? >>
??? >
??? >Aha. So your users browsers are sending NTLM auth instead of Kerberos.
??? >That is at least one part of the problem. NTLM handshake can take whole
??? >seconds and places a lot of extra load on the helpers. To resolve these
??? >the users software needs fixing to use Kerberos properly when Negotiate

??? >is offered.
??? >
??? >The other part is figuring out what amount of helpers is needed to meet
??? >the load requirements. With NTLM it is usually several hundred.
??? >


When i'm using proxy alone, squid stars 33 childrens, don't recive any NTLM errors, but internet start to lag. So the problem not in the NTLM software. I tryed to start 300 children for my 60 users, but still have huge lags, even when half was free.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150713/22ed30ad/attachment.htm>

From squid3 at treenet.co.nz  Mon Jul 13 18:57:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 06:57:19 +1200
Subject: [squid-users] Squid + kerberos, all childrens are busy
In-Reply-To: <1436810053.1887534@f178.i.mail.ru>
References: <1436435648.897981882@f301.i.mail.ru>
 <1436437932.137673239@f40.i.mail.ru> <559E6C3E.5030004@treenet.co.nz>
 <1436810053.1887534@f178.i.mail.ru>
Message-ID: <55A40A0F.3060100@treenet.co.nz>

On 14/07/2015 5:54 a.m., ??????? ???????? wrote:
> 
> 
> 
> 
> 
>     >>>>> Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
>     >>>>>
>     >>>>> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
>     >>>>>
>     >>>>> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy.
>     >>>>> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
>     >>>>>
>     >>>>
>     >>>> So 50 users / 60 helpers ... how many requests per second? and how
>     >>>> fast/slow is the helper responding?
>     >> Could you clarify how I can get value of requests per second and respond?
>     >
>     >The cachemgr "info" report. From the cachemgr.cgi tool, or "squidclient
>     >mgr:info" command line, or
>     >http://$visible_hostname:3128/squid-internal-mgr/info
>     >
>     > Or calculated from a quick count of the access.log lines over a few mins.
> 
>     ~600 lines per minute,
> 
> 
> 
>     >> Debugs show like 3-4 message per second like:
>     >>
<snip>
>     >> negotiate_kerberos_auth.cc(783): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: AF oYGyMIGvoAMKAQChCwYJKoZIgvcSAQICooGaBIGXYIGUBgkqhkiG9xIBAgICAG+BhDCBgaADAgEFoQMCAQ+idTBzoAMCAReibARqY4fSYtg+X4HhiH8dFmWxdn3wxtoKKZzEfUjLYibMoy0XAAWgkSYVXgC7gxO7cgCkOofEqZQhi/GKa4NZqn2dQqOJU/3y4zkPqBP9Ialh//BL5ov03L5BqjgthrbYbrcxJTo57EJIdO8O1g== avialex
>     >>
>     >> And errors like:
>     >> 2015/07/09 13:28:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
>     >> All my friends get the same error, but their squid is working fine.
>     >>


Okay, so the traffic arriving at Squid is ~10 req/sec, and the helpers
are processing only 4 req/sec successfully.

If we assume that also each client connection is attempting one NTLM
request before it gets to Kerberos (when it should be doing the
opposite). That allows for the helper rejecting 3-4 req/sec.

That makes a total of up to 8 req/sec being handled by the helpers.
Still leaving 2 req/sec building up in the queue.


I see two problems there.

Firstly, 3-4 req/sec seems to be a very slow response rate by the
helpers. If you can find some way to improve that enough to stop the
queue building up your problem should go away.
 - that might be done by increasing the startup= helpers count (and
maximum count)
 - that might be by improving the helper connectivity speed and access
to DNS and the backend AD system.


Secondly, that NTLM issue. The only fix for that is to get the client
devices configured to try the more secure Kerberos auth first (like they
should be doing anyway).
 - that may require disabling NTLM entirely for them.



>     >> Don't see anything else
>     >>
>     >
>     >Aha. So your users browsers are sending NTLM auth instead of Kerberos.
>     >That is at least one part of the problem. NTLM handshake can take whole
>     >seconds and places a lot of extra load on the helpers. To resolve these
>     >the users software needs fixing to use Kerberos properly when Negotiate
> 
>     >is offered.
>     >
>     >The other part is figuring out what amount of helpers is needed to meet
>     >the load requirements. With NTLM it is usually several hundred.
>     >
> 
> 
> When i'm using proxy alone, squid stars 33 childrens, don't recive any NTLM errors, but internet start to lag. So the problem not in the NTLM software. I tryed to start 300 children for my 60 users, but still have huge lags, even when half was free.
> 

I suggest For 60 users doing 10req/sec I suggest configuring Squid with:
 auth_param negotiate children 500 startup=120 idle=10


So what do you think the lag is coming from then?

And how are you defining "free" in terms of helpers?

Amos


From johnpearson555 at gmail.com  Mon Jul 13 20:15:27 2015
From: johnpearson555 at gmail.com (John Pearson)
Date: Mon, 13 Jul 2015 13:15:27 -0700
Subject: [squid-users] Transparent proxy before NAT
Message-ID: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>

Hi Everyone,

My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
Devices

Currently the Router is doing NAT and DHCP for the devices connected to it.
Squid is in transparent mode. I set up a bridge ( br0). I set up the
ebtables and iptables. It works but I want to figure out a way without
having to configure Squid server or Router with hardcoded addresses.

I have it working with either setup:
1. Remove the bridge ( br0) and setup the Squid server eth1 as a static IP
address and set Squid server IP address as gateway in Router settings.
2. Since Squid server is in bridge mode, I can hard code IP address in a
Squid ACL as all traffic appears to come this IP address from the router.

I want a way to do this without any setup, basically to take a Squid box
and place it before a Router. Is there a way to do this ?

A few ideas that might be wrong:
1. In bridge mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS
is the dynamic IP address provided the ISP ) Is there a way to obtain this
in the squid.conf file ?
2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
Router(DHCP, NAT) and have same dhcp address given to the Router in
squid.conf as http_access allow localnet

Thanks in advance!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150713/84e88c87/attachment.htm>

From yvoinov at gmail.com  Mon Jul 13 20:21:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Jul 2015 02:21:28 +0600
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
Message-ID: <55A41DC8.9090001@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Too complex setup for simple task.

You can simple re-connect squid box before router and configure it as
gateway for devices. And setup NAT redirection directly onto squid box.

Something like this:

Internet <-----> Router + DHCP + NAT <------> Squid box + NAT
redirection <----------> Devices

Router will use NAT from white IP to internal IP's. Squid box will use
port redirection and will configure like forwarding IP box.

That's it.

14.07.15 2:15, John Pearson ?????:
> Hi Everyone,
>
> My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
> Devices
>
> Currently the Router is doing NAT and DHCP for the devices connected
to it.
> Squid is in transparent mode. I set up a bridge ( br0). I set up the
> ebtables and iptables. It works but I want to figure out a way without
> having to configure Squid server or Router with hardcoded addresses.
>
> I have it working with either setup:
> 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static IP
> address and set Squid server IP address as gateway in Router settings.
> 2. Since Squid server is in bridge mode, I can hard code IP address in a
> Squid ACL as all traffic appears to come this IP address from the router.
>
> I want a way to do this without any setup, basically to take a Squid box
> and place it before a Router. Is there a way to do this ?
>
> A few ideas that might be wrong:
> 1. In bridge mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS
> is the dynamic IP address provided the ISP ) Is there a way to obtain this
> in the squid.conf file ?
> 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
> Router(DHCP, NAT) and have same dhcp address given to the Router in
> squid.conf as http_access allow localnet
>
> Thanks in advance!
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpB3IAAoJENNXIZxhPexGjJYH/R0ESKEeEzla/v/sceiPBmds
r9//Nif+sGgeD8rRzVdNOYwv2tR5OpSjRr4j8F2QQYg4wO+myEUL2V6a8ATsOcOa
WM6xNiK34fbzT48mOTwRB2tsbURdxWxl1HB+7RnjSw596i5Jb/c24AlSburUKFMI
iTBppm/9ROT8lDAUAWUUx1W0SLUvylvZp4wNdA5QAY0F7uLO1X8uFXMbJXRarTYy
9lahI4dOO4SakHtsHpIoIT0uu1GGWzWHhN4c1lsER5/wX+oukpe9hRMgPYqeKJox
M/wIn7EdX2DpnBt9bLZGgkcTtKDAE0j8yfFvB3/at81zvQq8MsJSh24Hq6e4I/I=
=UG9i
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/c0fb7deb/attachment.htm>

From yvoinov at gmail.com  Mon Jul 13 20:23:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Jul 2015 02:23:41 +0600
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
Message-ID: <55A41E4D.2060400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Note:

If you want to use two NIC onto Squid box, you need to configure this
box TCP stack as a static router.

But more better to aggregate both NIC and connect router and squid box
with switch.

14.07.15 2:15, John Pearson ?????:
> Hi Everyone,
>
> My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
> Devices
>
> Currently the Router is doing NAT and DHCP for the devices connected
to it.
> Squid is in transparent mode. I set up a bridge ( br0). I set up the
> ebtables and iptables. It works but I want to figure out a way without
> having to configure Squid server or Router with hardcoded addresses.
>
> I have it working with either setup:
> 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static IP
> address and set Squid server IP address as gateway in Router settings.
> 2. Since Squid server is in bridge mode, I can hard code IP address in a
> Squid ACL as all traffic appears to come this IP address from the router.
>
> I want a way to do this without any setup, basically to take a Squid box
> and place it before a Router. Is there a way to do this ?
>
> A few ideas that might be wrong:
> 1. In bridge mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS
> is the dynamic IP address provided the ISP ) Is there a way to obtain this
> in the squid.conf file ?
> 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
> Router(DHCP, NAT) and have same dhcp address given to the Router in
> squid.conf as http_access allow localnet
>
> Thanks in advance!
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpB5MAAoJENNXIZxhPexGedUH/j3tw39S2TmmU+NR/Y/ERvWK
xLwn+ixsahMtsV26M7Petp58D4mJp8ZZclFl1xf5MxOfyRv5c/n6U090asy08TRu
KBrC0rHwJr76tdRsNqLeKmGOKejGKh7H8Y24j8TZ+8dYA2Csv4DK5O8VXQAaTB9w
NIdsszXUvv2I9HtF+CPWbmIjljG0IzpqKKDMoEZtkhJXOoSzGYCO9HXNqF7H22Kz
6C7EOtOOUpu635I6IL1QLbkuoBNHgTuO4bVC8pa3unCGSdCDwOPPRbivcNEOI90x
dl5ehT7W2hQ1pZze7p5Wiy2h4AnyXc5c7bzZNOTE5JF95Kw+45Q/fRRbvXUhv/c=
=zEMT
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/916ea42a/attachment.htm>

From yvoinov at gmail.com  Mon Jul 13 20:24:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Jul 2015 02:24:27 +0600
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
Message-ID: <55A41E7B.4010004@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And beware: Your current configuration is insecure. Very insecure.
Especially if you haven't firewall configured on squid box.

14.07.15 2:15, John Pearson ?????:
> Hi Everyone,
>
> My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
> Devices
>
> Currently the Router is doing NAT and DHCP for the devices connected
to it.
> Squid is in transparent mode. I set up a bridge ( br0). I set up the
> ebtables and iptables. It works but I want to figure out a way without
> having to configure Squid server or Router with hardcoded addresses.
>
> I have it working with either setup:
> 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static IP
> address and set Squid server IP address as gateway in Router settings.
> 2. Since Squid server is in bridge mode, I can hard code IP address in a
> Squid ACL as all traffic appears to come this IP address from the router.
>
> I want a way to do this without any setup, basically to take a Squid box
> and place it before a Router. Is there a way to do this ?
>
> A few ideas that might be wrong:
> 1. In bridge mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS
> is the dynamic IP address provided the ISP ) Is there a way to obtain this
> in the squid.conf file ?
> 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
> Router(DHCP, NAT) and have same dhcp address given to the Router in
> squid.conf as http_access allow localnet
>
> Thanks in advance!
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpB57AAoJENNXIZxhPexG/JEIAI06Ksm0R7n2O8h5mHO0HgFe
8r/bmmcKcmkmRiWXJGAq/zKY5oBuzeNocuwS4HNkj+//hYkdRpTyF8+ozFNeoSYj
2AnEkmcjZLjGk3kG/RcBpdIY8n1iXBQuD0I/4UrTleeG282tVeZJbe+qWVXvG1nB
7N7dyB/kYeKnlmhUNfCCbhyoLD3dJyC+8ECYjwAKIWspdPnzAPUFMIPc1NmWnMWU
IiQJe73wCITVd100YCSeCBbOvlvoYbWbQrymOb7rWMVJJq/qQxa2R27660DHAvjj
pnF0bnh94kvjFJ7Pk3AXM3d4jXKt0DbJLiXuw6Ch2MzZcfN0cYfpTDiGvH6XcBY=
=dAJc
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/bb267e26/attachment.htm>

From yvoinov at gmail.com  Mon Jul 13 20:26:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Jul 2015 02:26:02 +0600
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
Message-ID: <55A41EDA.9030109@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ah,

forgot about:

Your squid in scheme I wrote will have static gray IP. And this IP must
be excluded from DHCP pool on router.

14.07.15 2:15, John Pearson ?????:
> Hi Everyone,
>
> My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
> Devices
>
> Currently the Router is doing NAT and DHCP for the devices connected
to it.
> Squid is in transparent mode. I set up a bridge ( br0). I set up the
> ebtables and iptables. It works but I want to figure out a way without
> having to configure Squid server or Router with hardcoded addresses.
>
> I have it working with either setup:
> 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static IP
> address and set Squid server IP address as gateway in Router settings.
> 2. Since Squid server is in bridge mode, I can hard code IP address in a
> Squid ACL as all traffic appears to come this IP address from the router.
>
> I want a way to do this without any setup, basically to take a Squid box
> and place it before a Router. Is there a way to do this ?
>
> A few ideas that might be wrong:
> 1. In bridge mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS
> is the dynamic IP address provided the ISP ) Is there a way to obtain this
> in the squid.conf file ?
> 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
> Router(DHCP, NAT) and have same dhcp address given to the Router in
> squid.conf as http_access allow localnet
>
> Thanks in advance!
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEbBAEBCAAGBQJVpB7aAAoJENNXIZxhPexGJcgH+IcaMqoEwlcRYFNCWqKT/Msc
I6aMD/82Uw5ow/HayX/GrxCHTzYjdCzXDXJTP9cAnHZaMnvOPxtCGuVocEHNEiOa
sDsZC9P074hoANDEAYXycWF73auCxYg4jcg8dRtbZwVEazwYsMVN6ye5a3i9EaZM
/DotQ78htLNRJrLhoCO9yQBtJObcUs+eyOie4oxk4YWSfQMcjZOXen7U8K8KGQuH
cOBcodLJv/eP1T+CcEe3ATr8Szo+zQ648jG27pdy7XuPecek7sWllRnyq93fpkID
FnvOr21R3gLBBdStYty43PKQ/4Z3d4vp56aYEweKBsGJV9kVC2QMjDXLOzrbug==
=1pgP
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/da9942fa/attachment.htm>

From johnpearson555 at gmail.com  Mon Jul 13 20:34:35 2015
From: johnpearson555 at gmail.com (John Pearson)
Date: Mon, 13 Jul 2015 13:34:35 -0700
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_wQeCVoXZ9xDz9JTBiEapmxx0OE7E=pp3SJW9CKU=q+9g@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
 <55A41EDA.9030109@gmail.com>
 <CAKNtY_wQeCVoXZ9xDz9JTBiEapmxx0OE7E=pp3SJW9CKU=q+9g@mail.gmail.com>
Message-ID: <CAKNtY_xjqZr=jMfJD3FAv9VEHkgzYOqt_3MzSSS3+mQzLjLyGw@mail.gmail.com>

Thanks Yuri for the response, I understand. I do have Shorewall configured
and I understand the security implications. My Router is also the Wireless
AP, so I want to try out this setup without having to buy another Wireless
AP.

I don't mind it being complex, do you have any suggestions on getting
Internet <---> Squid <---> Router (NAT) working ?

Thanks!

On Mon, Jul 13, 2015 at 1:33 PM, John Pearson <johnpearson555 at gmail.com>
wrote:

> Thanks Yuri for the response, I understand. I do have Shorewall configured
> and I understand the security implications. My Router is also the Wireless
> AP, so I want to try out this setup without having to buy another Wireless
> AP.
>
> I don't mind it being complex, do you have any suggestions on getting
> Internet <---> Squid <---> Router (NAT) working ?
>
> Thanks!
>
> On Mon, Jul 13, 2015 at 1:26 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>> Ah,
>>
>> forgot about:
>>
>> Your squid in scheme I wrote will have static gray IP. And this IP must
>> be excluded from DHCP pool on router.
>>
>> 14.07.15 2:15, John Pearson ?????:
>> > Hi Everyone,
>> >
>> > My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
>> > Devices
>> >
>> > Currently the Router is doing NAT and DHCP for the devices connected to
>> it.
>> > Squid is in transparent mode. I set up a bridge ( br0). I set up the
>> > ebtables and iptables. It works but I want to figure out a way without
>> > having to configure Squid server or Router with hardcoded addresses.
>> >
>> > I have it working with either setup:
>> > 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static
>> IP
>> > address and set Squid server IP address as gateway in Router settings.
>> > 2. Since Squid server is in bridge mode, I can hard code IP address in a
>> > Squid ACL as all traffic appears to come this IP address from the
>> router.
>> >
>> > I want a way to do this without any setup, basically to take a Squid box
>> > and place it before a Router. Is there a way to do this ?
>> >
>> > A few ideas that might be wrong:
>> > 1. In bridge mode, http_access allow CURRENTIPADDRESS  (
>> CURRENTIPADDRESS
>> > is the dynamic IP address provided the ISP ) Is there a way to obtain
>> this
>> > in the squid.conf file ?
>> > 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
>> > Router(DHCP, NAT) and have same dhcp address given to the Router in
>> > squid.conf as http_access allow localnet
>> >
>> > Thanks in advance!
>> >
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEbBAEBCAAGBQJVpB7aAAoJENNXIZxhPexGJcgH+IcaMqoEwlcRYFNCWqKT/Msc
>> I6aMD/82Uw5ow/HayX/GrxCHTzYjdCzXDXJTP9cAnHZaMnvOPxtCGuVocEHNEiOa
>> sDsZC9P074hoANDEAYXycWF73auCxYg4jcg8dRtbZwVEazwYsMVN6ye5a3i9EaZM
>> /DotQ78htLNRJrLhoCO9yQBtJObcUs+eyOie4oxk4YWSfQMcjZOXen7U8K8KGQuH
>> cOBcodLJv/eP1T+CcEe3ATr8Szo+zQ648jG27pdy7XuPecek7sWllRnyq93fpkID
>> FnvOr21R3gLBBdStYty43PKQ/4Z3d4vp56aYEweKBsGJV9kVC2QMjDXLOzrbug==
>> =1pgP
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150713/43aa6cd5/attachment.htm>

From sebag at vianetcon.com.ar  Mon Jul 13 20:58:58 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Mon, 13 Jul 2015 17:58:58 -0300
Subject: [squid-users] Is it possible to tunnelize http traffic?
Message-ID: <55A42692.3060207@vianetcon.com.ar>

Hello, I'm trying to improve the bypass system we use in our servers. 
When a site is not shown as it should, or something is broken because of 
a poor server's side implementation, we bypass traffic to that server at 
ebtables level. This works just as expected, squid never "sees" this 
traffic, but when you have problems with some site using akamai's cdn .. 
bypassing that network impacts negatively on squid's performance.
Is it possible to tunnelize (as in TCP_TUNNEL/200) some arbitrary 
traffic? URL regex would be awesome. Tried googling it but could only 
find https related material.


Thanks,
Sebastian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150713/788a4f66/attachment.htm>

From truniger at bluewin.ch  Mon Jul 13 21:16:01 2015
From: truniger at bluewin.ch (Othmar Truniger)
Date: Mon, 13 Jul 2015 23:16:01 +0200
Subject: [squid-users] cannot use squid-3.5.x for production
Message-ID: <000001d0bdb1$21d8ecf0$658ac6d0$@bluewin.ch>

Amos,
would you mind 'pinging' again. The bug is not even confirmed yet.

> The FTP handling code was re-written and shuffled around a lot to get
> FTP native support working. I've 'pinged' the author who did that about
> your bug report. Hopefully he will have some time to look at it.
> 
> Amos



From david at articatech.com  Tue Jul 14 00:09:19 2015
From: david at articatech.com (David Touzeau)
Date: Tue, 14 Jul 2015 02:09:19 +0200
Subject: [squid-users] [3.5.6]: assertion failed: store.cc:850:
	"store_status == STORE_PENDING"
Message-ID: <55A4532F.8080408@articatech.com>

Hi all

We receive this error in cache.log

assertion failed: store.cc:850: "store_status == STORE_PENDING"

Just after browser sends "ERR_PROXY_CONNECTION_FAILED"

What does it means ?

Best regards


From yvoinov at gmail.com  Tue Jul 14 05:52:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Jul 2015 11:52:22 +0600
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_xjqZr=jMfJD3FAv9VEHkgzYOqt_3MzSSS3+mQzLjLyGw@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
 <55A41EDA.9030109@gmail.com>
 <CAKNtY_wQeCVoXZ9xDz9JTBiEapmxx0OE7E=pp3SJW9CKU=q+9g@mail.gmail.com>
 <CAKNtY_xjqZr=jMfJD3FAv9VEHkgzYOqt_3MzSSS3+mQzLjLyGw@mail.gmail.com>
Message-ID: <55A4A396.8040900@gmail.com>

I use a bit another configuration:

http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2

As you can see, squid box placed between two routers. Front router uses 
NAT to white IP, back router has no NAT and configured with WCCPv2 
redirection. DMZ configured between two routers.

As I think, configuration you described will really strange and 
insecure. Yes, you can assign white IP to squid. No, you can't use squid 
as router or DHCP. Squid box can only work as intercepter for HTTP/HTTPS 
traffic and TCP/IP forwarder for another traffic.

As I said, more correct configuration will be:

Internet <-----> Router <-----> Transparent Squid box as gateway 
<-------> devices.

This configuration works, I use it on my testing environment.

14.07.15 2:34, John Pearson ?????:
> Thanks Yuri for the response, I understand. I do have Shorewall 
> configured and I understand the security implications. My Router is 
> also the Wireless AP, so I want to try out this setup without having 
> to buy another Wireless AP.
>
> I don't mind it being complex, do you have any suggestions on getting 
> Internet <---> Squid <---> Router (NAT) working ?
>
> Thanks!
>
> On Mon, Jul 13, 2015 at 1:33 PM, John Pearson 
> <johnpearson555 at gmail.com <mailto:johnpearson555 at gmail.com>> wrote:
>
>     Thanks Yuri for the response, I understand. I do have Shorewall
>     configured and I understand the security implications. My Router
>     is also the Wireless AP, so I want to try out this setup without
>     having to buy another Wireless AP.
>
>     I don't mind it being complex, do you have any suggestions on
>     getting Internet <---> Squid <---> Router (NAT) working ?
>
>     Thanks!
>
>     On Mon, Jul 13, 2015 at 1:26 PM, Yuri Voinov <yvoinov at gmail.com
>     <mailto:yvoinov at gmail.com>> wrote:
>
>
>         -----BEGIN PGP SIGNED MESSAGE-----
>         Hash: SHA256
>
>         Ah,
>
>         forgot about:
>
>         Your squid in scheme I wrote will have static gray IP. And
>         this IP must be excluded from DHCP pool on router.
>
>         14.07.15 2:15, John Pearson ?????:
>         > Hi Everyone, > > My setup is: Internet <--> Squid-eth0 <--> Squid-eth1
>         <--> Router <--> > Devices > > Currently the Router is doing
>         NAT and DHCP for the devices connected to it. > Squid is in
>         transparent mode. I set up a bridge ( br0). I set up the >
>         ebtables and iptables. It works but I want to figure out a way
>         without > having to configure Squid server or Router with
>         hardcoded addresses. > > I have it working with either setup:
>         > 1. Remove the bridge ( br0) and setup the Squid server eth1
>         as a static IP > address and set Squid server IP address as
>         gateway in Router settings. > 2. Since Squid server is in
>         bridge mode, I can hard code IP address in a > Squid ACL as
>         all traffic appears to come this IP address from the router. >
>         > I want a way to do this without any setup, basically to take
>         a Squid box > and place it before a Router. Is there a way to
>         do this ? > > A few ideas that might be wrong: > 1. In bridge
>         mode, http_access allow CURRENTIPADDRESS  ( CURRENTIPADDRESS >
>         is the dynamic IP address provided the ISP ) Is there a way to
>         obtain this > in the squid.conf file ? > 2. Setup a DHCP
>         server alongside Squid server and have Squid(DHCP) <--> >
>         Router(DHCP, NAT) and have same dhcp address given to the
>         Router in > squid.conf as http_access allow localnet > >
>         Thanks in advance! > > >
>         > _______________________________________________ >
>         squid-users mailing list > squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org> >
>         http://lists.squid-cache.org/listinfo/squid-users
>
>         -----BEGIN PGP SIGNATURE-----
>         Version: GnuPG v2
>
>         iQEbBAEBCAAGBQJVpB7aAAoJENNXIZxhPexGJcgH+IcaMqoEwlcRYFNCWqKT/Msc
>         I6aMD/82Uw5ow/HayX/GrxCHTzYjdCzXDXJTP9cAnHZaMnvOPxtCGuVocEHNEiOa
>         sDsZC9P074hoANDEAYXycWF73auCxYg4jcg8dRtbZwVEazwYsMVN6ye5a3i9EaZM
>         /DotQ78htLNRJrLhoCO9yQBtJObcUs+eyOie4oxk4YWSfQMcjZOXen7U8K8KGQuH
>         cOBcodLJv/eP1T+CcEe3ATr8Szo+zQ648jG27pdy7XuPecek7sWllRnyq93fpkID
>         FnvOr21R3gLBBdStYty43PKQ/4Z3d4vp56aYEweKBsGJV9kVC2QMjDXLOzrbug==
>         =1pgP
>         -----END PGP SIGNATURE-----
>
>
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/00b0cd92/attachment.htm>

From 2005now at mail.ru  Tue Jul 14 06:15:30 2015
From: 2005now at mail.ru (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0KDRg9C60LDQstGG0L7Qsg==?=)
Date: Tue, 14 Jul 2015 09:15:30 +0300
Subject: [squid-users]
 =?utf-8?q?Squid_+_kerberos=2C_all_childrens_are_bus?= =?utf-8?q?y?=
In-Reply-To: <55A40A0F.3060100@treenet.co.nz>
References: <1436435648.897981882@f301.i.mail.ru>
 <1436810053.1887534@f178.i.mail.ru>
 <55A40A0F.3060100@treenet.co.nz>
Message-ID: <1436854530.60924191@f194.i.mail.ru>


>>     >>>>> Hello, i have a problem here :) System - freebsd 10.1, squid 3.5.5 + kerberos (MIT), 50 users total.
>>>     >>>>>
>>>     >>>>> Without any auth my squid works fine, system is not loaded. When i enable Kerberos auth internet slowly goes down and crushing after a while, at logs i see:
>>     >>>>>>
>>     >>>>>> 2015/07/09 11:47:14 kid1| WARNING: All 60/60 negotiateauthenticator processes are busy.
>>     >>>>>> 2015/07/09 11:47:14 kid1| WARNING: 72 pending requests queued
>>>     >>>>>>
>>>     >>>>
>>>     >>>> So 50 users / 60 helpers ... how many requests per second? and how
>>>     >>>> fast/slow is the helper responding?
>>>     >> Could you clarify how I can get value of requests per second and respond?
>>>     >
>>>     >The cachemgr "info" report. From the cachemgr.cgi tool, or "squidclient
>>>     >mgr:info" command line, or
>>>     >http://$visible_hostname:3128/squid-internal-mgr/info
>>>     >
>>>     > Or calculated from a quick count of the access.log lines over a few mins.
>>> 
>>>     ~600 lines per minute,
>>> 
>>> 
>>> 
>>>     >> Debugs show like 3-4 message per second like:
>>>     >>
>>>     >> negotiate_kerberos_auth.cc(783): pid=1456 :2015/07/09 13:26:48| negotiate_kerberos_auth: DEBUG: AF oYGyMIGvoAMKAQChCwYJKoZIgvcSAQICooGaBIGXYIGUBgkqhkiG9xIBAgICAG+BhDCBgaADAgEFoQMCAQ+idTBzoAMCAReibARqY4fSYtg+X4HhiH8dFmWxdn3wxtoKKZzEfUjLYibMoy0XAAWgkSYVXgC7gxO7cgCkOofEqZQhi/GKa4NZqn2dQqOJU/3y4zkPqBP9Ialh//BL5ov03L5BqjgthrbYbrcxJTo57EJIdO8O1g== avialex
>>>     >>
>>>     >> And errors like:
>>>     >> 2015/07/09 13:28:03 kid1| ERROR: Negotiate Authentication validating user. Result: {result=BH, notes={message: received type 1 NTLM token; }}
>>     >>> All my friends get the same error, but their squid is working fine.
>>     >>>
>>
>>
>>Okay, so the traffic arriving at Squid is ~10 req/sec, and the helpers
>>are processing only 4 req/sec successfully.
>>
>>If we assume that also each client connection is attempting one NTLM
>>request before it gets to Kerberos (when it should be doing the
>>opposite). That allows for the helper rejecting 3-4 req/sec.
>>
>>That makes a total of up to 8 req/sec being handled by the helpers.
>>Still leaving 2 req/sec building up in the queue.
>>
>>
>>I see two problems there.
>>
>>Firstly, 3-4 req/sec seems to be a very slow response rate by the
>>helpers. If you can find some way to improve that enough to stop the
>>queue building up your problem should go away.
>>- that might be done by increasing the startup= helpers count (and
>>maximum count)
>>- that might be by improving the helper connectivity speed and access
>>to DNS and the backend AD system.
>>
>>
>>Secondly, that NTLM issue. The only fix for that is to get the client
>>devices configured to try the more secure Kerberos auth first (like they
>>should be doing anyway).
>?
>- that may require disabling NTLM entirely for them.
>>
>>
>>
>>>     >> Don't see anything else
>>>     >>
>>>     >
>>>     >Aha. So your users browsers are sending NTLM auth instead of Kerberos.
>>>     >That is at least one part of the problem. NTLM handshake can take whole
>>     >>seconds and places a lot of extra load on the helpers. To resolve these
>>     >>the users software needs fixing to use Kerberos properly when Negotiate
>> >
>>     >>is offered.
>>     >>
>>     >>The other part is figuring out what amount of helpers is needed to meet
>>     >>the load requirements. With NTLM it is usually several hundred.
>>     >>
>>> 
>> >
>> >When i'm using proxy alone, squid stars 33 childrens, don't recive any NTLM errors, but internet start to lag. So the problem not in the NTLM software. I tryed to start 300 children for my 60 users, but still have huge lags, even when half was free.
>>> 
>>
>>I suggest For 60 users doing 10req/sec I suggest configuring Squid with:
>?
>auth_param negotiate children 500 startup=120 idle=10
>>
>>
>>So what do you think the lag is coming from then?
>>
>>And how are you defining "free" in terms of helpers?
>>
>>Amos
when i started 150/300 children, 
ps -ax | grep negotiate | wc -l 
shows me that only 151 launched but there was lags

So i decided that settings isn't my problem, so only sofware left. So what i did:

I upgraded squid from 3.5.5 to 3.5.6 and changed kerberos realisation from MIT to heimdal and it works perfectly! Only 18 childrens are launched right now, NTLM errors still presents but i already know what soft makes it and will fix it.
Don't know what was wrong with MIT realisation of kerberos on my freebsd server, but heimdal works just fine.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/53900658/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 14 07:48:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 19:48:16 +1200
Subject: [squid-users] Is it possible to tunnelize http traffic?
In-Reply-To: <55A42692.3060207@vianetcon.com.ar>
References: <55A42692.3060207@vianetcon.com.ar>
Message-ID: <55A4BEC0.5020703@treenet.co.nz>

On 14/07/2015 8:58 a.m., Sebastian Goicochea wrote:
> Hello, I'm trying to improve the bypass system we use in our servers.
> When a site is not shown as it should, or something is broken because of
> a poor server's side implementation, we bypass traffic to that server at
> ebtables level. This works just as expected, squid never "sees" this
> traffic, but when you have problems with some site using akamai's cdn ..
> bypassing that network impacts negatively on squid's performance.
> Is it possible to tunnelize (as in TCP_TUNNEL/200) some arbitrary
> traffic? URL regex would be awesome. Tried googling it but could only
> find https related material.

No, TUNNEL means sending a CONNECT message to an upstream proxy - HTTP
forbids sending that to origin servers.

With "transparent interception" the client_dst_passthru feature of Squid
is what adds the transparent part. Relaying traffic to the same IP:port
the client was trying to reach. It is done by default on intercepted
traffic.
<http://www.squid-cache.org/Doc/config/client_dst_passthru/>

Amos



From squid3 at treenet.co.nz  Tue Jul 14 07:50:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 19:50:37 +1200
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <000001d0bdb1$21d8ecf0$658ac6d0$@bluewin.ch>
References: <000001d0bdb1$21d8ecf0$658ac6d0$@bluewin.ch>
Message-ID: <55A4BF4D.9070008@treenet.co.nz>

On 14/07/2015 9:16 a.m., Othmar Truniger wrote:
> Amos,
> would you mind 'pinging' again. The bug is not even confirmed yet.

CC'ing. He responded to me earlier with ath thanks for the info. But
AFAIK has been travelling over the last few weeks.

Amos

> 
>> The FTP handling code was re-written and shuffled around a lot to get
>> FTP native support working. I've 'pinged' the author who did that about
>> your bug report. Hopefully he will have some time to look at it.
>>
>> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jul 14 07:55:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 19:55:24 +1200
Subject: [squid-users] [3.5.6]: assertion failed: store.cc:850:
 "store_status == STORE_PENDING"
In-Reply-To: <55A4532F.8080408@articatech.com>
References: <55A4532F.8080408@articatech.com>
Message-ID: <55A4C06C.9050403@treenet.co.nz>

On 14/07/2015 12:09 p.m., David Touzeau wrote:
> Hi all
> 
> We receive this error in cache.log
> 
> assertion failed: store.cc:850: "store_status == STORE_PENDING"
> 

Means the store code has some data in-transit for the client. But is
trying to overwrite it with something else without going through the
proper cleanup process. IME that usually happens on incorrect connection
closures.

> Just after browser sends "ERR_PROXY_CONNECTION_FAILED"
> 
> What does it means ?

I suspect issues like (in this order of relevance) the so-called "happy
eyeballs" agorithm side effects, ECN or Window Scaling TCP issues, ICMP
blocking or path-MTUd issues, possibly even browser timeouts not being
long enough.

Amos



From squid3 at treenet.co.nz  Tue Jul 14 08:19:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 20:19:03 +1200
Subject: [squid-users] Transparent proxy before NAT
In-Reply-To: <CAKNtY_xjqZr=jMfJD3FAv9VEHkgzYOqt_3MzSSS3+mQzLjLyGw@mail.gmail.com>
References: <CAKNtY_wPp9S4XyJBbXNn35a06X7bAgXg63-hcthBy3nbD12TDQ@mail.gmail.com>
 <55A41EDA.9030109@gmail.com>
 <CAKNtY_wQeCVoXZ9xDz9JTBiEapmxx0OE7E=pp3SJW9CKU=q+9g@mail.gmail.com>
 <CAKNtY_xjqZr=jMfJD3FAv9VEHkgzYOqt_3MzSSS3+mQzLjLyGw@mail.gmail.com>
Message-ID: <55A4C5F7.9010109@treenet.co.nz>

On 14/07/2015 8:34 a.m., John Pearson wrote:
> Thanks Yuri for the response, I understand. I do have Shorewall configured
> and I understand the security implications. My Router is also the Wireless
> AP, so I want to try out this setup without having to buy another Wireless
> AP.
> 
> I don't mind it being complex, do you have any suggestions on getting
> Internet <---> Squid <---> Router (NAT) working ?
> 

You wont ever get that happening. The NAT intercept step between clients
and Squid must happen on the Squid device directly so Squid has access
to the kernel NAT mappings. This is not optional.


The best way if you are able to do it is to turn the Router into a plain
AP point. And the Squid device into router + NAT device.

Its easy enough to setup the Squid device with outgoing NAT rules same
as the router would have used. You can even do DHCP there as well if you
like.



The alternative is to go with the Squid device wired into the router so
traffic flow is:
 clients -> Router (no NAT!) -> Squid -> Router (NAT) -> Internet

"Router" can be one device if you have sufficient control over the
ebtables and iptables rules to split the pre-Squid and post-Squid packet
flows

But that has two big problems when only one router device is used:

 1) Most consumer grade wifi+modem+router devices and even some
commercial grade ones dont support the level of iptables config needed.

 2) With one router device the router<->Squid NIC cards bandwidth
capacity is halved, since all traffic travels over it twice. Likewise
the router CPU cyces for networking are halved.


In both configs setup the clients with the Squid device static IP as
their gateway as Yuri said. The router just happens to be the path they
use to reach the Squid gateway device.

The first config Squid uses Internet uplink directly as its gateway, and
performs NAT MASQUERADE for outgoing traffic.

The second config the Squid device uses the Router as its gateway (same
as the clients would normally have done). Packets go to the Internet via
there.


Amos



> Thanks!
> 
> On Mon, Jul 13, 2015 at 1:33 PM, John Pearson <johnpearson555 at gmail.com>
> wrote:
> 
>> Thanks Yuri for the response, I understand. I do have Shorewall configured
>> and I understand the security implications. My Router is also the Wireless
>> AP, so I want to try out this setup without having to buy another Wireless
>> AP.
>>
>> I don't mind it being complex, do you have any suggestions on getting
>> Internet <---> Squid <---> Router (NAT) working ?
>>
>> Thanks!
>>
>> On Mon, Jul 13, 2015 at 1:26 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
>>
>>>
> Ah,
> 
> forgot about:
> 
> Your squid in scheme I wrote will have static gray IP. And this IP must
> be excluded from DHCP pool on router.
> 
> 14.07.15 2:15, John Pearson ?????:
>>>>> Hi Everyone,
>>>>>
>>>>> My setup is: Internet <--> Squid-eth0 <--> Squid-eth1 <--> Router <-->
>>>>> Devices
>>>>>
>>>>> Currently the Router is doing NAT and DHCP for the devices connected to
> it.
>>>>> Squid is in transparent mode. I set up a bridge ( br0). I set up the
>>>>> ebtables and iptables. It works but I want to figure out a way without
>>>>> having to configure Squid server or Router with hardcoded addresses.
>>>>>
>>>>> I have it working with either setup:
>>>>> 1. Remove the bridge ( br0) and setup the Squid server eth1 as a static
> IP
>>>>> address and set Squid server IP address as gateway in Router settings.
>>>>> 2. Since Squid server is in bridge mode, I can hard code IP address in a
>>>>> Squid ACL as all traffic appears to come this IP address from the
> router.
>>>>>
>>>>> I want a way to do this without any setup, basically to take a Squid box
>>>>> and place it before a Router. Is there a way to do this ?
>>>>>
>>>>> A few ideas that might be wrong:
>>>>> 1. In bridge mode, http_access allow CURRENTIPADDRESS  (
> CURRENTIPADDRESS
>>>>> is the dynamic IP address provided the ISP ) Is there a way to obtain
> this
>>>>> in the squid.conf file ?
>>>>> 2. Setup a DHCP server alongside Squid server and have Squid(DHCP) <-->
>>>>> Router(DHCP, NAT) and have same dhcp address given to the Router in
>>>>> squid.conf as http_access allow localnet
>>>>>
>>>>> Thanks in advance!
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
> 
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jul 14 08:21:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jul 2015 20:21:42 +1200
Subject: [squid-users] Squid + kerberos, all childrens are busy
In-Reply-To: <1436854530.60924191@f194.i.mail.ru>
References: <1436435648.897981882@f301.i.mail.ru>
 <1436810053.1887534@f178.i.mail.ru> <55A40A0F.3060100@treenet.co.nz>
 <1436854530.60924191@f194.i.mail.ru>
Message-ID: <55A4C696.2090808@treenet.co.nz>

On 14/07/2015 6:15 p.m., ??????? ???????? wrote:
> 
> I upgraded squid from 3.5.5 to 3.5.6 and changed kerberos realisation
> from MIT to heimdal and it works perfectly! Only 18 childrens are
> launched right now, NTLM errors still presents but i already know
> what soft makes it and will fix it. Don't know what was wrong with
> MIT realisation of kerberos on my freebsd server, but heimdal works
> just fine.
> 

Aha, good to know it fixed.

I'm as much in the dark as you on those projects internals.

Amos


From philipp.wehling at megatel.de  Tue Jul 14 08:34:27 2015
From: philipp.wehling at megatel.de (Philipp Wehling)
Date: Tue, 14 Jul 2015 10:34:27 +0200 (CEST)
Subject: [squid-users] accessing google.com
In-Reply-To: <55A39D91.6070303@treenet.co.nz>
References: <1920124688.776.1436530881178.JavaMail.zimbra@megatel.de>
 <55A1BD5A.5030400@treenet.co.nz>
 <386647485.4484.1436783973377.JavaMail.zimbra@megatel.de>
 <55A39D91.6070303@treenet.co.nz>
Message-ID: <989593634.6519.1436862867634.JavaMail.zimbra@megatel.de>

Hello,

> Nothing that woudl be helpful Im afraid. Cleaning it up and writing a how-to blog has been on my todo list for years now :-(

Thats too bad. This really would be helpful.

> The debugs output should show you the squid.conf line its processing, then the 0/1/-1 (aka fail, pass, async-delay) results of each individual test run for that line. Then the line result action 0/1/-1 (aka allow, deny, dunno/skip)

I dont see any line number in cache.log - even with 28,9

> The debugs trace will result in a deny action in one of the access control lines. Which then logs something like "Request for ... DENIED due to ...".

I dont see any of this lines either. Same with 28,9. An I forced a ACL-Block.

> There are other things forbidden by the protocol, or errors that can happen which Squid just responds with a DENIED page to. No ACL processing required for that to happen.

This is the way im trying to find out what is causing the problems and pin down the failure.


Im using squid 3.1.22-1 on CentOS 5 - if this is important.



kind regards,
pwe

----- Urspr?ngliche Mail -----
Von: "Amos Jeffries" <squid3 at treenet.co.nz>
An: "Philipp Wehling" <philipp.wehling at megatel.de>
CC: squid-users at lists.squid-cache.org
Gesendet: Montag, 13. Juli 2015 13:14:25
Betreff: Re: [squid-users] accessing google.com

On 13/07/2015 10:39 p.m., Philipp Wehling wrote:
> Hello,
> 
> thank you for your answer.
> 
>> This is where you really do need to understand the 28,3 debug output.
> 
> I thought squid works in a "first-match"-manner... Do you have any documentation, I can read about?

Nothing that woudl be helpful Im afraid. Cleaning it up and writing a
how-to blog has been on my todo list for years now :-(

Its currently a matter of following the trace and eyeballing what it
says its doing vs what your squid.conf contains.

The debugs output should show you the squid.conf line its processing,
then the 0/1/-1 (aka fail, pass, async-delay) results of each individual
test run for that line.
Then the line result action 0/1/-1 (aka allow, deny, dunno/skip)


> 
>> That is of course following your assumption that Squid is actively rejecting the traffic with ACLs (you provide no evidence of that).
> 
> How can I proof that? When the error comes up, I get the default ACL-Block-Page.

The debugs trace will result in a deny action in one of the access
control lines. Which then logs something like "Request for ... DENIED
due to ...".

There are other things forbidden by the protocol, or errors that can
happen which Squid just responds with a DENIED page to. No ACL
processing required for that to happen.

> 
>> Chances are rather high these days that it has nothing to do with Squid since Google prefer to use HTTPS, QUIC, SPDY, or HTTP/2 protocols. All of which normally bypass Squid processing entirely.
> 
> That answer is interesting, because we have some company (blocking) policies regarding drive.google.com and everything works fine. How is it possible, that google.com bypasses these rules? Please give me more hints for my investigation.
> 

They hard-code the Chrome web browser and other apps or scripts they
produce to try the other protocols first when connnecting to their own
servers. HTTP messages coming out of their servers also contain headers
such as Alternative-Protocol or Alt-Svc requesting the cleint software
uses non-HTTP protocols.

The latest release of Squid automatically strip the HTTP headers away on
related responses. You still have to also firewall block ports UDP/443
and UDP/80 to avoid QUICK protocol. Only HTTPS decryption will prevent
HTTP/2, SPDY, and WebSockets usage since they are either inside TLS or
CONNECT tunnels.

Amos


From eliezer at ngtech.co.il  Tue Jul 14 09:13:08 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 14 Jul 2015 12:13:08 +0300
Subject: [squid-users] RPM for 3.5.6 CentOS 6.x
In-Reply-To: <55A35CD8.1050007@treenet.co.nz>
References: <CAEaSS0YLD0rZx+KdVZuPwjWFJEMNL+tQyDVibKarUpNx2uRJ9A@mail.gmail.com>
 <55A35CD8.1050007@treenet.co.nz>
Message-ID: <55A4D2A4.70408@ngtech.co.il>

Hey,

It is avaliable for both 6 and 7 centos.

Eliezer

On 13/07/2015 09:38, Amos Jeffries wrote:
> Eliezer mentioned having 3.5.6 RPMs available last night. I'm not sure
> if CentOS 6 was included in that first bunch, but it wouldn'y hirt to
> check with him.
>
> Amos





From tarotapprentice at yahoo.com  Tue Jul 14 12:59:34 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 14 Jul 2015 12:59:34 +0000 (UTC)
Subject: [squid-users] Which DNS to use
Message-ID: <233980444.1823852.1436878774304.JavaMail.yahoo@mail.yahoo.com>

 I have a domestic DSL router/firewall/wifi
When configuring squid should I refer to the routers IP address for DNS,?specify the one(s) the ISP?providing or?use alternative ones (like the examples in squid.conf). Is there a recommend way?
MarkJ
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150714/3530c78b/attachment.htm>

From Hullen at t-online.de  Tue Jul 14 16:10:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Tue, 14 Jul 2015 18:10:00 +0200
Subject: [squid-users] 3.5.6 compile error in Helper ServerBase
In-Reply-To: <>
Message-ID: <DKq$QuiPCXB@helmut.hullen.de>

Hallo, squid-users,

I've just tried to compile squid 3.5.6 using the script from  
slackbuilds.org.

        http://slackbuilds.org/slackbuilds/14.1/network/squid.tar.gz

With squid 3.4.x (and older versions) this script worked well. With  
3.5.6 compiling stops with

---------

In file included from Reply.cc:14:
../../src/helper.h:134: error: base `HelperServerBase' with only non- 
default constructor in class without a constructor
../../src/helper.h:147: error: base `HelperServerBase' with only non-default constructor in class without a constructor

make[3]: *** [Reply.lo] Error 1

[...]

---------

Where is the problem?

Viele Gruesse!
Helmut



From squid3 at treenet.co.nz  Tue Jul 14 16:20:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 04:20:57 +1200
Subject: [squid-users] Which DNS to use
In-Reply-To: <233980444.1823852.1436878774304.JavaMail.yahoo@mail.yahoo.com>
References: <233980444.1823852.1436878774304.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55A536E9.2050203@treenet.co.nz>

On 15/07/2015 12:59 a.m., TarotApprentice wrote:
>  I have a domestic DSL router/firewall/wifi
> When configuring squid should I refer to the routers IP address for DNS, specify the one(s) the ISP providing or use alternative ones (like the examples in squid.conf). Is there a recommend way?

There should not be any example of DNS servers configured in squid.conf.
Because configuring DNS servers directly in squid.conf is not a good idea.

Squid will auto-configure itself via /etc/resolv.conf. That in turn
should be auto-configured by the router DHCP or RAD the same way as your
clients DNS is auto-configured.

Amos



From squid3 at treenet.co.nz  Tue Jul 14 16:37:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 04:37:21 +1200
Subject: [squid-users] 3.5.6 compile error in Helper ServerBase
In-Reply-To: <DKq$QuiPCXB@helmut.hullen.de>
References: <DKq$QuiPCXB@helmut.hullen.de>
Message-ID: <55A53AC0.9060704@treenet.co.nz>

On 15/07/2015 4:10 a.m., Helmut Hullen wrote:
> Hallo, squid-users,
> 
> I've just tried to compile squid 3.5.6 using the script from  
> slackbuilds.org.
> 
>         http://slackbuilds.org/slackbuilds/14.1/network/squid.tar.gz
> 
> With squid 3.4.x (and older versions) this script worked well. With  
> 3.5.6 compiling stops with
> 
> ---------
> 
> In file included from Reply.cc:14:
> ../../src/helper.h:134: error: base `HelperServerBase' with only non- 
> default constructor in class without a constructor
> ../../src/helper.h:147: error: base `HelperServerBase' with only non-default constructor in class without a constructor
> 
> make[3]: *** [Reply.lo] Error 1
> 
> [...]
> 
> ---------
> 
> Where is the problem?

What compiler version?

Amos


From david at articatech.com  Tue Jul 14 18:02:08 2015
From: david at articatech.com (David Touzeau)
Date: Tue, 14 Jul 2015 20:02:08 +0200
Subject: [squid-users] [3.5.6]: assertion failed: store.cc:850:
 "store_status == STORE_PENDING"
In-Reply-To: <55A4C06C.9050403@treenet.co.nz>
References: <55A4532F.8080408@articatech.com> <55A4C06C.9050403@treenet.co.nz>
Message-ID: <55A54EA0.6000404@articatech.com>


I understand the relationship with connection closures but:

When store_status == STORE_PENDING appears, the proxy is crashing and nobody can surf.
We have tried without define caches in squid but issue still occurs.

I'm agree with broken connections, but why nobody can surf after this error ans we must restart squid.



Le 14/07/2015 09:55, Amos Jeffries a ?crit :
> On 14/07/2015 12:09 p.m., David Touzeau wrote:
>> Hi all
>>
>> We receive this error in cache.log
>>
>> assertion failed: store.cc:850: "store_status == STORE_PENDING"
>>
> Means the store code has some data in-transit for the client. But is
> trying to overwrite it with something else without going through the
> proper cleanup process. IME that usually happens on incorrect connection
> closures.
>
>> Just after browser sends "ERR_PROXY_CONNECTION_FAILED"
>>
>> What does it means ?
> I suspect issues like (in this order of relevance) the so-called "happy
> eyeballs" agorithm side effects, ECN or Window Scaling TCP issues, ICMP
> blocking or path-MTUd issues, possibly even browser timeouts not being
> long enough.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Jul 14 18:16:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 06:16:52 +1200
Subject: [squid-users] [3.5.6]: assertion failed: store.cc:850:
 "store_status == STORE_PENDING"
In-Reply-To: <55A54EA0.6000404@articatech.com>
References: <55A4532F.8080408@articatech.com> <55A4C06C.9050403@treenet.co.nz>
 <55A54EA0.6000404@articatech.com>
Message-ID: <55A55214.4050207@treenet.co.nz>

On 15/07/2015 6:02 a.m., David Touzeau wrote:
> 
> I understand the relationship with connection closures but:
> 
> When store_status == STORE_PENDING appears, the proxy is crashing and
> nobody can surf.
> We have tried without define caches in squid but issue still occurs.
> 
> I'm agree with broken connections, but why nobody can surf after this
> error ans we must restart squid.
> 

There have been a few different bugs leading to that assert over the years.

We curently have <http://bugs.squid-cache.org/show_bug.cgi?id=3968>
unresolved. But a backtrace or replicating it manually is needed to know
if you are hitting that bug or something else.

Amos



From Hullen at t-online.de  Tue Jul 14 18:34:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Tue, 14 Jul 2015 20:34:00 +0200
Subject: [squid-users] 3.5.6 compile error in Helper ServerBase
In-Reply-To: <55A53AC0.9060704@treenet.co.nz>
Message-ID: <DKq$nd6uCXB@helmut.hullen.de>

Hallo, Amos,

Du meintest am 15.07.15:

>> I've just tried to compile squid 3.5.6 using the script from
>> slackbuilds.org.
>>
>>         http://slackbuilds.org/slackbuilds/14.1/network/squid.tar.gz
>>
>> With squid 3.4.x (and older versions) this script worked well. With
>> 3.5.6 compiling stops with
>>
>> ---------
>>
>> In file included from Reply.cc:14:
>> ../../src/helper.h:134: error: base `HelperServerBase' with only
>> non- default constructor in class without a constructor
>> ../../src/helper.h:147: error: base `HelperServerBase' with only
>> non-default constructor in class without a constructor
>>
>> make[3]: *** [Reply.lo] Error 1
>>
>> [...]
>>

> What compiler version?


Thanks for that hint!

Changing from one of my machines, with gcc-3.4.6 (for older  
installations) to another machine with gcc-4.9.2 solved the problem.  
Compiling worked, and the new squid version seems to work as expected.  
Nice!

Viele Gruesse!
Helmut



From david at articatech.com  Tue Jul 14 19:08:56 2015
From: david at articatech.com (David Touzeau)
Date: Tue, 14 Jul 2015 21:08:56 +0200
Subject: [squid-users] [3.5.6]: assertion failed: store.cc:850:
 "store_status == STORE_PENDING"
In-Reply-To: <55A55214.4050207@treenet.co.nz>
References: <55A4532F.8080408@articatech.com> <55A4C06C.9050403@treenet.co.nz>
 <55A54EA0.6000404@articatech.com> <55A55214.4050207@treenet.co.nz>
Message-ID: <55A55E48.7070000@articatech.com>

We have found the cause and it should be interesting if you explain to 
us why this error.

We have created and external_acl

external_acl_type FileWatcherExt ttl=0 negative_ttl=0 
children-startup=20 children-idle=1 children-max=5 ipv4 ipv4 %% 
%EXT_USER %SRC %SRCEUI48 %>ha{X-Forwarded-For} %URI %<h{Content-Type} 
%<h{Content-Disposition} %<h{Content-Length} /usr/sbin/external_acl
acl FileWatcher external FileWatcherExt
http_reply_access deny FileWatcher all

We did not have any crash of the helper
When removing the helper, the error did not occurs.


Any tips ?


Le 14/07/2015 20:16, Amos Jeffries a ?crit :
> On 15/07/2015 6:02 a.m., David Touzeau wrote:
>> I understand the relationship with connection closures but:
>>
>> When store_status == STORE_PENDING appears, the proxy is crashing and
>> nobody can surf.
>> We have tried without define caches in squid but issue still occurs.
>>
>> I'm agree with broken connections, but why nobody can surf after this
>> error ans we must restart squid.
>>
> There have been a few different bugs leading to that assert over the years.
>
> We curently have <http://bugs.squid-cache.org/show_bug.cgi?id=3968>
> unresolved. But a backtrace or replicating it manually is needed to know
> if you are hitting that bug or something else.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul 15 03:48:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 15:48:55 +1200
Subject: [squid-users] 3.5.6 compile error in Helper ServerBase
In-Reply-To: <DKq$nd6uCXB@helmut.hullen.de>
References: <DKq$nd6uCXB@helmut.hullen.de>
Message-ID: <55A5D827.4050908@treenet.co.nz>

On 15/07/2015 6:34 a.m., Helmut Hullen wrote:
> Hallo, Amos,
> 
> Du meintest am 15.07.15:
> 
>>> I've just tried to compile squid 3.5.6 using the script from
>>> slackbuilds.org.
>>>
>>>         http://slackbuilds.org/slackbuilds/14.1/network/squid.tar.gz
>>>
>>> With squid 3.4.x (and older versions) this script worked well. With
>>> 3.5.6 compiling stops with
>>>
>>> ---------
>>>
>>> In file included from Reply.cc:14:
>>> ../../src/helper.h:134: error: base `HelperServerBase' with only
>>> non- default constructor in class without a constructor
>>> ../../src/helper.h:147: error: base `HelperServerBase' with only
>>> non-default constructor in class without a constructor
>>>
>>> make[3]: *** [Reply.lo] Error 1
>>>
>>> [...]
>>>
> 
>> What compiler version?
> 
> 
> Thanks for that hint!
> 
> Changing from one of my machines, with gcc-3.4.6 (for older  
> installations) to another machine with gcc-4.9.2 solved the problem.  
> Compiling worked, and the new squid version seems to work as expected.  
> Nice!

Cheers.

For the record (and anyone else finding this). Squid-3 requires GCC 4.x
to build.

Amos



From vdoctor at neuf.fr  Wed Jul 15 06:56:17 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 14 Jul 2015 23:56:17 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
Message-ID: <1436943377920-4672209.post@n4.nabble.com>

Hi All,

I face a weird issue regarding DISKS cache-dir model and I would like to
have your expertise here 

Here is the result of a cache object with an AUFS cache_dir:
1436916227.603    462 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
application/octet-stream 0x30

Now, here is the same object from the same Squid box but using the DISKD
cache_dir:
1436916293.648  24281 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
application/octet-stream 0x30

Do you see something weird ?
This is the same Squid (3.5.5), I just changed from AUFS to DISKD and
restarted the Squid...

Same object from the cache but *0.462 sec* in AUFS and *24.281 sec* in
DISKD.
52 times more fast in AUFS, why ?

Any idea to speed the diskd up or at least reduce it ?
I could understand the response times could not be the same, but here this
is the Grand Canyon !

My cache_dir option used in test:
cache_dir diskd /var/spool/squid3w1 190780 16 256 min-size=0
max-size=293038080


Thanks in advance for your input...

Bye Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Wed Jul 15 07:58:45 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 09:58:45 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436943377920-4672209.post@n4.nabble.com>
Message-ID: <416463990.30385884.1436947125524.JavaMail.root@zimbra4-e1.priv.proxad.net>

Very Interesting, I would add that I wonder which storage scheme in 2015 should we use to give better performance with recent hardware and high load (more than 800 r/s) ?
Have any recent benchmark somewhere ? 

In my case I'm using diskd with some system tuning, noatime, separate disks for caches, etc, but maybe it's time to move ...
I tried rock store before, there are two or three years maybe, but the performances and the stability was better with diskd.  


From yvoinov at gmail.com  Wed Jul 15 09:37:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 15:37:23 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436943377920-4672209.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
Message-ID: <55A629D3.40005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
The key question: which OS using?

15.07.15 12:56, Stakres ?????:
> Hi All,
>
> I face a weird issue regarding DISKS cache-dir model and I would like to
> have your expertise here
>
> Here is the result of a cache object with an AUFS cache_dir:
> 1436916227.603    462 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
> GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
> application/octet-stream 0x30
>
> Now, here is the same object from the same Squid box but using the DISKD
> cache_dir:
> 1436916293.648  24281 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
> GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
> application/octet-stream 0x30
>
> Do you see something weird ?
> This is the same Squid (3.5.5), I just changed from AUFS to DISKD and
> restarted the Squid...
>
> Same object from the cache but *0.462 sec* in AUFS and *24.281 sec* in
> DISKD.
> 52 times more fast in AUFS, why ?
>
> Any idea to speed the diskd up or at least reduce it ?
> I could understand the response times could not be the same, but here this
> is the Grand Canyon !
>
> My cache_dir option used in test:
> cache_dir diskd /var/spool/squid3w1 190780 16 256 min-size=0
> max-size=293038080
>
>
> Thanks in advance for your input...
>
> Bye Fred
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpinTAAoJENNXIZxhPexG5+YIAKlhTb4MzzlDCODWSHfKfght
DOkrJ8vlDhMMLspmOYF/SuUz2c/FV47OztQOVAK2QYHRqkWF7j3QVwAcVSA4RjmL
7BZZPdpNOhLnFLM0zKsSkUTBz69+igvaiISEC9mGL/d1u0qMmq8KC+HsRac71w2m
T4obhGuNljhDn4kO45reK5888LToMEF/XL5X8j27o0Xm1GDsrGQKbNSjHINbKaPB
hW0VlSC8pYC9pn7kxIuhVBhar6qbGKCT/fzXVIN36PgG1bsvcX9CZ60fg3kIKzO0
PDFqZoQIZIwGhtQyr2FCTeK6V0F1FdYK9JC+TjdCzHjtzFH5oS+p7s/KAA48+AQ=
=wpjs
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Wed Jul 15 09:40:02 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 02:40:02 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A629D3.40005@gmail.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com>
Message-ID: <1436953202097-4672212.post@n4.nabble.com>

Yuri,

Debian 7 or 8, tested on both...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672212.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jul 15 09:43:47 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 15:43:47 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436953202097-4672212.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com> <1436953202097-4672212.post@n4.nabble.com>
Message-ID: <55A62B53.7030306@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
DIskd works perfectly on some OS'es, like Solaris, BSD.

Linux-based OS, AFAIK, works with diskd so slow. And AUFS is the best
choise in this case. Depending system settings, of course.

AFAIK, on some OS (like.....hmmmm..... Windows) "aufs" leads "queue
congestion" issue and must be change to "ufs".

15.07.15 15:40, Stakres ?????:
> Yuri,
>
> Debian 7 or 8, tested on both...
>
> Bye Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672212.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpitTAAoJENNXIZxhPexGN0UH/35LtFFfOVEiQnXb/EjKA/nb
X7MRQmra01i52EiIecLntDEDUJhTpnV4C6Yc4VvcLtab/YKfDL7BQfHZSm4lSf9E
So6K4g80XdSzpTwoY8LGbPL52rC6rCEyvboBa+e/FMedThNeAIcOUIwzAennKSCC
qE9Xe/+QlvEPg3FqZcHGXI3Gbab9n6BsOEwSuTP6HY3Qo3LLrokvISaeHkcDl1oj
Mi2m7IcyzLoMOxNRf7f14JJgNZLLXDGU5EqSVSj7Cizbo9Tx/nbRPmz4dymGrmg5
Ih5mco2mQa4+IwooqzMvAHRbJ8UMzRi1nr0Yp/gHlDnPn8sUA9RuDxxWe4EruPI=
=DmAy
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Wed Jul 15 09:59:59 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 02:59:59 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A62B53.7030306@gmail.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com> <1436953202097-4672212.post@n4.nabble.com>
 <55A62B53.7030306@gmail.com>
Message-ID: <1436954399177-4672214.post@n4.nabble.com>

Yury,

you mean that having the DISKD 52 times slower then AUFS with linux OS is
normal ?
I cannot believe that, incredible !

I could understand the double or the triple, but here we're speaking about
50+ times... 

Fred.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672214.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jul 15 10:03:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 16:03:29 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436954399177-4672214.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com> <1436953202097-4672212.post@n4.nabble.com>
 <55A62B53.7030306@gmail.com> <1436954399177-4672214.post@n4.nabble.com>
Message-ID: <55A62FF1.10906@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Are you surprised that the IO modules may be specific for different
operating systems? :)


15.07.15 15:59, Stakres ?????:
> Yury,
>
> you mean that having the DISKD 52 times slower then AUFS with linux OS is
> normal ?
> I cannot believe that, incredible !
>
> I could understand the double or the triple, but here we're speaking about
> 50+ times...
>
> Fred.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672214.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpi/xAAoJENNXIZxhPexGtEgH/3aS30ns7n7P3u9vASS5NH7N
SZ7HIF3PHwIdZshFlRWefC9Jb/2IY1jWyA5FmW96JGl+GiCNLt13P9FVnasN3e3D
/AbusJXW/uUHIpZ8OXh+ZaYulCCvh8JYv5iSTum50iscVTpE05lpIbaykVp0SLu8
6zjNkf49vjjgQmCVIjDRjuBeRkW/fKzlyk8VI+dq6Mm0uOoP3OP9AoAPEVA4VcLt
JJ5ZzO7y2+s4tSvri+sCenKqShxbC+ORWlNOKLVmC98rUI7RrI65t1jAbpgcPQbZ
3JA0JdTMbvUeFAS5KhyEW3ekfAv9QXp/UiwZdtG7Om5r1FBmwu5u4wx6f2IF3qA=
=GsEk
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 10:08:30 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 16:08:30 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436954399177-4672214.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com> <1436953202097-4672212.post@n4.nabble.com>
 <55A62B53.7030306@gmail.com> <1436954399177-4672214.post@n4.nabble.com>
Message-ID: <55A6311E.4090805@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also - did you read this:

http://wiki.squid-cache.org/Features/DiskDaemon

?

Your seen, for which OS this feature designed? ;)

15.07.15 15:59, Stakres ?????:
> Yury,
>
> you mean that having the DISKD 52 times slower then AUFS with linux OS is
> normal ?
> I cannot believe that, incredible !
>
> I could understand the double or the triple, but here we're speaking about
> 50+ times...
>
> Fred.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672214.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpjEeAAoJENNXIZxhPexGDr0IAKMmw0s6oTeU2atiS5O3KeAt
GFdTyzq+zm9GUAvuxUlxbpqd+YeFcC7sR3+tjGShQREZYpm7phWJACDQalnsAS61
GSxZCz3sRBXCwWOnCQWkg5sYbqae+UweTMButHQkfyQ2xDt9gdbFwhJUJ7Cs3WO2
erjma4INdGSGIOX1n6S9LArPYS33b0Yf1uuvxElgZXYsLoRT/3N/ZX/Y1smVqMsK
bbmz3LNcVHJ/xPHMjm7/JkTyIdf67hkaGJOfcIAWgV647kXe8P/AhReEL8vlhXcW
S0bh8DGJPi5UxF72IEFy7AaxFBMbCL8f775dftEPccXlQfVeamAp8l6edPNhyZk=
=iSrO
-----END PGP SIGNATURE-----



From zvucetic at gmail.com  Wed Jul 15 10:09:32 2015
From: zvucetic at gmail.com (=?UTF-8?B?xb1lbGprbyBWdcSNZXRpxIc=?=)
Date: Wed, 15 Jul 2015 12:09:32 +0200
Subject: [squid-users] Squid transparent proxy - IP and MAC address spoofed
	(unchanged)
Message-ID: <CAF+14A2S_GNrkAws6OKJPpe3s5UQD9B7N90gucADvGT-9irBUA@mail.gmail.com>

Hi all,

Is it possible to spoof both IP and MAC addres of the http packets with the
squid?

With squid 3.3. I managed to spoof IP source address to be the client's
one, but mac address is still being changed.

(This manual wiki.squid-cache.org/Features/Tproxy4 )

Did someone manage to do something like this? Is it even possible with
squid?

We are trying to bring up SDN service chain with squid in it, so the mac
addresses must stay the same all the way.

Thank you in advance.

?eljko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150715/71899161/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 15 10:27:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 22:27:53 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436954399177-4672214.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A629D3.40005@gmail.com> <1436953202097-4672212.post@n4.nabble.com>
 <55A62B53.7030306@gmail.com> <1436954399177-4672214.post@n4.nabble.com>
Message-ID: <55A635A9.4020405@treenet.co.nz>

On 15/07/2015 9:59 p.m., Stakres wrote:
> Yury,
> 
> you mean that having the DISKD 52 times slower then AUFS with linux OS is
> normal ?
> I cannot believe that, incredible !
> 
> I could understand the double or the triple, but here we're speaking about
> 50+ times... 

Yes. Exactly so.

The difference is threading:
 diskd has 1 I/O thread.
 AUFS has 64 I/O threads.

On a 1:1 thread comparison AUFS logics is very slightly slower, but in
aggregate its orders of magnitude faster.

Amos



From fredbmail at free.fr  Wed Jul 15 10:46:14 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 12:46:14 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A6311E.4090805@gmail.com>
Message-ID: <1255111276.30858878.1436957174491.JavaMail.root@zimbra4-e1.priv.proxad.net>

Just a little word about aufs, just for information, to avoid 

squidaio_queue_request: WARNING - Queue congestion
squidaio_queue_request: WARNING - Queue congestion
squidaio_queue_request: WARNING - Queue congestion
squidaio_queue_request: WARNING - Queue congestion


I had increase this value (sorry I can't remember the exact number) 

      --with-aufs-threads=xxx

But the "load average" - top counter - was very important and I should back to diskd
So only diskd could works good (rock store was worst than aufs ...), with my load and my usage, but perhaps I should test with a more recent version   

Tested on Debian 


From david at articatech.com  Wed Jul 15 11:03:44 2015
From: david at articatech.com (David Touzeau)
Date: Wed, 15 Jul 2015 13:03:44 +0200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1255111276.30858878.1436957174491.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1255111276.30858878.1436957174491.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A63E10.2000008@articatech.com>

Your are right fred,

It is is a difficult deal for us too...

aufs -> good speed but more troubles ( assertion failed, "empty()", HTTP 
reply without date.... unstable rock system ) and must deal with squid 
crashes ( watchdog)

diskd -> more stable but slower...



Le 15/07/2015 12:46, FredB a ?crit :
> Just a little word about aufs, just for information, to avoid
>
> squidaio_queue_request: WARNING - Queue congestion
> squidaio_queue_request: WARNING - Queue congestion
> squidaio_queue_request: WARNING - Queue congestion
> squidaio_queue_request: WARNING - Queue congestion
>
>
> I had increase this value (sorry I can't remember the exact number)
>
>        --with-aufs-threads=xxx
>
> But the "load average" - top counter - was very important and I should back to diskd
> So only diskd could works good (rock store was worst than aufs ...), with my load and my usage, but perhaps I should test with a more recent version
>
> Tested on Debian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From fredbmail at free.fr  Wed Jul 15 11:18:31 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 13:18:31 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A63E10.2000008@articatech.com>
Message-ID: <906479546.30941336.1436959111883.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Your are right fred,
> 
> It is is a difficult deal for us too...
> 
> aufs -> good speed but more troubles ( assertion failed, "empty()",
> HTTP
> reply without date.... unstable rock system ) and must deal with
> squid
> crashes ( watchdog)


You mean "rock store" or aufs ? 
For me aufs seems stable too but slower than diskd after a point, something like beyond 400 r/s 

Rock store was very unstable -> Again, my tests are not very recent, do you make a try with 3.5.x ? 

I found this, my exact value was --with-aufs-threads=128, but "squidaio_queue_request: WARNING - Queue congestion" was just less present ...

> 
> diskd -> more stable but slower...
> 
> 

Very very stable, no error, no warning 



From yvoinov at gmail.com  Wed Jul 15 11:21:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 17:21:04 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <906479546.30941336.1436959111883.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <906479546.30941336.1436959111883.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A64220.4040302@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.07.15 17:18, FredB ?????:
>
>> Your are right fred,
>>
>> It is is a difficult deal for us too...
>>
>> aufs -> good speed but more troubles ( assertion failed, "empty()",
>> HTTP
>> reply without date.... unstable rock system ) and must deal with
>> squid
>> crashes ( watchdog)
>
>
> You mean "rock store" or aufs ?
> For me aufs seems stable too but slower than diskd after a point,
something like beyond 400 r/s
>
> Rock store was very unstable -> Again, my tests are not very recent,
do you make a try with 3.5.x ?
>
> I found this, my exact value was --with-aufs-threads=128, but
"squidaio_queue_request: WARNING - Queue congestion" was just less
present ...
>
>>
>> diskd -> more stable but slower...
Just use fast separate physical devices on separate controllers - and
all will be ok without any delays.

>>
>>
>>
>
> Very very stable, no error, no warning
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpkIfAAoJENNXIZxhPexGwZ4H/ikmHKulsvCaAp/0tYjheqzM
CuaFKSi0ZB+E4a/mX9SbfkXkU+M/soVmZoz0pMVZTOH/EzpfQuRTbWcK7AMBda7v
N7lyblQuDlqlJHWxE2KbKnRg5EYKSgSBusPrCeVsIfml2jHdkUXnS5XukcofQlwE
CFFGNTMVVZVxPZJLM8INwDUoSY2nH9xVp64oKp6CEU5nmsbSQ0yVUymCK6FPga51
xJnGj0R92pHgQcQSdKjvXSFVL3i1AdRex2hbJhwobQl4pEdFnUoVbZ4xNBtWB/eY
g4aUNTEHoCIUQm+3LBO1nNks6VQ+gGwJDmZ2fOGNn2VgDOINUPx4i4ZKZn0c6rY=
=bVts
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Jul 15 11:32:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jul 2015 23:32:21 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436943377920-4672209.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
Message-ID: <55A644C5.1090707@treenet.co.nz>

On 15/07/2015 6:56 p.m., Stakres wrote:
> Hi All,
> 
> I face a weird issue regarding DISKS cache-dir model and I would like to
> have your expertise here 
> 
> Here is the result of a cache object with an AUFS cache_dir:
> 1436916227.603    462 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
> GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
> application/octet-stream 0x30
> 
> Now, here is the same object from the same Squid box but using the DISKD
> cache_dir:
> 1436916293.648  24281 192.168.1.88 00:0c:29:6e:2c:99 TCP_HIT/200 10486356
> GET http://proof.ovh.net/files/10Mio.dat - HIER_NONE/-
> application/octet-stream 0x30
> 
> Do you see something weird ?
> This is the same Squid (3.5.5), I just changed from AUFS to DISKD and
> restarted the Squid...
> 
> Same object from the cache but *0.462 sec* in AUFS and *24.281 sec* in
> DISKD.
> 52 times more fast in AUFS, why ?
> 

Now theres a whole tale;

(NP: years are rough guesses)

[1980's]
In the beginning Squid was a single threaded process that did its own
HDD I/O access using fread()/fwrite() OSIX API calls to access a cache
format called UFS (Unix File System) with stored content.

Squid could then race at speeds of up to 50 req/sec on the hardware of
that day. In the ages of 9600 baud modems this was fine, but time flew
by, users increased in numbers and also went on to faster 56K modem
technology.

[1990's]
Experiments were done and it was shown that moving the I/O into a
separate process (a helper daemon) was 4x faster. Even though the file
data was copied in memory between the helper and Squid before relaying
to the users. Simply pushing the I/O load into a helper increased Squids
ability to seervice more requests. This was named diskd (Disk Daemon),
an alternative for UFS.

Squid could then race at speeds of up to 200 req/sec on the hardware of
those newer days. But time continues, user numbers kept increasing and
multi-MB speed DSL technology came along.

[2000's]
Multi-threaded applications became popular in mainstream computing. A
new module was added that pulled the I/O code back into the main Squid
process memory, but used threads to split the network and disk I/O
processing apart. (So Squid was no longer truely single-threaded
although people would continue for more than a decade making false
claims of such). This avoided the memory copying between Squid and diskd
helpers, and enabled much more parallel access to the HDD in
asynchronous operations. This was named AUFS (Asynchronous UFS).

Squid could then race at speeds of up to 600 req/sec on the hardware of
those newer days. But time continues still, user numbers keep on
increasing and multi-GB speed Fibre technology came along.

Hardware speeds have since doubled and tripled the basic achievable
speeds for each of these I/O methods. UFS achieving 150-200 req/sec,
diskd 200-300 req/sec and AUFS exploding up towards 600 req/sec on
multi-core CPUs. But still it was not enough. Too much time was spent in
waiting for disk reads.

[2005-ish]
Some people had the idea that all this waiting per-file was the cause of
their trouble and experimented with exotic things.

First came the Cyclic Object Storage System (COSS) which took responses
and stored them in great big groups. Reading and writing to the disk
only by the thousand. Tricky this was, with many bugs, and timing of
resposes began to matter a lot. Nevertheless despite those problems with
most I/O now only involving RAM Squid could reach speeds of nearly 900
req/sec.

Sadly these were also the years of the Squid-2.6/2.7 forking. The
Squid-3 version of COSS never saw many bug fixes implemeted for those
versions and was quite bad.

[2010's]
Later more experiments were done and a database storage system was
implemented (Rock [which does not stand for anything I'm aware of]).
Which contained all the benefits of memory access instead of disk for
most I/O, disk I/O for objects by the thousand, and also a more
predictable memory location for each regardless of whether it was
actually in memory or on disk.

At the same time multi-core processor support was being added to Squid
in the form of multi-process support. So the Rock storage was again
split off into a separate daemon-like process (Disker), but this time
utilizing threads and memory sharing as well.

Todays Squid can reach upwards of 2000 req/sec for a single
worker+Disker [on fairly average hardware]. Or upwards of 8000 req/sec
with several workers and Diskers on high end hardware.



> Any idea to speed the diskd up or at least reduce it ?

Nope. It's already going as fast as its little single-thread can race.

> I could understand the response times could not be the same, but here this
> is the Grand Canyon !
> 

:-) Have pity on those poor Windows admin then. Capped out at 200req/sec
with direct UFS I/O. *and* having to share a mere 2048 sockets between
both disk and network I/O.


> My cache_dir option used in test:
> cache_dir diskd /var/spool/squid3w1 190780 16 256 min-size=0
> max-size=293038080
> 


Most of your slowdown will be on small 0-32KB sized disk hits.

If you graph your traffic profile (count of objects at each size) you
will see a couple of significant bumps. One for small objects down
around 0-32KB range. One for larger media objects up around 5-20 MB.
These days maybe smaller bump(s) [wiggles?] around the 100-700 KB range.


Current recommendation is to use a Rock cache for the small objects. And
one or two of whichever is your OS fastest UFS-based cache type for the
larger ones.

If you have a significant number of those mid-range KB objects that need
high speeds, then an extra Rock cache tuned for larger cell sizes and
only storing those big-ish objects might also be useful.

Amos


From fredbmail at free.fr  Wed Jul 15 11:35:14 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 13:35:14 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A64220.4040302@gmail.com>
Message-ID: <1605985180.30997422.1436960114371.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Just use fast separate physical devices on separate controllers - and
> all will be ok without any delays.
> 

Of course, with this kind of load without separate disks Squid dies after some minutes :) 
I'm using separates drives with noatime file system and I never found a way to (completely) remove warning message from aufs  

I'm making a test just now

Diskd, 600 r/s, squid CPU usage = 40 %, load average 1, no warning in cache/kernel/syslog logs
Aufs, 600 r/s, squid CPU usage = 45 %, load average 3, many 'Queue congestion'

And no gain for hits % of all requests and bytes sent from squid cache

Squid 3.4.13 and Debian 


From fredbmail at free.fr  Wed Jul 15 11:39:05 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 13:39:05 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1605985180.30997422.1436960114371.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <64906346.31006473.1436960345945.JavaMail.root@zimbra4-e1.priv.proxad.net>


> I'm making a test just now
> 
> Diskd, 600 r/s, squid CPU usage = 40 %, load average 1, no warning in
> cache/kernel/syslog logs
> Aufs, 600 r/s, squid CPU usage = 45 %, load average 3, many 'Queue
> congestion'
> 
> And no gain for hits % of all requests and bytes sent from squid
> cache
> 
> Squid 3.4.13 and Debian
> _______________________________________________


Argh ! After some minutes 


2015/07/15 13:13:29 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:13:48 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:14:27 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:15:54 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:18:28 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:25:39 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/15 13:36:07 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory
2015/07/15 13:36:07 kid1| 	/cache2/1F/F9/007DF243


So I go back to diskd 


From vdoctor at neuf.fr  Wed Jul 15 11:41:13 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 04:41:13 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A644C5.1090707@treenet.co.nz>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz>
Message-ID: <1436960473258-4672226.post@n4.nabble.com>

Hi Amos,

Sorry but the Rock mode is totaly bugged, the worst mode to use here.
We did tons of tests, small, medium and big rock cache, all crash process
after process. We have definitively abandonned the Rock mode while it'll be
the same results.

So, it seems we'll have to switch all boxes from diskd to aufs, but I think
we could survive 
Anyway, we liked the diskd because we see good stability, but the HITed
objects are really too slow, all my clients are complaining, that's why we
did many tests yesterday and we found these times...

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672226.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Wed Jul 15 11:41:54 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 04:41:54 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <64906346.31006473.1436960345945.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1436953202097-4672212.post@n4.nabble.com>
 <55A62B53.7030306@gmail.com> <1436954399177-4672214.post@n4.nabble.com>
 <55A6311E.4090805@gmail.com>
 <1255111276.30858878.1436957174491.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A63E10.2000008@articatech.com>
 <906479546.30941336.1436959111883.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A64220.4040302@gmail.com>
 <1605985180.30997422.1436960114371.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <64906346.31006473.1436960345945.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436960514436-4672227.post@n4.nabble.com>

Fred,
Welcome to the club... 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672227.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Wed Jul 15 11:51:22 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 13:51:22 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436960473258-4672226.post@n4.nabble.com>
Message-ID: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>

sults.
> 
> So, it seems we'll have to switch all boxes from diskd to aufs, but I
> think
> we could survive
> Anyway, we liked the diskd because we see good stability, but the
> HITed
> objects are really too slow, all my clients are complaining, that's
> why we
> did many tests yesterday and we found these times...
> 
> Fred
> 

I'm interesting about your tests, which load ? I also quickly tested aufs this morning (my message before).
 
Your clients are complaining about reponse time ? There are good here, do you have enabled noatime in fstab and use separate drives ?  


From squid3 at treenet.co.nz  Wed Jul 15 12:04:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 00:04:19 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436960473258-4672226.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
Message-ID: <55A64C43.9020105@treenet.co.nz>

On 15/07/2015 11:41 p.m., Stakres wrote:
> Hi Amos,
> 
> Sorry but the Rock mode is totaly bugged, the worst mode to use here.
> We did tons of tests, small, medium and big rock cache, all crash process
> after process. We have definitively abandonned the Rock mode while it'll be
> the same results.

This seems to be specific to some use, but not others. Rock is young yet
so not all use-cases are stabilized. (Particularly if you're talking
about the Squid-3.4 in Debian's.)

Specifics about those issues would help a lot if you are able to assist
tracking that down.


> 
> So, it seems we'll have to switch all boxes from diskd to aufs, but I think
> we could survive 
> Anyway, we liked the diskd because we see good stability, but the HITed
> objects are really too slow, all my clients are complaining, that's why we
> did many tests yesterday and we found these times...

Thanks for that. The AUFS results bear out what I know of it. But I am
quite surprised you got diskd going that fast at all. What physical disk
backend are you using?

Amos



From vdoctor at neuf.fr  Wed Jul 15 12:08:07 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 05:08:07 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436962087598-4672230.post@n4.nabble.com>

Hi Fred,

We did the tests with 1 hard disk only (for testing), we used 150 req/sec,
load was around 0.7-0.8
Naaa, response times are crazy in DISKD/TCP_HIT (20+ sec instead 0.5 sec in
AUFS) but it concerns TCP_HIT only, the other flags are corrects in DISKD.

I'll try the "noatime"...

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672230.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Wed Jul 15 12:08:50 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 05:08:50 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436962130907-4672231.post@n4.nabble.com>

Hi Fred,

We did the tests with 1 hard disk only (for testing), we used 150 req/sec,
load was around 0.7-0.8
Naaa, response times are crazy in DISKD/TCP_HIT (20+ sec instead 0.5 sec in
AUFS) but it concerns TCP_HIT only, the other flags are corrects in DISKD.

I'll try the "noatime"...

Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672231.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Wed Jul 15 12:27:50 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 05:27:50 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A64C43.9020105@treenet.co.nz>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz>
Message-ID: <1436963270979-4672233.post@n4.nabble.com>

Amos,

We're using the latest 3.5.6 build, and we have not yet planed new tests
with the Rock. We were a bit disapointed with so we're not really "hot" to
spend time in testing it.

We're ok with the Diskd mode, except with the TCP_HIT objects (50+ times
slower).
We did tests on a basic server, i3 with 4GB memory, the disk is a 2.5' 80GB
not a rocket but good to figure out bad/good speed ;)

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672233.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Wed Jul 15 12:32:10 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 14:32:10 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436962130907-4672231.post@n4.nabble.com>
Message-ID: <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>




> 
> Hi Fred,
> 
> We did the tests with 1 hard disk only (for testing), we used 150
> req/sec,
> load was around 0.7-0.8
> Naaa, response times are crazy in DISKD/TCP_HIT (20+ sec instead 0.5
> sec in
> AUFS) but it concerns TCP_HIT only, the other flags are corrects in
> DISKD.
> 
> I'll try the "noatime"...
> 
> Fred
> 

Hi Fred too :)

So there is a problem somewhere I can reach 800 r/s with diskd, but in my case with aufs I can't exceed 400 r/s without many warning messages and "No such file"

client_http.requests= 481.95951/sec
Hit response time = more or less 1.5 s (for a file size 9.7 M) 6.27M/s

I don't re-try Rock store now because it was unusable after some minutes, this is not a "light" test for me 



From vdoctor at neuf.fr  Wed Jul 15 12:52:03 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 05:52:03 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436964723705-4672235.post@n4.nabble.com>

Fred,
(Guys, 2 french Fred here, but not the sames)

Did you check the TCP_HIT response times with the Diskd ?
During our tests, we have seen than it's sometime better to download the
object from internet again instead using the one from the cache, we got
better response times... 

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672235.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tarotapprentice at yahoo.com  Wed Jul 15 12:56:30 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Wed, 15 Jul 2015 12:56:30 +0000 (UTC)
Subject: [squid-users] Which DNS to use
In-Reply-To: <201507141555.44639.Antony.Stone@squid.open.source.it>
References: <201507141555.44639.Antony.Stone@squid.open.source.it>
Message-ID: <1346244471.2528879.1436964990224.JavaMail.yahoo@mail.yahoo.com>

I have around 25 client machines, so not quite "home" sized, but not big either.

I did try both the routers IP address and the ISP's advertised name servers. Speed-wise they seemed about the same. I'm on a DSL connection which says its link speed is 16Mbit down and 1Mbit up.

MarkJ
 
----- Original Message -----
> From: Antony Stone <Antony.Stone at squid.open.source.it>
> To: TarotApprentice <tarotapprentice at yahoo.com>
> Cc: 
> Sent: Wednesday, 15 July 2015, 0:55
> Subject: Re: [squid-users] Which DNS to use
> 
> On Tuesday 14 Jul 2015 at 13:59, TarotApprentice wrote:
> 
>>   I have a domestic DSL router/firewall/wifi
> 
> If, by "domestic", you mean something serving a home network with a 
> very small 
> number of users (eg: up to 10), I would say it doesn't really matter.
> 
> If you just mean it's one of those all-in-one devices, but you're 
> serving a 
> network with a decent number of users on it, then...
> 
>>  When configuring squid should I refer to the routers IP address for
>>  DNS, specify the one(s) the ISP providing or use alternative ones (like
>>  the examples in squid.conf). Is there a recommend way?
> 
> I would suggest installing a simple caching DNS server on the Squid box (with 
> a root zone file, such as is standard with ISC BIND) and let it do its own 
> lookups / caching.  Point your internal clients at it as well as Squid itself.
> 
> If you point Squid at the router IP, that's almost certainly just a (non-
> caching) forwarder to your ISP, so it's less efficient than pointing at the 
> ISP directly, and pointing at your ISP will be marginally slower and less 
> efficient than running your own caching DNS server.
> 
> However, depending on your network bandwidth and number of users, you may not 
> be able to tell the difference (the lower both these numbers are, the less 
> likely you'll notice anything by running your own instead of just pointing 
> at 
> 
> the ISP).
> 
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> "A person lives in the UK, but commutes to France daily for work.
> He belongs in the UK."
> 
> - From UK Revenue & Customs notice 741, page 13, paragraph 3.5.1
> - http://tinyurl.com/o7gnm4
> 
>                                                    Please reply to the list;
>                                                          please *don't* CC 
> me.
>  


From yvoinov at gmail.com  Wed Jul 15 13:02:37 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:02:37 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436964723705-4672235.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
Message-ID: <55A659ED.8060905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
AFAIK,

diskd speed depends from backend fs (OS level).

I use diskd over zfs with some tunables and has acceptable response
time, approx 0.1 sec.

15.07.15 18:52, Stakres ?????:
> Fred,
> (Guys, 2 french Fred here, but not the sames)
>
> Did you check the TCP_HIT response times with the Diskd ?
> During our tests, we have seen than it's sometime better to download the
> object from internet again instead using the one from the cache, we got
> better response times...
>
> Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672235.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVplnsAAoJENNXIZxhPexG0tUH/1h9VlUagDHBKezcgJRNzozn
U4Y4HUL8PHJa3Dma0kNlFtvU+LZENNbv941JHF/8kiSHBEFcd/ZPMu3YqJxtXj4H
hAgPxGcKGbYEfUzHHSYaPuWt08Xe9by0f+JguAHesBELMnSZIP+PJFjlsG6Ag8gy
qhvC31YrqZONeka3Ozgq0oQdHbSlZhKHaRqf2ZxqtFxqYacsbOMeLQrwAYNU8nRL
CbwkYwIP7poCq5qZg+uKDF1ngHOB/7uzV1nbYMfGVhmmOwAEWi5cQRNACCTJ0WNc
5iywDpEkRU4oi0LaKLUBBSf6mN9VB974u3P2lY+Io9GXNb3/Cw+kycKDoqGM4ao=
=INBP
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 13:18:12 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 15:18:12 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436964723705-4672235.post@n4.nabble.com>
Message-ID: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Did you check the TCP_HIT response times with the Diskd ?

Yes 

192.x.x.x - fred [15/Jul/2015:14:30:27 +0200] "GET http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
192.x.x.x - fred [15/Jul/2015:14:30:31 +0200] "GET http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
192.x.x.x - fred [15/Jul/2015:14:36:56 +0200] "GET http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"


>During our tests, we have seen than it's sometime better to download
> the
> object from internet again instead using the one from the cache, we
> got
> better response times...
> 

Now I can't tell because I have a lot of bandwidth free (hundreds), but a request from squid cache to user is fast
Something between 6 and 10 M/s (En Fran?ais 6 and 10 MO -> 32 and 80 mbits) and I'm not sure than my lan network is unused ...

So with hundreds of concurrent users I think that there is no special problem. 

FI more informations: username_cache tells me 4612 users

Diskd stats

squidclient -p 8080 mgr:diskd

sent_count: 23105099
recv_count: 23105042
max_away: 143
max_shmuse: 141
open_fail_queue_len: 40114
block_queue_len: 629596120

              OPS   SUCCESS    FAIL
   open   2192154   2192133       4
 create   1284546   1284546       0
  close   3476648   3476630       4
 unlink    981134    981131       2
   read   6541304   6541279       0
  write   8629313   8629313       0


Last five minutes:

squidclient -p 8080 mgr:5min


sample_start_time = 1436965191.905545 (Wed, 15 Jul 2015 12:59:51 GMT)
sample_end_time = 1436965491.907754 (Wed, 15 Jul 2015 13:04:51 GMT)
client_http.requests = 334.370871/sec
client_http.hits = 114.399158/sec
client_http.errors = 22.943164/sec
client_http.kbytes_in = 2948.928286/sec
client_http.kbytes_out = 6090.041824/sec
client_http.all_median_svc_time = 0.050460 seconds
client_http.miss_median_svc_time = 0.177113 seconds
client_http.nm_median_svc_time = 0.000911 seconds
client_http.nh_median_svc_time = 0.038286 seconds
client_http.hit_median_svc_time = 0.007665 seconds
server.all.requests = 237.598250/sec
server.all.errors = 0.000000/sec
server.all.kbytes_in = 5110.549036/sec
server.all.kbytes_out = 2765.982966/sec
server.http.requests = 185.981964/sec
server.http.errors = 0.000000/sec
server.http.kbytes_in = 1963.482209/sec
server.http.kbytes_out = 162.952133/sec
server.ftp.requests = 0.000000/sec
server.ftp.errors = 0.000000/sec
server.ftp.kbytes_in = 0.000000/sec
server.ftp.kbytes_out = 0.000000/sec
server.other.requests = 51.616287/sec
server.other.errors = 0.000000/sec
server.other.kbytes_in = 3147.070160/sec
server.other.kbytes_out = 2603.030833/sec
icp.pkts_sent = 0.000000/sec
icp.pkts_recv = 0.000000/sec
icp.queries_sent = 0.000000/sec
icp.replies_sent = 0.000000/sec
icp.queries_recv = 0.000000/sec
icp.replies_recv = 0.000000/sec
icp.replies_queued = 0.000000/sec
icp.query_timeouts = 0.000000/sec
icp.kbytes_sent = 0.000000/sec
icp.kbytes_recv = 0.000000/sec
icp.q_kbytes_sent = 0.000000/sec
icp.r_kbytes_sent = 0.000000/sec
icp.q_kbytes_recv = 0.000000/sec
icp.r_kbytes_recv = 0.000000/sec
icp.query_median_svc_time = 0.000000 seconds
icp.reply_median_svc_time = 0.000000 seconds
dns.median_svc_time = 0.005733 seconds
unlink.requests = 2.859979/sec
page_faults = 0.010000/sec
select_loops = 6428.069335/sec
select_fds = 7919.858350/sec
average_select_fd_period = 0.000000/fd
median_select_fds = 0.000000
swap.outs = 62.482873/sec
swap.ins = 129.082383/sec
swap.files_cleaned = 0.000000/sec
aborted_requests = 14.339894/sec
syscalls.disk.opens = 192.085252/sec
syscalls.disk.closes = 192.045253/sec
syscalls.disk.reads = 376.233896/sec
syscalls.disk.writes = 499.342990/sec
syscalls.disk.seeks = 0.000000/sec
syscalls.disk.unlinks = 57.879574/sec
syscalls.sock.accepts = 243.548207/sec
syscalls.sock.sockets = 131.322366/sec
syscalls.sock.connects = 131.322366/sec
syscalls.sock.binds = 0.000000/sec
syscalls.sock.closes = 377.267222/sec
syscalls.sock.reads = 3535.117303/sec
syscalls.sock.writes = 3999.197219/sec
syscalls.sock.recvfroms = 21.509842/sec
syscalls.sock.sendtos = 10.876587/sec
cpu_time = 170.718669 seconds
wall_time = 300.002209 seconds
cpu_usage = 56.905804%
  


From yvoinov at gmail.com  Wed Jul 15 13:26:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:26:33 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A65F89.9040104@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Here is my stats:

client_http.all_median_svc_time = 0.097357 seconds
client_http.miss_median_svc_time = 0.097357 seconds
client_http.nm_median_svc_time = 0.000000 seconds
client_http.nh_median_svc_time = 0.000000 seconds
client_http.hit_median_svc_time = 0.000000 seconds
icp.query_median_svc_time = 0.000000 seconds
icp.reply_median_svc_time = 0.000000 seconds
dns.median_svc_time = 0.000000 seconds
client_http.all_median_svc_time = 0.102812 seconds
client_http.miss_median_svc_time = 0.108570 seconds
client_http.nm_median_svc_time = 0.000000 seconds
client_http.nh_median_svc_time = 0.127833 seconds
client_http.hit_median_svc_time = 0.000000 seconds
icp.query_median_svc_time = 0.000000 seconds
icp.reply_median_svc_time = 0.000000 seconds
dns.median_svc_time = 0.000000 seconds
client_http.all_median_svc_time = 0.102812 seconds
client_http.miss_median_svc_time = 0.108570 seconds
client_http.nm_median_svc_time = 0.000911 seconds
client_http.nh_median_svc_time = 0.127833 seconds
client_http.hit_median_svc_time = 0.010350 seconds
icp.query_median_svc_time = 0.000000 seconds
icp.reply_median_svc_time = 0.000000 seconds
dns.median_svc_time = 0.000000 seconds

Diskd:

sent_count: 103224
recv_count: 103224
max_away: 129
max_shmuse: 129
open_fail_queue_len: 0
block_queue_len: 15478

              OPS   SUCCESS    FAIL
   open      4180      4180       0
 create      4057      4057       0
  close      8237      8237       0
 unlink       737       737       0
   read     16741     16741       0
  write     69272     69272       0

You has too long queue, so this indicates bottleneck in IO. Need to find
and resolve.

15.07.15 19:18, FredB ?????:
>
>>
>> Did you check the TCP_HIT response times with the Diskd ?
>
> Yes
>
> 192.x.x.x - fred [15/Jul/2015:14:30:27 +0200] "GET
http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
> 192.x.x.x - fred [15/Jul/2015:14:30:31 +0200] "GET
http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
> 192.x.x.x - fred [15/Jul/2015:14:36:56 +0200] "GET
http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
HTTP/1.0" 200 10096376 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
>
>
>> During our tests, we have seen than it's sometime better to download
>> the
>> object from internet again instead using the one from the cache, we
>> got
>> better response times...
>>
>
> Now I can't tell because I have a lot of bandwidth free (hundreds),
but a request from squid cache to user is fast
> Something between 6 and 10 M/s (En Fran?ais 6 and 10 MO -> 32 and 80
mbits) and I'm not sure than my lan network is unused ...
>
> So with hundreds of concurrent users I think that there is no special
problem.
>
> FI more informations: username_cache tells me 4612 users
>
> Diskd stats
>
> squidclient -p 8080 mgr:diskd
>
> sent_count: 23105099
> recv_count: 23105042
> max_away: 143
> max_shmuse: 141
> open_fail_queue_len: 40114
> block_queue_len: 629596120
>
>               OPS   SUCCESS    FAIL
>    open   2192154   2192133       4
>  create   1284546   1284546       0
>   close   3476648   3476630       4
>  unlink    981134    981131       2
>    read   6541304   6541279       0
>   write   8629313   8629313       0
>
>
> Last five minutes:
>
> squidclient -p 8080 mgr:5min
>
>
> sample_start_time = 1436965191.905545 (Wed, 15 Jul 2015 12:59:51 GMT)
> sample_end_time = 1436965491.907754 (Wed, 15 Jul 2015 13:04:51 GMT)
> client_http.requests = 334.370871/sec
> client_http.hits = 114.399158/sec
> client_http.errors = 22.943164/sec
> client_http.kbytes_in = 2948.928286/sec
> client_http.kbytes_out = 6090.041824/sec
> client_http.all_median_svc_time = 0.050460 seconds
> client_http.miss_median_svc_time = 0.177113 seconds
> client_http.nm_median_svc_time = 0.000911 seconds
> client_http.nh_median_svc_time = 0.038286 seconds
> client_http.hit_median_svc_time = 0.007665 seconds
> server.all.requests = 237.598250/sec
> server.all.errors = 0.000000/sec
> server.all.kbytes_in = 5110.549036/sec
> server.all.kbytes_out = 2765.982966/sec
> server.http.requests = 185.981964/sec
> server.http.errors = 0.000000/sec
> server.http.kbytes_in = 1963.482209/sec
> server.http.kbytes_out = 162.952133/sec
> server.ftp.requests = 0.000000/sec
> server.ftp.errors = 0.000000/sec
> server.ftp.kbytes_in = 0.000000/sec
> server.ftp.kbytes_out = 0.000000/sec
> server.other.requests = 51.616287/sec
> server.other.errors = 0.000000/sec
> server.other.kbytes_in = 3147.070160/sec
> server.other.kbytes_out = 2603.030833/sec
> icp.pkts_sent = 0.000000/sec
> icp.pkts_recv = 0.000000/sec
> icp.queries_sent = 0.000000/sec
> icp.replies_sent = 0.000000/sec
> icp.queries_recv = 0.000000/sec
> icp.replies_recv = 0.000000/sec
> icp.replies_queued = 0.000000/sec
> icp.query_timeouts = 0.000000/sec
> icp.kbytes_sent = 0.000000/sec
> icp.kbytes_recv = 0.000000/sec
> icp.q_kbytes_sent = 0.000000/sec
> icp.r_kbytes_sent = 0.000000/sec
> icp.q_kbytes_recv = 0.000000/sec
> icp.r_kbytes_recv = 0.000000/sec
> icp.query_median_svc_time = 0.000000 seconds
> icp.reply_median_svc_time = 0.000000 seconds
> dns.median_svc_time = 0.005733 seconds
> unlink.requests = 2.859979/sec
> page_faults = 0.010000/sec
> select_loops = 6428.069335/sec
> select_fds = 7919.858350/sec
> average_select_fd_period = 0.000000/fd
> median_select_fds = 0.000000
> swap.outs = 62.482873/sec
> swap.ins = 129.082383/sec
> swap.files_cleaned = 0.000000/sec
> aborted_requests = 14.339894/sec
> syscalls.disk.opens = 192.085252/sec
> syscalls.disk.closes = 192.045253/sec
> syscalls.disk.reads = 376.233896/sec
> syscalls.disk.writes = 499.342990/sec
> syscalls.disk.seeks = 0.000000/sec
> syscalls.disk.unlinks = 57.879574/sec
> syscalls.sock.accepts = 243.548207/sec
> syscalls.sock.sockets = 131.322366/sec
> syscalls.sock.connects = 131.322366/sec
> syscalls.sock.binds = 0.000000/sec
> syscalls.sock.closes = 377.267222/sec
> syscalls.sock.reads = 3535.117303/sec
> syscalls.sock.writes = 3999.197219/sec
> syscalls.sock.recvfroms = 21.509842/sec
> syscalls.sock.sendtos = 10.876587/sec
> cpu_time = 170.718669 seconds
> wall_time = 300.002209 seconds
> cpu_usage = 56.905804%
>  
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpl+JAAoJENNXIZxhPexGk2IH/1zGNmtNl9xvTRjoyS2T2Js2
Xl7k4jU4n/63hSWki8FbZ8vdcDaiErZKx2SwrYhGW144Xp2ldShnEi9O0vpt5NIa
3gT/J3xO8+GQTqnUa58BwFag+ROlvuvUYSqcJIMJ7O5bcPCOgYP+dZC1sEcGATQS
avq5F1NjmCn3f+Xk+lQP1xXipCI1gPYkvNXUbCkM0JODYsdGe2AdMoJ5fGQ397np
SpmSvkXdQxqL720qgtuFgjIH/xXomMsW9MPusDDUos2IMvMtz1m8OtZu1sK4J9SW
F5oAJowK7Y1Bd2L+DWKcwcXXdmpSuPuiUS9YLSEP+IJyAoKQceCC+O23UuHwu0A=
=NtRA
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 13:26:58 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 15:26:58 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>

Sorry, I forgot a real life test

time wget http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe -v
--2015-07-15 15:22:03--  http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
Connexion vers x.x.x.x:3128... connect??.
requ??te Proxy transmise, en attente de la r??ponse... 200 OK
Taille??: 10095849 (9,6M) [application/x-msdos-program]
Enregistre : ??youtube_downloader_hd_setup-2.9.9.23.exe.8??

100%[===========================================================================================================================================================================>] 10 095 849  9,26M/s   ds 1,0s    

2015-07-15 15:22:04 (9,26 MB/s) - ??youtube_downloader_hd_setup-2.9.9.23.exe.8?? enregistr?? [10095849/10095849]


real	0m1.049s
user	0m0.012s
sys	0m0.184s


Of course with TCP_HIT in access.log and not TCP_MEN_HIT (from memory)

Debian, squid 3.4.13, diskd, ext4 (noatime) 


From eliezer at ngtech.co.il  Wed Jul 15 13:33:19 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 15 Jul 2015 16:33:19 +0300
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436963270979-4672233.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz> <1436963270979-4672233.post@n4.nabble.com>
Message-ID: <55A6611F.30507@ngtech.co.il>

Just adding something to the subject.
HDD vs SSD speeds are quite something.
I have tried to test the benefits of a SSD in the past and in many cases 
it was a great addition of speed.

Eliezer

On 15/07/2015 15:27, Stakres wrote:
> Amos,
>
> We're using the latest 3.5.6 build, and we have not yet planed new tests
> with the Rock. We were a bit disapointed with so we're not really "hot" to
> spend time in testing it.
>
> We're ok with the Diskd mode, except with the TCP_HIT objects (50+ times
> slower).
> We did tests on a basic server, i3 with 4GB memory, the disk is a 2.5' 80GB
> not a rocket but good to figure out bad/good speed;)
>
> Fred




From yvoinov at gmail.com  Wed Jul 15 13:36:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:36:12 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A6611F.30507@ngtech.co.il>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz> <1436963270979-4672233.post@n4.nabble.com>
 <55A6611F.30507@ngtech.co.il>
Message-ID: <55A661CC.5040700@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
SSD as squid cache?! You are really rich, man!

15.07.15 19:33, Eliezer Croitoru ?????:
> Just adding something to the subject.
> HDD vs SSD speeds are quite something.
> I have tried to test the benefits of a SSD in the past and in many
cases it was a great addition of speed.
>
> Eliezer
>
> On 15/07/2015 15:27, Stakres wrote:
>> Amos,
>>
>> We're using the latest 3.5.6 build, and we have not yet planed new tests
>> with the Rock. We were a bit disapointed with so we're not really
"hot" to
>> spend time in testing it.
>>
>> We're ok with the Diskd mode, except with the TCP_HIT objects (50+ times
>> slower).
>> We did tests on a basic server, i3 with 4GB memory, the disk is a
2.5' 80GB
>> not a rocket but good to figure out bad/good speed;)
>>
>> Fred
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmHMAAoJENNXIZxhPexGy8wIAKfgerKcm+tO1vsI3OiR+T/I
bQrPHv6sZzEMuOtC7IdWrUzPKaABI2TSrU1g8+RCzyUO01FJQBUeoru3gDo8nqGa
GLCSPSbhZNMVSAzVRWS0PRwaq5WnEr75MqCJiuv89MpJylzQzN1SNSsQcVJuJfus
2vqNP5msNIUubfWkb2ExvZdFenNoW5TIfsx0btrqZGQ2BXWAAeW8xXOSiGrHD323
Yb117Ww0xlhUSe4CYpeVqryo7tsGkQNxQXAjwPT9jjS9TAIeTD+RerP5X1/AJRaP
lBET+Q3uJEdrU2ooR4368O6vjd6pgMVXWvaw1PBCrq5d3G67GtYVGDtz7fAu5Tk=
=CaiR
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 13:41:12 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 15:41:12 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A6611F.30507@ngtech.co.il>
Message-ID: <1506910328.31280426.1436967672628.JavaMail.root@zimbra4-e1.priv.proxad.net>

I agree, but what about the life time ? I change every two years (max 3) my sata drives


From yvoinov at gmail.com  Wed Jul 15 13:42:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:42:40 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A6611F.30507@ngtech.co.il>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz> <1436963270979-4672233.post@n4.nabble.com>
 <55A6611F.30507@ngtech.co.il>
Message-ID: <55A66350.5040608@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Speaking in essence: Performance depends strongly on the process model
used by the operating system, from settings, the hardware configuration
and the actual configuration of the operating system. And it can not be
considered in isolation from all these factors.

Tuning performance of a complex problem, and foolish to expect its
decision in a particular case configuration, which is chosen at random,
without understanding the process.

Different operating systems have different process models based on
different hardware and behave differently. Proxies can not depend on
these factors.

15.07.15 19:33, Eliezer Croitoru ?????:
> Just adding something to the subject.
> HDD vs SSD speeds are quite something.
> I have tried to test the benefits of a SSD in the past and in many
cases it was a great addition of speed.
>
> Eliezer
>
> On 15/07/2015 15:27, Stakres wrote:
>> Amos,
>>
>> We're using the latest 3.5.6 build, and we have not yet planed new tests
>> with the Rock. We were a bit disapointed with so we're not really
"hot" to
>> spend time in testing it.
>>
>> We're ok with the Diskd mode, except with the TCP_HIT objects (50+ times
>> slower).
>> We did tests on a basic server, i3 with 4GB memory, the disk is a
2.5' 80GB
>> not a rocket but good to figure out bad/good speed;)
>>
>> Fred
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmNQAAoJENNXIZxhPexG9o4IAL/aU4VNzxpmwhLZtpaX0Rm+
uhaH+9Z/0CenJ9HRMWiAijK0MWaGMYSEJaQtYm/RUDI3Tre6Ntc4IGjtIrPOsHyA
kQmPJUBGebr9g//dX/MtQYQZ9sXprAuDfPfs6K48BXsBEgmPnKbpw4MrTMv5Hp1E
/v9UqQXLxU2kTway5p+7ir3HuE4l8BfLIsE5E4KCnRtafvXUFndu0X+9MI9neAHz
/7R/xhMuJaAMw5q5MLHWEJxFAkejm03C3l4E4ssdu5q/8b4Uwc2razz+mDNoRA+b
AZQGgiYSyX6C2atkBP9IBEt4wD5Vy8OFT9rM3ra2Ac9DbMNo1azWl2bNsTIpDfY=
=vGY/
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 13:48:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:48:02 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1506910328.31280426.1436967672628.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1506910328.31280426.1436967672628.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A66492.4030109@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
It depends from your squid settings (memory cache size, etc), your OS
(as expected), your fs.

My installation works 4 years 24x7 with shipped HDD.


15.07.15 19:41, FredB ?????:
> I agree, but what about the life time ? I change every two years (max 3) my sata drives
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmSRAAoJENNXIZxhPexGnl0H/1UT1/kkVAyeUHMJTP4AXtX4
vnEHxwsDEJ4cEtSgvjzKvw3zAC9gA/c84TQ+mBrO+6nVTe0d7WCUdZlGlvo+w7Yk
gnX2pZiVdPU+C6drr/aRn+VLqkPINiaGzhvQQ1OxaolVt0HQ06445nz5WoCUWQyX
UZcSBOMF8ONqfimXm5sCpxWBBgQRPbW2zsNVs5vNuevYt+CuKDQHpcsnWJWY5ps/
Zqbexrs0dhyLAUPS2AwKSXwropXja6b2HnXqyT20khsAzTljcuBlvz84z85o4hAS
+PetoOjP4VexmvX7+6MSEcno1U8JOiThJb3rPJ7506oi1k88Ul3bQpS7u/b8dwI=
=83ai
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 13:51:03 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:51:03 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1506910328.31280426.1436967672628.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1506910328.31280426.1436967672628.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A66547.3040102@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Look:

root @ cthulhu / # zpool status data
  pool: data
 state: ONLINE
 scan: scrub repaired 0 in 1h49m with 0 errors on Sat Jul 11 07:49:01 2015
config:

        NAME          STATE     READ WRITE CKSUM
        data          ONLINE       0     0     0
          mirror-0    ONLINE       0     0     0
            c2t0d0s7  ONLINE       0     0     0
            c2t1d0s7  ONLINE       0     0     0

errors: No known data errors

Data pool uses as Squid cache. Disks was born in 2011. Still alive.

15.07.15 19:41, FredB ?????:
> I agree, but what about the life time ? I change every two years (max 3) my sata drives
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmVHAAoJENNXIZxhPexG1r0IAIuNLvwuOOvKtixblOXD4qBf
LXHysWux94l6y6jLDXimLPOClJE0PDTDN1sjIV8cA0WNEY9VheCC3IDqtnVvVC/S
R0n7J9xc0LU/BLNmlxIgbYUf5dJ1gCOrgLBqE+NKAyQF0jDJCzbkNMSE062oOuGz
15LfLnKMMjVjr4aN2zusGIsWsxOhgl+dWdP7GSqSf47fzX05K5x9NNGg8mxmpihw
p/Wt6+4ZQxbwiPlFtQuqAZTOyMS+vyWfvBdRORNA51Sv2QpJo172HFEMYUIc9j7f
OxP+RrleipEewcR0DVctHqJPmEb+QHtfDKTcnU0KBBtlu9mQzoT1BvOADJW2iDA=
=zthJ
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Wed Jul 15 13:51:38 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 06:51:38 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436968298185-4672247.post@n4.nabble.com>

Hi Fred,
tests from my side:
DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have tested.
AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url, same
all.

Wget with AUFS:
Length: 10095849 (9.6M) [application/x-msdos-program]
Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
100%[======================================>] 10,095,849  47.9M/s   in 0.2s
2015-07-15 15:48:29 (47.9 MB/s) - `youtube_downloader_hd_setup-2.9.9.23.exe'
saved

All,
We have switched some ISPs from DISKD to AUFS this morning, the "queue
congestion" appears at the begining then disappears from the cache.log. For
how long, nobody knows...

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672247.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jul 15 13:53:13 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 19:53:13 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436968298185-4672247.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com>
Message-ID: <55A665C9.1060307@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
queue congestion means IO bottleneck. This will appears on regular
basis. With client delays, of course.


15.07.15 19:51, Stakres ?????:
> Hi Fred,
> tests from my side:
> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
tested.
> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url, same
> all.
>
> Wget with AUFS:
> Length: 10095849 (9.6M) [application/x-msdos-program]
> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
> 100%[======================================>] 10,095,849  47.9M/s   in
0.2s
> 2015-07-15 15:48:29 (47.9 MB/s) -
`youtube_downloader_hd_setup-2.9.9.23.exe'
> saved
>
> All,
> We have switched some ISPs from DISKD to AUFS this morning, the "queue
> congestion" appears at the begining then disappears from the
cache.log. For
> how long, nobody knows...
>
> Fred
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672247.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmXIAAoJENNXIZxhPexGCqAH/jRYRkqjc7M8EApA8GdIsLK2
b/EWtHMBHSctv3imuqjaoVfeRRbJyZ+eHH6U4H1/YapPj2ySjiFHbDuWaPwRsC/C
T0F3ZK4wyYGhckwntm1VKny4XnMwQ3uY+c0s7g0G4OpJzokG856G78ClDvRdx2CU
eg92+WoEBFj1Mowxpo7o1+WlC0JXL+3o8qTktnX+9xFTeI2os+duzPyFC3GhQwce
w8UWLvCfsLI2bxAjT7k131ugqfq/TJ3ZegXWLA1Xrc+VqGlEtjnXl38bM3Uv1v/F
cu9viP4nuZyDoFCPG4WcuVJxWTM9gr0MFmmuaiZT6xFSx2JC/BjmB2OTM2UTzJw=
=bERu
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 13:57:11 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 15:57:11 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A66492.4030109@gmail.com>
Message-ID: <31616699.31310122.1436968631761.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> It depends from your squid settings (memory cache size, etc), your OS
> (as expected), your fs.
> 
> My installation works 4 years 24x7 with shipped HDD.
> 

Yes, in my case it depends of number of read/write by second, I know that I often reached some life time expectancy for a hard disk drive, but I mean, I heard (but maybe I'm wrong) that SSD still easily breakable and with short life time, at least more short, this is not dangerous for a cache ?  


From fredbmail at free.fr  Wed Jul 15 13:58:57 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 15:58:57 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436968298185-4672247.post@n4.nabble.com>
Message-ID: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>



> Objet: Re: [squid-users] AUFS vs. DISKS
> 
> Hi Fred,
> tests from my side:
> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
> tested.
> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url,
> same
> all.
> 
> Wget with AUFS:
> Length: 10095849 (9.6M) [application/x-msdos-program]
> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
> 100%[======================================>] 10,095,849  47.9M/s
>   in 0.2s
> 2015-07-15 15:48:29 (47.9 MB/s) -
> `youtube_downloader_hd_setup-2.9.9.23.exe'
> saved
> 
> All,
> We have switched some ISPs from DISKD to AUFS this morning, the
> "queue
> congestion" appears at the begining then disappears from the
> cache.log. For
> how long, nobody knows...
> 


Very very, strange, can you make a test with squid 3.4.13 ?


From yvoinov at gmail.com  Wed Jul 15 14:00:11 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:00:11 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <31616699.31310122.1436968631761.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <31616699.31310122.1436968631761.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A6676B.2020105@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think, that using datacenter (not consumer) class HDD is more
preferrable than SSD.

Cache content lost means cached traffic and money loss. And this is not
acceptable for big caches.

15.07.15 19:57, FredB ?????:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>> 
>> It depends from your squid settings (memory cache size, etc), your OS
>> (as expected), your fs.
>>
>> My installation works 4 years 24x7 with shipped HDD.
>>
>
> Yes, in my case it depends of number of read/write by second, I know
that I often reached some life time expectancy for a hard disk drive,
but I mean, I heard (but maybe I'm wrong) that SSD still easily
breakable and with short life time, at least more short, this is not
dangerous for a cache ? 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmdqAAoJENNXIZxhPexGHE0IAKPo73Tpd7LlIbcFXe0FceK4
6u/waFkb2Vtg2CbkEbI54l4w392kDNVEYcoy9CA0/i+6w8YnG/4lMDWNiueDzWUy
XS+lJW5LWcS/yLWAfRFJYNZPICzR8KqFDfQ6EG3QcAvVcnhRAkmv0Trq7gNx5n9T
HH9CTGzMLqXKaUhbALKeJl7JaP6VJPxaC8Zi1KXoE43XB3/XvmZx2OL+Y4t6nQq8
vuNZ0jRM+007hY1H7FDHbegbbFYQtI+DB7LfeKACVAmdFC4fAoq0EmH/ZQBpO15G
Dbf/c1ipeTI/gn04LY1m7XTrcOUZEA3vxwe+dBccvi+fU7SuBEN0bwALHZqPP6U=
=jgIC
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 14:01:32 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:01:32 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A667BC.8060708@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This test means nothing. Only very approximate overall IO performance
for IO subsystem.

15.07.15 19:58, FredB ?????:
>
>
>> Objet: Re: [squid-users] AUFS vs. DISKS
>>
>> Hi Fred,
>> tests from my side:
>> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
>> tested.
>> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url,
>> same
>> all.
>>
>> Wget with AUFS:
>> Length: 10095849 (9.6M) [application/x-msdos-program]
>> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
>> 100%[======================================>] 10,095,849  47.9M/s
>>   in 0.2s
>> 2015-07-15 15:48:29 (47.9 MB/s) -
>> `youtube_downloader_hd_setup-2.9.9.23.exe'
>> saved
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the
>> "queue
>> congestion" appears at the begining then disappears from the
>> cache.log. For
>> how long, nobody knows...
>>
>
>
> Very very, strange, can you make a test with squid 3.4.13 ?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpme7AAoJENNXIZxhPexGnOIIAJilTLizlRsRk0LDb315dyQH
QI2OWN3WKFTz46S3WuRnVyoLzhm81MnjvqNxjt/kNPZR0YZUr89vfO7RUf1sd2Ri
Q1V6wL2pgx1NhAF412Dx1AG9sZgbz4gDyC8y1aQL0VAAAynkE0WRym08Kq5P39tG
7oGoMbBJUKfvA+9pvjMw+TA+Gku/dNxvYQkDJTPJyD9k6mkY3jHTOI0hBXLVKwMY
bxykefqt8jcDCpSrBrndq866VxjxlL1UbYYJrSUHU/K5wLgG8fJgxUDGSn2CKpk2
HfLxylvZDMOtYMBpGYMhY/JUIeJ7N5MJuipmlJOm8AP/I5NapH8TSejfAhFpplc=
=dlUp
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 14:04:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:04:54 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A66886.6050309@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
And note this: TCP_HIT generally flows with other network traffic. We
don't know, how it handles during peak hours in network equipment, right?

To be sure, we must prioritizing TCP_HITS on network level, well?

15.07.15 19:58, FredB ?????:
>
>
>> Objet: Re: [squid-users] AUFS vs. DISKS
>>
>> Hi Fred,
>> tests from my side:
>> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
>> tested.
>> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url,
>> same
>> all.
>>
>> Wget with AUFS:
>> Length: 10095849 (9.6M) [application/x-msdos-program]
>> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
>> 100%[======================================>] 10,095,849  47.9M/s
>>   in 0.2s
>> 2015-07-15 15:48:29 (47.9 MB/s) -
>> `youtube_downloader_hd_setup-2.9.9.23.exe'
>> saved
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the
>> "queue
>> congestion" appears at the begining then disappears from the
>> cache.log. For
>> how long, nobody knows...
>>
>
>
> Very very, strange, can you make a test with squid 3.4.13 ?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmiGAAoJENNXIZxhPexGrgoIAK2pGtHW8TxyHiOA3epdev1z
x1jZZCkXykxIVofXlKnqJJ3rOTery1PtItlKICEyqVldtCQ9kzpBt8bO+RSJiV5e
Wi3MxzzOLi3ZF1hKlPspdCBfGuag79B60eirwucr6Fhh4keQ+2armglauSh9Liqt
6OlzIpmCiqfmqPDFUNKJZdFq8qMuAIUwVfxO602U/wBajpAB09oVNWdeMWakeeFc
X1XlDe4XZklXY4dGNfTjjBrnPZy4azU97tI+GUpgCAKPg8q2R6DcDvbkMoiP1j3p
ySmHAe0BcixTOqbg5aAQGog4ypHyTVktCcTxC6RTEopBE7NDvGvaND0yD33nOGQ=
=dZU8
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 14:06:05 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:06:05 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A668CD.7090702@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Just remember: performance tuning is complex problem, especially for
high load installations. And must be solved as complex.

15.07.15 19:58, FredB ?????:
>
>
>> Objet: Re: [squid-users] AUFS vs. DISKS
>>
>> Hi Fred,
>> tests from my side:
>> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
>> tested.
>> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url,
>> same
>> all.
>>
>> Wget with AUFS:
>> Length: 10095849 (9.6M) [application/x-msdos-program]
>> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
>> 100%[======================================>] 10,095,849  47.9M/s
>>   in 0.2s
>> 2015-07-15 15:48:29 (47.9 MB/s) -
>> `youtube_downloader_hd_setup-2.9.9.23.exe'
>> saved
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the
>> "queue
>> congestion" appears at the begining then disappears from the
>> cache.log. For
>> how long, nobody knows...
>>
>
>
> Very very, strange, can you make a test with squid 3.4.13 ?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmjMAAoJENNXIZxhPexG3IoH/RlJCP5c6IVoT6IdFxcbbH+0
DL6NTvuYHQ7qjcScvzR9NXOiW01vZdWIoSAQfp/MrHIxgNf8qXjtjqCx13Z797jg
LH8u6sag+vz/jdGtMv9Bid1lxvgNEBqKSe3kIxjKjgthFLCg2HBa9rKgJgKnLsme
v0iKUXaHPhvKA3uCEuaIo1Y/o1qrEM+HgYIs89u/VXItchR/ZOF+wtMS3ntnBxo+
z3XnfoGcDamJMx/VA26Dli+Woe733+fEdGanpOZS3N4JNo37B+nhR01i9L94BAOW
78TJcpVDDL+nLTGVNhVNQtDq6ODGuNlbSd87GmQlKO0gw02K3qPF4hiizIwLCMA=
=Or+H
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 14:06:34 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 16:06:34 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436968298185-4672247.post@n4.nabble.com>
Message-ID: <488243135.31334949.1436969194343.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> All,
> We have switched some ISPs from DISKD to AUFS this morning, the
> "queue
> congestion" appears at the begining then disappears from the
> cache.log. For
> how long, nobody knows...
>

Yes me too, but after a while I had 

2015/07/15 13:36:07 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory
2015/07/15 13:36:07 kid1|       /cache2/1F/F9/007DF243 

so I courageously ran back into diskd


From yvoinov at gmail.com  Wed Jul 15 14:08:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:08:04 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <488243135.31334949.1436969194343.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <488243135.31334949.1436969194343.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A66944.5030300@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
At this moment your user got partially loaded web page.....

15.07.15 20:06, FredB ?????:
>
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the
>> "queue
>> congestion" appears at the begining then disappears from the
>> cache.log. For
>> how long, nobody knows...
>>
>
> Yes me too, but after a while I had
>
> 2015/07/15 13:36:07 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or directory
> 2015/07/15 13:36:07 kid1|       /cache2/1F/F9/007DF243
>
> so I courageously ran back into diskd
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpmlDAAoJENNXIZxhPexGQUkH/2Z/dwvJ+LTitzp/YikJLvfr
0CW+w1eeOdKt0RbMtSr0fQqq3vyCPl0H7GmGOKd4QiMbfnPIGpeQJA3U315fBz6v
wxKc9Rk9qscMyHJpOcBEJmtJQr+CmKJf6HQsa4zBxQH25vWecV2EYUy3G2dVv+LR
DngSH1vvnkQ6bwjCtIXuxL902Mn0ieabPdTwhHmuW14ZfwDRexWhGxG+E5EEQter
7rxQw1jbw7PmiGYcQ9iqo6CfPIOECFTdtKKOjTOgHEkXvxSuFbN2+xw8hMeLB4iv
d4N+cv0hjaVU10Fl70PWosN1YKTwuQvq0epYU8Q8yb792M5vaCeq0YcewsmemPg=
=oSmA
-----END PGP SIGNATURE-----



From fredbmail at free.fr  Wed Jul 15 14:20:46 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 16:20:46 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A667BC.8060708@gmail.com>
Message-ID: <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>


> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> This test means nothing. Only very approximate overall IO performance
> for IO subsystem.
> 

Not nothing I don't agree, it's not sufficiently precise to indicate where the problem is, ok with that, but if you change only diskd by aufs you should not see a - huge - difference like this. It's just an end user experience but this explain the complaints of his users. 

Fred, I think you have made some tests, not just one ?

Here with more load I can't have a poor performance like this, no matter aufs or diskd. So, at least with 3.4.13, diskd can be more faster and you have a problem, it's not just because diskd is more slow.     




From fredbmail at free.fr  Wed Jul 15 14:27:55 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 16:27:55 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A66944.5030300@gmail.com>
Message-ID: <1651855330.31377126.1436970475652.JavaMail.root@zimbra4-e1.priv.proxad.net>



> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> At this moment your user got partially loaded web page.....
> 

Yes bad experience for me, I guess I reach some limitations about aufs, fortunately I have no problem with diskd but I like to increase the performances.

I will (re)test rock store soon with the latest 3.5.x


From vdoctor at neuf.fr  Wed Jul 15 14:31:59 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 07:31:59 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com>
 <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A667BC.8060708@gmail.com>
 <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436970719948-4672259.post@n4.nabble.com>

Fred,
We have upgraded 4 big ISPs to the latest 3.5.6 in AUFS, feedbacks are so
good. I can tell you clients see a big (positive) change here.
We use the same settings in the squid.conf but AUFS instead DISKD, the
difference is crazy...

In the past we moved to the Diskd due to too many errors in Aufs (isEmpty,
etc...), now it seems all these errors (welcome to new ones...) have been
fixed.
So, again, I only speak about the Diskd and the TCP_HIT, other flags are ok
with us.

Fred.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672259.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jul 15 14:39:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 02:39:18 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436968298185-4672247.post@n4.nabble.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com>
Message-ID: <55A67096.2060005@treenet.co.nz>

On 16/07/2015 1:51 a.m., Stakres wrote:
> Hi Fred,
> tests from my side:
> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have tested.
> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url, same
> all.
> 
> Wget with AUFS:
> Length: 10095849 (9.6M) [application/x-msdos-program]
> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
> 100%[======================================>] 10,095,849  47.9M/s   in 0.2s
> 2015-07-15 15:48:29 (47.9 MB/s) - `youtube_downloader_hd_setup-2.9.9.23.exe'
> saved
> 
> All,
> We have switched some ISPs from DISKD to AUFS this morning, the "queue
> congestion" appears at the begining then disappears from the cache.log. For
> how long, nobody knows...


Doh. Sorry its been so long I had forgotten one detail ...

Squid AUFS starts with a relatively short I/O queue length (8 entries)
and auto-grows it as needed by doubling the limit and displaying the
congestion message. On any busy proxy this can be seen for a while after
startup, then possibly during the first traffic peak after that. It is
not a problem unless it keeps growing indefinitely or appears suddenly
without a matching traffic spike (possible sign of disk failure).

I'm applying a patch to raise the baseline queue length from 8 to 8K,
and reporting in each message what the limit was changed to. That should
get rid of a dozen entries in your logs, and help show whats going on in
the remainder.

Amos



From mmonette at 2keys.ca  Wed Jul 15 14:42:51 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 15 Jul 2015 10:42:51 -0400 (EDT)
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
Message-ID: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>

Hello,

This might be a stupid question..

I started looking at squidGuard. Looks pretty straight forward and fairly easy to implement it but for some reason I could not get it to actually work, the blacklists were still being bypassed. I was using Squid-3.5.4 from source because I need the ssl::server_name ACL. I tried to install regular squid-3.1.10 from the YUM repo and using the same config file (disabling the at_step and ssl::servername stuff) and everything worked right away without needing to touch anything else in squid.conf. My blacklists were active.

Do I need to compile squid in a certain way to that url_rewrite works? Or is that something that works and is enabled by default?

Here are my ./configure options:

./configure --prefix=/usr --includedir=/usr/include --datadir=/usr/share --bindir=/usr/sbin --libexecdir=/usr/lib/squid \
--localstatedir=/var --sysconfdir=/etc/squid --with-included-ltdl --enable-ltdl-convenience --with-openssl --enable-ssl-crtd --with-logdir=/var/log/squid

My url_rewrite line from squid.conf:

url_rewrite_program /usr/local/bin/squidGuard -c /usr/local/squidGuard/squidGuard.conf

Let me know if I am missing anything. In the mean time I am going to keep playing with it it.

Thanks,

Mike


From squid3 at treenet.co.nz  Wed Jul 15 14:45:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 02:45:12 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1651855330.31377126.1436970475652.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1651855330.31377126.1436970475652.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A671F8.2090102@treenet.co.nz>

On 16/07/2015 2:27 a.m., FredB wrote:
> 
>> At this moment your user got partially loaded web page.....
>>
> 
> Yes bad experience for me, I guess I reach some limitations about aufs,

That is the SWAPFAIL part of SWAPFAIL_MISS. User should have simply
gitten a MISS fetched from the network. Maybe a smidgen slower than a
regular MISS since Squid went to the disk first. But nothing obvious to
the user.


> fortunately I have no problem with diskd but I like to increase the performances.
> 
> I will (re)test rock store soon with the latest 3.5.x

:-)

Amos



From fredbmail at free.fr  Wed Jul 15 14:45:48 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 16:45:48 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436970719948-4672259.post@n4.nabble.com>
Message-ID: <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Fred,
> We have upgraded 4 big ISPs to the latest 3.5.6 in AUFS, feedbacks
> are so
> good. I can tell you clients see a big (positive) change here.
> We use the same settings in the squid.conf but AUFS instead DISKD,
> the
> difference is crazy...
> 
> In the past we moved to the Diskd due to too many errors in Aufs
> (isEmpty,
> etc...), now it seems all these errors (welcome to new ones...) have
> been
> fixed.
> So, again, I only speak about the Diskd and the TCP_HIT, other flags
> are ok
> with us.
> 
> Fred.


OK, if you have some times please try squid 3.4.13, just to known if there is a bug or not 
A stupid question, are your using the good binary, from squid 3.5.6 not oldest ? /usr/lib/squid/diskd (in my case) 


From yvoinov at gmail.com  Wed Jul 15 14:57:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:57:09 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A671F8.2090102@treenet.co.nz>
References: <1651855330.31377126.1436970475652.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A671F8.2090102@treenet.co.nz>
Message-ID: <55A674C5.1040006@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


15.07.15 20:45, Amos Jeffries ?????:
> On 16/07/2015 2:27 a.m., FredB wrote:
>>
>>> At this moment your user got partially loaded web page.....
>>>
>>
>> Yes bad experience for me, I guess I reach some limitations about aufs,
>
> That is the SWAPFAIL part of SWAPFAIL_MISS. User should have simply
> gitten a MISS fetched from the network. Maybe a smidgen slower than a
> regular MISS since Squid went to the disk first. But nothing obvious to
> the user.
On my test system it was soooooooo sloooooooooooow loading page
elements.... ;) Disrupted.

>
>
>
>> fortunately I have no problem with diskd but I like to increase the
performances.
>>
>> I will (re)test rock store soon with the latest 3.5.x
>
> :-)
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnTFAAoJENNXIZxhPexGClYH/3bZC71hMJpiXLlrRxC1BJ3N
kYltAmwyQI1/3wyNtAdRZVDgiBkH5j7cadROYxxSTSP/9vGCBENUChbQma4VZkJy
dP/3j9fpAvVLyK/QcfVfoHdPKMg8h/41Tpo/iMTYvTadMx+NGEJ4SFXahvzCGDKb
76tY2GLcPqucVrp2VroBKnOV+GRYxgqP6JgTNjkWsCp6ZKnv4rFOT82AkmzqdzNP
wljKPaajls9HgrjTTweXFkjdY0DQkkMlZkp0U/Lhzi6gFR/lW9hXQozWEiJzcdLo
5A5gQU2dbvDZqR2jvjuyO35ahKeaigrQ4D9EfWMpi2RpkBHPseUXTAnjaI+5n40=
=O0Px
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed Jul 15 14:57:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 02:57:02 +1200
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
Message-ID: <55A674BE.90102@treenet.co.nz>

On 16/07/2015 2:42 a.m., Michael Monette wrote:
> Hello,
> 
> This might be a stupid question..
> 
> I started looking at squidGuard. Looks pretty straight forward and
> fairly easy to implement it but for some reason I could not get it to
> actually work, the blacklists were still being bypassed. I was using
> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
> tried to install regular squid-3.1.10 from the YUM repo and using the
> same config file (disabling the at_step and ssl::servername stuff)
> and everything worked right away without needing to touch anything
> else in squid.conf. My blacklists were active.
> 
> Do I need to compile squid in a certain way to that url_rewrite
> works? Or is that something that works and is enabled by default?

Nope, its an always-built component of Squid.

SquidGuard is an old and no longer maintained project. It may need
patching manually to work with current Squid versions
(<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).

ufdbGuard is much more up to date and performant if you actually have to
use a tool. Squid can be configured to do itself almost everything the
helpers do.

Amos


From yvoinov at gmail.com  Wed Jul 15 14:59:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 20:59:21 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A67096.2060005@treenet.co.nz>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
Message-ID: <55A67549.9020204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Amos,

I think, auds queue must be buffered more better and smoother. On some
OS (I've tested) peak loads leads performance degradation. Periodically.

That is why I'm not using aufs.

15.07.15 20:39, Amos Jeffries ?????:
> On 16/07/2015 1:51 a.m., Stakres wrote:
>> Hi Fred,
>> tests from my side:
>> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have
tested.
>> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url,
same
>> all.
>>
>> Wget with AUFS:
>> Length: 10095849 (9.6M) [application/x-msdos-program]
>> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
>> 100%[======================================>] 10,095,849  47.9M/s  
in 0.2s
>> 2015-07-15 15:48:29 (47.9 MB/s) -
`youtube_downloader_hd_setup-2.9.9.23.exe'
>> saved
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the "queue
>> congestion" appears at the begining then disappears from the
cache.log. For
>> how long, nobody knows...
>
>
> Doh. Sorry its been so long I had forgotten one detail ...
>
> Squid AUFS starts with a relatively short I/O queue length (8 entries)
> and auto-grows it as needed by doubling the limit and displaying the
> congestion message. On any busy proxy this can be seen for a while after
> startup, then possibly during the first traffic peak after that. It is
> not a problem unless it keeps growing indefinitely or appears suddenly
> without a matching traffic spike (possible sign of disk failure).
>
> I'm applying a patch to raise the baseline queue length from 8 to 8K,
> and reporting in each message what the limit was changed to. That should
> get rid of a dozen entries in your logs, and help show whats going on in
> the remainder.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnVJAAoJENNXIZxhPexGu5wIAMXIBY+tGHxMgjD5MnICAQq8
ziwovReJYHM9i4QApC0mnB8/7UV/aXkRge+EcC1fJCQT4Hf4ZqaboikmZ3nxbMw+
Tfr0t2uQ2Hnd1le/EqVZ4/+wvDIVuqcP7Il8sRve9i2guQeZVhus8DtSQYYU/9GQ
6Fwdyt3kcl7CfTdsrGpf38x0O/YwMyc6OYrIewm35EesCI0kbVDED1tnOeFKo96S
8mBbrmqTlK/b3hoyDGirRpI485t1lS+46PF95RGS3ynVUAAY0z+6Z5BrgfHo7y5g
oEUonXjKtqoFxjJLlvzBJxtHylzK9MYcdPnaKHDTFFs2K2+rPF/Q6rELqxFSbv4=
=kgZp
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Wed Jul 15 15:00:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 21:00:55 +0600
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <55A674BE.90102@treenet.co.nz>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
 <55A674BE.90102@treenet.co.nz>
Message-ID: <55A675A7.40005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Confirm.

ufdbguard is great redirector. It has a bit small problem with some
reporting tools (like SARG), but nothing important.

15.07.15 20:57, Amos Jeffries ?????:
> On 16/07/2015 2:42 a.m., Michael Monette wrote:
>> Hello,
>>
>> This might be a stupid question..
>>
>> I started looking at squidGuard. Looks pretty straight forward and
>> fairly easy to implement it but for some reason I could not get it to
>> actually work, the blacklists were still being bypassed. I was using
>> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
>> tried to install regular squid-3.1.10 from the YUM repo and using the
>> same config file (disabling the at_step and ssl::servername stuff)
>> and everything worked right away without needing to touch anything
>> else in squid.conf. My blacklists were active.
>>
>> Do I need to compile squid in a certain way to that url_rewrite
>> works? Or is that something that works and is enabled by default?
>
> Nope, its an always-built component of Squid.
>
> SquidGuard is an old and no longer maintained project. It may need
> patching manually to work with current Squid versions
> (<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).
>
> ufdbGuard is much more up to date and performant if you actually have to
> use a tool. Squid can be configured to do itself almost everything the
> helpers do.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnWnAAoJENNXIZxhPexGZ0cH/3OObwZEiQgn9d13WMy/dIGH
IB/KhQ9VMYk2YF/mZrhisSMIDb90Y7r2XmWSZHH46ZdbFtoXppBKrzUtqoy9RpkF
+UuhIHLb5bCIPO2DIFrMVQoF6ACCxL0jfML5LR5WHbwJy+B6u+x5WUERU/dR006W
2x2bUsrOz48KrBK4wwb9GFhdJDOs7gTfaClBa1gx5h3x1wtT8FCC0zahOBs3aMy5
bYSyb22a59gDFhfXPKum32o8Y3tRvUpCID8VSgxpKVeJcZNG8KGmh+vg/jrms5OK
6WDIlp9TpfNH/RTALfXyctp9Wr5smJSLkKuYaaAmAWUJMpH+zh7Vs/hblv/kmRg=
=dPG9
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Wed Jul 15 15:04:53 2015
From: vdoctor at neuf.fr (Stakres)
Date: Wed, 15 Jul 2015 08:04:53 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com>
 <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A667BC.8060708@gmail.com>
 <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436970719948-4672259.post@n4.nabble.com>
 <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1436972693462-4672268.post@n4.nabble.com>

Fred,

Not sure we'll have free time for testing the previous 3.4, we now have
dozens of boxes to manually upgrade to the 3.5.6...
yes, we do use the original squid 3.5.6 package, no build mix here.

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672268.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From mmonette at 2keys.ca  Wed Jul 15 15:15:07 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 15 Jul 2015 11:15:07 -0400 (EDT)
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <55A675A7.40005@gmail.com>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
 <55A674BE.90102@treenet.co.nz> <55A675A7.40005@gmail.com>
Message-ID: <1046753052.2660784.1436973307935.JavaMail.zimbra@2keys.ca>

Cool! 

I'm just beginning to explore these things, so I am glad I asked the question. I am going to check out ufbdguard now. 

Thank you all

----- Original Message -----
From: "Yuri Voinov" <yvoinov at gmail.com>
To: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:00:55 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Confirm.

ufdbguard is great redirector. It has a bit small problem with some
reporting tools (like SARG), but nothing important.

15.07.15 20:57, Amos Jeffries ?????:
> On 16/07/2015 2:42 a.m., Michael Monette wrote:
>> Hello,
>>
>> This might be a stupid question..
>>
>> I started looking at squidGuard. Looks pretty straight forward and
>> fairly easy to implement it but for some reason I could not get it to
>> actually work, the blacklists were still being bypassed. I was using
>> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
>> tried to install regular squid-3.1.10 from the YUM repo and using the
>> same config file (disabling the at_step and ssl::servername stuff)
>> and everything worked right away without needing to touch anything
>> else in squid.conf. My blacklists were active.
>>
>> Do I need to compile squid in a certain way to that url_rewrite
>> works? Or is that something that works and is enabled by default?
>
> Nope, its an always-built component of Squid.
>
> SquidGuard is an old and no longer maintained project. It may need
> patching manually to work with current Squid versions
> (<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).
>
> ufdbGuard is much more up to date and performant if you actually have to
> use a tool. Squid can be configured to do itself almost everything the
> helpers do.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnWnAAoJENNXIZxhPexGZ0cH/3OObwZEiQgn9d13WMy/dIGH
IB/KhQ9VMYk2YF/mZrhisSMIDb90Y7r2XmWSZHH46ZdbFtoXppBKrzUtqoy9RpkF
+UuhIHLb5bCIPO2DIFrMVQoF6ACCxL0jfML5LR5WHbwJy+B6u+x5WUERU/dR006W
2x2bUsrOz48KrBK4wwb9GFhdJDOs7gTfaClBa1gx5h3x1wtT8FCC0zahOBs3aMy5
bYSyb22a59gDFhfXPKum32o8Y3tRvUpCID8VSgxpKVeJcZNG8KGmh+vg/jrms5OK
6WDIlp9TpfNH/RTALfXyctp9Wr5smJSLkKuYaaAmAWUJMpH+zh7Vs/hblv/kmRg=
=dPG9
-----END PGP SIGNATURE-----

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From mmonette at 2keys.ca  Wed Jul 15 15:17:08 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 15 Jul 2015 11:17:08 -0400 (EDT)
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <1046753052.2660784.1436973307935.JavaMail.zimbra@2keys.ca>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
 <55A674BE.90102@treenet.co.nz> <55A675A7.40005@gmail.com>
 <1046753052.2660784.1436973307935.JavaMail.zimbra@2keys.ca>
Message-ID: <138271822.2661623.1436973428375.JavaMail.zimbra@2keys.ca>

I love how they say ufdbguard is Free and Open Source...followed by a pricing option.



----- Original Message -----
From: "Michael Monette" <mmonette at 2keys.ca>
To: "Yuri Voinov" <yvoinov at gmail.com>
Cc: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:15:07 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

Cool! 

I'm just beginning to explore these things, so I am glad I asked the question. I am going to check out ufbdguard now. 

Thank you all

----- Original Message -----
From: "Yuri Voinov" <yvoinov at gmail.com>
To: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:00:55 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Confirm.

ufdbguard is great redirector. It has a bit small problem with some
reporting tools (like SARG), but nothing important.

15.07.15 20:57, Amos Jeffries ?????:
> On 16/07/2015 2:42 a.m., Michael Monette wrote:
>> Hello,
>>
>> This might be a stupid question..
>>
>> I started looking at squidGuard. Looks pretty straight forward and
>> fairly easy to implement it but for some reason I could not get it to
>> actually work, the blacklists were still being bypassed. I was using
>> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
>> tried to install regular squid-3.1.10 from the YUM repo and using the
>> same config file (disabling the at_step and ssl::servername stuff)
>> and everything worked right away without needing to touch anything
>> else in squid.conf. My blacklists were active.
>>
>> Do I need to compile squid in a certain way to that url_rewrite
>> works? Or is that something that works and is enabled by default?
>
> Nope, its an always-built component of Squid.
>
> SquidGuard is an old and no longer maintained project. It may need
> patching manually to work with current Squid versions
> (<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).
>
> ufdbGuard is much more up to date and performant if you actually have to
> use a tool. Squid can be configured to do itself almost everything the
> helpers do.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnWnAAoJENNXIZxhPexGZ0cH/3OObwZEiQgn9d13WMy/dIGH
IB/KhQ9VMYk2YF/mZrhisSMIDb90Y7r2XmWSZHH46ZdbFtoXppBKrzUtqoy9RpkF
+UuhIHLb5bCIPO2DIFrMVQoF6ACCxL0jfML5LR5WHbwJy+B6u+x5WUERU/dR006W
2x2bUsrOz48KrBK4wwb9GFhdJDOs7gTfaClBa1gx5h3x1wtT8FCC0zahOBs3aMy5
bYSyb22a59gDFhfXPKum32o8Y3tRvUpCID8VSgxpKVeJcZNG8KGmh+vg/jrms5OK
6WDIlp9TpfNH/RTALfXyctp9Wr5smJSLkKuYaaAmAWUJMpH+zh7Vs/hblv/kmRg=
=dPG9
-----END PGP SIGNATURE-----

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From mmonette at 2keys.ca  Wed Jul 15 15:17:57 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 15 Jul 2015 11:17:57 -0400 (EDT)
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <138271822.2661623.1436973428375.JavaMail.zimbra@2keys.ca>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
 <55A674BE.90102@treenet.co.nz> <55A675A7.40005@gmail.com>
 <1046753052.2660784.1436973307935.JavaMail.zimbra@2keys.ca>
 <138271822.2661623.1436973428375.JavaMail.zimbra@2keys.ca>
Message-ID: <49183831.2662012.1436973477777.JavaMail.zimbra@2keys.ca>

Spoke to soon, there is a free one. Going to give it a shot, thanks again.

----- Original Message -----
From: "Michael Monette" <mmonette at 2keys.ca>
To: "Michael Monette" <mmonette at 2keys.ca>
Cc: "Yuri Voinov" <yvoinov at gmail.com>, "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:17:08 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

I love how they say ufdbguard is Free and Open Source...followed by a pricing option.



----- Original Message -----
From: "Michael Monette" <mmonette at 2keys.ca>
To: "Yuri Voinov" <yvoinov at gmail.com>
Cc: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:15:07 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

Cool! 

I'm just beginning to explore these things, so I am glad I asked the question. I am going to check out ufbdguard now. 

Thank you all

----- Original Message -----
From: "Yuri Voinov" <yvoinov at gmail.com>
To: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, July 15, 2015 11:00:55 AM
Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Confirm.

ufdbguard is great redirector. It has a bit small problem with some
reporting tools (like SARG), but nothing important.

15.07.15 20:57, Amos Jeffries ?????:
> On 16/07/2015 2:42 a.m., Michael Monette wrote:
>> Hello,
>>
>> This might be a stupid question..
>>
>> I started looking at squidGuard. Looks pretty straight forward and
>> fairly easy to implement it but for some reason I could not get it to
>> actually work, the blacklists were still being bypassed. I was using
>> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
>> tried to install regular squid-3.1.10 from the YUM repo and using the
>> same config file (disabling the at_step and ssl::servername stuff)
>> and everything worked right away without needing to touch anything
>> else in squid.conf. My blacklists were active.
>>
>> Do I need to compile squid in a certain way to that url_rewrite
>> works? Or is that something that works and is enabled by default?
>
> Nope, its an always-built component of Squid.
>
> SquidGuard is an old and no longer maintained project. It may need
> patching manually to work with current Squid versions
> (<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).
>
> ufdbGuard is much more up to date and performant if you actually have to
> use a tool. Squid can be configured to do itself almost everything the
> helpers do.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVpnWnAAoJENNXIZxhPexGZ0cH/3OObwZEiQgn9d13WMy/dIGH
IB/KhQ9VMYk2YF/mZrhisSMIDb90Y7r2XmWSZHH46ZdbFtoXppBKrzUtqoy9RpkF
+UuhIHLb5bCIPO2DIFrMVQoF6ACCxL0jfML5LR5WHbwJy+B6u+x5WUERU/dR006W
2x2bUsrOz48KrBK4wwb9GFhdJDOs7gTfaClBa1gx5h3x1wtT8FCC0zahOBs3aMy5
bYSyb22a59gDFhfXPKum32o8Y3tRvUpCID8VSgxpKVeJcZNG8KGmh+vg/jrms5OK
6WDIlp9TpfNH/RTALfXyctp9Wr5smJSLkKuYaaAmAWUJMpH+zh7Vs/hblv/kmRg=
=dPG9
-----END PGP SIGNATURE-----

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From marcus.kool at urlfilterdb.com  Wed Jul 15 15:37:01 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 15 Jul 2015 12:37:01 -0300
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A67096.2060005@treenet.co.nz>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
Message-ID: <55A67E1D.7040801@urlfilterdb.com>



On 07/15/2015 11:39 AM, Amos Jeffries wrote:
> On 16/07/2015 1:51 a.m., Stakres wrote:
>> Hi Fred,
>> tests from my side:
>> DISKD with TCP_HIT objects: 564KB/s with wget, the same url you have tested.
>> AUFS with TCP_HITS objects: 47.8M/s, same wget, same squid, same url, same
>> all.
>>
>> Wget with AUFS:
>> Length: 10095849 (9.6M) [application/x-msdos-program]
>> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe'
>> 100%[======================================>] 10,095,849  47.9M/s   in 0.2s
>> 2015-07-15 15:48:29 (47.9 MB/s) - `youtube_downloader_hd_setup-2.9.9.23.exe'
>> saved
>>
>> All,
>> We have switched some ISPs from DISKD to AUFS this morning, the "queue
>> congestion" appears at the begining then disappears from the cache.log. For
>> how long, nobody knows...
>
>
> Doh. Sorry its been so long I had forgotten one detail ...
>
> Squid AUFS starts with a relatively short I/O queue length (8 entries)
> and auto-grows it as needed by doubling the limit and displaying the
> congestion message. On any busy proxy this can be seen for a while after
> startup, then possibly during the first traffic peak after that. It is
> not a problem unless it keeps growing indefinitely or appears suddenly
> without a matching traffic spike (possible sign of disk failure).
>
> I'm applying a patch to raise the baseline queue length from 8 to 8K,
> and reporting in each message what the limit was changed to. That should
> get rid of a dozen entries in your logs, and help show whats going on in
> the remainder.

Since you will modify the log message from
    WARNING - Queue congestion
to something like
    INFO - the I/O queue lenght was X and has been doubled to 2X because of the current I/O load.
admins will understand _much_ better what is going on.

I think that changing the baseline to 8K is not required since the queue congestion
warning is normally seen only a few times, so the baseline value of 8 is doubled
only a few times.
A new baseline value of 256 (5 doublings) makes sense to me since most disks have
a hardware queue depth of 256.
For those systems where many disks are tortured with I/O the doubling will appear
a few (log2(#disks)) times.

Marcus

> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Wed Jul 15 15:37:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 03:37:17 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A67549.9020204@gmail.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67549.9020204@gmail.com>
Message-ID: <55A67E2D.5090505@treenet.co.nz>

On 16/07/2015 2:59 a.m., Yuri Voinov wrote:
> 
> Amos,
> 
> I think, auds queue must be buffered more better and smoother. On some
> OS (I've tested) peak loads leads performance degradation. Periodically.
> 

Buffering and I/O scheduling is all done by the system disk controller
AFAICT. Squid is just accumulating a queue of I/O requests while it
waits for the controller to signal that its ready for them. Then that
lot get sent to it. Et, loop..

It may be disk driver related. Or on the other end if Squid is too busy
processing other stuff when the FD signals come back from the disk
controller indicating the read()/write() API calls need to happen to
pick the data up.

Its running on libpthreads, which I know has issues on some systems. But
theres nothing we can do about that bit, and thing are improving slowly
anyway.

Amos


From stan.prescott at gmail.com  Wed Jul 15 15:38:36 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 15 Jul 2015 10:38:36 -0500
Subject: [squid-users] ufdbGuard cannot check ssl certs
Message-ID: <CANLNtGRtPv5m=rPBC2ou1OBKMQ1Bcp2kozbVTEFxEWRVpasTWQ@mail.gmail.com>

Hi all.

I've seen some folks asking questions about ufdbGuard and squidGuard here,
so I thought I would give it a try, too.

I am trying to integrate ufdbGuard to replace a working install of
squidGuard on our Smoothwall Express firewall distro with Squid 3.5.5.
Hopefully, if I can get it working, it is something I will be able to
provide to the Smoothwall community for our thousands of users. Maybe some
of those will subscribe to the URLFilterDB? :-)

Anyway, I am to the point where I am trying to start ufdbGuard from the
command line just using "/usr/sbin/ufdbGuard" for testing. I finally got
rid of all error messages except one,

FATAL Error: Cannot perform mandatory check of SSL certificates ****
Core dumped

I have read through the ufdbGuard reference manual and googled the error
and can't seem to find anything that deals with troubleshooting this error.

My compile options are

--prefix=/usr --with-ufdb-user=squid --localstatedir=/var/smoothwall \
               --with-ufdb-config=/var/smoothwall/ufdbguard \
               --with-ufdb-logdir=/var/log/ufdbguard
--with-ufdb-dbhome=/var/smoothwall/ufdbguard/blacklists \
               --with-ufdb-images_dir=/httpd/html/ui/img/ufdbguard
--with-ufdb-piddir=/var/run

Am I just not starting ufdbGuard correctly? Is it something else that
causes ufdbGuard from being unable to check the SSL certs?

Thanks.

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150715/58179a96/attachment.htm>

From stan.prescott at gmail.com  Wed Jul 15 15:42:51 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 15 Jul 2015 10:42:51 -0500
Subject: [squid-users] Compiling squid with 'url_rewrite' support?
In-Reply-To: <49183831.2662012.1436973477777.JavaMail.zimbra@2keys.ca>
References: <29855693.2654655.1436971371322.JavaMail.zimbra@2keys.ca>
 <55A674BE.90102@treenet.co.nz> <55A675A7.40005@gmail.com>
 <1046753052.2660784.1436973307935.JavaMail.zimbra@2keys.ca>
 <138271822.2661623.1436973428375.JavaMail.zimbra@2keys.ca>
 <49183831.2662012.1436973477777.JavaMail.zimbra@2keys.ca>
Message-ID: <CANLNtGTCvf3t57jf5pqtKYzXaofxWcDBpt5kNeBT6VHD=9G0wQ@mail.gmail.com>

I understand that the pricing option is only for the URLFilter database
they maintain. The ufdbGuard itself is free open source. You can use other
databases if you convert them to the ufdbGuard format using ufdbConvert

On Wed, Jul 15, 2015 at 10:17 AM, Michael Monette <mmonette at 2keys.ca> wrote:

> Spoke to soon, there is a free one. Going to give it a shot, thanks again.
>
> ----- Original Message -----
> From: "Michael Monette" <mmonette at 2keys.ca>
> To: "Michael Monette" <mmonette at 2keys.ca>
> Cc: "Yuri Voinov" <yvoinov at gmail.com>, "squid-users" <
> squid-users at lists.squid-cache.org>
> Sent: Wednesday, July 15, 2015 11:17:08 AM
> Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?
>
> I love how they say ufdbguard is Free and Open Source...followed by a
> pricing option.
>
>
>
> ----- Original Message -----
> From: "Michael Monette" <mmonette at 2keys.ca>
> To: "Yuri Voinov" <yvoinov at gmail.com>
> Cc: "squid-users" <squid-users at lists.squid-cache.org>
> Sent: Wednesday, July 15, 2015 11:15:07 AM
> Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?
>
> Cool!
>
> I'm just beginning to explore these things, so I am glad I asked the
> question. I am going to check out ufbdguard now.
>
> Thank you all
>
> ----- Original Message -----
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: "squid-users" <squid-users at lists.squid-cache.org>
> Sent: Wednesday, July 15, 2015 11:00:55 AM
> Subject: Re: [squid-users] Compiling squid with 'url_rewrite' support?
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Confirm.
>
> ufdbguard is great redirector. It has a bit small problem with some
> reporting tools (like SARG), but nothing important.
>
> 15.07.15 20:57, Amos Jeffries ?????:
> > On 16/07/2015 2:42 a.m., Michael Monette wrote:
> >> Hello,
> >>
> >> This might be a stupid question..
> >>
> >> I started looking at squidGuard. Looks pretty straight forward and
> >> fairly easy to implement it but for some reason I could not get it to
> >> actually work, the blacklists were still being bypassed. I was using
> >> Squid-3.5.4 from source because I need the ssl::server_name ACL. I
> >> tried to install regular squid-3.1.10 from the YUM repo and using the
> >> same config file (disabling the at_step and ssl::servername stuff)
> >> and everything worked right away without needing to touch anything
> >> else in squid.conf. My blacklists were active.
> >>
> >> Do I need to compile squid in a certain way to that url_rewrite
> >> works? Or is that something that works and is enabled by default?
> >
> > Nope, its an always-built component of Squid.
> >
> > SquidGuard is an old and no longer maintained project. It may need
> > patching manually to work with current Squid versions
> > (<http://bugs.squid-cache.org/show_bug.cgi?id=3978> has the patches).
> >
> > ufdbGuard is much more up to date and performant if you actually have to
> > use a tool. Squid can be configured to do itself almost everything the
> > helpers do.
> >
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVpnWnAAoJENNXIZxhPexGZ0cH/3OObwZEiQgn9d13WMy/dIGH
> IB/KhQ9VMYk2YF/mZrhisSMIDb90Y7r2XmWSZHH46ZdbFtoXppBKrzUtqoy9RpkF
> +UuhIHLb5bCIPO2DIFrMVQoF6ACCxL0jfML5LR5WHbwJy+B6u+x5WUERU/dR006W
> 2x2bUsrOz48KrBK4wwb9GFhdJDOs7gTfaClBa1gx5h3x1wtT8FCC0zahOBs3aMy5
> bYSyb22a59gDFhfXPKum32o8Y3tRvUpCID8VSgxpKVeJcZNG8KGmh+vg/jrms5OK
> 6WDIlp9TpfNH/RTALfXyctp9Wr5smJSLkKuYaaAmAWUJMpH+zh7Vs/hblv/kmRg=
> =dPG9
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150715/ac4b76b2/attachment.htm>

From marcus.kool at urlfilterdb.com  Wed Jul 15 15:47:55 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 15 Jul 2015 12:47:55 -0300
Subject: [squid-users] ufdbGuard cannot check ssl certs
In-Reply-To: <CANLNtGRtPv5m=rPBC2ou1OBKMQ1Bcp2kozbVTEFxEWRVpasTWQ@mail.gmail.com>
References: <CANLNtGRtPv5m=rPBC2ou1OBKMQ1Bcp2kozbVTEFxEWRVpasTWQ@mail.gmail.com>
Message-ID: <55A680AB.5090404@urlfilterdb.com>

Hi Stan,

ufdbGuard probably logs more error messages before "Cannot perform mandatory check of SSL certificates"
What are they ?

ufdbGuard then calls abort() which causes a core dump since it found something terribly wrong.

Please reply to me or the ufdbGuard list at http://sourceforge.net/p/ufdbguard/mailman/ufdbguard-support/ instead of this Squid list.

Marcus

On 07/15/2015 12:38 PM, Stanford Prescott wrote:
> Hi all.
>
> I've seen some folks asking questions about ufdbGuard and squidGuard here, so I thought I would give it a try, too.
>
> I am trying to integrate ufdbGuard to replace a working install of squidGuard on our Smoothwall Express firewall distro with Squid 3.5.5. Hopefully, if I can get it working, it is something I will be
> able to provide to the Smoothwall community for our thousands of users. Maybe some of those will subscribe to the URLFilterDB? :-)
>
> Anyway, I am to the point where I am trying to start ufdbGuard from the command line just using "/usr/sbin/ufdbGuard" for testing. I finally got rid of all error messages except one,
>
> FATAL Error: Cannot perform mandatory check of SSL certificates ****
> Core dumped
>
> I have read through the ufdbGuard reference manual and googled the error and can't seem to find anything that deals with troubleshooting this error.
>
> My compile options are
>
> --prefix=/usr --with-ufdb-user=squid --localstatedir=/var/smoothwall \
>                 --with-ufdb-config=/var/smoothwall/ufdbguard \
>                 --with-ufdb-logdir=/var/log/ufdbguard --with-ufdb-dbhome=/var/smoothwall/ufdbguard/blacklists \
>                 --with-ufdb-images_dir=/httpd/html/ui/img/ufdbguard --with-ufdb-piddir=/var/run
>
> Am I just not starting ufdbGuard correctly? Is it something else that causes ufdbGuard from being unable to check the SSL certs?
>
> Thanks.
>
> Stan
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From fredbmail at free.fr  Wed Jul 15 15:49:24 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 15 Jul 2015 17:49:24 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1436972693462-4672268.post@n4.nabble.com>
Message-ID: <1824528065.31565613.1436975364252.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Fred,
> 
> Not sure we'll have free time for testing the previous 3.4, we now
> have
> dozens of boxes to manually upgrade to the 3.5.6...
> yes, we do use the original squid 3.5.6 package, no build mix here.
> 

Ok I will, It would be interesting to understand what happen and if there is something wrong in latest versions 


From marcus.kool at urlfilterdb.com  Wed Jul 15 15:56:10 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 15 Jul 2015 12:56:10 -0300
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A67549.9020204@gmail.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67549.9020204@gmail.com>
Message-ID: <55A6829A.4080704@urlfilterdb.com>



On 07/15/2015 11:59 AM, Yuri Voinov wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Amos,
>
> I think, auds queue must be buffered more better and smoother. On some
> OS (I've tested) peak loads leads performance degradation. Periodically.
>
> That is why I'm not using aufs.

This makes sense if the doubling of the queue happens too fast.

See also my other message.  I think it makes sense to start with a baseline
of 256 and in case that a congestion is detected, double it once every
5 seconds.

Marcus


From eliezer at ngtech.co.il  Wed Jul 15 16:20:13 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 15 Jul 2015 19:20:13 +0300
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A661CC.5040700@gmail.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz> <1436963270979-4672233.post@n4.nabble.com>
 <55A6611F.30507@ngtech.co.il> <55A661CC.5040700@gmail.com>
Message-ID: <55A6883D.1010901@ngtech.co.il>

On 15/07/2015 16:36, Yuri Voinov wrote:
> SSD as squid cache?! You are really rich, man!

Please do separate two things Enterprise level SSD and Desktop SSD.
They are different by nature and they do not tend to "break" easily.
They do have different life spans and Enterprise grade HDDs tend to be 
more reliable in some setups.

Since AUFS and UFS will not work very well with SMP squid I would 
consider using only because of that ROCK.
I am testing and I do not have enough horse power to make sure that 
really high load will break rock cache.

If you do have a set of tests to run and give grounds to some 
assumptions or to remove specuilations I will be happy to run some.

Eliezer



From yvoinov at gmail.com  Wed Jul 15 17:00:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 15 Jul 2015 23:00:15 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A6883D.1010901@ngtech.co.il>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <55A64C43.9020105@treenet.co.nz> <1436963270979-4672233.post@n4.nabble.com>
 <55A6611F.30507@ngtech.co.il> <55A661CC.5040700@gmail.com>
 <55A6883D.1010901@ngtech.co.il>
Message-ID: <55A6919F.7080908@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think,

it's enough datacenter class HDD. Also I use it with mirror option for
speed and reliability in my setup. This is comprehensive enough to
enterprise-level proxy. ;)

Of course, I know you know the separation between two hardware clauses.
;) I know it too. ;)

15.07.15 22:20, Eliezer Croitoru ?????:
> On 15/07/2015 16:36, Yuri Voinov wrote:
>> SSD as squid cache?! You are really rich, man!
>
> Please do separate two things Enterprise level SSD and Desktop SSD.
> They are different by nature and they do not tend to "break" easily.
> They do have different life spans and Enterprise grade HDDs tend to be
more reliable in some setups.
>
> Since AUFS and UFS will not work very well with SMP squid I would
consider using only because of that ROCK.
> I am testing and I do not have enough horse power to make sure that
really high load will break rock cache.
>
> If you do have a set of tests to run and give grounds to some
assumptions or to remove specuilations I will be happy to run some.
>
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVppGfAAoJENNXIZxhPexGfXQH/3i4p8jDd+z4zprlTW9q98vg
bvLli6IxJlPsdHRgyApTwjCHAru6HqlV/DGBJKn+6k0BgfTjJnjOe63GhlK95yXt
Kn8U06cLhy7YL9osLcbWXRveozMiviSdmgHeWVM3ecBy0o0WAYhL8+NHL9a94b8R
JEUodWDX9p+WhaFhzunSo35MAdx6uNePW31WzrWBlS4uJ2EJfqdlT0fJJOMTlJk+
aWTV9wMLNGypeLcIoSuUGCG9a0qmqHdyYCz+XrDfkVu03A9Y8z4rA8HHsH4z9ioq
p7EStlVaJDgkDVXbxNfwIJk6mjcZ6R/T4R55+AAN1KFdv5B1dQ8LPVGf/aDGBX4=
=ypk/
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150715/0f16ce4e/attachment.htm>

From marke at wadafarms.com  Wed Jul 15 21:57:02 2015
From: marke at wadafarms.com (markme)
Date: Wed, 15 Jul 2015 14:57:02 -0700 (PDT)
Subject: [squid-users] Blocked DNS request from IDS causes Squid to not work
Message-ID: <1436997422360-4672281.post@n4.nabble.com>

I've been running Squid 3.3.8 on CentOS 7 for a few months now and every now
and then I will get a "Suspicious .pw DNS query" alert from my IDS which was
caused by Squid and it will be blocked. When this happens most clients start
to get a 503 error or NONE_ABORTED/000 in the access log and they can't
access the internet. To fix it I have just been issuing a reconfigure on
Squid and that seems to fix the problem until it happens again. Just that
one particular DNS query to our local DNS server gets blocked but everything
else goes through. Any ideas on what might be making Squid require a
reconfigure to start working again? Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Blocked-DNS-request-from-IDS-causes-Squid-to-not-work-tp4672281.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tmblue at gmail.com  Wed Jul 15 23:38:22 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 15 Jul 2015 16:38:22 -0700
Subject: [squid-users] 3.5.6 still get if-cached errors sourced from a
	sibling.
Message-ID: <CAEaSS0ZW2pFoqbKgvgdJ7RLek7EdE191U42hZxV0qHxy9xvyhA@mail.gmail.com>

Was hoping this was fixed in 3.5.6

The following error was encountered while trying to retrieve the URL:
http://view-dev.eng.domain.net/rimfire/adm/search?
<http://view-dev.eng.admission.net/rimfire/admission/search?>

*Valid document was not found in the cache and only-if-cached directive was
specified.*

You have issued a request with a only-if-cached cache control directive.
The document was not found in the cache, *or* it required revalidation
prohibited by the only-if-cached directive.

Your cache administrator is webmaster
<webmaster?subject=CacheErrorInfo%20-%20ERR_ONLY_IF_CACHED_MISS&body=CacheHost%3A%20devcache01%0D%0AErrPage%3A%20ERR_ONLY_IF_CACHED_MISS%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Wed,%2015%20Jul%202015%2023%3A35%3A21%20GMT%0D%0A%0D%0AClientIP%3A%2010.13.5.103%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Frimfire%2Fadmission%2Fsearch%3Foffset%3D0%26pa%3Doffers%26classification%3Doffer%26status%3DA%26maxadresults%3D25%26params%3D*%26sortby%3Dlisting_seq_id%26sortdir%3D-1%26filler%3D0%26format%3Djs%26onafter%3D%2524P%2528%2522AdvancedSearch%2522%2529.receiveResults%2528response%2529%26random%3D1437002462.097%26encode%3Dgzip%26tags%3Dmakechevrolet%2526customtest%26startdate%3D01%2F01%2F1970%26count%3D1%20HTTP%2F1.1%0AHost%3A%20view-dev.eng.admission.net%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,image%2Fwebp,*%2F*%3Bq%3D0.8%0D%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010_10_2)%20AppleWebKit%2F537.36%20(KHTML,%20like%20Gecko)%20Chrome%2F43.0.2357.134%20Safari%2F537.36%0D%0ADNT%3A%201%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20sdch%0D%0AAccept-Language%3A%20en-US,en%3Bq%3D0.8%0D%0ACookie%3A%20dtuid%3D1430258574364662427%0D%0AVia%3A%201.1%20devcache02-80%20(squid%2F3.5.6)%0D%0ASurrogate-Capability%3A%20devcache02%3D%22Surrogate%2F1.0%20ESI%2F1.0%22%0D%0AX-Forwarded-For%3A%2010.13.16.22%0D%0ACache-Control%3A%20max-age%3D0,%20only-if-cached%0D%0AConnection%3A%20keep-alive%0D%0A%0D%0A%0D%0A>
.


This only happens when I use HTCP between 2 siblings that otherwise have
paths to other parents which are origins.


This should work and thought that the release notes indicated this was
fixed.


Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150715/90fd214c/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 16 00:26:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 12:26:50 +1200
Subject: [squid-users] Blocked DNS request from IDS causes Squid to not
 work
In-Reply-To: <1436997422360-4672281.post@n4.nabble.com>
References: <1436997422360-4672281.post@n4.nabble.com>
Message-ID: <55A6FA4A.2090207@treenet.co.nz>

On 16/07/2015 9:57 a.m., markme wrote:
> I've been running Squid 3.3.8 on CentOS 7 for a few months now and every now
> and then I will get a "Suspicious .pw DNS query" alert from my IDS which was
> caused by Squid and it will be blocked. When this happens most clients start
> to get a 503 error or NONE_ABORTED/000 in the access log and they can't
> access the internet. To fix it I have just been issuing a reconfigure on
> Squid and that seems to fix the problem until it happens again. Just that
> one particular DNS query to our local DNS server gets blocked but everything
> else goes through. Any ideas on what might be making Squid require a
> reconfigure to start working again? Thanks!

Assertions and other detectable errors are logged in your cache.log.
Segmentation faults should be logged in your syslog (or OS messages log).

PS. please upgrade <http://wiki.squid-cache.org/KnowledgeBase/CentOS>

Amos



From squid3 at treenet.co.nz  Thu Jul 16 00:31:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Jul 2015 12:31:52 +1200
Subject: [squid-users] 3.5.6 still get if-cached errors sourced from a
 sibling.
In-Reply-To: <CAEaSS0ZW2pFoqbKgvgdJ7RLek7EdE191U42hZxV0qHxy9xvyhA@mail.gmail.com>
References: <CAEaSS0ZW2pFoqbKgvgdJ7RLek7EdE191U42hZxV0qHxy9xvyhA@mail.gmail.com>
Message-ID: <55A6FB78.9020307@treenet.co.nz>

On 16/07/2015 11:38 a.m., Tory M Blue wrote:
> Was hoping this was fixed in 3.5.6
> 
> The following error was encountered while trying to retrieve the URL:
> http://view-dev.eng.domain.net/rimfire/adm/search?
> <http://view-dev.eng.admission.net/rimfire/admission/search?>
> 
> *Valid document was not found in the cache and only-if-cached directive was
> specified.*

This is not an error. This is what the Cache-Control:only-if-cached
explicitly asks the sibling proxy to respond with.


> 
> This only happens when I use HTCP between 2 siblings that otherwise have
> paths to other parents which are origins.
> 
> 
> This should work and thought that the release notes indicated this was
> fixed.

Nope. The bug is still open
<http://bugs.squid-cache.org/show_bug.cgi?id=4223>

Amos



From s.kirschner at afa-finanz.de  Thu Jul 16 14:51:59 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Thu, 16 Jul 2015 14:51:59 +0000
Subject: [squid-users] Peek and Splice error SSL_accept failed
Message-ID: <2F3AADF230295040BDC74C6F96094F3D021DA900@SRVEXAFA.verwaltung.afa-ag.loc>

Hi I?m using squid with version 3.5.6 in an debian test system.

I try to bypass some sites using the "ssl::server_name" acl , to do that I need to peek the connection first to decide if should be spliced or bumped.

But if I use peek at Step 1 , errors "client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed." errors appear in the cache.log

Squid was built with following options
./configure --build=x86_64-linux-gnu \
--prefix=/usr \
--includedir=${prefix}/include \
--mandir=${prefix}/share/man \
--infodir=${prefix}/share/info \
--sysconfdir=/etc \
--localstatedir=/var \
--libexecdir=${prefix}/lib/squid3 \
--srcdir=. \
--disable-maintainer-mode \
--disable-dependency-tracking \
--disable-silent-rules \
--datadir=/usr/share/squid3 \
--sysconfdir=/etc/squid3 \
--mandir=/usr/share/man \
--enable-inline \
--disable-arch-native \
--enable-async-io=8 \
--enable-storeio=ufs,aufs,diskd,rock \
--enable-removal-policies=lru,heap \
--enable-delay-pools \
--enable-cache-digests \
--enable-icap-client \
--enable-follow-x-forwarded-for \
--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB \
--enable-auth-digest=file,LDAP \
--enable-auth-negotiate=kerberos,wrapper \
--enable-auth-ntlm=fake,smb_lm \
--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group \
--enable-url-rewrite-helpers=fake \
--enable-eui \
--enable-esi \
--enable-icmp \
--enable-zph-qos \
--enable-ecap \
--disable-translation \
--with-swapdir=/var/spool/squid3 \
--with-logdir=/var/squid/logs \
--with-pidfile=/var/run/squid3.pid \
--with-filedescriptors=65536 \
--with-large-files \
--with-default-user=proxy \
--with-openssl \
--with-open-ssl=/etc/ssl/openssl.cnf \
--enable-ssl-crtd \
--enable-linux-netfilter \
'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' \
'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' \
'CPPFLAGS=-D_FORTIFY_SOURCE=2' \
'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'

The squid.conf
http_port 192.168.1.104:3128 intercept
https_port 192.168.1.104:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=10MB cert=/etc/squid3/ssl_cert/myCA.pem
http_port 127.0.0.1:3120

icp_port 0
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user proxy
cache_effective_group proxy
error_default_language de-de
visible_hostname pfsense
cache_mgr admin at test
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /lib/squid3/pinger
sslproxy_capath /etc/ssl/certs
sslcrtd_program /lib/squid3/ssl_crtd -s /var/squid/certs -M 4MB -b 2048
sslproxy_cert_error allow all


logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  192.168.1.0/24
forwarded_for on
uri_whitespace strip

cache_mem 30 MB
maximum_object_size_in_memory 128 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
cache_dir ufs /var/squid/cache 100 16 256
minimum_object_size 0 KB
maximum_object_size 400 KB
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

# Setup some default acls
# From 3.2 further configuration cleanups have been done to make things easier and safer. The manager, localhost, and to_localhost ACL definitions are now built-in.
# acl localhost src 127.0.0.1/32
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128 3127 1025-65535
acl sslports port 443 563

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS
acl allowed_subnets src 192.168.1.0/24
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

# Debugging if needeed
debug_options all,2 16,0 18,0 19,0 22,0 47,0 79,0

# Setup allowed acls
# Allow local network(s) on interface(s)
http_access allow allowed_subnets
http_access allow localnet
# Default block all to be sure
http_access deny allsrc

acl step1 at_step SslBump1
acl step3 at_step SslBump3
acl bypass ssl::server_name .sparkasse.de, .internet-filiale.net

ssl_bump peek step1
ssl_bump splice bypass
ssl_bump bump step3

always_direct allow all
ssl_bump bump all
ssl_bump server-first

Mit freundlichen Gr??en / Best Regards

Sebastian?


From fredbmail at free.fr  Thu Jul 16 15:00:54 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 16 Jul 2015 17:00:54 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1824528065.31565613.1436975364252.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>


> > 
> > Not sure we'll have free time for testing the previous 3.4, we now
> > have
> > dozens of boxes to manually upgrade to the 3.5.6...
> > yes, we do use the original squid 3.5.6 package, no build mix here.
> > 
> 
> Ok I will, It would be interesting to understand what happen and if
> there is something wrong in latest versions
> _______________________________________________


Hi all,

Quickly tested with squid 3.5.6, yes there is a big difference  
Without any other load, just my test - wget with proxy 127.0.0.1 (no lan) - Debian wheezy 32 bits


aufs: 

[16/Jul/2015:16:15:25 +0200] "GET http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe HTTP/1.0" 200 10096328 TCP_HIT:HIER_NONE

100%[===========================================================================================================================================================================>] 10,095,849  26.1M/s   in 0.4s    

2015-07-16 16:15:25 (26.1 MB/s) - `youtube_downloader_hd_setup-2.9.9.23.exe.4' saved [10095849/10095849]


real	0m0.385s
user	0m0.028s
sys	0m0.164s


diskd:

[16/Jul/2015:16:17:29 +0200] "GET http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe HTTP/1.0" 200 10096328 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"


Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe.6'

100%[============================================================================================>] 10,095,849   901K/s   in 14s     

2015-07-16 16:17:29 (684 KB/s) - `youtube_downloader_hd_setup-2.9.9.23.exe.6' saved [10095849/10095849]


real	0m14.426s
user	0m0.016s
sys	0m0.156s

In this case it's better without cache. 

I understand that aufs should be better but diskd is unusable ...

I also found something else, with squid 3.4.13 diskd start the download slowly but after a short time it is fast, with 3.5.6 the download is always slow  
In my bench aufs is always faster (very) than diskd, no matter 3.4.3 or 3.5.6, but with high load I'm worried about warning messages in cache.log, diskd seems more stable in this case.  



From johnzeng2013 at yahoo.com  Thu Jul 16 15:27:58 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 16 Jul 2015 23:27:58 +0800
Subject: [squid-users] a problem about reverse proxy  and  $.ajax
Message-ID: <55A7CD7E.5060302@yahoo.com>


Hello dear All :

i am writing testing download rate program recently ,

and i hope use reverse proxy ( squid 3.5.x) too ,

but if we use reverse proxy and i found Ajax won[t succeed to download

, and success: function(html,textStatus) -- return value ( html ) is blank .


if possible , please give me some advisement .



squid config

http_port 4432 |accel| vport defaultsite=10.10.130.91
|cache_peer 127.0.0.1 parent 80 0 default name=ubuntu-lmr  |



Ajax config

$.ajax({
type: "GET",
url: load_urlv,
cache: false,
mimeType: 'text/plain; charset=x-user-defined',

beforeSend: function(){
$('#time0').html('<blink>download file...</blink>').show();
},

error: function(){
alert('Error loading XML document');
},
success: function(html,textStatus)
{

...........................

}




From yvoinov at gmail.com  Thu Jul 16 16:30:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 16 Jul 2015 22:30:22 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A7DC1E.2030303@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Fred.

It's depending your OS.

Depending your hardware.

Depending your OS configuration.

Tuning is very complex problem and tuning is EVIL.

Remember it.


PS. On MY platform diskd is the single choise. And it's very fast.

16.07.15 21:00, FredB ?????:


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVp9wLAAoJENNXIZxhPexGFsoH/1byROH4moelHGodjzNeite9
6CSEfGKMqmUhQniTYdq/wMdx3HmSKK0irl+UQReMGjh6hB+goT4ZUZds7ZNjI0D+
sNiKs8ewPbX3KBDp5gh4h5eXUyugI+KCHZDLIqvSltzndGz3HAsqKlEZmDWKCvja
b6rn1lSKyHfdfRG5myQCwHX3nkxypwjga+4sIKyxSQkl41K3cs8JtNDzTPIKQFTX
CdtWs2uTbIOKQyS5nq0+KkcNCiJ7Urs2/qpyZFIIUSuQQos7h+SL7fiTdYFuyE+m
Mym+eBiPBd0CkWTPB/6GsQzXGdxWA9arSbDAHeUE3tMBK25jCytx1I87BvBD5s8=
=SPzr
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150716/fe06ae9b/attachment.htm>

From yvoinov at gmail.com  Thu Jul 16 16:30:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 16 Jul 2015 22:30:22 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A7DC1E.7060000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Fred.

It's depending your OS.

Depending your hardware.

Depending your OS configuration.

Tuning is very complex problem and tuning is EVIL.

Remember it.


PS. On MY platform diskd is the single choise. And it's very fast. 0.1
sec latency.

16.07.15 21:00, FredB ?????:
>
>>>
>>> Not sure we'll have free time for testing the previous 3.4, we now
>>> have
>>> dozens of boxes to manually upgrade to the 3.5.6...
>>> yes, we do use the original squid 3.5.6 package, no build mix here.
>>>
>>
>> Ok I will, It would be interesting to understand what happen and if
>> there is something wrong in latest versions
>> _______________________________________________
>
>
> Hi all,
>
> Quickly tested with squid 3.5.6, yes there is a big difference 
> Without any other load, just my test - wget with proxy 127.0.0.1 (no
lan) - Debian wheezy 32 bits
>
>
> aufs:
>
> [16/Jul/2015:16:15:25 +0200] "GET
http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
HTTP/1.0" 200 10096328 TCP_HIT:HIER_NONE
>
>
100%[===========================================================================================================================================================================>]
10,095,849  26.1M/s   in 0.4s   
>
> 2015-07-16 16:15:25 (26.1 MB/s) -
`youtube_downloader_hd_setup-2.9.9.23.exe.4' saved [10095849/10095849]
>
>
> real    0m0.385s
> user    0m0.028s
> sys    0m0.164s
>
>
> diskd:
>
> [16/Jul/2015:16:17:29 +0200] "GET
http://ec.ccm2.net/www.commentcamarche.net/download/files/youtube_downloader_hd_setup-2.9.9.23.exe
HTTP/1.0" 200 10096328 TCP_HIT:HIER_NONE "Wget/1.13.4 (linux-gnu)"
>
>
> Saving to: `youtube_downloader_hd_setup-2.9.9.23.exe.6'
>
>
100%[============================================================================================>]
10,095,849   901K/s   in 14s    
>
> 2015-07-16 16:17:29 (684 KB/s) -
`youtube_downloader_hd_setup-2.9.9.23.exe.6' saved [10095849/10095849]
>
>
> real    0m14.426s
> user    0m0.016s
> sys    0m0.156s
>
> In this case it's better without cache.
>
> I understand that aufs should be better but diskd is unusable ...
>
> I also found something else, with squid 3.4.13 diskd start the
download slowly but after a short time it is fast, with 3.5.6 the
download is always slow 
> In my bench aufs is always faster (very) than diskd, no matter 3.4.3
or 3.5.6, but with high load I'm worried about warning messages in
cache.log, diskd seems more stable in this case. 
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVp9wcAAoJENNXIZxhPexG5SwH/0tHln3GBuPeCEdZb7vAocMQ
K5EVfS1TMqOnsud4CQBrUbzaBDqks016MCYgi/nUqmgGquTpLFi58SRDvnsv7rF1
4sVUeGCzBOAJFa0mwTwigVlTaShesq6kJlCmMEiUu9JDPk/n/+WrJ27GfJIeJ5VT
Cs+G6zet9ITvWCklZ8ml1vLX+K72QLoVA7JvbCqHmsxqWlmL82EPSq0+c5vQt8Va
jtjVPYFYncofyrgSbMNX7C3hqm8yt1+0+EHGGfT88fOG9Xe3prqbKWrfWmDqWsea
PmUdeQvmET2b6j5F5uSLSYjL17hFNl56vP7VuYxJ3lbbqsZfR/ihqF1+dbNjRwk=
=joRM
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150716/26e79fa5/attachment.htm>

From vdoctor at neuf.fr  Thu Jul 16 17:12:24 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 16 Jul 2015 10:12:24 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com>
 <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A667BC.8060708@gmail.com>
 <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436970719948-4672259.post@n4.nabble.com>
 <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436972693462-4672268.post@n4.nabble.com>
 <1824528065.31565613.1436975364252.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1437066744925-4672291.post@n4.nabble.com>

Hi Fred,
Same results from our side...
Does it mean we should catch the diskd engine from the 3.4.x and apply it
with the 3.5.x ? 
Should be a good try to see if it works....

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672291.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Thu Jul 16 17:13:59 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 16 Jul 2015 19:13:59 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A7DC1E.7060000@gmail.com>
Message-ID: <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
> 
> Fred.
> 
> It's depending your OS.
> 
> Depending your hardware.
> 
> Depending your OS configuration.
> 
> Tuning is very complex problem and tuning is EVIL.
> 
> Remember it.
> 


Yuri. my tests are very very basic

I think in this case this is not a matter of tuning, Squid 3.4.13 and squid 3.5.6, same OS, same config, same object downloaded, same cache size, no other user, no lan - directly connected -
The only difference is aufs/diskd in squid.conf
Result, very slow with 3.5.6 and diskd

Top

1 - squid 3.4.13/3.5.6 + aufs -> Fast, but with many users a lot of messages in cache.log, stress me ;)
2 - squid 3.4.13 -> diskd -> more slow
3 - squid 3.5.6 -> diskd -> very slow, unusable ?

Maybe my other problem with aufs (warning messages) needs tuning but I don't know what to do exactly.

As other Fred, I see that there is something strange, maybe you are right but I'm curious to know what special config (not needed before) I should put, my Internet access - 100 mbps - is, very, more faster than my direct access disk (again there is no other user in my test machine).

I hope I will find time to test rock store soon, with and without high load.

Please Amos, can you just make a test with the latest version ? Maybe there is something wrong here - and for the other Fred too -, just downloading a "large" file in cache without any other user, with aufs and also diskd ,if you can directly with loopback address.   




From yvoinov at gmail.com  Thu Jul 16 17:19:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 16 Jul 2015 23:19:12 +0600
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55A7E790.4020905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
In my case diskd only choice. On my platform aufs does not work at all.
And diskd gives the best result after careful tuning.

As I said earlier, the result is highly dependent on the platform,
hardware, and configuration. diskd was designed for a well-defined
process model. And on this platform it shows the best results in the
case of direct hand administrator. QED. Of course, ordinary
administrators need something simple out of the box. ;)

Extreme Tuning is a piece product that is impossible to get out of the
box. It takes effort and thought. ;)

Do not expect crazy results from the default settings and look for the
magic bullet. It does not exist, as is well known experienced
administrators.

16.07.15 23:13, FredB ?????:
>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>>
>> Fred.
>>
>> It's depending your OS.
>>
>> Depending your hardware.
>>
>> Depending your OS configuration.
>>
>> Tuning is very complex problem and tuning is EVIL.
>>
>> Remember it.
>>
>
>
> Yuri. my tests are very very basic
>
> I think in this case this is not a matter of tuning, Squid 3.4.13 and
squid 3.5.6, same OS, same config, same object downloaded, same cache
size, no other user, no lan - directly connected -
> The only difference is aufs/diskd in squid.conf
> Result, very slow with 3.5.6 and diskd
>
> Top
>
> 1 - squid 3.4.13/3.5.6 + aufs -> Fast, but with many users a lot of
messages in cache.log, stress me ;)
> 2 - squid 3.4.13 -> diskd -> more slow
> 3 - squid 3.5.6 -> diskd -> very slow, unusable ?
>
> Maybe my other problem with aufs (warning messages) needs tuning but I
don't know what to do exactly.
>
> As other Fred, I see that there is something strange, maybe you are
right but I'm curious to know what special config (not needed before) I
should put, my Internet access - 100 mbps - is, very, more faster than
my direct access disk (again there is no other user in my test machine).
>
> I hope I will find time to test rock store soon, with and without high
load.
>
> Please Amos, can you just make a test with the latest version ? Maybe
there is something wrong here - and for the other Fred too -, just
downloading a "large" file in cache without any other user, with aufs
and also diskd ,if you can directly with loopback address.  
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVp+eQAAoJENNXIZxhPexGns4H/3f9IhhTBUFZL4xHQoihbahU
D2j2sQ88K3KI38MQx9vCOXLW3yuXGTFwStAPQveGxmx9869rPmtOHL+MhbF0pvq6
+bVhkYbAmSyBXTIYx3T+0utdMnWY/aV2sLwBfOyEPuL5NQzIJNI60Fg+zhQDgeiW
LPxMaceRfNc4+zjwp935lyHcvbnLVmW/l5w3yGBoOIqkPhZk03me6jbYjJ4Szzz/
pNna6smmQm92sDy7L8qwxW5cHKnkg7CSpCJt40Cpwke7jvMXy6xEfsOgtR1vcuO4
vEtbZwWSknFcObzeToYhrIuYCdY4lU7mmCaYu9wyPQMsQnCtSDBEETZ/DBevwRc=
=CHxj
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Thu Jul 16 17:31:45 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 16 Jul 2015 10:31:45 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1673755973.31313976.1436968737868.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A667BC.8060708@gmail.com>
 <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436970719948-4672259.post@n4.nabble.com>
 <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436972693462-4672268.post@n4.nabble.com>
 <1824528065.31565613.1436975364252.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A7DC1E.7060000@gmail.com>
 <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1437067905093-4672294.post@n4.nabble.com>

Fred,
The AUFS works for us, we switched all our clients back to the AUFS from
DISKD.
Yes, there are some Queue congestions at the squid restart (during 30 min
maxi), but as Amos said the Squid will re-adapt its internal value to fit
the traffic, I can confirm that point.
After a while, the queue congestion disapears but we see many info from the
cache.log saying objects are not found but here I think we don't really care
as the Squid is smart enough to correct its index file...

We have ISPs with 1-2Gbps bandwidth, no complain anymore, only "Thx guys,
great job !"


Amos, all the Squid team,
You have fixed the issues with the AUFS (isEmpty, ...), so thanks to all of
you because with a previous unstable aufs, a still slow diskd and a bugged
rock, it was not easy to have a stable squid cache with good performances. 

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672294.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Thu Jul 16 18:29:54 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jul 2015 12:29:54 -0600
Subject: [squid-users] cannot use squid-3.5.x for production
In-Reply-To: <27680022.28371.1435754986747.JavaMail.webmail@bluewin.ch>
References: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>\r\n\t<21463528.6472.1435733021760.JavaMail.webmail@bluewin.ch>\r\n
 <CACLJR+Pr2FOZY0+n21M9Ympw-H2oFR0EkZUYDX5tHLRAtPQs2A@mail.gmail.com>\r\n
 <21842932.20345.1435742684665.JavaMail.webmail@bluewin.ch>
 <5593DB72.1080401@treenet.co.nz>
 <27680022.28371.1435754986747.JavaMail.webmail@bluewin.ch>
Message-ID: <55A7F822.7030705@measurement-factory.com>

On 07/01/2015 06:49 AM, Othmar Truniger wrote:

> I'm glad I got your attention on this. I realized that when I did a
> quick code comparison. I also hope we get a quick fix for this
> because I would like to upgrade to a supported version soon.

I have updated the bug report with the current status
http://bugs.squid-cache.org/show_bug.cgi?id=4279

Sorry, I am not promising a quick fix at this time.

Alex.



From fredbmail at free.fr  Thu Jul 16 19:07:12 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 16 Jul 2015 21:07:12 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1437067905093-4672294.post@n4.nabble.com>
Message-ID: <227537345.210367.1437073632682.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Fred,
> The AUFS works for us, we switched all our clients back to the AUFS
> from
> DISKD.
> Yes, there are some Queue congestions at the squid restart (during 30
> min
> maxi), but as Amos said the Squid will re-adapt its internal value to
> fit
> the traffic, I can confirm that point.
> After a while, the queue congestion disapears but we see many info
> from the
> cache.log saying objects are not found but here I think we don't
> really care
> as the Squid is smart enough to correct its index file...
> 
> We have ISPs with 1-2Gbps bandwidth, no complain anymore, only "Thx
> guys,
> great job !"
> 
> 
> Amos, all the Squid team,
> You have fixed the issues with the AUFS (isEmpty, ...), so thanks to
> all of
> you because with a previous unstable aufs, a still slow diskd and a
> bugged
> rock, it was not easy to have a stable squid cache with good
> performances.
> 
>

Hi,

Interesting , but I was worried about 

2015/07/15 13:36:07 kid1| DiskThreadsDiskFile::openDone: (2) No such file or directory

No message like this with 3.5 ?


From vdoctor at neuf.fr  Thu Jul 16 19:18:27 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 16 Jul 2015 12:18:27 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <227537345.210367.1437073632682.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1383270172.31363800.1436970046768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436970719948-4672259.post@n4.nabble.com>
 <1269061953.31416330.1436971548101.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436972693462-4672268.post@n4.nabble.com>
 <1824528065.31565613.1436975364252.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1809949937.33655301.1437058854230.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <55A7DC1E.7060000@gmail.com>
 <1984281001.57149.1437066839744.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1437067905093-4672294.post@n4.nabble.com>
 <227537345.210367.1437073632682.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1437074307399-4672297.post@n4.nabble.com>

Hi,

By "cache.log saying objects are not found" I meant
"DiskThreadsDiskFile::openDone: (2) No such file or directory".
(je n'avais plus le message en tete...)
Yes, still this message but it disapears at least 30 minutes later. So not a
problem to us with clients.

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672297.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Thu Jul 16 23:40:59 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 16 Jul 2015 16:40:59 -0700 (PDT)
Subject: [squid-users] redirect TCP_NONE
Message-ID: <1437090059610-4672298.post@n4.nabble.com>

i have an idea for solve problems with sites and app's that work on port 443
but cant establish connection with squid,
i see that when this connection cant established the TCP_NONE appear in
access.log,
then why we cant use an option that when this tcp_none come on some app
redirect it to TCP_TUNNEL and then it will bypassed and the connection will
be established without decryption but at minimum it will work automatically
without make to that ip ssl_bump none x.x.x.x
who support me ? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/redirect-TCP-NONE-tp4672298.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Fri Jul 17 00:03:37 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 16 Jul 2015 17:03:37 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1436433452435-4672125.post@n4.nabble.com>
References: <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435549243614-4671937.post@n4.nabble.com>
 <5590C6B1.1020706@treenet.co.nz> <1435575418899-4671942.post@n4.nabble.com>
 <55913789.7040605@treenet.co.nz> <1435650994449-4671958.post@n4.nabble.com>
 <1435651056189-4671959.post@n4.nabble.com> <559256E0.7070506@treenet.co.nz>
 <1436433452435-4672125.post@n4.nabble.com>
Message-ID: <1437091417884-4672299.post@n4.nabble.com>

using
range_offset_limit none
ovet HTTP sites work without any assertoin error

but using it with HTTPS sites make this assertion error,
so there are problem between this option and 443 connection, 
the problem is in https partial content only



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4672299.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul 17 04:09:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jul 2015 16:09:11 +1200
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <1437090059610-4672298.post@n4.nabble.com>
References: <1437090059610-4672298.post@n4.nabble.com>
Message-ID: <55A87FE7.50300@treenet.co.nz>

On 17/07/2015 11:40 a.m., HackXBack wrote:
> i have an idea for solve problems with sites and app's that work on port 443
> but cant establish connection with squid,
> i see that when this connection cant established the TCP_NONE appear in
> access.log,
> then why we cant use an option that when this tcp_none come on some app
> redirect it to TCP_TUNNEL and then it will bypassed and the connection will
> be established without decryption but at minimum it will work automatically
> without make to that ip ssl_bump none x.x.x.x
> who support me ? 

TCP_TUNNEL means TCP packets being passed through a CONNECT tunnel. No
TLS involvement in any way.

What you are thinking of would be labeled "TLS_SPLICE" (if we had such
labels - since we dont it gets "NONE"). Where Squid is mediating between
two TLS encrypted tunnels, has touched the non-crypted parts without
actively decrypting the payload.

The case where Squid peeks at the first few bytes and sees immediately
that its not even TLS, (or have configured "ssl_bump none" to happen)
will already TCP_TUNNEL automatically in Squid-3.5+.

Amos



From squid3 at treenet.co.nz  Fri Jul 17 04:24:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jul 2015 16:24:26 +1200
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
Message-ID: <55A8837A.1080901@treenet.co.nz>

On 19/05/2015 5:49 a.m., Andres Granados wrote:
> hello!I need help on how to block pornographic images of google, I
> was trying different options and still do not succeed, try:
> http_reply_access with request_header_add, and even with a
> configuration dns, I think is to request_header_add the best, though
> not it has worked for me, I hope your help, is to implement a school,
> thanks!
> 

FYI; Christos has added a tweak to the "ssl-bump splice" handling that
permits sending the traffic to a cache_peer configured something like this:

 acl example ssl::server_name .example.com
 ssl_bump splice example
 ssl_bump peek all

 cache_peer forcesafesearch.example.com parent 443 0 \
    name=GS \
    originserver no-query no-netdb-exchange no-digest

 acl search dstdomain .example.com
 cache_peer_access GS allow search
 cache_peer_access GS deny all

The idea being that you can use this on intercepted (or forward-proxy)
HTTPS traffic instead of hacking about with DNS to direct clients at the
servers Google use to present "safe" searching.

This should be available in 3.5.7, or the current 3.5 snaphots.

Cheers
Amos


From gkinkie at gmail.com  Fri Jul 17 08:08:02 2015
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 17 Jul 2015 10:08:02 +0200
Subject: [squid-users] a problem about reverse proxy and $.ajax
In-Reply-To: <55A7CD7E.5060302@yahoo.com>
References: <55A7CD7E.5060302@yahoo.com>
Message-ID: <CA+Y8hcMpLkQo7uZqs=VxSnE1=qFPYfze8t+OwNU7ZoqMKdxQDg@mail.gmail.com>

Hi,
  What is in the squid access.log for that request?

On Thu, Jul 16, 2015 at 5:27 PM, johnzeng <johnzeng2013 at yahoo.com> wrote:

>
> Hello dear All :
>
> i am writing testing download rate program recently ,
>
> and i hope use reverse proxy ( squid 3.5.x) too ,
>
> but if we use reverse proxy and i found Ajax won[t succeed to download
>
> , and success: function(html,textStatus) -- return value ( html ) is blank
> .
>
>
> if possible , please give me some advisement .
>
>
>
> squid config
>
> http_port 4432 |accel| vport defaultsite=10.10.130.91
> |cache_peer 127.0.0.1 parent 80 0 default name=ubuntu-lmr  |
>
>
>
> Ajax config
>
> $.ajax({
> type: "GET",
> url: load_urlv,
> cache: false,
> mimeType: 'text/plain; charset=x-user-defined',
>
> beforeSend: function(){
> $('#time0').html('<blink>download file...</blink>').show();
> },
>
> error: function(){
> alert('Error loading XML document');
> },
> success: function(html,textStatus)
> {
>
> ...........................
>
> }
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150717/71746b4c/attachment.htm>

From hack.back at hotmail.com  Fri Jul 17 08:43:02 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 17 Jul 2015 01:43:02 -0700 (PDT)
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <55A87FE7.50300@treenet.co.nz>
References: <1437090059610-4672298.post@n4.nabble.com>
 <55A87FE7.50300@treenet.co.nz>
Message-ID: <1437122582268-4672303.post@n4.nabble.com>

am using Squid-3.5.5
and am still getting TCP_NONE
and not TCP_TUNNEL automatically if packets not decrypted 
then what !



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/redirect-TCP-NONE-tp4672298p4672303.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul 17 09:35:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jul 2015 21:35:50 +1200
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <1437122582268-4672303.post@n4.nabble.com>
References: <1437090059610-4672298.post@n4.nabble.com>
 <55A87FE7.50300@treenet.co.nz> <1437122582268-4672303.post@n4.nabble.com>
Message-ID: <55A8CC76.8010507@treenet.co.nz>

On 17/07/2015 8:43 p.m., HackXBack wrote:
> am using Squid-3.5.5
> and am still getting TCP_NONE
> and not TCP_TUNNEL automatically if packets not decrypted 
> then what !

Stop worrying about the log entry itself? its just a lack of feedback info.

If there is actually a problem with whatever Squid is really doing you
need to look past the access.log at the cache.log traces or TLS protocol
flows themselves to find out the details of it.


Or if its 'just' OCD about access.log being annoying you can join me on
figuring out where to set the LogTags for Squid-4 to report TLS_*
accurately.  :-)

Amos



From hack.back at hotmail.com  Fri Jul 17 09:46:41 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 17 Jul 2015 02:46:41 -0700 (PDT)
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <55A8CC76.8010507@treenet.co.nz>
References: <1437090059610-4672298.post@n4.nabble.com>
 <55A87FE7.50300@treenet.co.nz> <1437122582268-4672303.post@n4.nabble.com>
 <55A8CC76.8010507@treenet.co.nz>
Message-ID: <1437126401118-4672305.post@n4.nabble.com>

yes there is real problem and not only log lines,
you can read my first post i described what is happening ...
all applications in mobiles cant run till we make ssl_bump none to the ip's
that these applications use ..
and this is to much job to do ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/redirect-TCP-NONE-tp4672298p4672305.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul 17 10:35:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jul 2015 22:35:31 +1200
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <1437126401118-4672305.post@n4.nabble.com>
References: <1437090059610-4672298.post@n4.nabble.com>
 <55A87FE7.50300@treenet.co.nz> <1437122582268-4672303.post@n4.nabble.com>
 <55A8CC76.8010507@treenet.co.nz> <1437126401118-4672305.post@n4.nabble.com>
Message-ID: <55A8DA73.6060409@treenet.co.nz>

On 17/07/2015 9:46 p.m., HackXBack wrote:
> yes there is real problem and not only log lines,
> you can read my first post i described what is happening ...
> all applications in mobiles cant run till we make ssl_bump none to the ip's
> that these applications use ..
> and this is to much job to do ..
> 

"Its not working" is back to very unhelpful info again. Specific details
are critical. As is confirming the very latest code still has the problem.


I mean details like;
* "client is sending bytes XYZ instead of a TLS ClientHello packet", or
* "client is sending clientHello with settings U,V which causes Squid to
do W", or
* "server is responding to client E,F,G,M settings with TLS alert Q,
Squid then does R".

TLS is hugely complicated set of details and a whole protocol by itself.
Any one of those could be causing what appears to you as a failure even
with Squid operating perfectly fine. Or Squid may be breaking something
deeply subtle while othewise appearing to run fine.


If I assume this is in regards to one of the other bugs you have brought
up (like the halfClosedReader one). How is Squid to identify in advance
that a certain request (but not the one before it, or after) is going to
hit a bug that we cannot even track down a cause for yet?
 If the problem cause is known, we (mostly Christos) probably already
fixed it. I know its frustrating, but sometimes reality really sucks.

Intercepting TLS in the first place is a nasty can of worms. The
resulting problems are plentiful and new ones continually being added
(it is an actual literal "arms race" situation). The best help we can do
to support Christos whose working on fixing it all is to keep up with
his development progress and trying to avoid noise about already fixed
things. (Sorry dont mean to lecture the choir).


Amos



From squid3 at treenet.co.nz  Fri Jul 17 10:36:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jul 2015 22:36:27 +1200
Subject: [squid-users] a problem about reverse proxy and $.ajax
In-Reply-To: <CA+Y8hcMpLkQo7uZqs=VxSnE1=qFPYfze8t+OwNU7ZoqMKdxQDg@mail.gmail.com>
References: <55A7CD7E.5060302@yahoo.com>
 <CA+Y8hcMpLkQo7uZqs=VxSnE1=qFPYfze8t+OwNU7ZoqMKdxQDg@mail.gmail.com>
Message-ID: <55A8DAAB.20503@treenet.co.nz>

On 17/07/2015 8:08 p.m., Kinkie wrote:
> Hi,
>   What is in the squid access.log for that request?
> 

... and the cache.log output for it when "debug_options 11,2" is used.

Amos

> On Thu, Jul 16, 2015 at 5:27 PM, johnzeng wrote:
> 
>>
>> Hello dear All :
>>
>> i am writing testing download rate program recently ,
>>
>> and i hope use reverse proxy ( squid 3.5.x) too ,
>>
>> but if we use reverse proxy and i found Ajax won[t succeed to download
>>
>> , and success: function(html,textStatus) -- return value ( html ) is blank
>> .
>>
>>
>> if possible , please give me some advisement .
>>
>>
>>
>> squid config
>>
>> http_port 4432 |accel| vport defaultsite=10.10.130.91
>> |cache_peer 127.0.0.1 parent 80 0 default name=ubuntu-lmr  |
>>
>>
>>
>> Ajax config
>>
>> $.ajax({
>> type: "GET",
>> url: load_urlv,
>> cache: false,
>> mimeType: 'text/plain; charset=x-user-defined',
>>
>> beforeSend: function(){
>> $('#time0').html('<blink>download file...</blink>').show();
>> },
>>
>> error: function(){
>> alert('Error loading XML document');
>> },
>> success: function(html,textStatus)
>> {
>>
>> ...........................
>>
>> }
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From laz at paravis.net  Fri Jul 17 13:42:45 2015
From: laz at paravis.net (Laz C. Peterson)
Date: Fri, 17 Jul 2015 06:42:45 -0700
Subject: [squid-users] Redirects error for only some Citrix sites
Message-ID: <93E16A0E-E298-474D-B004-F878FF0D4EF7@paravis.net>

Hello all,

Very weird issue here.  This happens to only select Citrix support articles.  (For example, http://support.citrix.com/article/CTX122972 <http://support.citrix.com/article/CTX122972> when searching Google for ?citrix netscaler expired password? which is the top link in my results, or also searching for the same article directly on Citrix support site.)

This is a new install of Squid 3 on Ubuntu 14.04.2 (from Ubuntu repository).  When clicking the Google link, I get ?too many redirects? error, saying that possibly the page refers to another page that is then redirected back to the original page.

I tried debugging but did not find much useful information.  Has anyone else seen behavior like this?

~ Laz Peterson
Paravis, LLC
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150717/88a8f9a6/attachment.htm>

From hack.back at hotmail.com  Sat Jul 18 13:11:39 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 18 Jul 2015 06:11:39 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
Message-ID: <1437225099222-4672309.post@n4.nabble.com>

cache.log

Squid Cache (Version 3.4.13-20150501-r13224): Terminated abnormally.
CPU Usage: 0.052 seconds = 0.024 user + 0.028 sys
Maximum Resident Size: 83440 KB
Page faults with physical i/o: 0
2015/07/18 23:06:55 kid1| Set Current Directory to /var/spool/squid
2015/07/18 23:06:55 kid1| Starting Squid Cache version
3.4.13-20150501-r13224 for x86_64-unknown-linux-gnu...
2015/07/18 23:06:55 kid1| Process ID 30443
2015/07/18 23:06:55 kid1| Process Roles: worker
2015/07/18 23:06:55 kid1| With 65535 file descriptors available
2015/07/18 23:06:55 kid1| Initializing IP Cache...
2015/07/18 23:06:55 kid1| DNS Socket created at 0.0.0.0, FD 7
2015/07/18 23:06:55 kid1| Adding nameserver 127.0.0.1 from squid.conf
2015/07/18 23:06:55 kid1| helperOpenServers: Starting 40/50 'ssl_crtd'
processes
2015/07/18 23:06:55 kid1| helperOpenServers: Starting 1/1 'rewriter.pl'
processes
2015/07/18 23:06:55 kid1| helperOpenServers: Starting 1/1 'storeid.pl'
processes
2015/07/18 23:06:55 kid1| Logfile: opening log /var/log/squid/access.log
2015/07/18 23:06:55 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/access.log'
FATAL: xcalloc: Unable to allocate 18446744073527142243 blocks of 1 bytes!


my configure option.


./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--with-pthreads --with-aufs-threads=512 --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
--enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
--enable-http-violations --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-swapdir=/var/spool/squid --enable-ltdl-convenience
--with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
--enable-referer-log --enable-truncate --disable-unlinkd
--enable-useragent-log --enable-eui --enable-large-cache-files
'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'


i have 82GB ram in this box
with 9 HDD 
when i remove the half cache_dir it will work , but this server was working
fine as it is so what happen ?
what may cause this error ?
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073527142243-blocks-of-1-bytes-tp4672309.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jul 19 10:03:53 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 03:03:53 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
In-Reply-To: <1437225099222-4672309.post@n4.nabble.com>
References: <1437225099222-4672309.post@n4.nabble.com>
Message-ID: <1437300233773-4672310.post@n4.nabble.com>

removing lines from my configure option make it work,
now i have 
./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-dlmalloc --enable-zph-qos
--with-default-user=proxy --with-logdir=/var/log/squid
--with-pidfile=/var/run/squid.pid --with-swapdir=/var/spool/squid
--with-filedescriptors=200000 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-large-cache-files 'CPPFLAGS=-I/usr/include/openssl'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073527142243-blocks-of-1-bytes-tp4672309p4672310.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jul 19 10:09:04 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 03:09:04 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <5595168D.5030504@treenet.co.nz>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz>
Message-ID: <1437300544323-4672311.post@n4.nabble.com>

but this happen only with version 3.5 , and it increase after restarting
squid or rebooting system
this is bug in 3.5 and it decrease the HIT ratio ,
you dont think so ? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672311.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Jul 19 10:12:13 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 19 Jul 2015 16:12:13 +0600
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
In-Reply-To: <1437300233773-4672310.post@n4.nabble.com>
References: <1437225099222-4672309.post@n4.nabble.com>
 <1437300233773-4672310.post@n4.nabble.com>
Message-ID: <55AB77FD.2020206@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Heh,

too much unknown options are dangerous. :)

19.07.15 16:03, HackXBack ?????:
> removing lines from my configure option make it work,
> now i have
> ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
> --libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
> --libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
> --infodir=/usr/share/info --mandir=/usr/share/man
--enable-storeio=ufs,aufs
> --enable-removal-policies=lru,heap --enable-linux-netfilter
> --enable-follow-x-forwarded-for --enable-dlmalloc --enable-zph-qos
> --with-default-user=proxy --with-logdir=/var/log/squid
> --with-pidfile=/var/run/squid.pid --with-swapdir=/var/spool/squid
> --with-filedescriptors=200000 --enable-ssl --enable-ssl-crtd
--with-openssl
> --enable-large-cache-files 'CPPFLAGS=-I/usr/include/openssl'
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073527142243-blocks-of-1-bytes-tp4672309p4672310.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVq3f8AAoJENNXIZxhPexGH8YH/1qLX5KTnKpw/7Cz6Hh4dG2U
xstxh51+0SzFWO2wurcm6QkMipux3eE1gH4slM7eab+NfZD6Ngg1yGNvVf0Vchgb
6WDa9RppxwDb2w7IfrKblmTAVheeRv7iltqr91RZOmuX36XLpf3LSEBUyWts05tY
2S5sTVgba4cAevOFVCusmpLNszdlzVvwlozdmdFI1T52n30/IPeXpQFXnQAUoyLa
aF7lhSnv/6Dna9/BMUxNasWBRshCzOemWoDP5RnAR49i4NZzFeb2ab+tRcVr+Xc0
FdI/RFqeGHS7EqAliLig7UenJuOBonSzupLNsTprIyJcRbfXZEti0ms0vkhHXVo=
=PF8F
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Sun Jul 19 10:12:39 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 03:12:39 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1437300544323-4672311.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
Message-ID: <1437300759438-4672313.post@n4.nabble.com>

2015/07/19 12:13:14 kid1|       /cache05/2/07/FF/0007FF14
2015/07/19 12:13:15 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:15 kid1|       /cache01/2/03/04/000304B7
2015/07/19 12:13:16 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:16 kid1|       /cache04/1/26/61/002661E8
2015/07/19 12:13:16 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:16 kid1|       /cache01/1/04/9B/00049B18
2015/07/19 12:13:16 kid1| Error negotiating SSL connection on FD 1222:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:17 kid1|       /cache03/1/07/85/0007856F
2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:17 kid1|       /cache02/1/32/8C/00328CCC
2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:17 kid1|       /cache01/1/03/A4/0003A4CD
2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:17 kid1|       /cache01/2/3C/9A/003C9ABD
2015/07/19 12:13:17 kid1| Error negotiating SSL connection on FD 1262:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:19 kid1| Error negotiating SSL connection on FD 634:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:19 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:19 kid1|       /cache02/1/00/F1/0000F18B
2015/07/19 12:13:19 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:19 kid1|       /cache01/2/00/D1/0000D1BA
2015/07/19 12:13:20 kid1| Error negotiating SSL connection on FD 625:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:20 kid1|       /cache03/1/07/85/0007856F
2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:20 kid1|       /cache02/1/32/8C/00328CCC
2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:20 kid1|       /cache01/1/03/A4/0003A4CD
2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:20 kid1|       /cache01/2/3C/9A/003C9ABD
2015/07/19 12:13:21 kid1| Error negotiating SSL connection on FD 426:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:21 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:21 kid1|       /cache02/1/18/82/00188224
2015/07/19 12:13:21 kid1| Error negotiating SSL connection on FD 819:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:23 kid1| Error negotiating SSL connection on FD 813:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:23 kid1| Error negotiating SSL connection on FD 426:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:24 kid1|       /cache02/1/32/8C/00328CCC
2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:24 kid1|       /cache03/1/07/85/0007856F
2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:24 kid1|       /cache01/1/03/A4/0003A4CD
2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:24 kid1|       /cache01/2/3C/9A/003C9ABD
2015/07/19 12:13:25 kid1| Error negotiating SSL connection on FD 187:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:25 kid1| Error negotiating SSL connection on FD 36:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
2015/07/19 12:13:26 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:26 kid1|       /cache02/2/03/41/0003413B
2015/07/19 12:13:28 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2015/07/19 12:13:28 kid1|       /cache02/2/02/97/00029750




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672313.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jul 19 10:13:38 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 03:13:38 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
In-Reply-To: <55AB77FD.2020206@gmail.com>
References: <1437225099222-4672309.post@n4.nabble.com>
 <1437300233773-4672310.post@n4.nabble.com> <55AB77FD.2020206@gmail.com>
Message-ID: <1437300818538-4672314.post@n4.nabble.com>

yes dear you are right



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073527142243-blocks-of-1-bytes-tp4672309p4672314.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Jul 19 10:14:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 19 Jul 2015 16:14:15 +0600
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1437300759438-4672313.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <1437300759438-4672313.post@n4.nabble.com>
Message-ID: <55AB7877.7080805@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Take a look on thread "AUFS vs. DISKD"

19.07.15 16:12, HackXBack ?????:
> 2015/07/19 12:13:14 kid1|       /cache05/2/07/FF/0007FF14
> 2015/07/19 12:13:15 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:15 kid1|       /cache01/2/03/04/000304B7
> 2015/07/19 12:13:16 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:16 kid1|       /cache04/1/26/61/002661E8
> 2015/07/19 12:13:16 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:16 kid1|       /cache01/1/04/9B/00049B18
> 2015/07/19 12:13:16 kid1| Error negotiating SSL connection on FD 1222:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:17 kid1|       /cache03/1/07/85/0007856F
> 2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:17 kid1|       /cache02/1/32/8C/00328CCC
> 2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:17 kid1|       /cache01/1/03/A4/0003A4CD
> 2015/07/19 12:13:17 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:17 kid1|       /cache01/2/3C/9A/003C9ABD
> 2015/07/19 12:13:17 kid1| Error negotiating SSL connection on FD 1262:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:19 kid1| Error negotiating SSL connection on FD 634:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:19 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:19 kid1|       /cache02/1/00/F1/0000F18B
> 2015/07/19 12:13:19 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:19 kid1|       /cache01/2/00/D1/0000D1BA
> 2015/07/19 12:13:20 kid1| Error negotiating SSL connection on FD 625:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:20 kid1|       /cache03/1/07/85/0007856F
> 2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:20 kid1|       /cache02/1/32/8C/00328CCC
> 2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:20 kid1|       /cache01/1/03/A4/0003A4CD
> 2015/07/19 12:13:20 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:20 kid1|       /cache01/2/3C/9A/003C9ABD
> 2015/07/19 12:13:21 kid1| Error negotiating SSL connection on FD 426:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:21 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:21 kid1|       /cache02/1/18/82/00188224
> 2015/07/19 12:13:21 kid1| Error negotiating SSL connection on FD 819:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:23 kid1| Error negotiating SSL connection on FD 813:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:23 kid1| Error negotiating SSL connection on FD 426:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:24 kid1|       /cache02/1/32/8C/00328CCC
> 2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:24 kid1|       /cache03/1/07/85/0007856F
> 2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:24 kid1|       /cache01/1/03/A4/0003A4CD
> 2015/07/19 12:13:24 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:24 kid1|       /cache01/2/3C/9A/003C9ABD
> 2015/07/19 12:13:25 kid1| Error negotiating SSL connection on FD 187:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:25 kid1| Error negotiating SSL connection on FD 36:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 2015/07/19 12:13:26 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:26 kid1|       /cache02/2/03/41/0003413B
> 2015/07/19 12:13:28 kid1| DiskThreadsDiskFile::openDone: (2) No such
file or
> directory
> 2015/07/19 12:13:28 kid1|       /cache02/2/02/97/00029750
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672313.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVq3h3AAoJENNXIZxhPexGgKIH/RuoUupzhOGu7dtTrhA4CvHH
60zZrEmgF/yUfVYAywzxksawRxHbF/twDExq+c4nqm/tVdb5p69QM3mZY+RqhwFj
bKP/Vq6jBCBUGaL1r2hq6IdNMUsnn9w+E2fUu0YKlKAYXFoJU8VWL0WEA0RfTQmV
d5U3wAGZPEIyxCqeKP8VbRChOOdFn8y5eKwGIo9YtIfevCscDKnjY/3kVhIvLOT/
RXU78cIRvKZDwvMf2gDEQnbQ8v2xObprDEk5o1ILBUlIzEeMQHoFmXRF1AzXqSws
WAJYVevvt7opL9m45nJQn+U5wsvLknxQMbwjX8/AXrRj5SFzno34or+aimGxJts=
=Y0p2
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Sun Jul 19 10:23:40 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 03:23:40 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <55AB7877.7080805@gmail.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <1437300759438-4672313.post@n4.nabble.com> <55AB7877.7080805@gmail.com>
Message-ID: <1437301420382-4672316.post@n4.nabble.com>

yes am using AUFS cache_dir directive



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672316.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jul 19 11:56:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Jul 2015 23:56:31 +1200
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
In-Reply-To: <1437300233773-4672310.post@n4.nabble.com>
References: <1437225099222-4672309.post@n4.nabble.com>
 <1437300233773-4672310.post@n4.nabble.com>
Message-ID: <55AB906F.7050101@treenet.co.nz>

On 19/07/2015 10:03 p.m., HackXBack wrote:
> removing lines from my configure option make it work,
> now i have 
> ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
> --libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
> --libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
> --infodir=/usr/share/info --mandir=/usr/share/man --enable-storeio=ufs,aufs
> --enable-removal-policies=lru,heap --enable-linux-netfilter
> --enable-follow-x-forwarded-for --enable-dlmalloc --enable-zph-qos
> --with-default-user=proxy --with-logdir=/var/log/squid
> --with-pidfile=/var/run/squid.pid --with-swapdir=/var/spool/squid
> --with-filedescriptors=200000 --enable-ssl --enable-ssl-crtd --with-openssl
> --enable-large-cache-files 'CPPFLAGS=-I/usr/include/openssl'


You can also remove these which dont exist:
 --enable-ssl
 --enable-dlmalloc
 --enable-large-cache-files

(was that last one supposed to mean --with-large-files ?)

Amos



From hack.back at hotmail.com  Sun Jul 19 12:38:38 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 05:38:38 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073527142243 blocks of 1 bytes!
In-Reply-To: <55AB906F.7050101@treenet.co.nz>
References: <1437225099222-4672309.post@n4.nabble.com>
 <1437300233773-4672310.post@n4.nabble.com> <55AB906F.7050101@treenet.co.nz>
Message-ID: <1437309518326-4672318.post@n4.nabble.com>

Okay sir,
Thank you



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073527142243-blocks-of-1-bytes-tp4672309p4672318.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sun Jul 19 13:22:48 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 19 Jul 2015 16:22:48 +0300
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1437301420382-4672316.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <1437300759438-4672313.post@n4.nabble.com> <55AB7877.7080805@gmail.com>
 <1437301420382-4672316.post@n4.nabble.com>
Message-ID: <55ABA4A8.80005@ngtech.co.il>

On 19/07/2015 13:23, HackXBack wrote:
> yes am using AUFS cache_dir directive

With how many workers?

Eliezer



From hack.back at hotmail.com  Sun Jul 19 14:24:14 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 07:24:14 -0700 (PDT)
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <55ABA4A8.80005@ngtech.co.il>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <1437300759438-4672313.post@n4.nabble.com> <55AB7877.7080805@gmail.com>
 <1437301420382-4672316.post@n4.nabble.com> <55ABA4A8.80005@ngtech.co.il>
Message-ID: <1437315854452-4672320.post@n4.nabble.com>

top shows only 1 worker for squid ..

top - 16:24:51 up 5 days,  3:22,  2 users,  load average: 2.06, 1.18, 0.82
Tasks: 158 total,   2 running, 156 sleeping,   0 stopped,   0 zombie
%Cpu(s):  1.7 us,  0.7 sy,  0.0 ni, 96.7 id,  0.3 wa,  0.0 hi,  0.6 si,  0.0
st
KiB Mem:  32928480 total, 29039108 used,  3889372 free,  4273996 buffers
KiB Swap:  9526268 total,        0 used,  9526268 free, 10857212 cached

  PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+  COMMAND
25178 proxy     20   0 8607m 8.2g 6912 R  17.9 26.1   2:29.69 squid
 9187 unbound   20   0 72860  24m 1208 S   0.3  0.1   1:00.63 unbound
25247 root      20   0     0    0    0 S   0.3  0.0   0:00.04 kworker/0:2
25587 root      20   0 23320 1704 1180 R   0.3  0.0   0:00.05 top
    1 root      20   0 10648  760  624 S   0.0  0.0   0:03.79 init
    2 root      20   0     0    0    0 S   0.0  0.0   0:00.04 kthreadd
    3 root      20   0     0    0    0 S   0.0  0.0   2:25.37 ksoftirqd/0
    5 root      20   0     0    0    0 S   0.0  0.0   0:00.00 kworker/u:0
    6 root      rt   0     0    0    0 S   0.0  0.0   0:01.97 migration/0
    7 root      rt   0     0    0    0 S   0.0  0.0   0:03.82 watchdog/0
    8 root      rt   0     0    0    0 S   0.0  0.0   0:00.01 migration/1
   10 root      20   0     0    0    0 S   0.0  0.0   0:05.54 ksoftirqd/1
   12 root      rt   0     0    0    0 S   0.0  0.0   0:00.69 watchdog/1
   13 root      rt   0     0    0    0 S   0.0  0.0   0:00.00 migration/2
   15 root      20   0     0    0    0 S   0.0  0.0   0:02.05 ksoftirqd/2
   16 root      rt   0     0    0    0 S   0.0  0.0   0:00.63 watchdog/2
   17 root      rt   0     0    0    0 S   0.0  0.0   0:00.00 migration/3
   19 root      20   0     0    0    0 S   0.0  0.0   0:01.93 ksoftirqd/3
   20 root      rt   0     0    0    0 S   0.0  0.0   0:00.61 watchdog/3
   21 root      rt   0     0    0    0 S   0.0  0.0   0:00.10 migration/4
   23 root      20   0     0    0    0 S   0.0  0.0   0:00.29 ksoftirqd/4
   24 root      rt   0     0    0    0 S   0.0  0.0   0:00.93 watchdog/4
   25 root      rt   0     0    0    0 S   0.0  0.0   0:00.10 migration/5
   27 root      20   0     0    0    0 S   0.0  0.0   0:00.22 ksoftirqd/5
   28 root      rt   0     0    0    0 S   0.0  0.0   0:00.96 watchdog/5
   29 root      rt   0     0    0    0 S   0.0  0.0   0:00.10 migration/6
   31 root      20   0     0    0    0 S   0.0  0.0   0:00.22 ksoftirqd/6
   32 root      rt   0     0    0    0 S   0.0  0.0   0:00.80 watchdog/6
   33 root      rt   0     0    0    0 S   0.0  0.0   0:00.11 migration/7
   35 root      20   0     0    0    0 S   0.0  0.0   0:00.22 ksoftirqd/7
   36 root      rt   0     0    0    0 S   0.0  0.0   0:00.79 watchdog/7
   37 root       0 -20     0    0    0 S   0.0  0.0   0:00.00 cpuset
   38 root       0 -20     0    0    0 S   0.0  0.0   0:00.00 khelper
   39 root      20   0     0    0    0 S   0.0  0.0   0:00.00 kdevtmpfs
   40 root       0 -20     0    0    0 S   0.0  0.0   0:00.00 netns
   41 root      20   0     0    0    0 S   0.0  0.0   6:50.08 sync_supers
   42 root      20   0     0    0    0 S   0.0  0.0   0:00.01 bdi-default
   43 root       0 -20     0    0    0 S   0.0  0.0   0:00.00 kintegrityd




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/a-lot-of-TCP-SWAPFAIL-MISS-200-tp4672011p4672320.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jul 19 15:30:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jul 2015 03:30:45 +1200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1437315854452-4672320.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
 <1437300759438-4672313.post@n4.nabble.com> <55AB7877.7080805@gmail.com>
 <1437301420382-4672316.post@n4.nabble.com> <55ABA4A8.80005@ngtech.co.il>
 <1437315854452-4672320.post@n4.nabble.com>
Message-ID: <55ABC2A5.70307@treenet.co.nz>

On 20/07/2015 2:24 a.m., HackXBack wrote:
> top shows only 1 worker for squid ..
> 

Top shows processes. worker is a type of process, but not the only type
for Squid.

There should be at least 2 Squid processes in most installations
(master/coordinator + one or more workers). Only MacOS, Ubuntu, and
systemd machines are expected to have exactly 1 process due to their use
of -N.

If you are running one of those systems then it is more likely you will
see cache corrupted files after a Squid shutdown. Since they make
assumptions about the child process they are controlling, use abrupt
kill signals and do not always wait for the Squid to terminate cleanly.


Which goes back to how the SWAPFAIL is part of Squid self-correcting
mechanisms. Dont be panic'd by it.

Sure there is some problem/cause ... BUT Squid has already dealt with
all that. Your part is a low-pressure job of efficiency tuning -
identifying if it can be avoided in future. If not, no worries, next
time will be dealt with the same as this was.

Amos



From squid3 at treenet.co.nz  Sun Jul 19 15:40:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jul 2015 03:40:06 +1200
Subject: [squid-users] a lot of TCP_SWAPFAIL_MISS/200
In-Reply-To: <1437300544323-4672311.post@n4.nabble.com>
References: <1435793483490-4672011.post@n4.nabble.com>
 <5595168D.5030504@treenet.co.nz> <1437300544323-4672311.post@n4.nabble.com>
Message-ID: <55ABC4D6.6090307@treenet.co.nz>

On 19/07/2015 10:09 p.m., HackXBack wrote:
> but this happen only with version 3.5 , and it increase after restarting
> squid or rebooting system
> this is bug in 3.5 and it decrease the HIT ratio ,
> you dont think so ? 

I've seen a lot of reports of it happening on all Squid versions where
shutdown did not properly finished saving to those cache files. Usually
gets bad with very short configured shutdown_lifetime values. Although
it can happen anytime shutdown interrupts a busy set of clients.

FYI: attached is a backport of a Squid-4 change which should reduce that
shutdown situation being a problem. If you would like to test we can see
if it helps with your case.


SWAPFAIL also gets seen a lot on caches (or individual files) that were
created by very old Squid versions (older than 3.1 or 3.3 IIRC) before
we added checksum validation. Those old objects metadata fail the
checksum due to format differences at the bit level and SWAPFAIL is done
to replace them with guaranteed clean content.

The same checksum fails can also still happen on objects created with a
32-bit Squid loaded by a 64-bit Squid, and vice-versa. For the same reasons.

Then there is genuine HDD corruption, mutiple Squid processes or workers
touching each others cache files, etc. other programs touching them etc.

So many causes it is guesswork. But an educated guess says it is
shutdown related errors.

Amos

-------------- next part --------------
=== modified file 'src/base/RunnersRegistry.cc'
--- src/base/RunnersRegistry.cc	2015-01-13 09:13:49 +0000
+++ src/base/RunnersRegistry.cc	2015-07-19 15:08:16 +0000
@@ -15,40 +15,48 @@
 /// all known runners
 static Runners *TheRunners = NULL;
 
 /// safely returns registered runners, initializing structures as needed
 static Runners &
 GetRunners()
 {
     if (!TheRunners)
         TheRunners = new Runners;
     return *TheRunners;
 }
 
 int
 RegisterRunner(RegisteredRunner *rr)
 {
     Runners &runners = GetRunners();
     runners.insert(rr);
     return runners.size();
 }
 
+int
+DeregisterRunner(RegisteredRunner *rr)
+{
+    Runners &runners = GetRunners();
+    runners.erase(rr);
+    return runners.size();
+}
+
 void
 RunRegistered(const RegisteredRunner::Method &m)
 {
     Runners &runners = GetRunners();
     typedef Runners::iterator RRI;
     for (RRI i = runners.begin(); i != runners.end(); ++i)
         ((*i)->*m)();
 
     if (m == &RegisteredRunner::finishShutdown) {
         delete TheRunners;
         TheRunners = NULL;
     }
 }
 
 bool
 UseThisStatic(const void *)
 {
     return true;
 }
 

=== modified file 'src/base/RunnersRegistry.h'
--- src/base/RunnersRegistry.h	2015-01-13 09:13:49 +0000
+++ src/base/RunnersRegistry.h	2015-07-19 15:08:16 +0000
@@ -51,54 +51,63 @@
     virtual void claimMemoryNeeds() {}
 
     /// Called after claimMemoryNeeds().
     /// Meant for activating modules and features using a finalized
     /// configuration with known memory requirements.
     virtual void useConfig() {}
 
     /* Reconfiguration events */
 
     /// Called after parsing squid.conf during reconfiguration.
     /// Meant for adjusting the module state based on configuration changes.
     virtual void syncConfig() {}
 
     /* Shutdown events */
 
     /// Called after receiving a shutdown request and before stopping the main
     /// loop. At least one main loop iteration is guaranteed after this call.
     /// Meant for cleanup and state saving that may require other modules.
     virtual void startShutdown() {}
 
+    /// Called after shutdown_lifetime grace period ends and before stopping
+    /// the main loop. At least one main loop iteration is guaranteed after
+    /// this call.
+    /// Meant for cleanup and state saving that may require other modules.
+    virtual void endingShutdown() {}
+
     /// Called after stopping the main loop and before releasing memory.
     /// Meant for quick/basic cleanup that does not require any other modules.
     virtual ~RegisteredRunner() {}
     /// exists to simplify caller interface; override the destructor instead
     void finishShutdown() { delete this; }
 
     /// a pointer to one of the above notification methods
     typedef void (RegisteredRunner::*Method)();
 
 };
 
 /// registers a given runner with the given registry and returns registry count
 int RegisterRunner(RegisteredRunner *rr);
 
+/// de-registers a given runner with the given registry and returns registry count
+int DeregisterRunner(RegisteredRunner *rr);
+
 /// Calls a given method of all runners.
 /// All runners are destroyed after the finishShutdown() call.
 void RunRegistered(const RegisteredRunner::Method &m);
 
 /// convenience macro to describe/debug the caller and the method being called
 #define RunRegisteredHere(m) \
     debugs(1, 2, "running " # m); \
     RunRegistered(&m)
 
 /// convenience function to "use" an otherwise unreferenced static variable
 bool UseThisStatic(const void *);
 
 /// convenience macro: register one RegisteredRunner kid as early as possible
 #define RunnerRegistrationEntry(Who) \
     static const bool Who ## _Registered_ = \
         RegisterRunner(new Who) > 0 && \
         UseThisStatic(& Who ## _Registered_);
 
 #endif /* SQUID_BASE_RUNNERSREGISTRY_H */
 

=== modified file 'src/client_side.cc'
--- src/client_side.cc	2015-06-05 23:41:22 +0000
+++ src/client_side.cc	2015-07-19 15:08:16 +0000
@@ -797,40 +797,41 @@
     if (aur != auth_) {
         debugs(33, 2, "ERROR: Closing " << clientConnection << " due to change of connection-auth from " << by);
         auth_->releaseAuthServer();
         auth_ = NULL;
         // this is a fatal type of problem.
         // Close the connection immediately with TCP RST to abort all traffic flow
         comm_reset_close(clientConnection);
         return;
     }
 
     /* NOT REACHABLE */
 }
 #endif
 
 // cleans up before destructor is called
 void
 ConnStateData::swanSong()
 {
     debugs(33, 2, HERE << clientConnection);
     flags.readMore = false;
+    DeregisterRunner(this);
     clientdbEstablished(clientConnection->remote, -1);  /* decrement */
     assert(areAllContextsForThisConnection());
     freeAllContexts();
 
     unpinConnection(true);
 
     if (Comm::IsConnOpen(clientConnection))
         clientConnection->close();
 
 #if USE_AUTH
     // NP: do this bit after closing the connections to avoid side effects from unwanted TCP RST
     setAuth(NULL, "ConnStateData::SwanSong cleanup");
 #endif
 
     BodyProducer::swanSong();
     flags.swanSang = true;
 }
 
 bool
 ConnStateData::isOpen() const
@@ -1874,40 +1875,66 @@
     }
 }
 
 ClientSocketContext *
 ConnStateData::abortRequestParsing(const char *const uri)
 {
     ClientHttpRequest *http = new ClientHttpRequest(this);
     http->req_sz = in.buf.length();
     http->uri = xstrdup(uri);
     setLogUri (http, uri);
     ClientSocketContext *context = new ClientSocketContext(clientConnection, http);
     StoreIOBuffer tempBuffer;
     tempBuffer.data = context->reqbuf;
     tempBuffer.length = HTTP_REQBUF_SZ;
     clientStreamInit(&http->client_stream, clientGetMoreData, clientReplyDetach,
                      clientReplyStatus, new clientReplyContext(http), clientSocketRecipient,
                      clientSocketDetach, context, tempBuffer);
     return context;
 }
 
+void
+ConnStateData::startShutdown()
+{
+    // RegisteredRunner API callback - Squid has been shut down
+
+    // if connection is idle terminate it now,
+    // otherwise wait for grace period to end
+    if (getConcurrentRequestCount() == 0)
+        endingShutdown();
+}
+
+void
+ConnStateData::endingShutdown()
+{
+    // RegisteredRunner API callback - Squid shutdown grace period is over
+
+    // force the client connection to close immediately
+    // swanSong() in the close handler will cleanup.
+    if (Comm::IsConnOpen(clientConnection))
+        clientConnection->close();
+
+    // deregister now to ensure finalShutdown() does not kill us prematurely.
+    // fd_table purge will cleanup if close handler was not fast enough.
+    DeregisterRunner(this);
+}
+
 char *
 skipLeadingSpace(char *aString)
 {
     char *result = aString;
 
     while (xisspace(*aString))
         ++aString;
 
     return result;
 }
 
 /**
  * 'end' defaults to NULL for backwards compatibility
  * remove default value if we ever get rid of NULL-terminated
  * request buffers.
  */
 const char *
 findTrailingHTTPVersion(const char *uriAndHTTPVersion, const char *end)
 {
     if (NULL == end) {
@@ -3502,40 +3529,44 @@
     signAlgorithm(Ssl::algSignTrusted),
 #endif
     stoppedSending_(NULL),
     stoppedReceiving_(NULL)
 {
     flags.readMore = true; // kids may overwrite
     flags.swanSang = false;
 
     pinning.host = NULL;
     pinning.port = -1;
     pinning.pinned = false;
     pinning.auth = false;
     pinning.zeroReply = false;
     pinning.peer = NULL;
 
     // store the details required for creating more MasterXaction objects as new requests come in
     clientConnection = xact->tcpClient;
     port = xact->squidPort;
     log_addr = xact->tcpClient->remote;
     log_addr.applyMask(Config.Addrs.client_netmask);
+
+    // register to receive notice of Squid signal events
+    // which may affect long persisting client connections
+    RegisterRunner(this);
 }
 
 void
 ConnStateData::start()
 {
     BodyProducer::start();
     HttpControlMsgSink::start();
 
     if (port->disable_pmtu_discovery != DISABLE_PMTU_OFF &&
             (transparent() || port->disable_pmtu_discovery == DISABLE_PMTU_ALWAYS)) {
 #if defined(IP_MTU_DISCOVER) && defined(IP_PMTUDISC_DONT)
         int i = IP_PMTUDISC_DONT;
         if (setsockopt(clientConnection->fd, SOL_IP, IP_MTU_DISCOVER, &i, sizeof(i)) < 0)
             debugs(33, 2, "WARNING: Path MTU discovery disabling failed on " << clientConnection << " : " << xstrerror());
 #else
         static bool reported = false;
 
         if (!reported) {
             debugs(33, DBG_IMPORTANT, "NOTICE: Path MTU discovery disabling is not supported on your platform.");
             reported = true;

=== modified file 'src/client_side.h'
--- src/client_side.h	2015-04-13 05:59:05 +0000
+++ src/client_side.h	2015-07-19 15:08:41 +0000
@@ -1,33 +1,34 @@
 /*
  * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
  *
  * Squid software is distributed under GPLv2+ license and includes
  * contributions from numerous individuals and organizations.
  * Please see the COPYING and CONTRIBUTORS files for details.
  */
 
 /* DEBUG: section 33    Client-side Routines */
 
 #ifndef SQUID_CLIENTSIDE_H
 #define SQUID_CLIENTSIDE_H
 
+#include "base/RunnersRegistry.h"
 #include "clientStreamForward.h"
 #include "comm.h"
 #include "helper/forward.h"
 #include "HttpControlMsg.h"
 #include "HttpParser.h"
 #include "ipc/FdNotes.h"
 #include "SBuf.h"
 #if USE_AUTH
 #include "auth/UserRequest.h"
 #endif
 #if USE_OPENSSL
 #include "ssl/support.h"
 #endif
 
 class ConnStateData;
 class ClientHttpRequest;
 class clientStreamNode;
 class ChunkedCodingParser;
 namespace AnyP
 {
@@ -152,41 +153,41 @@
 #if USE_OPENSSL
 namespace Ssl
 {
 class ServerBump;
 }
 #endif
 /**
  * Manages a connection to a client.
  *
  * Multiple requests (up to pipeline_prefetch) can be pipelined. This object is responsible for managing
  * which one is currently being fulfilled and what happens to the queue if the current one
  * causes the client connection to be closed early.
  *
  * Act as a manager for the connection and passes data in buffer to the current parser.
  * the parser has ambiguous scope at present due to being made from global functions
  * I believe this object uses the parser to identify boundaries and kick off the
  * actual HTTP request handling objects (ClientSocketContext, ClientHttpRequest, HttpRequest)
  *
  * If the above can be confirmed accurate we can call this object PipelineManager or similar
  */
-class ConnStateData : public BodyProducer, public HttpControlMsgSink
+class ConnStateData : public BodyProducer, public HttpControlMsgSink, public RegisteredRunner
 {
 
 public:
     explicit ConnStateData(const MasterXaction::Pointer &xact);
     virtual ~ConnStateData();
 
     void readSomeData();
     bool areAllContextsForThisConnection() const;
     void freeAllContexts();
     void notifyAllContexts(const int xerrno); ///< tell everybody about the err
     /// Traffic parsing
     bool clientParseRequests();
     void readNextRequest();
     ClientSocketContext::Pointer getCurrentContext() const;
     void addContextToQueue(ClientSocketContext * context);
     int getConcurrentRequestCount() const;
     bool isOpen() const;
 
     // HttpControlMsgSink API
     virtual void sendControlMsg(HttpControlMsg msg);
@@ -382,40 +383,44 @@
 
     /* clt_conn_tag=tag annotation access */
     const SBuf &connectionTag() const { return connectionTag_; }
     void connectionTag(const char *aTag) { connectionTag_ = aTag; }
 
     /// handle a control message received by context from a peer and call back
     virtual void writeControlMsgAndCall(ClientSocketContext *context, HttpReply *rep, AsyncCall::Pointer &call) = 0;
 
     /// ClientStream calls this to supply response header (once) and data
     /// for the current ClientSocketContext.
     virtual void handleReply(HttpReply *header, StoreIOBuffer receivedData) = 0;
 
     /// remove no longer needed leading bytes from the input buffer
     void consumeInput(const size_t byteCount);
 
     /* TODO: Make the methods below (at least) non-public when possible. */
 
     /// stop parsing the request and create context for relaying error info
     ClientSocketContext *abortRequestParsing(const char *const errUri);
 
+    /* Registered Runner API */
+    virtual void startShutdown();
+    virtual void endingShutdown();
+
 protected:
     void startDechunkingRequest();
     void finishDechunkingRequest(bool withSuccess);
     void abortChunkedRequestBody(const err_type error);
     err_type handleChunkedRequestBody(size_t &putSize);
 
     void startPinnedConnectionMonitoring();
     void clientPinnedConnectionRead(const CommIoCbParams &io);
 
     /// parse input buffer prefix into a single transfer protocol request
     /// return NULL to request more header bytes (after checking any limits)
     /// use abortRequestParsing() to handle parsing errors w/o creating request
     virtual ClientSocketContext *parseOneRequest(Http::ProtocolVersion &ver) = 0;
 
     /// start processing a freshly parsed request
     virtual void processParsedRequest(ClientSocketContext *context, const Http::ProtocolVersion &ver) = 0;
 
     /// returning N allows a pipeline of 1+N requests (see pipeline_prefetch)
     virtual int pipelinePrefetchMax() const;
 

=== modified file 'src/main.cc'
--- src/main.cc	2015-03-21 08:16:46 +0000
+++ src/main.cc	2015-07-19 15:11:12 +0000
@@ -186,40 +186,52 @@
 
 public:
     int checkEvents(int timeout) {
         Store::Root().callback();
         return EVENT_IDLE;
     };
 };
 
 class SignalEngine: public AsyncEngine
 {
 
 public:
     virtual int checkEvents(int timeout);
 
 private:
     static void StopEventLoop(void *) {
         if (EventLoop::Running)
             EventLoop::Running->stop();
     }
 
+    static void FinalShutdownRunners(void *) {
+        RunRegisteredHere(RegisteredRunner::endingShutdown);
+
+        // XXX: this should be a Runner.
+#if USE_AUTH
+        /* detach the auth components (only do this on full shutdown) */
+        Auth::Scheme::FreeAll();
+#endif
+
+        eventAdd("SquidTerminate", &StopEventLoop, NULL, 0, 1, false);
+    }
+
     void doShutdown(time_t wait);
 };
 
 int
 SignalEngine::checkEvents(int timeout)
 {
     PROF_start(SignalEngine_checkEvents);
 
     if (do_reconfigure) {
         mainReconfigureStart();
         do_reconfigure = 0;
     } else if (do_rotate) {
         mainRotate();
         do_rotate = 0;
     } else if (do_shutdown) {
         doShutdown(do_shutdown > 0 ? (int) Config.shutdownLifetime : 0);
         do_shutdown = 0;
     }
     BroadcastSignalIfAny(DebugSignal);
     BroadcastSignalIfAny(RotateSignal);
@@ -227,47 +239,42 @@
     BroadcastSignalIfAny(ShutdownSignal);
 
     PROF_stop(SignalEngine_checkEvents);
     return EVENT_IDLE;
 }
 
 void
 SignalEngine::doShutdown(time_t wait)
 {
     debugs(1, DBG_IMPORTANT, "Preparing for shutdown after " << statCounter.client_http.requests << " requests");
     debugs(1, DBG_IMPORTANT, "Waiting " << wait << " seconds for active connections to finish");
 
     shutting_down = 1;
 
 #if USE_WIN32_SERVICE
     WIN32_svcstatusupdate(SERVICE_STOP_PENDING, (wait + 1) * 1000);
 #endif
 
     /* run the closure code which can be shared with reconfigure */
     serverConnectionsClose();
-#if USE_AUTH
-    /* detach the auth components (only do this on full shutdown) */
-    Auth::Scheme::FreeAll();
-#endif
-
     RunRegisteredHere(RegisteredRunner::startShutdown);
-    eventAdd("SquidShutdown", &StopEventLoop, this, (double) (wait + 1), 1, false);
+    eventAdd("SquidShutdown", &FinalShutdownRunners, this, (double) (wait + 1), 1, false);
 }
 
 static void
 usage(void)
 {
     fprintf(stderr,
             "Usage: %s [-cdhvzCFNRVYX] [-n name] [-s | -l facility] [-f config-file] [-[au] port] [-k signal]"
 #if USE_WIN32_SERVICE
             "[-ir] [-O CommandLine]"
 #endif
             "\n"
             "       -a port   Specify HTTP port number (default: %d).\n"
             "       -d level  Write debugging to stderr also.\n"
             "       -f file   Use given config-file instead of\n"
             "                 %s\n"
             "       -h        Print help message.\n"
 #if USE_WIN32_SERVICE
             "       -i        Installs as a Windows Service (see -n option).\n"
 #endif
             "       -k reconfigure|rotate|shutdown|"


From squid3 at treenet.co.nz  Sun Jul 19 16:03:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jul 2015 04:03:24 +1200
Subject: [squid-users] Redirects error for only some Citrix sites
In-Reply-To: <93E16A0E-E298-474D-B004-F878FF0D4EF7@paravis.net>
References: <93E16A0E-E298-474D-B004-F878FF0D4EF7@paravis.net>
Message-ID: <55ABCA4C.1040704@treenet.co.nz>

On 18/07/2015 1:42 a.m., Laz C. Peterson wrote:
> Hello all,
> 
> Very weird issue here.  This happens to only select Citrix support articles.  (For example, http://support.citrix.com/article/CTX122972 <http://support.citrix.com/article/CTX122972> when searching Google for ?citrix netscaler expired password? which is the top link in my results, or also searching for the same article directly on Citrix support site.)
> 
> This is a new install of Squid 3 on Ubuntu 14.04.2 (from Ubuntu repository).  When clicking the Google link, I get ?too many redirects? error, saying that possibly the page refers to another page that is then redirected back to the original page.
> 
> I tried debugging but did not find much useful information.  Has anyone else seen behavior like this?
> 

The problem is the client fething URL X, gets a 30x redirect message
instructing it to contacts URL X instead (X being same URL X it *was*
fetching).

Usually that is a miconfiguration on the origin server itself. Fixable
only by the origin site authors. But there are also a few ways Squid can
play a part:

1) The 30x response pointing to itself (wrongly) really was generated by
the server and also explicitly stated that it should be cached [or you
configured Squid to force-cache].

Squid obeyed, and now you keep getting these loops. That will continue
until the cached content expires or is purged.


2) You are using Store-ID/Store-URL feature of Squid and did not check
that the URLs being merged were identical output. One of them produces a
302 redirect to X, which got cached. So now fetches for any URL in the
merged set (including the X itself) gets the cached 302 redirect back to X.
Again, that will continue until the cached content expires or is purged.


3) You are using a URL redirector that is generating the 302 response
loop. Usually redirectors with badly written (overly inclusive) regex
patterns causing similar behaviour to (2).


4) You are using a URL re-writer that is taking client request URL Y and
(wrongly) re-writing it to X. Squid fetches X from the backend server,
which replies with a redirect to Y (because Y != X). ... and loop.


5) You could be directing traffic using a cache_peer on port 80
regardless of http:// or https:// scheme received from the clients. If
the receiving peer/server emits a 302 for all traffic arriving on its
port 80 to a https:// URL this sort of loop happens. Its a slightly more
complicated form of (4), using a cache_peer equivalent of URL re-writer.
 The best fix for that is at the server. RFC 7230 section 5.5 has an
algorithm for what compliant servers should be doing. Squids job is to
relay the request and URL unchanged.

Amos



From laz at paravis.net  Sun Jul 19 16:22:55 2015
From: laz at paravis.net (Laz C. Peterson)
Date: Sun, 19 Jul 2015 09:22:55 -0700
Subject: [squid-users] Redirects error for only some Citrix sites
In-Reply-To: <55ABCA4C.1040704@treenet.co.nz>
References: <93E16A0E-E298-474D-B004-F878FF0D4EF7@paravis.net>
 <55ABCA4C.1040704@treenet.co.nz>
Message-ID: <F032BC13-1B70-4388-AFC2-22314C75C1FD@paravis.net>

Wow thank you Amos for that information.

I must read, research, digest, read and then attempt to figure out what the problem is. :-)

Will be in touch if there are any further issues.  Thank you.

~ Laz Peterson
Paravis, LLC

> On Jul 19, 2015, at 9:03 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 18/07/2015 1:42 a.m., Laz C. Peterson wrote:
>> Hello all,
>> 
>> Very weird issue here.  This happens to only select Citrix support articles.  (For example, http://support.citrix.com/article/CTX122972 <http://support.citrix.com/article/CTX122972> when searching Google for ?citrix netscaler expired password? which is the top link in my results, or also searching for the same article directly on Citrix support site.)
>> 
>> This is a new install of Squid 3 on Ubuntu 14.04.2 (from Ubuntu repository).  When clicking the Google link, I get ?too many redirects? error, saying that possibly the page refers to another page that is then redirected back to the original page.
>> 
>> I tried debugging but did not find much useful information.  Has anyone else seen behavior like this?
>> 
> 
> The problem is the client fething URL X, gets a 30x redirect message
> instructing it to contacts URL X instead (X being same URL X it *was*
> fetching).
> 
> Usually that is a miconfiguration on the origin server itself. Fixable
> only by the origin site authors. But there are also a few ways Squid can
> play a part:
> 
> 1) The 30x response pointing to itself (wrongly) really was generated by
> the server and also explicitly stated that it should be cached [or you
> configured Squid to force-cache].
> 
> Squid obeyed, and now you keep getting these loops. That will continue
> until the cached content expires or is purged.
> 
> 
> 2) You are using Store-ID/Store-URL feature of Squid and did not check
> that the URLs being merged were identical output. One of them produces a
> 302 redirect to X, which got cached. So now fetches for any URL in the
> merged set (including the X itself) gets the cached 302 redirect back to X.
> Again, that will continue until the cached content expires or is purged.
> 
> 
> 3) You are using a URL redirector that is generating the 302 response
> loop. Usually redirectors with badly written (overly inclusive) regex
> patterns causing similar behaviour to (2).
> 
> 
> 4) You are using a URL re-writer that is taking client request URL Y and
> (wrongly) re-writing it to X. Squid fetches X from the backend server,
> which replies with a redirect to Y (because Y != X). ... and loop.
> 
> 
> 5) You could be directing traffic using a cache_peer on port 80
> regardless of http:// or https:// scheme received from the clients. If
> the receiving peer/server emits a 302 for all traffic arriving on its
> port 80 to a https:// URL this sort of loop happens. Its a slightly more
> complicated form of (4), using a cache_peer equivalent of URL re-writer.
> The best fix for that is at the server. RFC 7230 section 5.5 has an
> algorithm for what compliant servers should be doing. Squids job is to
> relay the request and URL unchanged.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150719/d5de18b7/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 19 16:27:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jul 2015 04:27:20 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55A67E1D.7040801@urlfilterdb.com>
References: <1436943377920-4672209.post@n4.nabble.com>
 <55A644C5.1090707@treenet.co.nz> <1436960473258-4672226.post@n4.nabble.com>
 <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com>
Message-ID: <55ABCFE8.1020003@treenet.co.nz>

On 16/07/2015 3:37 a.m., Marcus Kool wrote:
> 
> I think that changing the baseline to 8K is not required since the queue
> congestion
> warning is normally seen only a few times, so the baseline value of 8 is
> doubled
> only a few times.
> A new baseline value of 256 (5 doublings) makes sense to me since most
> disks have
> a hardware queue depth of 256.
> For those systems where many disks are tortured with I/O the doubling
> will appear
> a few (log2(#disks)) times.

Good point. It is an arbitrary initial length.

Fred and Fred;

 Could you guys who have been seeing these warnings logged please
present a grep of those cache.log lines so I can get a better handle on
how many doublings your queues are actually requiring ?

I count 5 and 6 warnings respectively in FredB's two earlier log traces.
So 8*(2^6)=512 entries with an extra x2 normal I/O thread count making
it about 1K queue entries required minimum.

Cheers
Amos



From hack.back at hotmail.com  Sun Jul 19 19:12:40 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 19 Jul 2015 12:12:40 -0700 (PDT)
Subject: [squid-users] redirect TCP_NONE
In-Reply-To: <55A8DA73.6060409@treenet.co.nz>
References: <1437090059610-4672298.post@n4.nabble.com>
 <55A87FE7.50300@treenet.co.nz> <1437122582268-4672303.post@n4.nabble.com>
 <55A8CC76.8010507@treenet.co.nz> <1437126401118-4672305.post@n4.nabble.com>
 <55A8DA73.6060409@treenet.co.nz>
Message-ID: <1437333160564-4672326.post@n4.nabble.com>

in another meaning and with another way,
why we cant make https request pass as tcp_tunnel and dont decrypt the
connection if client not import certificate xD ??
at least the request will pass directly with out decryption 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/redirect-TCP-NONE-tp4672298p4672326.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Mon Jul 20 08:46:00 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 20 Jul 2015 10:46:00 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55ABCFE8.1020003@treenet.co.nz>
Message-ID: <424612827.6711365.1437381960565.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Fred and Fred;
> 
>  Could you guys who have been seeing these warnings logged please
> present a grep of those cache.log lines so I can get a better handle
> on
> how many doublings your queues are actually requiring ?
> 
> I count 5 and 6 warnings respectively in FredB's two earlier log
> traces.
> So 8*(2^6)=512 entries with an extra x2 normal I/O thread count
> making
> it about 1K queue entries required minimum.
> 
> Cheers
> Amos
> 
> 

Hi,

Tested one hour.

cache_dir aufs /cache1 250000 512 512
cache_dir aufs /cache2 250000 512 512

/dev/sdb1                                                413G  223G  170G  57% /cache1
/dev/sdc1                                                413G  223G  170G  57% /cache2


Squid Cache: Version 3.4.13
configure options:  '--build=x86_64-linux-gnu' '--prefix=/' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/usr/lib/squid' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-auth-negotiate--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-basic-auth-helpers=LDAP,digest' '--enable-digest-auth-helpers=ldap,password' '--enable-arp-acl' '--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid' '--with-filedescriptors=65536' '--with-large-files' '--disable-snmp' '--with-default-user=squid' '--disable-ipv6' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -g -Wall -O2' 'LDFLAGS=' 'CPPFLAGS=' 'CXXFLAGS=-g -O2 -g -Wall -O2' '--enable-ltdl-convenience'


grep "2015:09:40:" /var/log/squid/access.log | wc -l 
16408 -> 273 r/s

grep "2015:10:00:" /var/log/squid/access.log | wc -l 
18259 -> 304 r/s

 grep "2015:10:20:" /var/log/squid/access.log | wc -l 
17690 -> 294 r/s

grep "2015:10:30:" /var/log/squid/access.log | wc -l 
18294 -> 304 r/s

Unfortunately actually the load is reduced by 2 or 3, but I think it still interesting  

2015/07/20 09:34:50 kid1| Store rebuilding is 79.56% complete
2015/07/20 09:35:04 kid1| Done reading /cache2 swaplog (9705370 entries)
2015/07/20 09:35:04 kid1| Done reading /cache1 swaplog (9782135 entries)
2015/07/20 09:35:04 kid1| Finished rebuilding storage from disk.
2015/07/20 09:35:04 kid1|   19487505 Entries scanned
2015/07/20 09:35:04 kid1|         0 Invalid entries.
2015/07/20 09:35:04 kid1|         0 With invalid flags.
2015/07/20 09:35:04 kid1|   19487505 Objects loaded.
2015/07/20 09:35:04 kid1|         0 Objects expired.
2015/07/20 09:35:04 kid1|         0 Objects cancelled.
2015/07/20 09:35:04 kid1|         0 Duplicate URLs purged.
2015/07/20 09:35:04 kid1|         0 Swapfile clashes avoided.
2015/07/20 09:35:04 kid1|   Took 58.79 seconds (331500.06 objects/sec).
2015/07/20 09:35:04 kid1| Beginning Validation Procedure
2015/07/20 09:35:05 kid1|   262144 Entries Validated so far.
2015/07/20 09:35:05 kid1|   524288 Entries Validated so far.
2015/07/20 09:35:05 kid1|   786432 Entries Validated so far.
2015/07/20 09:35:05 kid1|   1048576 Entries Validated so far.
2015/07/20 09:35:05 kid1|   1310720 Entries Validated so far.
2015/07/20 09:35:05 kid1|   1572864 Entries Validated so far.
2015/07/20 09:35:05 kid1|   1835008 Entries Validated so far.
2015/07/20 09:35:05 kid1|   2097152 Entries Validated so far.
2015/07/20 09:35:05 kid1|   2359296 Entries Validated so far.
2015/07/20 09:35:05 kid1|   2621440 Entries Validated so far.
2015/07/20 09:35:05 kid1|   2883584 Entries Validated so far.
2015/07/20 09:35:05 kid1|   3145728 Entries Validated so far.
2015/07/20 09:35:05 kid1|   3407872 Entries Validated so far.
2015/07/20 09:35:05 kid1|   3670016 Entries Validated so far.
2015/07/20 09:35:05 kid1|   3932160 Entries Validated so far.
2015/07/20 09:35:05 kid1|   4194304 Entries Validated so far.
2015/07/20 09:35:05 kid1|   4456448 Entries Validated so far.
2015/07/20 09:35:05 kid1|   4718592 Entries Validated so far.
2015/07/20 09:35:05 kid1|   4980736 Entries Validated so far.
2015/07/20 09:35:05 kid1|   5242880 Entries Validated so far.
2015/07/20 09:35:06 kid1|   5505024 Entries Validated so far.
2015/07/20 09:35:06 kid1|   5767168 Entries Validated so far.
2015/07/20 09:35:06 kid1|   6029312 Entries Validated so far.
2015/07/20 09:35:06 kid1|   6291456 Entries Validated so far.
2015/07/20 09:35:06 kid1|   6553600 Entries Validated so far.
2015/07/20 09:35:06 kid1|   6815744 Entries Validated so far.
2015/07/20 09:35:06 kid1|   7077888 Entries Validated so far.
2015/07/20 09:35:06 kid1|   7340032 Entries Validated so far.
2015/07/20 09:35:06 kid1|   7602176 Entries Validated so far.
2015/07/20 09:35:06 kid1|   7864320 Entries Validated so far.
2015/07/20 09:35:06 kid1|   8126464 Entries Validated so far.
2015/07/20 09:35:06 kid1|   8388608 Entries Validated so far.
2015/07/20 09:35:06 kid1|   8650752 Entries Validated so far.
2015/07/20 09:35:06 kid1|   8912896 Entries Validated so far.
2015/07/20 09:35:06 kid1|   9175040 Entries Validated so far.
2015/07/20 09:35:06 kid1|   9437184 Entries Validated so far.
2015/07/20 09:35:06 kid1|   9699328 Entries Validated so far.
2015/07/20 09:35:06 kid1|   9961472 Entries Validated so far.
2015/07/20 09:35:06 kid1|   10223616 Entries Validated so far.
2015/07/20 09:35:06 kid1|   10485760 Entries Validated so far.
2015/07/20 09:35:06 kid1|   10747904 Entries Validated so far.
2015/07/20 09:35:07 kid1|   11010048 Entries Validated so far.
2015/07/20 09:35:07 kid1|   11272192 Entries Validated so far.
2015/07/20 09:35:07 kid1|   11534336 Entries Validated so far.
2015/07/20 09:35:07 kid1|   11796480 Entries Validated so far.
2015/07/20 09:35:07 kid1|   12058624 Entries Validated so far.
2015/07/20 09:35:07 kid1|   12320768 Entries Validated so far.
2015/07/20 09:35:07 kid1|   12582912 Entries Validated so far.
2015/07/20 09:35:07 kid1|   12845056 Entries Validated so far.
2015/07/20 09:35:07 kid1|   13107200 Entries Validated so far.
2015/07/20 09:35:07 kid1|   13369344 Entries Validated so far.
2015/07/20 09:35:07 kid1|   13631488 Entries Validated so far.
2015/07/20 09:35:07 kid1|   13893632 Entries Validated so far.
2015/07/20 09:35:07 kid1|   14155776 Entries Validated so far.
2015/07/20 09:35:07 kid1|   14417920 Entries Validated so far.
2015/07/20 09:35:07 kid1|   14680064 Entries Validated so far.
2015/07/20 09:35:07 kid1|   14942208 Entries Validated so far.
2015/07/20 09:35:07 kid1|   15204352 Entries Validated so far.
2015/07/20 09:35:07 kid1|   15466496 Entries Validated so far.
2015/07/20 09:35:07 kid1|   15728640 Entries Validated so far.
2015/07/20 09:35:07 kid1|   15990784 Entries Validated so far.
2015/07/20 09:35:08 kid1|   16252928 Entries Validated so far.
2015/07/20 09:35:08 kid1|   16515072 Entries Validated so far.
2015/07/20 09:35:08 kid1|   16777216 Entries Validated so far.
2015/07/20 09:35:08 kid1|   17039360 Entries Validated so far.
2015/07/20 09:35:08 kid1|   17301504 Entries Validated so far.
2015/07/20 09:35:08 kid1|   17563648 Entries Validated so far.
2015/07/20 09:35:08 kid1|   17825792 Entries Validated so far.
2015/07/20 09:35:08 kid1|   18087936 Entries Validated so far.
2015/07/20 09:35:08 kid1|   18350080 Entries Validated so far.
2015/07/20 09:35:08 kid1|   18612224 Entries Validated so far.
2015/07/20 09:35:08 kid1|   18874368 Entries Validated so far.
2015/07/20 09:35:08 kid1|   19136512 Entries Validated so far.
2015/07/20 09:35:08 kid1|   19398656 Entries Validated so far.
2015/07/20 09:35:08 kid1|   Completed Validation Procedure
2015/07/20 09:35:08 kid1|   Validated 19487497 Entries
2015/07/20 09:35:08 kid1|   store_swap_size = 460804616.00 KB
2015/07/20 09:35:08 kid1| storeLateRelease: released 0 objects
2015/07/20 09:35:14 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:35:16 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:35:37 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:36:17 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:37:02 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:37:23 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 09:38:46 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "34860-1341477620000" ? "34860-1341477620000"
2015/07/20 09:39:47 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:44:00 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:48:04 kid1| Could not parse headers from on disk object
2015/07/20 09:48:04 kid1| varyEvaluateMatch: Oops. Not a Vary object on second attempt, 'http://www.nafnaf.com/min/?f=/js/modernizr/modernizr.js,/js/jquery/lib/jquery/jquery.min.js,/js/jquery/lib/jquery/jquery-ui.
custom.min.js,/js/jquery/lib/jquery.easing.js,/js/jquery/lib/jquery.base64.js,/js/jquery/lib/jquery.cookie.js,/js/swiper/idangerous.swiper.min.js,/js/jquery/lib/jquery.popupwindows.js,/js/jquery/lib/fancybox/jquer
y.fancybox.pack.js,/js/jquery/lib/validate/jquery.validate.js,/js/jquery/lib/validate/additional-methods.js,/js/jquery/lib/jquery.ba-hashchange.min.js,/js/bootstrap/js/bootstrap.min.js,/js/bootstrap/js/bootstrap-d
ropdownhover.js,/js/jquery/page/jquery.ajax.js,/js/iscroll/iscroll.js,/js/jquery/page/notification/jquery.cookie.js,/js/jquery/page/pageslide/jquery.pageslide.js,/js/jquery/page/innermenu/jquery.innermenu.js,/js/j
query/customer/jquery.customer.js,/js/jquery/catalog/product/jquery.quickview.js,/js/jquery/lib/jquery.unveil.min.js,/js/jquery/catalog/product/jquery.addto.js,/js/jquery/catalog/product/jquery.quantity.js,/js/jqu
ery/checkout/cart/jquery.quantity.js,/js/jquery/checkout/cart/jquery.summary.js,/js/jquery/checkout/cart/jquery.confirmation.js,/js/jquery/wishlist/jquery.quantity.js,/js/jquery/wishlist/jquery.addto.js,/js/jquery
/wishlist/jquery.confirmation.js,/js/jquery/newsletter/jquery.newsletter.js,/js/mgt_developertoolbar/mgt_developertoolbar.js,/js/jquery/analytics/jquery.analytics.js,/js/jquery/analytics/gua/jquery.gua-main.js,/js
/jquery/analytics/gua/jquery.gua-addto.js,/js/jquery/analytics/gua/jquery.gua-customer.js,/js/jquery/analytics/gua/jquery.gua-newsletter.js,/js/jquery/analytics/jquery.mastertrack.js,/js/jquery/redirect/localizati
on/countries_locales_en.js,/js/jquery/redirect/jquery.redirect.js,/js/jquery/catalog/jquery.catalogurl.js,/js/jquery/catalog/jquery.catalogmetas.js,/js/jquery/catalog/product/view/media/jquery.media.js,/js/jquery/
catalog/product/jquery.product.js,/js/jquery/checkout/cart/jquery.addto.js,/js/jquery/reports/jquery.reports.js,/js/jquery/productalert/jquery.productalert.js,/js/jquery/analytics/jquery.tagcommander.js,/js/jquery
/analytics/jquery.eulerianretargeting.js,/js/jquery/analytics/gua/jquery.gua-product-options.js,/js/jquery/social/jquery.sendfriend.js,/js/jquery/webtostore/jquery.socloz.js,/js/jquery/lib/selectBoxIt/jquery.selec
tBoxIt.min.js,/js/jquery/social/jquery.social.js,/js/jquery/lib/validate/localization/messages_fr.js,/js/jquery/redirect/localization/countries_locales_fr.js,/skin/frontend/eplatform/default/js/efunctions.js,/skin
/frontend/eplatform/default/js/eforms.js,/skin/frontend/eplatform/nafnaf/js/functions.js&1434729537' 'accept-encoding="identity,gzip,deflate"'
2015/07/20 09:48:04 kid1| clientProcessHit: Vary object loop!
2015/07/20 09:50:42 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 09:51:06 kid1| Could not parse headers from on disk object
2015/07/20 09:51:06 kid1| varyEvaluateMatch: Oops. Not a Vary object on second attempt, 'http://www.nafnaf.com/min/?f=/js/modernizr/modernizr.js,/js/jquery/lib/jquery/jquery.min.js,/js/jquery/lib/jquery/jquery-ui.
custom.min.js,/js/jquery/lib/jquery.easing.js,/js/jquery/lib/jquery.base64.js,/js/jquery/lib/jquery.cookie.js,/js/swiper/idangerous.swiper.min.js,/js/jquery/lib/jquery.popupwindows.js,/js/jquery/lib/fancybox/jquer
y.fancybox.pack.js,/js/jquery/lib/validate/jquery.validate.js,/js/jquery/lib/validate/additional-methods.js,/js/jquery/lib/jquery.ba-hashchange.min.js,/js/bootstrap/js/bootstrap.min.js,/js/bootstrap/js/bootstrap-d
ropdownhover.js,/js/jquery/page/jquery.ajax.js,/js/iscroll/iscroll.js,/js/jquery/page/notification/jquery.cookie.js,/js/jquery/page/pageslide/jquery.pageslide.js,/js/jquery/page/innermenu/jquery.innermenu.js,/js/j
query/customer/jquery.customer.js,/js/jquery/catalog/product/jquery.quickview.js,/js/jquery/lib/jquery.unveil.min.js,/js/jquery/catalog/product/jquery.addto.js,/js/jquery/catalog/product/jquery.quantity.js,/js/jqu
ery/checkout/cart/jquery.quantity.js,/js/jquery/checkout/cart/jquery.summary.js,/js/jquery/checkout/cart/jquery.confirmation.js,/js/jquery/wishlist/jquery.quantity.js,/js/jquery/wishlist/jquery.addto.js,/js/jquery
/wishlist/jquery.confirmation.js,/js/jquery/newsletter/jquery.newsletter.js,/js/mgt_developertoolbar/mgt_developertoolbar.js,/js/jquery/analytics/jquery.analytics.js,/js/jquery/analytics/gua/jquery.gua-main.js,/js
/jquery/analytics/gua/jquery.gua-addto.js,/js/jquery/analytics/gua/jquery.gua-customer.js,/js/jquery/analytics/gua/jquery.gua-newsletter.js,/js/jquery/analytics/jquery.mastertrack.js,/js/jquery/redirect/localizati
on/countries_locales_en.js,/js/jquery/redirect/jquery.redirect.js,/js/jquery/checkout/cart/jquery.cart.js,/js/jquery/checkout/threestep/jquery.threestep.js,/js/jquery/analytics/gua/checkout/threestep/jquery.gua-fu
nnel.js,/js/jquery/analytics/jquery.tagcommander.js,/js/jquery/analytics/jquery.eulerianretargeting.js,/js/jquery/lib/validate/localization/messages_fr.js,/js/jquery/redirect/localization/countries_locales_fr.js,/
skin/frontend/eplatform/default/js/efunctions.js,/skin/frontend/eplatform/default/js/eforms.js,/skin/frontend/eplatform/nafnaf/js/functions.js,/skin/frontend/eplatform/default/js/checkout/threestep.js,/skin/fronte
nd/eplatform/default/js/checkout.js&1430749814' 'accept-encoding="identity,gzip,deflate"'
2015/07/20 09:51:06 kid1| clientProcessHit: Vary object loop!
2015/07/20 09:53:11 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "34028-1397030626000" ? "34028-1397030626000"
2015/07/20 09:55:55 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 09:58:18 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:02:19 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:06:54 kid1| squidaio_queue_request: WARNING - Queue congestion
2015/07/20 10:11:21 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "32422-1436345442000" ? "32422-1436345442000"
2015/07/20 10:11:36 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:11:47 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:20 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:25 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:32 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:36 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:39 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:12:44 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:15:00 kid1| Could not parse headers from on disk object
2015/07/20 10:15:00 kid1| varyEvaluateMatch: Oops. Not a Vary object on second attempt, 'http://www.aigle.com/min/?f=/js/modernizr/modernizr.js,/js/jquery/lib/jquery/jquery.min.js,/js/jquery/lib/jquery/jquery-ui.c
ustom.min.js,/js/jquery/lib/jquery.easing.js,/js/jquery/lib/jquery.base64.js,/js/jquery/lib/jquery.cookie.js,/js/swiper/idangerous.swiper.min.js,/js/jquery/lib/selectBoxIt/jquery.selectBoxIt.min.js,/js/jquery/lib/
jquery.popupwindows.js,/js/jquery/lib/fancybox/jquery.fancybox.pack.js,/js/jquery/lib/validate/jquery.validate.js,/js/jquery/lib/validate/additional-methods.js,/js/jquery/lib/jquery.ba-hashchange.min.js,/js/bootst
rap/js/bootstrap.min.js,/js/bootstrap/js/bootstrap-dropdownhover.js,/js/jquery/page/jquery.ajax.js,/js/iscroll/iscroll.js,/js/jquery/page/notification/jquery.cookie.js,/js/jquery/page/pageslide/jquery.pageslide.js
,/js/jquery/page/innermenu/jquery.innermenu.js,/js/jquery/customer/jquery.customer.js,/js/jquery/catalog/product/jquery.quickview.js,/js/jquery/lib/jquery.unveil.min.js,/js/jquery/catalog/product/jquery.addto.js,/
js/jquery/catalog/product/jquery.quantity.js,/js/jquery/checkout/cart/jquery.quantity.js,/js/jquery/checkout/cart/jquery.summary.js,/js/jquery/checkout/cart/jquery.confirmation.js,/js/jquery/wishlist/jquery.quanti
ty.js,/js/jquery/wishlist/jquery.addto.js,/js/jquery/wishlist/jquery.confirmation.js,/js/jquery/newsletter/jquery.newsletter.js,/js/mgt_developertoolbar/mgt_developertoolbar.js,/js/jquery/analytics/jquery.analytic
s.js,/js/jquery/analytics/jquery.mastertrack.js,/js/jquery/analytics/gtm/jquery.gtm-main.js,/js/jquery/analytics/gtm/jquery.gtm-newsletter.js,/js/jquery/analytics/gtm/jquery.gtm-cart.js,/js/jquery/analytics/gtm/jq
uery.gtm-wishlist.js,/js/jquery/analytics/gtm/jquery.gtm-quickview.js,/js/jquery/redirect/localization/countries_locales_en.js,/js/jquery/redirect/jquery.redirect.js,/js/jquery/productalert/jquery.productalert.js,
/js/jquery/catalog/jquery.catalogurl.js,/js/jquery/catalog/jquery.catalogmetas.js,/js/jquery/catalog/category/jquery.category.js,/js/jquery/catalog/category/jquery.featured.js,/js/jquery/analytics/jquery.tagcomman
der.js,/js/jquery/analytics/gtm/jquery.gtm-category.js,/js/jquery/lib/validate/localization/messages_fr.js,/js/jquery/redirect/localization/countries_locales_fr.js,/skin/frontend/eplatform/default/js/efunctions.js
,/skin/frontend/eplatform/default/js/eforms.js,/skin/frontend/eplatform/aigle/js/functions.js&1429540234' 'accept-encoding="identity,gzip,deflate"'
2015/07/20 10:15:00 kid1| clientProcessHit: Vary object loop!
2015/07/20 10:20:49 kid1| clientIfRangeMatch: Weak ETags are not allowed in If-Range: "b2222bfe4fbed01:0" ? "537965ecbcc2d01:0"
2015/07/20 10:22:50 kid1| urlParse: Illegal hostname '.xiti.com'
2015/07/20 10:30:03 kid1| urlParse: Illegal hostname '.xiti.com'






From fredbmail at free.fr  Mon Jul 20 08:52:43 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 20 Jul 2015 10:52:43 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <424612827.6711365.1437381960565.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <301851948.6723904.1437382363435.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 'accept-encoding="identity,gzip,deflate"'
> 2015/07/20 10:15:00 kid1| clientProcessHit: Vary object loop!
> 2015/07/20 10:20:49 kid1| clientIfRangeMatch: Weak ETags are not
> allowed in If-Range: "b2222bfe4fbed01:0" ? "537965ecbcc2d01:0"
> 2015/07/20 10:22:50 kid1| urlParse: Illegal hostname '.xiti.com'
> 2015/07/20 10:30:03 kid1| urlParse: Illegal hostname '.xiti.com'
> 
> 
> 
> 


Two others just after I wrote my message

10:32 and 10:47 

2015/07/20 10:47:30 kid1| squidaio_queue_request: WARNING - Queue congestion  


From fredbmail at free.fr  Mon Jul 20 09:09:24 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 20 Jul 2015 11:09:24 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <301851948.6723904.1437382363435.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <858518418.6758979.1437383364233.JavaMail.root@zimbra4-e1.priv.proxad.net>

Argh ! now crash


2015/07/20 11:06:36 kid1| WARNING: swapfile header inconsistent with available data
2015/07/20 11:06:36 kid1| Could not parse headers from on disk object
2015/07/20 11:06:36 kid1| BUG 3279: HTTP reply without Date:
2015/07/20 11:06:36 kid1| StoreEntry->key: F5761430F887925196458A4696151E9C
2015/07/20 11:06:36 kid1| StoreEntry->next: 0xcb6c85b8
2015/07/20 11:06:36 kid1| StoreEntry->mem_obj: 0x3485f19d0
2015/07/20 11:06:36 kid1| StoreEntry->timestamp: -1
2015/07/20 11:06:36 kid1| StoreEntry->lastref: 1437383196
2015/07/20 11:06:36 kid1| StoreEntry->expires: -1
2015/07/20 11:06:36 kid1| StoreEntry->lastmod: -1
2015/07/20 11:06:36 kid1| StoreEntry->swap_file_sz: 0
2015/07/20 11:06:36 kid1| StoreEntry->refcount: 1
2015/07/20 11:06:36 kid1| StoreEntry->flags: CACHABLE,DISPATCHED,PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/07/20 11:06:36 kid1| StoreEntry->swap_dirn: -1
2015/07/20 11:06:36 kid1| StoreEntry->swap_filen: -1
2015/07/20 11:06:36 kid1| StoreEntry->lock_count: 3
2015/07/20 11:06:36 kid1| StoreEntry->mem_status: 0
2015/07/20 11:06:36 kid1| StoreEntry->ping_status: 2
2015/07/20 11:06:36 kid1| StoreEntry->store_status: 1
2015/07/20 11:06:36 kid1| StoreEntry->swap_status: 0
2015/07/20 11:06:43 kid1| assertion failed: store.cc:1876: "isEmpty()"

I never saw this before (with diskd) 


From fredbmail at free.fr  Mon Jul 20 09:23:42 2015
From: fredbmail at free.fr (FredB)
Date: Mon, 20 Jul 2015 11:23:42 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <858518418.6758979.1437383364233.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <145042082.6801773.1437384222107.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> 2015/07/20 11:06:36 kid1| WARNING: swapfile header inconsistent with
> available data
> 2015/07/20 11:06:36 kid1| Could not parse headers from on disk object
> 2015/07/20 11:06:36 kid1| BUG 3279: HTTP reply without Date:
> 2015/07/20 11:06:36 kid1| StoreEntry->key:
> F5761430F887925196458A4696151E9C
> 2015/07/20 11:06:36 kid1| StoreEntry->next: 0xcb6c85b8
> 2015/07/20 11:06:36 kid1| StoreEntry->mem_obj: 0x3485f19d0
> 2015/07/20 11:06:36 kid1| StoreEntry->timestamp: -1
> 2015/07/20 11:06:36 kid1| StoreEntry->lastref: 1437383196
> 2015/07/20 11:06:36 kid1| StoreEntry->expires: -1
> 2015/07/20 11:06:36 kid1| StoreEntry->lastmod: -1
> 2015/07/20 11:06:36 kid1| StoreEntry->swap_file_sz: 0
> 2015/07/20 11:06:36 kid1| StoreEntry->refcount: 1
> 2015/07/20 11:06:36 kid1| StoreEntry->flags:
> CACHABLE,DISPATCHED,PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/07/20 11:06:36 kid1| StoreEntry->swap_dirn: -1
> 2015/07/20 11:06:36 kid1| StoreEntry->swap_filen: -1
> 2015/07/20 11:06:36 kid1| StoreEntry->lock_count: 3
> 2015/07/20 11:06:36 kid1| StoreEntry->mem_status: 0
> 2015/07/20 11:06:36 kid1| StoreEntry->ping_status: 2
> 2015/07/20 11:06:36 kid1| StoreEntry->store_status: 1
> 2015/07/20 11:06:36 kid1| StoreEntry->swap_status: 0
> 2015/07/20 11:06:43 kid1| assertion failed: store.cc:1876:
> "isEmpty()"
> 


(gdb) backtrace
#0  0x00007f0d86f6e165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f0d86f713e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x000000000055234f in xassert (msg=<optimized out>, file=<optimized out>, line=<optimized out>) at debug.cc:566
#3  0x000000000061e004 in StoreEntry::startWriting (this=0x3ba686d20) at store.cc:1876
#4  0x0000000000635a87 in ServerStateData::setFinalReply (this=this at entry=0x3b6187788, rep=rep at entry=0x3bf19a560) at Server.cc:177
#5  0x00000000006390ed in ServerStateData::handleAdaptedHeader (this=0x3b6187788, msg=<optimized out>) at Server.cc:701
#6  0x000000000075218a in JobDialer<Adaptation::Initiator>::dial (this=0x3ba8cc5c0, call=...) at ../../src/base/AsyncJobCalls.h:166
#7  0x00000000006c34e9 in AsyncCall::make (this=0x3ba8cc590) at AsyncCall.cc:32
#8  0x00000000006c714f in AsyncCallQueue::fireNext (this=this at entry=0x1d25120) at AsyncCallQueue.cc:52
#9  0x00000000006c7480 in AsyncCallQueue::fire (this=0x1d25120) at AsyncCallQueue.cc:38
#10 0x000000000056f79c in EventLoop::runOnce (this=this at entry=0x7fff3f50cdd0) at EventLoop.cc:135
#11 0x000000000056f928 in EventLoop::run (this=0x7fff3f50cdd0) at EventLoop.cc:99
#12 0x00000000005e5d44 in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1528
#13 0x00000000004f1fdb in SquidMainSafe (argv=<optimized out>, argc=<optimized out>) at main.cc:1260
#14 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1252
(gdb) bt


From vdoctor at neuf.fr  Mon Jul 20 15:19:02 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 20 Jul 2015 08:19:02 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <55ABCFE8.1020003@treenet.co.nz>
References: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com> <55ABCFE8.1020003@treenet.co.nz>
Message-ID: <1437405542420-4672331.post@n4.nabble.com>

Hi Amos,
Here is the cache.log to check:
http://utimg.unveiltech.com/tmp/amos-cache.tgz

Fred,
I compared the 2 source diskd.cc, squid 3.4.8 and 3.5.6 both official, no
dif.
So, using the diskd 3.4 with the 3.5 does not seem to be a good idea, result
should be the same.

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672331.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Mon Jul 20 15:27:30 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 20 Jul 2015 08:27:30 -0700 (PDT)
Subject: [squid-users] How to get the correct size of a denied object ?
Message-ID: <1437406050879-4672332.post@n4.nabble.com>

Hi All,

As you know, when an object is denied by an ACl or other, the size of the
object in the log file is the size of the ERR_* page.
Is there a way to get the correct/real size of the blocked object ?

I know the url is denied before squid gets the object from internet, but it
should be nice to have a special action/option to write to the access.log
the real size instead the ERR page size.
Because here we don't care the size of the ERR page, knowing the real size
of the denied object is much more important, not meaning the size we blocked
but what is the size we have not downloaded, this is a valuable data with
clients...

Possible to plan a solution for the next build ?
Just get the size by headers, deny the object then write the correct size to
the access.log 

Thanks in advance.

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-get-the-correct-size-of-a-denied-object-tp4672332.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jul 20 16:33:27 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 20 Jul 2015 17:33:27 +0100
Subject: [squid-users] How to get the correct size of a denied object ?
In-Reply-To: <1437406050879-4672332.post@n4.nabble.com>
References: <1437406050879-4672332.post@n4.nabble.com>
Message-ID: <201507201733.27926.Antony.Stone@squid.open.source.it>

On Monday 20 Jul 2015 at 16:27, Stakres wrote:

> As you know, when an object is denied by an ACl or other, the size of the
> object in the log file is the size of the ERR_* page.
> Is there a way to get the correct/real size of the blocked object ?
> 
> Because here we don't care the size of the ERR page, knowing the real size
> of the denied object is much more important, not meaning the size we
> blocked but what is the size we have not downloaded, this is a valuable
> data with clients...

Why not just post-process your log files, and do an HTTP HEAD on the denied 
objects to get their size (when this is available, obviously it won't be for 
everything, but then Squid would have that problem too).


Antony.

-- 
Never write it in Perl if you can do it in Awk.
Never do it in Awk if sed can handle it.
Never use sed when tr can do the job.
Never invoke tr when cat is sufficient.
Avoid using cat whenever possible.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vdoctor at neuf.fr  Mon Jul 20 16:40:47 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 20 Jul 2015 09:40:47 -0700 (PDT)
Subject: [squid-users] How to get the correct size of a denied object ?
In-Reply-To: <201507201733.27926.Antony.Stone@squid.open.source.it>
References: <1437406050879-4672332.post@n4.nabble.com>
 <201507201733.27926.Antony.Stone@squid.open.source.it>
Message-ID: <1437410447162-4672334.post@n4.nabble.com>

Antony,
I got this idea too, but here we "lose" (i mean it's not overwritten) the
info in the access.log and there is no effect of realtime if you see what I
mean...
An alternative could to catch the TCP_DENIED with a helper but I did not
find the way yet, i think it cannot be done this way.

The easy way would be to have a special "%<sd" (Size Denied) or a "%<sr"
(Size Real) in the logformat...

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-get-the-correct-size-of-a-denied-object-tp4672332p4672334.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jul 20 16:47:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 04:47:26 +1200
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1437405542420-4672331.post@n4.nabble.com>
References: <2003322715.31039244.1436961082213.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436962130907-4672231.post@n4.nabble.com>
 <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com> <55ABCFE8.1020003@treenet.co.nz>
 <1437405542420-4672331.post@n4.nabble.com>
Message-ID: <55AD261E.5020009@treenet.co.nz>

On 21/07/2015 3:19 a.m., Stakres wrote:
> Hi Amos,
> Here is the cache.log to check:
> http://utimg.unveiltech.com/tmp/amos-cache.tgz

Thanks. Looks like my guesstimate was good. You have 9 lines there (4K
queue). I'll backport the update shortly as-is.

Amos



From sebag at vianetcon.com.ar  Mon Jul 20 17:11:51 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Mon, 20 Jul 2015 14:11:51 -0300
Subject: [squid-users] Trying to eliminate field from header reply
Message-ID: <55AD2BD7.6000700@vianetcon.com.ar>

Hello, I'm trying to make some modifications to Squid source code. I 
want to eliminate some fields from the header reply that the server sends.
I've been searching through the code but I can't seem to find the exact 
point to make it work.
Does anyone have any clue where (as in which file/s) should I make the 
modification?


Thanks,
Sebastian


From sebag at vianetcon.com.ar  Mon Jul 20 17:15:09 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Mon, 20 Jul 2015 14:15:09 -0300
Subject: [squid-users] Trying to eliminate field from header reply
In-Reply-To: <55AD2BD7.6000700@vianetcon.com.ar>
References: <55AD2BD7.6000700@vianetcon.com.ar>
Message-ID: <55AD2C9D.4060709@vianetcon.com.ar>

Just to add some more information:

In squid 3.5.4, I got to the method processMiss inside 
client_side_reply.cc .. but still couldn't make it work

Thanks,
Sebastian

El 20/07/15 a las 14:11, Sebastian Goicochea escribi?:
> Hello, I'm trying to make some modifications to Squid source code. I 
> want to eliminate some fields from the header reply that the server 
> sends.
> I've been searching through the code but I can't seem to find the 
> exact point to make it work.
> Does anyone have any clue where (as in which file/s) should I make the 
> modification?
>
>
> Thanks,
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From skyvolt at zipmail.com.br  Mon Jul 20 17:24:25 2015
From: skyvolt at zipmail.com.br (Felipe Almeida)
Date: Mon, 20 Jul 2015 14:24:25 -0300
Subject: [squid-users] Block word in a URL part
Message-ID: <55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail>

Hi all,
?
I have a problem here where I need to block some words within a search result in a URL.
?
For example, a I have those two URLs below:
?
https://www.youtube.com/results?search_query=porn
                                            ------?
and
?
https://www.google.com.br/?gfe_rd=cr&ei=EC2tVcjaE_Op8weMyYDoDw&gws_rd=ssl#q=porn
                                                                           ------
?
In these two URLs the word "porn" is there, as a query search result.
?
Is there a way to create a blocking rule for that? So if a user type that word in a search engine, squid will block it.
?
I searched for this kind of configuration in the past emails here but couldn't find it anything regarding this.
?
Thank you.

From alex_wu2012 at hotmail.com  Mon Jul 20 17:28:49 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Mon, 20 Jul 2015 10:28:49 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
	redirection
Message-ID: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>

With 3.5.6 code, we found one thing is broken.

We used pyredir to rewrite request to a surrogated server enabled SSL connection. 

Also, we enable this in squid.conf:

url_rewrite_host_header on

We expect a request to www.foo.com is changed to www.foo-internal.com. 

squid sends the request with the host header rewritten by pyredir as www.foo-internal.com  , but it fails connecting to the server withSSL enabled due to SNI hostname selection (it is under SSLBUMP). We did this change to get it work:

--- a/squid-3.5.6/src/ssl/PeerConnector.cc
+++ b/squid-3.5.6/src/ssl/PeerConnector.cc
@@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()

             // Use SNI TLS extension only when we connect directly
             // to the origin server and we know the server host name.
-            const char *sniServer = hostName ? hostName->c_str() :
-                                    (!request->GetHostIsNumeric() ? request->GetHost() : NULL);
+            const char *sniServer = hostName->c_str();
+            if ( request->flags.redirected && ::Config.onoff.redir_rewrites_host) {
+                sniServer = !request->GetHostIsNumeric() ? request->GetHost() : NULL;
+            }
             if (sniServer) {
                 debugs(83, 5, "SNIserve " << sniServer);
                 Ssl::setClientSNI(ssl, sniServer);


Is this correct?

Alex
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150720/cd8406b6/attachment.htm>

From squid3 at treenet.co.nz  Mon Jul 20 17:34:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 05:34:00 +1200
Subject: [squid-users] How to get the correct size of a denied object ?
In-Reply-To: <1437410447162-4672334.post@n4.nabble.com>
References: <1437406050879-4672332.post@n4.nabble.com>
 <201507201733.27926.Antony.Stone@squid.open.source.it>
 <1437410447162-4672334.post@n4.nabble.com>
Message-ID: <55AD3108.9060705@treenet.co.nz>

On 21/07/2015 4:40 a.m., Stakres wrote:
> Antony,
> I got this idea too, but here we "lose" (i mean it's not overwritten) the
> info in the access.log and there is no effect of realtime if you see what I
> mean...
> An alternative could to catch the TCP_DENIED with a helper but I did not
> find the way yet, i think it cannot be done this way.

You do reaize that objects have many sizes right?

... the gzip compressed size? bzip2 compressed size? chunked encoding
size? what chunk size? (overheads differ by chunk size), what IMS
response size?
 identify size? if text 8-, 16- or 32-bit characters? what language?
(variable words)

 or should you log the size of response *actually* delivered to the
client? eg. the error message.

Most of those are not known without downloading the object in full. Any
of the client headers may also cause embeded content to change. Such as
language headers changing charsets of the content.

Amos



From squid3 at treenet.co.nz  Mon Jul 20 17:36:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 05:36:20 +1200
Subject: [squid-users] Trying to eliminate field from header reply
In-Reply-To: <55AD2C9D.4060709@vianetcon.com.ar>
References: <55AD2BD7.6000700@vianetcon.com.ar>
 <55AD2C9D.4060709@vianetcon.com.ar>
Message-ID: <55AD3194.2010905@treenet.co.nz>

On 21/07/2015 5:15 a.m., Sebastian Goicochea wrote:
> Just to add some more information:
> 
> In squid 3.5.4, I got to the method processMiss inside
> client_side_reply.cc .. but still couldn't make it work
> 

eCAP was created for this purpose.

Amos


From squid3 at treenet.co.nz  Mon Jul 20 17:40:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 05:40:00 +1200
Subject: [squid-users] Block word in a URL part
In-Reply-To: <55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail>
References: <55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail>
Message-ID: <55AD3270.7040309@treenet.co.nz>

On 21/07/2015 5:24 a.m., Felipe Almeida wrote:
> Hi all,
>  
> I have a problem here where I need to block some words within a search result in a URL.
>  
> For example, a I have those two URLs below:
>  
> https://www.youtube.com/results?search_query=porn
>                                             ------ 
> and
>  
> https://www.google.com.br/?gfe_rd=cr&ei=EC2tVcjaE_Op8weMyYDoDw&gws_rd=ssl#q=porn
>                                                                            ------
>  
> In these two URLs the word "porn" is there, as a query search result.
>  
> Is there a way to create a blocking rule for that? So if a user type that word in a search engine, squid will block it.
>  
> I searched for this kind of configuration in the past emails here but couldn't find it anything regarding this.
>  

URL #fragment are not sent "over the wire" as it were. Squid can do
nothing about that.

For the former there is urlpath_regex ACLs.

Amos



From vdoctor at neuf.fr  Mon Jul 20 17:51:24 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 20 Jul 2015 10:51:24 -0700 (PDT)
Subject: [squid-users] How to get the correct size of a denied object ?
In-Reply-To: <55AD3108.9060705@treenet.co.nz>
References: <1437406050879-4672332.post@n4.nabble.com>
 <201507201733.27926.Antony.Stone@squid.open.source.it>
 <1437410447162-4672334.post@n4.nabble.com> <55AD3108.9060705@treenet.co.nz>
Message-ID: <1437414684353-4672343.post@n4.nabble.com>

Amos,
How do you get the real size at the moment with a normal object ?
Just do the same 
I suppose you get the size from the headers, right ?

If we know the object is denied, we ask for a head request to know the size
and we use it in the log.
As the object will be blocked, we don't care this action will need 3 sec or
more, the user will not get the object...
See what I mean ?

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-get-the-correct-size-of-a-denied-object-tp4672332p4672343.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jul 20 18:58:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 06:58:32 +1200
Subject: [squid-users] How to get the correct size of a denied object ?
In-Reply-To: <1437414684353-4672343.post@n4.nabble.com>
References: <1437406050879-4672332.post@n4.nabble.com>
 <201507201733.27926.Antony.Stone@squid.open.source.it>
 <1437410447162-4672334.post@n4.nabble.com> <55AD3108.9060705@treenet.co.nz>
 <1437414684353-4672343.post@n4.nabble.com>
Message-ID: <55AD44D8.3070806@treenet.co.nz>

On 21/07/2015 5:51 a.m., Stakres wrote:
> Amos,
> How do you get the real size at the moment with a normal object ?
> Just do the same 
> I suppose you get the size from the headers, right ?
> 

The log contains the bytes *delivered to the client*.
When accounting bandwidth only the actual transfer sizes matter.


> If we know the object is denied, we ask for a head request to know the size
> and we use it in the log.
> As the object will be blocked, we don't care this action will need 3 sec or
> more, the user will not get the object...
> See what I mean ?

I know what you mean. HTTP does not work that way.


GET / HTTP/1.0
Host: example.com
User-Agent: squidclient/3.5.5
Accept: */*
Connection: close

HTTP/1.1 200 OK
Date: Mon, 20 Jul 2015 18:29:02 GMT
Server: Apache/2.4.12 (Debian)
Vary: Accept-Encoding
Content-Length: 747
Connection: close
Content-Type: text/html;charset=UTF-8


GET / HTTP/1.0
Host: example.com
User-Agent: squidclient/3.5.5
Accept: */*
Connection: close
Accept-Encoding:gzip

HTTP/1.1 200 OK
Date: Mon, 20 Jul 2015 18:30:38 GMT
Server: Apache/2.4.12 (Debian)
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Length: 405
Connection: close
Content-Type: text/html;charset=UTF-8


What the server was actually sending?

HEAD / HTTP/1.0
Host: example.com
User-Agent: squidclient/3.5.5
Accept: */*
Connection: close

HTTP/1.1 200 OK
Date: Mon, 20 Jul 2015 18:29:49 GMT
Server: Apache/2.4.12 (Debian)
Connection: close
Content-Type: text/html;charset=UTF-8


Amos



From rousskov at measurement-factory.com  Mon Jul 20 19:10:26 2015
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Jul 2015 13:10:26 -0600
Subject: [squid-users] SSL connction failed due to SNI after content
	redirection
In-Reply-To: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
Message-ID: <55AD47A2.90003@measurement-factory.com>

On 07/20/2015 11:28 AM, Alex Wu wrote:
> With 3.5.6 code, we found one thing is broken.
> 
> We used pyredir to rewrite request to a surrogated server enabled SSL
> connection.
> 
> Also, we enable this in squid.conf:
> 
> url_rewrite_host_header on
> 
> We expect a request to www.foo.com is changed to www.foo-internal.com.
> 
> squid sends the request with the host header rewritten by pyredir as
> www.foo-internal.com  , but it fails connecting to the server withSSL
> enabled due to SNI hostname selection (it is under SSLBUMP). We did this
> change to get it work:
> 
> --- a/squid-3.5.6/src/ssl/PeerConnector.cc
> +++ b/squid-3.5.6/src/ssl/PeerConnector.cc
> @@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()
> 
>              // Use SNI TLS extension only when we connect directly
>              // to the origin server and we know the server host name.
> -            const char *sniServer = hostName ? hostName->c_str() :
> -                                    (!request->GetHostIsNumeric() ?
> request->GetHost() : NULL);
> +            const char *sniServer = hostName->c_str();
> +            if ( request->flags.redirected &&
> ::Config.onoff.redir_rewrites_host) {
> +                sniServer = !request->GetHostIsNumeric() ?
> request->GetHost() : NULL;
> +            }
>              if (sniServer) {
>                  debugs(83, 5, "SNIserve " << sniServer);
>                  Ssl::setClientSNI(ssl, sniServer);
> 
> 
> Is this correct?


Not quite: Your code is unconditionally dereferencing hostName which
might be NULL. You also seem to disable the request->GetHost() path for
cases where flags.redirected && redir_rewrites_host is false. However, I
am not an expert on rewrite request APIs...

You may want to move this to squid-dev or Bugzilla.


HTH,

Alex.



From alex_wu2012 at hotmail.com  Mon Jul 20 19:34:05 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Mon, 20 Jul 2015 12:34:05 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <55AD47A2.90003@measurement-factory.com>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>,
 <55AD47A2.90003@measurement-factory.com>
Message-ID: <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>

That's right,

It should be as follows:

--- a/squid-3.5.6/src/ssl/PeerConnector.cc
+++ b/squid-3.5.6/src/ssl/PeerConnector.cc
@@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()

             // Use SNI TLS extension only when we connect directly
             // to the origin server and we know the server host name.
-            const char *sniServer = hostName ? hostName->c_str() :
-                                    (!request->GetHostIsNumeric() ? request->GetHost() : NULL);
+            const char *sniServer = hostName->c_str();
+            if (request->flags.redirected &&
+                ::Config.onoff.redir_rewrites_host &&
+                !request->GetHostIsNumeric() ) {
+                sniServer = request->GetHost();
+            }
             if (sniServer) {
                 debugs(83, 5, "SNIserve " << sniServer);
                 Ssl::setClientSNI(ssl, sniServer);

Let me see if I can get squid-dev.

Alex


> Date: Mon, 20 Jul 2015 13:10:26 -0600
> From: rousskov at measurement-factory.com
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL connction failed due to SNI after content redirection
> 
> On 07/20/2015 11:28 AM, Alex Wu wrote:
> > With 3.5.6 code, we found one thing is broken.
> > 
> > We used pyredir to rewrite request to a surrogated server enabled SSL
> > connection.
> > 
> > Also, we enable this in squid.conf:
> > 
> > url_rewrite_host_header on
> > 
> > We expect a request to www.foo.com is changed to www.foo-internal.com.
> > 
> > squid sends the request with the host header rewritten by pyredir as
> > www.foo-internal.com  , but it fails connecting to the server withSSL
> > enabled due to SNI hostname selection (it is under SSLBUMP). We did this
> > change to get it work:
> > 
> > --- a/squid-3.5.6/src/ssl/PeerConnector.cc
> > +++ b/squid-3.5.6/src/ssl/PeerConnector.cc
> > @@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()
> > 
> >              // Use SNI TLS extension only when we connect directly
> >              // to the origin server and we know the server host name.
> > -            const char *sniServer = hostName ? hostName->c_str() :
> > -                                    (!request->GetHostIsNumeric() ?
> > request->GetHost() : NULL);
> > +            const char *sniServer = hostName->c_str();
> > +            if ( request->flags.redirected &&
> > ::Config.onoff.redir_rewrites_host) {
> > +                sniServer = !request->GetHostIsNumeric() ?
> > request->GetHost() : NULL;
> > +            }
> >              if (sniServer) {
> >                  debugs(83, 5, "SNIserve " << sniServer);
> >                  Ssl::setClientSNI(ssl, sniServer);
> > 
> > 
> > Is this correct?
> 
> 
> Not quite: Your code is unconditionally dereferencing hostName which
> might be NULL. You also seem to disable the request->GetHost() path for
> cases where flags.redirected && redir_rewrites_host is false. However, I
> am not an expert on rewrite request APIs...
> 
> You may want to move this to squid-dev or Bugzilla.
> 
> 
> HTH,
> 
> Alex.
> 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150720/38527658/attachment.htm>

From alex_wu2012 at hotmail.com  Mon Jul 20 19:56:25 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Mon, 20 Jul 2015 12:56:25 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>, ,
 <55AD47A2.90003@measurement-factory.com>,
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
Message-ID: <BAY181-W89D782282C2A247846C96083850@phx.gbl>

Overlooked before.

This should be right now:

--- a/squid-3.5.6/src/ssl/PeerConnector.cc
+++ b/squid-3.5.6/src/ssl/PeerConnector.cc
@@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()

             // Use SNI TLS extension only when we connect directly
             // to the origin server and we know the server host name.
-            const char *sniServer = hostName ? hostName->c_str() :
-                                    (!request->GetHostIsNumeric() ? request->GetHost() : NULL);
+            const char *sniServer = hostName ? hostName->c_str() : NULL;
+            if (!sniServer ||
+                (request->flags.redirected &&
+                ::Config.onoff.redir_rewrites_host)) {
+                sniServer = !request->GetHostIsNumeric() ? request->GetHost() : NULL;
+            }
             if (sniServer) {
                 debugs(83, 5, "SNIserve " << sniServer);
                 Ssl::setClientSNI(ssl, sniServer);


Alex
From: alex_wu2012 at hotmail.com
To: rousskov at measurement-factory.com; squid-users at lists.squid-cache.org
Date: Mon, 20 Jul 2015 12:34:05 -0700
Subject: Re: [squid-users] SSL connction failed due to SNI after content redirection




That's right,

It should be as follows:

--- a/squid-3.5.6/src/ssl/PeerConnector.cc
+++ b/squid-3.5.6/src/ssl/PeerConnector.cc
@@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()

             // Use SNI TLS extension only when we connect directly
             // to the origin server and we know the server host name.
-            const char *sniServer = hostName ? hostName->c_str() :
-                                    (!request->GetHostIsNumeric() ? request->GetHost() : NULL);
+            const char *sniServer = hostName->c_str();
+            if (request->flags.redirected &&
+                ::Config.onoff.redir_rewrites_host &&
+                !request->GetHostIsNumeric() ) {
+                sniServer = request->GetHost();
+            }
             if (sniServer) {
                 debugs(83, 5, "SNIserve " << sniServer);
                 Ssl::setClientSNI(ssl, sniServer);

Let me see if I can get squid-dev.

Alex




> Date: Mon, 20 Jul 2015 13:10:26 -0600
> From: rousskov at measurement-factory.com
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL connction failed due to SNI after content redirection
> 
> On 07/20/2015 11:28 AM, Alex Wu wrote:
> > With 3.5.6 code, we found one thing is broken.
> > 
> > We used pyredir to rewrite request to a surrogated server enabled SSL
> > connection.
> > 
> > Also, we enable this in squid.conf:
> > 
> > url_rewrite_host_header on
> > 
> > We expect a request to www.foo.com is changed to www.foo-internal.com.
> > 
> > squid sends the request with the host header rewritten by pyredir as
> > www.foo-internal.com  , but it fails connecting to the server withSSL
> > enabled due to SNI hostname selection (it is under SSLBUMP). We did this
> > change to get it work:
> > 
> > --- a/squid-3.5.6/src/ssl/PeerConnector.cc
> > +++ b/squid-3.5.6/src/ssl/PeerConnector.cc
> > @@ -191,8 +194,10 @@ Ssl::PeerConnector::initializeSsl()
> > 
> >              // Use SNI TLS extension only when we connect directly
> >              // to the origin server and we know the server host name.
> > -            const char *sniServer = hostName ? hostName->c_str() :
> > -                                    (!request->GetHostIsNumeric() ?
> > request->GetHost() : NULL);
> > +            const char *sniServer = hostName->c_str();
> > +            if ( request->flags.redirected &&
> > ::Config.onoff.redir_rewrites_host) {
> > +                sniServer = !request->GetHostIsNumeric() ?
> > request->GetHost() : NULL;
> > +            }
> >              if (sniServer) {
> >                  debugs(83, 5, "SNIserve " << sniServer);
> >                  Ssl::setClientSNI(ssl, sniServer);
> > 
> > 
> > Is this correct?
> 
> 
> Not quite: Your code is unconditionally dereferencing hostName which
> might be NULL. You also seem to disable the request->GetHost() path for
> cases where flags.redirected && redir_rewrites_host is false. However, I
> am not an expert on rewrite request APIs...
> 
> You may want to move this to squid-dev or Bugzilla.
> 
> 
> HTH,
> 
> Alex.
> 
 		 	   		  

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150720/18ef6cd1/attachment.htm>

From stan.prescott at gmail.com  Mon Jul 20 21:36:27 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Mon, 20 Jul 2015 16:36:27 -0500
Subject: [squid-users] Squid 3.5.5 ssl_bump and ufdbGuard
Message-ID: <CANLNtGQQz9HSxoHAeK4Tr+bRVcZ7CF8zbCHHE6=-Cqv_MYgsag@mail.gmail.com>

This probably more rightly belongs in the ufdbGuard mailing list, but SF
has been down for several days and I cannot post there. There is a bit of
overlap with ssl_bump and ufdGuard with one of the issues I am having.
Maybe someone here who uses ufdbGuard or squidGuard could help me?

I am trying to replace our implementation of the old squidGuard with
ufdbGuard on Smoothwall Express v3.1 firewall distro. I have gotten
ufdbGuard running and filtering with Squid 3.5.5 using ssl_bump.My
questions:

1. With ssl_bump and squidGuard I was able to use the urlfilter to block
https sites like facebook.com. Allowed https sites would load in my browser
without errors with ssl_bump and squidGuard active. With ssl_bump and
ufdbGuard it is a lot more complicated, it seems.

-Squid+ssl_bump and ufdbGuard running I can access all HTTP sites without
errors. I cannot access any HTTPS sites at all. I get "Untrusted
connection" errors when trying to load any HTTPS site.

-If I restart squid without ssl_bump and ufdbGuard still running, I can
then access all HTTP and HTTPS sites and categories that I have blocked do
get blocked, but only HTTP.sites. All HTTPS sites will load, but none get
blocked that are supposed to be.

-If I then restart squid+ssl_bump (and ufdbGuard still running) I can now
access all HTTP and HTTPS sites. Also, all HTTP and HTTPS sites that are
supposed to be blocked by category, like porn for instance, do get blocked
like they are supposed to be. Except for domains in the alwaysdeny category
(but that will be a question for another time).

-When ufdbGuard and squid+ssl_bump are started (in that order) I see
processes running for squid, ssl_crtd, and ufdbguardd. I do not see any
processes for squid_redirect and ufdbgclient. If I enter and load a website
and then check the processes running I then see squid_redirect and
ufdbgclient. Is that supposed to happen like that?

2. I am using the Shalla blacklists for testing. I haven't been able to
sign up for a URLfilterDB free trial because I only use yahoo.com and
gmail.com for my email. Plus, I don't want to pay for a subscription until
I know I have this working. When I convert the Shalla blacklists to ufdb
format using ufdbConvertDB, only the domains are converted to the ufdb
format (domains.ufdb). The urls files are not converted, even when using
the "-u urls" switch.

My current ufdbGuard.conf file is attached..
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150720/5e5aceec/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ufdbGuard.conf
Type: application/octet-stream
Size: 21525 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150720/5e5aceec/attachment.obj>

From marcus.kool at urlfilterdb.com  Tue Jul 21 00:30:57 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 20 Jul 2015 21:30:57 -0300
Subject: [squid-users] Squid 3.5.5 ssl_bump and ufdbGuard
In-Reply-To: <CANLNtGQQz9HSxoHAeK4Tr+bRVcZ7CF8zbCHHE6=-Cqv_MYgsag@mail.gmail.com>
References: <CANLNtGQQz9HSxoHAeK4Tr+bRVcZ7CF8zbCHHE6=-Cqv_MYgsag@mail.gmail.com>
Message-ID: <55AD92C1.7060709@urlfilterdb.com>

First an introduction in blocking HTTPS:
HTTPS is a protocol that is designed to be non-interceptable, and if it is intercepted, the browser will notify the user about this interception.
This is very different from HTTP which can easily be intercepted and the interceptor can redirect a browser using a defined HTTP code for redirection.
So blocking HTTP sites is easy since the HTTP protocol supports redirection and because of the redirection feature, a blocked URL can be redirected to a human readable web page saying "site X is 
blocked since you have no access rights".

Blocking HTTPS by redirecting a browser request is not possible since HTTPS is encrypted.
To block such request, squidGuard and ufdbGuard can only instruct Squid to replace the HTTPS URL with another HTTPS URL.
squidGuard, which development stopped in 2010, does not support a HTTPS redirection URL and instead, sends the redirection URL for HTTP.
ufdbGuard uses 2 redirection URLs: one for HTTP and one for HTTPS, so the blocked HTTPS-based URL is redirected to another HTTPS-based URL.
But the browser notes this and display a warning. Most likely that the certificate is wrong.
After accepting the warning, a human-readable message about access being denied, is displayed.

Squids ssl-dump feature, if configured, changes the above.
ssl-bump intercepts HTTPS traffic but the browser detects this and warns about it.
To get rid of the warning permanently, one installs the certificate that Squid uses in the browser's certificate store.

Blocking HTTPS, however, remains a difficult issue.
For HTTPS websites, Squid sends to ufdbGuard/squidGuard first a CONNECT-URL and after that a GET-URL or POST-URL.
The CONNECT is not blockable in a sense that it can happen without browser warnings, so for a forbidden HTTPS site, the URL redirector must PASS the CONNECT-URL and wait for the GET/POST which it can 
block later.
This strategy works for regular HTTPS sites, where the site uses SSL-wrapped HTTP.
However, this strategy fails for all other sites that use different protocols, for example: chat, VPN, remote access software and SSH.
So for SSH, Squid only sends a CONNECT-URL to the URL redirector and it must decide whether to pass or allow on the CONNECT since there are no future GET/POST URLs that it may block.
This complicates things a lot.
The next version of ufdbGuard will have new features to attempt to get around these issues.

On 07/20/2015 06:36 PM, Stanford Prescott wrote:
> This probably more rightly belongs in the ufdbGuard mailing list, but SF has been down for several days and I cannot post there. There is a bit of overlap with ssl_bump and ufdGuard with one of the
> issues I am having. Maybe someone here who uses ufdbGuard or squidGuard could help me?

SF works now...

> I am trying to replace our implementation of the old squidGuard with ufdbGuard on Smoothwall Express v3.1 firewall distro. I have gotten ufdbGuard running and filtering with Squid 3.5.5 using
> ssl_bump.My questions:
>
> 1. With ssl_bump and squidGuard I was able to use the urlfilter to block https sites like facebook.com <http://facebook.com>. Allowed https sites would load in my browser without errors with ssl_bump
> and squidGuard active. With ssl_bump and ufdbGuard it is a lot more complicated, it seems.

Are you saying that blocking https://www.example.com with ufdbGuard and ssl-bumps works ?
What is the redirection URL ?

> -Squid+ssl_bump and ufdbGuard running I can access all HTTP sites without errors. I cannot access any HTTPS sites at all. I get "Untrusted connection" errors when trying to load any HTTPS site.

"*any* HTTPS site" ??
Awkward at least.  Can you send me your entire ufdbguardd.log and squid.conf ?  NOT on this list.

> -If I restart squid without ssl_bump and ufdbGuard still running, I can then access all HTTP and HTTPS sites and categories that I have blocked do get blocked, but only HTTP.sites. All HTTPS sites
> will load, but none get blocked that are supposed to be.

Again, awkward.  I have a suspicion that something is wrong in your configuration.

> -If I then restart squid+ssl_bump (and ufdbGuard still running) I can now access all HTTP and HTTPS sites. Also, all HTTP and HTTPS sites that are supposed to be blocked by category, like porn for
> instance, do get blocked like they are supposed to be. Except for domains in the alwaysdeny category (but that will be a question for another time).

Again, awkward...

> -When ufdbGuard and squid+ssl_bump are started (in that order) I see processes running for squid, ssl_crtd, and ufdbguardd. I do not see any processes for squid_redirect and ufdbgclient. If I enter
> and load a website and then check the processes running I then see squid_redirect and ufdbgclient. Is that supposed to happen like that?

Squid starts processes when it needs to and its behaviour is also controlled by the url_rewrite_children parameter.
If you have "url_rewrite_children 30 startup=3 idle=2 concurrency=2" you should see 3 processes after a fresh start.

I do not know "squid_redirect" processes.
What do you have configured for squid_redirect ?

> 2. I am using the Shalla blacklists for testing. I haven't been able to sign up for a URLfilterDB free trial because I only use yahoo.com <http://yahoo.com> and gmail.com <http://gmail.com> for my
> email. Plus, I don't want to pay for a subscription until I know I have this working. When I convert the Shalla blacklists to ufdb format using ufdbConvertDB, only the domains are converted to the
> ufdb format (domains.ufdb). The urls files are not converted, even when using the "-u urls" switch.

A trial is free. You do not pay if the trial was unsatisfactory.
ufdbConvertDB converts the files 'domains' and the optional file 'urls' into 'domains.ufdb'.  A 'urls.ufdb' file never exists since everything is in 'domains.ufdb'.

I think that almost all items are releated to ufdbGuard, so you can email the support desk of ufdbguard directly for assistance.
The support desk answers also those who use ufdbguard with a free database.

Marcus

> My current ufdbGuard.conf file is attached..
>
>


From fredbmail at free.fr  Tue Jul 21 06:53:04 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 21 Jul 2015 08:53:04 +0200 (CEST)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <1437405542420-4672331.post@n4.nabble.com>
Message-ID: <2129667346.8857723.1437461584471.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Fred,
> I compared the 2 source diskd.cc, squid 3.4.8 and 3.5.6 both
> official, no
> dif.
> So, using the diskd 3.4 with the 3.5 does not seem to be a good idea,
> result
> should be the same.
> 
> Fred

No crash for you ?

I confirm this discussion http://squid-web-proxy-cache.1019090.n4.nabble.com/BUG-3279-HTTP-reply-without-Date-td4664990.html
The crashes are related with aufs 


From wolle5050 at gmx.de  Tue Jul 21 07:59:56 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Tue, 21 Jul 2015 09:59:56 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
Message-ID: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>

I am running Squid3 3.3.8 on Ubuntu 14.04. Squid3 has been installed from the Ubuntu package repository. In my scenario, Squid has to cache big files >= 1 GB. At the moment, I am getting very bad transfer rates lower that 1 MB/sec. I have checked the connectivity using iperf3. It gives my a bandwith of 853 Mbits/sec between the nodes.

I have tried to investigate the problem and recognized that when there is no cache hit for a requested object, the Squid process reaches shortly after startup 100 % of one CPU core. The download rate drops down to 1 MB/sec. When I have a cache hit, I only get 30 MB/sec in my download.

Is there someting wrong with my config? I have already used Squid 3.3.14. I get the same result. Unfortunately, I was not able to build Squid 3.5.5 and 3.5.6.

Here is my squid.conf:
# ACCESS CONTROLS
# ----------------------------------------------------------------------------
  acl intranet    src 139.2.0.0/16
  acl intranet    src 193.96.112.0/21
  acl intranet    src 192.109.216.0/24
  acl intranet    src 100.1.4.0/22
  acl localnet    src 10.0.0.0/8
  acl localnet    src 172.16.0.0/12
  acl localnet    src 192.168.0.0/16
  acl localnet    src fc00::/7
  acl localnet    src fe80::/10
  acl to_intranet dst 139.2.0.0/16
  acl to_intranet dst 193.96.112.0/21
  acl to_intranet dst 192.109.216.0/24
  acl to_intranet dst 100.1.4.0/22
  acl to_localnet dst 10.0.0.0/8
  acl to_localnet dst 172.16.0.0/12
  acl to_localnet dst 192.168.0.0/16
  acl to_localnet dst fc00::/7
  acl to_localnet dst fe80::/10
  http_access allow manager localhost
  http_access deny  manager
  http_access allow localnet
  http_access allow localhost
  http_access deny all

# NETWORK OPTIONS
# ----------------------------------------------------------------------------
  http_port 0.0.0.0:3128

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# ----------------------------------------------------------------------------
  cache_peer proxy.mycompany.de parent 8080 0 no-query no-digest

# MEMORY CACHE OPTIONS
# ----------------------------------------------------------------------------
  maximum_object_size_in_memory 1 GB
  memory_replacement_policy heap LFUDA
  cache_mem 4 GB

# DISK CACHE OPTIONS
# ----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240

# LOGFILE OPTIONS
# ----------------------------------------------------------------------------
  access_log daemon:/var/log/squid3/access.log squid
  cache_store_log daemon:/var/log/squid3/store.log

# OPTIONS FOR TROUBLESHOOTING
# ----------------------------------------------------------------------------
  cache_log /var/log/squid3/cache.log
  coredump_dir /var/log/squid3

# OPTIONS FOR TUNING THE CACHE
# ----------------------------------------------------------------------------
  cache allow localnet
  cache allow localhost
  cache allow intranet
  cache deny  all
  refresh_pattern ^ftp:              1440    20%    10080
  refresh_pattern ^gopher:           1440     0%     1440
  refresh_pattern -i (/cgi-bin/|\?)     0     0%        0
  refresh_pattern .                     0    20%     4320

# HTTP OPTIONS
# ----------------------------------------------------------------------------
  via off

# ADMINISTRATIVE PARAMETERS
# ----------------------------------------------------------------------------
  cache_effective_user proxy
  cache_effective_group proxy

# ICP OPTIONS
# ----------------------------------------------------------------------------
  icp_port 0

# OPTIONS INFLUENCING REQUEST FORWARDING 
# ----------------------------------------------------------------------------
  nonhierarchical_direct on
  prefer_direct off
  always_direct allow to_localnet
  always_direct allow to_localhost
  always_direct allow to_intranet
  never_direct  allow all

# MISCELLANEOUS
# ----------------------------------------------------------------------------
  memory_pools off
  forwarded_for off


From vdoctor at neuf.fr  Tue Jul 21 08:26:39 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 21 Jul 2015 01:26:39 -0700 (PDT)
Subject: [squid-users] AUFS vs. DISKS
In-Reply-To: <2129667346.8857723.1437461584471.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1715566880.31130366.1436963530934.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436964723705-4672235.post@n4.nabble.com>
 <789294187.31229865.1436966292542.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1845281024.31248976.1436966818768.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1436968298185-4672247.post@n4.nabble.com> <55A67096.2060005@treenet.co.nz>
 <55A67E1D.7040801@urlfilterdb.com> <55ABCFE8.1020003@treenet.co.nz>
 <1437405542420-4672331.post@n4.nabble.com>
 <2129667346.8857723.1437461584471.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1437467199065-4672352.post@n4.nabble.com>

Hi Fred,

No error, no crash.
Some warnings only:
2015/07/21 11:21:02 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
But we can live with these warnings, Squid will take care the missing
objects...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/AUFS-vs-DISKS-tp4672209p4672352.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From meyer at makeit-so.de  Tue Jul 21 09:58:58 2015
From: meyer at makeit-so.de (Arnaud Meyer)
Date: Tue, 21 Jul 2015 11:58:58 +0200
Subject: [squid-users] squid 3.5.5 - assertion failed
Message-ID: <55AE17E2.40609@makeit-so.de>

Hi,

I'm using Squid 3.5.5 on Debian Wheezy as a transparent proxy with https 
interception. About once per hour I'm getting the following error 
message before Squid crashes:

    assertion failed: Read.cc:69: "fd_table[conn->fd].halfClosedReader
    != NULL"


The gdb stack trace is attached below.

Any help would be much appreciated.

Arnaud

-----------

Program received signal SIGABRT, Aborted.
0x00007ffff4b1b165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#0  0x00007ffff4b1b165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007ffff4b1e3e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00000000005a1adf in xassert (msg=<optimized out>, file=<optimized 
out>,
     line=<optimized out>) at debug.cc:544
#3  0x00000000007c65f8 in comm_read_base (conn=..., buf=0x99bb140 "", 
size=16383,
     callback=...) at Read.cc:69
#4  0x000000000067b75b in comm_read (callback=..., len=16383, 
buf=0x99bb140 "", conn=...)
     at comm/Read.h:58
#5  StoreEntry::delayAwareRead (this=<optimized out>, conn=..., 
buf=0x99bb140 "", len=16383,
     callback=...) at store.cc:255
#6  0x00000000005f90d2 in HttpStateData::maybeReadVirginBody 
(this=0x83f0ed8) at http.cc:1515
#7  0x00000000005f959c in HttpStateData::sendRequest 
(this=this at entry=0x83f0ed8)
     at http.cc:2154
#8  0x00000000005f9d7b in HttpStateData::start (this=0x83f0ed8) at 
http.cc:2268
#9  0x0000000000731393 in JobDialer<AsyncJob>::dial (this=0xaee57d0, 
call=...)
     at ../../src/base/AsyncJobCalls.h:174
#10 0x000000000072d909 in AsyncCall::make (this=0xaee57a0) at 
AsyncCall.cc:40
#11 0x0000000000731887 in AsyncCallQueue::fireNext 
(this=this at entry=0xe025f0)
     at AsyncCallQueue.cc:56
#12 0x0000000000731bd0 in AsyncCallQueue::fire (this=0xe025f0) at 
AsyncCallQueue.cc:42
#13 0x00000000005c128c in EventLoop::runOnce 
(this=this at entry=0x7fffffffe9f0)
     at EventLoop.cc:120
#14 0x00000000005c1430 in EventLoop::run (this=0x7fffffffe9f0) at 
EventLoop.cc:82
#15 0x0000000000627083 in SquidMain (argc=<optimized out>, 
argv=<optimized out>)
     at main.cc:1511
#16 0x000000000052b08b in SquidMainSafe (argv=<optimized out>, 
argc=<optimized out>)
     at main.cc:1243
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1236
A debugging session is active.

         Inferior 1 [process 10969] will be killed.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150721/70e2a450/attachment.htm>

From hack.back at hotmail.com  Tue Jul 21 10:55:19 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 21 Jul 2015 03:55:19 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 - assertion failed
In-Reply-To: <55AE17E2.40609@makeit-so.de>
References: <55AE17E2.40609@makeit-so.de>
Message-ID: <1437476119842-4672354.post@n4.nabble.com>

are you using range_offset_limit option ??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-assertion-failed-tp4672353p4672354.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From meyer at makeit-so.de  Tue Jul 21 11:12:40 2015
From: meyer at makeit-so.de (Arnaud Meyer)
Date: Tue, 21 Jul 2015 13:12:40 +0200
Subject: [squid-users] squid 3.5.5 - assertion failed
In-Reply-To: <1437476119842-4672354.post@n4.nabble.com>
References: <55AE17E2.40609@makeit-so.de>
 <1437476119842-4672354.post@n4.nabble.com>
Message-ID: <55AE2928.7090505@makeit-so.de>

No, I'm not using this option.

You can see my complete squid.conf here: http://pastebin.com/mzFBDLpY

Am 21.07.2015 um 12:55 schrieb HackXBack:
> are you using range_offset_limit option ??
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-assertion-failed-tp4672353p4672354.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Jul 21 11:42:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jul 2015 23:42:29 +1200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>
References: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>
Message-ID: <55AE3025.4010206@treenet.co.nz>

On 21/07/2015 7:59 p.m., Jens Offenbach wrote:
> I am running Squid3 3.3.8 on Ubuntu 14.04. Squid3 has been installed from the Ubuntu package repository. In my scenario, Squid has to cache big files >= 1 GB. At the moment, I am getting very bad transfer rates lower that 1 MB/sec. I have checked the connectivity using iperf3. It gives my a bandwith of 853 Mbits/sec between the nodes.
> 
> I have tried to investigate the problem and recognized that when there is no cache hit for a requested object, the Squid process reaches shortly after startup 100 % of one CPU core. The download rate drops down to 1 MB/sec. When I have a cache hit, I only get 30 MB/sec in my download.
> 
> Is there someting wrong with my config? I have already used Squid 3.3.14. I get the same result. Unfortunately, I was not able to build Squid 3.5.5 and 3.5.6.
> 

Squid-3 is better able to cope with large objects than Squid-2 was. But
there are still significant problems.


Firstly, you only have space in memory for 4x 1GB objects. Total. if you
are dealing with such large objects at any regular frequency, you need
at least a much larger cache_mem setting.


Secondly, consider that Squid-3.3 places *all* active transactions into
cache_mem. 4GB of memory cache can store ~4 million x 1KB transactions,
or only 4 x 1GB transactions.

 If you have a cache full of small objects happily sitting in memory.
Then a requests for a 1GB object comes in, a hugh number of those small
objects need to be pushed out of memory cache onto disk, the memory
reallocated for use by the big one, and possibly 1GB object loaded from
disk into memory cache.

Then consider that GB sized object sitting in cache as it gets near to
being the oldest in memory. The next request is probably a puny little
0-1KB object, Squid may have to repeat all the GB size shufflings to and
from disk just to make memory space for that KB.

As you can imagine any one part of that process takes a lot of work and
time with a big object involved as compared to only small objects being
involved. The whole set of actions can be excruciatingly painful if the
proxy is busy.


Thirdly, you also only have 88GB of disk cache total. Neither that nor
the memory cache is sufficient to be trying to cache GB sized objects.
The tradeoff is whether one GB size object is going to get enough HITs
often enough to be worth not caching the million or so smaller objects
that could be taking its place. For most uses the tradeoff only makes
sense with high traffic on the large objects and/or TB of disk space.


My rule-of-thumb advice for caching is to keep it so that you can store
at least a few thousand maximum-sized objects at once in the allocated size.

So 4GB memory cache reasonable for 1MB size objects, 80GB disk cache is
reasonable for ~100MB sized objects.

That keeps almost all web page traffic able to be in memory, bigger but
popular media/video objects on disk. And the big things like Windows
Service Packs or whole DVD downloads get slower network fetches as
needed. If those latter are actually a problem for you get a bigger disk
cache, you *will* need it.


And a free audit for your config...


> Here is my squid.conf:
> # ACCESS CONTROLS
> # ----------------------------------------------------------------------------
>   acl intranet    src 139.2.0.0/16
>   acl intranet    src 193.96.112.0/21
>   acl intranet    src 192.109.216.0/24
>   acl intranet    src 100.1.4.0/22
>   acl localnet    src 10.0.0.0/8
>   acl localnet    src 172.16.0.0/12
>   acl localnet    src 192.168.0.0/16
>   acl localnet    src fc00::/7
>   acl localnet    src fe80::/10
>   acl to_intranet dst 139.2.0.0/16
>   acl to_intranet dst 193.96.112.0/21
>   acl to_intranet dst 192.109.216.0/24
>   acl to_intranet dst 100.1.4.0/22
>   acl to_localnet dst 10.0.0.0/8
>   acl to_localnet dst 172.16.0.0/12
>   acl to_localnet dst 192.168.0.0/16
>   acl to_localnet dst fc00::/7
>   acl to_localnet dst fe80::/10

The intended purpose behind the localnet and to_localnet ACLs is that
they are matching your intranet / LAN  / local network ranges.

The ones we distribute are just common standard ranges. You can simplify
your config by adding the intranet ranges to localnet and dropping all
the 'intranet' ACLs.

... BUT ...


>   http_access allow manager localhost
>   http_access deny  manager
>   http_access allow localnet
>   http_access allow localhost
>   http_access deny all

... noting how the intranet ACLs are not used to permit access through
the proxy. Maybe just dropping them entirely is better. If this is a
working proxy they are not being used.


> 
> # NETWORK OPTIONS
> # ----------------------------------------------------------------------------
>   http_port 0.0.0.0:3128
> 
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
> # ----------------------------------------------------------------------------
>   cache_peer proxy.mycompany.de parent 8080 0 no-query no-digest
> 
> # MEMORY CACHE OPTIONS
> # ----------------------------------------------------------------------------
>   maximum_object_size_in_memory 1 GB
>   memory_replacement_policy heap LFUDA
>   cache_mem 4 GB
> 
> # DISK CACHE OPTIONS
> # ----------------------------------------------------------------------------
>   maximum_object_size 10 GB
>   cache_replacement_policy heap GDSF
>   cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240
> 
> # LOGFILE OPTIONS
> # ----------------------------------------------------------------------------
>   access_log daemon:/var/log/squid3/access.log squid
>   cache_store_log daemon:/var/log/squid3/store.log
> 
> # OPTIONS FOR TROUBLESHOOTING
> # ----------------------------------------------------------------------------
>   cache_log /var/log/squid3/cache.log
>   coredump_dir /var/log/squid3
> 
> # OPTIONS FOR TUNING THE CACHE
> # ----------------------------------------------------------------------------
>   cache allow localnet
>   cache allow localhost
>   cache allow intranet
>   cache deny  all

I think that does not do what you think.

The only traffic allowed to use this proxy (by http_access) is localnet
or localhost traffic.

Only traffic fetched by localnet or localhost cane be cached.
THerefore, all traffic allowed to go through this proxy can be cached.


You can simplify and speed up Squid a bit by removing the "cache ..."
ACLs entirely form your config. The default is "allow all"



>   refresh_pattern ^ftp:              1440    20%    10080
>   refresh_pattern ^gopher:           1440     0%     1440
>   refresh_pattern -i (/cgi-bin/|\?)     0     0%        0
>   refresh_pattern .                     0    20%     4320
> 
> # HTTP OPTIONS
> # ----------------------------------------------------------------------------
>   via off
> 
> # ADMINISTRATIVE PARAMETERS
> # ----------------------------------------------------------------------------
>   cache_effective_user proxy
>   cache_effective_group proxy
> 

The above should all be unnecessary for 99.99% of Squid instalalations.
In which case you would explicitly not have added the proxy user to
unnecessary system groups anyway.


> # ICP OPTIONS
> # ----------------------------------------------------------------------------
>   icp_port 0
> 
> # OPTIONS INFLUENCING REQUEST FORWARDING 
> # ----------------------------------------------------------------------------
>   nonhierarchical_direct on
>   prefer_direct off
>   always_direct allow to_localnet
>   always_direct allow to_localhost
>   always_direct allow to_intranet
>   never_direct  allow all
> 
> # MISCELLANEOUS
> # ----------------------------------------------------------------------------
>   memory_pools off
>   forwarded_for off

<http://www.squid-cache.org/Versions/v3/3.3/cfgman/forwarded_for.html>

Most Squid installations (and tuorials for very old Squid) using
"forwarded_for off" actually wanted to perform the action of "truncate".
Remainign few cases wanted "delete".

Ironicaly, revealing your clients LAN IPs is the *desirable* behaviour.
Since the purpose of XFF header is to allow sites like Wikipedia to
block individual malicious clients, without casting your entire network
into a traffic black hole. And client software with an interest in
privacy can alter the details it presents for the proxy to use in that
header - obfuscating their existance better than aggregating the proxy
traffic ready for trackers to use.

Amos



From eliezer at ngtech.co.il  Tue Jul 21 13:00:19 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 21 Jul 2015 16:00:19 +0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>
References: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>
Message-ID: <55AE4263.80409@ngtech.co.il>

On 21/07/2015 10:59, Jens Offenbach wrote:
> Is there someting wrong with my config? I have already used Squid 3.3.14. I get the same result. Unfortunately, I was not able to build Squid 3.5.5 and 3.5.6.

What was the issue?
I am using 3.5.6 on 14.04.2 64 bit.

Eliezer



From gortega at anses.gov.ar  Tue Jul 21 13:49:42 2015
From: gortega at anses.gov.ar (Ortega Gustavo Martin)
Date: Tue, 21 Jul 2015 13:49:42 +0000
Subject: [squid-users] squid 3.5.5 - assertion failed
In-Reply-To: <55AE2928.7090505@makeit-so.de>
References: <55AE17E2.40609@makeit-so.de>
 <1437476119842-4672354.post@n4.nabble.com> <55AE2928.7090505@makeit-so.de>
Message-ID: <20A2F27D26BDBF41B491D00A11D4A7A6015FBE9CC3@ANSESXGMBX04.anses.gov.ar>

Hi, i was having the same problem so we must downgrade to "Squid Cache: Version 3.4.13-20150709-r13225"

I hope someone can help us.

Regards.

Gustavo

-----Mensaje original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] En nombre de Arnaud Meyer
Enviado el: martes, 21 de julio de 2015 08:13 a.m.
Para: squid-users at lists.squid-cache.org
Asunto: Re: [squid-users] squid 3.5.5 - assertion failed

No, I'm not using this option.

You can see my complete squid.conf here: http://pastebin.com/mzFBDLpY

Am 21.07.2015 um 12:55 schrieb HackXBack:
> are you using range_offset_limit option ??
>
>
>
> --
> View this message in context: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-asserti
> on-failed-tp4672353p4672354.html Sent from the Squid - Users mailing 
> list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From skyvolt at zipmail.com.br  Tue Jul 21 14:03:56 2015
From: skyvolt at zipmail.com.br (Felipe Almeida)
Date: Tue, 21 Jul 2015 11:03:56 -0300
Subject: [squid-users] Block word in a URL part
In-Reply-To: ["55AD3270.7040309@treenet.co.nz"]
References: ["55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail",
 "55AD3270.7040309@treenet.co.nz"]
Message-ID: <55ae514c81b87_2dc9159249b253e0468aa@a4-winter13.mail>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150721/8f6ad473/attachment.htm>

From David.J.Berkes at pjc.com  Tue Jul 21 15:08:48 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Tue, 21 Jul 2015 15:08:48 +0000
Subject: [squid-users] suppress sending authentication prompt
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>

I have MDM solution that sets the basic proxy values on all iphones.
The "problem" is that periodically it prompts the user for the proxy login.
Its actually already authenticated and just need to hit the "cancel" and continues to work correctly.
How can I disable and/or redirect the pop-ups that ask for authentication?
Below is my very basic configuration.


---my config.  Version 3.5.

auth_param basic program /opt/squid/libexec/basic_ncsa_auth /opt/squid/etc/squid_passwd
auth_param basic children 10
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off


acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users
http_access deny all
________________________________

Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150721/9066bf03/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 21 15:17:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jul 2015 03:17:07 +1200
Subject: [squid-users] Block word in a URL part
In-Reply-To: <55ae514c81b87_2dc9159249b253e0468aa@a4-winter13.mail>
References: ["55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail"
 "55AD3270.7040309@treenet.co.nz"]
 <55ae514c81b87_2dc9159249b253e0468aa@a4-winter13.mail>
Message-ID: <55AE6272.3050509@treenet.co.nz>

On 22/07/2015 2:03 a.m., Felipe Almeida wrote:
> Amos, thank you. So as far as I understood, there is no way to use squid to 
> block that, right?
> What could be a good solution? Block the whole URL?
> Thank you once again.
> Felipe
> 

RFC 3986 section 3.5:
"
   Fragment identifiers have a special role in information retrieval
   systems as the primary form of client-side indirect referencing,

   ... the fragment identifier is not used in the scheme-specific
   processing of a URI; ...

   ... the fragment itself is dereferenced solely by the user agent,

   ... it also serves to prevent information
   providers from denying reference authors the right to refer to
   information within a resource selectively.
"

The URL #fragment in browser memory is effectively and semantically the
same as an idea in a users head.

Amos



From squid3 at treenet.co.nz  Tue Jul 21 15:30:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jul 2015 03:30:41 +1200
Subject: [squid-users] suppress sending authentication prompt
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>
Message-ID: <55AE65A1.2090603@treenet.co.nz>

On 22/07/2015 3:08 a.m., Berkes, David wrote:
> I have MDM solution that sets the basic proxy values on all iphones.
> The "problem" is that periodically it prompts the user for the proxy login.
> Its actually already authenticated and just need to hit the "cancel" and continues to work correctly.
> How can I disable and/or redirect the pop-ups that ask for authentication?

The popup is a browser Internal operation. It means the browser does not
know what credentials to send to the proxy.

If you have explicitly configured the browser with credentials. Then
something has gone wrong and its not able to find its own settings, or
been told those are unusable now.

Amos



From David.J.Berkes at pjc.com  Tue Jul 21 15:36:41 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Tue, 21 Jul 2015 15:36:41 +0000
Subject: [squid-users] suppress sending authentication prompt
In-Reply-To: <55AE65A1.2090603@treenet.co.nz>
References: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>
 <55AE65A1.2090603@treenet.co.nz>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53DDE3@ESCML200.corp.pjc.com>

Thank you.
From the tcpdump, I see the iphone sending requests to the proxy.  Sometimes with credentials and sometimes not.  How can I tell squid to not send 407 in response to the header with no credentials?  I have tried the following variations with no luck.

#reply_header_access WWW-Authenticate deny all

#acl ncsa_users proxy_auth REQUIRED
#http_access deny ncsa_users all


#acl ncsa_users proxy_auth REQUIRED
#acl authFailed src all
#http_access deny !ncsa_users authFailed
#deny_info http://nothing.com authFailed

#acl ncsa_users proxy_auth REQUIRED
#http_access deny !ncsa_users all
#deny_info http://nothing.com ncsa_users
#http_access deny all



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Tuesday, July 21, 2015 10:31 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] suppress sending authentication prompt

On 22/07/2015 3:08 a.m., Berkes, David wrote:
> I have MDM solution that sets the basic proxy values on all iphones.
> The "problem" is that periodically it prompts the user for the proxy login.
> Its actually already authenticated and just need to hit the "cancel" and continues to work correctly.
> How can I disable and/or redirect the pop-ups that ask for authentication?

The popup is a browser Internal operation. It means the browser does not know what credentials to send to the proxy.

If you have explicitly configured the browser with credentials. Then something has gone wrong and its not able to find its own settings, or been told those are unusable now.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From squid3 at treenet.co.nz  Tue Jul 21 15:37:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jul 2015 03:37:40 +1200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-003431c4-9ea5-4237-af49-8c8955b2962e-1437481860199@3capp-gmx-bs04>
References: <trinity-b1f67e0e-5175-4e9f-995a-49a82449c265-1437465596455@3capp-gmx-bs21>
 <55AE3025.4010206@treenet.co.nz>
 <trinity-003431c4-9ea5-4237-af49-8c8955b2962e-1437481860199@3capp-gmx-bs04>
Message-ID: <55AE6744.7010003@treenet.co.nz>

On 22/07/2015 12:31 a.m., Jens Offenbach wrote:
> Thank you very much for your detailed explainations. We want to use Squid in 
> order to accelerate our automated software setup processes via Puppet. Actually 
> Squid will host only a very short amount of large objects (10-20). Its purpose 
> is not to cache web traffic or little objects.

Ah, Squid does not "host", it caches. The difference may seem trivial at
first glance but it is the critical factor between whether a proxy or a
local web server is the best tool for the job.

>From my own experiences with Puppet, yes Squid is the right tool. But
only because the Puppet server was using relatively slow python code to
generate objects and not doing server-side caching on its own. If that
situation has changed in recent years then Squids usefulness will also
have changed.


> The hit-ratio for all the hosted 
> objects will be very high, because most of our VMs require the same software stack.
> I will update mit config regarding to your comments! Thanks a lot!
> But actually I have still no idea, why the download rates are so unsatisfying. 
> We are sill in the test phase. We have only one client that requests a large 
> object from Squid and the transfer rates are lower than 1 MB/sec during cache 
> build-up without any form of concurrency. Have vou got an idea what could be the 
> source of the problem here? Why causes the Squid process 100 % CPU usage.

I did not see any config causing the known 100% CPU bugs to be
encountered in your case (eg. HTTPS going through delay pools guarantees
100% CPU). Which leads me to think its probably related to memory
shuffling. (<http://bugs.squid-cache.org/show_bug.cgi?id=3189> appears
to be the same and still unidentified)

As for speed, if the CPU is maxed out by one particular action Squid
wont have time for much other work. So things go slow.

On the other hand Squid is also optimized for relatively high traffic
usage. For very small client counts (such as under-10) it is effectively
running in idle mode 99% of the time. The I/O event loop starts pausing
for 10ms blocks waiting to see if some more useful amount of work can be
done at the end of the wait. That can lead to apparent network slowdown
as TCP gets up to 10ms delay per packet. But that should not be visible
in CPU numbers.


That said, 1 client can still max out Squid CPU and/or NIC throughput
capacity on a single request if its pushing/pulling packets fast enough.


If you can attach the strace tool to Squid when its consuming the CPU
there might be some better hints about where to look.


Cheers
Amos



From skyvolt at zipmail.com.br  Tue Jul 21 16:05:20 2015
From: skyvolt at zipmail.com.br (Felipe Almeida)
Date: Tue, 21 Jul 2015 13:05:20 -0300
Subject: [squid-users] Block word in a URL part
In-Reply-To: ["55AE6272.3050509@treenet.co.nz"]
References: ["[\"55ad2ec9d00fa_73b31555d65ff3e8768d2@a4-winter15.mail\"
 \"55AD3270.7040309@treenet.co.nz\"]
 <55ae514c81b87_2dc9159249b253e0468aa@a4-winter13.mail>",
 "55AE6272.3050509@treenet.co.nz"]
Message-ID: <55ae6dc013650_1b77159249b253e046632@a4-winter13.mail>

<div>Amos, thank you. Good explanation.</div>
<div>?</div>
<div>?</div>
<div>?</div>
<div>Felipe</div>
<div><br /><br /></div>
<hr style="border-top: 1px solid #ccc;" />
<div><br /><strong>De:</strong> squid3 at treenet.co.nz<br /><strong>Enviada:</strong> Quarta-feira, 22 de Julho de 2015 03:17<br /><strong>Para:</strong> squid-users at lists.squid-cache.org<br /><strong>Assunto:</strong> [squid-users] Block word in a URL part<br /><br />On 22/07/2015 2:03 a.m., Felipe Almeida wrote:<br />&gt; Amos, thank you. So as far as I understood, there is no way to use squid to <br />&gt; block that, right?<br />&gt; What could be a good solution? Block the whole URL?<br />&gt; Thank you once again.<br />&gt; Felipe<br />&gt; <br /><br />RFC 3986 section 3.5:<br />"<br /> Fragment identifiers have a special role in information retrieval<br /> systems as the primary form of client-side indirect referencing,<br /><br /> ... the fragment identifier is not used in the scheme-specific<br /> processing of a URI; ...<br /><br /> ... the fragment itself is dereferenced solely by the user agent,<br /><br /> ... it also serves to prevent information<br /> providers from denying reference authors the right to refer to<br /> information within a resource selectively.<br />"<br /><br />The URL #fragment in browser memory is effectively and semantically the<br />same as an idea in a users head.<br /><br />Amos<br /><br />_______________________________________________<br />squid-users mailing list<br />squid-users at lists.squid-cache.org<br /><a href="http://lists.squid-cache.org/listinfo/squid-users" target="_blank">http://lists.squid-cache.org/listinfo/squid-users</a><br /><br /></div>

From hack.back at hotmail.com  Tue Jul 21 19:59:29 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 21 Jul 2015 12:59:29 -0700 (PDT)
Subject: [squid-users] SSL connction failed due to SNI after content
	redirection
In-Reply-To: <BAY181-W89D782282C2A247846C96083850@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
 <55AD47A2.90003@measurement-factory.com>
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>
Message-ID: <1437508769271-4672366.post@n4.nabble.com>

:~/squid-3.5.6-20150716-r13865# patch -p0 --verbose < sni.patch
Hmm...  Looks like a unified diff to me...
The text leading up to this was:
--------------------------
|--- src/ssl/PeerConnector.cc
|+++ src/ssl/PeerConnector.cc
--------------------------
Patching file src/ssl/PeerConnector.cc using Plan A...
patch: **** malformed patch at line 16:                  debugs(83, 5,
"SNIserve " << sniServer);





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-connction-failed-due-to-SNI-after-content-redirection-tp4672339p4672366.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alex_wu2012 at hotmail.com  Tue Jul 21 20:34:12 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Tue, 21 Jul 2015 13:34:12 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <1437508769271-4672366.post@n4.nabble.com>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>,
 <55AD47A2.90003@measurement-factory.com>,
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>,
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>,
 <1437508769271-4672366.post@n4.nabble.com>
Message-ID: <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>




The patch has been manually modified to meet code review.

Here is the patch without any manuall modification:

diff --git a/squid-3.5.6/src/ssl/PeerConnector.cc b/squid-3.5.6/src/ssl/PeerConnector.cc
index b4dfd8f..d307665 100644
--- a/squid-3.5.6/src/ssl/PeerConnector.cc
+++ b/squid-3.5.6/src/ssl/PeerConnector.cc
@@ -189,8 +189,13 @@ Ssl::PeerConnector::initializeSsl()

             // Use SNI TLS extension only when we connect directly
             // to the origin server and we know the server host name.
-            const char *sniServer = hostName ? hostName->c_str() :
-                                    (!request->GetHostIsNumeric() ? request->GetHost() : NULL);
+            const char *sniServer = NULL;
+            const bool redirected = request->flags.redirected && ::Config.onoff.redir_rewrites_host;
+            if (!hostName || redirected)
+                sniServer = request->GetHostIsNumeric() ? request->GetHost() : NULL;
+            else
+                sniServer = hostName->c_str();
+
             if (sniServer)
                 Ssl::setClientSNI(ssl, sniServer);
         }
~            

Alex


> Date: Tue, 21 Jul 2015 12:59:29 -0700
> From: hack.back at hotmail.com
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL connction failed due to SNI after content	redirection
> 
> :~/squid-3.5.6-20150716-r13865# patch -p0 --verbose < sni.patch
> Hmm...  Looks like a unified diff to me...
> The text leading up to this was:
> --------------------------
> |--- src/ssl/PeerConnector.cc
> |+++ src/ssl/PeerConnector.cc
> --------------------------
> Patching file src/ssl/PeerConnector.cc using Plan A...
> patch: **** malformed patch at line 16:                  debugs(83, 5,
> "SNIserve " << sniServer);
> 
> 
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-connction-failed-due-to-SNI-after-content-redirection-tp4672339p4672366.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150721/58890cc6/attachment.htm>

From hack.back at hotmail.com  Tue Jul 21 23:58:03 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 21 Jul 2015 16:58:03 -0700 (PDT)
Subject: [squid-users] SSL connction failed due to SNI after content
	redirection
In-Reply-To: <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
 <55AD47A2.90003@measurement-factory.com>
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>
 <1437508769271-4672366.post@n4.nabble.com>
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>
Message-ID: <1437523083670-4672368.post@n4.nabble.com>

~/squid-3.5.6-20150716-r13865# patch -p0 --verbose < sni.patch
Hmm...  Looks like a unified diff to me...
The text leading up to this was:
--------------------------
|diff --git src/ssl/PeerConnector.cc src/ssl/PeerConnector.cc
|index b4dfd8f..d307665 100644
|--- src/ssl/PeerConnector.cc
|+++ src/ssl/PeerConnector.cc
--------------------------
Patching file src/ssl/PeerConnector.cc using Plan A...
Hunk #1 succeeded at 189.
Hmm...  Ignoring the trailing garbage.
done




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-connction-failed-due-to-SNI-after-content-redirection-tp4672339p4672368.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Jul 22 00:27:43 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 21 Jul 2015 17:27:43 -0700 (PDT)
Subject: [squid-users] SSL connction failed due to SNI after content
	redirection
In-Reply-To: <1437523083670-4672368.post@n4.nabble.com>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
 <55AD47A2.90003@measurement-factory.com>
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>
 <1437508769271-4672366.post@n4.nabble.com>
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>
 <1437523083670-4672368.post@n4.nabble.com>
Message-ID: <1437524863407-4672369.post@n4.nabble.com>

i have some thing like this issue
ssl connection failed when using in mobile apps
your patch dont solve the problem
how i can tune what cause this problem ?
thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-connction-failed-due-to-SNI-after-content-redirection-tp4672339p4672369.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alex_wu2012 at hotmail.com  Wed Jul 22 00:44:13 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Tue, 21 Jul 2015 17:44:13 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <1437524863407-4672369.post@n4.nabble.com>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>,
 <55AD47A2.90003@measurement-factory.com>,
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>,
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>,
 <1437508769271-4672366.post@n4.nabble.com>,
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>,
 <1437523083670-4672368.post@n4.nabble.com>,
 <1437524863407-4672369.post@n4.nabble.com>
Message-ID: <BAY181-W4089B09CA79A17E696C43183830@phx.gbl>

it depends on how you set up squid, and where the connection is broken. The patch addessed the issue that occured using sslbump and content redirect together.

Alex

> Date: Tue, 21 Jul 2015 17:27:43 -0700
> From: hack.back at hotmail.com
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL connction failed due to SNI after content	redirection
> 
> i have some thing like this issue
> ssl connection failed when using in mobile apps
> your patch dont solve the problem
> how i can tune what cause this problem ?
> thanks.
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-connction-failed-due-to-SNI-after-content-redirection-tp4672339p4672369.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150721/832dd781/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 22 11:54:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jul 2015 23:54:51 +1200
Subject: [squid-users] suppress sending authentication prompt
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E53DDE3@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>
 <55AE65A1.2090603@treenet.co.nz>
 <916606669CFF224AB6997E9DB783F6EA7E53DDE3@ESCML200.corp.pjc.com>
Message-ID: <55AF848B.4010903@treenet.co.nz>

On 22/07/2015 3:36 a.m., Berkes, David wrote:
> Thank you.
> From the tcpdump, I see the iphone sending requests to the proxy.  Sometimes with credentials and sometimes not.  How can I tell squid to not send 407 in response to the header with no credentials?  I have tried the following variations with no luck.
> 

Think about that for a minute.

If Squid is never allowed to *ask* for credentials. How will it get them?

Do you really want the browser actively broadcasting usernames and
passwords in trivially decrypted format out into the network regardless
of where its connecting to?

You can block Squid actively requesting credentials by adding " all" to
the end of the http_access line(s) that would otherwise end with
ncsa_users ACL check. However, that will only cause the browser to
display an error page. Access Denied, end of transaction, full stop,
dont try again.



Remember that the popup is *not* part of HTTP messaging nor the HTTP
level authentication. It is purely a browser internal mechanism for
locating credentials.

407 is a perfectly normal HTTP operation. A working browser would always
answer Squid 407 queries by sending the MDM configured cerdentials, with
*zero* user involvement.

I suspect that perhapse your MDM system is tying the credentials to an
IPv4 address, and the iPhone using IPv6 on some traffic?
 Or maybe the browser really is braindead and forgetting how to lookup
the credentials.

Amos



From squid3 at treenet.co.nz  Wed Jul 22 12:21:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Jul 2015 00:21:31 +1200
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <BAY181-W4089B09CA79A17E696C43183830@phx.gbl>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>
 <55AD47A2.90003@measurement-factory.com>
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>
 <1437508769271-4672366.post@n4.nabble.com>
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>
 <1437523083670-4672368.post@n4.nabble.com>
 <1437524863407-4672369.post@n4.nabble.com>
 <BAY181-W4089B09CA79A17E696C43183830@phx.gbl>
Message-ID: <55AF8ACB.5040800@treenet.co.nz>

On 22/07/2015 12:44 p.m., Alex Wu wrote:
> it depends on how you set up squid, and where the connection is broken. The patch addessed the issue that occured using sslbump and content redirect together.
> 

I'd like some clarification what the exact problem symptoms are please.

AFAIK, both redirect and re-write actions happen a relatively long time
*after* the bumping TLS handshakes to server are completed. Its far too
late to send the pre-handshake SNI data to the server.

I can see this change as affecting reverse-proxy / CDN configurations
with TLS on both connections. But you said this was SSL-bumping, and
reverse-proxy configurations already have a cache_peer option to set the
internal domain name without re-write/redirect.

Amos



From wolle5050 at gmx.de  Wed Jul 22 13:24:04 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Wed, 22 Jul 2015 15:24:04 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
Message-ID: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150722/d9569cfe/attachment.htm>

From alex_wu2012 at hotmail.com  Wed Jul 22 14:41:08 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Wed, 22 Jul 2015 07:41:08 -0700
Subject: [squid-users] SSL connction failed due to SNI after content
 redirection
In-Reply-To: <55AF8ACB.5040800@treenet.co.nz>
References: <BAY181-W6434747B45B6AE8CEDD8FA83850@phx.gbl>,
 <55AD47A2.90003@measurement-factory.com>,
 <BAY181-W14AD7F32B7C363D238AD683850@phx.gbl>,
 <BAY181-W89D782282C2A247846C96083850@phx.gbl>,
 <1437508769271-4672366.post@n4.nabble.com>,
 <BAY181-W3C7AF60D7DEE662E1EC0D83840@phx.gbl>,
 <1437523083670-4672368.post@n4.nabble.com>,
 <1437524863407-4672369.post@n4.nabble.com>,
 <BAY181-W4089B09CA79A17E696C43183830@phx.gbl>,
 <55AF8ACB.5040800@treenet.co.nz>
Message-ID: <BAY181-W244B935D970106C8E5264F83830@phx.gbl>

We do not use cache-peer. I thought cache-peer is for connecting another squid-like proxy server.

Without ssl-bump, the connection is tunneled transparently, so there is no chance to redirect each HTTP requests proxied under SSL connection.

We want to redirec each HTTP requests under SSL, so we have to use ssl-bump to terminate the connection, and squid open another connection to the server we use content redirect to specify. The code take effect when the squid opens the new connection to each designed server for each HTTPS requests.

We terminated CONNECT call at squid also to ensure we can intercept HTTP requests at squid,

Alex
> To: squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Thu, 23 Jul 2015 00:21:31 +1200
> Subject: Re: [squid-users] SSL connction failed due to SNI after content redirection
> 
> On 22/07/2015 12:44 p.m., Alex Wu wrote:
> > it depends on how you set up squid, and where the connection is broken. The patch addessed the issue that occured using sslbump and content redirect together.
> > 
> 
> I'd like some clarification what the exact problem symptoms are please.
> 
> AFAIK, both redirect and re-write actions happen a relatively long time
> *after* the bumping TLS handshakes to server are completed. Its far too
> late to send the pre-handshake SNI data to the server.
> 
> I can see this change as affecting reverse-proxy / CDN configurations
> with TLS on both connections. But you said this was SSL-bumping, and
> reverse-proxy configurations already have a cache_peer option to set the
> internal domain name without re-write/redirect.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150722/6f7cf3d6/attachment.htm>

From David.J.Berkes at pjc.com  Wed Jul 22 15:10:31 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Wed, 22 Jul 2015 15:10:31 +0000
Subject: [squid-users] suppress sending authentication prompt
In-Reply-To: <55AF848B.4010903@treenet.co.nz>
References: <916606669CFF224AB6997E9DB783F6EA7E53DDC0@ESCML200.corp.pjc.com>
 <55AE65A1.2090603@treenet.co.nz>
 <916606669CFF224AB6997E9DB783F6EA7E53DDE3@ESCML200.corp.pjc.com>
 <55AF848B.4010903@treenet.co.nz>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53E0C9@ESCML200.corp.pjc.com>

Thank you very much for your help.  Yes, I agree it's not the approach I would like to take.  I believe it may be something to do with the MDM and/or the IOS.  I'm setting up a tcpdump to look at the packets.  What I see is the authentication "pop-up" occurs on the iphone, but the credentials have already authenticated.  So, the users hit the cancel button and traffic is allowed to proxy.  Below is output of the access log.  I do notice that the TCP_DENIED messages, which I don?t understand.  Maybe this is part of the issue?

---access.log
1437577600.112   1612 70.197.232.249 TCP_TUNNEL/200 1728 CONNECT myproxyserver.com:443 myproxyuser HIER_DIRECT/206.15.205.62 -
1437577600.120   2089 70.197.232.249 TCP_TUNNEL/200 1728 CONNECT myproxyserver.com:443 myproxyuser HIER_DIRECT/206.15.205.62 -
1437577601.253   2161 70.197.232.249 TCP_TUNNEL/200 5677 CONNECT myproxyserver.com:443 myproxyuser HIER_DIRECT/206.15.205.62 -
1437577601.362      0 70.197.232.249 TCP_DENIED/407 4074 CONNECT myproxyserver.com:443 - HIER_NONE/- text/html

Here is my configuration.  Can you tell me specifically where to place the "all" and/or oder to properly test and block Squid actively requesting credentials?

##############################################
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic children 20
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 8 hours
auth_param basic casesensitive on

acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users
http_access deny all

http_port 3128
##############################################

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Wednesday, July 22, 2015 6:55 AM
To: Berkes, David; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] suppress sending authentication prompt

On 22/07/2015 3:36 a.m., Berkes, David wrote:
> Thank you.
> From the tcpdump, I see the iphone sending requests to the proxy.  Sometimes with credentials and sometimes not.  How can I tell squid to not send 407 in response to the header with no credentials?  I have tried the following variations with no luck.
>

Think about that for a minute.

If Squid is never allowed to *ask* for credentials. How will it get them?

Do you really want the browser actively broadcasting usernames and passwords in trivially decrypted format out into the network regardless of where its connecting to?

You can block Squid actively requesting credentials by adding " all" to the end of the http_access line(s) that would otherwise end with ncsa_users ACL check. However, that will only cause the browser to display an error page. Access Denied, end of transaction, full stop, dont try again.



Remember that the popup is *not* part of HTTP messaging nor the HTTP level authentication. It is purely a browser internal mechanism for locating credentials.

407 is a perfectly normal HTTP operation. A working browser would always answer Squid 407 queries by sending the MDM configured cerdentials, with
*zero* user involvement.

I suspect that perhapse your MDM system is tying the credentials to an
IPv4 address, and the iPhone using IPv6 on some traffic?
 Or maybe the browser really is braindead and forgetting how to lookup the credentials.

Amos

________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From eliezer at ngtech.co.il  Wed Jul 22 16:28:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 22 Jul 2015 19:28:46 +0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
Message-ID: <55AFC4BE.2040404@ngtech.co.il>

Can you share the relevant squid.conf settings? Just to reproduce..

I have a dedicated testing server here which I can test the issue on.
8GB archive which might be an ISO and can be cached on AUFS\UFS and 
LARGE ROCK cache types.

I am pretty sure that the maximum cache object size is one thing to 
change and waht more?

 From What I understand it should not be different for 2GB cached 
archive and to 8 GB cached archive.
I have a local copy of centos 7 ISO which should be a test worthy object.
Anything more you can add to the test subject?

Eliezer

On 22/07/2015 16:24, Jens Offenbach wrote:
> I checked the bug you have mentioned and I think I am confronted with the same
> issue. I was able to build and test Squid 3.5.6 on Ubuntu 14.04.2 x84_64. I
> observed the same behavior. I have tested an 8 GB archive file and I get 100 %
> CPU usage and a download rate of nearly 500 KB/sec when the object gets cached.
> I have attached strace to the running process, but I killed it after 30 minutes.
> The whole takes hours, although we have a 1-GBit ethernet:
>
> Process 4091 attached
> Process 4091 detached
> % time seconds usecs/call calls errors syscall
> ------ ----------- ----------- --------- --------- ----------------
> 78.83 2.622879 1 1823951 write
> 12.29 0.408748 2 228029 2 read
> 6.18 0.205663 0 912431 1 epoll_wait
> 2.58 0.085921 0 456020 epoll_ctl
> 0.09 0.002919 0 6168 brk
> 0.02 0.000623 2 356 openat
> 0.01 0.000286 0 712 getdents
> 0.00 0.000071 1 91 getrusage
> 0.00 0.000038 0 362 close
> 0.00 0.000003 2 2 sendto
> 0.00 0.000001 0 3 1 recvfrom
> 0.00 0.000000 0 2 open
> 0.00 0.000000 0 3 stat
> 0.00 0.000000 0 1 1 rt_sigreturn
> 0.00 0.000000 0 1 kill
> 0.00 0.000000 0 4 fcntl
> 0.00 0.000000 0 2 2 unlink
> 0.00 0.000000 0 1 getppid
> ------ ----------- ----------- --------- --------- ----------------
> 100.00 3.327152 3428139 7 total
>
> Can I do anything that helps to get ride of this problem?
>
>
> Gesendet: Dienstag, 21. Juli 2015 um 17:37 Uhr
> Von: "Amos Jeffries" <squid3 at treenet.co.nz>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, "squid-users at lists.squid-cache.org"
> <squid-users at lists.squid-cache.org>
> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
> On 22/07/2015 12:31 a.m., Jens Offenbach wrote:
>   > Thank you very much for your detailed explainations. We want to use Squid in
>   > order to accelerate our automated software setup processes via Puppet. Actually
>   > Squid will host only a very short amount of large objects (10-20). Its purpose
>   > is not to cache web traffic or little objects.
>
> Ah, Squid does not "host", it caches. The difference may seem trivial at
> first glance but it is the critical factor between whether a proxy or a
> local web server is the best tool for the job.
>
>   From my own experiences with Puppet, yes Squid is the right tool. But
> only because the Puppet server was using relatively slow python code to
> generate objects and not doing server-side caching on its own. If that
> situation has changed in recent years then Squids usefulness will also
> have changed.
>
>
>   > The hit-ratio for all the hosted
>   > objects will be very high, because most of our VMs require the same software
> stack.
>   > I will update mit config regarding to your comments! Thanks a lot!
>   > But actually I have still no idea, why the download rates are so unsatisfying.
>   > We are sill in the test phase. We have only one client that requests a large
>   > object from Squid and the transfer rates are lower than 1 MB/sec during cache
>   > build-up without any form of concurrency. Have vou got an idea what could be the
>   > source of the problem here? Why causes the Squid process 100 % CPU usage.
>
> I did not see any config causing the known 100% CPU bugs to be
> encountered in your case (eg. HTTPS going through delay pools guarantees
> 100% CPU). Which leads me to think its probably related to memory
> shuffling. (<http://bugs.squid-cache.org/show_bug.cgi?id=3189
> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fbugs.squid-cache.org%2Fshow_bug.cgi%3Fid%3D3189>>
> appears
> to be the same and still unidentified)
>
> As for speed, if the CPU is maxed out by one particular action Squid
> wont have time for much other work. So things go slow.
>
> On the other hand Squid is also optimized for relatively high traffic
> usage. For very small client counts (such as under-10) it is effectively
> running in idle mode 99% of the time. The I/O event loop starts pausing
> for 10ms blocks waiting to see if some more useful amount of work can be
> done at the end of the wait. That can lead to apparent network slowdown
> as TCP gets up to 10ms delay per packet. But that should not be visible
> in CPU numbers.
>
>
> That said, 1 client can still max out Squid CPU and/or NIC throughput
> capacity on a single request if its pushing/pulling packets fast enough.
>
>
> If you can attach the strace tool to Squid when its consuming the CPU
> there might be some better hints about where to look.
>
>
> Cheers
> Amos
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From wolle5050 at gmx.de  Wed Jul 22 18:07:15 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Wed, 22 Jul 2015 20:07:15 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55AFC4BE.2040404@ngtech.co.il>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>, 
 <55AFC4BE.2040404@ngtech.co.il>
Message-ID: <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>

I will send you my current settings tomorrow. I have used AUFS as caching format, but I have also tested UFS. The format seems to have no influence on the issue.

I have tested the 1 GB Ubuntu 15.04 image (ubuntu-15.04-desktop-amd64.iso). This is the link http://releases.ubuntu.com/15.04/ubuntu-15.04-desktop-amd64.iso.

If you want to stress caching more with large files. You can use one of those:
https://surfer.nmr.mgh.harvard.edu/fswiki/Download

But I think the Centos 7 ISO are large enough, In my test scenario, I have put all files on an internal web server with gives them in stable 120 MB/sec. So the problem does not come from a slow network connection. I have checked network connectivity with Iperf3 (>= 900 MBit/sec) and made a direct wget without Squid. The file gets downloaded in high speed. Adding Squid in the communication flow which caches the file on the first request and the issue occurrs. After some minutes, the download rate drops to 500 KByte/sec and stays on this level together with 100 % CPU load. The download rate corresponds with the disk IO. The file gets written with 500 KByte/sec.

Thank you very much!?
?

Gesendet:?Mittwoch, 22. Juli 2015 um 18:28 Uhr
Von:?"Eliezer Croitoru" <eliezer at ngtech.co.il>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
Can you share the relevant squid.conf settings? Just to reproduce..

I have a dedicated testing server here which I can test the issue on.
8GB archive which might be an ISO and can be cached on AUFS\UFS and
LARGE ROCK cache types.

I am pretty sure that the maximum cache object size is one thing to
change and waht more?

>From What I understand it should not be different for 2GB cached
archive and to 8 GB cached archive.
I have a local copy of centos 7 ISO which should be a test worthy object.
Anything more you can add to the test subject?

Eliezer

On 22/07/2015 16:24, Jens Offenbach wrote:
> I checked the bug you have mentioned and I think I am confronted with the same
> issue. I was able to build and test Squid 3.5.6 on Ubuntu 14.04.2 x84_64. I
> observed the same behavior. I have tested an 8 GB archive file and I get 100 %
> CPU usage and a download rate of nearly 500 KB/sec when the object gets cached.
> I have attached strace to the running process, but I killed it after 30 minutes.
> The whole takes hours, although we have a 1-GBit ethernet:
>
> Process 4091 attached
> Process 4091 detached
> % time seconds usecs/call calls errors syscall
> ------ ----------- ----------- --------- --------- ----------------
> 78.83 2.622879 1 1823951 write
> 12.29 0.408748 2 228029 2 read
> 6.18 0.205663 0 912431 1 epoll_wait
> 2.58 0.085921 0 456020 epoll_ctl
> 0.09 0.002919 0 6168 brk
> 0.02 0.000623 2 356 openat
> 0.01 0.000286 0 712 getdents
> 0.00 0.000071 1 91 getrusage
> 0.00 0.000038 0 362 close
> 0.00 0.000003 2 2 sendto
> 0.00 0.000001 0 3 1 recvfrom
> 0.00 0.000000 0 2 open
> 0.00 0.000000 0 3 stat
> 0.00 0.000000 0 1 1 rt_sigreturn
> 0.00 0.000000 0 1 kill
> 0.00 0.000000 0 4 fcntl
> 0.00 0.000000 0 2 2 unlink
> 0.00 0.000000 0 1 getppid
> ------ ----------- ----------- --------- --------- ----------------
> 100.00 3.327152 3428139 7 total
>
> Can I do anything that helps to get ride of this problem?
>
>
> Gesendet: Dienstag, 21. Juli 2015 um 17:37 Uhr
> Von: "Amos Jeffries" <squid3 at treenet.co.nz>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, "squid-users at lists.squid-cache.org"
> <squid-users at lists.squid-cache.org>
> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
> On 22/07/2015 12:31 a.m., Jens Offenbach wrote:
> > Thank you very much for your detailed explainations. We want to use Squid in
> > order to accelerate our automated software setup processes via Puppet. Actually
> > Squid will host only a very short amount of large objects (10-20). Its purpose
> > is not to cache web traffic or little objects.
>
> Ah, Squid does not "host", it caches. The difference may seem trivial at
> first glance but it is the critical factor between whether a proxy or a
> local web server is the best tool for the job.
>
> From my own experiences with Puppet, yes Squid is the right tool. But
> only because the Puppet server was using relatively slow python code to
> generate objects and not doing server-side caching on its own. If that
> situation has changed in recent years then Squids usefulness will also
> have changed.
>
>
> > The hit-ratio for all the hosted
> > objects will be very high, because most of our VMs require the same software
> stack.
> > I will update mit config regarding to your comments! Thanks a lot!
> > But actually I have still no idea, why the download rates are so unsatisfying.
> > We are sill in the test phase. We have only one client that requests a large
> > object from Squid and the transfer rates are lower than 1 MB/sec during cache
> > build-up without any form of concurrency. Have vou got an idea what could be the
> > source of the problem here? Why causes the Squid process 100 % CPU usage.
>
> I did not see any config causing the known 100% CPU bugs to be
> encountered in your case (eg. HTTPS going through delay pools guarantees
> 100% CPU). Which leads me to think its probably related to memory
> shuffling. (<http://bugs.squid-cache.org/show_bug.cgi?id=3189
> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fbugs.squid-cache.org%2Fshow_bug.cgi%3Fid%3D3189[https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fbugs.squid-cache.org%2Fshow_bug.cgi%3Fid%3D3189]>>
> appears
> to be the same and still unidentified)
>
> As for speed, if the CPU is maxed out by one particular action Squid
> wont have time for much other work. So things go slow.
>
> On the other hand Squid is also optimized for relatively high traffic
> usage. For very small client counts (such as under-10) it is effectively
> running in idle mode 99% of the time. The I/O event loop starts pausing
> for 10ms blocks waiting to see if some more useful amount of work can be
> done at the end of the wait. That can lead to apparent network slowdown
> as TCP gets up to 10ms delay per packet. But that should not be visible
> in CPU numbers.
>
>
> That said, 1 client can still max out Squid CPU and/or NIC throughput
> capacity on a single request if its pushing/pulling packets fast enough.
>
>
> If you can attach the strace tool to Squid when its consuming the CPU
> there might be some better hints about where to look.
>
>
> Cheers
> Amos
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
>


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]


From eliezer at ngtech.co.il  Wed Jul 22 18:59:32 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 22 Jul 2015 21:59:32 +0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
Message-ID: <55AFE814.3020504@ngtech.co.il>

Hey Jens,

I have tested the issue with LARGE ROCK and not AUFS or UFS.
Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
Squid is sitting on an intel atom with SSD drive and on a HIT case the 
download speed is more then doubled to 4.5 MBps(36Mbps).
I have not tried it with AUFS yet.

My testing machine is an ARCH linux with self compiled squid with the 
replacement of diskd to rock from the compilation options of arch linux.

You can take a look at the HIT log at:
http://paste.ngtech.co.il/pnhkglgsu

Eliezer

On 22/07/2015 21:07, Jens Offenbach wrote:
> I will send you my current settings tomorrow. I have used AUFS as caching format, but I have also tested UFS. The format seems to have no influence on the issue.
>
> I have tested the 1 GB Ubuntu 15.04 image (ubuntu-15.04-desktop-amd64.iso). This is the link http://releases.ubuntu.com/15.04/ubuntu-15.04-desktop-amd64.iso.
>
> If you want to stress caching more with large files. You can use one of those:
> https://surfer.nmr.mgh.harvard.edu/fswiki/Download
>
> But I think the Centos 7 ISO are large enough, In my test scenario, I have put all files on an internal web server with gives them in stable 120 MB/sec. So the problem does not come from a slow network connection. I have checked network connectivity with Iperf3 (>= 900 MBit/sec) and made a direct wget without Squid. The file gets downloaded in high speed. Adding Squid in the communication flow which caches the file on the first request and the issue occurrs. After some minutes, the download rate drops to 500 KByte/sec and stays on this level together with 100 % CPU load. The download rate corresponds with the disk IO. The file gets written with 500 KByte/sec.
>
> Thank you very much!
>
>
> Gesendet: Mittwoch, 22. Juli 2015 um 18:28 Uhr
> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> An: squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> Can you share the relevant squid.conf settings? Just to reproduce..
>
> I have a dedicated testing server here which I can test the issue on.
> 8GB archive which might be an ISO and can be cached on AUFS\UFS and
> LARGE ROCK cache types.
>
> I am pretty sure that the maximum cache object size is one thing to
> change and waht more?
>
>  From What I understand it should not be different for 2GB cached
> archive and to 8 GB cached archive.
> I have a local copy of centos 7 ISO which should be a test worthy object.
> Anything more you can add to the test subject?
>
> Eliezer
>
> On 22/07/2015 16:24, Jens Offenbach wrote:
>> I checked the bug you have mentioned and I think I am confronted with the same
>> issue. I was able to build and test Squid 3.5.6 on Ubuntu 14.04.2 x84_64. I
>> observed the same behavior. I have tested an 8 GB archive file and I get 100 %
>> CPU usage and a download rate of nearly 500 KB/sec when the object gets cached.
>> I have attached strace to the running process, but I killed it after 30 minutes.
>> The whole takes hours, although we have a 1-GBit ethernet:
>>
>> Process 4091 attached
>> Process 4091 detached
>> % time seconds usecs/call calls errors syscall
>> ------ ----------- ----------- --------- --------- ----------------
>> 78.83 2.622879 1 1823951 write
>> 12.29 0.408748 2 228029 2 read
>> 6.18 0.205663 0 912431 1 epoll_wait
>> 2.58 0.085921 0 456020 epoll_ctl
>> 0.09 0.002919 0 6168 brk
>> 0.02 0.000623 2 356 openat
>> 0.01 0.000286 0 712 getdents
>> 0.00 0.000071 1 91 getrusage
>> 0.00 0.000038 0 362 close
>> 0.00 0.000003 2 2 sendto
>> 0.00 0.000001 0 3 1 recvfrom
>> 0.00 0.000000 0 2 open
>> 0.00 0.000000 0 3 stat
>> 0.00 0.000000 0 1 1 rt_sigreturn
>> 0.00 0.000000 0 1 kill
>> 0.00 0.000000 0 4 fcntl
>> 0.00 0.000000 0 2 2 unlink
>> 0.00 0.000000 0 1 getppid
>> ------ ----------- ----------- --------- --------- ----------------
>> 100.00 3.327152 3428139 7 total
>>
>> Can I do anything that helps to get ride of this problem?
>>
>>
>> Gesendet: Dienstag, 21. Juli 2015 um 17:37 Uhr
>> Von: "Amos Jeffries" <squid3 at treenet.co.nz>
>> An: "Jens Offenbach" <wolle5050 at gmx.de>, "squid-users at lists.squid-cache.org"
>> <squid-users at lists.squid-cache.org>
>> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
>> On 22/07/2015 12:31 a.m., Jens Offenbach wrote:
>>> Thank you very much for your detailed explainations. We want to use Squid in
>>> order to accelerate our automated software setup processes via Puppet. Actually
>>> Squid will host only a very short amount of large objects (10-20). Its purpose
>>> is not to cache web traffic or little objects.
>>
>> Ah, Squid does not "host", it caches. The difference may seem trivial at
>> first glance but it is the critical factor between whether a proxy or a
>> local web server is the best tool for the job.
>>
>>  From my own experiences with Puppet, yes Squid is the right tool. But
>> only because the Puppet server was using relatively slow python code to
>> generate objects and not doing server-side caching on its own. If that
>> situation has changed in recent years then Squids usefulness will also
>> have changed.
>>
>>
>>> The hit-ratio for all the hosted
>>> objects will be very high, because most of our VMs require the same software
>> stack.
>>> I will update mit config regarding to your comments! Thanks a lot!
>>> But actually I have still no idea, why the download rates are so unsatisfying.
>>> We are sill in the test phase. We have only one client that requests a large
>>> object from Squid and the transfer rates are lower than 1 MB/sec during cache
>>> build-up without any form of concurrency. Have vou got an idea what could be the
>>> source of the problem here? Why causes the Squid process 100 % CPU usage.
>>
>> I did not see any config causing the known 100% CPU bugs to be
>> encountered in your case (eg. HTTPS going through delay pools guarantees
>> 100% CPU). Which leads me to think its probably related to memory
>> shuffling. (<http://bugs.squid-cache.org/show_bug.cgi?id=3189
>> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fbugs.squid-cache.org%2Fshow_bug.cgi%3Fid%3D3189[https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fbugs.squid-cache.org%2Fshow_bug.cgi%3Fid%3D3189]>>
>> appears
>> to be the same and still unidentified)
>>
>> As for speed, if the CPU is maxed out by one particular action Squid
>> wont have time for much other work. So things go slow.
>>
>> On the other hand Squid is also optimized for relatively high traffic
>> usage. For very small client counts (such as under-10) it is effectively
>> running in idle mode 99% of the time. The I/O event loop starts pausing
>> for 10ms blocks waiting to see if some more useful amount of work can be
>> done at the end of the wait. That can lead to apparent network slowdown
>> as TCP gets up to 10ms delay per packet. But that should not be visible
>> in CPU numbers.
>>
>>
>> That said, 1 client can still max out Squid CPU and/or NIC throughput
>> capacity on a single request if its pushing/pulling packets fast enough.
>>
>>
>> If you can attach the strace tool to Squid when its consuming the CPU
>> there might be some better hints about where to look.
>>
>>
>> Cheers
>> Amos
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
>




From eliezer at ngtech.co.il  Wed Jul 22 19:47:16 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 22 Jul 2015 22:47:16 +0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55AFE814.3020504@ngtech.co.il>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>
Message-ID: <55AFF344.1030806@ngtech.co.il>

On 22/07/2015 21:59, Eliezer Croitoru wrote:
> Hey Jens,
>
> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> download speed is more then doubled to 4.5 MBps(36Mbps).
> I have not tried it with AUFS yet.


And I must admit that AUFS beats rock cache with speed.
I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs 
"cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT 
results more then doubles 3 the speed rock gave with default settings.

So about 15MBps which is 120Mbps.
I do not seem to feel what Jens feels but the 100% CPU might be because 
of spinning disk hangs while reading the file from disk.

Amos, I remember that there were some suggestions how to tune large rock.
Any hints?
I can test it and make it a suggestion for big files.

Eliezer



From wolle5050 at gmx.de  Thu Jul 23 06:18:48 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 23 Jul 2015 08:18:48 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55AFF344.1030806@ngtech.co.il>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
Message-ID: <trinity-5129c212-ce17-4826-9825-e17a171cebdf-1437632328442@3capp-gmx-bs71>

I have not tested rock yet, but I will give it a try and will report the results as soon as possible.

I have tested Squid 3.5.6 in three different virtualized environments (VMware vSphere, OpenStack Icehouse, OpenStack Kilo).

For clarification:
The 100 % CPU usage and the low download speed occurs only when there is NO cache hit and when the object is added to disk cache FOR THE FIRST TIME. When the object is in the cache, the download speed and the CPU usage of Squid (3.3.8 and 3.5.6) is optimal. I have tested it with an Ubuntu ISO image (>= 1 GB).

This is my current squid.config.

# ACCESS CONTROLS
# -----------------------------------------------------------------------------
  acl localnet    src 139.2.0.0/16
  acl localnet    src 193.96.112.0/21
  acl localnet    src 192.109.216.0/24
  acl localnet    src 100.1.4.0/22
  acl localnet    src 10.0.0.0/8
  acl localnet    src 172.16.0.0/12
  acl localnet    src 192.168.0.0/16
  http_access allow manager localhost
  http_access deny  manager
  http_access allow localnet
  http_access allow localhost
  http_access deny all

# NETWORK OPTIONS
# -----------------------------------------------------------------------------
  http_port 0.0.0.0:3128

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 128 MB
  memory_replacement_policy heap LFUDA
  cache_mem 4 GB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240

# LOGFILE OPTIONS
# -----------------------------------------------------------------------------
  access_log daemon:/var/log/squid3/access.log squid
  cache_store_log daemon:/var/log/squid3/store.log

# OPTIONS FOR TROUBLESHOOTING
# -----------------------------------------------------------------------------
  cache_log /var/log/squid3/cache.log
  coredump_dir /var/log/squid3
  
# OPTIONS FOR TUNING THE CACHE
# -----------------------------------------------------------------------------
  cache allow all
  
# OPTIONS INFLUENCING REQUEST FORWARDING 
# -----------------------------------------------------------------------------
  always_direct allow all

# MISCELLANEOUS
# -----------------------------------------------------------------------------
  memory_pools off


Thank you very much for your help! It is good to know that you will take care of my problem.

Regards,
Jens
?

Gesendet:?Mittwoch, 22. Juli 2015 um 21:47 Uhr
Von:?"Eliezer Croitoru" <eliezer at ngtech.co.il>
An:?squid-users at lists.squid-cache.org
Cc:?"Amos Jeffries" <squid3 at treenet.co.nz>
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
On 22/07/2015 21:59, Eliezer Croitoru wrote:
> Hey Jens,
>
> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> download speed is more then doubled to 4.5 MBps(36Mbps).
> I have not tried it with AUFS yet.


And I must admit that AUFS beats rock cache with speed.
I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
"cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
results more then doubles 3 the speed rock gave with default settings.

So about 15MBps which is 120Mbps.
I do not seem to feel what Jens feels but the 100% CPU might be because
of spinning disk hangs while reading the file from disk.

Amos, I remember that there were some suggestions how to tune large rock.
Any hints?
I can test it and make it a suggestion for big files.

Eliezer
?


From wolle5050 at gmx.de  Thu Jul 23 07:25:18 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 23 Jul 2015 09:25:18 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55AFF344.1030806@ngtech.co.il>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
Message-ID: <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>

A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
?
I cleared the cache:
rm -rf /var/cache/squid3/*
squid -z
squid
http_proxy=http://139.2.57.120:3128/ wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar

The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.

During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.

For further investigations I give you my build properties:
squid -v
Squid Cache: Version 3.5.6
Service Name: squid
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
?

Gesendet:?Mittwoch, 22. Juli 2015 um 21:47 Uhr
Von:?"Eliezer Croitoru" <eliezer at ngtech.co.il>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
On 22/07/2015 21:59, Eliezer Croitoru wrote:
> Hey Jens,
>
> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> download speed is more then doubled to 4.5 MBps(36Mbps).
> I have not tried it with AUFS yet.


And I must admit that AUFS beats rock cache with speed.
I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
"cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
results more then doubles 3 the speed rock gave with default settings.

So about 15MBps which is 120Mbps.
I do not seem to feel what Jens feels but the 100% CPU might be because
of spinning disk hangs while reading the file from disk.

Amos, I remember that there were some suggestions how to tune large rock.
Any hints?
I can test it and make it a suggestion for big files.

Eliezer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From jorgeley at gmail.com  Thu Jul 23 11:23:57 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 23 Jul 2015 08:23:57 -0300
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
Message-ID: <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>

> Hi guys.
> I have a RedHat 6.6 + squid 3.5.6 + basic_ncsa_auth + chroot and is
> crashing only when I do an authentication.
>
> Here is the main confs:
> auth_param basic program /libexec/basic_ncsa_auth /regras/usuarios
> auth_param basic children 10 startup=0 idle=1
> auth_param basic realm INTERNET-LOGIN NECESSARIO
> ... (other confs) ...
> acl usuarios            proxy_auth -i   "/etc/squid-3.5.6/regras/usuarios"
> ... (other confs) ...
> chroot /etc/squid-3.5.6
>
> Here is what I find in the cache.log:
> 2015/07/22 18:47:27.866 kid1| WARNING: no_suid: setuid(0): (1) Operation
> not permitted
> 2015/07/22 18:48:01.735 kid1| ipcCreate: /libexec/basic_ncsa_auth: (2) No
> such file or directory
> 2015/07/22 18:47:27.866 kid1| WARNING: basicauthenticator #Hlpr13818 exited
>
> What is the ipcCreate and why he is not findind the file?
>
About the libs needed when I do the chroot, I have to copy them to the
squid folder or I need to create the same structure like
/squid-3.5.6/libs,  /squid-3.5.6/lib64?

> Any ideas? Thanks since now.
> --
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/8082fbf0/attachment.htm>

From marcus.kool at urlfilterdb.com  Thu Jul 23 11:29:43 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 23 Jul 2015 08:29:43 -0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>
Message-ID: <55B0D027.4000501@urlfilterdb.com>

I am not sure if it is relevant, maybe it is:

I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
Squid sends the object to the ICAP server in small chunks of varying sizes:
4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
Note that the interval of receiving the chunks is 1/1000th of a second.
It seems that Squid forwards the object to the ICAP server every time it receives
one or a few TCP packets.

I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.

Amos, is there a debug setting that can confirm/reject this suspicion?

Marcus


On 07/23/2015 04:25 AM, Jens Offenbach wrote:
> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>
> I cleared the cache:
> rm -rf /var/cache/squid3/*
> squid -z
> squid
> http_proxy=http://139.2.57.120:3128/ wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>
> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>
> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>
> For further investigations I give you my build properties:
> squid -v
> Squid Cache: Version 3.5.6
> Service Name: squid
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
 s
i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>
>
> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> An: squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>> Hey Jens,
>>
>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>> download speed is more then doubled to 4.5 MBps(36Mbps).
>> I have not tried it with AUFS yet.
>
>
> And I must admit that AUFS beats rock cache with speed.
> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
> results more then doubles 3 the speed rock gave with default settings.
>
> So about 15MBps which is 120Mbps.
> I do not seem to feel what Jens feels but the 100% CPU might be because
> of spinning disk hangs while reading the file from disk.
>
> Amos, I remember that there were some suggestions how to tune large rock.
> Any hints?
> I can test it and make it a suggestion for big files.
>
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From wolle5050 at gmx.de  Thu Jul 23 13:39:27 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 23 Jul 2015 15:39:27 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B0D027.4000501@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
Message-ID: <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/88c46b30/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 23 14:42:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 02:42:40 +1200
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
Message-ID: <55B0FD60.9070705@treenet.co.nz>

On 23/07/2015 11:23 p.m., Jorgeley Junior wrote:
>> Hi guys.
>> I have a RedHat 6.6 + squid 3.5.6 + basic_ncsa_auth + chroot and is
>> crashing only when I do an authentication.
>>
>> Here is the main confs:
>> auth_param basic program /libexec/basic_ncsa_auth /regras/usuarios
>> auth_param basic children 10 startup=0 idle=1
>> auth_param basic realm INTERNET-LOGIN NECESSARIO
>> ... (other confs) ...
>> acl usuarios            proxy_auth -i   "/etc/squid-3.5.6/regras/usuarios"
>> ... (other confs) ...
>> chroot /etc/squid-3.5.6
>>
>> Here is what I find in the cache.log:
>> 2015/07/22 18:47:27.866 kid1| WARNING: no_suid: setuid(0): (1) Operation
>> not permitted
>> 2015/07/22 18:48:01.735 kid1| ipcCreate: /libexec/basic_ncsa_auth: (2) No
>> such file or directory
>> 2015/07/22 18:47:27.866 kid1| WARNING: basicauthenticator #Hlpr13818 exited
>>
>> What is the ipcCreate and why he is not findind the file?

It is the code that runs the helper.

The "/libexec/basic_ncsa_auth" does not exist as an exectuable binary
inside your chroot.


>>
> About the libs needed when I do the chroot, I have to copy them to the
> squid folder or I need to create the same structure like
> /squid-3.5.6/libs,  /squid-3.5.6/lib64?

They must match the OS layout where Squid (and everything else that will
run in the chroot) expects to find them.

Amos



From squid3 at treenet.co.nz  Thu Jul 23 14:53:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 02:53:07 +1200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B0D027.4000501@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il> <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>
 <55B0D027.4000501@urlfilterdb.com>
Message-ID: <55B0FFD3.7070400@treenet.co.nz>

On 23/07/2015 11:29 p.m., Marcus Kool wrote:
> I am not sure if it is relevant, maybe it is:
> 
> I am developing an ICAP daemon and after the ICAP server sends a "100
> continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it
> receives
> one or a few TCP packets.
> 
> I have a suspicion that in the scenario of 100% CPU, large #write calls
> and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e.
> every time one or a few TCP packets arrive.

If testing with low traffic (one connection) that guess is correct.
Squid is so fast that it cycles through the whole
allocate-memory->read()->process->write()-to-icap cycle in under a
millisecond.

It takes a few dozen busy clients in parallel to slow Slow down enough
to increase the chunk sizes.


> 
> Amos, is there a debug setting that can confirm/reject this suspicion?

Not that I recall at the moment.

Amos



From hack.back at hotmail.com  Thu Jul 23 15:18:18 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 23 Jul 2015 08:18:18 -0700 (PDT)
Subject: [squid-users] squid 3.5.6 and ecap
Message-ID: <1437664698592-4672387.post@n4.nabble.com>

after installing libecap and ecap_adapter
and compile squid with --enable-ecap
when i want to start squid i got this error
[....] Restarting Squid HTTP Proxy 3.X: squid/usr/sbin/squid: error while
loading shared libraries: libecap.so.3: cannot open shared object file: No
such file or directory
 failed!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-6-and-ecap-tp4672387.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From wolle5050 at gmx.de  Thu Jul 23 15:58:18 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 23 Jul 2015 17:58:18 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B0F941.2040003@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B0F941.2040003@urlfilterdb.com>
Message-ID: <trinity-7dbe03cf-f964-4328-a54c-ba34694ec662-1437667098027@3capp-gmx-bs71>

Ok. I am sorry,

Please, try this one:

http://wikisend.com/download/413650/squid.strace

Regards,
Jens
?

Gesendet:?Donnerstag, 23. Juli 2015 um 16:25 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>
Betreff:?Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
I get a 403 Access Denied error trying to download the trace file

Marcus


On 07/23/2015 10:39 AM, Jens Offenbach wrote:
> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
> Here is the download link:
> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace
> I hope it can help you.
> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
> I am not sure if it is relevant, maybe it is:
>
> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it receives
> one or a few TCP packets.
>
> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>
> Amos, is there a debug setting that can confirm/reject this suspicion?
>
> Marcus
>
>
> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
> > A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
> >
> > I cleared the cache:
> > rm -rf /var/cache/squid3/*
> > squid -z
> > squid
> > http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
> >
> > The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
> >
> > During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
> >
> > For further investigations I give you my build properties:
> > squid -v
> > Squid Cache: Version 3.5.6
> > Service Name: squid
> > configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
> s
> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
> >
> >
> > Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
> > Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> > An: squid-users at lists.squid-cache.org
> > Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> > On 22/07/2015 21:59, Eliezer Croitoru wrote:
> >> Hey Jens,
> >>
> >> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> >> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> >> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> >> download speed is more then doubled to 4.5 MBps(36Mbps).
> >> I have not tried it with AUFS yet.
> >
> >
> > And I must admit that AUFS beats rock cache with speed.
> > I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
> > "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
> > results more then doubles 3 the speed rock gave with default settings.
> >
> > So about 15MBps which is 120Mbps.
> > I do not seem to feel what Jens feels but the 100% CPU might be because
> > of spinning disk hangs while reading the file from disk.
> >
> > Amos, I remember that there were some suggestions how to tune large rock.
> > Any hints?
> > I can test it and make it a suggestion for big files.
> >
> > Eliezer
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
> >


From chip_pop at hotmail.com  Thu Jul 23 16:00:43 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 23 Jul 2015 09:00:43 -0700 (PDT)
Subject: [squid-users] squid youtube caching
Message-ID: <1437667243296-4672389.post@n4.nabble.com>

my English not grait so be pattion tks
hi i setup yt caching working perfect but i need to ask
first squid 3.5.6 
i need to know how is yt detect and send partial video
i have 2 computer same flash v. same firefox v. all identical exept one
windowsxp another is win7
i cache html5 on win7 yt send partial video on winxp send full video
i put none to
request_header_access Accept-Ranges deny all
reply_header_access Accept-Ranges deny all
request_header_replace Accept-Ranges none
reply_header_replace Accept-Ranges none

so Wat cause the partial video on win7 is it some header or ??

you thing deny Accept-Ranges not working  ?
or some other thing tks if any help



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ulises at vianetcon.com.ar  Thu Jul 23 16:02:35 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Thu, 23 Jul 2015 13:02:35 -0300
Subject: [squid-users] TCP_MISS in images
Message-ID: <55B1101B.8010408@vianetcon.com.ar>

Hello,

I have a basic squid 3.5 configuration with

maximum_object_size_in_memory 64 KB
maximum_object_size 100000 KB
minimum_object_size 512 bytes

refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 10080 
override-expire ignore-no-cache ignore-private
refresh_pattern -i (/cgi-bin/)                  0       0%      0
refresh_pattern .                                       0 20%     4320


cache_dir rock  /cache1/rock1 256  min-size=500 max-size=32767 
max-swap-rate=250 swap-timeout=350
cache_dir diskd /cache2/diskd1 1000 16 256 min-size=32768 max-size=1048576
cache_dir diskd /cache2/diskd2 100000 16 256 min-size=1048576


But when I test it against my webserver, using only one client PC, the 
only thing I get are TCP_MISSes of my images.

1437664284.339     11 192.168.2.103 TCP_MISS/200 132417 GET 
http://test-server.com/images/imagen3.jpg - HIER_DIRECT/192.168.2.10 
image/jpeg
1437664549.753      5 192.168.2.103 TCP_MISS/200 53933 GET 
http://test-server.com/images/imagen1.gif - HIER_DIRECT/192.168.2.10 
image/gif
1437665917.469     18 192.168.2.103 TCP_MISS/200 8319 GET 
http://test-server.com/images/icono.png - HIER_DIRECT/192.168.2.10 image/png

The response headers don't have Vary tags or any other that may impede 
caching

Accept-Ranges    bytes
Connection    close
Content-Length    53644
Content-Type    image/gif
Date    Thu, 23 Jul 2015 15:56:07 GMT
Etag    "e548d4-d18c-51b504b95dec0"
Last-Modified    Mon, 20 Jul 2015 15:36:03 GMT
Server    Apache/2.2.22 (EL)




Is it necessary a certain amount of requests of a single object to be 
cached (mem o disk) or am I facing some other problem here?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/0aaab52e/attachment.htm>

From yvoinov at gmail.com  Thu Jul 23 16:04:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 23 Jul 2015 22:04:31 +0600
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B1101B.8010408@vianetcon.com.ar>
References: <55B1101B.8010408@vianetcon.com.ar>
Message-ID: <55B1108F.1030007@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Heh. Looks like images is less than:

minimum_object_size 512 bytes

this parameter. :)



23.07.15 22:02, Ulises Nicolini ?????:
> minimum_object_size 512 bytes 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsRCPAAoJENNXIZxhPexG7f8H/iuEMqYKJhD2aC7CG620nwii
SNZwJJGw4HDMsYbRZfLuTow/wQ11np7z0+33Ucb0T4F1MMPmd73p5lY1jI37Opu3
gZc1USGwvWzRA6FcTVWUjtnHqsMhf9VBEU69EXpj+H71U2bDKox31qsMOTWbt056
Bx0xWmdpvfIRBVH+6fE+WgkLz2uACusSaztRdBo9EFhNFH6egJ8x5E+H3BtUiNmH
f+uItP6iFRPO7FrdNVra/uvCWFNq/wpOyloTyxDUgRsCSkhiZthlzBTw6bRsYf48
158nUtepNV2amiW2itKOYZi4FWrG8o08TMeEqbyxaCI/A3xSf0lO56ZoO0V6jO4=
=GNtZ
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Thu Jul 23 16:05:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 23 Jul 2015 22:05:41 +0600
Subject: [squid-users] squid 3.5.6 and ecap
In-Reply-To: <1437664698592-4672387.post@n4.nabble.com>
References: <1437664698592-4672387.post@n4.nabble.com>
Message-ID: <55B110D5.9000400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
 No such file or directory

means "No such file or directory" exactly. :)

Your squid can't find libecap.

Simple.


23.07.15 21:18, HackXBack ?????:
>  No
> such file or directory

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsRDVAAoJENNXIZxhPexGJikH/3cKfhv51jk918zWhbUDP4iG
9zb40W+3irVdo7lGVMu6G4CqOQzlcjlKcB2s2iS8hHFUkXbcJKk9Lo6TLd2UALK1
A75amrFljfFPYZauq1+mtM0f2AaDWgFfRLCKOydGiidyL6oI15+AkOH3AiDt4sCP
y+0guiwKtJyjFImao24ApEzgxbY6pU6EZEEOzoC/s3ciKuFmLm5C08qMOJPs0KLP
RiZkBXPkN2V4dLW5ZUox4PLaNyg0TgTgTwgV0n9i+8hDqt9+eYYCmzg/lNffCASs
Ezd4m4veayZPAp2gqK7BjOnI6ECXcN/ilASfi8NjSon9S0UiY988MumDhE2KxuY=
=kNUp
-----END PGP SIGNATURE-----



From ulises at vianetcon.com.ar  Thu Jul 23 16:53:24 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Thu, 23 Jul 2015 13:53:24 -0300
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B1108F.1030007@gmail.com>
References: <55B1101B.8010408@vianetcon.com.ar> <55B1108F.1030007@gmail.com>
Message-ID: <55B11C04.4000006@vianetcon.com.ar>

Hello Yuri,

All images have grater than 512 bytes

132Kb http://test-server.com/images/imagen3.jpg
53Kb GET http://test-server.com/images/imagen1.gif
8Kb GET http://test-server.com/images/icono.png

This is the list of the objects in the linux file system to confirm the 
flie sizes

-rw-r--r-- 1 root            root             7.9K Jun 12  2014 icono.png
-rw-r--r-- 1 root            root             53K Jul 20 12:36 imagen1.gif
-rw-r--r-- 1 root            root            130K Jul 21 19:27 imagen3.jpg


I don't think tahat the size is the problem.

Thanks

Ulises



El 23/07/15 13:04, Yuri Voinov escribi?:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> Heh. Looks like images is less than:
>
> minimum_object_size 512 bytes
>
> this parameter. :)
>
>
>
> 23.07.15 22:02, Ulises Nicolini ?????:
>> minimum_object_size 512 bytes
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJVsRCPAAoJENNXIZxhPexG7f8H/iuEMqYKJhD2aC7CG620nwii
> SNZwJJGw4HDMsYbRZfLuTow/wQ11np7z0+33Ucb0T4F1MMPmd73p5lY1jI37Opu3
> gZc1USGwvWzRA6FcTVWUjtnHqsMhf9VBEU69EXpj+H71U2bDKox31qsMOTWbt056
> Bx0xWmdpvfIRBVH+6fE+WgkLz2uACusSaztRdBo9EFhNFH6egJ8x5E+H3BtUiNmH
> f+uItP6iFRPO7FrdNVra/uvCWFNq/wpOyloTyxDUgRsCSkhiZthlzBTw6bRsYf48
> 158nUtepNV2amiW2itKOYZi4FWrG8o08TMeEqbyxaCI/A3xSf0lO56ZoO0V6jO4=
> =GNtZ
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/7c96baa1/attachment.htm>

From hack.back at hotmail.com  Thu Jul 23 17:13:52 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 23 Jul 2015 10:13:52 -0700 (PDT)
Subject: [squid-users] squid 3.5.6 and ecap
In-Reply-To: <55B110D5.9000400@gmail.com>
References: <1437664698592-4672387.post@n4.nabble.com>
 <55B110D5.9000400@gmail.com>
Message-ID: <1437671632416-4672394.post@n4.nabble.com>

nano /etc/ld.so.conf

Add /usr/local/lib

ldconfig


Solved



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-6-and-ecap-tp4672387p4672394.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jul 23 17:19:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 05:19:25 +1200
Subject: [squid-users] squid 3.5.6 and ecap
In-Reply-To: <55B110D5.9000400@gmail.com>
References: <1437664698592-4672387.post@n4.nabble.com>
 <55B110D5.9000400@gmail.com>
Message-ID: <55B1221D.2010407@treenet.co.nz>

On 24/07/2015 4:05 a.m., Yuri Voinov wrote:
> 
>  No such file or directory
> 
> means "No such file or directory" exactly. :)
> 
> Your squid can't find libecap.
> 
> Simple.
> 

In particular it can't find the libecap with .so version 3.

That so version number is used by libecap 1.0.x.

Since you built it yourself please ensure you built the right libecap
version. And installed in properly in the right place. And built Sqiud
with correct paths to locate it by.


FYI for anyone using Debian; squid 3.5.6 and libecap 1.0.1 are now
available in the Sid/unstable repositories.

Amos


From hack.back at hotmail.com  Thu Jul 23 17:33:54 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 23 Jul 2015 10:33:54 -0700 (PDT)
Subject: [squid-users] ecap and https
Message-ID: <1437672834370-4672396.post@n4.nabble.com>

when we can use ecap with https contents ?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Jul 23 17:51:30 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 23 Jul 2015 23:51:30 +0600
Subject: [squid-users] ecap and https
In-Reply-To: <1437672834370-4672396.post@n4.nabble.com>
References: <1437672834370-4672396.post@n4.nabble.com>
Message-ID: <55B129A2.4050609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Sure. HTTPS also uses GZip ;)

23.07.15 23:33, HackXBack ?????:
> when we can use ecap with https contents ?
> Thanks.
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsSmiAAoJENNXIZxhPexGMAIH/iCiuMurZ0tTlStsFjYql1Ki
rZS9txU1vCOtnidNLamv+uJ+wUBV1awJAVHh7lrbd6cnw6CH1uNwftfwWmRmNmBK
txMPwesFIkCjgfLtLzG5+QgmVdR/IfTbxmSqYAe/GrU24oC/IaaVF2d5AH6tjW7a
GBCjjfhrwA9H7UWP59VX139m0vXGT33SJ9sGulkW8EcDmYNBNrowxmRid9PhIEEF
7KJD4VMNWgp4poKcSxT2jN8FsuMGjGWXFnOePtDQp7s5JbTenn4Otsb3Cv7G3N2d
nuxlAMo+pxDfNstt3E/Dv8qwhPm/dwdBE0kjlbIoOH5o27lN7Pqh5cEXTLCLig4=
=Aa7h
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Thu Jul 23 17:55:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 23 Jul 2015 23:55:28 +0600
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B11C04.4000006@vianetcon.com.ar>
References: <55B1101B.8010408@vianetcon.com.ar> <55B1108F.1030007@gmail.com>
 <55B11C04.4000006@vianetcon.com.ar>
Message-ID: <55B12A90.5000903@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Aha.

So, another question.

Does server uses compression? I.e. GZip, or something else?

23.07.15 22:53, Ulises Nicolini ?????:
> Hello Yuri,
>
> All images have grater than 512 bytes
>
> 132Kb http://test-server.com/images/imagen3.jpg
> 53Kb GET http://test-server.com/images/imagen1.gif
> 8Kb GET http://test-server.com/images/icono.png
>
> This is the list of the objects in the linux file system to confirm
the flie sizes
>
> -rw-r--r-- 1 root            root             7.9K Jun 12  2014 icono.png
> -rw-r--r-- 1 root            root             53K Jul 20 12:36 imagen1.gif
> -rw-r--r-- 1 root            root            130K Jul 21 19:27 imagen3.jpg
>
>
> I don't think tahat the size is the problem.
>
> Thanks
>
> Ulises
>
>
>
> El 23/07/15 13:04, Yuri Voinov escribi?:
> minimum_object_size 512 bytes
>
> this parameter. :)
>
>
>
> 23.07.15 22:02, Ulises Nicolini ?????:
> >>> minimum_object_size 512 bytes
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsSqQAAoJENNXIZxhPexGqzsH/3DW9Buzo/n3eKcxokGEaoQ1
5+vyXaeXgiTSI1eXEDT+1qp1kT5ZhcvW093Xn1/37RC9vnvaAp21B9ARKmQMkzhP
OEHH9F6FM96ORnnE21lK5xVZZgMAfJDzPkxOX399I2VTcArbtleQXr/Zi2sdZgYO
HptuM+t5GJEX8M3wATLFVRYDVADMDvGiwSMkMspYPxCzujZZdY0a0sQn9Q0iC9hs
KzwUnTbCB53EWpizlEspK1oHF2YDdvBmLXG3OexA8I2afzFtHW2IIHMkrEzsIygk
Pg+jl3JfmpdVAs8O866F2VCpDcESyIH1j6vlRU8qLVj3WJU1yp/XSFhn+3JMXZA=
=yO/c
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/eff08cbb/attachment.htm>

From jorgeley at gmail.com  Thu Jul 23 16:31:31 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 23 Jul 2015 13:31:31 -0300
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437667243296-4672389.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
Message-ID: <CAMeoTHnod62nwcKuh1RpUfsXBtnzQXgS8RSajXPATs6fCRFxkw@mail.gmail.com>

Hi Joe, I had similar problem with youtube, I did not sure if it's the same
with you, but I passed the youtube.com domain out of the cache with 'cache
deny' directive.
I'm using squid-3.5.6 and I'm in trouble with the authentication on chroot
environment, so, maybe you cand help me, is your squid on a chroot? do you
use authentication?

2015-07-23 13:00 GMT-03:00 joe <chip_pop at hotmail.com>:

> my English not grait so be pattion tks
> hi i setup yt caching working perfect but i need to ask
> first squid 3.5.6
> i need to know how is yt detect and send partial video
> i have 2 computer same flash v. same firefox v. all identical exept one
> windowsxp another is win7
> i cache html5 on win7 yt send partial video on winxp send full video
> i put none to
> request_header_access Accept-Ranges deny all
> reply_header_access Accept-Ranges deny all
> request_header_replace Accept-Ranges none
> reply_header_replace Accept-Ranges none
>
> so Wat cause the partial video on win7 is it some header or ??
>
> you thing deny Accept-Ranges not working  ?
> or some other thing tks if any help
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/abd35be8/attachment.htm>

From hack.back at hotmail.com  Thu Jul 23 17:56:06 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 23 Jul 2015 10:56:06 -0700 (PDT)
Subject: [squid-users] ecap and https
In-Reply-To: <55B129A2.4050609@gmail.com>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B129A2.4050609@gmail.com>
Message-ID: <1437674166316-4672400.post@n4.nabble.com>

request_header_access Accept-Encoding deny all
loadable_modules /usr/local/lib/ecap_adapter_modifying.so
ecap_enable on
ecap_service ecapModifier respmod_precache \
        uri=ecap://e-cap.org/ecap/services/sample/modifying \
        victim=bb \
        replacement=aa
adaptation_access ecapModifier allow all


i use this conf to edit in https page content but no change happen



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672400.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Jul 23 17:58:36 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 23 Jul 2015 23:58:36 +0600
Subject: [squid-users] ecap and https
In-Reply-To: <1437674166316-4672400.post@n4.nabble.com>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B129A2.4050609@gmail.com> <1437674166316-4672400.post@n4.nabble.com>
Message-ID: <55B12B4C.8030907@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
HHhhhhhhhhh..... what this module does?

And - for what you deny Accept-Encoding header?!

23.07.15 23:56, HackXBack ?????:
> request_header_access Accept-Encoding deny all
> loadable_modules /usr/local/lib/ecap_adapter_modifying.so
> ecap_enable on
> ecap_service ecapModifier respmod_precache \
>         uri=ecap://e-cap.org/ecap/services/sample/modifying \
>         victim=bb \
>         replacement=aa
> adaptation_access ecapModifier allow all
>
>
> i use this conf to edit in https page content but no change happen
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672400.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsStMAAoJENNXIZxhPexGcrEH/1g31FwXhzp5DIBvNMyPV3Yy
R+I0j5/W99Pq8UKPa5sTCjg8cAYeFJKCM3kceT3EYxmzqRjX+aFgFyy8JP2GoHfw
mGjSte2HLXSYtUXH8ilNgr6its3LFIY408MdxkejCfpJfWRgH5EdVfHMMmQGPbvf
xsHNLTCJZYI8YBsEq4/9mWZzgOINiy0RT0ymu8CfAZ+3LZWWs0PF4bqRGOASXuSH
4dDw2CNAInnhTyACXFdIlEcX0oM9yRyBQYMAk29/XRt/IhIkGh6viGYwB6/zRjuB
9RYZ3p2pDyTxuF9l0hbBujpSrObmn79OFaqZ3qIJlw3fdS/VETaDeOOsV3BIFQM=
=ofIF
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Thu Jul 23 17:57:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 05:57:58 +1200
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B1101B.8010408@vianetcon.com.ar>
References: <55B1101B.8010408@vianetcon.com.ar>
Message-ID: <55B12B26.2090605@treenet.co.nz>

On 24/07/2015 4:02 a.m., Ulises Nicolini wrote:
> Hello,
> 
> I have a basic squid 3.5 configuration with
> 
> maximum_object_size_in_memory 64 KB
> maximum_object_size 100000 KB
> minimum_object_size 512 bytes
> 
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 10080
> override-expire ignore-no-cache ignore-private
> refresh_pattern -i (/cgi-bin/)                  0       0%      0
> refresh_pattern .                                       0 20%     4320
> 

ignore-no-cache has no meaning for Squid-3.5.

ignore-private does nothing on your images. It makes current Squid act
like must-revalidate was set on the response instead of "private".

override-expire also does nothing on your images. As used above it makes
Squid act like "s-maxage=604800" was given instead of any Expires:
header or max-age=N / s-maxage=N Cache-Control values.


> 
> cache_dir rock  /cache1/rock1 256  min-size=500 max-size=32767
> max-swap-rate=250 swap-timeout=350
> cache_dir diskd /cache2/diskd1 1000 16 256 min-size=32768 max-size=1048576
> cache_dir diskd /cache2/diskd2 100000 16 256 min-size=1048576
> 
> 
> But when I test it against my webserver, using only one client PC, the
> only thing I get are TCP_MISSes of my images.
> 
> 1437664284.339     11 192.168.2.103 TCP_MISS/200 132417 GET
> http://test-server.com/images/imagen3.jpg - HIER_DIRECT/192.168.2.10
> image/jpeg
> 1437664549.753      5 192.168.2.103 TCP_MISS/200 53933 GET
> http://test-server.com/images/imagen1.gif - HIER_DIRECT/192.168.2.10
> image/gif
> 1437665917.469     18 192.168.2.103 TCP_MISS/200 8319 GET
> http://test-server.com/images/icono.png - HIER_DIRECT/192.168.2.10
> image/png
> 
> The response headers don't have Vary tags or any other that may impede
> caching
> 
> Accept-Ranges    bytes
> Connection    close
> Content-Length    53644
> Content-Type    image/gif
> Date    Thu, 23 Jul 2015 15:56:07 GMT
> Etag    "e548d4-d18c-51b504b95dec0"
> Last-Modified    Mon, 20 Jul 2015 15:36:03 GMT
> Server    Apache/2.2.22 (EL)
> 

Your refresh pattern says to only cache these objects for +90% of their
current age, so long as that period is longer than 1 day (1440 mins) and
no more that 7 days (10080 mins).

Which means;
 they are 3 days 20 mins 4 secs old right now (260404 secs).
 90% of that is 2 days 17 hrs 6 mins 3 secs (234363 secs).

So the object "e548d4-d18c-51b504b95dec0" will stay in cache for the
next 2 days 17hrs etc.

I notice though that the Content-Length size does not match any of the
logged transfer sizes. Which makes me wonder if the object is actually
varying despite the lack of Vary headers.


> 
> Is it necessary a certain amount of requests of a single object to be
> cached (mem o disk) or am I facing some other problem here?

Yes. Two requests. The first (a MISS) will add it to cache the second
and later should be HITs on the now cached object.

BUT, only if you are not force-reloading the browser for your tests.
Force-reload instructs Squid it ignore its cached content and replace it
with another MISS.


Amos


From jorgeley at gmail.com  Thu Jul 23 16:28:27 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Thu, 23 Jul 2015 13:28:27 -0300
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <55B0FD60.9070705@treenet.co.nz>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
Message-ID: <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>

Befor all, thanks so so much for the answears!!!
It's exist, I'm sure.
This is my chroot structre:
/ (linux root)
/etc
     squid-3.5.6/
                      bin/
                           purge
                           squidclient
                      cache/
                           (squid cache dirs generated by squid -z)
                      etc/
                            cachemgr.conf
                            errorpage.css
                            group
                            gshadow
                            hosts
                            localtime
                            mime.conf
                            nsswitch.conf
                            passwd
                            resolv.conf
                            shadow
                            squid.conf
                       lib64/
                             (a lot of libs here, discovered with ldd
command)
                       libexec/
                             basic_ncsa_auth
                             diskd
                             (other default squid libs)
                       regras/
                             (my acl files rules)
                       sbin/
                             squid
                       share/
                               errors/
                                       (default dir squid errors)
                               icons/
                                       (default squid icons
                               man/
                                       (default man squid pages)
                       usr/
                              lib64/
                                       (a lot of libs here, discovered with
ldd command)
                       var/
                              logs/
                                       (default squid logs)
                              run/
                                    squid.pid

I did the command:
chroot /etc/squid-3.5.6 /libexec/basic_ncsa_auth
It runs, that's why I'm sure the chroot environment, unless for the
ncsa_auth, is correct

Any more suggestions?

2015-07-23 11:42 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 23/07/2015 11:23 p.m., Jorgeley Junior wrote:
> >> Hi guys.
> >> I have a RedHat 6.6 + squid 3.5.6 + basic_ncsa_auth + chroot and is
> >> crashing only when I do an authentication.
> >>
> >> Here is the main confs:
> >> auth_param basic program /libexec/basic_ncsa_auth /regras/usuarios
> >> auth_param basic children 10 startup=0 idle=1
> >> auth_param basic realm INTERNET-LOGIN NECESSARIO
> >> ... (other confs) ...
> >> acl usuarios            proxy_auth -i
>  "/etc/squid-3.5.6/regras/usuarios"
> >> ... (other confs) ...
> >> chroot /etc/squid-3.5.6
> >>
> >> Here is what I find in the cache.log:
> >> 2015/07/22 18:47:27.866 kid1| WARNING: no_suid: setuid(0): (1) Operation
> >> not permitted
> >> 2015/07/22 18:48:01.735 kid1| ipcCreate: /libexec/basic_ncsa_auth: (2)
> No
> >> such file or directory
> >> 2015/07/22 18:47:27.866 kid1| WARNING: basicauthenticator #Hlpr13818
> exited
> >>
> >> What is the ipcCreate and why he is not findind the file?
>
> It is the code that runs the helper.
>
> The "/libexec/basic_ncsa_auth" does not exist as an exectuable binary
> inside your chroot.
>
>
> >>
> > About the libs needed when I do the chroot, I have to copy them to the
> > squid folder or I need to create the same structure like
> > /squid-3.5.6/libs,  /squid-3.5.6/lib64?
>
> They must match the OS layout where Squid (and everything else that will
> run in the chroot) expects to find them.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/6198e0be/attachment.htm>

From yvoinov at gmail.com  Thu Jul 23 18:02:49 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 00:02:49 +0600
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B12B26.2090605@treenet.co.nz>
References: <55B1101B.8010408@vianetcon.com.ar>
 <55B12B26.2090605@treenet.co.nz>
Message-ID: <55B12C49.8080308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


23.07.15 23:57, Amos Jeffries ?????:
> On 24/07/2015 4:02 a.m., Ulises Nicolini wrote:
>> Hello,
>>
>> I have a basic squid 3.5 configuration with
>>
>> maximum_object_size_in_memory 64 KB
>> maximum_object_size 100000 KB
>> minimum_object_size 512 bytes
>>
>> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 10080
>> override-expire ignore-no-cache ignore-private
>> refresh_pattern -i (/cgi-bin/)                  0       0%      0
>> refresh_pattern .                                       0 20%     4320
>>
>
> ignore-no-cache has no meaning for Squid-3.5.
>
> ignore-private does nothing on your images. It makes current Squid act
> like must-revalidate was set on the response instead of "private".
>
> override-expire also does nothing on your images. As used above it makes
> Squid act like "s-maxage=604800" was given instead of any Expires:
> header or max-age=N / s-maxage=N Cache-Control values.
>
>
>>
>> cache_dir rock  /cache1/rock1 256  min-size=500 max-size=32767
>> max-swap-rate=250 swap-timeout=350
>> cache_dir diskd /cache2/diskd1 1000 16 256 min-size=32768
max-size=1048576
>> cache_dir diskd /cache2/diskd2 100000 16 256 min-size=1048576
>>
>>
>> But when I test it against my webserver, using only one client PC, the
>> only thing I get are TCP_MISSes of my images.
>>
>> 1437664284.339     11 192.168.2.103 TCP_MISS/200 132417 GET
>> http://test-server.com/images/imagen3.jpg - HIER_DIRECT/192.168.2.10
>> image/jpeg
>> 1437664549.753      5 192.168.2.103 TCP_MISS/200 53933 GET
>> http://test-server.com/images/imagen1.gif - HIER_DIRECT/192.168.2.10
>> image/gif
>> 1437665917.469     18 192.168.2.103 TCP_MISS/200 8319 GET
>> http://test-server.com/images/icono.png - HIER_DIRECT/192.168.2.10
>> image/png
>>
>> The response headers don't have Vary tags or any other that may impede
>> caching
>>
>> Accept-Ranges    bytes
>> Connection    close
>> Content-Length    53644
>> Content-Type    image/gif
>> Date    Thu, 23 Jul 2015 15:56:07 GMT
>> Etag    "e548d4-d18c-51b504b95dec0"
>> Last-Modified    Mon, 20 Jul 2015 15:36:03 GMT
>> Server    Apache/2.2.22 (EL)
>>
>
> Your refresh pattern says to only cache these objects for +90% of their
> current age, so long as that period is longer than 1 day (1440 mins) and
> no more that 7 days (10080 mins).
>
> Which means;
>  they are 3 days 20 mins 4 secs old right now (260404 secs).
>  90% of that is 2 days 17 hrs 6 mins 3 secs (234363 secs).
>
> So the object "e548d4-d18c-51b504b95dec0" will stay in cache for the
> next 2 days 17hrs etc.
>
> I notice though that the Content-Length size does not match any of the
> logged transfer sizes. Which makes me wonder if the object is actually
> varying despite the lack of Vary headers.
>
>
>>
>> Is it necessary a certain amount of requests of a single object to be
>> cached (mem o disk) or am I facing some other problem here?
>
> Yes. Two requests. The first (a MISS) will add it to cache the second
> and later should be HITs on the now cached object.
>
> BUT, only if you are not force-reloading the browser for your tests.
> Force-reload instructs Squid it ignore its cached content and replace it
> with another MISS.
Amos, this behaviour depends from refresh_pattern and often can be
ignore (with reload-into-ims, for example).
>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsSxJAAoJENNXIZxhPexGIIkH/R3kAoVeGgAGl7aTqzB3W3kU
cScH0b0fosX/tYnAEUBlIgXFnc7vfCld8KqnYoydyL4JM9opvKZ30NAdsVDTPMH5
RRGa+AGlqQlduVc4f2sMiVa3k5ukf3930wbNa3kpEUt0Hp/cpcP0Gud3sRL4u/wa
f/W67fbwZ+JJfMq4IvYDnqxv2dcXQkLbgv56fDjPMvn+cW9ar/IkCFk5Joz35o/z
8CcgY8+wUug7JMULyql88AM9Qm0FE8zWQ7eFSBQ6UTaeKa7Z/q3IO5iP3XymsmW4
biRCSqtNpesmVImIzpM9lza7FXyzqNZXTjXjXojEXjCnKWL+LDuJnDoGuyLr1iQ=
=Ii5/
-----END PGP SIGNATURE-----



From marcus.kool at urlfilterdb.com  Thu Jul 23 18:08:28 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 23 Jul 2015 15:08:28 -0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>
Message-ID: <55B12D9C.2030509@urlfilterdb.com>

The strace output shows this loop:

    Squid reads 16K-1 bytes from FD 13    webserver
    Squid writes 4 times 4K to FD 17      /var/cache/squid3/00/00/00000000
    Squid writes 4 times 4K to FD 12      browser

But this loop does not explain the 100% CPU usage...

Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?

I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
I never looked at Squid CPU usage and do not know what is reasonable but it feels high.

With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
This is a possible explanation for the fact that you see the performance going down and up.
Can you do the same test on the host (i.e. not inside a VM).

Marcus



On 07/23/2015 10:39 AM, Jens Offenbach wrote:
> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
> Here is the download link:
> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace
> I hope it can help you.
> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
> I am not sure if it is relevant, maybe it is:
>
> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it receives
> one or a few TCP packets.
>
> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>
> Amos, is there a debug setting that can confirm/reject this suspicion?
>
> Marcus
>
>
> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>  > A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>  >
>  > I cleared the cache:
>  > rm -rf /var/cache/squid3/*
>  > squid -z
>  > squid
>  > http_proxy=http://139.2.57.120:3128/ wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>  >
>  > The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>  >
>  > During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>  >
>  > For further investigations I give you my build properties:
>  > squid -v
>  > Squid Cache: Version 3.5.6
>  > Service Name: squid
>  > configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
> s
> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>  >
>  >
>  > Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>  > Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>  > An: squid-users at lists.squid-cache.org
>  > Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>  > On 22/07/2015 21:59, Eliezer Croitoru wrote:
>  >> Hey Jens,
>  >>
>  >> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>  >> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>  >> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>  >> download speed is more then doubled to 4.5 MBps(36Mbps).
>  >> I have not tried it with AUFS yet.
>  >
>  >
>  > And I must admit that AUFS beats rock cache with speed.
>  > I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>  > "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>  > results more then doubles 3 the speed rock gave with default settings.
>  >
>  > So about 15MBps which is 120Mbps.
>  > I do not seem to feel what Jens feels but the 100% CPU might be because
>  > of spinning disk hangs while reading the file from disk.
>  >
>  > Amos, I remember that there were some suggestions how to tune large rock.
>  > Any hints?
>  > I can test it and make it a suggestion for big files.
>  >
>  > Eliezer
>  >
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org
>  > http://lists.squid-cache.org/listinfo/squid-users
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org
>  > http://lists.squid-cache.org/listinfo/squid-users
>  >


From chip_pop at hotmail.com  Thu Jul 23 18:21:02 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 23 Jul 2015 11:21:02 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <CAMeoTHnod62nwcKuh1RpUfsXBtnzQXgS8RSajXPATs6fCRFxkw@mail.gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <CAMeoTHnod62nwcKuh1RpUfsXBtnzQXgS8RSajXPATs6fCRFxkw@mail.gmail.com>
Message-ID: <1437675662527-4672406.post@n4.nabble.com>

sorry i don't use authentication just simple nat forwerd to the transparent
squid

//Hi Joe, I had similar problem with youtube, I did not sure if it's the
same with you, but I passed the //youtube.com domain out of the cache with
'cache deny' directive.
i didnt get it !!  the main domain youtube.com i only cache img video stuf
like that start from 1k size up
and its weard that i dont see it why wen i prevent range with acl or all it
still partial more funny is identical
test on 2 computer just the windows is different  win7 and xp on xp i get
full video file and in quality just 380p no other
on win7 i get partial and all quality exist ps...on html5 i need to know if
its header essue or some api....inside the flash detect opirating then send
link that has jumping partial with diff quality
look like this im gessing only
GET
/api/stats/qoe?event=streamingstats&fmt=133&afmt=251&cpn=162FVeXKqfd1Fojf&ei=QTCxVYfpKM6OiQa9soGAAw&el=detailpage&docid=-Y-CJjJ-jtw&ns=yt&fexp=3300100%2C3300133%2C3300137%2C3300164%2C3310700%2C3312381%2C3312543%2C901816%2C936117%2C9408623%2C9408710%2C9415429%2C9415435%2C9415873%2C9415942%2C9416046%2C9416126%2C9416293%2C9416729%2C9417927&cl=98840873&html5=1&c=WEB&cver=html5&cplayer=UNIPLAYER&vps=0.000:N,0.136:N&afs=0.136:251::i&vfs=0.136:133:134::i&view=0.136:640:390&bwe=0.136:91668&bh=0.136:0.000&cmt=0.136:0
HTTP/1.1





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672406.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Thu Jul 23 18:27:24 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 23 Jul 2015 11:27:24 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437675662527-4672406.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <CAMeoTHnod62nwcKuh1RpUfsXBtnzQXgS8RSajXPATs6fCRFxkw@mail.gmail.com>
 <1437675662527-4672406.post@n4.nabble.com>
Message-ID: <1437676044023-4672407.post@n4.nabble.com>

forgot
on that link there is  fmt=133&afmt=251   they ar the  itag jumping 

this one frommy xp window the one work non range has one 
http://s.youtube.com/api/stats/qoe?event=streamingstats&fmt=43&cpn=b-DgHXDzC4p2xiSg&ei=VzGxVYbPIbGGiAaS05PoCg&el=detailpage&docid=BQnejVZLQ6w&ns=yt&fexp=901816%2C9405973%2C9407662%2C9408710%2C9412490%2C9414764%2C9416104%2C9416126%2C9416200%2C9416268%2C9416336%2C9416688%2C9416729&cl=98840873&html5=1&c=WEB&cver=html5&cplayer=UNIPLAYER&vps=0.000:N,0.031:N&vfs=0.031:43:43::i&view=0.031:640:390&bh=0.031:0.000&cmt=0.031:0

if u notice  just fmt=43 wish is the itag  380p on html5



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672407.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stan.prescott at gmail.com  Thu Jul 23 20:21:20 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Thu, 23 Jul 2015 15:21:20 -0500
Subject: [squid-users] SIGTERM SIGKILL causes issues with squid shutdown
	during reboot
Message-ID: <CANLNtGS8k57QFHi-vWvY4cjNNkCgbZBTjS1iUCZAP1Li0=dUTg@mail.gmail.com>

After bumping Squid from 3.4.x to 3.5.x in our implementation of Squid in
the Smoothwall Express v3.1 firewall distro we have begun to have
complaints from our users about "erratic behavior" of Squid shutting down
during reboots or network drops causing reboots.

It appears that squid (v3.5.[5-6]) does not respond well to SIGTERM during
system shutdown; the cache index almost invariably needs to be rebuilt on
next boot. It is suggested that we use squid to shut squid down. While
using squid to stop the squid daemon is very doable, this requirement runs
contrary to the longstanding, traditional UNIX method of "SIGTERM, pause,
SIGKILL".

During normal system operation, squid *ought* to be able to take as much
time as it wants to shut down. But it still shouldn't take more than 10-20
seconds; 'shutdown' is a command, not a request to be honored at squid's
leisure. After all, the CPU could be on fire....

This raises a few questions that are intended to foster fresh discussion,
not to re-hash old arguments. They are really more rhetorical in nature;
the goal is to find the root cause of the problem.

   - Why do these latest versions of Squid 3 behave oddly in this respect?
   - What is it about shutting squid down that corrupts the cache index?
   - Does it take more than a few seconds to write the index to disk?
   - Does squid use the very slow 'writethrough' method instead of trusting
   the Linux disk cache to properly save the cache?
   - Should squid write the cache index to disk more often?
   - Should squid track its own 'dirty pages' in its in-core index to
   reduce the time it takes to write the index?
   - Should squid implement a journal (akin to EXT/ReiserFS and others) so
   the on-disk index structure is always OK?
   - Is the problem related to clients actively receiving web pages from
   squid?
   - Could squid's signal handling be adjusted to treat those clients more
   harshly? That is, terminate the transfers early because squid shutting down
   is much the same as the network interface going down.
   - Is the only viable solution to use squid to stop the daemon? Does
   'squid -k shutdown' exit only after the daemon is dead? The squid man page
   indicates that it sends the shutdown 'signal' but does not await a reply;
   thus a potentially lengthy pause in system shutdown may be required anyway.
   - Is pausing the system shutdown for 10-15 seconds the only really
   viable shutdown solution?
   - Or is it best to delete and rebuild the cache index on every system
   startup?

Any thoughts on any of the above statements or issues?

As always, I appreciate any help and suggestions.

Regards

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/3ba679ff/attachment.htm>

From hack.back at hotmail.com  Thu Jul 23 21:53:45 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 23 Jul 2015 14:53:45 -0700 (PDT)
Subject: [squid-users] ecap and https
In-Reply-To: <55B12B4C.8030907@gmail.com>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B129A2.4050609@gmail.com> <1437674166316-4672400.post@n4.nabble.com>
 <55B12B4C.8030907@gmail.com>
Message-ID: <1437688425574-4672409.post@n4.nabble.com>

read the Documentation

http://www.e-cap.org/Documentation



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672409.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Thu Jul 23 22:24:36 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 24 Jul 2015 01:24:36 +0300
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437667243296-4672389.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
Message-ID: <55B169A4.8070601@ngtech.co.il>

Hey Joe,

I understand the need for caching youtube but it might be not as 
possible as in the past.

There was someone here on the list that offers a product that helps to 
cache youtube videos but I do not know the secret behind it.

The partial content has special key in it and youtube kind of changed 
couple things to allow variable bitrate.

All The Bests,
Eliezer

On 23/07/2015 19:00, joe wrote:
> my English not grait so be pattion tks
> hi i setup yt caching working perfect but i need to ask
> first squid 3.5.6
> i need to know how is yt detect and send partial video
> i have 2 computer same flash v. same firefox v. all identical exept one
> windowsxp another is win7
> i cache html5 on win7 yt send partial video on winxp send full video
> i put none to
> request_header_access Accept-Ranges deny all
> reply_header_access Accept-Ranges deny all
> request_header_replace Accept-Ranges none
> reply_header_replace Accept-Ranges none
>
> so Wat cause the partial video on win7 is it some header or ??
>
> you thing deny Accept-Ranges not working  ?
> or some other thing tks if any help
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From chip_pop at hotmail.com  Thu Jul 23 22:46:17 2015
From: chip_pop at hotmail.com (joe)
Date: Thu, 23 Jul 2015 15:46:17 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B169A4.8070601@ngtech.co.il>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il>
Message-ID: <1437691577069-4672411.post@n4.nabble.com>

hi Eliezer i do cache yt  all is fine tks and i do can force html5 as
bluecoat did i think they ar not secret any more  all my problem or my Quest
was as i sayd in first post  i have 2 setup computer one winxp and one win7
with both using firefox v39 and latest identical flash player on winxp i get
full video not partialon win7 its partial so how and wat caus it tksand the
video id my owen simple no mather wat they do they cant force diferent id
caus my store-id is privet yet hope so hehe



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672411.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From dan at getbusi.com  Fri Jul 24 04:33:05 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Thu, 23 Jul 2015 21:33:05 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B169A4.8070601@ngtech.co.il>
References: <55B169A4.8070601@ngtech.co.il>
Message-ID: <1437712385363.74b82703@Nodemailer>

Not to go off-topic here, but you folks are all SSL Bumping youtube.com / googlevideo.com in order to do this caching, right?




Want to make sure I?m not missing some secret way to make YouTube use plain HTTP.

On Fri, Jul 24, 2015 at 8:24 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey Joe,
> I understand the need for caching youtube but it might be not as 
> possible as in the past.
> There was someone here on the list that offers a product that helps to 
> cache youtube videos but I do not know the secret behind it.
> The partial content has special key in it and youtube kind of changed 
> couple things to allow variable bitrate.
> All The Bests,
> Eliezer
> On 23/07/2015 19:00, joe wrote:
>> my English not grait so be pattion tks
>> hi i setup yt caching working perfect but i need to ask
>> first squid 3.5.6
>> i need to know how is yt detect and send partial video
>> i have 2 computer same flash v. same firefox v. all identical exept one
>> windowsxp another is win7
>> i cache html5 on win7 yt send partial video on winxp send full video
>> i put none to
>> request_header_access Accept-Ranges deny all
>> reply_header_access Accept-Ranges deny all
>> request_header_replace Accept-Ranges none
>> reply_header_replace Accept-Ranges none
>>
>> so Wat cause the partial video on win7 is it some header or ??
>>
>> you thing deny Accept-Ranges not working  ?
>> or some other thing tks if any help
>>
>>
>>
>> --
>> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150723/aeed5b69/attachment.htm>

From wolle5050 at gmx.de  Fri Jul 24 05:54:57 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Fri, 24 Jul 2015 07:54:57 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B12D9C.2030509@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>
Message-ID: <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>

It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.

The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
http://wikisend.com/download/293856/squid.strace2

@Marcus:
Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?

@Amos, Eliezer
Is someone able to reproduce the disk caching effect?

Regards,
Jens


Gesendet:?Donnerstag, 23. Juli 2015 um 20:08 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
Betreff:?Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
The strace output shows this loop:

Squid reads 16K-1 bytes from FD 13 webserver
Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
Squid writes 4 times 4K to FD 12 browser

But this loop does not explain the 100% CPU usage...

Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?

I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
I never looked at Squid CPU usage and do not know what is reasonable but it feels high.

With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
This is a possible explanation for the fact that you see the performance going down and up.
Can you do the same test on the host (i.e. not inside a VM).

Marcus



On 07/23/2015 10:39 AM, Jens Offenbach wrote:
> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
> Here is the download link:
> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace
> I hope it can help you.
> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
> I am not sure if it is relevant, maybe it is:
>
> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it receives
> one or a few TCP packets.
>
> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>
> Amos, is there a debug setting that can confirm/reject this suspicion?
>
> Marcus
>
>
> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
> > A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
> >
> > I cleared the cache:
> > rm -rf /var/cache/squid3/*
> > squid -z
> > squid
> > http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
> >
> > The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
> >
> > During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
> >
> > For further investigations I give you my build properties:
> > squid -v
> > Squid Cache: Version 3.5.6
> > Service Name: squid
> > configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
> s
> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
> >
> >
> > Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
> > Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> > An: squid-users at lists.squid-cache.org
> > Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> > On 22/07/2015 21:59, Eliezer Croitoru wrote:
> >> Hey Jens,
> >>
> >> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> >> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> >> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> >> download speed is more then doubled to 4.5 MBps(36Mbps).
> >> I have not tried it with AUFS yet.
> >
> >
> > And I must admit that AUFS beats rock cache with speed.
> > I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
> > "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
> > results more then doubles 3 the speed rock gave with default settings.
> >
> > So about 15MBps which is 120Mbps.
> > I do not seem to feel what Jens feels but the 100% CPU might be because
> > of spinning disk hangs while reading the file from disk.
> >
> > Amos, I remember that there were some suggestions how to tune large rock.
> > Any hints?
> > I can test it and make it a suggestion for big files.
> >
> > Eliezer
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
> >


From wolle5050 at gmx.de  Fri Jul 24 06:25:57 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Fri, 24 Jul 2015 08:25:57 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
Message-ID: <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>

I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
?

Gesendet:?Freitag, 24. Juli 2015 um 07:54 Uhr
Von:?"Jens Offenbach" <wolle5050 at gmx.de>
An:?"Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.

The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
http://wikisend.com/download/293856/squid.strace2

@Marcus:
Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?

@Amos, Eliezer
Is someone able to reproduce the disk caching effect?

Regards,
Jens


Gesendet:?Donnerstag, 23. Juli 2015 um 20:08 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
Betreff:?Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
The strace output shows this loop:

Squid reads 16K-1 bytes from FD 13 webserver
Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
Squid writes 4 times 4K to FD 12 browser

But this loop does not explain the 100% CPU usage...

Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?

I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
I never looked at Squid CPU usage and do not know what is reasonable but it feels high.

With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
This is a possible explanation for the fact that you see the performance going down and up.
Can you do the same test on the host (i.e. not inside a VM).

Marcus



On 07/23/2015 10:39 AM, Jens Offenbach wrote:
> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
> Here is the download link:
> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]
> I hope it can help you.
> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
> I am not sure if it is relevant, maybe it is:
>
> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it receives
> one or a few TCP packets.
>
> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>
> Amos, is there a debug setting that can confirm/reject this suspicion?
>
> Marcus
>
>
> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
> > A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
> >
> > I cleared the cache:
> > rm -rf /var/cache/squid3/*
> > squid -z
> > squid
> > http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
> >
> > The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
> >
> > During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
> >
> > For further investigations I give you my build properties:
> > squid -v
> > Squid Cache: Version 3.5.6
> > Service Name: squid
> > configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
> s
> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
> >
> >
> > Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
> > Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> > An: squid-users at lists.squid-cache.org
> > Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> > On 22/07/2015 21:59, Eliezer Croitoru wrote:
> >> Hey Jens,
> >>
> >> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> >> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> >> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> >> download speed is more then doubled to 4.5 MBps(36Mbps).
> >> I have not tried it with AUFS yet.
> >
> >
> > And I must admit that AUFS beats rock cache with speed.
> > I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
> > "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
> > results more then doubles 3 the speed rock gave with default settings.
> >
> > So about 15MBps which is 120Mbps.
> > I do not seem to feel what Jens feels but the 100% CPU might be because
> > of spinning disk hangs while reading the file from disk.
> >
> > Amos, I remember that there were some suggestions how to tune large rock.
> > Any hints?
> > I can test it and make it a suggestion for big files.
> >
> > Eliezer
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
> >
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]


From chip_pop at hotmail.com  Fri Jul 24 07:25:37 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 00:25:37 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437712385363.74b82703@Nodemailer>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
Message-ID: <1437722737301-4672415.post@n4.nabble.com>

http bro i have i have 300 client and i like to staty standard not violating
priv....that much as bluecoat thundercache use http the other use ssl they
have to work in country that alow thim to use ssl



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672415.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From wolle5050 at gmx.de  Fri Jul 24 07:49:01 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Fri, 24 Jul 2015 09:49:01 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>,
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>
Message-ID: <trinity-deb70e70-4635-4362-838a-69ac41efd453-1437724141663@3capp-gmx-bs56>

I have found something out... Hopefully, it helps to reproduce and solve the issue. 

I got it working with a good download rate, but very high CPU usage on Squid 3.3.8 and Squid 3.5.6. There seems to be problem with large files that get cached on disk in combination with memory caching. When I use these settings, memory usage of Squid grows step-by-step with 100% CPU usage and 500 KB/sec download rate:

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 1 GB
  memory_replacement_policy heap LFUDA
  cache_mem 4 GB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir aufs /var/cache/squid3 25600 16 256

I decided to turn off memory caching completely and used the following settings:

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 0 GB
  memory_replacement_policy heap LFUDA
  cache_mem 0 GB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir aufs /var/cache/squid3 25600 16 256

Now, I get stable and high download rates even on a cache miss.

@Marcus:
Could you please post your squid.config
?

Gesendet:?Freitag, 24. Juli 2015 um 08:25 Uhr
Von:?"Jens Offenbach" <wolle5050 at gmx.de>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
?

Gesendet:?Freitag, 24. Juli 2015 um 07:54 Uhr
Von:?"Jens Offenbach" <wolle5050 at gmx.de>
An:?"Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching
It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.

The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
http://wikisend.com/download/293856/squid.strace2

@Marcus:
Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?

@Amos, Eliezer
Is someone able to reproduce the disk caching effect?

Regards,
Jens


Gesendet:?Donnerstag, 23. Juli 2015 um 20:08 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
Betreff:?Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
The strace output shows this loop:

Squid reads 16K-1 bytes from FD 13 webserver
Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
Squid writes 4 times 4K to FD 12 browser

But this loop does not explain the 100% CPU usage...

Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?

I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
I never looked at Squid CPU usage and do not know what is reasonable but it feels high.

With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
This is a possible explanation for the fact that you see the performance going down and up.
Can you do the same test on the host (i.e. not inside a VM).

Marcus



On 07/23/2015 10:39 AM, Jens Offenbach wrote:
> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
> Here is the download link:
> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]]
> I hope it can help you.
> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
> I am not sure if it is relevant, maybe it is:
>
> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
> Squid sends the object to the ICAP server in small chunks of varying sizes:
> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
> Note that the interval of receiving the chunks is 1/1000th of a second.
> It seems that Squid forwards the object to the ICAP server every time it receives
> one or a few TCP packets.
>
> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>
> Amos, is there a debug setting that can confirm/reject this suspicion?
>
> Marcus
>
>
> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
> > A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
> >
> > I cleared the cache:
> > rm -rf /var/cache/squid3/*
> > squid -z
> > squid
> > http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
> >
> > The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
> >
> > During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
> >
> > For further investigations I give you my build properties:
> > squid -v
> > Squid Cache: Version 3.5.6
> > Service Name: squid
> > configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
> s
> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
> >
> >
> > Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
> > Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
> > An: squid-users at lists.squid-cache.org
> > Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> > On 22/07/2015 21:59, Eliezer Croitoru wrote:
> >> Hey Jens,
> >>
> >> I have tested the issue with LARGE ROCK and not AUFS or UFS.
> >> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
> >> Squid is sitting on an intel atom with SSD drive and on a HIT case the
> >> download speed is more then doubled to 4.5 MBps(36Mbps).
> >> I have not tried it with AUFS yet.
> >
> >
> > And I must admit that AUFS beats rock cache with speed.
> > I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
> > "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
> > results more then doubles 3 the speed rock gave with default settings.
> >
> > So about 15MBps which is 120Mbps.
> > I do not seem to feel what Jens feels but the 100% CPU might be because
> > of spinning disk hangs while reading the file from disk.
> >
> > Amos, I remember that there were some suggestions how to tune large rock.
> > Any hints?
> > I can test it and make it a suggestion for big files.
> >
> > Eliezer
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
> >
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]


From angelo.bruno at vigilfuoco.it  Fri Jul 24 07:57:39 2015
From: angelo.bruno at vigilfuoco.it (Posta Esterna)
Date: Fri, 24 Jul 2015 09:57:39 +0200
Subject: [squid-users] error windbind
Message-ID: <55B1EFF3.7050905@vigilfuoco.it>

Hi all....
i'm very new about squid. I'm tryng to start a squid service (2.6 
stable) on a server linux centos 5... it has to connect to an AD server 
to authenticate users for internet access...

squid restart is not so good because in the cache.log file i see this error

(ntlm_auth) invalid option --  -

in my squid.conf file i'm trying to use this lines:

auth_param ntlm program ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm keep alive on
auth_param ntlm children 10

Other information in the cache.log

(ntlm_auth) invalid option --  h
...

(ntlm_auth) invalid option --  e
...

(ntlm_auth) invalid option --  p
...

It seems it can't recognize the simple --helper-protocol as a single 
option! Is it possible?

On the other side if i try
wbinfo -t

i get:
"checking the trust secret via RPC calls failed
error code was   (0x0)
Could not check secret"

and wbinfo -p

"Ping to windbindd failed on fd -l
could not ping windbindd!"

Windbindd is not running... but it's a samba component... i don't need 
to use my server as a samba server, how i configure samba to start 
windbindd?

Please HELP

-- 
VCTI Ing. Angelo Bruno



From squid3 at treenet.co.nz  Fri Jul 24 08:22:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 20:22:55 +1200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B12D9C.2030509@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il> <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>
 <55B12D9C.2030509@urlfilterdb.com>
Message-ID: <55B1F5DF.4010502@treenet.co.nz>

On 24/07/2015 6:08 a.m., Marcus Kool wrote:
> The strace output shows this loop:
> 
>    Squid reads 16K-1 bytes from FD 13    webserver
>    Squid writes 4 times 4K to FD 17      /var/cache/squid3/00/00/00000000
>    Squid writes 4 times 4K to FD 12      browser
> 
> But this loop does not explain the 100% CPU usage...
> 
> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?

Yes, several (UFS / AUFS 3, or diskd 5).

TCP buffer -> FD 13 read buffer

FD 13 read buffer -> 4x mem_node (4KB each)
 ** walk the length of the in-memory part of the object to find where to
attach the mem_node. (once per each node?)
  - this has been a big CPU hog in the past (Squid-2 did it twice per
node insertion)

4x mem_node -> SHM memory buffer
  - diskd only, AUFS uses mem_node directly

SHM memory buffer -> FD 17 disk write latency
  - happens with both diskd (single treaded) and AUFS (x64 threads)
  - wait latency until completion event is seen by Squid ...

4x mem_node write() copy to FD 12 TCP buffer (OS dependent)


If you are doing any kind of ICAP processing you can add +3 copies per
service processing the transaction body.


> 
> I did the download test with Squid 3.4.12 AUFS on an idle system with a
> 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
> The first download used 35% of 1 CPU core with a steady download speed
> of 62 MB/sec.
> The second (cached) download used 50% of 1 CPU core with a steady
> download speed of 87 MB/sec.
> I never looked at Squid CPU usage and do not know what is reasonable but
> it feels high.
> 
> With respect to the 100% CPU issue of Jens, one factor is that Squid
> runs in a virtual machine.
> Squid in a virtual machine cannot be compared with a wget test since
> Squid allocates a lot of memory that the host must manage.
> This is a possible explanation for the fact that you see the performance
> going down and up.
> Can you do the same test on the host (i.e. not inside a VM).
> 
> Marcus
> 
> 
> 
> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>> I have attached strace to Squid and waited until the download rate has
>> decreased to 500 KB/sec.
>> I used "cache_dir aufs /var/cache/squid3 88894 16 256
>> max-size=10737418240".
>> Here is the download link:
>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace
>>
>> I hope it can help you.
>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru"
>> <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>,
>> squid-users at lists.squid-cache.org
>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>> I am not sure if it is relevant, maybe it is:
>>
>> I am developing an ICAP daemon and after the ICAP server sends a "100
>> continue"
>> Squid sends the object to the ICAP server in small chunks of varying
>> sizes:
>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>> Note that the interval of receiving the chunks is 1/1000th of a second.
>> It seems that Squid forwards the object to the ICAP server every time
>> it receives
>> one or a few TCP packets.
>>
>> I have a suspicion that in the scenario of 100% CPU, large #write
>> calls and low throughput a similar thing is happening:
>> Squid physically stores a small part of the object many times, i.e.
>> every time one or a few TCP packets arrive.
>>
>> Amos, is there a debug setting that can confirm/reject this suspicion?

After a bit more thought and Marcus feedback ; store.cc, mem_node
operations, and fd.cc and comm.cc are probably all worth watching.
"debug_options ALL,9" will get you everything Squid has to offer of course.

But be aware that the debugging itself adds a horribly large amount of
overheads for each line logged. At the highest levels it may noticably
impact the high-speed core routines you are trying to measure by skewing
latency into those with more debugs() statements.

Amos



From squid3 at treenet.co.nz  Fri Jul 24 08:29:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 20:29:33 +1200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-deb70e70-4635-4362-838a-69ac41efd453-1437724141663@3capp-gmx-bs56>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il> <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>
 <55B12D9C.2030509@urlfilterdb.com>
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>
 <trinity-deb70e70-4635-4362-838a-69ac41efd453-1437724141663@3capp-gmx-bs56>
Message-ID: <55B1F76D.3040800@treenet.co.nz>

On 24/07/2015 7:49 p.m., Jens Offenbach wrote:
> I have found something out... Hopefully, it helps to reproduce and solve the issue. 
> 
> I got it working with a good download rate, but very high CPU usage on Squid 3.3.8 and Squid 3.5.6. There seems to be problem with large files that get cached on disk in combination with memory caching. When I use these settings, memory usage of Squid grows step-by-step with 100% CPU usage and 500 KB/sec download rate:
> 
> # MEMORY CACHE OPTIONS
> # -----------------------------------------------------------------------------
>   maximum_object_size_in_memory 1 GB
>   memory_replacement_policy heap LFUDA
>   cache_mem 4 GB
> 
> # DISK CACHE OPTIONS
> # -----------------------------------------------------------------------------
>   maximum_object_size 10 GB
>   cache_replacement_policy heap GDSF
>   cache_dir aufs /var/cache/squid3 25600 16 256
> 
> I decided to turn off memory caching completely and used the following settings:
> 
> # MEMORY CACHE OPTIONS
> # -----------------------------------------------------------------------------
>   maximum_object_size_in_memory 0 GB
>   memory_replacement_policy heap LFUDA
>   cache_mem 0 GB
> 
> # DISK CACHE OPTIONS
> # -----------------------------------------------------------------------------
>   maximum_object_size 10 GB
>   cache_replacement_policy heap GDSF
>   cache_dir aufs /var/cache/squid3 25600 16 256
> 
> Now, I get stable and high download rates even on a cache miss.
> 

Damn. That gives me ~90% confidence its the mem_node walking as new 4KB
chunks of memory are appended to the memory copy of the object.


I would expect to see a reduced effect in 3.5 that kicks in around
maximum_object_size_in_memory. Since the memory copies are now split
into cache_mem objects, vs transients (disk cache only, or totally
non-cacheable). With the transients getting their unnecessary in-memory
sections pruned away regularly.

Amos


From squid3 at treenet.co.nz  Fri Jul 24 09:22:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 21:22:13 +1200
Subject: [squid-users] error windbind
In-Reply-To: <55B1EFF3.7050905@vigilfuoco.it>
References: <55B1EFF3.7050905@vigilfuoco.it>
Message-ID: <55B203C5.60005@treenet.co.nz>

On 24/07/2015 7:57 p.m., Posta Esterna wrote:
> Hi all....
> i'm very new about squid. I'm tryng to start a squid service (2.6
> stable) on a server linux centos 5... it has to connect to an AD server
> to authenticate users for internet access...

Yeesh. Please upgrade. Squid 2.6 was end-of-life'd June 2008.

It is also very unlikely that your Squid will operate both safely or
correctly on a lot of modern Internet traffic.

CentOS 5 itself is rather old too. CentOS 7 is what we are building and
testing current Squid with.

> 
> squid restart is not so good because in the cache.log file i see this error
> 
> (ntlm_auth) invalid option --  -
> 
> in my squid.conf file i'm trying to use this lines:
> 
> auth_param ntlm program ntlm_auth --helper-protocol=squid-2.5-ntlmssp

Use the full path to the helper and ensure that it is the *Samba* helper
binary being used. There is an identically named binary installed by
Squid which operates VERY differently and does not peform NTLM properly.

Other problems can and need to wait on that being fixed.

Amos



From squid3 at treenet.co.nz  Fri Jul 24 09:26:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 21:26:39 +1200
Subject: [squid-users] TCP_MISS in images
In-Reply-To: <55B12C49.8080308@gmail.com>
References: <55B1101B.8010408@vianetcon.com.ar>
 <55B12B26.2090605@treenet.co.nz> <55B12C49.8080308@gmail.com>
Message-ID: <55B204CF.2030007@treenet.co.nz>

On 24/07/2015 6:02 a.m., Yuri Voinov wrote:
> 
> 
> 
> 23.07.15 23:57, Amos Jeffries ?????:
>> On 24/07/2015 4:02 a.m., Ulises Nicolini wrote:
>>> Hello,
>>>
>>> I have a basic squid 3.5 configuration with
>>>
>>> maximum_object_size_in_memory 64 KB
>>> maximum_object_size 100000 KB
>>> minimum_object_size 512 bytes
>>>
>>> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 10080
>>> override-expire ignore-no-cache ignore-private
>>> refresh_pattern -i (/cgi-bin/)                  0       0%      0
>>> refresh_pattern .                                       0 20%     4320
>>>
> 
>> ignore-no-cache has no meaning for Squid-3.5.
> 
>> ignore-private does nothing on your images. It makes current Squid act
>> like must-revalidate was set on the response instead of "private".
> 
>> override-expire also does nothing on your images. As used above it makes
>> Squid act like "s-maxage=604800" was given instead of any Expires:
>> header or max-age=N / s-maxage=N Cache-Control values.
> 
> 
>>>
>>> cache_dir rock  /cache1/rock1 256  min-size=500 max-size=32767
>>> max-swap-rate=250 swap-timeout=350
>>> cache_dir diskd /cache2/diskd1 1000 16 256 min-size=32768
> max-size=1048576
>>> cache_dir diskd /cache2/diskd2 100000 16 256 min-size=1048576
>>>
>>>
>>> But when I test it against my webserver, using only one client PC, the
>>> only thing I get are TCP_MISSes of my images.
>>>
>>> 1437664284.339     11 192.168.2.103 TCP_MISS/200 132417 GET
>>> http://test-server.com/images/imagen3.jpg - HIER_DIRECT/192.168.2.10
>>> image/jpeg
>>> 1437664549.753      5 192.168.2.103 TCP_MISS/200 53933 GET
>>> http://test-server.com/images/imagen1.gif - HIER_DIRECT/192.168.2.10
>>> image/gif
>>> 1437665917.469     18 192.168.2.103 TCP_MISS/200 8319 GET
>>> http://test-server.com/images/icono.png - HIER_DIRECT/192.168.2.10
>>> image/png
>>>
>>> The response headers don't have Vary tags or any other that may impede
>>> caching
>>>
>>> Accept-Ranges    bytes
>>> Connection    close
>>> Content-Length    53644
>>> Content-Type    image/gif
>>> Date    Thu, 23 Jul 2015 15:56:07 GMT
>>> Etag    "e548d4-d18c-51b504b95dec0"
>>> Last-Modified    Mon, 20 Jul 2015 15:36:03 GMT
>>> Server    Apache/2.2.22 (EL)
>>>
> 
>> Your refresh pattern says to only cache these objects for +90% of their
>> current age, so long as that period is longer than 1 day (1440 mins) and
>> no more that 7 days (10080 mins).
> 
>> Which means;
>>  they are 3 days 20 mins 4 secs old right now (260404 secs).
>>  90% of that is 2 days 17 hrs 6 mins 3 secs (234363 secs).
> 
>> So the object "e548d4-d18c-51b504b95dec0" will stay in cache for the
>> next 2 days 17hrs etc.
> 
>> I notice though that the Content-Length size does not match any of the
>> logged transfer sizes. Which makes me wonder if the object is actually
>> varying despite the lack of Vary headers.
> 
> 
>>>
>>> Is it necessary a certain amount of requests of a single object to be
>>> cached (mem o disk) or am I facing some other problem here?
> 
>> Yes. Two requests. The first (a MISS) will add it to cache the second
>> and later should be HITs on the now cached object.
> 
>> BUT, only if you are not force-reloading the browser for your tests.
>> Force-reload instructs Squid it ignore its cached content and replace it
>> with another MISS.

> Amos, this behaviour depends from refresh_pattern and often can be
> ignore (with reload-into-ims, for example).

I know. You know that. The Q about number of requests is a big hint that
he is very new to this and may not be aware yet.

He has not configured anything that would affect it. So it is still
relevant to his Squid and still #1 most common mistake when testing
these things.

Amos


From squid3 at treenet.co.nz  Fri Jul 24 09:29:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jul 2015 21:29:32 +1200
Subject: [squid-users] ecap and https
In-Reply-To: <1437672834370-4672396.post@n4.nabble.com>
References: <1437672834370-4672396.post@n4.nabble.com>
Message-ID: <55B2057C.8000706@treenet.co.nz>

On 24/07/2015 5:33 a.m., HackXBack wrote:
> when we can use ecap with https contents ?

Yes, *if* the TLS part of HTTPS has been terminated by Squid.
ie. HTTPS reverse-proxy or SSL-bump interception.

Amos



From angelo.bruno at vigilfuoco.it  Fri Jul 24 11:24:17 2015
From: angelo.bruno at vigilfuoco.it (Posta Esterna)
Date: Fri, 24 Jul 2015 13:24:17 +0200
Subject: [squid-users] error windbind
In-Reply-To: <55B203C5.60005@treenet.co.nz>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
Message-ID: <55B22061.3060407@vigilfuoco.it>



Il 24/07/2015 11.22, Amos Jeffries ha scritto:
> On 24/07/2015 7:57 p.m., Posta Esterna wrote:
>> Hi all....
>> i'm very new about squid. I'm tryng to start a squid service (2.6
>> stable) on a server linux centos 5... it has to connect to an AD server
>> to authenticate users for internet access...
> Yeesh. Please upgrade. Squid 2.6 was end-of-life'd June 2008.
>
> It is also very unlikely that your Squid will operate both safely or
> correctly on a lot of modern Internet traffic.
>
> CentOS 5 itself is rather old too. CentOS 7 is what we are building and
> testing current Squid with.
>
>> squid restart is not so good because in the cache.log file i see this error
>>
>> (ntlm_auth) invalid option --  -
>>
>> in my squid.conf file i'm trying to use this lines:
>>
>> auth_param ntlm program ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> Use the full path to the helper and ensure that it is the *Samba* helper
> binary being used. There is an identically named binary installed by
> Squid which operates VERY differently and does not peform NTLM properly.
>
> Other problems can and need to wait on that being fixed.
>
> Amos
>
>

Thanx Amos,
Squid was the first problem... using /usr/lib/squid/ntlm_auth instead of 
/usr/bin/ntlm_auth

About upgrading unfortunatelly i have only an old DELL P4 (10 years 
old?) with 1GB of RAM... and for my fortune this is only the PROXY n?2 
(the backup).... The PROXY n?1 goes well with a version of KERIO Control...

I've still have problems....
it says:
....
[2015/07/24 12:06:41, 0] utils/ntlm_auth.c:get_windbind_domain(146)
   could not obtain windbind domain name!
.....

-- 
VCTI Ing. Angelo Bruno



From yvoinov at gmail.com  Fri Jul 24 11:36:10 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 17:36:10 +0600
Subject: [squid-users] ecap and https
In-Reply-To: <1437688425574-4672409.post@n4.nabble.com>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B129A2.4050609@gmail.com> <1437674166316-4672400.post@n4.nabble.com>
 <55B12B4C.8030907@gmail.com> <1437688425574-4672409.post@n4.nabble.com>
Message-ID: <55B2232A.6060509@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Well, and so what? What exactly your doing with this adapter?

24.07.15 3:53, HackXBack ?????:
> read the Documentation
>
> http://www.e-cap.org/Documentation
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672409.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsiMqAAoJENNXIZxhPexGrqUH/jASXQe9voHn/7ILXkn0T2RS
JyEQRAW7os8uiUEtCI7Pbe8cF64JnuEKxgWJOU6Sj1C0ssradNsreAVxThz3db1N
5Sj2L82tB0DNnatsPamt7l9ij3U2c2FEtGl1Fpo8XyH/x90GFPkM0zq79vIRw6zz
31JP4W7qOl8bTlC8Ob/2G4Mf7J+6+pDtt2/Ygul68kzLBKo0Ig47KJjyOfn4xvKV
ldRp1R36+Y2MoXryV5ZdMC578fAOybNOBsb7jfh9pdR0khnh7U/NsiTfO6wpwRHI
fVkjsYr5peNqYPyRaW31MBBRqNqY70uwaVVQTO5/ZK3WBTduZk7smEbyugF7SPE=
=m0iv
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/1f91d3e7/attachment.htm>

From yvoinov at gmail.com  Fri Jul 24 11:37:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 17:37:44 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437712385363.74b82703@Nodemailer>
References: <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
Message-ID: <55B22388.1010506@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also your can disable HSTS........ ;)

24.07.15 10:33, dan at getbusi.com ?????:
> Not to go off-topic here, but you folks are all SSL Bumping youtube.com / googlevideo.com in order to
do this caching, right?
>
>
>
>
> Want to make sure I?m not missing some secret way to make YouTube use
plain HTTP.
>
> On Fri, Jul 24, 2015 at 8:24 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
>> Hey Joe,
>> I understand the need for caching youtube but it might be not as
>> possible as in the past.
>> There was someone here on the list that offers a product that helps to
>> cache youtube videos but I do not know the secret behind it.
>> The partial content has special key in it and youtube kind of changed
>> couple things to allow variable bitrate.
>> All The Bests,
>> Eliezer
>> On 23/07/2015 19:00, joe wrote:
>>> my English not grait so be pattion tks
>>> hi i setup yt caching working perfect but i need to ask
>>> first squid 3.5.6
>>> i need to know how is yt detect and send partial video
>>> i have 2 computer same flash v. same firefox v. all identical exept one
>>> windowsxp another is win7
>>> i cache html5 on win7 yt send partial video on winxp send full video
>>> i put none to
>>> request_header_access Accept-Ranges deny all
>>> reply_header_access Accept-Ranges deny all
>>> request_header_replace Accept-Ranges none
>>> reply_header_replace Accept-Ranges none
>>>
>>> so Wat cause the partial video on win7 is it some header or ??
>>>
>>> you thing deny Accept-Ranges not working  ?
>>> or some other thing tks if any help
>>>
>>>
>>>
>>> --
>>> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsiOHAAoJENNXIZxhPexGC2YH/RPfHHw44yf0S4I1FjuiM9S7
S7hQ8Ec14g78og5YOZKudO03sxLkrLIgF9x8INUDWP57nq4KAwJbHBqtkIBH98kx
niI4bWg2dYxBxpPV1Tk5tuACPFKYzwiNE4MlwebLP5p1Dm8GvjWdA/vk5FGyaM+r
EUdglwUxou2aH/P+50tS3zarh+9qwEt+dbfr6NS15djpdO1citKe5CTRFL+7JZjG
WqYg1jvT9hFDWwemZCYjwhQgLB+s7r+YDaXEH03vdLKYFtmvU1bwAFCwkoLw/GsS
RmXuv5vsDJNudMKc96qml4GBn5cikaON1WCGIK87TFywJB8EKaNNVNFN60Uo42E=
=od9l
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/5d915df9/attachment.htm>

From yvoinov at gmail.com  Fri Jul 24 11:43:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 17:43:29 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437722737301-4672415.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com>
Message-ID: <55B224E1.2070003@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Either privacy - or caching. Now you haven't alternative. HTTPS without
bump can't be caching. Never.

Antispam reading your letters - you is annoying? Do you want to talk
about it? You strains that Squid will see SSL? Turn strip_query_terms on
and forget about it, the administrator will not see private data of your
users.

24.07.15 13:25, joe ?????:
> http bro i have i have 300 client and i like to staty standard not violating
> priv....that much as bluecoat thundercache use http the other use ssl they
> have to work in country that alow thim to use ssl
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672415.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsiThAAoJENNXIZxhPexGUDYH/0XojjRtLy0TAyD8USF6s03B
h53D6iQEdvLkagFxIUkLPgrkKy8RGTXBv5j8/NHJhb6xbXpd74G6HSWCOdGBwSsg
/o9huJJkT8o84xQyCCwhFfwA+hmjuaMljEZNDYQPj2rSSAUV4jtROiNphm550p7g
vVP8VuQNWupy5fRJXjzn+igUhOU5tCKP0tKkuV6AF2vLfkr1mn+HrldjnSqPPNV/
/E4TSHQB2GxqF7et16HcvYRQpZnRuf9Y0oLZGxPLo5/8m7g8NgyuYu6TzZrJ2L9T
P0CD6EsLYDQJRrfT24rzjpfdM7R6AgCegVGLd33QUBl2rn8LSFbaYXafcEYWxYw=
=bKyD
-----END PGP SIGNATURE-----



From chip_pop at hotmail.com  Fri Jul 24 12:01:51 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 05:01:51 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B224E1.2070003@gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
Message-ID: <1437739311277-4672427.post@n4.nabble.com>

http bro no ssl no https 
plain http any one know the way to force yt to use http
you can force google and yt to use http.. other site hard to do 

thre is a way i did not try it for facebook and some other site to cache
http insted of https but still u have to use ssl connect on main domain and
there is lots of rewriter to forwerd to http most of the sub domain ar https
but without connect safe to re write   https>http but they ar tunneled



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672427.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Jul 24 12:08:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 18:08:34 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437739311277-4672427.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com>
Message-ID: <55B22AC2.5060208@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Firefox and Chrome use HSTS for yt and some other hardcoded sites, like
twitter. This means force use TLS. From client side.

24.07.15 18:01, joe ?????:
> http bro no ssl no https 
> plain http any one know the way to force yt to use http
> you can force google and yt to use http.. other site hard to do
>
> thre is a way i did not try it for facebook and some other site to cache
> http insted of https but still u have to use ssl connect on main
domain and
> there is lots of rewriter to forwerd to http most of the sub domain ar
https
> but without connect safe to re write   https>http but they ar tunneled
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672427.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsirBAAoJENNXIZxhPexGA8QH/0/BYmMcG/0GyVyshuVVmo0O
KB1pYJyps8zLNppsVE/bMic3+9d66S9qfquRsp+kw3S4eM4AWBjfkUHl+alGVhX2
Hty9y6vw185vW3vo6Vned5xvrPufyvRkpMJf709bOIk+Ga1ge3g9FEerOlDsWUjt
K08sY45KZf3ugVSiYifKg1cZ0OTGJ4uPjomA+stizeq1GPnURuvzuW+F6HpHHP9B
5mP3Q8PirULFuN7YPDin9dO+d1ksnSwVswnLJh28syNpKl+XYfHUmKu+lyMDZXj0
dSzKdWPs0EXfwR1PGfdjOavto9NbvpVlbVihGq8slYe07AHUsH3QVmx1c2zft5Y=
=iS/T
-----END PGP SIGNATURE-----



From s.kirschner at afa-finanz.de  Fri Jul 24 12:09:19 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Fri, 24 Jul 2015 12:09:19 +0000
Subject: [squid-users] RE Peek and Splice error SSL_accept failed
Message-ID: <2F3AADF230295040BDC74C6F96094F3D021E961A@SRVEXAFA.verwaltung.afa-ag.loc>

Hi ,

I minimized the configuration a little bit(you could see it at the bottom of these message).

Also I still try to understand why these error happen , I increased the Debug level and saw that squid tried 48 times to peek but failed.
At the end It says that it got an "Hello", does it mean that squid received after 48 tries the "Hello" ?

If yes why it does need so many tries ?

-> Part of debug log <-
2015/07/24 11:05:42.866 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/07/24 11:05:42.866 kid1| bio.cc(120) read: FD 11 read 11 <= 11
2015/07/24 11:05:42.866 kid1| bio.cc(146) readAndBuffer: read 11 out of 11 bytes
2015/07/24 11:05:42.866 kid1| bio.cc(150) readAndBuffer: recorded 11 bytes of TLS client Hello
2015/07/24 11:05:42.866 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
2015/07/24 11:05:42.866 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.
.
.
.
2015/07/24 11:05:42.874 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
2015/07/24 11:05:42.874 kid1| bio.cc(120) read: FD 11 read 6 <= 11
2015/07/24 11:05:42.874 kid1| bio.cc(146) readAndBuffer: read 6 out of 11 bytes
2015/07/24 11:05:42.874 kid1| bio.cc(150) readAndBuffer: recorded 6 bytes of TLS client Hello
2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2040 from c-string, n=0)
2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2038 from c-string, n=13)
2015/07/24 11:05:42.875 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
2015/07/24 11:05:42.875 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.
2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2025 from c-string, n=4294967295)
2015/07/24 11:05:42.875 kid1| client_side.cc(4259) clientPeekAndSpliceSSL: I got hello. Start forwarding the request!!!

-> new configuration <-
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# Listening Ports
http_port 127.0.0.1:3120
http_port 192.168.1.104:3128 intercept
https_port 192.168.1.104:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=10MB cert=/etc/squid3/ssl_cert/myCA.pem

# some configuration options
cache_effective_user proxy
cache_effective_group proxy
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
pinger_enable on
pinger_program /lib/squid3/pinger
sslproxy_capath /etc/ssl/certs
sslcrtd_program /lib/squid3/ssl_crtd -s /var/squid/certs -M 4MB -b 2048

#ACLs
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl bypass ssl::server_name www.google.de

ssl_bump peek step1
ssl_bump splice bypass step2
ssl_bump bump all

# Debugging if needeed
debug_options all,6 6,0 16,0 18,0 19,0 20,0 32,0 47,0 79,0 90,0 92,0

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid3

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320


Mit freundlichen Gr??en / Best Regards

Sebastian


From jorgeley at gmail.com  Fri Jul 24 12:10:12 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 24 Jul 2015 09:10:12 -0300
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
 <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
Message-ID: <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>

please guys, help me.
Any suggestions?

2015-07-23 13:28 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> Befor all, thanks so so much for the answears!!!
> It's exist, I'm sure.
> This is my chroot structre:
> / (linux root)
> /etc
>      squid-3.5.6/
>                       bin/
>                            purge
>                            squidclient
>                       cache/
>                            (squid cache dirs generated by squid -z)
>                       etc/
>                             cachemgr.conf
>                             errorpage.css
>                             group
>                             gshadow
>                             hosts
>                             localtime
>                             mime.conf
>                             nsswitch.conf
>                             passwd
>                             resolv.conf
>                             shadow
>                             squid.conf
>                        lib64/
>                              (a lot of libs here, discovered with ldd
> command)
>                        libexec/
>                              basic_ncsa_auth
>                              diskd
>                              (other default squid libs)
>                        regras/
>                              (my acl files rules)
>                        sbin/
>                              squid
>                        share/
>                                errors/
>                                        (default dir squid errors)
>                                icons/
>                                        (default squid icons
>                                man/
>                                        (default man squid pages)
>                        usr/
>                               lib64/
>                                        (a lot of libs here, discovered
> with ldd command)
>                        var/
>                               logs/
>                                        (default squid logs)
>                               run/
>                                     squid.pid
>
> I did the command:
> chroot /etc/squid-3.5.6 /libexec/basic_ncsa_auth
> It runs, that's why I'm sure the chroot environment, unless for the
> ncsa_auth, is correct
>
> Any more suggestions?
>
> 2015-07-23 11:42 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 23/07/2015 11:23 p.m., Jorgeley Junior wrote:
>> >> Hi guys.
>> >> I have a RedHat 6.6 + squid 3.5.6 + basic_ncsa_auth + chroot and is
>> >> crashing only when I do an authentication.
>> >>
>> >> Here is the main confs:
>> >> auth_param basic program /libexec/basic_ncsa_auth /regras/usuarios
>> >> auth_param basic children 10 startup=0 idle=1
>> >> auth_param basic realm INTERNET-LOGIN NECESSARIO
>> >> ... (other confs) ...
>> >> acl usuarios            proxy_auth -i
>>  "/etc/squid-3.5.6/regras/usuarios"
>> >> ... (other confs) ...
>> >> chroot /etc/squid-3.5.6
>> >>
>> >> Here is what I find in the cache.log:
>> >> 2015/07/22 18:47:27.866 kid1| WARNING: no_suid: setuid(0): (1)
>> Operation
>> >> not permitted
>> >> 2015/07/22 18:48:01.735 kid1| ipcCreate: /libexec/basic_ncsa_auth: (2)
>> No
>> >> such file or directory
>> >> 2015/07/22 18:47:27.866 kid1| WARNING: basicauthenticator #Hlpr13818
>> exited
>> >>
>> >> What is the ipcCreate and why he is not findind the file?
>>
>> It is the code that runs the helper.
>>
>> The "/libexec/basic_ncsa_auth" does not exist as an exectuable binary
>> inside your chroot.
>>
>>
>> >>
>> > About the libs needed when I do the chroot, I have to copy them to the
>> > squid folder or I need to create the same structure like
>> > /squid-3.5.6/libs,  /squid-3.5.6/lib64?
>>
>> They must match the OS layout where Squid (and everything else that will
>> run in the chroot) expects to find them.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/90fc14e8/attachment.htm>

From chip_pop at hotmail.com  Fri Jul 24 12:22:39 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 05:22:39 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B22AC2.5060208@gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
Message-ID: <1437740559798-4672431.post@n4.nabble.com>

you can deny those protocol
reply_header_access alternate-protocol deny all 
so it wont push the client to use udp 443 or udp 80 
that wat they ar doing



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672431.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Jul 24 12:30:07 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 18:30:07 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437740559798-4672431.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com>
Message-ID: <55B22FCF.2060802@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Wrong. To block HSTS you need use

# Disable HSTS
reply_header_access Strict-Transport-Security deny all

alternate-protocol - this from another opera.

UDP/80 and UDP/443 - this about QUIC and SPDY protocol. It's nothing to
HSTS not.

Learn more ;)

24.07.15 18:22, joe ?????:
> you can deny those protocol
> reply_header_access alternate-protocol deny all
> so it wont push the client to use udp 443 or udp 80
> that wat they ar doing
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672431.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsi/PAAoJENNXIZxhPexGuccH/17MPWDEAMSx1viebAWd4x94
YQ7Ir3ywQgeRLQb2DvQcNv9se4QKSujjxzhGzRumOhNVGL9fVCHqG2Z9SgmbdE0+
tWhsJE9wWeWRD4O3upAxnE0wzDDu88xlOgk+VfgBi/oqkaWXWUKO/HI3IERTl3ia
W83t4zRMZ58L5e+NU2E664Ix3VMA5J9o5Rz1CuIx30HpQ55QadMcwZ+qTACR+Wa7
XFBvbvN8D207vI/0TZ7mSUTuYaUKUBn54FkX1cb7HU+O/U6eZgTz6iQmQjJGG5OC
4s1TnRSDdjQEp14npJld3GQt/EDIl6bCNjxoKpq+GuPSr6pdI0oCQvd9NvCP+6I=
=iJu+
-----END PGP SIGNATURE-----



From marcus.kool at urlfilterdb.com  Fri Jul 24 12:33:13 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 24 Jul 2015 09:33:13 -0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>
Message-ID: <55B23089.9040108@urlfilterdb.com>



On 07/24/2015 03:25 AM, Jens Offenbach wrote:
> I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
>
>
> Gesendet: Freitag, 24. Juli 2015 um 07:54 Uhr
> Von: "Jens Offenbach" <wolle5050 at gmx.de>
> An: "Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.
>
> The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
> http://wikisend.com/download/293856/squid.strace2
>
> @Marcus:
> Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?

I screwed up earlier since the maximum_object_size was too low for the test with a 1 GB file and did a new test.

The system has 64 GB memory and for sure the entire file is in the file system cache.  The disk system is HW RAID-1 with 1 GB cache.
The OS is Linux 3.10, CentOS 7 latest patches.

New test:
test system: 1 CPU with 4 cores/8 threads @ 3.7 GHz, 64 GB memory, AUFS, 1 Gbit pipe, 500 mbit guaranteed

with Squid 3.4.12 :
1st download starts with 90 MB/sec and halfway drops to 30 MB/sec.  My guess is that the file system cache got stressed and slowed things down.
2nd cached download with 190 MB/sec sustained and 120% CPU time.

With Squid 3.5.6 :
1st download starts with 90 MB/sec sustained and 80% CPU time.
2nd cached download with 190 MB/sec sustained and 120% CPU time.

As a comparison, I did "dd if=test of=test2 bs=4k" which uses 100% CPU time and has a throughput of 1200 MB/sec.
With bs=16k the throughput is 1300 MB/sec and with bs=64k the throughput is 1400 MB/sec.

relevant parameters :
read_ahead_gap 64 KB
cache_mem 256 MB
maximum_object_size_in_memory 8 MB
maximum_object_size 8000 MB
cache_dir aufs /local/squid34/cache 10000 32 256
cache_swap_low 92
cache_swap_high 93
# also ICAP daemon and URL rewriter configured
debug_options ALL,1 93,3 61,9

configure options:
'--prefix=/local/squid35' '--disable-ipv6' '--enable-fd-config' '--with-maxfd=3200' '--enable-async-io=64' '--enable-storeio=aufs' '--with-pthreads' '--enable-removal-policies=lru' 
'--disable-auto-locale' '--enable-default-err-language=English' '--enable-err-languages=Dutch English Portuguese' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' 
'--enable-cachemgr-hostname=localhost' '--enable-cache-digests' '--enable-follow-x-forwarded-for' '--enable-xmalloc-statistics' '--disable-hostname-checks' '--enable-epoll' '--enable-icap-client' 
'--enable-useragent-log' '--enable-referer-log' '--enable-stacktraces' '--enable-underscores' '--disable-icmp' '--mandir=/usr/local/share' 'CC=gcc' 'CFLAGS=-g -O2 -Wall -march=native' 'CXXFLAGS=-g -O2 
-Wall -march=native' --enable-ltdl-convenience

As you can see the cache_mem is small, If Amos finds it useful, I can do another test with a larger cache_mem.

Jens, since all your tests have a drop to 500 KB/sec I think the cause is somewhere is the configuration (Squid and/or OS).

Marcus


> @Amos, Eliezer
> Is someone able to reproduce the disk caching effect?
>
> Regards,
> Jens
>
>
> Gesendet: Donnerstag, 23. Juli 2015 um 20:08 Uhr
> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
> The strace output shows this loop:
>
> Squid reads 16K-1 bytes from FD 13 webserver
> Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
> Squid writes 4 times 4K to FD 12 browser
>
> But this loop does not explain the 100% CPU usage...
>
> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?
>
> I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
> The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
> The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
> I never looked at Squid CPU usage and do not know what is reasonable but it feels high.
>
> With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
> Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
> This is a possible explanation for the fact that you see the performance going down and up.
> Can you do the same test on the host (i.e. not inside a VM).
>
> Marcus
>
>
>
> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
>> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
>> Here is the download link:
>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]
>> I hope it can help you.
>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>> I am not sure if it is relevant, maybe it is:
>>
>> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
>> Squid sends the object to the ICAP server in small chunks of varying sizes:
>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>> Note that the interval of receiving the chunks is 1/1000th of a second.
>> It seems that Squid forwards the object to the ICAP server every time it receives
>> one or a few TCP packets.
>>
>> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
>> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>>
>> Amos, is there a debug setting that can confirm/reject this suspicion?
>>
>> Marcus
>>
>>
>> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>>> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>>>
>>> I cleared the cache:
>>> rm -rf /var/cache/squid3/*
>>> squid -z
>>> squid
>>> http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>>>
>>> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
>> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>>>
>>> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>>>
>>> For further investigations I give you my build properties:
>>> squid -v
>>> Squid Cache: Version 3.5.6
>>> Service Name: squid
>>> configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
>> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
>> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
>> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
>> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
>> s
>> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
>> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>>
>>>
>>> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>>> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>>> An: squid-users at lists.squid-cache.org
>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>>>> Hey Jens,
>>>>
>>>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>>>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>>>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>>>> download speed is more then doubled to 4.5 MBps(36Mbps).
>>>> I have not tried it with AUFS yet.
>>>
>>>
>>> And I must admit that AUFS beats rock cache with speed.
>>> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>>> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>>> results more then doubles 3 the speed rock gave with default settings.
>>>
>>> So about 15MBps which is 120Mbps.
>>> I do not seem to feel what Jens feels but the 100% CPU might be because
>>> of spinning disk hangs while reading the file from disk.
>>>
>>> Amos, I remember that there were some suggestions how to tune large rock.
>>> Any hints?
>>> I can test it and make it a suggestion for big files.
>>>
>>> Eliezer
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
>>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From chip_pop at hotmail.com  Fri Jul 24 12:33:46 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 05:33:46 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B22FCF.2060802@gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
Message-ID: <1437741226992-4672434.post@n4.nabble.com>

i dont see Strict-Transport-Security  in my log header
only alternate-protocol
can you post an example link pls



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672434.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Fri Jul 24 12:36:18 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Jul 2015 06:36:18 -0600
Subject: [squid-users] RE Peek and Splice error SSL_accept failed
In-Reply-To: <2F3AADF230295040BDC74C6F96094F3D021E961A@SRVEXAFA.verwaltung.afa-ag.loc>
References: <2F3AADF230295040BDC74C6F96094F3D021E961A@SRVEXAFA.verwaltung.afa-ag.loc>
Message-ID: <1437741378.7042.1.camel@JamesiMac>

On Fri, 2015-07-24 at 12:09 +0000, Sebastian Kirschner wrote:

> Hi ,
> 
> I minimized the configuration a little bit(you could see it at the bottom of these message).
> 
> Also I still try to understand why these error happen , I increased the Debug level and saw that squid tried 48 times to peek but failed.
> At the end It says that it got an "Hello", does it mean that squid received after 48 tries the "Hello" ?
> 
> If yes why it does need so many tries ?
> 
> -> Part of debug log <-
> 2015/07/24 11:05:42.866 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
> 2015/07/24 11:05:42.866 kid1| bio.cc(120) read: FD 11 read 11 <= 11
> 2015/07/24 11:05:42.866 kid1| bio.cc(146) readAndBuffer: read 11 out of 11 bytes
> 2015/07/24 11:05:42.866 kid1| bio.cc(150) readAndBuffer: recorded 11 bytes of TLS client Hello
> 2015/07/24 11:05:42.866 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
> 2015/07/24 11:05:42.866 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.
> .
> .
> .
> 2015/07/24 11:05:42.874 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
> 2015/07/24 11:05:42.874 kid1| bio.cc(120) read: FD 11 read 6 <= 11
> 2015/07/24 11:05:42.874 kid1| bio.cc(146) readAndBuffer: read 6 out of 11 bytes
> 2015/07/24 11:05:42.874 kid1| bio.cc(150) readAndBuffer: recorded 6 bytes of TLS client Hello
> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2040 from c-string, n=0)
> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2038 from c-string, n=13)
> 2015/07/24 11:05:42.875 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
> 2015/07/24 11:05:42.875 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.
> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2025 from c-string, n=4294967295)
> 2015/07/24 11:05:42.875 kid1| client_side.cc(4259) clientPeekAndSpliceSSL: I got hello. Start forwarding the request!!!
> 
> -> new configuration <-
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> 
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> # Listening Ports
> http_port 127.0.0.1:3120
> http_port 192.168.1.104:3128 intercept
> https_port 192.168.1.104:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=10MB cert=/etc/squid3/ssl_cert/myCA.pem
> 
> # some configuration options
> cache_effective_user proxy
> cache_effective_group proxy
> access_log /var/squid/logs/access.log
> cache_log /var/squid/logs/cache.log
> pinger_enable on
> pinger_program /lib/squid3/pinger
> sslproxy_capath /etc/ssl/certs
> sslcrtd_program /lib/squid3/ssl_crtd -s /var/squid/certs -M 4MB -b 2048
> 
> #ACLs
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> acl bypass ssl::server_name www.google.de
> 
> ssl_bump peek step1
> ssl_bump splice bypass step2
> ssl_bump bump all
> 
> # Debugging if needeed
> debug_options all,6 6,0 16,0 18,0 19,0 20,0 32,0 47,0 79,0 90,0 92,0
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid3
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> 
> Mit freundlichen Gr??en / Best Regards
> 
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Is that all sites or just a few special sites?

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/587c83c6/attachment.htm>

From yvoinov at gmail.com  Fri Jul 24 12:37:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 18:37:39 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437740559798-4672431.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com>
Message-ID: <55B23193.3080405@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
BTW, it you are concern about user's privacy, you must not block neither
QUIC/SPDY nor HSTS. This all about user's privacy.

But in this case forget about caching yt or something. Completely.

24.07.15 18:22, joe ?????:
> you can deny those protocol
> reply_header_access alternate-protocol deny all
> so it wont push the client to use udp 443 or udp 80
> that wat they ar doing
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672431.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsjGTAAoJENNXIZxhPexGU2IH/0Llfdo59NMNjApxGGHmMZJ/
9vsSTiSUuVu/ivNvu3zKEaEUCjm6RGsU9upH8fLwsedZrdoxXK1gPKCYYNgKW+pe
3szl2gul1eM+ektmhXlkQV+4vHGbsXudunMI6M6ukdB34UiPO3f6aMQnCg+fJ771
BTZSYVMh2veoO8EQcUBMUJHjZtk1Hh5s+h/L7gcD1JlWRU1IgRrKOTf1t0B3G41x
kNjQBJOyh08s+uubd3QiQZceTgwqmaQM5hMmxz7p2PrFFWkZayshthFVrzK+nrcP
ecS98xU0ONxKnKhyG+RCrs3yATGDsL4hlzsOD54lTnzso6kIqcJxDubRNUvIeXw=
=58ge
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Fri Jul 24 12:38:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 18:38:27 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437741226992-4672434.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com>
Message-ID: <55B231C3.1030005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security

24.07.15 18:33, joe ?????:
> i dont see Strict-Transport-Security  in my log header
> only alternate-protocol
> can you post an example link pls
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672434.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsjHDAAoJENNXIZxhPexGi/cH/2hgze8gbSSwLvxvffVK0AUX
FMLZSxJGvb+D9WBJDdmAJeCrto57uqfdWklIH5XUh8mQcStzGGa4ndRrQZ0o41Gq
DrVNJd+NK+Jxksyi58JnnxsBopp023ytxkFjLqrWHfp6jgOvbNaGlfo/vk366LOg
obadFOTXbVZDSUvXIicsYEV5k5HpIM//XQbvV+ysyoFI7Ka64pgq6wXGrkXk0i+m
ZfpT6TXmibMxnoaxNhWsZoP1X9myNwF/CQDN1XnqU+cbg9LJyVSB8VKVUGhXcVqy
ltPoFzpevTxrztX8ZV3lKCyqvIgvaoAzLQaMrVGStxJEC/8wpeFCVZQRMUYN7bg=
=zPud
-----END PGP SIGNATURE-----



From kevin at rentec.com  Fri Jul 24 12:43:34 2015
From: kevin at rentec.com (Kevin Kretz)
Date: Fri, 24 Jul 2015 08:43:34 -0400 (EDT)
Subject: [squid-users] log source port from squid server?
In-Reply-To: <1756445983.2440412.1437741561792.JavaMail.zimbra@rentec.com>
Message-ID: <99770680.2442109.1437741814753.JavaMail.zimbra@rentec.com>

Hi,

We're working on correlating our squid logs with other logs upstream in our network.  We'd like to be able to identify a proxied request by network information from squid's log.  Currently we have the squid server IP address, the destination server's IP address and the destination server's listening port.  

>From the documentation and reading back through this list's archive, I don't see a format code for squid server source port.  Has there ever been interest in this?  


thanks

Kevin


From chip_pop at hotmail.com  Fri Jul 24 12:43:47 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 05:43:47 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B231C3.1030005@gmail.com>
References: <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
Message-ID: <1437741827420-4672439.post@n4.nabble.com>

as long as i dont use ssl in my cache man in the midle im safe gov..wise
all other i can do



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672439.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jorgeley at gmail.com  Fri Jul 24 12:44:11 2015
From: jorgeley at gmail.com (Jorgeley)
Date: Fri, 24 Jul 2015 05:44:11 -0700 (PDT)
Subject: [squid-users] Squid-3.5.6 + Chroot + Authentication
Message-ID: <1437741851046-4672440.post@n4.nabble.com>

Hi guys.
I have a squid 3.5.6 + basic_ncsa_auth + chroot and is crashing only when I
do an authentication.

Here is the main confs:
*auth_param basic program /libexec/basic_ncsa_auth /regras/usuarios
auth_param basic children 10 startup=0 idle=1
auth_param basic realm INTERNET-LOGIN NECESSARIO
... (other confs) ...
acl usuarios            proxy_auth -i   "/etc/squid-3.5.6/regras/usuarios"
... (other confs) ...
chroot /etc/squid-3.5.6*

This is my chroot structre:
*/ (linux root)
/etc
     squid-3.5.6/
                      bin/
                           purge
                           squidclient
                      cache/
                           (squid cache dirs generated by squid -z)
                      etc/
                            cachemgr.conf
                            errorpage.css
                            group
                            gshadow
                            hosts
                            localtime
                            mime.conf
                            nsswitch.conf
                            passwd
                            resolv.conf
                            shadow
                            squid.conf
                       lib64/
                             (a lot of libs here, discovered with ldd
command)
                       libexec/
                             basic_ncsa_auth
                             diskd
                             (other default squid libs)
                       regras/
                             (my acl files rules)
                       sbin/
                             squid
                       share/
                               errors/
                                       (default dir squid errors)
                               icons/
                                       (default squid icons
                               man/
                                       (default man squid pages)
                       usr/
                              lib64/
                                       (a lot of libs here, discovered with
ldd command)
                       var/
                              logs/
                                       (default squid logs)
                              run/
                                    squid.pid*

I did the command:
chroot /etc/squid-3.5.6 /libexec/basic_ncsa_auth
It runs, that's why I'm sure the chroot environment, unless for the
ncsa_auth, is correct


Here is what I find in the cache.log:
*2015/07/22 18:47:27.866 kid1| WARNING: no_suid: setuid(0): (1) Operation
not permitted
2015/07/22 18:48:01.735 kid1| ipcCreate: /libexec/basic_ncsa_auth: (2) No
such file or directory
2015/07/22 18:47:27.866 kid1| WARNING: basicauthenticator #Hlpr13818 exited*

What is the ipcCreate and why he is not findind the file?
Any ideas? Thanks since now.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-6-Chroot-Authentication-tp4672440.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Fri Jul 24 12:49:13 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 24 Jul 2015 13:49:13 +0100
Subject: [squid-users] log source port from squid server?
In-Reply-To: <99770680.2442109.1437741814753.JavaMail.zimbra@rentec.com>
References: <99770680.2442109.1437741814753.JavaMail.zimbra@rentec.com>
Message-ID: <201507241349.13408.Antony.Stone@squid.open.source.it>

On Friday 24 Jul 2015 at 13:43, Kevin Kretz wrote:

> From the documentation and reading back through this list's archive, I
> don't see a format code for squid server source port.  Has there ever been
> interest in this?

Does http://www.squid-cache.org/Doc/config/logformat/ help?


Antony.

-- 
Perfection in design is achieved not when there is nothing left to add, but 
rather when there is nothing left to take away.

 - Antoine de Saint-Exupery

                                                   Please reply to the list;
                                                         please *don't* CC me.


From s.kirschner at afa-finanz.de  Fri Jul 24 12:55:05 2015
From: s.kirschner at afa-finanz.de (Sebastian Kirschner)
Date: Fri, 24 Jul 2015 12:55:05 +0000
Subject: [squid-users] RE Peek and Splice error SSL_accept failed
Message-ID: <2F3AADF230295040BDC74C6F96094F3D021E964B@SRVEXAFA.verwaltung.afa-ag.loc>

>Is that all sites or just a few special sites?

>James

I tested a few sites like google , youtube , sparkasse, sparklabs, all with the same issue.


Mit freundlichen Gr??en / Best Regards

Sebastian 


From squid3 at treenet.co.nz  Fri Jul 24 13:10:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 01:10:15 +1200
Subject: [squid-users] SIGTERM SIGKILL causes issues with squid shutdown
 during reboot
In-Reply-To: <CANLNtGS8k57QFHi-vWvY4cjNNkCgbZBTjS1iUCZAP1Li0=dUTg@mail.gmail.com>
References: <CANLNtGS8k57QFHi-vWvY4cjNNkCgbZBTjS1iUCZAP1Li0=dUTg@mail.gmail.com>
Message-ID: <55B23937.2040809@treenet.co.nz>

Response inline.

On 24/07/2015 8:21 a.m., Stanford Prescott wrote:
> After bumping Squid from 3.4.x to 3.5.x in our implementation of Squid in
> the Smoothwall Express v3.1 firewall distro we have begun to have
> complaints from our users about "erratic behavior" of Squid shutting down
> during reboots or network drops causing reboots.
> 
> It appears that squid (v3.5.[5-6]) does not respond well to SIGTERM during
> system shutdown; the cache index almost invariably needs to be rebuilt on
> next boot. It is suggested that we use squid to shut squid down. While
> using squid to stop the squid daemon is very doable, this requirement runs
> contrary to the longstanding, traditional UNIX method of "SIGTERM, pause,
> SIGKILL".

Squid is designed to operate in UNIX style. It contains its own daemon
manager, and a worker process.

When SIGTERM is received by the manager process, it relays that to the
worker which begins shutdown immediately but with shutdown_lifetime
grace period for active clients to finish. A second SIGTERM will act
like shutdown_lifetime having expired, everything gets closed immediately.

SIGKILL causes the OS init system to erase the Squid processes from
system memory, aborts all the open sockets and disk I/O, etc. The
nuclear option as some often called it.

This causes issues with systemd, openrc, or upstart where the init
system insists on being a daemon manager itself. IIRC there was an
excellent tutorial somewhere (by DJB?) that explained in great detail
why having a daemon manager to operate other daemon managers was a
terrible idea. Squid hits pretty much all the relevant nasty side
effects as each piece of the system makes bad assumptions or gets
confused about other bits activities.

Meanwhile if just left alone the worker is still trying to gracefully
close client transactions for shutdown_lifetime (30sec default). Then
abort those and save the final cache index to a new swap.state file on disk.


> 
> During normal system operation, squid *ought* to be able to take as much
> time as it wants to shut down. But it still shouldn't take more than 10-20
> seconds; 'shutdown' is a command, not a request to be honored at squid's
> leisure. After all, the CPU could be on fire....
> 
> This raises a few questions that are intended to foster fresh discussion,
> not to re-hash old arguments. They are really more rhetorical in nature;
> the goal is to find the root cause of the problem.
> 
>    - Why do these latest versions of Squid 3 behave oddly in this respect?

systemd is becoming popular. See above. Impatience and incorrect init
scripts using SIGKIL instead of SIGTERM does seem to be the main pathway
to having issues.

Squid-3 has become a SMP multi-process thing. Which increases the number
of workers, and things they are still doing during shtutdown.

Squid-3.1 & 3.2 added various checksums to swap.state format protecting
against file corruption propigating into the memory index of next
started Squid process.

>    - What is it about shutting squid down that corrupts the cache index?

"corruption" in the cache index means the swap.state file for each cache
has not been completely written. On next load the file checksum does not
match the contents checksum. So reverts to a relatively slow scan of the
disk to see what is there.

SIGKILL being used during the time that index is being written to disk
FD will cause it nearly 100% of the time. Being used before old
swap.state is replaced will lead to a lot of SWAPFAIL as objects on disk
today dont match what was on disk when Squid restarted a week ago.

What corruption happens also varies between disk controllers. Some
controllers complete already scheduled writes. Some drop them. SIGKILL
may be relevant in that decision too. The ones that complete all
scheduled writes and close the file hit corruption less often, or less
badly.


>    - Does it take more than a few seconds to write the index to disk?

No. But it can take days for active clients to finish off long-polling
HTTP requests or CONNECT tunnels. shutdown_lifetime is what they are
allowed. IME, Squid workers usually completely exit within 5 seconds of
that grace period being over. It takes under a ms to close a socket, but
many tens of thousands of sockets may be open on a busy proxy.


>    - Does squid use the very slow 'writethrough' method instead of trusting
>    the Linux disk cache to properly save the cache?

Squid is generating new content and writing it to a file with write(2)
API. That content is anything up to 68MB in size *per cache_dir*. How
fast can your disk save that much data when its passed in ~100 byte IO
chunks?
 Whatever OS filesystem does to optimize I/O happens on top of that
process-level activity.


As mentioned above SIGKILL also does not shutdown the process (that
would be SIGTERM). Just drops it all on the floor. Using SIGKILL on a
process currently writing to disk has varying results from the disk
driver layers - usually not good.

SIGTERM will almost guarantee the files complete writing and things get
gracefully resolved.


If you haven't guessed already IMO you should at worst use SIGTERM twice
on Squid instead of SIGKILL even once. If Squid is actually operational
a second SIGTERM will have the same effect as shutdown_lifetime having
finished early.

Sadly systemd and init scripts can be rather friendly with SIGKILL.
Which can be problematic.


>    - Should squid write the cache index to disk more often?

The cache index is extremely volatile. Every single request through it
adds or removes an entry. Which is a lot of disk overhead on a very busy
proxy.

The compromise is to write it only once on shutdown/restart when all
updates have been finished. Or on "-k rotate" (SIGHUP) when explicitly
asked to auto-save all state for log file rotations.


>    - Should squid track its own 'dirty pages' in its in-core index to
>    reduce the time it takes to write the index?

There is not much to track. The "pages" as it were are little marker
records a few hundred bytes stating what HTTP message is contained in
any given on-disk file. The memory copy gets erased as soon as the disk
controller has been asked to delete the file. The running worker does
not track whether the delete worked or not, just that all index entries
are supposedly valid HTTP objects. Whatever files get left undeleted on
disk can be overwritten with new content. [enter a few race condition bugs].

On shutdown the (up to 2^27 x 512 bytes) set of then-valid records are
dumped to disk in a new swap.state.


>    - Should squid implement a journal (akin to EXT/ReiserFS and others) so
>    the on-disk index structure is always OK?

Been tried. store_log is the journal. You can turn it on if you have any
need for external software to track what Squid cache contains in near
real-time.

Overall it slows Squid down and it is not useful to remember objects
that were saved X days ago and deleted a few seconds later.

Wont protect against swap.state corruption anyway, since the journal/log
file would get truncated/corrupted in the same ways as swap.state.


>    - Is the problem related to clients actively receiving web pages from
>    squid?

Indirectly. They delay added by waiting for them increases the
impatience of external daemon managers and admins chance of using
SIGKILL on the worker.


>    - Could squid's signal handling be adjusted to treat those clients more
>    harshly? That is, terminate the transfers early because squid shutting down
>    is much the same as the network interface going down.

It could. I have actually just applied a patch to Squid-4 that makes
Squid abort idle client connections on teh first SIGTERM and more
cleanly close *all* client FDs at the end of the shutdown_lifetime (or
second SIGTERM) before it uses abort() on whats left. That will probably
be in 3.5.7 as well.

Hopefully that will result in less SWAPFAIL corruption issues in future
for some people. It may or may not help with swap.fail corruption since
thet is a step later.

Long term plan is to handle shutdown in such a way that non-busy Squid
may even exit before shutdown_lifetime is over. Which should definitely
include proper swap.state closure.



>    - Is the only viable solution to use squid to stop the daemon? Does
>    'squid -k shutdown' exit only after the daemon is dead? The squid man page
>    indicates that it sends the shutdown 'signal' but does not await a reply;
>    thus a potentially lengthy pause in system shutdown may be required anyway.

The man page is correct. "squid -k shutdown" sends a SIGTERM signal to
the process listed in squids PID file.
 NP: any other UNIX style process could happily do the same to control
Squid.

The -k process itself exits immediately when that signal is sent.

The running squid daemon manager (or coordinator) process handles that
signal as mentioned above.


>    - Is pausing the system shutdown for 10-15 seconds the only really
>    viable shutdown solution?

In current Squid releases yes. The alternative is to risk the corruption
problems.

>    - Or is it best to delete and rebuild the cache index on every system
>    startup?

No. Just configure shutdown_lifetime to a few seconds, and require the
init scripts system to wait that long plus 2-3 more sec for the whole
bunch of Squid processes to complete normally. Use SIGTERM if really
necessary to speed things up.

There are a occasional exceptions, you may be one, but for most
installations it seems to still work fine that way.

HTH
Amos


From kevin at rentec.com  Fri Jul 24 13:34:33 2015
From: kevin at rentec.com (Kevin Kretz)
Date: Fri, 24 Jul 2015 09:34:33 -0400 (EDT)
Subject: [squid-users] log source port from squid server?
In-Reply-To: <201507241349.13408.Antony.Stone@squid.open.source.it>
References: <99770680.2442109.1437741814753.JavaMail.zimbra@rentec.com>
 <201507241349.13408.Antony.Stone@squid.open.source.it>
Message-ID: <457484461.2450263.1437744873213.JavaMail.zimbra@rentec.com>



----- Original Message -----
From: "Antony Stone" <Antony.Stone at squid.open.source.it>
To: squid-users at lists.squid-cache.org
Sent: Friday, July 24, 2015 8:49:13 AM
Subject: Re: [squid-users] log source port from squid server?

> Does http://www.squid-cache.org/Doc/config/logformat/ help?


I saw that page earlier but misunderstood what this meant:

<lp     Local port number of the last server or peer connection

Looks like that does what I want.  Thank you for the quick assistance.


From kevin at rentec.com  Fri Jul 24 13:34:33 2015
From: kevin at rentec.com (Kevin Kretz)
Date: Fri, 24 Jul 2015 09:34:33 -0400 (EDT)
Subject: [squid-users] log source port from squid server?
In-Reply-To: <201507241349.13408.Antony.Stone@squid.open.source.it>
References: <99770680.2442109.1437741814753.JavaMail.zimbra@rentec.com>
 <201507241349.13408.Antony.Stone@squid.open.source.it>
Message-ID: <457484461.2450263.1437744873213.JavaMail.zimbra@rentec.com>



----- Original Message -----
From: "Antony Stone" <Antony.Stone at squid.open.source.it>
To: squid-users at lists.squid-cache.org
Sent: Friday, July 24, 2015 8:49:13 AM
Subject: Re: [squid-users] log source port from squid server?

> Does http://www.squid-cache.org/Doc/config/logformat/ help?


I saw that page earlier but misunderstood what this meant:

<lp     Local port number of the last server or peer connection

Looks like that does what I want.  Thank you for the quick assistance.


From chip_pop at hotmail.com  Fri Jul 24 13:43:15 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 06:43:15 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437667243296-4672389.post@n4.nabble.com>
References: <1437667243296-4672389.post@n4.nabble.com>
Message-ID: <1437745395229-4672445.post@n4.nabble.com>

squid v 3.5.6
i dont think range_offset_limit none google
or range_offset_limit -1 google
or
request_header_access Accept-Ranges deny all
reply_header_access Accept-Ranges deny all
request_header_replace Accept-Ranges none
reply_header_replace Accept-Ranges none
ar working any one try v3.5.6 and see if they do pls i keep getting partial
file on yt
is there somthing i shuld do to make thim active ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672445.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul 24 13:50:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 01:50:11 +1200
Subject: [squid-users] error windbind
In-Reply-To: <55B22061.3060407@vigilfuoco.it>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
 <55B22061.3060407@vigilfuoco.it>
Message-ID: <55B24293.1090604@treenet.co.nz>

On 24/07/2015 11:24 p.m., Posta Esterna wrote:
> 
> Thanx Amos,
> Squid was the first problem... using /usr/lib/squid/ntlm_auth instead of
> /usr/bin/ntlm_auth
> 
> About upgrading unfortunatelly i have only an old DELL P4 (10 years
> old?) with 1GB of RAM... and for my fortune this is only the PROXY n?2
> (the backup).... The PROXY n?1 goes well with a version of KERIO Control...
> 

FWIW, I run a few customer sites with older hardware than that. Of
course those dont service much traffic. But Squid-3.5 with small cache
is not even taxing the hardware.

Kind of my specialty hobby now. Running Squid on recycled and low-spec
hardware :-)


> I've still have problems....
> it says:
> ....
> [2015/07/24 12:06:41, 0] utils/ntlm_auth.c:get_windbind_domain(146)
>   could not obtain windbind domain name!
> .....
> 

That seems to be an internal winbind / Samba problem.

Once you have the right helper Squids part is reduced to ferrying the
HTTP Auth header contents to and from it.


PS. Unless you are fighting with similarly ancient Windows 2K boxen I
suggest looking into Negotiate/Kerberos (squid_kerb_auth etc should work
okay with 2.6) instead of, or as well as, NTLM. Sometimes its a bit more
painful to setup, but much more resource efficient _and_ secure.

Amos



From squid3 at treenet.co.nz  Fri Jul 24 14:02:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 02:02:06 +1200
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
 <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
 <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>
Message-ID: <55B2455E.9090404@treenet.co.nz>

On 25/07/2015 12:10 a.m., Jorgeley Junior wrote:
> please guys, help me.
> Any suggestions?
> 

Squid is not generally run in a chroot. The master / coordinator daemon
manager process requires root access for several things and spawns
workers that are dropped automatically to highly restricted access
anyway. You already found out how big the dependency pool of libraries is.

I guess what I'm getting at is that this is a rarely tested situation.

To complicate matters there are three different combinations of "chroot"
that Squid can run.

* External chroot. Where you enter the chroot before starting Squid and
it thinks the chroot content is the whole system.

* configured chroot. Where you configure Squid master process to chroot
its low-privilege workers with the squid.conf chroot directive.

* Linux containers. Similar to the first, but you dont have to copy
files into a separate chroot area. Just assign visibility/access to the
OS areas.


The error is pretty clear though. The problem is that something is
unable to load a file during helper startup.
Either Squid is unable to read/open/see the helper binary file itself.
Or the helper is unable to open a file it needs to operate.

"ipcCreate:" is a big hint that its Squid not finding the helper binary
named.

So is Squid being run from inside the chroot, or using "chroot"
directive in squid.conf?


Amos




From jorgeley at gmail.com  Fri Jul 24 14:22:58 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 24 Jul 2015 11:22:58 -0300
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <55B2455E.9090404@treenet.co.nz>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
 <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
 <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>
 <55B2455E.9090404@treenet.co.nz>
Message-ID: <CAMeoTH=02cRGiVjZk=cyFtgjv0GU7aJOLidPdx_SLYBeD6Dx5Q@mail.gmail.com>

Thank you so much for the help.
So, I use the directive 'chroot' in the squid.conf.
I start squid this way:
cd /etc/squid-3.5.6
sbin/squid
and it starts normally, but when I open the client browser and do an
authentication it logs the errors and don't authenticate, but the squid
doesn't stop running, just it logs the error and do not authenticate.
How I told you before, if I do: chroot /etc/squid-3.5.6
libexec/basic_ncsa_auth it runs, that's why I'm sure that basic_ncsa_auth
it's running correctly, I suspect maybe this IPCcreate run as another user
that cannot access the basic_ncsa_auth or maybe IPCcreate its located in a
directory that cannot see the libexec/basice_ncsa relative path
That's a weird scenario.

2015-07-24 11:02 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 25/07/2015 12:10 a.m., Jorgeley Junior wrote:
> > please guys, help me.
> > Any suggestions?
> >
>
> Squid is not generally run in a chroot. The master / coordinator daemon
> manager process requires root access for several things and spawns
> workers that are dropped automatically to highly restricted access
> anyway. You already found out how big the dependency pool of libraries is.
>
> I guess what I'm getting at is that this is a rarely tested situation.
>
> To complicate matters there are three different combinations of "chroot"
> that Squid can run.
>
> * External chroot. Where you enter the chroot before starting Squid and
> it thinks the chroot content is the whole system.
>
> * configured chroot. Where you configure Squid master process to chroot
> its low-privilege workers with the squid.conf chroot directive.
>
> * Linux containers. Similar to the first, but you dont have to copy
> files into a separate chroot area. Just assign visibility/access to the
> OS areas.
>
>
> The error is pretty clear though. The problem is that something is
> unable to load a file during helper startup.
> Either Squid is unable to read/open/see the helper binary file itself.
> Or the helper is unable to open a file it needs to operate.
>
> "ipcCreate:" is a big hint that its Squid not finding the helper binary
> named.
>
> So is Squid being run from inside the chroot, or using "chroot"
> directive in squid.conf?
>
>
> Amos
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/f8ad5471/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 24 14:35:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 02:35:59 +1200
Subject: [squid-users] RE Peek and Splice error SSL_accept failed
In-Reply-To: <2F3AADF230295040BDC74C6F96094F3D021E961A@SRVEXAFA.verwaltung.afa-ag.loc>
References: <2F3AADF230295040BDC74C6F96094F3D021E961A@SRVEXAFA.verwaltung.afa-ag.loc>
Message-ID: <55B24D4F.6070003@treenet.co.nz>

On 25/07/2015 12:09 a.m., Sebastian Kirschner wrote:
> Hi ,
> 
> I minimized the configuration a little bit(you could see it at the bottom of these message).
> 
> Also I still try to understand why these error happen ,

Lets be clear. "error" are not happening. If errors happend Squid logs
them with a huge "ERROR:" or "FATAL:" log entry.

Specific things trying to happen can fail, or not. That is normal and
hapens all the time when trying to interpret externally generated
information such as the bytes arriving in over network socket.

In this case Squid is trying to figure out of the connection is actually
a TLS connection *or something else*.

> I increased the Debug level and saw that squid tried 48 times to peek but failed.
> At the end It says that it got an "Hello", does it mean that squid received after 48 tries the "Hello" ?
> 
> If yes why it does need so many tries ?

Depends on what a "try" is, and why it was tried.

> 
> -> Part of debug log <-
> 2015/07/24 11:05:42.866 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
> 2015/07/24 11:05:42.866 kid1| bio.cc(120) read: FD 11 read 11 <= 11
> 2015/07/24 11:05:42.866 kid1| bio.cc(146) readAndBuffer: read 11 out of 11 bytes
> 2015/07/24 11:05:42.866 kid1| bio.cc(150) readAndBuffer: recorded 11 bytes of TLS client Hello
> 2015/07/24 11:05:42.866 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
> 2015/07/24 11:05:42.866 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.

Could be 11 bytes of anything.

Squid believes the 11 bytes maybe are a clientHello (which is 11 bytes
in size). Sends them to processing which fails to parse a clientHello
out of it.
Squid goes back to read(2) to see if anything else arrives.


> .
> 2015/07/24 11:05:42.874 kid1| client_side.cc(4242) clientPeekAndSpliceSSL: Start peek and splice on FD 11
> 2015/07/24 11:05:42.874 kid1| bio.cc(120) read: FD 11 read 6 <= 11
> 2015/07/24 11:05:42.874 kid1| bio.cc(146) readAndBuffer: read 6 out of 11 bytes
> 2015/07/24 11:05:42.874 kid1| bio.cc(150) readAndBuffer: recorded 6 bytes of TLS client Hello


This is a bit obscure. 6 *more* bytes arrive from FD 11.

Now we probably have 17 bytes in the I/O buffer. The bio.c code knows
that 6 bytes alone is not enough for clientHello. But it seems to be
ignoring or forgotten about the previous read.


> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2040 from c-string, n=0)
> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2038 from c-string, n=13)

The buffer says 13 bytes to be processed. Note that 13 >= 11 for the
clientHello.

I have no idea what the first 4 bytes were or went. Guess maybe some SSL
alert notice the SSL_accept() code absorbed and adjusted the context to
use later? But not part of a clientHello either way.


> 2015/07/24 11:05:42.875 kid1| ModEpoll.cc(116) SetSelect: FD 11, type=1, handler=1, client_data=0x7effbd078458, timeout=0
> 2015/07/24 11:05:42.875 kid1| client_side.cc(4245) clientPeekAndSpliceSSL: SSL_accept failed.
> 2015/07/24 11:05:42.875 kid1| SBuf.cc(152) assign: SBuf2025 from c-string, n=4294967295)
> 2015/07/24 11:05:42.875 kid1| client_side.cc(4259) clientPeekAndSpliceSSL: I got hello. Start forwarding the request!!!


Thus Squid believes the 13 bytes has in bufer are a clientHello (which
is 11? bytes). Sends them to processing which succeeds to parse it as a
clientHello. SSL-Bump "peek" at stage 1 completed.

In Summary:
 Looks like normal network operations to me. No errors. Just a temporary
failure to have the whole thing in memory on first parse attempt.


Take that with a grain of salt though. Squid is event driven and there
is no guarantee that any two adjacent log lines are even reporting about
the same transaction. These could be two entirely separate TCP
connections independently arriving and being assigned to FD 11. One
having non-TLS protocol delivered and rejected, one having TLS started.
I assumed that the '...' added dont omit any other FD 11 lines.

I/we would have to see the input data analysis by wireshark and
cross-check it against the Squid code to be sure of the above. But I've
~80% confident thats correct interpretation of the log.


Amos



From squid3 at treenet.co.nz  Fri Jul 24 14:57:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 02:57:24 +1200
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <CAMeoTH=02cRGiVjZk=cyFtgjv0GU7aJOLidPdx_SLYBeD6Dx5Q@mail.gmail.com>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
 <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
 <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>
 <55B2455E.9090404@treenet.co.nz>
 <CAMeoTH=02cRGiVjZk=cyFtgjv0GU7aJOLidPdx_SLYBeD6Dx5Q@mail.gmail.com>
Message-ID: <55B25254.9020902@treenet.co.nz>

On 25/07/2015 2:22 a.m., Jorgeley Junior wrote:
> Thank you so much for the help.

Cant be much help sorry. I'm just guessing here. Never actually run
Squid in a chroot myself.

> So, I use the directive 'chroot' in the squid.conf.
> I start squid this way:
> cd /etc/squid-3.5.6
> sbin/squid
> and it starts normally, but when I open the client browser and do an
> authentication it logs the errors and don't authenticate, but the squid
> doesn't stop running, just it logs the error and do not authenticate.

I've just looked up what is displaying that error and why. It is more of
the code wrongly using errno to display error text. So the message
itself may be bogus, but some error is happening when fork()'ing and
execv()'ing the helper process.

Some things I think you should try;

1) configure Squid with the full non-chroot path of the binary in the
auth_param line.

2) enter the chroot, downgrade yourself to the squid low-privilege user,
then try running the helper. Thats whats Squid is doing.

3) try the chroot directive in squid.conf with a '/' on the end

I'm out of ideas at this point. Apart from patching your squid to fix
the errno usage in ipcCreate() just to see if some other error message
appears. Sad thing about thtat is that I'm not sure what syscall is
supposed to be error-reported there, quite a few happen in sequence.

Amos



From jorgeley at gmail.com  Fri Jul 24 15:01:29 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 24 Jul 2015 12:01:29 -0300
Subject: [squid-users] squid 3.5 with auth and chroot
In-Reply-To: <55B25254.9020902@treenet.co.nz>
References: <mailman.10068.1437591964.2789.squid-users@lists.squid-cache.org>
 <CAMeoTHm2kYwD8ND12dNK6q3u1uf8R5yW8Dhjy=2WYwoK=TAZvQ@mail.gmail.com>
 <55B0FD60.9070705@treenet.co.nz>
 <CAMeoTHn4uFvABjJvvNDuc3EaHEBZpDX+hJHn6BPGWvfWu1Z0PA@mail.gmail.com>
 <CAMeoTHmMFdvhcv6G=mPmDpND5ZkFrKBQeVsSZLdokjsXa-6VQg@mail.gmail.com>
 <55B2455E.9090404@treenet.co.nz>
 <CAMeoTH=02cRGiVjZk=cyFtgjv0GU7aJOLidPdx_SLYBeD6Dx5Q@mail.gmail.com>
 <55B25254.9020902@treenet.co.nz>
Message-ID: <CAMeoTHmv1_Z0CJaq7mgGnrRPtkuD90mftUT1Qg6B5zno65_kMw@mail.gmail.com>

That's are good ideas, I'll try them.
Thanks!!!

2015-07-24 11:57 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 25/07/2015 2:22 a.m., Jorgeley Junior wrote:
> > Thank you so much for the help.
>
> Cant be much help sorry. I'm just guessing here. Never actually run
> Squid in a chroot myself.
>
> > So, I use the directive 'chroot' in the squid.conf.
> > I start squid this way:
> > cd /etc/squid-3.5.6
> > sbin/squid
> > and it starts normally, but when I open the client browser and do an
> > authentication it logs the errors and don't authenticate, but the squid
> > doesn't stop running, just it logs the error and do not authenticate.
>
> I've just looked up what is displaying that error and why. It is more of
> the code wrongly using errno to display error text. So the message
> itself may be bogus, but some error is happening when fork()'ing and
> execv()'ing the helper process.
>
> Some things I think you should try;
>
> 1) configure Squid with the full non-chroot path of the binary in the
> auth_param line.
>
> 2) enter the chroot, downgrade yourself to the squid low-privilege user,
> then try running the helper. Thats whats Squid is doing.
>
> 3) try the chroot directive in squid.conf with a '/' on the end
>
> I'm out of ideas at this point. Apart from patching your squid to fix
> the errno usage in ipcCreate() just to see if some other error message
> appears. Sad thing about thtat is that I'm not sure what syscall is
> supposed to be error-reported there, quite a few happen in sequence.
>
> Amos
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/7f4af5e6/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 24 15:15:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 03:15:15 +1200
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B231C3.1030005@gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
Message-ID: <55B25683.4090003@treenet.co.nz>

On 25/07/2015 12:38 a.m., Yuri Voinov wrote:
> 
> https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security
> 
> 24.07.15 18:33, joe ?????:
>> i dont see Strict-Transport-Security  in my log header
>> only alternate-protocol
>> can you post an example link pls
> 

Note that the header may be sent over HTTP or HTTPS connection just once
with a value of up to 68 years. And the domain will be HTTPS from then
on as far as that client is concerned.

Dropping Strict-Transport-Security therefore does nothing useful.

But Squid replacing it with a new value of "max-age=0;
includeSubDomains" will turn off the HSTS in the client for that domain.

Be careful with that though. HSTS is actually a good thing most of the
time. No matter how annoying it is to us proxying.


Regarding Alternate-Protocol;
 The latest Squid will auto-remove *always*. It usually indicates an
protocol experiment taking place by the website being visited (ie Google
and QUIC/SPDY) and does a lot of real damage to network security and
usability in any proxied network.

Amos


From stan.prescott at gmail.com  Fri Jul 24 15:23:15 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 24 Jul 2015 10:23:15 -0500
Subject: [squid-users] SIGTERM SIGKILL causes issues with squid shutdown
 during reboot
In-Reply-To: <55B23937.2040809@treenet.co.nz>
References: <CANLNtGS8k57QFHi-vWvY4cjNNkCgbZBTjS1iUCZAP1Li0=dUTg@mail.gmail.com>
 <55B23937.2040809@treenet.co.nz>
Message-ID: <CANLNtGTTKV+cYDpUU4+kAs41SC1DdtYGkPHaP76vNwb1TQF7qg@mail.gmail.com>

Thanks, Amos. That was very helpful. Smoothwall Express does not and never
has used systemd, precisely because of the reasons you mention. It does use
udev and we are considering bumping to eudev, but that is a fairly large
change, but likely worth it.

We have some things to think about now with possibly redesigning the system
shutdown.

Regards.

Stan


On Fri, Jul 24, 2015 at 8:10 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Response inline.
>
> On 24/07/2015 8:21 a.m., Stanford Prescott wrote:
> > After bumping Squid from 3.4.x to 3.5.x in our implementation of Squid in
> > the Smoothwall Express v3.1 firewall distro we have begun to have
> > complaints from our users about "erratic behavior" of Squid shutting down
> > during reboots or network drops causing reboots.
> >
> > It appears that squid (v3.5.[5-6]) does not respond well to SIGTERM
> during
> > system shutdown; the cache index almost invariably needs to be rebuilt on
> > next boot. It is suggested that we use squid to shut squid down. While
> > using squid to stop the squid daemon is very doable, this requirement
> runs
> > contrary to the longstanding, traditional UNIX method of "SIGTERM, pause,
> > SIGKILL".
>
> Squid is designed to operate in UNIX style. It contains its own daemon
> manager, and a worker process.
>
> When SIGTERM is received by the manager process, it relays that to the
> worker which begins shutdown immediately but with shutdown_lifetime
> grace period for active clients to finish. A second SIGTERM will act
> like shutdown_lifetime having expired, everything gets closed immediately.
>
> SIGKILL causes the OS init system to erase the Squid processes from
> system memory, aborts all the open sockets and disk I/O, etc. The
> nuclear option as some often called it.
>
> This causes issues with systemd, openrc, or upstart where the init
> system insists on being a daemon manager itself. IIRC there was an
> excellent tutorial somewhere (by DJB?) that explained in great detail
> why having a daemon manager to operate other daemon managers was a
> terrible idea. Squid hits pretty much all the relevant nasty side
> effects as each piece of the system makes bad assumptions or gets
> confused about other bits activities.
>
> Meanwhile if just left alone the worker is still trying to gracefully
> close client transactions for shutdown_lifetime (30sec default). Then
> abort those and save the final cache index to a new swap.state file on
> disk.
>
>
> >
> > During normal system operation, squid *ought* to be able to take as much
> > time as it wants to shut down. But it still shouldn't take more than
> 10-20
> > seconds; 'shutdown' is a command, not a request to be honored at squid's
> > leisure. After all, the CPU could be on fire....
> >
> > This raises a few questions that are intended to foster fresh discussion,
> > not to re-hash old arguments. They are really more rhetorical in nature;
> > the goal is to find the root cause of the problem.
> >
> >    - Why do these latest versions of Squid 3 behave oddly in this
> respect?
>
> systemd is becoming popular. See above. Impatience and incorrect init
> scripts using SIGKIL instead of SIGTERM does seem to be the main pathway
> to having issues.
>
> Squid-3 has become a SMP multi-process thing. Which increases the number
> of workers, and things they are still doing during shtutdown.
>
> Squid-3.1 & 3.2 added various checksums to swap.state format protecting
> against file corruption propigating into the memory index of next
> started Squid process.
>
> >    - What is it about shutting squid down that corrupts the cache index?
>
> "corruption" in the cache index means the swap.state file for each cache
> has not been completely written. On next load the file checksum does not
> match the contents checksum. So reverts to a relatively slow scan of the
> disk to see what is there.
>
> SIGKILL being used during the time that index is being written to disk
> FD will cause it nearly 100% of the time. Being used before old
> swap.state is replaced will lead to a lot of SWAPFAIL as objects on disk
> today dont match what was on disk when Squid restarted a week ago.
>
> What corruption happens also varies between disk controllers. Some
> controllers complete already scheduled writes. Some drop them. SIGKILL
> may be relevant in that decision too. The ones that complete all
> scheduled writes and close the file hit corruption less often, or less
> badly.
>
>
> >    - Does it take more than a few seconds to write the index to disk?
>
> No. But it can take days for active clients to finish off long-polling
> HTTP requests or CONNECT tunnels. shutdown_lifetime is what they are
> allowed. IME, Squid workers usually completely exit within 5 seconds of
> that grace period being over. It takes under a ms to close a socket, but
> many tens of thousands of sockets may be open on a busy proxy.
>
>
> >    - Does squid use the very slow 'writethrough' method instead of
> trusting
> >    the Linux disk cache to properly save the cache?
>
> Squid is generating new content and writing it to a file with write(2)
> API. That content is anything up to 68MB in size *per cache_dir*. How
> fast can your disk save that much data when its passed in ~100 byte IO
> chunks?
>  Whatever OS filesystem does to optimize I/O happens on top of that
> process-level activity.
>
>
> As mentioned above SIGKILL also does not shutdown the process (that
> would be SIGTERM). Just drops it all on the floor. Using SIGKILL on a
> process currently writing to disk has varying results from the disk
> driver layers - usually not good.
>
> SIGTERM will almost guarantee the files complete writing and things get
> gracefully resolved.
>
>
> If you haven't guessed already IMO you should at worst use SIGTERM twice
> on Squid instead of SIGKILL even once. If Squid is actually operational
> a second SIGTERM will have the same effect as shutdown_lifetime having
> finished early.
>
> Sadly systemd and init scripts can be rather friendly with SIGKILL.
> Which can be problematic.
>
>
> >    - Should squid write the cache index to disk more often?
>
> The cache index is extremely volatile. Every single request through it
> adds or removes an entry. Which is a lot of disk overhead on a very busy
> proxy.
>
> The compromise is to write it only once on shutdown/restart when all
> updates have been finished. Or on "-k rotate" (SIGHUP) when explicitly
> asked to auto-save all state for log file rotations.
>
>
> >    - Should squid track its own 'dirty pages' in its in-core index to
> >    reduce the time it takes to write the index?
>
> There is not much to track. The "pages" as it were are little marker
> records a few hundred bytes stating what HTTP message is contained in
> any given on-disk file. The memory copy gets erased as soon as the disk
> controller has been asked to delete the file. The running worker does
> not track whether the delete worked or not, just that all index entries
> are supposedly valid HTTP objects. Whatever files get left undeleted on
> disk can be overwritten with new content. [enter a few race condition
> bugs].
>
> On shutdown the (up to 2^27 x 512 bytes) set of then-valid records are
> dumped to disk in a new swap.state.
>
>
> >    - Should squid implement a journal (akin to EXT/ReiserFS and others)
> so
> >    the on-disk index structure is always OK?
>
> Been tried. store_log is the journal. You can turn it on if you have any
> need for external software to track what Squid cache contains in near
> real-time.
>
> Overall it slows Squid down and it is not useful to remember objects
> that were saved X days ago and deleted a few seconds later.
>
> Wont protect against swap.state corruption anyway, since the journal/log
> file would get truncated/corrupted in the same ways as swap.state.
>
>
> >    - Is the problem related to clients actively receiving web pages from
> >    squid?
>
> Indirectly. They delay added by waiting for them increases the
> impatience of external daemon managers and admins chance of using
> SIGKILL on the worker.
>
>
> >    - Could squid's signal handling be adjusted to treat those clients
> more
> >    harshly? That is, terminate the transfers early because squid
> shutting down
> >    is much the same as the network interface going down.
>
> It could. I have actually just applied a patch to Squid-4 that makes
> Squid abort idle client connections on teh first SIGTERM and more
> cleanly close *all* client FDs at the end of the shutdown_lifetime (or
> second SIGTERM) before it uses abort() on whats left. That will probably
> be in 3.5.7 as well.
>
> Hopefully that will result in less SWAPFAIL corruption issues in future
> for some people. It may or may not help with swap.fail corruption since
> thet is a step later.
>
> Long term plan is to handle shutdown in such a way that non-busy Squid
> may even exit before shutdown_lifetime is over. Which should definitely
> include proper swap.state closure.
>
>
>
> >    - Is the only viable solution to use squid to stop the daemon? Does
> >    'squid -k shutdown' exit only after the daemon is dead? The squid man
> page
> >    indicates that it sends the shutdown 'signal' but does not await a
> reply;
> >    thus a potentially lengthy pause in system shutdown may be required
> anyway.
>
> The man page is correct. "squid -k shutdown" sends a SIGTERM signal to
> the process listed in squids PID file.
>  NP: any other UNIX style process could happily do the same to control
> Squid.
>
> The -k process itself exits immediately when that signal is sent.
>
> The running squid daemon manager (or coordinator) process handles that
> signal as mentioned above.
>
>
> >    - Is pausing the system shutdown for 10-15 seconds the only really
> >    viable shutdown solution?
>
> In current Squid releases yes. The alternative is to risk the corruption
> problems.
>
> >    - Or is it best to delete and rebuild the cache index on every system
> >    startup?
>
> No. Just configure shutdown_lifetime to a few seconds, and require the
> init scripts system to wait that long plus 2-3 more sec for the whole
> bunch of Squid processes to complete normally. Use SIGTERM if really
> necessary to speed things up.
>
> There are a occasional exceptions, you may be one, but for most
> installations it seems to still work fine that way.
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/6617c229/attachment.htm>

From yvoinov at gmail.com  Fri Jul 24 15:34:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 21:34:25 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B25683.4090003@treenet.co.nz>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
 <55B25683.4090003@treenet.co.nz>
Message-ID: <55B25B01.6010000@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


24.07.15 21:15, Amos Jeffries ?????:
> On 25/07/2015 12:38 a.m., Yuri Voinov wrote:
>>
>> https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security
>>
>> 24.07.15 18:33, joe ?????:
>>> i dont see Strict-Transport-Security  in my log header
>>> only alternate-protocol
>>> can you post an example link pls
>>
>
> Note that the header may be sent over HTTP or HTTPS connection just once
> with a value of up to 68 years. And the domain will be HTTPS from then
> on as far as that client is concerned.
>
> Dropping Strict-Transport-Security therefore does nothing useful.
In my setup it works for Chrome when user type "youtube.com" in command
line. Browser goes into http. Always.
>
>
> But Squid replacing it with a new value of "max-age=0;
> includeSubDomains" will turn off the HSTS in the client for that domain.
Which Squid?
>
>
> Be careful with that though. HSTS is actually a good thing most of the
> time. No matter how annoying it is to us proxying.
This is security illusion. Which is more bad than insecure.
>
>
>
> Regarding Alternate-Protocol;
>  The latest Squid will auto-remove *always*. It usually indicates an
> protocol experiment taking place by the website being visited (ie Google
> and QUIC/SPDY) and does a lot of real damage to network security and
> usability in any proxied network.
No network security during DPI. So, all of this things is meaningless. IMHO.

All usability we are need - HTTP does.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVslsBAAoJENNXIZxhPexGRK0H/0Y4ga/K6aNbYKLMaLFgcMmC
UjZ5MbI4EqftW6z2Yn4G2RoQp3BjoZuKRbdzIDkzpqZJnc4MfoeqlCHlcfCyV7S0
v/qeygrh8BycU/VNZzCnZn8xVq32MfhO8l6A4yI/d4OW0yEBG1m9bHgykB0+cvLo
U1w7oUc8j6CJ0AAxCpvux5ZFidf/E4zbvR1/RDhaOlRe0hx39K6ErUjrqiAjtiii
4AMneYgXn5LGD6LOwTumP7Zw4H4PklmeIlgLULK/Fr9f7m3OuPOcWBl1w4t/V+Xm
cxDfp4ewdVDbhwHNdu+GXP4JaQsuBxk+bYMNArDQtyzNY6h3OaDoURkse0eW6kM=
=5Rui
-----END PGP SIGNATURE-----



From jagannath.naidu at fosteringlinux.com  Fri Jul 24 15:35:17 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Fri, 24 Jul 2015 21:05:17 +0530
Subject: [squid-users] ISSUE accssing content
Message-ID: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>

Dear List,

I have been working on this for last two weeks, but never got it resolved.

We have a application server (SERVER) in our local network and a desktop
 application (CLIENT). The application picks proxy settings from IE. And we
also have a wensense proxy server

case 1: when there is no proxy set
application works. No logs in squid server access.log

case 2: when proxy ip address set and checked "bypass local network"
application works. No logs in squid server access.log

case 3: when proxy ip address is set to wensense proxy server. UNCHECKED
"bypass local network"
application works. We dont have access to websense server and hence we can
not check logs


case 4: when proxy ip address is set to proxy server ip address. UNCHECKED
"bypass local network"
application does not work :-(. Below are the logs.


1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET
http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application -
HIER_DIRECT/10.1.4.46 text/html
1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT
0.client-channel.google.com:443 - HIER_NONE/- text/html
1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT
0.client-channel.google.com:443 - HIER_NONE/- text/html
1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET
http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/10.1.4.46
text/html
1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST
http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
- HIER_NONE/- text/html

squid -v
Squid Cache: Version 3.3.8
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--disable-strict-error-checking'
'--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-eui'
'--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
'--enable-auth-ntlm=smb_lm,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos'
'--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
'--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
'--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
-g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'


squid.conf

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 8180
acl CONNECT method CONNECT
acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54 10.1.4.46
10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4
http_access allow wvdial
acl dialer dstdomain .htmedia.net
http_access allow dialer
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
visible_hostname = NOIDAPROXY01.MYDOMAIN.NET
append_domain  .mydomain.net
ignore_expect_100 on
dns_v4_first on
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
auth_param ntlm children 1000
auth_param ntlm keep_alive off
auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 100
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl auth proxy_auth REQUIRED
http_access allow all auth
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 0.0.0.0:8080
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320


It was the same behavior with squid-3.1.10-19. I thought, upgrading to
squid 3.3 would help. Please help me resolving this mystery.


-- 
Thanks & Regards

Jagannath Naidu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/5dac8d3f/attachment.htm>

From chip_pop at hotmail.com  Fri Jul 24 15:34:36 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 08:34:36 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B25683.4090003@treenet.co.nz>
References: <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
 <55B25683.4090003@treenet.co.nz>
Message-ID: <1437752076077-4672455.post@n4.nabble.com>

tks amos so
doing replace beter as 
reply_header_access Strict-Transport-Security deny all 

request_header_replace Strict-Transport-Security max-age=0
right ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672455.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri Jul 24 15:37:19 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 24 Jul 2015 21:37:19 +0600
Subject: [squid-users] squid youtube caching
In-Reply-To: <1437752076077-4672455.post@n4.nabble.com>
References: <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
 <55B25683.4090003@treenet.co.nz> <1437752076077-4672455.post@n4.nabble.com>
Message-ID: <55B25BAF.5090204@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
No. He said that Squid does that itself. The only question - which Squid.

24.07.15 21:34, joe ?????:
> tks amos so
> doing replace beter as
> reply_header_access Strict-Transport-Security deny all
>
> request_header_replace Strict-Transport-Security max-age=0
> right ?
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672455.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVsluvAAoJENNXIZxhPexGv4AH/3z+XFId3yB+MYAq5cQl9sdg
Kg6aqAiPPB+Ti9uWJH5Jl/GP3OpMP2kxPfo2qTVovvfrpWkuDzzL8WuVzmtE1uuH
k88bnRHYxgNsxqgMNkm2vw+Q2cl9+xhi5NOlFZ7UwIQF8l5hxYSPGXHoQ5hV89ZB
4W/T+aDRe9L/5A1kPexnCGN2gem//iVXoZu883/KUfshtlUJUpdneFVDY6W7jCEp
fT7tOkpc2yt656M/gY7S8LD3ae1wQ79WAvc2zqixlbFgyRSbGlccxPWTGDNtWlD8
Il92k3RiMhrqaxSjQtdeeg6NX+J3bO3ZqfRG+3jSfwaioJrnbX72iaoIAi1NlRw=
=jDEv
-----END PGP SIGNATURE-----



From jagannath.naidu at fosteringlinux.com  Fri Jul 24 15:40:08 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Fri, 24 Jul 2015 21:10:08 +0530
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
Message-ID: <CA+8bHvxW=goVDdmBLjF-tdbGOup7ejAcbTTOQFrxN4rZ12aQMA@mail.gmail.com>

On 24 July 2015 at 21:05, Jagannath Naidu <
jagannath.naidu at fosteringlinux.com> wrote:

> Dear List,
>
> I have been working on this for last two weeks, but never got it resolved.
>
> We have a application server (SERVER) in our local network and a desktop
>  application (CLIENT). The application picks proxy settings from IE. And we
> also have a wensense proxy server
>
> case 1: when there is no proxy set
> application works. No logs in squid server access.log
>
> case 2: when proxy ip address set and checked "bypass local network"
> application works. No logs in squid server access.log
>
> case 3: when proxy ip address is set to wensense proxy server. UNCHECKED
> "bypass local network"
> application works. We dont have access to websense server and hence we can
> not check logs
>
>
> case 4: when proxy ip address is set to proxy server ip address. UNCHECKED
> "bypass local network"
> application does not work :-(. Below are the logs.
>
>
> 1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET
> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
> - HIER_DIRECT/10.1.4.46 text/html
> 1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT
> 0.client-channel.google.com:443 - HIER_NONE/- text/html
> 1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT
> 0.client-channel.google.com:443 - HIER_NONE/- text/html
> 1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET
> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
> 10.1.4.46 text/html
> 1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST
> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
> - HIER_NONE/- text/html
>
> UPDATE: correct logs

1437752279.774      6 192.168.122.1 TCP_MISS/404 579 GET
http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application -
HIER_DIRECT/10.1.4.46 text/html
1437752281.854      5 192.168.122.1 TCP_MISS/404 579 GET
http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/10.1.4.46
text/html
1437752284.265      2 192.168.122.1 TCP_MISS/503 4048 POST
http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
- HIER_NONE/- text/html



> squid -v
> Squid Cache: Version 3.3.8
> configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--disable-strict-error-checking'
> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=$(localstatedir)/log/squid'
> '--with-pidfile=$(localstatedir)/run/squid.pid'
> '--disable-dependency-tracking' '--enable-eui'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos'
> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter'
> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
> '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
> -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>
>
> squid.conf
>
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl Safe_ports port 8180
> acl CONNECT method CONNECT
> acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54 10.1.4.46
> 10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4
> http_access allow wvdial
> acl dialer dstdomain .htmedia.net
> http_access allow dialer
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> visible_hostname = NOIDAPROXY01.MYDOMAIN.NET
> append_domain  .mydomain.net
> ignore_expect_100 on
> dns_v4_first on
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> auth_param ntlm children 1000
> auth_param ntlm keep_alive off
> auth_param basic program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-basic
> auth_param basic children 100
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> acl auth proxy_auth REQUIRED
> http_access allow all auth
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 0.0.0.0:8080
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
>
> It was the same behavior with squid-3.1.10-19. I thought, upgrading to
> squid 3.3 would help. Please help me resolving this mystery.
>
>
> --
> Thanks & Regards
>
> Jagannath Naidu
>
>


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/dedd53b1/attachment.htm>

From wolle5050 at gmx.de  Fri Jul 24 16:01:49 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Fri, 24 Jul 2015 18:01:49 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B23089.9040108@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>,
 <55B23089.9040108@urlfilterdb.com>
Message-ID: <trinity-81e5734b-fa69-41ff-9168-1ef784f453ab-1437753709668@3capp-gmx-bs51>

@Marcus:
I am not sure what exactly causes the problems, but could you please make a test with these two different settings:
cache_mem 4 GB
maximum_object_size_in_memory 1 GB

I think you will observe the behavior, that I was confronted with. The bad download rates of 500 KB/sec are gone, when I used the following settings:
cache_mem 256 MB
maximum_object_size_in_memory 16 MB

I think Amos has an idea what seems to be the source of the problem:
http://lists.squid-cache.org/pipermail/squid-users/2015-July/004728.html
?
Regards,
Jens


Gesendet:?Freitag, 24. Juli 2015 um 14:33 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid3: 100 % CPU load during object caching

On 07/24/2015 03:25 AM, Jens Offenbach wrote:
> I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
>
>
> Gesendet: Freitag, 24. Juli 2015 um 07:54 Uhr
> Von: "Jens Offenbach" <wolle5050 at gmx.de>
> An: "Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
> It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.
>
> The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
> http://wikisend.com/download/293856/squid.strace2
>
> @Marcus:
> Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?

I screwed up earlier since the maximum_object_size was too low for the test with a 1 GB file and did a new test.

The system has 64 GB memory and for sure the entire file is in the file system cache. The disk system is HW RAID-1 with 1 GB cache.
The OS is Linux 3.10, CentOS 7 latest patches.

New test:
test system: 1 CPU with 4 cores/8 threads @ 3.7 GHz, 64 GB memory, AUFS, 1 Gbit pipe, 500 mbit guaranteed

with Squid 3.4.12 :
1st download starts with 90 MB/sec and halfway drops to 30 MB/sec. My guess is that the file system cache got stressed and slowed things down.
2nd cached download with 190 MB/sec sustained and 120% CPU time.

With Squid 3.5.6 :
1st download starts with 90 MB/sec sustained and 80% CPU time.
2nd cached download with 190 MB/sec sustained and 120% CPU time.

As a comparison, I did "dd if=test of=test2 bs=4k" which uses 100% CPU time and has a throughput of 1200 MB/sec.
With bs=16k the throughput is 1300 MB/sec and with bs=64k the throughput is 1400 MB/sec.

relevant parameters :
read_ahead_gap 64 KB
cache_mem 256 MB
maximum_object_size_in_memory 8 MB
maximum_object_size 8000 MB
cache_dir aufs /local/squid34/cache 10000 32 256
cache_swap_low 92
cache_swap_high 93
# also ICAP daemon and URL rewriter configured
debug_options ALL,1 93,3 61,9

configure options:
'--prefix=/local/squid35' '--disable-ipv6' '--enable-fd-config' '--with-maxfd=3200' '--enable-async-io=64' '--enable-storeio=aufs' '--with-pthreads' '--enable-removal-policies=lru'
'--disable-auto-locale' '--enable-default-err-language=English' '--enable-err-languages=Dutch English Portuguese' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd'
'--enable-cachemgr-hostname=localhost' '--enable-cache-digests' '--enable-follow-x-forwarded-for' '--enable-xmalloc-statistics' '--disable-hostname-checks' '--enable-epoll' '--enable-icap-client'
'--enable-useragent-log' '--enable-referer-log' '--enable-stacktraces' '--enable-underscores' '--disable-icmp' '--mandir=/usr/local/share' 'CC=gcc' 'CFLAGS=-g -O2 -Wall -march=native' 'CXXFLAGS=-g -O2
-Wall -march=native' --enable-ltdl-convenience

As you can see the cache_mem is small, If Amos finds it useful, I can do another test with a larger cache_mem.

Jens, since all your tests have a drop to 500 KB/sec I think the cause is somewhere is the configuration (Squid and/or OS).

Marcus


> @Amos, Eliezer
> Is someone able to reproduce the disk caching effect?
>
> Regards,
> Jens
>
>
> Gesendet: Donnerstag, 23. Juli 2015 um 20:08 Uhr
> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
> The strace output shows this loop:
>
> Squid reads 16K-1 bytes from FD 13 webserver
> Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
> Squid writes 4 times 4K to FD 12 browser
>
> But this loop does not explain the 100% CPU usage...
>
> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?
>
> I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
> The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
> The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
> I never looked at Squid CPU usage and do not know what is reasonable but it feels high.
>
> With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
> Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
> This is a possible explanation for the fact that you see the performance going down and up.
> Can you do the same test on the host (i.e. not inside a VM).
>
> Marcus
>
>
>
> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
>> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
>> Here is the download link:
>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]]
>> I hope it can help you.
>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>> I am not sure if it is relevant, maybe it is:
>>
>> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
>> Squid sends the object to the ICAP server in small chunks of varying sizes:
>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>> Note that the interval of receiving the chunks is 1/1000th of a second.
>> It seems that Squid forwards the object to the ICAP server every time it receives
>> one or a few TCP packets.
>>
>> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
>> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>>
>> Amos, is there a debug setting that can confirm/reject this suspicion?
>>
>> Marcus
>>
>>
>> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>>> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>>>
>>> I cleared the cache:
>>> rm -rf /var/cache/squid3/*
>>> squid -z
>>> squid
>>> http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>>>
>>> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
>> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>>>
>>> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>>>
>>> For further investigations I give you my build properties:
>>> squid -v
>>> Squid Cache: Version 3.5.6
>>> Service Name: squid
>>> configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
>> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
>> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
>> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
>> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
>> s
>> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
>> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>>
>>>
>>> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>>> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>>> An: squid-users at lists.squid-cache.org
>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>>>> Hey Jens,
>>>>
>>>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>>>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>>>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>>>> download speed is more then doubled to 4.5 MBps(36Mbps).
>>>> I have not tried it with AUFS yet.
>>>
>>>
>>> And I must admit that AUFS beats rock cache with speed.
>>> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>>> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>>> results more then doubles 3 the speed rock gave with default settings.
>>>
>>> So about 15MBps which is 120Mbps.
>>> I do not seem to feel what Jens feels but the 100% CPU might be because
>>> of spinning disk hangs while reading the file from disk.
>>>
>>> Amos, I remember that there were some suggestions how to tune large rock.
>>> Any hints?
>>> I can test it and make it a suggestion for big files.
>>>
>>> Eliezer
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
>


From jagannath.naidu at fosteringlinux.com  Fri Jul 24 16:59:19 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Fri, 24 Jul 2015 22:29:19 +0530
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <CA+8bHvxW=goVDdmBLjF-tdbGOup7ejAcbTTOQFrxN4rZ12aQMA@mail.gmail.com>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <CA+8bHvxW=goVDdmBLjF-tdbGOup7ejAcbTTOQFrxN4rZ12aQMA@mail.gmail.com>
Message-ID: <CA+8bHvxaSmLjvviyONXQo-my3mxRw7DvhsPjg1=R31+SPKbBYg@mail.gmail.com>

1. Its not  a transparent proxy.

2. My clients get wpad configuration from AD server. So there are two
question.
 2.1 :I know that wpad is used to identify proxy server and port(and rest
other bypass rules).  When clients resolve to wpad.abc.com, is there way
that I can overwrite the wpad file off client. Like creating a webserver to
to serve wpad file and I change /etc/hosts file to "<myhwebserveripaddress>
wpad.abc.com"
2.2 Is there any other way to tell clients via squid server, to do not come
to squid server and re initiate the request.

On 24 July 2015 at 21:10, Jagannath Naidu <
jagannath.naidu at fosteringlinux.com> wrote:

>
>
> On 24 July 2015 at 21:05, Jagannath Naidu <
> jagannath.naidu at fosteringlinux.com> wrote:
>
>> Dear List,
>>
>> I have been working on this for last two weeks, but never got it
>> resolved.
>>
>> We have a application server (SERVER) in our local network and a desktop
>>  application (CLIENT). The application picks proxy settings from IE. And we
>> also have a wensense proxy server
>>
>> case 1: when there is no proxy set
>> application works. No logs in squid server access.log
>>
>> case 2: when proxy ip address set and checked "bypass local network"
>> application works. No logs in squid server access.log
>>
>> case 3: when proxy ip address is set to wensense proxy server. UNCHECKED
>> "bypass local network"
>> application works. We dont have access to websense server and hence we
>> can not check logs
>>
>>
>> case 4: when proxy ip address is set to proxy server ip address.
>> UNCHECKED "bypass local network"
>> application does not work :-(. Below are the logs.
>>
>>
>> 1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET
>> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
>> - HIER_DIRECT/10.1.4.46 text/html
>> 1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT
>> 0.client-channel.google.com:443 - HIER_NONE/- text/html
>> 1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT
>> 0.client-channel.google.com:443 - HIER_NONE/- text/html
>> 1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET
>> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
>> 10.1.4.46 text/html
>> 1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST
>> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
>> - HIER_NONE/- text/html
>>
>> UPDATE: correct logs
>
> 1437752279.774      6 192.168.122.1 TCP_MISS/404 579 GET
> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
> - HIER_DIRECT/10.1.4.46 text/html
> 1437752281.854      5 192.168.122.1 TCP_MISS/404 579 GET
> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
> 10.1.4.46 text/html
> 1437752284.265      2 192.168.122.1 TCP_MISS/503 4048 POST
> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
> - HIER_NONE/- text/html
>
>
>
>> squid -v
>> Squid Cache: Version 3.3.8
>> configure options:  '--build=x86_64-redhat-linux-gnu'
>> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
>> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
>> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
>> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
>> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
>> '--infodir=/usr/share/info' '--disable-strict-error-checking'
>> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
>> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
>> '--with-logdir=$(localstatedir)/log/squid'
>> '--with-pidfile=$(localstatedir)/run/squid.pid'
>> '--disable-dependency-tracking' '--enable-eui'
>> '--enable-follow-x-forwarded-for' '--enable-auth'
>> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
>> '--enable-auth-ntlm=smb_lm,fake'
>> '--enable-auth-digest=file,LDAP,eDirectory'
>> '--enable-auth-negotiate=kerberos'
>> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
>> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
>> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
>> '--enable-ident-lookups' '--enable-linux-netfilter'
>> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
>> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
>> '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
>> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
>> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
>> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
>> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
>> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
>> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
>> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
>> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
>> -m64 -mtune=generic -fpie'
>> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>>
>>
>> squid.conf
>>
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
>> machines
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl Safe_ports port 8180
>> acl CONNECT method CONNECT
>> acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54
>> 10.1.4.46 10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4
>> http_access allow wvdial
>> acl dialer dstdomain .htmedia.net
>> http_access allow dialer
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> visible_hostname = NOIDAPROXY01.MYDOMAIN.NET
>> append_domain  .mydomain.net
>> ignore_expect_100 on
>> dns_v4_first on
>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
>> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>> auth_param ntlm children 1000
>> auth_param ntlm keep_alive off
>> auth_param basic program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-basic
>> auth_param basic children 100
>> auth_param basic realm Squid proxy-caching web server
>> auth_param basic credentialsttl 2 hours
>> acl auth proxy_auth REQUIRED
>> http_access allow all auth
>> http_access allow localnet
>> http_access allow localhost
>> http_access deny all
>> http_port 0.0.0.0:8080
>> coredump_dir /var/spool/squid
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>>
>> It was the same behavior with squid-3.1.10-19. I thought, upgrading to
>> squid 3.3 would help. Please help me resolving this mystery.
>>
>>
>> --
>> Thanks & Regards
>>
>> Jagannath Naidu
>>
>>
>
>
> --
> Thanks & Regards
>
> B Jagannath
> Keen & Able Computers Pvt. Ltd.
> +919871324006
>



-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/126a491b/attachment.htm>

From marcus.kool at urlfilterdb.com  Fri Jul 24 17:01:42 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 24 Jul 2015 14:01:42 -0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-81e5734b-fa69-41ff-9168-1ef784f453ab-1437753709668@3capp-gmx-bs51>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>,
 <55B23089.9040108@urlfilterdb.com>
 <trinity-81e5734b-fa69-41ff-9168-1ef784f453ab-1437753709668@3capp-gmx-bs51>
Message-ID: <55B26F76.5030108@urlfilterdb.com>



On 07/24/2015 01:01 PM, Jens Offenbach wrote:
> @Marcus:
> I am not sure what exactly causes the problems, but could you please make a test with these two different settings:
> cache_mem 4 GB
> maximum_object_size_in_memory 1 GB

I think this setting for maximum_object_size_in_memory is too high, independent of how the performance is.
The tests also show that large objects cached on disk have a good performance.
The perfect place for a large ISO image is the disk cache.

I did the test with squid 3.5.6 and got the same result as you have:
the download starts fast but quickly drops.  Squid uses 100% CPU.
wget displays 14 MB/sec ... 10 MB/sec ... 8 7 6 5 4 3 2 MB/sec and stays there for a long time.
At 50% downloaded the speed drops more to 1 MB/sec and at the end of the download I got 500 KB/sec *average*.
The second cached download was sustained 190 MB/sec and 120% CPU.

I did a second test with
cache_mem 4 GB
maximum_object_size_in_memory 200 MB

The download speed varied a lot: started with 30 MB/sec and went down and up many times between 6 MB/sec and 35 MB/sec.
The final average download speed was 31 MB/sec.  100% CPU.
The second cached download was sustained 190 MB/sec and 120% CPU.

Third test with
cache_mem 4 GB
maximum_object_size_in_memory 8 MB

The download speed started with 70 MB/sec and increased to 87 MB/sec.   100% CPU
The second cached download was sustained 190 MB/sec and 120% CPU.

4th test with
cache_mem 4 GB
maximum_object_size_in_memory 32 MB

The download speed started with 40 MB/sec and increased to 75 MB/sec.   100% CPU.
The second cached download was sustained 190 MB/sec and 120% CPU.

So Squid appears to have an issue with higher values of maximum_object_size_in_memory, the higher they are, the worse the performance.
For now, I would not go beyond 16 MB.
The question is, what is a reasonable size that you would like to be able to use for maximum_object_size_in_memory.
Do you have any particular requirement for a high maximum_object_size_in_memory ?

Marcus

> I think you will observe the behavior, that I was confronted with. The bad download rates of 500 KB/sec are gone, when I used the following settings:
> cache_mem 256 MB
> maximum_object_size_in_memory 16 MB
>
> I think Amos has an idea what seems to be the source of the problem:
> http://lists.squid-cache.org/pipermail/squid-users/2015-July/004728.html
>
> Regards,
> Jens
>
>
> Gesendet: Freitag, 24. Juli 2015 um 14:33 Uhr
> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>
> On 07/24/2015 03:25 AM, Jens Offenbach wrote:
>> I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
>>
>>
>> Gesendet: Freitag, 24. Juli 2015 um 07:54 Uhr
>> Von: "Jens Offenbach" <wolle5050 at gmx.de>
>> An: "Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>> It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.
>>
>> The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
>> http://wikisend.com/download/293856/squid.strace2
>>
>> @Marcus:
>> Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?
>
> I screwed up earlier since the maximum_object_size was too low for the test with a 1 GB file and did a new test.
>
> The system has 64 GB memory and for sure the entire file is in the file system cache. The disk system is HW RAID-1 with 1 GB cache.
> The OS is Linux 3.10, CentOS 7 latest patches.
>
> New test:
> test system: 1 CPU with 4 cores/8 threads @ 3.7 GHz, 64 GB memory, AUFS, 1 Gbit pipe, 500 mbit guaranteed
>
> with Squid 3.4.12 :
> 1st download starts with 90 MB/sec and halfway drops to 30 MB/sec. My guess is that the file system cache got stressed and slowed things down.
> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>
> With Squid 3.5.6 :
> 1st download starts with 90 MB/sec sustained and 80% CPU time.
> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>
> As a comparison, I did "dd if=test of=test2 bs=4k" which uses 100% CPU time and has a throughput of 1200 MB/sec.
> With bs=16k the throughput is 1300 MB/sec and with bs=64k the throughput is 1400 MB/sec.
>
> relevant parameters :
> read_ahead_gap 64 KB
> cache_mem 256 MB
> maximum_object_size_in_memory 8 MB
> maximum_object_size 8000 MB
> cache_dir aufs /local/squid34/cache 10000 32 256
> cache_swap_low 92
> cache_swap_high 93
> # also ICAP daemon and URL rewriter configured
> debug_options ALL,1 93,3 61,9
>
> configure options:
> '--prefix=/local/squid35' '--disable-ipv6' '--enable-fd-config' '--with-maxfd=3200' '--enable-async-io=64' '--enable-storeio=aufs' '--with-pthreads' '--enable-removal-policies=lru'
> '--disable-auto-locale' '--enable-default-err-language=English' '--enable-err-languages=Dutch English Portuguese' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd'
> '--enable-cachemgr-hostname=localhost' '--enable-cache-digests' '--enable-follow-x-forwarded-for' '--enable-xmalloc-statistics' '--disable-hostname-checks' '--enable-epoll' '--enable-icap-client'
> '--enable-useragent-log' '--enable-referer-log' '--enable-stacktraces' '--enable-underscores' '--disable-icmp' '--mandir=/usr/local/share' 'CC=gcc' 'CFLAGS=-g -O2 -Wall -march=native' 'CXXFLAGS=-g -O2
> -Wall -march=native' --enable-ltdl-convenience
>
> As you can see the cache_mem is small, If Amos finds it useful, I can do another test with a larger cache_mem.
>
> Jens, since all your tests have a drop to 500 KB/sec I think the cause is somewhere is the configuration (Squid and/or OS).
>
> Marcus
>
>
>> @Amos, Eliezer
>> Is someone able to reproduce the disk caching effect?
>>
>> Regards,
>> Jens
>>
>>
>> Gesendet: Donnerstag, 23. Juli 2015 um 20:08 Uhr
>> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> An: "Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
>> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
>> The strace output shows this loop:
>>
>> Squid reads 16K-1 bytes from FD 13 webserver
>> Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
>> Squid writes 4 times 4K to FD 12 browser
>>
>> But this loop does not explain the 100% CPU usage...
>>
>> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?
>>
>> I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
>> The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
>> The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
>> I never looked at Squid CPU usage and do not know what is reasonable but it feels high.
>>
>> With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
>> Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
>> This is a possible explanation for the fact that you see the performance going down and up.
>> Can you do the same test on the host (i.e. not inside a VM).
>>
>> Marcus
>>
>>
>>
>> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>>> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
>>> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
>>> Here is the download link:
>>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]]
>>> I hope it can help you.
>>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> I am not sure if it is relevant, maybe it is:
>>>
>>> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
>>> Squid sends the object to the ICAP server in small chunks of varying sizes:
>>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>>> Note that the interval of receiving the chunks is 1/1000th of a second.
>>> It seems that Squid forwards the object to the ICAP server every time it receives
>>> one or a few TCP packets.
>>>
>>> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
>>> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>>>
>>> Amos, is there a debug setting that can confirm/reject this suspicion?
>>>
>>> Marcus
>>>
>>>
>>> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>>>> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>>>>
>>>> I cleared the cache:
>>>> rm -rf /var/cache/squid3/*
>>>> squid -z
>>>> squid
>>>> http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>>>>
>>>> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
>>> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>>>>
>>>> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>>>>
>>>> For further investigations I give you my build properties:
>>>> squid -v
>>>> Squid Cache: Version 3.5.6
>>>> Service Name: squid
>>>> configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
>>> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
>>> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
>>> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
>>> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
>>> s
>>> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
>>> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>>> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>>>
>>>>
>>>> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>>>> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>>>> An: squid-users at lists.squid-cache.org
>>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>>> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>>>>> Hey Jens,
>>>>>
>>>>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>>>>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>>>>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>>>>> download speed is more then doubled to 4.5 MBps(36Mbps).
>>>>> I have not tried it with AUFS yet.
>>>>
>>>>
>>>> And I must admit that AUFS beats rock cache with speed.
>>>> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>>>> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>>>> results more then doubles 3 the speed rock gave with default settings.
>>>>
>>>> So about 15MBps which is 120Mbps.
>>>> I do not seem to feel what Jens feels but the 100% CPU might be because
>>>> of spinning disk hangs while reading the file from disk.
>>>>
>>>> Amos, I remember that there were some suggestions how to tune large rock.
>>>> Any hints?
>>>> I can test it and make it a suggestion for big files.
>>>>
>>>> Eliezer
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]
>>
>
>


From hack.back at hotmail.com  Fri Jul 24 18:33:14 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 24 Jul 2015 11:33:14 -0700 (PDT)
Subject: [squid-users] ecap and https
In-Reply-To: <55B2057C.8000706@treenet.co.nz>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B2057C.8000706@treenet.co.nz>
Message-ID: <1437762794834-4672462.post@n4.nabble.com>

Dear Amos,
you mean if the https is decrypted ?
so yes it is decrypted and full url shown in access.log
and not this adapter didnt work on https pages,
it can edit content in http pages and not in https pages .




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672462.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From mcsnv96 at afo.net  Fri Jul 24 18:57:20 2015
From: mcsnv96 at afo.net (Mike)
Date: Fri, 24 Jul 2015 13:57:20 -0500
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
Message-ID: <55B28A90.1010006@afo.net>

I see a few issues.

1. The report from the log shows a 192.168.*.* address, common LAN IP

Then in the squid.conf:
2. You have wvdial destination as 10.1.*.* addresses, which is a 
completely different internal network.
Typically there will be no internal routing or communication from a 
192.168..*.* address to/from a 10.*.*.* address without a custom routing 
server with 2 network connections, one from each IP set and to act as 
the DNS intermediary for routing. Otherwise for network/internet 
connections, the computer/browser sees its own IP as local network, and 
everything else including 10.*.*.* as an external address out on the 
internet. I would suggest getting both the browsing computer and the 
server on the same IP subset, as in 192.168.122.x or 10.1.4.x, otherwise 
these issues are likely to continue.

3. Next in the squid.conf is http_port which should be port number only, 
no IP address, especially 0.0.0.0 which can cause conflicts with squid 
3.x versions. Best bet is use just port only, as in: "http_port 3128" or 
in your case "http_port 8080", which is the port (with server IP found 
in ifconfig) the browser will use to connect through the squid server.
4. The bypass local network means any IP connection attempt to a local 
network IP will not use the proxy. This goes back to the 2 different IP 
subsets. One option is to enter a proxy exception as 10.*.*.* (if the 
websense server is using 10.x.x.x IP address).


Mike


On 7/24/2015 10:35 AM, Jagannath Naidu wrote:
> Dear List,
>
> I have been working on this for last two weeks, but never got it 
> resolved.
>
> We have a application server (SERVER) in our local network and a 
> desktop  application (CLIENT). The application picks proxy settings 
> from IE. And we also have a wensense proxy server
>
> case 1: when there is no proxy set
> application works. No logs in squid server access.log
>
> case 2: when proxy ip address set and checked "bypass local network"
> application works. No logs in squid server access.log
>
> case 3: when proxy ip address is set to wensense proxy server. 
> UNCHECKED "bypass local network"
> application works. We dont have access to websense server and hence we 
> can not check logs
>
>
> case 4: when proxy ip address is set to proxy server ip address. 
> UNCHECKED "bypass local network"
> application does not work :-(. Below are the logs.
>
>
> 1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET 
> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application 
> - HIER_DIRECT/10.1.4.46 <http://10.1.4.46> text/html
> 1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT 
> 0.client-channel.google.com:443 
> <http://0.client-channel.google.com:443> - HIER_NONE/- text/html
> 1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT 
> 0.client-channel.google.com:443 
> <http://0.client-channel.google.com:443> - HIER_NONE/- text/html
> 1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET 
> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - 
> HIER_DIRECT/10.1.4.46 <http://10.1.4.46> text/html
> 1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST 
> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal 
> - HIER_NONE/- text/html
>
> squid -v
> Squid Cache: Version 3.3.8
> configure options:  '--build=x86_64-redhat-linux-gnu' 
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' 
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' 
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' 
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' 
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' 
> '--infodir=/usr/share/info' '--disable-strict-error-checking' 
> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' 
> '--localstatedir=/var' '--datadir=/usr/share/squid' 
> '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' 
> '--with-pidfile=$(localstatedir)/run/squid.pid' 
> '--disable-dependency-tracking' '--enable-eui' 
> '--enable-follow-x-forwarded-for' '--enable-auth' 
> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' 
> '--enable-auth-ntlm=smb_lm,fake' 
> '--enable-auth-digest=file,LDAP,eDirectory' 
> '--enable-auth-negotiate=kerberos' 
> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' 
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' 
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' 
> '--enable-ident-lookups' '--enable-linux-netfilter' 
> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' 
> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' 
> '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' 
> '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' 
> '--with-openssl' '--with-pthreads' 
> 'build_alias=x86_64-redhat-linux-gnu' 
> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall 
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong 
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic 
> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 
> 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions 
> -fstack-protector-strong --param=ssp-buffer-size=4 
> -grecord-gcc-switches   -m64 -mtune=generic -fpie' 
> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>
>
> squid.conf
>
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>     # RFC1918 possible 
> internal network
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918 
> possible internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC1918 
> possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly 
> plugged) machines
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl Safe_ports port 8180
> acl CONNECT method CONNECT
> acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54 
> 10.1.4.46 10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4
> http_access allow wvdial
> acl dialer dstdomain .htmedia.net <http://htmedia.net>
> http_access allow dialer
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> visible_hostname = NOIDAPROXY01.MYDOMAIN.NET 
> <http://NOIDAPROXY01.MYDOMAIN.NET>
> append_domain  .mydomain.net <http://mydomain.net>
> ignore_expect_100 on
> dns_v4_first on
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics 
> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET 
> <http://HTMEDIA.NET>
> auth_param ntlm children 1000
> auth_param ntlm keep_alive off
> auth_param basic program /usr/bin/ntlm_auth 
> --helper-protocol=squid-2.5-basic
> auth_param basic children 100
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> acl auth proxy_auth REQUIRED
> http_access allow all auth
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 0.0.0.0:8080 <http://0.0.0.0:8080>
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
>
> It was the same behavior with squid-3.1.10-19. I thought, upgrading to 
> squid 3.3 would help. Please help me resolving this mystery.
>
>
> -- 
> Thanks & Regards
>
> Jagannath Naidu
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/eb73c1ce/attachment.htm>

From alex_wu2012 at hotmail.com  Fri Jul 24 19:24:26 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Fri, 24 Jul 2015 12:24:26 -0700
Subject: [squid-users] cannot leave empty workers
Message-ID: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>

If I define 4 workers, and use the following way to allocate workers:

if ${process_number} = 4
//do something
else
endif

I leave other workers as empty after else, then we encounter this error:

FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory

If I fill one more workers,especially ${process_number} = 1, then squid can launch workers now,

Alex
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/824d6537/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 24 20:44:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 08:44:32 +1200
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B25B01.6010000@gmail.com>
References: <1437667243296-4672389.post@n4.nabble.com>
 <55B169A4.8070601@ngtech.co.il> <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
 <55B25683.4090003@treenet.co.nz> <55B25B01.6010000@gmail.com>
Message-ID: <55B2A3B0.3080306@treenet.co.nz>

On 25/07/2015 3:34 a.m., Yuri Voinov wrote:
> 
> 24.07.15 21:15, Amos Jeffries ?????:
>> On 25/07/2015 12:38 a.m., Yuri Voinov wrote:
>>>
>>> https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security
>>>
>>> 24.07.15 18:33, joe ?????:
>>>> i dont see Strict-Transport-Security  in my log header
>>>> only alternate-protocol
>>>> can you post an example link pls
>>>
> 
>> Note that the header may be sent over HTTP or HTTPS connection just once
>> with a value of up to 68 years. And the domain will be HTTPS from then
>> on as far as that client is concerned.
> 
>> Dropping Strict-Transport-Security therefore does nothing useful.
> In my setup it works for Chrome when user type "youtube.com" in command
> line. Browser goes into http. Always.

Great to hear. I assume they are not placing a long duration on their
HSTS header then. Or that you successfully turned it off in some HTTPS
you intercepted sometime.

Like I said they *could* send 68 Years as the duration of non-HTTP.

> 
>> But Squid replacing it with a new value of "max-age=0;
>> includeSubDomains" will turn off the HSTS in the client for that domain.
> Which Squid?

I think 3.4+ . The ones supporting reply_header_access and
reply_header_replace with custom header names. It was such a small
rarely mentioned update I've forgotten when it happened.


> 
>> Be careful with that though. HSTS is actually a good thing most of the
>> time. No matter how annoying it is to us proxying.
> This is security illusion. Which is more bad than insecure.
> 

No HSTS is not illusion. At least not beyond the illusions offered by
TLS itself (which ssl-bump shines a light on).

HSTS is just telling the client to use https:// on its URLs even if the
user types http:// or any page it gets contains a http:// URL. The TLS
connection goes to where the user actually wanted to go, and is as
secure a TLS is. Nothing transferred over plain-text HTTP that could be
used to divert where the TLS was going to.
 All else being equal (ie assuming TLS was secure) attackers would have
to control port 443 on the servers belonging to the host who happened to
only be offering port 80 service. Pretty rare thing that.

In contrast there *is* illusion when an http:// redirects to https://
because the http:// part can be intercepted and attacker replace the
redirect URL with its own https:// URL. HSTS avoids using the redirect
part at all.


> 
> 
>> Regarding Alternate-Protocol;
>>  The latest Squid will auto-remove *always*. It usually indicates an
>> protocol experiment taking place by the website being visited (ie Google
>> and QUIC/SPDY) and does a lot of real damage to network security and
>> usability in any proxied network.
> No network security during DPI. So, all of this things is meaningless. IMHO.
> 

DPI ?

You recall why I put it in right? all the complains from people about
users bypassing their security rules and not being able to identify how
it was happening. It was a bit noisy in here a while back about all that.
Thats what I mean by damage. If the person in charge of security don't
even know where the traffic is, they got problems.


> All usability we are need - HTTP does.
> 


Amos


From squid3 at treenet.co.nz  Fri Jul 24 20:54:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 08:54:31 +1200
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B25BAF.5090204@gmail.com>
References: <1437712385363.74b82703@Nodemailer>
 <1437722737301-4672415.post@n4.nabble.com> <55B224E1.2070003@gmail.com>
 <1437739311277-4672427.post@n4.nabble.com> <55B22AC2.5060208@gmail.com>
 <1437740559798-4672431.post@n4.nabble.com> <55B22FCF.2060802@gmail.com>
 <1437741226992-4672434.post@n4.nabble.com> <55B231C3.1030005@gmail.com>
 <55B25683.4090003@treenet.co.nz> <1437752076077-4672455.post@n4.nabble.com>
 <55B25BAF.5090204@gmail.com>
Message-ID: <55B2A607.8060602@treenet.co.nz>

On 25/07/2015 3:37 a.m., Yuri Voinov wrote:
> 
> No. He said that Squid does that itself. The only question - which Squid.
> 

I said that for Alternate-Protocol.
It went into 3.4.10 and 3.5.0.3.


> 24.07.15 21:34, joe ?????:
>> tks amos so
>> doing replace beter as
>> reply_header_access Strict-Transport-Security deny all
> 
>> request_header_replace Strict-Transport-Security max-age=0
>> right ?
> 

 reply_header_access Strict-Transport-Security deny all
 reply_header_replace Strict-Transport-Security max-age=0

Should work.

For Yuri: custom header names that depends on seems to have gone into 3.2.

Amos


From squid3 at treenet.co.nz  Fri Jul 24 21:16:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 09:16:20 +1200
Subject: [squid-users] ecap and https
In-Reply-To: <1437762794834-4672462.post@n4.nabble.com>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B2057C.8000706@treenet.co.nz> <1437762794834-4672462.post@n4.nabble.com>
Message-ID: <55B2AB24.4000708@treenet.co.nz>

On 25/07/2015 6:33 a.m., HackXBack wrote:
> Dear Amos,
> you mean if the https is decrypted ?

Yes.

> so yes it is decrypted and full url shown in access.log
> and not this adapter didnt work on https pages,
> it can edit content in http pages and not in https pages .
> 

Strange. AFAIK there is nothing different/special for handling of the
messages. Once decryption is done its all just HTTP on the inside.

Amos


From hack.back at hotmail.com  Fri Jul 24 21:24:13 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 24 Jul 2015 14:24:13 -0700 (PDT)
Subject: [squid-users] ecap and https
In-Reply-To: <55B2AB24.4000708@treenet.co.nz>
References: <1437672834370-4672396.post@n4.nabble.com>
 <55B2057C.8000706@treenet.co.nz> <1437762794834-4672462.post@n4.nabble.com>
 <55B2AB24.4000708@treenet.co.nz>
Message-ID: <1437773053210-4672468.post@n4.nabble.com>

with this conf it work on the same site in http and not in https
the site is youtube.

#request_header_access Accept-Encoding deny all
#loadable_modules /usr/local/lib/ecap_adapter_modifying.so
#ecap_enable on
#ecap_service ecapModifier respmod_precache \
#        uri=ecap://e-cap.org/ecap/services/sample/modifying \
#        victim=channels \
#        replacement=aaaaaaa
#adaptation_access ecapModifier allow all


can you give a try ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ecap-and-https-tp4672396p4672468.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jul 24 21:29:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 09:29:41 +1200
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <55B28A90.1010006@afo.net>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <55B28A90.1010006@afo.net>
Message-ID: <55B2AE45.1010209@treenet.co.nz>

On 25/07/2015 6:57 a.m., Mike wrote:
> I see a few issues.
> 
> 1. The report from the log shows a 192.168.*.* address, common LAN IP
> 
> Then in the squid.conf:
> 2. You have wvdial destination as 10.1.*.* addresses, which is a
> completely different internal network.
> Typically there will be no internal routing or communication from a
> 192.168..*.* address to/from a 10.*.*.* address without a custom routing
> server with 2 network connections, one from each IP set and to act as
> the DNS intermediary for routing. Otherwise for network/internet
> connections, the computer/browser sees its own IP as local network, and
> everything else including 10.*.*.* as an external address out on the
> internet. I would suggest getting both the browsing computer and the
> server on the same IP subset, as in 192.168.122.x or 10.1.4.x, otherwise
> these issues are likely to continue.

WTF? Thats IPv4, no IP-range segmentation in that protocol except 127/8.
As long as a route exists 192.* can talk to 10.* no problems.

Also, he has indicated direct connectivity tests are working fine already.

Also, Squid is an application layer gateway. As long as Squid has access
to both networks it should be fine regardless of any obstructions direct
access might have. In fact its often used to get around that type of
problem, such as IPv4<->IPv6 translation.


> 
> 3. Next in the squid.conf is http_port which should be port number only,
> no IP address, especially 0.0.0.0 which can cause conflicts with squid
> 3.x versions. Best bet is use just port only, as in: "http_port 3128" or
> in your case "http_port 8080", which is the port (with server IP found
> in ifconfig) the browser will use to connect through the squid server.

Nope again. The IP address is fine. In the case of 0.0.0.0 it forces
Squid to IPv4-only service on that port. Making way for another service
to run IPv6 in parallel with same ports. Or IPv6 clients to get rejected
at TCP level.

>From the logs presented we can see traffic arriving at Squid and being
serviced. Just not with the desired responses.


Amos


From squid3 at treenet.co.nz  Fri Jul 24 21:50:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 09:50:08 +1200
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <CA+8bHvxaSmLjvviyONXQo-my3mxRw7DvhsPjg1=R31+SPKbBYg@mail.gmail.com>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <CA+8bHvxW=goVDdmBLjF-tdbGOup7ejAcbTTOQFrxN4rZ12aQMA@mail.gmail.com>
 <CA+8bHvxaSmLjvviyONXQo-my3mxRw7DvhsPjg1=R31+SPKbBYg@mail.gmail.com>
Message-ID: <55B2B310.6050608@treenet.co.nz>

On 25/07/2015 4:59 a.m., Jagannath Naidu wrote:
> 1. Its not  a transparent proxy.
> 
> 2. My clients get wpad configuration from AD server. So there are two
> question.
>  2.1 :I know that wpad is used to identify proxy server and port(and rest
> other bypass rules).  When clients resolve to wpad.abc.com, is there way
> that I can overwrite the wpad file off client. Like creating a webserver to
> to serve wpad file and I change /etc/hosts file to "<myhwebserveripaddress>
> wpad.abc.com"
> 2.2 Is there any other way to tell clients via squid server, to do not come
> to squid server and re initiate the request.

Exactly that if you wish. Its not clear whether WPAD is the problem though.

The fact that you have Squid logs showing access indicates the traffic
us actually getting there okay. The responses do seem to be coming back
from 10.* servers as well.
So what is happening is something is causing those servers not to like
the traffic being requested from them.


> 
> On 24 July 2015 at 21:10, Jagannath Naidu <
> jagannath.naidu at fosteringlinux.com> wrote:
> 
>>
>>
>> On 24 July 2015 at 21:05, Jagannath Naidu <
>> jagannath.naidu at fosteringlinux.com> wrote:
>>
>>> Dear List,
>>>
>>> I have been working on this for last two weeks, but never got it
>>> resolved.
>>>
>>> We have a application server (SERVER) in our local network and a desktop
>>>  application (CLIENT). The application picks proxy settings from IE. And we
>>> also have a wensense proxy server
>>>
>>> case 1: when there is no proxy set
>>> application works. No logs in squid server access.log
>>>
>>> case 2: when proxy ip address set and checked "bypass local network"
>>> application works. No logs in squid server access.log
>>>
>>> case 3: when proxy ip address is set to wensense proxy server. UNCHECKED
>>> "bypass local network"
>>> application works. We dont have access to websense server and hence we
>>> can not check logs

Can you explain "not works" in any better detail?
 application expected vs actual behaviour?
 if you can relate that to particular HTTP messages even better.


>>>
>>>
>>> case 4: when proxy ip address is set to proxy server ip address.
>>> UNCHECKED "bypass local network"
>>> application does not work :-(. Below are the logs.
>>>
>>>
>>> 1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET
>>> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
>>> - HIER_DIRECT/10.1.4.46 text/html

404. The URL you see above references an object that does not exist on
that server.

Things to look into:
 Is it the right server?
 Is it the right URL?
 Why was it requested?
 Does the server actually know its "dlwvdialce.htmedia.net" name?


>>> 1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT
>>> 0.client-channel.google.com:443 - HIER_NONE/- text/html
>>> 1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT
>>> 0.client-channel.google.com:443 - HIER_NONE/- text/html


Authentication. Normal I think.

>>> 1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET
>>> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
>>> 10.1.4.46 text/html

Same as the first 404'd URL.

>>> 1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST
>>> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
>>> - HIER_NONE/- text/html

503 usually indicates the attempted server failed.

Makes sense if TCP to cs-711-core.htmedia.net port 8180 did not work.
Which would also match the lack of server IP in the log.


>>>
>>> UPDATE: correct logs
>>
>> 1437752279.774      6 192.168.122.1 TCP_MISS/404 579 GET
>> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
>> - HIER_DIRECT/10.1.4.46 text/html
>> 1437752281.854      5 192.168.122.1 TCP_MISS/404 579 GET
>> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
>> 10.1.4.46 text/html
>> 1437752284.265      2 192.168.122.1 TCP_MISS/503 4048 POST
>> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
>> - HIER_NONE/- text/html
>>

Same comments as above.

>>
>>
>>> squid -v
>>> Squid Cache: Version 3.3.8
>>> configure options:  '--build=x86_64-redhat-linux-gnu'
>>> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
>>> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
>>> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
>>> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
>>> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
>>> '--infodir=/usr/share/info' '--disable-strict-error-checking'
>>> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
>>> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
>>> '--with-logdir=$(localstatedir)/log/squid'
>>> '--with-pidfile=$(localstatedir)/run/squid.pid'
>>> '--disable-dependency-tracking' '--enable-eui'
>>> '--enable-follow-x-forwarded-for' '--enable-auth'
>>> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
>>> '--enable-auth-ntlm=smb_lm,fake'
>>> '--enable-auth-digest=file,LDAP,eDirectory'
>>> '--enable-auth-negotiate=kerberos'
>>> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
>>> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
>>> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
>>> '--enable-ident-lookups' '--enable-linux-netfilter'
>>> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
>>> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
>>> '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
>>> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
>>> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
>>> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
>>> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
>>> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
>>> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
>>> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
>>> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
>>> -m64 -mtune=generic -fpie'
>>> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
>>>
>>>
>>> squid.conf
>>>
>>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>> acl localnet src fc00::/7       # RFC 4193 local private network range
>>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
>>> machines
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80          # http
>>> acl Safe_ports port 21          # ftp
>>> acl Safe_ports port 443         # https
>>> acl Safe_ports port 70          # gopher
>>> acl Safe_ports port 210         # wais
>>> acl Safe_ports port 1025-65535  # unregistered ports
>>> acl Safe_ports port 280         # http-mgmt
>>> acl Safe_ports port 488         # gss-http
>>> acl Safe_ports port 591         # filemaker
>>> acl Safe_ports port 777         # multiling http
>>> acl Safe_ports port 8180
>>> acl CONNECT method CONNECT
>>> acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54
>>> 10.1.4.46 10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4

For easier reading:
  acl wvdial dst 10.1.4.45-10.1.4.55/27 10.1.2.4

(at least I think they are all in one /27, double-check that)

>>> http_access allow wvdial
>>> acl dialer dstdomain .htmedia.net
>>> http_access allow dialer
>>> http_access deny !Safe_ports
>>> http_access deny CONNECT !SSL_ports
>>> http_access allow localhost manager
>>> http_access deny manager
>>> visible_hostname = NOIDAPROXY01.MYDOMAIN.NET

 "=" is a funny domain name. I suspect you wanted the domain-name part
of the line to be used instead. Remove the "= " bit.

>>> append_domain  .mydomain.net
>>> ignore_expect_100 on

The ignore_* directive should not be useful in 3.3. You can remove it now.

>>> dns_v4_first on
>>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
>>> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>>> auth_param ntlm children 1000
>>> auth_param ntlm keep_alive off
>>> auth_param basic program /usr/bin/ntlm_auth
>>> --helper-protocol=squid-2.5-basic
>>> auth_param basic children 100
>>> auth_param basic realm Squid proxy-caching web server
>>> auth_param basic credentialsttl 2 hours
>>> acl auth proxy_auth REQUIRED
>>> http_access allow all auth

"allow all auth" means the same as "allow auth".

"all" only has meaning on the end (right-hand side) of the line which
would otherwise end in a proxy_auth ACL.
It should either be on the end of that line, or not used at all.


>>> http_access allow localnet
>>> http_access allow localhost
>>> http_access deny all
>>> http_port 0.0.0.0:8080
>>> coredump_dir /var/spool/squid
>>> refresh_pattern ^ftp:           1440    20%     10080
>>> refresh_pattern ^gopher:        1440    0%      1440
>>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>> refresh_pattern .               0       20%     4320
>>>
>>>
>>> It was the same behavior with squid-3.1.10-19. I thought, upgrading to
>>> squid 3.3 would help. Please help me resolving this mystery.

Looks to me like the server at 10.1.4.46 does not know what to do with
the URLs requested.

I would start looking at whether the application is actually supposed to
be going there for its requests.

Amos


From squid3 at treenet.co.nz  Fri Jul 24 22:07:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 10:07:18 +1200
Subject: [squid-users] cannot leave empty workers
In-Reply-To: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>
References: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>
Message-ID: <55B2B716.3070500@treenet.co.nz>

On 25/07/2015 7:24 a.m., Alex Wu wrote:
> If I define 4 workers, and use the following way to allocate workers:
> 
> if ${process_number} = 4
> //do something
> else
> endif

The "else" means the wrapped config bit applies to *all* workers and
processes of Squid except the one in the if-condition (process #4). It
is optional.

if ${process_number} = 4
 # do something
endif

It does not even do anything in the code except invert a bitmask. An
"endif" then erases that bitmask. So an empty "else" is effectively
doing nothing at all.
 Just like one would expect reading that config.

The bug is elsewhere (sorry for the pun).

> 
> I leave other workers as empty after else, then we encounter this error:
> 
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
> 
> If I fill one more workers,especially ${process_number} = 1, then squid can launch workers now,
> 

Was that really the full config?

I dont see "workers 4" in there at all and something must have been
configured to use the shared memory TLS/SSL session cache.

Amos



From chip_pop at hotmail.com  Fri Jul 24 22:18:38 2015
From: chip_pop at hotmail.com (joe)
Date: Fri, 24 Jul 2015 15:18:38 -0700 (PDT)
Subject: [squid-users] squid youtube caching
In-Reply-To: <55B2A607.8060602@treenet.co.nz>
References: <1437739311277-4672427.post@n4.nabble.com>
 <55B22AC2.5060208@gmail.com> <1437740559798-4672431.post@n4.nabble.com>
 <55B22FCF.2060802@gmail.com> <1437741226992-4672434.post@n4.nabble.com>
 <55B231C3.1030005@gmail.com> <55B25683.4090003@treenet.co.nz>
 <1437752076077-4672455.post@n4.nabble.com> <55B25BAF.5090204@gmail.com>
 <55B2A607.8060602@treenet.co.nz>
Message-ID: <1437776318275-4672472.post@n4.nabble.com>

tks amos for the info 
now i figure why forcing full video on some Firefox i tough v3.5.6 has bug
it turn out its 
some firefox has media.mediasource.webm.enabled;false that will play webm yt
full video and u only see 380p
no other on quality setting if you enable this to true all the quality in yt
setting enabled and yt start working partial video jumping from quality to
another wile watching suks and wat ever i do in squid to force full video on
yt dos int have afect at all weard
is there any way to do it from squid or they force it with all new browser
i try squid on some partial download
with
range_offset_limit none 
quick_abort_min -1 
it work fine 
newer browser suks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-youtube-caching-tp4672389p4672472.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From stan.prescott at gmail.com  Fri Jul 24 22:25:34 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 24 Jul 2015 17:25:34 -0500
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
Message-ID: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>

I have a working implementation of Squid 3.5.5 with ssl-bump. When 3.5.5 is
started with ssl-bump enabled all the squid and ssl_crtd processes start
and Squid functions as intended when bumping ssl sites. However, when I
bump Squid to 3.5.6 squid seems to start but ssl_crtd does not and Squid
3.5.6 cannot successfully bump ssl.

These are the config options I use for both 3.5.5 and 3.5.6.

--enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter \
--enable-removal-policies="heap,lru" --enable-delay-pools
--libdir=/usr/lib/ \
--localstatedir=/var --with-dl --with-openssl --enable-http-violations \
--with-large-files --with-libcap --disable-ipv6
--with-swapdir=/var/spool/squid \
 --enable-ssl-crtd --enable-follow-x-forwarded-for

This is the squid.conf file used for both versions.

visible_hostname smoothwallu3

# Uncomment the following to send debug info to /var/log/squid/cache.log
debug_options ALL,1 33,2 28,9

# ACCESS CONTROLS
# ----------------------------------------------------------------
acl localhostgreen src 10.20.20.1
acl localnetgreen src 10.20.20.0/24

acl SSL_ports port 445 443 441 563
acl Safe_ports port 80            # http
acl Safe_ports port 81            # smoothwall http
acl Safe_ports port 21            # ftp
acl Safe_ports port 445 443 441 563    # https, snews
acl Safe_ports port 70             # gopher
acl Safe_ports port 210               # wais
acl Safe_ports port 1025-65535        # unregistered ports
acl Safe_ports port 280               # http-mgmt
acl Safe_ports port 488               # gss-http
acl Safe_ports port 591               # filemaker
acl Safe_ports port 777               # multiling http

acl CONNECT method CONNECT

# TAG: http_access
# ----------------------------------------------------------------



http_access allow localhost
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localnetgreen
http_access allow CONNECT localnetgreen

http_access allow localhostgreen
http_access allow CONNECT localhostgreen

# http_port and https_port
#----------------------------------------------------------------------------

# For forward-proxy port. Squid uses this port to serve error pages, ftp
icons and communication with other proxies.
#----------------------------------------------------------------------------
http_port 3127

http_port 10.20.20.1:800 intercept
https_port 10.20.20.1:808 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem


http_port 127.0.0.1:800 intercept

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
sslproxy_session_cache_size 4 MB

ssl_bump none localhostgreen

acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1
ssl_bump bump all

sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
sslcrtd_children 5

http_access deny all

cache_replacement_policy heap GDSF
memory_replacement_policy heap GDSF

# CACHE OPTIONS
#
----------------------------------------------------------------------------
cache_effective_user squid
cache_effective_group squid

cache_swap_high 100
cache_swap_low 80

cache_access_log stdio:/var/log/squid/access.log
cache_log /var/log/squid/cache.log
cache_mem 64 MB

cache_dir diskd /var/spool/squid/cache 1024 16 256

maximum_object_size 33 MB

minimum_object_size 0 KB


request_body_max_size 0 KB

# OTHER OPTIONS
#
----------------------------------------------------------------------------
#via off
forwarded_for off

pid_filename /var/run/squid.pid

shutdown_lifetime 30 seconds
icp_port 3130

half_closed_clients off
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache
icap://localhost:1344/squidclamav bypass=off
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache
icap://localhost:1344/squidclamav bypass=on
adaptation_access service_avi_resp allow all

umask 022

logfile_rotate 0

strip_query_terms off

redirect_program /usr/sbin/squidGuard
url_rewrite_children 5

And the cache.log file when starting 3.5.6 with debug options on in
squid.conf




























































































































*2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
adaptation_access2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
adaptation_access2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232|
Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL:
freeing ACL 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL
2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24
17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 2015/07/24 17:15:06 kid1|
Current Directory is /2015/07/24 17:15:06 kid1| Starting Squid Cache
version 3.5.6 for i586-pc-linux-gnu...2015/07/24 17:15:06 kid1| Service
Name: squid2015/07/24 17:15:06 kid1| Process ID 29072015/07/24 17:15:06
kid1| Process Roles: worker2015/07/24 17:15:06 kid1| With 1024 file
descriptors available2015/07/24 17:15:06 kid1| Initializing IP
Cache...2015/07/24 17:15:06 kid1| DNS Socket created at 0.0.0.0, FD
82015/07/24 17:15:06 kid1| Adding nameserver 127.0.0.1 from
/etc/resolv.conf2015/07/24 17:15:06 kid1| helperOpenServers: Starting 0/5
'squidGuard' processes2015/07/24 17:15:06 kid1| helperOpenServers: No
'squidGuard' processes needed.2015/07/24 17:15:06 kid1| Logfile: opening
log stdio:/var/log/squid/access.log2015/07/24 17:15:06 kid1| Unlinkd pipe
opened on FD 152015/07/24 17:15:06 kid1| Store logging disabled2015/07/24
17:15:06 kid1| Swap maxSize 1048576 + 65536 KB, estimated 85700
objects2015/07/24 17:15:06 kid1| Target number of buckets: 42852015/07/24
17:15:06 kid1| Using 8192 Store buckets2015/07/24 17:15:06 kid1| Max Mem
size: 65536 KB2015/07/24 17:15:06 kid1| Max Swap size: 1048576 KB2015/07/24
17:15:06 kid1| Rebuilding storage in /var/spool/squid/cache (dirty
log)2015/07/24 17:15:06 kid1| Using Least Load store dir
selection2015/07/24 17:15:06 kid1| Current Directory is /2015/07/24
17:15:06 kid1| Finished loading MIME types and icons.2015/07/24
17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
clientListenerConnectionOpened constructed, this=0x946d218
[call5]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call
clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)
[call5]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
AsyncCall clientListenerConnectionOpened constructed, this=0x946d3a8
[call7]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call
clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)
[call7]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
AsyncCall clientListenerConnectionOpened constructed, this=0x946d510
[call9]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call
clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)
[call9]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
AsyncCall clientListenerConnectionOpened constructed, this=0x946d6b0
[call11]2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
StartListening.cc(59) will call
clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)
[call11]2015/07/24 17:15:06.578 kid1| HTCP Disabled.2015/07/24 17:15:06.578
kid1| Squid plugin modules loaded: 02015/07/24 17:15:06.578 kid1|
Adaptation support is on2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55)
fireNext: entering clientListenerConnectionOpened(local=0.0.0.0:3127
<http://0.0.0.0:3127> remote=[::] FD 20 flags=9, err=0, HTTP Socket
port=0x946d24c)2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make
call clientListenerConnectionOpened [call5]2015/07/24 17:15:06.578 kid1|
Accepting HTTP Socket connections at local=0.0.0.0:3127
<http://0.0.0.0:3127> remote=[::] FD 20 flags=92015/07/24 17:15:06.578
kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)2015/07/24
17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)2015/07/24
17:15:06.578 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call7]2015/07/24 17:15:06.578 kid1|
Accepting NAT intercepted HTTP Socket connections at local=10.20.20.1:800
<http://10.20.20.1:800> remote=[::] FD 21 flags=412015/07/24 17:15:06.578
kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)2015/07/24
17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)2015/07/24
17:15:06.579 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call9]2015/07/24 17:15:06.579 kid1|
Accepting NAT intercepted HTTP Socket connections at local=127.0.0.1:800
<http://127.0.0.1:800> remote=[::] FD 22 flags=412015/07/24 17:15:06.579
kid1| AsyncCallQueue.cc(57) fireNext: leaving
clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)2015/07/24
17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)2015/07/24
17:15:06.579 kid1| AsyncCall.cc(38) make: make call
clientListenerConnectionOpened [call11]2015/07/24 17:15:06.579 kid1|
Accepting NAT intercepted SSL bumped HTTPS Socket connections at
local=10.20.20.1:808 <http://10.20.20.1:808> remote=[::] FD 23
flags=412015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext:
leaving clientListenerConnectionOpened(local=10.20.20.1:808
<http://10.20.20.1:808> remote=[::] FD 23 flags=41, err=0, HTTPS Socket
port=0x946d6e4)2015/07/24 17:15:06.579 kid1| Accepting ICP messages on
0.0.0.0:3130 <http://0.0.0.0:3130>2015/07/24 17:15:06.579 kid1| Sending ICP
messages from 0.0.0.0:3130 <http://0.0.0.0:3130>2015/07/24 17:15:06.579
kid1| Done reading /var/spool/squid/cache swaplog (12 entries)2015/07/24
17:15:06.579 kid1| Finished rebuilding storage from disk.2015/07/24
17:15:06.579 kid1|        12 Entries scanned2015/07/24 17:15:06.579
kid1|         0 Invalid entries.2015/07/24 17:15:06.579 kid1|         0
With invalid flags.2015/07/24 17:15:06.579 kid1|        12 Objects
loaded.2015/07/24 17:15:06.579 kid1|         0 Objects expired.2015/07/24
17:15:06.579 kid1|         0 Objects cancelled.2015/07/24 17:15:06.579
kid1|         0 Duplicate URLs purged.2015/07/24 17:15:06.579 kid1|
0 Swapfile clashes avoided.2015/07/24 17:15:06.579 kid1|   Took 0.06
seconds (210.47 objects/sec).2015/07/24 17:15:06.579 kid1| Beginning
Validation Procedure2015/07/24 17:15:06.579 kid1|   Completed Validation
Procedure2015/07/24 17:15:06.579 kid1|   Validated 12 Entries2015/07/24
17:15:06.579 kid1|   store_swap_size = 1444.00 KB2015/07/24 17:15:07 kid1|
storeLateRelease: released 0 objects*

Any help or suggestions greatly appreciated.

Regards

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/8415964d/attachment.htm>

From alex_wu2012 at hotmail.com  Fri Jul 24 22:28:06 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Fri, 24 Jul 2015 15:28:06 -0700
Subject: [squid-users] cannot leave empty workers
In-Reply-To: <55B2B716.3070500@treenet.co.nz>
References: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>,
 <55B2B716.3070500@treenet.co.nz>
Message-ID: <BAY181-W48911B30204BDB0252A1A383810@phx.gbl>

There is a problem

The code isSslServer looks for https configuration. If no one found, it will not create /run/shm/ssl_session_cache.shm.

Late, the code somewhere else can not find it, so the process would not start it self.

I am not clear which worker is called first to initialize_session_cache.

We see master and coordinator start properly. so I suspect coordinator might be one to initialze ssl_session_cache?

Or since all my http_port are listed in worker process 4, so isSllServer cannot find https_port, so it will not initialize ssl_session_cache.shm.

Somewhere, something is odd.

THX

Alex
> To: squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Sat, 25 Jul 2015 10:07:18 +1200
> Subject: Re: [squid-users] cannot leave empty workers
> 
> On 25/07/2015 7:24 a.m., Alex Wu wrote:
> > If I define 4 workers, and use the following way to allocate workers:
> > 
> > if ${process_number} = 4
> > //do something
> > else
> > endif
> 
> The "else" means the wrapped config bit applies to *all* workers and
> processes of Squid except the one in the if-condition (process #4). It
> is optional.
> 
> if ${process_number} = 4
>  # do something
> endif
> 
> It does not even do anything in the code except invert a bitmask. An
> "endif" then erases that bitmask. So an empty "else" is effectively
> doing nothing at all.
>  Just like one would expect reading that config.
> 
> The bug is elsewhere (sorry for the pun).
> 
> > 
> > I leave other workers as empty after else, then we encounter this error:
> > 
> > FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
> > 
> > If I fill one more workers,especially ${process_number} = 1, then squid can launch workers now,
> > 
> 
> Was that really the full config?
> 
> I dont see "workers 4" in there at all and something must have been
> configured to use the shared memory TLS/SSL session cache.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/511bd95d/attachment.htm>

From alex_wu2012 at hotmail.com  Fri Jul 24 23:53:06 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Fri, 24 Jul 2015 16:53:06 -0700
Subject: [squid-users] cannot leave empty workers
In-Reply-To: <BAY181-W48911B30204BDB0252A1A383810@phx.gbl>
References: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>, ,
 <55B2B716.3070500@treenet.co.nz>,
 <BAY181-W48911B30204BDB0252A1A383810@phx.gbl>
Message-ID: <BAY181-W79195CF421D92AE7D025A183810@phx.gbl>

further analysis indicated that master process created quid-ssl_session_cache.shm.

In other words, it needs a https_port or http_port with ssl-bump in outside any process number to create this shared memeory segment.

Furthermore, the code  should be simplied like this:

diff --git a/squid-3.5.6/src/ssl/support.cc b/squid-3.5.6/src/ssl/support.cc
index 85305ce..0ce95f9 100644
--- a/squid-3.5.6/src/ssl/support.cc
+++ b/squid-3.5.6/src/ssl/support.cc
@@ -2084,9 +2084,6 @@ SharedSessionCacheRr::useConfig()
 void
 SharedSessionCacheRr::create()
 {
-    if (!isSslServer()) //no need to configure ssl session cache.
-        return;
-
     int items;
     items = Config.SSL.sessionCacheSize / sizeof(Ipc::MemMap::Slot);
     if (items)



This code is called in master that may not have configuration to ensure isSsslServer return true.

Alex
From: alex_wu2012 at hotmail.com
To: squid3 at treenet.co.nz; squid-users at lists.squid-cache.org
Date: Fri, 24 Jul 2015 15:28:06 -0700
Subject: Re: [squid-users] cannot leave empty workers




There is a problem

The code isSslServer looks for https configuration. If no one found, it will not create /run/shm/ssl_session_cache.shm.

Late, the code somewhere else can not find it, so the process would not start it self.

I am not clear which worker is called first to initialize_session_cache.

We see master and coordinator start properly. so I suspect coordinator might be one to initialze ssl_session_cache?

Or since all my http_port are listed in worker process 4, so isSllServer cannot find https_port, so it will not initialize ssl_session_cache.shm.

Somewhere, something is odd.

THX

Alex


> To: squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Sat, 25 Jul 2015 10:07:18 +1200
> Subject: Re: [squid-users] cannot leave empty workers
> 
> On 25/07/2015 7:24 a.m., Alex Wu wrote:
> > If I define 4 workers, and use the following way to allocate workers:
> > 
> > if ${process_number} = 4
> > //do something
> > else
> > endif
> 
> The "else" means the wrapped config bit applies to *all* workers and
> processes of Squid except the one in the if-condition (process #4). It
> is optional.
> 
> if ${process_number} = 4
>  # do something
> endif
> 
> It does not even do anything in the code except invert a bitmask. An
> "endif" then erases that bitmask. So an empty "else" is effectively
> doing nothing at all.
>  Just like one would expect reading that config.
> 
> The bug is elsewhere (sorry for the pun).
> 
> > 
> > I leave other workers as empty after else, then we encounter this error:
> > 
> > FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
> > 
> > If I fill one more workers,especially ${process_number} = 1, then squid can launch workers now,
> > 
> 
> Was that really the full config?
> 
> I dont see "workers 4" in there at all and something must have been
> configured to use the shared memory TLS/SSL session cache.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/2de739ec/attachment.htm>

From jlay at slave-tothe-box.net  Sat Jul 25 00:07:09 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Jul 2015 18:07:09 -0600
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
Message-ID: <1437782829.7042.5.camel@JamesiMac>

On Fri, 2015-07-24 at 17:25 -0500, Stanford Prescott wrote:
> I have a working implementation of Squid 3.5.5 with ssl-bump. When
> 3.5.5 is started with ssl-bump enabled all the squid and ssl_crtd
> processes start and Squid functions as intended when bumping ssl
> sites. However, when I bump Squid to 3.5.6 squid seems to start but
> ssl_crtd does not and Squid 3.5.6 cannot successfully bump ssl.
> 
> 
> 
> These are the config options I use for both 3.5.5 and 3.5.6.
> 
> 
> --enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter \
> --enable-removal-policies="heap,lru" --enable-delay-pools
> --libdir=/usr/lib/ \
> --localstatedir=/var --with-dl --with-openssl --enable-http-violations
> \
> --with-large-files --with-libcap --disable-ipv6
> --with-swapdir=/var/spool/squid \
>  --enable-ssl-crtd --enable-follow-x-forwarded-for
> 
> 
> 
> 
> 
> This is the squid.conf file used for both versions.
> 
> 
> visible_hostname smoothwallu3
> 
> # Uncomment the following to send debug info
> to /var/log/squid/cache.log
> debug_options ALL,1 33,2 28,9
> 
> # ACCESS CONTROLS
> # ----------------------------------------------------------------
> acl localhostgreen src 10.20.20.1
> acl localnetgreen src 10.20.20.0/24
> 
> acl SSL_ports port 445 443 441 563
> acl Safe_ports port 80            # http
> acl Safe_ports port 81            # smoothwall http
> acl Safe_ports port 21            # ftp 
> acl Safe_ports port 445 443 441 563    # https, snews
> acl Safe_ports port 70             # gopher
> acl Safe_ports port 210               # wais  
> acl Safe_ports port 1025-65535        # unregistered ports
> acl Safe_ports port 280               # http-mgmt
> acl Safe_ports port 488               # gss-http 
> acl Safe_ports port 591               # filemaker
> acl Safe_ports port 777               # multiling http
> 
> acl CONNECT method CONNECT
> 
> # TAG: http_access
> # ----------------------------------------------------------------
> 
> 
> 
> http_access allow localhost
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localnetgreen
> http_access allow CONNECT localnetgreen
> 
> http_access allow localhostgreen
> http_access allow CONNECT localhostgreen
> 
> # http_port and https_port
> #----------------------------------------------------------------------------
> 
> # For forward-proxy port. Squid uses this port to serve error pages,
> ftp icons and communication with other proxies.
> #----------------------------------------------------------------------------
> http_port 3127
> 
> http_port 10.20.20.1:800 intercept
> https_port 10.20.20.1:808 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
> 
> 
> http_port 127.0.0.1:800 intercept
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_session_cache_size 4 MB
> 
> ssl_bump none localhostgreen
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1
> ssl_bump bump all
> 
> sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd
> -s /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
> sslcrtd_children 5
> 
> http_access deny all
> 
> cache_replacement_policy heap GDSF
> memory_replacement_policy heap GDSF
> 
> # CACHE OPTIONS
> #
> ----------------------------------------------------------------------------
> cache_effective_user squid
> cache_effective_group squid
> 
> cache_swap_high 100
> cache_swap_low 80
> 
> cache_access_log stdio:/var/log/squid/access.log
> cache_log /var/log/squid/cache.log
> cache_mem 64 MB
> 
> cache_dir diskd /var/spool/squid/cache 1024 16 256
> 
> maximum_object_size 33 MB
> 
> minimum_object_size 0 KB
> 
> 
> request_body_max_size 0 KB
> 
> # OTHER OPTIONS
> #
> ----------------------------------------------------------------------------
> #via off
> forwarded_for off
> 
> pid_filename /var/run/squid.pid
> 
> shutdown_lifetime 30 seconds
> icp_port 3130
> 
> half_closed_clients off
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_encode off
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
> icap://localhost:1344/squidclamav bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
> icap://localhost:1344/squidclamav bypass=on
> adaptation_access service_avi_resp allow all
> 
> umask 022
> 
> logfile_rotate 0
> 
> strip_query_terms off
> 
> redirect_program /usr/sbin/squidGuard
> url_rewrite_children 5
> 
> 
> 
> And the cache.log file when starting 3.5.6 with debug options on in
> squid.conf
> 
> 
> 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
> adaptation_access
> 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
> adaptation_access
> 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
> 2015/07/24 17:15:06 kid1| Current Directory is /
> 2015/07/24 17:15:06 kid1| Starting Squid Cache version 3.5.6 for
> i586-pc-linux-gnu...
> 2015/07/24 17:15:06 kid1| Service Name: squid
> 2015/07/24 17:15:06 kid1| Process ID 2907
> 2015/07/24 17:15:06 kid1| Process Roles: worker
> 2015/07/24 17:15:06 kid1| With 1024 file descriptors available
> 2015/07/24 17:15:06 kid1| Initializing IP Cache...
> 2015/07/24 17:15:06 kid1| DNS Socket created at 0.0.0.0, FD 8
> 2015/07/24 17:15:06 kid1| Adding nameserver 127.0.0.1
> from /etc/resolv.conf
> 2015/07/24 17:15:06 kid1| helperOpenServers: Starting 0/5 'squidGuard'
> processes
> 2015/07/24 17:15:06 kid1| helperOpenServers: No 'squidGuard' processes
> needed.
> 2015/07/24 17:15:06 kid1| Logfile: opening log
> stdio:/var/log/squid/access.log
> 2015/07/24 17:15:06 kid1| Unlinkd pipe opened on FD 15
> 2015/07/24 17:15:06 kid1| Store logging disabled
> 2015/07/24 17:15:06 kid1| Swap maxSize 1048576 + 65536 KB, estimated
> 85700 objects
> 2015/07/24 17:15:06 kid1| Target number of buckets: 4285
> 2015/07/24 17:15:06 kid1| Using 8192 Store buckets
> 2015/07/24 17:15:06 kid1| Max Mem  size: 65536 KB
> 2015/07/24 17:15:06 kid1| Max Swap size: 1048576 KB
> 2015/07/24 17:15:06 kid1| Rebuilding storage in /var/spool/squid/cache
> (dirty log)
> 2015/07/24 17:15:06 kid1| Using Least Load store dir selection
> 2015/07/24 17:15:06 kid1| Current Directory is /
> 2015/07/24 17:15:06 kid1| Finished loading MIME types and icons.
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
> AsyncCall clientListenerConnectionOpened constructed, this=0x946d218
> [call5]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=0.0.0.0:3127 remote=[::] FD 20
> flags=9, err=0, HTTP Socket port=0x946d24c) [call5]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
> AsyncCall clientListenerConnectionOpened constructed, this=0x946d3a8
> [call7]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:800 remote=[::] FD 21
> flags=41, err=0, HTTP Socket port=0x946d3dc) [call7]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
> AsyncCall clientListenerConnectionOpened constructed, this=0x946d510
> [call9]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=127.0.0.1:800 remote=[::] FD 22
> flags=41, err=0, HTTP Socket port=0x946d544) [call9]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The
> AsyncCall clientListenerConnectionOpened constructed, this=0x946d6b0
> [call11]
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:808 remote=[::] FD 23
> flags=41, err=0, HTTPS Socket port=0x946d6e4) [call11]
> 2015/07/24 17:15:06.578 kid1| HTCP Disabled.
> 2015/07/24 17:15:06.578 kid1| Squid plugin modules loaded: 0
> 2015/07/24 17:15:06.578 kid1| Adaptation support is on
> 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=0.0.0.0:3127 remote=[::] FD 20
> flags=9, err=0, HTTP Socket port=0x946d24c)
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call5]
> 2015/07/24 17:15:06.578 kid1| Accepting HTTP Socket connections at
> local=0.0.0.0:3127 remote=[::] FD 20 flags=9
> 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=0.0.0.0:3127 remote=[::] FD 20
> flags=9, err=0, HTTP Socket port=0x946d24c)
> 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:800 remote=[::] FD 21
> flags=41, err=0, HTTP Socket port=0x946d3dc)
> 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call7]
> 2015/07/24 17:15:06.578 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=10.20.20.1:800 remote=[::] FD 21 flags=41
> 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:800 remote=[::] FD 21
> flags=41, err=0, HTTP Socket port=0x946d3dc)
> 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=127.0.0.1:800 remote=[::] FD 22
> flags=41, err=0, HTTP Socket port=0x946d544)
> 2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call9]
> 2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=127.0.0.1:800 remote=[::] FD 22 flags=41
> 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=127.0.0.1:800 remote=[::] FD 22
> flags=41, err=0, HTTP Socket port=0x946d544)
> 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:808 remote=[::] FD 23
> flags=41, err=0, HTTPS Socket port=0x946d6e4)
> 2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call11]
> 2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted SSL bumped
> HTTPS Socket connections at local=10.20.20.1:808 remote=[::] FD 23
> flags=41
> 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:808 remote=[::] FD 23
> flags=41, err=0, HTTPS Socket port=0x946d6e4)
> 2015/07/24 17:15:06.579 kid1| Accepting ICP messages on 0.0.0.0:3130
> 2015/07/24 17:15:06.579 kid1| Sending ICP messages from 0.0.0.0:3130
> 2015/07/24 17:15:06.579 kid1| Done reading /var/spool/squid/cache
> swaplog (12 entries)
> 2015/07/24 17:15:06.579 kid1| Finished rebuilding storage from disk.
> 2015/07/24 17:15:06.579 kid1|        12 Entries scanned
> 2015/07/24 17:15:06.579 kid1|         0 Invalid entries.
> 2015/07/24 17:15:06.579 kid1|         0 With invalid flags.
> 2015/07/24 17:15:06.579 kid1|        12 Objects loaded.
> 2015/07/24 17:15:06.579 kid1|         0 Objects expired.
> 2015/07/24 17:15:06.579 kid1|         0 Objects cancelled.
> 2015/07/24 17:15:06.579 kid1|         0 Duplicate URLs purged.
> 2015/07/24 17:15:06.579 kid1|         0 Swapfile clashes avoided.
> 2015/07/24 17:15:06.579 kid1|   Took 0.06 seconds (210.47
> objects/sec).
> 2015/07/24 17:15:06.579 kid1| Beginning Validation Procedure
> 2015/07/24 17:15:06.579 kid1|   Completed Validation Procedure
> 2015/07/24 17:15:06.579 kid1|   Validated 12 Entries
> 2015/07/24 17:15:06.579 kid1|   store_swap_size = 1444.00 KB
> 2015/07/24 17:15:07 kid1| storeLateRelease: released 0 objects
> 
> 
> 
> 
> 
> Any help or suggestions greatly appreciated.
> 
> 
> 
> Regards
> 
> 
> 
> Stan
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


I do not experience this issue:

[18:04:56 jlay:~/nobackup/build$] ps aux | egrep "ssl|squid"
root      3173  0.0  0.0  18840   372 ?        Ss   Jul23
0:00 /opt/sbin/squid
nobody    3175  0.0  1.2  52856 39744 ?        S    Jul23   0:47
(squid-1)
nobody    3177  0.0  0.0   5916  2040 ?        S    Jul23   0:05
(ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
nobody    3178  0.0  0.0   5828  1840 ?        S    Jul23   0:00
(ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
nobody    3179  0.0  0.0   5828  1708 ?        S    Jul23   0:00
(ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
nobody    3180  0.0  0.0   5648   912 ?        S    Jul23   0:00
(ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
nobody    3181  0.0  0.0   5648   912 ?        S    Jul23   0:00
(ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096

my config line:
./configure --prefix=/opt --with-openssl --enable-ssl --enable-ssl-crtd
--enable-linux-netfilter --enable-follow-x-forwarded-for
--with-large-files --sysconfdir=/opt/etc/squid
--enable-external-acl-helpers=none

Squid Cache: Version 3.5.6

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/a04fea00/attachment.htm>

From stan.prescott at gmail.com  Sat Jul 25 00:15:05 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 24 Jul 2015 19:15:05 -0500
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <1437782829.7042.5.camel@JamesiMac>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
Message-ID: <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>

Thanks for that. Any ideas why I am experiencing that?

Stan


On Fri, Jul 24, 2015 at 7:07 PM, James Lay <jlay at slave-tothe-box.net> wrote:

>  On Fri, 2015-07-24 at 17:25 -0500, Stanford Prescott wrote:
>
> I have a working implementation of Squid 3.5.5 with ssl-bump. When 3.5.5
> is started with ssl-bump enabled all the squid and ssl_crtd processes start
> and Squid functions as intended when bumping ssl sites. However, when I
> bump Squid to 3.5.6 squid seems to start but ssl_crtd does not and Squid
> 3.5.6 cannot successfully bump ssl.
>
>
>  These are the config options I use for both 3.5.5 and 3.5.6.
>
>  --enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter \
> --enable-removal-policies="heap,lru" --enable-delay-pools
> --libdir=/usr/lib/ \
> --localstatedir=/var --with-dl --with-openssl --enable-http-violations \
> --with-large-files --with-libcap --disable-ipv6
> --with-swapdir=/var/spool/squid \
>  --enable-ssl-crtd --enable-follow-x-forwarded-for
>
>
>
>  This is the squid.conf file used for both versions.
>
>  visible_hostname smoothwallu3
>
> # Uncomment the following to send debug info to /var/log/squid/cache.log
> debug_options ALL,1 33,2 28,9
>
> # ACCESS CONTROLS
> # ----------------------------------------------------------------
> acl localhostgreen src 10.20.20.1
> acl localnetgreen src 10.20.20.0/24
>
> acl SSL_ports port 445 443 441 563
> acl Safe_ports port 80            # http
> acl Safe_ports port 81            # smoothwall http
> acl Safe_ports port 21            # ftp
> acl Safe_ports port 445 443 441 563    # https, snews
> acl Safe_ports port 70             # gopher
> acl Safe_ports port 210               # wais
> acl Safe_ports port 1025-65535        # unregistered ports
> acl Safe_ports port 280               # http-mgmt
> acl Safe_ports port 488               # gss-http
> acl Safe_ports port 591               # filemaker
> acl Safe_ports port 777               # multiling http
>
> acl CONNECT method CONNECT
>
> # TAG: http_access
> # ----------------------------------------------------------------
>
>
>
> http_access allow localhost
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
>
> http_access allow localnetgreen
> http_access allow CONNECT localnetgreen
>
> http_access allow localhostgreen
> http_access allow CONNECT localhostgreen
>
> # http_port and https_port
>
> #----------------------------------------------------------------------------
>
> # For forward-proxy port. Squid uses this port to serve error pages, ftp
> icons and communication with other proxies.
>
> #----------------------------------------------------------------------------
> http_port 3127
>
> http_port 10.20.20.1:800 intercept
> https_port 10.20.20.1:808 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>
>
> http_port 127.0.0.1:800 intercept
>
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_session_cache_size 4 MB
>
> ssl_bump none localhostgreen
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1
> ssl_bump bump all
>
> sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
> sslcrtd_children 5
>
> http_access deny all
>
> cache_replacement_policy heap GDSF
> memory_replacement_policy heap GDSF
>
> # CACHE OPTIONS
> #
> ----------------------------------------------------------------------------
> cache_effective_user squid
> cache_effective_group squid
>
> cache_swap_high 100
> cache_swap_low 80
>
> cache_access_log stdio:/var/log/squid/access.log
> cache_log /var/log/squid/cache.log
> cache_mem 64 MB
>
> cache_dir diskd /var/spool/squid/cache 1024 16 256
>
> maximum_object_size 33 MB
>
> minimum_object_size 0 KB
>
>
> request_body_max_size 0 KB
>
> # OTHER OPTIONS
> #
> ----------------------------------------------------------------------------
> #via off
> forwarded_for off
>
> pid_filename /var/run/squid.pid
>
> shutdown_lifetime 30 seconds
> icp_port 3130
>
> half_closed_clients off
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_encode off
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
> icap://localhost:1344/squidclamav bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
> icap://localhost:1344/squidclamav bypass=on
> adaptation_access service_avi_resp allow all
>
> umask 022
>
> logfile_rotate 0
>
> strip_query_terms off
>
> redirect_program /usr/sbin/squidGuard
> url_rewrite_children 5
>
>
>  And the cache.log file when starting 3.5.6 with debug options on in
> squid.conf
>
>  *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
> adaptation_access*
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL adaptation_access*
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06 kid1| Current Directory is /*
> *2015/07/24 17:15:06 kid1| Starting Squid Cache version 3.5.6 for
> i586-pc-linux-gnu...*
> *2015/07/24 17:15:06 kid1| Service Name: squid*
> *2015/07/24 17:15:06 kid1| Process ID 2907*
> *2015/07/24 17:15:06 kid1| Process Roles: worker*
> *2015/07/24 17:15:06 kid1| With 1024 file descriptors available*
> *2015/07/24 17:15:06 kid1| Initializing IP Cache...*
> *2015/07/24 17:15:06 kid1| DNS Socket created at 0.0.0.0, FD 8*
> *2015/07/24 17:15:06 kid1| Adding nameserver 127.0.0.1 from
> /etc/resolv.conf*
> *2015/07/24 17:15:06 kid1| helperOpenServers: Starting 0/5 'squidGuard'
> processes*
> *2015/07/24 17:15:06 kid1| helperOpenServers: No 'squidGuard' processes
> needed.*
> *2015/07/24 17:15:06 kid1| Logfile: opening log
> stdio:/var/log/squid/access.log*
> *2015/07/24 17:15:06 kid1| Unlinkd pipe opened on FD 15*
> *2015/07/24 17:15:06 kid1| Store logging disabled*
> *2015/07/24 17:15:06 kid1| Swap maxSize 1048576 + 65536 KB, estimated
> 85700 objects*
> *2015/07/24 17:15:06 kid1| Target number of buckets: 4285*
> *2015/07/24 17:15:06 kid1| Using 8192 Store buckets*
> *2015/07/24 17:15:06 kid1| Max Mem  size: 65536 KB*
> *2015/07/24 17:15:06 kid1| Max Swap size: 1048576 KB*
> *2015/07/24 17:15:06 kid1| Rebuilding storage in /var/spool/squid/cache
> (dirty log)*
> *2015/07/24 17:15:06 kid1| Using Least Load store dir selection*
> *2015/07/24 17:15:06 kid1| Current Directory is /*
> *2015/07/24 17:15:06 kid1| Finished loading MIME types and icons.*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d218 [call5]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c) [call5]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d3a8 [call7]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc) [call7]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d510 [call9]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544) [call9]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d6b0 [call11]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4) [call11]*
> *2015/07/24 17:15:06.578 kid1| HTCP Disabled.*
> *2015/07/24 17:15:06.578 kid1| Squid plugin modules loaded: 0*
> *2015/07/24 17:15:06.578 kid1| Adaptation support is on*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call5]*
> *2015/07/24 17:15:06.578 kid1| Accepting HTTP Socket connections at
> local=0.0.0.0:3127 <http://0.0.0.0:3127> remote=[::] FD 20 flags=9*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call7]*
> *2015/07/24 17:15:06.578 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=10.20.20.1:800 <http://10.20.20.1:800> remote=[::] FD
> 21 flags=41*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)*
> *2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call9]*
> *2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=127.0.0.1:800 <http://127.0.0.1:800> remote=[::] FD 22
> flags=41*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)*
> *2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call11]*
> *2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted SSL bumped HTTPS
> Socket connections at local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)*
> *2015/07/24 17:15:06.579 kid1| Accepting ICP messages on 0.0.0.0:3130
> <http://0.0.0.0:3130>*
> *2015/07/24 17:15:06.579 kid1| Sending ICP messages from 0.0.0.0:3130
> <http://0.0.0.0:3130>*
> *2015/07/24 17:15:06.579 kid1| Done reading /var/spool/squid/cache swaplog
> (12 entries)*
> *2015/07/24 17:15:06.579 kid1| Finished rebuilding storage from disk.*
> *2015/07/24 17:15:06.579 kid1|        12 Entries scanned*
> *2015/07/24 17:15:06.579 kid1|         0 Invalid entries.*
> *2015/07/24 17:15:06.579 kid1|         0 With invalid flags.*
> *2015/07/24 17:15:06.579 kid1|        12 Objects loaded.*
> *2015/07/24 17:15:06.579 kid1|         0 Objects expired.*
> *2015/07/24 17:15:06.579 kid1|         0 Objects cancelled.*
> *2015/07/24 17:15:06.579 kid1|         0 Duplicate URLs purged.*
> *2015/07/24 17:15:06.579 kid1|         0 Swapfile clashes avoided.*
> *2015/07/24 17:15:06.579 kid1|   Took 0.06 seconds (210.47 objects/sec).*
> *2015/07/24 17:15:06.579 kid1| Beginning Validation Procedure*
> *2015/07/24 17:15:06.579 kid1|   Completed Validation Procedure*
> *2015/07/24 17:15:06.579 kid1|   Validated 12 Entries*
> *2015/07/24 17:15:06.579 kid1|   store_swap_size = 1444.00 KB*
> *2015/07/24 17:15:07 kid1| storeLateRelease: released 0 objects*
>
>
>
>  Any help or suggestions greatly appreciated.
>
>
>  Regards
>
>
>  Stan
>
>
>
>  _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
> I do not experience this issue:
>
> [18:04:56 jlay <jlay at gateway>:~/nobackup/build$] ps aux | egrep
> "ssl|squid"
> root      3173  0.0  0.0  18840   372 ?        Ss   Jul23   0:00
> /opt/sbin/squid
> nobody    3175  0.0  1.2  52856 39744 ?        S    Jul23   0:47 (squid-1)
> nobody    3177  0.0  0.0   5916  2040 ?        S    Jul23   0:05
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3178  0.0  0.0   5828  1840 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3179  0.0  0.0   5828  1708 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3180  0.0  0.0   5648   912 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3181  0.0  0.0   5648   912 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>
> my config line:
> ./configure --prefix=/opt --with-openssl --enable-ssl --enable-ssl-crtd
> --enable-linux-netfilter --enable-follow-x-forwarded-for --with-large-files
> --sysconfdir=/opt/etc/squid --enable-external-acl-helpers=none
>
> Squid Cache: Version 3.5.6
>
> James
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/aaec97c9/attachment.htm>

From jlay at slave-tothe-box.net  Sat Jul 25 00:24:33 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Jul 2015 18:24:33 -0600
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
Message-ID: <1437783873.7042.9.camel@JamesiMac>

On Fri, 2015-07-24 at 19:15 -0500, Stanford Prescott wrote:
> Thanks for that. Any ideas why I am experiencing that?
> 
> 
> 
> Stan
> 
> 
> 
> 
> On Fri, Jul 24, 2015 at 7:07 PM, James Lay <jlay at slave-tothe-box.net>
> wrote:
> 
>         On Fri, 2015-07-24 at 17:25 -0500, Stanford Prescott wrote: 
>         
>         > I have a working implementation of Squid 3.5.5 with
>         > ssl-bump. When 3.5.5 is started with ssl-bump enabled all
>         > the squid and ssl_crtd processes start and Squid functions
>         > as intended when bumping ssl sites. However, when I bump
>         > Squid to 3.5.6 squid seems to start but ssl_crtd does not
>         > and Squid 3.5.6 cannot successfully bump ssl.
>         > 
>         > 
>         > These are the config options I use for both 3.5.5 and 3.5.6.
>         > 
>         > --enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter \
>         > --enable-removal-policies="heap,lru" --enable-delay-pools
>         > --libdir=/usr/lib/ \
>         > --localstatedir=/var --with-dl --with-openssl
>         > --enable-http-violations \
>         > --with-large-files --with-libcap --disable-ipv6
>         > --with-swapdir=/var/spool/squid \
>         >  --enable-ssl-crtd --enable-follow-x-forwarded-for
>         > 
>         > 
>         > 
>         > This is the squid.conf file used for both versions.
>         > 
>         > visible_hostname smoothwallu3
>         > 
>         > # Uncomment the following to send debug info
>         > to /var/log/squid/cache.log
>         > debug_options ALL,1 33,2 28,9
>         > 
>         > # ACCESS CONTROLS
>         > #
>         > ----------------------------------------------------------------
>         > acl localhostgreen src 10.20.20.1
>         > acl localnetgreen src 10.20.20.0/24
>         > 
>         > acl SSL_ports port 445 443 441 563
>         > acl Safe_ports port 80            # http
>         > acl Safe_ports port 81            # smoothwall http
>         > acl Safe_ports port 21            # ftp 
>         > acl Safe_ports port 445 443 441 563    # https, snews
>         > acl Safe_ports port 70             # gopher
>         > acl Safe_ports port 210               # wais  
>         > acl Safe_ports port 1025-65535        # unregistered ports
>         > acl Safe_ports port 280               # http-mgmt
>         > acl Safe_ports port 488               # gss-http 
>         > acl Safe_ports port 591               # filemaker
>         > acl Safe_ports port 777               # multiling http
>         > 
>         > acl CONNECT method CONNECT
>         > 
>         > # TAG: http_access
>         > #
>         > ----------------------------------------------------------------
>         > 
>         > 
>         > 
>         > http_access allow localhost
>         > http_access deny !Safe_ports
>         > http_access deny CONNECT !SSL_ports
>         > 
>         > http_access allow localnetgreen
>         > http_access allow CONNECT localnetgreen
>         > 
>         > http_access allow localhostgreen
>         > http_access allow CONNECT localhostgreen
>         > 
>         > # http_port and https_port
>         > #----------------------------------------------------------------------------
>         > 
>         > # For forward-proxy port. Squid uses this port to serve
>         > error pages, ftp icons and communication with other proxies.
>         > #----------------------------------------------------------------------------
>         > http_port 3127
>         > 
>         > http_port 10.20.20.1:800 intercept
>         > https_port 10.20.20.1:808 intercept ssl-bump
>         > generate-host-certificates=on
>         > dynamic_cert_mem_cache_size=4MB
>         > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>         > 
>         > 
>         > http_port 127.0.0.1:800 intercept
>         > 
>         > sslproxy_cert_error allow all
>         > sslproxy_flags DONT_VERIFY_PEER
>         > sslproxy_session_cache_size 4 MB
>         > 
>         > ssl_bump none localhostgreen
>         > 
>         > acl step1 at_step SslBump1
>         > acl step2 at_step SslBump2
>         > ssl_bump peek step1
>         > ssl_bump bump all
>         > 
>         > sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd
>         > -s /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
>         > sslcrtd_children 5
>         > 
>         > http_access deny all
>         > 
>         > cache_replacement_policy heap GDSF
>         > memory_replacement_policy heap GDSF
>         > 
>         > # CACHE OPTIONS
>         > #
>         > ----------------------------------------------------------------------------
>         > cache_effective_user squid
>         > cache_effective_group squid
>         > 
>         > cache_swap_high 100
>         > cache_swap_low 80
>         > 
>         > cache_access_log stdio:/var/log/squid/access.log
>         > cache_log /var/log/squid/cache.log
>         > cache_mem 64 MB
>         > 
>         > cache_dir diskd /var/spool/squid/cache 1024 16 256
>         > 
>         > maximum_object_size 33 MB
>         > 
>         > minimum_object_size 0 KB
>         > 
>         > 
>         > request_body_max_size 0 KB
>         > 
>         > # OTHER OPTIONS
>         > #
>         > ----------------------------------------------------------------------------
>         > #via off
>         > forwarded_for off
>         > 
>         > pid_filename /var/run/squid.pid
>         > 
>         > shutdown_lifetime 30 seconds
>         > icp_port 3130
>         > 
>         > half_closed_clients off
>         > icap_enable on
>         > icap_send_client_ip on
>         > icap_send_client_username on
>         > icap_client_username_encode off
>         > icap_client_username_header X-Authenticated-User
>         > icap_preview_enable on
>         > icap_preview_size 1024
>         > icap_service service_avi_req reqmod_precache
>         > icap://localhost:1344/squidclamav bypass=off
>         > adaptation_access service_avi_req allow all
>         > icap_service service_avi_resp respmod_precache
>         > icap://localhost:1344/squidclamav bypass=on
>         > adaptation_access service_avi_resp allow all
>         > 
>         > umask 022
>         > 
>         > logfile_rotate 0
>         > 
>         > strip_query_terms off
>         > 
>         > redirect_program /usr/sbin/squidGuard
>         > url_rewrite_children 5
>         > 
>         > 
>         > And the cache.log file when starting 3.5.6 with debug
>         > options on in squid.conf
>         > 
>         > 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
>         > adaptation_access
>         > 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL
>         > adaptation_access
>         > 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL 
>         > 2015/07/24 17:15:06 kid1| Current Directory is /
>         > 2015/07/24 17:15:06 kid1| Starting Squid Cache version 3.5.6
>         > for i586-pc-linux-gnu...
>         > 2015/07/24 17:15:06 kid1| Service Name: squid
>         > 2015/07/24 17:15:06 kid1| Process ID 2907
>         > 2015/07/24 17:15:06 kid1| Process Roles: worker
>         > 2015/07/24 17:15:06 kid1| With 1024 file descriptors
>         > available
>         > 2015/07/24 17:15:06 kid1| Initializing IP Cache...
>         > 2015/07/24 17:15:06 kid1| DNS Socket created at 0.0.0.0, FD
>         > 8
>         > 2015/07/24 17:15:06 kid1| Adding nameserver 127.0.0.1
>         > from /etc/resolv.conf
>         > 2015/07/24 17:15:06 kid1| helperOpenServers: Starting 0/5
>         > 'squidGuard' processes
>         > 2015/07/24 17:15:06 kid1| helperOpenServers: No 'squidGuard'
>         > processes needed.
>         > 2015/07/24 17:15:06 kid1| Logfile: opening log
>         > stdio:/var/log/squid/access.log
>         > 2015/07/24 17:15:06 kid1| Unlinkd pipe opened on FD 15
>         > 2015/07/24 17:15:06 kid1| Store logging disabled
>         > 2015/07/24 17:15:06 kid1| Swap maxSize 1048576 + 65536 KB,
>         > estimated 85700 objects
>         > 2015/07/24 17:15:06 kid1| Target number of buckets: 4285
>         > 2015/07/24 17:15:06 kid1| Using 8192 Store buckets
>         > 2015/07/24 17:15:06 kid1| Max Mem  size: 65536 KB
>         > 2015/07/24 17:15:06 kid1| Max Swap size: 1048576 KB
>         > 2015/07/24 17:15:06 kid1| Rebuilding storage
>         > in /var/spool/squid/cache (dirty log)
>         > 2015/07/24 17:15:06 kid1| Using Least Load store dir
>         > selection
>         > 2015/07/24 17:15:06 kid1| Current Directory is /
>         > 2015/07/24 17:15:06 kid1| Finished loading MIME types and
>         > icons.
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall:
>         > The AsyncCall clientListenerConnectionOpened constructed,
>         > this=0x946d218 [call5]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
>         > StartListening.cc(59) will call
>         > clientListenerConnectionOpened(local=0.0.0.0:3127
>         > remote=[::] FD 20 flags=9, err=0, HTTP Socket
>         > port=0x946d24c) [call5]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall:
>         > The AsyncCall clientListenerConnectionOpened constructed,
>         > this=0x946d3a8 [call7]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
>         > StartListening.cc(59) will call
>         > clientListenerConnectionOpened(local=10.20.20.1:800
>         > remote=[::] FD 21 flags=41, err=0, HTTP Socket
>         > port=0x946d3dc) [call7]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall:
>         > The AsyncCall clientListenerConnectionOpened constructed,
>         > this=0x946d510 [call9]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
>         > StartListening.cc(59) will call
>         > clientListenerConnectionOpened(local=127.0.0.1:800
>         > remote=[::] FD 22 flags=41, err=0, HTTP Socket
>         > port=0x946d544) [call9]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall:
>         > The AsyncCall clientListenerConnectionOpened constructed,
>         > this=0x946d6b0 [call11]
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
>         > StartListening.cc(59) will call
>         > clientListenerConnectionOpened(local=10.20.20.1:808
>         > remote=[::] FD 23 flags=41, err=0, HTTPS Socket
>         > port=0x946d6e4) [call11]
>         > 2015/07/24 17:15:06.578 kid1| HTCP Disabled.
>         > 2015/07/24 17:15:06.578 kid1| Squid plugin modules loaded: 0
>         > 2015/07/24 17:15:06.578 kid1| Adaptation support is on
>         > 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55)
>         > fireNext: entering
>         > clientListenerConnectionOpened(local=0.0.0.0:3127
>         > remote=[::] FD 20 flags=9, err=0, HTTP Socket
>         > port=0x946d24c)
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make
>         > call clientListenerConnectionOpened [call5]
>         > 2015/07/24 17:15:06.578 kid1| Accepting HTTP Socket
>         > connections at local=0.0.0.0:3127 remote=[::] FD 20 flags=9
>         > 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57)
>         > fireNext: leaving
>         > clientListenerConnectionOpened(local=0.0.0.0:3127
>         > remote=[::] FD 20 flags=9, err=0, HTTP Socket
>         > port=0x946d24c)
>         > 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55)
>         > fireNext: entering
>         > clientListenerConnectionOpened(local=10.20.20.1:800
>         > remote=[::] FD 21 flags=41, err=0, HTTP Socket
>         > port=0x946d3dc)
>         > 2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make
>         > call clientListenerConnectionOpened [call7]
>         > 2015/07/24 17:15:06.578 kid1| Accepting NAT intercepted HTTP
>         > Socket connections at local=10.20.20.1:800 remote=[::] FD 21
>         > flags=41
>         > 2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57)
>         > fireNext: leaving
>         > clientListenerConnectionOpened(local=10.20.20.1:800
>         > remote=[::] FD 21 flags=41, err=0, HTTP Socket
>         > port=0x946d3dc)
>         > 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55)
>         > fireNext: entering
>         > clientListenerConnectionOpened(local=127.0.0.1:800
>         > remote=[::] FD 22 flags=41, err=0, HTTP Socket
>         > port=0x946d544)
>         > 2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make
>         > call clientListenerConnectionOpened [call9]
>         > 2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted HTTP
>         > Socket connections at local=127.0.0.1:800 remote=[::] FD 22
>         > flags=41
>         > 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57)
>         > fireNext: leaving
>         > clientListenerConnectionOpened(local=127.0.0.1:800
>         > remote=[::] FD 22 flags=41, err=0, HTTP Socket
>         > port=0x946d544)
>         > 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55)
>         > fireNext: entering
>         > clientListenerConnectionOpened(local=10.20.20.1:808
>         > remote=[::] FD 23 flags=41, err=0, HTTPS Socket
>         > port=0x946d6e4)
>         > 2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make
>         > call clientListenerConnectionOpened [call11]
>         > 2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted SSL
>         > bumped HTTPS Socket connections at local=10.20.20.1:808
>         > remote=[::] FD 23 flags=41
>         > 2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57)
>         > fireNext: leaving
>         > clientListenerConnectionOpened(local=10.20.20.1:808
>         > remote=[::] FD 23 flags=41, err=0, HTTPS Socket
>         > port=0x946d6e4)
>         > 2015/07/24 17:15:06.579 kid1| Accepting ICP messages on
>         > 0.0.0.0:3130
>         > 2015/07/24 17:15:06.579 kid1| Sending ICP messages from
>         > 0.0.0.0:3130
>         > 2015/07/24 17:15:06.579 kid1| Done
>         > reading /var/spool/squid/cache swaplog (12 entries)
>         > 2015/07/24 17:15:06.579 kid1| Finished rebuilding storage
>         > from disk.
>         > 2015/07/24 17:15:06.579 kid1|        12 Entries scanned
>         > 2015/07/24 17:15:06.579 kid1|         0 Invalid entries.
>         > 2015/07/24 17:15:06.579 kid1|         0 With invalid flags.
>         > 2015/07/24 17:15:06.579 kid1|        12 Objects loaded.
>         > 2015/07/24 17:15:06.579 kid1|         0 Objects expired.
>         > 2015/07/24 17:15:06.579 kid1|         0 Objects cancelled.
>         > 2015/07/24 17:15:06.579 kid1|         0 Duplicate URLs
>         > purged.
>         > 2015/07/24 17:15:06.579 kid1|         0 Swapfile clashes
>         > avoided.
>         > 2015/07/24 17:15:06.579 kid1|   Took 0.06 seconds (210.47
>         > objects/sec).
>         > 2015/07/24 17:15:06.579 kid1| Beginning Validation Procedure
>         > 2015/07/24 17:15:06.579 kid1|   Completed Validation
>         > Procedure
>         > 2015/07/24 17:15:06.579 kid1|   Validated 12 Entries
>         > 2015/07/24 17:15:06.579 kid1|   store_swap_size = 1444.00 KB
>         > 2015/07/24 17:15:07 kid1| storeLateRelease: released 0
>         > objects
>         > 
>         > 
>         > 
>         > Any help or suggestions greatly appreciated.
>         > 
>         > 
>         > Regards
>         > 
>         > 
>         > Stan
>         > 
>         > 
>         > 
>         > _______________________________________________
>         > squid-users mailing list
>         > squid-users at lists.squid-cache.org
>         > http://lists.squid-cache.org/listinfo/squid-users
>         
>         
>         I do not experience this issue:
>         
>         [18:04:56 jlay:~/nobackup/build$] ps aux | egrep "ssl|squid"
>         root      3173  0.0  0.0  18840   372 ?        Ss   Jul23
>         0:00 /opt/sbin/squid
>         nobody    3175  0.0  1.2  52856 39744 ?        S    Jul23
>         0:47 (squid-1)
>         nobody    3177  0.0  0.0   5916  2040 ?        S    Jul23
>         0:05 (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>         nobody    3178  0.0  0.0   5828  1840 ?        S    Jul23
>         0:00 (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>         nobody    3179  0.0  0.0   5828  1708 ?        S    Jul23
>         0:00 (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>         nobody    3180  0.0  0.0   5648   912 ?        S    Jul23
>         0:00 (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>         nobody    3181  0.0  0.0   5648   912 ?        S    Jul23
>         0:00 (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>         
>         my config line:
>         ./configure --prefix=/opt --with-openssl --enable-ssl
>         --enable-ssl-crtd --enable-linux-netfilter
>         --enable-follow-x-forwarded-for --with-large-files
>         --sysconfdir=/opt/etc/squid --enable-external-acl-helpers=none
>         
>         Squid Cache: Version 3.5.6
>         
>         James
>         
>         
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         http://lists.squid-cache.org/listinfo/squid-users
>         
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


I recall when just starting out with ssl_crtd and had issue until I set
the user running as squid  on my ssl_db dir:

drwxr-xr-x 3 nobody root 4096 May 30 17:22 ssl_db

My ssl_crtd lines:
sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

Hope it helps.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150724/f9f9baf8/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul 25 00:54:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Jul 2015 12:54:47 +1200
Subject: [squid-users] cannot leave empty workers
In-Reply-To: <BAY181-W79195CF421D92AE7D025A183810@phx.gbl>
References: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>
 <55B2B716.3070500@treenet.co.nz>
 <BAY181-W48911B30204BDB0252A1A383810@phx.gbl>
 <BAY181-W79195CF421D92AE7D025A183810@phx.gbl>
Message-ID: <55B2DE57.2030106@treenet.co.nz>

On 25/07/2015 11:53 a.m., Alex Wu wrote:
> further analysis indicated that master process created quid-ssl_session_cache.shm.
> 
> In other words, it needs a https_port or http_port with ssl-bump in outside any process number to create this shared memeory segment.
> 
> Furthermore, the code  should be simplied like this:
> 
> diff --git a/squid-3.5.6/src/ssl/support.cc b/squid-3.5.6/src/ssl/support.cc
> index 85305ce..0ce95f9 100644
> --- a/squid-3.5.6/src/ssl/support.cc
> +++ b/squid-3.5.6/src/ssl/support.cc
> @@ -2084,9 +2084,6 @@ SharedSessionCacheRr::useConfig()
>  void
>  SharedSessionCacheRr::create()
>  {
> -    if (!isSslServer()) //no need to configure ssl session cache.
> -        return;
> -
>      int items;
>      items = Config.SSL.sessionCacheSize / sizeof(Ipc::MemMap::Slot);
>      if (items)
> 
> 
> 
> This code is called in master that may not have configuration to ensure isSsslServer return true.
> 

The bug is in why that SharedSessionCacheRr is not being run by the worker.

AFAIK, it is the way the worker is supposed to attach to the shared
memory. First process to access the SHM does the create, others attach.

Amos



From jagannath.naidu at fosteringlinux.com  Sat Jul 25 05:31:09 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Sat, 25 Jul 2015 11:01:09 +0530
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <55B2B310.6050608@treenet.co.nz>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <CA+8bHvxW=goVDdmBLjF-tdbGOup7ejAcbTTOQFrxN4rZ12aQMA@mail.gmail.com>
 <CA+8bHvxaSmLjvviyONXQo-my3mxRw7DvhsPjg1=R31+SPKbBYg@mail.gmail.com>
 <55B2B310.6050608@treenet.co.nz>
Message-ID: <CA+8bHvwN3m_K3bbN_HqRaeJH=fxMyAmtaarj74ZofKtz0Z=ioQ@mail.gmail.com>

Thanks Amos,, mike



On 25 July 2015 at 03:20, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 25/07/2015 4:59 a.m., Jagannath Naidu wrote:
> > 1. Its not  a transparent proxy.
> >
> > 2. My clients get wpad configuration from AD server. So there are two
> > question.
> >  2.1 :I know that wpad is used to identify proxy server and port(and rest
> > other bypass rules).  When clients resolve to wpad.abc.com, is there way
> > that I can overwrite the wpad file off client. Like creating a webserver
> to
> > to serve wpad file and I change /etc/hosts file to
> "<myhwebserveripaddress>
> > wpad.abc.com"
> > 2.2 Is there any other way to tell clients via squid server, to do not
> come
> > to squid server and re initiate the request.
>
> Exactly that if you wish. Its not clear whether WPAD is the problem though.
>
> The fact that you have Squid logs showing access indicates the traffic
> us actually getting there okay. The responses do seem to be coming back
> from 10.* servers as well.
> So what is happening is something is causing those servers not to like
> the traffic being requested from them.
>
>
> >
> > On 24 July 2015 at 21:10, Jagannath Naidu <
> > jagannath.naidu at fosteringlinux.com> wrote:
> >
> >>
> >>
> >> On 24 July 2015 at 21:05, Jagannath Naidu <
> >> jagannath.naidu at fosteringlinux.com> wrote:
> >>
> >>> Dear List,
> >>>
> >>> I have been working on this for last two weeks, but never got it
> >>> resolved.
> >>>
> >>> We have a application server (SERVER) in our local network and a
> desktop
> >>>  application (CLIENT). The application picks proxy settings from IE.
> And we
> >>> also have a wensense proxy server
> >>>
> >>> case 1: when there is no proxy set
> >>> application works. No logs in squid server access.log
> >>>
> >>> case 2: when proxy ip address set and checked "bypass local network"
> >>> application works. No logs in squid server access.log
> >>>
> >>> case 3: when proxy ip address is set to wensense proxy server.
> UNCHECKED
> >>> "bypass local network"
> >>> application works. We dont have access to websense server and hence we
> >>> can not check logs
>
> Can you explain "not works" in any better detail?
>  application expected vs actual behaviour?
>  if you can relate that to particular HTTP messages even better.
>
The application is "aspect unified ip agent desktop". It is a dialer
application (VOIP). Used on windows machine.
Rest cases :

When application is launched, it shows that it has joined domain "HTP". HTP
is default, we can change to other from the drop down list.

Case 4: not works.

But in this case, it shows no drop down list, nor with a single option like
"HTP". Application can connect to server anymore. And I can not call or
receive calls anymore.


>
> >>>
> >>>
> >>> case 4: when proxy ip address is set to proxy server ip address.
> >>> UNCHECKED "bypass local network"
> >>> application does not work :-(. Below are the logs.
> >>>
> >>>
> >>> 1437751240.149      7 192.168.122.1 TCP_MISS/404 579 GET
> >>>
> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
> >>> - HIER_DIRECT/10.1.4.46 text/html
>
> 404. The URL you see above references an object that does not exist on
> that server.
>
> Things to look into:
>  Is it the right server?
>
Yes

>  Is it the right URL?
>
Yes

>  Why was it requested?
>
Don't know. These were the only logs I can get from access.log. The server
is "Microsoft IIS HTTP/1.1"


>  Does the server actually know its "dlwvdialce.htmedia.net" name?
>
Yes. It is resolvable 1) ping dlwvdialce works 2) ping
dlwvdialce.htmedia.net works

initially "dlwvdialce" was not resolving to any host. That's where used
"append_domain .htmedia.net" is squid.conf (worked for other applications).


>
>
> >>> 1437751240.992     94 192.168.122.1 TCP_DENIED/407 3757 CONNECT
> >>> 0.client-channel.google.com:443 - HIER_NONE/- text/html
> >>> 1437751240.996      0 192.168.122.1 TCP_DENIED/407 4059 CONNECT
> >>> 0.client-channel.google.com:443 - HIER_NONE/- text/html
>
>
> Authentication. Normal I think.
>
Yes, NTLM auth.


>
> >>> 1437751242.327      5 192.168.122.1 TCP_MISS/404 579 GET
> >>> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
> >>> 10.1.4.46 text/html
>
> Same as the first 404'd URL.
>
>
>> 1437751244.777      1 192.168.122.1 TCP_MISS/503 4048 POST
> >>>
> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
> >>> - HIER_NONE/- text/html
>
> 503 usually indicates the attempted server failed.
>
> Makes sense if TCP to cs-711-core.htmedia.net port 8180 did not work.
> Which would also match the lack of server IP in the log.
>

 1)  ping cs-711-core.htmedia.net  does not work "no such host"
 2) ping  cs-711-core <http://cs-711-core.htmedia.net/> does not work "no
such host"


>
>
> >>>
> >>> UPDATE: correct logs
> >>
> >> 1437752279.774      6 192.168.122.1 TCP_MISS/404 579 GET
> >>
> http://dlwvdialce.htmedia.net/UADInstall/UADPresentationLayer.application
> >> - HIER_DIRECT/10.1.4.46 text/html
> >> 1437752281.854      5 192.168.122.1 TCP_MISS/404 579 GET
> >> http://dlwvdialce.htmedia.net/UADInstall/uadprop.htm - HIER_DIRECT/
> >> 10.1.4.46 text/html
> >> 1437752284.265      2 192.168.122.1 TCP_MISS/503 4048 POST
> >>
> http://cs-711-core.htmedia.net:8180/ConcertoAgentPortal/services/ConcertoAgentPortal
> >> - HIER_NONE/- text/html
> >>
>
> Same comments as above.
>
> >>
> >>
> >>> squid -v
> >>> Squid Cache: Version 3.3.8
> >>> configure options:  '--build=x86_64-redhat-linux-gnu'
> >>> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> >>> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> >>> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> >>> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> >>> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> >>> '--infodir=/usr/share/info' '--disable-strict-error-checking'
> >>> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid'
> '--localstatedir=/var'
> >>> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> >>> '--with-logdir=$(localstatedir)/log/squid'
> >>> '--with-pidfile=$(localstatedir)/run/squid.pid'
> >>> '--disable-dependency-tracking' '--enable-eui'
> >>> '--enable-follow-x-forwarded-for' '--enable-auth'
> >>>
> '--enable-auth-basic=DB,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam'
> >>> '--enable-auth-ntlm=smb_lm,fake'
> >>> '--enable-auth-digest=file,LDAP,eDirectory'
> >>> '--enable-auth-negotiate=kerberos'
> >>>
> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group'
> >>> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> >>> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> >>> '--enable-ident-lookups' '--enable-linux-netfilter'
> >>> '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl'
> >>> '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2'
> >>> '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid'
> >>> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
> >>> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> >>> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
> >>> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> >>> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> >>> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now'
> 'CXXFLAGS=-O2
> >>> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> >>> -fstack-protector-strong --param=ssp-buffer-size=4
> -grecord-gcc-switches
> >>> -m64 -mtune=generic -fpie'
> >>>
> 'PKG_CONFIG_PATH=%{_PKG_CONFIG_PATH}:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> >>>
> >>>
> >>> squid.conf
> >>>
> >>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> >>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> >>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> >>> acl localnet src fc00::/7       # RFC 4193 local private network range
> >>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> plugged)
> >>> machines
> >>> acl SSL_ports port 443
> >>> acl Safe_ports port 80          # http
> >>> acl Safe_ports port 21          # ftp
> >>> acl Safe_ports port 443         # https
> >>> acl Safe_ports port 70          # gopher
> >>> acl Safe_ports port 210         # wais
> >>> acl Safe_ports port 1025-65535  # unregistered ports
> >>> acl Safe_ports port 280         # http-mgmt
> >>> acl Safe_ports port 488         # gss-http
> >>> acl Safe_ports port 591         # filemaker
> >>> acl Safe_ports port 777         # multiling http
> >>> acl Safe_ports port 8180
> >>> acl CONNECT method CONNECT
> >>> acl wvdial dst 10.1.4.45 10.1.4.50 10.1.4.53 10.1.4.48 10.1.4.54
> >>> 10.1.4.46 10.1.4.51 10.1.4.47 10.1.4.55 10.1.4.49 10.1.4.52 10.1.2.4
>
> For easier reading:
>   acl wvdial dst 10.1.4.45-10.1.4.55/27 10.1.2.4
>
> (at least I think they are all in one /27, double-check that)
>
> >>> http_access allow wvdial
> >>> acl dialer dstdomain .htmedia.net
> >>> http_access allow dialer
> >>> http_access deny !Safe_ports
> >>> http_access deny CONNECT !SSL_ports
> >>> http_access allow localhost manager
> >>> http_access deny manager
> >>> visible_hostname = NOIDAPROXY01.MYDOMAIN.NET
>
>  "=" is a funny domain name. I suspect you wanted the domain-name part
> of the line to be used instead. Remove the "= " bit.
>
> Removed = bit.


> >>> append_domain  .mydomain.net
> >>> ignore_expect_100 on
>
> The ignore_* directive should not be useful in 3.3. You can remove it now.
>
> >>> dns_v4_first on
> >>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> >>> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
> >>> auth_param ntlm children 1000
> >>> auth_param ntlm keep_alive off
> >>> auth_param basic program /usr/bin/ntlm_auth
> >>> --helper-protocol=squid-2.5-basic
> >>> auth_param basic children 100
> >>> auth_param basic realm Squid proxy-caching web server
> >>> auth_param basic credentialsttl 2 hours
> >>> acl auth proxy_auth REQUIRED
> >>> http_access allow all auth
>
> "allow all auth" means the same as "allow auth".
>
> "all" only has meaning on the end (right-hand side) of the line which
> would otherwise end in a proxy_auth ACL.
> It should either be on the end of that line, or not used at all.
>
>
> >>> http_access allow localnet
> >>> http_access allow localhost
> >>> http_access deny all
> >>> http_port 0.0.0.0:8080
> >>> coredump_dir /var/spool/squid
> >>> refresh_pattern ^ftp:           1440    20%     10080
> >>> refresh_pattern ^gopher:        1440    0%      1440
> >>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> >>> refresh_pattern .               0       20%     4320
> >>>
> >>>
> >>> It was the same behavior with squid-3.1.10-19. I thought, upgrading to
> >>> squid 3.3 would help. Please help me resolving this mystery.
>
> Looks to me like the server at 10.1.4.46 does not know what to do with
> the URLs requested.
>
> I would start looking at whether the application is actually supposed to
> be going there for its requests.
>
> How can do that ?
I can install wireshark on client and test the result.

Am I missing any information to give ?


> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150725/128eeb2c/attachment.htm>

From jagannath.naidu at fosteringlinux.com  Sat Jul 25 05:44:32 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Sat, 25 Jul 2015 11:14:32 +0530
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <55B28A90.1010006@afo.net>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <55B28A90.1010006@afo.net>
Message-ID: <CA+8bHvxgm6Y3wbqsHTp-x+9QqSmWJkFnBhazEibdC2QvgjWCoA@mail.gmail.com>

Thanks mike.
But I think Amos is right.

On 25 July 2015 at 00:27, Mike <mcsnv96 at afo.net> wrote:

>  I see a few issues.
>
> 1. The report from the log shows a 192.168.*.* address, common LAN IP
>

The ip 192.168.122.1 is the ip address of  virtual interface (acts as a
default gateway for Virtual machines). I did NATing using iptables.

>
> Then in the squid.conf:
> 2. You have wvdial destination as 10.1.*.* addresses, which is a
> completely different internal network.
> Typically there will be no internal routing or communication from a
> 192.168..*.* address to/from a 10.*.*.* address without a custom routing
> server with 2 network connections, one from each IP set and to act as the
> DNS intermediary for routing. Otherwise for network/internet connections,
> the computer/browser sees its own IP as local network, and everything else
> including 10.*.*.* as an external address out on the internet. I would
> suggest getting both the browsing computer and the server on the same IP
> subset, as in 192.168.122.x or 10.1.4.x, otherwise these issues are likely
> to continue.
>

I have two squid servers.
1. squid 3.1 on physical server
2. squid 3.3 on VM hosted by 1

Same logs. No different results.

So when the client requests 8080 . 3.1 serves. When the client requests
3128 3.3 serves.
This application behavior is same for both.


>
> 3. Next in the squid.conf is http_port which should be port number only,
> no IP address, especially 0.0.0.0 which can cause conflicts with squid 3.x
> versions. Best bet is use just port only, as in: "http_port 3128" or in
> your case "http_port 8080", which is the port (with server IP found in
> ifconfig) the browser will use to connect through the squid server.
>

I tried your suggestion. But not worked. Same results :-(


> 4. The bypass local network means any IP connection attempt to a local
> network IP will not use the proxy. This goes back to the 2 different IP
> subsets. One option is to enter a proxy exception as 10.*.*.* (if the
> websense server is using 10.x.x.x IP address).
>

I was thinking, what would websense have deployed.

@amos, mike: Can we overwrite wpad of a client using squid server or any
means automatically ?????


>
> Mike
>

Jagannath Naidu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150725/8d8a595d/attachment.htm>

From wolle5050 at gmx.de  Sat Jul 25 08:25:32 2015
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Sat, 25 Jul 2015 10:25:32 +0200
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <55B26F76.5030108@urlfilterdb.com>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>,
 <55B23089.9040108@urlfilterdb.com>
 <trinity-81e5734b-fa69-41ff-9168-1ef784f453ab-1437753709668@3capp-gmx-bs51>,
 <55B26F76.5030108@urlfilterdb.com>
Message-ID: <trinity-28445ed1-e2a9-4f13-a1ee-c5774c7ffcf1-1437812732812@3capp-gmx-bs31>

Thanks a lot for your comprehensive testing. Now we are sure that the issue is independent of the underlying OS.

I can easily reduce the maximum_object_size_in_memory. I have some constraints from our IT at the moment. I am confronted with a very bad disk IO in our BladeCenter, so I am forced to put as many objects in memory as possible in order to get adequate performance values. We are also rare with disk space. RAM is not an issue right now.

Maybe Amos finds a way to fix the issue. Fortunately, we have a find a workaround.

Have a nice weekend!

Regards,
Jens


Gesendet:?Freitag, 24. Juli 2015 um 19:01 Uhr
Von:?"Marcus Kool" <marcus.kool at urlfilterdb.com>
An:?"Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
Betreff:?Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching

On 07/24/2015 01:01 PM, Jens Offenbach wrote:
> @Marcus:
> I am not sure what exactly causes the problems, but could you please make a test with these two different settings:
> cache_mem 4 GB
> maximum_object_size_in_memory 1 GB

I think this setting for maximum_object_size_in_memory is too high, independent of how the performance is.
The tests also show that large objects cached on disk have a good performance.
The perfect place for a large ISO image is the disk cache.

I did the test with squid 3.5.6 and got the same result as you have:
the download starts fast but quickly drops. Squid uses 100% CPU.
wget displays 14 MB/sec ... 10 MB/sec ... 8 7 6 5 4 3 2 MB/sec and stays there for a long time.
At 50% downloaded the speed drops more to 1 MB/sec and at the end of the download I got 500 KB/sec *average*.
The second cached download was sustained 190 MB/sec and 120% CPU.

I did a second test with
cache_mem 4 GB
maximum_object_size_in_memory 200 MB

The download speed varied a lot: started with 30 MB/sec and went down and up many times between 6 MB/sec and 35 MB/sec.
The final average download speed was 31 MB/sec. 100% CPU.
The second cached download was sustained 190 MB/sec and 120% CPU.

Third test with
cache_mem 4 GB
maximum_object_size_in_memory 8 MB

The download speed started with 70 MB/sec and increased to 87 MB/sec. 100% CPU
The second cached download was sustained 190 MB/sec and 120% CPU.

4th test with
cache_mem 4 GB
maximum_object_size_in_memory 32 MB

The download speed started with 40 MB/sec and increased to 75 MB/sec. 100% CPU.
The second cached download was sustained 190 MB/sec and 120% CPU.

So Squid appears to have an issue with higher values of maximum_object_size_in_memory, the higher they are, the worse the performance.
For now, I would not go beyond 16 MB.
The question is, what is a reasonable size that you would like to be able to use for maximum_object_size_in_memory.
Do you have any particular requirement for a high maximum_object_size_in_memory ?

Marcus

> I think you will observe the behavior, that I was confronted with. The bad download rates of 500 KB/sec are gone, when I used the following settings:
> cache_mem 256 MB
> maximum_object_size_in_memory 16 MB
>
> I think Amos has an idea what seems to be the source of the problem:
> http://lists.squid-cache.org/pipermail/squid-users/2015-July/004728.html
>
> Regards,
> Jens
>
>
> Gesendet: Freitag, 24. Juli 2015 um 14:33 Uhr
> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>
> On 07/24/2015 03:25 AM, Jens Offenbach wrote:
>> I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
>>
>>
>> Gesendet: Freitag, 24. Juli 2015 um 07:54 Uhr
>> Von: "Jens Offenbach" <wolle5050 at gmx.de>
>> An: "Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>> It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.
>>
>> The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
>> http://wikisend.com/download/293856/squid.strace2[http://wikisend.com/download/293856/squid.strace2]
>>
>> @Marcus:
>> Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?
>
> I screwed up earlier since the maximum_object_size was too low for the test with a 1 GB file and did a new test.
>
> The system has 64 GB memory and for sure the entire file is in the file system cache. The disk system is HW RAID-1 with 1 GB cache.
> The OS is Linux 3.10, CentOS 7 latest patches.
>
> New test:
> test system: 1 CPU with 4 cores/8 threads @ 3.7 GHz, 64 GB memory, AUFS, 1 Gbit pipe, 500 mbit guaranteed
>
> with Squid 3.4.12 :
> 1st download starts with 90 MB/sec and halfway drops to 30 MB/sec. My guess is that the file system cache got stressed and slowed things down.
> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>
> With Squid 3.5.6 :
> 1st download starts with 90 MB/sec sustained and 80% CPU time.
> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>
> As a comparison, I did "dd if=test of=test2 bs=4k" which uses 100% CPU time and has a throughput of 1200 MB/sec.
> With bs=16k the throughput is 1300 MB/sec and with bs=64k the throughput is 1400 MB/sec.
>
> relevant parameters :
> read_ahead_gap 64 KB
> cache_mem 256 MB
> maximum_object_size_in_memory 8 MB
> maximum_object_size 8000 MB
> cache_dir aufs /local/squid34/cache 10000 32 256
> cache_swap_low 92
> cache_swap_high 93
> # also ICAP daemon and URL rewriter configured
> debug_options ALL,1 93,3 61,9
>
> configure options:
> '--prefix=/local/squid35' '--disable-ipv6' '--enable-fd-config' '--with-maxfd=3200' '--enable-async-io=64' '--enable-storeio=aufs' '--with-pthreads' '--enable-removal-policies=lru'
> '--disable-auto-locale' '--enable-default-err-language=English' '--enable-err-languages=Dutch English Portuguese' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd'
> '--enable-cachemgr-hostname=localhost' '--enable-cache-digests' '--enable-follow-x-forwarded-for' '--enable-xmalloc-statistics' '--disable-hostname-checks' '--enable-epoll' '--enable-icap-client'
> '--enable-useragent-log' '--enable-referer-log' '--enable-stacktraces' '--enable-underscores' '--disable-icmp' '--mandir=/usr/local/share' 'CC=gcc' 'CFLAGS=-g -O2 -Wall -march=native' 'CXXFLAGS=-g -O2
> -Wall -march=native' --enable-ltdl-convenience
>
> As you can see the cache_mem is small, If Amos finds it useful, I can do another test with a larger cache_mem.
>
> Jens, since all your tests have a drop to 500 KB/sec I think the cause is somewhere is the configuration (Squid and/or OS).
>
> Marcus
>
>
>> @Amos, Eliezer
>> Is someone able to reproduce the disk caching effect?
>>
>> Regards,
>> Jens
>>
>>
>> Gesendet: Donnerstag, 23. Juli 2015 um 20:08 Uhr
>> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> An: "Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
>> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
>> The strace output shows this loop:
>>
>> Squid reads 16K-1 bytes from FD 13 webserver
>> Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
>> Squid writes 4 times 4K to FD 12 browser
>>
>> But this loop does not explain the 100% CPU usage...
>>
>> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?
>>
>> I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
>> The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
>> The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
>> I never looked at Squid CPU usage and do not know what is reasonable but it feels high.
>>
>> With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
>> Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
>> This is a possible explanation for the fact that you see the performance going down and up.
>> Can you do the same test on the host (i.e. not inside a VM).
>>
>> Marcus
>>
>>
>>
>> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>>> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
>>> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
>>> Here is the download link:
>>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]]]
>>> I hope it can help you.
>>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> I am not sure if it is relevant, maybe it is:
>>>
>>> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
>>> Squid sends the object to the ICAP server in small chunks of varying sizes:
>>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>>> Note that the interval of receiving the chunks is 1/1000th of a second.
>>> It seems that Squid forwards the object to the ICAP server every time it receives
>>> one or a few TCP packets.
>>>
>>> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
>>> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>>>
>>> Amos, is there a debug setting that can confirm/reject this suspicion?
>>>
>>> Marcus
>>>
>>>
>>> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>>>> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>>>>
>>>> I cleared the cache:
>>>> rm -rf /var/cache/squid3/*
>>>> squid -z
>>>> squid
>>>> http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>>>>
>>>> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
>>> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>>>>
>>>> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>>>>
>>>> For further investigations I give you my build properties:
>>>> squid -v
>>>> Squid Cache: Version 3.5.6
>>>> Service Name: squid
>>>> configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
>>> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
>>> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
>>> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
>>> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
>>> s
>>> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
>>> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>>> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>>>
>>>>
>>>> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>>>> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>>>> An: squid-users at lists.squid-cache.org
>>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>>> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>>>>> Hey Jens,
>>>>>
>>>>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>>>>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>>>>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>>>>> download speed is more then doubled to 4.5 MBps(36Mbps).
>>>>> I have not tried it with AUFS yet.
>>>>
>>>>
>>>> And I must admit that AUFS beats rock cache with speed.
>>>> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>>>> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>>>> results more then doubles 3 the speed rock gave with default settings.
>>>>
>>>> So about 15MBps which is 120Mbps.
>>>> I do not seem to feel what Jens feels but the 100% CPU might be because
>>>> of spinning disk hangs while reading the file from disk.
>>>>
>>>> Amos, I remember that there were some suggestions how to tune large rock.
>>>> Any hints?
>>>> I can test it and make it a suggestion for big files.
>>>>
>>>> Eliezer
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]]
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]]
>>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
>>
>
>


From jagannath.naidu at fosteringlinux.com  Sat Jul 25 09:38:21 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Sat, 25 Jul 2015 15:08:21 +0530
Subject: [squid-users] ISSUE accssing content
In-Reply-To: <CA+8bHvxgm6Y3wbqsHTp-x+9QqSmWJkFnBhazEibdC2QvgjWCoA@mail.gmail.com>
References: <CA+8bHvzvh=RYRPUFV4KYmeOD6-RVqV2tTqvYM6A1Q0TFkJn2Gw@mail.gmail.com>
 <55B28A90.1010006@afo.net>
 <CA+8bHvxgm6Y3wbqsHTp-x+9QqSmWJkFnBhazEibdC2QvgjWCoA@mail.gmail.com>
Message-ID: <CA+8bHvxhRXjUnG-_vT4=-tXYG3YTAMVOpTpAzkBKwnt4-tfT5Q@mail.gmail.com>

Any one ?? !!!!

On 25 July 2015 at 11:14, Jagannath Naidu <
jagannath.naidu at fosteringlinux.com> wrote:

>
> Thanks mike.
> But I think Amos is right.
>
> On 25 July 2015 at 00:27, Mike <mcsnv96 at afo.net> wrote:
>
>>  I see a few issues.
>>
>> 1. The report from the log shows a 192.168.*.* address, common LAN IP
>>
>
> The ip 192.168.122.1 is the ip address of  virtual interface (acts as a
> default gateway for Virtual machines). I did NATing using iptables.
>
>>
>> Then in the squid.conf:
>> 2. You have wvdial destination as 10.1.*.* addresses, which is a
>> completely different internal network.
>> Typically there will be no internal routing or communication from a
>> 192.168..*.* address to/from a 10.*.*.* address without a custom routing
>> server with 2 network connections, one from each IP set and to act as the
>> DNS intermediary for routing. Otherwise for network/internet connections,
>> the computer/browser sees its own IP as local network, and everything else
>> including 10.*.*.* as an external address out on the internet. I would
>> suggest getting both the browsing computer and the server on the same IP
>> subset, as in 192.168.122.x or 10.1.4.x, otherwise these issues are likely
>> to continue.
>>
>
> I have two squid servers.
> 1. squid 3.1 on physical server
> 2. squid 3.3 on VM hosted by 1
>
> Same logs. No different results.
>
> So when the client requests 8080 . 3.1 serves. When the client requests
> 3128 3.3 serves.
> This application behavior is same for both.
>
>
>>
>> 3. Next in the squid.conf is http_port which should be port number only,
>> no IP address, especially 0.0.0.0 which can cause conflicts with squid 3.x
>> versions. Best bet is use just port only, as in: "http_port 3128" or in
>> your case "http_port 8080", which is the port (with server IP found in
>> ifconfig) the browser will use to connect through the squid server.
>>
>
> I tried your suggestion. But not worked. Same results :-(
>
>
>> 4. The bypass local network means any IP connection attempt to a local
>> network IP will not use the proxy. This goes back to the 2 different IP
>> subsets. One option is to enter a proxy exception as 10.*.*.* (if the
>> websense server is using 10.x.x.x IP address).
>>
>
> I was thinking, what would websense have deployed.
>
> @amos, mike: Can we overwrite wpad of a client using squid server or any
> means automatically ?????
>
>
>>
>> Mike
>>
>
> Jagannath Naidu
>



-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150725/c0979825/attachment.htm>

From marcus.kool at urlfilterdb.com  Sat Jul 25 11:23:04 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 25 Jul 2015 08:23:04 -0300
Subject: [squid-users] Squid3: 100 % CPU load during object caching
In-Reply-To: <trinity-28445ed1-e2a9-4f13-a1ee-c5774c7ffcf1-1437812732812@3capp-gmx-bs31>
References: <trinity-d64e9ab9-c789-4ec7-91d1-a230cdb1fe25-1437571443960@3capp-gmx-bs53>
 <55AFC4BE.2040404@ngtech.co.il>
 <trinity-025f97bd-3c3b-4841-9984-4b0f008a2ae5-1437588435598@3capp-gmx-bs28>
 <55AFE814.3020504@ngtech.co.il>, <55AFF344.1030806@ngtech.co.il>
 <trinity-0d2f8cb1-10d6-43f2-ac5b-e22c61d99c67-1437636318501@3capp-gmx-bs71>,
 <55B0D027.4000501@urlfilterdb.com>
 <trinity-dc6ba571-1d90-49d7-bd5e-bdbded6d7dcc-1437658767783@3capp-gmx-bs71>,
 <55B12D9C.2030509@urlfilterdb.com>,
 <trinity-e98a73c1-7875-48f0-97b7-d33de7a8b8a1-1437717297801@3capp-gmx-bs43>
 <trinity-17c4b9df-31a2-4ba2-adfe-2e50c3c8ecc4-1437719157787@3capp-gmx-bs56>,
 <55B23089.9040108@urlfilterdb.com>
 <trinity-81e5734b-fa69-41ff-9168-1ef784f453ab-1437753709668@3capp-gmx-bs51>,
 <55B26F76.5030108@urlfilterdb.com>
 <trinity-28445ed1-e2a9-4f13-a1ee-c5774c7ffcf1-1437812732812@3capp-gmx-bs31>
Message-ID: <55B37198.60106@urlfilterdb.com>



On 07/25/2015 05:25 AM, Jens Offenbach wrote:
> Thanks a lot for your comprehensive testing. Now we are sure that the issue is independent of the underlying OS.
>
> I can easily reduce the maximum_object_size_in_memory. I have some constraints from our IT at the moment. I am confronted with a very bad disk IO in our BladeCenter, so I am forced to put as many objects in memory as possible in order to get adequate performance values. We are also rare with disk space. RAM is not an issue right now.
>
> Maybe Amos finds a way to fix the issue. Fortunately, we have a find a workaround.

If memory is plentyful, just make sure that the OS has a large file system cache.
So reduce mem_cahce of Squid a little and tune the OS with
vm.swappiness=10
in /etc/sysctl.conf

Best regards

Marcus


> Have a nice weekend!
>
> Regards,
> Jens
>
>
> Gesendet: Freitag, 24. Juli 2015 um 19:01 Uhr
> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
> An: "Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
>
> On 07/24/2015 01:01 PM, Jens Offenbach wrote:
>> @Marcus:
>> I am not sure what exactly causes the problems, but could you please make a test with these two different settings:
>> cache_mem 4 GB
>> maximum_object_size_in_memory 1 GB
>
> I think this setting for maximum_object_size_in_memory is too high, independent of how the performance is.
> The tests also show that large objects cached on disk have a good performance.
> The perfect place for a large ISO image is the disk cache.
>
> I did the test with squid 3.5.6 and got the same result as you have:
> the download starts fast but quickly drops. Squid uses 100% CPU.
> wget displays 14 MB/sec ... 10 MB/sec ... 8 7 6 5 4 3 2 MB/sec and stays there for a long time.
> At 50% downloaded the speed drops more to 1 MB/sec and at the end of the download I got 500 KB/sec *average*.
> The second cached download was sustained 190 MB/sec and 120% CPU.
>
> I did a second test with
> cache_mem 4 GB
> maximum_object_size_in_memory 200 MB
>
> The download speed varied a lot: started with 30 MB/sec and went down and up many times between 6 MB/sec and 35 MB/sec.
> The final average download speed was 31 MB/sec. 100% CPU.
> The second cached download was sustained 190 MB/sec and 120% CPU.
>
> Third test with
> cache_mem 4 GB
> maximum_object_size_in_memory 8 MB
>
> The download speed started with 70 MB/sec and increased to 87 MB/sec. 100% CPU
> The second cached download was sustained 190 MB/sec and 120% CPU.
>
> 4th test with
> cache_mem 4 GB
> maximum_object_size_in_memory 32 MB
>
> The download speed started with 40 MB/sec and increased to 75 MB/sec. 100% CPU.
> The second cached download was sustained 190 MB/sec and 120% CPU.
>
> So Squid appears to have an issue with higher values of maximum_object_size_in_memory, the higher they are, the worse the performance.
> For now, I would not go beyond 16 MB.
> The question is, what is a reasonable size that you would like to be able to use for maximum_object_size_in_memory.
> Do you have any particular requirement for a high maximum_object_size_in_memory ?
>
> Marcus
>
>> I think you will observe the behavior, that I was confronted with. The bad download rates of 500 KB/sec are gone, when I used the following settings:
>> cache_mem 256 MB
>> maximum_object_size_in_memory 16 MB
>>
>> I think Amos has an idea what seems to be the source of the problem:
>> http://lists.squid-cache.org/pipermail/squid-users/2015-July/004728.html
>>
>> Regards,
>> Jens
>>
>>
>> Gesendet: Freitag, 24. Juli 2015 um 14:33 Uhr
>> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
>> An: "Jens Offenbach" <wolle5050 at gmx.de>, squid-users at lists.squid-cache.org
>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>
>> On 07/24/2015 03:25 AM, Jens Offenbach wrote:
>>> I have made a quick test of Squid 3.3.8 on Ubuntu 15.04 and I get the same problem: 100 % CPU usage, 500 KB/sec download rate.
>>>
>>>
>>> Gesendet: Freitag, 24. Juli 2015 um 07:54 Uhr
>>> Von: "Jens Offenbach" <wolle5050 at gmx.de>
>>> An: "Marcus Kool" <marcus.kool at urlfilterdb.com>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> It is not easy for me, but I have tested Squid 3.3.8 from the Ubuntu packaging on a "real" physical infrastructure. I get the same results on the physical machine (1x Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz, 32 GB RAM, 1 TB disk) where Squid is running: 100 % CPU usage, 500 KB/sec download rate. All machines are idle and we have 1 GBit ethernet.
>>>
>>> The strace log from the physical test scenario can be found here, but I think it does not differ from the "virtual" test scenario:
>>> http://wikisend.com/download/293856/squid.strace2[http://wikisend.com/download/293856/squid.strace2]
>>>
>>> @Marcus:
>>> Have you verified that the file does not fit into memory and gets cached on disk? On which OS is Squid running? What are your build options of Squid (squid -v)? Is it possible that the issue is not part of 3.4.12? Do we have a regression?
>>
>> I screwed up earlier since the maximum_object_size was too low for the test with a 1 GB file and did a new test.
>>
>> The system has 64 GB memory and for sure the entire file is in the file system cache. The disk system is HW RAID-1 with 1 GB cache.
>> The OS is Linux 3.10, CentOS 7 latest patches.
>>
>> New test:
>> test system: 1 CPU with 4 cores/8 threads @ 3.7 GHz, 64 GB memory, AUFS, 1 Gbit pipe, 500 mbit guaranteed
>>
>> with Squid 3.4.12 :
>> 1st download starts with 90 MB/sec and halfway drops to 30 MB/sec. My guess is that the file system cache got stressed and slowed things down.
>> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>>
>> With Squid 3.5.6 :
>> 1st download starts with 90 MB/sec sustained and 80% CPU time.
>> 2nd cached download with 190 MB/sec sustained and 120% CPU time.
>>
>> As a comparison, I did "dd if=test of=test2 bs=4k" which uses 100% CPU time and has a throughput of 1200 MB/sec.
>> With bs=16k the throughput is 1300 MB/sec and with bs=64k the throughput is 1400 MB/sec.
>>
>> relevant parameters :
>> read_ahead_gap 64 KB
>> cache_mem 256 MB
>> maximum_object_size_in_memory 8 MB
>> maximum_object_size 8000 MB
>> cache_dir aufs /local/squid34/cache 10000 32 256
>> cache_swap_low 92
>> cache_swap_high 93
>> # also ICAP daemon and URL rewriter configured
>> debug_options ALL,1 93,3 61,9
>>
>> configure options:
>> '--prefix=/local/squid35' '--disable-ipv6' '--enable-fd-config' '--with-maxfd=3200' '--enable-async-io=64' '--enable-storeio=aufs' '--with-pthreads' '--enable-removal-policies=lru'
>> '--disable-auto-locale' '--enable-default-err-language=English' '--enable-err-languages=Dutch English Portuguese' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd'
>> '--enable-cachemgr-hostname=localhost' '--enable-cache-digests' '--enable-follow-x-forwarded-for' '--enable-xmalloc-statistics' '--disable-hostname-checks' '--enable-epoll' '--enable-icap-client'
>> '--enable-useragent-log' '--enable-referer-log' '--enable-stacktraces' '--enable-underscores' '--disable-icmp' '--mandir=/usr/local/share' 'CC=gcc' 'CFLAGS=-g -O2 -Wall -march=native' 'CXXFLAGS=-g -O2
>> -Wall -march=native' --enable-ltdl-convenience
>>
>> As you can see the cache_mem is small, If Amos finds it useful, I can do another test with a larger cache_mem.
>>
>> Jens, since all your tests have a drop to 500 KB/sec I think the cause is somewhere is the configuration (Squid and/or OS).
>>
>> Marcus
>>
>>
>>> @Amos, Eliezer
>>> Is someone able to reproduce the disk caching effect?
>>>
>>> Regards,
>>> Jens
>>>
>>>
>>> Gesendet: Donnerstag, 23. Juli 2015 um 20:08 Uhr
>>> Von: "Marcus Kool" <marcus.kool at urlfilterdb.com>
>>> An: "Jens Offenbach" <wolle5050 at gmx.de>, "Amos Jeffries" <squid3 at treenet.co.nz>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, squid-users at lists.squid-cache.org
>>> Betreff: Re: Aw: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>> The strace output shows this loop:
>>>
>>> Squid reads 16K-1 bytes from FD 13 webserver
>>> Squid writes 4 times 4K to FD 17 /var/cache/squid3/00/00/00000000
>>> Squid writes 4 times 4K to FD 12 browser
>>>
>>> But this loop does not explain the 100% CPU usage...
>>>
>>> Does Squid do a buffer reshuffle when it reads 16K-1 and writes 16K ?
>>>
>>> I did the download test with Squid 3.4.12 AUFS on an idle system with a 500 mbit connection and 1 CPU with 4 cores @ 3.7 GHz.
>>> The first download used 35% of 1 CPU core with a steady download speed of 62 MB/sec.
>>> The second (cached) download used 50% of 1 CPU core with a steady download speed of 87 MB/sec.
>>> I never looked at Squid CPU usage and do not know what is reasonable but it feels high.
>>>
>>> With respect to the 100% CPU issue of Jens, one factor is that Squid runs in a virtual machine.
>>> Squid in a virtual machine cannot be compared with a wget test since Squid allocates a lot of memory that the host must manage.
>>> This is a possible explanation for the fact that you see the performance going down and up.
>>> Can you do the same test on the host (i.e. not inside a VM).
>>>
>>> Marcus
>>>
>>>
>>>
>>> On 07/23/2015 10:39 AM, Jens Offenbach wrote:
>>>> I have attached strace to Squid and waited until the download rate has decreased to 500 KB/sec.
>>>> I used "cache_dir aufs /var/cache/squid3 88894 16 256 max-size=10737418240".
>>>> Here is the download link:
>>>> http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace][http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace[http://w1.wikisend.com/node-fs/download/6a004a416f65b4cdf7f8eff4ff961199/squid.strace]]]
>>>> I hope it can help you.
>>>> *Gesendet:* Donnerstag, 23. Juli 2015 um 13:29 Uhr
>>>> *Von:* "Marcus Kool" <marcus.kool at urlfilterdb.com>
>>>> *An:* "Jens Offenbach" <wolle5050 at gmx.de>, "Eliezer Croitoru" <eliezer at ngtech.co.il>, "Amos Jeffries" <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org
>>>> *Betreff:* Re: [squid-users] Squid3: 100 % CPU load during object caching
>>>> I am not sure if it is relevant, maybe it is:
>>>>
>>>> I am developing an ICAP daemon and after the ICAP server sends a "100 continue"
>>>> Squid sends the object to the ICAP server in small chunks of varying sizes:
>>>> 4095, 5813, 1448, 4344, 1448, 1448, 2896, etc.
>>>> Note that the interval of receiving the chunks is 1/1000th of a second.
>>>> It seems that Squid forwards the object to the ICAP server every time it receives
>>>> one or a few TCP packets.
>>>>
>>>> I have a suspicion that in the scenario of 100% CPU, large #write calls and low throughput a similar thing is happening:
>>>> Squid physically stores a small part of the object many times, i.e. every time one or a few TCP packets arrive.
>>>>
>>>> Amos, is there a debug setting that can confirm/reject this suspicion?
>>>>
>>>> Marcus
>>>>
>>>>
>>>> On 07/23/2015 04:25 AM, Jens Offenbach wrote:
>>>>> A test with ROCK "cache_dir rock /var/cache/squid3 51200" gives very confusing results.
>>>>>
>>>>> I cleared the cache:
>>>>> rm -rf /var/cache/squid3/*
>>>>> squid -z
>>>>> squid
>>>>> http_proxy=http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]][http://139.2.57.120:3128/[http://139.2.57.120:3128/][http://139.2.57.120:3128/[http://139.2.57.120:3128/]]]] wget http://test-server/freesurfer-Linux-centos6_x86_64-stable-pub-v5.3.0.tar
>>>>>
>>>>> The download starts with 10 MB/sec and stays constant for 1 minutes, then it drops gradually to 1 MB/sec and stays there for some time. After 5 minutes the download rate returns back to 10 MB/sec
>>>> very quickly and drops again step-by-step to 1 MB/sec. After 5-6 minutes the download rates rises again to 10 MB/sec and drops again gradually to 1 MB/sec.
>>>>>
>>>>> During caching progress, we have 100 % CPU usage and a disk IO that is corresponds with the download rate.
>>>>>
>>>>> For further investigations I give you my build properties:
>>>>> squid -v
>>>>> Squid Cache: Version 3.5.6
>>>>> Service Name: squid
>>>>> configure options: '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
>>>> '--libexecdir=/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
>>>> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
>>>> '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
>>>> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>>>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-e
>>>> s
>>>> i' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/cache/squid3' '--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
>>>> '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
>>>> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>>>> -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>>>>>
>>>>>
>>>>> Gesendet: Mittwoch, 22. Juli 2015 um 21:47 Uhr
>>>>> Von: "Eliezer Croitoru" <eliezer at ngtech.co.il>
>>>>> An: squid-users at lists.squid-cache.org
>>>>> Betreff: Re: [squid-users] Squid3: 100 % CPU load during object caching
>>>>> On 22/07/2015 21:59, Eliezer Croitoru wrote:
>>>>>> Hey Jens,
>>>>>>
>>>>>> I have tested the issue with LARGE ROCK and not AUFS or UFS.
>>>>>> Using squid or not my connection to the server is about 2.5 MBps (20Mbps).
>>>>>> Squid is sitting on an intel atom with SSD drive and on a HIT case the
>>>>>> download speed is more then doubled to 4.5 MBps(36Mbps).
>>>>>> I have not tried it with AUFS yet.
>>>>>
>>>>>
>>>>> And I must admit that AUFS beats rock cache with speed.
>>>>> I have tried rock with basic "cache_dir rock /var/spool/squid 8000" vs
>>>>> "cache_dir aufs /var/spool/squid 8000 16 256" and the aufs cache HIT
>>>>> results more then doubles 3 the speed rock gave with default settings.
>>>>>
>>>>> So about 15MBps which is 120Mbps.
>>>>> I do not seem to feel what Jens feels but the 100% CPU might be because
>>>>> of spinning disk hangs while reading the file from disk.
>>>>>
>>>>> Amos, I remember that there were some suggestions how to tune large rock.
>>>>> Any hints?
>>>>> I can test it and make it a suggestion for big files.
>>>>>
>>>>> Eliezer
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]]
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]]
>>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]]
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users][http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]]
>>>
>>
>>
>
>


From alex_wu2012 at hotmail.com  Sat Jul 25 15:55:49 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Sat, 25 Jul 2015 08:55:49 -0700
Subject: [squid-users] cannot leave empty workers
In-Reply-To: <55B2DE57.2030106@treenet.co.nz>
References: <BAY181-W69B860586ACE9C6EA8CDC983810@phx.gbl>,
 <55B2B716.3070500@treenet.co.nz>,
 <BAY181-W48911B30204BDB0252A1A383810@phx.gbl>,
 <BAY181-W79195CF421D92AE7D025A183810@phx.gbl>,
 <55B2DE57.2030106@treenet.co.nz>
Message-ID: <BAY181-W63F1DD705D3F0C90962B2E83800@phx.gbl>

I may concur with the way it is.

For shared memory, if it is created by worker, it might trigger a race condition. Also, the size of shared segment is fixed across all processes, so each worker cannot create a different size of shared memory. So the current code simply lets master process create it, and wokrer just attaches to it.

Alex


> Subject: Re: [squid-users] cannot leave empty workers
> To: alex_wu2012 at hotmail.com; squid-users at lists.squid-cache.org
> From: squid3 at treenet.co.nz
> Date: Sat, 25 Jul 2015 12:54:47 +1200
> 
> On 25/07/2015 11:53 a.m., Alex Wu wrote:
> > further analysis indicated that master process created quid-ssl_session_cache.shm.
> > 
> > In other words, it needs a https_port or http_port with ssl-bump in outside any process number to create this shared memeory segment.
> > 
> > Furthermore, the code  should be simplied like this:
> > 
> > diff --git a/squid-3.5.6/src/ssl/support.cc b/squid-3.5.6/src/ssl/support.cc
> > index 85305ce..0ce95f9 100644
> > --- a/squid-3.5.6/src/ssl/support.cc
> > +++ b/squid-3.5.6/src/ssl/support.cc
> > @@ -2084,9 +2084,6 @@ SharedSessionCacheRr::useConfig()
> >  void
> >  SharedSessionCacheRr::create()
> >  {
> > -    if (!isSslServer()) //no need to configure ssl session cache.
> > -        return;
> > -
> >      int items;
> >      items = Config.SSL.sessionCacheSize / sizeof(Ipc::MemMap::Slot);
> >      if (items)
> > 
> > 
> > 
> > This code is called in master that may not have configuration to ensure isSsslServer return true.
> > 
> 
> The bug is in why that SharedSessionCacheRr is not being run by the worker.
> 
> AFAIK, it is the way the worker is supposed to attach to the shared
> memory. First process to access the SHM does the create, others attach.
> 
> Amos
> 
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150725/02040cf2/attachment.htm>

From stan.prescott at gmail.com  Sun Jul 26 00:33:10 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sat, 25 Jul 2015 19:33:10 -0500
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <1437783873.7042.9.camel@JamesiMac>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
 <1437783873.7042.9.camel@JamesiMac>
Message-ID: <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>

I did a new install of Squid 3.5.6 and it seems to be working now.

On Fri, Jul 24, 2015 at 7:24 PM, James Lay <jlay at slave-tothe-box.net> wrote:

>  On Fri, 2015-07-24 at 19:15 -0500, Stanford Prescott wrote:
>
> Thanks for that. Any ideas why I am experiencing that?
>
>
>  Stan
>
>
>
>  On Fri, Jul 24, 2015 at 7:07 PM, James Lay <jlay at slave-tothe-box.net>
> wrote:
>
>  On Fri, 2015-07-24 at 17:25 -0500, Stanford Prescott wrote:
>
> I have a working implementation of Squid 3.5.5 with ssl-bump. When 3.5.5
> is started with ssl-bump enabled all the squid and ssl_crtd processes start
> and Squid functions as intended when bumping ssl sites. However, when I
> bump Squid to 3.5.6 squid seems to start but ssl_crtd does not and Squid
> 3.5.6 cannot successfully bump ssl.
>
>
> These are the config options I use for both 3.5.5 and 3.5.6.
>
> --enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter \
> --enable-removal-policies="heap,lru" --enable-delay-pools
> --libdir=/usr/lib/ \
> --localstatedir=/var --with-dl --with-openssl --enable-http-violations \
> --with-large-files --with-libcap --disable-ipv6
> --with-swapdir=/var/spool/squid \
>  --enable-ssl-crtd --enable-follow-x-forwarded-for
>
>
>
> This is the squid.conf file used for both versions.
>
> visible_hostname smoothwallu3
>
> # Uncomment the following to send debug info to /var/log/squid/cache.log
> debug_options ALL,1 33,2 28,9
>
> # ACCESS CONTROLS
> # ----------------------------------------------------------------
> acl localhostgreen src 10.20.20.1
> acl localnetgreen src 10.20.20.0/24
>
> acl SSL_ports port 445 443 441 563
> acl Safe_ports port 80            # http
> acl Safe_ports port 81            # smoothwall http
> acl Safe_ports port 21            # ftp
> acl Safe_ports port 445 443 441 563    # https, snews
> acl Safe_ports port 70             # gopher
> acl Safe_ports port 210               # wais
> acl Safe_ports port 1025-65535        # unregistered ports
> acl Safe_ports port 280               # http-mgmt
> acl Safe_ports port 488               # gss-http
> acl Safe_ports port 591               # filemaker
> acl Safe_ports port 777               # multiling http
>
> acl CONNECT method CONNECT
>
> # TAG: http_access
> # ----------------------------------------------------------------
>
>
>
> http_access allow localhost
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
>
> http_access allow localnetgreen
> http_access allow CONNECT localnetgreen
>
> http_access allow localhostgreen
> http_access allow CONNECT localhostgreen
>
> # http_port and https_port
>
> #----------------------------------------------------------------------------
>
> # For forward-proxy port. Squid uses this port to serve error pages, ftp
> icons and communication with other proxies.
>
> #----------------------------------------------------------------------------
> http_port 3127
>
> http_port 10.20.20.1:800 intercept
> https_port 10.20.20.1:808 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>
>
> http_port 127.0.0.1:800 intercept
>
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_session_cache_size 4 MB
>
> ssl_bump none localhostgreen
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1
> ssl_bump bump all
>
> sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
> sslcrtd_children 5
>
> http_access deny all
>
> cache_replacement_policy heap GDSF
> memory_replacement_policy heap GDSF
>
> # CACHE OPTIONS
> #
> ----------------------------------------------------------------------------
> cache_effective_user squid
> cache_effective_group squid
>
> cache_swap_high 100
> cache_swap_low 80
>
> cache_access_log stdio:/var/log/squid/access.log
> cache_log /var/log/squid/cache.log
> cache_mem 64 MB
>
> cache_dir diskd /var/spool/squid/cache 1024 16 256
>
> maximum_object_size 33 MB
>
> minimum_object_size 0 KB
>
>
> request_body_max_size 0 KB
>
> # OTHER OPTIONS
> #
> ----------------------------------------------------------------------------
> #via off
> forwarded_for off
>
> pid_filename /var/run/squid.pid
>
> shutdown_lifetime 30 seconds
> icp_port 3130
>
> half_closed_clients off
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_encode off
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache
> icap://localhost:1344/squidclamav bypass=off
> adaptation_access service_avi_req allow all
> icap_service service_avi_resp respmod_precache
> icap://localhost:1344/squidclamav bypass=on
> adaptation_access service_avi_resp allow all
>
> umask 022
>
> logfile_rotate 0
>
> strip_query_terms off
>
> redirect_program /usr/sbin/squidGuard
> url_rewrite_children 5
>
>
> And the cache.log file when starting 3.5.6 with debug options on in
> squid.conf
>
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL adaptation_access*
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL adaptation_access*
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.230| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.231| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06.232| Acl.cc(380) ~ACL: freeing ACL *
> *2015/07/24 17:15:06 kid1| Current Directory is /*
> *2015/07/24 17:15:06 kid1| Starting Squid Cache version 3.5.6 for
> i586-pc-linux-gnu...*
> *2015/07/24 17:15:06 kid1| Service Name: squid*
> *2015/07/24 17:15:06 kid1| Process ID 2907*
> *2015/07/24 17:15:06 kid1| Process Roles: worker*
> *2015/07/24 17:15:06 kid1| With 1024 file descriptors available*
> *2015/07/24 17:15:06 kid1| Initializing IP Cache...*
> *2015/07/24 17:15:06 kid1| DNS Socket created at 0.0.0.0, FD 8*
> *2015/07/24 17:15:06 kid1| Adding nameserver 127.0.0.1 from
> /etc/resolv.conf*
> *2015/07/24 17:15:06 kid1| helperOpenServers: Starting 0/5 'squidGuard'
> processes*
> *2015/07/24 17:15:06 kid1| helperOpenServers: No 'squidGuard' processes
> needed.*
> *2015/07/24 17:15:06 kid1| Logfile: opening log
> stdio:/var/log/squid/access.log*
> *2015/07/24 17:15:06 kid1| Unlinkd pipe opened on FD 15*
> *2015/07/24 17:15:06 kid1| Store logging disabled*
> *2015/07/24 17:15:06 kid1| Swap maxSize 1048576 + 65536 KB, estimated
> 85700 objects*
> *2015/07/24 17:15:06 kid1| Target number of buckets: 4285*
> *2015/07/24 17:15:06 kid1| Using 8192 Store buckets*
> *2015/07/24 17:15:06 kid1| Max Mem  size: 65536 KB*
> *2015/07/24 17:15:06 kid1| Max Swap size: 1048576 KB*
> *2015/07/24 17:15:06 kid1| Rebuilding storage in /var/spool/squid/cache
> (dirty log)*
> *2015/07/24 17:15:06 kid1| Using Least Load store dir selection*
> *2015/07/24 17:15:06 kid1| Current Directory is /*
> *2015/07/24 17:15:06 kid1| Finished loading MIME types and icons.*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d218 [call5]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c) [call5]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d3a8 [call7]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc) [call7]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d510 [call9]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544) [call9]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
> clientListenerConnectionOpened constructed, this=0x946d6b0 [call11]*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(93) ScheduleCall:
> StartListening.cc(59) will call
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4) [call11]*
> *2015/07/24 17:15:06.578 kid1| HTCP Disabled.*
> *2015/07/24 17:15:06.578 kid1| Squid plugin modules loaded: 0*
> *2015/07/24 17:15:06.578 kid1| Adaptation support is on*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call5]*
> *2015/07/24 17:15:06.578 kid1| Accepting HTTP Socket connections at
> local=0.0.0.0:3127 <http://0.0.0.0:3127> remote=[::] FD 20 flags=9*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=0.0.0.0:3127 <http://0.0.0.0:3127>
> remote=[::] FD 20 flags=9, err=0, HTTP Socket port=0x946d24c)*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)*
> *2015/07/24 17:15:06.578 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call7]*
> *2015/07/24 17:15:06.578 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=10.20.20.1:800 <http://10.20.20.1:800> remote=[::] FD
> 21 flags=41*
> *2015/07/24 17:15:06.578 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:800 <http://10.20.20.1:800>
> remote=[::] FD 21 flags=41, err=0, HTTP Socket port=0x946d3dc)*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)*
> *2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call9]*
> *2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted HTTP Socket
> connections at local=127.0.0.1:800 <http://127.0.0.1:800> remote=[::] FD 22
> flags=41*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=127.0.0.1:800 <http://127.0.0.1:800>
> remote=[::] FD 22 flags=41, err=0, HTTP Socket port=0x946d544)*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(55) fireNext: entering
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)*
> *2015/07/24 17:15:06.579 kid1| AsyncCall.cc(38) make: make call
> clientListenerConnectionOpened [call11]*
> *2015/07/24 17:15:06.579 kid1| Accepting NAT intercepted SSL bumped HTTPS
> Socket connections at local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41*
> *2015/07/24 17:15:06.579 kid1| AsyncCallQueue.cc(57) fireNext: leaving
> clientListenerConnectionOpened(local=10.20.20.1:808 <http://10.20.20.1:808>
> remote=[::] FD 23 flags=41, err=0, HTTPS Socket port=0x946d6e4)*
> *2015/07/24 17:15:06.579 kid1| Accepting ICP messages on 0.0.0.0:3130
> <http://0.0.0.0:3130>*
> *2015/07/24 17:15:06.579 kid1| Sending ICP messages from 0.0.0.0:3130
> <http://0.0.0.0:3130>*
> *2015/07/24 17:15:06.579 kid1| Done reading /var/spool/squid/cache swaplog
> (12 entries)*
> *2015/07/24 17:15:06.579 kid1| Finished rebuilding storage from disk.*
> *2015/07/24 17:15:06.579 kid1|        12 Entries scanned*
> *2015/07/24 17:15:06.579 kid1|         0 Invalid entries.*
> *2015/07/24 17:15:06.579 kid1|         0 With invalid flags.*
> *2015/07/24 17:15:06.579 kid1|        12 Objects loaded.*
> *2015/07/24 17:15:06.579 kid1|         0 Objects expired.*
> *2015/07/24 17:15:06.579 kid1|         0 Objects cancelled.*
> *2015/07/24 17:15:06.579 kid1|         0 Duplicate URLs purged.*
> *2015/07/24 17:15:06.579 kid1|         0 Swapfile clashes avoided.*
> *2015/07/24 17:15:06.579 kid1|   Took 0.06 seconds (210.47 objects/sec).*
> *2015/07/24 17:15:06.579 kid1| Beginning Validation Procedure*
> *2015/07/24 17:15:06.579 kid1|   Completed Validation Procedure*
> *2015/07/24 17:15:06.579 kid1|   Validated 12 Entries*
> *2015/07/24 17:15:06.579 kid1|   store_swap_size = 1444.00 KB*
> *2015/07/24 17:15:07 kid1| storeLateRelease: released 0 objects*
>
>
>
> Any help or suggestions greatly appreciated.
>
>
> Regards
>
>
> Stan
>
>
>    _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
> I do not experience this issue:
>
> [18:04:56 jlay <jlay at gateway>:~/nobackup/build$] ps aux | egrep
> "ssl|squid"
> root      3173  0.0  0.0  18840   372 ?        Ss   Jul23   0:00
> /opt/sbin/squid
> nobody    3175  0.0  1.2  52856 39744 ?        S    Jul23   0:47 (squid-1)
> nobody    3177  0.0  0.0   5916  2040 ?        S    Jul23   0:05
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3178  0.0  0.0   5828  1840 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3179  0.0  0.0   5828  1708 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3180  0.0  0.0   5648   912 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
> nobody    3181  0.0  0.0   5648   912 ?        S    Jul23   0:00
> (ssl_crtd) -s /opt/var/ssl_db -M 4MB -b 4096
>
> my config line:
> ./configure --prefix=/opt --with-openssl --enable-ssl --enable-ssl-crtd
> --enable-linux-netfilter --enable-follow-x-forwarded-for --with-large-files
> --sysconfdir=/opt/etc/squid --enable-external-acl-helpers=none
>
> Squid Cache: Version 3.5.6
>
> James
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>  _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
> I recall when just starting out with ssl_crtd and had issue until I set
> the user running as squid  on my ssl_db dir:
>
> drwxr-xr-x 3 nobody root 4096 May 30 17:22 ssl_db
>
> My ssl_crtd lines:
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> sslcrtd_children 5
>
> Hope it helps.
>
> James
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150725/dcfd5568/attachment.htm>

From spider at smoothnet.org  Sun Jul 26 05:09:30 2015
From: spider at smoothnet.org (The_Spider)
Date: Sun, 26 Jul 2015 00:09:30 -0500
Subject: [squid-users] Investigating squid crash.
Message-ID: <CAJfE2f6PYbg=b-_zcX9586kRJ0qTRr595ah5z5eCjh3f-ywQ0Q@mail.gmail.com>

To anyone willing to assist.

My Squid continues to crash for some un-known reason only giving
'FATAL: Received Segment Violation...dying.' I have attached a copy of
the cache log with the crash occurring and output debugging.

Aparently I'm not the first to experience this, but I have yet to find
any documentation on how to resolve this issue. This is not unique to
one single machine, it happens on a few different boxes with similar
configuration, I have not been able to identify what configuration
parameter has been causing the issue.

I will attempt to provide any information possible to help resolve
this issue. Hopefully this is a good start.

cache.log = http://filebin.ca/29vS8jkEfskt/cache.log

squid -v:
Squid Cache: Version 3.5.6-20150725-r13869
Service Name: squid
configure options:  '--prefix=/usr' '--exec-prefix=/usr'
'--includedir=/usr/include' '--datadir=/usr/share'
'--libdir=/usr/lib64' '--libexecdir=/usr/lib64/squid'
'--localstatedir=/var' '--sysconfdir=/etc/squid'
'--sharedstatedir=/var/lib' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
'--enable-silent-rules' '--enable-dependency-tracking'
'--with-openssl' '--enable-icmp' '--enable-delay-pools'
'--enable-useragent-log' '--enable-esi'
'--enable-follow-x-forwarded-for' '--enable-auth'
--enable-ltdl-convenience

cat /etc/redhat-release
CentOS Linux release 7.1.1503 (Core)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 7704 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150726/33c02b51/attachment.obj>

From eliezer at ngtech.co.il  Sun Jul 26 15:15:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 26 Jul 2015 18:15:50 +0300
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
 <1437783873.7042.9.camel@JamesiMac>
 <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>
Message-ID: <55B4F9A6.2000609@ngtech.co.il>

On 26/07/2015 03:33, Stanford Prescott wrote:
> I did a new install of Squid 3.5.6 and it seems to be working now.
On what OS?

Eliezer


From stan.prescott at gmail.com  Sun Jul 26 15:26:52 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 26 Jul 2015 10:26:52 -0500
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <55B4F9A6.2000609@ngtech.co.il>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
 <1437783873.7042.9.camel@JamesiMac>
 <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>
 <55B4F9A6.2000609@ngtech.co.il>
Message-ID: <CANLNtGSU3Vup85mxCa0qP=z4kSkoMB9LUdcDuPQOaKTbApng9Q@mail.gmail.com>

The OS is Smoothwall Express v3.1. A linux firewall distro not really based
on any other of the major distros.

On Sun, Jul 26, 2015 at 10:15 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> On 26/07/2015 03:33, Stanford Prescott wrote:
>
>> I did a new install of Squid 3.5.6 and it seems to be working now.
>>
> On what OS?
>
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150726/2177fabb/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 26 20:53:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Jul 2015 08:53:29 +1200
Subject: [squid-users] Investigating squid crash.
In-Reply-To: <CAJfE2f6PYbg=b-_zcX9586kRJ0qTRr595ah5z5eCjh3f-ywQ0Q@mail.gmail.com>
References: <CAJfE2f6PYbg=b-_zcX9586kRJ0qTRr595ah5z5eCjh3f-ywQ0Q@mail.gmail.com>
Message-ID: <55B548C9.4070103@treenet.co.nz>

On 26/07/2015 5:09 p.m., The_Spider wrote:
> To anyone willing to assist.
> 
> My Squid continues to crash for some un-known reason only giving
> 'FATAL: Received Segment Violation...dying.' I have attached a copy of
> the cache log with the crash occurring and output debugging.
> 
> Aparently I'm not the first to experience this, but I have yet to find
> any documentation on how to resolve this issue. This is not unique to
> one single machine, it happens on a few different boxes with similar
> configuration, I have not been able to identify what configuration
> parameter has been causing the issue.
> 

Please follow the steps for obtaining a traceback as outlined at
<http://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps>

For SEGFAULT errors that is the only easy way to identify what the
problem is. Trial-and-error can take a very, very long time.


As for the config. You certainly have a thing for regex. Most if not all
of the dstdom_regex patterns would be far better written as dstdomain ACLs.

Although note that neither dstdomain nor dstdom_regex work properly in
the ssl_bump directive lines. Use "ssl::server_name" ACL for those checks.



> I will attempt to provide any information possible to help resolve
> this issue. Hopefully this is a good start.
> 
> cache.log = http://filebin.ca/29vS8jkEfskt/cache.log
> 
> squid -v:
> Squid Cache: Version 3.5.6-20150725-r13869
> Service Name: squid
> configure options:  '--prefix=/usr' '--exec-prefix=/usr'
> '--includedir=/usr/include' '--datadir=/usr/share'
> '--libdir=/usr/lib64' '--libexecdir=/usr/lib64/squid'
> '--localstatedir=/var' '--sysconfdir=/etc/squid'
> '--sharedstatedir=/var/lib' '--with-logdir=/var/log/squid'
> '--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
> '--enable-silent-rules' '--enable-dependency-tracking'
> '--with-openssl' '--enable-icmp' '--enable-delay-pools'
> '--enable-useragent-log'

--enable-useragent-log no longer exists.

> '--enable-esi'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> --enable-ltdl-convenience
> 
> cat /etc/redhat-release
> CentOS Linux release 7.1.1503 (Core)
> 

Amos



From spider at smoothnet.org  Sun Jul 26 22:47:14 2015
From: spider at smoothnet.org (The_Spider)
Date: Sun, 26 Jul 2015 17:47:14 -0500
Subject: [squid-users] Investigating squid crash.
Message-ID: <CAJfE2f4BgK=dcYnddOHa7bZk-m+mNsrh+CWgHD_OoPjkJow3Lg@mail.gmail.com>

I shall follow the directions as instructed and report later this evening.

Yes, I have a thing for regex, and will take your advice and re-write
the config after I report the results you requested.

Is there a method of posting the results the list would prefer should
the results extend beyond the 100k attachment limit?

Responce:
A stack trace / backtrace should only be a few dozen lines at most.

I expect that if it turns out to be one of the rare cases with many
pages long only the first and last ~40 lines will matter. So you can
start with just that.
Amos


From david at articatech.com  Mon Jul 27 00:06:14 2015
From: david at articatech.com (David Touzeau)
Date: Mon, 27 Jul 2015 02:06:14 +0200
Subject: [squid-users] 3.5.6: need help: FATAL: No valid signing SSL
 certificate but openssl verify is OK
Message-ID: <55B575F6.3060403@articatech.com>


Dear

My certificate and my own Root CA's that are already installed on all 
computers and need to use it in Squid.

using

The Certificate :
--------------------------------------------------------------------------------------------------
openssl x509 -subject -issuer -enddate -noout -in certificate.pem
subject= /C=FR/ST=Ile de France/L=Paris/O=My Company/OU=IT 
service/CN=proxyweb.domain.tld
issuer= /CN=ACTISSIA-CA
notAfter=Jul  8 12:32:53 2016 GMT

The Root CA
--------------------------------------------------------------------------------------------------
openssl x509 -subject -issuer -enddate -noout -in /etc/squid3/Cafile.ca
subject= /CN=ACTISSIA-CA
issuer= /CN=ACTISSIA-CA
notAfter=Apr 10 08:03:12 2019 GMT


Verify certificate and Root's CA:
--------------------------------------------------------------------------------------------------
/usr/bin/openssl verify -verbose -CAfile /etc/squid3/Cafile.ca 
/etc/squid3/certificate.pem
certificate.pem: OK


i have create the chain

cat /etc/squid3/Cafile.ca >/etc/squid3/chain.pem
cat /etc/squid3/certificate.pem >>/etc/squid3/chain.pem

Added :
http_port 0.0.0.0:3128  ssl-bump  generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/chain.pem

But i was unable to start squid with the error

2015/07/27 00:57:43| Using certificate in /etc/squid3/ssl/calast.pem
2015/07/27 00:57:43| storeDirWriteCleanLogs: Starting...
2015/07/27 00:57:43|   Finished.  Wrote 0 entries.
2015/07/27 00:57:43|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: No valid signing SSL certificate configured for HTTP_port 
0.0.0.0:3128
Squid Cache (Version 3.5.6): Terminated abnormally.










From squid3 at treenet.co.nz  Mon Jul 27 00:24:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Jul 2015 12:24:35 +1200
Subject: [squid-users] 3.5.6: need help: FATAL: No valid signing SSL
 certificate but openssl verify is OK
In-Reply-To: <55B575F6.3060403@articatech.com>
References: <55B575F6.3060403@articatech.com>
Message-ID: <55B57A43.5090901@treenet.co.nz>

On 27/07/2015 12:06 p.m., David Touzeau wrote:
> 
> Dear
> 
> My certificate and my own Root CA's that are already installed on all
> computers and need to use it in Squid.
> 
> using
> 
> The Certificate :
> --------------------------------------------------------------------------------------------------
> 
> openssl x509 -subject -issuer -enddate -noout -in certificate.pem
> subject= /C=FR/ST=Ile de France/L=Paris/O=My Company/OU=IT
> service/CN=proxyweb.domain.tld
> issuer= /CN=ACTISSIA-CA
> notAfter=Jul  8 12:32:53 2016 GMT
> 
> The Root CA
> --------------------------------------------------------------------------------------------------
> 
> openssl x509 -subject -issuer -enddate -noout -in /etc/squid3/Cafile.ca
> subject= /CN=ACTISSIA-CA
> issuer= /CN=ACTISSIA-CA
> notAfter=Apr 10 08:03:12 2019 GMT
> 
> 
> Verify certificate and Root's CA:
> --------------------------------------------------------------------------------------------------
> 
> /usr/bin/openssl verify -verbose -CAfile /etc/squid3/Cafile.ca
> /etc/squid3/certificate.pem
> certificate.pem: OK
> 
> 
> i have create the chain
> 
> cat /etc/squid3/Cafile.ca >/etc/squid3/chain.pem
> cat /etc/squid3/certificate.pem >>/etc/squid3/chain.pem
> 
> Added :
> http_port 0.0.0.0:3128  ssl-bump  generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/chain.pem
> 
> But i was unable to start squid with the error
> 
> 2015/07/27 00:57:43| Using certificate in /etc/squid3/ssl/calast.pem
> 2015/07/27 00:57:43| storeDirWriteCleanLogs: Starting...
> 2015/07/27 00:57:43|   Finished.  Wrote 0 entries.
> 2015/07/27 00:57:43|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: No valid signing SSL certificate configured for HTTP_port
> 0.0.0.0:3128
> Squid Cache (Version 3.5.6): Terminated abnormally.
> 

Firstly;

 Notice that what Squid is loading a file called calast.pem. Not the
chain.pem one your config snippet shows.


Secondly;

What happens in ssl-bump generate-host-certificates=on is that Squid
generates a *third* certificate tied specifically to the domain the
client asked for, and sends that to the client.

It needs to be signed by a CA the client trusts.

Does /etc/squid3/chain.pem contain the private key of a CA whose public
key is trusted by the client already?


*without* the generate-host-certificates Squid would just be loading the
certificate.pem part out of chain.pem and sending that to the client.


Amos


From spider at smoothnet.org  Mon Jul 27 03:50:13 2015
From: spider at smoothnet.org (The_Spider)
Date: Sun, 26 Jul 2015 22:50:13 -0500
Subject: [squid-users] Investigating squid crash.
Message-ID: <CAJfE2f5keMSOAW39ww71m5Z7QRyZ4ZRuNEZ_=UjnA4YauWgEZg@mail.gmail.com>

Attached is the backtrace from the gdb debugger. Again, if there is
any thing I can do to assist further, please ask.

#0  0x00007ffff53e05d7 in raise () from /lib64/libc.so.6
#1  0x00007ffff53e1cc8 in abort () from /lib64/libc.so.6
#2  0x00000000005770ef in xassert (msg=<optimized out>,
file=<optimized out>, line=<optimized out>) at debug.cc:544
#3  0x0000000000791758 in comm_read_base (conn=...,
buf=buf at entry=0xa2b3a240 "", size=16383, callback=...) at Read.cc:69
#4  0x0000000000648791 in comm_read (callback=..., len=<optimized
out>, buf=0xa2b3a240 "", conn=...) at comm/Read.h:58
#5  StoreEntry::delayAwareRead (this=this at entry=0xd805d0f0, conn=...,
buf=0xa2b3a240 "", len=16383, callback=...) at store.cc:255
#6  0x0000000000648b48 in StoreEntry::DeferReader
(theContext=0xd805d0f0, aRead=...) at store.cc:214
#7  0x00000000006fbb0f in DeferredReadManager::kickARead
(this=<optimized out>, aRead=...) at comm.cc:1809
#8  0x0000000000702231 in DeferredReadManager::flushReads
(this=0xdf42a8) at comm.cc:1794
#9  0x0000000000702569 in DeferredReadManager::kickReads
(this=0xdf42a8, count=<optimized out>) at comm.cc:1769
#10 0x0000000000577c07 in DelayPools::Update (unused=0x2d8a) at
delay_pools.cc:571
#11 0x00000000006f2479 in AsyncCall::make (this=0xd7d24580) at AsyncCall.cc:40
#12 0x00000000006f6265 in AsyncCallQueue::fireNext
(this=this at entry=0x27dc8b0) at AsyncCallQueue.cc:56
#13 0x00000000006f6660 in AsyncCallQueue::fire (this=0x27dc8b0) at
AsyncCallQueue.cc:42
#14 0x0000000000595391 in dispatchCalls (this=0x7fffffffe350) at
EventLoop.cc:143
#15 EventLoop::runOnce (this=this at entry=0x7fffffffe350) at EventLoop.cc:108
#16 0x0000000000595580 in EventLoop::run
(this=this at entry=0x7fffffffe350) at EventLoop.cc:82
#17 0x00000000005f9dac in SquidMain (argc=<optimized out>,
argv=<optimized out>) at main.cc:1518
#18 0x00000000005031bb in SquidMainSafe (argv=<optimized out>,
argc=<optimized out>) at main.cc:1250
#19 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1243


From david at articatech.com  Mon Jul 27 08:20:10 2015
From: david at articatech.com (David Touzeau)
Date: Mon, 27 Jul 2015 10:20:10 +0200
Subject: [squid-users] 3.5.6: need help: FATAL: No valid signing SSL
 certificate but openssl verify is OK
In-Reply-To: <55B57A43.5090901@treenet.co.nz>
References: <55B575F6.3060403@articatech.com> <55B57A43.5090901@treenet.co.nz>
Message-ID: <55B5E9BA.3010809@articatech.com>





Thanks Amos, i have removed the generate-host-certificates

http_port 0.0.0.0:3128  ssl-bump  dynamic_cert_mem_cache_size=4MB 
cert=/etc/squid3/ssl/chain.pem



But Squid still not want load the couple of Ca and certificate.


2015/07/27 10:16:30| Using certificate in /etc/squid3/ssl/chain.pem
2015/07/27 10:16:30| storeDirWriteCleanLogs: Starting...
2015/07/27 10:16:30|   Finished.  Wrote 0 entries.
2015/07/27 10:16:30|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: No valid signing SSL certificate configured for HTTP_port 
0.0.0.0:3128
Squid Cache (Version 3.5.5-20150619-r13846): Terminated abnormally.
CPU Usage: 0.008 seconds = 0.004 user + 0.004 sys
Maximum Resident Size: 33408 KB
Page faults with physical i/o: 0

Does /etc/squid3/chain.pem contain the private key of a CA whose public
key is trusted by the client already?

No the chain.pem, contains only the Root CA's and the certificate









From eliezer at ngtech.co.il  Mon Jul 27 12:33:54 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 27 Jul 2015 15:33:54 +0300
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <CANLNtGSU3Vup85mxCa0qP=z4kSkoMB9LUdcDuPQOaKTbApng9Q@mail.gmail.com>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
 <1437783873.7042.9.camel@JamesiMac>
 <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>
 <55B4F9A6.2000609@ngtech.co.il>
 <CANLNtGSU3Vup85mxCa0qP=z4kSkoMB9LUdcDuPQOaKTbApng9Q@mail.gmail.com>
Message-ID: <55B62532.1080009@ngtech.co.il>

It's pretty famous.
I have even used it for sometime in the past and from many firewall 
distros it was one of the good ones.

Eliezer

On 26/07/2015 18:26, Stanford Prescott wrote:
> The OS is Smoothwall Express v3.1. A linux firewall distro not really based
> on any other of the major distros.



From stan.prescott at gmail.com  Mon Jul 27 14:22:33 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Mon, 27 Jul 2015 09:22:33 -0500
Subject: [squid-users] ssl_crtd process doesn't start with Squid 3.5.6
In-Reply-To: <55B62532.1080009@ngtech.co.il>
References: <CANLNtGSFwfOPhsC2D8x6Anh-iq5Y0tdac-4Uzb9JhjHqOiFreg@mail.gmail.com>
 <1437782829.7042.5.camel@JamesiMac>
 <CANLNtGT1tLEHaz6vxLwiUn971qRL-i188a082CeWuRKmsSF5Sw@mail.gmail.com>
 <1437783873.7042.9.camel@JamesiMac>
 <CANLNtGS_UQMK=cT-w9RjpceXxrM5uEhTg3_rwuQD-uxdL2WJ_w@mail.gmail.com>
 <55B4F9A6.2000609@ngtech.co.il>
 <CANLNtGSU3Vup85mxCa0qP=z4kSkoMB9LUdcDuPQOaKTbApng9Q@mail.gmail.com>
 <55B62532.1080009@ngtech.co.il>
Message-ID: <CANLNtGSR7BWDt8Yd7wrx7w=cETaNHeRaVrt0mgGsSmHqYotbWw@mail.gmail.com>

Thanks for the props about Smoothwall. For SOHO environments, it's one of
the best, IMHO. But then, I am not prejudiced at all. ;-)

On Mon, Jul 27, 2015 at 7:33 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> It's pretty famous.
> I have even used it for sometime in the past and from many firewall
> distros it was one of the good ones.
>
> Eliezer
>
> On 26/07/2015 18:26, Stanford Prescott wrote:
>
>> The OS is Smoothwall Express v3.1. A linux firewall distro not really
>> based
>> on any other of the major distros.
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150727/59dfdd98/attachment.htm>

From stan.prescott at gmail.com  Mon Jul 27 14:43:45 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Mon, 27 Jul 2015 09:43:45 -0500
Subject: [squid-users] Investigating squid crash.
In-Reply-To: <55B548C9.4070103@treenet.co.nz>
References: <CAJfE2f6PYbg=b-_zcX9586kRJ0qTRr595ah5z5eCjh3f-ywQ0Q@mail.gmail.com>
 <55B548C9.4070103@treenet.co.nz>
Message-ID: <CANLNtGT6oBZtLhDdK_d=gDJbWMnVwzFBMgUfv6K-ZmHyyg95OA@mail.gmail.com>

The developers of Smoothwall Express v3.1 have been trying to address this
issue for a few days now. We have had users complaining of this same issue
with Squid 3.5.5 and 3.5.6. It didn't seem to happen with prior versions.
We (or at least our lead developer Neal Murphy) thinks it is related to
shutting down Squid with a single SIGTERM followed by a SIGKILL a few
seconds later (or just using squid -k shutdown) is causing squid to close
and exit before the swap.state file is written and saved. This causes a
corrupted swap.state file to exist when squid is restarted. When Squid
crashes, rebuilding the swap.state anew seems to fix the problem and Squid
can restart.

So, to fix the crashing problem, for us at least, seems to involve
redesigning the shutdown and restarting of squid. Neal has come up with a
process and scripts that seem to fix the issue, at least for low loads on
squid that typically are what our users use. Neal has run some tests on
higher loads and it seems to work as well for those environments as well,
but we are continuing to test. Here is a link to the discussion in the
Smoothwall community forums if anyone is interested.

http://community.smoothwall.org/forum/viewtopic.php?p=340353#p340353

On Sun, Jul 26, 2015 at 3:53 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/07/2015 5:09 p.m., The_Spider wrote:
> > To anyone willing to assist.
> >
> > My Squid continues to crash for some un-known reason only giving
> > 'FATAL: Received Segment Violation...dying.' I have attached a copy of
> > the cache log with the crash occurring and output debugging.
> >
> > Aparently I'm not the first to experience this, but I have yet to find
> > any documentation on how to resolve this issue. This is not unique to
> > one single machine, it happens on a few different boxes with similar
> > configuration, I have not been able to identify what configuration
> > parameter has been causing the issue.
> >
>
> Please follow the steps for obtaining a traceback as outlined at
> <http://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps>
>
> For SEGFAULT errors that is the only easy way to identify what the
> problem is. Trial-and-error can take a very, very long time.
>
>
> As for the config. You certainly have a thing for regex. Most if not all
> of the dstdom_regex patterns would be far better written as dstdomain ACLs.
>
> Although note that neither dstdomain nor dstdom_regex work properly in
> the ssl_bump directive lines. Use "ssl::server_name" ACL for those checks.
>
>
>
> > I will attempt to provide any information possible to help resolve
> > this issue. Hopefully this is a good start.
> >
> > cache.log = http://filebin.ca/29vS8jkEfskt/cache.log
> >
> > squid -v:
> > Squid Cache: Version 3.5.6-20150725-r13869
> > Service Name: squid
> > configure options:  '--prefix=/usr' '--exec-prefix=/usr'
> > '--includedir=/usr/include' '--datadir=/usr/share'
> > '--libdir=/usr/lib64' '--libexecdir=/usr/lib64/squid'
> > '--localstatedir=/var' '--sysconfdir=/etc/squid'
> > '--sharedstatedir=/var/lib' '--with-logdir=/var/log/squid'
> > '--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
> > '--enable-silent-rules' '--enable-dependency-tracking'
> > '--with-openssl' '--enable-icmp' '--enable-delay-pools'
> > '--enable-useragent-log'
>
> --enable-useragent-log no longer exists.
>
> > '--enable-esi'
> > '--enable-follow-x-forwarded-for' '--enable-auth'
> > --enable-ltdl-convenience
> >
> > cat /etc/redhat-release
> > CentOS Linux release 7.1.1503 (Core)
> >
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150727/6ff7c0ad/attachment.htm>

From David.J.Berkes at pjc.com  Mon Jul 27 15:53:32 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Mon, 27 Jul 2015 15:53:32 +0000
Subject: [squid-users] random forward proxy authentication pop-up
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>


I have squid configured as a forward proxy with basic authentication.  All traffic flows as expected, but periodically I get an authentication pop-up indicating an origin server is requiring credentials.  I check the URL via non-proxy browser and does not ask for proxy credentials?  So to summarize.  Some origin URL's past the forward proxy are asking for basic authentication credentials.  These are not secured with authentication, but give me a pop-up asking for credentials?  Any help would be appreciated.


________________________________

Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150727/9dc44a56/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jul 27 16:08:17 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 27 Jul 2015 17:08:17 +0100
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
Message-ID: <201507271708.17324.Antony.Stone@squid.open.source.it>

On Monday 27 Jul 2015 at 16:53, Berkes, David wrote:

> I have squid configured as a forward proxy with basic authentication.  All
> traffic flows as expected, but periodically I get an authentication pop-up
> indicating an origin server is requiring credentials.  I check the URL via
> non-proxy browser and does not ask for proxy credentials?  So to
> summarize.  Some origin URL's past the forward proxy are asking for basic
> authentication credentials.  These are not secured with authentication,
> but give me a pop-up asking for credentials?  Any help would be
> appreciated.

It would help if you gave us:

a) an example URL which demonstrates this behaviour

b) the corresponding entries from your Squid access log when the above 
described behaviour occurs

c) your squid.conf without comments or blank lines.

Without the above information we'd just be guessing at what an unknown squid 
configuration does with an unknown URL, and we don't have the log file to 
debug the problem.


Regards,


Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From David.J.Berkes at pjc.com  Mon Jul 27 16:21:32 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Mon, 27 Jul 2015 16:21:32 +0000
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <201507271708.17324.Antony.Stone@squid.open.source.it>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
 <201507271708.17324.Antony.Stone@squid.open.source.it>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>

Here is the information requested.  From the log, everything looks to be normal.  The log example is from the cdn0.vox-cdn.com traffic.




**** ORIGIN URL's
pixel.adsafeprotected.com
cdn0.vox-cdn.com
sb.scorecardresearch.com

**** SQUID LOG
access.log.2:1437683164.693      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 29114 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 72579 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1582 70.197.241.219 TCP_TUNNEL/200 39476 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1583 70.197.241.219 TCP_TUNNEL/200 5909 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683167.244   2354 70.197.241.219 TCP_TUNNEL/200 59238 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683167.244   2362 70.197.241.219 TCP_TUNNEL/200 75369 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -

**** CONFIG
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic children 20
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 8 hours
auth_param basic casesensitive on

acl whitelist1 dstdomain pipergo.pjc.com .apple.com .yahoo.com .wp.com
acl whitelist2 dstdom_regex (^|\.)*img\.com$
acl ncsa_users proxy_auth REQUIRED

http_access allow whitelist1
http_access allow whitelist2
http_access allow ncsa_users
http_access deny all

cache_mem 4096 MB
memory_cache_mode always
refresh_pattern . 1440 100% 525949 ignore-auth
cache_dir aufs /squid/cache 40000 128 512
maximum_object_size 200 MB
maximum_object_size_in_memory 2 MB
cache_swap_low 90
cache_swap_high 95
buffered_logs on

#
half_closed_clients off
memory_pools off

# DNS-record cache
ipcache_size 10240
ipcache_low 90
ipcache_high 95
negative_dns_ttl 5 minutes

# listening port
http_port 3128

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Monday, July 27, 2015 11:08 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] random forward proxy authentication pop-up

On Monday 27 Jul 2015 at 16:53, Berkes, David wrote:

> I have squid configured as a forward proxy with basic authentication.
> All traffic flows as expected, but periodically I get an
> authentication pop-up indicating an origin server is requiring
> credentials.  I check the URL via non-proxy browser and does not ask
> for proxy credentials?  So to summarize.  Some origin URL's past the
> forward proxy are asking for basic authentication credentials.  These
> are not secured with authentication, but give me a pop-up asking for
> credentials?  Any help would be appreciated.

It would help if you gave us:

a) an example URL which demonstrates this behaviour

b) the corresponding entries from your Squid access log when the above described behaviour occurs

c) your squid.conf without comments or blank lines.

Without the above information we'd just be guessing at what an unknown squid configuration does with an unknown URL, and we don't have the log file to debug the problem.


Regards,


Antony.

--
"The problem with television is that the people must sit and keep their eyes glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From David.J.Berkes at pjc.com  Mon Jul 27 16:21:32 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Mon, 27 Jul 2015 16:21:32 +0000
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <201507271708.17324.Antony.Stone@squid.open.source.it>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
 <201507271708.17324.Antony.Stone@squid.open.source.it>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>

Here is the information requested.  From the log, everything looks to be normal.  The log example is from the cdn0.vox-cdn.com traffic.




**** ORIGIN URL's
pixel.adsafeprotected.com
cdn0.vox-cdn.com
sb.scorecardresearch.com

**** SQUID LOG
access.log.2:1437683164.693      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213 CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 29114 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 72579 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1582 70.197.241.219 TCP_TUNNEL/200 39476 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683166.464   1583 70.197.241.219 TCP_TUNNEL/200 5909 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683167.244   2354 70.197.241.219 TCP_TUNNEL/200 59238 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
access.log.2:1437683167.244   2362 70.197.241.219 TCP_TUNNEL/200 75369 CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -

**** CONFIG
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic children 20
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 8 hours
auth_param basic casesensitive on

acl whitelist1 dstdomain pipergo.pjc.com .apple.com .yahoo.com .wp.com
acl whitelist2 dstdom_regex (^|\.)*img\.com$
acl ncsa_users proxy_auth REQUIRED

http_access allow whitelist1
http_access allow whitelist2
http_access allow ncsa_users
http_access deny all

cache_mem 4096 MB
memory_cache_mode always
refresh_pattern . 1440 100% 525949 ignore-auth
cache_dir aufs /squid/cache 40000 128 512
maximum_object_size 200 MB
maximum_object_size_in_memory 2 MB
cache_swap_low 90
cache_swap_high 95
buffered_logs on

#
half_closed_clients off
memory_pools off

# DNS-record cache
ipcache_size 10240
ipcache_low 90
ipcache_high 95
negative_dns_ttl 5 minutes

# listening port
http_port 3128

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Monday, July 27, 2015 11:08 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] random forward proxy authentication pop-up

On Monday 27 Jul 2015 at 16:53, Berkes, David wrote:

> I have squid configured as a forward proxy with basic authentication.
> All traffic flows as expected, but periodically I get an
> authentication pop-up indicating an origin server is requiring
> credentials.  I check the URL via non-proxy browser and does not ask
> for proxy credentials?  So to summarize.  Some origin URL's past the
> forward proxy are asking for basic authentication credentials.  These
> are not secured with authentication, but give me a pop-up asking for
> credentials?  Any help would be appreciated.

It would help if you gave us:

a) an example URL which demonstrates this behaviour

b) the corresponding entries from your Squid access log when the above described behaviour occurs

c) your squid.conf without comments or blank lines.

Without the above information we'd just be guessing at what an unknown squid configuration does with an unknown URL, and we don't have the log file to debug the problem.


Regards,


Antony.

--
"The problem with television is that the people must sit and keep their eyes glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From spider at smoothnet.org  Mon Jul 27 16:39:34 2015
From: spider at smoothnet.org (The_Spider)
Date: Mon, 27 Jul 2015 11:39:34 -0500
Subject: [squid-users] Investigating squid crash.
In-Reply-To: <CANLNtGT6oBZtLhDdK_d=gDJbWMnVwzFBMgUfv6K-ZmHyyg95OA@mail.gmail.com>
References: <CAJfE2f6PYbg=b-_zcX9586kRJ0qTRr595ah5z5eCjh3f-ywQ0Q@mail.gmail.com>
 <55B548C9.4070103@treenet.co.nz>
 <CANLNtGT6oBZtLhDdK_d=gDJbWMnVwzFBMgUfv6K-ZmHyyg95OA@mail.gmail.com>
Message-ID: <CAJfE2f7ZZ-Pb51uDKV-2A3h_8K+iYVy28hnhp-pt6fOjGnrAOA@mail.gmail.com>

The version numbering you have identified seems to coincide with my issues
as well. I moved to the newer versions of squid for the newer enhanced ssl
bumping features but ran into this issue. I was hoping my stacktrace would
finally allow the squid developers to identify and resolve the issue. What
your describing is an improper shutdown procedure. I?m using a custom
written system script to handle my squid process, and that may very well be
the issue. But I have also experienced the random crash issue on
configurations where no cache was configured.


*** Begin /etc/systemd/system/squid.service ***

[Unit]

Description=Squid caching proxy

After=syslog.target network.target nss-lookup.target


[Service]

Type=forking

EnvironmentFile=/etc/sysconfig/squid

ExecStart=/usr/sbin/squid $SQUID_OPTS -f $SQUID_CONF

ExecReload=/usr/sbin/squid $SQUID_OPTS -k reconfigure -f $SQUID_CONF

ExecStop=/usr/sbin/squid $SQUID_OPTS -k shutdown -f $SQUID_CONF

PIDFile=/var/run/squid.pid


[Install]

WantedBy=multi-user.target

*** End /etc/systemd/system/squid.service ***



*From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
Behalf Of *Stanford Prescott
*Sent:* Monday, July 27, 2015 9:44 AM
*To:* Amos Jeffries <squid3 at treenet.co.nz>
*Cc:* squid-users <squid-users at lists.squid-cache.org>
*Subject:* Re: [squid-users] Investigating squid crash.



The developers of Smoothwall Express v3.1 have been trying to address this
issue for a few days now. We have had users complaining of this same issue
with Squid 3.5.5 and 3.5.6. It didn't seem to happen with prior versions.
We (or at least our lead developer Neal Murphy) thinks it is related to
shutting down Squid with a single SIGTERM followed by a SIGKILL a few
seconds later (or just using squid -k shutdown) is causing squid to close
and exit before the swap.state file is written and saved. This causes a
corrupted swap.state file to exist when squid is restarted. When Squid
crashes, rebuilding the swap.state anew seems to fix the problem and Squid
can restart.

So, to fix the crashing problem, for us at least, seems to involve
redesigning the shutdown and restarting of squid. Neal has come up with a
process and scripts that seem to fix the issue, at least for low loads on
squid that typically are what our users use. Neal has run some tests on
higher loads and it seems to work as well for those environments as well,
but we are continuing to test. Here is a link to the discussion in the
Smoothwall community forums if anyone is interested.

http://community.smoothwall.org/forum/viewtopic.php?p=340353#p340353



On Sun, Jul 26, 2015 at 3:53 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

On 26/07/2015 5:09 p.m., The_Spider wrote:
> To anyone willing to assist.
>
> My Squid continues to crash for some un-known reason only giving
> 'FATAL: Received Segment Violation...dying.' I have attached a copy of
> the cache log with the crash occurring and output debugging.
>
> Aparently I'm not the first to experience this, but I have yet to find
> any documentation on how to resolve this issue. This is not unique to
> one single machine, it happens on a few different boxes with similar
> configuration, I have not been able to identify what configuration
> parameter has been causing the issue.
>

Please follow the steps for obtaining a traceback as outlined at
<http://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps>

For SEGFAULT errors that is the only easy way to identify what the
problem is. Trial-and-error can take a very, very long time.


As for the config. You certainly have a thing for regex. Most if not all
of the dstdom_regex patterns would be far better written as dstdomain ACLs.

Although note that neither dstdomain nor dstdom_regex work properly in
the ssl_bump directive lines. Use "ssl::server_name" ACL for those checks.



> I will attempt to provide any information possible to help resolve
> this issue. Hopefully this is a good start.
>
> cache.log = http://filebin.ca/29vS8jkEfskt/cache.log
>
> squid -v:
> Squid Cache: Version 3.5.6-20150725-r13869
> Service Name: squid
> configure options:  '--prefix=/usr' '--exec-prefix=/usr'
> '--includedir=/usr/include' '--datadir=/usr/share'
> '--libdir=/usr/lib64' '--libexecdir=/usr/lib64/squid'
> '--localstatedir=/var' '--sysconfdir=/etc/squid'
> '--sharedstatedir=/var/lib' '--with-logdir=/var/log/squid'
> '--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
> '--enable-silent-rules' '--enable-dependency-tracking'
> '--with-openssl' '--enable-icmp' '--enable-delay-pools'
> '--enable-useragent-log'

--enable-useragent-log no longer exists.

> '--enable-esi'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> --enable-ltdl-convenience
>
> cat /etc/redhat-release
> CentOS Linux release 7.1.1503 (Core)
>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150727/fbfbdbef/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jul 27 16:56:41 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 27 Jul 2015 17:56:41 +0100
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
 <201507271708.17324.Antony.Stone@squid.open.source.it>
 <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>
Message-ID: <201507271756.41564.Antony.Stone@squid.open.source.it>

On Monday 27 Jul 2015 at 17:21, Berkes, David wrote:

> Here is the information requested.  From the log, everything looks to be
> normal.  The log example is from the cdn0.vox-cdn.com traffic.
> 
> **** ORIGIN URL's
> pixel.adsafeprotected.com
> cdn0.vox-cdn.com
> sb.scorecardresearch.com
> 
> **** SQUID LOG
> access.log.2:1437683164.693      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 29114
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 72579
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1582 70.197.241.219 TCP_TUNNEL/200 39476
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1583 70.197.241.219 TCP_TUNNEL/200 5909
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683167.244   2354 70.197.241.219 TCP_TUNNEL/200 59238
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683167.244   2362 70.197.241.219 TCP_TUNNEL/200 75369
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -

The first obvious thing that stands out to me from this is that these are 
HTTPS connections, not HTTP, so I'm going to let someone more familiar with 
Squid's current handling of HTTPS pass further comment, except for my question 
further down...

> **** CONFIG
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth
> /etc/squid/squid_passwd auth_param basic children 20
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 8 hours
> auth_param basic casesensitive on
> 
> acl whitelist1 dstdomain pipergo.pjc.com .apple.com .yahoo.com .wp.com
> acl whitelist2 dstdom_regex (^|\.)*img\.com$
> acl ncsa_users proxy_auth REQUIRED
> 
> http_access allow whitelist1
> http_access allow whitelist2
> http_access allow ncsa_users
> http_access deny all
> 
> cache_mem 4096 MB
> memory_cache_mode always
> refresh_pattern . 1440 100% 525949 ignore-auth
> cache_dir aufs /squid/cache 40000 128 512
> maximum_object_size 200 MB
> maximum_object_size_in_memory 2 MB
> cache_swap_low 90
> cache_swap_high 95
> buffered_logs on
> 
> #
> half_closed_clients off
> memory_pools off
> 
> # DNS-record cache
> ipcache_size 10240
> ipcache_low 90
> ipcache_high 95
> negative_dns_ttl 5 minutes
> 
> # listening port
> http_port 3128

When the unexpected authentication dialog appears:

1. what page do you see if you fail to authenticate correctly - is it from the 
origin server cdn0.vox-cdn.com (in this case) or is it the page your users 
would see if they failed to correctly authenticate to squid in the first 
place?

2. can you authenticate, and get the expected page from the origin server, by 
using the user's Squid credentials?


Regards,


Antony.

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From David.J.Berkes at pjc.com  Mon Jul 27 17:09:29 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Mon, 27 Jul 2015 17:09:29 +0000
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <201507271756.41564.Antony.Stone@squid.open.source.it>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
 <201507271708.17324.Antony.Stone@squid.open.source.it>
 <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>
 <201507271756.41564.Antony.Stone@squid.open.source.it>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E53F632@ESCML200.corp.pjc.com>

Thanks.  Here is my feedback.

1. what page do you see if you fail to authenticate correctly - is it from the origin server cdn0.vox-cdn.com (in this case) or is it the page your users would see if they failed to correctly authenticate to squid in the first place?

> When the origin server prompts for credentials, I hit "cancel" or "login" without credentials and it continues to work and load the page.  I don?t get any other prompts from either action.  I don?t get an authentication pop-up from squid and/or a message from the squid server.

2. can you authenticate, and get the expected page from the origin server, by using the user's Squid credentials?

> I have not tried that because if I hit cancel or login, the authentication pop-up goes away and the page loads..  I know these origin servers are not secured with authentication and can test that from my non-proxy browser.  I have all my companies iPhones set to use the squid proxy, but this has been causing a lot of user grief as you can expect.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Monday, July 27, 2015 11:57 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] random forward proxy authentication pop-up

On Monday 27 Jul 2015 at 17:21, Berkes, David wrote:

> Here is the information requested.  From the log, everything looks to
> be normal.  The log example is from the cdn0.vox-cdn.com traffic.
>
> **** ORIGIN URL's
> pixel.adsafeprotected.com
> cdn0.vox-cdn.com
> sb.scorecardresearch.com
>
> **** SQUID LOG
> access.log.2:1437683164.693      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.815      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683164.816      0 70.197.241.219 TCP_DENIED/407 4213
> CONNECT cdn0.vox-cdn.com:443 - HIER_NONE/- text/html
> access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 29114
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1590 70.197.241.219 TCP_TUNNEL/200 72579
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1582 70.197.241.219 TCP_TUNNEL/200 39476
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683166.464   1583 70.197.241.219 TCP_TUNNEL/200 5909
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683167.244   2354 70.197.241.219 TCP_TUNNEL/200 59238
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -
> access.log.2:1437683167.244   2362 70.197.241.219 TCP_TUNNEL/200 75369
> CONNECT cdn0.vox-cdn.com:443 proxyid HIER_DIRECT/54.192.120.85 -

The first obvious thing that stands out to me from this is that these are HTTPS connections, not HTTP, so I'm going to let someone more familiar with Squid's current handling of HTTPS pass further comment, except for my question further down...

> **** CONFIG
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth
> /etc/squid/squid_passwd auth_param basic children 20 auth_param basic
> realm Squid proxy-caching web server auth_param basic credentialsttl 8
> hours auth_param basic casesensitive on
>
> acl whitelist1 dstdomain pipergo.pjc.com .apple.com .yahoo.com .wp.com
> acl whitelist2 dstdom_regex (^|\.)*img\.com$ acl ncsa_users proxy_auth
> REQUIRED
>
> http_access allow whitelist1
> http_access allow whitelist2
> http_access allow ncsa_users
> http_access deny all
>
> cache_mem 4096 MB
> memory_cache_mode always
> refresh_pattern . 1440 100% 525949 ignore-auth cache_dir aufs
> /squid/cache 40000 128 512 maximum_object_size 200 MB
> maximum_object_size_in_memory 2 MB cache_swap_low 90 cache_swap_high
> 95 buffered_logs on
>
> #
> half_closed_clients off
> memory_pools off
>
> # DNS-record cache
> ipcache_size 10240
> ipcache_low 90
> ipcache_high 95
> negative_dns_ttl 5 minutes
>
> # listening port
> http_port 3128

When the unexpected authentication dialog appears:

1. what page do you see if you fail to authenticate correctly - is it from the origin server cdn0.vox-cdn.com (in this case) or is it the page your users would see if they failed to correctly authenticate to squid in the first place?

2. can you authenticate, and get the expected page from the origin server, by using the user's Squid credentials?


Regards,


Antony.

--
You can tell that the day just isn't going right when you find yourself using
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From hack.back at hotmail.com  Mon Jul 27 17:45:16 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 27 Jul 2015 10:45:16 -0700 (PDT)
Subject: [squid-users] useragent.log
Message-ID: <1438019116110-4672505.post@n4.nabble.com>

how i can use useragent log in 3.5.6
while no user agent log in access.log like it mentioned in 
http://wiki.squid-cache.org/SquidFaq/SquidLogs

thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/useragent-log-tp4672505.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jul 27 17:55:19 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 27 Jul 2015 18:55:19 +0100
Subject: [squid-users] useragent.log
In-Reply-To: <1438019116110-4672505.post@n4.nabble.com>
References: <1438019116110-4672505.post@n4.nabble.com>
Message-ID: <201507271855.19770.Antony.Stone@squid.open.source.it>

On Monday 27 Jul 2015 at 18:45, HackXBack wrote:

> how i can use useragent log in 3.5.6
> while no user agent log in access.log like it mentioned in
> http://wiki.squid-cache.org/SquidFaq/SquidLogs

As the page says "this log has become one of the default access.log formats 
and is always available for use"

http://www.squid-cache.org/Doc/config/logformat/ (at the end) gives further 
details:

logformat useragent  %>a [%tl] "%{User-Agent}>h"

Regards,


Antony.

-- 
Most people are aware that the Universe is big.

 - Paul Davies, Professor of Theoretical Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From mcsnv96 at afo.net  Mon Jul 27 20:38:31 2015
From: mcsnv96 at afo.net (Mike)
Date: Mon, 27 Jul 2015 15:38:31 -0500
Subject: [squid-users] dns failover failing with 3.4.7
Message-ID: <55B696C7.2050704@afo.net>

Running into an issue, using the squid.conf entry
dns_nameservers 72.x.x.x 72.x.y.y

These are different servers (under our control) for the purpose of 
filtering than listed in resolv.conf (which are out of our control, used 
for server IP routing by upstream host).

The problem we found like this weekend is if the primary listed dns 
server is unavailable, squid fails to use the secondary listed server. 
Instead it displays the "unable to connect" type messages with all 
websites.

How do we fix this so if primary fails it goes to secondary (and 
possibly tertiary)?

Thanks in advance
Mike



From hack.back at hotmail.com  Mon Jul 27 21:06:49 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 27 Jul 2015 14:06:49 -0700 (PDT)
Subject: [squid-users] useragent.log
In-Reply-To: <201507271855.19770.Antony.Stone@squid.open.source.it>
References: <1438019116110-4672505.post@n4.nabble.com>
 <201507271855.19770.Antony.Stone@squid.open.source.it>
Message-ID: <1438031209371-4672508.post@n4.nabble.com>

this log format didnt work and no thing about useragent in access.log ....



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/useragent-log-tp4672505p4672508.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jul 27 21:21:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Jul 2015 09:21:38 +1200
Subject: [squid-users] random forward proxy authentication pop-up
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E53F632@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E53F5BF@ESCML200.corp.pjc.com>
 <201507271708.17324.Antony.Stone@squid.open.source.it>
 <916606669CFF224AB6997E9DB783F6EA7E53F5E6@ESCML200.corp.pjc.com>
 <201507271756.41564.Antony.Stone@squid.open.source.it>
 <916606669CFF224AB6997E9DB783F6EA7E53F632@ESCML200.corp.pjc.com>
Message-ID: <55B6A0E2.8080205@treenet.co.nz>

iPhones and mysterious auth popups on CONNECT requests where the cancel
continues as if nothing happened.

Sounds very familiar.

Each time so far that I've been through and debugged these symptoms in
detail it has turned out Squid was working correctly. The 407 were
correct normal HTTP responses to which the browser had only to send the
credentials it has. But for some reason the browser was still annoying
the user.

So far the best theory is a iPhone bug somewhere in the deep HTTP
handling layers close to TCP. On the iPhone itself, not Squid so we
can't fix it.


It is weird that it should be the origin server listed as auth realm
though. Try configuring something very distinct like a random word. The
Squid realm text should appear in the popup boxes title bar or
description when credentials are being requested by Squid:

   auth_param basic realm Squid ghAIec54j

If that test shows the origin really is requiring authentication - then
the problem has nothing to do with Squid.

Amos



From squid3 at treenet.co.nz  Mon Jul 27 21:56:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Jul 2015 09:56:14 +1200
Subject: [squid-users] useragent.log
In-Reply-To: <1438031209371-4672508.post@n4.nabble.com>
References: <1438019116110-4672505.post@n4.nabble.com>
 <201507271855.19770.Antony.Stone@squid.open.source.it>
 <1438031209371-4672508.post@n4.nabble.com>
Message-ID: <55B6A8FE.5010609@treenet.co.nz>

On 28/07/2015 9:06 a.m., HackXBack wrote:
> this log format didnt work and no thing about useragent in access.log ....
> 

Works for me...

squid.conf:
  access_log stdio:/var/logs/UA.log useragent


# squidclient http://example.com/ >/dev/null
# tail /var/logs/UA.log

::1 [27/Jul/2015:14:53:10 -0700] "squidclient/3.5.6"


Amos



From squid3 at treenet.co.nz  Mon Jul 27 22:25:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 28 Jul 2015 10:25:56 +1200
Subject: [squid-users] dns failover failing with 3.4.7
In-Reply-To: <55B696C7.2050704@afo.net>
References: <55B696C7.2050704@afo.net>
Message-ID: <55B6AFF4.3090407@treenet.co.nz>

On 28/07/2015 8:38 a.m., Mike wrote:
> Running into an issue, using the squid.conf entry
> dns_nameservers 72.x.x.x 72.x.y.y
> 
> These are different servers (under our control) for the purpose of
> filtering than listed in resolv.conf (which are out of our control, used
> for server IP routing by upstream host).
> 
> The problem we found like this weekend is if the primary listed dns
> server is unavailable, squid fails to use the secondary listed server.
> Instead it displays the "unable to connect" type messages with all
> websites.

Details please. How do you know the secondary is not even being tried?

What is Squid getting back from the primary when its "down" ?
 or just dns_timeout being hit?

Add this to squid.conf to get a cache.log trace of the DNS activity:
  debug_options 78,6


Amos



From marko.cupac at mimar.rs  Tue Jul 28 16:01:02 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Tue, 28 Jul 2015 18:01:02 +0200
Subject: [squid-users] please help me test ext_ldap_group_acl from command
	line
Message-ID: <20150728180102.16910420@efreet>

Hi,

I am testing ext_ldap_group_acl from command line in squid-3.5.6 on
FreeBSD 10.1-RELEASE-p15 amd64, but I can't make it work with Active
Directory.

My query is as follows:
./ext_ldap_group_acl -d -b "DC=mimar,DC=rs" \
	-f "CN=squid_noaccess" -d ldapbinder at mimar.rs -W "mypass" \
	-h dc1.mimar.rs

After I type user and group name I get:
pacija squid_noaccess
ext_ldap_group_acl.cc(579): pid=1550 :Connected OK
ext_ldap_group_acl.cc(718): pid=1550 :group filter 'CN=squid_noaccess', searchbase 'DC=mimar,DC=rs'
ext_ldap_group_acl: WARNING: LDAP search error 'Operations error'
ERR

If I understand well, if user pacija is a member of squid_noaccess
group, correctly construed query should give me OK. How do I achieve
this?

I am already using this AD for various other queries, such as
extracting valid recipients for postfix, apache authentication etc.

Thank you in advance,
-- 
Marko Cupa?
https://www.mimar.rs/


From tmblue at gmail.com  Tue Jul 28 17:53:12 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 28 Jul 2015 10:53:12 -0700
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
Message-ID: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>

squid-3.5.5-1.el6.x86_64

CentOS 6.6

 This looks like a bug in Squid v3 or a difference from 2.7.  Our monitor
couldn't be simpler.  It requests the SAME URL twice (identical in every
way, same hostname too), and expects the 2nd response to contain the
X-Squid hit header.  If it does not, then Squid has some sort of race
condition going on its code.

 I just reproduced this by hand, using an HTTP sniffer tool.  I requested
the same URL twice, with about a 0.25 second delay between fetches, and the
2nd attempt was ALSO A MISS.  Then I waited 1 second and tried a 3rd time,
and it was FINALLY a hit.

Squid v3 seems to have changed the way it stores objects.  Maybe it is
doing some sort of "asynchronous" background store now, so if you send in
sequential requests without a delay between, it may not actually have
finished storing it yet, so it doesn't report a "hit".  Meaning, the first
"miss" response may have fired off a thread to store the object, and not
doing it in the main thread anymore like in v2, if you get my meaning.

For the Squid dev team, here are the headers we are sending back from the
origin App VIP:


Accept-Ranges: none

Access-Control-Allow-Origin: *

*Cache-Control: max-age=300*

Connection: close

Content-Length: 403

Content-Type: image/jpeg

Date: Tue, 28 Jul 2015 17:25:36 GMT

Expires: Tue, 28 Jul 2015 17:30:36 GMT

Last-Modified: Mon, 11 Jun 2012 04:25:18 GMT

Server: Apache/2.2.26 (Unix)


This should very much be cached right away and it's a simple tiny image.


One thing we also notice is this only occurs doing load, meaning when we
have production load traffic this fails, but if there is no other
connections to the box, no other queries, this does not fail. Is it
possible that squid is ejecting this that fast or is there another
possibility here? Not sure what other data I can provide but will if asked.


Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150728/e311d9de/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 28 17:56:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 05:56:07 +1200
Subject: [squid-users] please help me test ext_ldap_group_acl from
 command line
In-Reply-To: <20150728180102.16910420@efreet>
References: <20150728180102.16910420@efreet>
Message-ID: <55B7C237.1040009@treenet.co.nz>

On 29/07/2015 4:01 a.m., Marko Cupa? wrote:
> Hi,
> 
> I am testing ext_ldap_group_acl from command line in squid-3.5.6 on
> FreeBSD 10.1-RELEASE-p15 amd64, but I can't make it work with Active
> Directory.
> 
> My query is as follows:
> ./ext_ldap_group_acl -d -b "DC=mimar,DC=rs" \
> 	-f "CN=squid_noaccess" -d ldapbinder at mimar.rs -W "mypass" \
> 	-h dc1.mimar.rs
> 
> After I type user and group name I get:
> pacija squid_noaccess
> ext_ldap_group_acl.cc(579): pid=1550 :Connected OK
> ext_ldap_group_acl.cc(718): pid=1550 :group filter 'CN=squid_noaccess', searchbase 'DC=mimar,DC=rs'
> ext_ldap_group_acl: WARNING: LDAP search error 'Operations error'
> ERR
> 
> If I understand well, if user pacija is a member of squid_noaccess
> group, correctly construed query should give me OK. How do I achieve
> this?

Start by typing in the input using external ACL helpers input format.
I assume your squid.conf uses %LOGIN. Which is actually user:password

Notice the colon.

Follow that by running the helper as Squid low-privileged user account.
There's no gain testing that admin account can access things. You want
it working when run by Squid.

Amos



From vdoctor at neuf.fr  Tue Jul 28 19:07:21 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 28 Jul 2015 12:07:21 -0700 (PDT)
Subject: [squid-users] 2015/07/28 22:04:49 kid1| assertion failed:
 filemap.cc:50: "capacity_ <= (1 << 24)"
Message-ID: <1438110441865-4672516.post@n4.nabble.com>

Hi All,

Squid 3.5.6 in AUFS.
Any idea why this error happens ?
/2015/07/28 22:04:49 kid1| assertion failed: filemap.cc:50: "capacity_ <= (1
<< 24)"/

Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/2015-07-28-22-04-49-kid1-assertion-failed-filemap-cc-50-capacity-1-24-tp4672516.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jul 28 19:54:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 07:54:24 +1200
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
In-Reply-To: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
References: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
Message-ID: <55B7DDF0.3070006@treenet.co.nz>

On 29/07/2015 5:53 a.m., Tory M Blue wrote:
> squid-3.5.5-1.el6.x86_64
> 
> CentOS 6.6
> 
>  This looks like a bug in Squid v3 or a difference from 2.7.  Our monitor
> couldn't be simpler.  It requests the SAME URL twice (identical in every
> way, same hostname too), and expects the 2nd response to contain the
> X-Squid hit header.  If it does not, then Squid has some sort of race
> condition going on its code.
> 

No HTTP does not work that way, and Squid certainly does not.

Squid is event driven and Squid-3 is also asynchronously interleaving
processing of those events sub-steps where Squid-2 was just using a huge
stack / call chain per-event. When operating under load there is always
a slight delay between asynchronous operations being scheduled, and
being run.

You may as well sends a request to two completely separate pieces of
hardware and try to draw a conclusion based on one of those being a HIT.


In Squid *every* transaction processed is racing against everything else
that needs to happen *all* of the time.

A code "race condition" under those circumstances means that the steps
of a *single* transaction are running against each other. Or two things
that should not interact are clobbering each others state data. Anything
else Squid may be doing in parallel is irrelvant.


HTTP itself is stateless, always has been stateless. Any stateful
interaction between *different* transactions has always been an
illusion. Caching brings that illusion a bit closer to solidity, but its
still an illusion.

For example your "fail" result does not distinguish between the second
"MISS" being a cache near-HIT, a full MISS, or a revalidation MISS.

HTTP/1.1 can do some funky stuff sometimes. Picture a
Cache-Control:no-cache header causing replies with auth credentials
embeded to be stored for later HITs by other users. Yes thats right, and
its actually one of the desirable features - we just have to fetch new
headers from the server to attach to the object on later HIT. (Thats a
near-HIT BTW).


>  I just reproduced this by hand, using an HTTP sniffer tool.  I requested
> the same URL twice, with about a 0.25 second delay between fetches, and the
> 2nd attempt was ALSO A MISS.  Then I waited 1 second and tried a 3rd time,
> and it was FINALLY a hit.

Had the first request finished being delivered to your testing tool
before the second request was sent?

If the answer is no, then this test itself in invalid. Because of the
next thing ...

> 
> Squid v3 seems to have changed the way it stores objects.  Maybe it is
> doing some sort of "asynchronous" background store now, so if you send in
> sequential requests without a delay between, it may not actually have
> finished storing it yet, so it doesn't report a "hit".  Meaning, the first
> "miss" response may have fired off a thread to store the object, and not
> doing it in the main thread anymore like in v2, if you get my meaning.

Squid-3 in-transit objects are not listed as existing in cache_mem
storage until they have completely (and successfully) finished their
first use.


You can configure "collapsed_forwarding on" to enable the Squid-2.7
behaviour. Treating the in-transit objects list as if it were another
cache area.

But be aware that speed of the first client reading the object is
applied, even if later clients require faster delivery. The way HTTP/1.1
variant objects work also means some clients may have added lag waiting
for an object they discover to be unusable, so then have to go fetch
their correct one - resulting in 2x normal MISS latency.
 These are not new problems though 2.7 had them in its own way.


> 
> For the Squid dev team, here are the headers we are sending back from the
> origin App VIP:
> 
> 
> Accept-Ranges: none
> 
> Access-Control-Allow-Origin: *
> 
> *Cache-Control: max-age=300*
> 
> Connection: close
> 
> Content-Length: 403
> 
> Content-Type: image/jpeg
> 
> Date: Tue, 28 Jul 2015 17:25:36 GMT
> 
> Expires: Tue, 28 Jul 2015 17:30:36 GMT
> 
> Last-Modified: Mon, 11 Jun 2012 04:25:18 GMT
> 
> Server: Apache/2.2.26 (Unix)
> 
> 
> This should very much be cached right away and it's a simple tiny image.
> 
> 
> One thing we also notice is this only occurs doing load, meaning when we
> have production load traffic this fails, but if there is no other
> connections to the box, no other queries, this does not fail. Is it
> possible that squid is ejecting this that fast or is there another
> possibility here? Not sure what other data I can provide but will if asked.
> 


Timing between *completion* of the first request and starting of the
second is the critical. If the first request has not finished, theres no
hope of a HIT.

Also size of memory cache relative to the churn currently going on also
matters. If you wait too long between the test requests it will be
pushed out and get a MISS again anyway. But I dont think that is a
factor with 250ms being your timing.


Amos


From squid3 at treenet.co.nz  Tue Jul 28 20:32:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 08:32:44 +1200
Subject: [squid-users] 2015/07/28 22:04:49 kid1| assertion failed:
 filemap.cc:50: "capacity_ <= (1 << 24)"
In-Reply-To: <1438110441865-4672516.post@n4.nabble.com>
References: <1438110441865-4672516.post@n4.nabble.com>
Message-ID: <55B7E6EC.3090707@treenet.co.nz>

On 29/07/2015 7:07 a.m., Stakres wrote:
> Hi All,
> 
> Squid 3.5.6 in AUFS.
> Any idea why this error happens ?
> /2015/07/28 22:04:49 kid1| assertion failed: filemap.cc:50: "capacity_ <= (1
> << 24)"/
> 

That is <http://bugs.squid-cache.org/show_bug.cgi?id=3566>

Amos



From squid3 at treenet.co.nz  Tue Jul 28 20:55:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 08:55:30 +1200
Subject: [squid-users] please help me test ext_ldap_group_acl from
 command line
In-Reply-To: <55B7C237.1040009@treenet.co.nz>
References: <20150728180102.16910420@efreet> <55B7C237.1040009@treenet.co.nz>
Message-ID: <55B7EC42.5020007@treenet.co.nz>

On 29/07/2015 5:56 a.m., Amos Jeffries wrote:
> On 29/07/2015 4:01 a.m., Marko Cupa? wrote:
>> Hi,
>>
>> I am testing ext_ldap_group_acl from command line in squid-3.5.6 on
>> FreeBSD 10.1-RELEASE-p15 amd64, but I can't make it work with Active
>> Directory.
>>
>> My query is as follows:
>> ./ext_ldap_group_acl -d -b "DC=mimar,DC=rs" \
>> 	-f "CN=squid_noaccess" -d ldapbinder at mimar.rs -W "mypass" \
>> 	-h dc1.mimar.rs
>>
>> After I type user and group name I get:
>> pacija squid_noaccess
>> ext_ldap_group_acl.cc(579): pid=1550 :Connected OK
>> ext_ldap_group_acl.cc(718): pid=1550 :group filter 'CN=squid_noaccess', searchbase 'DC=mimar,DC=rs'
>> ext_ldap_group_acl: WARNING: LDAP search error 'Operations error'
>> ERR
>>
>> If I understand well, if user pacija is a member of squid_noaccess
>> group, correctly construed query should give me OK. How do I achieve
>> this?
> 
> Start by typing in the input using external ACL helpers input format.
> I assume your squid.conf uses %LOGIN. Which is actually user:password
> 
> Notice the colon.

Oops. Sorry, looked in the wrong formatter. It is just username like you
had.

But no group name unless the group is explicitly named in the 'acl ...
external ...' line parameters.


This bit still applies though:

> 
> Follow that by running the helper as Squid low-privileged user account.
> There's no gain testing that admin account can access things. You want
> it working when run by Squid.


And maybe alter the -f parameter value to tell it where to find the %u
(username).

Amos



From vdoctor at neuf.fr  Tue Jul 28 20:55:25 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 28 Jul 2015 13:55:25 -0700 (PDT)
Subject: [squid-users] 2015/07/28 22:04:49 kid1| assertion failed:
 filemap.cc:50: "capacity_ <= (1 << 24)"
In-Reply-To: <55B7E6EC.3090707@treenet.co.nz>
References: <1438110441865-4672516.post@n4.nabble.com>
 <55B7E6EC.3090707@treenet.co.nz>
Message-ID: <1438116925512-4672520.post@n4.nabble.com>

Hi Amos,

/cache_dir aufs /cachesde/spool1 1560132 16 256 min-size=0 max-size=32768/

Will this bug be fixed in a near future, or do we have to increase the
max-size to 128kb or more ?

Fred





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/2015-07-28-22-04-49-kid1-assertion-failed-filemap-cc-50-capacity-1-24-tp4672516p4672520.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tmblue at gmail.com  Tue Jul 28 22:43:53 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 28 Jul 2015 15:43:53 -0700
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
In-Reply-To: <55B7DDF0.3070006@treenet.co.nz>
References: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
 <55B7DDF0.3070006@treenet.co.nz>
Message-ID: <CAEaSS0ZZiqyGzSoX_mysk=-VRq5aagbokni7cvPSF6VYxWfjEg@mail.gmail.com>

On Tue, Jul 28, 2015 at 12:54 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 29/07/2015 5:53 a.m., Tory M Blue wrote:
>
>
> >  I just reproduced this by hand, using an HTTP sniffer tool.  I requested
> > the same URL twice, with about a 0.25 second delay between fetches, and
> the
> > 2nd attempt was ALSO A MISS.  Then I waited 1 second and tried a 3rd
> time,
> > and it was FINALLY a hit.
>
> Had the first request finished being delivered to your testing tool
> before the second request was sent?
>


Yes our testing is completely sequential meaning the connection has to
complete before we launch another, even by hand.



>
> > Squid v3 seems to have changed the way it stores objects.  Maybe it is
> > doing some sort of "asynchronous" background store now, so if you send in
> > sequential requests without a delay between, it may not actually have
> > finished storing it yet, so it doesn't report a "hit".  Meaning, the
> first
> > "miss" response may have fired off a thread to store the object, and not
> > doing it in the main thread anymore like in v2, if you get my meaning.
>
> Squid-3 in-transit objects are not listed as existing in cache_mem
> storage until they have completely (and successfully) finished their
> first use.
>
>
> You can configure "collapsed_forwarding on" to enable the Squid-2.7
> behaviour. Treating the in-transit objects list as if it were another
> cache area.
>
>
> The document cite this is quite a bit different and not something I want
to embark upon.

>
> >
>
>
> Timing between *completion* of the first request and starting of the
> second is the critical. If the first request has not finished, theres no
> hope of a HIT.
>
> Also size of memory cache relative to the churn currently going on also
> matters. If you wait too long between the test requests it will be
> pushed out and get a MISS again anyway. But I dont think that is a
> factor with 250ms being your timing.
>


Again completely synchronous, something is not happening as it should under
a busy condition, we tested this as far out as 5 seconds and still got a
miss.  We have added "5" retries to our monitor, but I do think something
is  a miss (no pun intended).  Single client hitting a single server in a
synchronous manner, should allow for a pretty quick cache hit, but it's
not.

my high/low water marks are so

cache_swap_high 90

cache_swap_low 80


My space available is so

Filesystem            Size  Used Avail Use% Mounted on

/dev/ram0              17G   14G  2.3G  86% /cache01

Any other data I could grab from the stats that would help. I do believe
something is not quite right but we have ways to get around it for now,


Thanks Amons

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150728/7e12c474/attachment.htm>

From spider at smoothnet.org  Tue Jul 28 23:07:19 2015
From: spider at smoothnet.org (The_Spider)
Date: Tue, 28 Jul 2015 18:07:19 -0500
Subject: [squid-users]  Investigating squid crash.
Message-ID: <CAJfE2f4dDPqPh_d6QdgF_Wgg6Mmr7JfCdWFxx-KuZEORGdQ+7w@mail.gmail.com>

Will there be any investigation to the backtrace I have posted as requested?

> Attached is the backtrace from the gdb debugger. Again, if there is any thing I can do to assist further, please ask.
>
> #0  0x00007ffff53e05d7 in raise () from /lib64/libc.so.6
> #1  0x00007ffff53e1cc8 in abort () from /lib64/libc.so.6
> #2  0x00000000005770ef in xassert (msg=<optimized out>, file=<optimized out>, line=<optimized out>) at debug.cc:544
> #3  0x0000000000791758 in comm_read_base (conn=..., buf=buf at entry=0xa2b3a240 "", size=16383, callback=...) at Read.cc:69
> #4  0x0000000000648791 in comm_read (callback=..., len=<optimized out>, buf=0xa2b3a240 "", conn=...) at comm/Read.h:58
> #5  StoreEntry::delayAwareRead (this=this at entry=0xd805d0f0, conn=..., buf=0xa2b3a240 "", len=16383, callback=...) at store.cc:255
> #6  0x0000000000648b48 in StoreEntry::DeferReader (theContext=0xd805d0f0, aRead=...) at store.cc:214
> #7  0x00000000006fbb0f in DeferredReadManager::kickARead (this=<optimized out>, aRead=...) at comm.cc:1809
> #8  0x0000000000702231 in DeferredReadManager::flushReads (this=0xdf42a8) at comm.cc:1794
> #9  0x0000000000702569 in DeferredReadManager::kickReads (this=0xdf42a8, count=<optimized out>) at comm.cc:1769
> #10 0x0000000000577c07 in DelayPools::Update (unused=0x2d8a) at delay_pools.cc:571
> #11 0x00000000006f2479 in AsyncCall::make (this=0xd7d24580) at AsyncCall.cc:40
> #12 0x00000000006f6265 in AsyncCallQueue::fireNext (this=this at entry=0x27dc8b0) at AsyncCallQueue.cc:56
> #13 0x00000000006f6660 in AsyncCallQueue::fire (this=0x27dc8b0) at AsyncCallQueue.cc:42
> #14 0x0000000000595391 in dispatchCalls (this=0x7fffffffe350) at EventLoop.cc:143
> #15 EventLoop::runOnce (this=this at entry=0x7fffffffe350) at EventLoop.cc:108
> #16 0x0000000000595580 in EventLoop::run (this=this at entry=0x7fffffffe350) at EventLoop.cc:82
> #17 0x00000000005f9dac in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1518
> #18 0x00000000005031bb in SquidMainSafe (argv=<optimized out>, argc=<optimized out>) at main.cc:1250
> #19 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1243


From squid3 at treenet.co.nz  Wed Jul 29 00:50:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 12:50:05 +1200
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
In-Reply-To: <CAEaSS0ZZiqyGzSoX_mysk=-VRq5aagbokni7cvPSF6VYxWfjEg@mail.gmail.com>
References: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
 <55B7DDF0.3070006@treenet.co.nz>
 <CAEaSS0ZZiqyGzSoX_mysk=-VRq5aagbokni7cvPSF6VYxWfjEg@mail.gmail.com>
Message-ID: <55B8233D.1060906@treenet.co.nz>

On 29/07/2015 10:43 a.m., Tory M Blue wrote:
> On Tue, Jul 28, 2015 at 12:54 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
>> On 29/07/2015 5:53 a.m., Tory M Blue wrote:
>>
>>
>>>  I just reproduced this by hand, using an HTTP sniffer tool.  I requested
>>> the same URL twice, with about a 0.25 second delay between fetches, and
>> the
>>> 2nd attempt was ALSO A MISS.  Then I waited 1 second and tried a 3rd
>> time,
>>> and it was FINALLY a hit.
>>
>> Had the first request finished being delivered to your testing tool
>> before the second request was sent?
>>
> 
> 
> Yes our testing is completely sequential meaning the connection has to
> complete before we launch another, even by hand.
> 
> 
> 
>>
>>> Squid v3 seems to have changed the way it stores objects.  Maybe it is
>>> doing some sort of "asynchronous" background store now, so if you send in
>>> sequential requests without a delay between, it may not actually have
>>> finished storing it yet, so it doesn't report a "hit".  Meaning, the
>> first
>>> "miss" response may have fired off a thread to store the object, and not
>>> doing it in the main thread anymore like in v2, if you get my meaning.
>>
>> Squid-3 in-transit objects are not listed as existing in cache_mem
>> storage until they have completely (and successfully) finished their
>> first use.
>>
>>
>> You can configure "collapsed_forwarding on" to enable the Squid-2.7
>> behaviour. Treating the in-transit objects list as if it were another
>> cache area.
>>
>>
>> The document cite this is quite a bit different and not something I want
> to embark upon.
> 
>>
>>>
>>
>>
>> Timing between *completion* of the first request and starting of the
>> second is the critical. If the first request has not finished, theres no
>> hope of a HIT.
>>
>> Also size of memory cache relative to the churn currently going on also
>> matters. If you wait too long between the test requests it will be
>> pushed out and get a MISS again anyway. But I dont think that is a
>> factor with 250ms being your timing.
>>
> 
> 
> Again completely synchronous, something is not happening as it should under
> a busy condition, we tested this as far out as 5 seconds and still got a
> miss.  We have added "5" retries to our monitor, but I do think something
> is  a miss (no pun intended).  Single client hitting a single server in a
> synchronous manner, should allow for a pretty quick cache hit, but it's
> not.
> 
> my high/low water marks are so
> 
> cache_swap_high 90
> 
> cache_swap_low 80
> 
> 
> My space available is so
> 
> Filesystem            Size  Used Avail Use% Mounted on
> 
> /dev/ram0              17G   14G  2.3G  86% /cache01
> 
> Any other data I could grab from the stats that would help. I do believe
> something is not quite right but we have ways to get around it for now,
> 

That depends on how busy. A debug trace at level ALL,7 would be best.
But the amount of date logged can be a problem if Squid is running
several hundred or thousand req/sec.

That can be resolved somewhat by turning the logging on dynamically with
"squid -k debug" shortly before and after a test is run.

Amos



From squid3 at treenet.co.nz  Wed Jul 29 01:29:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 13:29:34 +1200
Subject: [squid-users] Investigating squid crash.
In-Reply-To: <CAJfE2f4dDPqPh_d6QdgF_Wgg6Mmr7JfCdWFxx-KuZEORGdQ+7w@mail.gmail.com>
References: <CAJfE2f4dDPqPh_d6QdgF_Wgg6Mmr7JfCdWFxx-KuZEORGdQ+7w@mail.gmail.com>
Message-ID: <55B82C7E.9040607@treenet.co.nz>

On 29/07/2015 11:07 a.m., The_Spider wrote:
> Will there be any investigation to the backtrace I have posted as requested?

Yes. I looked at it but did no have time to go into it in detail until now.

It is this:
  // if the assertion below fails, we have an active comm_read conflict
  assert(fd_table[conn->fd].halfClosedReader != NULL);

Which is <http://bugs.squid-cache.org/show_bug.cgi?id=3775> once one
accounts for code movement since 3.2.

You may note there was a patch for this applied in 3.5.4. That was an
experiment though, Christos was awaiting feedback on whether it resolved
the whole problem or just the bit he could replicate.
 It seems you have the answer - and its not a good one :-(

Amos



From spider at smoothnet.org  Wed Jul 29 02:55:32 2015
From: spider at smoothnet.org (The_Spider)
Date: Tue, 28 Jul 2015 21:55:32 -0500
Subject: [squid-users] Investigating squid crash.
In-Reply-To: <55B82C7E.9040607@treenet.co.nz>
References: <CAJfE2f4dDPqPh_d6QdgF_Wgg6Mmr7JfCdWFxx-KuZEORGdQ+7w@mail.gmail.com>
 <55B82C7E.9040607@treenet.co.nz>
Message-ID: <CAJfE2f544w5q5RteHYCM2ZuXUqCuNupOjO7ydkAVO-1jDft2BQ@mail.gmail.com>

Amos,
Thank you for investigating the issue and replying to my frequent
messages. I know this is a free project and I understand the trials
that developers must go through. Just knowing that the issue has been
confirmed is of great relief to me. Please let me know what I can do
to help get this resolved. I'll recompile as many times as you need!
Just know, it took more then 6 hours to get a crash last time. I'll be
looking forward to working with you as much as possible.

Best of luck,
The_Spider

On Tue, Jul 28, 2015 at 8:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 29/07/2015 11:07 a.m., The_Spider wrote:
>> Will there be any investigation to the backtrace I have posted as requested?
>
> Yes. I looked at it but did no have time to go into it in detail until now.
>
> It is this:
>   // if the assertion below fails, we have an active comm_read conflict
>   assert(fd_table[conn->fd].halfClosedReader != NULL);
>
> Which is <http://bugs.squid-cache.org/show_bug.cgi?id=3775> once one
> accounts for code movement since 3.2.
>
> You may note there was a patch for this applied in 3.5.4. That was an
> experiment though, Christos was awaiting feedback on whether it resolved
> the whole problem or just the bit he could replicate.
>  It seems you have the answer - and its not a good one :-(
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From J.Xu at uws.edu.au  Wed Jul 29 02:59:01 2015
From: J.Xu at uws.edu.au (Julie Xu)
Date: Wed, 29 Jul 2015 02:59:01 +0000
Subject: [squid-users] unsubcribe
Message-ID: <ABCD499AC5326B4F828DBB3933CFE87E8479D6E9@hall.AD.UWS.EDU.AU>



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150729/69cd439a/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 29 04:32:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Jul 2015 16:32:38 +1200
Subject: [squid-users] unsubcribe
In-Reply-To: <ABCD499AC5326B4F828DBB3933CFE87E8479D6E9@hall.AD.UWS.EDU.AU>
References: <ABCD499AC5326B4F828DBB3933CFE87E8479D6E9@hall.AD.UWS.EDU.AU>
Message-ID: <55B85766.6090308@treenet.co.nz>

On 29/07/2015 2:59 p.m., Julie Xu wrote:
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

To unsubscribe please use the form on the mailing list info page whose
URL is helpfully printed at the bottom of every single list message.

Cheers
Amos


From itpc.vivek at gmail.com  Wed Jul 29 07:07:23 2015
From: itpc.vivek at gmail.com (vivek singh)
Date: Wed, 29 Jul 2015 12:37:23 +0530
Subject: [squid-users] error : The description for Event ID 0 from source
	squid cannot be found.
Message-ID: <CACeaud=3YoODuRKOT+XBi4AWDsjqZHcLm4a+bwndy=YNEHiKYA@mail.gmail.com>

Please help me out of the below error

The description for Event ID 0 from source squid cannot be found. Either
the component that raises this event is not installed on your local
computer or the installation is corrupted. You can install or repair the
component on the local computer


Vivek Kumar Singh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150729/fb21f2af/attachment.htm>

From eliezer at ngtech.co.il  Wed Jul 29 09:16:31 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 29 Jul 2015 12:16:31 +0300
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
In-Reply-To: <55B8233D.1060906@treenet.co.nz>
References: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
 <55B7DDF0.3070006@treenet.co.nz>
 <CAEaSS0ZZiqyGzSoX_mysk=-VRq5aagbokni7cvPSF6VYxWfjEg@mail.gmail.com>
 <55B8233D.1060906@treenet.co.nz>
Message-ID: <55B899EF.4060802@ngtech.co.il>

Just wondering how new is this option?

Eliezer

On 29/07/2015 03:50, Amos Jeffries wrote:
> That can be resolved somewhat by turning the logging on dynamically with
> "squid -k debug" shortly before and after a test is run.
>
> Amos



From lameventanas at gmail.com  Wed Jul 29 14:12:14 2015
From: lameventanas at gmail.com (Alan)
Date: Wed, 29 Jul 2015 23:12:14 +0900
Subject: [squid-users] Squid 3.5.6 crash
Message-ID: <CAFKaCCQim3LJiUCK2YxpUKHz=c1TJkgSk+2HWK57v=k0vOW+YQ@mail.gmail.com>

This might be the same crash reported in another thread, or a new one.
I noticed it after upgrading from 3.3.11 to 3.5.6.

I'm using negotiate authentication (kerberos).

Auth settings:
auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth
auth_param negotiate children 80 startup=5 idle=5
auth_param negotiate keep_alive on

Logging setting:
debug_options ALL,1 29,3

Syslog:
Squid Parent: (squid-1) process 24158 exited due to signal 6 with status 0

cache.log:
2015/07/29 19:32:09.728 kid1| User.cc(305) addIp: user 'LinArlene at TUV.GROUP'
has been seen at a new IP address (10.144.138.48:58283)
2015/07/29 19:32:11.677 kid1| User.cc(185) cacheCleanup: Cleaning the user
cache now
2015/07/29 19:32:11.677 kid1| User.cc(186) cacheCleanup: Current time:
1438169531
2015/07/29 19:32:23 kid1| Set Current Directory to /var/cache/squid
2015/07/29 19:32:23 kid1| Starting Squid Cache version 3.5.6 for
i686-pc-linux-gnu...


Backtrace:

Core was generated by `(squid-1)'.
Program terminated with signal 6, Aborted.
#0  0xb7792424 in __kernel_vsyscall ()
(gdb) bt
#0  0xb7792424 in __kernel_vsyscall ()
#1  0xb7372bf1 in raise () from /lib/libc.so.6
#2  0xb73743ce in abort () from /lib/libc.so.6
#3  0xb736b798 in __assert_fail () from /lib/libc.so.6
#4  0x08460627 in hash_remove_link (hid=0x872f098, hl=0xe739580) at
hash.cc:240
#5  0x0831dc4a in Auth::User::cacheCleanup (datanotused=0x0) at User.cc:208
#6  0x08226795 in EventDialer::dial (this=0x14f535dc) at event.cc:41
#7  0x08226a2d in AsyncCallT<EventDialer>::fire (this=0x14f535c0) at
../src/base/AsyncCall.h:145
#8  0x0835fa28 in AsyncCall::make (this=0x14f535c0) at AsyncCall.cc:40
#9  0x08362f9f in AsyncCallQueue::fireNext (this=0x872f100) at
AsyncCallQueue.cc:56
#10 0x08362d32 in AsyncCallQueue::fire (this=0x872f100) at
AsyncCallQueue.cc:42
#11 0x08226f5f in EventLoop::dispatchCalls (this=0xbfaa66c8) at
EventLoop.cc:143
#12 0x08226dad in EventLoop::runOnce (this=0xbfaa66c8) at EventLoop.cc:108
#13 0x08226c96 in EventLoop::run (this=0xbfaa66c8) at EventLoop.cc:82
#14 0x0827f753 in SquidMain (argc=1, argv=0xbfaa6844) at main.cc:1511
#15 0x0827eb9f in SquidMainSafe (argc=1, argv=0xbfaa6844) at main.cc:1243
#16 0x0827eb84 in main (argc=1, argv=0xbfaa6844) at main.cc:1236
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150729/290d4fe6/attachment.htm>

From mmonette at 2keys.ca  Wed Jul 29 14:21:18 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 29 Jul 2015 10:21:18 -0400 (EDT)
Subject: [squid-users] LDAP Auth re-prompting for credentials on browser
 close, need suggestions
Message-ID: <815762963.2988024.1438179678564.JavaMail.zimbra@2keys.ca>

Hey,

I configured the basic_ldap_helper from Squid to my LDAP. Everytime I open the browser I am forced to re-auth. All of them except for Internet Explorer..But who uses IE anyways? It seems like this is not a Squid issue, but a browser thing.

Found this post asking the same question: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Ldap-Authen-AD-how-to-make-authentication-persistent-td3604487.html and it 

There were two suggestions that stood out: 

There used to be a "authenticate_ip_shortcircuit_ttl" option in squid 2.7 that when authenticated successfully, it would remember the users IP for X amount of time and would let you avoid logging in every time you reopen your browser. They removed that in later versions unfortunately, I guess because someone could take over your IP and would be able to authenticate as you(which is not a concern to me, at all).

The other suggestion would be to use an external ACL helper but they did not include one or any example in the post. 

Would anyone happen to have an example of a helper that does this or some other way I can go about pulling this off? 

Thanks in advance.

Mike


From marko.cupac at mimar.rs  Wed Jul 29 15:12:38 2015
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Wed, 29 Jul 2015 17:12:38 +0200
Subject: [squid-users] please help me test ext_ldap_group_acl from
 command line
In-Reply-To: <55B7EC42.5020007@treenet.co.nz>
References: <20150728180102.16910420@efreet> <55B7C237.1040009@treenet.co.nz>
 <55B7EC42.5020007@treenet.co.nz>
Message-ID: <20150729171238.00c447ea@efreet>

Hi,

I finally made it work. It does not matter if helper is started under
my account or under squid account, it works both ways.

Here's full command:
./ext_ldap_group_acl -R \
	-b "DC=mimar,DC=rs" \
	-D "CN=LDAP Binder,OU=00-System-00,OU=Users,OU=BG,OU=RS,DC=mimar,DC=rs" \
	-w "mypass" \
	-f "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,OU=Web Services,OU=Groups,OU=BG,OU=RS,DC=mimar,DC=rs))" \
	-h dc1.mimar.rs

So, if i have user 'pacija', who is a member of security group
'squid_noaccess' which resides in 'Web Services OU'...

mimar.rs
 -> RS
  -> BG
   -> Groups
    -> Web Services
       - squid_noaccess

...typing in:
pacija squid_noaccess

...returns OK.

Regards,
-- 
Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Wed Jul 29 20:53:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 08:53:11 +1200
Subject: [squid-users] Squid 3.5.6 crash
In-Reply-To: <CAFKaCCQim3LJiUCK2YxpUKHz=c1TJkgSk+2HWK57v=k0vOW+YQ@mail.gmail.com>
References: <CAFKaCCQim3LJiUCK2YxpUKHz=c1TJkgSk+2HWK57v=k0vOW+YQ@mail.gmail.com>
Message-ID: <55B93D37.40108@treenet.co.nz>

On 30/07/2015 2:12 a.m., Alan wrote:
> This might be the same crash reported in another thread, or a new one.
> I noticed it after upgrading from 3.3.11 to 3.5.6.
> 

This appears to be bug 4227.

Please upgrade to the 3.5.6 snapshot to avoid it (or 3.5.7 will be out
in a few days).

Amos




From squid3 at treenet.co.nz  Wed Jul 29 20:55:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 08:55:59 +1200
Subject: [squid-users] Possible bug in 3.5.5 or a store change from 2.7?
In-Reply-To: <55B899EF.4060802@ngtech.co.il>
References: <CAEaSS0aBfNq8upzz39O-PJt0kCyNsxGhFqGi+su_ybdZ1GLnFA@mail.gmail.com>
 <55B7DDF0.3070006@treenet.co.nz>
 <CAEaSS0ZZiqyGzSoX_mysk=-VRq5aagbokni7cvPSF6VYxWfjEg@mail.gmail.com>
 <55B8233D.1060906@treenet.co.nz> <55B899EF.4060802@ngtech.co.il>
Message-ID: <55B93DDF.7020506@treenet.co.nz>

On 29/07/2015 9:16 p.m., Eliezer Croitoru wrote:
> Just wondering how new is this option?
> 

Very old. And ararelyusd, it sets a fixed debug level ALL,7. so the
amount of data produced is almost as much as ALL,9. I keep forgetting
about it and thinking we have to swap config files under Squid. :-(

Amos

> Eliezer
> 
> On 29/07/2015 03:50, Amos Jeffries wrote:
>> That can be resolved somewhat by turning the logging on dynamically with
>> "squid -k debug" shortly before and after a test is run.
>>
>> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul 29 21:20:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 09:20:17 +1200
Subject: [squid-users] LDAP Auth re-prompting for credentials on browser
 close, need suggestions
In-Reply-To: <815762963.2988024.1438179678564.JavaMail.zimbra@2keys.ca>
References: <815762963.2988024.1438179678564.JavaMail.zimbra@2keys.ca>
Message-ID: <55B94391.2020203@treenet.co.nz>

On 30/07/2015 2:21 a.m., Michael Monette wrote:
> Hey,
> 
> I configured the basic_ldap_helper from Squid to my LDAP. Everytime I
> open the browser I am forced to re-auth. All of them except for
> Internet Explorer..But who uses IE anyways? It seems like this is not
> a Squid issue, but a browser thing.
> 

How very .. expected. see end of this email.


> Found this post asking the same question:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Ldap-Authen-AD-how-to-make-authentication-persistent-td3604487.html
> and it
> 
> There were two suggestions that stood out:
> 
> There used to be a "authenticate_ip_shortcircuit_ttl" option in squid
> 2.7 that when authenticated successfully, it would remember the users
> IP for X amount of time and would let you avoid logging in every time
> you reopen your browser. They removed that in later versions
> unfortunately, I guess because someone could take over your IP and
> would be able to authenticate as you(which is not a concern to me, at
> all).

It should.

Theres this thing called NAT you see, which places multiple users behind
a single IP. The first one to login with IP-based auth. Since IPv4 ran
out back in 2003 a number of networks have started using one layer.
Since 2010 when IPv4 stopped being readily available its become more and
more popular to use 2 and even 3 layers of NAT between any two machines.
Just so they can talk.

Then there is this thing called DHCP. I guess this is what you mean by
one user taking over anothers IP. Since the DHCP service allocates any
available IP to user devices on request. If a device goes away its IP
can get re-used immediately by another device. Its uite difficult to get
Squid to be aware of any of those changes and update its auth information.

Then there is the thing called "privacy addressing". In IPv6 its
built-in, with IPv4 its done using DHCP short dynamic assignments. It
means the IP address assigned to user devices is guaranteed to change
frequently and randomly.

Now, if your network can operate without NAT or DHCP, or IPv6. You are
one of the very rare lucky people for whom IP-shortcut based auth
*might* work. But only until you have a malicious user contact the
network and start spoofing users IPs.


IP address based authentication is, well. Dead.


> 
> The other suggestion would be to use an external ACL helper but they
> did not include one or any example in the post.
> 
> Would anyone happen to have an example of a helper that does this or
> some other way I can go about pulling this off?


 "Windows Integrated Authentication" is what IE is using not to have to
ask user for credentials. Some credentials were given when they logged
into the machine, and are used by the browser to send to Squid as needed
(and only as needed). Sometimes called Single-Sign-On or Federated
authentication.

I hear the other browsers need some config to use it. But can't recall
right now what that is.

For Squid it should work with Basic auth. Dont believe the myths that
say Windows auth == NTLM.


Whether *a* popup is seen also depends on whether the browser password
manager is in use. One always need to unlock that manually when opening
a browser. The actual Squid credentials are fetched from there after its
opened. And no that popup is *not* part of HTTP auth.

Amos


From sebag at vianetcon.com.ar  Wed Jul 29 21:23:09 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 29 Jul 2015 18:23:09 -0300
Subject: [squid-users] Negative value for total memory accounted
Message-ID: <55B9443D.1070702@vianetcon.com.ar>

Hello, I'm having a problem monitoring squid memory usage.

Using SNMP:
SQUID-MIB::cacheMemUsage.0 = INTEGER: -1355105

Using squid-client:
Memory accounted for:
     Total accounted:       -1369659 KB
     memPoolAlloc calls:      1995
     memPoolFree calls:  653296188

Note that the value is negative (-)

The problem arose when I set "cache_mem 2048 MB" (it was 1024 before)
I've read about the malloc problem .. just wanted to know if someone 
found a solution other than obtaining it through ps


Thanks,
Sebastian


From hack.back at hotmail.com  Wed Jul 29 21:26:21 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 29 Jul 2015 14:26:21 -0700 (PDT)
Subject: [squid-users] useragent.log
In-Reply-To: <55B6A8FE.5010609@treenet.co.nz>
References: <1438019116110-4672505.post@n4.nabble.com>
 <201507271855.19770.Antony.Stone@squid.open.source.it>
 <1438031209371-4672508.post@n4.nabble.com> <55B6A8FE.5010609@treenet.co.nz>
Message-ID: <1438205181731-4672537.post@n4.nabble.com>

ok bro thanks,
and whats about the cookies that the site used ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/useragent-log-tp4672505p4672537.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jul 29 21:29:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 30 Jul 2015 03:29:35 +0600
Subject: [squid-users] Negative value for total memory accounted
In-Reply-To: <55B9443D.1070702@vianetcon.com.ar>
References: <55B9443D.1070702@vianetcon.com.ar>
Message-ID: <55B945BF.8050107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Squid is 32 bit? And OS?

30.07.15 3:23, Sebastian Goicochea ?????:


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVuUW8AAoJENNXIZxhPexGDFoH/0i3JgkQYY40rnOuPBffv8H3
wMgLiqQJ322RB8bJKo+pQsG6mEdQiXsgXS2qOIVvAjLme3TIZgwntcA5UoaWo5vY
UYDKA0/mRT7os6eANgD/qai4d/aNo5G8Ik1CBwbxTheD1AcFLe6gVi/7z0GBFB7A
cS6LQMed4Q5qnag8OEa0Eok1b/J1o9dWybsIKsqzbHLuFt+XIYBk82cyJwNVA7DJ
9lbRejA8+svIPK/CnmmO1Eku4DbaGJgJVBl6KsdsfMZaUxkvFmyS7Y2zThpGTjxr
1M6x5RXi3NpGdhaAEhvuvurqZKcoGzfnO1PEAEljl1xCL3MB0NFH/kYm/8vescY=
=IQfi
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150730/5fa59031/attachment.htm>

From yvoinov at gmail.com  Wed Jul 29 21:29:34 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 30 Jul 2015 03:29:34 +0600
Subject: [squid-users] Negative value for total memory accounted
In-Reply-To: <55B9443D.1070702@vianetcon.com.ar>
References: <55B9443D.1070702@vianetcon.com.ar>
Message-ID: <55B945BE.7020400@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Squid is 32 bit? And OS?

30.07.15 3:23, Sebastian Goicochea ?????:
> Hello, I'm having a problem monitoring squid memory usage.
>
> Using SNMP:
> SQUID-MIB::cacheMemUsage.0 = INTEGER: -1355105
>
> Using squid-client:
> Memory accounted for:
>     Total accounted:       -1369659 KB
>     memPoolAlloc calls:      1995
>     memPoolFree calls:  653296188
>
> Note that the value is negative (-)
>
> The problem arose when I set "cache_mem 2048 MB" (it was 1024 before)
> I've read about the malloc problem .. just wanted to know if someone
found a solution other than obtaining it through ps
>
>
> Thanks,
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVuUW+AAoJENNXIZxhPexG7TAIAJzUf3rswAkgoGUIJJwtvPX0
MotGmQJfgYBGd3YAvFlRP42/R3zoTm7HWixwCJIPEn+Ghw6KS4A95ZSzccIzmdCh
ZbWeRMBcQSAO971MpMT9jaCyj2r+8Os2GpuOfDJAYI4NoE6/SNHhxLFWE8Hn1se1
Bwupi2p8ZArIlUhwC68/gcEAJWuiGjIYy/F4yp6flBdiVuqcbU0J19E9caGluviz
37Un5Pzkz/B99Ck9JFUplmuN1PvnErB2jEZbDxMEV/O/3fzcpPMOpMSNZvR/Vqp3
mrKS1wpdI4I8Xb67pMzUyjqX9zVoA6e8URQ85kj2SQRoKXUsC7niXe/OLSV1+Zg=
=L8/g
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150730/45ec271a/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 29 21:46:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 09:46:03 +1200
Subject: [squid-users] useragent.log
In-Reply-To: <1438205181731-4672537.post@n4.nabble.com>
References: <1438019116110-4672505.post@n4.nabble.com>
 <201507271855.19770.Antony.Stone@squid.open.source.it>
 <1438031209371-4672508.post@n4.nabble.com> <55B6A8FE.5010609@treenet.co.nz>
 <1438205181731-4672537.post@n4.nabble.com>
Message-ID: <55B9499B.20300@treenet.co.nz>

On 30/07/2015 9:26 a.m., HackXBack wrote:
> ok bro thanks,
> and whats about the cookies that the site used ?

If you want to log those you need to create a custom log format.

Its privacy related data useless to Squid and we dont enscourage
recording that kind of thing in places that could be published.

Amos



From sebag at vianetcon.com.ar  Wed Jul 29 21:49:43 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 29 Jul 2015 18:49:43 -0300
Subject: [squid-users] Negative value for total memory accounted
In-Reply-To: <55B945BF.8050107@gmail.com>
References: <55B9443D.1070702@vianetcon.com.ar> <55B945BF.8050107@gmail.com>
Message-ID: <55B94A77.1030506@vianetcon.com.ar>

Sorry, both 64:

Linux 3.1.10-1.16-desktop #1 SMP PREEMPT Wed Jun 27 05:21:40 UTC 2012 
(d016078) x86_64 x86_64 x86_64 GNU/Linux

/usr/local/sbin/squid: ELF 64-bit LSB executable, x86-64, version 1 
(GNU/Linux), dynamically linked (uses shared libs), for GNU/Linux 
2.6.16, BuildID[sha1]=0x1b614725b310b2cb069d6e3a381faaafa9831daf, not 
stripped




El 29/07/15 a las 18:29, Yuri Voinov escribi?:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Squid is 32 bit? And OS?
>
> 30.07.15 3:23, Sebastian Goicochea ?????:
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVuUW8AAoJENNXIZxhPexGDFoH/0i3JgkQYY40rnOuPBffv8H3
> wMgLiqQJ322RB8bJKo+pQsG6mEdQiXsgXS2qOIVvAjLme3TIZgwntcA5UoaWo5vY
> UYDKA0/mRT7os6eANgD/qai4d/aNo5G8Ik1CBwbxTheD1AcFLe6gVi/7z0GBFB7A
> cS6LQMed4Q5qnag8OEa0Eok1b/J1o9dWybsIKsqzbHLuFt+XIYBk82cyJwNVA7DJ
> 9lbRejA8+svIPK/CnmmO1Eku4DbaGJgJVBl6KsdsfMZaUxkvFmyS7Y2zThpGTjxr
> 1M6x5RXi3NpGdhaAEhvuvurqZKcoGzfnO1PEAEljl1xCL3MB0NFH/kYm/8vescY=
> =IQfi
> -----END PGP SIGNATURE-----
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150729/fa1f0885/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 29 22:04:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 10:04:15 +1200
Subject: [squid-users] Negative value for total memory accounted
In-Reply-To: <55B945BE.7020400@gmail.com>
References: <55B9443D.1070702@vianetcon.com.ar> <55B945BE.7020400@gmail.com>
Message-ID: <55B94DDF.7030007@treenet.co.nz>

On 30/07/2015 9:29 a.m., Yuri Voinov wrote:
> 
> Squid is 32 bit? And OS?

Well, technically SNMP used by Squid is 32-bit. Squid itsef may be a
64-bit build and produce the same thing.

> 
> 30.07.15 3:23, Sebastian Goicochea ?????:
>> Hello, I'm having a problem monitoring squid memory usage.
> 
>> Using SNMP:
>> SQUID-MIB::cacheMemUsage.0 = INTEGER: -1355105
> 
>> Using squid-client:
>> Memory accounted for:
>>     Total accounted:       -1369659 KB
>>     memPoolAlloc calls:      1995
>>     memPoolFree calls:  653296188
> 
>> Note that the value is negative (-)
> 
>> The problem arose when I set "cache_mem 2048 MB" (it was 1024 before)
>> I've read about the malloc problem .. just wanted to know if someone
> found a solution other than obtaining it through ps

Not easily. The maths is a bit tricky. But if you know how, convert the
negative it to an unsigned 32-bit value.

Amos


From sebag at vianetcon.com.ar  Wed Jul 29 22:54:30 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 29 Jul 2015 19:54:30 -0300
Subject: [squid-users] Negative value for total memory accounted
In-Reply-To: <55B94DDF.7030007@treenet.co.nz>
References: <55B9443D.1070702@vianetcon.com.ar> <55B945BE.7020400@gmail.com>
 <55B94DDF.7030007@treenet.co.nz>
Message-ID: <55B959A6.4060805@vianetcon.com.ar>

Amos, it makes perfect sense now. After the convertion I have the real value


Thanks,
Sebastian

El 29/07/15 a las 19:04, Amos Jeffries escribi?:
> On 30/07/2015 9:29 a.m., Yuri Voinov wrote:
>> Squid is 32 bit? And OS?
> Well, technically SNMP used by Squid is 32-bit. Squid itsef may be a
> 64-bit build and produce the same thing.
>
>> 30.07.15 3:23, Sebastian Goicochea ?????:
>>> Hello, I'm having a problem monitoring squid memory usage.
>>> Using SNMP:
>>> SQUID-MIB::cacheMemUsage.0 = INTEGER: -1355105
>>> Using squid-client:
>>> Memory accounted for:
>>>      Total accounted:       -1369659 KB
>>>      memPoolAlloc calls:      1995
>>>      memPoolFree calls:  653296188
>>> Note that the value is negative (-)
>>> The problem arose when I set "cache_mem 2048 MB" (it was 1024 before)
>>> I've read about the malloc problem .. just wanted to know if someone
>> found a solution other than obtaining it through ps
> Not easily. The maths is a bit tricky. But if you know how, convert the
> negative it to an unsigned 32-bit value.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dan at getbusi.com  Thu Jul 30 03:43:35 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 30 Jul 2015 13:43:35 +1000
Subject: [squid-users] Squid 3.4.14
Message-ID: <CAN8nrKAXzao0rXXkdNDqhOq3gPvV=vD8Qt5ucF8mgyeaAY9ckQ@mail.gmail.com>

Hey folks

Is 3.4.14 going to be a thing or should we be moving to v3.5 if we want new
bug fixes?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150730/1a81415c/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 30 04:45:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 16:45:02 +1200
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <CAN8nrKAXzao0rXXkdNDqhOq3gPvV=vD8Qt5ucF8mgyeaAY9ckQ@mail.gmail.com>
References: <CAN8nrKAXzao0rXXkdNDqhOq3gPvV=vD8Qt5ucF8mgyeaAY9ckQ@mail.gmail.com>
Message-ID: <55B9ABCE.2030409@treenet.co.nz>

On 30/07/2015 3:43 p.m., Dan Charlesworth wrote:
> Hey folks
> 
> Is 3.4.14 going to be a thing or should we be moving to v3.5 if we want new
> bug fixes?
> 

Well. Yes an 3.4 has a serious CVE that needs releasing. So it will be a
thing this weekend.

But no other bug fixes in the past few months qualify as security
issues. So yes you need to be moving on to 3.5. Especially if you are
using the ssl-bump features.

Amos



From fredbmail at free.fr  Thu Jul 30 08:08:18 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 30 Jul 2015 10:08:18 +0200 (CEST)
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <55B9ABCE.2030409@treenet.co.nz>
Message-ID: <1299596190.18572939.1438243698255.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Well. Yes an 3.4 has a serious CVE that needs releasing. So it will
> be a
> thing this weekend.
> 
> But no other bug fixes in the past few months qualify as security
> issues. So yes you need to be moving on to 3.5. Especially if you are
> using the ssl-bump features.
> 
> Amos
> 


So, no chance for http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13867.patch
I known, I should migrate  




From squid3 at treenet.co.nz  Thu Jul 30 09:21:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jul 2015 21:21:19 +1200
Subject: [squid-users] Squid 3.4.14
In-Reply-To: <1299596190.18572939.1438243698255.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1299596190.18572939.1438243698255.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55B9EC8F.8010009@treenet.co.nz>

On 30/07/2015 8:08 p.m., FredB wrote:
> 
>>
>> Well. Yes an 3.4 has a serious CVE that needs releasing. So it will
>> be a
>> thing this weekend.
>>
>> But no other bug fixes in the past few months qualify as security
>> issues. So yes you need to be moving on to 3.5. Especially if you are
>> using the ssl-bump features.
>>
>> Amos
>>
> 
> 
> So, no chance for http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13867.patch
> I known, I should migrate  
> 

Im not sure that even qualifies as a bug really. Just polishing the
debug output to avoid future confusion like we went through.

Amos



From angelo.bruno at vigilfuoco.it  Thu Jul 30 14:49:02 2015
From: angelo.bruno at vigilfuoco.it (Posta Esterna)
Date: Thu, 30 Jul 2015 16:49:02 +0200
Subject: [squid-users] error windbind
In-Reply-To: <55B24293.1090604@treenet.co.nz>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
 <55B22061.3060407@vigilfuoco.it> <55B24293.1090604@treenet.co.nz>
Message-ID: <55BA395E.5060300@vigilfuoco.it>

Thanx..

I don't understand a something...

Where do i need to find Samba? On the Proxy or on the AD Server?

I don't think i have to create a Samba server on the Proxy... is it true?

So how can i tell Squid to connect to the right Samba Server?




Il 24/07/2015 15.50, Amos Jeffries ha scritto:
> On 24/07/2015 11:24 p.m., Posta Esterna wrote:
>> Thanx Amos,
>> Squid was the first problem... using /usr/lib/squid/ntlm_auth instead of
>> /usr/bin/ntlm_auth
>>
>> About upgrading unfortunatelly i have only an old DELL P4 (10 years
>> old?) with 1GB of RAM... and for my fortune this is only the PROXY n?2
>> (the backup).... The PROXY n?1 goes well with a version of KERIO Control...
>>
> FWIW, I run a few customer sites with older hardware than that. Of
> course those dont service much traffic. But Squid-3.5 with small cache
> is not even taxing the hardware.
>
> Kind of my specialty hobby now. Running Squid on recycled and low-spec
> hardware :-)
>
>
>> I've still have problems....
>> it says:
>> ....
>> [2015/07/24 12:06:41, 0] utils/ntlm_auth.c:get_windbind_domain(146)
>>    could not obtain windbind domain name!
>> .....
>>
> That seems to be an internal winbind / Samba problem.
>
> Once you have the right helper Squids part is reduced to ferrying the
> HTTP Auth header contents to and from it.
>
>
> PS. Unless you are fighting with similarly ancient Windows 2K boxen I
> suggest looking into Negotiate/Kerberos (squid_kerb_auth etc should work
> okay with 2.6) instead of, or as well as, NTLM. Sometimes its a bit more
> painful to setup, but much more resource efficient _and_ secure.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
VCTI Ing. Angelo Bruno



From mmonette at 2keys.ca  Thu Jul 30 15:36:07 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Thu, 30 Jul 2015 11:36:07 -0400
Subject: [squid-users] LDAP Auth re-prompting for credentials on browser
 close, need suggestions
In-Reply-To: <55B94391.2020203@treenet.co.nz>
References: <815762963.2988024.1438179678564.JavaMail.zimbra@2keys.ca>
 <55B94391.2020203@treenet.co.nz>
Message-ID: <55BA4467.6000202@2keys.ca>



On 7/29/2015 5:20 PM, Amos Jeffries wrote:
>
>> Found this post asking the same question:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Ldap-Authen-AD-how-to-make-authentication-persistent-td3604487.html
>> and it
>>
>> There were two suggestions that stood out:
>>
>> There used to be a "authenticate_ip_shortcircuit_ttl" option in squid
>> 2.7 that when authenticated successfully, it would remember the users
>> IP for X amount of time and would let you avoid logging in every time
>> you reopen your browser. They removed that in later versions
>> unfortunately, I guess because someone could take over your IP and
>> would be able to authenticate as you(which is not a concern to me, at
>> all).
> It should.
>
> Theres this thing called NAT you see, which places multiple users behind
> a single IP. The first one to login with IP-based auth. Since IPv4 ran
> out back in 2003 a number of networks have started using one layer.
> Since 2010 when IPv4 stopped being readily available its become more and
> more popular to use 2 and even 3 layers of NAT between any two machines.
> Just so they can talk.
>
> Then there is this thing called DHCP. I guess this is what you mean by
> one user taking over anothers IP. Since the DHCP service allocates any
> available IP to user devices on request. If a device goes away its IP
> can get re-used immediately by another device. Its uite difficult to get
> Squid to be aware of any of those changes and update its auth information.
>
> Then there is the thing called "privacy addressing". In IPv6 its
> built-in, with IPv4 its done using DHCP short dynamic assignments. It
> means the IP address assigned to user devices is guaranteed to change
> frequently and randomly.
>
> Now, if your network can operate without NAT or DHCP, or IPv6. You are
> one of the very rare lucky people for whom IP-shortcut based auth
> *might* work. But only until you have a malicious user contact the
> network and start spoofing users IPs.
>
>
> IP address based authentication is, well. Dead.
>
Okay, well I know how NAT and DHCP works, so I guess I am one of those 
rare cases you talk about.  We have no NATs, I am only trying to use 
squid on a small section of our network which has statically assigned IP 
addresses and they have no admin rights to change it. They are open 24/7 
so some stranger walking in, plugging his laptop on our network and 
trying to figure out which IP is already authenticated is very unlikely 
since everybody knows each other and it noticed/reported. That's why 
this does not concern me. I was also only planning to have it remember 
the IP for maybe 1 hour.

On the other hand, having users re-authenticate every time they close 
their browser would irritate them and possibly cause confusion as most 
of them are not very technical(It might just be something they will need 
to adjust to after all).

So in my case, either I figure out a way to go about this (I saw your 
suggestions below and am going to do some reading) or I might have to 
not implement any authentication at all, which I think is worse.

>> The other suggestion would be to use an external ACL helper but they
>> did not include one or any example in the post.
>>
>> Would anyone happen to have an example of a helper that does this or
>> some other way I can go about pulling this off?
>
>   "Windows Integrated Authentication" is what IE is using not to have to
> ask user for credentials. Some credentials were given when they logged
> into the machine, and are used by the browser to send to Squid as needed
> (and only as needed). Sometimes called Single-Sign-On or Federated
> authentication.
>
> I hear the other browsers need some config to use it. But can't recall
> right now what that is.

I will look around for this config option and check out SSO/Federated 
authentication, I appreciate the hint.
If I can't figure out a way to get this going I will speak to management 
to see if this is an acceptable tradeoff. I am just exploring my options 
before I do.
>
> For Squid it should work with Basic auth. Dont believe the myths that
> say Windows auth == NTLM.
>
>
> Whether *a* popup is seen also depends on whether the browser password
> manager is in use. One always need to unlock that manually when opening
> a browser. The actual Squid credentials are fetched from there after its
> opened. And no that popup is *not* part of HTTP auth.
Thanks for clearing that up and your responses, much appreciated.


From mcsnv96 at afo.net  Thu Jul 30 15:48:42 2015
From: mcsnv96 at afo.net (Mike)
Date: Thu, 30 Jul 2015 10:48:42 -0500
Subject: [squid-users] dns failover failing with 3.4.7
In-Reply-To: <55B6AFF4.3090407@treenet.co.nz>
References: <55B696C7.2050704@afo.net> <55B6AFF4.3090407@treenet.co.nz>
Message-ID: <55BA475A.1070004@afo.net>

On 7/27/2015 17:25 PM, Amos Jeffries wrote:
> On 28/07/2015 8:38 a.m., Mike wrote:
>> Running into an issue, using the squid.conf entry
>> dns_nameservers 72.x.x.x 72.x.y.y
>>
>> These are different servers (under our control) for the purpose of
>> filtering than listed in resolv.conf (which are out of our control, used
>> for server IP routing by upstream host).
>>
>> The problem we found like this weekend is if the primary listed dns
>> server is unavailable, squid fails to use the secondary listed server.
>> Instead it displays the "unable to connect" type messages with all
>> websites.
> Details please. How do you know the secondary is not even being tried?
>
> What is Squid getting back from the primary when its "down" ?
>   or just dns_timeout being hit?
>
> Add this to squid.conf to get a cache.log trace of the DNS activity:
>    debug_options 78,6
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
Amos,

If it was using the secondary server listed, connections to almost all 
websites would not be failing to load if primary was down. For the test 
we temporarily took the primary DNS server offline (per the example 
above 72.x.x.x), and no websites would load unless it was in the squid 
cache, but any elements that required additional data failed to load 
causing formatting issues with the displayed website. If we swap the 
setting to the "secondary" with the first IP (per the example above) as
dns_nameservers 72.x.y.y 72.x.x.x
and it works the same way, take the ".y.y" down and it refuses to use 
the secondary listed IP ".x.x" for DNS, instead displays the website 
could not be displayed error in the browsers. We even tried another test 
(per the example above) dns_nameservers 72.x.x.x 8.8.8.8 then let it run 
for an hour or so. Then we took down the primary which means it should 
use the secondary google IP of 8.8.8.8, but it doesn't, goes right back 
to the "website could not be displayed" error in the browsers.
I was wonder if this might be a bug. This is happening on multiple 
servers, one has squid 3.4.7, another has 3.4.6 and problem occurs on both.

Thanks
Mike






From jmakarevic at gmail.com  Thu Jul 30 20:05:35 2015
From: jmakarevic at gmail.com (Josip Makarevic)
Date: Thu, 30 Jul 2015 22:05:35 +0200
Subject: [squid-users] squid centos and osq_lock
Message-ID: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>

Hi,

I have a problem with squid setup (squid version 3.5.6, built from source,
centos 6.6)
I've tried 2 options:
1. SMP
2. NON-SMP

I've decided to stick with custom build non-smp version and the thing is:
- i don't need cache - any kind of it
- I have DNS cache just for that
- squid has to listen on 1024 ports on 23 instances.
each instance listens on set of ports and each port has different outgoing
ip address.

The thing is this:
It's alll good until we hit it with more than 150mbits then...

(output from perf top)
 84.57%  [kernel]                  [k] osq_lock
  4.62%  [kernel]                  [k] mutex_spin_on_owner
  1.41%  [kernel]                  [k] memcpy
  0.79%  [kernel]                  [k] inet_dump_ifaddr
  0.62%  [kernel]                  [k] memset

 21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84, 23.33
(yes, we have 24 cores)
Same behavior is with SMP and NON-SMP setup (SMP setup is all in one file
with workers 23 option but then I have to use rock cache)

so, my question is....what...how to optimize this.....whatever....I'm stuck
for days, I've tried many sysctl options but none of them works.
Any help, info, something else?


Thanks,
J.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150730/5589247f/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 30 20:58:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Jul 2015 08:58:23 +1200
Subject: [squid-users] error windbind
In-Reply-To: <55BA395E.5060300@vigilfuoco.it>
References: <55B1EFF3.7050905@vigilfuoco.it> <55B203C5.60005@treenet.co.nz>
 <55B22061.3060407@vigilfuoco.it> <55B24293.1090604@treenet.co.nz>
 <55BA395E.5060300@vigilfuoco.it>
Message-ID: <55BA8FEF.7020707@treenet.co.nz>

On 31/07/2015 2:49 a.m., Posta Esterna wrote:
> Thanx..
> 
> I don't understand a something...
> 
> Where do i need to find Samba? On the Proxy or on the AD Server?

AD server is the server. Samba contains parts that are clients. Winbind,
nmblookup, smbclient, and the ntlm_auth helper are the client parts most
frequently used with Squid. Which one(s) depend on the auth login and
group helpers you want to use.

So you need at least those parts of Samba installed on the Squid
machine. At least the client parts of it.


> 
> I don't think i have to create a Samba server on the Proxy... is it true?

I believe that is correct.

> 
> So how can i tell Squid to connect to the right Samba Server?

Squid uses a Samba helper to connect to AD server. It is told which AD
domain/realm to lookup by the users credentials token. You may (or may
not) need to configure the helper with what domains/realms it is to use
and where the server for each auth domain/realm are.

Amos



From squid3 at treenet.co.nz  Thu Jul 30 21:21:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Jul 2015 09:21:55 +1200
Subject: [squid-users] LDAP Auth re-prompting for credentials on browser
 close, need suggestions
In-Reply-To: <55BA4467.6000202@2keys.ca>
References: <815762963.2988024.1438179678564.JavaMail.zimbra@2keys.ca>
 <55B94391.2020203@treenet.co.nz> <55BA4467.6000202@2keys.ca>
Message-ID: <55BA9573.90702@treenet.co.nz>

On 31/07/2015 3:36 a.m., Michael Monette wrote:
> 
> 
> On 7/29/2015 5:20 PM, Amos Jeffries wrote:
>>
>>> Found this post asking the same question:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Ldap-Authen-AD-how-to-make-authentication-persistent-td3604487.html
>>>
>>> and it
>>>
>>> There were two suggestions that stood out:
>>>
>>> There used to be a "authenticate_ip_shortcircuit_ttl" option in squid
>>> 2.7 that when authenticated successfully, it would remember the users
>>> IP for X amount of time and would let you avoid logging in every time
>>> you reopen your browser. They removed that in later versions
>>> unfortunately, I guess because someone could take over your IP and
>>> would be able to authenticate as you(which is not a concern to me, at
>>> all).
>> It should.
>>
>> Theres this thing called NAT you see, which places multiple users behind
>> a single IP. The first one to login with IP-based auth. Since IPv4 ran
>> out back in 2003 a number of networks have started using one layer.
>> Since 2010 when IPv4 stopped being readily available its become more and
>> more popular to use 2 and even 3 layers of NAT between any two machines.
>> Just so they can talk.
>>
>> Then there is this thing called DHCP. I guess this is what you mean by
>> one user taking over anothers IP. Since the DHCP service allocates any
>> available IP to user devices on request. If a device goes away its IP
>> can get re-used immediately by another device. Its uite difficult to get
>> Squid to be aware of any of those changes and update its auth
>> information.
>>
>> Then there is the thing called "privacy addressing". In IPv6 its
>> built-in, with IPv4 its done using DHCP short dynamic assignments. It
>> means the IP address assigned to user devices is guaranteed to change
>> frequently and randomly.
>>
>> Now, if your network can operate without NAT or DHCP, or IPv6. You are
>> one of the very rare lucky people for whom IP-shortcut based auth
>> *might* work. But only until you have a malicious user contact the
>> network and start spoofing users IPs.
>>
>>
>> IP address based authentication is, well. Dead.
>>
> Okay, well I know how NAT and DHCP works, so I guess I am one of those
> rare cases you talk about.  We have no NATs, I am only trying to use
> squid on a small section of our network which has statically assigned IP
> addresses and they have no admin rights to change it. They are open 24/7
> so some stranger walking in, plugging his laptop on our network and
> trying to figure out which IP is already authenticated is very unlikely
> since everybody knows each other and it noticed/reported. That's why
> this does not concern me. I was also only planning to have it remember
> the IP for maybe 1 hour.

Yes it would seem you are.

> 
> On the other hand, having users re-authenticate every time they close
> their browser would irritate them and possibly cause confusion as most
> of them are not very technical(It might just be something they will need
> to adjust to after all).

They will have to face it in every other authentication-related use of
browsers. So thay should not be a problem.

> 
> So in my case, either I figure out a way to go about this (I saw your
> suggestions below and am going to do some reading) or I might have to
> not implement any authentication at all, which I think is worse.

Yes and no.

The auth system is only a problem if you want user/browser to be the one
supplying credentials. For that Squid has to validate the users claim
about who they are and deal with incorrect/missing claims etc. The pop
is resulting from all that.

If you are happy to avoid user-supplied credentials entirely you can use
an external ACL helper that takes %SRC (the client IP) and provides
Squid with "OK user=username" with the username to log about who is
supposed to be using that machine IP or "ERR" for unauthorized access.

Incidentally that IP->username mapping exactly what the ip_shortcut 2.7
feature used to do. But without any possible way to control on your part
about what usernames were supplied for which IP.


These two helpers are provided with Squid that do this for different
backend databases of IP->user mapping:

<http://www.squid-cache.org/Versions/v4/manuals/ext_edirectory_userip_acl.html>

<http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html>

The SQL one is what I use for my own clients when they have an office
setup like you describe and a very stable / long term group of trusted
workers.

Amos


From squid3 at treenet.co.nz  Thu Jul 30 21:30:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Jul 2015 09:30:21 +1200
Subject: [squid-users] dns failover failing with 3.4.7
In-Reply-To: <55BA475A.1070004@afo.net>
References: <55B696C7.2050704@afo.net> <55B6AFF4.3090407@treenet.co.nz>
 <55BA475A.1070004@afo.net>
Message-ID: <55BA976D.2060607@treenet.co.nz>

On 31/07/2015 3:48 a.m., Mike wrote:
> On 7/27/2015 17:25 PM, Amos Jeffries wrote:
>> On 28/07/2015 8:38 a.m., Mike wrote:
>>> Running into an issue, using the squid.conf entry
>>> dns_nameservers 72.x.x.x 72.x.y.y
>>>
>>> These are different servers (under our control) for the purpose of
>>> filtering than listed in resolv.conf (which are out of our control, used
>>> for server IP routing by upstream host).
>>>
>>> The problem we found like this weekend is if the primary listed dns
>>> server is unavailable, squid fails to use the secondary listed server.
>>> Instead it displays the "unable to connect" type messages with all
>>> websites.
>> Details please. How do you know the secondary is not even being tried?
>>
>> What is Squid getting back from the primary when its "down" ?
>>   or just dns_timeout being hit?
>>
>> Add this to squid.conf to get a cache.log trace of the DNS activity:
>>    debug_options 78,6
>>
> Amos,
> 
> If it was using the secondary server listed, connections to almost all
> websites would not be failing to load if primary was down. For the test
> we temporarily took the primary DNS server offline (per the example
> above 72.x.x.x), and no websites would load unless it was in the squid
> cache, but any elements that required additional data failed to load
> causing formatting issues with the displayed website. If we swap the
> setting to the "secondary" with the first IP (per the example above) as
> dns_nameservers 72.x.y.y 72.x.x.x
> and it works the same way, take the ".y.y" down and it refuses to use
> the secondary listed IP ".x.x" for DNS, instead displays the website
> could not be displayed error in the browsers. We even tried another test
> (per the example above) dns_nameservers 72.x.x.x 8.8.8.8 then let it run
> for an hour or so. Then we took down the primary which means it should
> use the secondary google IP of 8.8.8.8, but it doesn't, goes right back
> to the "website could not be displayed" error in the browsers.
> I was wonder if this might be a bug. This is happening on multiple
> servers, one has squid 3.4.7, another has 3.4.6 and problem occurs on both.
> 

Thank you exactly the kind of answer I was looking for question #1.
(Evidence that the problem is what you think it is before digging for a
cause).

Kind of answers Q2 a "nothing", implying that Q3 is "yes" dns_timeout is
happening.


 Is your dns_timeout (default 30 sec *total* DNS lookup timeout) larger
than your dns_retransmit_interval (default 5 sec per-query timeout) setting?


Amos


From David.J.Berkes at pjc.com  Thu Jul 30 21:57:42 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Thu, 30 Jul 2015 21:57:42 +0000
Subject: [squid-users] forward proxy - many users with one login/passwd.
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E540187@ESCML200.corp.pjc.com>


Just a basic question.  I have a 3.5.0.4 forward proxy setup with basic authentication for my MDM proxy (iphones).  All iphones are set with the global proxy and identical user-name/password.  They will be on an LTE network and will be switching IP's often.  The forward proxy user-name/password will always be the same from each iphone. I have read several things about (max_user_ip, authenticate_ip_ttl) and concerned with the setup.  I essentially don?t want to limit any number of source connections using the same credentials.   Please advise of any pitfalls and/or settings for many users switching IP's frequent, using the same login/passwd.

Thank you
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From mcsnv96 at afo.net  Thu Jul 30 22:39:46 2015
From: mcsnv96 at afo.net (Mike)
Date: Thu, 30 Jul 2015 17:39:46 -0500
Subject: [squid-users] dns failover failing with 3.4.7
In-Reply-To: <55BA976D.2060607@treenet.co.nz>
References: <55B696C7.2050704@afo.net> <55B6AFF4.3090407@treenet.co.nz>
 <55BA475A.1070004@afo.net> <55BA976D.2060607@treenet.co.nz>
Message-ID: <55BAA7B2.9020802@afo.net>

On 7/30/2015 16:30 PM, Amos Jeffries wrote:
> On 31/07/2015 3:48 a.m., Mike wrote:
>> On 7/27/2015 17:25 PM, Amos Jeffries wrote:
>>> On 28/07/2015 8:38 a.m., Mike wrote:
>>>> Running into an issue, using the squid.conf entry
>>>> dns_nameservers 72.x.x.x 72.x.y.y
>>>>
>>>> These are different servers (under our control) for the purpose of
>>>> filtering than listed in resolv.conf (which are out of our control, used
>>>> for server IP routing by upstream host).
>>>>
>>>> The problem we found like this weekend is if the primary listed dns
>>>> server is unavailable, squid fails to use the secondary listed server.
>>>> Instead it displays the "unable to connect" type messages with all
>>>> websites.
>>> Details please. How do you know the secondary is not even being tried?
>>>
>>> What is Squid getting back from the primary when its "down" ?
>>>    or just dns_timeout being hit?
>>>
>>> Add this to squid.conf to get a cache.log trace of the DNS activity:
>>>     debug_options 78,6
>>>
>> Amos,
>>
>> If it was using the secondary server listed, connections to almost all
>> websites would not be failing to load if primary was down. For the test
>> we temporarily took the primary DNS server offline (per the example
>> above 72.x.x.x), and no websites would load unless it was in the squid
>> cache, but any elements that required additional data failed to load
>> causing formatting issues with the displayed website. If we swap the
>> setting to the "secondary" with the first IP (per the example above) as
>> dns_nameservers 72.x.y.y 72.x.x.x
>> and it works the same way, take the ".y.y" down and it refuses to use
>> the secondary listed IP ".x.x" for DNS, instead displays the website
>> could not be displayed error in the browsers. We even tried another test
>> (per the example above) dns_nameservers 72.x.x.x 8.8.8.8 then let it run
>> for an hour or so. Then we took down the primary which means it should
>> use the secondary google IP of 8.8.8.8, but it doesn't, goes right back
>> to the "website could not be displayed" error in the browsers.
>> I was wonder if this might be a bug. This is happening on multiple
>> servers, one has squid 3.4.7, another has 3.4.6 and problem occurs on both.
>>
> Thank you exactly the kind of answer I was looking for question #1.
> (Evidence that the problem is what you think it is before digging for a
> cause).
>
> Kind of answers Q2 a "nothing", implying that Q3 is "yes" dns_timeout is
> happening.
>
>
>   Is your dns_timeout (default 30 sec *total* DNS lookup timeout) larger
> than your dns_retransmit_interval (default 5 sec per-query timeout) setting?
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
We do not have a dns_timeout or retransmit entry in the squid.conf, only 
using the dns_nameservers entry allowing the default timeout timeframes 
since so few websites should take much longer than that to load, and if 
they are, it is a misspelled URL, a foreign server (which so few of our 
customers use), or likely having issues anyways.

I suspect this may be a bug with squid 3.4.x since this issue happened 
on 2 different squid servers, one is 3.4.6, another is 3.4.7. Yet on the 
backups to each, one has 3.5.1 and other has 3.5.6 (I updated it today), 
and they are not affected by this, both of these squid v3.5.x servers 
properly see the primary is not reachable and uses the secondary DNS IP.

Thanks Amos,


Mike



From squid3 at treenet.co.nz  Thu Jul 30 22:42:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Jul 2015 10:42:11 +1200
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
Message-ID: <55BAA843.5090001@treenet.co.nz>

On 31/07/2015 8:05 a.m., Josip Makarevic wrote:
> Hi,
> 
> I have a problem with squid setup (squid version 3.5.6, built from source,
> centos 6.6)
> I've tried 2 options:
> 1. SMP
> 2. NON-SMP
> 
> I've decided to stick with custom build non-smp version and the thing is:
> - i don't need cache - any kind of it

 cache_mem 0
 cache deny all

That is it. All other caches used by Squid *are* mandatory for good
performance. And are only used anyway when the component that needs them
is actively used.


> - I have DNS cache just for that
> - squid has to listen on 1024 ports on 23 instances.
> each instance listens on set of ports and each port has different outgoing
> ip address.

And how many NIC do you have that spread over?

> 
> The thing is this:
> It's alll good until we hit it with more than 150mbits then...
> 
> (output from perf top)
>  84.57%  [kernel]                  [k] osq_lock
>   4.62%  [kernel]                  [k] mutex_spin_on_owner
>   1.41%  [kernel]                  [k] memcpy
>   0.79%  [kernel]                  [k] inet_dump_ifaddr
>   0.62%  [kernel]                  [k] memset
> 
>  21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84, 23.33
> (yes, we have 24 cores)
> Same behavior is with SMP and NON-SMP setup (SMP setup is all in one file
> with workers 23 option but then I have to use rock cache)
> 
> so, my question is....what...how to optimize this.....whatever....I'm stuck
> for days, I've tried many sysctl options but none of them works.
> Any help, info, something else?

None of those are Squid functionality. If you want help optimizing your
config and are willing to post it to the list I am happy to do a quick
audit and point out any problem areas for you.

But tuning the internal locking code of the kernel is way off topic.

Amos



From squid3 at treenet.co.nz  Thu Jul 30 22:51:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Jul 2015 10:51:13 +1200
Subject: [squid-users] dns failover failing with 3.4.7
In-Reply-To: <55BAA7B2.9020802@afo.net>
References: <55B696C7.2050704@afo.net> <55B6AFF4.3090407@treenet.co.nz>
 <55BA475A.1070004@afo.net> <55BA976D.2060607@treenet.co.nz>
 <55BAA7B2.9020802@afo.net>
Message-ID: <55BAAA61.9050006@treenet.co.nz>

On 31/07/2015 10:39 a.m., Mike wrote:
> I suspect this may be a bug with squid 3.4.x since this issue happened
> on 2 different squid servers, one is 3.4.6, another is 3.4.7. Yet on the
> backups to each, one has 3.5.1 and other has 3.5.6 (I updated it today),
> and they are not affected by this, both of these squid v3.5.x servers
> properly see the primary is not reachable and uses the secondary DNS IP.
> 

Yes I agree with that conclusion. Good to know its fixed, whatever it
was. :-)

Cheers
Amos



From eliezer at ngtech.co.il  Thu Jul 30 23:01:49 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 31 Jul 2015 02:01:49 +0300
Subject: [squid-users] LDAP related question.
Message-ID: <55BAACDD.4020403@ngtech.co.il>

I wanted to test the ext_ldap_group_acl so I created a ldap domain.
The command I am testing is:
/usr/lib/squid3/ext_ldap_group_acl -b "DC=ngtech,DC=local" -D 
"CN=admin,DC=ngtech,DC=local" -w "password" -f 
"(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,DC=ngtech,DC=local))" 
-h 127.0.0.1

Now I have entered "user1 int" and it should to my understanding reply 
with OK but it return ERR:
user1 int
ext_ldap_group_acl.cc(587): pid=27778 :Connected OK
ext_ldap_group_acl.cc(726): pid=27778 :group filter 
'(&(objectclass=person)(sAMAccountName=user1)(memberof=CN=int,DC=ngtech,DC=local))', 
searchbase 'DC=ngtech,DC=local'
ERR


Now the ldap structure is like this:
DC=ngtech, DC=local
-> CN=int
member-->user1
-> OU=users
--> CN=user1
(Not such a great painter.)

I was wondering that since it works for others I am doing something 
wrong but unsure what.
I was thinking of maybe I am doing something wrong but the next ldap 
search works:
  ldapsearch -h 127.0.0.1 -x -b "dc=ngtech,dc=local" "(cn=int)" memberUid
# extended LDIF
#
# LDAPv3
# base <dc=ngtech,dc=local> with scope subtree
# filter: (cn=int)
# requesting: memberUid
#

# int, ngtech.local
dn: cn=int,dc=ngtech,dc=local
memberUid: user1

# search result
search: 2
result: 0 Success

# numResponses: 2
# numEntries: 1


So I am wondering what might be the cause for the issue? any ideas?

Eliezer



From jmakarevic at gmail.com  Fri Jul 31 06:52:34 2015
From: jmakarevic at gmail.com (Josip Makarevic)
Date: Fri, 31 Jul 2015 08:52:34 +0200
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <55BAA843.5090001@treenet.co.nz>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
 <55BAA843.5090001@treenet.co.nz>
Message-ID: <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>

Hi Amos,

 cache_mem 0
 cache deny all

already there.
Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
interface.

Well, entire config would be way too long but here is the static part:
via off
cpu_affinity_map process_numbers=1 cores=2
forwarded_for delete
visible_hostname squid1
pid_filename /var/run/squid1.pid
icp_port 0
htcp_port 0
icp_access deny all
htcp_access deny all
snmp_port 0
snmp_access deny all
dns_nameservers x.x.x.x
cache_mem 0
cache deny all
pipeline_prefetch on
memory_pools off
maximum_object_size 16 KB
maximum_object_size_in_memory 16 KB
ipcache_size 0
cache_store_log none
half_closed_clients off
include /etc/squid/rules
access_log /var/log/squid/squid1-access.log
cache_log /var/log/squid/squid1-cache.log
coredump_dir /var/spool/squid/squid1
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

acl port0 myport 30000
http_access allow testhost
tcp_outgoing_address x.x.x.x port0

include is there for basic ACL - safe ports and so on - to minimize config
file footprint since it's static and same for every worker.

and so on 44 more times in this config file

Do you know of any good article hot to tune kernel locking or have any idea
why is it happening?
I cannot find any good info on it and all I've found are bits and peaces of
kernel source code.


Tnx.
J.

2015-07-31 0:42 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 31/07/2015 8:05 a.m., Josip Makarevic wrote:
> > Hi,
> >
> > I have a problem with squid setup (squid version 3.5.6, built from
> source,
> > centos 6.6)
> > I've tried 2 options:
> > 1. SMP
> > 2. NON-SMP
> >
> > I've decided to stick with custom build non-smp version and the thing is:
> > - i don't need cache - any kind of it
>
>  cache_mem 0
>  cache deny all
>
> That is it. All other caches used by Squid *are* mandatory for good
> performance. And are only used anyway when the component that needs them
> is actively used.
>
>
> > - I have DNS cache just for that
> > - squid has to listen on 1024 ports on 23 instances.
> > each instance listens on set of ports and each port has different
> outgoing
> > ip address.
>
> And how many NIC do you have that spread over?
>
> >
> > The thing is this:
> > It's alll good until we hit it with more than 150mbits then...
> >
> > (output from perf top)
> >  84.57%  [kernel]                  [k] osq_lock
> >   4.62%  [kernel]                  [k] mutex_spin_on_owner
> >   1.41%  [kernel]                  [k] memcpy
> >   0.79%  [kernel]                  [k] inet_dump_ifaddr
> >   0.62%  [kernel]                  [k] memset
> >
> >  21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84, 23.33
> > (yes, we have 24 cores)
> > Same behavior is with SMP and NON-SMP setup (SMP setup is all in one file
> > with workers 23 option but then I have to use rock cache)
> >
> > so, my question is....what...how to optimize this.....whatever....I'm
> stuck
> > for days, I've tried many sysctl options but none of them works.
> > Any help, info, something else?
>
> None of those are Squid functionality. If you want help optimizing your
> config and are willing to post it to the list I am happy to do a quick
> audit and point out any problem areas for you.
>
> But tuning the internal locking code of the kernel is way off topic.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/5daf07cf/attachment.htm>

From gkinkie at gmail.com  Fri Jul 31 08:55:50 2015
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 31 Jul 2015 10:55:50 +0200
Subject: [squid-users] forward proxy - many users with one login/passwd.
In-Reply-To: <916606669CFF224AB6997E9DB783F6EA7E540187@ESCML200.corp.pjc.com>
References: <916606669CFF224AB6997E9DB783F6EA7E540187@ESCML200.corp.pjc.com>
Message-ID: <CA+Y8hcMpm0TWtj12_W9Bq81ys6+MuNehkhs2kqcHN_gHG_pcmg@mail.gmail.com>

On Thu, Jul 30, 2015 at 11:57 PM, Berkes, David <David.J.Berkes at pjc.com>
wrote:

>
> Just a basic question.  I have a 3.5.0.4 forward proxy setup with basic
> authentication for my MDM proxy (iphones).  All iphones are set with the
> global proxy and identical user-name/password.  They will be on an LTE
> network and will be switching IP's often.  The forward proxy
> user-name/password will always be the same from each iphone. I have read
> several things about (max_user_ip, authenticate_ip_ttl) and concerned with
> the setup.  I essentially don?t want to limit any number of source
> connections using the same credentials.   Please advise of any pitfalls
> and/or settings for many users switching IP's frequent, using the same
> login/passwd.
>
>
Hi,
  there's none that I can think of.

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/e908a6c5/attachment.htm>

From truniger at bluewin.ch  Fri Jul 31 09:05:39 2015
From: truniger at bluewin.ch (Othmar Truniger)
Date: Fri, 31 Jul 2015 09:05:39 +0000 (GMT+00:00)
Subject: [squid-users] LDAP related question.
Message-ID: <27722252.13211.1438333539140.JavaMail.webmail@bluewin.ch>

Regarding your filter:
- I think it should be %u instead of %v for user
- never heard of %a, usage says %v, man page says %g for group


From dan at djph.net  Fri Jul 31 09:45:43 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 31 Jul 2015 05:45:43 -0400
Subject: [squid-users] LDAP related question.
In-Reply-To: <55BAACDD.4020403@ngtech.co.il>
References: <55BAACDD.4020403@ngtech.co.il>
Message-ID: <20150731054543.Horde.Etf08GMv31HvEj4hhIaaAg1@192.168.10.20>

Quoting Eliezer Croitoru <eliezer at ngtech.co.il>:

> I wanted to test the ext_ldap_group_acl so I created a ldap domain.
> The command I am testing is:
> /usr/lib/squid3/ext_ldap_group_acl -b "DC=ngtech,DC=local" -D  
> "CN=admin,DC=ngtech,DC=local" -w "password" -f  
> "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,DC=ngtech,DC=local))" -h  
> 127.0.0.1


Looks like your command is a bit off.  Here's my LDAP one which works  
(some variant of squid3 -- I only have the config file on my local PC,  
and no over-the-internet access to this particular proxy, as at work).  
  Please note that I redacted the actual domain name, and replaced it  
with "example".

external_acl_type ldapgroup %LOGIN /usr/lib/squid3/ext_ldap_group_acl  
-b "ou=users,dc=example,dc=org" -D "cn=admin,dc=example,dc=org" -W  
/etc/squid3/pass.in -f  
(&(objectClass=*)(uid=%u)(memberof=cn=%g,ou=ldapGroups,dc=example,dc=org)) -h  
ldap.example.org

I was having trouble with the object class myself ... but the LDAP  
group is small (like 30 people, and nothing else like printers or  
anything), so having a "too big" objectClass base isn't the end of the  
world.

then the acls are pretty simple:

acl ldap-kids external ldapgroup kids
acl ldap-parents external ldapgroup parents

acl allow [...] kids
acl deny kids all <-- not 100% sure this one is necessary, but I'm  
also not 100% certain how squid reacts to a couple "allow" rules,  
followed by "allow" rules for a different group, but this seems to work.

acl allow [...] parents
acl deny ad_sites parents all <-- death to ads ;)

acl deny all

>
> Now I have entered "user1 int" and it should to my understanding  
> reply with OK but it return ERR:
> user1 int
> ext_ldap_group_acl.cc(587): pid=27778 :Connected OK
> ext_ldap_group_acl.cc(726): pid=27778 :group filter  
> '(&(objectclass=person)(sAMAccountName=user1)(memberof=CN=int,DC=ngtech,DC=local))', searchbase  
> 'DC=ngtech,DC=local'
> ERR
>
>
> Now the ldap structure is like this:
> DC=ngtech, DC=local
> -> CN=int
> member-->user1
> -> OU=users
> --> CN=user1
> (Not such a great painter.)


I think you're missing an OU in there, my LDAP server is ordered like this:

dc=example,dc=org
|
-> ou=ldapGroups
||
|-> cn=kids
|-> cn=parents
|
-> ou=users
  |
  -> cn=[user1]
  -> cn=[user2]
  -> [...]

How did you create things?  I found that using ldif files caused  
trouble (or at least the ones from the examples I had), whereas just  
installing phpldapadmin and poking around got me up and running in  
almost no time flat.

> [snip]
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4387 bytes
Desc: S/MIME Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/7ee2c75e/attachment.bin>

From eliezer at ngtech.co.il  Fri Jul 31 12:29:13 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 31 Jul 2015 15:29:13 +0300
Subject: [squid-users] LDAP related question.
In-Reply-To: <20150731054543.Horde.Etf08GMv31HvEj4hhIaaAg1@192.168.10.20>
References: <55BAACDD.4020403@ngtech.co.il>
 <20150731054543.Horde.Etf08GMv31HvEj4hhIaaAg1@192.168.10.20>
Message-ID: <55BB6A19.9020201@ngtech.co.il>

I managed to make it work!
I am using ubuntu 14.04.2 with openLDAP and phpldapadmin.
I have changed my server to look like yours and it still didn't work.
So what I did was this: I changed the command to:
/usr/lib/squid3/ext_ldap_group_acl -d -b "dc=ngtech,dc=local" -D 
"cn=admin,dc=ngtech,dc=local" -w password-f 
"(&(objectClass=*)(memberUid=%u)(cn=%g))" -h 127.0.0.1

Which actually works great.
I enter:"user1 parents" and it says OK.

I have been reading that there might be a reason that memberOf will not 
work as expected and was hoping someone here might know about it.

Thanks,
Eliezer

On 31/07/2015 12:45, Dan Purgert wrote:
> external_acl_type ldapgroup %LOGIN /usr/lib/squid3/ext_ldap_group_acl -b
> "ou=users,dc=example,dc=org" -D "cn=admin,dc=example,dc=org" -W
> /etc/squid3/pass.in -f
> (&(objectClass=*)(uid=%u)(memberof=cn=%g,ou=ldapGroups,dc=example,dc=org))
> -h ldap.example.org
>
> I was having trouble with the object class myself ... but the LDAP group
> is small (like 30 people, and nothing else like printers or anything),
> so having a "too big" objectClass base isn't the end of the world.



From dan at djph.net  Fri Jul 31 12:34:14 2015
From: dan at djph.net (Dan Purgert)
Date: Fri, 31 Jul 2015 08:34:14 -0400
Subject: [squid-users] LDAP related question.
In-Reply-To: <55BB6A19.9020201@ngtech.co.il>
References: <55BAACDD.4020403@ngtech.co.il>
 <20150731054543.Horde.Etf08GMv31HvEj4hhIaaAg1@192.168.10.20>
 <55BB6A19.9020201@ngtech.co.il>
Message-ID: <20150731083414.Horde.O7Y8v3AWuEI4ynTA-wxyvw1@192.168.10.20>

Quoting Eliezer Croitoru <eliezer at ngtech.co.il>:

> I managed to make it work!
> I am using ubuntu 14.04.2 with openLDAP and phpldapadmin.
> I have changed my server to look like yours and it still didn't work.
> So what I did was this: I changed the command to:
> /usr/lib/squid3/ext_ldap_group_acl -d -b "dc=ngtech,dc=local" -D  
> "cn=admin,dc=ngtech,dc=local" -w password-f  
> "(&(objectClass=*)(memberUid=%u)(cn=%g))" -h 127.0.0.1
>
> Which actually works great.
> I enter:"user1 parents" and it says OK.
>
> I have been reading that there might be a reason that memberOf will  
> not work as expected and was hoping someone here might know about it.
>


Oh right, I had to compile in(?) something to make "memberOf" play  
nice.  Don't remember if it was in slapd or squid though... would need  
to grab my setup notes from that server to see.

Glad to hear you got it working though!

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4387 bytes
Desc: S/MIME Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/1c4e5566/attachment.bin>

From bpk678 at gmail.com  Fri Jul 31 12:37:34 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Fri, 31 Jul 2015 08:37:34 -0400
Subject: [squid-users] LDAP related question.
Message-ID: <CAARxGthe4iUvydQgsDgf_7o8Ai-uf8HuYg2Kcc4Wbb_G+fkLEw@mail.gmail.com>

Pretty sure memberOf is an overlay you have to enable in openldap
On Jul 31, 2015 8:34 AM, "Dan Purgert" <dan at djph.net> wrote:

Quoting Eliezer Croitoru <eliezer at ngtech.co.il>:

I managed to make it work!
> I am using ubuntu 14.04.2 with openLDAP and phpldapadmin.
> I have changed my server to look like yours and it still didn't work.
> So what I did was this: I changed the command to:
> /usr/lib/squid3/ext_ldap_group_acl -d -b "dc=ngtech,dc=local" -D
> "cn=admin,dc=ngtech,dc=local" -w password-f
> "(&(objectClass=*)(memberUid=%u)(cn=%g))" -h 127.0.0.1
>
> Which actually works great.
> I enter:"user1 parents" and it says OK.
>
> I have been reading that there might be a reason that memberOf will not
> work as expected and was hoping someone here might know about it.
>
>

Oh right, I had to compile in(?) something to make "memberOf" play nice.
Don't remember if it was in slapd or squid though... would need to grab my
setup notes from that server to see.

Glad to hear you got it working though!


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/b735bdd7/attachment.htm>

From marcus.kool at urlfilterdb.com  Fri Jul 31 12:53:15 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 31 Jul 2015 09:53:15 -0300
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
 <55BAA843.5090001@treenet.co.nz>
 <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>
Message-ID: <55BB6FBB.1060002@urlfilterdb.com>

osq_lock is used in the kenel for the implementation of a mutex.
It is not clear which mutex so we can only guess.

Which version of the kernel and distro do you use?

Since mutexes are used by Squid SMP, I suggest to switch for now to Squid non-SMP.

What is the value of cpu_affinity_map in all config files?
You say they are static. But do you allocate each instance on a different core?
Does 'top' show that all CPUs are used?

Do you have 24 cores or 12 hyperthreaded cores?
In case you have 12 real cores, you might want to experiment with 12 instances of Squid and then try to upscale.

Make maximum_object_size large, a max size of 16K will prohibit the retrieval of objects larger than 16K.
I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be infinite and do not worry since
cache_mem is zero.

Marcus


On 07/31/2015 03:52 AM, Josip Makarevic wrote:
> Hi Amos,
>
>   cache_mem 0
>   cache deny all
>
> already there.
> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding interface.
>
> Well, entire config would be way too long but here is the static part:
> via off
> cpu_affinity_map process_numbers=1 cores=2
> forwarded_for delete
> visible_hostname squid1
> pid_filename /var/run/squid1.pid
> icp_port 0
> htcp_port 0
> icp_access deny all
> htcp_access deny all
> snmp_port 0
> snmp_access deny all
> dns_nameservers x.x.x.x
> cache_mem 0
> cache deny all
> pipeline_prefetch on
> memory_pools off
> maximum_object_size 16 KB
> maximum_object_size_in_memory 16 KB
> ipcache_size 0
> cache_store_log none
> half_closed_clients off
> include /etc/squid/rules
> access_log /var/log/squid/squid1-access.log
> cache_log /var/log/squid/squid1-cache.log
> coredump_dir /var/spool/squid/squid1
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> acl port0 myport 30000
> http_access allow testhost
> tcp_outgoing_address x.x.x.x port0
>
> include is there for basic ACL - safe ports and so on - to minimize config file footprint since it's static and same for every worker.
>
> and so on 44 more times in this config file
>
> Do you know of any good article hot to tune kernel locking or have any idea why is it happening?
> I cannot find any good info on it and all I've found are bits and peaces of kernel source code.
>
>
> Tnx.
> J.
>
> 2015-07-31 0:42 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>:
>
>     On 31/07/2015 8:05 a.m., Josip Makarevic wrote:
>     > Hi,
>     >
>     > I have a problem with squid setup (squid version 3.5.6, built from source,
>     > centos 6.6)
>     > I've tried 2 options:
>     > 1. SMP
>     > 2. NON-SMP
>     >
>     > I've decided to stick with custom build non-smp version and the thing is:
>     > - i don't need cache - any kind of it
>
>       cache_mem 0
>       cache deny all
>
>     That is it. All other caches used by Squid *are* mandatory for good
>     performance. And are only used anyway when the component that needs them
>     is actively used.
>
>
>     > - I have DNS cache just for that
>     > - squid has to listen on 1024 ports on 23 instances.
>     > each instance listens on set of ports and each port has different outgoing
>     > ip address.
>
>     And how many NIC do you have that spread over?
>
>     >
>     > The thing is this:
>     > It's alll good until we hit it with more than 150mbits then...
>     >
>     > (output from perf top)
>     >  84.57%  [kernel]                  [k] osq_lock
>     >   4.62%  [kernel]                  [k] mutex_spin_on_owner
>     >   1.41%  [kernel]                  [k] memcpy
>     >   0.79%  [kernel]                  [k] inet_dump_ifaddr
>     >   0.62%  [kernel]                  [k] memset
>     >
>     >  21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84, 23.33
>     > (yes, we have 24 cores)
>     > Same behavior is with SMP and NON-SMP setup (SMP setup is all in one file
>     > with workers 23 option but then I have to use rock cache)
>     >
>     > so, my question is....what...how to optimize this.....whatever....I'm stuck
>     > for days, I've tried many sysctl options but none of them works.
>     > Any help, info, something else?
>
>     None of those are Squid functionality. If you want help optimizing your
>     config and are willing to post it to the list I am happy to do a quick
>     audit and point out any problem areas for you.
>
>     But tuning the internal locking code of the kernel is way off topic.
>
>     Amos
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From jorgeley at gmail.com  Fri Jul 31 14:23:24 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 31 Jul 2015 11:23:24 -0300
Subject: [squid-users] ident ACL
Message-ID: <CAMeoTH=5UfhTVzvy+SqsGyFgfgrpo2yWhBOVhQ+J7JxQkC6vqw@mail.gmail.com>

Hi guys.
ident ACL was discontinued on Squid 3.5.6???
I didn't found it in compilation options and it's unknown by squid.conf
Any help?

--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/08d30cd5/attachment.htm>

From eliezer at ngtech.co.il  Fri Jul 31 14:31:03 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 31 Jul 2015 17:31:03 +0300
Subject: [squid-users] LDAP related question.
In-Reply-To: <CAARxGthe4iUvydQgsDgf_7o8Ai-uf8HuYg2Kcc4Wbb_G+fkLEw@mail.gmail.com>
References: <CAARxGthe4iUvydQgsDgf_7o8Ai-uf8HuYg2Kcc4Wbb_G+fkLEw@mail.gmail.com>
Message-ID: <55BB86A7.5030008@ngtech.co.il>

On 31/07/2015 15:37, brendan kearney wrote:
> Pretty sure memberOf is an overlay you have to enable in openldap

I have tried to use this:
http://www.schenkels.nl/2013/03/how-to-setup-openldap-with-memberof-overlay-ubuntu-12-04/

But it doesn't mention that you need to put the file in the scheme 
settings directory and also openLDAP would not survive a restart so I am 
open to understand how to do it.

Thanks,
Eliezer


From bpk678 at gmail.com  Fri Jul 31 14:36:18 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Fri, 31 Jul 2015 10:36:18 -0400
Subject: [squid-users] LDAP related question.
In-Reply-To: <55BB86A7.5030008@ngtech.co.il>
References: <CAARxGthe4iUvydQgsDgf_7o8Ai-uf8HuYg2Kcc4Wbb_G+fkLEw@mail.gmail.com>
 <55BB86A7.5030008@ngtech.co.il>
Message-ID: <CAARxGtiURnF_3xv6QX5cWqXUch=NdTgRw4dK4aG7Pr93-DinCA@mail.gmail.com>

Not near my gear and notes, but will get you what I have later.
On Jul 31, 2015 10:31 AM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

> On 31/07/2015 15:37, brendan kearney wrote:
>
>> Pretty sure memberOf is an overlay you have to enable in openldap
>>
>
> I have tried to use this:
>
> http://www.schenkels.nl/2013/03/how-to-setup-openldap-with-memberof-overlay-ubuntu-12-04/
>
> But it doesn't mention that you need to put the file in the scheme
> settings directory and also openLDAP would not survive a restart so I am
> open to understand how to do it.
>
> Thanks,
> Eliezer
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/2b8800a8/attachment.htm>

From jorgeley at gmail.com  Fri Jul 31 14:46:35 2015
From: jorgeley at gmail.com (Jorgeley Junior)
Date: Fri, 31 Jul 2015 11:46:35 -0300
Subject: [squid-users] ident ACL
In-Reply-To: <CAMeoTH=5UfhTVzvy+SqsGyFgfgrpo2yWhBOVhQ+J7JxQkC6vqw@mail.gmail.com>
References: <CAMeoTH=5UfhTVzvy+SqsGyFgfgrpo2yWhBOVhQ+J7JxQkC6vqw@mail.gmail.com>
Message-ID: <CAMeoTHn=qvQ=CEV6B2yBL4xW_x_Bmhn+1tdyMXeLkx+0xqjP7g@mail.gmail.com>

Hi guys, about the prior problem, I solved it, I was compiled with option
'--disable-ident-acl', thats why it was not running.
now I have another problem, my *ident acl* itsn't working, my purpose it's
enable access to cachemgr just to user JORGELEY, here is my conf:

auth_param basic program /etc/squid-3.5.6/libexec/basic_ncsa_auth
/regras/usuarios

auth_param basic children 10 startup=1 idle=1

auth_param basic realm INTERNET-LOGIN NECESSARIO


acl localnet src 192.168.0.0/16

acl jorgeley ident jorgeley

acl PURGE method PURGE

acl usuarios proxy_auth -i "regras/usuarios"

acl usuarios_liberados proxy_auth -i "regras/usuarios_liberados"

acl sem_delay_pool url_regex -i 192.168

acl com_delay_pool url_regex -i ftp .exe .mp3 .vqf .tar.gz .gz .rpm .zip
.rar .avi .mpeg .mpe .mpg .qt .ram .rm .iso .raw .wav .mov .ogg .mp4 .vob
.iso .flv .mkv youtube

acl palavras_proibidas url_regex -i "regras/palavras_proibidas"

acl palavras_liberadas url_regex -i "regras/palavras_liberadas"

acl dominios_proibidos dstdomain "regras/dominios_proibidos"

acl dominios_liberados dstdomain "regras/dominios_liberados"

acl ips_bloqueados src "regras/ips_bloqueados"

acl ips_liberados src "regras/ips_liberados"

acl conexoes maxconn 10

acl winupdate dstdomain .windowsupdate.com .microsoft.com

acl periodo_winupdate time SMTWHFA 8:00-18:00

acl youtube dstdomain .youtube.com

acl prefeitura dstdomain .rioverdegoias.com.br

acl SSL_ports port 443

acl CONNECT method CONNECT


http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow PURGE localhost

http_access deny PURGE

http_access allow localhost jorgeley manager

http_access deny manager

http_access allow usuarios_liberados

http_access allow localhost

http_access allow palavras_liberadas

http_access allow dominios_liberados

http_access deny palavras_proibidas

http_access deny dominios_proibidos

http_access deny conexoes localnet

http_access allow usuarios

http_access allow localnet

http_access deny all


reply_body_max_size 100 MB


http_port 192.168.0.254:8213


cache_mem 3000 MB


maximum_object_size_in_memory 2 MB


memory_cache_mode always


memory_replacement_policy heap GDSF


cache_replacement_policy heap LFUDA


minimum_object_size 0 KB


maximum_object_size 96 MB


cache_dir diskd /cache 7168 16 256 Q1=64 Q2=72

cache_dir diskd /cache 7168 16 256 Q1=64 Q2=72

cache_dir diskd /cache 7168 16 256 Q1=64 Q2=72

cache_dir diskd /cache 7168 16 256 Q1=64 Q2=72

cache_dir diskd /cache 7168 16 256 Q1=64 Q2=72


store_dir_select_algorithm least-load|round-robin


max_open_disk_fds 512000


cache_swap_low 96


cache_swap_high 97


access_log stdio:/var/logs/access.log squid


logfile_daemon /libexec/log_file_daemon


cache_store_log none


logfile_rotate 3


mime_table /etc/mime.conf


pid_filename /var/run/squid.pid


cache_log /var/logs/cache.log


debug_options ALL,1


coredump_dir /cache


ftp_user none


ftp_passive on


ftp_telnet_protocol off


diskd_program /libexec/diskd


unlinkd_program /libexec/unlinkd


cache deny youtube

cache deny prefeitura

cache deny localnet


refresh_pattern ^ftp: 1440 20% 10080

refresh_pattern ^gopher: 1440 0% 1440

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

refresh_pattern . 0 20% 4320

refresh_pattern -i ^http:\/\/www\.google\.com\/$ 0 20% 360 override-expire
override-lastmod ignore-reload ignore-no-cache ignore-no-store
reload-into-ims ignore-must-revalidate


quick_abort_min 1024 KB

quick_abort_max 2048 KB

quick_abort_pct 90



negative_ttl 10 seconds

negative_dns_ttl 30 seconds


range_offset_limit 0


request_header_max_size 2 KB

request_body_max_size 2 MB


ie_refresh off


connect_timeout 30 seconds

read_timeout 5 minutes

request_timeout 1 minutes


client_lifetime 1 day


cache_mgr jorgeleygpa at gmail.com

cache_effective_user squid

cache_effective_group squid


httpd_suppress_version_string on


visible_hostname firewall


delay_pools 2

delay_class 1 2

delay_class 2 2

delay_access 1 allow sem_delay_pool

delay_access 2 allow com_delay_pool

delay_parameters 1 -1/-1 -1/-1

delay_parameters 2 8000/8000 8000/8000


icon_directory /share/icons

error_directory /share/errors/pt-br

err_page_stylesheet /etc/errorpage.css

err_html_text mailto:jorgeleygpa at gmail.com

email_err_data on


deny_info ERR_ACCESS_DENIED dominios_proibidos palavras_proibidas


check_hostnames off


dns_nameservers 8.8.4.4 8.8.8.8


hosts_file /etc/hosts


client_db on


chroot /etc/squid-3.5.6


high_memory_warning 4000 MB


max_filedescriptors 512000


redirect_program /bannerfilter-1.31/redirector.pl


2015-07-31 11:23 GMT-03:00 Jorgeley Junior <jorgeley at gmail.com>:

> Hi guys.
> ident ACL was discontinued on Squid 3.5.6???
> I didn't found it in compilation options and it's unknown by squid.conf
> Any help?
>
> --
>
>
>


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/8ee3519e/attachment.htm>

From jmakarevic at gmail.com  Fri Jul 31 16:06:46 2015
From: jmakarevic at gmail.com (Josip Makarevic)
Date: Fri, 31 Jul 2015 18:06:46 +0200
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <55BB6FBB.1060002@urlfilterdb.com>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
 <55BAA843.5090001@treenet.co.nz>
 <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>
 <55BB6FBB.1060002@urlfilterdb.com>
Message-ID: <CAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g@mail.gmail.com>

Marcus, tnx for your info.
OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
Yes, cpu_affinity_map is good and with 6 instances there is load only on
first 6 cores and the server is 12 core, 24 HT
each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
and so on so that should not be the problem.
I've tried with 12 workers but that's even worse.

Let me try to explain:
on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
in that case, actual user time is about 10-20% and 70-80% is sys time
(osq_lock) and there are no connection timeouts.

If I switch to SMP 6 workers user time goes up but sys time goes up too and
there are connection timeouts and the load jumps to ~12.
If I give it more workers only load jumps and more connections are being
dropped to the point that load goes to 23/24 and the entire server is slow
as hell.

So, best performance so far are with 6 non-smp workers.

For now I have 2 options:
1. Install older squid (3.1.10 centos repo) and try it then
2. build custom 64bit kernel with RCU and specific cpu family support (in
progress).

The end idea is to be able to sustain 1gig of traffic on this server :)
Any advice is welcome


J.

2015-07-31 14:53 GMT+02:00 Marcus Kool <marcus.kool at urlfilterdb.com>:

> osq_lock is used in the kenel for the implementation of a mutex.
> It is not clear which mutex so we can only guess.
>
> Which version of the kernel and distro do you use?
>
> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
> non-SMP.
>
> What is the value of cpu_affinity_map in all config files?
> You say they are static. But do you allocate each instance on a different
> core?
> Does 'top' show that all CPUs are used?
>
> Do you have 24 cores or 12 hyperthreaded cores?
> In case you have 12 real cores, you might want to experiment with 12
> instances of Squid and then try to upscale.
>
> Make maximum_object_size large, a max size of 16K will prohibit the
> retrieval of objects larger than 16K.
> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
> infinite and do not worry since
> cache_mem is zero.
>
> Marcus
>
>
>
> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
>
>> Hi Amos,
>>
>>   cache_mem 0
>>   cache deny all
>>
>> already there.
>> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
>> interface.
>>
>> Well, entire config would be way too long but here is the static part:
>> via off
>> cpu_affinity_map process_numbers=1 cores=2
>> forwarded_for delete
>> visible_hostname squid1
>> pid_filename /var/run/squid1.pid
>> icp_port 0
>> htcp_port 0
>> icp_access deny all
>> htcp_access deny all
>> snmp_port 0
>> snmp_access deny all
>> dns_nameservers x.x.x.x
>> cache_mem 0
>> cache deny all
>> pipeline_prefetch on
>> memory_pools off
>> maximum_object_size 16 KB
>> maximum_object_size_in_memory 16 KB
>> ipcache_size 0
>> cache_store_log none
>> half_closed_clients off
>> include /etc/squid/rules
>> access_log /var/log/squid/squid1-access.log
>> cache_log /var/log/squid/squid1-cache.log
>> coredump_dir /var/spool/squid/squid1
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>> acl port0 myport 30000
>> http_access allow testhost
>> tcp_outgoing_address x.x.x.x port0
>>
>> include is there for basic ACL - safe ports and so on - to minimize
>> config file footprint since it's static and same for every worker.
>>
>> and so on 44 more times in this config file
>>
>> Do you know of any good article hot to tune kernel locking or have any
>> idea why is it happening?
>> I cannot find any good info on it and all I've found are bits and peaces
>> of kernel source code.
>>
>>
>> Tnx.
>> J.
>>
>> 2015-07-31 0:42 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz <mailto:
>> squid3 at treenet.co.nz>>:
>>
>>
>>     On 31/07/2015 8:05 a.m., Josip Makarevic wrote:
>>     > Hi,
>>     >
>>     > I have a problem with squid setup (squid version 3.5.6, built from
>> source,
>>     > centos 6.6)
>>     > I've tried 2 options:
>>     > 1. SMP
>>     > 2. NON-SMP
>>     >
>>     > I've decided to stick with custom build non-smp version and the
>> thing is:
>>     > - i don't need cache - any kind of it
>>
>>       cache_mem 0
>>       cache deny all
>>
>>     That is it. All other caches used by Squid *are* mandatory for good
>>     performance. And are only used anyway when the component that needs
>> them
>>     is actively used.
>>
>>
>>     > - I have DNS cache just for that
>>     > - squid has to listen on 1024 ports on 23 instances.
>>     > each instance listens on set of ports and each port has different
>> outgoing
>>     > ip address.
>>
>>     And how many NIC do you have that spread over?
>>
>>     >
>>     > The thing is this:
>>     > It's alll good until we hit it with more than 150mbits then...
>>     >
>>     > (output from perf top)
>>     >  84.57%  [kernel]                  [k] osq_lock
>>     >   4.62%  [kernel]                  [k] mutex_spin_on_owner
>>     >   1.41%  [kernel]                  [k] memcpy
>>     >   0.79%  [kernel]                  [k] inet_dump_ifaddr
>>     >   0.62%  [kernel]                  [k] memset
>>     >
>>     >  21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84,
>> 23.33
>>     > (yes, we have 24 cores)
>>     > Same behavior is with SMP and NON-SMP setup (SMP setup is all in
>> one file
>>     > with workers 23 option but then I have to use rock cache)
>>     >
>>     > so, my question is....what...how to optimize
>> this.....whatever....I'm stuck
>>     > for days, I've tried many sysctl options but none of them works.
>>     > Any help, info, something else?
>>
>>     None of those are Squid functionality. If you want help optimizing
>> your
>>     config and are willing to post it to the list I am happy to do a quick
>>     audit and point out any problem areas for you.
>>
>>     But tuning the internal locking code of the kernel is way off topic.
>>
>>     Amos
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org <mailto:
>> squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/1f5a4294/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 31 17:32:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Aug 2015 05:32:15 +1200
Subject: [squid-users] forward proxy - many users with one login/passwd.
In-Reply-To: <CA+Y8hcMpm0TWtj12_W9Bq81ys6+MuNehkhs2kqcHN_gHG_pcmg@mail.gmail.com>
References: <916606669CFF224AB6997E9DB783F6EA7E540187@ESCML200.corp.pjc.com>
 <CA+Y8hcMpm0TWtj12_W9Bq81ys6+MuNehkhs2kqcHN_gHG_pcmg@mail.gmail.com>
Message-ID: <55BBB11F.9010000@treenet.co.nz>

On 31/07/2015 8:55 p.m., Kinkie wrote:
> On Thu, Jul 30, 2015 at 11:57 PM, Berkes, David <David.J.Berkes at pjc.com>
> wrote:
> 
>>
>> Just a basic question.  I have a 3.5.0.4 forward proxy setup with basic
>> authentication for my MDM proxy (iphones).  All iphones are set with the
>> global proxy and identical user-name/password.  They will be on an LTE
>> network and will be switching IP's often.  The forward proxy
>> user-name/password will always be the same from each iphone. I have read
>> several things about (max_user_ip, authenticate_ip_ttl) and concerned with
>> the setup.  I essentially don?t want to limit any number of source
>> connections using the same credentials.   Please advise of any pitfalls
>> and/or settings for many users switching IP's frequent, using the same
>> login/passwd.
>>
>>
> Hi,
>   there's none that I can think of.
> 

Indeed.

HTTP authentication has to re-authenticate on every single request -
even within a persistent connection. It is naturally independent of IP
unless you force them into a relationship.

That forcing is what all the max-IP and user-IP external ACL helpers are
for. Simply dont use them and you will be fine even if each TCP
connection has unique IP addressing.

Amos



From David.J.Berkes at pjc.com  Fri Jul 31 17:52:44 2015
From: David.J.Berkes at pjc.com (Berkes, David)
Date: Fri, 31 Jul 2015 17:52:44 +0000
Subject: [squid-users] forward proxy - many users with one login/passwd.
In-Reply-To: <55BBB11F.9010000@treenet.co.nz>
References: <916606669CFF224AB6997E9DB783F6EA7E540187@ESCML200.corp.pjc.com>
 <CA+Y8hcMpm0TWtj12_W9Bq81ys6+MuNehkhs2kqcHN_gHG_pcmg@mail.gmail.com>
 <55BBB11F.9010000@treenet.co.nz>
Message-ID: <916606669CFF224AB6997E9DB783F6EA7E54043B@ESCML200.corp.pjc.com>

Thanks again.  That?s what I was looking to clarify!

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, July 31, 2015 12:32 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] forward proxy - many users with one login/passwd.

On 31/07/2015 8:55 p.m., Kinkie wrote:
> On Thu, Jul 30, 2015 at 11:57 PM, Berkes, David
> <David.J.Berkes at pjc.com>
> wrote:
>
>>
>> Just a basic question.  I have a 3.5.0.4 forward proxy setup with
>> basic authentication for my MDM proxy (iphones).  All iphones are set
>> with the global proxy and identical user-name/password.  They will be
>> on an LTE network and will be switching IP's often.  The forward
>> proxy user-name/password will always be the same from each iphone. I
>> have read several things about (max_user_ip, authenticate_ip_ttl) and
>> concerned with the setup.  I essentially don?t want to limit any number of source
>> connections using the same credentials.   Please advise of any pitfalls
>> and/or settings for many users switching IP's frequent, using the
>> same login/passwd.
>>
>>
> Hi,
>   there's none that I can think of.
>

Indeed.

HTTP authentication has to re-authenticate on every single request - even within a persistent connection. It is naturally independent of IP unless you force them into a relationship.

That forcing is what all the max-IP and user-IP external ACL helpers are for. Simply dont use them and you will be fine even if each TCP connection has unique IP addressing.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
________________________________


Piper Jaffray & Co. Since 1895. Member SIPC and NYSE. Learn more at www.piperjaffray.com. Piper Jaffray corporate headquarters is located at 800 Nicollet Mall, Minneapolis, MN 55402.

Piper Jaffray outgoing and incoming e-mail is electronically archived and recorded and is subject to review, monitoring and/or disclosure to someone other than the recipient. This e-mail may be considered an advertisement or solicitation for purposes of regulation of commercial electronic mail messages. If you do not wish to receive commercial e-mail communications from Piper Jaffray, go to: www.piperjaffray.com/do_not_email to review the details and submit your request to be added to the Piper Jaffray "Do Not E-mail Registry." For additional disclosure information see www.piperjaffray.com/disclosures

From squid3 at treenet.co.nz  Fri Jul 31 18:56:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Aug 2015 06:56:03 +1200
Subject: [squid-users] squid centos and osq_lock
In-Reply-To: <CAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g@mail.gmail.com>
References: <CAAg-RYsmVXi+Kb+rSBvDzyVW-cmH2_EhbMbXCf0Gb1KwxoOg0w@mail.gmail.com>
 <55BAA843.5090001@treenet.co.nz>
 <CAAg-RYsvPKLeMP31evCGshROxNF1VEsNUK2OJMJLEgfrzV-krw@mail.gmail.com>
 <55BB6FBB.1060002@urlfilterdb.com>
 <CAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g@mail.gmail.com>
Message-ID: <55BBC4C3.3090801@treenet.co.nz>

On 1/08/2015 4:06 a.m., Josip Makarevic wrote:
> Marcus, tnx for your info.
> OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
> Yes, cpu_affinity_map is good and with 6 instances there is load only on
> first 6 cores and the server is 12 core, 24 HT

Then I suspect that mutex and locking will be the kernel scheduling work
on the HT cores.
 In high performance Squid will max out a physical cores worth of
cycles. HT essentially tries to over-clock physical cores. But trying to
reach 200% capacity into a physical core with Squid workloads only leads
to trouble.
 It is far better to tie Squid with affinity to one instance per
physical core and let the extra HT capacity be available to the OS and
other supporting things the Squid instance needs to have happen externally.


> each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
> and so on so that should not be the problem.
> I've tried with 12 workers but that's even worse.

You do need to be very careful about which core numbers are the HT core
vs the physical core ID. Last time I saw anyone doing it, every second
number was a real physical core ID. YMMV.

> 
> Let me try to explain:
> on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
> in that case, actual user time is about 10-20% and 70-80% is sys time
> (osq_lock) and there are no connection timeouts.
> 
> If I switch to SMP 6 workers user time goes up but sys time goes up too and
> there are connection timeouts and the load jumps to ~12.
> If I give it more workers only load jumps and more connections are being
> dropped to the point that load goes to 23/24 and the entire server is slow
> as hell.
> 
> So, best performance so far are with 6 non-smp workers.
> 
> For now I have 2 options:
> 1. Install older squid (3.1.10 centos repo) and try it then
> 2. build custom 64bit kernel with RCU and specific cpu family support (in
> progress).
> 
> The end idea is to be able to sustain 1gig of traffic on this server :)
> Any advice is welcome

I agree with Marcus then. The non-SMP then is the way to go at present.
The main benefit of SMP support in current Squid is for caching
de-duplication (ie rock store).


Also some things to note:

* a good percentage of the speed of Squid is the 20-40% caching HIT rate
normal HTTP traffic has. Albeit memory-only caching on highest
performance boxen. Memory hits are 4-6 orders of magnitude faster than
network fetches. This has little to do with anything you can control
(normally). The (relatively) slow speed of origin servers creating the
content is the bottleneck. Even "static" content may be encoded to the
clients requested desire on each fetch, which takes time.


* Going by out lab tests and real-world results so far I rate Squid
per-worker at ~50Mbps on 3.1GHz core, and ~70Mbps on 3.7GHz. Your 12
cores will only get you up around 800 Mbits IMHO (thats after tuning). I
would gladly be proven wrong though :-)


* Squid effectively *polls* all the listening ports every 10ms or once
every 10 I/O events (whichever is faster). So running with 1024
listening ports is a bit counter-productive, more time could be spent
checking those ports than doing work.
 That said going from one to multiple listening ports does make a speed
improvement. Finding the sweet spot between those trends is something
else to tune for.
 <http://wiki.squid-cache.org/MultipleInstances#Tips>


> 2015-07-31 14:53 GMT+02:00 Marcus Kool:
> 
>> osq_lock is used in the kenel for the implementation of a mutex.
>> It is not clear which mutex so we can only guess.
>>
>> Which version of the kernel and distro do you use?
>>
>> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
>> non-SMP.
>>
>> What is the value of cpu_affinity_map in all config files?
>> You say they are static. But do you allocate each instance on a different
>> core?
>> Does 'top' show that all CPUs are used?
>>
>> Do you have 24 cores or 12 hyperthreaded cores?
>> In case you have 12 real cores, you might want to experiment with 12
>> instances of Squid and then try to upscale.
>>
>> Make maximum_object_size large, a max size of 16K will prohibit the
>> retrieval of objects larger than 16K.
>> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
>> infinite and do not worry since
>> cache_mem is zero.
>>
>> Marcus
>>
>>
>>
>> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
>>
>>> Hi Amos,
>>>
>>>   cache_mem 0
>>>   cache deny all
>>>
>>> already there.
>>> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
>>> interface.
>>>
>>> Well, entire config would be way too long but here is the static part:
>>> via off
>>> cpu_affinity_map process_numbers=1 cores=2
>>> forwarded_for delete
>>> visible_hostname squid1
>>> pid_filename /var/run/squid1.pid

Remove these...

>>> icp_port 0
>>> htcp_port 0
>>> icp_access deny all
>>> htcp_access deny all
>>> snmp_port 0
>>> snmp_access deny all

... to here. They do nothing but slow Squid-3 down.

>>> dns_nameservers x.x.x.x
>>> cache_mem 0
>>> cache deny all
>>> pipeline_prefetch on

In Squid-3.4 and later this is set to the length of pipeline you want to
accept.

NP: 'on' traditionally has meant pipeline length of 1 (two parallel
requests). Longer lengths are not yet well tested but generally it seems
to work okay.


>>> memory_pools off
>>> maximum_object_size 16 KB
>>> maximum_object_size_in_memory 16 KB

Like Marcus said. Without even memory caching these two have no useful
effects.

There is one related setting "read_ahead_gap" which affects performance
by tuning the amount of undelivered object data Squid will buffer in
transient memory. Higher value for that mean faster servers can finish
sending earlier and resources for them released for other uses.
 Tuning this is a fine art since it modulates how much Squid internal
buffers (and pipieline prefetching) read off TCP buffers. And all of
those buffers have limits of their own and may contain multiple requests
data.


>>> ipcache_size 0

Remove this. Without IP cache Squid will be forced to do about 4x remote
DNS lookup for every single HTTP request - *minimum*. Maybe more if you
apply any access controls to the traffic.
 If anything increase the ipcache size to store more results.


>>> cache_store_log none

Not needed in Squid-3. You can remove.

>>> half_closed_clients off
>>> include /etc/squid/rules
>>> access_log /var/log/squid/squid1-access.log

Logging I/O slows Squid down. I suggest making that a daemon, TCP or UDP
log output.


>>> cache_log /var/log/squid/squid1-cache.log
>>> coredump_dir /var/spool/squid/squid1
>>> refresh_pattern ^ftp:           1440    20%     10080
>>> refresh_pattern ^gopher:        1440    0%      1440
>>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>> refresh_pattern .               0       20%     4320

Without caching you can remove these *entirely*.

>>>
>>> acl port0 myport 30000

Mumble. Less reliable than myportname, but it is infintessimally faster
when it does work at all.

>>> http_access allow testhost
>>> tcp_outgoing_address x.x.x.x port0
>>>
>>> include is there for basic ACL - safe ports and so on - to minimize
>>> config file footprint since it's static and same for every worker.
>>>
>>> and so on 44 more times in this config file

Only put allow testhost once. Every time you test ACLs Squid slows down.

Some ACLs are worse drag than others. You can probably optimize even the
default recommended security settings you shuffled into "rules" file to
operate better.


>>>
>>> Do you know of any good article hot to tune kernel locking or have any
>>> idea why is it happening?
>>> I cannot find any good info on it and all I've found are bits and peaces
>>> of kernel source code.

Sorry no. All I found was the same.

Though I do know that one of the big differences between Linux 2.6 and
3.0 was the removal of the "Big Kernel Lock" system that allowed Linux
to run on multi-core systems properly. It could be CentOS 6 itelf biting
you with its ancient kernel version.


Amos


From stan.prescott at gmail.com  Fri Jul 31 20:49:42 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 31 Jul 2015 15:49:42 -0500
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <55A8837A.1080901@treenet.co.nz>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <55A8837A.1080901@treenet.co.nz>
Message-ID: <CANLNtGQwEyrXuyEbd+MjJqSLt3EdBqv8F1sbVsi2py36COe9YA@mail.gmail.com>

Hi Amos. I wanted to try out the "ssl-bump splice" to send traffic to a
peer found in the recent snapshots for 3.5.6/7 to block Google images. I
compiled configured and installed the latest 3.5 snapshot and added the
directives you listed above to squid.conf but I am not sure I got them
right.
























*acl s1_tls_connect      at_step SslBump1acl s2_tls_client_hello at_step
SslBump2acl s3_tls_server_hello at_step SslBump3acl tls_server_name_is_ip
ssl::server_name_regex ^[0-9]+.[0-9]+.[0-9]+.[0-9]+nacl google
ssl::server_name .google.com <http://google.com>ssl_bump peek
s1_tls_connect      allacl nobumpSites ssl::server_name .wellsfargo.com
<http://wellsfargo.com>ssl_bump splice s2_tls_client_hello
nobumpSitesssl_bump splice s2_tls_client_hello googlessl_bump stare
s2_tls_client_hello allssl_bump bump  s3_tls_server_hello allcache_peer
forcesafesearch.google.com <http://forcesafesearch.google.com> parent 443 0
name=GS originserver no-query no-netdb-exchange no-digestacl search
dstdomain .google.com <http://google.com>cache_peer_access GS allow
searchcache_peer_access GS deny allsslproxy_cert_error allow
tls_server_name_is_ipsslproxy_cert_error deny allsslproxy_flags
DONT_VERIFY_PEER*
When restarting Squid and searching in Google images for "sex" it still
shows images that I want to be able to block with safesearch.

On Thu, Jul 16, 2015 at 11:24 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 19/05/2015 5:49 a.m., Andres Granados wrote:
> > hello!I need help on how to block pornographic images of google, I
> > was trying different options and still do not succeed, try:
> > http_reply_access with request_header_add, and even with a
> > configuration dns, I think is to request_header_add the best, though
> > not it has worked for me, I hope your help, is to implement a school,
> > thanks!
> >
>
> FYI; Christos has added a tweak to the "ssl-bump splice" handling that
> permits sending the traffic to a cache_peer configured something like this:
>
>  acl example ssl::server_name .example.com
>  ssl_bump splice example
>  ssl_bump peek all
>
>  cache_peer forcesafesearch.example.com parent 443 0 \
>     name=GS \
>     originserver no-query no-netdb-exchange no-digest
>
>  acl search dstdomain .example.com
>  cache_peer_access GS allow search
>  cache_peer_access GS deny all
>
> The idea being that you can use this on intercepted (or forward-proxy)
> HTTPS traffic instead of hacking about with DNS to direct clients at the
> servers Google use to present "safe" searching.
>
> This should be available in 3.5.7, or the current 3.5 snaphots.
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/67558ba0/attachment.htm>

From bpk678 at gmail.com  Fri Jul 31 21:18:23 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Fri, 31 Jul 2015 17:18:23 -0400
Subject: [squid-users] LDAP related question.
In-Reply-To: <20150731083414.Horde.O7Y8v3AWuEI4ynTA-wxyvw1@192.168.10.20>
References: <55BAACDD.4020403@ngtech.co.il>
 <20150731054543.Horde.Etf08GMv31HvEj4hhIaaAg1@192.168.10.20>
 <55BB6A19.9020201@ngtech.co.il>
 <20150731083414.Horde.O7Y8v3AWuEI4ynTA-wxyvw1@192.168.10.20>
Message-ID: <55BBE61F.6030409@gmail.com>

On 07/31/2015 08:34 AM, Dan Purgert wrote:
> Quoting Eliezer Croitoru <eliezer at ngtech.co.il>:
>
>> I managed to make it work!
>> I am using ubuntu 14.04.2 with openLDAP and phpldapadmin.
>> I have changed my server to look like yours and it still didn't work.
>> So what I did was this: I changed the command to:
>> /usr/lib/squid3/ext_ldap_group_acl -d -b "dc=ngtech,dc=local" -D 
>> "cn=admin,dc=ngtech,dc=local" -w password-f 
>> "(&(objectClass=*)(memberUid=%u)(cn=%g))" -h 127.0.0.1
>>
>> Which actually works great.
>> I enter:"user1 parents" and it says OK.
>>
>> I have been reading that there might be a reason that memberOf will 
>> not work as expected and was hoping someone here might know about it.
>>
>
>
> Oh right, I had to compile in(?) something to make "memberOf" play 
> nice.  Don't remember if it was in slapd or squid though... would need 
> to grab my setup notes from that server to see.
>
> Glad to hear you got it working though!
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
since you have phpLDAPAdmin, my exports should be a near 1:1 import for you.

load the module:

dn: cn=module{2},cn=config #<-- adjust the number between { and } to 
your env
cn: module{2}  # <-- same adjustment as above
objectclass: olcModuleList
objectclass: top
olcmoduleload: {0}memberof.la  # <-- this is 0 because its the first 
module loaded in this cn
olcmodulepath: /usr/lib64/openldap #<-- adjust for your env, this where 
fedora places the *.la files; memberof.la should be in this dir

load the overlay into the database (not the DIT):

dn: olcOverlay={2}memberof,olcDatabase={2}mdb,cn=config  #<-- again 
adjust for your env  it is coincidence that both #s are 2 in my env.
objectclass: olcOverlayConfig
objectclass: olcMemberOf
objectclass: top
olcmemberofrefint: TRUE
olcoverlay: {2}memberof  # <-- adjust for your env, too

i will send screenshots from my phpLDAPAdmin to you off list












-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/6e40090d/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 31 21:19:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Aug 2015 09:19:10 +1200
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <CANLNtGQwEyrXuyEbd+MjJqSLt3EdBqv8F1sbVsi2py36COe9YA@mail.gmail.com>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <55A8837A.1080901@treenet.co.nz>
 <CANLNtGQwEyrXuyEbd+MjJqSLt3EdBqv8F1sbVsi2py36COe9YA@mail.gmail.com>
Message-ID: <55BBE64E.5070302@treenet.co.nz>

On 1/08/2015 8:49 a.m., Stanford Prescott wrote:
> Hi Amos. I wanted to try out the "ssl-bump splice" to send traffic to a
> peer found in the recent snapshots for 3.5.6/7 to block Google images. I
> compiled configured and installed the latest 3.5 snapshot and added the
> directives you listed above to squid.conf but I am not sure I got them
> right.
> 
> 
> acl s1_tls_connect      at_step SslBump1
> acl s2_tls_client_hello at_step SslBump2
> acl s3_tls_server_hello at_step SslBump3
> acl tls_server_name_is_ip ssl::server_name_regex ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
> acl google ssl::server_name .google.com
>
> ssl_bump peek s1_tls_connect      all
> acl nobumpSites ssl::server_name .wellsfargo.com
> ssl_bump splice s2_tls_client_hello nobumpSites
> ssl_bump splice s2_tls_client_hello google
> ssl_bump stare s2_tls_client_hello all
> ssl_bump bump  s3_tls_server_hello all
>
> cache_peer forcesafesearch.google.com parent 443 0 \
> name=GS originserver no-query no-netdb-exchange no-digest

Sorry, I missed out the 'ssl' option on the peer.

> acl search dstdomain .google.com
> cache_peer_access GS allow search
>
cache_peer_access GS deny all
> sslproxy_cert_error allow tls_server_name_is_ip
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
>
> When restarting Squid and searching in Google images for "sex" it still
> shows images that I want to be able to block with safesearch.

Other than the it I missed out mentioning. it looks okay to me. Though I
have not tested any of this myself so YMMV.

Amos

> 
> On Thu, Jul 16, 2015 at 11:24 PM, Amos Jeffries wrote:
> 
>> On 19/05/2015 5:49 a.m., Andres Granados wrote:
>>> hello!I need help on how to block pornographic images of google, I
>>> was trying different options and still do not succeed, try:
>>> http_reply_access with request_header_add, and even with a
>>> configuration dns, I think is to request_header_add the best, though
>>> not it has worked for me, I hope your help, is to implement a school,
>>> thanks!
>>>
>>
>> FYI; Christos has added a tweak to the "ssl-bump splice" handling that
>> permits sending the traffic to a cache_peer configured something like this:
>>
>>  acl example ssl::server_name .example.com
>>  ssl_bump splice example
>>  ssl_bump peek all
>>
>>  cache_peer forcesafesearch.example.com parent 443 0 \
>>     name=GS \
>>     originserver no-query no-netdb-exchange no-digest
>>
>>  acl search dstdomain .example.com
>>  cache_peer_access GS allow search
>>  cache_peer_access GS deny all
>>
>> The idea being that you can use this on intercepted (or forward-proxy)
>> HTTPS traffic instead of hacking about with DNS to direct clients at the
>> servers Google use to present "safe" searching.
>>
>> This should be available in 3.5.7, or the current 3.5 snaphots.
>>
>> Cheers
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From luis.daniel.lucio at gmail.com  Fri Jul 31 23:34:23 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Fri, 31 Jul 2015 17:34:23 -0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <55BBE64E.5070302@treenet.co.nz>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <55A8837A.1080901@treenet.co.nz>
 <CANLNtGQwEyrXuyEbd+MjJqSLt3EdBqv8F1sbVsi2py36COe9YA@mail.gmail.com>
 <55BBE64E.5070302@treenet.co.nz>
Message-ID: <CAFLo2QymyReQhzgRh5j3vQADqJaLuShnTZ8sxN1_-2Lmngjmvw@mail.gmail.com>

There is a project for icap that does exactly what you want. This is like a
L8 filter, meanwhile dns is L5.

The higher, the better
On Jul 31, 2015 5:20 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 1/08/2015 8:49 a.m., Stanford Prescott wrote:
> > Hi Amos. I wanted to try out the "ssl-bump splice" to send traffic to a
> > peer found in the recent snapshots for 3.5.6/7 to block Google images. I
> > compiled configured and installed the latest 3.5 snapshot and added the
> > directives you listed above to squid.conf but I am not sure I got them
> > right.
> >
> >
> > acl s1_tls_connect      at_step SslBump1
> > acl s2_tls_client_hello at_step SslBump2
> > acl s3_tls_server_hello at_step SslBump3
> > acl tls_server_name_is_ip ssl::server_name_regex
> ^[0-9]+.[0-9]+.[0-9]+.[0-9]+n
> > acl google ssl::server_name .google.com
> >
> > ssl_bump peek s1_tls_connect      all
> > acl nobumpSites ssl::server_name .wellsfargo.com
> > ssl_bump splice s2_tls_client_hello nobumpSites
> > ssl_bump splice s2_tls_client_hello google
> > ssl_bump stare s2_tls_client_hello all
> > ssl_bump bump  s3_tls_server_hello all
> >
> > cache_peer forcesafesearch.google.com parent 443 0 \
> > name=GS originserver no-query no-netdb-exchange no-digest
>
> Sorry, I missed out the 'ssl' option on the peer.
>
> > acl search dstdomain .google.com
> > cache_peer_access GS allow search
> >
> cache_peer_access GS deny all
> > sslproxy_cert_error allow tls_server_name_is_ip
> > sslproxy_cert_error deny all
> > sslproxy_flags DONT_VERIFY_PEER
> >
> > When restarting Squid and searching in Google images for "sex" it still
> > shows images that I want to be able to block with safesearch.
>
> Other than the it I missed out mentioning. it looks okay to me. Though I
> have not tested any of this myself so YMMV.
>
> Amos
>
> >
> > On Thu, Jul 16, 2015 at 11:24 PM, Amos Jeffries wrote:
> >
> >> On 19/05/2015 5:49 a.m., Andres Granados wrote:
> >>> hello!I need help on how to block pornographic images of google, I
> >>> was trying different options and still do not succeed, try:
> >>> http_reply_access with request_header_add, and even with a
> >>> configuration dns, I think is to request_header_add the best, though
> >>> not it has worked for me, I hope your help, is to implement a school,
> >>> thanks!
> >>>
> >>
> >> FYI; Christos has added a tweak to the "ssl-bump splice" handling that
> >> permits sending the traffic to a cache_peer configured something like
> this:
> >>
> >>  acl example ssl::server_name .example.com
> >>  ssl_bump splice example
> >>  ssl_bump peek all
> >>
> >>  cache_peer forcesafesearch.example.com parent 443 0 \
> >>     name=GS \
> >>     originserver no-query no-netdb-exchange no-digest
> >>
> >>  acl search dstdomain .example.com
> >>  cache_peer_access GS allow search
> >>  cache_peer_access GS deny all
> >>
> >> The idea being that you can use this on intercepted (or forward-proxy)
> >> HTTPS traffic instead of hacking about with DNS to direct clients at the
> >> servers Google use to present "safe" searching.
> >>
> >> This should be available in 3.5.7, or the current 3.5 snaphots.
> >>
> >> Cheers
> >> Amos
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/45c5bb9e/attachment.htm>

