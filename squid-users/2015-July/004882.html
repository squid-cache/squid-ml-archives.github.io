<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] squid centos and osq_lock
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3CCAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="004877.html">
   <LINK REL="Next"  HREF="004885.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] squid centos and osq_lock</H1>
    <B>Josip Makarevic</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3CCAAg-RYuB9eLmiv0RHis_MGf8MkC1enTMEW3vTsYwayz8ZrV77g%40mail.gmail.com%3E"
       TITLE="[squid-users] squid centos and osq_lock">jmakarevic at gmail.com
       </A><BR>
    <I>Fri Jul 31 16:06:46 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="004877.html">[squid-users] squid centos and osq_lock
</A></li>
        <LI>Next message (by thread): <A HREF="004885.html">[squid-users] squid centos and osq_lock
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4882">[ date ]</a>
              <a href="thread.html#4882">[ thread ]</a>
              <a href="subject.html#4882">[ subject ]</a>
              <a href="author.html#4882">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Marcus, tnx for your info.
OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
Yes, cpu_affinity_map is good and with 6 instances there is load only on
first 6 cores and the server is 12 core, 24 HT
each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
and so on so that should not be the problem.
I've tried with 12 workers but that's even worse.

Let me try to explain:
on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
in that case, actual user time is about 10-20% and 70-80% is sys time
(osq_lock) and there are no connection timeouts.

If I switch to SMP 6 workers user time goes up but sys time goes up too and
there are connection timeouts and the load jumps to ~12.
If I give it more workers only load jumps and more connections are being
dropped to the point that load goes to 23/24 and the entire server is slow
as hell.

So, best performance so far are with 6 non-smp workers.

For now I have 2 options:
1. Install older squid (3.1.10 centos repo) and try it then
2. build custom 64bit kernel with RCU and specific cpu family support (in
progress).

The end idea is to be able to sustain 1gig of traffic on this server :)
Any advice is welcome


J.

2015-07-31 14:53 GMT+02:00 Marcus Kool &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">marcus.kool at urlfilterdb.com</A>&gt;:

&gt;<i> osq_lock is used in the kenel for the implementation of a mutex.
</I>&gt;<i> It is not clear which mutex so we can only guess.
</I>&gt;<i>
</I>&gt;<i> Which version of the kernel and distro do you use?
</I>&gt;<i>
</I>&gt;<i> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
</I>&gt;<i> non-SMP.
</I>&gt;<i>
</I>&gt;<i> What is the value of cpu_affinity_map in all config files?
</I>&gt;<i> You say they are static. But do you allocate each instance on a different
</I>&gt;<i> core?
</I>&gt;<i> Does 'top' show that all CPUs are used?
</I>&gt;<i>
</I>&gt;<i> Do you have 24 cores or 12 hyperthreaded cores?
</I>&gt;<i> In case you have 12 real cores, you might want to experiment with 12
</I>&gt;<i> instances of Squid and then try to upscale.
</I>&gt;<i>
</I>&gt;<i> Make maximum_object_size large, a max size of 16K will prohibit the
</I>&gt;<i> retrieval of objects larger than 16K.
</I>&gt;<i> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
</I>&gt;<i> infinite and do not worry since
</I>&gt;<i> cache_mem is zero.
</I>&gt;<i>
</I>&gt;<i> Marcus
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Hi Amos,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>   cache_mem 0
</I>&gt;&gt;<i>   cache deny all
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> already there.
</I>&gt;&gt;<i> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
</I>&gt;&gt;<i> interface.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Well, entire config would be way too long but here is the static part:
</I>&gt;&gt;<i> via off
</I>&gt;&gt;<i> cpu_affinity_map process_numbers=1 cores=2
</I>&gt;&gt;<i> forwarded_for delete
</I>&gt;&gt;<i> visible_hostname squid1
</I>&gt;&gt;<i> pid_filename /var/run/squid1.pid
</I>&gt;&gt;<i> icp_port 0
</I>&gt;&gt;<i> htcp_port 0
</I>&gt;&gt;<i> icp_access deny all
</I>&gt;&gt;<i> htcp_access deny all
</I>&gt;&gt;<i> snmp_port 0
</I>&gt;&gt;<i> snmp_access deny all
</I>&gt;&gt;<i> dns_nameservers x.x.x.x
</I>&gt;&gt;<i> cache_mem 0
</I>&gt;&gt;<i> cache deny all
</I>&gt;&gt;<i> pipeline_prefetch on
</I>&gt;&gt;<i> memory_pools off
</I>&gt;&gt;<i> maximum_object_size 16 KB
</I>&gt;&gt;<i> maximum_object_size_in_memory 16 KB
</I>&gt;&gt;<i> ipcache_size 0
</I>&gt;&gt;<i> cache_store_log none
</I>&gt;&gt;<i> half_closed_clients off
</I>&gt;&gt;<i> include /etc/squid/rules
</I>&gt;&gt;<i> access_log /var/log/squid/squid1-access.log
</I>&gt;&gt;<i> cache_log /var/log/squid/squid1-cache.log
</I>&gt;&gt;<i> coredump_dir /var/spool/squid/squid1
</I>&gt;&gt;<i> refresh_pattern ^ftp:           1440    20%     10080
</I>&gt;&gt;<i> refresh_pattern ^gopher:        1440    0%      1440
</I>&gt;&gt;<i> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
</I>&gt;&gt;<i> refresh_pattern .               0       20%     4320
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> acl port0 myport 30000
</I>&gt;&gt;<i> http_access allow testhost
</I>&gt;&gt;<i> tcp_outgoing_address x.x.x.x port0
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> include is there for basic ACL - safe ports and so on - to minimize
</I>&gt;&gt;<i> config file footprint since it's static and same for every worker.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> and so on 44 more times in this config file
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Do you know of any good article hot to tune kernel locking or have any
</I>&gt;&gt;<i> idea why is it happening?
</I>&gt;&gt;<i> I cannot find any good info on it and all I've found are bits and peaces
</I>&gt;&gt;<i> of kernel source code.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Tnx.
</I>&gt;&gt;<i> J.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> 2015-07-31 0:42 GMT+02:00 Amos Jeffries &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid3 at treenet.co.nz</A> &lt;mailto:
</I>&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid3 at treenet.co.nz</A>&gt;&gt;:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     On 31/07/2015 8:05 a.m., Josip Makarevic wrote:
</I>&gt;&gt;<i>     &gt; Hi,
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt; I have a problem with squid setup (squid version 3.5.6, built from
</I>&gt;&gt;<i> source,
</I>&gt;&gt;<i>     &gt; centos 6.6)
</I>&gt;&gt;<i>     &gt; I've tried 2 options:
</I>&gt;&gt;<i>     &gt; 1. SMP
</I>&gt;&gt;<i>     &gt; 2. NON-SMP
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt; I've decided to stick with custom build non-smp version and the
</I>&gt;&gt;<i> thing is:
</I>&gt;&gt;<i>     &gt; - i don't need cache - any kind of it
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>       cache_mem 0
</I>&gt;&gt;<i>       cache deny all
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     That is it. All other caches used by Squid *are* mandatory for good
</I>&gt;&gt;<i>     performance. And are only used anyway when the component that needs
</I>&gt;&gt;<i> them
</I>&gt;&gt;<i>     is actively used.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     &gt; - I have DNS cache just for that
</I>&gt;&gt;<i>     &gt; - squid has to listen on 1024 ports on 23 instances.
</I>&gt;&gt;<i>     &gt; each instance listens on set of ports and each port has different
</I>&gt;&gt;<i> outgoing
</I>&gt;&gt;<i>     &gt; ip address.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     And how many NIC do you have that spread over?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt; The thing is this:
</I>&gt;&gt;<i>     &gt; It's alll good until we hit it with more than 150mbits then...
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt; (output from perf top)
</I>&gt;&gt;<i>     &gt;  84.57%  [kernel]                  [k] osq_lock
</I>&gt;&gt;<i>     &gt;   4.62%  [kernel]                  [k] mutex_spin_on_owner
</I>&gt;&gt;<i>     &gt;   1.41%  [kernel]                  [k] memcpy
</I>&gt;&gt;<i>     &gt;   0.79%  [kernel]                  [k] inet_dump_ifaddr
</I>&gt;&gt;<i>     &gt;   0.62%  [kernel]                  [k] memset
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt;  21:53:39 up 7 days, 10:38,  1 user,  load average: 24.01, 23.84,
</I>&gt;&gt;<i> 23.33
</I>&gt;&gt;<i>     &gt; (yes, we have 24 cores)
</I>&gt;&gt;<i>     &gt; Same behavior is with SMP and NON-SMP setup (SMP setup is all in
</I>&gt;&gt;<i> one file
</I>&gt;&gt;<i>     &gt; with workers 23 option but then I have to use rock cache)
</I>&gt;&gt;<i>     &gt;
</I>&gt;&gt;<i>     &gt; so, my question is....what...how to optimize
</I>&gt;&gt;<i> this.....whatever....I'm stuck
</I>&gt;&gt;<i>     &gt; for days, I've tried many sysctl options but none of them works.
</I>&gt;&gt;<i>     &gt; Any help, info, something else?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     None of those are Squid functionality. If you want help optimizing
</I>&gt;&gt;<i> your
</I>&gt;&gt;<i>     config and are willing to post it to the list I am happy to do a quick
</I>&gt;&gt;<i>     audit and point out any problem areas for you.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     But tuning the internal locking code of the kernel is way off topic.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     Amos
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>     _______________________________________________
</I>&gt;&gt;<i>     squid-users mailing list
</I>&gt;&gt;<i>     <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A> &lt;mailto:
</I>&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>&gt;
</I>&gt;&gt;<i>     <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> squid-users mailing list
</I>&gt;&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;&gt;<i> <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/1f5a4294/attachment.htm">http://lists.squid-cache.org/pipermail/squid-users/attachments/20150731/1f5a4294/attachment.htm</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="004877.html">[squid-users] squid centos and osq_lock
</A></li>
	<LI>Next message (by thread): <A HREF="004885.html">[squid-users] squid centos and osq_lock
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4882">[ date ]</a>
              <a href="thread.html#4882">[ thread ]</a>
              <a href="subject.html#4882">[ subject ]</a>
              <a href="author.html#4882">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
