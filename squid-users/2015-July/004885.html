<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] squid centos and osq_lock
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3C55BBC4C3.3090801%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="004882.html">
   <LINK REL="Next"  HREF="004865.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] squid centos and osq_lock</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20squid%20centos%20and%20osq_lock&In-Reply-To=%3C55BBC4C3.3090801%40treenet.co.nz%3E"
       TITLE="[squid-users] squid centos and osq_lock">squid3 at treenet.co.nz
       </A><BR>
    <I>Fri Jul 31 18:56:03 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="004882.html">[squid-users] squid centos and osq_lock
</A></li>
        <LI>Next message (by thread): <A HREF="004865.html">[squid-users] forward proxy - many users with one login/passwd.
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4885">[ date ]</a>
              <a href="thread.html#4885">[ thread ]</a>
              <a href="subject.html#4885">[ subject ]</a>
              <a href="author.html#4885">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 1/08/2015 4:06 a.m., Josip Makarevic wrote:
&gt;<i> Marcus, tnx for your info.
</I>&gt;<i> OS is centos 6 w kernel  2.6.32-504.30.3.el6.x86_64
</I>&gt;<i> Yes, cpu_affinity_map is good and with 6 instances there is load only on
</I>&gt;<i> first 6 cores and the server is 12 core, 24 HT
</I>
Then I suspect that mutex and locking will be the kernel scheduling work
on the HT cores.
 In high performance Squid will max out a physical cores worth of
cycles. HT essentially tries to over-clock physical cores. But trying to
reach 200% capacity into a physical core with Squid workloads only leads
to trouble.
 It is far better to tie Squid with affinity to one instance per
physical core and let the extra HT capacity be available to the OS and
other supporting things the Squid instance needs to have happen externally.


&gt;<i> each instance is bound to 1 core. Instance 1 = core1, instance 2 = core 2
</I>&gt;<i> and so on so that should not be the problem.
</I>&gt;<i> I've tried with 12 workers but that's even worse.
</I>
You do need to be very careful about which core numbers are the HT core
vs the physical core ID. Last time I saw anyone doing it, every second
number was a real physical core ID. YMMV.

&gt;<i> 
</I>&gt;<i> Let me try to explain:
</I>&gt;<i> on non-smp with traffic at ~300mbits we have load of ~4 (on 6 workers).
</I>&gt;<i> in that case, actual user time is about 10-20% and 70-80% is sys time
</I>&gt;<i> (osq_lock) and there are no connection timeouts.
</I>&gt;<i> 
</I>&gt;<i> If I switch to SMP 6 workers user time goes up but sys time goes up too and
</I>&gt;<i> there are connection timeouts and the load jumps to ~12.
</I>&gt;<i> If I give it more workers only load jumps and more connections are being
</I>&gt;<i> dropped to the point that load goes to 23/24 and the entire server is slow
</I>&gt;<i> as hell.
</I>&gt;<i> 
</I>&gt;<i> So, best performance so far are with 6 non-smp workers.
</I>&gt;<i> 
</I>&gt;<i> For now I have 2 options:
</I>&gt;<i> 1. Install older squid (3.1.10 centos repo) and try it then
</I>&gt;<i> 2. build custom 64bit kernel with RCU and specific cpu family support (in
</I>&gt;<i> progress).
</I>&gt;<i> 
</I>&gt;<i> The end idea is to be able to sustain 1gig of traffic on this server :)
</I>&gt;<i> Any advice is welcome
</I>
I agree with Marcus then. The non-SMP then is the way to go at present.
The main benefit of SMP support in current Squid is for caching
de-duplication (ie rock store).


Also some things to note:

* a good percentage of the speed of Squid is the 20-40% caching HIT rate
normal HTTP traffic has. Albeit memory-only caching on highest
performance boxen. Memory hits are 4-6 orders of magnitude faster than
network fetches. This has little to do with anything you can control
(normally). The (relatively) slow speed of origin servers creating the
content is the bottleneck. Even &quot;static&quot; content may be encoded to the
clients requested desire on each fetch, which takes time.


* Going by out lab tests and real-world results so far I rate Squid
per-worker at ~50Mbps on 3.1GHz core, and ~70Mbps on 3.7GHz. Your 12
cores will only get you up around 800 Mbits IMHO (thats after tuning). I
would gladly be proven wrong though :-)


* Squid effectively *polls* all the listening ports every 10ms or once
every 10 I/O events (whichever is faster). So running with 1024
listening ports is a bit counter-productive, more time could be spent
checking those ports than doing work.
 That said going from one to multiple listening ports does make a speed
improvement. Finding the sweet spot between those trends is something
else to tune for.
 &lt;<A HREF="http://wiki.squid-cache.org/MultipleInstances#Tips">http://wiki.squid-cache.org/MultipleInstances#Tips</A>&gt;


&gt;<i> 2015-07-31 14:53 GMT+02:00 Marcus Kool:
</I>&gt;<i> 
</I>&gt;&gt;<i> osq_lock is used in the kenel for the implementation of a mutex.
</I>&gt;&gt;<i> It is not clear which mutex so we can only guess.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Which version of the kernel and distro do you use?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Since mutexes are used by Squid SMP, I suggest to switch for now to Squid
</I>&gt;&gt;<i> non-SMP.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> What is the value of cpu_affinity_map in all config files?
</I>&gt;&gt;<i> You say they are static. But do you allocate each instance on a different
</I>&gt;&gt;<i> core?
</I>&gt;&gt;<i> Does 'top' show that all CPUs are used?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Do you have 24 cores or 12 hyperthreaded cores?
</I>&gt;&gt;<i> In case you have 12 real cores, you might want to experiment with 12
</I>&gt;&gt;<i> instances of Squid and then try to upscale.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Make maximum_object_size large, a max size of 16K will prohibit the
</I>&gt;&gt;<i> retrieval of objects larger than 16K.
</I>&gt;&gt;<i> I am not sure about 'maximum_object_size_in_memory 16 KB' but let it be
</I>&gt;&gt;<i> infinite and do not worry since
</I>&gt;&gt;<i> cache_mem is zero.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Marcus
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On 07/31/2015 03:52 AM, Josip Makarevic wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Hi Amos,
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>   cache_mem 0
</I>&gt;&gt;&gt;<i>   cache deny all
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> already there.
</I>&gt;&gt;&gt;<i> Regarding number of nic ports we have 4 10G eth cards 2 in each bonding
</I>&gt;&gt;&gt;<i> interface.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Well, entire config would be way too long but here is the static part:
</I>&gt;&gt;&gt;<i> via off
</I>&gt;&gt;&gt;<i> cpu_affinity_map process_numbers=1 cores=2
</I>&gt;&gt;&gt;<i> forwarded_for delete
</I>&gt;&gt;&gt;<i> visible_hostname squid1
</I>&gt;&gt;&gt;<i> pid_filename /var/run/squid1.pid
</I>
Remove these...

&gt;&gt;&gt;<i> icp_port 0
</I>&gt;&gt;&gt;<i> htcp_port 0
</I>&gt;&gt;&gt;<i> icp_access deny all
</I>&gt;&gt;&gt;<i> htcp_access deny all
</I>&gt;&gt;&gt;<i> snmp_port 0
</I>&gt;&gt;&gt;<i> snmp_access deny all
</I>
... to here. They do nothing but slow Squid-3 down.

&gt;&gt;&gt;<i> dns_nameservers x.x.x.x
</I>&gt;&gt;&gt;<i> cache_mem 0
</I>&gt;&gt;&gt;<i> cache deny all
</I>&gt;&gt;&gt;<i> pipeline_prefetch on
</I>
In Squid-3.4 and later this is set to the length of pipeline you want to
accept.

NP: 'on' traditionally has meant pipeline length of 1 (two parallel
requests). Longer lengths are not yet well tested but generally it seems
to work okay.


&gt;&gt;&gt;<i> memory_pools off
</I>&gt;&gt;&gt;<i> maximum_object_size 16 KB
</I>&gt;&gt;&gt;<i> maximum_object_size_in_memory 16 KB
</I>
Like Marcus said. Without even memory caching these two have no useful
effects.

There is one related setting &quot;read_ahead_gap&quot; which affects performance
by tuning the amount of undelivered object data Squid will buffer in
transient memory. Higher value for that mean faster servers can finish
sending earlier and resources for them released for other uses.
 Tuning this is a fine art since it modulates how much Squid internal
buffers (and pipieline prefetching) read off TCP buffers. And all of
those buffers have limits of their own and may contain multiple requests
data.


&gt;&gt;&gt;<i> ipcache_size 0
</I>
Remove this. Without IP cache Squid will be forced to do about 4x remote
DNS lookup for every single HTTP request - *minimum*. Maybe more if you
apply any access controls to the traffic.
 If anything increase the ipcache size to store more results.


&gt;&gt;&gt;<i> cache_store_log none
</I>
Not needed in Squid-3. You can remove.

&gt;&gt;&gt;<i> half_closed_clients off
</I>&gt;&gt;&gt;<i> include /etc/squid/rules
</I>&gt;&gt;&gt;<i> access_log /var/log/squid/squid1-access.log
</I>
Logging I/O slows Squid down. I suggest making that a daemon, TCP or UDP
log output.


&gt;&gt;&gt;<i> cache_log /var/log/squid/squid1-cache.log
</I>&gt;&gt;&gt;<i> coredump_dir /var/spool/squid/squid1
</I>&gt;&gt;&gt;<i> refresh_pattern ^ftp:           1440    20%     10080
</I>&gt;&gt;&gt;<i> refresh_pattern ^gopher:        1440    0%      1440
</I>&gt;&gt;&gt;<i> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
</I>&gt;&gt;&gt;<i> refresh_pattern .               0       20%     4320
</I>
Without caching you can remove these *entirely*.

&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> acl port0 myport 30000
</I>
Mumble. Less reliable than myportname, but it is infintessimally faster
when it does work at all.

&gt;&gt;&gt;<i> http_access allow testhost
</I>&gt;&gt;&gt;<i> tcp_outgoing_address x.x.x.x port0
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> include is there for basic ACL - safe ports and so on - to minimize
</I>&gt;&gt;&gt;<i> config file footprint since it's static and same for every worker.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> and so on 44 more times in this config file
</I>
Only put allow testhost once. Every time you test ACLs Squid slows down.

Some ACLs are worse drag than others. You can probably optimize even the
default recommended security settings you shuffled into &quot;rules&quot; file to
operate better.


&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Do you know of any good article hot to tune kernel locking or have any
</I>&gt;&gt;&gt;<i> idea why is it happening?
</I>&gt;&gt;&gt;<i> I cannot find any good info on it and all I've found are bits and peaces
</I>&gt;&gt;&gt;<i> of kernel source code.
</I>
Sorry no. All I found was the same.

Though I do know that one of the big differences between Linux 2.6 and
3.0 was the removal of the &quot;Big Kernel Lock&quot; system that allowed Linux
to run on multi-core systems properly. It could be CentOS 6 itelf biting
you with its ancient kernel version.


Amos

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="004882.html">[squid-users] squid centos and osq_lock
</A></li>
	<LI>Next message (by thread): <A HREF="004865.html">[squid-users] forward proxy - many users with one login/passwd.
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4885">[ date ]</a>
              <a href="thread.html#4885">[ thread ]</a>
              <a href="subject.html#4885">[ subject ]</a>
              <a href="author.html#4885">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
