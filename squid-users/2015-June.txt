From squid3 at treenet.co.nz  Mon Jun  1 00:13:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 01 Jun 2015 12:13:37 +1200
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <1433115308548-4671461.post@n4.nabble.com>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
Message-ID: <556BA3B1.7050701@treenet.co.nz>

On 1/06/2015 11:35 a.m., Marcel Fossua wrote:
> HI Amos thanks for your reply I just upgrade to 3.5.5 but compiling from
> source to get --enable-ecap
> but I can't figure out what you means exactly concerning the TOS part
> did you means what I set is ok or not?
>
> qos_flows tos

The above line does nothing.


> qos_flows local-hit=0x30

  0x30 & 0xFA  -> 0x30

OK.

> qos_flows parent-hit=0x32

  0x32 & 0xFA  -> 0x30

not OK.

The second hex digit value can only be 0x0, 0x4, 0x8, or 0xC.

Also, the qos_flows lines are missing the identifier to determine
whether its TCP TOS or iptables NFMARK being set...

  qos_flows tos local-hit=0x30
  qos_flows tos parent-hit=0x34

Or just:
  qos_flows tos local-hit=0x30 parent-hit=0x34

Amos



From marcel at guineanet.net  Mon Jun  1 00:24:10 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Sun, 31 May 2015 17:24:10 -0700 (PDT)
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <556BA3B1.7050701@treenet.co.nz>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
 <556BA3B1.7050701@treenet.co.nz>
Message-ID: <1433118250561-4671465.post@n4.nabble.com>

Thanks Amos
I will try it.

Rgds



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459p4671465.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun  1 01:00:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 01 Jun 2015 13:00:13 +1200
Subject: [squid-users] Ssl-bump deep dive (intercept last post and final
 thoughts)
In-Reply-To: <1433116572.3710.63.camel@JamesiMac>
References: <1433116572.3710.63.camel@JamesiMac>
Message-ID: <556BAE9D.4040700@treenet.co.nz>

On 1/06/2015 11:56 a.m., James Lay wrote:
> So this has been REALLY good!  The tl;dr:  ssl-bumping is pretty easy
> even with intercept, ssl-bumping with access control is a little more
> difficult...jump to the config to skip the chit chat.
> 
> My goal has always been to a content filter based on url regex.  This
> works just fine for http traffic, but is much more difficult for https
> traffic just for the case of you may or may not know the host you're
> going to, depending on the site/app.  I'll be real honest here....I'm
> only doing this to protect/filter the traffic of two kids, on laptops,
> iPhone, and Android phone, so it's a mixed bag of content and, since
> it's just the two of them in a home environment, I get to play around
> and see what works and what doesn't.
> 
> Below is a close as I can get transparent intercept ssl-bump with
> content filtering with using a list of domains/urls with both http and
> https.  I still have to use a list of broken sites, which are large
> netblocks (17.0.0.0/8..Apple anyone?) because some of these I just can't
> seem to get host/domain information during the ssl handshake.  As I
> discovered after attempting to put this into "production", I have not
> been able to emulate using wget or curl an https session that doesn't
> have any SNI information, so that threw me for a loop.  TextNow is a
> great example (I'm including a packet capture of this in this post).
> There's no host information in the client hello....there's no host
> information in the server hello.....buried deep in the certificate ONLY
> is the "commonName=.*textnow.me"...that's it.  This dashed my hopes of
> using an url_regex for access control with all https sessions.  I have
> "%ssl::>cert_subject" in my logging, and I never did see this log in any
> of my tests...and I tested a BUNCH of different peek/stare/splice/bump
> cominations..so I don't think squid is actually seeing this from the
> certificate.
> 
> Another challenge is getting http url_regex filtering to work with https
> filtering.  My method of filtering means not having an "http_access
> allow localnet", which directly conflicted with also trying to filter
> https.  The solution was to add an acl for port 443, then http_access to
> just allow it, as our filtering was going to happen for https further
> down.
> 
> I know there's a fair amount of people who just want to plop in some
> config files, run a few commands, and be up and running.  The below
> configuration has two additional files it references, http_url.txt,
> which is an a list of domains/urls (\.apple\.com for example), and the
> aptly named broken, which is a IP list (17.0.0.0/8).  The broken list
> should be (semi) trusted and are sites that we just can't get SNI or
> hostname information from.  If you've created a single cert/key pair
> from the Squid documentation, you won't need the key= line in your
> https_port directive.  If you've followed along in my posts, you already
> have the configure line from my previous posts.  Change the
> commands/config to fir where your squid config and ssl_db are.  So after
> configuring, make sure you:
> 
> sudo /opt/libexec/ssl_crtd -c -s /opt/var/ssl_db
> sudo chown -R nobody /opt/var/ssl_db/
> 
> As I believe in a lot of logging, and actually looking at said logging,
> below is what you can expect to see in your logs (mine logs to syslog,
> again, change this if you log to a different file):
> 
> Allowed http to .apple.com in http_url.txt:
> May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
> [31/May/2015:17:03:48 -0600] "GET
> http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag? HTTP/1.1" - -
> 200 5243 TCP_MISS:ORIGINAL_DST -
> Denied http to symcb.com not in http_url.txt
> May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
> [31/May/2015:17:03:48 -0600] "GET http://sd.symcb.com/sd.crt HTTP/1.1" -
> - 403 3618 TCP_DENIED:HIER_NONE -
> Spliced https IP in broken.txt (google block 216.58.192.0/19)
> May 31 17:04:34 gateway (squid-1): 192.168.1.101 - -
> [31/May/2015:17:04:34 -0600] "CONNECT 216.58.216.138:443 HTTP/1.1" - -
> 200 568 TCP_TUNNEL:ORIGINAL_DST peek
> Spliced https IP in broken.txt that we got SNI or bumped site in
> http_url.txt look exactly the same
> May 31 17:09:45 gateway (squid-1): 192.168.1.100 - -
> [31/May/2015:17:09:45 -0600] "CONNECT 23.222.157.21:443 HTTP/1.1"
> init.itunes.apple.com - 200 30314 TCP_TUNNEL:ORIGINAL_DST peek
> 
> The only drag with the configuration is you won't see when an https
> session is terminated when the IP/url is not in the broken.txt, or the
> http_url.txt:
> 
> [17:20:53 jlay at analysis:~$] wget -d
> --ca-certificate=/etc/ssl/certs/sslsplit.crt https://www.yahoo.com
> Setting --ca-certificate (cacertificate) to /etc/ssl/certs/sslsplit.crt
> DEBUG output created by Wget 1.16.1 on linux-gnu.
> 
> URI encoding = ?UTF-8?
> --2015-05-31 17:20:59--  https://www.yahoo.com/
> Resolving www.yahoo.com (www.yahoo.com)... 206.190.36.45,
> 206.190.36.105, 2001:4998:c:a06::2:4008
> Caching www.yahoo.com => 206.190.36.45 206.190.36.105
> 2001:4998:c:a06::2:4008
> Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.45|:443...
> connected.
> Created socket 3.
> Releasing 0x00007fdf67eecdd0 (new refcount 1).
> Initiating SSL handshake.
> SSL handshake failed.
> Closed fd 3
> Unable to establish SSL connection.
> 
> May 31 17:20:59 gateway (squid-1): 192.168.1.6 - - [31/May/2015:17:20:59
> -0600] "CONNECT 206.190.36.45:443 HTTP/1.1" www.yahoo.com - 200 0
> TAG_NONE:ORIGINAL_DST peek 
> 
> Full config below:
> ####################################
> acl localnet src 192.168.1.0/24
> 
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 443
> 
> acl CONNECT method CONNECT
> 
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> acl allow_https port 443

Note how SSL_ports and allow_https ACL definitions are identical apart
from the name.

You can replace all uses of "allow_https" ACL in the rest of your config
with "SSL_Ports".


> acl broken dst "/opt/etc/squid/broken.txt"
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_Ports
> 
> http_access allow allow_https
> http_access allow allowed_http_sites
> http_access deny !allowed_http_sites
> 
> http_access deny all

Two deny lines in a row are redundant when one is "deny all".

You can drop the "deny !allowed_http_sites".


> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 broken
> ssl_bump peek step2 broken
> ssl_bump splice broken
> ssl_bump peek step1 all
> ssl_bump peek step2 all

The above lines are logically the same as:

 ssl_bump splice step3 broken
 ssl_bump peek all

> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump bump allowed_https_sites
> ssl_bump terminate !allowed_https_sites

If you used reject instead of terminate on this rule, you should get the
log entries you mentioned wanting when connections are terminated.

The not logging of "terminate" connectisno is a bug. Please report to
bugzilla so we dont forget it.

> 
> sslproxy_cert_error allow all
> sslproxy_capath /etc/ssl/certs
> sslproxy_flags DONT_VERIFY_PEER 

The point of TLS is to verify the other endpoint of the connection is
actually who they claim to be. The above config settings are a) not
bothering to even check the claim, and b) silently ignoring whatever
result the security check produces.

* DONT_VERIFY_PEER is a debugging option. Not for use in "production"
systems. It should not even be used in any real operational testing,
only to see if the peer verification failures was the source of an issue.

* sslproxy_cert_error allow all - is pure evil. The directive was only
added so networks could continue to work during transition periods when
a major vulnerability like CRIME/ BREECH / POODLE were discovered and
their library started erroring out on bad bevahiour, but the remote
sites were being laggards.
 Theres a limited number of errors which are safe to ignore, and an even
smaller sub-set of those which are actually needed on any given network.


> sslproxy_options ALL

"ALL" enables a lot of old features and hacks in OpenSSL (export grade
ciphers and plaintext passwords, woohoo!) that are increasingly found to
cause security vulenrabilities.


> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> sslcrtd_children 5
> 
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump
> cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> key=/opt/etc/squid/certs/sslsplit_ca_key.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslflags=NO_SESSION_REUSE
> 
> logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %
> ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode 
> 
> access_log syslog:daemon.info mine
> 
> refresh_pattern -i (cgi-bin|\?)	0	0%	0
> refresh_pattern .		0	20%	4320
> 
> coredump_dir /opt/var
> ##############################
> 
> Thanks all for being patient while I continued to post my learning and
> all my mistakes.  If there's anything that I've missed, or if there's
> another method for trying to accomplish what I've tried to do I'm all
> eyes.
> 
> James
> 
> P.S. Things I'd love to see in Squid some day:
> 
> acl's being AND'd (http_access allow allowed_sites AND localnet)

That aready exists. The AND is implicit in the order of the ACL checks,
so the "AND " is redundant waste of space in the config file.

eg, your above "allowed_sites AND localnet" rule is configured:
  http_access allow allowed_sites localnet

This is why I keep pointing out how useless rules like "ssl_bump peek
step1 all" are. So you want to peek when "step1", just peek based on
step1, no need to check if true == true.


> Full on separate http_access, https_access directives
> 

I consider this every now and again. But HTTPS is not actually a real
thing. It is a TLS connection delivering regular HTTP messages. By the
time the messages are inside Squid they dont have that TLS part any more
than clear text HTTP has TCP headers.


That said, if you want to go fancy you can use the "allof" ACL type.
This ACL type lets you write an entire sub-tree of logical ACL tests and
give it a name. Its not quite the same though.

 acl HTTP proto HTTP
 acl HTTPS proto HTTPS

 acl sites dstdomain ...

 acl sites2 dstdomain ...
 acl site2paths urlpath_regex ...

 acl 443 port 443

 acl plain allof CONNECT 443
 acl plain allof sites
 acl plain allof sites2 site2paths
 acl plain allof localnet

 acl decrypted allof sites
 acl decrypted allof sites2 site2paths
 acl decrypted allof localnet

 http_access allow HTTP plain
 http_access allow HTTPS decrypted
 http_access deny all

Amos


From marcel at guineanet.net  Mon Jun  1 01:19:06 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Sun, 31 May 2015 18:19:06 -0700 (PDT)
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <1433118250561-4671465.post@n4.nabble.com>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
 <556BA3B1.7050701@treenet.co.nz> <1433118250561-4671465.post@n4.nabble.com>
Message-ID: <1433121546223-4671467.post@n4.nabble.com>

No luck 
Still not getting result at all I think the issue could be with my Mikrotik
box 

# Marking packets with DSCP (for Mikrotik 6.x) for cache hit content coming
from SQUID Proxy

/ip firewall mangle 
add action=mark-packet chain=prerouting  disabled=no dscp=12
new-packet-mark=squid-connection passthrough=no comment="==SQUID - TOS 12
=="

<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4671467/Captura_de_pantalla_2015-05-29_a_las_21.png> 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459p4671467.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Mon Jun  1 01:46:54 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 31 May 2015 19:46:54 -0600
Subject: [squid-users] Ssl-bump deep dive (intercept last post and final
 thoughts)
In-Reply-To: <556BAE9D.4040700@treenet.co.nz>
References: <1433116572.3710.63.camel@JamesiMac>
 <556BAE9D.4040700@treenet.co.nz>
Message-ID: <1433123214.3738.2.camel@JamesiMac>

On Mon, 2015-06-01 at 13:00 +1200, Amos Jeffries wrote:

> On 1/06/2015 11:56 a.m., James Lay wrote:
> > So this has been REALLY good!  The tl;dr:  ssl-bumping is pretty easy
> > even with intercept, ssl-bumping with access control is a little more
> > difficult...jump to the config to skip the chit chat.
> > 
> > My goal has always been to a content filter based on url regex.  This
> > works just fine for http traffic, but is much more difficult for https
> > traffic just for the case of you may or may not know the host you're
> > going to, depending on the site/app.  I'll be real honest here....I'm
> > only doing this to protect/filter the traffic of two kids, on laptops,
> > iPhone, and Android phone, so it's a mixed bag of content and, since
> > it's just the two of them in a home environment, I get to play around
> > and see what works and what doesn't.
> > 
> > Below is a close as I can get transparent intercept ssl-bump with
> > content filtering with using a list of domains/urls with both http and
> > https.  I still have to use a list of broken sites, which are large
> > netblocks (17.0.0.0/8..Apple anyone?) because some of these I just can't
> > seem to get host/domain information during the ssl handshake.  As I
> > discovered after attempting to put this into "production", I have not
> > been able to emulate using wget or curl an https session that doesn't
> > have any SNI information, so that threw me for a loop.  TextNow is a
> > great example (I'm including a packet capture of this in this post).
> > There's no host information in the client hello....there's no host
> > information in the server hello.....buried deep in the certificate ONLY
> > is the "commonName=.*textnow.me"...that's it.  This dashed my hopes of
> > using an url_regex for access control with all https sessions.  I have
> > "%ssl::>cert_subject" in my logging, and I never did see this log in any
> > of my tests...and I tested a BUNCH of different peek/stare/splice/bump
> > cominations..so I don't think squid is actually seeing this from the
> > certificate.
> > 
> > Another challenge is getting http url_regex filtering to work with https
> > filtering.  My method of filtering means not having an "http_access
> > allow localnet", which directly conflicted with also trying to filter
> > https.  The solution was to add an acl for port 443, then http_access to
> > just allow it, as our filtering was going to happen for https further
> > down.
> > 
> > I know there's a fair amount of people who just want to plop in some
> > config files, run a few commands, and be up and running.  The below
> > configuration has two additional files it references, http_url.txt,
> > which is an a list of domains/urls (\.apple\.com for example), and the
> > aptly named broken, which is a IP list (17.0.0.0/8).  The broken list
> > should be (semi) trusted and are sites that we just can't get SNI or
> > hostname information from.  If you've created a single cert/key pair
> > from the Squid documentation, you won't need the key= line in your
> > https_port directive.  If you've followed along in my posts, you already
> > have the configure line from my previous posts.  Change the
> > commands/config to fir where your squid config and ssl_db are.  So after
> > configuring, make sure you:
> > 
> > sudo /opt/libexec/ssl_crtd -c -s /opt/var/ssl_db
> > sudo chown -R nobody /opt/var/ssl_db/
> > 
> > As I believe in a lot of logging, and actually looking at said logging,
> > below is what you can expect to see in your logs (mine logs to syslog,
> > again, change this if you log to a different file):
> > 
> > Allowed http to .apple.com in http_url.txt:
> > May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
> > [31/May/2015:17:03:48 -0600] "GET
> > http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag? HTTP/1.1" - -
> > 200 5243 TCP_MISS:ORIGINAL_DST -
> > Denied http to symcb.com not in http_url.txt
> > May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
> > [31/May/2015:17:03:48 -0600] "GET http://sd.symcb.com/sd.crt HTTP/1.1" -
> > - 403 3618 TCP_DENIED:HIER_NONE -
> > Spliced https IP in broken.txt (google block 216.58.192.0/19)
> > May 31 17:04:34 gateway (squid-1): 192.168.1.101 - -
> > [31/May/2015:17:04:34 -0600] "CONNECT 216.58.216.138:443 HTTP/1.1" - -
> > 200 568 TCP_TUNNEL:ORIGINAL_DST peek
> > Spliced https IP in broken.txt that we got SNI or bumped site in
> > http_url.txt look exactly the same
> > May 31 17:09:45 gateway (squid-1): 192.168.1.100 - -
> > [31/May/2015:17:09:45 -0600] "CONNECT 23.222.157.21:443 HTTP/1.1"
> > init.itunes.apple.com - 200 30314 TCP_TUNNEL:ORIGINAL_DST peek
> > 
> > The only drag with the configuration is you won't see when an https
> > session is terminated when the IP/url is not in the broken.txt, or the
> > http_url.txt:
> > 
> > [17:20:53 jlay at analysis:~$] wget -d
> > --ca-certificate=/etc/ssl/certs/sslsplit.crt https://www.yahoo.com
> > Setting --ca-certificate (cacertificate) to /etc/ssl/certs/sslsplit.crt
> > DEBUG output created by Wget 1.16.1 on linux-gnu.
> > 
> > URI encoding = ?UTF-8?
> > --2015-05-31 17:20:59--  https://www.yahoo.com/
> > Resolving www.yahoo.com (www.yahoo.com)... 206.190.36.45,
> > 206.190.36.105, 2001:4998:c:a06::2:4008
> > Caching www.yahoo.com => 206.190.36.45 206.190.36.105
> > 2001:4998:c:a06::2:4008
> > Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.45|:443...
> > connected.
> > Created socket 3.
> > Releasing 0x00007fdf67eecdd0 (new refcount 1).
> > Initiating SSL handshake.
> > SSL handshake failed.
> > Closed fd 3
> > Unable to establish SSL connection.
> > 
> > May 31 17:20:59 gateway (squid-1): 192.168.1.6 - - [31/May/2015:17:20:59
> > -0600] "CONNECT 206.190.36.45:443 HTTP/1.1" www.yahoo.com - 200 0
> > TAG_NONE:ORIGINAL_DST peek 
> > 
> > Full config below:
> > ####################################
> > acl localnet src 192.168.1.0/24
> > 
> > acl SSL_ports port 443
> > acl Safe_ports port 80
> > acl Safe_ports port 443
> > 
> > acl CONNECT method CONNECT
> > 
> > acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> > acl allow_https port 443
> 
> Note how SSL_ports and allow_https ACL definitions are identical apart
> from the name.
> 
> You can replace all uses of "allow_https" ACL in the rest of your config
> with "SSL_Ports".
> 
> 
> > acl broken dst "/opt/etc/squid/broken.txt"
> > 
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_Ports
> > 
> > http_access allow allow_https
> > http_access allow allowed_http_sites
> > http_access deny !allowed_http_sites
> > 
> > http_access deny all
> 
> Two deny lines in a row are redundant when one is "deny all".
> 
> You can drop the "deny !allowed_http_sites".
> 
> 
> > 
> > acl step1 at_step SslBump1
> > acl step2 at_step SslBump2
> > acl step3 at_step SslBump3
> > 
> > ssl_bump peek step1 broken
> > ssl_bump peek step2 broken
> > ssl_bump splice broken
> > ssl_bump peek step1 all
> > ssl_bump peek step2 all
> 
> The above lines are logically the same as:
> 
>  ssl_bump splice step3 broken
>  ssl_bump peek all
> 
> > acl allowed_https_sites ssl::server_name_regex
> > "/opt/etc/squid/http_url.txt"
> > ssl_bump bump allowed_https_sites
> > ssl_bump terminate !allowed_https_sites
> 
> If you used reject instead of terminate on this rule, you should get the
> log entries you mentioned wanting when connections are terminated.
> 
> The not logging of "terminate" connectisno is a bug. Please report to
> bugzilla so we dont forget it.
> 
> > 
> > sslproxy_cert_error allow all
> > sslproxy_capath /etc/ssl/certs
> > sslproxy_flags DONT_VERIFY_PEER 
> 
> The point of TLS is to verify the other endpoint of the connection is
> actually who they claim to be. The above config settings are a) not
> bothering to even check the claim, and b) silently ignoring whatever
> result the security check produces.
> 
> * DONT_VERIFY_PEER is a debugging option. Not for use in "production"
> systems. It should not even be used in any real operational testing,
> only to see if the peer verification failures was the source of an issue.
> 
> * sslproxy_cert_error allow all - is pure evil. The directive was only
> added so networks could continue to work during transition periods when
> a major vulnerability like CRIME/ BREECH / POODLE were discovered and
> their library started erroring out on bad bevahiour, but the remote
> sites were being laggards.
>  Theres a limited number of errors which are safe to ignore, and an even
> smaller sub-set of those which are actually needed on any given network.
> 
> 
> > sslproxy_options ALL
> 
> "ALL" enables a lot of old features and hacks in OpenSSL (export grade
> ciphers and plaintext passwords, woohoo!) that are increasingly found to
> cause security vulenrabilities.
> 
> 
> > 
> > sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> > sslcrtd_children 5
> > 
> > http_port 3128 intercept
> > https_port 3129 intercept ssl-bump
> > cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> > cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> > key=/opt/etc/squid/certs/sslsplit_ca_key.pem
> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> > sslflags=NO_SESSION_REUSE
> > 
> > logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %
> > ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode 
> > 
> > access_log syslog:daemon.info mine
> > 
> > refresh_pattern -i (cgi-bin|\?)	0	0%	0
> > refresh_pattern .		0	20%	4320
> > 
> > coredump_dir /opt/var
> > ##############################
> > 
> > Thanks all for being patient while I continued to post my learning and
> > all my mistakes.  If there's anything that I've missed, or if there's
> > another method for trying to accomplish what I've tried to do I'm all
> > eyes.
> > 
> > James
> > 
> > P.S. Things I'd love to see in Squid some day:
> > 
> > acl's being AND'd (http_access allow allowed_sites AND localnet)
> 
> That aready exists. The AND is implicit in the order of the ACL checks,
> so the "AND " is redundant waste of space in the config file.
> 
> eg, your above "allowed_sites AND localnet" rule is configured:
>   http_access allow allowed_sites localnet
> 
> This is why I keep pointing out how useless rules like "ssl_bump peek
> step1 all" are. So you want to peek when "step1", just peek based on
> step1, no need to check if true == true.
> 
> 
> > Full on separate http_access, https_access directives
> > 
> 
> I consider this every now and again. But HTTPS is not actually a real
> thing. It is a TLS connection delivering regular HTTP messages. By the
> time the messages are inside Squid they dont have that TLS part any more
> than clear text HTTP has TCP headers.
> 
> 
> That said, if you want to go fancy you can use the "allof" ACL type.
> This ACL type lets you write an entire sub-tree of logical ACL tests and
> give it a name. Its not quite the same though.
> 
>  acl HTTP proto HTTP
>  acl HTTPS proto HTTPS
> 
>  acl sites dstdomain ...
> 
>  acl sites2 dstdomain ...
>  acl site2paths urlpath_regex ...
> 
>  acl 443 port 443
> 
>  acl plain allof CONNECT 443
>  acl plain allof sites
>  acl plain allof sites2 site2paths
>  acl plain allof localnet
> 
>  acl decrypted allof sites
>  acl decrypted allof sites2 site2paths
>  acl decrypted allof localnet
> 
>  http_access allow HTTP plain
>  http_access allow HTTPS decrypted
>  http_access deny all
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Awesome, awesome, and more awesome.  Amos that you SO much for the
information...I'll make the optimizations and changes and test...I won't
bug the list with this anymore unless requested however.  I will
absolutely file the bug report on terminate.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150531/84035226/attachment.htm>

From nathan at getoffmalawn.com  Mon Jun  1 02:12:33 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 1 Jun 2015 12:12:33 +1000
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <9c4651e1184467ff80e9187bcc27981f@localhost>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
Message-ID: <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>

Hello,

Here are some excerpts of what I've used, and an example Python helper:

https_port 60099 intercept ssl-bump tcpkeepalive
cert=/path/to/cert.pem key=/path/to/key.pem options=NO_SSLv2,NO_SSLv3
generate-host-certificates=on

external_acl_type sni ttl=30 concurrency=X children-max=Y
children-startup=Z %ssl::>sni /path/to/your/helper

acl sni_exclusions external sni
acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2

ssl_bump peek tcp_level all
ssl_bump splice client_hello_peeked sni_exclusions
ssl_bump bump all

Helper:

import sys

line = sys.stdin.read()

# run loop until an empty read, which indicates the process should shut down.
while line:
    concurrency_id, sni = line.split()

    if sni == 'wellsfargo.com':
        sys.stdout.write('%s OK\n' % concurrency_id)
    else:
        sys.stdout.write('%s ERR\n' % concurrency_id)

    line = sys.stdin.read()

Hope that helps,

Nathan.

On 30 May 2015 at 01:14, James Lay <jlay at slave-tothe-box.net> wrote:
> On 2015-05-29 08:57 AM, Nathan Hoad wrote:
>>
>> Yes, I have it working on about a dozen deployments so far, using an
>> external ACL to make bumping decisions based on the SNI server name
>> and a few other things. No complaints from me, it Just Works.
>> On 29/05/2015 5:50 pm, "sp_" <apani at yandex.ru> wrote:
>>
>>> Hello,
>>>
>>> does anyone have the working squid 3.5 with intercept + https?
>>> I've googled a lot, but seems there is no any positive experience
>>> with it.
>>>
>>> --
>>> View this message in context:
>>>
>>
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
>>>
>>> [1]
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users [2]
>>
>>
>>
>> Links:
>> ------
>> [1]
>>
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
>> [2] http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> Nathan,
>
> Care to post your config and external helper?  I know I'd love to see
> concrete examples.  Thank you.
>
> James
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From vdoctor at neuf.fr  Mon Jun  1 06:44:33 2015
From: vdoctor at neuf.fr (Stakres)
Date: Sun, 31 May 2015 23:44:33 -0700 (PDT)
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraries/CDNs Booster
In-Reply-To: <1426261064321-4670396.post@n4.nabble.com>
References: <1420464046420-4668929.post@n4.nabble.com>
 <54AA918C.10300@gmail.com> <1420465642362-4668933.post@n4.nabble.com>
 <54AA979B.7060803@gmail.com> <1420470567538-4668941.post@n4.nabble.com>
 <1421655817441-4669159.post@n4.nabble.com>
 <1422466278065-4669395.post@n4.nabble.com>
 <1423553249913-4669653.post@n4.nabble.com>
 <1424604199892-4670015.post@n4.nabble.com>
 <1426261064321-4670396.post@n4.nabble.com>
Message-ID: <1433141073516-4671470.post@n4.nabble.com>

Hi All,

Advanced Caching Add-On for Linux Squid Proxy Cache v2.7, v3.4 and v3.5 with
Videos, Music, Images, Libraries and CDNs.

New  version 2.528 <https://sourceforge.net/projects/squidvideosbooster/>  
- *June 1st 2015*.
- New websites
- YouTube mobile app improved
More details on  https://svb.unveiltech.com <https://svb.unveiltech.com/>  

Enjoy

Bye Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-2-7-3-4-and-3-5-Videos-Music-Images-Libraries-CDNs-Booster-tp4668683p4671470.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Mon Jun  1 11:10:48 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 01 Jun 2015 05:10:48 -0600
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
 <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
Message-ID: <1433157048.3738.14.camel@JamesiMac>

On Mon, 2015-06-01 at 12:12 +1000, Nathan Hoad wrote:

> Hello,
> 
> Here are some excerpts of what I've used, and an example Python helper:
> 
> https_port 60099 intercept ssl-bump tcpkeepalive
> cert=/path/to/cert.pem key=/path/to/key.pem options=NO_SSLv2,NO_SSLv3
> generate-host-certificates=on
> 
> external_acl_type sni ttl=30 concurrency=X children-max=Y
> children-startup=Z %ssl::>sni /path/to/your/helper
> 
> acl sni_exclusions external sni
> acl tcp_level at_step SslBump1
> acl client_hello_peeked at_step SslBump2
> 
> ssl_bump peek tcp_level all
> ssl_bump splice client_hello_peeked sni_exclusions
> ssl_bump bump all
> 
> Helper:
> 
> import sys
> 
> line = sys.stdin.read()
> 
> # run loop until an empty read, which indicates the process should shut down.
> while line:
>     concurrency_id, sni = line.split()
> 
>     if sni == 'wellsfargo.com':
>         sys.stdout.write('%s OK\n' % concurrency_id)
>     else:
>         sys.stdout.write('%s ERR\n' % concurrency_id)
> 
>     line = sys.stdin.read()
> 
> Hope that helps,
> 
> Nathan.
> 
> On 30 May 2015 at 01:14, James Lay <jlay at slave-tothe-box.net> wrote:
> > On 2015-05-29 08:57 AM, Nathan Hoad wrote:
> >>
> >> Yes, I have it working on about a dozen deployments so far, using an
> >> external ACL to make bumping decisions based on the SNI server name
> >> and a few other things. No complaints from me, it Just Works.
> >> On 29/05/2015 5:50 pm, "sp_" <apani at yandex.ru> wrote:
> >>
> >>> Hello,
> >>>
> >>> does anyone have the working squid 3.5 with intercept + https?
> >>> I've googled a lot, but seems there is no any positive experience
> >>> with it.
> >>>
> >>> --
> >>> View this message in context:
> >>>
> >>
> >> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
> >>>
> >>> [1]
> >>> Sent from the Squid - Users mailing list archive at Nabble.com.
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users [2]
> >>
> >>
> >>
> >> Links:
> >> ------
> >> [1]
> >>
> >> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
> >> [2] http://lists.squid-cache.org/listinfo/squid-users
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >
> >
> > Nathan,
> >
> > Care to post your config and external helper?  I know I'd love to see
> > concrete examples.  Thank you.
> >
> > James
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users


Thank you Nathan.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150601/d63b52bb/attachment.htm>

From grusha at flywild.net  Mon Jun  1 16:40:04 2015
From: grusha at flywild.net (dkandle)
Date: Mon, 1 Jun 2015 09:40:04 -0700 (PDT)
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
Message-ID: <1433176804852-4671472.post@n4.nabble.com>

I am using Ubuntu 14.04 on a server with multiple NICs. I would like to set
it up as a transparent proxy. I have the router working and I had squid
working as an explicit proxy (where I set the IP address of the server as
the proxy in my client's browser). 
Is there a good tutorial which covers this set-up? I've tried setting the
iptables as some have advised but it has issues.
It is not at all clear to me how squid will know which interface faces the
Internet and which faces my client's subnet.

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Mon Jun  1 17:09:41 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 01 Jun 2015 11:09:41 -0600
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <1433176804852-4671472.post@n4.nabble.com>
References: <1433176804852-4671472.post@n4.nabble.com>
Message-ID: <2f76befd10cd632bc907028111f2c81d@localhost>

On 2015-06-01 10:40 AM, dkandle wrote:
> I am using Ubuntu 14.04 on a server with multiple NICs. I would like to 
> set
> it up as a transparent proxy. I have the router working and I had squid
> working as an explicit proxy (where I set the IP address of the server 
> as
> the proxy in my client's browser).
> Is there a good tutorial which covers this set-up? I've tried setting 
> the
> iptables as some have advised but it has issues.
> It is not at all clear to me how squid will know which interface faces 
> the
> Internet and which faces my client's subnet.
> 
> Thanks
> 
> 
> 
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

The official tutorials:

http://wiki.squid-cache.org/ConfigExamples#Interception

You'll most likely want:

http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

Of interest is the lack of interface specification, so here's what I'm 
using on a box that has an internal nic(192.168.1.0/24) and an external 
nic(real world external IP):

$IPTABLES -t nat -A PREROUTING -i eth0 -s 192.168.1.96/28 -p tcp --dport 
80 -j REDIRECT --to-port 3128
$IPTABLES -t nat -A PREROUTING -i eth0 -s 192.168.1.96/28 -p tcp --dport 
443 -j REDIRECT --to-port 3129

This redirects traffic from clients coming in on eth0 to Squid listening 
process on eth0.  If your squid listening process is not on the same 
nic, you'll need to use DNAT instead:

$IPTABLES -t nat -A PREROUTING -i eth0 -s 192.168.1.96/28 -p tcp --dport 
80 -j DNAT --to-destination ip.that.squid.listens.on:3128
$IPTABLES -t nat -A PREROUTING -i eth0 -s 192.168.1.96/28 -p tcp --dport 
443 -j DNAT --to-destination  ip.that.squid.listens.on:3129

Hope that helps.

James


From hussam at visp.net.lb  Mon Jun  1 17:13:27 2015
From: hussam at visp.net.lb (Hussam Al-Tayeb)
Date: Mon, 01 Jun 2015 20:13:27 +0300
Subject: [squid-users] rock storage
Message-ID: <5119925.Vk30sxBg4L@hades>

Hello, I added a 5000MB rock storage entry in squid.conf
when it filled up, squid cache manager said:
	Storage Swap size:	5120000 KB
	Storage Swap capacity:	100.0% used,  0.0% free

but du -BM says 4703M is the size of the rock storage file.
ls -l says 5242880000.

Since it is full, shouldn't du -BM reading be closer to 5000M?
Thank you.


From squid3 at treenet.co.nz  Mon Jun  1 20:22:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 02 Jun 2015 08:22:16 +1200
Subject: [squid-users] rock storage
In-Reply-To: <5119925.Vk30sxBg4L@hades>
References: <5119925.Vk30sxBg4L@hades>
Message-ID: <556CBEF7.8030103@treenet.co.nz>

On 2/06/2015 5:13 a.m., Hussam Al-Tayeb wrote:
> Hello, I added a 5000MB rock storage entry in squid.conf
> when it filled up, squid cache manager said:
> 	Storage Swap size:	5120000 KB
> 	Storage Swap capacity:	100.0% used,  0.0% free
> 
> but du -BM says 4703M is the size of the rock storage file.
> ls -l says 5242880000.
> 
> Since it is full, shouldn't du -BM reading be closer to 5000M?

Not particularly. Squid is counting the size of data blocks (cells) in
the DB. The FS may have completely different ideas about how much space
that data takes up after physical storage features have processed it.

Particularly if one rock DB cell takes up multiple FS blocks / inodes.
The incompletely filled rock DB cells (204, 304 or very small responses)
may only be using half (or less) the space on disk of filled cells.

4703 M10 is a lot closer than you may think as well. It is 4912 MiB.

Amos



From grusha at flywild.net  Mon Jun  1 22:54:27 2015
From: grusha at flywild.net (dkandle)
Date: Mon, 1 Jun 2015 15:54:27 -0700 (PDT)
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <2f76befd10cd632bc907028111f2c81d@localhost>
References: <1433176804852-4671472.post@n4.nabble.com>
 <2f76befd10cd632bc907028111f2c81d@localhost>
Message-ID: <1433199267899-4671476.post@n4.nabble.com>

Thanks James. That got me most of the way there. 
I have the HTTP proxy working now but I have some strange (to me) issues
with SSL traffic.
I am using the ssl-bump 
I am redirecting both ports 80 and 443
with:
iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.1/28 -p tcp --dport 80 -j
REDIRECT --to-port 3128 
iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.1/28 -p tcp --dport 443 -j
REDIRECT --to-port 3129
I never get any invalid cert messages (which I expected) and when I stop
squid the HTTP traffic stops, but not the HTTPS.
Even with squid not running HTTPS keeps on going.
With Wireshark I see the traffic from my client sending to port 443. How is
it getting out when squid is not running?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472p4671476.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From grusha at flywild.net  Mon Jun  1 22:57:59 2015
From: grusha at flywild.net (dkandle)
Date: Mon, 1 Jun 2015 15:57:59 -0700 (PDT)
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <1433199267899-4671476.post@n4.nabble.com>
References: <1433176804852-4671472.post@n4.nabble.com>
 <2f76befd10cd632bc907028111f2c81d@localhost>
 <1433199267899-4671476.post@n4.nabble.com>
Message-ID: <1433199479370-4671477.post@n4.nabble.com>

Slight correction to my iptables:
iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.200/28 -p tcp --dport 80 -j
REDIRECT --to-port 3128 
iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.200/28 -p tcp --dport 443
-j REDIRECT --to-port 3129

I also don't understand why the first ip address specification didn't work
but I had to change the 10.1.10.1 to 10.100 which is the exact ip address of
the client. I thought the /28 would have caused this to match any IP address
in the subnet.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472p4671477.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun  1 23:50:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 02 Jun 2015 11:50:13 +1200
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <1433199479370-4671477.post@n4.nabble.com>
References: <1433176804852-4671472.post@n4.nabble.com>
 <2f76befd10cd632bc907028111f2c81d@localhost>
 <1433199267899-4671476.post@n4.nabble.com>
 <1433199479370-4671477.post@n4.nabble.com>
Message-ID: <556CEFB5.4010308@treenet.co.nz>

On 2/06/2015 10:57 a.m., dkandle wrote:
> Slight correction to my iptables:
> iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.200/28 -p tcp --dport 80 -j
> REDIRECT --to-port 3128 
> iptables -t nat -A PREROUTING -i eth2 -s 10.1.10.200/28 -p tcp --dport 443
> -j REDIRECT --to-port 3129
> 
> I also don't understand why the first ip address specification didn't work
> but I had to change the 10.1.10.1 to 10.100 which is the exact ip address of
> the client. I thought the /28 would have caused this to match any IP address
> in the subnet.

It does.  Those .1 and .100 and .200 are each in completely different
subnets.

Hint: /28 is not /24.

Amos



From grusha at flywild.net  Mon Jun  1 23:41:58 2015
From: grusha at flywild.net (dkandle)
Date: Mon, 1 Jun 2015 16:41:58 -0700 (PDT)
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <556CEFB5.4010308@treenet.co.nz>
References: <1433176804852-4671472.post@n4.nabble.com>
 <2f76befd10cd632bc907028111f2c81d@localhost>
 <1433199267899-4671476.post@n4.nabble.com>
 <1433199479370-4671477.post@n4.nabble.com> <556CEFB5.4010308@treenet.co.nz>
Message-ID: <1433202118998-4671479.post@n4.nabble.com>

Amos Jeffries wrote
> On 2/06/2015 10:57 a.m., dkandle wrote:
> 
>> I also don't understand why the first ip address specification didn't
>> work
>> but I had to change the 10.1.10.1 to 10.100 which is the exact ip address
>> of
>> the client. I thought the /28 would have caused this to match any IP
>> address
>> in the subnet.
> 
> It does.  Those .1 and .100 and .200 are each in completely different
> subnets.
> 
> Hint: /28 is not /24.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Thanks. That was a dumb mistake on my part.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472p4671479.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From grusha at flywild.net  Tue Jun  2 01:11:41 2015
From: grusha at flywild.net (dkandle)
Date: Mon, 1 Jun 2015 18:11:41 -0700 (PDT)
Subject: [squid-users] Looking for a recomendation for tutorial for
 transparent proxy under Ubuntu
In-Reply-To: <1433202118998-4671479.post@n4.nabble.com>
References: <1433176804852-4671472.post@n4.nabble.com>
 <2f76befd10cd632bc907028111f2c81d@localhost>
 <1433199267899-4671476.post@n4.nabble.com>
 <1433199479370-4671477.post@n4.nabble.com> <556CEFB5.4010308@treenet.co.nz>
 <1433202118998-4671479.post@n4.nabble.com>
Message-ID: <1433207501985-4671480.post@n4.nabble.com>

I got things working (at least they appear to be working now). For some
unknown reason when I changed the iptables entry for port 443 from
10.1.10.200/28 to 10.1.10.0/24 it started to work. Even though the HTTP side
worked either way.
If anyone knows what was going on there I would appreciate understanding why
port 443 behaves differently from port 80



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Looking-for-a-recomendation-for-tutorial-for-transparent-proxy-under-Ubuntu-tp4671472p4671480.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From kl at vsen.dk  Tue Jun  2 08:33:32 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 02 Jun 2015 10:33:32 +0200
Subject: [squid-users] ssl_crtd breaks after short time
Message-ID: <556D6A5C.7020703@vsen.dk>

I've got squid 3.4.12 on centos 7, running with ssl bumping.
options for ssl_crtd in squid.conf: -s /etc/ssl/certs/cache/ -M 4MB -b 4096

After a while ssl stops working.

How can I make squid or ssl_crtd actually log errors?
Any hints as to what I can investigate to figure out what is happening here?

Details:
After a little while, the clients start doing this:

[root at web-t01 ~]# curl 
https://www.googleapis.com/analytics/v2.4/management/accounts/~all/webproperties/~all/profiles 

curl: (35) SSL connect error

for urls that have not been accessed successfully since recreation of certs.

And this for sites which HAVE been accessed successfully (after 
recreation - and before it breaks itself):

[root at web-t01 ~]# curl https://kbenhavns-kommune.clients.ubivox.com/xmlrpc/
curl: (51) SSL: certificate subject name 'squid CA' does not match 
target host name 'kbenhavns-kommune.clients.ubivox.com'

if I then recreate my certs folder for ssl_crtd cache folder (on squid 
server)- both work again:
[root at web-t01 ~]# curl https://kbenhavns-kommune.clients.ubivox.com/xmlrpc/
[root at web-t01 ~]# curl 
https://www.googleapis.com/analytics/v2.4/management/accounts/~all/webproperties/~all/profiles 

<?xml version="1.0" encoding="UTF-8"?><errors 
xmlns="http://schemas.google.com/g/2005"><error><domain>GData</domain><code>required</code><location 
type="header">Authorization</location>

there's no errors in squid logs.

If I try to run ssl_crtd to issue cert:
# /usr/lib64/squid/ssl_crtd -s /etc/ssl/certs/cache/ -M 4MB -b 4096
new_certificate 13 host=www.googleapis.com
/usr/lib64/squid/ssl_crtd: Error while parsing the crtd request: Broken 
signing certificate!

even though squid works.. so I seem to be testing wrongly..



-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From su.maji.ke at gmail.com  Tue Jun  2 09:15:36 2015
From: su.maji.ke at gmail.com (Irimajiri keisuke)
Date: Tue, 2 Jun 2015 18:15:36 +0900
Subject: [squid-users] Error Resolution (TunnelStateData::Connection:: error
	)
Message-ID: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>

Dear all,

I have to build a proxy server by using the squid.
The number of clients is 400 people.

I do not know the cause of the error message that appears in the cache.log.
In the weekday, I have come up with an error every few hours 8:00 to 18:00.
Access concentration I look like does not matter.

[cache.log error message]
2015/05/11 13:37:24| TunnelStateData::Connection:: error : FD 610:
read/write failure: (110) Connection timed out

Why I want to know whether this error has occurred.
Also, I want to know the impact on the user.

[squidclient mgr:filedescriptor]
Every five minutes record
extract FD610

It looks like an error has occurred in the use to which the terminal
of xxx.xxx.2.115 user.
Is it a problem of communication of the user and the proxy?

Active file descriptors:
File Type   Tout Nread  * Nwrite * Remote Address        Description
---- ------ ---- -------- -------- ---------------------
------------------------------
 610 Socket  893    39494*   50228  xxx.xxx.xxx.162:443
outlook.office365.com:443       2015/05/11_13:08:29
 610 Socket 86329   45754*  103329  xxx.xxx.6.141:50174   Reading next
request         2015/05/11_13:13:29
 610 Socket 86258    6516*   13975  xxx.xxx.2.115:50820   Reading next
request         2015/05/11_13:18:29
 610 Socket 85958   12472*   34531* xxx.xxx.2.115:50820   Reading next
request         2015/05/11_13:23:29
 610 Socket 85657   12472*   34531* xxx.xxx.2.115:50820   Reading next
request         2015/05/11_13:28:29
 610 Socket 85357   12472*   34531* xxx.xxx.2.115:50820   Reading next
request         2015/05/11_13:33:29
 610 Socket 86336    3652*    8003  xxx.xxx.3.152:50817   Reading next
request         2015/05/11_13:38:29

[access.log]
I do not see suspicious error log I tried to extract the address xxx.xxx.2.115.

Please tell me a good idea toward someone solve.
I look forward to hearing from you soon.

ken


From ahmed.zaeem at netstream.ps  Tue Jun  2 22:03:16 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Tue, 2 Jun 2015 15:03:16 -0700
Subject: [squid-users] squid return (TCP_MISS/403 353) on some specific
	Links on YouTube
Message-ID: <004701d09d7f$edce1da0$c96a58e0$@netstream.ps>

Hi , 

A developed  guy developed a script that convert Links of sommon sites like
youtube to other Links

The sciprt connecto to proxy and  transfer the Link.

 

The script works well for some websites and give response like :
https://www.youtube.com/watch?v=zYBgFeLCp3E

response on cache.log

==============

1433246384.626    245 195.154.200.58 TCP_MISS/200 38660 GET
http://www.youtube.com/get_video_info? - HIER_DIRECT/195.95.178.110
application/x-www-form-urlencoded

1433246384.802     62 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246385.027    125 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246386.239    123 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246386.469    121 195.154.200.58 TCP_MISS/200 455 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/x-flv

1433246386.709    139 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/3gpp

1433246386.941    121 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/3gpp

1433246387.181    131 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246387.334     61 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246387.756     61 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246387.927     61 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246388.097     71 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246388.267     72 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246389.432     62 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246389.614     62 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246389.798     82 195.154.200.58 TCP_MISS/200 452 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246389.958     72 195.154.200.58 TCP_MISS/200 452 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 audio/mp4

1433246390.127     63 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 audio/webm

===============

Response feedback from script :

xxxx:40000

HTTP/1.1 200 OK 
Content-Type: video/mp4 
Last-Modified: Thu, 23 Apr 2015 05:52:23 GMT 
Date: Tue, 02 Jun 2015 11:51:24 GMT 
Expires: Tue, 02 Jun 2015 11:51:24 GMT 
Cache-Control: private, max-age=21300 
Accept-Ranges: bytes 
Content-Length: 165062830 
HTTP/1.1 200 OK 
Content-Type: video/webm 
Last-Modified: Fri, 12 Dec 2014 12:46:34 GMT 
Date: Tue, 02 Jun 2015 11:51:24 GMT 
Expires: Tue, 02 Jun 2015 11:51:24 GMT 
Cache-Control: private, max-age=21300 
Accept-Ranges: bytes 
Content-Length: 52618326 
HTTP/1.1 200 OK 
Content-Type: video/mp4 
Last-Modified: Thu, 23 Apr 2015 05:49:33 GMT 
Date: Tue, 02 Jun 2015 11:51:24 GMT 
Expires: Tue, 02 Jun 2015 11:51:24 GMT 
Cache-Control: private, max-age=21300 
Accept-Ranges: bytes 
Content-Length: 46393506 
HTTP/1.1 200 OK 
Content-Type: video/x-flv 
Last-Modified: Fri, 12 Dec 2014 12:31:53 GMT 
Date: Tue, 02 Jun 2015 11:51:25 GMT 
Expires: Tue, 02 Jun 2015 11:51:25 GMT 
Cache-Control: private, max-age=21299 
Accept-Ranges: bytes 
Content-Length: 25122119 
HTTP/1.1 200 OK 
Content-Type: video/3gpp 
Last-Modified: Fri, 12 Dec 2014 12:40:48 GMT 
Date: Tue, 02 Jun 2015 11:51:25 GMT 
Expires: Tue, 02 Jun 2015 11:51:25 GMT 
Cache-Control: private, max-age=21299 
Accept-Ranges: bytes 
Content-Length: 17139471 
HTTP/1.1 200 OK 
Content-Type: video/3gpp 
Last-Modified: Fri, 12 Dec 2014 12:39:17 GMT 
Date: Tue, 02 Jun 2015 11:51:25 GMT 
Expires: Tue, 02 Jun 2015 11:51:25 GMT 
Cache-Control: private, max-age=21299 
Accept-Ranges: bytes 
Content-Length: 6248631 



=============

For some youtube links like vevo videos we have error TCPMIS/403 !!!

https://www.youtube.com/watch?v=ab9176Srb5Y

 

 

RECT/62.252.232.19 text/plain

1433246591.307    128 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.530    129 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.752    121 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.977    120 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246592.218    125 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

 

SCRIPT FEEDBACK 

 

XXX:40000

HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:03 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 


HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:04 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 


HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:05 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 


HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:05 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 


HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:06 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 


HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
HTTP/1.1 403 Forbidden 
Last-Modified: Wed, 02 May 2007 10:26:10 GMT 
Content-Type: text/plain 
Content-Length: 0 
X-Content-Type-Options: nosniff 
Date: Tue, 02 Jun 2015 12:03:06 GMT 
Server: gvs 1.0 
X-Cache: MISS from Largerock-squid 
X-Cache-Lookup: MISS from Largerock-squid:40000 
Via: 1.1 Largerock-squid (squid/3.5.2) 
Connection: keep-alive 

 

Squid.conf file :

 

cache_effective_user squid

cache_effective_group squid

#

# Recommended minimum configuration:

#

 

# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src xxxx/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

 

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

 

#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

 

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

 

# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager

 

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost

 

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

 

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost

 

# And finally deny all other access to this proxy

http_access deny all

 

# Squid normally listens to port 3128

http_port 3128

######################################

################################################

# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/cache/squid 100 16 256

 

# Leave coredumps in the first cache dir

coredump_dir /var/cache/squid

 

#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

Squid Cache: Version 3.5.2

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=xxx '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-esi'
'--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd'
'--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000' '--with-openssl'
'--enable-snmp' '--with-included-ltdl'

root at box2:~#

 

 

Any help why some sites give me response 403 and some sites ok on youtyube
????

 

 

thankx

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150602/1bb5e60d/attachment.htm>

From reet.vyas28 at gmail.com  Tue Jun  2 12:31:15 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Tue, 2 Jun 2015 18:01:15 +0530
Subject: [squid-users] Transparent Squid Proxy Server
Message-ID: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>

I am trying to configure transparent squid proxy on ubuntu 14.04 Server and
squid 3.3 version I am using

My Lan and Wan settings

eth0      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:74
          inet addr:116.72.*.*  Bcast:116.72.155.255  Mask:255.255.252.0
          inet6 addr: fe80::21e:67ff:fecf:5974/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:238950 errors:0 dropped:0 overruns:0 frame:0
          TX packets:236104 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:22219047 (22.2 MB)  TX bytes:17390502 (17.3 MB)
          Interrupt:16 Memory:d0a00000-d0a20000

eth1      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:75
          inet addr:192.168.0.200  Bcast:192.168.0.255  Mask:255.255.255.0
          inet6 addr: fe80::21e:67ff:fecf:5975/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:96965 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11785 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:10764615 (10.7 MB)  TX bytes:7151763 (7.1 MB)
          Interrupt:17 Memory:d0900000-d0920000

my squid.conf file

acl mynet src 116.72.152.37 192.168.0.0/16    # RFC1918 possible internal
network
acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow mynet
http_access allow localhost
http_access allow all
http_port 3128
cache_dir ufs /usr/local/cache 10000 16 256
coredump_dir /var/spool/squid3
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
refresh_pattern .        0    20%    4320


but when I use 192.168.0.200 in my client machine as gateway ... internet
is not working and I cant see logs in access.log

But when I use this IP in my browser it is working and showing logs but
with my tplink router  gateway i.e 192.168.0.1.

IPTable rules :
num  target     prot opt source               destination
1    DNAT       tcp  --  anywhere             anywhere             tcp
dpt:http to:192.168.0.200:3128
2    REDIRECT   tcp  --  anywhere             anywhere             tcp
dpt:http redir ports 3128

Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
num  target     prot opt source               destination


Please tell me what I am missing in IPtables and squid3 configuration . I
tried both transparent as well as intercept option but I think I have issue
with iptables or may be configuration issue.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150602/4204df57/attachment.htm>

From kl at vsen.dk  Tue Jun  2 13:20:03 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 02 Jun 2015 15:20:03 +0200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
Message-ID: <556DAD83.2030006@vsen.dk>

I have this in my squid server for it to work:
*mangle
:PREROUTING ACCEPT [190:618576]
:INPUT ACCEPT [190:618576]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [163:41506]
:POSTROUTING ACCEPT [166:42334]
-A PREROUTING -d $myip/32 -p tcp -m multiport --dports 3129 -m comment 
--comment "002 drop squid direct traffic http - we only allow captured 
traffic" -j DROP
-A PREROUTING -d $myip/32 -p tcp -m multiport --dports 3130 -m comment 
--comment "002 drop squid direct traffic https - we only allow captured 
traffic" -j DROP
COMMIT
# Completed on Wed Apr  1 10:28:22 2015
# Generated by iptables-save v1.4.21 on Wed Apr  1 10:28:22 2015
*nat
:PREROUTING ACCEPT [1:36]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [30:2079]
:POSTROUTING ACCEPT [30:2079]
-A PREROUTING -s $myip/32 -p tcp -m multiport --dports 80 -m comment 
--comment "000 allow squid http - so its traffic does not get captured" 
-j ACCEPT
-A PREROUTING -s $myip/32 -p tcp -m multiport --dports 443 -m comment 
--comment "000 allow squid https - so its traffic does not get captured" 
-j ACCEPT
-A PREROUTING -p tcp -m multiport --dports 80 -m comment --comment "001 
capture http to squid" -j DNAT --to-destination $myip:3129
-A PREROUTING -p tcp -m multiport --dports 443 -m comment --comment "001 
capture https to squid" -j DNAT --to-destination $myip:3130
COMMIT
# Completed on Wed Apr  1 10:28:22 2015
# Generated by iptables-save v1.4.21 on Wed Apr  1 10:28:22 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [1:184]
-A INPUT -p tcp -m multiport --ports 3129 -m comment --comment "000 
allow squid http intercept" -j ACCEPT
-A INPUT -p tcp -m multiport --ports 3130 -m comment --comment "000 
allow squid https intercept" -j ACCEPT
-A INPUT -p tcp -m multiport --ports 3128 -m comment --comment "000 
allow squid proxy" -j ACCEPT

and squid conf (mind you - squid 3.4)
ssl_bump                       server-first all
sslproxy_flags                 DONT_VERIFY_PEER
sslcrtd_children               8 startup=1 idle=1
sslcrtd_program                /usr/lib64/squid/ssl_crtd -s 
/etc/ssl/certs/cache/ -M 4MB
https_port                     3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
key=/etc/squid/ca.private cert=/etc/squid/ca.cert
shutdown_lifetime              3
always_direct                  allow all
sslproxy_cert_error            allow all
http_port                      3129 intercept

Reet Vyas wrote on 06/02/2015 02:31 PM:
> I am trying to configure transparent squid proxy on ubuntu 14.04 Server
> and squid 3.3 version I am using
>
> My Lan and Wan settings
>
> eth0      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:74
>            inet addr:116.72.*.*  Bcast:116.72.155.255  Mask:255.255.252.0
>            inet6 addr: fe80::21e:67ff:fecf:5974/64 Scope:Link
>            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
>            RX packets:238950 errors:0 dropped:0 overruns:0 frame:0
>            TX packets:236104 errors:0 dropped:0 overruns:0 carrier:0
>            collisions:0 txqueuelen:1000
>            RX bytes:22219047 (22.2 MB)  TX bytes:17390502 (17.3 MB)
>            Interrupt:16 Memory:d0a00000-d0a20000
>
> eth1      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:75
>            inet addr:192.168.0.200  Bcast:192.168.0.255  Mask:255.255.255.0
>            inet6 addr: fe80::21e:67ff:fecf:5975/64 Scope:Link
>            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
>            RX packets:96965 errors:0 dropped:0 overruns:0 frame:0
>            TX packets:11785 errors:0 dropped:0 overruns:0 carrier:0
>            collisions:0 txqueuelen:1000
>            RX bytes:10764615 (10.7 MB)  TX bytes:7151763 (7.1 MB)
>            Interrupt:17 Memory:d0900000-d0920000
>
> my squid.conf file
>
> acl mynet src 116.72.152.37 192.168.0.0/16 <http://192.168.0.0/16>    #
> RFC1918 possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow mynet
> http_access allow localhost
> http_access allow all
> http_port 3128
> cache_dir ufs /usr/local/cache 10000 16 256
> coredump_dir /var/spool/squid3
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
> refresh_pattern .        0    20%    4320
>
>
> but when I use 192.168.0.200 in my client machine as gateway ...
> internet is not working and I cant see logs in access.log
>
> But when I use this IP in my browser it is working and showing logs but
> with my tplink router  gateway i.e 192.168.0.1.
>
> IPTable rules :
> num  target     prot opt source               destination
> 1    DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:http to:192.168.0.200:3128 <http://192.168.0.200:3128>
> 2    REDIRECT   tcp  --  anywhere             anywhere             tcp
> dpt:http redir ports 3128
>
> Chain INPUT (policy ACCEPT)
> num  target     prot opt source               destination
>
> Chain OUTPUT (policy ACCEPT)
> num  target     prot opt source               destination
>
> Chain POSTROUTING (policy ACCEPT)
> num  target     prot opt source               destination
>
>
> Please tell me what I am missing in IPtables and squid3 configuration .
> I tried both transparent as well as intercept option but I think I have
> issue with iptables or may be configuration issue.
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Jun  2 13:38:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 01:38:57 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556D6A5C.7020703@vsen.dk>
References: <556D6A5C.7020703@vsen.dk>
Message-ID: <556DB1F1.1030309@treenet.co.nz>

On 2/06/2015 8:33 p.m., Klavs Klavsen wrote:
> I've got squid 3.4.12 on centos 7, running with ssl bumping.
> options for ssl_crtd in squid.conf: -s /etc/ssl/certs/cache/ -M 4MB -b 4096
> 
> After a while ssl stops working.

This would be one (or two) of the bugs fixed in the 3.4.13 release.

NOTE: please ensure you are using the latest version of Squid (today
thats 3.5.5) when SSL-bumping. That feature set is still quite volatile
and being updated constantly as issues are uncovered both in Squid and
in the TLS environment itself.

Amos



From kl at vsen.dk  Tue Jun  2 13:45:28 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 02 Jun 2015 15:45:28 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DB1F1.1030309@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
Message-ID: <556DB378.3020305@vsen.dk>

Thank you Amos.

I'll build 3.5.5 then..

any config changes I need to be aware of?

Amos Jeffries wrote on 06/02/2015 03:38 PM:
> On 2/06/2015 8:33 p.m., Klavs Klavsen wrote:
>> I've got squid 3.4.12 on centos 7, running with ssl bumping.
>> options for ssl_crtd in squid.conf: -s /etc/ssl/certs/cache/ -M 4MB -b 4096
>>
>> After a while ssl stops working.
>
> This would be one (or two) of the bugs fixed in the 3.4.13 release.
>
> NOTE: please ensure you are using the latest version of Squid (today
> thats 3.5.5) when SSL-bumping. That feature set is still quite volatile
> and being updated constantly as issues are uncovered both in Squid and
> in the TLS environment itself.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Jun  2 14:07:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 02:07:26 +1200
Subject: [squid-users] Error Resolution (TunnelStateData::Connection::
 error )
In-Reply-To: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>
References: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>
Message-ID: <556DB89E.1050305@treenet.co.nz>

On 2/06/2015 9:15 p.m., Irimajiri keisuke wrote:
> Dear all,
> 
> I have to build a proxy server by using the squid.
> The number of clients is 400 people.
> 
> I do not know the cause of the error message that appears in the cache.log.
> In the weekday, I have come up with an error every few hours 8:00 to 18:00.
> Access concentration I look like does not matter.
> 
> [cache.log error message]
> 2015/05/11 13:37:24| TunnelStateData::Connection:: error : FD 610:
> read/write failure: (110) Connection timed out
> 
> Why I want to know whether this error has occurred.

Yes it has occured. You would not be seeing it otherwise.

> Also, I want to know the impact on the user.

The user who is causing the problem is probably not impacted at all.
Every other user sharing the proxy is impacted by the reduction in
available network socket, memory and CPU resources.


> 
> [squidclient mgr:filedescriptor]
> Every five minutes record
> extract FD610
> 
> It looks like an error has occurred in the use to which the terminal
> of xxx.xxx.2.115 user.
> Is it a problem of communication of the user and the proxy?
> 

Nothing happened on a TCP conection for a long time. It was closed by
the networking sub-systems somewhere between Squid and the client.


> Active file descriptors:
> File Type   Tout Nread  * Nwrite * Remote Address        Description
> ---- ------ ---- -------- -------- ---------------------
> ------------------------------
>  610 Socket  893    39494*   50228  xxx.xxx.xxx.162:443
> outlook.office365.com:443       2015/05/11_13:08:29
>  610 Socket 86329   45754*  103329  xxx.xxx.6.141:50174   Reading next
> request         2015/05/11_13:13:29
>  610 Socket 86258    6516*   13975  xxx.xxx.2.115:50820   Reading next
> request         2015/05/11_13:18:29
>  610 Socket 85958   12472*   34531* xxx.xxx.2.115:50820   Reading next
> request         2015/05/11_13:23:29
>  610 Socket 85657   12472*   34531* xxx.xxx.2.115:50820   Reading next
> request         2015/05/11_13:28:29
>  610 Socket 85357   12472*   34531* xxx.xxx.2.115:50820   Reading next
> request         2015/05/11_13:33:29
>  610 Socket 86336    3652*    8003  xxx.xxx.3.152:50817   Reading next
> request         2015/05/11_13:38:29
> 
> [access.log]
> I do not see suspicious error log I tried to extract the address xxx.xxx.2.115.
> 
> Please tell me a good idea toward someone solve.

Please provided additional details:
 * Squid version
 * Squid configuration


I suspect you have a quite old verion of Squid. That particular error
message does not even exist in the code any more. The current releases
display much more TCP details about the connection where the error occured.

Amos


From squid3 at treenet.co.nz  Tue Jun  2 14:10:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 02:10:09 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DB378.3020305@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk>
Message-ID: <556DB941.3050804@treenet.co.nz>

On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
> Thank you Amos.
> 
> I'll build 3.5.5 then..
> 
> any config changes I need to be aware of?

--with-openssl instead of --enable-ssl is the only one that comes to
mind right now. The release notes for 3.4 and 3.5 have the lists.

Amos

> 
> Amos Jeffries wrote on 06/02/2015 03:38 PM:
>> On 2/06/2015 8:33 p.m., Klavs Klavsen wrote:
>>> I've got squid 3.4.12 on centos 7, running with ssl bumping.
>>> options for ssl_crtd in squid.conf: -s /etc/ssl/certs/cache/ -M 4MB
>>> -b 4096
>>>
>>> After a while ssl stops working.
>>
>> This would be one (or two) of the bugs fixed in the 3.4.13 release.
>>
>> NOTE: please ensure you are using the latest version of Squid (today
>> thats 3.5.5) when SSL-bumping. That feature set is still quite volatile
>> and being updated constantly as issues are uncovered both in Squid and
>> in the TLS environment itself.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> 



From squid3 at treenet.co.nz  Tue Jun  2 14:34:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 02:34:59 +1200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <556DAD83.2030006@vsen.dk>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk>
Message-ID: <556DBF13.4070000@treenet.co.nz>

On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
> I have this in my squid server for it to work:

The key words there are ... *in my Squid server*

Reet did it on the router. Which was the first mistake.

The router needs routing rules (not NAT) to deliver the clients packets
to Squid machine where the interception happens like below.

The second mistake was http_port configuration. Squid requires two
http_port lines. Port 3128 for regular proxy traffic, and another random
port for interception (our how-tos use 3129).


> *mangle
> :PREROUTING ACCEPT [190:618576]
> :INPUT ACCEPT [190:618576]
> :FORWARD ACCEPT [0:0]
> :OUTPUT ACCEPT [163:41506]
> :POSTROUTING ACCEPT [166:42334]
> -A PREROUTING -d $myip/32 -p tcp -m multiport --dports 3129 -m comment
> --comment "002 drop squid direct traffic http - we only allow captured
> traffic" -j DROP
> -A PREROUTING -d $myip/32 -p tcp -m multiport --dports 3130 -m comment
> --comment "002 drop squid direct traffic https - we only allow captured
> traffic" -j DROP
> COMMIT


NOTE to Klavs:
  loading the "multiport" kernel module seems overkill for a single-port
match.

> # Completed on Wed Apr  1 10:28:22 2015
> # Generated by iptables-save v1.4.21 on Wed Apr  1 10:28:22 2015
> *nat
> :PREROUTING ACCEPT [1:36]
> :INPUT ACCEPT [0:0]
> :OUTPUT ACCEPT [30:2079]
> :POSTROUTING ACCEPT [30:2079]
> -A PREROUTING -s $myip/32 -p tcp -m multiport --dports 80 -m comment
> --comment "000 allow squid http - so its traffic does not get captured"
> -j ACCEPT
> -A PREROUTING -s $myip/32 -p tcp -m multiport --dports 443 -m comment
> --comment "000 allow squid https - so its traffic does not get captured"
> -j ACCEPT
> -A PREROUTING -p tcp -m multiport --dports 80 -m comment --comment "001
> capture http to squid" -j DNAT --to-destination $myip:3129
> -A PREROUTING -p tcp -m multiport --dports 443 -m comment --comment "001
> capture https to squid" -j DNAT --to-destination $myip:3130
> COMMIT
> # Completed on Wed Apr  1 10:28:22 2015
> # Generated by iptables-save v1.4.21 on Wed Apr  1 10:28:22 2015
> *filter
> :INPUT ACCEPT [0:0]
> :FORWARD ACCEPT [0:0]
> :OUTPUT ACCEPT [1:184]
> -A INPUT -p tcp -m multiport --ports 3129 -m comment --comment "000
> allow squid http intercept" -j ACCEPT
> -A INPUT -p tcp -m multiport --ports 3130 -m comment --comment "000
> allow squid https intercept" -j ACCEPT
> -A INPUT -p tcp -m multiport --ports 3128 -m comment --comment "000
> allow squid proxy" -j ACCEPT
> 
> and squid conf (mind you - squid 3.4)
> ssl_bump                       server-first all
> sslproxy_flags                 DONT_VERIFY_PEER
> sslcrtd_children               8 startup=1 idle=1
> sslcrtd_program                /usr/lib64/squid/ssl_crtd -s
> /etc/ssl/certs/cache/ -M 4MB
> https_port                     3130 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> key=/etc/squid/ca.private cert=/etc/squid/ca.cert
> shutdown_lifetime              3
> always_direct                  allow all
> sslproxy_cert_error            allow all
> http_port                      3129 intercept
> 

FYI: DONT_VERIFY_PEER, "always_direct allow all", and
"slproxy_cert_error allow all" have not been good ideas since 3.2.
dont-verify actually inhibits the Mimic functions which give
server-first bumping most of its usefulness.



> Reet Vyas wrote on 06/02/2015 02:31 PM:
>> I am trying to configure transparent squid proxy on ubuntu 14.04 Server
>> and squid 3.3 version I am using
>>
>> My Lan and Wan settings
>>
>> eth0      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:74
>>            inet addr:116.72.*.*  Bcast:116.72.155.255  Mask:255.255.252.0
>>            inet6 addr: fe80::21e:67ff:fecf:5974/64 Scope:Link
>>            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
>>            RX packets:238950 errors:0 dropped:0 overruns:0 frame:0
>>            TX packets:236104 errors:0 dropped:0 overruns:0 carrier:0
>>            collisions:0 txqueuelen:1000
>>            RX bytes:22219047 (22.2 MB)  TX bytes:17390502 (17.3 MB)
>>            Interrupt:16 Memory:d0a00000-d0a20000
>>
>> eth1      Link encap:Ethernet  HWaddr 00:1e:67:cf:59:75
>>            inet addr:192.168.0.200  Bcast:192.168.0.255 
>> Mask:255.255.255.0
>>            inet6 addr: fe80::21e:67ff:fecf:5975/64 Scope:Link
>>            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
>>            RX packets:96965 errors:0 dropped:0 overruns:0 frame:0
>>            TX packets:11785 errors:0 dropped:0 overruns:0 carrier:0
>>            collisions:0 txqueuelen:1000
>>            RX bytes:10764615 (10.7 MB)  TX bytes:7151763 (7.1 MB)
>>            Interrupt:17 Memory:d0900000-d0920000


Er, thems not settings. Thems traffic statistics.

Not that it matters, but give these a try:
 ip addr show
 ip -4 route show
 ip -6 route show


>>
>> my squid.conf file
>>
>> acl mynet src 116.72.152.37 192.168.0.0/16 <http://192.168.0.0/16>    #
>> RFC1918 possible internal network
>> acl SSL_ports port 443
>> acl Safe_ports port 80        # http
>> acl Safe_ports port 21        # ftp
>> acl Safe_ports port 443        # https
>> acl Safe_ports port 70        # gopher
>> acl Safe_ports port 210        # wais
>> acl Safe_ports port 1025-65535    # unregistered ports
>> acl Safe_ports port 280        # http-mgmt
>> acl Safe_ports port 488        # gss-http
>> acl Safe_ports port 591        # filemaker
>> acl Safe_ports port 777        # multiling http
>> acl CONNECT method CONNECT
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> http_access allow mynet
>> http_access allow localhost
>> http_access allow all
>> http_port 3128

One listening port setup to receive explicit proxy traffic (ie from a
maually configured browser).

... missing an intercept port.


>> cache_dir ufs /usr/local/cache 10000 16 256
>> coredump_dir /var/spool/squid3
>> refresh_pattern ^ftp:        1440    20%    10080
>> refresh_pattern ^gopher:    1440    0%    1440
>> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
>> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
>> refresh_pattern .        0    20%    4320
>>
>>
>> but when I use 192.168.0.200 in my client machine as gateway ...
>> internet is not working and I cant see logs in access.log
>>
>> But when I use this IP in my browser it is working and showing logs but
>> with my tplink router  gateway i.e 192.168.0.1.
>>
>> IPTable rules :
>> num  target     prot opt source               destination
>> 1    DNAT       tcp  --  anywhere             anywhere             tcp
>> dpt:http to:192.168.0.200:3128 <http://192.168.0.200:3128>
>> 2    REDIRECT   tcp  --  anywhere             anywhere             tcp
>> dpt:http redir ports 3128
>>
>> Chain INPUT (policy ACCEPT)
>> num  target     prot opt source               destination
>>
>> Chain OUTPUT (policy ACCEPT)
>> num  target     prot opt source               destination
>>
>> Chain POSTROUTING (policy ACCEPT)
>> num  target     prot opt source               destination
>>
>>
>> Please tell me what I am missing in IPtables and squid3 configuration .
>> I tried both transparent as well as intercept option but I think I have
>> issue with iptables or may be configuration issue.
>>


see the wiki page(s):

One of these two configs on the router:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute>


This one on the Squid box:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Amos


From kl at vsen.dk  Tue Jun  2 14:44:07 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 02 Jun 2015 16:44:07 +0200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <556DBF13.4070000@treenet.co.nz>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
Message-ID: <556DC137.10106@vsen.dk>

Amos Jeffries wrote on 06/02/2015 04:34 PM:
> On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
>> I have this in my squid server for it to work:
>
> The key words there are ... *in my Squid server*
>
indeed :)

>
> NOTE to Klavs:
>    loading the "multiport" kernel module seems overkill for a single-port
> match.
>
it's puppets firewall module.. haven't had enough time to fix that module :)

>
> FYI: DONT_VERIFY_PEER, "always_direct allow all", and
> "slproxy_cert_error allow all" have not been good ideas since 3.2.
> dont-verify actually inhibits the Mimic functions which give
> server-first bumping most of its usefulness.
>
Thank you for those tips.

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From kl at vsen.dk  Tue Jun  2 14:46:31 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 02 Jun 2015 16:46:31 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DB941.3050804@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
Message-ID: <556DC1C7.6050806@vsen.dk>

Amos Jeffries wrote on 06/02/2015 04:10 PM:
> On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
>> Thank you Amos.
>>
>> I'll build 3.5.5 then..
>>
>> any config changes I need to be aware of?
>
> --with-openssl instead of --enable-ssl is the only one that comes to
> mind right now. The release notes for 3.4 and 3.5 have the lists.
>

I borrowed the spec from fedora 23.. :)

After installing 3.5.5 instead - it now complains when trying to issue 
certificate :(

squid cache log says:
Error negotiating SSL connection on FD 10: error:14094412:SSL 
routines:SSL3_READ_BYTES:sslv3 alert bad certificate

client gets:
curl: (51) SSL: certificate subject name '64.233.184.103' does not match 
target host name 'www.google.com'

any hints for tests I can do, to figure out the problem would be very 
much appreciated :)
-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Jun  2 15:05:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 03:05:54 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DC1C7.6050806@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk>
Message-ID: <556DC652.5080005@treenet.co.nz>

On 3/06/2015 2:46 a.m., Klavs Klavsen wrote:
> Amos Jeffries wrote on 06/02/2015 04:10 PM:
>> On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>> Thank you Amos.
>>>
>>> I'll build 3.5.5 then..
>>>
>>> any config changes I need to be aware of?
>>
>> --with-openssl instead of --enable-ssl is the only one that comes to
>> mind right now. The release notes for 3.4 and 3.5 have the lists.
>>
> 
> I borrowed the spec from fedora 23.. :)
> 
> After installing 3.5.5 instead - it now complains when trying to issue
> certificate :(
> 
> squid cache log says:
> Error negotiating SSL connection on FD 10: error:14094412:SSL
> routines:SSL3_READ_BYTES:sslv3 alert bad certificate
> 
> client gets:
> curl: (51) SSL: certificate subject name '64.233.184.103' does not match
> target host name 'www.google.com'
> 
> any hints for tests I can do, to figure out the problem would be very
> much appreciated :)

James Lay has just done some good investigations in his "SSL-bump deep
dive" thread(s). Compare what he came up with to your config

Amos


From turgut at kalfaoglu.com  Tue Jun  2 15:58:32 2015
From: turgut at kalfaoglu.com (=?UTF-8?B?dHVyZ3V0IGthbGZhb8SfbHU=?=)
Date: Tue, 02 Jun 2015 18:58:32 +0300
Subject: [squid-users] ssl_crtd helpers crashing too rapidly..
Message-ID: <556DD2A8.7080102@kalfaoglu.com>

Hello everyone.. I have been a squid user for a very long time.

Currently I set it up as transparent proxy at a small LAN, proxying http
and https as best as I can.

I get the
 (squid-1): The ssl_crtd helpers are crashing too rapidly, need help!
error.. selinux is disabled, and that ssl_db folder appears normal; all
files having the same size more or less.

I'm running 3.4.12.. I wrote a script to delete the ssl_db folder and
re-create it, and that fixes the issue.
I just wanted you to know that the bug exists at this version.

Regards, -turgut



From squid3 at treenet.co.nz  Wed Jun  3 04:45:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 16:45:51 +1200
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <1433121546223-4671467.post@n4.nabble.com>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
 <556BA3B1.7050701@treenet.co.nz> <1433118250561-4671465.post@n4.nabble.com>
 <1433121546223-4671467.post@n4.nabble.com>
Message-ID: <556E867F.8040603@treenet.co.nz>

On 1/06/2015 1:19 p.m., Marcel Fossua wrote:
> No luck 
> Still not getting result at all I think the issue could be with my Mikrotik
> box 
> 
> # Marking packets with DSCP (for Mikrotik 6.x) for cache hit content coming
> from SQUID Proxy
> 
> /ip firewall mangle 
> add action=mark-packet chain=prerouting  disabled=no dscp=12
> new-packet-mark=squid-connection passthrough=no comment="==SQUID - TOS 12
> =="
> 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4671467/Captura_de_pantalla_2015-05-29_a_las_21.png> 
> 

Um. Do you mean you are go with having the router mark the packets
instead of Squid?

Amos



From dm at belkam.com  Wed Jun  3 05:06:51 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Wed, 03 Jun 2015 09:06:51 +0400
Subject: [squid-users] 3.5.5 https problem
Message-ID: <556E8B6B.4010203@belkam.com>

Hello!

Just tried to install 3.5.5 on production proxy, users complained about 
slow https connections,
I see errors in cache.log like
2015/06/03 09:00:34 kid1| local=192.168.42.130:32922 
remote=213.180.193.119:443 FD 964 flags=1: read/write failure: (32) 
Broken pipe
2015/06/03 09:00:46 kid1| local=192.168.42.130:52239 
remote=178.154.131.216:443 FD 1111 flags=1: read/write failure: (32) 
Broken pipe
2015/06/03 09:01:56 kid1| local=192.168.42.130:34841 
remote=213.180.193.119:443 FD 467 flags=1: read/write failure: (32) 
Broken pipe

Switching back to 3.4.13 solved problem, but..
Any ideas what can cause this ?

Thank you!



From squid3 at treenet.co.nz  Wed Jun  3 05:42:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 17:42:03 +1200
Subject: [squid-users] Squid 3.5.4 OpenBSD workers registration timed out
In-Reply-To: <55697623.4050301@ifw-dresden.de>
References: <5566D9F7.70304@ifw-dresden.de> <55697623.4050301@ifw-dresden.de>
Message-ID: <556E93AB.9000905@treenet.co.nz>

On 30/05/2015 8:34 p.m., Henri Wahl wrote:
> 
>> Thanks for any hint and best regards
>>
> 
> Is there really nobody else using this combo of OpenBSD + Squid workers?

Quite possible. OpenBSD support was broken for a while during 3.3/3.4
lifecycles and most Squid installs are using Linux.

The workers registration problem should not have anything to do with
OpenBSD specificaly though. Its directly related to size of rock caches
and the disk I/O speeds.


If I assume you have say a 50GB rock cache...

 (50 GB / 32 KB blocks) * 2 IO per block
    => 3 million disk I/O cycles to load the cache.

Your Squid has 6 seconds to do that, allocate the index memory in many
small blocks as it goes, and then respond to the coordinator.

Recalculate for your actual cache size and check:
* Can your disk handle that speed for non-sequential I/O ?
 - technically sequential, but skipping 7 out of each 8 inode "blocks"
can do weird things in the disk controller.

If you have more than one rock cache you need to add them all together.
UFS/aufs/diskd caches are much faster but the swap.state journals can
still be quite large with lots of I/O required to load - and also add
that as non-sequential I/O on top of the pile from rock cache.


I'm working on a patch to make the timeout slightly more configurable
(at build time) and fix a bug I found in the messaging retries. Would
you be happy to test that out?

Amos



From squid3 at treenet.co.nz  Wed Jun  3 06:01:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 18:01:33 +1200
Subject: [squid-users] 3.5.5 https problem
In-Reply-To: <556E8B6B.4010203@belkam.com>
References: <556E8B6B.4010203@belkam.com>
Message-ID: <556E983D.3040302@treenet.co.nz>

On 3/06/2015 5:06 p.m., Dmitry Melekhov wrote:
> Hello!
> 
> Just tried to install 3.5.5 on production proxy, users complained about
> slow https connections,
> I see errors in cache.log like
> 2015/06/03 09:00:34 kid1| local=192.168.42.130:32922
> remote=213.180.193.119:443 FD 964 flags=1: read/write failure: (32)
> Broken pipe
> 2015/06/03 09:00:46 kid1| local=192.168.42.130:52239
> remote=178.154.131.216:443 FD 1111 flags=1: read/write failure: (32)
> Broken pipe
> 2015/06/03 09:01:56 kid1| local=192.168.42.130:34841
> remote=213.180.193.119:443 FD 467 flags=1: read/write failure: (32)
> Broken pipe
> 
> Switching back to 3.4.13 solved problem, but..
> Any ideas what can cause this ?

It is a socket being closed from the other end. The closure signal
arrives while Squid is trying to write to it.

Could be bug 3329 causing Squid confusion though.

NP: The patch for that bug is now in Squid-4 (trunk / HEAD / dev /
"pre-beta") and will be backported to 3.5 when its had some time to
settle, feedback from testing will speed that up if you can and wish to
assist.

Amos



From kl at vsen.dk  Wed Jun  3 06:17:57 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Wed, 03 Jun 2015 08:17:57 +0200
Subject: [squid-users] ssl_crtd helpers crashing too rapidly..
In-Reply-To: <556DD2A8.7080102@kalfaoglu.com>
References: <556DD2A8.7080102@kalfaoglu.com>
Message-ID: <556E9C15.4020507@vsen.dk>

I just wrote with the same issue.. According to Amos this is fixed in 
later version, and you should run latest (currently 3.5.5) if you want 
transparent proxy'ing with https to work.

I haven't gotten 3.5.5 to work yet in my end.

turgut kalfao?lu wrote on 06/02/2015 05:58 PM:
> Hello everyone.. I have been a squid user for a very long time.
>
> Currently I set it up as transparent proxy at a small LAN, proxying http
> and https as best as I can.
>
> I get the
>   (squid-1): The ssl_crtd helpers are crashing too rapidly, need help!
> error.. selinux is disabled, and that ssl_db folder appears normal; all
> files having the same size more or less.
>
> I'm running 3.4.12.. I wrote a script to delete the ssl_db folder and
> re-create it, and that fixes the issue.
> I just wanted you to know that the bug exists at this version.
>
> Regards, -turgut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From reet.vyas28 at gmail.com  Wed Jun  3 06:50:57 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Wed, 3 Jun 2015 12:20:57 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <556DC137.10106@vsen.dk>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
Message-ID: <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>

Hi

Thanks for reply. As of now we don't have router I have directly connected
my machine to internet and other to LAN and I have configured client
machine ubuntu to test squid which is in switch where other users are
connected using gateway of router 192.168.0.1.

I read your valuable suggestions, but I still confused with IPtables and
squid 3.3 setting ,transparent and intercept options .

root at squid:/home/squid#   ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group
default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state
UP group default qlen 1000
    link/ether 00:1e:67:cf:59:74 brd ff:ff:ff:ff:ff:ff
    inet 116.72.*.*/22 brd 116.72.155.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::21e:67ff:fecf:5974/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state
UP group default qlen 1000
    link/ether 00:1e:67:cf:59:75 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.200/24 brd 192.168.0.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::21e:67ff:fecf:5975/64 scope link
       valid_lft forever preferred_lft forever

root at squid:/home/squid#  ip -4 route show
default via 116.72.152.1 dev eth0
116.72.152.0/22 dev eth0  proto kernel  scope link  src 116.72.152.37
192.168.0.0/24 dev eth1  proto kernel  scope link  src 192.168.0.200





To use transparent/intercept what I have to set in my config file http_port
3128 intercept or transparent

and Iptables rules , I have tried this rules

http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

But not working

Can you please tell me the firewall rules and let me know why my firewall
rules are not working.

On Tue, Jun 2, 2015 at 8:14 PM, Klavs Klavsen <kl at vsen.dk> wrote:

> Amos Jeffries wrote on 06/02/2015 04:34 PM:
>
>> On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
>>
>>> I have this in my squid server for it to work:
>>>
>>
>> The key words there are ... *in my Squid server*
>>
>>  indeed :)
>
>
>> NOTE to Klavs:
>>    loading the "multiport" kernel module seems overkill for a single-port
>> match.
>>
>>  it's puppets firewall module.. haven't had enough time to fix that
> module :)
>
>
>> FYI: DONT_VERIFY_PEER, "always_direct allow all", and
>> "slproxy_cert_error allow all" have not been good ideas since 3.2.
>> dont-verify actually inhibits the Mimic functions which give
>> server-first bumping most of its usefulness.
>>
>>  Thank you for those tips.
>
> --
> Regards,
> Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200
>
> "Those who do not understand Unix are condemned to reinvent it, poorly."
>   --Henry Spencer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150603/df28eecb/attachment.htm>

From kl at vsen.dk  Wed Jun  3 07:34:53 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Wed, 03 Jun 2015 09:34:53 +0200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
Message-ID: <556EAE1D.8020507@vsen.dk>

Your client needs to use your squid server as default gateway.

And then you need the iptables rules I wrote about to direct traffic 
into squid for certain ports.

Reet Vyas wrote on 06/03/2015 08:50 AM:
> Hi
>
> Thanks for reply. As of now we don't have router I have directly
> connected my machine to internet and other to LAN and I have configured
> client machine ubuntu to test squid which is in switch where other users
> are connected using gateway of router 192.168.0.1.
>
> I read your valuable suggestions, but I still confused with IPtables and
> squid 3.3 setting ,transparent and intercept options .
>
> root at squid:/home/squid#   ip addr show
> 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
> group default
>      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
>      inet 127.0.0.1/8 <http://127.0.0.1/8> scope host lo
>         valid_lft forever preferred_lft forever
>      inet6 ::1/128 scope host
>         valid_lft forever preferred_lft forever
> 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
> state UP group default qlen 1000
>      link/ether 00:1e:67:cf:59:74 brd ff:ff:ff:ff:ff:ff
>      inet 116.72.*.*/22 brd 116.72.155.255 scope global eth0
>         valid_lft forever preferred_lft forever
>      inet6 fe80::21e:67ff:fecf:5974/64 scope link
>         valid_lft forever preferred_lft forever
> 3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
> state UP group default qlen 1000
>      link/ether 00:1e:67:cf:59:75 brd ff:ff:ff:ff:ff:ff
>      inet 192.168.0.200/24 <http://192.168.0.200/24> brd 192.168.0.255
> scope global eth1
>         valid_lft forever preferred_lft forever
>      inet6 fe80::21e:67ff:fecf:5975/64 scope link
>         valid_lft forever preferred_lft forever
>
> root at squid:/home/squid#  ip -4 route show
> default via 116.72.152.1 dev eth0
> 116.72.152.0/22 <http://116.72.152.0/22> dev eth0  proto kernel  scope
> link  src 116.72.152.37
> 192.168.0.0/24 <http://192.168.0.0/24> dev eth1  proto kernel  scope
> link  src 192.168.0.200
>
>
>
>
>
> To use transparent/intercept what I have to set in my config file
> http_port 3128 intercept or transparent
>
> and Iptables rules , I have tried this rules
>
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
>
> But not working
>
> Can you please tell me the firewall rules and let me know why my
> firewall rules are not working.
>
> On Tue, Jun 2, 2015 at 8:14 PM, Klavs Klavsen <kl at vsen.dk
> <mailto:kl at vsen.dk>> wrote:
>
>     Amos Jeffries wrote on 06/02/2015 04:34 PM:
>
>         On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
>
>             I have this in my squid server for it to work:
>
>
>         The key words there are ... *in my Squid server*
>
>     indeed :)
>
>
>         NOTE to Klavs:
>             loading the "multiport" kernel module seems overkill for a
>         single-port
>         match.
>
>     it's puppets firewall module.. haven't had enough time to fix that
>     module :)
>
>
>         FYI: DONT_VERIFY_PEER, "always_direct allow all", and
>         "slproxy_cert_error allow all" have not been good ideas since 3.2.
>         dont-verify actually inhibits the Mimic functions which give
>         server-first bumping most of its usefulness.
>
>     Thank you for those tips.
>
>     --
>     Regards,
>     Klavs Klavsen, GSEC - kl at vsen.dk <mailto:kl at vsen.dk> -
>     http://www.vsen.dk - Tlf. 61281200
>
>     "Those who do not understand Unix are condemned to reinvent it, poorly."
>        --Henry Spencer
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From marcel at guineanet.net  Wed Jun  3 10:46:38 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Wed, 3 Jun 2015 03:46:38 -0700 (PDT)
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <556E867F.8040603@treenet.co.nz>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
 <556BA3B1.7050701@treenet.co.nz> <1433118250561-4671465.post@n4.nabble.com>
 <1433121546223-4671467.post@n4.nabble.com> <556E867F.8040603@treenet.co.nz>
Message-ID: <556EDF59.8010509@guineanet.net>

Hi Amos not really
after setting TOS config on Squid the idea is to allow Mikrotik router 
recognize
marked  paquets (as on previous squid 3.1.x)
and then mark cache content, so that it can later pick by Mikrotik
to deliver the already cached content to user at full lan speed, no 
queue on cache content.

 1.
    /ip firewall mangle
 2.
    add action=mark-connection chain=postrouting comment="==SQUID - TOS
    12==" disabled=no dscp=12 \
 3.
    new-connection-mark=squid-connection passthrough=yes protocol=tcp
    src-address=192.168.10.2
 4.
    add action=mark-packet chain=postrouting
    connection-mark=squid-connection disabled=\
 5.
    no new-packet-mark=squid-packs passthrough=yes



El 3/6/15 a las 5:28, Amos Jeffries [via Squid Web Proxy Cache] escribi?:
> On 1/06/2015 1:19 p.m., Marcel Fossua wrote:
>
> > No luck
> > Still not getting result at all I think the issue could be with my 
> Mikrotik
> > box
> >
> > # Marking packets with DSCP (for Mikrotik 6.x) for cache hit content 
> coming
> > from SQUID Proxy
> >
> > /ip firewall mangle
> > add action=mark-packet chain=prerouting  disabled=no dscp=12
> > new-packet-mark=squid-connection passthrough=no comment="==SQUID - 
> TOS 12
> > =="
> >
> > 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4671467/Captura_de_pantalla_2015-05-29_a_las_21.png> 
>
> >
>
> Um. Do you mean you are go with having the router mark the packets
> instead of Squid?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=4671496&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459p4671496.html 
>
> To unsubscribe from TOS squid-3.5.0.4, click here 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4671459&code=bWFyY2VsQGd1aW5lYW5ldC5uZXR8NDY3MTQ1OXw4NDM0NzU1NzE=>.
> NAML 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>

-- 
Fossua-vcard
	 Marcel Fossua
Unix/Linux Network Administrator
   Tel: 0240 222299448
www.guineanet.net <http://www.guineanet.net>/ www.familyfossua.com 
<http://www.familyfossua.com>










guineanet.png (33K) <http://squid-web-proxy-cache.1019090.n4.nabble.com/attachment/4671503/0/guineanet.png>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459p4671503.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jun  3 11:23:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 03 Jun 2015 23:23:26 +1200
Subject: [squid-users] when using a search box on een website my hole
 internet explorer freezes incuding earlier opend tabs
In-Reply-To: <510FCECD6E595A4D83BF67FC07028E7504F63A60@BHMB-01.bnh.local>
References: <510FCECD6E595A4D83BF67FC07028E75D39F6F@BHMB-02.bnh.local>
 <54C8C717.2030508@treenet.co.nz>
 <510FCECD6E595A4D83BF67FC07028E7504F63A60@BHMB-01.bnh.local>
Message-ID: <556EE3AE.7030107@treenet.co.nz>

On 3/06/2015 10:38 p.m., Jeroen Ruijter wrote:
> Dear Amos,
> 
> When we use this website www.rechtspraak.nl and enter a search term in the search box the internet explorer session freezes.
> We are unable to close a window with control + w or with the mouse pressing the cross at the corner.
> 
> I cannot start a new tab, all I can do is start e new instance of internet explorer but the previous session stays open and can still not be closed.
> 

Well, that sounds like your browser literally F**ing itself over.

There is nothing I'm aware of that Squid does that would cause that
extreme result, if there was it qualifies as a remotely executable
security vulnerability since any attacker web server could also do it.


> Only with powershell I can close internet explorer.
> 
> We use squid with authentication against our active directory (LDAP) and I cannot find any leads in the access log file when it happens.
> 
> Any ideas?

It's not Squid. The worst Squid can do is not respond, respond with
wrong content, or with unexpected HTTP header set (that includes
omitting application-expected ones). That is all perfectly normal error
cases, no reason the browser should even blink.

Sometimes browser bugs or broken javascripts can cause blank pages or
hanging tabs. But that is not Squid related, and nowhere near as extreme
as the symptoms you are mentioning.

Amos



From turgut at kalfaoglu.com  Wed Jun  3 13:51:51 2015
From: turgut at kalfaoglu.com (=?UTF-8?B?dHVyZ3V0IGthbGZhb8SfbHU=?=)
Date: Wed, 03 Jun 2015 16:51:51 +0300
Subject: [squid-users] when using a search box on een website my hole
 internet explorer freezes incuding earlier opend tabs
In-Reply-To: <556EE3AE.7030107@treenet.co.nz>
References: <510FCECD6E595A4D83BF67FC07028E75D39F6F@BHMB-02.bnh.local>
 <54C8C717.2030508@treenet.co.nz>
 <510FCECD6E595A4D83BF67FC07028E7504F63A60@BHMB-01.bnh.local>
 <556EE3AE.7030107@treenet.co.nz>
Message-ID: <556F0677.9090402@kalfaoglu.com>

On 06/03/2015 02:23 PM, Amos Jeffries wrote:
> On 3/06/2015 10:38 p.m., Jeroen Ruijter wrote:
>> Dear Amos,
>>
>> When we use this website www.rechtspraak.nl and enter a search term in the search box the internet explorer session freezes.
>> We are unable to close a window with control + w or with the mouse pressing the cross at the corner.
I believe that's the normal behavior for Internet Explorer..
-turgut



From apani at yandex.ru  Wed Jun  3 14:27:27 2015
From: apani at yandex.ru (sp_)
Date: Wed, 3 Jun 2015 07:27:27 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
References: <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
 <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
Message-ID: <1433341647461-4671506.post@n4.nabble.com>

Hello Nathan,

thank you for an example.

What version of squid are you running?
Mine is:


I've tried to apply the config you've posted, but with no luck. Squid can't
get the domain:




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671506.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jun  3 15:05:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 03:05:36 +1200
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1433341647461-4671506.post@n4.nabble.com>
References: <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
 <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
 <1433341647461-4671506.post@n4.nabble.com>
Message-ID: <556F17C0.4000601@treenet.co.nz>

On 4/06/2015 2:27 a.m., sp_ wrote:
> Hello Nathan,
> 
> thank you for an example.
> 
> What version of squid are you running?
> Mine is:
> 
> 
> I've tried to apply the config you've posted, but with no luck. Squid can't
> get the domain:
> 
> 

Well, its not a simple situation. Lets start with clarifying some of the
details...

 SNI is a relatively new feature of TLS. There is no guarantee of a
domain name actually existing in the bumped (step1) metadata.

So, Squid may have to do a peek at step2 to get the server cert details
before it has any clue about what domain *might* be.

Also that means the %ssl::>sni helper format token depended on with the
ACL helper approach will be "-" for these requests.

To resolve that use the new (in squid-3.5.4) ssl::server_name ACL
instead. Which checks against the CONNECT hostname (if any) at step1+,
SNI domain (if any) at step2+, and server cert domain at step3.

Amos



From rafael.akchurin at diladele.com  Wed Jun  3 17:00:59 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 3 Jun 2015 17:00:59 +0000
Subject: [squid-users] when using a search box on een website my hole
 internet explorer freezes incuding earlier opend tabs
In-Reply-To: <556F0677.9090402@kalfaoglu.com>
References: <510FCECD6E595A4D83BF67FC07028E75D39F6F@BHMB-02.bnh.local>
 <54C8C717.2030508@treenet.co.nz>
 <510FCECD6E595A4D83BF67FC07028E7504F63A60@BHMB-01.bnh.local>
 <556EE3AE.7030107@treenet.co.nz> <556F0677.9090402@kalfaoglu.com>
Message-ID: <DB5PR04MB1128B03EDEC4ECA6DB9612098FB40@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hi all,

I am seeing the same in the IE and also in FF - but FF does not freeze everything - just this window.
It may mean something is wrong with the site itself - of course IE is incorrect but nevertheless...

Raf

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of turgut kalfaoglu
Sent: Wednesday, June 3, 2015 3:52 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] when using a search box on een website my hole internet explorer freezes incuding earlier opend tabs

On 06/03/2015 02:23 PM, Amos Jeffries wrote:
> On 3/06/2015 10:38 p.m., Jeroen Ruijter wrote:
>> Dear Amos,
>>
>> When we use this website www.rechtspraak.nl and enter a search term in the search box the internet explorer session freezes.
>> We are unable to close a window with control + w or with the mouse pressing the cross at the corner.
I believe that's the normal behavior for Internet Explorer..
-turgut

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From marcel at guineanet.net  Wed Jun  3 17:18:00 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Wed, 3 Jun 2015 10:18:00 -0700 (PDT)
Subject: [squid-users] worker per cache_dir
Message-ID: <1433351880614-4671510.post@n4.nabble.com>

Hi All 
hope someone can give me a way to accomplish what I have in mind 
I have 3.5.5 running with 9 workers active (the best numbers a get without
errors)
so I just set 1 worker per disk as schema below but I have a jbod with lot
of disk I would like to add on squid.conf obviously the idea of 1 worker
/disk is not longer a good deal then what is the best config to make let say
worker 1 deal with several cache (3 or 4 for the occurence) 
so I could set 3 o 4 cache_dir per worker.

Thanks

/cache_dir rock /cache1 460000 min-size=1 max-size=31000 max-swap-rate=200 
swap-timeout=300

#  200GB x 8 caches of large ( over 32KB) objects per-worker 
if ${process_number} = 1
cache_dir aufs /cache2   275000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 2
cache_dir aufs /cache3   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 3
cache_dir aufs /cache4   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 4
cache_dir aufs /cache5   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 5
cache_dir aufs /cache6   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 6
cache_dir aufs /cache7   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 7
cache_dir aufs /cache8   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 8
cache_dir aufs /cache9   190000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 9
cache_dir aufs /cache10   190000  32 256 min-size=31001 max-size=1048576000
endif/



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/worker-per-cache-dir-tp4671510.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jonathan.filogna at tasso.com.ar  Wed Jun  3 18:34:41 2015
From: jonathan.filogna at tasso.com.ar (rocaembole)
Date: Wed, 3 Jun 2015 11:34:41 -0700 (PDT)
Subject: [squid-users] Skype issue
In-Reply-To: <1400769327828-4666074.post@n4.nabble.com>
References: <1400769327828-4666074.post@n4.nabble.com>
Message-ID: <1433356481708-4671511.post@n4.nabble.com>

Hey guys, here from Argentina, i am having the same issue.

when Skype is trying to log in, this is what i've found at access.log

1433357138.206     31 10.0.0.110 TCP_DENIED/403 3437 CONNECT
157.55.130.161:443 - NONE/- text/html
1433357139.216     30 10.0.0.110 TCP_DENIED/403 3437 CONNECT
157.55.130.148:443 - NONE/- text/html
1433357140.263     49 10.0.0.110 TCP_DENIED/403 3433 CONNECT
65.55.223.38:443 - NONE/- text/html
1433357141.267      9 10.0.0.110 TCP_DENIED/403 3437 CONNECT
157.55.130.175:443 - NONE/- text/html
1433357143.230     35 10.0.0.110 TCP_DENIED/403 3435 CONNECT
111.221.74.33:443 - NONE/- text/html
1433357144.243     38 10.0.0.110 TCP_DENIED/403 3439 CONNECT
213.199.179.140:443 - NONE/- text/html


I'm getting a 403 because squid is not receiving the user credentials (AD)
and i can't find the problem.

Any ideas?

thank you all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Skype-issue-tp4666074p4671511.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun  4 02:36:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 14:36:48 +1200
Subject: [squid-users] Skype issue
In-Reply-To: <1433356481708-4671511.post@n4.nabble.com>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com>
Message-ID: <556FB9C0.5070301@treenet.co.nz>

On 4/06/2015 6:34 a.m., rocaembole wrote:
> Hey guys, here from Argentina, i am having the same issue.
> 
> when Skype is trying to log in, this is what i've found at access.log
> 
> 1433357138.206     31 10.0.0.110 TCP_DENIED/403 3437 CONNECT
> 157.55.130.161:443 - NONE/- text/html
> 1433357139.216     30 10.0.0.110 TCP_DENIED/403 3437 CONNECT
> 157.55.130.148:443 - NONE/- text/html
> 1433357140.263     49 10.0.0.110 TCP_DENIED/403 3433 CONNECT
> 65.55.223.38:443 - NONE/- text/html
> 1433357141.267      9 10.0.0.110 TCP_DENIED/403 3437 CONNECT
> 157.55.130.175:443 - NONE/- text/html
> 1433357143.230     35 10.0.0.110 TCP_DENIED/403 3435 CONNECT
> 111.221.74.33:443 - NONE/- text/html
> 1433357144.243     38 10.0.0.110 TCP_DENIED/403 3439 CONNECT
> 213.199.179.140:443 - NONE/- text/html
> 
> 
> I'm getting a 403 because squid is not receiving the user credentials (AD)
> and i can't find the problem.
> 
> Any ideas?

Squid should be emitting "407 Proxy Authentication Required" if
credentials were the problem. "403 Forbidden" is absolute denial, with
no automated recovery possible by the client.

Amos



From squid3 at treenet.co.nz  Thu Jun  4 02:43:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 14:43:04 +1200
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433351880614-4671510.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
Message-ID: <556FBB38.4090708@treenet.co.nz>

On 4/06/2015 5:18 a.m., Marcel Fossua wrote:
> Hi All 
> hope someone can give me a way to accomplish what I have in mind 
> I have 3.5.5 running with 9 workers active (the best numbers a get without
> errors)
> so I just set 1 worker per disk as schema below but I have a jbod with lot
> of disk I would like to add on squid.conf obviously the idea of 1 worker
> /disk is not longer a good deal then what is the best config to make let say
> worker 1 deal with several cache (3 or 4 for the occurence) 
> so I could set 3 o 4 cache_dir per worker.
> 
> Thanks

Sure, inside the worker if...endif block list the other cache_dir you
want it to deal with. Each worker can have up to 63.

So long as the AUFS cache_dir are only used by one worker it will work
fine. It is the other way around which is broken - multiple worker using
one AUFS cache_dir.

Amos



From reet.vyas28 at gmail.com  Thu Jun  4 06:43:16 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Thu, 4 Jun 2015 12:13:16 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <556EAE1D.8020507@vsen.dk>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
Message-ID: <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>

Hi,

I changed the iptables still no luck :( but I am using squid 3.3 only can I
didn't understand why you have configured 3129 ,3130 and 3128 port?

On Wed, Jun 3, 2015 at 1:04 PM, Klavs Klavsen <kl at vsen.dk> wrote:

> Your client needs to use your squid server as default gateway.
>
> And then you need the iptables rules I wrote about to direct traffic into
> squid for certain ports.
>
> Reet Vyas wrote on 06/03/2015 08:50 AM:
>
>> Hi
>>
>> Thanks for reply. As of now we don't have router I have directly
>> connected my machine to internet and other to LAN and I have configured
>> client machine ubuntu to test squid which is in switch where other users
>> are connected using gateway of router 192.168.0.1.
>>
>> I read your valuable suggestions, but I still confused with IPtables and
>> squid 3.3 setting ,transparent and intercept options .
>>
>> root at squid:/home/squid#   ip addr show
>> 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
>> group default
>>      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
>>      inet 127.0.0.1/8 <http://127.0.0.1/8> scope host lo
>>         valid_lft forever preferred_lft forever
>>      inet6 ::1/128 scope host
>>         valid_lft forever preferred_lft forever
>> 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
>> state UP group default qlen 1000
>>      link/ether 00:1e:67:cf:59:74 brd ff:ff:ff:ff:ff:ff
>>      inet 116.72.*.*/22 brd 116.72.155.255 scope global eth0
>>         valid_lft forever preferred_lft forever
>>      inet6 fe80::21e:67ff:fecf:5974/64 scope link
>>         valid_lft forever preferred_lft forever
>> 3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
>> state UP group default qlen 1000
>>      link/ether 00:1e:67:cf:59:75 brd ff:ff:ff:ff:ff:ff
>>      inet 192.168.0.200/24 <http://192.168.0.200/24> brd 192.168.0.255
>> scope global eth1
>>         valid_lft forever preferred_lft forever
>>      inet6 fe80::21e:67ff:fecf:5975/64 scope link
>>         valid_lft forever preferred_lft forever
>>
>> root at squid:/home/squid#  ip -4 route show
>> default via 116.72.152.1 dev eth0
>> 116.72.152.0/22 <http://116.72.152.0/22> dev eth0  proto kernel  scope
>> link  src 116.72.152.37
>> 192.168.0.0/24 <http://192.168.0.0/24> dev eth1  proto kernel  scope
>> link  src 192.168.0.200
>>
>>
>>
>>
>>
>> To use transparent/intercept what I have to set in my config file
>> http_port 3128 intercept or transparent
>>
>> and Iptables rules , I have tried this rules
>>
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
>>
>> But not working
>>
>> Can you please tell me the firewall rules and let me know why my
>> firewall rules are not working.
>>
>> On Tue, Jun 2, 2015 at 8:14 PM, Klavs Klavsen <kl at vsen.dk
>> <mailto:kl at vsen.dk>> wrote:
>>
>>     Amos Jeffries wrote on 06/02/2015 04:34 PM:
>>
>>         On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
>>
>>             I have this in my squid server for it to work:
>>
>>
>>         The key words there are ... *in my Squid server*
>>
>>     indeed :)
>>
>>
>>         NOTE to Klavs:
>>             loading the "multiport" kernel module seems overkill for a
>>         single-port
>>         match.
>>
>>     it's puppets firewall module.. haven't had enough time to fix that
>>     module :)
>>
>>
>>         FYI: DONT_VERIFY_PEER, "always_direct allow all", and
>>         "slproxy_cert_error allow all" have not been good ideas since 3.2.
>>         dont-verify actually inhibits the Mimic functions which give
>>         server-first bumping most of its usefulness.
>>
>>     Thank you for those tips.
>>
>>     --
>>     Regards,
>>     Klavs Klavsen, GSEC - kl at vsen.dk <mailto:kl at vsen.dk> -
>>     http://www.vsen.dk - Tlf. 61281200
>>
>>     "Those who do not understand Unix are condemned to reinvent it,
>> poorly."
>>        --Henry Spencer
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> --
> Regards,
> Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200
>
> "Those who do not understand Unix are condemned to reinvent it, poorly."
>   --Henry Spencer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/44970420/attachment.htm>

From apani at yandex.ru  Thu Jun  4 06:29:32 2015
From: apani at yandex.ru (sp_)
Date: Wed, 3 Jun 2015 23:29:32 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <556F17C0.4000601@treenet.co.nz>
References: <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
 <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
 <1433341647461-4671506.post@n4.nabble.com> <556F17C0.4000601@treenet.co.nz>
Message-ID: <1433399372421-4671515.post@n4.nabble.com>

Hello Amos,

thank you for your reply.

Let's take for instance this line:


I have dumped the traffic passing through the interface on the router during
this request. 
In client hello in Extension "server_name" I can see the domain:


According to RFC, domain is a must in Client Hello, when SNI is used. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671515.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From kl at vsen.dk  Thu Jun  4 07:55:20 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 09:55:20 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DC652.5080005@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
Message-ID: <55700468.2050807@vsen.dk>

Hi Amos,

I tried taking the config from James.. but I have the exact same issue 
as described below :(

After adding the extra logging from James config - I get this in access_log:
1433404085.331      0 10.47.171.244 TCP_DENIED/200 0 CONNECT 
216.58.209.106:443 - HIER_NONE/- -

which makes it seem as if squid does NOT see the url I'm trying to access :(

Remember all this worked with 3.4.12 :(

My config as it is now:
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) 
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#only contains one ip range - which I'm not accessing
#I don't quite understand what the purpose of this "broken" thing
# is and what it does :(
acl broken dst "/etc/squid/broken.txt"
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 broken
ssl_bump peek step2 broken
ssl_bump splice broken
ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump bump all

sslproxy_capath /etc/ssl/certs
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# user-defined ACLs
acl okweb-urls url_regex ^http://www.youtube.com/ 
^http://vimeo.com/api/oembed.json$ 
^https://www.google.com/accounts/ClientLogin$ 
^https://www.googleapis.com/analytics/
acl testurls url_regex ^http://www.dr.dk/$ ^https://www.google.dk/$
acl testbox src 10.xx.138.168
acl testsrv1 src 10.xx.130.50

http_access allow testurls testbox
http_access allow testurls testsrv1
http_access allow okweb-urls testsrv1
http_access deny all

http_port 3128
coredump_dir                   /var/spool/squid
maximum_object_size_in_memory  512 KB
maximum_object_size            4096 KB
ignore_expect_100              off
cache_mgr                      root
client_persistent_connections  on
server_persistent_connections  on
access_log                     /var/log/squid/access.log squid

# user-defined configuration settings from config_hash
ssl_bump                       server-first all
sslcrtd_children               8 startup=1 idle=1
sslcrtd_program                /usr/lib64/squid/ssl_crtd -s 
/etc/ssl/certs/cache/ -M 4MB
https_port                     3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
cert=/etc/squid/ca.cert cafile=/etc/squid/ca.cert 
key=/etc/squid/ca.private sslflags=NO_SESSION_REUSE
http_port                      3129 intercept

logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni 
%ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode

access_log syslog:daemon.info mine


Amos Jeffries wrote on 06/02/2015 05:05 PM:
> On 3/06/2015 2:46 a.m., Klavs Klavsen wrote:
>> Amos Jeffries wrote on 06/02/2015 04:10 PM:
>>> On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>>> Thank you Amos.
>>>>
>>>> I'll build 3.5.5 then..
>>>>
>>>> any config changes I need to be aware of?
>>>
>>> --with-openssl instead of --enable-ssl is the only one that comes to
>>> mind right now. The release notes for 3.4 and 3.5 have the lists.
>>>
>>
>> I borrowed the spec from fedora 23.. :)
>>
>> After installing 3.5.5 instead - it now complains when trying to issue
>> certificate :(
>>
>> squid cache log says:
>> Error negotiating SSL connection on FD 10: error:14094412:SSL
>> routines:SSL3_READ_BYTES:sslv3 alert bad certificate
>>
>> client gets:
>> curl: (51) SSL: certificate subject name '64.233.184.103' does not match
>> target host name 'www.google.com'
>>
>> any hints for tests I can do, to figure out the problem would be very
>> much appreciated :)
>
> James Lay has just done some good investigations in his "SSL-bump deep
> dive" thread(s). Compare what he came up with to your config
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From eliezer at ngtech.co.il  Thu Jun  4 08:05:59 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 04 Jun 2015 11:05:59 +0300
Subject: [squid-users] Fwd: TOS squid-3.5.0.4
In-Reply-To: <556AEF79.4050404@guineanet.net>
References: <5568D233.8050900@guineanet.net> <556AEF79.4050404@guineanet.net>
Message-ID: <557006E7.3060308@ngtech.co.il>

Hey Marcel,

First goes first... update to latest 3.5.5.
After the update We might be able to see the full picture.

Eliezer

On 31/05/2015 14:24, Marcel wrote:
> Hi All
> let see if some of you can help me troubleshoot the issue I have with
> squid-3.5.0.4
> on centos 6.6 configure with tproxy
> in fact the issue is relate to qos stuff  i just set things according to
> manual
> *
> qos_flows tos local-hit=0x30
> qos_flows mark local-hit=0x30
> qos_flows tos sibling-hit=0x31
> qos_flows mark sibling-hit=0x31
> qos_flows tos parent-hit=0x32
> qos_flows mark parent-hit=0x32
> qos_flows tos disable-preserve-miss*
> tcpdump output
>
> *tcpdump -vni eth1 | grep 'tos 0x30'*
>
> *tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size
> 65535 bytes***
>
> *01:37:24.787867 IP (tos 0x30, ttl 64, id 38723, offset 0, flags [DF],
> proto TCP (6), length 534)*
>
> *01:37:24.788003 IP (tos 0x30, ttl 64, id 38724, offset 0, flags [DF],
> proto TCP (6), length 2920)*
>
> *01:37:24.788019 IP (tos 0x30, ttl 64, id 38726, offset 0, flags [DF],
> proto TCP (6), length 1256)*
>
> *01:37:24.788141 IP (tos 0x30, ttl 64, id 38727, offset 0, flags [DF],
> proto TCP (6), length 2920)*
>
> but for sure it's not marking anything while send traffic to my pppoe
> BRAS (MK)
>
> any trick to make me solve this will be higly appreciate
> Bests Rgds
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From reet.vyas28 at gmail.com  Thu Jun  4 08:30:13 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Thu, 4 Jun 2015 14:00:13 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
Message-ID: <CAA8ViV9GVQdN3JQRubiq9fu=18WthG1TVpgLGq_BZZwrMC7WUg@mail.gmail.com>

Hi

I got it half working My chat is working I can search google, but I cant
browse websites ,

My configuration now

acl mynet src 116.72.152.37 192.168.0.0/16    # RFC1918 possible internal
network
acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow mynet
http_access allow localhost
http_access allow all
http_port 3129
http_port 3128 intercept

cache_dir ufs /usr/local/cache 10000 16 256
coredump_dir /var/spool/squid3
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
refresh_pattern .        0    20%    4320



Iptables:

root at squid:/home/squid# iptables -t nat -L -n -v
Chain PREROUTING (policy ACCEPT 77928 packets, 4272K bytes)
 pkts bytes target     prot opt in     out     source
destination
  290 17312 DNAT       tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 to:192.168.0.200:3128
    0     0 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3128

Chain INPUT (policy ACCEPT 75943 packets, 4074K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
  847 56477 MASQUERADE  all  --  *      eth0    192.168.0.0/24
0.0.0.0/0

On Thu, Jun 4, 2015 at 12:13 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:

> Hi,
>
> I changed the iptables still no luck :( but I am using squid 3.3 only can
> I didn't understand why you have configured 3129 ,3130 and 3128 port?
>
> On Wed, Jun 3, 2015 at 1:04 PM, Klavs Klavsen <kl at vsen.dk> wrote:
>
>> Your client needs to use your squid server as default gateway.
>>
>> And then you need the iptables rules I wrote about to direct traffic into
>> squid for certain ports.
>>
>> Reet Vyas wrote on 06/03/2015 08:50 AM:
>>
>>> Hi
>>>
>>> Thanks for reply. As of now we don't have router I have directly
>>> connected my machine to internet and other to LAN and I have configured
>>> client machine ubuntu to test squid which is in switch where other users
>>> are connected using gateway of router 192.168.0.1.
>>>
>>> I read your valuable suggestions, but I still confused with IPtables and
>>> squid 3.3 setting ,transparent and intercept options .
>>>
>>> root at squid:/home/squid#   ip addr show
>>> 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
>>> group default
>>>      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
>>>      inet 127.0.0.1/8 <http://127.0.0.1/8> scope host lo
>>>         valid_lft forever preferred_lft forever
>>>      inet6 ::1/128 scope host
>>>         valid_lft forever preferred_lft forever
>>> 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
>>> state UP group default qlen 1000
>>>      link/ether 00:1e:67:cf:59:74 brd ff:ff:ff:ff:ff:ff
>>>      inet 116.72.*.*/22 brd 116.72.155.255 scope global eth0
>>>         valid_lft forever preferred_lft forever
>>>      inet6 fe80::21e:67ff:fecf:5974/64 scope link
>>>         valid_lft forever preferred_lft forever
>>> 3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
>>> state UP group default qlen 1000
>>>      link/ether 00:1e:67:cf:59:75 brd ff:ff:ff:ff:ff:ff
>>>      inet 192.168.0.200/24 <http://192.168.0.200/24> brd 192.168.0.255
>>> scope global eth1
>>>         valid_lft forever preferred_lft forever
>>>      inet6 fe80::21e:67ff:fecf:5975/64 scope link
>>>         valid_lft forever preferred_lft forever
>>>
>>> root at squid:/home/squid#  ip -4 route show
>>> default via 116.72.152.1 dev eth0
>>> 116.72.152.0/22 <http://116.72.152.0/22> dev eth0  proto kernel  scope
>>> link  src 116.72.152.37
>>> 192.168.0.0/24 <http://192.168.0.0/24> dev eth1  proto kernel  scope
>>> link  src 192.168.0.200
>>>
>>>
>>>
>>>
>>>
>>> To use transparent/intercept what I have to set in my config file
>>> http_port 3128 intercept or transparent
>>>
>>> and Iptables rules , I have tried this rules
>>>
>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
>>>
>>> But not working
>>>
>>> Can you please tell me the firewall rules and let me know why my
>>> firewall rules are not working.
>>>
>>> On Tue, Jun 2, 2015 at 8:14 PM, Klavs Klavsen <kl at vsen.dk
>>> <mailto:kl at vsen.dk>> wrote:
>>>
>>>     Amos Jeffries wrote on 06/02/2015 04:34 PM:
>>>
>>>         On 3/06/2015 1:20 a.m., Klavs Klavsen wrote:
>>>
>>>             I have this in my squid server for it to work:
>>>
>>>
>>>         The key words there are ... *in my Squid server*
>>>
>>>     indeed :)
>>>
>>>
>>>         NOTE to Klavs:
>>>             loading the "multiport" kernel module seems overkill for a
>>>         single-port
>>>         match.
>>>
>>>     it's puppets firewall module.. haven't had enough time to fix that
>>>     module :)
>>>
>>>
>>>         FYI: DONT_VERIFY_PEER, "always_direct allow all", and
>>>         "slproxy_cert_error allow all" have not been good ideas since
>>> 3.2.
>>>         dont-verify actually inhibits the Mimic functions which give
>>>         server-first bumping most of its usefulness.
>>>
>>>     Thank you for those tips.
>>>
>>>     --
>>>     Regards,
>>>     Klavs Klavsen, GSEC - kl at vsen.dk <mailto:kl at vsen.dk> -
>>>     http://www.vsen.dk - Tlf. 61281200
>>>
>>>     "Those who do not understand Unix are condemned to reinvent it,
>>> poorly."
>>>        --Henry Spencer
>>>
>>>     _______________________________________________
>>>     squid-users mailing list
>>>     squid-users at lists.squid-cache.org
>>>     <mailto:squid-users at lists.squid-cache.org>
>>>     http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>> --
>> Regards,
>> Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200
>>
>> "Those who do not understand Unix are condemned to reinvent it, poorly."
>>   --Henry Spencer
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/7c0c9bf5/attachment.htm>

From carlo.filippetto at gmail.com  Thu Jun  4 09:39:19 2015
From: carlo.filippetto at gmail.com (Carlo Filippetto)
Date: Thu, 4 Jun 2015 11:39:19 +0200
Subject: [squid-users] Restore Job on secondary site
Message-ID: <CANhYgHkwvo=r494qz8xgwWSMMsqHvcmafbquns7ofGJ=3cQ7Tw@mail.gmail.com>

Hi all,
how can I restore production server on quiescent machine on a DR site?
I need to have a DR site that is a copy of the production one, how can I
restore Incremental BCK on those servers (that must be offline)?

Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/e7d6b6e2/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun  4 09:47:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 21:47:42 +1200
Subject: [squid-users] Restore Job on secondary site
In-Reply-To: <CANhYgHkwvo=r494qz8xgwWSMMsqHvcmafbquns7ofGJ=3cQ7Tw@mail.gmail.com>
References: <CANhYgHkwvo=r494qz8xgwWSMMsqHvcmafbquns7ofGJ=3cQ7Tw@mail.gmail.com>
Message-ID: <55701EBE.9040107@treenet.co.nz>

On 4/06/2015 9:39 p.m., Carlo Filippetto wrote:
> Hi all,
> how can I restore production server on quiescent machine on a DR site?
> I need to have a DR site that is a copy of the production one, how can I
> restore Incremental BCK on those servers (that must be offline)?
> 

Hi Carlo, I think you maybe sent this to the wrong place.

Caching proxies do not have anything to mirror except possibly their
configurations. The caches themselves are dynamic and generated from the
live traffic each proxy sees differently.

Amos



From squid3 at treenet.co.nz  Thu Jun  4 10:07:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 22:07:00 +1200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
Message-ID: <55702344.2060703@treenet.co.nz>

On 4/06/2015 6:43 p.m., Reet Vyas wrote:
> Hi,
> 
> I changed the iptables still no luck :( but I am using squid 3.3 only can I
> didn't understand why you have configured 3129 ,3130 and 3128 port?

Because due to historic (browser war politics) reasons there are three
different protocol message syntax in HTTP/1.x - depending whether the
traffic is on port 80 (HTTP origin), 443 (HTTPS origin), or 3128 (HTTP
proxy).


* Normal forward/explicit proxy traffic occurs on port 3128. Squid needs
this port regardless of whether your main traffic use is on another port
type, because some proxy responses will have URLs generated for embeded
content to be fetched from the proxy itself.

* NAT intercepted port 80 traffic needs to be delivered to a different
proxy http_port with the "intercept" flag. The tutorials use 3129 to
make it clear its not to be 3128, but it SHOULD be something random you
make up that you can also have the firewall blocking connections
directly to it by clients.

* NAT intercepted port 443 traffic needs https_port directive (note the
's') which means another port number separate from the port 80 one.


Amos



From squid3 at treenet.co.nz  Thu Jun  4 10:26:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 22:26:51 +1200
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1433399372421-4671515.post@n4.nabble.com>
References: <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
 <9c4651e1184467ff80e9187bcc27981f@localhost>
 <CAGUJm7ap7oCeDy+BGW45w6fLKH=L=0_bbUpjmnmjQ6tjnab_Fg@mail.gmail.com>
 <1433341647461-4671506.post@n4.nabble.com> <556F17C0.4000601@treenet.co.nz>
 <1433399372421-4671515.post@n4.nabble.com>
Message-ID: <557027EB.5080307@treenet.co.nz>

On 4/06/2015 6:29 p.m., sp_ wrote:
>  Hello Amos,
> 
> thank you for your reply.
> 
> Let's take for instance this line:
> 
> 192.168.78.31 - - [04/Jun/2015:09:41:22 +0300] "CONNECT 173.194.122.233:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE
> 
> 
> I have dumped the traffic passing through the interface on the router during this request.
> In client hello in Extension "server_name" I can see the domain:
> 
> Server Name: clients4.google.com

In your packet trace look at the details in the TCP SYN packet *only* to
see what the Squid CONNECT has available.

> 
> 
> According to RFC, domain is a must in Client Hello, when SNI is used.

Yes. But the ClientHello is not part of a TCP SYN packet - which is what
Squid is working with when it does that fake CONNECT message processing.

The TLS packets have explicitly not been read into Squid yet in case
splice, none, or terminate actions are to be done by the ssl_bump step1
rules.


If the bumping is successful there will be other requests from inside
the TLS that get logged with the domain etc.

For now Squid does not log any of the SSL-bumping process itself. There
is an open bug about that now.

Amos



From squid3 at treenet.co.nz  Thu Jun  4 11:24:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 23:24:56 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55700468.2050807@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55700468.2050807@vsen.dk>
Message-ID: <55703588.2060009@treenet.co.nz>

On 4/06/2015 7:55 p.m., Klavs Klavsen wrote:
> Hi Amos,
> 
> I tried taking the config from James.. but I have the exact same issue
> as described below :(
> 
> After adding the extra logging from James config - I get this in
> access_log:
> 1433404085.331      0 10.47.171.244 TCP_DENIED/200 0 CONNECT
> 216.58.209.106:443 - HIER_NONE/- -
> 
> which makes it seem as if squid does NOT see the url I'm trying to
> access :(

That log line is generated *before* TLS bumping is started. All Squid is
working with at that point is the TCP SYN packet details.

The DENIED makes sense because all your http_access allow rules require
"scheme://..." absolute-URI syntax which never exists in CONNECT, even
if a domain name known. HTTP CONNECT requests *always* contains
authority-URI syntax URLs (IP:port or hostname:port).

The 200 is odd. It implies that Squid spliced or bumped the connection
despite the denial. Though its probably just a default value sliping in
wrongly from somewhere.


> 
> Remember all this worked with 3.4.12 :(

No, the code does not even exist in those older "working" versions.


Squid 3.1, 3.2 bumping:
 fetch the ClientHello and bump

Squid 3.3, 3.4 bumping:
 fetch the ClientHello and maybe bump, or ...
 fetch the ServerHello and bump with mimic certs.

Squid 3.5 bumping:
 fetch TCP details (fake a CONNECT IP:port request), check if pass-thru,
or ...
 fetch TLS ClientHello, check what to do and do it, or ...
 fetch TLS ServerHello, check what to do and do it.


What you are seeing in access.log is that new first step.


> 
> My config as it is now:
> acl localhost src 127.0.0.1/32 ::1
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> #only contains one ip range - which I'm not accessing
> #I don't quite understand what the purpose of this "broken" thing
> # is and what it does :(

AFAIK it lists IPs of servers where TLS is being used properly so
bumping does not work, or TLS used so badly that they break when bumped.
Its hard to tell automatically, but if Squid can identify such by itself
it will just splice. This is to catch the other cases.

 Its your choice whether you allow them to try and bump based on server
cert details at step3, or to outright reject.

The way its used here those servers will always be spliced even if SNI
and/or cert domain are found. The breakage might be a cipher choice for
example.


> acl broken dst "/etc/squid/broken.txt"
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 broken
> ssl_bump peek step2 broken
> ssl_bump splice broken
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump bump all
> 
> sslproxy_capath /etc/ssl/certs
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> # user-defined ACLs
> acl okweb-urls url_regex ^http://www.youtube.com/
> ^http://vimeo.com/api/oembed.json$
> ^https://www.google.com/accounts/ClientLogin$
> ^https://www.googleapis.com/analytics/
> acl testurls url_regex ^http://www.dr.dk/$ ^https://www.google.dk/$
> acl testbox src 10.xx.138.168
> acl testsrv1 src 10.xx.130.50
> 
> http_access allow testurls testbox
> http_access allow testurls testsrv1
> http_access allow okweb-urls testsrv1


Where is the rules that will allow the CONNECT raw-IP:port request
through to start bumping? the url_regex ACLs do not match for them.

I suggest:
 acl bumpedPorts myportname 3129
 acl bumpedPorts myportname 3130
 http_access allow CONNECT bumpedPorts


> http_access deny all
> 
> http_port 3128
> coredump_dir                   /var/spool/squid
> maximum_object_size_in_memory  512 KB
> maximum_object_size            4096 KB
> ignore_expect_100              off
> cache_mgr                      root
> client_persistent_connections  on
> server_persistent_connections  on
> access_log                     /var/log/squid/access.log squid
> 
> # user-defined configuration settings from config_hash
> ssl_bump                       server-first all
> sslcrtd_children               8 startup=1 idle=1
> sslcrtd_program                /usr/lib64/squid/ssl_crtd -s
> /etc/ssl/certs/cache/ -M 4MB
> https_port                     3130 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/etc/squid/ca.cert cafile=/etc/squid/ca.cert
> key=/etc/squid/ca.private sslflags=NO_SESSION_REUSE
> http_port                      3129 intercept
> 
> logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni
> %ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode
> 
> access_log syslog:daemon.info mine
> 

Amos



From squid3 at treenet.co.nz  Thu Jun  4 11:33:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 23:33:21 +1200
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <556EDF59.8010509@guineanet.net>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz> <1433115308548-4671461.post@n4.nabble.com>
 <556BA3B1.7050701@treenet.co.nz> <1433118250561-4671465.post@n4.nabble.com>
 <1433121546223-4671467.post@n4.nabble.com> <556E867F.8040603@treenet.co.nz>
 <556EDF59.8010509@guineanet.net>
Message-ID: <55703781.8020602@treenet.co.nz>

On 3/06/2015 10:46 p.m., Marcel Fossua wrote:
> Hi Amos not really
> after setting TOS config on Squid the idea is to allow Mikrotik router 
> recognize
> marked  paquets (as on previous squid 3.1.x)
> and then mark cache content, so that it can later pick by Mikrotik
> to deliver the already cached content to user at full lan speed, no 
> queue on cache content.
> 
>  1.
>     /ip firewall mangle
>  2.
>     add action=mark-connection chain=postrouting comment="==SQUID - TOS
>     12==" disabled=no dscp=12 \
>  3.
>     new-connection-mark=squid-connection passthrough=yes protocol=tcp
>     src-address=192.168.10.2
>  4.
>     add action=mark-packet chain=postrouting
>     connection-mark=squid-connection disabled=\
>  5.
>     no new-packet-mark=squid-packs passthrough=yes
> 


I'm not very familair with Mikrotik syntax so I may be misunderstanding
their syntax completely.

 But those "action=mark-connection" and
"new-connection-mark=squid-connection" looks suspiciously like its
requesting the Mikrotik to change the packet markings itself rather than
just recognise them.


Amos



From squid3 at treenet.co.nz  Thu Jun  4 11:38:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 04 Jun 2015 23:38:30 +1200
Subject: [squid-users] Fwd: TOS squid-3.5.0.4
In-Reply-To: <557006E7.3060308@ngtech.co.il>
References: <5568D233.8050900@guineanet.net> <556AEF79.4050404@guineanet.net>
 <557006E7.3060308@ngtech.co.il>
Message-ID: <557038B6.2030104@treenet.co.nz>

On 4/06/2015 8:05 p.m., Eliezer Croitoru wrote:
> Hey Marcel,
> 
> First goes first... update to latest 3.5.5.
> After the update We might be able to see the full picture.
> 

FYI: This is another duplicate thread. I've been following up in the
other one started a few minutes after this.

Amos



From ahmed.zaeem at netstream.ps  Thu Jun  4 21:36:56 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 4 Jun 2015 14:36:56 -0700
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error
Message-ID: <000b01d09f0e$94ae8c10$be0ba430$@netstream.ps>

 

 

I have a proxy squid 3.5.2 that has an app to connect to it remotely to
access YouTube Links

 

This App some time works and open the link without problems and its response
as below :

 

    

==============

1433246384.626    245 195.154.200.58 TCP_MISS/200 38660 GET
http://www.youtube.com/get_video_info? - HIER_DIRECT/195.95.178.110
application/x-www-form-urlencoded

1433246384.802     62 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246385.027    125 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246386.239    123 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246386.469    121 195.154.200.58 TCP_MISS/200 455 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/x-flv

1433246386.709    139 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/3gpp

1433246386.941    121 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/3gpp

1433246387.181    131 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

1433246387.334     61 195.154.200.58 TCP_MISS/200 454 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/webm

1433246387.756     61 195.154.200.58 TCP_MISS/200 453 HEAD
http://r2---sn-8pgbpohxqp5-ac5e.googlevideo.com/videoplayback? -
HIER_DIRECT/82.15.95.141 video/mp4

And im some youtube videos , it don't work and has the repsonce as below :

433246591.307    128 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.530    129 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.752    121 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246591.977    120 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

1433246592.218    125 195.154.200.58 TCP_MISS/403 353 HEAD
http://r8---sn-8pgbpohxqp5-ac5l.googlevideo.com/videoplayback? -
HIER_DIRECT/62.252.232.19 text/plain

 

 

The question here is , can I know why squid on some youtube movies  give
error 403 and some videos it works ???

Can I fix the issue of ((TCP_MISS/403)) of those vides ?

 

Here is my squid.conf :

 

squid -v

Squid Cache: Version 3.5.2

Service Name: squid

configure options:  '--prefix=/usr' '--includedir=/include'
'--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc'
'--enable-cachemgr-hostname=Ahmad-Allzaeem' '--localstatedir=/var'
'--libexecdir=/lib/squid' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam
,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-esi'
'--disable-translation' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=131072'
'--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter'
'--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd'
'--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=20000' '--with-openssl'
'--enable-snmp' '--with-included-ltdl'

 

 

config :

root at box2:~# cat /etc/squid/squid.conf

cache_effective_user squid

cache_effective_group squid

#########################################

#dns_nameservers 8.8.8.8

#client_dst_passthru off

#host_verify_strict off

#range_offset_limit -1 

#quick_abort_min -1

#This is special work for best proxy worker "Ahmed M H Allzaeem"###

##https://www.elance.com/s/edit/noshutdown/######

########################################################################

# Lockdown Procedures

#auth_param basic program /lib/squid/basic_ncsa_auth /etc/squid/squid_user

#acl ncsa_users proxy_auth REQUIRED

#http_access allow ncsa_users

############################

####################################

#

# Recommended minimum configuration:

#

 

# Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src xxxxxx/xxxx possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

 

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

 

#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

 

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

 

# Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager

 

# We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost

 

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

 

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost

 

# And finally deny all other access to this proxy

http_access deny all

 

# Squid normally listens to port 3128

#http_port 3128

######################################

################################################

# Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/cache/squid 100 16 256

 

# Leave coredumps in the first cache dir

coredump_dir /var/cache/squid

 

#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/332b46b4/attachment.htm>

From jonathan.filogna at tasso.com.ar  Thu Jun  4 12:05:43 2015
From: jonathan.filogna at tasso.com.ar (rocaembole)
Date: Thu, 4 Jun 2015 05:05:43 -0700 (PDT)
Subject: [squid-users] Skype issue
In-Reply-To: <556FB9C0.5070301@treenet.co.nz>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
Message-ID: <1433419543048-4671527.post@n4.nabble.com>

here's my squid.conf

##NTLM
#
##DECLARED
auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
auth_param ntlm keep_alive on

external_acl_type ntlm_group ttl=f3600 children=100 %LOGIN
/usr/lib/squid3/wbinfo_group.pl

##SRC

acl administrador external ntlm_group "/etc/squid3/lists/UserSkype.lst"
##UserSkype is a group on AD with 3 users
acl all src all
acl localnet src 10.0.0.0/8
acl webserver src 10.0.0.11
acl manager proto cache_object
#acl skype 
acl localhost src 127.0.0.1/32
acl SSL_ports port 443        # https
acl SSL_ports port 563        # snews
acl SSL_ports port 873        # rsync
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl Safe_ports port 631        # cups
acl Safe_ports port 873        # rsync
acl Safe_ports port 901        # SWAT
acl CONNECT method CONNECT
##SRC'S DECLARED
#
##skype
acl numeric_IPs dstdom_regex
^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\])):443
acl Skype_UA browser ^skype
acl validUserAgent browser \S+
http_access deny numeric_IPS Skype_UA !validUserAgent !administrador
http_access allow numeric_IPS Skype_UA validUserAgent administrador
http_access deny numeric_IPS 
http_access deny Skype_UA
http_access deny !validUserAgent
## *MAYBE I'M HAVING A SYNTAX PROBLEM?*

http_access allow manager webserver
http_access deny manager

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports






acl ntlm proxy_auth REQUIRED
http_access deny !ntlm
http_access allow ntlm
http_access deny all

http_port 3128
dns_v4_first on
access_log /var/log/squid3/access.log squid
url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 10

----------------------------------------------------------------------------------------------------------------
Some users can use skype and the rest of them can't use it. Maybe someone
can help me with this issue? i can't find a problem. 

Thank you all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Skype-issue-tp4666074p4671527.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From reet.vyas28 at gmail.com  Thu Jun  4 12:55:28 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Thu, 4 Jun 2015 18:25:28 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <55702344.2060703@treenet.co.nz>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
Message-ID: <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>

Thank you everyone for helping me to setup squid , Now its working but in
access.logs  I only see tcp_miss if m using same website. I mean squid is
not caching

Logs

43 192.168.0.198 TCP_MISS/200 384461 GET
http://www.horlicksquad.com/images/tc-pic.png - HIER_DIRECT/52.74.133.61
image/png
1433422076.988    309 192.168.0.198 TCP_MISS/200 38007 GET
http://www.horlicksquad.com/about-us - HIER_DIRECT/52.74.133.61 text/html
1433422077.188    224 192.168.0.198 TCP_MISS/200 17622 GET
http://www.horlicksquad.com/images/panel05.png - HIER_DIRECT/52.74.133.61
image/png
1433422077.226    140 192.168.0.198 TCP_MISS/200 13840 GET
http://www.horlicksquad.com/images/au-bg.png - HIER_DIRECT/52.74.133.61
image/png
1433422077.261    208 192.168.0.198 TCP_MISS/200 60858 GET
http://www.horlicksquad.com/images/sonny-horlicks-abtus.png - HIER_DIRECT/
52.74.133.61 image/png

How to check cache is working or not. I want to cache videos images css

On Thu, Jun 4, 2015 at 3:37 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/06/2015 6:43 p.m., Reet Vyas wrote:
> > Hi,
> >
> > I changed the iptables still no luck :( but I am using squid 3.3 only
> can I
> > didn't understand why you have configured 3129 ,3130 and 3128 port?
>
> Because due to historic (browser war politics) reasons there are three
> different protocol message syntax in HTTP/1.x - depending whether the
> traffic is on port 80 (HTTP origin), 443 (HTTPS origin), or 3128 (HTTP
> proxy).
>
>
> * Normal forward/explicit proxy traffic occurs on port 3128. Squid needs
> this port regardless of whether your main traffic use is on another port
> type, because some proxy responses will have URLs generated for embeded
> content to be fetched from the proxy itself.
>
> * NAT intercepted port 80 traffic needs to be delivered to a different
> proxy http_port with the "intercept" flag. The tutorials use 3129 to
> make it clear its not to be 3128, but it SHOULD be something random you
> make up that you can also have the firewall blocking connections
> directly to it by clients.
>
> * NAT intercepted port 443 traffic needs https_port directive (note the
> 's') which means another port number separate from the port 80 one.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/3c3f003d/attachment.htm>

From kl at vsen.dk  Thu Jun  4 13:01:30 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:01:30 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55703588.2060009@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55700468.2050807@vsen.dk> <55703588.2060009@treenet.co.nz>
Message-ID: <55704C2A.8000505@vsen.dk>

Amos Jeffries wrote on 06/04/2015 01:24 PM:
>   acl bumpedPorts myportname 3129
>   acl bumpedPorts myportname 3130
>   http_access allow CONNECT bumpedPorts

Adding that worked.. I did not have any of that ssl_stuff in my 3.4 
config (and it worked without).

Thank you very much.

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From kl at vsen.dk  Thu Jun  4 13:06:02 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:06:02 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55703588.2060009@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55700468.2050807@vsen.dk> <55703588.2060009@treenet.co.nz>
Message-ID: <55704D3A.2080805@vsen.dk>

One thing.. now when access a site.. f.ex. https://www.dr.dk

the access log says:
1433423013.540    196 10.47.171.244 TCP_TUNNEL/200 187877 CONNECT 
159.20.6.6:443 - ORIGINAL_DST/159.20.6.6 -

instead of logging the url that was accessed..

How can I make it log the url as it did in 3.4.12?



Amos Jeffries wrote on 06/04/2015 01:24 PM:
>   acl bumpedPorts myportname 3129
>   acl bumpedPorts myportname 3130
>   http_access allow CONNECT bumpedPorts


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From kl at vsen.dk  Thu Jun  4 13:08:24 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:08:24 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55704D3A.2080805@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55700468.2050807@vsen.dk> <55703588.2060009@treenet.co.nz>
 <55704D3A.2080805@vsen.dk>
Message-ID: <55704DC8.3050000@vsen.dk>

oops.. forget it.. I missed I had two access logs.. the format from 
James Lay - works perfectly.. sorry :)

Klavs Klavsen wrote on 06/04/2015 03:06 PM:
> One thing.. now when access a site.. f.ex. https://www.dr.dk
>
> the access log says:
> 1433423013.540    196 10.47.171.244 TCP_TUNNEL/200 187877 CONNECT
> 159.20.6.6:443 - ORIGINAL_DST/159.20.6.6 -
>
> instead of logging the url that was accessed..
>
> How can I make it log the url as it did in 3.4.12?
>
>
>
> Amos Jeffries wrote on 06/04/2015 01:24 PM:
>>   acl bumpedPorts myportname 3129
>>   acl bumpedPorts myportname 3130
>>   http_access allow CONNECT bumpedPorts
>
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Thu Jun  4 13:08:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 01:08:31 +1200
Subject: [squid-users] Skype issue
In-Reply-To: <1433419543048-4671527.post@n4.nabble.com>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
 <1433419543048-4671527.post@n4.nabble.com>
Message-ID: <55704DCF.5060106@treenet.co.nz>

On 5/06/2015 12:05 a.m., rocaembole wrote:
> here's my squid.conf
> 
> ##NTLM
> #
> ##DECLARED
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
> auth_param ntlm keep_alive on
> 
> external_acl_type ntlm_group ttl=f3600 children=100 %LOGIN
> /usr/lib/squid3/wbinfo_group.pl
> 
> ##SRC
> 
> acl administrador external ntlm_group "/etc/squid3/lists/UserSkype.lst"

> acl numeric_IPs dstdom_regex
> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\])):443
> acl Skype_UA browser ^skype
> acl validUserAgent browser \S+
> http_access deny numeric_IPS Skype_UA !validUserAgent !administrador
> http_access allow numeric_IPS Skype_UA validUserAgent administrador
> http_access deny numeric_IPS 
> http_access deny Skype_UA
> http_access deny !validUserAgent
> ## *MAYBE I'M HAVING A SYNTAX PROBLEM?*

This config example cut-n-pasted from the wiki tutorial on *Blocking
Skype* seems to be doing exactly what it was designed for. Delivering a 403.


Amos


From squid3 at treenet.co.nz  Thu Jun  4 13:18:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 01:18:01 +1200
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
 <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
Message-ID: <55705009.9080200@treenet.co.nz>

On 5/06/2015 12:55 a.m., Reet Vyas wrote:
> Thank you everyone for helping me to setup squid , Now its working but in
> access.logs  I only see tcp_miss if m using same website. I mean squid is
> not caching

You will get MISS a fair bit more with intercepted traffic than with
normal proxied traffic. Particularly on certain major CDN who play
tricks with DNS.

The reasons and some workarounds to need to be doing are explained in
<http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

Amos



From su.maji.ke at gmail.com  Thu Jun  4 13:18:43 2015
From: su.maji.ke at gmail.com (Iruma Keisuke)
Date: Thu, 4 Jun 2015 22:18:43 +0900
Subject: [squid-users] Error Resolution (TunnelStateData::Connection::
 error )
In-Reply-To: <556DB89E.1050305@treenet.co.nz>
References: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>
 <556DB89E.1050305@treenet.co.nz>
Message-ID: <CAKLXwH4Ov-YhY0C1vmbgpqDgDrxpt+4fdYNtRV2-KakAQg5Bqw@mail.gmail.com>

Thank you Amos.

2015-06-02 23:07 GMT+09:00, Amos Jeffries <squid3 at treenet.co.nz>:
> On 2/06/2015 9:15 p.m., Irimajiri keisuke wrote:
>> Dear all,
>>
>> I have to build a proxy server by using the squid.
>> The number of clients is 400 people.
>>
>> I do not know the cause of the error message that appears in the
>> cache.log.
>> In the weekday, I have come up with an error every few hours 8:00 to
>> 18:00.
>> Access concentration I look like does not matter.
>>
>> [cache.log error message]
>> 2015/05/11 13:37:24| TunnelStateData::Connection:: error : FD 610:
>> read/write failure: (110) Connection timed out
>>
>> Why I want to know whether this error has occurred.
>
> Yes it has occured. You would not be seeing it otherwise.
>
>> Also, I want to know the impact on the user.
>
> The user who is causing the problem is probably not impacted at all.
> Every other user sharing the proxy is impacted by the reduction in
> available network socket, memory and CPU resources.
>
It seems to be no abnormality in the state of network sockets and
memory and CPU.
Is it safe to ignore this error?

>
>>
>> [squidclient mgr:filedescriptor]
>> Every five minutes record
>> extract FD610
>>
>> It looks like an error has occurred in the use to which the terminal
>> of xxx.xxx.2.115 user.
>> Is it a problem of communication of the user and the proxy?
>>
>
> Nothing happened on a TCP conection for a long time. It was closed by
> the networking sub-systems somewhere between Squid and the client.
>

Do error is not out on the web browser?
Could you detailed information about TCP state and the state of the
user when an error has occurred.


>> Active file descriptors:
>> File Type   Tout Nread  * Nwrite * Remote Address        Description
>> ---- ------ ---- -------- -------- ---------------------
>> ------------------------------
>>  610 Socket  893    39494*   50228  xxx.xxx.xxx.162:443
>> outlook.office365.com:443       2015/05/11_13:08:29
>>  610 Socket 86329   45754*  103329  xxx.xxx.6.141:50174   Reading next
>> request         2015/05/11_13:13:29
>>  610 Socket 86258    6516*   13975  xxx.xxx.2.115:50820   Reading next
>> request         2015/05/11_13:18:29
>>  610 Socket 85958   12472*   34531* xxx.xxx.2.115:50820   Reading next
>> request         2015/05/11_13:23:29
>>  610 Socket 85657   12472*   34531* xxx.xxx.2.115:50820   Reading next
>> request         2015/05/11_13:28:29
>>  610 Socket 85357   12472*   34531* xxx.xxx.2.115:50820   Reading next
>> request         2015/05/11_13:33:29
>>  610 Socket 86336    3652*    8003  xxx.xxx.3.152:50817   Reading next
>> request         2015/05/11_13:38:29
>>
>> [access.log]
>> I do not see suspicious error log I tried to extract the address
>> xxx.xxx.2.115.
>>
>> Please tell me a good idea toward someone solve.
>
> Please provided additional details:
>  * Squid version
>  * Squid configuration
>
>
> I suspect you have a quite old verion of Squid. That particular error
> message does not even exist in the code any more. The current releases
> display much more TCP details about the connection where the error occured.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

squid version is squid-3.1.10-29.
This is the latest version that RedHat is delivering.

[squid.conf]
------------------------------------
acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines
acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
http_access allow manager localhost
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow all
http_port 192.168.1.1:8080
hierarchy_stoplist cgi-bin ?
coredump_dir /var/spool/squid
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

cache_mem 2048 MB
cache_store_log none
visible_hostname unknown
request_header_access X-FORWARDED-FOR deny all
request_header_access Via deny all
max_filedesc 10240
ipcache_size 10240
-----------------------------------------------

Please let me ask further questions
Are these has to do with the error?
http://www.squid-cache.org/Doc/code/tunnel_8cc_source.html

472 TunnelStateData::Connection::error(int const xerrno)
473 {
474  /* XXX fixme xstrerror and xerrno... */
475  errno = xerrno;
476
477  debugs(50, debugLevelForError(xerrno), HERE << conn << ":
read/write failure: " << xstrerror());
478
479  if (!ignoreErrno(xerrno))
480  conn->close();
481 }

536  /* Bump the dest connection read timeout on any activity */
537  /* see Bug 3659: tunnels can be weird, with very long one-way transfers */
538  if (Comm::IsConnOpen(to.conn)) {
539  AsyncCall::Pointer timeoutCall = commCbCall(5, 4, "tunnelTimeout",
540  CommTimeoutCbPtrFun(tunnelTimeout, this));
541  commSetConnTimeout(to.conn, Config.Timeout.read, timeoutCall);
542  }
543


From kl at vsen.dk  Thu Jun  4 13:20:02 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:20:02 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <556DC652.5080005@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
Message-ID: <55705082.2060501@vsen.dk>

Hi,

I added the bumpedports - and now traffic works and is allowed.. but it 
allows everything on https.. :(

Log says:
10.xx.130.50 - - [04/Jun/2015:15:16:07 +0200] "CONNECT 72.51.34.34:443 
HTTP/1.1" lwn.net - 200 28189 TCP_TUNNEL:ORIGINAL_DST peek

so it doesn't seem to check the http_access lines for testsrv1.

My config as it is now:
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) 
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#only contains one ip range - which I'm not accessing
#I don't quite understand what the purpose of this "broken" thing
# is and what it does :(
acl broken dst "/etc/squid/broken.txt"
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 broken
ssl_bump peek step2 broken
ssl_bump splice broken
ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump bump all

sslproxy_capath /etc/ssl/certs
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# user-defined ACLs
acl okweb-urls url_regex ^http://www.youtube.com/ 
^http://vimeo.com/api/oembed.json$ 
^https://www.google.com/accounts/ClientLogin$ 
^https://www.googleapis.com/analytics/
acl testurls url_regex ^http://www.dr.dk/$ ^https://www.google.dk/$
acl testbox src 10.xx.138.168
acl testsrv1 src 10.xx.130.50

acl bumpedPorts myportname 3129
acl bumpedPorts myportname 3130
http_access allow CONNECT bumpedPorts

http_access allow testurls testbox
http_access allow testurls testsrv1
http_access allow okweb-urls testsrv1
http_access deny all

http_port 3128
coredump_dir                   /var/spool/squid
maximum_object_size_in_memory  512 KB
maximum_object_size            4096 KB
ignore_expect_100              off
cache_mgr                      root
client_persistent_connections  on
server_persistent_connections  on

# user-defined configuration settings from config_hash
ssl_bump                       server-first all
sslcrtd_children               8 startup=1 idle=1
sslcrtd_program                /usr/lib64/squid/ssl_crtd -s 
/etc/ssl/certs/cache/ -M 4MB
https_port                     3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
cert=/etc/squid/ca.cert cafile=/etc/squid/ca.cert 
key=/etc/squid/ca.private sslflags=NO_SESSION_REUSE
http_port                      3129 intercept

logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni 
%ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode

access_log /var/log/squid/access.log mine


Amos Jeffries wrote on 06/02/2015 05:05 PM:
> On 3/06/2015 2:46 a.m., Klavs Klavsen wrote:
>> Amos Jeffries wrote on 06/02/2015 04:10 PM:
>>> On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>>> Thank you Amos.
>>>>
>>>> I'll build 3.5.5 then..
>>>>
>>>> any config changes I need to be aware of?
>>>
>>> --with-openssl instead of --enable-ssl is the only one that comes to
>>> mind right now. The release notes for 3.4 and 3.5 have the lists.
>>>
>>
>> I borrowed the spec from fedora 23.. :)
>>
>> After installing 3.5.5 instead - it now complains when trying to issue
>> certificate :(
>>
>> squid cache log says:
>> Error negotiating SSL connection on FD 10: error:14094412:SSL
>> routines:SSL3_READ_BYTES:sslv3 alert bad certificate
>>
>> client gets:
>> curl: (51) SSL: certificate subject name '64.233.184.103' does not match
>> target host name 'www.google.com'
>>
>> any hints for tests I can do, to figure out the problem would be very
>> much appreciated :)
>
> James Lay has just done some good investigations in his "SSL-bump deep
> dive" thread(s). Compare what he came up with to your config
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From jonathan.filogna at tasso.com.ar  Thu Jun  4 13:26:48 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Thu, 04 Jun 2015 10:26:48 -0300
Subject: [squid-users] Skype issue
In-Reply-To: <55704DCF.5060106@treenet.co.nz>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
 <1433419543048-4671527.post@n4.nabble.com> <55704DCF.5060106@treenet.co.nz>
Message-ID: <55705218.2060904@tasso.com.ar>

And if i want to make exceptions to memberships on AD, how can i do it? 
That's what i need.

El 04/06/15 a las 10:08, Amos Jeffries escibi?:
> On 5/06/2015 12:05 a.m., rocaembole wrote:
>> here's my squid.conf
>>
>> ##NTLM
>> #
>> ##DECLARED
>> auth_param ntlm program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
>> auth_param ntlm keep_alive on
>>
>> external_acl_type ntlm_group ttl=f3600 children=100 %LOGIN
>> /usr/lib/squid3/wbinfo_group.pl
>>
>> ##SRC
>>
>> acl administrador external ntlm_group "/etc/squid3/lists/UserSkype.lst"
>> acl numeric_IPs dstdom_regex
>> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?\])):443
>> acl Skype_UA browser ^skype
>> acl validUserAgent browser \S+
>> http_access deny numeric_IPS Skype_UA !validUserAgent !administrador
>> http_access allow numeric_IPS Skype_UA validUserAgent administrador
>> http_access deny numeric_IPS
>> http_access deny Skype_UA
>> http_access deny !validUserAgent
>> ## *MAYBE I'M HAVING A SYNTAX PROBLEM?*
> This config example cut-n-pasted from the wiki tutorial on *Blocking
> Skype* seems to be doing exactly what it was designed for. Delivering a 403.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jun  4 13:38:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 01:38:51 +1200
Subject: [squid-users] Error Resolution (TunnelStateData::Connection::
 error )
In-Reply-To: <CAKLXwH4Ov-YhY0C1vmbgpqDgDrxpt+4fdYNtRV2-KakAQg5Bqw@mail.gmail.com>
References: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>	<556DB89E.1050305@treenet.co.nz>
 <CAKLXwH4Ov-YhY0C1vmbgpqDgDrxpt+4fdYNtRV2-KakAQg5Bqw@mail.gmail.com>
Message-ID: <557054EB.4020900@treenet.co.nz>

On 5/06/2015 1:18 a.m., Iruma Keisuke wrote:
> Thank you Amos.
> 
> 2015-06-02 23:07 GMT+09:00, Amos Jeffries <squid3 at treenet.co.nz>:
>> On 2/06/2015 9:15 p.m., Irimajiri keisuke wrote:
>>> Dear all,
>>>
>>> I have to build a proxy server by using the squid.
>>> The number of clients is 400 people.
>>>
>>> I do not know the cause of the error message that appears in the
>>> cache.log.
>>> In the weekday, I have come up with an error every few hours 8:00 to
>>> 18:00.
>>> Access concentration I look like does not matter.
>>>
>>> [cache.log error message]
>>> 2015/05/11 13:37:24| TunnelStateData::Connection:: error : FD 610:
>>> read/write failure: (110) Connection timed out
>>>
>>> Why I want to know whether this error has occurred.
>>
>> Yes it has occured. You would not be seeing it otherwise.
>>
>>> Also, I want to know the impact on the user.
>>
>> The user who is causing the problem is probably not impacted at all.
>> Every other user sharing the proxy is impacted by the reduction in
>> available network socket, memory and CPU resources.
>>
> It seems to be no abnormality in the state of network sockets and
> memory and CPU.
> Is it safe to ignore this error?

If you think your service is operating fine. It does mean the proxy has
a lower threshold of tolerance for network congestion than normal.


>>>
>>> [squidclient mgr:filedescriptor]
>>> Every five minutes record
>>> extract FD610
>>>
>>> It looks like an error has occurred in the use to which the terminal
>>> of xxx.xxx.2.115 user.
>>> Is it a problem of communication of the user and the proxy?
>>>
>>
>> Nothing happened on a TCP conection for a long time. It was closed by
>> the networking sub-systems somewhere between Squid and the client.
>>
> 
> Do error is not out on the web browser?
> Could you detailed information about TCP state and the state of the
> user when an error has occurred.

It might be, or it might not be. Others before you who noticed the same
messages have a mixed set of reasons found for it. Some was Chrome
browser happy eyeballs algorithm leaking its second connection until it
got dropped by the network TCP stack. Some it was F5 load balancers
breaking. Others it was NAT timeout in users home routers. Some have not
bothered to dig deep so it may be other causes.

All that is certain is that something between Squid and user is closing
the connection while it is in an HTTP idle state.

> 
>>> Active file descriptors:
>>> File Type   Tout Nread  * Nwrite * Remote Address        Description
>>> ---- ------ ---- -------- -------- ---------------------
>>> ------------------------------
>>>  610 Socket  893    39494*   50228  xxx.xxx.xxx.162:443
>>> outlook.office365.com:443       2015/05/11_13:08:29
>>>  610 Socket 86329   45754*  103329  xxx.xxx.6.141:50174   Reading next
>>> request         2015/05/11_13:13:29
>>>  610 Socket 86258    6516*   13975  xxx.xxx.2.115:50820   Reading next
>>> request         2015/05/11_13:18:29
>>>  610 Socket 85958   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>> request         2015/05/11_13:23:29
>>>  610 Socket 85657   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>> request         2015/05/11_13:28:29
>>>  610 Socket 85357   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>> request         2015/05/11_13:33:29
>>>  610 Socket 86336    3652*    8003  xxx.xxx.3.152:50817   Reading next
>>> request         2015/05/11_13:38:29
>>>
>>> [access.log]
>>> I do not see suspicious error log I tried to extract the address
>>> xxx.xxx.2.115.
>>>
>>> Please tell me a good idea toward someone solve.
>>
>> Please provided additional details:
>>  * Squid version
>>  * Squid configuration
>>
>>
>> I suspect you have a quite old verion of Squid. That particular error
>> message does not even exist in the code any more. The current releases
>> display much more TCP details about the connection where the error occured.
> 
> squid version is squid-3.1.10-29.
> This is the latest version that RedHat is delivering.

Ah, yes RHEL policy of not updating unless explicit bug reports exist
and supporting things for 10 years+ >

You may be interested in the official unofficial packages (accepted by
the Squid Project and community as reasonable packages for use, but not
RHEL official supported).
<http://wiki.squid-cache.org/KnowledgeBase/RedHat>


> 
> [squid.conf]
> ------------------------------------
> acl manager proto cache_object
> acl localhost src 127.0.0.1/32 ::1
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> plugged) machines
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> acl CONNECT method CONNECT
> http_access allow manager localhost
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow all

!!!! Open proxy !!!


> http_port 192.168.1.1:8080
> hierarchy_stoplist cgi-bin ?
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> 
> cache_mem 2048 MB
> cache_store_log none
> visible_hostname unknown
> request_header_access X-FORWARDED-FOR deny all
> request_header_access Via deny all
> max_filedesc 10240
> ipcache_size 10240
> -----------------------------------------------
> 
> Please let me ask further questions
> Are these has to do with the error?
> http://www.squid-cache.org/Doc/code/tunnel_8cc_source.html
> 
> 472 TunnelStateData::Connection::error(int const xerrno)
> 473 {
> 474  /* XXX fixme xstrerror and xerrno... */
> 475  errno = xerrno;
> 476
> 477  debugs(50, debugLevelForError(xerrno), HERE << conn << ":
> read/write failure: " << xstrerror());
> 478
> 479  if (!ignoreErrno(xerrno))
> 480  conn->close();
> 481 }
> 
> 536  /* Bump the dest connection read timeout on any activity */
> 537  /* see Bug 3659: tunnels can be weird, with very long one-way transfers */
> 538  if (Comm::IsConnOpen(to.conn)) {
> 539  AsyncCall::Pointer timeoutCall = commCbCall(5, 4, "tunnelTimeout",
> 540  CommTimeoutCbPtrFun(tunnelTimeout, this));
> 541  commSetConnTimeout(to.conn, Config.Timeout.read, timeoutCall);
> 542  }
> 543
> 

Possibly, but predates all that codes existence.

Amos



From kl at vsen.dk  Thu Jun  4 13:39:43 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:39:43 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55705082.2060501@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk>
Message-ID: <5570551F.4050304@vsen.dk>

I tried this:
http_access allow CONNECT testurls testsrv1

But that doesn't work.

Klavs Klavsen wrote on 06/04/2015 03:20 PM:
> Hi,
>
> I added the bumpedports - and now traffic works and is allowed.. but it
> allows everything on https.. :(
>
> Log says:
> 10.xx.130.50 - - [04/Jun/2015:15:16:07 +0200] "CONNECT 72.51.34.34:443
> HTTP/1.1" lwn.net - 200 28189 TCP_TUNNEL:ORIGINAL_DST peek
>
> so it doesn't seem to check the http_access lines for testsrv1.
>
> My config as it is now:
> acl localhost src 127.0.0.1/32 ::1
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #only contains one ip range - which I'm not accessing
> #I don't quite understand what the purpose of this "broken" thing
> # is and what it does :(
> acl broken dst "/etc/squid/broken.txt"
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek step1 broken
> ssl_bump peek step2 broken
> ssl_bump splice broken
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump bump all
>
> sslproxy_capath /etc/ssl/certs
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
>
> # user-defined ACLs
> acl okweb-urls url_regex ^http://www.youtube.com/
> ^http://vimeo.com/api/oembed.json$
> ^https://www.google.com/accounts/ClientLogin$
> ^https://www.googleapis.com/analytics/
> acl testurls url_regex ^http://www.dr.dk/$ ^https://www.google.dk/$
> acl testbox src 10.xx.138.168
> acl testsrv1 src 10.xx.130.50
>
> acl bumpedPorts myportname 3129
> acl bumpedPorts myportname 3130
> http_access allow CONNECT bumpedPorts
>
> http_access allow testurls testbox
> http_access allow testurls testsrv1
> http_access allow okweb-urls testsrv1
> http_access deny all
>
> http_port 3128
> coredump_dir                   /var/spool/squid
> maximum_object_size_in_memory  512 KB
> maximum_object_size            4096 KB
> ignore_expect_100              off
> cache_mgr                      root
> client_persistent_connections  on
> server_persistent_connections  on
>
> # user-defined configuration settings from config_hash
> ssl_bump                       server-first all
> sslcrtd_children               8 startup=1 idle=1
> sslcrtd_program                /usr/lib64/squid/ssl_crtd -s
> /etc/ssl/certs/cache/ -M 4MB
> https_port                     3130 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/etc/squid/ca.cert cafile=/etc/squid/ca.cert
> key=/etc/squid/ca.private sslflags=NO_SESSION_REUSE
> http_port                      3129 intercept
>
> logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni
> %ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode
>
> access_log /var/log/squid/access.log mine
>
>
> Amos Jeffries wrote on 06/02/2015 05:05 PM:
>> On 3/06/2015 2:46 a.m., Klavs Klavsen wrote:
>>> Amos Jeffries wrote on 06/02/2015 04:10 PM:
>>>> On 3/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>>>> Thank you Amos.
>>>>>
>>>>> I'll build 3.5.5 then..
>>>>>
>>>>> any config changes I need to be aware of?
>>>>
>>>> --with-openssl instead of --enable-ssl is the only one that comes to
>>>> mind right now. The release notes for 3.4 and 3.5 have the lists.
>>>>
>>>
>>> I borrowed the spec from fedora 23.. :)
>>>
>>> After installing 3.5.5 instead - it now complains when trying to issue
>>> certificate :(
>>>
>>> squid cache log says:
>>> Error negotiating SSL connection on FD 10: error:14094412:SSL
>>> routines:SSL3_READ_BYTES:sslv3 alert bad certificate
>>>
>>> client gets:
>>> curl: (51) SSL: certificate subject name '64.233.184.103' does not match
>>> target host name 'www.google.com'
>>>
>>> any hints for tests I can do, to figure out the problem would be very
>>> much appreciated :)
>>
>> James Lay has just done some good investigations in his "SSL-bump deep
>> dive" thread(s). Compare what he came up with to your config
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Thu Jun  4 13:42:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 01:42:25 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55705082.2060501@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk>
Message-ID: <557055C1.9080502@treenet.co.nz>

On 5/06/2015 1:20 a.m., Klavs Klavsen wrote:
> Hi,
> 
> I added the bumpedports - and now traffic works and is allowed.. but it
> allows everything on https.. :(
> 
> Log says:
> 10.xx.130.50 - - [04/Jun/2015:15:16:07 +0200] "CONNECT 72.51.34.34:443
> HTTP/1.1" lwn.net - 200 28189 TCP_TUNNEL:ORIGINAL_DST peek
> 
> so it doesn't seem to check the http_access lines for testsrv1.

So, you maybe need to put the bumpedPorts check down just before the
"deny all" line.

Amos


From kl at vsen.dk  Thu Jun  4 13:45:03 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 15:45:03 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <557055C1.9080502@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
Message-ID: <5570565F.4090609@vsen.dk>

after moving it here:

http_access allow okweb-urls testsrv1
http_access allow CONNECT bumpedPorts
http_access deny all

it still allows everything..

Amos Jeffries wrote on 06/04/2015 03:42 PM:
> On 5/06/2015 1:20 a.m., Klavs Klavsen wrote:
>> Hi,
>>
>> I added the bumpedports - and now traffic works and is allowed.. but it
>> allows everything on https.. :(
>>
>> Log says:
>> 10.xx.130.50 - - [04/Jun/2015:15:16:07 +0200] "CONNECT 72.51.34.34:443
>> HTTP/1.1" lwn.net - 200 28189 TCP_TUNNEL:ORIGINAL_DST peek
>>
>> so it doesn't seem to check the http_access lines for testsrv1.
>
> So, you maybe need to put the bumpedPorts check down just before the
> "deny all" line.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From marcel at guineanet.net  Thu Jun  4 13:27:58 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Thu, 4 Jun 2015 06:27:58 -0700 (PDT)
Subject: [squid-users] worker per cache_dir
In-Reply-To: <556FBB38.4090708@treenet.co.nz>
References: <1433351880614-4671510.post@n4.nabble.com>
 <556FBB38.4090708@treenet.co.nz>
Message-ID: <1433424478176-4671541.post@n4.nabble.com>

Cool Thanks 
but I have an error while doing that maybe it could be the HDD size
By the way Amos what could you suggest me to handle disks
I have a jbod with 15 disks (4TB) each 
I read on of your comment stipulating to set a cache_dir per drive (o I'm
totaly wrong)
with this worker/disk  distribution I end up with error 

cache_dir rock /cache16 500000 min-size=1 max-size=31000 max-swap-rate=200 
swap-timeout=300

if ${process_number} = 1
cache_dir aufs /cache0   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache1   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 2
cache_dir aufs /cache2   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache3   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 3
cache_dir aufs /cache4   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache5   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 4
cache_dir aufs /cache6   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache7   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 5
cache_dir aufs /cache8   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache9   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 6
cache_dir aufs /cache10   3500000 32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache11   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 7
cache_dir aufs /cache12   3500000 32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache13   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 8
cache_dir aufs /cache14   3500000  32 256 min-size=31001 max-size=1048576000
cache_dir aufs /cache15   3500000  32 256 min-size=31001 max-size=1048576000
endif




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/worker-per-cache-dir-tp4671510p4671541.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun  4 14:01:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 02:01:23 +1200
Subject: [squid-users] Skype issue
In-Reply-To: <55705218.2060904@tasso.com.ar>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
 <1433419543048-4671527.post@n4.nabble.com> <55704DCF.5060106@treenet.co.nz>
 <55705218.2060904@tasso.com.ar>
Message-ID: <55705A33.6060509@treenet.co.nz>

On 5/06/2015 1:26 a.m., Jonathan Filogna wrote:
> And if i want to make exceptions to memberships on AD, how can i do it?
> That's what i need.

You can do it two ways.

A) place the "!administrador" test on the end of each of the skype deny
lines.

B) place an "allow administrador" line above the skype rules


In both cases remove the two custom administrador related lines you have
now, and move the skype rules down below your "deny !ntlm" line and
above the "allow ntlm" line.

Like so:

  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow manager webserver
  http_access deny manager
  acl ntlm proxy_auth REQUIRED
  http_access deny !ntlm

 # (for the A way)
  http_access deny numeric_IPS !administrador
  http_access deny Skype_UA !administrador
  http_access deny !validUserAgent !administrador

# (for the B way)
  http_access allow administrador
  http_access deny numeric_IPS
  http_access deny Skype_UA
  http_access deny !validUserAgent


  http_access allow ntlm
  http_access deny all

Amos



From jonathan.filogna at tasso.com.ar  Thu Jun  4 14:09:12 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Thu, 04 Jun 2015 11:09:12 -0300
Subject: [squid-users] Skype issue
In-Reply-To: <55705A33.6060509@treenet.co.nz>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
 <1433419543048-4671527.post@n4.nabble.com> <55704DCF.5060106@treenet.co.nz>
 <55705218.2060904@tasso.com.ar> <55705A33.6060509@treenet.co.nz>
Message-ID: <55705C08.3000005@tasso.com.ar>

Amos, i'll test it

Thank you very, very much

El 04/06/15 a las 11:01, Amos Jeffries escibi?:
> On 5/06/2015 1:26 a.m., Jonathan Filogna wrote:
>> And if i want to make exceptions to memberships on AD, how can i do it?
>> That's what i need.
> You can do it two ways.
>
> A) place the "!administrador" test on the end of each of the skype deny
> lines.
>
> B) place an "allow administrador" line above the skype rules
>
>
> In both cases remove the two custom administrador related lines you have
> now, and move the skype rules down below your "deny !ntlm" line and
> above the "allow ntlm" line.
>
> Like so:
>
>    http_access deny !Safe_ports
>    http_access deny CONNECT !SSL_ports
>    http_access allow manager webserver
>    http_access deny manager
>    acl ntlm proxy_auth REQUIRED
>    http_access deny !ntlm
>
>   # (for the A way)
>    http_access deny numeric_IPS !administrador
>    http_access deny Skype_UA !administrador
>    http_access deny !validUserAgent !administrador
>
> # (for the B way)
>    http_access allow administrador
>    http_access deny numeric_IPS
>    http_access deny Skype_UA
>    http_access deny !validUserAgent
>
>
>    http_access allow ntlm
>    http_access deny all
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jun  4 14:11:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 02:11:51 +1200
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433424478176-4671541.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
 <556FBB38.4090708@treenet.co.nz> <1433424478176-4671541.post@n4.nabble.com>
Message-ID: <55705CA7.7000109@treenet.co.nz>

On 5/06/2015 1:27 a.m., Marcel Fossua wrote:
> Cool Thanks 
> but I have an error while doing that maybe it could be the HDD size
> By the way Amos what could you suggest me to handle disks
> I have a jbod with 15 disks (4TB) each 
> I read on of your comment stipulating to set a cache_dir per drive (o I'm
> totaly wrong)

That is correct, only place one *AUFS/UFS/diskd cache_dir* per physical
drive disk. But that is because Squid will overload the HDD controller
with seek and I/O collisions when 2 or more are stored together.


> with this worker/disk  distribution I end up with error 
> 

Aha, which you have forgotten to copy so we can see exactly what you are
going on about.


If I assume, I would guess it is more likely a 32-bit rounding error
calculating how big in Bytes the cache is or the max-size=N object
parameter. Though the total size should be in 64-bit math by now.

Amos


From jonathan.filogna at tasso.com.ar  Thu Jun  4 14:15:55 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Thu, 04 Jun 2015 11:15:55 -0300
Subject: [squid-users] Skype issue
In-Reply-To: <55705A33.6060509@treenet.co.nz>
References: <1400769327828-4666074.post@n4.nabble.com>
 <1433356481708-4671511.post@n4.nabble.com> <556FB9C0.5070301@treenet.co.nz>
 <1433419543048-4671527.post@n4.nabble.com> <55704DCF.5060106@treenet.co.nz>
 <55705218.2060904@tasso.com.ar> <55705A33.6060509@treenet.co.nz>
Message-ID: <55705D9B.3090601@tasso.com.ar>

Thank you Amos, really. I own you a wine (?)

Have a nice day

Cheers

Jonathan

El 04/06/15 a las 11:01, Amos Jeffries escibi?:
> On 5/06/2015 1:26 a.m., Jonathan Filogna wrote:
>> And if i want to make exceptions to memberships on AD, how can i do it?
>> That's what i need.
> You can do it two ways.
>
> A) place the "!administrador" test on the end of each of the skype deny
> lines.
>
> B) place an "allow administrador" line above the skype rules
>
>
> In both cases remove the two custom administrador related lines you have
> now, and move the skype rules down below your "deny !ntlm" line and
> above the "allow ntlm" line.
>
> Like so:
>
>    http_access deny !Safe_ports
>    http_access deny CONNECT !SSL_ports
>    http_access allow manager webserver
>    http_access deny manager
>    acl ntlm proxy_auth REQUIRED
>    http_access deny !ntlm
>
>   # (for the A way)
>    http_access deny numeric_IPS !administrador
>    http_access deny Skype_UA !administrador
>    http_access deny !validUserAgent !administrador
>
> # (for the B way)
>    http_access allow administrador
>    http_access deny numeric_IPS
>    http_access deny Skype_UA
>    http_access deny !validUserAgent
>
>
>    http_access allow ntlm
>    http_access deny all
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jun  4 14:19:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 02:19:14 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <5570565F.4090609@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk>
Message-ID: <55705E62.6060801@treenet.co.nz>

On 5/06/2015 1:45 a.m., Klavs Klavsen wrote:
> after moving it here:
> 
> http_access allow okweb-urls testsrv1
> http_access allow CONNECT bumpedPorts
> http_access deny all
> 
> it still allows everything..

Sigh. Sorry I must be half aslep right now.

Your rules say:

  allow ...
  allow ...
  allow ...

So why would anything be denied?


Secondly, the log line you pointed out was for peek operation. URL (for
url_regex ACLs to match) is not known or available until bumping
(specifically the full "bump" action) has been completed.

Amos



From kl at vsen.dk  Thu Jun  4 14:50:00 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 16:50:00 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55705E62.6060801@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
Message-ID: <55706598.7070800@vsen.dk>

Amos Jeffries wrote on 06/04/2015 04:19 PM:
> On 5/06/2015 1:45 a.m., Klavs Klavsen wrote:
>> after moving it here:
>>
>> http_access allow okweb-urls testsrv1
>> http_access allow CONNECT bumpedPorts
>> http_access deny all
>>
>> it still allows everything..
>
> Sigh. Sorry I must be half aslep right now.
>
> Your rules say:
>
>    allow ...
>    allow ...
>    allow ...
>
> So why would anything be denied?
>

last line says: deny all

and it works for http urls.. it denies the websites not listed in 
testurls for testsrv1.

>
> Secondly, the log line you pointed out was for peek operation. URL (for
> url_regex ACLs to match) is not known or available until bumping
> (specifically the full "bump" action) has been completed.
>
but the "allow CONNECT" line, seems to make it skip the
http_access deny all

at the bottom.. (and not parse the allows in between which should be the 
ones allowing certain websites on https as well..

do I need to change:
ssl_bump bump all

to list every https site
acl ok-httpsurls url_regex ^https://www.google.dk/$
ssl_bump bump ok-httpsurls
ssl_bump reject !ok-httpsurls

(so I an only use http_access for http intercept and must use ssl_bump 
for https urls) ?


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From marcel at guineanet.net  Thu Jun  4 15:20:14 2015
From: marcel at guineanet.net (Marcel)
Date: Thu, 04 Jun 2015 16:20:14 +0100
Subject: [squid-users] worker per cache_dir
In-Reply-To: <55705CA7.7000109@treenet.co.nz>
References: <1433351880614-4671510.post@n4.nabble.com>
 <556FBB38.4090708@treenet.co.nz> <1433424478176-4671541.post@n4.nabble.com>
 <55705CA7.7000109@treenet.co.nz>
Message-ID: <55706CAE.6020708@guineanet.net>

Hi Amos this is the output error

*2015/06/04 16:18:22 kid1| Logfile: opening log 
stdio:/var/log/squid/error.log
FATAL: xcalloc: Unable to allocate 18446744073689603781 blocks of 1 bytes!


2015/06/04 16:18:37 kid4| Logfile: opening log 
stdio:/var/log/squid/error.log
FATAL: xcalloc: Unable to allocate 18446744073689603781 blocks of 1 bytes!

......
*
El 4/6/15 a las 15:11, Amos Jeffries escribi?:
> On 5/06/2015 1:27 a.m., Marcel Fossua wrote:
>> Cool Thanks
>> but I have an error while doing that maybe it could be the HDD size
>> By the way Amos what could you suggest me to handle disks
>> I have a jbod with 15 disks (4TB) each
>> I read on of your comment stipulating to set a cache_dir per drive (o I'm
>> totaly wrong)
> That is correct, only place one *AUFS/UFS/diskd cache_dir* per physical
> drive disk. But that is because Squid will overload the HDD controller
> with seek and I/O collisions when 2 or more are stored together.
>
>
>> with this worker/disk  distribution I end up with error
>>
> Aha, which you have forgotten to copy so we can see exactly what you are
> going on about.
>
>
> If I assume, I would guess it is more likely a 32-bit rounding error
> calculating how big in Bytes the cache is or the max-size=N object
> parameter. Though the total size should be in 64-bit math by now.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Fossua-vcard
	 Marcel Fossua
Unix/Linux Network Administrator
   Tel: 0240 222299448
www.guineanet.net <http://www.guineanet.net>/ www.familyfossua.com 
<http://www.familyfossua.com>








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/31c9d996/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: guineanet.png
Type: image/png
Size: 24663 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/31c9d996/attachment.png>

From kl at vsen.dk  Thu Jun  4 15:34:41 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 04 Jun 2015 17:34:41 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55706598.7070800@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk>
Message-ID: <55707011.7070003@vsen.dk>

I would be perfectly fine with allowing the SSL bumping to finish for
ALL https sites - and then only block when the http request comes..

I'm hoping someone can tell me what I've done wrong in my config.. I'm
obviously not understanding how it works when https is envolved.. it
works as intended with http..

Klavs Klavsen wrote on 2015-06-04 16:50:
> Amos Jeffries wrote on 06/04/2015 04:19 PM:
>> On 5/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>> after moving it here:
>>>
>>> http_access allow okweb-urls testsrv1
>>> http_access allow CONNECT bumpedPorts
>>> http_access deny all
>>>
>>> it still allows everything..
>>
>> Sigh. Sorry I must be half aslep right now.
>>
>> Your rules say:
>>
>>    allow ...
>>    allow ...
>>    allow ...
>>
>> So why would anything be denied?
>>
> 
> last line says: deny all
> 
> and it works for http urls.. it denies the websites not listed in
> testurls for testsrv1.
> 
>>
>> Secondly, the log line you pointed out was for peek operation. URL (for
>> url_regex ACLs to match) is not known or available until bumping
>> (specifically the full "bump" action) has been completed.
>>
> but the "allow CONNECT" line, seems to make it skip the
> http_access deny all
> 
> at the bottom.. (and not parse the allows in between which should be the
> ones allowing certain websites on https as well..
> 
> do I need to change:
> ssl_bump bump all
> 
> to list every https site
> acl ok-httpsurls url_regex ^https://www.google.dk/$
> ssl_bump bump ok-httpsurls
> ssl_bump reject !ok-httpsurls
> 
> (so I an only use http_access for http intercept and must use ssl_bump
> for https urls) ?
> 
> 


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
  --Henry Spencer



From atman.sense at zise.de  Thu Jun  4 15:59:24 2015
From: atman.sense at zise.de (Atman Sense)
Date: Thu, 04 Jun 2015 17:59:24 +0200
Subject: [squid-users] grab hostnames via SNI to use it for parent proxy
Message-ID: <62869e10c6d5ea46130fa093a8af6a65@zise.de>

Hi,

I'm using privoxy in transparent/intercepting mode to filter tracking 
sites. Because many sites switched to https I want to block https sites, 
too (only by hostnames, I don't want to decrypt the SSL connections).

My idea was to use squid to intercept https connections and peek/splice 
to get the hostname via SNI.

The problem is: When using a parent proxy, squid always "CONNECT" the IP 
and not the hostname, even if it is aware of it through SNI. Can I get 
squid to use the hostnames instead of IPs?



From tmblue at gmail.com  Thu Jun  4 17:58:21 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 4 Jun 2015 10:58:21 -0700
Subject: [squid-users] How to stop " ICP is disabled! Cannot send ICP
	request to peer."
Message-ID: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>

I am running HDCP or at least testing with it and thus have ICP disabled. I
know it's disabled but I don't need it yelling at me every few
minutes/seconds. How can I tell Squid, yes thank you, I'm aware I'm not
using ICP and it's disabled, now quiet?!

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150604/00be062d/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun  4 22:18:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 10:18:35 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55707011.7070003@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
Message-ID: <5570CEBB.1040607@treenet.co.nz>

On 5/06/2015 3:34 a.m., Klavs Klavsen wrote:
> I would be perfectly fine with allowing the SSL bumping to finish for
> ALL https sites - and then only block when the http request comes..
> 
> I'm hoping someone can tell me what I've done wrong in my config.. I'm
> obviously not understanding how it works when https is envolved.. it
> works as intended with http..

It should be working. I'm a bit confused myself now why that CONNECT
line would be matching the decrypted requests, they definitely should
not be having the CONNECT request method as they are destined to an
origin server.

We've missed something basic, and will probably kick ourselves at how
simple when its reavealed. :-(
 All I can think of now is that James log format should be indicating
more clearly whats going on than the default Squid one will.

Amos



From squid3 at treenet.co.nz  Thu Jun  4 22:22:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 10:22:47 +1200
Subject: [squid-users] grab hostnames via SNI to use it for parent proxy
In-Reply-To: <62869e10c6d5ea46130fa093a8af6a65@zise.de>
References: <62869e10c6d5ea46130fa093a8af6a65@zise.de>
Message-ID: <5570CFB7.3000207@treenet.co.nz>

On 5/06/2015 3:59 a.m., Atman Sense wrote:
> Hi,
> 
> I'm using privoxy in transparent/intercepting mode to filter tracking
> sites. Because many sites switched to https I want to block https sites,
> too (only by hostnames, I don't want to decrypt the SSL connections).
> 
> My idea was to use squid to intercept https connections and peek/splice
> to get the hostname via SNI.
> 
> The problem is: When using a parent proxy, squid always "CONNECT" the IP
> and not the hostname, even if it is aware of it through SNI. Can I get
> squid to use the hostnames instead of IPs?

You can block by SNI in the ssl_bump checks without having bumped the
connection.

Like so:

 # get the public TLS metadata (includes SNI)
 ssl_bump peek all

 # block based on SNI matching (or server cert matching)
 acl blocked ssl::server_name .example.com
 ssl_bump terminate blocked

 # tunnel (no decrypting) for everything else
 ssl_bump splice all


Note that you do have to allow the "CONNECT raw-IP:443 ..." requests
through http_access to the bumping logics.

Amos



From squid3 at treenet.co.nz  Thu Jun  4 22:33:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 10:33:39 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55706598.7070800@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk>
Message-ID: <5570D243.1050409@treenet.co.nz>

On 5/06/2015 2:50 a.m., Klavs Klavsen wrote:
> Amos Jeffries wrote on 06/04/2015 04:19 PM:
>> On 5/06/2015 1:45 a.m., Klavs Klavsen wrote:
>>> after moving it here:
>>>
>>> http_access allow okweb-urls testsrv1
>>> http_access allow CONNECT bumpedPorts
>>> http_access deny all
>>>
>>> it still allows everything..
>>
>> Sigh. Sorry I must be half aslep right now.
>>
>> Your rules say:
>>
>>    allow ...
>>    allow ...
>>    allow ...
>>
>> So why would anything be denied?
>>
> 
> last line says: deny all
> 
> and it works for http urls.. it denies the websites not listed in
> testurls for testsrv1.

Okay. Those *are* the decrypted messages.

What you just said there tels me that your ACLs are working correctly.
Picture me confused. :-O

> 
>>
>> Secondly, the log line you pointed out was for peek operation. URL (for
>> url_regex ACLs to match) is not known or available until bumping
>> (specifically the full "bump" action) has been completed.
>>
> but the "allow CONNECT" line, seems to make it skip the
> http_access deny all
> 
> at the bottom.. (and not parse the allows in between which should be the
> ones allowing certain websites on https as well..
> 
> do I need to change:
> ssl_bump bump all
> 
> to list every https site
> acl ok-httpsurls url_regex ^https://www.google.dk/$
> ssl_bump bump ok-httpsurls
> ssl_bump reject !ok-httpsurls

Er, yes. The scheme is assumed to be https:// due to TLS existence, the
domain is given in SNI. But the URL path is still private/encrypted.

So the URL will never match any pattern with path component, and is
unlikely to even attempt matching in current Squid.

> 
> (so I an only use http_access for http intercept and must use ssl_bump
> for https urls) ?
> 

https:// URL requests will be passed by http_access like any other
traffic, but with the caveat that it happens only after the connection
has already been/being decrypted. AKA *after* "ssl_bump bump ..." has
been matched.

Amos


From squid3 at treenet.co.nz  Thu Jun  4 22:46:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 10:46:03 +1200
Subject: [squid-users] grab hostnames via SNI to use it for parent proxy
In-Reply-To: <5570CFB7.3000207@treenet.co.nz>
References: <62869e10c6d5ea46130fa093a8af6a65@zise.de>
 <5570CFB7.3000207@treenet.co.nz>
Message-ID: <5570D52B.50807@treenet.co.nz>

On 5/06/2015 10:22 a.m., Amos Jeffries wrote:
> On 5/06/2015 3:59 a.m., Atman Sense wrote:
>> Hi,
>>
>> I'm using privoxy in transparent/intercepting mode to filter tracking
>> sites. Because many sites switched to https I want to block https sites,
>> too (only by hostnames, I don't want to decrypt the SSL connections).
>>
>> My idea was to use squid to intercept https connections and peek/splice
>> to get the hostname via SNI.
>>
>> The problem is: When using a parent proxy, squid always "CONNECT" the IP
>> and not the hostname, even if it is aware of it through SNI. Can I get
>> squid to use the hostnames instead of IPs?

Sorry, I was not reading your questio fully and correctly.

The default log records the requested URL. On intercepted TLS
connections there is none (just raw-IP:port), SNI is its own thing
separately.

You can log SNI, but with the custom log format tag %ssl::>sni

Amos



From squid3 at treenet.co.nz  Thu Jun  4 22:56:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 05 Jun 2015 10:56:44 +1200
Subject: [squid-users] How to stop " ICP is disabled! Cannot send ICP
 request to peer."
In-Reply-To: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>
References: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>
Message-ID: <5570D7AC.9080905@treenet.co.nz>

On 5/06/2015 5:58 a.m., Tory M Blue wrote:
> I am running HDCP or at least testing with it and thus have ICP disabled. I
> know it's disabled but I don't need it yelling at me every few
> minutes/seconds. How can I tell Squid, yes thank you, I'm aware I'm not
> using ICP and it's disabled, now quiet?!

You have to configure the 'htcp' option on cache_peer lines in squid.conf.

Amos



From rafael.akchurin at diladele.com  Fri Jun  5 08:01:22 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 5 Jun 2015 08:01:22 +0000
Subject: [squid-users] Squid 3.5.5 for Microsoft Windows 64-bit is available
Message-ID: <DB5PR04MB11281DAC96ED2608BC54E06C8FB20@DB5PR04MB1128.eurprd04.prod.outlook.com>

Greetings everyone,


The CygWin based build of Squid proxy for Microsoft Windows version 3.5.5 is now available (amd64 only!).

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.5-RELEASENOTES.html.
* Ready to use MSI package can be downloaded from http://squid.diladele.com.<http://squid.diladele.com>
* List of open issues for the installer - https://github.com/diladele/squid3-windows/issues

Thanks a lot for Squid developers for making this great software!

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com.

NB: tested on the upcoming release of Windows 10 - seems to work pretty good. Also fixed the installation glitch on Windows 2012 R2.

Best regards,
Rafael Akchurin
Diladele B.V.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150605/203faf85/attachment.htm>

From reet.vyas28 at gmail.com  Fri Jun  5 08:13:43 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Fri, 5 Jun 2015 13:43:43 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <55705009.9080200@treenet.co.nz>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
 <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
 <55705009.9080200@treenet.co.nz>
Message-ID: <CAA8ViV8==pthmX4X4r3RZ9JTjSCyaTv1ZJUp0adr4cJnaR6kDA@mail.gmail.com>

Hi

Thanks for reply. I am trying to cache youtube using this wiki
http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube but I
cant cache youtube.

I want to cache facebook and youtube. SSl certificate installation that I
have to do . Please suggest some links.

On Thu, Jun 4, 2015 at 6:48 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/06/2015 12:55 a.m., Reet Vyas wrote:
> > Thank you everyone for helping me to setup squid , Now its working but in
> > access.logs  I only see tcp_miss if m using same website. I mean squid is
> > not caching
>
> You will get MISS a fair bit more with intercepted traffic than with
> normal proxied traffic. Particularly on certain major CDN who play
> tricks with DNS.
>
> The reasons and some workarounds to need to be doing are explained in
> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150605/e993b126/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Jun  5 18:47:02 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Fri, 5 Jun 2015 11:47:02 -0700
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
Message-ID: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>

Hello All 

I want to ask how could I get out the log

TCP_MISS/403 353 HEAD text/plain  Error ?????

I have many logs in cach.log about that and I want to figure out this issue
!

Aby help ?

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150605/794f53ad/attachment.htm>

From yashvinder.edx at gmail.com  Fri Jun  5 12:03:42 2015
From: yashvinder.edx at gmail.com (Edx gmail)
Date: Fri, 05 Jun 2015 17:33:42 +0530
Subject: [squid-users] Traffic redirection to squid socket
Message-ID: <20150605120342.5402704.50028.732@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150605/88196787/attachment.htm>

From davincy9102 at hotmail.com  Fri Jun  5 14:51:51 2015
From: davincy9102 at hotmail.com (davincy)
Date: Fri, 5 Jun 2015 07:51:51 -0700 (PDT)
Subject: [squid-users] Help with squid 4.5. on centos 6.6. filter Https
In-Reply-To: <1433515533331-4671561.post@n4.nabble.com>
References: <1433515533331-4671561.post@n4.nabble.com>
Message-ID: <1433515911998-4671562.post@n4.nabble.com>

Hi Everybody

Im young and curious with the new theme, filter https with squid

Im from Colombia, Im trying apply the new options of squid but I don?t find
some directory when you make examples or modification for make funcional
squid

"squid3" or the directory on opt whit the cert_ssl, ssl_db I need create
that manually?

Im using Shorewall for administrate my Iptables

Someone can help me with some example for centos or explain to me what I
need do for the directory that not find

thanks, sorry for my english 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-with-squid-4-5-on-centos-6-6-filter-Https-tp4671561p4671562.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From dwd at fnal.gov  Fri Jun  5 17:20:56 2015
From: dwd at fnal.gov (Dave Dykstra)
Date: Fri, 5 Jun 2015 12:20:56 -0500
Subject: [squid-users] what happened to squid-3.5.5 release announcement?
Message-ID: <20150605172056.GA44283@fnal.gov>

I was surprised to see an announcment of squid-3.5.5 for Windows, when
I never saw a squid-3.5.5 release announcement.  Indeed the squid-cache.org
website has it, but there's no announcement in
    http://lists.squid-cache.org/pipermail/squid-announce/2015-May/thread.html

I do recall having a problems accessing list.squid-cache.org in May,
maybe the announcement was sent but needs to be re-sent?

Dave

On Fri, Jun 05, 2015 at 08:01:22AM +0000, Rafael Akchurin wrote:
> Subject: Re: [squid-users] Squid 3.5.5 for Microsoft Windows 64-bit is available
> Greetings everyone,
> 
> 
> The CygWin based build of Squid proxy for Microsoft Windows version 3.5.5 is now available (amd64 only!).
> 
> * Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.5-RELEASENOTES.html.
> * Ready to use MSI package can be downloaded from http://squid.diladele.com.<http://squid.diladele.com>
> * List of open issues for the installer - https://github.com/diladele/squid3-windows/issues
> 
> Thanks a lot for Squid developers for making this great software!
> 
> Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
> https://github.com/diladele/squid3-windows. Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com.
> 
> NB: tested on the upcoming release of Windows 10 - seems to work pretty good. Also fixed the installation glitch on Windows 2012 R2.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> 
> 

> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jun  5 19:22:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 07:22:24 +1200
Subject: [squid-users] Traffic redirection to squid socket
In-Reply-To: <20150605120342.5402704.50028.732@gmail.com>
References: <20150605120342.5402704.50028.732@gmail.com>
Message-ID: <5571F6F0.9060608@treenet.co.nz>

On 6/06/2015 12:03 a.m., Edx gmail wrote:
> Hi,
> I am using squid on Openwrt(chaos calmer) and want to redirect all requests for 
> port 80 to 3128 port of squid  so that i can filter the traffic there on squid 
> box. I am using wpad for auto proxy and want non-proxy user's traffic for port 
> 80 to go to squid socket which is in my case 192.168.1.1:3128
> 
> I have included
> "http_port 3128 intercept" in squid.conf
> and also made rule in firewall
> config redirect
>          option src 'lan'
>          option src_dport '80'
>          option src_ip '!192.168.1.1'
>          option dest 'wan'
>          option dest_port '3128'
>          option dest_ip '192.168.1.1'
>          option proto 'tcp'
>          option target 'DNAT'
> 
> Using the above configuration i am able to full fill my purpose but if do so 
> then users with auto proxy in their browers ?can't access internet at all while 
> in access.log it shows everything perfect.


You SHOULD NOT use port 3128 for receiving intercepted port 80 HTTP
origin traffic. It is registered officially for HTTP proxy traffic and
you cant mix the two traffic types.

Select a non-3128 port of your choice for Squid to listen for the
intercepted traffic. That will also allow easy firewalling without
showing port scanners that Squid is using it.
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>

Also, note that the NAT operations MUST be done on the Squid machine.
Squid requires internal access to the kernel NAT sytsem records to
intercept safely, you cant do that across machines.


Amos


From squid3 at treenet.co.nz  Fri Jun  5 21:00:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 09:00:49 +1200
Subject: [squid-users] what happened to squid-3.5.5 release announcement?
In-Reply-To: <20150605172056.GA44283@fnal.gov>
References: <20150605172056.GA44283@fnal.gov>
Message-ID: <55720E01.5090600@treenet.co.nz>

On 6/06/2015 5:20 a.m., Dave Dykstra wrote:
> I was surprised to see an announcment of squid-3.5.5 for Windows, when
> I never saw a squid-3.5.5 release announcement.  Indeed the squid-cache.org
> website has it, but there's no announcement in
>     http://lists.squid-cache.org/pipermail/squid-announce/2015-May/thread.html
> 
> I do recall having a problems accessing list.squid-cache.org in May,
> maybe the announcement was sent but needs to be re-sent?

Sory. Just a very busy week for me. Writing it up now.

Amos



From squid3 at treenet.co.nz  Fri Jun  5 22:53:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 10:53:08 +1200
Subject: [squid-users] Help with squid 4.5. on centos 6.6. filter Https
In-Reply-To: <1433515911998-4671562.post@n4.nabble.com>
References: <1433515533331-4671561.post@n4.nabble.com>
 <1433515911998-4671562.post@n4.nabble.com>
Message-ID: <55722854.5090607@treenet.co.nz>

On 6/06/2015 2:51 a.m., davincy wrote:
> Hi Everybody
> 
> Im young and curious with the new theme, filter https with squid
> 

Check your local laws before going any further. HTTPS interception and
decryption is illegal in most countries, and even where it is legal may
still require a government license or approval.


> Im from Colombia, Im trying apply the new options of squid but I don?t find
> some directory when you make examples or modification for make funcional
> squid
> 
> "squid3" or the directory on opt whit the cert_ssl, ssl_db I need create
> that manually?

"squid3" is the Debian package naming scheme. CentOS work differently.

Configuration should be in /etc/squid/

> 
> Im using Shorewall for administrate my Iptables
> 
> Someone can help me with some example for centos or explain to me what I
> need do for the directory that not find
> 
> thanks, sorry for my english 
> 

I suggest you start with the 3.5 package
<http://wiki.squid-cache.org/KnowledgeBase/CentOS> provided by Eliezer.
That includes OpenSSL support for these HTTPS features.

The wiki documentation is the next place to go for information. However
be aware that what people are calling "SSL-Bump" is now on its third
generation of features and some of the wiki documents are not exactly up
to date.

The articlaes relevant to Squid-3.5 are:
 <http://wiki.squid-cache.org/Features/SslPeekAndSplice>
 <http://wiki.squid-cache.org/Features/DynamicSslCert>
 <http://wiki.squid-cache.org/Features/MimicSslServerCert>


Amos


From squid3 at treenet.co.nz  Fri Jun  5 23:05:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 11:05:03 +1200
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
Message-ID: <55722B1F.4050007@treenet.co.nz>

On 6/06/2015 6:47 a.m., snakeeyes wrote:
> Hello All 
> 
> I want to ask how could I get out the log
> 
> TCP_MISS/403 353 HEAD text/plain  Error ?????
> 
> I have many logs in cach.log about that and I want to figure out this issue
> !
> 
> Aby help ?
> 


The posts you made earlier (including the private one) do not have
sufficient content to be able to help you.

Firstly, we definitely need to see the full URLs in access.log. So your
squid.conf needs to contain "strip_query_terms off" to see what stage
the YT request sequence is at.

For tracking down why the status is being given you will want
"debug_options 11,2 28,3" to log in cache.og the HTTP request/reply
headers and ACL proccessing decisions.

If you can run with debugs set to ALL,5 to get a more full trace that
would be excellent. But it will definitely produce a lot of data, so
thats optional. The above mentioned two sections should be sufficient to
identify teh next steps forward, and will produce a lot of info anyway.

Amos



From jlay at slave-tothe-box.net  Sat Jun  6 00:35:44 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 05 Jun 2015 18:35:44 -0600
Subject: [squid-users] Utilities for testing question
Message-ID: <1433550944.3640.1.camel@JamesiMac>

All,

I'm looking for a command line app like wget or curl that I can use to
test TLS.  I'm trying to find out how to send a get request without
sending the SNI.  Any pointers would be appreciated.  Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150605/bc680fa1/attachment.htm>

From marcel at guineanet.net  Sat Jun  6 00:26:52 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Fri, 5 Jun 2015 17:26:52 -0700 (PDT)
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433351880614-4671510.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
Message-ID: <1433550412150-4671571.post@n4.nabble.com>

Hi Amos 
any hints to help me with this issue 
I not able to set more than on cache_dir per worker because
it's alway end up with errors  (but no errors appears if only one cache_dir
is set per worker)


2015/06/06 01:36:09 kid7| Starting eCAP service:
ecap://www.vigos.com/ecap_gzip
2015/06/06 01:36:09 kid7| commBind: Cannot bind socket FD 9 to [::]: (13)
Permission denied
2015/06/06 01:36:09 kid6| commBind: Cannot bind socket FD 21 to [::]: (13)
Permission denied
2015/06/06 01:36:09 kid6| Store rebuilding is 0.00% complete
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
(2) No such file or directory

Squid Cache (Version 3.5.5): Terminated abnormally.
CPU Usage: 0.016 seconds = 0.016 user + 0.000 sys
Maximum Resident Size: 31344 KB
Page faults with physical i/o: 0
2015/06/06 01:36:14 kid6| Store rebuilding is 43.40% complete
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
(2) No such file or directory

Squid Cache (Version 3.5.5): Terminated abnormally.
CPU Usage: 0.016 seconds = 0.016 user + 0.000 sys
Maximum Resident Size: 31328 KB
Page faults with physical i/o: 0
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
(2) No such file or directory

Squid Cache (Version 3.5.5): Terminated abnormally.
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
(2) No such file or directory

one cache_dir/worker
******************
cache_dir rock /var/spool/squid3/cache10 460000 min-size=1 max-size=31000
max-swap-rate=200  swap-timeout=300

# a 200GB x 8 caches of large ( over 32KB) objects per-worker 
if ${process_number} = 1
cache_dir aufs /cache0  3500000  32 256 min-size=31001 max-size=104857600
endif
if ${process_number} = 2
cache_dir aufs /cache1   3500000  32 256 min-size=31001 max-size=1048576000
endif

if ${process_number} = 3
cache_dir aufs /cache2  3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 4
cache_dir aufs /cache3   3500000  32 256 min-size=31001 max-size=1048576000
endif
if ${process_number} = 5
cache_dir aufs /cache4  3500000  32 256 min-size=31001 max-size=1048576000
endif

2 cache_dir/worker
*****************
#if ${process_number} = 4
#cache_dir aufs /cache6/${process_number}   3500000  32 256 min-size=31001
max-size=1048576000
#cache_dir aufs /cache7/${process_number}   3500000  32 256 min-size=31001
max-size=1048576000
#endif
#if ${process_number} = 5
#cache_dir aufs /cache8/${process_number}   3500000  32 256 min-size=31001
max-size=1048576000
#cache_dir aufs /cache9/${process_number}   3500000  32 256 min-size=31001
max-size=1048576000
#endif


Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/worker-per-cache-dir-tp4671510p4671571.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Jun  6 01:48:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 13:48:25 +1200
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433550412150-4671571.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
 <1433550412150-4671571.post@n4.nabble.com>
Message-ID: <55725169.7090106@treenet.co.nz>

On 6/06/2015 12:26 p.m., Marcel Fossua wrote:
> Hi Amos 
> any hints to help me with this issue 
> I not able to set more than on cache_dir per worker because
> it's alway end up with errors  (but no errors appears if only one cache_dir
> is set per worker)
> 
> 
> 2015/06/06 01:36:09 kid7| Starting eCAP service:
> ecap://www.vigos.com/ecap_gzip
> 2015/06/06 01:36:09 kid7| commBind: Cannot bind socket FD 9 to [::]: (13)
> Permission denied
> 2015/06/06 01:36:09 kid6| commBind: Cannot bind socket FD 21 to [::]: (13)
> Permission denied

These are a Squid process not being able to open /dev/shm UDS sockets.

> 2015/06/06 01:36:09 kid6| Store rebuilding is 0.00% complete
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
> (2) No such file or directory
> 
> Squid Cache (Version 3.5.5): Terminated abnormally.
> CPU Usage: 0.016 seconds = 0.016 user + 0.000 sys
> Maximum Resident Size: 31344 KB
> Page faults with physical i/o: 0
> 2015/06/06 01:36:14 kid6| Store rebuilding is 43.40% complete
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
> (2) No such file or directory
> 
> Squid Cache (Version 3.5.5): Terminated abnormally.
> CPU Usage: 0.016 seconds = 0.016 user + 0.000 sys
> Maximum Resident Size: 31328 KB
> Page faults with physical i/o: 0
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
> (2) No such file or directory
> 
> Squid Cache (Version 3.5.5): Terminated abnormally.
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm):
> (2) No such file or directory
> 

These ones are the result of /dev/shm UDS sockets not existing.

Check the /dev/shm has write permission for everyone. And that Squid is
fully shutdown and restarted (not just reconfigured). And that no other
Squid process is already running - that includes a previous "squid -z".

Amos


From squid3 at treenet.co.nz  Sat Jun  6 01:49:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 13:49:39 +1200
Subject: [squid-users] Utilities for testing question
In-Reply-To: <1433550944.3640.1.camel@JamesiMac>
References: <1433550944.3640.1.camel@JamesiMac>
Message-ID: <557251B3.3000409@treenet.co.nz>

On 6/06/2015 12:35 p.m., James Lay wrote:
> All,
> 
> I'm looking for a command line app like wget or curl that I can use to
> test TLS.  I'm trying to find out how to send a get request without
> sending the SNI.  Any pointers would be appreciated.  Thank you.

The latest squidclient tool built with Squid has basic HTTPS
capabilities and does not send SNI.

Amos



From alex at samad.com.au  Sat Jun  6 06:43:21 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 6 Jun 2015 16:43:21 +1000
Subject: [squid-users] netflix
Message-ID: <CAJ+Q1PXEz5FQV=ZyM+oERQp-x5EqPQc5zPSq+e_t7RBED39D6g@mail.gmail.com>

Hi

I remember seeing some rules for caching microsoft updates.  Is there
anything special to cache netflix ?

Alex


From pugalscout at gmail.com  Sat Jun  6 07:09:52 2015
From: pugalscout at gmail.com (Pugal Scout)
Date: Sat, 6 Jun 2015 00:09:52 -0700
Subject: [squid-users] PAM/Radius authentication question with Squid.3.1.10
Message-ID: <CA+HgQ8sCQBixoD_6mXKxNTZd26T4f8e8BQfnRknhc1A50FwkJw@mail.gmail.com>

Hi,

Apologies if this question has been asked earlier. Is it possible to setup
squid authentication as below:
Use /etc/passwd for user account verification and radius for password?

I have verified radius authentication is successful on the squid server.
auth_param basic program /usr/lib64/squid/squid_radius_auth -f
/etc/radius_config

The radius database contains lot more users then I want to let them in. I
only want to allow users who have local accounts on the squid server in
/etc/passwd and radius for password verification?

I have also verified PAM radius authentication through SSH.  I tried to use
pam_auth, but it worked one time successfully but has not worked after
that. It is popping with login screen but I do not see any traffic reach
the Radius server.

My /etc/pam.d/squid looks like this
auth        include        /lib64/security/pam_auth (I have also tried
pam_radius_auth.so)
account   include        /lib64/security/pam_auth (I have also tried
pam_radius_auth.so)

Appreciate any guidance on how to setup this up. What needs to be
configured on /etc/pam.d/squid? If there are any permissions that needed to
be provided? Appreciate any feedback.

With regards

Sean
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150606/352cfc02/attachment.htm>

From marcel at guineanet.net  Sat Jun  6 13:40:27 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Sat, 6 Jun 2015 06:40:27 -0700 (PDT)
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433351880614-4671510.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
Message-ID: <1433598027313-4671576.post@n4.nabble.com>

ls -la /dev/shm/
total 0
drwxrwxrwt  2 proxy proxy   40 Jun  6 14:56 .
drwxr-xr-x 14 root  root  3920 Jun  6 02:07 ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/worker-per-cache-dir-tp4671510p4671576.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jun  5 22:06:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 06 Jun 2015 10:06:18 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.5 is available
Message-ID: <55721D5A.2070802@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.5 release!


This release is a bug fix and stability release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:


* Regression: comm_connect_addr on failures returns Comm:OK

This regression in IPv6/IPv4 failover was introduced by the fix for bug
4238 in Squid-3.5.4 due to incorrect use of the 'errno' system API.

The Squid use of this system API is now undergoing a full audit. Several
other patches included in this release have come about as a result. and
more will be coming in later releases. The other misuses largely appear
to be resulting in incorrect or confusing debug information (eg. Bug
4236: SSL negotiation error of 'success').


* Bug 3930: assertion 'connIsUsable(http->getConn())'

This bug appears when a perfect storm of conditions occur; Squid with
many asynchronous helpers and/or ICAP adaptation responding slowly, on
high speed networking is running at or near its maximum capacity of
traffic loading.


* Bug 4132: regression in short_icon_urls with global_internal_static

This regression in Squid-3.2 is user visible, but only as an annoyance.
When generating FTP directory listings or HTTPS error messages Squid
would incorrectly respond with an error page indicating the icon was not
available.

It is also related to the cache manager HTTPS access denial issues in
earlier releases. Although fixing this does not fully resolve those issues.


* Bug 4238: assertion Read.cc:205: "params.data == data"

This bug appears when Squid operates with a large number of idle server
connections. Occasionally it has to close them without an active request
signalling closure. Wrong close event sequencing resulted in this
unexpected state assertion.


* Fix missing external ACL helper notes

external ACL helper notes were only added onto the HTTP request that
kicked off the external ACL lookup, and not cached ACL responses.
Configurations that depend on external ACL helper notes during later
processing have not been behaving as expected.


* Multiple stability fixes

Alongside the above major issues a number of other issues including
assertions, incorrect traffic rejections, unnecessary resource
consumption, output messages, and default configuration settings have
been resolved in this release.


* HTTP/2 compatibility

HTTP/2 is now a published RFC standard. This releases documentation is
updated to reflect that and is mentioned in the ChangeLog. However it
should be noted that Squid-3.5 remains HTTP/1.1 software. All it
contains is compatibility logics to detect and properly reject or bypass
HTTP/2 messages.



 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From jlay at slave-tothe-box.net  Sat Jun  6 15:42:13 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 06 Jun 2015 09:42:13 -0600
Subject: [squid-users] Utilities for testing question
In-Reply-To: <557251B3.3000409@treenet.co.nz>
References: <1433550944.3640.1.camel@JamesiMac>
 <557251B3.3000409@treenet.co.nz>
Message-ID: <1433605333.4667.0.camel@JamesiMac>

On Sat, 2015-06-06 at 13:49 +1200, Amos Jeffries wrote:

> On 6/06/2015 12:35 p.m., James Lay wrote:
> > All,
> > 
> > I'm looking for a command line app like wget or curl that I can use to
> > test TLS.  I'm trying to find out how to send a get request without
> > sending the SNI.  Any pointers would be appreciated.  Thank you.
> 
> The latest squidclient tool built with Squid has basic HTTPS
> capabilities and does not send SNI.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Thanks Amos...looks like in order to test intercept I'll need to run it
from a remote machine...luckily I can do that :)  Thanks again.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150606/bcac30ef/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: face-smile.png
Type: image/png
Size: 925 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150606/bcac30ef/attachment.png>

From mcsnv96 at afo.net  Sat Jun  6 16:49:15 2015
From: mcsnv96 at afo.net (Mike)
Date: Sat, 06 Jun 2015 11:49:15 -0500
Subject: [squid-users] Website causing 3.5.5 squid crash
Message-ID: <5573248B.2090401@afo.net>

Running Scientific Linux 6.6 (based on CentOS), compiled squid 3.5.5 
with no errors.
Issue happens with both squid 3.5.5, and the newer 3.5.5-20150528-r13841.
Starting squid service and running for a while, any attempt to access a 
website like www.nwfdailynews.com causes squid to crash and restart. The 
associated squid log entries from the same time squid crashes, seems to 
be several "TCP_SWAPFAIL_MISS" at that time:

1433608644.053    117 192.168.2.110 TCP_SWAPFAIL_MISS/200 17246 GET 
http://launch.newsinc.com/77/css/NdnEmbed.css - 
HIER_DIRECT/184.51.234.134 text/css

Other normal use including facebook and other website do not have any 
problems, so it has to be something relating to this site setup causing 
the crash. We are using this in a testing server for now but hoping to 
roll out to production level used by a few thousand people.

These are the only modified entries in squid:

-----
http_port 3128
# this port is what will be used for SSL Proxy on client browser
http_port 3129 intercept
https_port 3130 intercept ssl-bump connection-auth=off 
generate-host-certificates=on dynamic_cert_mem_cache_size=16MB 
cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key 
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
sslcrtd_children 50 startup=5 idle=1
ssl_bump server-first all
ssl_bump none localhost

#Also for log watching to check for errors, add these lines:

cache_log /var/log/squid/cache.log
cache_effective_user squid
debug_options ALL,0
logfile_rotate 10
cache_mgr myemail at myemail.com
pinger_enable off
visible_hostname A7750

# Uncomment and adjust the following to add a disk cache directory.
cache_dir aufs /var/cache/squid 10000 32 512
-----

Let me know anything else you may need or suggestions.

Mike



From tarotapprentice at yahoo.com  Sun Jun  7 10:26:26 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 7 Jun 2015 10:26:26 +0000 (UTC)
Subject: [squid-users] 3.5.5 Win x64 SquidTray crash
Message-ID: <2004860945.6497750.1433672786321.JavaMail.yahoo@mail.yahoo.com>

Installed 3.5.5 on Server 2008 R2 x64. At login SquidTray crashes and windows gives the following minimal output. I can live without SquidTray anyway but thought I should report it.

I did have 3.5.1 working previously.
MarkJ
-------
Description:
  Stopped working
Problem signature:
  Problem Event Name: CLR20r3
  Problem Signature 01: diladele.squid.tray.exe
  Problem Signature 02: 1.0.0.0
  Problem Signature 03: 55715464
  Problem Signature 04: mscorlib
  Problem Signature 05: 2.0.0.0
  Problem Signature 06: 53a11de1
  Problem Signature 07: 123f
  Problem Signature 08: 5f
  Problem Signature 09: System.IO.FileNotFoundException
  OS Version: 6.1.7601.2.1.0.272.7
  Locale ID: 3081
-----


From tarotapprentice at yahoo.com  Sun Jun  7 10:35:02 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 7 Jun 2015 10:35:02 +0000 (UTC)
Subject: [squid-users] Can files be placed on a RAID volume now?
Message-ID: <1636064617.6544935.1433673302325.JavaMail.yahoo@mail.yahoo.com>

I recall from Squid 2.7 days the recommendation not to put the cache files on a RAID volume under Windows. Does that restriction still apply?

Related does the windows version use the different file system types (ie rock, aufs, ufs) for the disk cache or is it irrelevant under windows.

Cheers,
MarkJ


From jbuda at noticiasargentinas.com  Sun Jun  7 10:57:22 2015
From: jbuda at noticiasargentinas.com (Jose Julian Buda)
Date: Sun, 07 Jun 2015 07:57:22 -0300
Subject: [squid-users] squidGuard configuration test - echo test
Message-ID: <baa5e9d38ddb80790c4e28e03e0509f9@noticiasargentinas.com>


Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an user 
based filter, made the src/dest/acl setting and then test with :

echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" | 
squidGuard -d

..............
2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started 
(1433646524.285)
2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests 
(1433646524.286)
ERR
2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287)
..............


this "someuserfromauth" is supposed to "pass" the filter and get the 
"testsite".

My question is what mean the "ERR" line?

In the squid.conf said that the result field "ERR" from 
url_rewrite_program mean "Do not change the url"
, so it is that what it mean? or it should be "OK" ?

on some mailing list answers said that this lines shoulbe empty for 
passing request.

is it ok with ERR on this line?

There is no way to test with a browser, it is a remote(ssh) 
configuration and i need it on production by monday.

Thank you in advance

Julian


From gkinkie at gmail.com  Sun Jun  7 12:49:09 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 7 Jun 2015 14:49:09 +0200
Subject: [squid-users] Can files be placed on a RAID volume now?
In-Reply-To: <1636064617.6544935.1433673302325.JavaMail.yahoo@mail.yahoo.com>
References: <1636064617.6544935.1433673302325.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+Y8hcNMEzTf04Br_BHaR3y=X0Jd6yek6YP_LpN4KR6Msz4gJQ@mail.gmail.com>

Yes, it still applies; it is a FAQ.
See http://wiki.squid-cache.org/SquidFaq/RAID


On Sun, Jun 7, 2015 at 12:35 PM, TarotApprentice
<tarotapprentice at yahoo.com> wrote:
> I recall from Squid 2.7 days the recommendation not to put the cache files on a RAID volume under Windows. Does that restriction still apply?
>
> Related does the windows version use the different file system types (ie rock, aufs, ufs) for the disk cache or is it irrelevant under windows.
>
> Cheers,
> MarkJ
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From marcus.kool at urlfilterdb.com  Sun Jun  7 13:24:37 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sun, 07 Jun 2015 10:24:37 -0300
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <baa5e9d38ddb80790c4e28e03e0509f9@noticiasargentinas.com>
References: <baa5e9d38ddb80790c4e28e03e0509f9@noticiasargentinas.com>
Message-ID: <55744615.2050004@urlfilterdb.com>

The URL director interface was changed with Squid 3.4, see also http://wiki.squid-cache.org/Features/Redirectors
The latest version of squidguard is 1.5 beta from 2010 and squidGuard does not support the new interface of Squid.

ufdbGuard is also a URL redirector and since it has regular updates, ufdbGuard is compatible with the new URL redirector interface of Squid.
ufdbGuard is free software with a GPL2 license, 99% compatible with squidGuard, 3x faster, has a lower memory footprint and maintained by the Dutch company URLfilterDB B.V.
So you have every reason to switch from squiddGuard to ufdbGuard.

Marcus


On 06/07/2015 07:57 AM, Jose Julian Buda wrote:
>
> Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an user based filter, made the src/dest/acl setting and then test with :
>
> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" | squidGuard -d
>
> ..............
> 2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started (1433646524.285)
> 2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests (1433646524.286)
> ERR
> 2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287)
> ..............
>
>
> this "someuserfromauth" is supposed to "pass" the filter and get the "testsite".
>
> My question is what mean the "ERR" line?
>
> In the squid.conf said that the result field "ERR" from url_rewrite_program mean "Do not change the url"
> , so it is that what it mean? or it should be "OK" ?
>
> on some mailing list answers said that this lines shoulbe empty for passing request.
>
> is it ok with ERR on this line?
>
> There is no way to test with a browser, it is a remote(ssh) configuration and i need it on production by monday.
>
> Thank you in advance
>
> Julian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jbuda at noticiasargentinas.com  Sun Jun  7 13:50:21 2015
From: jbuda at noticiasargentinas.com (Jose Julian Buda)
Date: Sun, 07 Jun 2015 10:50:21 -0300
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <55744615.2050004@urlfilterdb.com>
References: <baa5e9d38ddb80790c4e28e03e0509f9@noticiasargentinas.com>
 <55744615.2050004@urlfilterdb.com>
Message-ID: <931fac61f85e362430fea5103156eca0@noticiasargentinas.com>

thank Marcus for the answer

but maybe at this time squidguard does work on debian jessie :

https://packages.debian.org/jessie/squidguard

http://metadata.ftp-master.debian.org/changelogs/main/s/squidguard/stable_changelog

http://metadata.ftp-master.debian.org/changelogs/main/s/squidguard/squidguard_1.5-4_squidguard.README.Debian


for another hand, ufdbGuard is not on the debian repository

my question is what mean this lines when i run the echo test.

.....
2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started 
(1433646524.285)
2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests 
(1433646524.286)
ERR
2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287)
.....

as i can see in squid.conf, squid wait for this kind of answer("ERR") 
from the redirector


but if it does'nt work anyway, is there another squid 
url_rewrite_program supported by debian ?

thank you in advance

Julian



On 2015-06-07 10:24, Marcus Kool wrote:
> The URL director interface was changed with Squid 3.4, see also
> http://wiki.squid-cache.org/Features/Redirectors
> The latest version of squidguard is 1.5 beta from 2010 and squidGuard
> does not support the new interface of Squid.
>
> ufdbGuard is also a URL redirector and since it has regular updates,
> ufdbGuard is compatible with the new URL redirector interface of
> Squid.
> ufdbGuard is free software with a GPL2 license, 99% compatible with
> squidGuard, 3x faster, has a lower memory footprint and maintained by
> the Dutch company URLfilterDB B.V.
> So you have every reason to switch from squiddGuard to ufdbGuard.
>
> Marcus
>
>
> On 06/07/2015 07:57 AM, Jose Julian Buda wrote:
>>
>> Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an 
>> user based filter, made the src/dest/acl setting and then test with :
>>
>> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" | 
>> squidGuard -d
>>
>> ..............
>> 2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started 
>> (1433646524.285)
>> 2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests 
>> (1433646524.286)
>> ERR
>> 2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287)
>> ..............
>>
>>
>> this "someuserfromauth" is supposed to "pass" the filter and get the 
>> "testsite".
>>
>> My question is what mean the "ERR" line?
>>
>> In the squid.conf said that the result field "ERR" from 
>> url_rewrite_program mean "Do not change the url"
>> , so it is that what it mean? or it should be "OK" ?
>>
>> on some mailing list answers said that this lines shoulbe empty for 
>> passing request.
>>
>> is it ok with ERR on this line?
>>
>> There is no way to test with a browser, it is a remote(ssh) 
>> configuration and i need it on production by monday.
>>
>> Thank you in advance
>>
>> Julian
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From Hullen at t-online.de  Sun Jun  7 15:15:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Sun, 7 Jun 2015 17:15:00 +0200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <55744615.2050004@urlfilterdb.com>
Message-ID: <DIOWb+puCXB@helmut.hullen.de>

Hallo, Marcus,

Du meintest am 07.06.15:

>> Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an
>> user based filter, made the src/dest/acl setting and then test with
>> :
>>
>> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" |
>> squidGuard -d

> The URL director interface was changed with Squid 3.4, see also
> http://wiki.squid-cache.org/Features/RedirectorsThe latest version of
> squidguard is 1.5 beta from 2010 and squidGuard does not support the
> new interface of Squid.

Sure?
I run squid-3.4.10 and squidGuard-1.5beta on many machines, without  
having changed the "Redirector" line in "/etc/squid/squid.conf".

Viele Gruesse!
Helmut



From jbuda at noticiasargentinas.com  Sun Jun  7 17:20:03 2015
From: jbuda at noticiasargentinas.com (Jose Julian Buda)
Date: Sun, 07 Jun 2015 14:20:03 -0300
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <DIOWb+puCXB@helmut.hullen.de>
References: <DIOWb+puCXB@helmut.hullen.de>
Message-ID: <90c9c4bcce33560cb6f7d51f5fc31a02@noticiasargentinas.com>

On 2015-06-07 12:15, Helmut Hullen wrote:
> Hallo, Marcus,
>
> Du meintest am 07.06.15:
>
>>> Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an
>>> user based filter, made the src/dest/acl setting and then test with
>>> :
>>>
>>> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" |
>>> squidGuard -d
>
>> The URL director interface was changed with Squid 3.4, see also
>> http://wiki.squid-cache.org/Features/RedirectorsThe latest version 
>> of
>> squidguard is 1.5 beta from 2010 and squidGuard does not support the
>> new interface of Squid.
>
> Sure?
> I run squid-3.4.10 and squidGuard-1.5beta on many machines, without
> having changed the "Redirector" line in "/etc/squid/squid.conf".
>
> Viele Gruesse!
> Helmut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



Thank you Helmut for this information.
I guess squidGuard 1.5.4 should work on DebianStable's Squid3.4
So anything about the "ERR" answer for this test?

echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" | 
squidGuard -d

...
2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started 
(1433646524.285)
2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests 
(1433646524.286)
ERR
2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287
...

is this what is expected for a passing request?
Can anyone do me a favor, do this and tell me what see?


Thank you very much in advance
Julian


From tarotapprentice at yahoo.com  Mon Jun  8 03:25:59 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Mon, 8 Jun 2015 03:25:59 +0000 (UTC)
Subject: [squid-users] Fw: 3.5.5 Win x64 SquidTray crash
In-Reply-To: <2004860945.6497750.1433672786321.JavaMail.yahoo@mail.yahoo.com>
References: <2004860945.6497750.1433672786321.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <311445993.6840076.1433733959111.JavaMail.yahoo@mail.yahoo.com>

Reinstalled 3.5.1 and it too had the same problem.

Also on Server2008 it has what Microsoft call the "advanced firewall" which seems to block inbound to the machine so I had to adjust the firewall rules even though the installer had added a rule.

MarkJ
 
----- Forwarded Message -----
> From: TarotApprentice <tarotapprentice at yahoo.com>
> To: Squid-users <squid-users at lists.squid-cache.org>
> Cc: 
> Sent: Sunday, 7 June 2015, 20:26
> Subject: 3.5.5 Win x64 SquidTray crash
> 
> Installed 3.5.5 on Server 2008 R2 x64. At login SquidTray crashes and windows 
> gives the following minimal output. I can live without SquidTray anyway but 
> thought I should report it.
> 
> I did have 3.5.1 working previously.
> MarkJ
> -------
> Description:
>   Stopped working
> Problem signature:
>   Problem Event Name: CLR20r3
>   Problem Signature 01: diladele.squid.tray.exe
>   Problem Signature 02: 1.0.0.0
>   Problem Signature 03: 55715464
>   Problem Signature 04: mscorlib
>   Problem Signature 05: 2.0.0.0
>   Problem Signature 06: 53a11de1
>   Problem Signature 07: 123f
>   Problem Signature 08: 5f
>   Problem Signature 09: System.IO.FileNotFoundException
>   OS Version: 6.1.7601.2.1.0.272.7
>   Locale ID: 3081
> -----
>  


From Jason_Haar at trimble.com  Mon Jun  8 03:45:12 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 8 Jun 2015 15:45:12 +1200
Subject: [squid-users] Fw: 3.5.5 Win x64 SquidTray crash
In-Reply-To: <311445993.6840076.1433733959111.JavaMail.yahoo@mail.yahoo.com>
References: <2004860945.6497750.1433672786321.JavaMail.yahoo@mail.yahoo.com>
 <311445993.6840076.1433733959111.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55750FC8.9000106@trimble.com>

On 08/06/15 15:25, TarotApprentice wrote:
> Reinstalled 3.5.1 and it too had the same problem.
>
> Also on Server2008 it has what Microsoft call the "advanced firewall" which seems to block inbound to the machine so I had to adjust the firewall rules even though the installer had added a rule.

Yeah - windows firewall is a major pain. Better to turn the darn thing
off and rely on something else


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From tarotapprentice at yahoo.com  Mon Jun  8 06:07:06 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Mon, 8 Jun 2015 06:07:06 +0000 (UTC)
Subject: [squid-users] Squid 3.5.5 on Win doesn't release files after rotate
Message-ID: <596909412.6953151.1433743626329.JavaMail.yahoo@mail.yahoo.com>

Back to Squid 3.5.5 on Server 2008 x64.

Had an access.log and cache.log in /squid/var/log/squid. After doing a squid -k rotate it successfully created new ones and old ones became access.log.0 and cache.log.0 as expected. However cannot open cache.log.0 as its still in use. access.log.0 can be opened.

Oh and can we get carriage return and line feeds in the cache.log please. Access.log has them.

Cheers,
MarkJ


From squid3 at treenet.co.nz  Mon Jun  8 07:18:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 19:18:02 +1200
Subject: [squid-users] Squid 3.5.5 on Win doesn't release files after
 rotate
In-Reply-To: <596909412.6953151.1433743626329.JavaMail.yahoo@mail.yahoo.com>
References: <596909412.6953151.1433743626329.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <557541AA.8060304@treenet.co.nz>

On 8/06/2015 6:07 p.m., TarotApprentice wrote:
> Back to Squid 3.5.5 on Server 2008 x64.
> 
> Had an access.log and cache.log in /squid/var/log/squid. After doing a squid -k rotate it successfully created new ones and old ones became access.log.0 and cache.log.0 as expected. However cannot open cache.log.0 as its still in use. access.log.0 can be opened.
> 

Do you still have helpers shutting down?
 Squid should we waiting for the last of the pre-rotate ones to finish
up their pending requests and exist before the cache.log becomes available.

> Oh and can we get carriage return and line feeds in the cache.log please. Access.log has them.
> 

That is not under our control. The compiler decides what characters to
output for '\n'.

Amos



From squid3 at treenet.co.nz  Mon Jun  8 07:39:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 19:39:08 +1200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <90c9c4bcce33560cb6f7d51f5fc31a02@noticiasargentinas.com>
References: <DIOWb+puCXB@helmut.hullen.de>
 <90c9c4bcce33560cb6f7d51f5fc31a02@noticiasargentinas.com>
Message-ID: <5575469C.7090900@treenet.co.nz>

On 8/06/2015 5:20 a.m., Jose Julian Buda wrote:
> On 2015-06-07 12:15, Helmut Hullen wrote:
>> Hallo, Marcus,
>>
>> Du meintest am 07.06.15:
>>
>>>> Hi, i have installed squidGuard 1.5 on Debian Jessie and i need an
>>>> user based filter, made the src/dest/acl setting and then test with
>>>> :
>>>>
>>>> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" |
>>>> squidGuard -d
>>
>>> The URL director interface was changed with Squid 3.4, see also
>>> http://wiki.squid-cache.org/Features/RedirectorsThe latest version of
>>> squidguard is 1.5 beta from 2010 and squidGuard does not support the
>>> new interface of Squid.
>>
>> Sure?
>> I run squid-3.4.10 and squidGuard-1.5beta on many machines, without
>> having changed the "Redirector" line in "/etc/squid/squid.conf".
>>
> 
> 
> Thank you Helmut for this information.
> I guess squidGuard 1.5.4 should work on DebianStable's Squid3.4
> So anything about the "ERR" answer for this test?

That wiki page Marcus references you to documents it in detail.


> 
> echo "http://www.testsite.com 192.168.0.82/ someuserfromauth GET" |
> squidGuard -d
> 
> ...
> 2015-06-07 00:08:44 [3359] INFO: squidGuard 1.5 started (1433646524.285)
> 2015-06-07 00:08:44 [3359] INFO: squidGuard ready for requests
> (1433646524.286)
> ERR
> 2015-06-07 00:08:44 [3359] INFO: squidGuard stopped (1433646524.287
> ...
> 
> is this what is expected for a passing request?

>From that wiki page:
"
result
  One of the result codes:

    OK - Success. A new URL is presented.

    ERR - Success. No action for this URL.

    BH - Failure. The helper encountered a problem.

  {i} the result field is only accepted by Squid-3.4 and newer.
"

In other words, SG is working and telling Squid/you not to do anything
with the URL you gave it.

If you have rules setup that should be making it an OK with URL output
its probably because of ... the URL given to SG is not valid (missing
path), also the FQDN, myip=, and myport= fields are missing.

Amos


From squid3 at treenet.co.nz  Mon Jun  8 07:43:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 19:43:26 +1200
Subject: [squid-users] PAM/Radius authentication question with
	Squid.3.1.10
In-Reply-To: <CA+HgQ8sCQBixoD_6mXKxNTZd26T4f8e8BQfnRknhc1A50FwkJw@mail.gmail.com>
References: <CA+HgQ8sCQBixoD_6mXKxNTZd26T4f8e8BQfnRknhc1A50FwkJw@mail.gmail.com>
Message-ID: <5575479E.5060002@treenet.co.nz>

On 6/06/2015 7:09 p.m., Pugal Scout wrote:
> Hi,
> 
> Apologies if this question has been asked earlier. Is it possible to setup
> squid authentication as below:
> Use /etc/passwd for user account verification and radius for password?

The provided auth helpers for Squid can check against *one* backend type
for each auth scheme. Anything more compicated requires a custom written
auth helper. Although technically some of the helpers can auto-detect or
be configured for multiple backends of the same type.

> 
> I have verified radius authentication is successful on the squid server.
> auth_param basic program /usr/lib64/squid/squid_radius_auth -f
> /etc/radius_config
> 
> The radius database contains lot more users then I want to let them in. I
> only want to allow users who have local accounts on the squid server in
> /etc/passwd and radius for password verification?

The usual way to do this is by using "groups" of users, one group having
access permission to use the proxy.

See the external ACL helpers for options about looking up user group(s).

Amos



From squid3 at treenet.co.nz  Mon Jun  8 07:55:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 19:55:40 +1200
Subject: [squid-users] Website causing 3.5.5 squid crash
In-Reply-To: <5573248B.2090401@afo.net>
References: <5573248B.2090401@afo.net>
Message-ID: <55754A7C.80103@treenet.co.nz>

On 7/06/2015 4:49 a.m., Mike wrote:
> Running Scientific Linux 6.6 (based on CentOS), compiled squid 3.5.5
> with no errors.
> Issue happens with both squid 3.5.5, and the newer 3.5.5-20150528-r13841.
> Starting squid service and running for a while, any attempt to access a
> website like www.nwfdailynews.com causes squid to crash and restart. The
> associated squid log entries from the same time squid crashes, seems to
> be several "TCP_SWAPFAIL_MISS" at that time:
> 
> 1433608644.053    117 192.168.2.110 TCP_SWAPFAIL_MISS/200 17246 GET
> http://launch.newsinc.com/77/css/NdnEmbed.css -
> HIER_DIRECT/184.51.234.134 text/css
> 

SWAPFAIL is either a disk I/O error, or the file has been erased from
disk without going through Squid making the cache index entry pointing
nowhere.

>From a service point of view thay are nearly harmless, though possibly a
sign that you need to check the HDD integrity.


> Other normal use including facebook and other website do not have any
> problems, so it has to be something relating to this site setup causing
> the crash. We are using this in a testing server for now but hoping to
> roll out to production level used by a few thousand people.
> 

That domain has bigger problems than what you see.

redbot.org shows its origin server is not generating Vary headers
properly. Which means visitors to the website that go through a proxy
(any HTTP proxy) will be getting randomly broken responses.

Furthermore, it appears to be served up to the public through a CDN
built from Squid proxies. So that is affecting *every* visitor.


Amos



From squid3 at treenet.co.nz  Mon Jun  8 08:04:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 20:04:40 +1200
Subject: [squid-users] netflix
In-Reply-To: <CAJ+Q1PXEz5FQV=ZyM+oERQp-x5EqPQc5zPSq+e_t7RBED39D6g@mail.gmail.com>
References: <CAJ+Q1PXEz5FQV=ZyM+oERQp-x5EqPQc5zPSq+e_t7RBED39D6g@mail.gmail.com>
Message-ID: <55754C98.2060100@treenet.co.nz>

On 6/06/2015 6:43 p.m., Alex Samad wrote:
> Hi
> 
> I remember seeing some rules for caching microsoft updates.  Is there
> anything special to cache netflix ?

Last I heard netflix were using RTMP (not HTTP) to deliver their video
streams and the web portions were setup to cache properly.

Caching netflix has not had much of a peep from anyone in a long time.
So I assume its still the case or if they moved to HTTP the videos are
properly done.


FYI: the issue with Microsoft Updates is Squid lack of Range/206 object
caching, the traffic itself is perfectly cacheable once you get around that.
 YouTube/GoogleVideo is actively hostile against HTTP caching. Flash
based services were cloned off it which sadly gives video caching in
general a needlessly bad reputation.

Amos



From squid3 at treenet.co.nz  Mon Jun  8 08:30:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 20:30:33 +1200
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
 <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
Message-ID: <557552A9.4050603@treenet.co.nz>

On 8/06/2015 5:36 a.m., snakeeyes wrote:
> Hi Amos thank you so much 
> Again , this App IS REMOTE and as a black box .
> It works 100  % on other proxies but I need to let it work on my own proxy.
> 
> Now what I did is :
> I added 2  directives to my squid.conf :
> 
> strip_query_terms off
> debug_options 11,2 28,3
> 
> 
> then restarted squid and monitored the logs .
> 
> again I have 2 files monitored when it worked and when it failed.
> 
> But the strange that the problem is it work on some youtube vidoes and don?t work on the others
> 
> those files were just as an example :
> file 1 didn?t work and give error 403 ===> https://www.youtube.com/watch?v=MYSVMgRr6pw
> 
> and other file worked ===>
> https://www.youtube.com/watch?v=p0g9_osImd0
> 
> now I will test the app with those links , note that that 1sst line will fail and  the 2nd Link will success.
> 
> I had graped the log files andf attached them because they are big
> Name for failed file is =>debug_failed.txt
> Name for succeded file ==>debug_worked.txt
> 
> Thanks a lot
> 
> cheers

Your Squid is letting all of both types of traffic through, and it
appears not to be caching the results.

That is good in a way. It means the problem is clearly something between
the browser and Google servers, not Squid.

I can see several differences between the client requests. The working
ones have a line or so more query parameters than the forbidden ones.
They are also consistently going to a different server (working *.12 ,
failing *.14).

Amos



From squid3 at treenet.co.nz  Mon Jun  8 08:31:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 20:31:50 +1200
Subject: [squid-users] worker per cache_dir
In-Reply-To: <1433598027313-4671576.post@n4.nabble.com>
References: <1433351880614-4671510.post@n4.nabble.com>
 <1433598027313-4671576.post@n4.nabble.com>
Message-ID: <557552F6.9080300@treenet.co.nz>

On 7/06/2015 1:40 a.m., Marcel Fossua wrote:
> ls -la /dev/shm/
> total 0
> drwxrwxrwt  2 proxy proxy   40 Jun  6 14:56 .
> drwxr-xr-x 14 root  root  3920 Jun  6 02:07 ..
> 

That should be root:root I believe. Its an operating system component.

Though I'm not quite as certain that it will work any different once you
fix that. Maybe, maybe not.

Amos


From Hullen at t-online.de  Mon Jun  8 08:38:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 8 Jun 2015 10:38:00 +0200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <5575469C.7090900@treenet.co.nz>
Message-ID: <DISXZCpPCXB@helmut.hullen.de>

Hallo, Amos,

Du meintest am 08.06.15:

>>>> The URL director interface was changed with Squid 3.4, see also
>>>> http://wiki.squid-cache.org/Features/Redirectors

>>>> The latest version of squidguard is 1.5 beta from 2010 and
>>>> squidGuard does not support the new interface of Squid.

[...]

> That wiki page Marcus references you to documents it in detail.

Sorry - I'd like to get some help.

Under squid 3.4 (and many earlier versions) I use

        url_rewrite_program /usr/bin/squidGuard

How must I change this line for squid 3.5?

The above page mentions

   http://www.eu.squid-cache.org/Doc/config/url_rewrite_extras

but  this page doesn't yet exist.

Same problem with

   http://www.eu.squid-cache.org/Doc/config/url_rewrite_program

Viele Gruesse!
Helmut



From squid3 at treenet.co.nz  Mon Jun  8 08:59:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 08 Jun 2015 20:59:20 +1200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <DISXZCpPCXB@helmut.hullen.de>
References: <DISXZCpPCXB@helmut.hullen.de>
Message-ID: <55755968.8060204@treenet.co.nz>

On 8/06/2015 8:38 p.m., Helmut Hullen wrote:
> Hallo, Amos,
> 
> Du meintest am 08.06.15:
> 
>>>>> The URL director interface was changed with Squid 3.4, see also
>>>>> http://wiki.squid-cache.org/Features/Redirectors
> 
>>>>> The latest version of squidguard is 1.5 beta from 2010 and
>>>>> squidGuard does not support the new interface of Squid.
> 
> [...]
> 
>> That wiki page Marcus references you to documents it in detail.
> 
> Sorry - I'd like to get some help.
> 
> Under squid 3.4 (and many earlier versions) I use
> 
>         url_rewrite_program /usr/bin/squidGuard
> 
> How must I change this line for squid 3.5?
> 

You should not have to change the SG command line or configuration.

Whats needed is a patch from
<http://bugs.squid-cache.org/show_bug.cgi?id=3978> to be applied to SG
itself. If you are using an OS provided SG binary check to see if they
have already patched it.


> The above page mentions
> 
>    http://www.eu.squid-cache.org/Doc/config/url_rewrite_extras
> 
> but  this page doesn't yet exist.
> 
> Same problem with
> 
>    http://www.eu.squid-cache.org/Doc/config/url_rewrite_program
> 

That should be:

  <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>
and
  <http://www.squid-cache.org/Doc/config/url_rewrite_program/>


Thanks for pointing this out. I've fixed the wiki auto-linking now.


Amos


From ahmed.zaeem at netstream.ps  Mon Jun  8 19:29:55 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Mon, 8 Jun 2015 12:29:55 -0700
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <557552A9.4050603@treenet.co.nz>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
 <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
 <557552A9.4050603@treenet.co.nz>
Message-ID: <006601d0a221$804cc640$80e652c0$@netstream.ps>

Hi Amos thanks for explanation

But the issue is it works fine from other paid proxies .

Again , the app is a link converter and it connect to proxy 
I will give u sample how app works :

It request link as below:
http://convertlink-bla-bla-bla.com/ytd/Youtube.class.php?url=https://www.youtube.com/watch?v=MYSVMgRr6pw&proxy=xxxx:yyyy


so the xxx:yyyy is the proxy ip:port

again 	Amos , since its outside squid and it works on other proxies , do u think we can do anything on our squid ?

any other ideas??

cheers

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Monday, June 8, 2015 1:31 AM
To: snakeeyes; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP_MISS/403 353 HEAD text/plain Error help !!

On 8/06/2015 5:36 a.m., snakeeyes wrote:
> Hi Amos thank you so much
> Again , this App IS REMOTE and as a black box .
> It works 100  % on other proxies but I need to let it work on my own proxy.
> 
> Now what I did is :
> I added 2  directives to my squid.conf :
> 
> strip_query_terms off
> debug_options 11,2 28,3
> 
> 
> then restarted squid and monitored the logs .
> 
> again I have 2 files monitored when it worked and when it failed.
> 
> But the strange that the problem is it work on some youtube vidoes and 
> don?t work on the others
> 
> those files were just as an example :
> file 1 didn?t work and give error 403 ===> 
> https://www.youtube.com/watch?v=MYSVMgRr6pw
> 
> and other file worked ===>
> https://www.youtube.com/watch?v=p0g9_osImd0
> 
> now I will test the app with those links , note that that 1sst line will fail and  the 2nd Link will success.
> 
> I had graped the log files andf attached them because they are big 
> Name for failed file is =>debug_failed.txt Name for succeded file 
> ==>debug_worked.txt
> 
> Thanks a lot
> 
> cheers

Your Squid is letting all of both types of traffic through, and it appears not to be caching the results.

That is good in a way. It means the problem is clearly something between the browser and Google servers, not Squid.

I can see several differences between the client requests. The working ones have a line or so more query parameters than the forbidden ones.
They are also consistently going to a different server (working *.12 , failing *.14).

Amos



From vdoctor at neuf.fr  Mon Jun  8 09:20:51 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 8 Jun 2015 02:20:51 -0700 (PDT)
Subject: [squid-users] Squid cache_peer in tproxy
Message-ID: <1433755251731-4671602.post@n4.nabble.com>

Hi All,

We're facing a weird issue with the cache_peer and tproxy.

Squid 3.5.4
users -> squid1 -> squid2/squid3 -> internet

squid1:
http_port 3128
http_port 3129 tproxy
icp_port 3130
cache_peer 192.168.1.2 parent 3128 3130  proxy-only weighted-round-robin
background-ping no-digest 
cache_peer 192.168.1.3 parent 3128 3130  proxy-only weighted-round-robin
background-ping no-digest 

squid2:
http_port 3128
icp_port 3130
cache_peer 192.168.1.3 sibling 3128 3130 proxy-only no-digest

squid3:
http_port 3128
icp_port 3130
cache_peer 192.168.1.2 sibling 3128 3130 proxy-only no-digest

The user IPs are corrects at the squid1 but we lost them at the
squid2/squid3, here we see the squid1 ip only.
Where are we wrong ?
Thanks for your inputs 

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-cache-peer-in-tproxy-tp4671602.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From wrkilu at wp.pl  Mon Jun  8 10:53:00 2015
From: wrkilu at wp.pl (Robert Lasota)
Date: Mon, 08 Jun 2015 12:53:00 +0200
Subject: [squid-users] Squid doesn't write logs via rsyslog
Message-ID: <5575740ca94bd1.78241436@wp.pl>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/cc51b9f0/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jun  8 11:02:07 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 8 Jun 2015 13:02:07 +0200
Subject: [squid-users] Squid doesn't write logs via rsyslog
In-Reply-To: <5575740ca94bd1.78241436@wp.pl>
References: <5575740ca94bd1.78241436@wp.pl>
Message-ID: <201506081302.07944.Antony.Stone@squid.open.source.it>

On Monday 08 June 2015 at 12:53:00 (EU time), Robert Lasota wrote:

> the problem is it still writes logs to files /var/log/access.log or
> /opt/var/log/access.log (depends what I set in conf) but never to rsyslog.
>
> I mean, I have set rsyslog to it send logs to remote central server, and
> from other apps like sshd or named its working and rsyslog send them , but
> Squid still not care that and writes locally to files.
>
> I set different combinations in squid.conf but nothing, even:
> access_log syslog squid
> cache_log syslog squid.
> ..also nothing

You appear to be missing the facility and priority settings (ie: telling 
syslogd how to handle the messages).

See http://www.squid-cache.org/Doc/config/access_log/

Try something such as:

access_log syslog:daemon.info


Regards,


Antony.

-- 
I don't know, maybe if we all waited then cosmic rays would write all our 
software for us. Of course it might take a while.

 - Ron Minnich, Los Alamos National Laboratory

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Hullen at t-online.de  Mon Jun  8 11:10:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 8 Jun 2015 13:10:00 +0200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <55755968.8060204@treenet.co.nz>
Message-ID: <DISXrzjeCXB@helmut.hullen.de>

Hallo, Amos,

Du meintest am 08.06.15:

>> Under squid 3.4 (and many earlier versions) I use
>>
>>         url_rewrite_program /usr/bin/squidGuard
>>
>> How must I change this line for squid 3.5?

> You should not have to change the SG command line or configuration.

Ok!

> Whats needed is a patch from
> <http://bugs.squid-cache.org/show_bug.cgi?id=3978> to be applied to
> SGitself. If you are using an OS provided SG binary check to see if
> theyhave already patched it.

It's not patched in my version, but it works under squid 3.4.10 -  
strange.

>> The above page mentions
>>
>>    http://www.eu.squid-cache.org/Doc/config/url_rewrite_extras
>>
>> but  this page doesn't yet exist.

[...]

> That should be:

>   <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>
> and
>   <http://www.squid-cache.org/Doc/config/url_rewrite_program/>

Ok - now I can read the pages!

Viele Gruesse!
Helmut



From Hullen at t-online.de  Mon Jun  8 11:20:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 8 Jun 2015 13:20:00 +0200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <55744615.2050004@urlfilterdb.com>
Message-ID: <DISXs8N9CXB@helmut.hullen.de>

Hallo, Marcus,

Du meintest am 07.06.15:

> The URL director interface was changed with Squid 3.4, see also
> http://wiki.squid-cache.org/Features/RedirectorsThe latest version of
> squidguard is 1.5 beta from 2010 and squidGuard does not support the
> new interface of Squid.

> ufdbGuard is also a URL redirector and since it has regular updates,
> ufdbGuard is compatible with the new URL redirector interface of
> Squid.ufdbGuard is free software with a GPL2 license,

But what is the meaning of


   https://www.urlfilterdb.com/downloads/trialregistration.html

and

   https://www.urlfilterdb.com/pricing/licenses.html

I don't find there that using "ufdbguard" without having payed a  
"licence fee" is legal use.

And the users of the "c't/ODS-Schulserver"

     http://de.wikipedia.org/wiki/Arktur-Schulserver

which I now maintain also may ask this question ...

Viele Gruesse!
Helmut



From pkryon at gmail.com  Mon Jun  8 12:51:06 2015
From: pkryon at gmail.com (Patrick)
Date: Mon, 8 Jun 2015 08:51:06 -0400
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <DISXs8N9CXB@helmut.hullen.de>
References: <55744615.2050004@urlfilterdb.com> <DISXs8N9CXB@helmut.hullen.de>
Message-ID: <CAJgbPpewP+ONEV-Ri7QBSG3ZUq03oMJRd_93QKHQBaDoVTn-wg@mail.gmail.com>

We switched to ufdbGuard recently and were a bit confused on that point as
well.  But see: https://www.urlfilterdb.com/products/ufdbguard.html
"ufdbGuard is copyright-protected, but free for use.  Also in commercial
environments."

The pricing page and licensing model you have linked is only for their
blacklist database, "URLfilterDB", which is subscription based (and worth
considering).  But you're free to use the ufdbGuard software with other
blacklists.  It imports and works flat file based blacklists like Shalla
very well.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/6a9c17cd/attachment.htm>

From wrkilu at wp.pl  Mon Jun  8 14:22:33 2015
From: wrkilu at wp.pl (Robert Lasota)
Date: Mon, 08 Jun 2015 16:22:33 +0200
Subject: [squid-users] Odp: Re:  Squid doesn't write logs via rsyslog
Message-ID: <5575a52966e927.42375951@wp.pl>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/fb42ef37/attachment.htm>

From jonathan.filogna at tasso.com.ar  Mon Jun  8 14:37:51 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Mon, 08 Jun 2015 11:37:51 -0300
Subject: [squid-users] Block whatsapp with transparent proxy
Message-ID: <5575A8BF.8050603@tasso.com.ar>

Hi all, greetings from Argentina

I want to know if can be possible block whatsapp for mobiles with a 
transparent proxy

which squid version should i install?

thank you all


From yvoinov at gmail.com  Mon Jun  8 15:32:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 08 Jun 2015 21:32:15 +0600
Subject: [squid-users] Block whatsapp with transparent proxy
In-Reply-To: <5575A8BF.8050603@tasso.com.ar>
References: <5575A8BF.8050603@tasso.com.ar>
Message-ID: <5575B57F.3080306@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
https://www.google.com/search?q=squid+watsup+blocking

Feel free to Google your question first.

08.06.15 20:37, Jonathan Filogna ?????:
> Hi all, greetings from Argentina
>
> I want to know if can be possible block whatsapp for mobiles with a
transparent proxy
>
> which squid version should i install?
>
> thank you all
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVdbV+AAoJENNXIZxhPexG1W4H/jFxKUzoH7ILo8/PztVijrGT
EJTb3bwuIh6bfuDwJZ04O7yro4hw1n26B+1ARKgiIBNlQa0oys1pCklsbFlQ6KbV
VtCbU5xTLw9nqBVLQzv1lqlQ7Sz3KNzElissAIOysQnxsoJ5lxkcMNtXD2Y7uU5o
NZ8lVs9QKAK3PqCDnaPc4bJLuLiwPWcjNLg6xrnGYb/Fjb2dGr3tA1CvjxjuCa5V
/NXffgKXYKOHpEbt/995F10eSmIP6ycUqOl0ea7g3KwJXG9gUEX5eqWKRH/jG5H/
EZDzt/UzzgNPz30ywgrn7jUxjBOGJ5fgaVQNky1k5E+mNoR++VZCQN8wMbYL8LQ=
=TJqN
-----END PGP SIGNATURE-----



From pugalscout at gmail.com  Mon Jun  8 15:53:01 2015
From: pugalscout at gmail.com (Pugal Scout)
Date: Mon, 8 Jun 2015 08:53:01 -0700
Subject: [squid-users] PAM/Radius authentication question with
	Squid.3.1.10
In-Reply-To: <5575479E.5060002@treenet.co.nz>
References: <CA+HgQ8sCQBixoD_6mXKxNTZd26T4f8e8BQfnRknhc1A50FwkJw@mail.gmail.com>
 <5575479E.5060002@treenet.co.nz>
Message-ID: <CA+HgQ8vHE7_yrP+iWuo=T6x9vqZ=SF4voEfxo2F8GDtdoeXN2A@mail.gmail.com>

Thank you Amos. That is helpful, user groups seems to be a good option. I
will try that.

-JP

On Mon, Jun 8, 2015 at 12:43 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/06/2015 7:09 p.m., Pugal Scout wrote:
> > Hi,
> >
> > Apologies if this question has been asked earlier. Is it possible to
> setup
> > squid authentication as below:
> > Use /etc/passwd for user account verification and radius for password?
>
> The provided auth helpers for Squid can check against *one* backend type
> for each auth scheme. Anything more compicated requires a custom written
> auth helper. Although technically some of the helpers can auto-detect or
> be configured for multiple backends of the same type.
>
> >
> > I have verified radius authentication is successful on the squid server.
> > auth_param basic program /usr/lib64/squid/squid_radius_auth -f
> > /etc/radius_config
> >
> > The radius database contains lot more users then I want to let them in. I
> > only want to allow users who have local accounts on the squid server in
> > /etc/passwd and radius for password verification?
>
> The usual way to do this is by using "groups" of users, one group having
> access permission to use the proxy.
>
> See the external ACL helpers for options about looking up user group(s).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/506e916b/attachment.htm>

From marcus.kool at urlfilterdb.com  Mon Jun  8 17:18:47 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 08 Jun 2015 14:18:47 -0300
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <DISXs8N9CXB@helmut.hullen.de>
References: <DISXs8N9CXB@helmut.hullen.de>
Message-ID: <5575CE77.2080106@urlfilterdb.com>

Helmut,

you can download ufdbGuard here:
    https://www.urlfilterdb.com/downloads/software_doc.html
and here:
    http://sourceforge.net/projects/ufdbguard/

ufdbGuard is just like Squid free Open Source Software.

The trial license on www.urlfilterdb.com is about the URL database.

Best regards,
Marcus


On 06/08/2015 08:20 AM, Helmut Hullen wrote:
> Hallo, Marcus,
>
> Du meintest am 07.06.15:
>
>> The URL director interface was changed with Squid 3.4, see also
>> http://wiki.squid-cache.org/Features/RedirectorsThe latest version of
>> squidguard is 1.5 beta from 2010 and squidGuard does not support the
>> new interface of Squid.
>
>> ufdbGuard is also a URL redirector and since it has regular updates,
>> ufdbGuard is compatible with the new URL redirector interface of
>> Squid.ufdbGuard is free software with a GPL2 license,
>
> But what is the meaning of
>
>
>     https://www.urlfilterdb.com/downloads/trialregistration.html
>
> and
>
>     https://www.urlfilterdb.com/pricing/licenses.html
>
> I don't find there that using "ufdbguard" without having payed a
> "licence fee" is legal use.
>
> And the users of the "c't/ODS-Schulserver"
>
>       http://de.wikipedia.org/wiki/Arktur-Schulserver
>
> which I now maintain also may ask this question ...
>
> Viele Gruesse!
> Helmut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From jonathan.filogna at tasso.com.ar  Mon Jun  8 18:11:18 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Mon, 08 Jun 2015 15:11:18 -0300
Subject: [squid-users] Block whatsapp with transparent proxy
In-Reply-To: <5575B57F.3080306@gmail.com>
References: <5575A8BF.8050603@tasso.com.ar> <5575B57F.3080306@gmail.com>
Message-ID: <5575DAC6.4090806@tasso.com.ar>

ty yuki, but i finally decided to block whatsapp with pfSense via 
firewall rules and aliases



El 08/06/15 a las 12:32, Yuri Voinov escibi?:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>   
> https://www.google.com/search?q=squid+watsup+blocking
>
> Feel free to Google your question first.
>
> 08.06.15 20:37, Jonathan Filogna ?????:
>> Hi all, greetings from Argentina
>>
>> I want to know if can be possible block whatsapp for mobiles with a
> transparent proxy
>> which squid version should i install?
>>
>> thank you all
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>   
> iQEcBAEBCAAGBQJVdbV+AAoJENNXIZxhPexG1W4H/jFxKUzoH7ILo8/PztVijrGT
> EJTb3bwuIh6bfuDwJZ04O7yro4hw1n26B+1ARKgiIBNlQa0oys1pCklsbFlQ6KbV
> VtCbU5xTLw9nqBVLQzv1lqlQ7Sz3KNzElissAIOysQnxsoJ5lxkcMNtXD2Y7uU5o
> NZ8lVs9QKAK3PqCDnaPc4bJLuLiwPWcjNLg6xrnGYb/Fjb2dGr3tA1CvjxjuCa5V
> /NXffgKXYKOHpEbt/995F10eSmIP6ycUqOl0ea7g3KwJXG9gUEX5eqWKRH/jG5H/
> EZDzt/UzzgNPz30ywgrn7jUxjBOGJ5fgaVQNky1k5E+mNoR++VZCQN8wMbYL8LQ=
> =TJqN
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Mon Jun  8 18:17:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 09 Jun 2015 00:17:02 +0600
Subject: [squid-users] Block whatsapp with transparent proxy
In-Reply-To: <5575DAC6.4090806@tasso.com.ar>
References: <5575A8BF.8050603@tasso.com.ar> <5575B57F.3080306@gmail.com>
 <5575DAC6.4090806@tasso.com.ar>
Message-ID: <5575DC1E.6090208@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
This is the best solution in many cases. ;)

09.06.15 0:11, Jonathan Filogna ?????:
> ty yuki, but i finally decided to block whatsapp with pfSense via firewall rules and aliases
>
>
>
> El 08/06/15 a las 12:32, Yuri Voinov escibi?:
> Feel free to Google your question first.
>
> 08.06.15 20:37, Jonathan Filogna ?????:
> >>> Hi all, greetings from Argentina
> >>>
> >>> I want to know if can be possible block whatsapp for mobiles with a
> transparent proxy
> >>> which squid version should i install?
> >>>
> >>> thank you all
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVddweAAoJENNXIZxhPexGn3EH/0im8uvwxlcjRyNyepqjHUGd
PhQ/0p9lNMn39j3YunD6/HX1GPLipqfj2S4KOz0HfOF2m/TkxMsfamevKoFVMN55
N3E+SHYVw/4Bo5LOgkNJCj9Lon24PZbhaBAWOb1OKks/3iBMg85+NYOlEK0oPTy1
amJUn6YxyzRjv6rQDAShm20f+OrYpbo7z1LIl9RoEAEdh8h3fGH3508O7rH1ZJYc
S/uDJxrOyjN370RYnJ5OJux3DuNvvUzFPGk+9MetBzVDhobZ8pT5jGW4dkZxBHIU
UtLCvzQfkvH2lo8gFC1VJQG52caSxd1jXLJyMSkikLTnGwvgDx9tgeGEDdRVcIs=
=BRk0
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150609/16378d76/attachment.htm>

From Hullen at t-online.de  Mon Jun  8 18:29:00 2015
From: Hullen at t-online.de (Helmut Hullen)
Date: Mon, 8 Jun 2015 20:29:00 +0200
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <5575CE77.2080106@urlfilterdb.com>
Message-ID: <DISYrkVPCXB@helmut.hullen.de>

Hallo, Marcus,

Du meintest am 08.06.15:

> you can download ufdbGuard here:
>     https://www.urlfilterdb.com/downloads/software_doc.html
> and here:
>     http://sourceforge.net/projects/ufdbguard/

> ufdbGuard is just like Squid free Open Source Software.

> The trial license on www.urlfilterdb.com is about the URL database.

Thank you - sounds good!

Viele Gruesse!
Helmut



From rafael.akchurin at diladele.com  Mon Jun  8 20:23:41 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 8 Jun 2015 20:23:41 +0000
Subject: [squid-users] High-availability and load-balancing between N squid
	servers
Message-ID: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hello all,

What is the recommended approach to perform load balancing and high availability between N squid servers? 
I have the following list of requirements to fullfil:

1) Manage N squid servers that share cache (as far as i understand is done using cache_peer). Desirable.
2) Availability: if any of the N servers fails the clients are redirected to the rest N-1. Prefferable.
3) Scalability: The load is distributed (round-roubin or some other algorithm) between N servers, if a new server is added (N + 1) new clients will be able to use it reducing the load on the rest N. Prefferable.
4) I need to be able to identify client IP addresses on the squid side and/or perform Squid authentication. The client IP and User Name are later to be passed to the ICAP server to scan the HTTP(S) request/response contents using icap_send_client_ip and icap_send_client_username. Very important requirement.
5) I need to support both HTTP and HTTPS connections with support of selective SSL Bump. I.e. for some web sites I do not want to look inside SSL so that original site's certificates?are used for encryption. Very important requirement too.

I know that strictly for HTTP I could use HAProxy with Forward-For or something similar, but the 5th requirement is very important and I could not find a way to handle SSL with HAProxy properly.

The only idea that comes to my mind is?to use some form of round-robin load balancing on the level of DNS, but it has its own drawbacks (should be able to check availability of my N servers + not real balancing, more like distribution).
Any help/thoughts are appreciated.

Thank you!

Best regards,
Rafael Akchurin
Diladele B.V.



From jrswartz at ncswi.com  Mon Jun  8 21:21:41 2015
From: jrswartz at ncswi.com (JR Swartz)
Date: Mon, 8 Jun 2015 16:21:41 -0500
Subject: [squid-users] Lag Time Displaying SVG files
Message-ID: <003001d0a231$1cf72300$56e56900$@com>

We have a customer that uses Squid (3.x).  When viewing the
www.fmcdealer.com web site for their business, standard web pages load as
expected.  However, when drilling down into the wiring diagrams (which are
in SVG format), there is exactly a 2 minute (120 sec) delay before the
diagrams are displayed.

 

We've tested this on several svg diagrams and, regardless of the size of the
diagram, the delay is exactly 2 minutes.

 

Additionally, we've run tcpdump -vv and it appears there is no traffic
during this 2 minute period.

 

We're hoping someone has seen this or the 120 second delay may trigger
someone's memory about this issue?

 

 

 

==============================

J.R. Swartz

Northern Computer Service, LLC

Owner

 

8821 Hwy 47 East

Woodruff, WI 54568

715.358.9806                                                  

Email:  jrswartz at ncswi.com

Web Site:  www.ncswi.com

windows_genuine.jpgfedora-logo.jpg

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/4f2485ec/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image005.jpg
Type: image/jpeg
Size: 2519 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/4f2485ec/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image006.jpg
Type: image/jpeg
Size: 2067 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/4f2485ec/attachment-0001.jpg>

From bpk678 at gmail.com  Mon Jun  8 21:22:35 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Mon, 08 Jun 2015 17:22:35 -0400
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
Message-ID: <5576079B.1030203@gmail.com>

On 06/08/2015 04:23 PM, Rafael Akchurin wrote:
> Hello all,
>
> What is the recommended approach to perform load balancing and high availability between N squid servers?
> I have the following list of requirements to fullfil:
>
> 1) Manage N squid servers that share cache (as far as i understand is done using cache_peer). Desirable.
> 2) Availability: if any of the N servers fails the clients are redirected to the rest N-1. Prefferable.
> 3) Scalability: The load is distributed (round-roubin or some other algorithm) between N servers, if a new server is added (N + 1) new clients will be able to use it reducing the load on the rest N. Prefferable.
> 4) I need to be able to identify client IP addresses on the squid side and/or perform Squid authentication. The client IP and User Name are later to be passed to the ICAP server to scan the HTTP(S) request/response contents using icap_send_client_ip and icap_send_client_username. Very important requirement.
> 5) I need to support both HTTP and HTTPS connections with support of selective SSL Bump. I.e. for some web sites I do not want to look inside SSL so that original site's certificates are used for encryption. Very important requirement too.
>
> I know that strictly for HTTP I could use HAProxy with Forward-For or something similar, but the 5th requirement is very important and I could not find a way to handle SSL with HAProxy properly.
>
> The only idea that comes to my mind is to use some form of round-robin load balancing on the level of DNS, but it has its own drawbacks (should be able to check availability of my N servers + not real balancing, more like distribution).
> Any help/thoughts are appreciated.
>
> Thank you!
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
1 - you are likely going to want to create a config for peers:
acl peers src 192.168.1.1/32
acl peers src 192.168.1.2/32
...
http_access allow peers
...
cache_peer 192.168.1.1 sibling 3128 4827 htcp=no-clr
cache_peer 192.168.1.2 sibling 3128 4827 htcp=no-clr
...

remember to make sure all peers are properly represented

2 - use a load balancing device or software, and this is part-and-parcel 
to that functionality

3 - load balancers give you several options.  i use "least connections" 
which is a bit more intelligent that straight round-robin.

4 - depending on how you setup your load balancer, you might be able to 
get the client IP without playing games.  if you cant see the client IP 
as the source of the connection, you will have to work with the 
"X-Forwarded-For" header, like so:
...
follow_x_forwarded_for allow svc_chk
follow_x_forwarded_for deny all
...*
*acl_uses_indirect_client on*
*...*
*log_uses_indirect_client on*
*...

Also, your auth methods may have nuances that need to be accounted for 
(Kerberos and load balancing requires some extra steps).

  5 - HAProxy will work for HTTP and HTTPS.  remember, your clients 
arent talking to HAProxy for HTTP proxying.  the port you load balance 
on with HAProxy is not the HTTP proxy process.  All HAProxy has to do is 
hand the connection off to Squid which will handle the HTTP or HTTPS or 
HTTPS-with-SSL-Bump, independent of anything HAProxy sees or cares about.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150608/001004b4/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun  8 22:42:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 09 Jun 2015 10:42:33 +1200
Subject: [squid-users] Squid cache_peer in tproxy
In-Reply-To: <1433755251731-4671602.post@n4.nabble.com>
References: <1433755251731-4671602.post@n4.nabble.com>
Message-ID: <55761A59.7080106@treenet.co.nz>

On 8/06/2015 9:20 p.m., Stakres wrote:
> Hi All,
> 
> We're facing a weird issue with the cache_peer and tproxy.
> 
> Squid 3.5.4
> users -> squid1 -> squid2/squid3 -> internet
> 
> squid1:
> http_port 3128
> http_port 3129 tproxy
> icp_port 3130
> cache_peer 192.168.1.2 parent 3128 3130  proxy-only weighted-round-robin
> background-ping no-digest 
> cache_peer 192.168.1.3 parent 3128 3130  proxy-only weighted-round-robin
> background-ping no-digest 
> 
> squid2:
> http_port 3128
> icp_port 3130
> cache_peer 192.168.1.3 sibling 3128 3130 proxy-only no-digest
> 
> squid3:
> http_port 3128
> icp_port 3130
> cache_peer 192.168.1.2 sibling 3128 3130 proxy-only no-digest
> 
> The user IPs are corrects at the squid1 but we lost them at the
> squid2/squid3, here we see the squid1 ip only.
> Where are we wrong ?

Lost in what way?

The expected behaviour is that the client IP is used as src-IP on the
squid1->squid2 or squid1->squid3 TCP connections. TPROXY in squid1 has
no involvement with squid2/squid3 past that.

Since a cache_peer proxy cannot be a TPROXY (origin server) by
definition the other proxies connect out with their own IPs to upstream
servers.

Amos



From squid3 at treenet.co.nz  Mon Jun  8 22:46:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 09 Jun 2015 10:46:22 +1200
Subject: [squid-users] Squid doesn't write logs via rsyslog
In-Reply-To: <201506081302.07944.Antony.Stone@squid.open.source.it>
References: <5575740ca94bd1.78241436@wp.pl>
 <201506081302.07944.Antony.Stone@squid.open.source.it>
Message-ID: <55761B3E.5030803@treenet.co.nz>

On 8/06/2015 11:02 p.m., Antony Stone wrote:
> On Monday 08 June 2015 at 12:53:00 (EU time), Robert Lasota wrote:
> 
>> the problem is it still writes logs to files /var/log/access.log or
>> /opt/var/log/access.log (depends what I set in conf) but never to rsyslog.
>>
>> I mean, I have set rsyslog to it send logs to remote central server, and
>> from other apps like sshd or named its working and rsyslog send them , but
>> Squid still not care that and writes locally to files.
>>
>> I set different combinations in squid.conf but nothing, even:
>> access_log syslog squid
>> cache_log syslog squid.
>> ..also nothing
> 
> You appear to be missing the facility and priority settings (ie: telling 
> syslogd how to handle the messages).
> 
> See http://www.squid-cache.org/Doc/config/access_log/
> 
> Try something such as:
> 
> access_log syslog:daemon.info


Also, cache.log is the unified stderr output of all Squid sub-processes
(workers, diskers, helpers etc). It cannot use syslog at this time.

You can possibly make cache.log file point at a unix socket device that
pipes somewhere like syslog though.

Amos



From vdoctor at neuf.fr  Mon Jun  8 22:32:28 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 8 Jun 2015 15:32:28 -0700 (PDT)
Subject: [squid-users] Squid cache_peer in tproxy
In-Reply-To: <55761A59.7080106@treenet.co.nz>
References: <1433755251731-4671602.post@n4.nabble.com>
 <55761A59.7080106@treenet.co.nz>
Message-ID: <1433802748137-4671621.post@n4.nabble.com>

Hi Amos,

Ok, it does confirm our tests.

Is there a way to do this:
users -> squid1 tproxy -> squid2/squid3 tproxy -> internet (seeing the user
ips) ?
or is it impossible ?

in the wiki, there is an option "no-tproxy" in the cache_peer, it would be
nice to have a "keep-tproxy" 

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-cache-peer-in-tproxy-tp4671602p4671621.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From bpk678 at gmail.com  Mon Jun  8 23:04:00 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Mon, 08 Jun 2015 19:04:00 -0400
Subject: [squid-users] Squid doesn't write logs via rsyslog
In-Reply-To: <55761B3E.5030803@treenet.co.nz>
References: <5575740ca94bd1.78241436@wp.pl>
 <201506081302.07944.Antony.Stone@squid.open.source.it>
 <55761B3E.5030803@treenet.co.nz>
Message-ID: <55761F60.3060404@gmail.com>

On 06/08/2015 06:46 PM, Amos Jeffries wrote:
> On 8/06/2015 11:02 p.m., Antony Stone wrote:
>> On Monday 08 June 2015 at 12:53:00 (EU time), Robert Lasota wrote:
>>
>>> the problem is it still writes logs to files /var/log/access.log or
>>> /opt/var/log/access.log (depends what I set in conf) but never to rsyslog.
>>>
>>> I mean, I have set rsyslog to it send logs to remote central server, and
>>> from other apps like sshd or named its working and rsyslog send them , but
>>> Squid still not care that and writes locally to files.
>>>
>>> I set different combinations in squid.conf but nothing, even:
>>> access_log syslog squid
>>> cache_log syslog squid.
>>> ..also nothing
>> You appear to be missing the facility and priority settings (ie: telling
>> syslogd how to handle the messages).
>>
>> See http://www.squid-cache.org/Doc/config/access_log/
>>
>> Try something such as:
>>
>> access_log syslog:daemon.info
>
> Also, cache.log is the unified stderr output of all Squid sub-processes
> (workers, diskers, helpers etc). It cannot use syslog at this time.
>
> You can possibly make cache.log file point at a unix socket device that
> pipes somewhere like syslog though.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
to stop rsyslog from writing something, i use:

if $programname startswith 'NetworkManager' then -/dev/null
&~

all messages from NetworkManager are written out to /dev/null in 
asynchronous fashion (does not wait for confirmation of the write action 
succeeding, or fire-and-forget mode).  the &~ is a hard stop action so 
all processing of rules stops if the criteria are met.

you would probably want something like that, but will have to play 
around with it, to make it do what you want.

by the by, are you using plain rsyslog forwarding ala:

*.* @@remote-host:514

i am using RELP (Reliable Event Log Processing) to forward all logs from 
all my boxes to a central device where they are loaded into mariadb.  
the relp module creates a "store-and-forward" fifo queue that can 
overcome network outages (length of outage handled is dictated by queue 
size), and also uses TCP for reliability.  there are modules for 
encryption, authentication, etc for relp, too. there is also phplogcon, 
which i use to review the logs in the database.


From squid3 at treenet.co.nz  Mon Jun  8 23:06:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 09 Jun 2015 11:06:21 +1200
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <5576079B.1030203@gmail.com>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com>
Message-ID: <55761FED.4030900@treenet.co.nz>

On 9/06/2015 9:22 a.m., Brendan Kearney wrote:
> On 06/08/2015 04:23 PM, Rafael Akchurin wrote:
>> Hello all,
>>
>> What is the recommended approach to perform load balancing and high
>> availability between N squid servers?
>> I have the following list of requirements to fullfil:
>>
>> 1) Manage N squid servers that share cache (as far as i understand is
>> done using cache_peer). Desirable.
>> 2) Availability: if any of the N servers fails the clients are
>> redirected to the rest N-1. Prefferable.
>> 3) Scalability: The load is distributed (round-roubin or some other
>> algorithm) between N servers, if a new server is added (N + 1) new
>> clients will be able to use it reducing the load on the rest N.
>> Prefferable.
>> 4) I need to be able to identify client IP addresses on the squid side
>> and/or perform Squid authentication. The client IP and User Name are
>> later to be passed to the ICAP server to scan the HTTP(S)
>> request/response contents using icap_send_client_ip and
>> icap_send_client_username. Very important requirement.
>> 5) I need to support both HTTP and HTTPS connections with support of
>> selective SSL Bump. I.e. for some web sites I do not want to look
>> inside SSL so that original site's certificates are used for
>> encryption. Very important requirement too.
>>
>> I know that strictly for HTTP I could use HAProxy with Forward-For or
>> something similar, but the 5th requirement is very important and I
>> could not find a way to handle SSL with HAProxy properly.
>>
>> The only idea that comes to my mind is to use some form of round-robin
>> load balancing on the level of DNS, but it has its own drawbacks
>> (should be able to check availability of my N servers + not real
>> balancing, more like distribution).
>> Any help/thoughts are appreciated.
>>
>> Thank you!
>>
>> Best regards,
>> Rafael Akchurin
>> Diladele B.V.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 1 - you are likely going to want to create a config for peers:
> acl peers src 192.168.1.1/32
> acl peers src 192.168.1.2/32
> ...
> http_access allow peers
> ...
> cache_peer 192.168.1.1 sibling 3128 4827 htcp=no-clr
> cache_peer 192.168.1.2 sibling 3128 4827 htcp=no-clr
> ...
> 
> remember to make sure all peers are properly represented
> 
> 2 - use a load balancing device or software, and this is part-and-parcel
> to that functionality
> 
> 3 - load balancers give you several options.  i use "least connections"
> which is a bit more intelligent that straight round-robin.
> 
> 4 - depending on how you setup your load balancer, you might be able to
> get the client IP without playing games.  if you cant see the client IP
> as the source of the connection, you will have to work with the
> "X-Forwarded-For" header, like so:
> ...
> follow_x_forwarded_for allow svc_chk
> follow_x_forwarded_for deny all
> ...*
> *acl_uses_indirect_client on*
> *...*
> *log_uses_indirect_client on*
> *...
> 
> Also, your auth methods may have nuances that need to be accounted for
> (Kerberos and load balancing requires some extra steps).
> 
>  5 - HAProxy will work for HTTP and HTTPS.  remember, your clients arent
> talking to HAProxy for HTTP proxying.  the port you load balance on with
> HAProxy is not the HTTP proxy process.  All HAProxy has to do is hand
> the connection off to Squid which will handle the HTTP or HTTPS or
> HTTPS-with-SSL-Bump, independent of anything HAProxy sees or cares about.

There seems to be a bit of a myth going around about how HAProxy does
load balancing.

HAProxy is an HTTP layer proxy. Just like Squid.

They both do the same things to received TCP connections. But HAProxy
supports less HTTP features, so its somewhat simpler processing is also
a bit faster when you want it to be a semi-dumb load balancer.


We are somewhat recently added basic support for the PROXY protocol to
Squid. So HAProxy can relay port 80 connections to Squid-3.5+ without
processing them fully. However Squid does not yet support that on
https_port, which means the TLS connections still wont have client IP
details passed through.

Amos


From squid3 at treenet.co.nz  Mon Jun  8 23:35:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 09 Jun 2015 11:35:04 +1200
Subject: [squid-users] Squid cache_peer in tproxy
In-Reply-To: <1433802748137-4671621.post@n4.nabble.com>
References: <1433755251731-4671602.post@n4.nabble.com>
 <55761A59.7080106@treenet.co.nz> <1433802748137-4671621.post@n4.nabble.com>
Message-ID: <557626A8.9060806@treenet.co.nz>

On 9/06/2015 10:32 a.m., Stakres wrote:
> Hi Amos,
> 
> Ok, it does confirm our tests.
> 
> Is there a way to do this:
> users -> squid1 tproxy -> squid2/squid3 tproxy -> internet (seeing the user
> ips) ?
> or is it impossible ?


Only by intercepting the squid1 port 80 outgoing connections. cache_peer
cannot do that.


> 
> in the wiki, there is an option "no-tproxy" in the cache_peer, it would be
> nice to have a "keep-tproxy" 

I think you misunderstand how TPROXY works. It is part of the *TCP
connection* state. There are different TCP connections inbound and
outbound from each Squid.

Software X opens a TPROXY listening port, TCP connections delivered
there are remembered by the kernel. When software X opens outgoing
connections and tells the kernel they were received via the TPROXY port
the kernel then allows software X to bind() the src/dst IPs on those
outgoing TCP connections to any of the IPs it has on the open
connections to the TPROXY listening port.

Now...

* the *dst-IP* on a cache_peer connection is the peer proxy IP. Always,
otherwise it would not arrive at the peer.

* Squid interceptors will ensure that any port 80/443 outgoing TCP
connection is going to the same server TPROXY tells it the client was
going to (ORIGINAL_DST). **


Therefore if squid1 has a cache_peer pointing at a TPROXY kernel rule on
squid2/squid3 the outbound connections from squid2/squid3 will be
guaranteed to be sent back to themselves. Repeat until either
squid2/squid3 notices the loop, or the server TCP sockets are all gone,
or server runs out of memory, or the TCP networking stack collapses.
Whichever occurs first and none of it nice.


** due to CVE-2009-0801. The rather innocent vuln text description is
the tip of an iceberg of problems worthy of sinking the titanic.

Amos



From squid3 at treenet.co.nz  Mon Jun  8 23:45:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 09 Jun 2015 11:45:25 +1200
Subject: [squid-users] Lag Time Displaying SVG files
In-Reply-To: <003001d0a231$1cf72300$56e56900$@com>
References: <003001d0a231$1cf72300$56e56900$@com>
Message-ID: <55762915.6040102@treenet.co.nz>

On 9/06/2015 9:21 a.m., JR Swartz wrote:
> We have a customer that uses Squid (3.x).  When viewing the
> www.fmcdealer.com web site for their business, standard web pages load as
> expected.  However, when drilling down into the wiring diagrams (which are
> in SVG format), there is exactly a 2 minute (120 sec) delay before the
> diagrams are displayed.
> 
>  
> 
> We've tested this on several svg diagrams and, regardless of the size of the
> diagram, the delay is exactly 2 minutes.
> 
>  
> 
> Additionally, we've run tcpdump -vv and it appears there is no traffic
> during this 2 minute period.
> 
>  
> 
> We're hoping someone has seen this or the 120 second delay may trigger
> someone's memory about this issue?

If the end of the server reply message has not yet been received then
this is not a Squid problem.


It may be server processing delays, but given that exact timing it is
more likely to be a TCP timeout on the connection.


If the server emits a message without Content-Length or
Transfer-Encoding headers then end-of-message is the TCP connection
close signal.
 If the server fails to close Squid is left waiting for more bytes which
will never arrive, until the TCP networking stack times out and closes
it. Then Squid relays the end-of-message signal to the client and
everything works again.

Amos



From kl at vsen.dk  Tue Jun  9 06:44:13 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 09 Jun 2015 08:44:13 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <5570CEBB.1040607@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz>
Message-ID: <55768B3D.3000302@vsen.dk>

Hi,

James Lay just replied to me with his current config.. (pretty much like 
what he posted), and it seems he does not even try to use http_access 
rules to filter on urls from https requests..

@Amos: are you certain that there's not an error in how http_access 
rules are applied to bumped connections?

What I noted was:

Instead of having:
http_access allow CONNECT bumpedPorts

he has:
http_access allow SSL_ports

which somehow seems to work instead.

He only uses http_access allow rules for http sites.. he filters https 
on domain only - using:
acl allowed_https_sites ssl::server_name_regex "/opt/etc/squid/http_url.txt"
ssl_bump bump allowed_https_sites
ssl_bump terminate !allowed_https_sites

in my access log - using james lay's format - squid only logs CONNECT.. 
so it seems its not registering the step AFTER CONNECT as something 
seperate - which would explain why its not applying http_access 
filtering to it ?

10.xx.131.244 - - [09/Jun/2015:08:40:15 +0200] "CONNECT 
64.233.184.94:443 HTTP/1.1" www.google.dk - 200 20042 
TCP_TUNNEL:ORIGINAL_DST peek
10.xx.131.244 - - [09/Jun/2015:08:40:19 +0200] "CONNECT 72.51.34.34:443 
HTTP/1.1" lwn.net - 200 28295 TCP_TUNNEL:ORIGINAL_DST peek
10.xx.131.244 - - [09/Jun/2015:08:42:30 +0200] "CONNECT 72.51.34.34:443 
HTTP/1.1" lwn.net - 200 28258 TCP_TUNNEL:ORIGINAL_DST peek


Amos Jeffries wrote on 06/05/2015 12:18 AM:
> On 5/06/2015 3:34 a.m., Klavs Klavsen wrote:
>> I would be perfectly fine with allowing the SSL bumping to finish for
>> ALL https sites - and then only block when the http request comes..
>>
>> I'm hoping someone can tell me what I've done wrong in my config.. I'm
>> obviously not understanding how it works when https is envolved.. it
>> works as intended with http..
>
> It should be working. I'm a bit confused myself now why that CONNECT
> line would be matching the decrypted requests, they definitely should
> not be having the CONNECT request method as they are destined to an
> origin server.
>
> We've missed something basic, and will probably kick ourselves at how
> simple when its reavealed. :-(
>   All I can think of now is that James log format should be indicating
> more clearly whats going on than the default Squid one will.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From rafael.akchurin at diladele.com  Tue Jun  9 07:15:06 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 9 Jun 2015 07:15:06 +0000
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <55761FED.4030900@treenet.co.nz>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com>,<55761FED.4030900@treenet.co.nz>
Message-ID: <DB5PR04MB11288E5C59069436EF4702D58FBE0@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hi Amos,

<snip>

> There seems to be a bit of a myth going around about how HAProxy does
> load balancing. HAProxy is an HTTP layer proxy. Just like Squid.
> 
> They both do the same things to received TCP connections. But HAProxy
> supports less HTTP features, so its somewhat simpler processing is also
> a bit faster when you want it to be a semi-dumb load balancer.

> We are somewhat recently added basic support for the PROXY protocol to Squid. 
> So HAProxy can relay port 80 connections to Squid-3.5+ without
> processing them fully. However Squid does not yet support that on
> https_port, which means the TLS connections still wont have client IP
> details passed through.

So what would be your proposition for the case of SSL Bump? 
How to get the connecting client IP and authenticated user name passed to the ICAP server when a cluster of squids somehow getting the CONNECT tunnel established? 

Assume we left away the haproxy and rely solely on squid - how would you approach this and how many instances of squid would you deploy?

>From my limited knowledge the FQDN proxy name being resolved to a number of IP addresses running one squid per IP address is the simplest approach. 


Best regards,
Rafael

From eliezer at ngtech.co.il  Tue Jun  9 09:36:09 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 09 Jun 2015 12:36:09 +0300
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <55761FED.4030900@treenet.co.nz>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com> <55761FED.4030900@treenet.co.nz>
Message-ID: <5576B389.9070305@ngtech.co.il>

Hey Amos,

I didn't had the chance to follow the PROXY protocol advancements.
Was there any fix for the PROXY protocol issue that I can test?

Thanks,
Eliezer

On 09/06/2015 02:06, Amos Jeffries wrote:
> We are somewhat recently added basic support for the PROXY protocol to
> Squid. So HAProxy can relay port 80 connections to Squid-3.5+ without
> processing them fully. However Squid does not yet support that on
> https_port, which means the TLS connections still wont have client IP
> details passed through.
>
> Amos




From squid3 at treenet.co.nz  Tue Jun  9 12:43:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 00:43:26 +1200
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <5576B389.9070305@ngtech.co.il>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com> <55761FED.4030900@treenet.co.nz>
 <5576B389.9070305@ngtech.co.il>
Message-ID: <5576DF6E.5040405@treenet.co.nz>

On 9/06/2015 9:36 p.m., Eliezer Croitoru wrote:
> Hey Amos,
> 
> I didn't had the chance to follow the PROXY protocol advancements.
> Was there any fix for the PROXY protocol issue that I can test?

IIRC the issues we found are all resolved. Though I've had no confirmation.

Amos


From squid3 at treenet.co.nz  Tue Jun  9 12:51:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 00:51:51 +1200
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <DB5PR04MB11288E5C59069436EF4702D58FBE0@DB5PR04MB1128.eurprd04.prod.outlook.com>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com>, <55761FED.4030900@treenet.co.nz>
 <DB5PR04MB11288E5C59069436EF4702D58FBE0@DB5PR04MB1128.eurprd04.prod.outlook.com>
Message-ID: <5576E167.2090806@treenet.co.nz>

On 9/06/2015 7:15 p.m., Rafael Akchurin wrote:
> Hi Amos,
> 
> <snip>
> 
>> There seems to be a bit of a myth going around about how HAProxy does
>> load balancing. HAProxy is an HTTP layer proxy. Just like Squid.
>>
>> They both do the same things to received TCP connections. But HAProxy
>> supports less HTTP features, so its somewhat simpler processing is also
>> a bit faster when you want it to be a semi-dumb load balancer.
> 
>> We are somewhat recently added basic support for the PROXY protocol to Squid. 
>> So HAProxy can relay port 80 connections to Squid-3.5+ without
>> processing them fully. However Squid does not yet support that on
>> https_port, which means the TLS connections still wont have client IP
>> details passed through.
> 
> So what would be your proposition for the case of SSL Bump? 
> How to get the connecting client IP and authenticated user name passed to the ICAP server when a cluster of squids somehow getting the CONNECT tunnel established? 
> 
> Assume we left away the haproxy and rely solely on squid - how would you approach this and how many instances of squid would you deploy?
> 
> From my limited knowledge the FQDN proxy name being resolved to a number of IP addresses running one squid per IP address is the simplest approach. 
> 

Yes, it would seem to be the only form which meets all your criteria
too. Everything else runs up against the HTTPS brick wall.

Amos


From squid3 at treenet.co.nz  Tue Jun  9 13:06:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 01:06:12 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <55768B3D.3000302@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
Message-ID: <5576E4C4.3000508@treenet.co.nz>

On 9/06/2015 6:44 p.m., Klavs Klavsen wrote:
> Hi,
> 
> James Lay just replied to me with his current config.. (pretty much like
> what he posted), and it seems he does not even try to use http_access
> rules to filter on urls from https requests..
> 
> @Amos: are you certain that there's not an error in how http_access
> rules are applied to bumped connections?

As far as I know its working as designed.

You can enable "debug_options 28,5" to see what access controls are
being run.


> 
> What I noted was:
> 
> Instead of having:
> http_access allow CONNECT bumpedPorts

... which matches only the pre-bumping CONNECT requests.

> 
> he has:
> http_access allow SSL_ports

... which matches anything going to port 443 etc. *bumped or not.*

> 
> which somehow seems to work instead.

The "working" config when applied to HTTPS requests is equivalent to:

  http_access deny CONNECT !SSL_Bump
  http_access allow all


> 
> He only uses http_access allow rules for http sites..

Yes, read that back to yourself.


> he filters https
> on domain only - using:
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump bump allowed_https_sites
> ssl_bump terminate !allowed_https_sites
> 
> in my access log - using james lay's format - squid only logs CONNECT..
> so it seems its not registering the step AFTER CONNECT as something
> seperate - which would explain why its not applying http_access
> filtering to it ?

The HTTP message log (access.log) is only logging the HTTP(S) messages.
The non-HTTP protools are not logged.

> 
> 10.xx.131.244 - - [09/Jun/2015:08:40:15 +0200] "CONNECT
> 64.233.184.94:443 HTTP/1.1" www.google.dk - 200 20042
> TCP_TUNNEL:ORIGINAL_DST peek

This got peeked then spliced (not decrypted). There is no decrypted
message(s) to be logged or even to pass through http_access.

Amos


From tarotapprentice at yahoo.com  Tue Jun  9 14:09:12 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 9 Jun 2015 14:09:12 +0000 (UTC)
Subject: [squid-users] Upload issue with squid 3.5.5
Message-ID: <507901884.6958311.1433858952212.JavaMail.yahoo@mail.yahoo.com>

I have a number of machines running BOINC which are having issues uploading with one particular project (climateprediction.net) however if I redirect the client to a Squid 2.7 server they work fine. It doesn't do it every time, some files work just fine. They are usually 15Mb or 47Mb uploads.

Below is the http debug messages from a BOINC client indicating it can't understand the encoding after its made initial contact. This was taken when talking to Squid 3.5.5 (Win Server 2008).

Cheers,
MarkJ


09-06-2015 11:45 PM Started upload of hadam3p_anz_n9oe_2007_1_009869130_0_12.zip 
09-06-2015 11:45 PM [http] [ID#82] Info:  timeout on name lookup is not supported 
09-06-2015 11:45 PM [http] [ID#82] Info:  Hostname was found in DNS cache 
09-06-2015 11:45 PM [http] [ID#82] Info:    Trying 192.168.0.*... 
09-06-2015 11:45 PM [http] [ID#82] Info:  Connected to abc123 (192.168.0.*) port 3128 (#132) 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: POST http://cpdn-upload4.oerc.ox.ac.uk/cgi-bin/file_upload_handler HTTP/1.0
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: User-Agent: BOINC client (windows_x86_64 7.6.2)
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Host: cpdn-upload4.oerc.ox.ac.uk
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Accept: */*
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Accept-Encoding: deflate, gzip
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Proxy-Connection: Keep-Alive
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Content-Type: application/x-www-form-urlencoded
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Accept-Language: en_AU
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: Content-Length: 294
 
09-06-2015 11:45 PM [http] [ID#82] Sent header to server: 
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: HTTP/1.1 200 OK
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Date: Tue, 09 Jun 2015 13:45:10 GMT
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Server: Apache/2.4.7 (Ubuntu)
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Vary: Accept-Encoding
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Content-Encoding: gzip
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Content-Length: 75
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Content-Type: text/plain
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: X-Cache: MISS from abc123
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Via: 1.1 abc123 (squid/3.5.5)
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: Connection: keep-alive
 
09-06-2015 11:45 PM [http] [ID#82] Received header from server: 
 
09-06-2015 11:45 PM [http] [ID#82] Info:  Error while processing content unencoding: invalid block type 
09-06-2015 11:45 PM [http] [ID#82] Info:  Closing connection 132 
09-06-2015 11:45 PM [http] HTTP error: Unrecognized or bad HTTP Content or Transfer-Encoding *


From squid at bloms.de  Tue Jun  9 14:33:32 2015
From: squid at bloms.de (Dieter Bloms)
Date: Tue, 9 Jun 2015 16:33:32 +0200
Subject: [squid-users] howto disable tls compression when using sslbump in
 squid-3.5.5 between squid and https webserver ?
Message-ID: <20150609143329.GA4959@bloms.de>

Hello,

I use squid 3.5.5 and use the sslbump feature.
When I activate sslbump, the browsertest on www.ssllabs.com
( https://www.ssllabs.com/ssltest/viewMyClient.html )
says TLS compression is activated and insecure.
I use openssl 1.0.1m on my proxyserver

I tried some settings like:

sslproxy_flags No_Compression

but squid claims "FATAL: Unknown ssl flag 'No_Compression'".

Is it possible to disable TLS compression for the connection from squid
to the webserver when sslbump is used ?

Thank you very much.


-- 
Regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From kl at vsen.dk  Tue Jun  9 14:51:41 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 09 Jun 2015 16:51:41 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <5576E4C4.3000508@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
 <5576E4C4.3000508@treenet.co.nz>
Message-ID: <5576FD7D.4070109@vsen.dk>

Amos Jeffries wrote on 06/09/2015 03:06 PM:
>
> The HTTP message log (access.log) is only logging the HTTP(S) messages.
> The non-HTTP protools are not logged.
>
>>
>> 10.xx.131.244 - - [09/Jun/2015:08:40:15 +0200] "CONNECT
>> 64.233.184.94:443 HTTP/1.1" www.google.dk - 200 20042
>> TCP_TUNNEL:ORIGINAL_DST peek
>
> This got peeked then spliced (not decrypted). There is no decrypted
> message(s) to be logged or even to pass through http_access.
>
I'm obviously not understanding something.. I would like squid to "fake 
the certificate" - and then when the clients sends an actual request - 
run that through http_access.. so I can match on urls..

I'd rather not filter on only domain if possible..

Is that not possible currently with squid?

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Jun  9 15:10:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 03:10:35 +1200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <5576FD7D.4070109@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
 <5576E4C4.3000508@treenet.co.nz> <5576FD7D.4070109@vsen.dk>
Message-ID: <557701EB.7010607@treenet.co.nz>

On 10/06/2015 2:51 a.m., Klavs Klavsen wrote:
> Amos Jeffries wrote on 06/09/2015 03:06 PM:
>>
>> The HTTP message log (access.log) is only logging the HTTP(S) messages.
>> The non-HTTP protools are not logged.
>>
>>>
>>> 10.xx.131.244 - - [09/Jun/2015:08:40:15 +0200] "CONNECT
>>> 64.233.184.94:443 HTTP/1.1" www.google.dk - 200 20042
>>> TCP_TUNNEL:ORIGINAL_DST peek
>>
>> This got peeked then spliced (not decrypted). There is no decrypted
>> message(s) to be logged or even to pass through http_access.
>>
> I'm obviously not understanding something.. I would like squid to "fake
> the certificate" - and then when the clients sends an actual request -
> run that through http_access.. so I can match on urls..
> 
> I'd rather not filter on only domain if possible..
> 
> Is that not possible currently with squid?

That is the "bump" action and depends on what TLS details are presented
by client and server vs what you have configured to be done.

You have to first configure ssl_bump in a way that lets Squid receive
the clientHello message (step1 -> peek) AND the serverHello message
(step2 -> peek). Then you can use those cert details to bump (step3 ->
bump).
The config is quite simple:
  ssl_bump peek all
  ssl_bump bump all


But there are cases like the client is resuming a previous TLS session
where there is no certificates involved. Squid cannot do anything, so it
automatically splices (3.5.4+ at least do). Or if you have configured
your Squid in a way that there are no mutually supported ciphers.


It may just be your ssl_bump rules. But given that this is a google
domain there is a strong chance that you are encountering one of those
special case.

Amos



From squid3 at treenet.co.nz  Tue Jun  9 15:17:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 03:17:20 +1200
Subject: [squid-users] howto disable tls compression when using sslbump
 in squid-3.5.5 between squid and https webserver ?
In-Reply-To: <20150609143329.GA4959@bloms.de>
References: <20150609143329.GA4959@bloms.de>
Message-ID: <55770380.1010605@treenet.co.nz>

On 10/06/2015 2:33 a.m., Dieter Bloms wrote:
> Hello,
> 
> I use squid 3.5.5 and use the sslbump feature.
> When I activate sslbump, the browsertest on www.ssllabs.com
> ( https://www.ssllabs.com/ssltest/viewMyClient.html )
> says TLS compression is activated and insecure.
> I use openssl 1.0.1m on my proxyserver
> 
> I tried some settings like:
> 
> sslproxy_flags No_Compression
> 
> but squid claims "FATAL: Unknown ssl flag 'No_Compression'".
> 
> Is it possible to disable TLS compression for the connection from squid
> to the webserver when sslbump is used ?
> 

That is an OpenSSL library option. Use it in sslproxy_options.

Amos



From squid3 at treenet.co.nz  Tue Jun  9 15:26:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 03:26:52 +1200
Subject: [squid-users] Upload issue with squid 3.5.5
In-Reply-To: <507901884.6958311.1433858952212.JavaMail.yahoo@mail.yahoo.com>
References: <507901884.6958311.1433858952212.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <557705BC.3010109@treenet.co.nz>

On 10/06/2015 2:09 a.m., TarotApprentice wrote:
> I have a number of machines running BOINC which are having issues uploading with one particular project (climateprediction.net) however if I redirect the client to a Squid 2.7 server they work fine. It doesn't do it every time, some files work just fine. They are usually 15Mb or 47Mb uploads.
> 
> Below is the http debug messages from a BOINC client indicating it can't understand the encoding after its made initial contact. This was taken when talking to Squid 3.5.5 (Win Server 2008).
> 
> Cheers,
> MarkJ
> 


> 09-06-2015 11:45 PM [http] [ID#82] Info:  Error while processing content unencoding: invalid block type 
> 09-06-2015 11:45 PM [http] [ID#82] Info:  Closing connection 132 
> 09-06-2015 11:45 PM [http] HTTP error: Unrecognized or bad HTTP Content or Transfer-Encoding *

That is the origin server producing something as message payload that
the client cannot parse.

Squid is just blindly relaying the payload bytes the server gave. Not
even caching. So there appears to be no Squid problem.
 What does the working/2.7 trace look like?


Amos



From jrswartz at ncswi.com  Tue Jun  9 16:14:23 2015
From: jrswartz at ncswi.com (JR Swartz)
Date: Tue, 9 Jun 2015 11:14:23 -0500
Subject: [squid-users] Lag Time Displaying SVG files
In-Reply-To: <55762915.6040102@treenet.co.nz>
References: <003001d0a231$1cf72300$56e56900$@com>
 <55762915.6040102@treenet.co.nz>
Message-ID: <006901d0a2cf$598d3220$0ca79660$@com>

I traced the problem to the persistent_request_timeout variable.  Once I set this from 2 Min to 10 Seconds, it resolved the issue.



==============================
J.R. Swartz
Northern Computer Service, LLC
Owner

8821 Hwy 47 East
Woodruff, WI 54568
715.358.9806	
Email:  jrswartz at ncswi.com
Web Site:  www.ncswi.com




-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, June 08, 2015 6:45 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Lag Time Displaying SVG files

On 9/06/2015 9:21 a.m., JR Swartz wrote:
> We have a customer that uses Squid (3.x).  When viewing the 
> www.fmcdealer.com web site for their business, standard web pages load 
> as expected.  However, when drilling down into the wiring diagrams 
> (which are in SVG format), there is exactly a 2 minute (120 sec) delay 
> before the diagrams are displayed.
> 
>  
> 
> We've tested this on several svg diagrams and, regardless of the size 
> of the diagram, the delay is exactly 2 minutes.
> 
>  
> 
> Additionally, we've run tcpdump -vv and it appears there is no traffic 
> during this 2 minute period.
> 
>  
> 
> We're hoping someone has seen this or the 120 second delay may trigger 
> someone's memory about this issue?

If the end of the server reply message has not yet been received then this is not a Squid problem.


It may be server processing delays, but given that exact timing it is more likely to be a TCP timeout on the connection.


If the server emits a message without Content-Length or Transfer-Encoding headers then end-of-message is the TCP connection close signal.
 If the server fails to close Squid is left waiting for more bytes which will never arrive, until the TCP networking stack times out and closes it. Then Squid relays the end-of-message signal to the client and everything works again.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From michael.pelletier at palmbeachschools.org  Tue Jun  9 17:24:13 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Tue, 9 Jun 2015 13:24:13 -0400
Subject: [squid-users] assertion failed: Read.cc:69:
	"fd_table[conn->fd].halfClosedReader != NULL"
Message-ID: <CAEnCSG5b=U=WnSH+-7QTL=G1WZCu+czAJtUSy05fVAv8m8VP2g@mail.gmail.com>

Hello,

I am getting these errors on 3.5.5 any ideas? Here is my build configuration

INSTALL_DIR=/opt/Squid
INSTALL_DIR_CACHE=/opt/Squid/Cache/AUFS
MAN_DIR=/opt/man
USER=squid
LOG_FILE=/var/log/Squid
PID_FILE=/var/run/Squid/squid.pid
NUMBER_OF_FILE_DESCRIPTORS=65536
OPENSSL_DIR=/opt/OpenSSL

  CC=gcc  LDFLAGS="-L/opt/Openssl -L/opt/OpenSSL/lib -L/opt/OpenLDAP/lib"
CPPFLAGS="-I/opt/OpenLDAP/include -I/opt/OpenSSL/include" ./configure
--prefix=$INSTALL_DIR --mandir=$MAN_DIR    \
        --with-openssl=$OPENSSL_DIR                        \
        --with-pthreads                                    \
        --disable-ipv6                                     \
        --with-included-ltdl                               \
        --enable-cache-digests                             \
        --enable-snmp                                      \
        --enable-icmp                                      \
        --enable-ssl                                       \
        --enable-ssl-crtd                                  \
        --enable-auth                                      \
        --enable-auth-negotiate                            \
        --enable-auth-ntlm                                 \
        --enable-auth-basic                                \
        --enable-storeio="aufs"                            \
        --with-swapdir=$INSTALL_DIR_CACHE                  \
        --with-aufs-threads=128                            \
        --enable-removal-policies                          \
        --enable-http-violations                           \
        --enable-follow-x-forwarded-for                    \
        --enable-external-acl-helpers                      \
        --with-default-user=$USER                          \
        --with-logdir=$LOG_FILE                            \
        --with-pidfile=$PID_FILE                           \
        --with-filedescriptors=$NUMBER_OF_FILE_DESCRIPTORS \
        --with-large-files                                 \
        --enable-disk-io                                   \
        --enable-linux-netfilter                           \
        --enable-delay-pools                               \
        --enable-icap-client

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150609/845e1cdc/attachment.htm>

From kl at vsen.dk  Tue Jun  9 19:39:04 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 09 Jun 2015 21:39:04 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <557701EB.7010607@treenet.co.nz>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
 <5576E4C4.3000508@treenet.co.nz> <5576FD7D.4070109@vsen.dk>
 <557701EB.7010607@treenet.co.nz>
Message-ID: <557740D8.1080709@vsen.dk>

Amos Jeffries wrote on 2015-06-09 17:10:
[CUT]
> You have to first configure ssl_bump in a way that lets Squid receive
> the clientHello message (step1 -> peek) AND the serverHello message
> (step2 -> peek). Then you can use those cert details to bump (step3 ->
> bump).
> The config is quite simple:
>   ssl_bump peek all
>   ssl_bump bump all
> 
I have this:
ssl_bump peek step1 broken
ssl_bump peek step2 broken
ssl_bump splice broken
ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump bump all

> 
> But there are cases like the client is resuming a previous TLS session
> where there is no certificates involved. Squid cannot do anything, so it
> automatically splices (3.5.4+ at least do). Or if you have configured
> your Squid in a way that there are no mutually supported ciphers.
> 

My client is curl.. I don't think that its caching any TLS sessions.

> 
> It may just be your ssl_bump rules. But given that this is a google
> domain there is a strong chance that you are encountering one of those
> special case.
>
I'd like squid to disallow queries where it cannot see what domain name
/ url is going to be accessed.

I'd like all GET/POST etc. requests to go through squid - so they are
controlled by the normal http_access rules as http (intercepted) is
currently.

This worked with 3.4.12 :( (but only for 30 minutes or less)

You saw my full config.. how is it supposed to look with 3.5.5, for this
to work as it did with 3.4.12 ?

sorry I'm a bit frustrated.. I can't seem to grasp what changed from
3.4.12 to 3.5.5, which means I suddenly can't filter https traffic
anymore :(

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
  --Henry Spencer



From tarotapprentice at yahoo.com  Wed Jun 10 00:35:24 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 9 Jun 2015 17:35:24 -0700
Subject: [squid-users] Recommended multi-worker setup?
Message-ID: <1433896524.95987.YahooMailMobile@web120806.mail.ne1.yahoo.com>

In the examples on the squid site it gives a multi-worker example using carp (http://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster). Now that rock storage has been updated with 3.5.5 is that still the best approach?

I was thinking of a single rock cache so the workers could share it rather than the example which has a shared rock cache plus each worker having its own aufs cache. I need to cache windows updates which can get fairly big and would rather not duplicate them between workers.

Cheers,
MarkJ
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150609/0dfe01e6/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 10 00:57:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 12:57:25 +1200
Subject: [squid-users] Recommended multi-worker setup?
In-Reply-To: <1433896524.95987.YahooMailMobile@web120806.mail.ne1.yahoo.com>
References: <1433896524.95987.YahooMailMobile@web120806.mail.ne1.yahoo.com>
Message-ID: <55778B75.6000803@treenet.co.nz>

On 10/06/2015 12:35 p.m., TarotApprentice wrote:
> In the examples on the squid site it gives a multi-worker example using carp (http://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster). Now that rock storage has been updated with 3.5.5 is that still the best approach?
> 
> I was thinking of a single rock cache so the workers could share it rather than the example which has a shared rock cache plus each worker having its own aufs cache. I need to cache windows updates which can get fairly big and would rather not duplicate them between workers.
> 

It is still far better to use UFS caches for large objects.

Amos



From squid3 at treenet.co.nz  Wed Jun 10 02:07:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jun 2015 14:07:21 +1200
Subject: [squid-users] Upload issue with squid 3.5.5
In-Reply-To: <1481A36A-A8A6-4E43-A757-9732D334F57E@yahoo.com>
References: <557705BC.3010109@treenet.co.nz>
 <2108619432.22697.1433895056668.JavaMail.yahoo@mail.yahoo.com>
 <55778CC2.3040704@treenet.co.nz>
 <1481A36A-A8A6-4E43-A757-9732D334F57E@yahoo.com>
Message-ID: <55779BD9.1040807@treenet.co.nz>

On 10/06/2015 1:11 p.m., TarotApprentice wrote:
> Yes I noticed that and assumed that was because 2.7 wasn't able to handle HTTP 1.1 fully.
> 
> I think I better keep the squid 2.7 machine around for a bit. It was due to be retired as it's an old WinXP machine.
> 

Maybe not.

I took a look through the BOINC code and found that your version will
only output "HTTP/1.0" like I see in your logs if it is forced to.

>From the BOINC client release notes:
"
David  19 Dec 2006
 - core client: add "<http_1_0/>" config flag for
                 people with proxies that require HTTP 1.0.
                 Curl's default is 1.1
"

Please try removing that config option from the boinc-client config file
for the Squid-3 traffic.

Amos



From alex at samad.com.au  Wed Jun 10 03:59:03 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 10 Jun 2015 13:59:03 +1000
Subject: [squid-users] High-availability and load-balancing between N
 squid servers
In-Reply-To: <5576E167.2090806@treenet.co.nz>
References: <DB5PR04MB1128EB4550FB52189F4C523C8FBF0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576079B.1030203@gmail.com> <55761FED.4030900@treenet.co.nz>
 <DB5PR04MB11288E5C59069436EF4702D58FBE0@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <5576E167.2090806@treenet.co.nz>
Message-ID: <CAJ+Q1PW0BOCjdePbUYkMbBXUd1Lg_gNdnAGuqNUoiU56AsDrww@mail.gmail.com>

Hi

I run 2 squid boxes, and I use pacemaker to float 2 VIP's between the 2 boxes.

Basically I just run squid on both and I create a VIP resource that
test if squid is running to allocate the VIP.

But this doesn't really give you load balancing. but very good resilience.


Pacemaker and Linux have the ability to do load balancing, by using a
share IP and some hashing algo , I haven't tested it though



On 9 June 2015 at 22:51, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 9/06/2015 7:15 p.m., Rafael Akchurin wrote:
>> Hi Amos,
>>
>> <snip>
>>
>>> There seems to be a bit of a myth going around about how HAProxy does
>>> load balancing. HAProxy is an HTTP layer proxy. Just like Squid.
>>>
>>> They both do the same things to received TCP connections. But HAProxy
>>> supports less HTTP features, so its somewhat simpler processing is also
>>> a bit faster when you want it to be a semi-dumb load balancer.
>>
>>> We are somewhat recently added basic support for the PROXY protocol to Squid.
>>> So HAProxy can relay port 80 connections to Squid-3.5+ without
>>> processing them fully. However Squid does not yet support that on
>>> https_port, which means the TLS connections still wont have client IP
>>> details passed through.
>>
>> So what would be your proposition for the case of SSL Bump?
>> How to get the connecting client IP and authenticated user name passed to the ICAP server when a cluster of squids somehow getting the CONNECT tunnel established?
>>
>> Assume we left away the haproxy and rely solely on squid - how would you approach this and how many instances of squid would you deploy?
>>
>> From my limited knowledge the FQDN proxy name being resolved to a number of IP addresses running one squid per IP address is the simplest approach.
>>
>
> Yes, it would seem to be the only form which meets all your criteria
> too. Everything else runs up against the HTTPS brick wall.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From grusha at flywild.net  Wed Jun 10 04:46:25 2015
From: grusha at flywild.net (dkandle)
Date: Tue, 9 Jun 2015 21:46:25 -0700 (PDT)
Subject: [squid-users] Installing certificate on Andriod to use with SSL-bump
Message-ID: <1433911585718-4671645.post@n4.nabble.com>

I would like to be able to inspect traffic from my android device. I have a
transparent squid proxy working with SSL bump (using WiFi to get traffic
through my proxy server). Everything works fine as long as I go through a
browser. But I would like to see the other traffic which the OS and other
apps are sending. Squid uses a certificate I generated for the web sites and
I create an exception for those without issue.
If I install my certificate on the phone will it then accept the certificate
when squid returns it during the ssl setup? To be clear, I see the phone use
port 443 to setup a secure session. However it rejects the certificate (as
it should) and terminates the session with no data being passed. I can
install my certificate on the phone, but will the android OS use that
certificate for all services or only for browser sessions? If not, is there
some other way I can get my fake certificate accepted for all sessions for
which it is used?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Installing-certificate-on-Andriod-to-use-with-SSL-bump-tp4671645.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Frank.Diercks at lbm.rlp.de  Wed Jun 10 09:39:09 2015
From: Frank.Diercks at lbm.rlp.de (Diercks, Frank (VRZ Koblenz))
Date: Wed, 10 Jun 2015 09:39:09 +0000
Subject: [squid-users] Migration from squid 3.1.20 to 3.4.8
Message-ID: <776a9177abd047d8a2034eb16c88656a@exmbko02.lsv.intra>

Hallo squid-users,

i migrated our Proxy from 3.1.20 to 3.4.8. Here are the changes I made:

I commented out:
#acl manager proto cache_object
#acl localhost src 127.0.0.1/32 ::1
#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

And added the following entry:
http_port xxx.xxx.xxx.xxx.:3129 intercept (port 3128 was already without "intercept" configured).

Everything runs well, except an application which sends data via html to our internetserver.

The first entry is from squid 3.1.20, which runs without problems:
. .. TCP_MISS/200 317 POST http://xxx.xxx.xxx.xxx/SWIS-Web/TLSReceiver - DIRECT/xxx.xxx.xxx.xxx -

The second entry is from squid 3.4.8:
... TAG_NONE/400 3585 POST /SWIS-Web/TLSReceiver - HIER_NONE/- text/html

As you can see, squid 3.4.8 doesn't log the http-part. I have no idea why. The entries from cache.log and access.log are clean.
Every hint is welcome.

Regards
Frank
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/4fe81b91/attachment.htm>

From jlay at slave-tothe-box.net  Wed Jun 10 13:18:56 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 10 Jun 2015 07:18:56 -0600
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <557740D8.1080709@vsen.dk>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
 <5576E4C4.3000508@treenet.co.nz> <5576FD7D.4070109@vsen.dk>
 <557701EB.7010607@treenet.co.nz> <557740D8.1080709@vsen.dk>
Message-ID: <1433942336.3697.0.camel@JamesiMac>

On Tue, 2015-06-09 at 21:39 +0200, Klavs Klavsen wrote:

> Amos Jeffries wrote on 2015-06-09 17:10:
> [CUT]
> > You have to first configure ssl_bump in a way that lets Squid receive
> > the clientHello message (step1 -> peek) AND the serverHello message
> > (step2 -> peek). Then you can use those cert details to bump (step3 ->
> > bump).
> > The config is quite simple:
> >   ssl_bump peek all
> >   ssl_bump bump all
> > 
> I have this:
> ssl_bump peek step1 broken
> ssl_bump peek step2 broken
> ssl_bump splice broken
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump bump all
> 
> > 
> > But there are cases like the client is resuming a previous TLS session
> > where there is no certificates involved. Squid cannot do anything, so it
> > automatically splices (3.5.4+ at least do). Or if you have configured
> > your Squid in a way that there are no mutually supported ciphers.
> > 
> 
> My client is curl.. I don't think that its caching any TLS sessions.
> 
> > 
> > It may just be your ssl_bump rules. But given that this is a google
> > domain there is a strong chance that you are encountering one of those
> > special case.
> >
> I'd like squid to disallow queries where it cannot see what domain name
> / url is going to be accessed.
> 
> I'd like all GET/POST etc. requests to go through squid - so they are
> controlled by the normal http_access rules as http (intercepted) is
> currently.
> 
> This worked with 3.4.12 :( (but only for 30 minutes or less)
> 
> You saw my full config.. how is it supposed to look with 3.5.5, for this
> to work as it did with 3.4.12 ?
> 
> sorry I'm a bit frustrated.. I can't seem to grasp what changed from
> 3.4.12 to 3.5.5, which means I suddenly can't filter https traffic
> anymore :(
> 


Gents,

I'm going to spin this off into a new thread..."Filtering http and https
traffic" sometime later today.  I have some questions, and maybe
solutions.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/cff98f29/attachment.htm>

From mmonette at 2keys.ca  Wed Jun 10 14:25:21 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 10 Jun 2015 10:25:21 -0400 (EDT)
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <55664FF1.8080000@treenet.co.nz>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
 <5565FCE1.2010909@treenet.co.nz>
 <1178914605.1395397.1432751610933.JavaMail.zimbra@2keys.ca>
 <55664FF1.8080000@treenet.co.nz>
Message-ID: <1980764740.1709373.1433946321634.JavaMail.zimbra@2keys.ca>

Hi again,

I finally had some time to get back into this, been a busy couple weeks. I compiled squid with the "--with-openssl --enable-ssl-crtd" you mentioned, and now things seem to be working better with ssl::servername. But for some reason I can't get HTTPS traffic to get a cert from squid. All HTTPS traffic is getting their certificate from the real sites and I don't really know why because it's the same config as before.

Here's a small capture of the logs:

1433945978.888     95 10.117.67.157 TCP_MISS/302 694 GET http://a.tribalfusion.com/z/i.match? - HIER_DIRECT/204.11.109.68 text/html
1433945978.918    306 10.117.67.157 TCP_MISS/302 658 GET http://pixel.advertising.com/ups/50/sync? - HIER_DIRECT/149.174.67.72 -
1433945978.994     72 10.117.67.157 TCP_MISS/204 737 GET http://su.addthis.com/red/usync? - HIER_DIRECT/104.16.24.235 image/png
1433945979.147     65 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945979.152     58 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945979.972   1068 10.117.67.157 TCP_MISS/204 719 GET http://su.addthis.com/red/usync? - HIER_DIRECT/104.16.24.235 image/png
1433945981.527     50 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945981.753     52 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945982.006    100 10.117.67.157 TCP_MISS/200 546 GET http://www.google.ca/ads/user-lists/1072396910/? - HIER_DIRECT/216.254.140.45 text/html
1433945983.769     55 10.117.67.157 TCP_MISS/200 546 GET http://www.google.ca/ads/user-lists/1072396910/? - HIER_DIRECT/216.254.140.45 text/html


All the HTTPS traffic are just CONNECT's. I feel like I ran into this problem when I had been working on this a couple weeks and I was able to get myself out of it by messing with the bump steps, but I can't seem to figure it out this time(or I just can't remember). Hoping for some guidance or hints.

Here's my log again:

# cat /etc/squid/squid.conf
~
debug_options ALL,9

acl localnet src 10.0.0.0/8        # RFC1918 possible internal network
acl localnet src 172.16.0.0/12        # RFC1918 possible internal network
acl localnet src 192.168.0.0/16        # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80                # http
acl Safe_ports port 21                # ftp
acl Safe_ports port 443                # https
acl Safe_ports port 70                # gopher
acl Safe_ports port 210                # wais
acl Safe_ports port 1025-65535        # unregistered ports
acl Safe_ports port 280                # http-mgmt
acl Safe_ports port 488                # gss-http
acl Safe_ports port 591                # filemaker
acl Safe_ports port 777                # multiling http
acl CONNECT method CONNECT


http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump bump step2 all
ssl_bump bump step3 all

acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
#acl bl1 url_regex -i ^http(s)?://gmail.com
#acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
#acl bl3 url_regex -i ^http(s)?://moz.com.*
#acl bl4 url_regex -i moz.com
deny_info http://ask.com bl1 # I was testing redirecting stuff, but since the acl is not even picked up, this stuff is useless.
http_reply_access deny bl1 # useless
#http_access deny bl1 
#http_access deny bl1 CONNECT

http_access allow localnet
http_access allow localhost

http_access allow all

http_port 3128 accel vhost allow-direct

#https_port 3129 transparent ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

sslproxy_options NO_SSLv2
sslproxy_options NO_SSLv3

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

#cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid

refresh_pattern ^ftp:                1440        20%        10080
refresh_pattern ^gopher:        1440        0%        1440
refresh_pattern -i (/cgi-bin/|\?) 0        0%        0
refresh_pattern .                0        20%        4320


----- Original Message -----
From: "Amos Jeffries" <squid3 at treenet.co.nz>
To: "Michael Monette" <mmonette at 2keys.ca>
Cc: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, May 27, 2015 7:14:57 PM
Subject: Re: [squid-users] Squid, Gmail.com and HSTS.

On 28/05/2015 6:33 a.m., Michael Monette wrote:
> Yeah I don't know what I am doing wrong but I don't have these ACL types..Or I am somehow not copy & pasting properly:
> 
> FATAL: Invalid ACL type 'ssl::server_name'
> FATAL: Bungled /etc/squid/squid.conf line 54: acl nobumpsites ssl::server_name .google.com
> Squid Cache (Version 3.5.4): Terminated abnormally.
> CPU Usage: 0.005 seconds = 0.003 user + 0.002 sys
> Maximum Resident Size: 24096 KB
> Page faults with physical i/o: 0
> Squid restarted
> [root at ottt-corp-paz-squid-1 squid-3.5.4]# squid -v
> Squid Cache: Version 3.5.4
> Service Name: squid
> configure options:  '--prefix=/usr' '--includedir=/usr/include' '--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-included-ltdl' --enable-ltdl-convenience
> 

You are missing the --with-openssl --enable-ssl-crtd options on this build.

Amos


From mmonette at 2keys.ca  Wed Jun 10 14:42:35 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 10 Jun 2015 10:42:35 -0400 (EDT)
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <1980764740.1709373.1433946321634.JavaMail.zimbra@2keys.ca>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
 <5565FCE1.2010909@treenet.co.nz>
 <1178914605.1395397.1432751610933.JavaMail.zimbra@2keys.ca>
 <55664FF1.8080000@treenet.co.nz>
 <1980764740.1709373.1433946321634.JavaMail.zimbra@2keys.ca>
Message-ID: <350113947.1709836.1433947355703.JavaMail.zimbra@2keys.ca>

Sorry for the noise - I figured it out. 

HTTPS was completely dead which made me wonder if squid was working properly. It turns out I had some folder permission issues. I needed to chmod -R 777 /var/lib/ssl_db. I guess lack of permissions to that directory caused cert generation to fail and HTTPS to break..Thanks for reading

----- Original Message -----
From: "Michael Monette" <mmonette at 2keys.ca>
To: "Amos Jeffries" <squid3 at treenet.co.nz>
Cc: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, June 10, 2015 10:25:21 AM
Subject: Re: [squid-users] Squid, Gmail.com and HSTS.

Hi again,

I finally had some time to get back into this, been a busy couple weeks. I compiled squid with the "--with-openssl --enable-ssl-crtd" you mentioned, and now things seem to be working better with ssl::servername. But for some reason I can't get HTTPS traffic to get a cert from squid. All HTTPS traffic is getting their certificate from the real sites and I don't really know why because it's the same config as before.

Here's a small capture of the logs:

1433945978.888     95 10.117.67.157 TCP_MISS/302 694 GET http://a.tribalfusion.com/z/i.match? - HIER_DIRECT/204.11.109.68 text/html
1433945978.918    306 10.117.67.157 TCP_MISS/302 658 GET http://pixel.advertising.com/ups/50/sync? - HIER_DIRECT/149.174.67.72 -
1433945978.994     72 10.117.67.157 TCP_MISS/204 737 GET http://su.addthis.com/red/usync? - HIER_DIRECT/104.16.24.235 image/png
1433945979.147     65 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945979.152     58 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945979.972   1068 10.117.67.157 TCP_MISS/204 719 GET http://su.addthis.com/red/usync? - HIER_DIRECT/104.16.24.235 image/png
1433945981.527     50 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945981.753     52 10.117.67.157 TAG_NONE/200 0 CONNECT 104.236.7.74:443 - ORIGINAL_DST/104.236.7.74 -
1433945982.006    100 10.117.67.157 TCP_MISS/200 546 GET http://www.google.ca/ads/user-lists/1072396910/? - HIER_DIRECT/216.254.140.45 text/html
1433945983.769     55 10.117.67.157 TCP_MISS/200 546 GET http://www.google.ca/ads/user-lists/1072396910/? - HIER_DIRECT/216.254.140.45 text/html


All the HTTPS traffic are just CONNECT's. I feel like I ran into this problem when I had been working on this a couple weeks and I was able to get myself out of it by messing with the bump steps, but I can't seem to figure it out this time(or I just can't remember). Hoping for some guidance or hints.

Here's my log again:

# cat /etc/squid/squid.conf
~
debug_options ALL,9

acl localnet src 10.0.0.0/8        # RFC1918 possible internal network
acl localnet src 172.16.0.0/12        # RFC1918 possible internal network
acl localnet src 192.168.0.0/16        # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80                # http
acl Safe_ports port 21                # ftp
acl Safe_ports port 443                # https
acl Safe_ports port 70                # gopher
acl Safe_ports port 210                # wais
acl Safe_ports port 1025-65535        # unregistered ports
acl Safe_ports port 280                # http-mgmt
acl Safe_ports port 488                # gss-http
acl Safe_ports port 591                # filemaker
acl Safe_ports port 777                # multiling http
acl CONNECT method CONNECT


http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump bump step2 all
ssl_bump bump step3 all

acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
#acl bl1 url_regex -i ^http(s)?://gmail.com
#acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
#acl bl3 url_regex -i ^http(s)?://moz.com.*
#acl bl4 url_regex -i moz.com
deny_info http://ask.com bl1 # I was testing redirecting stuff, but since the acl is not even picked up, this stuff is useless.
http_reply_access deny bl1 # useless
#http_access deny bl1 
#http_access deny bl1 CONNECT

http_access allow localnet
http_access allow localhost

http_access allow all

http_port 3128 accel vhost allow-direct

#https_port 3129 transparent ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

sslproxy_options NO_SSLv2
sslproxy_options NO_SSLv3

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

#cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid

refresh_pattern ^ftp:                1440        20%        10080
refresh_pattern ^gopher:        1440        0%        1440
refresh_pattern -i (/cgi-bin/|\?) 0        0%        0
refresh_pattern .                0        20%        4320


----- Original Message -----
From: "Amos Jeffries" <squid3 at treenet.co.nz>
To: "Michael Monette" <mmonette at 2keys.ca>
Cc: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, May 27, 2015 7:14:57 PM
Subject: Re: [squid-users] Squid, Gmail.com and HSTS.

On 28/05/2015 6:33 a.m., Michael Monette wrote:
> Yeah I don't know what I am doing wrong but I don't have these ACL types..Or I am somehow not copy & pasting properly:
> 
> FATAL: Invalid ACL type 'ssl::server_name'
> FATAL: Bungled /etc/squid/squid.conf line 54: acl nobumpsites ssl::server_name .google.com
> Squid Cache (Version 3.5.4): Terminated abnormally.
> CPU Usage: 0.005 seconds = 0.003 user + 0.002 sys
> Maximum Resident Size: 24096 KB
> Page faults with physical i/o: 0
> Squid restarted
> [root at ottt-corp-paz-squid-1 squid-3.5.4]# squid -v
> Squid Cache: Version 3.5.4
> Service Name: squid
> configure options:  '--prefix=/usr' '--includedir=/usr/include' '--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-included-ltdl' --enable-ltdl-convenience
> 

You are missing the --with-openssl --enable-ssl-crtd options on this build.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From leolistas at solutti.com.br  Wed Jun 10 14:45:31 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 10 Jun 2015 11:45:31 -0300
Subject: [squid-users] Migration from squid 3.1.20 to 3.4.8
In-Reply-To: <776a9177abd047d8a2034eb16c88656a@exmbko02.lsv.intra>
References: <776a9177abd047d8a2034eb16c88656a@exmbko02.lsv.intra>
Message-ID: <55784D8B.2020004@solutti.com.br>

On 10/06/15 06:39, Diercks, Frank (VRZ Koblenz) wrote:
>
> Hallo squid-users,
>
> i migrated our Proxy from 3.1.20 to 3.4.8. Here are the changes I made:
>
>

     why going to 3.4 if it's already 'old' code ? Why not going 
straight to 3.5 which is the current release ?


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/6b6d13f4/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 10 15:12:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 03:12:58 +1200
Subject: [squid-users] Migration from squid 3.1.20 to 3.4.8
In-Reply-To: <776a9177abd047d8a2034eb16c88656a@exmbko02.lsv.intra>
References: <776a9177abd047d8a2034eb16c88656a@exmbko02.lsv.intra>
Message-ID: <557853FA.2030405@treenet.co.nz>

On 10/06/2015 9:39 p.m., Diercks, Frank (VRZ Koblenz) wrote:
> Hallo squid-users,
> 
> i migrated our Proxy from 3.1.20 to 3.4.8. Here are the changes I made:
> 
> I commented out:
> #acl manager proto cache_object
> #acl localhost src 127.0.0.1/32 ::1
> #acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> 
> And added the following entry:
> http_port xxx.xxx.xxx.xxx.:3129 intercept (port 3128 was already without "intercept" configured).
> 
> Everything runs well, except an application which sends data via html to our internetserver.
> 
> The first entry is from squid 3.1.20, which runs without problems:
> . .. TCP_MISS/200 317 POST http://xxx.xxx.xxx.xxx/SWIS-Web/TLSReceiver - DIRECT/xxx.xxx.xxx.xxx -
> 
> The second entry is from squid 3.4.8:
> ... TAG_NONE/400 3585 POST /SWIS-Web/TLSReceiver - HIER_NONE/- text/html
> 
> As you can see, squid 3.4.8 doesn't log the http-part. I have no idea why. The entries from cache.log and access.log are clean.
> Every hint is welcome.

Looks like it is being intercepted. But does it contain a Host: header?

Try "debug_options 11,2" to see in cache.log what the messages are.

Amos


From squid3 at treenet.co.nz  Wed Jun 10 16:11:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 04:11:11 +1200
Subject: [squid-users] assertion failed: Read.cc:69:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <CAEnCSG5b=U=WnSH+-7QTL=G1WZCu+czAJtUSy05fVAv8m8VP2g@mail.gmail.com>
References: <CAEnCSG5b=U=WnSH+-7QTL=G1WZCu+czAJtUSy05fVAv8m8VP2g@mail.gmail.com>
Message-ID: <5578619F.4050107@treenet.co.nz>

On 10/06/2015 5:24 a.m., Michael Pelletier wrote:
> Hello,
> 
> I am getting these errors on 3.5.5 any ideas? Here is my build configuration
> 

Please try with the latest 3.5 snapshot. There is a pinning related
patch there which may help.

Amos



From squid3 at treenet.co.nz  Wed Jun 10 16:22:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 04:22:51 +1200
Subject: [squid-users] Installing certificate on Andriod to use with
	SSL-bump
In-Reply-To: <1433911585718-4671645.post@n4.nabble.com>
References: <1433911585718-4671645.post@n4.nabble.com>
Message-ID: <5578645B.8070909@treenet.co.nz>

On 10/06/2015 4:46 p.m., dkandle wrote:
> I would like to be able to inspect traffic from my android device. I have a
> transparent squid proxy working with SSL bump (using WiFi to get traffic
> through my proxy server). Everything works fine as long as I go through a
> browser. But I would like to see the other traffic which the OS and other
> apps are sending. Squid uses a certificate I generated for the web sites and
> I create an exception for those without issue.
> If I install my certificate on the phone will it then accept the certificate
> when squid returns it during the ssl setup?

Maybe.

> To be clear, I see the phone use
> port 443 to setup a secure session. However it rejects the certificate (as
> it should) and terminates the session with no data being passed. I can
> install my certificate on the phone, but will the android OS use that
> certificate for all services or only for browser sessions?

Maybe.

> If not, is there
> some other way I can get my fake certificate accepted for all sessions for
> which it is used?

Only by adding the CA cert your Squid signs with to the OS certificate
set. Whether it is actually used from there is application specific and
none of us have control over that.

Amos


From jlay at slave-tothe-box.net  Wed Jun 10 16:28:58 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 10 Jun 2015 10:28:58 -0600
Subject: [squid-users] Installing certificate on Andriod to use with
 SSL-bump
In-Reply-To: <5578645B.8070909@treenet.co.nz>
References: <1433911585718-4671645.post@n4.nabble.com>
 <5578645B.8070909@treenet.co.nz>
Message-ID: <6ecc03925aba91ecbcfa73209474e5fa@localhost>

On 2015-06-10 10:22 AM, Amos Jeffries wrote:
> On 10/06/2015 4:46 p.m., dkandle wrote:
>> I would like to be able to inspect traffic from my android device. I 
>> have a
>> transparent squid proxy working with SSL bump (using WiFi to get 
>> traffic
>> through my proxy server). Everything works fine as long as I go 
>> through a
>> browser. But I would like to see the other traffic which the OS and 
>> other
>> apps are sending. Squid uses a certificate I generated for the web 
>> sites and
>> I create an exception for those without issue.
>> If I install my certificate on the phone will it then accept the 
>> certificate
>> when squid returns it during the ssl setup?
> 
> Maybe.
> 
>> To be clear, I see the phone use
>> port 443 to setup a secure session. However it rejects the certificate 
>> (as
>> it should) and terminates the session with no data being passed. I can
>> install my certificate on the phone, but will the android OS use that
>> certificate for all services or only for browser sessions?
> 
> Maybe.
> 
>> If not, is there
>> some other way I can get my fake certificate accepted for all sessions 
>> for
>> which it is used?
> 
> Only by adding the CA cert your Squid signs with to the OS certificate
> set. Whether it is actually used from there is application specific and
> none of us have control over that.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

What kinda device?  I've put my ca cert on a couple Android 
devices...ranging from just email the cert and import all the way to 
cracking open a certificate .db file and inserting.

James


From tolga.cengiz at hctbilisim.com  Wed Jun 10 16:46:38 2015
From: tolga.cengiz at hctbilisim.com (tolga.cengiz at hctbilisim.com)
Date: Wed, 10 Jun 2015 19:46:38 +0300
Subject: [squid-users] Installing certificate on Andriod to use with
 SSL-bump
In-Reply-To: <6ecc03925aba91ecbcfa73209474e5fa@localhost>
References: <1433911585718-4671645.post@n4.nabble.com>
 <5578645B.8070909@treenet.co.nz>
 <6ecc03925aba91ecbcfa73209474e5fa@localhost>
Message-ID: <cd9f4cf1868a3aa3325b19a2233f59e1@hctbilisim.com>

2015-06-10 19:28, James Lay yazm??:
> On 2015-06-10 10:22 AM, Amos Jeffries wrote:
>> On 10/06/2015 4:46 p.m., dkandle wrote:
>>> I would like to be able to inspect traffic from my android device. I 
>>> have a
>>> transparent squid proxy working with SSL bump (using WiFi to get 
>>> traffic
>>> through my proxy server). Everything works fine as long as I go 
>>> through a
>>> browser. But I would like to see the other traffic which the OS and 
>>> other
>>> apps are sending. Squid uses a certificate I generated for the web 
>>> sites and
>>> I create an exception for those without issue.
>>> If I install my certificate on the phone will it then accept the 
>>> certificate
>>> when squid returns it during the ssl setup?
>> 
>> Maybe.
>> 
>>> To be clear, I see the phone use
>>> port 443 to setup a secure session. However it rejects the 
>>> certificate (as
>>> it should) and terminates the session with no data being passed. I 
>>> can
>>> install my certificate on the phone, but will the android OS use that
>>> certificate for all services or only for browser sessions?
>> 
>> Maybe.
>> 
>>> If not, is there
>>> some other way I can get my fake certificate accepted for all 
>>> sessions for
>>> which it is used?
>> 
>> Only by adding the CA cert your Squid signs with to the OS certificate
>> set. Whether it is actually used from there is application specific 
>> and
>> none of us have control over that.
>> 
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> What kinda device?  I've put my ca cert on a couple Android
> devices...ranging from just email the cert and import all the way to
> cracking open a certificate .db file and inserting.
> 
> James
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Thu Jun 11 02:50:56 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Wed, 10 Jun 2015 19:50:56 -0700
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
 <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
 <557552A9.4050603@treenet.co.nz> 
Message-ID: <001701d0a3f1$723758a0$56a609e0$@netstream.ps>


Amos , it worked great from other paid proxy 
Can you help plz ?

I used proxy 	  186.93.127.34:8080

And it worked !!

Can you assit me plz ?

Subject: RE: [squid-users] TCP_MISS/403 353 HEAD text/plain Error help !!

Hi Amos thanks for explanation

But the issue is it works fine from other paid proxies .

Again , the app is a link converter and it connect to proxy I will give u sample how app works :

It request link as below:
http://convertlink-bla-bla-bla.com/ytd/Youtube.class.php?url=https://www.youtube.com/watch?v=MYSVMgRr6pw&proxy=xxxx:yyyy


so the xxx:yyyy is the proxy ip:port

again 	Amos , since its outside squid and it works on other proxies , do u think we can do anything on our squid ?

any other ideas??

cheers

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Monday, June 8, 2015 1:31 AM
To: snakeeyes; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP_MISS/403 353 HEAD text/plain Error help !!

On 8/06/2015 5:36 a.m., snakeeyes wrote:
> Hi Amos thank you so much
> Again , this App IS REMOTE and as a black box .
> It works 100  % on other proxies but I need to let it work on my own proxy.
> 
> Now what I did is :
> I added 2  directives to my squid.conf :
> 
> strip_query_terms off
> debug_options 11,2 28,3
> 
> 
> then restarted squid and monitored the logs .
> 
> again I have 2 files monitored when it worked and when it failed.
> 
> But the strange that the problem is it work on some youtube vidoes and 
> don?t work on the others
> 
> those files were just as an example :
> file 1 didn?t work and give error 403 ===> 
> https://www.youtube.com/watch?v=MYSVMgRr6pw
> 
> and other file worked ===>
> https://www.youtube.com/watch?v=p0g9_osImd0
> 
> now I will test the app with those links , note that that 1sst line will fail and  the 2nd Link will success.
> 
> I had graped the log files andf attached them because they are big 
> Name for failed file is =>debug_failed.txt Name for succeded file 
> ==>debug_worked.txt
> 
> Thanks a lot
> 
> cheers

Your Squid is letting all of both types of traffic through, and it appears not to be caching the results.

That is good in a way. It means the problem is clearly something between the browser and Google servers, not Squid.

I can see several differences between the client requests. The working ones have a line or so more query parameters than the forbidden ones.
They are also consistently going to a different server (working *.12 , failing *.14).

Amos



From sebag at vianetcon.com.ar  Wed Jun 10 17:16:21 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 10 Jun 2015 14:16:21 -0300
Subject: [squid-users] Noticeable difference in DNS Service times after
	upgrade
Message-ID: <557870E5.9060303@vianetcon.com.ar>

Hello everyone, I just have a quick question
Is there any difference in how Squid 3.5 measures DNS Service Time 
compared to 2.7 branch?
We monitor this value using SNMP and it has been nearly 0 for months, 
but after the upgrade it went up to 6ms (with 8ms peaks)
All other Service times have varied but apparently they improved


Here's the graphic: http://imgur.com/4pCK3cY


From michael.pelletier at palmbeachschools.org  Wed Jun 10 17:26:26 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 10 Jun 2015 13:26:26 -0400
Subject: [squid-users] assertion failed: Read.cc:69:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <5578619F.4050107@treenet.co.nz>
References: <CAEnCSG5b=U=WnSH+-7QTL=G1WZCu+czAJtUSy05fVAv8m8VP2g@mail.gmail.com>
 <5578619F.4050107@treenet.co.nz>
Message-ID: <CAEnCSG6ZhRukQ6WfTx52ZBAu-6_ERH3izjUaygn6MUnrqgBBtA@mail.gmail.com>

OK. I went back to 3.4.13 for prod. I will try upgrading one proxy this
weekend.

On Wed, Jun 10, 2015 at 12:11 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 10/06/2015 5:24 a.m., Michael Pelletier wrote:
> > Hello,
> >
> > I am getting these errors on 3.5.5 any ideas? Here is my build
> configuration
> >
>
> Please try with the latest 3.5 snapshot. There is a pinning related
> patch there which may help.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/5c03fc0e/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 10 17:27:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 05:27:42 +1200
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <001701d0a3f1$723758a0$56a609e0$@netstream.ps>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
 <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
 <557552A9.4050603@treenet.co.nz>
 <001701d0a3f1$723758a0$56a609e0$@netstream.ps>
Message-ID: <5578738E.3070801@treenet.co.nz>

On 11/06/2015 2:50 p.m., snakeeyes wrote:
> 
> Amos , it worked great from other paid proxy 
> Can you help plz ?
> 
> I used proxy 	  186.93.127.34:8080
> 
> And it worked !!
> 
> Can you assit me plz ?
> 

I cant with the data available sorry. You will have to find out what
that other proxy is doing differently.

Amos



From jonathan.filogna at tasso.com.ar  Wed Jun 10 17:39:06 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Wed, 10 Jun 2015 14:39:06 -0300
Subject: [squid-users] spotify blocked by squid
Message-ID: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>

Hi all, it's me  again, just a simple question

I've configured an squid 2.7 with ntlm auth and i want to let some AD users
to listen spotify

My problem is that spotify streaming is being blocked by squid to this
group and idk why. Maybe another syntax problem?

here's my squid.conf


###########################SQUID.CONF

visible_hostname prana

auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
auth_param ntlm keep_alive on


external_acl_type ntlm_group ttl=3600 children=100 %LOGIN /usr/lib/squid/
wbinfo_group.pl


acl porno url_regex -i "/etc/squid/listas/porno.lst"
acl permitidos dstdomain -i "/etc/squid/listas/permitidos.lst"
acl directo url_regex -i "/etc/squid/listas/direct.lst"
acl vidyaud rep_mime_type -i "/etc/squid/listas/blockstr.lst"
acl useragent browser -i "/etc/squid/blockejec/browser.lst"
acl blockstr req_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
acl blockejec url_regex -i "/etc/squid/blockejec/blockejec.lst"
acl audyvid req_mime_type -i "/etc/squid/listas/blockstr.lst"
acl blockstr2 rep_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
acl destinolimitado dstdomain -i "/etc/squid/listas/limitado.lst"

acl all src all
acl CONNECT method CONNECT
acl manager proto cache_object
acl webserver src 192.168.8.121/255.255.255.255
http_access allow manager webserver
http_reply_access allow manager webserver
http_access deny manager

http_access deny porno all
http_reply_access deny porno all
acl uservipstr external ntlm_group "/etc/squid/listas/uservipstr.lst"

http_access deny blockejec uservipstr

http_access allow uservipstr
http_reply_access allow uservipstr

http_access deny blockstr !uservipstr all
http_reply_access deny blockstr !uservipstr all
http_access deny blockstr2 !uservipstr all
http_reply_access deny blockstr2 !uservipstr all
http_access deny audyvid !uservipstr all
http_access deny vidyaud !uservipstr all
http_reply_access deny audyvid !uservipstr all
http_reply_access deny vidyaud !uservipstr all

acl SSL_ports port 443 # https
acl SSL_ports port 563 # snews
acl SSL_ports port 873 # rsync
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 631 # cups
acl Safe_ports port 873 # rsync
acl Safe_ports port 901 # SWAT
acl Safe_ports port 78 69 #Spotify



# Deny requests to unknown ports
#http_access allow Safe_ports
http_access deny !Safe_ports
# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

acl ntlm proxy_auth REQUIRED
http_access allow ntlm
http_reply_access allow ntlm
http_access deny all
http_reply_access deny all

###########

thank you all

-- 
Jonathan Filogna
It Senior
Tasso SRL
4702 1910
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/a7eaf10d/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 10 17:39:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 05:39:51 +1200
Subject: [squid-users] Noticeable difference in DNS Service times after
 upgrade
In-Reply-To: <557870E5.9060303@vianetcon.com.ar>
References: <557870E5.9060303@vianetcon.com.ar>
Message-ID: <55787667.40304@treenet.co.nz>

On 11/06/2015 5:16 a.m., Sebastian Goicochea wrote:
> Hello everyone, I just have a quick question
> Is there any difference in how Squid 3.5 measures DNS Service Time
> compared to 2.7 branch?
> We monitor this value using SNMP and it has been nearly 0 for months,
> but after the upgrade it went up to 6ms (with 8ms peaks)
> All other Service times have varied but apparently they improved
> 
> 
> Here's the graphic: http://imgur.com/4pCK3cY


Not that I know of, but there is both more and less being done.

The "more":

* IPv6 support adds an AAAA lookup.

* CVE-2009-0801 protection for intercepted traffic requires A+AAAA
lookup in situations which may previously not done any at all.

* hosts without FQDN may now have a .local variant looked up.

The "less":

* re-sends of HTTP messages no longer perform full lookups on each
outbound connection attempt. Just one set per message now.

* .local domains no longer have any global DNS lookups. multicast-DNS is
used instead.

Amos



From jonathan.filogna at tasso.com.ar  Wed Jun 10 17:44:39 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Wed, 10 Jun 2015 14:44:39 -0300
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
Message-ID: <CAPnpmpnKFoOHs+Spoti2DLJJn37hFO3+sJFDpGW7SAOK427OUw@mail.gmail.com>

FYI access.log

1433958220.321    227 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4370 proxyvipstr DIRECT/127.0.0.1 -
1433958220.421      2 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4371 proxyvipstr DIRECT/127.0.0.1 -
1433958220.595      3 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4372 proxyvipstr DIRECT/127.0.0.1 -
1433958220.664      2 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4373 proxyvipstr DIRECT/127.0.0.1 -
1433958220.795      2 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4374 proxyvipstr DIRECT/127.0.0.1 -
1433958220.812      1 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4375 proxyvipstr DIRECT/127.0.0.1 -
1433958220.824      2 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4376 proxyvipstr DIRECT/127.0.0.1 -
1433958220.838      1 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4377 proxyvipstr DIRECT/127.0.0.1 -
1433958220.853      1 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4378 proxyvipstr DIRECT/127.0.0.1 -
1433958220.877      3 192.168.27.81 TCP_MISS/504 0 CONNECT
wevhbpyvhx.spotilocal.com:4379 proxyvipstr DIRECT/127.0.0.1 -


2015-06-10 14:39 GMT-03:00 Jonathan Filogna <jonathan.filogna at tasso.com.ar>:

> Hi all, it's me  again, just a simple question
>
> I've configured an squid 2.7 with ntlm auth and i want to let some AD
> users to listen spotify
>
> My problem is that spotify streaming is being blocked by squid to this
> group and idk why. Maybe another syntax problem?
>
> here's my squid.conf
>
>
> ###########################SQUID.CONF
>
> visible_hostname prana
>
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
> auth_param ntlm keep_alive on
>
>
> external_acl_type ntlm_group ttl=3600 children=100 %LOGIN /usr/lib/squid/
> wbinfo_group.pl
>
>
> acl porno url_regex -i "/etc/squid/listas/porno.lst"
> acl permitidos dstdomain -i "/etc/squid/listas/permitidos.lst"
> acl directo url_regex -i "/etc/squid/listas/direct.lst"
> acl vidyaud rep_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl useragent browser -i "/etc/squid/blockejec/browser.lst"
> acl blockstr req_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl blockejec url_regex -i "/etc/squid/blockejec/blockejec.lst"
> acl audyvid req_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl blockstr2 rep_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl destinolimitado dstdomain -i "/etc/squid/listas/limitado.lst"
>
> acl all src all
> acl CONNECT method CONNECT
> acl manager proto cache_object
> acl webserver src 192.168.8.121/255.255.255.255
> http_access allow manager webserver
> http_reply_access allow manager webserver
> http_access deny manager
>
> http_access deny porno all
> http_reply_access deny porno all
> acl uservipstr external ntlm_group "/etc/squid/listas/uservipstr.lst"
>
> http_access deny blockejec uservipstr
>
> http_access allow uservipstr
> http_reply_access allow uservipstr
>
> http_access deny blockstr !uservipstr all
> http_reply_access deny blockstr !uservipstr all
> http_access deny blockstr2 !uservipstr all
> http_reply_access deny blockstr2 !uservipstr all
> http_access deny audyvid !uservipstr all
> http_access deny vidyaud !uservipstr all
> http_reply_access deny audyvid !uservipstr all
> http_reply_access deny vidyaud !uservipstr all
>
> acl SSL_ports port 443 # https
> acl SSL_ports port 563 # snews
> acl SSL_ports port 873 # rsync
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl Safe_ports port 631 # cups
> acl Safe_ports port 873 # rsync
> acl Safe_ports port 901 # SWAT
> acl Safe_ports port 78 69 #Spotify
>
>
>
> # Deny requests to unknown ports
> #http_access allow Safe_ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports
>
> acl ntlm proxy_auth REQUIRED
> http_access allow ntlm
> http_reply_access allow ntlm
> http_access deny all
> http_reply_access deny all
>
> ###########
>
> thank you all
>
> --
> Jonathan Filogna
> It Senior
> Tasso SRL
> 4702 1910
>



-- 
Jonathan Filogna
It Senior
Tasso SRL
4702 1910
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/0f37e3bc/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 10 18:02:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 06:02:45 +1200
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
Message-ID: <55787BC5.3070102@treenet.co.nz>

On 11/06/2015 5:39 a.m., Jonathan Filogna wrote:
> Hi all, it's me  again, just a simple question
> 
> I've configured an squid 2.7 with ntlm auth and i want to let some AD users
> to listen spotify
> 
> My problem is that spotify streaming is being blocked by squid to this
> group and idk why. Maybe another syntax problem?

Some possibilities:

* 2.7 has an old bug where CONNECT requests could drop the first few
bytes of a connection if they were received in the same packets as the
HTTP message itself. Modern uses of port 443 depends on that case working.

* NTLM severely violates many requirements of HTTP. The only way for it
to have half a chance of working with CONNECT is "auth_param ntlm
keep_alive off"

* 504 means the connection to upstream timed out. Could be both of the
above happening at once. So what should go to the server first didnt get
sent, nothing comes back as server waits, then


PS. Squid-2.7 and even NTLM are both more than 5 years since they
stopped receiving support. Please upgrade. The version difference to 3.5
is so great now that it may involve some time/pain but is well worth it.

Amos



From jonathan.filogna at tasso.com.ar  Wed Jun 10 18:39:34 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Wed, 10 Jun 2015 15:39:34 -0300
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <55787BC5.3070102@treenet.co.nz>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
 <55787BC5.3070102@treenet.co.nz>
Message-ID: <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>

Ty Amos.

one more question

if i run apt-get install squid3 on my debian server, i must change some
lines like http_body_reply. But i can conserve my old squid.conf right?

I meant, how can i upgrade succesfully?
 should i start the installation from scratch?
This server's almost on production but this bug is a real pain.

Jonathan.

2015-06-10 15:02 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 11/06/2015 5:39 a.m., Jonathan Filogna wrote:
> > Hi all, it's me  again, just a simple question
> >
> > I've configured an squid 2.7 with ntlm auth and i want to let some AD
> users
> > to listen spotify
> >
> > My problem is that spotify streaming is being blocked by squid to this
> > group and idk why. Maybe another syntax problem?
>
> Some possibilities:
>
> * 2.7 has an old bug where CONNECT requests could drop the first few
> bytes of a connection if they were received in the same packets as the
> HTTP message itself. Modern uses of port 443 depends on that case working.
>
> * NTLM severely violates many requirements of HTTP. The only way for it
> to have half a chance of working with CONNECT is "auth_param ntlm
> keep_alive off"
>
> * 504 means the connection to upstream timed out. Could be both of the
> above happening at once. So what should go to the server first didnt get
> sent, nothing comes back as server waits, then
>
>
> PS. Squid-2.7 and even NTLM are both more than 5 years since they
> stopped receiving support. Please upgrade. The version difference to 3.5
> is so great now that it may involve some time/pain but is well worth it.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Jonathan Filogna
It Senior
Tasso SRL
4702 1910
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/c3c7f3fb/attachment.htm>

From jonathan.filogna at tasso.com.ar  Wed Jun 10 18:48:33 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Wed, 10 Jun 2015 15:48:33 -0300
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>
 <55787BC5.3070102@treenet.co.nz>
 <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>
Message-ID: <CAPnpmp=DWEdCQ5AeMxHNPuc1mCR8cK_juBD1r7Z3jNoa5O_-+A@mail.gmail.com>

where saids http_body_reply should said reply_body_max_size


i'm so tired right now...i apologize

Jonathan

2015-06-10 15:39 GMT-03:00 Jonathan Filogna <jonathan.filogna at tasso.com.ar>:

> Ty Amos.
>
> one more question
>
> if i run apt-get install squid3 on my debian server, i must change some
> lines like http_body_reply. But i can conserve my old squid.conf right?
>
> I meant, how can i upgrade succesfully?
>  should i start the installation from scratch?
> This server's almost on production but this bug is a real pain.
>
> Jonathan.
>
> 2015-06-10 15:02 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 11/06/2015 5:39 a.m., Jonathan Filogna wrote:
>> > Hi all, it's me  again, just a simple question
>> >
>> > I've configured an squid 2.7 with ntlm auth and i want to let some AD
>> users
>> > to listen spotify
>> >
>> > My problem is that spotify streaming is being blocked by squid to this
>> > group and idk why. Maybe another syntax problem?
>>
>> Some possibilities:
>>
>> * 2.7 has an old bug where CONNECT requests could drop the first few
>> bytes of a connection if they were received in the same packets as the
>> HTTP message itself. Modern uses of port 443 depends on that case working.
>>
>> * NTLM severely violates many requirements of HTTP. The only way for it
>> to have half a chance of working with CONNECT is "auth_param ntlm
>> keep_alive off"
>>
>> * 504 means the connection to upstream timed out. Could be both of the
>> above happening at once. So what should go to the server first didnt get
>> sent, nothing comes back as server waits, then
>>
>>
>> PS. Squid-2.7 and even NTLM are both more than 5 years since they
>> stopped receiving support. Please upgrade. The version difference to 3.5
>> is so great now that it may involve some time/pain but is well worth it.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
> Jonathan Filogna
> It Senior
> Tasso SRL
> 4702 1910
>



-- 
Jonathan Filogna
It Senior
Tasso SRL
4702 1910
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/4bd64d79/attachment.htm>

From jbuda at noticiasargentinas.com  Wed Jun 10 19:00:59 2015
From: jbuda at noticiasargentinas.com (Jose Julian Buda)
Date: Wed, 10 Jun 2015 16:00:59 -0300
Subject: [squid-users] squidGuard configuration test - echo test
In-Reply-To: <DISXrzjeCXB@helmut.hullen.de>
References: <DISXrzjeCXB@helmut.hullen.de>
Message-ID: <5578896B.3010501@noticiasargentinas.com>



On 08/06/15 08:10, Helmut Hullen wrote:
> Hallo, Amos,
>
> Du meintest am 08.06.15:
>
>>> Under squid 3.4 (and many earlier versions) I use
>>>
>>>          url_rewrite_program /usr/bin/squidGuard
>>>
>>> How must I change this line for squid 3.5?
>
>> You should not have to change the SG command line or configuration.
>
> Ok!
>
>> Whats needed is a patch from
>> <http://bugs.squid-cache.org/show_bug.cgi?id=3978>  to be applied to
>> SGitself. If you are using an OS provided SG binary check to see if
>> theyhave already patched it.
>
> It's not patched in my version, but it works under squid 3.4.10 -
> strange.
>
>>> The above page mentions
>>>
>>>     http://www.eu.squid-cache.org/Doc/config/url_rewrite_extras
>>>
>>> but  this page doesn't yet exist.
>
> [...]
>
>> That should be:
>
>>    <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>
>> and
>>    <http://www.squid-cache.org/Doc/config/url_rewrite_program/>
>
> Ok - now I can read the pages!
>
> Viele Gruesse!
> Helmut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Thank you all, squidGuard 1.5.4 from Debian's repository, work fine on 
DebianJessie's Squid 3.4,  i've made it work yesterday, and it does it good.
"ERR" from squidGuard means "Do not change the URL", and let squid pass 
the request.

Thank you for your time.

Julian


From jbuda at noticiasargentinas.com  Wed Jun 10 19:01:57 2015
From: jbuda at noticiasargentinas.com (Jose Julian Buda)
Date: Wed, 10 Jun 2015 16:01:57 -0300
Subject: [squid-users] squidGuard configuration test - echo test [SOLVED]
In-Reply-To: <DISXrzjeCXB@helmut.hullen.de>
References: <DISXrzjeCXB@helmut.hullen.de>
Message-ID: <557889A5.2070608@noticiasargentinas.com>



On 08/06/15 08:10, Helmut Hullen wrote:
> Hallo, Amos,
>
> Du meintest am 08.06.15:
>
>>> Under squid 3.4 (and many earlier versions) I use
>>>
>>>          url_rewrite_program /usr/bin/squidGuard
>>>
>>> How must I change this line for squid 3.5?
>
>> You should not have to change the SG command line or configuration.
>
> Ok!
>
>> Whats needed is a patch from
>> <http://bugs.squid-cache.org/show_bug.cgi?id=3978>  to be applied to
>> SGitself. If you are using an OS provided SG binary check to see if
>> theyhave already patched it.
>
> It's not patched in my version, but it works under squid 3.4.10 -
> strange.
>
>>> The above page mentions
>>>
>>>     http://www.eu.squid-cache.org/Doc/config/url_rewrite_extras
>>>
>>> but  this page doesn't yet exist.
>
> [...]
>
>> That should be:
>
>>    <http://www.squid-cache.org/Doc/config/url_rewrite_extras/>
>> and
>>    <http://www.squid-cache.org/Doc/config/url_rewrite_program/>
>
> Ok - now I can read the pages!
>
> Viele Gruesse!
> Helmut
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thank you all, squidGuard 1.5.4 from Debian's repository, work fine on 
DebianJessie's Squid 3.4,  i've made it work yesterday, and it does it good.
"ERR" from squidGuard means "Do not change the URL", and let squid pass 
the request.

Thank you for your time.

Julian


From squid3 at treenet.co.nz  Wed Jun 10 19:25:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2015 07:25:39 +1200
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>	<55787BC5.3070102@treenet.co.nz>
 <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>
Message-ID: <55788F33.6000408@treenet.co.nz>

On 11/06/2015 6:39 a.m., Jonathan Filogna wrote:
> Ty Amos.
> 
> one more question
> 
> if i run apt-get install squid3 on my debian server, i must change some
> lines like http_body_reply. But i can conserve my old squid.conf right?
> 

Yes. The squid3 package will currently install a whole separate set of
binaries and directories so your 'squid' install is all still in place
and working. You can copy the old config or not, up to you. If you do,
use "squid3 -k parse" to see what minimally needs changing for it to run.

It happens that I'm working through in the process of making a new squid
package that auto-upgrades from 2.7 to 3.5. So I am interested in
hearing about any ERROR/FATAL messages -k parse produces on your old config.

Amos



From jonathan.filogna at tasso.com.ar  Wed Jun 10 19:35:24 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Wed, 10 Jun 2015 16:35:24 -0300
Subject: [squid-users] spotify blocked by squid
In-Reply-To: <55788F33.6000408@treenet.co.nz>
References: <CAPnpmpmpS7rp=7xZmCb-TwL9dnTcL_1tQc7NSxB5jh08sOsPZQ@mail.gmail.com>	<55787BC5.3070102@treenet.co.nz>
 <CAPnpmp=RgJaZJ3xDHzFQpqGVo3QT3rYaHoYTvNTb2qsPNcYJgg@mail.gmail.com>
 <55788F33.6000408@treenet.co.nz>
Message-ID: <5578917C.7030007@tasso.com.ar>

i'll glad to sent you those errors

Amos, thank you so much for your attention and participation.

Jonathan
El 10/06/15 a las 16:25, Amos Jeffries escibi?:
> On 11/06/2015 6:39 a.m., Jonathan Filogna wrote:
>> Ty Amos.
>>
>> one more question
>>
>> if i run apt-get install squid3 on my debian server, i must change some
>> lines like http_body_reply. But i can conserve my old squid.conf right?
>>
> Yes. The squid3 package will currently install a whole separate set of
> binaries and directories so your 'squid' install is all still in place
> and working. You can copy the old config or not, up to you. If you do,
> use "squid3 -k parse" to see what minimally needs changing for it to run.
>
> It happens that I'm working through in the process of making a new squid
> package that auto-upgrades from 2.7 to 3.5. So I am interested in
> hearing about any ERROR/FATAL messages -k parse produces on your old config.
>
> Amos
>



From marcel at guineanet.net  Wed Jun 10 20:55:02 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Wed, 10 Jun 2015 13:55:02 -0700 (PDT)
Subject: [squid-users] cgi-bin
Message-ID: <1433969702604-4671670.post@n4.nabble.com>

Hi Mate 
I have this set on my squid.conf 
but seems that this is obsolete so how can nicely convert that for that
version is true that log suggest 
always_direct 

hierarchy_stoplist cgi-bin ? .js .jsp
acl QUERY urlpath_regex cgi-bin \? .js .jsp
no_cache deny QUERY

2015/06/10 20:53:42| ERROR: Directive 'hierarchy_stoplist' is obsolete.
2015/06/10 20:53:42| hierarchy_stoplist : Remove this line. Use
always_direct or cache_peer_access ACLs instead if you need to prevent
cache_peer use.

just to confirm this is the right way ??
always_direct cgi-bin ? .js .jsp
acl QUERY urlpath_regex cgi-bin \? .js .jsp
no_cache deny QUERY


Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cgi-bin-tp4671670.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ahmed.zaeem at netstream.ps  Thu Jun 11 08:33:43 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 11 Jun 2015 01:33:43 -0700
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <5578738E.3070801@treenet.co.nz>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
 <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>
 <557552A9.4050603@treenet.co.nz>
 <001701d0a3f1$723758a0$56a609e0$@netstream.ps>
 <5578738E.3070801@treenet.co.nz>
Message-ID: <000d01d0a421$5457a350$fd06e9f0$@netstream.ps>

Amos ,
Do u want me do for you more  debug ??


thankx

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Wednesday, June 10, 2015 10:28 AM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP_MISS/403 353 HEAD text/plain Error help !!

On 11/06/2015 2:50 p.m., snakeeyes wrote:
> 
> Amos , it worked great from other paid proxy Can you help plz ?
> 
> I used proxy 	  186.93.127.34:8080
> 
> And it worked !!
> 
> Can you assit me plz ?
> 

I cant with the data available sorry. You will have to find out what that other proxy is doing differently.

Amos



From jlay at slave-tothe-box.net  Thu Jun 11 00:16:36 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 10 Jun 2015 18:16:36 -0600
Subject: [squid-users] Quick peek-splice clarification
Message-ID: <1433981796.4674.2.camel@JamesiMac>

All,

>From the docs at:

http://wiki.squid-cache.org/Features/SslPeekAndSplice

peek


step1, step2


Receive SNI and client
certificate (step1), or
server certificate
(step2) while preserving
the possibility of
splicing the connection.
Peeking at the server
certificate usually
precludes future bumping
of the connection (see
Limitations). This
action is the focus of
this project.


stare


step1, step2


Receive SNI and client
certificate (step1), or
server certificate
(step2) while preserving
the possibility of
bumping the connection.
Staring at the server
certificate usually
precludes future
splicing of the
connection. Currently,
we are not aware of any
work being done to
support this action.



I see a lot of:

ssl_bump peek all

Does this perform both step1 with SNI and client cert, AND server cert?
Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150610/28f13d62/attachment.htm>

From sachixakechi at gmail.com  Thu Jun 11 00:23:07 2015
From: sachixakechi at gmail.com (Stacy Yeh)
Date: Wed, 10 Jun 2015 17:23:07 -0700
Subject: [squid-users] Question about patch for CVE-2014-7141 and -7142
Message-ID: <CAM3vcPc179a=YBGpbZ1OKL0jSFDy=pYLAxQzN_Azukt99T5+bw@mail.gmail.com>

Hello,

I am attempting to patch the security issues from CVE-2014-7141 and
CVE-2014-7142 for Squid 3.1.23 using the 3.1 patch provided here:
http://www.squid-cache.org/Advisories/SQUID-2014_4.txt

However, I am running into the following error:

    /builds/sachi/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:
    In member function 'virtual void Icmp4::Recv()':
    /builds/sachi/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:203:9:
    error: 'Ip' has not been declared
            Ip::Address::FreeAddrInfo(from);
            ^
    /builds/sachi/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:261:9:
    error: 'Ip' has not been declared
            Ip::Address::FreeAddrInfo(from);
            ^
    make[4]: *** [Icmp4.o] Error 1
    make[4]: Leaving directory
    '/builds/sachi/squid-component/components/squid/build/amd64/src/icmp'
    make[3]: *** [all-recursive] Error 1
    make[3]: Leaving directory
    '/builds/sachi/squid-component/components/squid/build/amd64/src'
    make[2]: *** [all] Error 2
    make[2]: Leaving directory
    '/builds/sachi/squid-component/components/squid/build/amd64/src'
    make[1]: *** [all-recursive] Error 1
    make[1]: Leaving directory
    '/builds/sachi/squid-component/components/squid/build/amd64'
    gmake: ***
    [/builds/sachi/squid-component/components/squid/build/amd64/.built]
    Error 2

Can anyone help me resolve this?

Thank you,
Stacy


From yashvinder.edx at gmail.com  Thu Jun 11 04:47:00 2015
From: yashvinder.edx at gmail.com (yashvinder hooda)
Date: Thu, 11 Jun 2015 10:17:00 +0530
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission denied
Message-ID: <CAD8CJ-oXVBq5CfbOCJFGE3UFVnuv+ZdSurXG51h7fWv==31ZnA@mail.gmail.com>

Squid log says Permission denied for the file /etc/squid/mime.conf
While permission on it is....

-rwxrwxrwx    1 nobody   root         11364 May  9 15:40 mime.conf
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/c7fe13a1/attachment.htm>

From kl at vsen.dk  Thu Jun 11 07:13:17 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 11 Jun 2015 09:13:17 +0200
Subject: [squid-users] ssl_crtd breaks after short time
In-Reply-To: <1433942336.3697.0.camel@JamesiMac>
References: <556D6A5C.7020703@vsen.dk> <556DB1F1.1030309@treenet.co.nz>
 <556DB378.3020305@vsen.dk> <556DB941.3050804@treenet.co.nz>
 <556DC1C7.6050806@vsen.dk> <556DC652.5080005@treenet.co.nz>
 <55705082.2060501@vsen.dk> <557055C1.9080502@treenet.co.nz>
 <5570565F.4090609@vsen.dk> <55705E62.6060801@treenet.co.nz>
 <55706598.7070800@vsen.dk> <55707011.7070003@vsen.dk>
 <5570CEBB.1040607@treenet.co.nz> <55768B3D.3000302@vsen.dk>
 <5576E4C4.3000508@treenet.co.nz> <5576FD7D.4070109@vsen.dk>
 <557701EB.7010607@treenet.co.nz> <557740D8.1080709@vsen.dk>
 <1433942336.3697.0.camel@JamesiMac>
Message-ID: <5579350D.4060506@vsen.dk>

James Lay wrote on 06/10/2015 03:18 PM:
[CUT]
> I'm going to spin this off into a new thread..."Filtering http and https
> traffic" sometime later today.  I have some questions, and maybe solutions.
>
Much appreciated and much looked forward to.. hoping I can get what I 
had working with 3.4.12 - working with latest squid version.. :)

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From e2ia.ci at gmail.com  Thu Jun 11 12:28:46 2015
From: e2ia.ci at gmail.com (CIROBOTICS)
Date: Thu, 11 Jun 2015 12:28:46 +0000
Subject: [squid-users] How to mark trafic from squid and squidguard
Message-ID: <CAMhThxJvPmZ2R7qQLZggf0Hm4GPpzTkPdigaQp0m4Jhev32ZtQ@mail.gmail.com>

Hi dear is there a mean to mark trafic from squid/squidguard?
I'd like for example mark all trafic to pornsite instead of redirecting it
with squidguard.
regards.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/87e5e8c1/attachment.htm>

From Brian.Snyder at beavercreek.k12.oh.us  Thu Jun 11 13:38:41 2015
From: Brian.Snyder at beavercreek.k12.oh.us (Snyder, Brian)
Date: Thu, 11 Jun 2015 13:38:41 +0000
Subject: [squid-users] forwarded_for
Message-ID: <D2E66EB8081A7D4C86314759C29AAFE00DAE070F@BHSMSX03.bvrcrk.k12.oh.us>

Hello All,
I am running squid 3.3.8 on CentOS 7.1.
The kernel is 3.10.0-229.4.2.el7.x86_64.

I am having an issue where the forwarded_for directive is not working correctly in squid.conf. I initially started the server hiding the client IP using the "delete" setting. We have now changed direction and I wish to use the client IP instead of the proxy. However, no setting will change the server IP presented. I have even tried to reconfigure with the line completely removed from the config with no luck. Stopping the service does not help. Even a server reboot does nothing. I have thought about just doing a rebuild.

Any help would be appreciated.

Thanks,
Brian

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/23371290/attachment.htm>

From eliezer at ngtech.co.il  Thu Jun 11 15:10:30 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 11 Jun 2015 18:10:30 +0300
Subject: [squid-users] forwarded_for
In-Reply-To: <D2E66EB8081A7D4C86314759C29AAFE00DAE070F@BHSMSX03.bvrcrk.k12.oh.us>
References: <D2E66EB8081A7D4C86314759C29AAFE00DAE070F@BHSMSX03.bvrcrk.k12.oh.us>
Message-ID: <5579A4E6.5070809@ngtech.co.il>

Hey Brian,

Can you test this issue with the 3.5.x or 3.4.x RPM's I released?
I have couple production servers running with 3.4 and 3.5 with 
"truncate" option to allow the backhand servers "see" the client IP.

Eliezer

* http://wiki.squid-cache.org/KnowledgeBase/CentOS

On 11/06/2015 16:38, Snyder, Brian wrote:
> Hello All,
> I am running squid 3.3.8 on CentOS 7.1.
> The kernel is 3.10.0-229.4.2.el7.x86_64.
>
> I am having an issue where the forwarded_for directive is not working correctly in squid.conf. I initially started the server hiding the client IP using the "delete" setting. We have now changed direction and I wish to use the client IP instead of the proxy. However, no setting will change the server IP presented. I have even tried to reconfigure with the line completely removed from the config with no luck. Stopping the service does not help. Even a server reboot does nothing. I have thought about just doing a rebuild.
>
> Any help would be appreciated.
>
> Thanks,
> Brian
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From jlay at slave-tothe-box.net  Thu Jun 11 16:27:40 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 11 Jun 2015 10:27:40 -0600
Subject: [squid-users] Properly filtering http and https traffic in a
 transparent proxy environment
Message-ID: <1434040060.3610.56.camel@JamesiMac>

Resending this with photobucket links instead of including images:

http://i290.photobucket.com/albums/ll269/DigiDemon/allowed.png
http://i290.photobucket.com/albums/ll269/DigiDemon/terminate.png


Hey All,

So....here's what I have for filtering http and https in the same
instance.  This is using iptables with -j REDIRECT lines.  Below is my
entire squid.conf, documented as well as I can:

#allow local network to connect to squid
acl localnet src 192.168.1.0/24

#safe ports are 80 and 443 in one acl, port 443 is another acl
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

#allow the http CONNECT method
acl CONNECT method CONNECT

#our regex list of sites and domains that we allow ie, www\.apple\.com
and \.google\.com
acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"

#we don't want to allow anything besides port 80 and port 443
http_access deny !Safe_ports

#we don't want CONNECT if we're not going to port 443
http_access deny CONNECT !SSL_Ports

#since we may not know the https site we're going to (ie connect direct
by IP), we must initially allow all https
http_access allow SSL_ports

#we allow http, but only sites and domains in our regex http_url.txt
list above
http_access allow allowed_http_sites

#drop any other http requests that are not in our regex http_url.txt
list above
http_access deny all

#break out the ssl_bump process by steps
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

#look for site or domain name either by SNI in request (step1), or
server subject in certificate in response (step2)
ssl_bump peek step1 all
ssl_bump peek step2 all

#see if the server name we obtained from the previous peek's above are
in our http_url.txt list above
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/http_url.txt"

#if the server name is in our http_url.txt, allow it 
ssl_bump splice step3 allowed_https_sites

#if the server name is not in our http_url.txt terminate the handshake
it
ssl_bump terminate all

#cert path and allow all the ssl options
sslproxy_capath /etc/ssl/certs
sslproxy_options ALL

#standard crtdaemon options
sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

#intercept 3128 for port 80, and 3129 for port 443.  Cert, cacert (these
are the same, read on the list this fixed an issue), and key, generate
ssl certs
http_port 3128 intercept
https_port 3129 intercept ssl-bump
cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE

#normal-ish log format, but we want to see the SNI, server cert subject,
and how we are bumping
logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %
ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode 

#log the above format name (mine) to syslog
access_log syslog:daemon.info mine

refresh_pattern -i (cgi-bin|\?) 0 0% 0
refresh_pattern . 0 20% 4320

coredump_dir /opt/var 

One caveat I've seen so far is that I have only seen is I only ever see
peek in the logs, though I know it's been spliced as shown below:

[09:35:19 jlay at gateway:/opt/etc/squid$] grep textnow http_url.txt 
api\.textnow\.me

[09:34:57 jlay at gateway:~$] dig api.textnow.me
<snip>
;; ANSWER SECTION:
api.textnow.me. 600 IN A 209.59.180.48

>From the client:
[09:34:27 jlay at analysis:~/dev/squid$] openssl s_client -connect
209.59.180.48:443
CONNECTED(00000003)
depth=1 C = US, ST = Arizona, L = Scottsdale, O = "GoDaddy.com, Inc.",
OU = http://certificates.godaddy.com/repository, CN = Go Daddy Secure
Certification Authority, serialNumber = 07969287
verify error:num=20:unable to get local issuer certificate
verify return:0
---
Certificate chain
0 s:/OU=Domain Control Validated/CN=*.textnow.me
<snip>

GET / HTTP/1.1

HTTP/1.1 200 OK
Server: nginx
Date: Thu, 11 Jun 2015 15:36:54 GMT
Content-Type: text/plain
Transfer-Encoding: chunked
Connection: keep-alive
X-Powered-By: PHP/5.5.16
Expires: Thu, 01 Jan 1970 00:00:00 GMT
Cache-Control: no-cache, no-store, must-revalidate, max-age=0

Server Squid log entry:
Jun 11 09:36:55 gateway (squid-1): 192.168.1.6 - - [11/Jun/2015:09:36:55
-0600] "CONNECT 209.59.180.48:443 HTTP/1.1" - - 200 364
TCP_TUNNEL:ORIGINAL_DST peek

I also notice that the CN does not show up in the logs...it WAS spliced
however because it matches our http_url.txt.  From the above, it appears
that only the step1 is getting logged.   The below log entry was used
with wget (sends SNI by default):
Jun 11 08:51:05 gateway squid: 192.168.1.6 - - [11/Jun/2015:08:51:05
-0600] "CONNECT 23.211.252.28:443 HTTP/1.1" www.apple.com - 200 14388
TCP_TUNNEL:ORIGINAL_DST peek

The above shows that I logged and retrieved my SNI at step1....the
subsequent splice was not logged.  I still note that terminates do not
get logged ( I have a bug open for that but I think the core bug may be
that when using atStep you only see step1 regardless, but I could be
wrong).  I'm enclosing an image of what a https terminate looks like
from the server and the client (msn.com isn't in the http_url.txt), and
also what an https allow looks like (apple.com is in the list).

That's it.  I can verify that the above works for a single list of
allowed hosts(www.apple.com) and domains (google.com).  If there's
something I missed...something wrong...ways to improve...ANYTHING for
the betterment of Squid users please don't hesitate to jump in here.
Thank you. 

James


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/63d56035/attachment.htm>

From stacy.yeh at oracle.com  Thu Jun 11 16:30:45 2015
From: stacy.yeh at oracle.com (Stacy Yeh)
Date: Thu, 11 Jun 2015 09:30:45 -0700 (PDT)
Subject: [squid-users] Patch for CVE-2014-7141 and CVE-2014-7142
Message-ID: <2ebfd7e4-8e38-45e2-961b-8fbdd70b8066@default>

Hello,

I am attempting to patch the security issues from CVE-2014-7141 and CVE-2014-7142 for Squid 3.1.23 using the 3.1 patch provided here:
http://www.squid-cache.org/Advisories/SQUID-2014_4.txt

I am running into the following error when I attempt to build Squid:
/builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc: In member function 'virtual void Icmp4::Recv()':
/builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:203:9: error: 'Ip' has not been declared
         Ip::Address::FreeAddrInfo(from);
         ^
/builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:261:9: error: 'Ip' has not been declared
         Ip::Address::FreeAddrInfo(from);
         ^
make[4]: *** [Icmp4.o] Error 1
make[4]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src/icmp'
make[3]: *** [all-recursive] Error 1
make[3]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src'
make[2]: *** [all] Error 2
make[2]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src'
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64'
gmake: *** [/builds/skyeh/squid-component/components/squid/build/amd64/.built] Error 2


Can anyone shed some light on how to fix this? 

Best,
Stacy Yeh


From tmblue at gmail.com  Thu Jun 11 18:29:46 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Jun 2015 11:29:46 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
Message-ID: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>

I've got logs and now finally a core (the whole "'squid' isn't signed with
proper key") thang took a bit to get around.

Rather not post the core here, is there a better place, not sure it's a bug
yet so opening a bug seems premature?

Thanks
Tory

3.5.0.2
CentOS

I've tried using the -N option and it doesn't seem to help, this appears to
be truly traffic driven.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/b91f80ff/attachment.htm>

From eliezer at ngtech.co.il  Thu Jun 11 19:25:19 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 11 Jun 2015 22:25:19 +0300
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
Message-ID: <5579E09F.7020205@ngtech.co.il>

What is the issue??
Did you tried the latest RPM's ??
http://wiki.squid-cache.org/KnowledgeBase/CentOS

Eliezer

On 11/06/2015 21:29, Tory M Blue wrote:
> I've got logs and now finally a core (the whole "'squid' isn't signed with
> proper key") thang took a bit to get around.
>
> Rather not post the core here, is there a better place, not sure it's a bug
> yet so opening a bug seems premature?
>
> Thanks
> Tory
>
> 3.5.0.2
> CentOS
>
> I've tried using the -N option and it doesn't seem to help, this appears to
> be truly traffic driven.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From acrow at integrafin.co.uk  Thu Jun 11 19:51:29 2015
From: acrow at integrafin.co.uk (Alex Crow)
Date: Thu, 11 Jun 2015 20:51:29 +0100
Subject: [squid-users] Centos7 rpms?
In-Reply-To: <5579E09F.7020205@ngtech.co.il>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
Message-ID: <5579E6C1.7060304@integrafin.co.uk>

On 11/06/15 20:25, Eliezer Croitoru wrote:
> What is the issue??
> Did you tried the latest RPM's ??
> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>
> Eliezer 

Hi,

Are there any plans to build centos/rhev7 packages? Native LVM caching 
on SSD is something that may well benefit Squid performance.

Cheers

Alex


From tmblue at gmail.com  Thu Jun 11 19:51:35 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Jun 2015 12:51:35 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <5579E09F.7020205@ngtech.co.il>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
Message-ID: <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>

On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> What is the issue??
> Did you tried the latest RPM's ??
> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>
> Eliezer
>
>
I spun my own as there were no precompiled versions when I started with
3.5.x. I'll look at your rpms and test those, but in a nutshell after
putting a server in production I get the following errors, where squid
seems to restart

*FATAL: Received Segment Violation...dying.*

2015/06/11 11:25:00 kid1| Closing HTTP port [::]:80

2015/06/11 11:25:00 kid1| storeDirWriteCleanLogs: Starting...

2015/06/11 11:25:00 kid1|     65536 entries written so far.

2015/06/11 11:25:00 kid1|    131072 entries written so far.

2015/06/11 11:25:00 kid1|    196608 entries written so far.

2015/06/11 11:25:00 kid1|   Finished.  Wrote 246863 entries.

2015/06/11 11:25:00 kid1|   Took 0.06 seconds (4364312.99 entries/sec).

CPU Usage: 40.481 seconds = 24.353 user + 16.128 sys

Maximum Resident Size: 345808 KB
Page faults with physical i/o: 0

Which results in a core

Jun 11 11:21:55 cache03 squid[23658]: Squid Parent: will start 1 kids

Jun 11 11:21:55 cache03 squid[23658]: Squid Parent: (squid-1) process 23660
started

Jun 11 11:25:01 cache03 abrt[23710]: Saved core dump of pid 23660
(/usr/sbin/squid) to /var/spool/abrt/ccpp-2015-06-11-11:25:00-23660
(94121984 bytes)

Jun 11 11:25:01 cache03 abrtd: Directory 'ccpp-2015-06-11-11:25:00-23660'
creation detected

Jun 11 11:25:01 cache03 squid[23658]: Squid Parent: (squid-1) process 23660
exited due to signal 6 with status 0

Jun 11 11:25:02 cache03 kernel: Bridge firewalling registered

Jun 11 11:25:04 cache03 squid[23658]: Squid Parent: (squid-1) process 23918
started

Jun 11 11:25:18 cache03 abrtd: New problem directory
/var/spool/abrt/ccpp-2015-06-11-11:25:00-23660, processing


This runs fine with a single proc box in a low volume environment. This
does not seem to work in a multi core server in a very busy environment.
I've read some things and it appears others have had issues and your
suggestions and others was to try to run with -N, that doesn't seem to
affect my issue and I get the FATAL's and core dumps still


I'lll try your rpms first

Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/74c0ec79/attachment.htm>

From tmblue at gmail.com  Thu Jun 11 21:48:11 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Jun 2015 14:48:11 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
 <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
Message-ID: <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>

On Thu, Jun 11, 2015 at 12:51 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
>
> On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
>> What is the issue??
>> Did you tried the latest RPM's ??
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>>
>> Eliezer
>>
>>
> I spun my own as there were no precompiled versions when I started with
> 3.5.x. I'll look at your rpms and test those, but in a nutshell after
> putting a server in production I get the following errors, where squid
> seems to restart
>
>
> Okay well 3.5.0.4 seems to make my production environment happy, 3.5.0.2
was not happy at all. Also luckily your rpm roll was on par with mine so it
allowed an in place update vs having to erase and replace configs etc. So
that's good.

my production cache has been running 5.2.0.4 on CentOS 6.6 for the last 40
minutes without a hiccup, it only took about 2 minutes for 3.5.0.2 to barf
up a lung

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/70d39378/attachment.htm>

From brian at interlinx.bc.ca  Thu Jun 11 21:56:12 2015
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Thu, 11 Jun 2015 17:56:12 -0400
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
Message-ID: <1434059772.16022.67.camel@interlinx.bc.ca>

At least from here, irc.bcwireless.net:6667 is non-functional
(connection times out) on IPv6 but works on IPv4:

# telnet irc.bcwireless.net 6667
Trying fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f...
telnet: connect to address fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f: Connection timed out
Trying 198.27.79.54...
Connected to irc.bcwireless.net (198.27.79.54).
Escape character is '^]'.
:wtf.bcwireless.net NOTICE AUTH :*** Looking up your hostname...
:wtf.bcwireless.net NOTICE AUTH :*** Checking ident...
:wtf.bcwireless.net NOTICE AUTH :*** Found your hostname
:wtf.bcwireless.net NOTICE AUTH :*** Received identd response

And while telnet is properly falling back to the IPv4 address, Squid
3.4.3 does not seem to be, despite:

http://readlist.com/lists/squid-cache.org/squid-users/11/58389.html

Trying to connect through squid returns a 503 and I can see on the
outgoing interface that no IPv4 connection attempt is made.

Is there possibly anything in my squid.conf that is preventing this IPv4
fallback from working?  I couldn't find anything that had to do with
that.

Cheers,
b.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/6bf23355/attachment.sig>

From squid3 at treenet.co.nz  Thu Jun 11 22:13:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 10:13:20 +1200
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <1434059772.16022.67.camel@interlinx.bc.ca>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
Message-ID: <557A0800.7040207@treenet.co.nz>

On 12/06/2015 9:56 a.m., Brian J. Murrell wrote:
> At least from here, irc.bcwireless.net:6667 is non-functional
> (connection times out) on IPv6 but works on IPv4:
> 
> # telnet irc.bcwireless.net 6667
> Trying fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f...
> telnet: connect to address fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f: Connection timed out
> Trying 198.27.79.54...
> Connected to irc.bcwireless.net (198.27.79.54).
> Escape character is '^]'.
> :wtf.bcwireless.net NOTICE AUTH :*** Looking up your hostname...
> :wtf.bcwireless.net NOTICE AUTH :*** Checking ident...
> :wtf.bcwireless.net NOTICE AUTH :*** Found your hostname
> :wtf.bcwireless.net NOTICE AUTH :*** Received identd response
> 
> And while telnet is properly falling back to the IPv4 address, Squid
> 3.4.3 does not seem to be, despite:
> 
> http://readlist.com/lists/squid-cache.org/squid-users/11/58389.html
> 

see <http://readlist.com/lists/squid-cache.org/squid-users/11/58405.html>


> Trying to connect through squid returns a 503 and I can see on the
> outgoing interface that no IPv4 connection attempt is made.
> 
> Is there possibly anything in my squid.conf that is preventing this IPv4
> fallback from working?  I couldn't find anything that had to do with
> that.
> 

details, details.
- You speak of a squid.conf but dont show any of its contents.
- You speak of 503 but show now trace or evidence from cache.log (or
even access.log) that might hint as a reason for it.

Amos



From squid3 at treenet.co.nz  Thu Jun 11 22:21:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 10:21:22 +1200
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
 <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
 <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>
Message-ID: <557A09E2.2090907@treenet.co.nz>

On 12/06/2015 9:48 a.m., Tory M Blue wrote:
> On Thu, Jun 11, 2015 at 12:51 PM, Tory M Blue <tmblue at gmail.com> wrote:
> 
>>
>>
>> On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
>> wrote:
>>
>>> What is the issue??
>>> Did you tried the latest RPM's ??
>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>>>
>>> Eliezer
>>>
>>>
>> I spun my own as there were no precompiled versions when I started with
>> 3.5.x. I'll look at your rpms and test those, but in a nutshell after
>> putting a server in production I get the following errors, where squid
>> seems to restart
>>
>>
>> Okay well 3.5.0.4 seems to make my production environment happy, 3.5.0.2
> was not happy at all. Also luckily your rpm roll was on par with mine so it
> allowed an in place update vs having to erase and replace configs etc. So
> that's good.
> 
> my production cache has been running 5.2.0.4 on CentOS 6.6 for the last 40
> minutes without a hiccup, it only took about 2 minutes for 3.5.0.2 to barf
> up a lung

Eh? 5.2.0.4 is not an existing Squid version.

Please ensure you are using the *3.5.5* packages (as of today thats the
latest). Squid-3.5 has had a bit of a volatile beginning.

Amos



From tmblue at gmail.com  Thu Jun 11 22:26:29 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Jun 2015 15:26:29 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <557A09E2.2090907@treenet.co.nz>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
 <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
 <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>
 <557A09E2.2090907@treenet.co.nz>
Message-ID: <CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>

On Thu, Jun 11, 2015 at 3:21 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 12/06/2015 9:48 a.m., Tory M Blue wrote:
> > On Thu, Jun 11, 2015 at 12:51 PM, Tory M Blue <tmblue at gmail.com> wrote:
> >
> >>
> >>
> >> On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <
> eliezer at ngtech.co.il>
> >> wrote:
> >>
> >>> What is the issue??
> >>> Did you tried the latest RPM's ??
> >>> http://wiki.squid-cache.org/KnowledgeBase/CentOS
> >>>
> >>> Eliezer
> >>>
> >>>
> >> I spun my own as there were no precompiled versions when I started with
> >> 3.5.x. I'll look at your rpms and test those, but in a nutshell after
> >> putting a server in production I get the following errors, where squid
> >> seems to restart
> >>
> >>
> >> Okay well 3.5.0.4 seems to make my production environment happy, 3.5.0.2
> > was not happy at all. Also luckily your rpm roll was on par with mine so
> it
> > allowed an in place update vs having to erase and replace configs etc. So
> > that's good.
> >
> > my production cache has been running 5.2.0.4 on CentOS 6.6 for the last
> 40
> > minutes without a hiccup, it only took about 2 minutes for 3.5.0.2 to
> barf
> > up a lung
>
> Eh? 5.2.0.4 is not an existing Squid version.
>
> Please ensure you are using the *3.5.5* packages (as of today thats the
> latest). Squid-3.5 has had a bit of a volatile beginning.
>
> Amos
>
>
OOPS sorry, head spinning 3.5.0.4..... I didn't see 3.5.0.5 up there.

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/45b85b2a/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 11 22:46:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 10:46:04 +1200
Subject: [squid-users] Patch for CVE-2014-7141 and CVE-2014-7142
In-Reply-To: <2ebfd7e4-8e38-45e2-961b-8fbdd70b8066@default>
References: <2ebfd7e4-8e38-45e2-961b-8fbdd70b8066@default>
Message-ID: <557A0FAC.80803@treenet.co.nz>

On 12/06/2015 4:30 a.m., Stacy Yeh wrote:
> Hello,
> 
> I am attempting to patch the security issues from CVE-2014-7141 and CVE-2014-7142 for Squid 3.1.23 using the 3.1 patch provided here:
> http://www.squid-cache.org/Advisories/SQUID-2014_4.txt
> 
> I am running into the following error when I attempt to build Squid:
> /builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc: In member function 'virtual void Icmp4::Recv()':
> /builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:203:9: error: 'Ip' has not been declared
>          Ip::Address::FreeAddrInfo(from);
>          ^
> /builds/skyeh/squid-component/components/squid/squid-3.1.23/src/icmp/Icmp4.cc:261:9: error: 'Ip' has not been declared
>          Ip::Address::FreeAddrInfo(from);
>          ^
> make[4]: *** [Icmp4.o] Error 1
> make[4]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src/icmp'
> make[3]: *** [all-recursive] Error 1
> make[3]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src'
> make[2]: *** [all] Error 2
> make[2]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64/src'
> make[1]: *** [all-recursive] Error 1
> make[1]: Leaving directory `/builds/skyeh/squid-component/components/squid/build/amd64'
> gmake: *** [/builds/skyeh/squid-component/components/squid/build/amd64/.built] Error 2
> 
> 
> Can anyone shed some light on how to fix this? 
> 

Gah! So much for the unit testing. Sorry about that. The class name is
IpAddress in 3.1, (no ::'s)

New patch and advisory update underway already and just waiting the
mirrors servers. New URL will be:
 <http://www.squid-cache.org/Versions/v3/3.1/changesets/SQUID-2014_4.patch>


PS. Are you able to upgrade away from 3.1? current supported series is
3.5 and much more capable in dealing with the current Internet.

Amos



From squid3 at treenet.co.nz  Thu Jun 11 22:56:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 10:56:10 +1200
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>	<5579E09F.7020205@ngtech.co.il>	<CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>	<CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>	<557A09E2.2090907@treenet.co.nz>
 <CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>
Message-ID: <557A120A.50203@treenet.co.nz>

On 12/06/2015 10:26 a.m., Tory M Blue wrote:
> On Thu, Jun 11, 2015 at 3:21 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 12/06/2015 9:48 a.m., Tory M Blue wrote:
>>> On Thu, Jun 11, 2015 at 12:51 PM, Tory M Blue <tmblue at gmail.com> wrote:
>>>
>>>>
>>>>
>>>> On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <
>> eliezer at ngtech.co.il>
>>>> wrote:
>>>>
>>>>> What is the issue??
>>>>> Did you tried the latest RPM's ??
>>>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>>>>>
>>>>> Eliezer
>>>>>
>>>>>
>>>> I spun my own as there were no precompiled versions when I started with
>>>> 3.5.x. I'll look at your rpms and test those, but in a nutshell after
>>>> putting a server in production I get the following errors, where squid
>>>> seems to restart
>>>>
>>>>
>>>> Okay well 3.5.0.4 seems to make my production environment happy, 3.5.0.2
>>> was not happy at all. Also luckily your rpm roll was on par with mine so
>> it
>>> allowed an in place update vs having to erase and replace configs etc. So
>>> that's good.
>>>
>>> my production cache has been running 5.2.0.4 on CentOS 6.6 for the last
>> 40
>>> minutes without a hiccup, it only took about 2 minutes for 3.5.0.2 to
>> barf
>>> up a lung
>>
>> Eh? 5.2.0.4 is not an existing Squid version.
>>
>> Please ensure you are using the *3.5.5* packages (as of today thats the
>> latest). Squid-3.5 has had a bit of a volatile beginning.
>>
>> Amos
>>
>>
> OOPS sorry, head spinning 3.5.0.4..... I didn't see 3.5.0.5 up there.

Should be no .0's. In Squid versions a .0 means beta-testing code.

As in:
 Squid version 3. feature set/series 5. release 5

versus:
 Squid version 3. feature set/series 5. release 0.4 (beta)


:-P

Amos


From tmblue at gmail.com  Thu Jun 11 23:07:01 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Jun 2015 16:07:01 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <557A120A.50203@treenet.co.nz>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
 <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
 <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>
 <557A09E2.2090907@treenet.co.nz>
 <CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>
 <557A120A.50203@treenet.co.nz>
Message-ID: <CAEaSS0bfEgUb87-xeOUM1MjUZLEtF+2hmnCfPux+Dnx2sd8KTQ@mail.gmail.com>

Okay well I took the RPM from your counter parts link and it gave me

http://wiki.squid-cache.org/KnowledgeBase/CentOS

squid-3.5.0.4-1.el6.x86_64.rpm


So am I using the wrong version?

Thank you sir! :)

Tory

On Thu, Jun 11, 2015 at 3:56 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 12/06/2015 10:26 a.m., Tory M Blue wrote:
> > On Thu, Jun 11, 2015 at 3:21 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> >
> >> On 12/06/2015 9:48 a.m., Tory M Blue wrote:
> >>> On Thu, Jun 11, 2015 at 12:51 PM, Tory M Blue <tmblue at gmail.com>
> wrote:
> >>>
> >>>>
> >>>>
> >>>> On Thu, Jun 11, 2015 at 12:25 PM, Eliezer Croitoru <
> >> eliezer at ngtech.co.il>
> >>>> wrote:
> >>>>
> >>>>> What is the issue??
> >>>>> Did you tried the latest RPM's ??
> >>>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS
> >>>>>
> >>>>> Eliezer
> >>>>>
> >>>>>
> >>>> I spun my own as there were no precompiled versions when I started
> with
> >>>> 3.5.x. I'll look at your rpms and test those, but in a nutshell after
> >>>> putting a server in production I get the following errors, where squid
> >>>> seems to restart
> >>>>
> >>>>
> >>>> Okay well 3.5.0.4 seems to make my production environment happy,
> 3.5.0.2
> >>> was not happy at all. Also luckily your rpm roll was on par with mine
> so
> >> it
> >>> allowed an in place update vs having to erase and replace configs etc.
> So
> >>> that's good.
> >>>
> >>> my production cache has been running 5.2.0.4 on CentOS 6.6 for the last
> >> 40
> >>> minutes without a hiccup, it only took about 2 minutes for 3.5.0.2 to
> >> barf
> >>> up a lung
> >>
> >> Eh? 5.2.0.4 is not an existing Squid version.
> >>
> >> Please ensure you are using the *3.5.5* packages (as of today thats the
> >> latest). Squid-3.5 has had a bit of a volatile beginning.
> >>
> >> Amos
> >>
> >>
> > OOPS sorry, head spinning 3.5.0.4..... I didn't see 3.5.0.5 up there.
>
> Should be no .0's. In Squid versions a .0 means beta-testing code.
>
> As in:
>  Squid version 3. feature set/series 5. release 5
>
> versus:
>  Squid version 3. feature set/series 5. release 0.4 (beta)
>
>
> :-P
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/de412eff/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 12 00:21:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 12:21:48 +1200
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0bfEgUb87-xeOUM1MjUZLEtF+2hmnCfPux+Dnx2sd8KTQ@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>	<5579E09F.7020205@ngtech.co.il>	<CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>	<CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>	<557A09E2.2090907@treenet.co.nz>	<CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>	<557A120A.50203@treenet.co.nz>
 <CAEaSS0bfEgUb87-xeOUM1MjUZLEtF+2hmnCfPux+Dnx2sd8KTQ@mail.gmail.com>
Message-ID: <557A261C.60308@treenet.co.nz>

On 12/06/2015 11:07 a.m., Tory M Blue wrote:
> Okay well I took the RPM from your counter parts link and it gave me
> 
> http://wiki.squid-cache.org/KnowledgeBase/CentOS
> 
> squid-3.5.0.4-1.el6.x86_64.rpm
> 
> 
> So am I using the wrong version?

Yeah, I see squid-3.5.5-1.el6.src.rpm at the bottom of the linked SRPM
downloads page and theres a matching squid-3.5.5-1.el6.x86_64.rpm in the
64-bit binaries dir.
 <http://www1.ngtech.co.il/rpm/centos/6/x86_64/squid-3.5.5-1.el6.x86_64.rpm>

Amos



From squid3 at treenet.co.nz  Fri Jun 12 00:46:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 12:46:07 +1200
Subject: [squid-users] forwarded_for
In-Reply-To: <D2E66EB8081A7D4C86314759C29AAFE00DAE070F@BHSMSX03.bvrcrk.k12.oh.us>
References: <D2E66EB8081A7D4C86314759C29AAFE00DAE070F@BHSMSX03.bvrcrk.k12.oh.us>
Message-ID: <557A2BCF.5030605@treenet.co.nz>

On 12/06/2015 1:38 a.m., Snyder, Brian wrote:
> Hello All,
> I am running squid 3.3.8 on CentOS 7.1.
> The kernel is 3.10.0-229.4.2.el7.x86_64.
> 

> I am having an issue where the forwarded_for directive is not
> working
correctly in squid.conf. I initially started the server hiding the
client IP using the "delete" setting. We have now changed direction and
I wish to use the client IP instead of the proxy. However, no setting
will change the server IP presented. I have even tried to reconfigure
with the line completely removed from the config with no luck. Stopping
the service does not help. Even a server reboot does nothing. I have
thought about just doing a rebuild.


Removing the directive entirely from your config should work.

Note that the recipient server must be interpreting the X-Forwarded-For
header *correctly* (as a reverse-path list of mixed IPv4, IPv6, and text
labels - not just a single IP). Also any other middleware between your
proxy and the server touching the header will affect the contents the
origin server receives.

Amos



From squid3 at treenet.co.nz  Fri Jun 12 00:48:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 12:48:06 +1200
Subject: [squid-users] How to mark trafic from squid and squidguard
In-Reply-To: <CAMhThxJvPmZ2R7qQLZggf0Hm4GPpzTkPdigaQp0m4Jhev32ZtQ@mail.gmail.com>
References: <CAMhThxJvPmZ2R7qQLZggf0Hm4GPpzTkPdigaQp0m4Jhev32ZtQ@mail.gmail.com>
Message-ID: <557A2C46.9050407@treenet.co.nz>

On 12/06/2015 12:28 a.m., CIROBOTICS wrote:
> Hi dear is there a mean to mark trafic from squid/squidguard?
> I'd like for example mark all trafic to pornsite instead of redirecting it
> with squidguard.

No. SG is too outdated for any of the Squid features that might allow
that (helper annotations combined with tcp_outgoing_tos).

Amos



From squid3 at treenet.co.nz  Fri Jun 12 01:36:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 13:36:41 +1200
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
 denied
In-Reply-To: <CAD8CJ-oXVBq5CfbOCJFGE3UFVnuv+ZdSurXG51h7fWv==31ZnA@mail.gmail.com>
References: <CAD8CJ-oXVBq5CfbOCJFGE3UFVnuv+ZdSurXG51h7fWv==31ZnA@mail.gmail.com>
Message-ID: <557A37A9.1010901@treenet.co.nz>

On 11/06/2015 4:47 p.m., yashvinder hooda wrote:
> Squid log says Permission denied for the file /etc/squid/mime.conf
> While permission on it is....
> 
> -rwxrwxrwx    1 nobody   root         11364 May  9 15:40 mime.conf
> 

And the SELinux or AppArmor permissions?

Amos


From squid3 at treenet.co.nz  Fri Jun 12 01:57:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 13:57:01 +1200
Subject: [squid-users] cgi-bin
In-Reply-To: <1433969702604-4671670.post@n4.nabble.com>
References: <1433969702604-4671670.post@n4.nabble.com>
Message-ID: <557A3C6D.70505@treenet.co.nz>

On 11/06/2015 8:55 a.m., Marcel Fossua wrote:
> Hi Mate 
> I have this set on my squid.conf 
> but seems that this is obsolete so how can nicely convert that for that
> version is true that log suggest 
> always_direct 
> 
> hierarchy_stoplist cgi-bin ? .js .jsp
> acl QUERY urlpath_regex cgi-bin \? .js .jsp
> no_cache deny QUERY

Youch.

> 
> 2015/06/10 20:53:42| ERROR: Directive 'hierarchy_stoplist' is obsolete.
> 2015/06/10 20:53:42| hierarchy_stoplist : Remove this line. Use
> always_direct or cache_peer_access ACLs instead if you need to prevent
> cache_peer use.
> 
> just to confirm this is the right way ??
> always_direct cgi-bin ? .js .jsp
> acl QUERY urlpath_regex cgi-bin \? .js .jsp
> no_cache deny QUERY


No. Those regex patterns are quite badly broken.

If I make a few assumtions about what those lines are supposed to means
I guess you are wanting this:

 acl QUERY urlpath_regex /cgi-bin/ \? \.jsp?
 cache deny QUERY
 always_direct allow QUERY


Although. I question what you think it does, and why you think you need
it at all.

Modern Squid are perfectly capable of caching dynamic content. And there
does not seems to be any obvious reason for avoiding requesting it
through a trusted peer proxy if other traffic is fine going there.

In other words, unless you have good reason for keeping it, you can
erase all the above lines. Just make sure you have the refresh_pattern
rule for safe storage of cgi-bin responses just above the '.'
refresh_pattern:
  refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

Amos



From yashvinder.edx at gmail.com  Fri Jun 12 05:03:02 2015
From: yashvinder.edx at gmail.com (yashvinder hooda)
Date: Fri, 12 Jun 2015 10:33:02 +0530
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
	denied
In-Reply-To: <557A37A9.1010901@treenet.co.nz>
References: <CAD8CJ-oXVBq5CfbOCJFGE3UFVnuv+ZdSurXG51h7fWv==31ZnA@mail.gmail.com>
 <557A37A9.1010901@treenet.co.nz>
Message-ID: <20150612050302.5304400.19828.108@gmail.com>

Hi,

Sir, I am ?using squid on openwrt , so I guess Selinux or AppArmor shouldn't be an issue there.

Regards,
Yashvinder


? Original Message ?
From: Amos Jeffries
Sent: Friday 12 June 2015 7:07 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission denied

On 11/06/2015 4:47 p.m., yashvinder hooda wrote:
> Squid log says Permission denied for the file /etc/squid/mime.conf
> While permission on it is....
> 
> -rwxrwxrwx 1 nobody root 11364 May 9 15:40 mime.conf
> 

And the SELinux or AppArmor permissions?

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From su.maji.ke at gmail.com  Fri Jun 12 05:39:25 2015
From: su.maji.ke at gmail.com (Iruma Keisuke)
Date: Fri, 12 Jun 2015 14:39:25 +0900
Subject: [squid-users] Error Resolution (TunnelStateData::Connection::
 error )
In-Reply-To: <557054EB.4020900@treenet.co.nz>
References: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>
 <556DB89E.1050305@treenet.co.nz>
 <CAKLXwH4Ov-YhY0C1vmbgpqDgDrxpt+4fdYNtRV2-KakAQg5Bqw@mail.gmail.com>
 <557054EB.4020900@treenet.co.nz>
Message-ID: <CAKLXwH5XfNB6ENkYN1MyCuJiDVRp-XN0JGXPN+mF7grSei-iXw@mail.gmail.com>

Thank you Amos.
I really appreciate your response.

We analyzed the trend of FD an error has occurred.

2015/06/01_08:52:35 nsu01pint-int01 [cache]2015/06/01 08:52:32|
TunnelStateData::Connection:: error : FD 81: read/write failure: (110)
Connection timed out
Active file descriptors:
 File Type   Tout Nread  * Nwrite * Remote Address        Description
  ---- ------ ---- -------- -------- ---------------------
--------------------- ---------
   81 Socket 86282    8100*   67977  XXX.XXXX.2.136:49907   Reading
next request Date: Sun, 31 May 2015 23:32:30 GMT
   81 Socket 86302    8100*   16753  XXX.XXXX.2.136:49944   Reading
next request Date: Sun, 31 May 2015 23:37:30 GMT
   81 Socket 86002    8100*   17395* XXX.XXXX.2.136:49944   Reading
next request Date: Sun, 31 May 2015 23:42:30 GMT
   81 Socket 85702    8100*   17395* XXX.XXXX.2.136:49944   Reading
next request Date: Sun, 31 May 2015 23:47:30 GMT
   81 Socket 85402    8100*   17395* XXX.XXXX.2.136:49944   Reading
next request Date: Sun, 31 May 2015 23:52:30 GMT
   81 Socket 86354   56810*   40697  XXX.XXXX.6.114:49687   Reading
next request Date: Sun, 31 May 2015 23:57:30 GMT
                                 ?
                         Error while writing ?

All it seems to have time out in writing.
And, the time until the timeout is between 10 to 15 minutes.

Are "Write_timeout" and "read_timeout" directive related to the error?
"write_timeout" is a directive that does not exist in the 3.1 version.
Though "write_timeout" can not be set, it works and cause a timeout in
15 minutes?

I think this is also a relationship.
http://www.squid-cache.org/Doc/config/half_closed_clients/
> Squid can not tell the difference between a half-closed connection, and a fully closed one.
3.1 version "half_closed_clients" is off by default.

I have this kind of guess.

1. Change the client to "half_closed".
2. squid write to FD.
3. Change the client to "fully_losed".
4. Since the "fully_losed" squid can not understand, squid attempts to
write to the FD.
5. After 15 minutes, "write_timeout" occurs in squid

Can I get your opinion?

2015-06-04 22:38 GMT+09:00, Amos Jeffries <squid3 at treenet.co.nz>:
> On 5/06/2015 1:18 a.m., Iruma Keisuke wrote:
>> Thank you Amos.
>>
>> 2015-06-02 23:07 GMT+09:00, Amos Jeffries <squid3 at treenet.co.nz>:
>>> On 2/06/2015 9:15 p.m., Irimajiri keisuke wrote:
>>>> Dear all,
>>>>
>>>> I have to build a proxy server by using the squid.
>>>> The number of clients is 400 people.
>>>>
>>>> I do not know the cause of the error message that appears in the
>>>> cache.log.
>>>> In the weekday, I have come up with an error every few hours 8:00 to
>>>> 18:00.
>>>> Access concentration I look like does not matter.
>>>>
>>>> [cache.log error message]
>>>> 2015/05/11 13:37:24| TunnelStateData::Connection:: error : FD 610:
>>>> read/write failure: (110) Connection timed out
>>>>
>>>> Why I want to know whether this error has occurred.
>>>
>>> Yes it has occured. You would not be seeing it otherwise.
>>>
>>>> Also, I want to know the impact on the user.
>>>
>>> The user who is causing the problem is probably not impacted at all.
>>> Every other user sharing the proxy is impacted by the reduction in
>>> available network socket, memory and CPU resources.
>>>
>> It seems to be no abnormality in the state of network sockets and
>> memory and CPU.
>> Is it safe to ignore this error?
>
> If you think your service is operating fine. It does mean the proxy has
> a lower threshold of tolerance for network congestion than normal.
>
>
>>>>
>>>> [squidclient mgr:filedescriptor]
>>>> Every five minutes record
>>>> extract FD610
>>>>
>>>> It looks like an error has occurred in the use to which the terminal
>>>> of xxx.xxx.2.115 user.
>>>> Is it a problem of communication of the user and the proxy?
>>>>
>>>
>>> Nothing happened on a TCP conection for a long time. It was closed by
>>> the networking sub-systems somewhere between Squid and the client.
>>>
>>
>> Do error is not out on the web browser?
>> Could you detailed information about TCP state and the state of the
>> user when an error has occurred.
>
> It might be, or it might not be. Others before you who noticed the same
> messages have a mixed set of reasons found for it. Some was Chrome
> browser happy eyeballs algorithm leaking its second connection until it
> got dropped by the network TCP stack. Some it was F5 load balancers
> breaking. Others it was NAT timeout in users home routers. Some have not
> bothered to dig deep so it may be other causes.
>
> All that is certain is that something between Squid and user is closing
> the connection while it is in an HTTP idle state.
>
>>
>>>> Active file descriptors:
>>>> File Type   Tout Nread  * Nwrite * Remote Address        Description
>>>> ---- ------ ---- -------- -------- ---------------------
>>>> ------------------------------
>>>>  610 Socket  893    39494*   50228  xxx.xxx.xxx.162:443
>>>> outlook.office365.com:443       2015/05/11_13:08:29
>>>>  610 Socket 86329   45754*  103329  xxx.xxx.6.141:50174   Reading next
>>>> request         2015/05/11_13:13:29
>>>>  610 Socket 86258    6516*   13975  xxx.xxx.2.115:50820   Reading next
>>>> request         2015/05/11_13:18:29
>>>>  610 Socket 85958   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>>> request         2015/05/11_13:23:29
>>>>  610 Socket 85657   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>>> request         2015/05/11_13:28:29
>>>>  610 Socket 85357   12472*   34531* xxx.xxx.2.115:50820   Reading next
>>>> request         2015/05/11_13:33:29
>>>>  610 Socket 86336    3652*    8003  xxx.xxx.3.152:50817   Reading next
>>>> request         2015/05/11_13:38:29
>>>>
>>>> [access.log]
>>>> I do not see suspicious error log I tried to extract the address
>>>> xxx.xxx.2.115.
>>>>
>>>> Please tell me a good idea toward someone solve.
>>>
>>> Please provided additional details:
>>>  * Squid version
>>>  * Squid configuration
>>>
>>>
>>> I suspect you have a quite old verion of Squid. That particular error
>>> message does not even exist in the code any more. The current releases
>>> display much more TCP details about the connection where the error
>>> occured.
>>
>> squid version is squid-3.1.10-29.
>> This is the latest version that RedHat is delivering.
>
> Ah, yes RHEL policy of not updating unless explicit bug reports exist
> and supporting things for 10 years+ >
>
> You may be interested in the official unofficial packages (accepted by
> the Squid Project and community as reasonable packages for use, but not
> RHEL official supported).
> <http://wiki.squid-cache.org/KnowledgeBase/RedHat>
>
>
>>
>> [squid.conf]
>> ------------------------------------
>> acl manager proto cache_object
>> acl localhost src 127.0.0.1/32 ::1
>> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
>> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>> plugged) machines
>> acl SSL_ports port 443
>> acl Safe_ports port 80		# http
>> acl Safe_ports port 21		# ftp
>> acl Safe_ports port 443		# https
>> acl Safe_ports port 70		# gopher
>> acl Safe_ports port 210		# wais
>> acl Safe_ports port 1025-65535	# unregistered ports
>> acl Safe_ports port 280		# http-mgmt
>> acl Safe_ports port 488		# gss-http
>> acl Safe_ports port 591		# filemaker
>> acl Safe_ports port 777		# multiling http
>> acl CONNECT method CONNECT
>> http_access allow manager localhost
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow all
>
> !!!! Open proxy !!!
>
>
>> http_port 192.168.1.1:8080
>> hierarchy_stoplist cgi-bin ?
>> coredump_dir /var/spool/squid
>> refresh_pattern ^ftp:		1440	20%	10080
>> refresh_pattern ^gopher:	1440	0%	1440
>> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
>> refresh_pattern .		0	20%	4320
>>
>> cache_mem 2048 MB
>> cache_store_log none
>> visible_hostname unknown
>> request_header_access X-FORWARDED-FOR deny all
>> request_header_access Via deny all
>> max_filedesc 10240
>> ipcache_size 10240
>> -----------------------------------------------
>>
>> Please let me ask further questions
>> Are these has to do with the error?
>> http://www.squid-cache.org/Doc/code/tunnel_8cc_source.html
>>
>> 472 TunnelStateData::Connection::error(int const xerrno)
>> 473 {
>> 474  /* XXX fixme xstrerror and xerrno... */
>> 475  errno = xerrno;
>> 476
>> 477  debugs(50, debugLevelForError(xerrno), HERE << conn << ":
>> read/write failure: " << xstrerror());
>> 478
>> 479  if (!ignoreErrno(xerrno))
>> 480  conn->close();
>> 481 }
>>
>> 536  /* Bump the dest connection read timeout on any activity */
>> 537  /* see Bug 3659: tunnels can be weird, with very long one-way
>> transfers */
>> 538  if (Comm::IsConnOpen(to.conn)) {
>> 539  AsyncCall::Pointer timeoutCall = commCbCall(5, 4, "tunnelTimeout",
>> 540  CommTimeoutCbPtrFun(tunnelTimeout, this));
>> 541  commSetConnTimeout(to.conn, Config.Timeout.read, timeoutCall);
>> 542  }
>> 543
>>
>
> Possibly, but predates all that codes existence.
>
> Amos
>
>


From fredbmail at free.fr  Fri Jun 12 09:25:26 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 12 Jun 2015 11:25:26 +0200 (CEST)
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13)
	Permission	denied
In-Reply-To: <20150612050302.5304400.19828.108@gmail.com>
Message-ID: <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> On 11/06/2015 4:47 p.m., yashvinder hooda wrote:
> > Squid log says Permission denied for the file /etc/squid/mime.conf
> > While permission on it is....
> > 
> > -rwxrwxrwx 1 nobody root 11364 May 9 15:40 mime.conf
> > 

And the squid directory permissions ?

user nobody ? can you try try chown -Rf "yourusersquid" /etc/squid



From yashvinder.edx at gmail.com  Fri Jun 12 09:35:36 2015
From: yashvinder.edx at gmail.com (yashvinder hooda)
Date: Fri, 12 Jun 2015 15:05:36 +0530
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13)
	Permission	denied
In-Reply-To: <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <20150612050302.5304400.19828.108@gmail.com>
 <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <20150612093536.5304400.72998.114@gmail.com>

?Hi,?
Fred?

Squid directory permission is 644 with nobody:root and same is for mime.conf and squid.conf?

Regards,
Yashvinder



? Original Message ?
From: FredB
Sent: Friday 12 June 2015 2:55 PM
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission	denied


> 
> On 11/06/2015 4:47 p.m., yashvinder hooda wrote:
> > Squid log says Permission denied for the file /etc/squid/mime.conf
> > While permission on it is....
> > 
> > -rwxrwxrwx 1 nobody root 11364 May 9 15:40 mime.conf
> > 

And the squid directory permissions ?

user nobody ? can you try try chown -Rf "yourusersquid" /etc/squid

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Jun 12 09:44:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 21:44:25 +1200
Subject: [squid-users] Error Resolution (TunnelStateData::Connection::
 error )
In-Reply-To: <CAKLXwH5XfNB6ENkYN1MyCuJiDVRp-XN0JGXPN+mF7grSei-iXw@mail.gmail.com>
References: <CAKLXwH7_Uxt_YniOs7vvHhaDkbz4DjQ_ChpVe3ehvrbSFsz1aw@mail.gmail.com>	<556DB89E.1050305@treenet.co.nz>	<CAKLXwH4Ov-YhY0C1vmbgpqDgDrxpt+4fdYNtRV2-KakAQg5Bqw@mail.gmail.com>	<557054EB.4020900@treenet.co.nz>
 <CAKLXwH5XfNB6ENkYN1MyCuJiDVRp-XN0JGXPN+mF7grSei-iXw@mail.gmail.com>
Message-ID: <557AA9F9.8000008@treenet.co.nz>

On 12/06/2015 5:39 p.m., Iruma Keisuke wrote:
> Thank you Amos.
> I really appreciate your response.
> 
> We analyzed the trend of FD an error has occurred.
> 
> 2015/06/01_08:52:35 nsu01pint-int01 [cache]2015/06/01 08:52:32|
> TunnelStateData::Connection:: error : FD 81: read/write failure: (110)
> Connection timed out
> Active file descriptors:
>  File Type   Tout Nread  * Nwrite * Remote Address        Description
>   ---- ------ ---- -------- -------- ---------------------
> --------------------- ---------
>    81 Socket 86282    8100*   67977  XXX.XXXX.2.136:49907   Reading
> next request Date: Sun, 31 May 2015 23:32:30 GMT
>    81 Socket 86302    8100*   16753  XXX.XXXX.2.136:49944   Reading
> next request Date: Sun, 31 May 2015 23:37:30 GMT
>    81 Socket 86002    8100*   17395* XXX.XXXX.2.136:49944   Reading
> next request Date: Sun, 31 May 2015 23:42:30 GMT
>    81 Socket 85702    8100*   17395* XXX.XXXX.2.136:49944   Reading
> next request Date: Sun, 31 May 2015 23:47:30 GMT
>    81 Socket 85402    8100*   17395* XXX.XXXX.2.136:49944   Reading
> next request Date: Sun, 31 May 2015 23:52:30 GMT
>    81 Socket 86354   56810*   40697  XXX.XXXX.6.114:49687   Reading
> next request Date: Sun, 31 May 2015 23:57:30 GMT
>                                  ?
>                          Error while writing ?
> 
> All it seems to have time out in writing.
> And, the time until the timeout is between 10 to 15 minutes.
> 
> Are "Write_timeout" and "read_timeout" directive related to the error?
> "write_timeout" is a directive that does not exist in the 3.1 version.
> Though "write_timeout" can not be set, it works and cause a timeout in
> 15 minutes?

There is no write timeout, things either write immediately or as soon as
space on the network pipe becomes available. The processing Job waiting
for the write to finish can timeout, but that is completely different
from sockets layer.

It might be read_timeout, but IIRC that is internal and Squid uses
close() on the socket to send the signal to the other end. Not try to
read() and fail.


> 
> I think this is also a relationship.
> http://www.squid-cache.org/Doc/config/half_closed_clients/
>> Squid can not tell the difference between a half-closed connection, and a fully closed one.
> 3.1 version "half_closed_clients" is off by default.
> 
> I have this kind of guess.
> 
> 1. Change the client to "half_closed".
> 2. squid write to FD.
> 3. Change the client to "fully_losed".
> 4. Since the "fully_losed" squid can not understand, squid attempts to
> write to the FD.
> 5. After 15 minutes, "write_timeout" occurs in squid
> 
> Can I get your opinion?

Squid does understand fully closed (the state, no directive). If the
socket/FD were fully closed it would be a "read/write error connection
closed" message in the log.

In the world of sockets a close event by the remote end shows up as a
signal requesting a read(), when that read() is performed an error is
returned which explains what happened. I think this is what you are
seeing (eg "connection timed out" happend).
 Why/how its let to get into that state is unclear and may be related to
a bug somewhere. But still unknown whether that place would be Squid,
client or server.

Timeout signalled by the TCP stack is possible and normal, though not
exactly common. As you saw it takes a while and usually connections get
closed when finished with instead of left waiting for a long time like
that, so you dont see this.
 The exception is CONNECT tunnels (TunnelStateData) - Squid is not aware
of the protocol inside so must keep them open until the very last bytes
is passed and either the remote end or the client requesting it goes away.


PS. Eliezer packages current versions of Squid unofficially for RHEL.
You may want to try upgrading to that. It is not supported by RHEL, but
by Eliezer and us here. If you want to continue with the RHEL package
then I suggest its best to get in touch with them about problems (if
this is one).

Amos



From squid3 at treenet.co.nz  Fri Jun 12 09:48:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2015 21:48:27 +1200
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
 denied
In-Reply-To: <20150612093536.5304400.72998.114@gmail.com>
References: <20150612050302.5304400.19828.108@gmail.com>
 <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <20150612093536.5304400.72998.114@gmail.com>
Message-ID: <557AAAEB.9030101@treenet.co.nz>

On 12/06/2015 9:35 p.m., yashvinder hooda wrote:
> ?Hi, 
> Fred 
> 
> Squid directory permission is 644 with nobody:root and same is for mime.conf and squid.conf 
> 
> Regards,
> Yashvinder

Okay. Wierd. Its not even like Squid is trying to open for write or
anything fancy. Its just reading.

Are you using the latest available Squid version?

Amos



From fredbmail at free.fr  Fri Jun 12 09:51:28 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 12 Jun 2015 11:51:28 +0200 (CEST)
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13)
	Permission	denied
In-Reply-To: <20150612093536.5304400.72998.114@gmail.com>
Message-ID: <868882115.629370912.1434102688453.JavaMail.root@zimbra4-e1.priv.proxad.net>



> 
> ?Hi,
> Fred
> 
> Squid directory permission is 644 with nobody:root and same is for
> mime.conf and squid.conf
> 

Hi

Your cache user name is ? nobody ? 

eg my configuration:

cache_effective_user squid in squid.conf 

/etc/squid
drwxr-xr-x  2 squid      root    4096 juin   3 10:48 squid

Maybe you can try with cache_effective_user nobody ? Or, if you use another value change the rights to files (and directory)    
On Debian the default user is "proxy", it depends


From yashvinder.edx at gmail.com  Fri Jun 12 10:00:34 2015
From: yashvinder.edx at gmail.com (yashvinder hooda)
Date: Fri, 12 Jun 2015 15:30:34 +0530
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
	denied
In-Reply-To: <557AAAEB.9030101@treenet.co.nz>
References: <20150612050302.5304400.19828.108@gmail.com>
 <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <20150612093536.5304400.72998.114@gmail.com>
 <557AAAEB.9030101@treenet.co.nz>
Message-ID: <20150612100034.5304400.82697.119@gmail.com>

Hi Amos

Squid pkg version is 3.5.2 and ?it's running on openwrt.
In logs I can see ?one more permission related error ? and that is:
ParseEtcHosts: /etc/hosts ()Permission denied

Regards,
Yashvinder
? Original Message ?
From: Amos Jeffries
Sent: Friday 12 June 2015 3:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission denied

On 12/06/2015 9:35 p.m., yashvinder hooda wrote:
> ?Hi, 
> Fred 
> 
> Squid directory permission is 644 with nobody:root and same is for mime.conf and squid.conf 
> 
> Regards,
> Yashvinder

Okay. Wierd. Its not even like Squid is trying to open for write or
anything fancy. Its just reading.

Are you using the latest available Squid version?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From gkinkie at gmail.com  Fri Jun 12 10:05:50 2015
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 12 Jun 2015 12:05:50 +0200
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
	denied
In-Reply-To: <20150612100034.5304400.82697.119@gmail.com>
References: <20150612050302.5304400.19828.108@gmail.com>
 <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <20150612093536.5304400.72998.114@gmail.com> <557AAAEB.9030101@treenet.co.nz>
 <20150612100034.5304400.82697.119@gmail.com>
Message-ID: <CA+Y8hcPjHCWFY=J7gzKXzn0QgsLepvLJUmrbxy6D+x1OqLkQbA@mail.gmail.com>

Hi all,
  file system corruption at times manifests itself as permission problems.
Can you fsck?

On Fri, Jun 12, 2015 at 12:00 PM, yashvinder hooda
<yashvinder.edx at gmail.com> wrote:
> Hi Amos
>
> Squid pkg version is 3.5.2 and ?it's running on openwrt.
> In logs I can see  one more permission related error ? and that is:
> ParseEtcHosts: /etc/hosts ()Permission denied
>
> Regards,
> Yashvinder
>   Original Message
> From: Amos Jeffries
> Sent: Friday 12 June 2015 3:18 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission denied
>
> On 12/06/2015 9:35 p.m., yashvinder hooda wrote:
>> ?Hi,
>> Fred
>>
>> Squid directory permission is 644 with nobody:root and same is for mime.conf and squid.conf
>>
>> Regards,
>> Yashvinder
>
> Okay. Wierd. Its not even like Squid is trying to open for write or
> anything fancy. Its just reading.
>
> Are you using the latest available Squid version?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From brian at interlinx.bc.ca  Fri Jun 12 11:48:49 2015
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Fri, 12 Jun 2015 07:48:49 -0400
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <557A0800.7040207@treenet.co.nz>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz>
Message-ID: <1434109729.15657.7.camel@interlinx.bc.ca>

On Fri, 2015-06-12 at 10:13 +1200, Amos Jeffries wrote:
> 
> see <http://readlist.com/lists/squid-cache.org/squid-users/11/58405.html>

Of course, I did see the rest of the messages in the thread.  I'm not
sure what I'm supposed to be seeing in that particular message though
other than 3.4.3 worked for the person reporting that.

So maybe I have a different problem?

> - You speak of a squid.conf but dont show any of its contents.

auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/lib/squid3/negotiate_kerberos_auth
auth_param negotiate children 20 startup=1 idle=1
auth_param negotiate keep_alive on
auth_param basic program /usr/lib/squid3/basic_pam_auth
auth_param basic children 3 startup=3 idle=1
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
include /etc/squid3/squid-acls.conf
include /etc/squid3/squid-http_access.conf
http_access allow localhost
http_access deny all
http_port 3128
http_port 13128 intercept
cache_dir aufs /var/spool/squid3 9000 16 256
maximum_object_size 404800 KB
cache_store_log stdio:/var/log/squid3/store.log
strip_query_terms off
coredump_dir /var/spool/squid3
url_rewrite_program /usr/bin/squidGuard
url_rewrite_children 10
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern \.rpm$          144000  100%    525600
refresh_pattern \.deb$          144000  100%    525600
refresh_pattern .               0       20%     4320
connect_timeout 10 seconds
snmp_port 3401
snmp_access allow snmppublic linux_pc 
snmp_access deny all
no_cache deny url_deb 

> - You speak of 503 but show now trace or evidence from cache.log (or
> even access.log) that might hint as a reason for it.

From access.log:

1434107701.067   9698 10.75.22.1 TCP_MISS/503 0 CONNECT irc.bcwireless.net:6667 brian HIER_NONE/- -

I'm happy to provide some debug trace.  I assume you don't want ALL, but
if you let me know which debugging sections and levels would be useful
I'd be happy to provide them.

Cheers,
b.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/b35a2092/attachment.sig>

From tarotapprentice at yahoo.com  Fri Jun 12 11:47:51 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Fri, 12 Jun 2015 11:47:51 +0000 (UTC)
Subject: [squid-users] Is there a way to limit the number of upload/downloads
Message-ID: <1611783266.1652475.1434109671641.JavaMail.yahoo@mail.yahoo.com>

I've looked through the reference and the nearest thing I can find is the delay pools. I am trying to limit the number of concurrent downloads and uploads to save saturating a DSL link.

Delay_pools seems to be based upon throttling the speed rather than limiting the number of concurrent up/downloads. Can Squid limit the number of concurrent down/uploads?

Running 3.5.5 on Windows.

Cheers,
MarkJ


From e2ia.ci at gmail.com  Fri Jun 12 13:24:40 2015
From: e2ia.ci at gmail.com (CIROBOTICS)
Date: Fri, 12 Jun 2015 13:24:40 +0000
Subject: [squid-users] How to mark trafic from squid and squidguard
In-Reply-To: <557A2C46.9050407@treenet.co.nz>
References: <CAMhThxJvPmZ2R7qQLZggf0Hm4GPpzTkPdigaQp0m4Jhev32ZtQ@mail.gmail.com>
 <557A2C46.9050407@treenet.co.nz>
Message-ID: <CAMhThxKzrD7tUHBEFQeTh-eCE2xv-CfgxjahCGgD429qquOYWg@mail.gmail.com>

Thx for your reply.
how could I test the visiting website to the pornsite databe and set the
tos.
regards.

2015-06-12 0:48 GMT+00:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 12/06/2015 12:28 a.m., CIROBOTICS wrote:
> > Hi dear is there a mean to mark trafic from squid/squidguard?
> > I'd like for example mark all trafic to pornsite instead of redirecting
> it
> > with squidguard.
>
> No. SG is too outdated for any of the Squid features that might allow
> that (helper annotations combined with tcp_outgoing_tos).
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/7153d7b1/attachment.htm>

From bielsk at us.ibm.com  Fri Jun 12 14:08:59 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Fri, 12 Jun 2015 10:08:59 -0400
Subject: [squid-users] reverse proxies and Host request header
In-Reply-To: <1434040060.3610.56.camel@JamesiMac>
References: <1434040060.3610.56.camel@JamesiMac>
Message-ID: <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>

I have a general question about the use of the http Host request header by
reverse proxies.

As I understand it, the Host request header is used by transparent forward
proxies as a way to route a request to the correct
origin server since, unlike with an explicit proxy, the host is not
included in the URI portion of the http method line.

However, reverse proxies are always "transparent" from the perspective of
the client and the Host header is often used by the proxy
to map to the correct back end origin server.

I also think they usually pass the Host header as-is to the origin server.
This last
piece puzzles me because it means that the origin server is being given a
different host name than itself in the header. Is this
behavior "correct"? Does it ever cause problems?

J. Bielski
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/edcc48f1/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jun 12 14:14:00 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 12 Jun 2015 16:14:00 +0200
Subject: [squid-users] reverse proxies and Host request header
In-Reply-To: <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>
References: <1434040060.3610.56.camel@JamesiMac>
 <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>
Message-ID: <201506121614.00896.Antony.Stone@squid.open.source.it>

On Friday 12 June 2015 at 16:08:59 (EU time), Julianne Bielski wrote:

> reverse proxies are always "transparent" from the perspective of
> the client and the Host header is often used by the proxy
> to map to the correct back end origin server.
> 
> I also think they usually pass the Host header as-is to the origin server.
> This last piece puzzles me because it means that the origin server is being
> given a different host name than itself in the header. Is this behavior
> "correct"? Does it ever cause problems?

How is this different from a normal web server serving multiple virtual host 
sites?

The web server doesn't care who "it" is, it just cares which virtual host it's 
being asked to serve pages for.  A reverse proxy in the way basically makes no 
difference.


Regards,


Antony.

-- 
All generalisations are inaccurate.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bielsk at us.ibm.com  Fri Jun 12 15:49:38 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Fri, 12 Jun 2015 11:49:38 -0400
Subject: [squid-users] reverse proxies and Host request header
In-Reply-To: <201506121614.00896.Antony.Stone@squid.open.source.it>
References: <1434040060.3610.56.camel@JamesiMac>
 <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>
 <201506121614.00896.Antony.Stone@squid.open.source.it>
Message-ID: <OF5B7543D0.D9796684-ON87257E62.005666BC-85257E62.0056ED5E@us.ibm.com>


With virtual hosting, the client is asking for a virtual origin server's
host and DNS enables the ip address for the physical host to respond. The
virtual host still sees "its" hostname in the host header, not the physical
host's.

With a reverse proxy, the backend origin server doesn't see its Host name,
it sees the proxy's, even though the proxy is an http client with respect
to the origin server.



From:	Antony Stone <Antony.Stone at squid.open.source.it>
To:	squid-users at lists.squid-cache.org
Date:	06/12/2015 10:14 AM
Subject:	Re: [squid-users] reverse proxies and Host request header
Sent by:	"squid-users" <squid-users-bounces at lists.squid-cache.org>



On Friday 12 June 2015 at 16:08:59 (EU time), Julianne Bielski wrote:

> reverse proxies are always "transparent" from the perspective of
> the client and the Host header is often used by the proxy
> to map to the correct back end origin server.
>
> I also think they usually pass the Host header as-is to the origin
server.
> This last piece puzzles me because it means that the origin server is
being
> given a different host name than itself in the header. Is this behavior
> "correct"? Does it ever cause problems?

How is this different from a normal web server serving multiple virtual
host
sites?

The web server doesn't care who "it" is, it just cares which virtual host
it's
being asked to serve pages for.  A reverse proxy in the way basically makes
no
difference.


Regards,


Antony.

--
All generalisations are inaccurate.

                                                   Please reply to the
list;
                                                         please *don't* CC
me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/115a2a8c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/115a2a8c/attachment.gif>

From tmblue at gmail.com  Fri Jun 12 17:11:20 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 12 Jun 2015 10:11:20 -0700
Subject: [squid-users] 3.5.0.2 core, constant under heavy load.
In-Reply-To: <CAEaSS0bfEgUb87-xeOUM1MjUZLEtF+2hmnCfPux+Dnx2sd8KTQ@mail.gmail.com>
References: <CAEaSS0ZYdWDfoC2Q4CJrBvDepWL7sEMUi=Sr=RLEVbhnbrFb5g@mail.gmail.com>
 <5579E09F.7020205@ngtech.co.il>
 <CAEaSS0aWwVfNKLdA9kRWW-XR__7ALxn4i+GKhipV6UAHFSgejg@mail.gmail.com>
 <CAEaSS0b531L8+D4P8Mk1+FqV=dHvYwqtLJHuPC2WzGRGOws4gg@mail.gmail.com>
 <557A09E2.2090907@treenet.co.nz>
 <CAEaSS0Zde+1HQ4vHFuy_-YfFUOiH4r4w4GS5FreC-DbL8GXHHQ@mail.gmail.com>
 <557A120A.50203@treenet.co.nz>
 <CAEaSS0bfEgUb87-xeOUM1MjUZLEtF+2hmnCfPux+Dnx2sd8KTQ@mail.gmail.com>
Message-ID: <CAEaSS0Zwb1Vb9e5hxa4zFiwYn3uV8QYDBFuYvizJFqgPhOQAmw@mail.gmail.com>

On Thu, Jun 11, 2015 at 4:07 PM, Tory M Blue <tmblue at gmail.com> wrote:

> Okay well I took the RPM from your counter parts link and it gave me
>
> http://wiki.squid-cache.org/KnowledgeBase/CentOS
>
> squid-3.5.0.4-1.el6.x86_64.rpm
>
>
> So am I using the wrong version?
>
> Thank you sir! :)
>
> Tory
>
> Okay Amos, set me straight and got me the right production versions. I'm
running 3.5.5 now vs 3.5.0.2/3.5.0.4.. My servers are still happy not
seeing any errors, no cores, no FATAL's, things are running much better.
3.5.0.2 only worked in low volume environment, but 3.5.4 and the latest
3.5.5 seem to have had some major stuff fixed.

Thanks all

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/4f5c5c7d/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jun 12 19:09:26 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 12 Jun 2015 21:09:26 +0200
Subject: [squid-users] reverse proxies and Host request header
In-Reply-To: <OF5B7543D0.D9796684-ON87257E62.005666BC-85257E62.0056ED5E@us.ibm.com>
References: <1434040060.3610.56.camel@JamesiMac>
 <201506121614.00896.Antony.Stone@squid.open.source.it>
 <OF5B7543D0.D9796684-ON87257E62.005666BC-85257E62.0056ED5E@us.ibm.com>
Message-ID: <201506122109.26434.Antony.Stone@squid.open.source.it>

On Friday 12 June 2015 at 17:49:38 (EU time), Julianne Bielski wrote:

> With virtual hosting, the client is asking for a virtual origin server's
> host and DNS enables the ip address for the physical host to respond. The
> virtual host still sees "its" hostname in the host header, not the physical
> host's.

There is no "virtual host" to "see its hostname" in the header.  There is a 
single real web server (which might be a virtual machine, but that's a 
different use of the term "virtual", and doesn't matter here one way or the 
other), which can happily handle requests for multiple websites (which you 
might want to call hostnames, domains, etc).

The single web server never sees its own hostname in the header (assuming that 
its real hostname is not one of the virtual names it serves web pages for), 
and the web service is quite happy with this.

> With a reverse proxy, the backend origin server doesn't see its Host name,
> it sees the proxy's, even though the proxy is an http client with respect
> to the origin server.

The backend server sees the "hostname" (if you want to call it that) of the 
website for which it is supposed to serve pages.

An example might help:

	www.example.com
	www.example.net
	www.example.org
	downloads.example.org

may all have DNS entries pointing to a single IP address.

The machine at that IP address (it doesn't matter what its own hostname is, so 
I won't even suggest what it might be) has a web server configured to respond 
to requests for any of those web sites, therefore clients get the expected 
answers, without necessarily realising that they're all served by the same 
machine at a single IP address.

Now put a reverse proxy in the way - the above hostnames now resolve to the IP 
address of the reverse proxy, it's configured to pass the requests on to the 
appropriate backend server/s, and if that's just a single machine, it is 
configured exactly the same way as the original web server was - it responds to 
requests for any of the virtual site names.


I hope that helps;

Regards,


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jonathan.filogna at tasso.com.ar  Fri Jun 12 19:27:26 2015
From: jonathan.filogna at tasso.com.ar (Jonathan Filogna)
Date: Fri, 12 Jun 2015 16:27:26 -0300
Subject: [squid-users] Proxy Parent
Message-ID: <CAPnpmpkrT3P2s=z2_5Vt9RTtKGg5hWNbderY+Lc=by8XeaV-VA@mail.gmail.com>

Hi all, here's my new situation (still on squid 2.7)

i want to send by DIRECT uservipstr, uservip
i want to send by PARENT userti, userlimitado, user200mb, userinternet

i want to send by DIRECT all the NTLM users that don't belong to any list
of above

(ikr, my english sucks)

i want to block streaming (blockstr, blockstr2, audyvid, vidyaud) for all
but uservipstr

if i remove the line "always_direct allow ntlm" DIRECT/PARENT tules works
but doesn't streaming rules

if i let that line, streaming works but doesn't DIRECT/PARENT

here's my squid.conf. I'll put here all because can't find where's my error


########################

##NOMBRE VISIBLE DEL PROXY

visible_hostname prana

##NTLM
#
##DECLARADO

auth_param ntlm program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
auth_param ntlm keep_alive off

##DECLARACION DE NTLM EXTERNO PARA BLOQUEO DE DESCARGA DE ARCHIVOS
##BALANCEO DE CARGA Y TAMA?OS DE ARCHIVOS DESCARGADOS
#
##DECLARADO

external_acl_type ntlm_group ttl=3600 children=100 %LOGIN /usr/lib/squid/
wbinfo_group.pl

##ACA DECLARO LISTAS DE ACCESO DE ROEMMERS
#
##DECLARADO

acl porno url_regex -i "/etc/squid/listas/porno.lst"
acl permitidos dstdomain -i "/etc/squid/listas/permitidos.lst"
acl directo url_regex -i "/etc/squid/listas/direct.lst"
acl vidyaud rep_mime_type -i "/etc/squid/listas/blockstr.lst"
acl useragent browser -i "/etc/squid/blockejec/browser.lst"
acl blockstr req_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
acl blockejec url_regex -i "/etc/squid/blockejec/blockejec.lst"
acl audyvid req_mime_type -i "/etc/squid/listas/blockstr.lst"
acl blockstr2 rep_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
acl destinolimitado dstdomain -i "/etc/squid/listas/limitado.lst"

###ACL DE SKYPE
acl skype external ntlm_group "/etc/squid/listas/skype.lst"
acl numeric_ips dstdom_regex
^(([0-9]+.[0-9]+.[0-9]+.[0-9]+)|([([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?])):443
acl skype_ua browser ^skype
acl validuseragent browser \S+
#
##DECLARADO
acl all src all
acl CONNECT method CONNECT
##DECLARO SQSTAT
##ACL SQSTAT
acl manager proto cache_object
acl webserver src 192.168.8.121/255.255.255.255
http_access allow manager webserver
http_reply_access allow manager webserver
http_access deny manager

#REGLAS DE NAVEGACION
http_access deny porno all
http_reply_access deny porno all
deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
acl uservipstr external ntlm_group "/etc/squid/listas/uservipstr.lst"
http_access deny blockejec uservipstr
http_access allow uservipstr
http_reply_access allow uservipstr
http_access deny blockstr !uservipstr all
http_reply_access deny blockstr !uservipstr all
http_access deny blockstr2 !uservipstr all
http_reply_access deny blockstr2 !uservipstr all
http_access deny audyvid !uservipstr all
http_access deny vidyaud !uservipstr all
http_reply_access deny audyvid !uservipstr all
http_reply_access deny vidyaud !uservipstr all
reply_body_max_size 9999999999999999999999999999999 deny uservipstr
acl uservip external ntlm_group "/etc/squid/listas/uservip.lst"
http_access deny blockejec uservip
http_access allow uservip
reply_body_max_size 9999999999999999999999999999999 deny uservip
http_reply_access allow uservip
always_direct allow uservip
acl userti external ntlm_group "/etc/squid/listas/userti.lst"
http_access deny blockejec !userti
http_access allow userti
http_reply_access allow userti

reply_body_max_size 9999999999999999999999999999999 deny userti
acl user200mb external ntlm_group "/etc/squid/listas/user200mb.lst"
http_access allow user200mb
http_reply_access allow user200mb
reply_body_max_size 500000000 deny user200mb
acl userinternet external ntlm_group "/etc/squid/listas/userinternet.lst"
http_access allow userinternet
http_reply_access allow userinternet
reply_body_max_size 45000000 deny userinternet
acl userlimitado external ntlm_group "/etc/squid/listas/userlimitado.lst"
http_access deny userlimitado !destinolimitado
http_reply_access deny userlimitado !destinolimitado
never_direct allow userlimitado
#deny
deny_info http://www.pranaglobal.com.ar/restringidos/roemmers
destinolimitado
reply_body_max_size 45000000 deny userlimitado
##DECLARO LISTAS DE ACCESO EXTRAS



##LISTO

##ACL COMUNES
acl localnet src 192.168.0.0/16
acl SSL_ports port 443 # https
acl SSL_ports port 563 # snews
acl SSL_ports port 873 # rsync
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 631 # cups
acl Safe_ports port 873 # rsync
acl Safe_ports port 901 # SWAT
acl Safe_ports port 78 69 #Spotify

##SRC'S DECLARADAS
#
##ACA DECLARO ACCESOS HTTP Y FILTRADO POR GRUPO DE AD



# Deny requests to unknown ports
#http_access allow Safe_ports
http_access deny !Safe_ports
# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports
##ACCESOS HTTP DECLARADOS
#
##ACA INICIA SSO
acl ntlm proxy_auth REQUIRED
#http_access deny !ntlm
########################################## DESCOMENTAR SI VAMOS CON
BLACKLIST
http_access deny numeric_ips !skype
http_access deny skype_ua !skype
http_access deny !validuseragent !skype
##########################################
http_access allow permitidos ntlm
http_reply_access allow permitidos ntlm
http_access allow permitidos !userlimitado
http_reply_access allow permitidos !userlimitado
http_access deny all
http_reply_access deny all
reply_body_max_size 500000 deny all
##ACA TERMINA
#
##Allow ICP queries from local networks only
icp_access allow localnet
icp_access deny all
##
#
## Squid normally listens to port 3128
http_port 3128
##PUERTO SQUID DECLARADO
#
##LOG
access_log /var/log/squid/access.log squid
##HECHO
#
#LIMITANDO DESCARGA A 40 MB
#reply_body_max_size 0 allow userti
#reply_body_max_size 0 allow uservip
#reply_body_max_size 0 allow uservipstr
#reply_body_max_size 4000000 allow user200mb
#reply_body_max_size 4000  allow userinternet
#reply_body_max_size 4000 allow userlimitado
#reply_body_max_size 0 deny all
##HECHO

##PROXY PARENT!! EN CASO DE QUE SE CAIGA EL PROXY PARENT
## O AL MOMENTO DE REEMPLAZAR EL FIREWALL POR UN ACTIVO-ACTIVO
##COMENTAR ESTAS LINEAS
cache_peer 192.168.26.15 parent 3128 0 no-digest proxy-only no-delay
no-query

dead_peer_timeout 30 seconds
#
#HECHO

##EN QUE CASOS ES DIRECT?
##
##EL RESTO NAVEGARA POR PARENT
always_direct allow uservipstr
always_direct allow uservip
always_direct allow directo
always_direct allow blockejec
always_direct deny blockstr
always_direct allow permitidos all
never_direct allow blockstr
never_direct allow userti
always_direct allow ntlm
always_direct deny all
never_direct allow all


##LLAMADO A SQUIDGUARD
url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 50

##############################

Thanks for your attention
-- 
Jonathan Filogna
It Senior
Tasso SRL
4702 1910
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/16aef73c/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 12 20:42:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 08:42:54 +1200
Subject: [squid-users] mimeInit: /etc/squid/mime.conf: (13) Permission
 denied
In-Reply-To: <CA+Y8hcPjHCWFY=J7gzKXzn0QgsLepvLJUmrbxy6D+x1OqLkQbA@mail.gmail.com>
References: <20150612050302.5304400.19828.108@gmail.com>
 <80015412.629288138.1434101126958.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <20150612093536.5304400.72998.114@gmail.com> <557AAAEB.9030101@treenet.co.nz>
 <20150612100034.5304400.82697.119@gmail.com>
 <CA+Y8hcPjHCWFY=J7gzKXzn0QgsLepvLJUmrbxy6D+x1OqLkQbA@mail.gmail.com>
Message-ID: <557B444E.1080607@treenet.co.nz>

On 12/06/2015 10:05 p.m., Kinkie wrote:
> Hi all,
>   file system corruption at times manifests itself as permission problems.
> Can you fsck?
> 

Good idea.

I'm also wondering about that use of execute permission on text files
inside /etc subdirectory. Seems bonkers. And I've seen in passing a few
systems that refused to load files if they had extremely strange
permissions like that enabled.

FTR: I'm out of ideas on this now.
Amos



From squid3 at treenet.co.nz  Fri Jun 12 20:59:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 08:59:16 +1200
Subject: [squid-users] Is there a way to limit the number of
	upload/downloads
In-Reply-To: <1611783266.1652475.1434109671641.JavaMail.yahoo@mail.yahoo.com>
References: <1611783266.1652475.1434109671641.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <557B4824.4020206@treenet.co.nz>

On 12/06/2015 11:47 p.m., TarotApprentice wrote:
> I've looked through the reference and the nearest thing I can find is
> the delay pools. I am trying to limit the number of concurrent
> downloads and uploads to save saturating a DSL link.
> 

That would be because the number of transactions is meaningless.

I can (and have) fully saturated a 100Mbps link using just 2 downloads.
Then a few minutes later pushed through 300K downloads without getting
more than a minor bump in its capacity.

Size and Speed of each transaction is what matters for saturation. thats
where delay pools comes in. It throttles the per-second size of bandwith
each client consumes. Slowing down only the heavy users.


Delay pools even works when the users try to abuse things with a
download accelerator program. Under your counting design these users
consume N slots and leave others unable to connect. With pools all their
attempts count against one pool, other clients get a fair(er) share.


> Delay_pools seems to be based upon throttling the speed rather than
> limiting the number of concurrent up/downloads. Can Squid limit the
> number of concurrent down/uploads?
> 
> Running 3.5.5 on Windows.

Windows has a limit of 2K FDs which does that in a way. 2K means a
maximum capacity of only between 400-900 client connections in total.

Amos


From atman.sense at zise.de  Fri Jun 12 21:04:39 2015
From: atman.sense at zise.de (Atman Sense)
Date: Fri, 12 Jun 2015 23:04:39 +0200
Subject: [squid-users] grab hostnames via SNI to use it for parent proxy
In-Reply-To: <5570CFB7.3000207@treenet.co.nz>
References: <62869e10c6d5ea46130fa093a8af6a65@zise.de>
 <5570CFB7.3000207@treenet.co.nz>
Message-ID: <b219dbaabdd86781a431a13ff7f65b10@zise.de>

Am 2015-06-05 00:22, schrieb Amos Jeffries:
> 
> You can block by SNI in the ssl_bump checks without having bumped the
> connection.
> 
> Like so:
> 
>  # get the public TLS metadata (includes SNI)
>  ssl_bump peek all
> 
>  # block based on SNI matching (or server cert matching)
>  acl blocked ssl::server_name .example.com
>  ssl_bump terminate blocked
> 
>  # tunnel (no decrypting) for everything else
>  ssl_bump splice all
> 
> 
> Note that you do have to allow the "CONNECT raw-IP:443 ..." requests
> through http_access to the bumping logics.
> 

that's nice. Thanks for that.

It would be nice if I could handle the blocklist on privoxy centrally 
(my users want to disable the blocks occasionally and can do that 
through a privoxy web interface). I tried to find out when squid is 
sending "CONNECT IP:PORT" to the parent proxy in hope to manipulate it 
to "CONNECT HOSTNAME:PORT". And I found it in tunnel.cc:1052 
(mb.Printf("CONNECT %s HTTP/1.1\r\n", tunnelState->url);). After some 
investigating with gdb, I found the SNI hostname in this context in 
tunnelState->http->getConn()->serverBump()->clientSni.c_str(). Currently 
I'm testing with this patch:

--- src/tunnel.cc   2015-05-01 13:27:20.000000000 +0200
+++ src/tunnel.cc   2015-06-07 14:10:37.098895939 +0200
@@ -1049,7 +1049,13 @@
      flags.proxying = tunnelState->request->flags.proxying;
      MemBuf mb;
      mb.init();
-    mb.Printf("CONNECT %s HTTP/1.1\r\n", tunnelState->url);
+    //use SNI hostname if it exists
+    if 
(strlen(tunnelState->http->getConn()->serverBump()->clientSni.c_str()) > 
1) {
+        mb.Printf("CONNECT %s:%hu HTTP/1.1\r\n", 
tunnelState->http->getConn()->serverBump()->clientSni.c_str(), 
tunnelState->request->port);
+    } else {
+        mb.Printf("CONNECT %s HTTP/1.1\r\n", tunnelState->url);
+    }
+
      
HttpStateData::httpBuildRequestHeader(tunnelState->request.getRaw(),
                                            NULL,         /* StoreEntry 
*/
                                            tunnelState->al,          /* 
AccessLogEntry */

This works quite well, but when privoxy blocks a "CONNECT" request, 
squid doesn't understand it and the client connection is keeped open 
until the client times out:
     HttpMsg.cc(176) parse: HttpMsg::parse success (275 bytes) near 
'HTTP/1.1 403 Request blocked by Privoxy'
     tunnel.cc(459) logicError: local=xxx:42093 remote=xxx:8118 FD 17 
flags=1 closing on error: unsupported CONNECT response status code
A look in tunnel.cc reveals, that it only accept HTTP 200. Thats ok, but 
it would be nice to disconnect both client and parent proxy to avoid 
timeouts. Do you have an idea how to disconnect the client immediately 
after non HTTP 200 responses?



From luis.daniel.lucio at gmail.com  Fri Jun 12 21:05:03 2015
From: luis.daniel.lucio at gmail.com (Luis Daniel Lucio Quiroz)
Date: Fri, 12 Jun 2015 17:05:03 -0400
Subject: [squid-users] Proxy Parent
In-Reply-To: <CAPnpmpkrT3P2s=z2_5Vt9RTtKGg5hWNbderY+Lc=by8XeaV-VA@mail.gmail.com>
References: <CAPnpmpkrT3P2s=z2_5Vt9RTtKGg5hWNbderY+Lc=by8XeaV-VA@mail.gmail.com>
Message-ID: <CAFLo2QwRPUuEWb1gy4evxkjCOF=CxMqjY3zg-ixSAzA3tbimxg@mail.gmail.com>

Quieres que te hagan el trabajo :) jejeje

mandame email

Luis Daniel Lucio Quiroz
CISSP, CISM, CISA
Linux, VoIP and much more fun
www.okay.com.mx

Need LCR? Check out LCR for FusionPBX with FreeSWITCH
Need Billing? Check out Billing for FusionPBX with FreeSWITCH

2015-06-12 15:27 GMT-04:00 Jonathan Filogna <jonathan.filogna at tasso.com.ar>:

> Hi all, here's my new situation (still on squid 2.7)
>
> i want to send by DIRECT uservipstr, uservip
> i want to send by PARENT userti, userlimitado, user200mb, userinternet
>
> i want to send by DIRECT all the NTLM users that don't belong to any list
> of above
>
> (ikr, my english sucks)
>
> i want to block streaming (blockstr, blockstr2, audyvid, vidyaud) for all
> but uservipstr
>
> if i remove the line "always_direct allow ntlm" DIRECT/PARENT tules works
> but doesn't streaming rules
>
> if i let that line, streaming works but doesn't DIRECT/PARENT
>
> here's my squid.conf. I'll put here all because can't find where's my error
>
>
> ########################
>
> ##NOMBRE VISIBLE DEL PROXY
>
> visible_hostname prana
>
> ##NTLM
> #
> ##DECLARADO
>
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
> auth_param ntlm keep_alive off
>
> ##DECLARACION DE NTLM EXTERNO PARA BLOQUEO DE DESCARGA DE ARCHIVOS
> ##BALANCEO DE CARGA Y TAMA?OS DE ARCHIVOS DESCARGADOS
> #
> ##DECLARADO
>
> external_acl_type ntlm_group ttl=3600 children=100 %LOGIN /usr/lib/squid/
> wbinfo_group.pl
>
> ##ACA DECLARO LISTAS DE ACCESO DE ROEMMERS
> #
> ##DECLARADO
>
> acl porno url_regex -i "/etc/squid/listas/porno.lst"
> acl permitidos dstdomain -i "/etc/squid/listas/permitidos.lst"
> acl directo url_regex -i "/etc/squid/listas/direct.lst"
> acl vidyaud rep_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl useragent browser -i "/etc/squid/blockejec/browser.lst"
> acl blockstr req_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl blockejec url_regex -i "/etc/squid/blockejec/blockejec.lst"
> acl audyvid req_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl blockstr2 rep_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl destinolimitado dstdomain -i "/etc/squid/listas/limitado.lst"
>
> ###ACL DE SKYPE
> acl skype external ntlm_group "/etc/squid/listas/skype.lst"
> acl numeric_ips dstdom_regex
> ^(([0-9]+.[0-9]+.[0-9]+.[0-9]+)|([([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?])):443
> acl skype_ua browser ^skype
> acl validuseragent browser \S+
> #
> ##DECLARADO
> acl all src all
> acl CONNECT method CONNECT
> ##DECLARO SQSTAT
> ##ACL SQSTAT
> acl manager proto cache_object
> acl webserver src 192.168.8.121/255.255.255.255
> http_access allow manager webserver
> http_reply_access allow manager webserver
> http_access deny manager
>
> #REGLAS DE NAVEGACION
> http_access deny porno all
> http_reply_access deny porno all
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
> acl uservipstr external ntlm_group "/etc/squid/listas/uservipstr.lst"
> http_access deny blockejec uservipstr
> http_access allow uservipstr
> http_reply_access allow uservipstr
> http_access deny blockstr !uservipstr all
> http_reply_access deny blockstr !uservipstr all
> http_access deny blockstr2 !uservipstr all
> http_reply_access deny blockstr2 !uservipstr all
> http_access deny audyvid !uservipstr all
> http_access deny vidyaud !uservipstr all
> http_reply_access deny audyvid !uservipstr all
> http_reply_access deny vidyaud !uservipstr all
> reply_body_max_size 9999999999999999999999999999999 deny uservipstr
> acl uservip external ntlm_group "/etc/squid/listas/uservip.lst"
> http_access deny blockejec uservip
> http_access allow uservip
> reply_body_max_size 9999999999999999999999999999999 deny uservip
> http_reply_access allow uservip
> always_direct allow uservip
> acl userti external ntlm_group "/etc/squid/listas/userti.lst"
> http_access deny blockejec !userti
> http_access allow userti
> http_reply_access allow userti
>
> reply_body_max_size 9999999999999999999999999999999 deny userti
> acl user200mb external ntlm_group "/etc/squid/listas/user200mb.lst"
> http_access allow user200mb
> http_reply_access allow user200mb
> reply_body_max_size 500000000 deny user200mb
> acl userinternet external ntlm_group "/etc/squid/listas/userinternet.lst"
> http_access allow userinternet
> http_reply_access allow userinternet
> reply_body_max_size 45000000 deny userinternet
> acl userlimitado external ntlm_group "/etc/squid/listas/userlimitado.lst"
> http_access deny userlimitado !destinolimitado
> http_reply_access deny userlimitado !destinolimitado
> never_direct allow userlimitado
> #deny
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers
> destinolimitado
> reply_body_max_size 45000000 deny userlimitado
> ##DECLARO LISTAS DE ACCESO EXTRAS
>
>
>
> ##LISTO
>
> ##ACL COMUNES
> acl localnet src 192.168.0.0/16
> acl SSL_ports port 443 # https
> acl SSL_ports port 563 # snews
> acl SSL_ports port 873 # rsync
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl Safe_ports port 631 # cups
> acl Safe_ports port 873 # rsync
> acl Safe_ports port 901 # SWAT
> acl Safe_ports port 78 69 #Spotify
>
> ##SRC'S DECLARADAS
> #
> ##ACA DECLARO ACCESOS HTTP Y FILTRADO POR GRUPO DE AD
>
>
>
> # Deny requests to unknown ports
> #http_access allow Safe_ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports
> ##ACCESOS HTTP DECLARADOS
> #
> ##ACA INICIA SSO
> acl ntlm proxy_auth REQUIRED
> #http_access deny !ntlm
> ########################################## DESCOMENTAR SI VAMOS CON
> BLACKLIST
> http_access deny numeric_ips !skype
> http_access deny skype_ua !skype
> http_access deny !validuseragent !skype
> ##########################################
> http_access allow permitidos ntlm
> http_reply_access allow permitidos ntlm
> http_access allow permitidos !userlimitado
> http_reply_access allow permitidos !userlimitado
> http_access deny all
> http_reply_access deny all
> reply_body_max_size 500000 deny all
> ##ACA TERMINA
> #
> ##Allow ICP queries from local networks only
> icp_access allow localnet
> icp_access deny all
> ##
> #
> ## Squid normally listens to port 3128
> http_port 3128
> ##PUERTO SQUID DECLARADO
> #
> ##LOG
> access_log /var/log/squid/access.log squid
> ##HECHO
> #
> #LIMITANDO DESCARGA A 40 MB
> #reply_body_max_size 0 allow userti
> #reply_body_max_size 0 allow uservip
> #reply_body_max_size 0 allow uservipstr
> #reply_body_max_size 4000000 allow user200mb
> #reply_body_max_size 4000  allow userinternet
> #reply_body_max_size 4000 allow userlimitado
> #reply_body_max_size 0 deny all
> ##HECHO
>
> ##PROXY PARENT!! EN CASO DE QUE SE CAIGA EL PROXY PARENT
> ## O AL MOMENTO DE REEMPLAZAR EL FIREWALL POR UN ACTIVO-ACTIVO
> ##COMENTAR ESTAS LINEAS
> cache_peer 192.168.26.15 parent 3128 0 no-digest proxy-only no-delay
> no-query
>
> dead_peer_timeout 30 seconds
> #
> #HECHO
>
> ##EN QUE CASOS ES DIRECT?
> ##
> ##EL RESTO NAVEGARA POR PARENT
> always_direct allow uservipstr
> always_direct allow uservip
> always_direct allow directo
> always_direct allow blockejec
> always_direct deny blockstr
> always_direct allow permitidos all
> never_direct allow blockstr
> never_direct allow userti
> always_direct allow ntlm
> always_direct deny all
> never_direct allow all
>
>
> ##LLAMADO A SQUIDGUARD
> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> url_rewrite_children 50
>
> ##############################
>
> Thanks for your attention
> --
> Jonathan Filogna
> It Senior
> Tasso SRL
> 4702 1910
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/08667c56/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 12 22:29:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 10:29:24 +1200
Subject: [squid-users] reverse proxies and Host request header
In-Reply-To: <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>
References: <1434040060.3610.56.camel@JamesiMac>
 <OFD8025D87.760B6606-ON87257E62.004CC6EB-85257E62.004DB6A2@us.ibm.com>
Message-ID: <557B5D44.9030502@treenet.co.nz>

On 13/06/2015 2:08 a.m., Julianne Bielski wrote:
> I have a general question about the use of the http Host request header by
> reverse proxies.
> 
> As I understand it, the Host request header is used by transparent forward
> proxies as a way to route a request to the correct
> origin server since, unlike with an explicit proxy, the host is not
> included in the URI portion of the http method line.
> 
> However, reverse proxies are always "transparent" from the perspective of
> the client and the Host header is often used by the proxy
> to map to the correct back end origin server.

The term "transparent" has been overloaded so much its meaningless by
itself.

Beyond what you are asking and Anthony already answered well. There is
the key difference of DNS involvement between reverse and interception
proxy.

A DNS lookup is used by the client to find and explicitly contact the
reverse proxy. The proxy is able to use that as a guarantee that if the
Host header does not contain a name its pre-configured for handling,
that it can/must reject the request entirely.

The same guarnatee for reverse proxies allows it make free use of all
the origin server features of HTTP without causing security problems to
the client/user. Thus the long list of problems at
<http://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Concepts_of_Interception_Caching>
does not apply to reverse proxies.

Amos



From squid3 at treenet.co.nz  Fri Jun 12 23:09:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 11:09:13 +1200
Subject: [squid-users] Proxy Parent
In-Reply-To: <CAPnpmpkrT3P2s=z2_5Vt9RTtKGg5hWNbderY+Lc=by8XeaV-VA@mail.gmail.com>
References: <CAPnpmpkrT3P2s=z2_5Vt9RTtKGg5hWNbderY+Lc=by8XeaV-VA@mail.gmail.com>
Message-ID: <557B6699.4050902@treenet.co.nz>

On 13/06/2015 7:27 a.m., Jonathan Filogna wrote:
> Hi all, here's my new situation (still on squid 2.7)

Much easier to do these types of things during or after upgrading to
current version.
1) The current Squid actually obey HTTP behaviour requirements by
default a LOT better than 2.7 is even capable.
2) The current Squid can audit your squid.conf with just "squid -k
parse" and suggest improvements where things have changed.
3) we can possibly suggest using features only in current version that
might do what you want (if its possible).



> 
> i want to send by DIRECT uservipstr, uservip
> i want to send by PARENT userti, userlimitado, user200mb, userinternet
> 
> i want to send by DIRECT all the NTLM users that don't belong to any list
> of above
> 
> (ikr, my english sucks)
> 
> i want to block streaming (blockstr, blockstr2, audyvid, vidyaud) for all
> but uservipstr
> 
> if i remove the line "always_direct allow ntlm" DIRECT/PARENT tules works
> but doesn't streaming rules
> 
> if i let that line, streaming works but doesn't DIRECT/PARENT
> 
> here's my squid.conf. I'll put here all because can't find where's my error
> 
> 
> ########################
> 
> ##NOMBRE VISIBLE DEL PROXY
> 
> visible_hostname prana
> 
> ##NTLM
> #
> ##DECLARADO
> 
> auth_param ntlm program /usr/bin/ntlm_auth
> --helper-protocol=squid-2.5-ntlmssp auth_param ntlm children 5
> auth_param ntlm keep_alive off
> 
> ##DECLARACION DE NTLM EXTERNO PARA BLOQUEO DE DESCARGA DE ARCHIVOS
> ##BALANCEO DE CARGA Y TAMA?OS DE ARCHIVOS DESCARGADOS
> #
> ##DECLARADO
> 
> external_acl_type ntlm_group ttl=3600 children=100 %LOGIN /usr/lib/squid/
> wbinfo_group.pl
> 
> ##ACA DECLARO LISTAS DE ACCESO DE ROEMMERS
> #
> ##DECLARADO
> 
> acl porno url_regex -i "/etc/squid/listas/porno.lst"
> acl permitidos dstdomain -i "/etc/squid/listas/permitidos.lst"
> acl directo url_regex -i "/etc/squid/listas/direct.lst"
> acl vidyaud rep_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl useragent browser -i "/etc/squid/blockejec/browser.lst"
> acl blockstr req_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl blockejec url_regex -i "/etc/squid/blockejec/blockejec.lst"
> acl audyvid req_mime_type -i "/etc/squid/listas/blockstr.lst"
> acl blockstr2 rep_mime_type -i "/etc/squid/blockejec/blocstreaming.lst"
> acl destinolimitado dstdomain -i "/etc/squid/listas/limitado.lst"
> 
> ###ACL DE SKYPE
> acl skype external ntlm_group "/etc/squid/listas/skype.lst"
> acl numeric_ips dstdom_regex
> ^(([0-9]+.[0-9]+.[0-9]+.[0-9]+)|([([0-9af]+)?:([0-9af:]+)?:([0-9af]+)?])):443
> acl skype_ua browser ^skype
> acl validuseragent browser \S+
> #
> ##DECLARADO
> acl all src all
> acl CONNECT method CONNECT
> ##DECLARO SQSTAT
> ##ACL SQSTAT
> acl manager proto cache_object
> acl webserver src 192.168.8.121/255.255.255.255
> http_access allow manager webserver
> http_reply_access allow manager webserver
> http_access deny manager
> 
> #REGLAS DE NAVEGACION
> http_access deny porno all
> http_reply_access deny porno all
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers porno
> acl uservipstr external ntlm_group "/etc/squid/listas/uservipstr.lst"
> http_access deny blockejec uservipstr
> http_access allow uservipstr
> http_reply_access allow uservipstr
> http_access deny blockstr !uservipstr all
> http_reply_access deny blockstr !uservipstr all
> http_access deny blockstr2 !uservipstr all
> http_reply_access deny blockstr2 !uservipstr all
> http_access deny audyvid !uservipstr all
> http_access deny vidyaud !uservipstr all

vidyaud is checkign a details about the *reply*. Which does not exist in
requests so does nothing in http_access. Remove all the useless access
control lines doing things like that. It will be much easier to read and
understand.


> http_reply_access deny audyvid !uservipstr all
> http_reply_access deny vidyaud !uservipstr all
> reply_body_max_size 9999999999999999999999999999999 deny uservipstr

Ah, 32-bit rounding. The above is a number between 0 and 2GB, it is
guaranteed smaller than you expect.

Hint: use "none" for no limit (or "0" if thats not accepted by 2.7)

Same for the other lines below.

> acl uservip external ntlm_group "/etc/squid/listas/uservip.lst"
> http_access deny blockejec uservip
> http_access allow uservip
> reply_body_max_size 9999999999999999999999999999999 deny uservip
> http_reply_access allow uservip
> always_direct allow uservip
> acl userti external ntlm_group "/etc/squid/listas/userti.lst"
> http_access deny blockejec !userti
> http_access allow userti
> http_reply_access allow userti
> 
> reply_body_max_size 9999999999999999999999999999999 deny userti
> acl user200mb external ntlm_group "/etc/squid/listas/user200mb.lst"
> http_access allow user200mb
> http_reply_access allow user200mb
> reply_body_max_size 500000000 deny user200mb
> acl userinternet external ntlm_group "/etc/squid/listas/userinternet.lst"
> http_access allow userinternet
> http_reply_access allow userinternet
> reply_body_max_size 45000000 deny userinternet
> acl userlimitado external ntlm_group "/etc/squid/listas/userlimitado.lst"
> http_access deny userlimitado !destinolimitado
> http_reply_access deny userlimitado !destinolimitado
> never_direct allow userlimitado
> #deny
> deny_info http://www.pranaglobal.com.ar/restringidos/roemmers
> destinolimitado
> reply_body_max_size 45000000 deny userlimitado
> ##DECLARO LISTAS DE ACCESO EXTRAS
> 
> 
> 
> ##LISTO
> 
> ##ACL COMUNES
> acl localnet src 192.168.0.0/16
> acl SSL_ports port 443 # https
> acl SSL_ports port 563 # snews
> acl SSL_ports port 873 # rsync
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl Safe_ports port 631 # cups
> acl Safe_ports port 873 # rsync
> acl Safe_ports port 901 # SWAT
> acl Safe_ports port 78 69 #Spotify
> 
> ##SRC'S DECLARADAS
> #
> ##ACA DECLARO ACCESOS HTTP Y FILTRADO POR GRUPO DE AD
> 
> 

Um. The purpose of these ...

> 
> # Deny requests to unknown ports
> #http_access allow Safe_ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports
> ##ACCESOS HTTP DECLARADOS

... down to here. Is to protect the proxy, the network, and prevent a
lot of CPU cycles or expensive socket resources being consumed by attackers.

They only can be succesful at that task if you put them above the
http_access controls that do a lot of that work. Such as all the many
many external ACL group lookups earlier in your config.


> #
> ##ACA INICIA SSO
> acl ntlm proxy_auth REQUIRED
> #http_access deny !ntlm
> ########################################## DESCOMENTAR SI VAMOS CON
> BLACKLIST
> http_access deny numeric_ips !skype
> http_access deny skype_ua !skype
> http_access deny !validuseragent !skype
> ##########################################
> http_access allow permitidos ntlm
> http_reply_access allow permitidos ntlm
> http_access allow permitidos !userlimitado
> http_reply_access allow permitidos !userlimitado
> http_access deny all
> http_reply_access deny all
> reply_body_max_size 500000 deny all
> ##ACA TERMINA
> #
> ##Allow ICP queries from local networks only
> icp_access allow localnet
> icp_access deny all
> ##
> #
> ## Squid normally listens to port 3128
> http_port 3128
> ##PUERTO SQUID DECLARADO
> #
> ##LOG
> access_log /var/log/squid/access.log squid
> ##HECHO
> #
> #LIMITANDO DESCARGA A 40 MB
> #reply_body_max_size 0 allow userti
> #reply_body_max_size 0 allow uservip
> #reply_body_max_size 0 allow uservipstr
> #reply_body_max_size 4000000 allow user200mb
> #reply_body_max_size 4000  allow userinternet
> #reply_body_max_size 4000 allow userlimitado
> #reply_body_max_size 0 deny all
> ##HECHO
> 
> ##PROXY PARENT!! EN CASO DE QUE SE CAIGA EL PROXY PARENT
> ## O AL MOMENTO DE REEMPLAZAR EL FIREWALL POR UN ACTIVO-ACTIVO
> ##COMENTAR ESTAS LINEAS
> cache_peer 192.168.26.15 parent 3128 0 no-digest proxy-only no-delay
> no-query
> 
> dead_peer_timeout 30 seconds
> #
> #HECHO
> 
> ##EN QUE CASOS ES DIRECT?
> ##
> ##EL RESTO NAVEGARA POR PARENT
> always_direct allow uservipstr
> always_direct allow uservip
> always_direct allow directo
> always_direct allow blockejec
> always_direct deny blockstr
> always_direct allow permitidos all
> never_direct allow blockstr
> never_direct allow userti
> always_direct allow ntlm
> always_direct deny all
> never_direct allow all
> 
> 
> ##LLAMADO A SQUIDGUARD
> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> url_rewrite_children 50
> 
> ##############################
> 
> Thanks for your attention
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From shakirgil at yahoo.com  Sat Jun 13 05:43:06 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Sat, 13 Jun 2015 05:43:06 +0000 (UTC)
Subject: [squid-users] strip_query_terms parameter
Message-ID: <325735005.2125558.1434174186764.JavaMail.yahoo@mail.yahoo.com>

We are using squid 3.4.9 on centos 64bit and getting following issue.

In access.log we can not see the complete log of filehippo.com see the access.log

TCP_MISS/301 336 GET http://filehippo.com/download/file/f9efedf505ee8f42fcaab2569982d439e08c4e88dd569e7c2e68efed55ace44e/

But when we pause the download and resume it then access.log show us.

TCP_MISS_ABORTED/206 344150 GET http://fs32.filehippo.com/4054/c6334025422b451f98163c369aaf3eeb/vlc-2.2.1-win32.exe

To get complete url log we switch off this parameter.

strip_query_terms off

But getting the same issue.


From squid3 at treenet.co.nz  Sat Jun 13 06:38:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 18:38:46 +1200
Subject: [squid-users] strip_query_terms parameter
In-Reply-To: <325735005.2125558.1434174186764.JavaMail.yahoo@mail.yahoo.com>
References: <325735005.2125558.1434174186764.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <557BCFF6.2000409@treenet.co.nz>

On 13/06/2015 5:43 p.m., Mohammad Shakir wrote:
> We are using squid 3.4.9 on centos 64bit and getting following issue.
> 
> In access.log we can not see the complete log of filehippo.com see the access.log
> 
> TCP_MISS/301 336 GET http://filehippo.com/download/file/f9efedf505ee8f42fcaab2569982d439e08c4e88dd569e7c2e68efed55ace44e/
> 
> But when we pause the download and resume it then access.log show us.
> 
> TCP_MISS_ABORTED/206 344150 GET http://fs32.filehippo.com/4054/c6334025422b451f98163c369aaf3eeb/vlc-2.2.1-win32.exe
> 

If I am interpreting those log lines and what you are saying correctly
then the top log line is the fetch, which redirects (301 status) to the
second URL.

Either the download was fetched with Range header and the
ABORTED/disconnection is how Squid receives the "pause" action from the
client, (thats normal)
 OR
the resume uses a Range request to fetch the incomplete part of the
object. The client then aborts with it still incomplete for some unknown
reason. (thats strange)

Its not clear what the problem is you speak of. What more can you say
about it?


> To get complete url log we switch off this parameter.
> 
> strip_query_terms off
> 
> But getting the same issue.

Those log lines contain no query-string. You can identify lines affected
by that parameter because they all end with a '?' in the log when its
turned on.

Amos



From tarotapprentice at yahoo.com  Sat Jun 13 08:05:10 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 13 Jun 2015 08:05:10 +0000 (UTC)
Subject: [squid-users] Suggested refresh_pattern for debian repos?
Message-ID: <1425098251.2156232.1434182710249.JavaMail.yahoo@mail.yahoo.com>

I've seen a couple of refresh_pattern combinations that people use to cache Linux repos. Does anyone have a working one for Debian repos?

MarkJ


From squid3 at treenet.co.nz  Sat Jun 13 09:36:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 21:36:24 +1200
Subject: [squid-users] Suggested refresh_pattern for debian repos?
In-Reply-To: <1425098251.2156232.1434182710249.JavaMail.yahoo@mail.yahoo.com>
References: <1425098251.2156232.1434182710249.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <557BF998.5020000@treenet.co.nz>

On 13/06/2015 8:05 p.m., TarotApprentice wrote:
> I've seen a couple of refresh_pattern combinations that people use to cache Linux repos. Does anyone have a working one for Debian repos?
> 

The debian repository responses simply dont have the right headers for
older HTTP/1.0 Squid to cache objects properly. If you try to force it
you will end up with checksum mismatch and other download errors.

The HTTP/1.1 versions of Squid (3.2+) the default refresh_patterns
should be sufficient.

That said, I have seen some of the repository servers are not producing
adequate Last-Modified headers on the .deb themselves. The popular rule
you will find all over the place for ".deb" pattern (and only that line)
seems to make it work though.

Amos


From squid3 at treenet.co.nz  Sat Jun 13 09:49:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jun 2015 21:49:16 +1200
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <1434109729.15657.7.camel@interlinx.bc.ca>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz> <1434109729.15657.7.camel@interlinx.bc.ca>
Message-ID: <557BFC9C.9050606@treenet.co.nz>

On 12/06/2015 11:48 p.m., Brian J. Murrell wrote:
> On Fri, 2015-06-12 at 10:13 +1200, Amos Jeffries wrote:
>>
>> see <http://readlist.com/lists/squid-cache.org/squid-users/11/58405.html>
> 
> Of course, I did see the rest of the messages in the thread.  I'm not
> sure what I'm supposed to be seeing in that particular message though
> other than 3.4.3 worked for the person reporting that.
> 
> So maybe I have a different problem?

On the side of "probably".

> 
>> - You speak of a squid.conf but dont show any of its contents.
> 
> auth_param negotiate program /usr/local/bin/negotiate_wrapper -d --ntlm /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/lib/squid3/negotiate_kerberos_auth
> auth_param negotiate children 20 startup=1 idle=1
> auth_param negotiate keep_alive on
> auth_param basic program /usr/lib/squid3/basic_pam_auth
> auth_param basic children 3 startup=3 idle=1
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive off
> include /etc/squid3/squid-acls.conf
> include /etc/squid3/squid-http_access.conf
> http_access allow localhost
> http_access deny all
> http_port 3128
> http_port 13128 intercept
> cache_dir aufs /var/spool/squid3 9000 16 256
> maximum_object_size 404800 KB
> cache_store_log stdio:/var/log/squid3/store.log
> strip_query_terms off
> coredump_dir /var/spool/squid3
> url_rewrite_program /usr/bin/squidGuard
> url_rewrite_children 10
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern \.rpm$          144000  100%    525600
> refresh_pattern \.deb$          144000  100%    525600
> refresh_pattern .               0       20%     4320
> connect_timeout 10 seconds
> snmp_port 3401
> snmp_access allow snmppublic linux_pc 
> snmp_access deny all
> no_cache deny url_deb 
> 
>> - You speak of 503 but show now trace or evidence from cache.log (or
>> even access.log) that might hint as a reason for it.
> 
> From access.log:
> 
> 1434107701.067   9698 10.75.22.1 TCP_MISS/503 0 CONNECT irc.bcwireless.net:6667 brian HIER_NONE/- -
> 

If I assume it got as far as trying to connect it looks like it took
10sec waiting for some response.

So,
* can all of the DNS servers in your /etc/resolv.conf return results for
irc.bcwireless.net?



> I'm happy to provide some debug trace.  I assume you don't want ALL, but
> if you let me know which debugging sections and levels would be useful
> I'd be happy to provide them.

Probably these:
  debug_options ALL,1 11,2 44,2 5,5 17,5

Amos


From tarotapprentice at yahoo.com  Sat Jun 13 14:23:59 2015
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 13 Jun 2015 14:23:59 +0000 (UTC)
Subject: [squid-users] Suggested refresh_pattern for debian repos?
In-Reply-To: <557BF998.5020000@treenet.co.nz>
References: <557BF998.5020000@treenet.co.nz>
Message-ID: <896224582.2243363.1434205439581.JavaMail.yahoo@mail.yahoo.com>

I updated 3 machines by doing apt-get update, apt-get upgrade on each in turn. I have a 10proxy file in the apt.conf.d directory that has an "acquire::http:proxy" statement. Its pointing to Squid 3.5.5.

It gave approx 50% hit rate even though they should all be identical machines. The pattern I used was:

refresh_pattern (\.deb|\.udeb)$ 1440 80% 10080
 
Should I also have a "reload-into-ims"?

MarkJ

----- Original Message -----
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Cc: 
> Sent: Saturday, 13 June 2015, 19:36
> Subject: Re: [squid-users] Suggested refresh_pattern for debian repos?
> 
> On 13/06/2015 8:05 p.m., TarotApprentice wrote:
> 
>>  I've seen a couple of refresh_pattern combinations that people use to 
> cache Linux repos. Does anyone have a working one for Debian repos?
>> 
> 
> The debian repository responses simply dont have the right headers for
> older HTTP/1.0 Squid to cache objects properly. If you try to force it
> you will end up with checksum mismatch and other download errors.
> 
> The HTTP/1.1 versions of Squid (3.2+) the default refresh_patterns
> should be sufficient.
> 
> That said, I have seen some of the repository servers are not producing
> adequate Last-Modified headers on the .deb themselves. The popular rule
> you will find all over the place for ".deb" pattern (and only that 
> line)
> seems to make it work though.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From Antony.Stone at squid.open.source.it  Sat Jun 13 14:47:25 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 13 Jun 2015 16:47:25 +0200
Subject: [squid-users] Suggested refresh_pattern for debian repos?
In-Reply-To: <896224582.2243363.1434205439581.JavaMail.yahoo@mail.yahoo.com>
References: <557BF998.5020000@treenet.co.nz>
 <896224582.2243363.1434205439581.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201506131647.25405.Antony.Stone@squid.open.source.it>

On Saturday 13 June 2015 at 16:23:59 (EU time), TarotApprentice wrote:

> I updated 3 machines by doing apt-get update, apt-get upgrade on each in
> turn. I have a 10proxy file in the apt.conf.d directory that has an
> "acquire::http:proxy" statement. Its pointing to Squid 3.5.5.
> 
> It gave approx 50% hit rate even though they should all be identical
> machines.

If you're specifically looking for a good cache hit rate for Debian packages, 
you might want to install https://packages.debian.org/jessie/apt-cacher-ng 
(maybe even on the same machine as you're running Squid on) and then point 
your /etc/apt.conf.d/10proxy files at that machine's port 3142 - it does a very 
good job for this particular purpose.


Regards,


Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From brian at interlinx.bc.ca  Sun Jun 14 14:42:58 2015
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Sun, 14 Jun 2015 10:42:58 -0400
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <557BFC9C.9050606@treenet.co.nz>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz> <1434109729.15657.7.camel@interlinx.bc.ca>
 <557BFC9C.9050606@treenet.co.nz>
Message-ID: <1434292978.32027.6.camel@interlinx.bc.ca>

On Sat, 2015-06-13 at 21:49 +1200, Amos Jeffries wrote:
> On 12/06/2015 11:48 p.m., Brian J. Murrell wrote:
> > On Fri, 2015-06-12 at 10:13 +1200, Amos Jeffries wrote:
> >>
> >> see <http://readlist.com/lists/squid-cache.org/squid-users/11/58405.html>
> > 
> > Of course, I did see the rest of the messages in the thread.  I'm not
> > sure what I'm supposed to be seeing in that particular message though
> > other than 3.4.3 worked for the person reporting that.
> > 
> > So maybe I have a different problem?
> 
> On the side of "probably".
> 
> > 
> >> - You speak of a 


> If I assume it got as far as trying to connect it looks like it took
> 10sec waiting for some response.

Indeed.  That's what it looks like to me.

> So,
> * can all of the DNS servers in your /etc/resolv.conf return results for
> irc.bcwireless.net?

Yes.  There is only one DNS server in my resolv.conf and that's the one
that runs on the same host as squid:

$  dig irc.bcwireless.net any

; <<>> DiG 9.8.1-P1 <<>> irc.bcwireless.net any
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52707
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 0

;; QUESTION SECTION:
;irc.bcwireless.net.            IN      ANY

;; ANSWER SECTION:
irc.bcwireless.net.     300     IN      AAAA    fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f
irc.bcwireless.net.     300     IN      A       198.27.79.54

;; AUTHORITY SECTION:
bcwireless.net.         162451  IN      NS      kim.ns.cloudflare.com.
bcwireless.net.         162451  IN      NS      kip.ns.cloudflare.com.

;; Query time: 52 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Sun Jun 14 10:39:21 2015
;; MSG SIZE  rcvd: 133

> Probably these:
>   debug_options ALL,1 11,2 44,2 5,5 17,5

I sent you a private e-mail with the log since it's long and as much as
I think I might have sanitized it, I don't want to unnecessarily leak
sensitive info.

Cheers,
b.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150614/c29d42a1/attachment.sig>

From squid3 at treenet.co.nz  Sun Jun 14 18:47:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 15 Jun 2015 06:47:38 +1200
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <1434292978.32027.6.camel@interlinx.bc.ca>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz> <1434109729.15657.7.camel@interlinx.bc.ca>
 <557BFC9C.9050606@treenet.co.nz> <1434292978.32027.6.camel@interlinx.bc.ca>
Message-ID: <557DCC4A.50102@treenet.co.nz>

On 15/06/2015 2:42 a.m., Brian J. Murrell wrote:
> On Sat, 2015-06-13 at 21:49 +1200, Amos Jeffries wrote:
>> On 12/06/2015 11:48 p.m., Brian J. Murrell wrote:
>>> On Fri, 2015-06-12 at 10:13 +1200, Amos Jeffries wrote:
>>>>
>>>> see <http://readlist.com/lists/squid-cache.org/squid-users/11/58405.html>
>>>
>>> Of course, I did see the rest of the messages in the thread.  I'm not
>>> sure what I'm supposed to be seeing in that particular message though
>>> other than 3.4.3 worked for the person reporting that.
>>>
>>> So maybe I have a different problem?
>>
>> On the side of "probably".
>>
>>>
>>>> - You speak of a 
> 
> 
>> If I assume it got as far as trying to connect it looks like it took
>> 10sec waiting for some response.
> 
> Indeed.  That's what it looks like to me.
> 
>> So,
>> * can all of the DNS servers in your /etc/resolv.conf return results for
>> irc.bcwireless.net?
> 
> Yes.  There is only one DNS server in my resolv.conf and that's the one
> that runs on the same host as squid:
> 
> $  dig irc.bcwireless.net any
> 
> ; <<>> DiG 9.8.1-P1 <<>> irc.bcwireless.net any
> ;; global options: +cmd
> ;; Got answer:
> ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52707
> ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 0
> 
> ;; QUESTION SECTION:
> ;irc.bcwireless.net.            IN      ANY
> 
> ;; ANSWER SECTION:
> irc.bcwireless.net.     300     IN      AAAA    fcaa:8ef7:51b9:8f04:58f1:7364:e16e:fe2f
> irc.bcwireless.net.     300     IN      A       198.27.79.54
> 
> ;; AUTHORITY SECTION:
> bcwireless.net.         162451  IN      NS      kim.ns.cloudflare.com.
> bcwireless.net.         162451  IN      NS      kip.ns.cloudflare.com.
> 
> ;; Query time: 52 msec
> ;; SERVER: 127.0.0.1#53(127.0.0.1)
> ;; WHEN: Sun Jun 14 10:39:21 2015
> ;; MSG SIZE  rcvd: 133
> 
>> Probably these:
>>   debug_options ALL,1 11,2 44,2 5,5 17,5
> 
> I sent you a private e-mail with the log since it's long and as much as
> I think I might have sanitized it, I don't want to unnecessarily leak
> sensitive info.

Sure. Thank you.


1)
I have confirmed my suspicion that your IPv6 routing is a bit broken.

The IPv6 address is in a private IP range fc00::/7. Which is the IPv6
equivalent of RFC1918 10.0.0.0/8 or 182.168.0.0/16. When your Squid
contacts it nothing at all happens within 10 seconds.

What should be happening is that when Squid tries to contact a network
range which is not your own LAN fd31:aeb1:48df::/48 sub-net. The routing
system in your Squid machine responds with a "network unavailable"
within nanoseconds. This signal which is not appearing is what Squid
depends on to to do the failover.

 Are you blocking ICMP? if so remove that. ICMP is not optional even in
IPv4.

 Do you have an entry in the routing table for fc00::/7 or /8 ? in
particular one larger than the fd31::/16 network your machines are
talking on. If so, narrow it down to only your actual LAN range.


2)
Squid should still be attemoting the IPv4 address though. Instead its
just responding with an error and closing.doing failover though, but the
CONNECT handling is for some reason seems to be generating an error page
and deliverign it to teh celint iinstead of trying the IPv4.


I also see you have Squid-3.4.3. Can you try an upgrade to 3.5.5 ? there
have been some deep code stability fixes in this area since your version.

If the problem persists after that upgrade. Please redo with 26,5 in the
debug list.

Also, check your proxy connect_timeout is shorter than forward_timeout.
The connect timout affects these individual connections, forward_timeout
needs to allow mulitiple (25?) of them for failover to have a chance in
lost packet situations like this.

Amos



From hack.back at hotmail.com  Sun Jun 14 21:17:00 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 14 Jun 2015 14:17:00 -0700 (PDT)
Subject: [squid-users] Installing certificate on Andriod to use with
	SSL-bump
In-Reply-To: <1433911585718-4671645.post@n4.nabble.com>
References: <1433911585718-4671645.post@n4.nabble.com>
Message-ID: <1434316620122-4671732.post@n4.nabble.com>

> To be clear, I see the phone use
> port 443 to setup a secure session. However it rejects the certificate (as
> it should) and terminates the session with no data being passed. I can
> install my certificate on the phone, but will the android OS use that
> certificate for all services or only for browser sessions? 

yes the certificate will work for all services on the ondroid OS .
but you will get warning message that your mobile maybe monitored by 3rd
party.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Installing-certificate-on-Andriod-to-use-with-SSL-bump-tp4671645p4671732.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jun 14 21:21:22 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 14 Jun 2015 14:21:22 -0700 (PDT)
Subject: [squid-users] problem with some ssl services
Message-ID: <1434316882187-4671733.post@n4.nabble.com>

In some applications on mobiles, (ANDROID , APPLE)
there is problem with ssl connections from squid.
like GOOGLE PLAY app, facebook app, some games app,
the app will not open when i redirect traffic to squid , but when i make
torch on the traffic and i got the ip that are not passed, and then i put
this ip in ssl none bump then the app work.
this happen weekly, every week i need to bypass none ssl bump new ip's to
make these app's working fine,
what cause this problem and how we can not face it ?
Thanks .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/problem-with-some-ssl-services-tp4671733.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jun 14 23:58:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 15 Jun 2015 11:58:52 +1200
Subject: [squid-users] problem with some ssl services
In-Reply-To: <1434316882187-4671733.post@n4.nabble.com>
References: <1434316882187-4671733.post@n4.nabble.com>
Message-ID: <557E153C.1030308@treenet.co.nz>

On 15/06/2015 9:21 a.m., HackXBack wrote:
> In some applications on mobiles, (ANDROID , APPLE)
> there is problem with ssl connections from squid.
> like GOOGLE PLAY app, facebook app, some games app,
> the app will not open when i redirect traffic to squid , but when i make
> torch on the traffic and i got the ip that are not passed, and then i put
> this ip in ssl none bump then the app work.
> this happen weekly, every week i need to bypass none ssl bump new ip's to
> make these app's working fine,
> what cause this problem and how we can not face it ?

Software which is correctly using TLS cannot be bumped.

The "problem" is that you are attacking those applications TLS
connections. They are simply defending against you by improving their
use of TLS.

Ensure that you are using the very latest Squid version to avoid
problems with unsupported TLS mechanisms. The latest Squid will also
automatically splice if its determined that the TLS connection cannot be
bumped.

Amos



From vdoctor at neuf.fr  Mon Jun 15 07:44:50 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 15 Jun 2015 00:44:50 -0700 (PDT)
Subject: [squid-users] TCP_MISS_ABORTED/000 with SIBLING
Message-ID: <1434354290052-4671735.post@n4.nabble.com>

Hi All,

Weird issue with 2 Squid 3.5.5 in sibling mode, here is the trace:
...... TCP_MISS_ABORTED/000 0 GET http://www.greatandhra.com/ -
SIBLING_HIT/x.x.x.x

Any idea ?
Thanks in advance for your inputs...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-ABORTED-000-with-SIBLING-tp4671735.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Jun 15 08:05:41 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 15 Jun 2015 01:05:41 -0700 (PDT)
Subject: [squid-users] problem with some ssl services
In-Reply-To: <557E153C.1030308@treenet.co.nz>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz>
Message-ID: <1434355541146-4671736.post@n4.nabble.com>

what peak and splice conf should i use to make it work fine ?
am still using 3.4, i will upgrade to 3.5



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/problem-with-some-ssl-services-tp4671733p4671736.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From budiyt at gmail.com  Mon Jun 15 10:28:44 2015
From: budiyt at gmail.com (budsz)
Date: Mon, 15 Jun 2015 17:28:44 +0700
Subject: [squid-users] Redirection not work properly
Message-ID: <CADM2n7h4vzm=jH6yyOY160MCtmyGfsWVuW=2-5NXDPDOa7EOLg@mail.gmail.com>

Hi,

I use squid 3.5.5, in squid 2.7 series this problem doesn't show.

in squid.conf
url_rewrite_program /usr/local/libexec/jesred
url_rewrite_children 30

in jesred, redirect.rules contain something like this:
regex   ^.*/(.*)_chrome_installer\.exe$
http://files.example.net/rdr/43.0.2357.124_chrome_installer.exe

in webserver, showing error log, which is missing filename, of cause
produce 404 (page not found error).

xxx.xxx.xxx.xxx files.example.net - [15/Jun/2015:17:01:23 +0700] "GET
/rdr/43.0.2357.124_chrome_installer.exeyyy.yyy.yyy.yyy/pcbill.example.net-GET
HTTP/1.1" 404 345 "-" "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36"

The right filename is: "43.0.2357.124_chrome_installer.exe" NOT
"43.0.2357.124_chrome_installer.exeyyy.yyy.yyy.yyy/pcbill.example.net-GET"

Anyone have suggestion for this issue?

Thank you

-- 
budsz


From david at articatech.com  Mon Jun 15 11:10:18 2015
From: david at articatech.com (David Touzeau)
Date: Mon, 15 Jun 2015 13:10:18 +0200
Subject: [squid-users] BUG 3279: HTTP reply without Date:
In-Reply-To: <CALP3=x9O7GTf5h=GE98wB2x2hKc2LEfc8XJFLmb4J_yOyAVOaA@mail.gmail.com>
References: <CALP3=x9O7GTf5h=GE98wB2x2hKc2LEfc8XJFLmb4J_yOyAVOaA@mail.gmail.com>
Message-ID: <557EB29A.5070001@articatech.com>

We encounter the same issue with Squid 3.5.5
using diskd limit crashes but access to web pages are freezed trough squid

Le 12/04/2015 16:55, Monah Baki a ?crit :
> Hi all,
>
> Compiled squid 3.5.2 on CentOS 6.6 as follows:
> $ ./configure --prefix=/home/cache --enable-follow-x-forwarded-for 
> --with-large-files --enable-ssl --disable-ipv6 --enable-esi 
> --enable-kill-parent-hack --enable-snmp --with-pthreads 
> --with-filedescriptors=65535 --enable-cachemgr-hostname=hostname 
> --enable-storeio=ufs,aufs,diskd,rock
>
> After approx 24 hours I am seeing this error on my squid 3.5.2 with 
> one user connected for testing:
>
> 2015/04/11 15:02:58| Logfile: closing log 
> daemon:/home/cache/var/logs/access.log
> 2015/04/11 15:02:58| Logfile Daemon: closing log 
> daemon:/home/cache/var/logs/access.log
> 2015/04/11 15:02:58| Open FD UNSTARTED     0 stdin
> 2015/04/11 15:02:58| Open FD UNSTARTED     1 stdout
> 2015/04/11 15:02:58| Open FD UNSTARTED     2 stderr
> 2015/04/11 15:02:58| Open FD UNSTARTED     8 DNS Socket IPv4
> 2015/04/11 15:02:58| Open FD UNSTARTED     9 IPC UNIX STREAM Parent
> 2015/04/11 15:02:58| Squid Cache (Version 3.5.2): Exiting normally.
> 2015/04/11 15:06:52| Set Current Directory to 
> /usr/local/squid/var/cache/squid
> 2015/04/11 15:06:52| Starting Squid Cache version 3.5.2 for 
> x86_64-unknown-linux-gnu...
> 2015/04/11 15:06:52| Service Name: squid
> 2015/04/11 15:06:52| Process ID 2005
> 2015/04/11 15:06:52| Process Roles: master worker
> 2015/04/11 15:06:52| With 65536 file descriptors available
> 2015/04/11 15:06:52| Initializing IP Cache...
> 2015/04/11 15:06:52| DNS Socket created at 0.0.0.0, FD 8
> 2015/04/11 15:06:52| Adding nameserver 8.8.8.8 from squid.conf
> 2015/04/11 15:06:52| Adding nameserver 41.78.211.30 from squid.conf
> 2015/04/11 15:06:52| Logfile: opening log 
> daemon:/home/cache/var/logs/access.log
> 2015/04/11 15:06:52| Logfile Daemon: opening log 
> /home/cache/var/logs/access.log
> 2015/04/11 15:06:52| Store logging disabled
> 2015/04/11 15:06:52| Swap maxSize 358400000 + 9437184 KB, estimated 
> 28295168 objects
> 2015/04/11 15:06:52| Target number of buckets: 1414758
> 2015/04/11 15:06:52| Using 2097152 Store buckets
> 2015/04/11 15:06:52| Max Mem  size: 9437184 KB
> 2015/04/11 15:06:52| Max Swap size: 358400000 KB
> 2015/04/11 15:06:52| Rebuilding storage in /home/cache/var/cache/squid 
> (clean log)
> 2015/04/11 15:06:52| Using Least Load store dir selection
> 2015/04/11 15:06:52| Set Current Directory to 
> /usr/local/squid/var/cache/squid
> 2015/04/11 15:06:52| Finished loading MIME types and icons.
> 2015/04/11 15:06:52| HTCP Disabled.
> 2015/04/11 15:06:52| Sending SNMP messages from 0.0.0.0:3401 
> <http://0.0.0.0:3401>
> 2015/04/11 15:06:52| Squid plugin modules loaded: 0
> 2015/04/11 15:06:52| Adaptation support is off.
> 2015/04/11 15:06:52| Accepting HTTP Socket connections at 
> local=0.0.0.0:3128 <http://0.0.0.0:3128> remote=[::] FD 13 flags=9
> 2015/04/11 15:06:52| Accepting NAT intercepted HTTP Socket connections 
> at local=0.0.0.0:3129 <http://0.0.0.0:3129> remote=[::] FD 14 flags=41
> 2015/04/11 15:06:52| Accepting SNMP messages on 0.0.0.0:3401 
> <http://0.0.0.0:3401>
> 2015/04/11 15:06:52| Done reading /home/cache/var/cache/squid swaplog 
> (94 entries)
> 2015/04/11 15:06:52| Finished rebuilding storage from disk.
> 2015/04/11 15:06:52|        94 Entries scanned
> 2015/04/11 15:06:52|         0 Invalid entries.
> 2015/04/11 15:06:52|         0 With invalid flags.
> 2015/04/11 15:06:52|        94 Objects loaded.
> 2015/04/11 15:06:52|         0 Objects expired.
> 2015/04/11 15:06:52|         0 Objects cancelled.
> 2015/04/11 15:06:52|         0 Duplicate URLs purged.
> 2015/04/11 15:06:52|         0 Swapfile clashes avoided.
> 2015/04/11 15:06:52|   Took 0.05 seconds (2036.97 objects/sec).
> 2015/04/11 15:06:52| Beginning Validation Procedure
> 2015/04/11 15:06:52|   Completed Validation Procedure
> 2015/04/11 15:06:52|   Validated 94 Entries
> 2015/04/11 15:06:52|   store_swap_size = 2000.00 KB
> 2015/04/11 15:06:53| storeLateRelease: released 0 objects
> 2015/04/11 15:48:51| WARNING: 1 swapin MD5 mismatches
> 2015/04/11 15:48:51| Could not parse headers from on disk object
> 2015/04/11 15:48:51| BUG 3279: HTTP reply without Date:
> 2015/04/11 15:48:51| StoreEntry->key: 039CA6C6725D0A9F31B498354995DE50
> 2015/04/11 15:48:51| StoreEntry->next: 0
> 2015/04/11 15:48:51| StoreEntry->mem_obj: 0x21ecd40
> 2015/04/11 15:48:51| StoreEntry->timestamp: -1
> 2015/04/11 15:48:51| StoreEntry->lastref: 1428763731
> 2015/04/11 15:48:51| StoreEntry->expires: -1
> 2015/04/11 15:48:51| StoreEntry->lastmod: -1
> 2015/04/11 15:48:51| StoreEntry->swap_file_sz: 0
> 2015/04/11 15:48:51| StoreEntry->refcount: 1
> 2015/04/11 15:48:51| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/04/11 15:48:51| StoreEntry->swap_dirn: -1
> 2015/04/11 15:48:51| StoreEntry->swap_filen: -1
> 2015/04/11 15:48:51| StoreEntry->lock_count: 2
> 2015/04/11 15:48:51| StoreEntry->mem_status: 0
> 2015/04/11 15:48:51| StoreEntry->ping_status: 2
> 2015/04/11 15:48:51| StoreEntry->store_status: 1
> 2015/04/11 15:48:51| StoreEntry->swap_status: 0
> 2015/04/11 15:49:55| Could not parse headers from on disk object
> 2015/04/11 20:10:06| BUG 3279: HTTP reply without Date:
> 2015/04/11 20:10:06| StoreEntry->key: 8749EF6C14DB515AA7E09A4ED2019298
> 2015/04/11 20:10:06| StoreEntry->next: 0
> 2015/04/11 20:10:06| StoreEntry->mem_obj: 0x224f3f0
> 2015/04/11 20:10:06| StoreEntry->timestamp: -1
> 2015/04/11 20:10:06| StoreEntry->lastref: 1428779406
> 2015/04/11 20:10:06| StoreEntry->expires: -1
> 2015/04/11 20:10:06| StoreEntry->lastmod: -1
> 2015/04/11 20:10:06| StoreEntry->swap_file_sz: 0
> 2015/04/11 20:10:06| StoreEntry->refcount: 1
> 2015/04/11 20:10:06| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/04/11 20:10:06| StoreEntry->swap_dirn: -1
> 2015/04/11 20:10:06| StoreEntry->swap_filen: -1
> 2015/04/11 20:10:06| StoreEntry->lock_count: 2
> 2015/04/11 20:10:06| StoreEntry->mem_status: 0
> 2015/04/11 20:10:06| StoreEntry->ping_status: 2
> 2015/04/11 20:10:06| StoreEntry->store_status: 1
> 2015/04/11 20:10:06| StoreEntry->swap_status: 0
> 2015/04/12 03:54:21| Could not parse headers from on disk object
> 2015/04/12 03:54:21| BUG 3279: HTTP reply without Date:
> 2015/04/12 03:54:21| StoreEntry->key: 2664F79F89A842E097DCD721C4417644
> 2015/04/12 03:54:21| StoreEntry->next: 0
> 2015/04/12 03:54:21| StoreEntry->mem_obj: 0x23686e0
> 2015/04/12 03:54:21| StoreEntry->timestamp: -1
> 2015/04/12 03:54:21| StoreEntry->lastref: 1428807261
> 2015/04/12 03:54:21| StoreEntry->expires: -1
> 2015/04/12 03:54:21| StoreEntry->lastmod: -1
> 2015/04/12 03:54:21| StoreEntry->swap_file_sz: 0
> 2015/04/12 03:54:21| StoreEntry->refcount: 1
> 2015/04/12 03:54:21| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/04/12 03:54:21| StoreEntry->swap_dirn: -1
> 2015/04/12 03:54:21| StoreEntry->swap_filen: -1
> 2015/04/12 03:54:21| StoreEntry->lock_count: 2
> 2015/04/12 03:54:21| StoreEntry->mem_status: 0
> 2015/04/12 03:54:21| StoreEntry->ping_status: 2
> 2015/04/12 03:54:21| StoreEntry->store_status: 1
> 2015/04/12 03:54:21| StoreEntry->swap_status: 0
> 2015/04/12 03:55:24| Could not parse headers from on disk object
> 2015/04/12 03:56:24| StoreEntry->swap_status: 0
> 2015/04/12 03:56:24| assertion failed: store.cc:1885: "isEmpty()"
>
>
>
>
> Thank you
> Monah
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150615/bf7f7b3b/attachment.htm>

From tmblue at gmail.com  Mon Jun 15 22:25:31 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 15 Jun 2015 15:25:31 -0700
Subject: [squid-users] How to stop " ICP is disabled! Cannot send ICP
 request to peer."
In-Reply-To: <5570D7AC.9080905@treenet.co.nz>
References: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>
 <5570D7AC.9080905@treenet.co.nz>
Message-ID: <CAEaSS0YjM7HuQ6GXAi-jxnBh8g04t35wm0q8qEDjZWP6zt2oig@mail.gmail.com>

On Thu, Jun 4, 2015 at 3:56 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 5/06/2015 5:58 a.m., Tory M Blue wrote:
> > I am running HDCP or at least testing with it and thus have ICP
> disabled. I
> > know it's disabled but I don't need it yelling at me every few
> > minutes/seconds. How can I tell Squid, yes thank you, I'm aware I'm not
> > using ICP and it's disabled, now quiet?!
>
> You have to configure the 'htcp' option on cache_peer lines in squid.conf.
>
> Amos
>
>
> Thanks Amos

I thought I was there but am not.

2015/06/15 15:23:20 kid1| ICP is disabled! Cannot send ICP request to peer.

http_port 80 accel vhost

cache_peer apps.domain.net parent 80 0 no-digest no-query originserver
no-netdb-exchange

cache_peer cache02.domain.net sibling 80 4827 no-delay no-netdb-exchange

htcp_port 4827

htcp_access allow domain


What am I missing, sorry for being dense!


Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150615/b32d99ae/attachment.htm>

From brian at interlinx.bc.ca  Mon Jun 15 23:12:20 2015
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jun 2015 19:12:20 -0400
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <557DCC4A.50102@treenet.co.nz>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz> <1434109729.15657.7.camel@interlinx.bc.ca>
 <557BFC9C.9050606@treenet.co.nz> <1434292978.32027.6.camel@interlinx.bc.ca>
 <557DCC4A.50102@treenet.co.nz>
Message-ID: <1434409940.32027.102.camel@interlinx.bc.ca>

On Mon, 2015-06-15 at 06:47 +1200, Amos Jeffries wrote:
> 
> 1)
> I have confirmed my suspicion that your IPv6 routing is a bit broken.

I'm not sure I agree with you entirely on that (more below)...

> The IPv6 address is in a private IP range fc00::/7.

Oh damn.  It's a ULA address.  I did not even recognize that.  The IPv6
addressing space is still foreign enough to me that I don't immediately
recognize what bits of the space certain addresses are in, unlike my
instant recognition of IPv4 private IP space.

> Which is the IPv6
> equivalent of RFC1918 10.0.0.0/8 or 182.168.0.0/16.

s/182/192/, but yes, indeed.  I see that now.

> When your Squid
> contacts it nothing at all happens within 10 seconds.

Unsurprisingly, of course.

> What should be happening is that when Squid tries to contact a network
> range which is not your own LAN fd31:aeb1:48df::/48 sub-net.

It should forward it to the Squid machine's default router.

> The routing
> system in your Squid machine responds with a "network unavailable"
> within nanoseconds.

I disagree.  While I will agree that it is good network management
practice to not allow RFC-1918 and IPv6's corresponding ULA addresses to
leak outside of your network, AFAIK, it's not a requirement.  RFC 4193
4.3 makes it a recommendation ("should") rather than a requirement
("MUST").

This is really no different than RFC-1918 addresses, which were not
always designated for their current practice and only since that
designation are also only recommended against blocking at the site
router.

> This signal which is not appearing is what Squid
> depends on to to do the failover.

(As you point out below, so I think we are in agreement) I would think
that a simple timeout to connect should be just as sufficient, just like
any other piece of software, such as telnet for example.  I would think
no software should really rely on the behaviors of (only) recommended
practices.

> Do you have an entry in the routing table for fc00::/7 or /8 ?

No.

> 2)
> Squid should still be attemoting the IPv4 address though.

Agreed.

> I also see you have Squid-3.4.3. Can you try an upgrade to 3.5.5 ?

OK.  Built and installed.

Still doesn't seem to be working.

> If the problem persists after that upgrade. Please redo with 26,5 in the
> debug list.

Sent to your e-mail.

> Also, check your proxy connect_timeout is shorter than forward_timeout.

#Default:
# forward_timeout 4 minutes

So seems it is.

> The connect timout affects these individual connections, forward_timeout
> needs to allow mulitiple (25?) of them for failover to have a chance in
> lost packet situations like this.

Interestingly enough, my override of the default of connect_timeout to
10 seconds actually makes it as close to 25 times less than
forward_timeout as I could reasonably get.  :-)

b.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150615/62a30f29/attachment.sig>

From squid3 at treenet.co.nz  Mon Jun 15 23:34:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jun 2015 11:34:03 +1200
Subject: [squid-users] How to stop " ICP is disabled! Cannot send ICP
 request to peer."
In-Reply-To: <CAEaSS0YjM7HuQ6GXAi-jxnBh8g04t35wm0q8qEDjZWP6zt2oig@mail.gmail.com>
References: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>	<5570D7AC.9080905@treenet.co.nz>
 <CAEaSS0YjM7HuQ6GXAi-jxnBh8g04t35wm0q8qEDjZWP6zt2oig@mail.gmail.com>
Message-ID: <557F60EB.1040705@treenet.co.nz>

On 16/06/2015 10:25 a.m., Tory M Blue wrote:
> On Thu, Jun 4, 2015 at 3:56 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 5/06/2015 5:58 a.m., Tory M Blue wrote:
>>> I am running HDCP or at least testing with it and thus have ICP
>> disabled. I
>>> know it's disabled but I don't need it yelling at me every few
>>> minutes/seconds. How can I tell Squid, yes thank you, I'm aware I'm not
>>> using ICP and it's disabled, now quiet?!
>>
>> You have to configure the 'htcp' option on cache_peer lines in squid.conf.
>>
>> Amos
>>
>>
>> Thanks Amos
> 
> I thought I was there but am not.
> 
> 2015/06/15 15:23:20 kid1| ICP is disabled! Cannot send ICP request to peer.
> 
> http_port 80 accel vhost
> 
> cache_peer apps.domain.net parent 80 0 no-digest no-query originserver
> no-netdb-exchange
> 
> cache_peer cache02.domain.net sibling 80 4827 no-delay no-netdb-exchange

Add "htcp" option to switch from ICP to HTCP.

> 
> htcp_port 4827
> 
> htcp_access allow domain
> 
> 
> What am I missing, sorry for being dense!
> 
> 
> Tory
> 

Amos


From tmblue at gmail.com  Mon Jun 15 23:35:11 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 15 Jun 2015 16:35:11 -0700
Subject: [squid-users] How to stop " ICP is disabled! Cannot send ICP
 request to peer."
In-Reply-To: <557F60EB.1040705@treenet.co.nz>
References: <CAEaSS0Zv2juJvOhxirTxOMBGEW2uo4kUiKPRzoDVmQDVZMSTUg@mail.gmail.com>
 <5570D7AC.9080905@treenet.co.nz>
 <CAEaSS0YjM7HuQ6GXAi-jxnBh8g04t35wm0q8qEDjZWP6zt2oig@mail.gmail.com>
 <557F60EB.1040705@treenet.co.nz>
Message-ID: <CAEaSS0at95s4sdoOixehrGsS2ULkbXuL_WzSgv=+13o54Q_rXw@mail.gmail.com>

On Mon, Jun 15, 2015 at 4:34 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 16/06/2015 10:25 a.m., Tory M Blue wrote:
> > On Thu, Jun 4, 2015 at 3:56 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> >
> >> On 5/06/2015 5:58 a.m., Tory M Blue wrote:
> >>> I am running HDCP or at least testing with it and thus have ICP
> >> disabled. I
> >>> know it's disabled but I don't need it yelling at me every few
> >>> minutes/seconds. How can I tell Squid, yes thank you, I'm aware I'm not
> >>> using ICP and it's disabled, now quiet?!
> >>
> >> You have to configure the 'htcp' option on cache_peer lines in
> squid.conf.
> >>
> >> Amos
> >>
> >>
> >> Thanks Amos
> >
> > I thought I was there but am not.
> >
> > 2015/06/15 15:23:20 kid1| ICP is disabled! Cannot send ICP request to
> peer.
> >
> > http_port 80 accel vhost
> >
> > cache_peer apps.domain.net parent 80 0 no-digest no-query originserver
> > no-netdb-exchange
> >
> > cache_peer cache02.domain.net sibling 80 4827 no-delay no-netdb-exchange
>
> Add "htcp" option to switch from ICP to HTCP.
>
> >
> > htcp_port 4827
> >
> > htcp_access allow domain
> >
> >
> > What am I missing, sorry for being dense!
> >
> >
> > Tory
> >
>
> Amos
>

BAHHH thought that the port number was all that was needed..

Thanks!! I did honestly consider putting the word htcp there :)

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150615/4c1ce759/attachment.htm>

From tmblue at gmail.com  Mon Jun 15 23:38:45 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 15 Jun 2015 16:38:45 -0700
Subject: [squid-users] New 3.5.1 error
Message-ID: <CAEaSS0Yh-R-5NizbvAXDMeZn55D0yo8H6=+Vpvc0A5X0Upcx8A@mail.gmail.com>

2015/06/15 16:36:29 kid1|
'/usr/share/squid/errors/es-us/ERR_ONLY_IF_CACHED_MISS': (2) No such file
or directory

2015/06/15 16:36:29 kid1| WARNING: Error Pages Missing Language: es-us


got this a few times when trying to enable htcp.


Any ideas?


Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150615/388c49f6/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 15 23:41:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jun 2015 11:41:50 +1200
Subject: [squid-users] Redirection not work properly
In-Reply-To: <CADM2n7h4vzm=jH6yyOY160MCtmyGfsWVuW=2-5NXDPDOa7EOLg@mail.gmail.com>
References: <CADM2n7h4vzm=jH6yyOY160MCtmyGfsWVuW=2-5NXDPDOa7EOLg@mail.gmail.com>
Message-ID: <557F62BE.7010707@treenet.co.nz>

On 15/06/2015 10:28 p.m., budsz wrote:
> Hi,
> 
> I use squid 3.5.5, in squid 2.7 series this problem doesn't show.
> 

I suggest to use Store-ID instead of redirection and retain the chrome
version number in the store key. That way you do not have to manually
handle when chrome binaries change.

Amos



From squid3 at treenet.co.nz  Mon Jun 15 23:59:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jun 2015 11:59:22 +1200
Subject: [squid-users] New 3.5.1 error
In-Reply-To: <CAEaSS0Yh-R-5NizbvAXDMeZn55D0yo8H6=+Vpvc0A5X0Upcx8A@mail.gmail.com>
References: <CAEaSS0Yh-R-5NizbvAXDMeZn55D0yo8H6=+Vpvc0A5X0Upcx8A@mail.gmail.com>
Message-ID: <557F66DA.2020403@treenet.co.nz>

On 16/06/2015 11:38 a.m., Tory M Blue wrote:
> 2015/06/15 16:36:29 kid1|
> '/usr/share/squid/errors/es-us/ERR_ONLY_IF_CACHED_MISS': (2) No such file
> or directory
> 
> 2015/06/15 16:36:29 kid1| WARNING: Error Pages Missing Language: es-us
> 
> 
> got this a few times when trying to enable htcp.
> 
> 
> Any ideas?

Nothing major. Just a dialect alias missing from the translation
tempates. I have added it now. Thanks.

Amos



From squid3 at treenet.co.nz  Tue Jun 16 02:30:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jun 2015 14:30:06 +1200
Subject: [squid-users] no fallback to ipv4 if ipv6 remote address is
	non-functional
In-Reply-To: <1434409940.32027.102.camel@interlinx.bc.ca>
References: <1434059772.16022.67.camel@interlinx.bc.ca>
 <557A0800.7040207@treenet.co.nz> <1434109729.15657.7.camel@interlinx.bc.ca>
 <557BFC9C.9050606@treenet.co.nz> <1434292978.32027.6.camel@interlinx.bc.ca>
 <557DCC4A.50102@treenet.co.nz> <1434409940.32027.102.camel@interlinx.bc.ca>
Message-ID: <557F8A2E.3060505@treenet.co.nz>

On 16/06/2015 11:12 a.m., Brian J. Murrell wrote:
> On Mon, 2015-06-15 at 06:47 +1200, Amos Jeffries wrote:
>>
>> 1)
>> I have confirmed my suspicion that your IPv6 routing is a bit broken.
> 
> I'm not sure I agree with you entirely on that (more below)...
> 
>> The IPv6 address is in a private IP range fc00::/7.
> 
> Oh damn.  It's a ULA address.  I did not even recognize that.  The IPv6
> addressing space is still foreign enough to me that I don't immediately
> recognize what bits of the space certain addresses are in, unlike my
> instant recognition of IPv4 private IP space.
> 
>> Which is the IPv6
>> equivalent of RFC1918 10.0.0.0/8 or 182.168.0.0/16.
> 
> s/182/192/, but yes, indeed.  I see that now.
> 
>> When your Squid
>> contacts it nothing at all happens within 10 seconds.
> 
> Unsurprisingly, of course.
> 
>> What should be happening is that when Squid tries to contact a network
>> range which is not your own LAN fd31:aeb1:48df::/48 sub-net.
> 
> It should forward it to the Squid machine's default router.
> 
>> The routing
>> system in your Squid machine responds with a "network unavailable"
>> within nanoseconds.
> 
> I disagree.  While I will agree that it is good network management
> practice to not allow RFC-1918 and IPv6's corresponding ULA addresses to
> leak outside of your network, AFAIK, it's not a requirement.  RFC 4193
> 4.3 makes it a recommendation ("should") rather than a requirement
> ("MUST").
> 
> This is really no different than RFC-1918 addresses, which were not
> always designated for their current practice and only since that
> designation are also only recommended against blocking at the site
> router.

There are hardware requirements on router devices to reject F000::/4
packets which does not exist in IPv4. But things get very muddy with
virtual routers etc, so effectively yes.

2000::/3 is global IPv6 allocation space. These packets should be the
only ones using default route.

F000::/4 is various internal LAN spaces. Packets should never cross a
border gateway/router and not follow a default route.
 - Unless its explicitly defaulting 'inwards' to the LAN. But even then
very carefully so as to ensure external packets are rejected on arrival
at the border.

All other IPv6 ranges are bogon at present and need to be rejected.


> 
>> This signal which is not appearing is what Squid
>> depends on to to do the failover.
> 
> (As you point out below, so I think we are in agreement) I would think
> that a simple timeout to connect should be just as sufficient, just like
> any other piece of software, such as telnet for example.  I would think
> no software should really rely on the behaviors of (only) recommended
> practices.

Not relying on it for regular use. One never knows whether that fc*
range is the local LAN range or not. But when its acting properly there
is no timeout delay and the network operates at peak efficiency.

> 
>> Do you have an entry in the routing table for fc00::/7 or /8 ?
> 
> No.
> 
>> 2)
>> Squid should still be attemoting the IPv4 address though.
> 
> Agreed.
> 
>> I also see you have Squid-3.4.3. Can you try an upgrade to 3.5.5 ?
> 
> OK.  Built and installed.
> 
> Still doesn't seem to be working.
> 
>> If the problem persists after that upgrade. Please redo with 26,5 in the
>> debug list.
> 
> Sent to your e-mail.
> 
>> Also, check your proxy connect_timeout is shorter than forward_timeout.
> 
> #Default:
> # forward_timeout 4 minutes
> 
> So seems it is.
> 
>> The connect timout affects these individual connections, forward_timeout
>> needs to allow mulitiple (25?) of them for failover to have a chance in
>> lost packet situations like this.
> 
> Interestingly enough, my override of the default of connect_timeout to
> 10 seconds actually makes it as close to 25 times less than
> forward_timeout as I could reasonably get.  :-)

Good.

Amos


From shakirgil at yahoo.com  Tue Jun 16 05:31:10 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Tue, 16 Jun 2015 05:31:10 +0000 (UTC)
Subject: [squid-users] strip_query_terms parameter
In-Reply-To: <557BCFF6.2000409@treenet.co.nz>
References: <557BCFF6.2000409@treenet.co.nz>
Message-ID: <415448635.3707725.1434432670083.JavaMail.yahoo@mail.yahoo.com>

We are using store_id_program to cache CDN objects due to (301 status) we can not catch the actual url.

Is there any other procedure to override this problem.

given below is our configuration for store_id_program


acl gotostoreid dstdomain .filehippo.com
acl banned_methods method PUT POST PURGE
acl norewrite url_regex -i \.(youtube|googlevideo)\.com\/videoplayback.*redirect_counter=1.*cms_redirect=yes.* \.(youtube|googlevideo)\.com\/videoplayback.*cms_redirect=yes.*redirect_counter=1.*

store_id_bypass on

store_id_program /etc/squid/storeid.pl
store_id_children 40 startup=10 idle=1 concurrency=0
store_id_access allow gotostoreid
store_id_access deny norewrite
store_id_access deny banned_methods
store_id_access deny all




On Saturday, June 13, 2015 11:39 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 13/06/2015 5:43 p.m., Mohammad Shakir wrote:
> We are using squid 3.4.9 on centos 64bit and getting following issue.
> 
> In access.log we can not see the complete log of filehippo.com see the access.log
> 
> TCP_MISS/301 336 GET http://filehippo.com/download/file/f9efedf505ee8f42fcaab2569982d439e08c4e88dd569e7c2e68efed55ace44e/
> 
> But when we pause the download and resume it then access.log show us.
> 
> TCP_MISS_ABORTED/206 344150 GET http://fs32.filehippo.com/4054/c6334025422b451f98163c369aaf3eeb/vlc-2.2.1-win32.exe
> 

If I am interpreting those log lines and what you are saying correctly
then the top log line is the fetch, which redirects (301 status) to the
second URL.

Either the download was fetched with Range header and the
ABORTED/disconnection is how Squid receives the "pause" action from the
client, (thats normal)
OR
the resume uses a Range request to fetch the incomplete part of the
object. The client then aborts with it still incomplete for some unknown
reason. (thats strange)

Its not clear what the problem is you speak of. What more can you say
about it?



> To get complete url log we switch off this parameter.
> 
> strip_query_terms off
> 
> But getting the same issue.

Those log lines contain no query-string. You can identify lines affected
by that parameter because they all end with a '?' in the log when its
turned on.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From david at articatech.com  Tue Jun 16 13:11:10 2015
From: david at articatech.com (David Touzeau)
Date: Tue, 16 Jun 2015 15:11:10 +0200
Subject: [squid-users] Tips to reduce cache
Message-ID: <5580206E.9040800@articatech.com>


Hi all,

I need to force squid to cache some websites only for one hour ( no more)
did this refresh_pattern directive is able to answerto this need ?

What is the best refresh_pattern value  to force a website to be cached 
only for one hour ?

best regards


From squid3 at treenet.co.nz  Tue Jun 16 18:26:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2015 06:26:12 +1200
Subject: [squid-users] Tips to reduce cache
In-Reply-To: <5580206E.9040800@articatech.com>
References: <5580206E.9040800@articatech.com>
Message-ID: <55806A44.7050008@treenet.co.nz>

On 17/06/2015 1:11 a.m., David Touzeau wrote:
> 
> Hi all,
> 
> I need to force squid to cache some websites only for one hour ( no more)
> did this refresh_pattern directive is able to answerto this need ?
> 
> What is the best refresh_pattern value  to force a website to be cached
> only for one hour ?

For that to happen one of three things need to happen:
* The client needs to send Cache-Control:max-age=3600 to Squid in its
request.
* The server sends Cache-Control:max-age=3600, or
Cache-Control:s-maxage=3600 or Expires: 1hr in the future in its reply.
* The server sends Date and Last-Modified BUT NOT Expires or
Cache-Control:, AND you have a refresh_pattern with value "60" in the
max-age field for the relevant URLs.

Why the bother?

Amos



From squid3 at treenet.co.nz  Tue Jun 16 18:35:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2015 06:35:16 +1200
Subject: [squid-users] strip_query_terms parameter
In-Reply-To: <415448635.3707725.1434432670083.JavaMail.yahoo@mail.yahoo.com>
References: <557BCFF6.2000409@treenet.co.nz>
 <415448635.3707725.1434432670083.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55806C64.6060501@treenet.co.nz>

On 16/06/2015 5:31 p.m., Mohammad Shakir wrote:
> We are using store_id_program to cache CDN objects due to (301 status) we can not catch the actual url.
> 
> Is there any other procedure to override this problem.
> 
> given below is our configuration for store_id_program
> 
> 
> acl gotostoreid dstdomain .filehippo.com
> acl banned_methods method PUT POST PURGE
> acl norewrite url_regex -i \.(youtube|googlevideo)\.com\/videoplayback.*redirect_counter=1.*cms_redirect=yes.* \.(youtube|googlevideo)\.com\/videoplayback.*cms_redirect=yes.*redirect_counter=1.*
> 
> store_id_bypass on
> 
> store_id_program /etc/squid/storeid.pl
> store_id_children 40 startup=10 idle=1 concurrency=0
> store_id_access allow gotostoreid
> store_id_access deny norewrite
> store_id_access deny banned_methods
> store_id_access deny all
> 


If you have the latest version of Squid please try this:

 acl status3xx http_status 300-399
 store_miss deny status3xx gotostoreid

Amos



From Brian.Snyder at beavercreek.k12.oh.us  Tue Jun 16 19:05:33 2015
From: Brian.Snyder at beavercreek.k12.oh.us (Snyder, Brian)
Date: Tue, 16 Jun 2015 19:05:33 +0000
Subject: [squid-users] Squid + M86 Filter
Message-ID: <D2E66EB8081A7D4C86314759C29AAFE00DB0FADF@BHSMSX03.bvrcrk.k12.oh.us>

Hey Guys and Gals,

I have a weird one for you. This may not even be a Squid problem I'm not sure yet.

Ok I have two squid servers. Identical versions. Identical OS and kernel. Identical config. Same switch. Same subnet.

When server 1 passes info to the M86 box the ip of the client comes up and the active directory group as it should.
When server 2 (the new one) passes info I just get the ip of the proxy and no ad group information.

I have tried messing with the forwarded_for and via settings. I have even verified they are passing the same http header information.

I have checked that the same settings for both are in every place I know of. I can't for the life of me see the difference. If anyone has any suggestions, please let me know!

-Brian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150616/b17e8143/attachment.htm>

From tmblue at gmail.com  Tue Jun 16 22:28:01 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 16 Jun 2015 15:28:01 -0700
Subject: [squid-users] kid1| WARNING: 1 swapin MD5 mismatches 3.5
Message-ID: <CAEaSS0ZbS7A3ctaGTvB7ZWzejTyituBd7YGGhNfF6+ppKNDt6w@mail.gmail.com>

So appears I have some corruption? I can hit this server with tests just
fine, but if I put it into production I get these errors.


I've got no debug statements active, so this has me a bit concerned


Tory


2015/06/16 15:26:08 kid1| WARNING: 1 swapin MD5 mismatches

2015/06/16 15:26:08 kid1| Could not parse headers from on disk object

2015/06/16 15:26:08 kid1| BUG 3279: HTTP reply without Date:

2015/06/16 15:26:08 kid1| StoreEntry->key: 5993A2A07D185EA408F308BB6F4664C9

2015/06/16 15:26:08 kid1| StoreEntry->next: 0x1c603fd8

2015/06/16 15:26:08 kid1| StoreEntry->mem_obj: 0x2d72b00

2015/06/16 15:26:08 kid1| StoreEntry->timestamp: -1

2015/06/16 15:26:08 kid1| StoreEntry->lastref: 1434493568

2015/06/16 15:26:08 kid1| StoreEntry->expires: -1

2015/06/16 15:26:08 kid1| StoreEntry->lastmod: -1

2015/06/16 15:26:08 kid1| StoreEntry->swap_file_sz: 0

2015/06/16 15:26:08 kid1| StoreEntry->refcount: 1

2015/06/16 15:26:08 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED

2015/06/16 15:26:08 kid1| StoreEntry->swap_dirn: -1

2015/06/16 15:26:08 kid1| StoreEntry->swap_filen: -1

2015/06/16 15:26:08 kid1| StoreEntry->lock_count: 2

2015/06/16 15:26:08 kid1| StoreEntry->mem_status: 0

2015/06/16 15:26:08 kid1| StoreEntry->ping_status: 2

2015/06/16 15:26:08 kid1| StoreEntry->store_status: 1

2015/06/16 15:26:08 kid1| StoreEntry->swap_status: 0

2015/06/16 15:26:12 kid1| Could not parse headers from on disk object

2015/06/16 15:26:12 kid1| BUG 3279: HTTP reply without Date:

2015/06/16 15:26:12 kid1| StoreEntry->key: 98378018F7E195D8DF490B34598A6E84

2015/06/16 15:26:12 kid1| StoreEntry->next: 0x1d1a6d98

2015/06/16 15:26:12 kid1| StoreEntry->mem_obj: 0x1d6e40e0

2015/06/16 15:26:12 kid1| StoreEntry->timestamp: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastref: 1434493572

2015/06/16 15:26:12 kid1| StoreEntry->expires: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastmod: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_file_sz: 0

2015/06/16 15:26:12 kid1| StoreEntry->refcount: 1

2015/06/16 15:26:12 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED

2015/06/16 15:26:12 kid1| StoreEntry->swap_dirn: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_filen: -1

2015/06/16 15:26:12 kid1| StoreEntry->lock_count: 2

2015/06/16 15:26:12 kid1| StoreEntry->mem_status: 0

2015/06/16 15:26:12 kid1| StoreEntry->ping_status: 2

2015/06/16 15:26:12 kid1| StoreEntry->store_status: 1

2015/06/16 15:26:12 kid1| StoreEntry->swap_status: 0

2015/06/16 15:26:12 kid1| Could not parse headers from on disk object

2015/06/16 15:26:12 kid1| BUG 3279: HTTP reply without Date:

2015/06/16 15:26:12 kid1| StoreEntry->key: D952451B47EF10EC480C043FD63BFB6D

2015/06/16 15:26:12 kid1| StoreEntry->next: 0x1c8b1c58

2015/06/16 15:26:12 kid1| StoreEntry->mem_obj: 0x1d6feff0

2015/06/16 15:26:12 kid1| StoreEntry->timestamp: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastref: 1434493572

2015/06/16 15:26:12 kid1| StoreEntry->expires: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastmod: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_file_sz: 0

2015/06/16 15:26:12 kid1| StoreEntry->refcount: 1

2015/06/16 15:26:12 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED

2015/06/16 15:26:12 kid1| StoreEntry->swap_dirn: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_filen: -1

2015/06/16 15:26:12 kid1| StoreEntry->lock_count: 2

2015/06/16 15:26:12 kid1| StoreEntry->mem_status: 0

2015/06/16 15:26:12 kid1| StoreEntry->ping_status: 2

2015/06/16 15:26:12 kid1| StoreEntry->store_status: 1

2015/06/16 15:26:12 kid1| StoreEntry->swap_status: 0

2015/06/16 15:26:12 kid1| Could not parse headers from on disk object

2015/06/16 15:26:12 kid1| BUG 3279: HTTP reply without Date:

2015/06/16 15:26:12 kid1| StoreEntry->key: C44DA9C9F8CC73F64D37EDA4FC074FE3

2015/06/16 15:26:12 kid1| StoreEntry->next: 0x1ad1a4f8

2015/06/16 15:26:12 kid1| StoreEntry->mem_obj: 0x1d6feff0

2015/06/16 15:26:12 kid1| StoreEntry->timestamp: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastref: 1434493572

2015/06/16 15:26:12 kid1| StoreEntry->expires: -1

2015/06/16 15:26:12 kid1| StoreEntry->lastmod: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_file_sz: 0

2015/06/16 15:26:12 kid1| StoreEntry->refcount: 1

2015/06/16 15:26:12 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED

2015/06/16 15:26:12 kid1| StoreEntry->swap_dirn: -1

2015/06/16 15:26:12 kid1| StoreEntry->swap_filen: -1

2015/06/16 15:26:12 kid1| StoreEntry->lock_count: 2

2015/06/16 15:26:12 kid1| StoreEntry->mem_status: 0

2015/06/16 15:26:12 kid1| StoreEntry->ping_status: 2

2015/06/16 15:26:12 kid1| StoreEntry->store_status: 1

2015/06/16 15:26:12 kid1| StoreEntry->swap_status: 0

2015/06/16 15:26:12 kid1| assertion failed: store.cc:1885: "isEmpty()"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150616/a7ca9b64/attachment.htm>

From stacy.yeh at oracle.com  Tue Jun 16 22:47:29 2015
From: stacy.yeh at oracle.com (Stacy Yeh)
Date: Tue, 16 Jun 2015 15:47:29 -0700
Subject: [squid-users] Squid 3.5.5 fails to build for Solaris
Message-ID: <5580A781.5040804@oracle.com>

Hi All,

I am attempting to update from Squid 3.1.23 to the latest version 3.5.5 
for Solaris and am running into the following build error. From my 
understanding (correct me if I'm wrong), the issue is that the link is 
trying to link against the 32-bit version of libtool, although a 64-bit 
version also exists.

[snip]
libtool: link: /usr/gcc/4.8/bin/g++ -m64 -Wall -Wpointer-arith 
-Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT -pthreads 
-D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -g -O2 -march=native 
-std=c++11 -m64 -o basic_ncsa_auth basic_ncsa_auth.o crypt_md5.o -m64  
../../../lib/.libs/libmisccontainers.a 
../../../lib/.libs/libmiscencoding.a 
../../../compat/.libs/libcompat-squid.a -lcrypt -lmd5 -lm -lresolv -pthreads
ld: warning: file ../../../lib/.libs/libmiscencoding.a(md5.o): wrong ELF 
class: ELFCLASS32
Undefined                       first referenced
  symbol                             in file
rfc1738_unescape                    basic_ncsa_auth.o
SquidMD5Update                      crypt_md5.o
SquidMD5Init                        crypt_md5.o
SquidMD5Final                       crypt_md5.o
ld: fatal: symbol referencing errors
collect2: error: ld returned 1 exit status
make[4]: *** [basic_ncsa_auth] Error 1
make[4]: Leaving directory 
`/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers/basic_auth/NCSA'
make[3]: *** [all-recursive] Error 1
make[3]: Leaving directory 
`/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers/basic_auth'
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory 
`/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers'
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory 
`/builds/skyeh/squid-19581055-s12/components/squid/build/amd64'
gmake: *** 
[/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/.built] 
Error 2


Any suggestions on how to fix this? For what it's worth, here are the 
configure options I am using:

CONFIGURE_OPTIONS += --enable-arp-acl
CONFIGURE_OPTIONS += 
--enable-auth-basic='DB,NCSA,LDAP,PAM,getpwnam,MSNT-multi-domain,POP3,SMB,SASL'
CONFIGURE_OPTIONS += --enable-cache-digests
CONFIGURE_OPTIONS += --enable-carp
CONFIGURE_OPTIONS += --enable-coss-aio-ops
CONFIGURE_OPTIONS += --enable-delay-pools
CONFIGURE_OPTIONS += --enable-auth-digest='LDAP'
CONFIGURE_OPTIONS += 
--enable-external-acl-helpers='file_userip,unix_group,LDAP_group,wbinfo_group'
CONFIGURE_OPTIONS += --enable-follow-x-forwarded-for
CONFIGURE_OPTIONS += --enable-forward-log
CONFIGURE_OPTIONS += --enable-forw-via-db
CONFIGURE_OPTIONS += --enable-htcp
CONFIGURE_OPTIONS += --enable-icmp
CONFIGURE_OPTIONS += --enable-large-cache-files
CONFIGURE_OPTIONS += --enable-multicast-miss
CONFIGURE_OPTIONS += --enable-auth-negotiate='kerberos'
CONFIGURE_OPTIONS += --enable-auth-ntlm='smb_lm,fake'
CONFIGURE_OPTIONS += --enable-ntlm-fail-open
CONFIGURE_OPTIONS += --enable-removal-policies='heap,lru'
CONFIGURE_OPTIONS += --enable-snmp
CONFIGURE_OPTIONS += --enable-ssl
CONFIGURE_OPTIONS += --enable-storeio='aufs,diskd,ufs'
CONFIGURE_OPTIONS += --enable-x-accelerator-vary
CONFIGURE_OPTIONS += --with-aio
CONFIGURE_OPTIONS += --with-aufs-threads=8
CONFIGURE_OPTIONS += --with-large-files
CONFIGURE_OPTIONS += --with-build-environment=POSIX_V6_ILP32_OFFBIG
CONFIGURE_OPTIONS += --with-pthreads


Best,
Stacy Yeh


From squid3 at treenet.co.nz  Tue Jun 16 23:29:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2015 11:29:29 +1200
Subject: [squid-users] kid1| WARNING: 1 swapin MD5 mismatches 3.5
In-Reply-To: <CAEaSS0ZbS7A3ctaGTvB7ZWzejTyituBd7YGGhNfF6+ppKNDt6w@mail.gmail.com>
References: <CAEaSS0ZbS7A3ctaGTvB7ZWzejTyituBd7YGGhNfF6+ppKNDt6w@mail.gmail.com>
Message-ID: <5580B159.2070302@treenet.co.nz>

On 17/06/2015 10:28 a.m., Tory M Blue wrote:
> So appears I have some corruption? I can hit this server with tests just
> fine, but if I put it into production I get these errors.
> 
> 
> I've got no debug statements active, so this has me a bit concerned
> 

This is almost always seen as a result of a previous crash which left
some object in the cache in a corrupted (incomplete write) state.

If you can track down which object it was and erase it (or drop the
whole cache and restart). It should disappear.

Although any assistance you are able to give in tracking down what the
root cause of that assert "isEmpty()" is would be welcome. So far we are
all unable to .

Amos



From squid3 at treenet.co.nz  Tue Jun 16 23:32:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2015 11:32:08 +1200
Subject: [squid-users] Squid 3.5.5 fails to build for Solaris
In-Reply-To: <5580A781.5040804@oracle.com>
References: <5580A781.5040804@oracle.com>
Message-ID: <5580B1F8.6060706@treenet.co.nz>

On 17/06/2015 10:47 a.m., Stacy Yeh wrote:
> Hi All,
> 
> I am attempting to update from Squid 3.1.23 to the latest version 3.5.5
> for Solaris and am running into the following build error. From my
> understanding (correct me if I'm wrong), the issue is that the link is
> trying to link against the 32-bit version of libtool, although a 64-bit
> version also exists.
> 

Its not the libtool libaries. Its libmiscencoding from Squid has been
built wrong.
Perhapse something used in CXXFLAGS missing from CFLAGS values?

Amos



From tmblue at gmail.com  Tue Jun 16 23:35:06 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 16 Jun 2015 16:35:06 -0700
Subject: [squid-users] kid1| WARNING: 1 swapin MD5 mismatches 3.5
In-Reply-To: <5580B159.2070302@treenet.co.nz>
References: <CAEaSS0ZbS7A3ctaGTvB7ZWzejTyituBd7YGGhNfF6+ppKNDt6w@mail.gmail.com>
 <5580B159.2070302@treenet.co.nz>
Message-ID: <CAEaSS0Zanto_9FE3HFzse_4VoNy3MBJZDzLjWfPRH8Ej__=5bw@mail.gmail.com>

On Tue, Jun 16, 2015 at 4:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 17/06/2015 10:28 a.m., Tory M Blue wrote:
> > So appears I have some corruption? I can hit this server with tests just
> > fine, but if I put it into production I get these errors.
> >
> >
> > I've got no debug statements active, so this has me a bit concerned
> >
>
> This is almost always seen as a result of a previous crash which left
> some object in the cache in a corrupted (incomplete write) state.
>
> If you can track down which object it was and erase it (or drop the
> whole cache and restart). It should disappear.
>
> Although any assistance you are able to give in tracking down what the
> root cause of that assert "isEmpty()" is would be welcome. So far we are
> all unable to .
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

Well I can run with 2 caches in prod without an issue, so if I can help
with debug or something on this box in the weird state, i'll be more than
happy too.

I believe the issue was i disabled ipv6 while squid was running and it hard
a hard time with that, so I stopped restarted, same error as soon as
traffic came in. I then moved my cache data to local disk (it runs in
memory), and rebooted , restored the data and same error (just wanted to
make sure squid was clean).

So let me know if I can help you track that down.

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150616/9b71fe94/attachment.htm>

From Jason_Haar at trimble.com  Wed Jun 17 06:52:31 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Wed, 17 Jun 2015 18:52:31 +1200
Subject: [squid-users] problem with some ssl services
In-Reply-To: <557E153C.1030308@treenet.co.nz>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz>
Message-ID: <5581192F.9060109@trimble.com>

On 15/06/15 11:58, Amos Jeffries wrote:
> Ensure that you are using the very latest Squid version to avoid
> problems with unsupported TLS mechanisms. The latest Squid will also
> automatically splice if its determined that the TLS connection cannot be
> bumped.
Is that supposed to be in 3.5.5? I just noticed a problem with bumping
that came down to the
web server requiring client cert validation and squid-3.5.5 failed to
splice - so it failed going through bump
(as you'd expect).

I guess I'm asking if this new "SSL determination" includes detecting
client certs, because that would be a
good one to detect if possible?

Now that I think of it, that might be a mugs game. The site I'm
referring to had a "SSLVerifyClient optional"
on a subdirectory - so it's probably quite unfair to expect a TLS
Intercept to "magically" know what encrypted
urls it can fiddle with and what ones it can't ;-) Hmmm, OTOH maybe  if
squid decides a server is asking
for even optional client certs, that it declares the entire SNI to be
splice instead of bump - frankly I'd live with
that (ie it might start out bumping, but then flick to splice on the
first bit of evidence that some part needed
client certs - even optional)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From yvoinov at gmail.com  Wed Jun 17 07:59:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 17 Jun 2015 13:59:56 +0600
Subject: [squid-users] Squid 3.5.5 fails to build for Solaris
In-Reply-To: <5580A781.5040804@oracle.com>
References: <5580A781.5040804@oracle.com>
Message-ID: <558128FC.9020506@gmail.com>

I use this configuration parameters to build 64 bit 3.5.x Squid on Solaris:

'--prefix=/usr/local/squid' '--enable-translation' 
'--enable-external-acl-helpers=none' '--enable-ecap' 
'--enable-ipf-transparent' '--enable-storeio=diskd' 
'--enable-removal-policies=lru,heap' '--disable-wccp' 
'--enable-http-violations' '--enable-follow-x-forwarded-for' 
'--enable-arp-acl' '--enable-htcp' '--enable-cache-digests' '--with-dl' 
'--enable-auth-negotiate=none' '--disable-auth-digest' 
'--disable-auth-ntlm' '--disable-url-rewrite-helpers' 
'--enable-storeid-rewrite-helpers=file' 
'--enable-log-daemon-helpers=file' '--enable-ssl-crtd' 
'--with-openssl=/opt/csw' '--enable-zph-qos' '--disable-snmp' 
'--with-build-environment=POSIX_V6_LP64_OFF64' 'CFLAGS=-O3 -m64 
-mtune=core2 -pipe -Wno-write-strings' 'CXXFLAGS=-O3 -m64 -mtune=core2 
-pipe -Wno-write-strings' 'LIBOPENSSL_CFLAGS=-I/opt/csw/include/openssl' 
'CPPFLAGS=-I/opt/csw/include' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig' 
--enable-build-info="Intercept/WCCPv2/OpenSSL/CRTD/DISKD/ECAP/64/GCC 
Production"

This is good enough to build well 64 bit executables.

Note: I've use dual 32/54 bit libraries from OpenCSW repository and 
specified crle libraries path:

root @ cthulhu / # crle

Configuration file [version 4]: /var/ld/ld.config
   Platform:     32-bit LSB 80386
   Default Library Path (ELF): 
/lib:/usr/lib:/usr/local/lib:/opt/csw/lib:/usr/sfw/lib
   Trusted Directories (ELF):    /lib/secure:/usr/lib/secure  (system 
default)

Command line:
   crle -c /var/ld/ld.config -l 
/lib:/usr/lib:/usr/local/lib:/opt/csw/lib:/usr/sfw/lib

root @ cthulhu / # crle -64

Configuration file [version 4]: /var/ld/64/ld.config
   Platform:     64-bit LSB AMD64
   Default Library Path (ELF): 
/lib/64:/usr/lib/64:/opt/csw/lib/64:/usr/sfw/lib/64
   Trusted Directories (ELF):    /lib/secure/64:/usr/lib/secure/64 
(system default)

Command line:
   crle -64 -c /var/ld/64/ld.config -l 
/lib/64:/usr/lib/64:/opt/csw/lib/64:/usr/sfw/lib/64

Hope this helps.

17.06.15 4:47, Stacy Yeh ?????:
> Hi All,
>
> I am attempting to update from Squid 3.1.23 to the latest version 
> 3.5.5 for Solaris and am running into the following build error. From 
> my understanding (correct me if I'm wrong), the issue is that the link 
> is trying to link against the 32-bit version of libtool, although a 
> 64-bit version also exists.
>
> [snip]
> libtool: link: /usr/gcc/4.8/bin/g++ -m64 -Wall -Wpointer-arith 
> -Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT 
> -pthreads -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -g -O2 
> -march=native -std=c++11 -m64 -o basic_ncsa_auth basic_ncsa_auth.o 
> crypt_md5.o -m64  ../../../lib/.libs/libmisccontainers.a 
> ../../../lib/.libs/libmiscencoding.a 
> ../../../compat/.libs/libcompat-squid.a -lcrypt -lmd5 -lm -lresolv 
> -pthreads
> ld: warning: file ../../../lib/.libs/libmiscencoding.a(md5.o): wrong 
> ELF class: ELFCLASS32
> Undefined                       first referenced
>  symbol                             in file
> rfc1738_unescape                    basic_ncsa_auth.o
> SquidMD5Update                      crypt_md5.o
> SquidMD5Init                        crypt_md5.o
> SquidMD5Final                       crypt_md5.o
> ld: fatal: symbol referencing errors
> collect2: error: ld returned 1 exit status
> make[4]: *** [basic_ncsa_auth] Error 1
> make[4]: Leaving directory 
> `/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers/basic_auth/NCSA'
> make[3]: *** [all-recursive] Error 1
> make[3]: Leaving directory 
> `/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers/basic_auth'
> make[2]: *** [all-recursive] Error 1
> make[2]: Leaving directory 
> `/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/helpers'
> make[1]: *** [all-recursive] Error 1
> make[1]: Leaving directory 
> `/builds/skyeh/squid-19581055-s12/components/squid/build/amd64'
> gmake: *** 
> [/builds/skyeh/squid-19581055-s12/components/squid/build/amd64/.built] 
> Error 2
>
>
> Any suggestions on how to fix this? For what it's worth, here are the 
> configure options I am using:
>
> CONFIGURE_OPTIONS += --enable-arp-acl
> CONFIGURE_OPTIONS += 
> --enable-auth-basic='DB,NCSA,LDAP,PAM,getpwnam,MSNT-multi-domain,POP3,SMB,SASL'
> CONFIGURE_OPTIONS += --enable-cache-digests
> CONFIGURE_OPTIONS += --enable-carp
> CONFIGURE_OPTIONS += --enable-coss-aio-ops
> CONFIGURE_OPTIONS += --enable-delay-pools
> CONFIGURE_OPTIONS += --enable-auth-digest='LDAP'
> CONFIGURE_OPTIONS += 
> --enable-external-acl-helpers='file_userip,unix_group,LDAP_group,wbinfo_group'
> CONFIGURE_OPTIONS += --enable-follow-x-forwarded-for
> CONFIGURE_OPTIONS += --enable-forward-log
> CONFIGURE_OPTIONS += --enable-forw-via-db
> CONFIGURE_OPTIONS += --enable-htcp
> CONFIGURE_OPTIONS += --enable-icmp
> CONFIGURE_OPTIONS += --enable-large-cache-files
> CONFIGURE_OPTIONS += --enable-multicast-miss
> CONFIGURE_OPTIONS += --enable-auth-negotiate='kerberos'
> CONFIGURE_OPTIONS += --enable-auth-ntlm='smb_lm,fake'
> CONFIGURE_OPTIONS += --enable-ntlm-fail-open
> CONFIGURE_OPTIONS += --enable-removal-policies='heap,lru'
> CONFIGURE_OPTIONS += --enable-snmp
> CONFIGURE_OPTIONS += --enable-ssl
> CONFIGURE_OPTIONS += --enable-storeio='aufs,diskd,ufs'
> CONFIGURE_OPTIONS += --enable-x-accelerator-vary
> CONFIGURE_OPTIONS += --with-aio
> CONFIGURE_OPTIONS += --with-aufs-threads=8
> CONFIGURE_OPTIONS += --with-large-files
> CONFIGURE_OPTIONS += --with-build-environment=POSIX_V6_ILP32_OFFBIG
> CONFIGURE_OPTIONS += --with-pthreads
>
>
> Best,
> Stacy Yeh
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yashvinder.edx at gmail.com  Wed Jun 17 09:07:01 2015
From: yashvinder.edx at gmail.com (yashvinder hooda)
Date: Wed, 17 Jun 2015 14:37:01 +0530
Subject: [squid-users] bypass proxy
Message-ID: <CAD8CJ-p4RZxS44-eJDzO=wnsU+RcXsk9Eo6vi87K47EGmvfgag@mail.gmail.com>

Hi All,

I have 2 issues

First one: How can i bypass proxy for an IP in LAN.


Second one:
I am running squid on openwrt and i want to allow some websites to bypass
proxy and want to allow them go direct.
For that i am using wpad with PAC file but the problem is for some websites
it works and for some it doesn't.

Here is my PAC file



function FindProxyForURL(url, host)
{
    // The 1st if function tests if the URI should be by-passed
    // Proxy By-Pass List
    if (
        // ignore RFC 1918 internal addreses
        isInNet(host, "10.0.0.0", "255.0.0.0") ||
        isInNet(host, "172.16.0.0", "255.240.0.0") ||
        isInNet(host, "192.168.0.0", "255.255.0.0") ||

        // is url is like http://server by-pass
        isPlainHostName(host) ||

        // localhost!!
        localHostOrDomainIs(host, "127.0.0.1") ||

        // by-pass internal URLS
        dnsDomainIs(host, ".flipkart.com") ||
        dnsDomainIs(host, ".apple.com") ||
        dnsDomainIs(host, ".linuxbite.com") ||
        dnsDomainIs(host, ".rediff.com") ||

        // by-pass FTP
//        shExpMatch(url, "ftp:*")
        url.substring(0, 4)=="ftp:"
        )

        // If True, tell the browser to go direct
        return "DIRECT";

        // If False, it's not on the by-pass then Proxy the request if you
fail to connect to the proxy, try direct.

return "PROXY 192.168.1.1:3128";
//return "DIRECT";
}



To be precise it works for apple.com but doesn't work for rest of the
websites.
Please enlighten me.

-- 
Regards,
Yashvinder
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150617/6c4a6fde/attachment.htm>

From Horvath.Szabolcs at t-systems.hu  Wed Jun 17 10:11:04 2015
From: Horvath.Szabolcs at t-systems.hu (=?iso-8859-2?Q?Horv=E1th_Szabolcs?=)
Date: Wed, 17 Jun 2015 12:11:04 +0200
Subject: [squid-users] squid 3.1 with https traffic and delay pools is
 flooding network with hundreds of thousands 65-70 bytes packets (and
 killing the routers, anyway)
Message-ID: <A3EB676EF70FD04695F1ADED0A79AD8E875323A35F@K-MAIL3.kfki.corp>

Hello!

We're having serious problems with a squid proxy server. 

The good news is the problem can be reproduced at any time in our production squid system.

Environment:
- CentOS release 6.5 (Final) with Linux kernel 2.6.32-431.29.2.el6.x86_64
- squid-3.1.10-22.el6_5.x86_64 (a bit old, CentOS ships this version)

Problem description:
- if we have a few mbytes/sec https traffic AND
- delay_classes are in place AND
- delay pools are full (I mean the available bandwidth for the customer are used)

-> then squid is trickling https traffic down to the clients in 65-70 byte packets.

Our WAN routers are not designed to handle thousands of 65-70 byte packets per seconds and therefore we have some network stability issues.

I tracked down the following:
- if delay_pools are commented out (clients can go with full speed as they like) -> the problem eliminates, https traffic flows with ~1500 byte packets
- if we use only http traffic, there is no problem: http traffic flows with ~1500 byte packets even if the delay pools are full

Our test URL is www.opengroup.org/infosrv/DCE/dce122.tar.gz, which is available both on http and https protocol.

Resources can be found at http://support.iqsys.hu/logs/

1. squid.conf -> squid configuration file
2. http-delaypool.pcap: 
	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are active
	- http flows with 1500 byte packets
3. http-nodelaypool.pcap: 
	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are INACTIVE
	- http flows with 1500 byte packets
4. https-delaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are active
	- http flows with 69 byte packets -> this is extremely bad
5. https-nodelaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are INACTIVE
	- http flows with 1500 byte packets

My question is: is it a known bug?

If yes, which version(s) contain the fix (I read through the changelog several times, but I can't found the exact bug)
	- if 3.1 branch fixes this, upgrade is easy
	- upgrading major versions (3.4, 3.5) is not so trivial due to the complex environment (telling you the truth, installing a new squid server and migrating to it would be much easier than in-place upgrade)	

If not, how can I track down the issue?
	- as far as I understand squid configuration, it's not too complex
	- although ICAP is enabled (squidclamav is used), it's not the root of the problem -> when ICAP is commented out, the problem remains

Any ideas are appreciated. 

Thanks for reading this.

Best regards,
  Szabolcs Horvath



From Horvath.Szabolcs at t-systems.hu  Wed Jun 17 10:14:47 2015
From: Horvath.Szabolcs at t-systems.hu (=?utf-8?B?SG9ydsOhdGggU3phYm9sY3M=?=)
Date: Wed, 17 Jun 2015 12:14:47 +0200
Subject: [squid-users] squid 3.1 with https traffic and delay pools is
 flooding network with hundreds of thousands 65-70 bytes packets (and
 killing the routers, anyway)
In-Reply-To: <A3EB676EF70FD04695F1ADED0A79AD8E875323A35F@K-MAIL3.kfki.corp>
References: <A3EB676EF70FD04695F1ADED0A79AD8E875323A35F@K-MAIL3.kfki.corp>
Message-ID: <A3EB676EF70FD04695F1ADED0A79AD8E875323A361@K-MAIL3.kfki.corp>

Hello again!

Sorry for the typos, case #4 and case #5 are https tests, not http:

4. https-delaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are active
	- HTTPS flows with 69 byte packets -> this is extremely bad
5. https-nodelaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are INACTIVE
	- HTTPS flows with 1500 byte packets

Szabolcs

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Horv?th Szabolcs
Sent: Wednesday, June 17, 2015 12:11 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] squid 3.1 with https traffic and delay pools is flooding network with hundreds of thousands 65-70 bytes packets (and killing the routers, anyway)

Hello!

We're having serious problems with a squid proxy server. 

The good news is the problem can be reproduced at any time in our production squid system.

Environment:
- CentOS release 6.5 (Final) with Linux kernel 2.6.32-431.29.2.el6.x86_64
- squid-3.1.10-22.el6_5.x86_64 (a bit old, CentOS ships this version)

Problem description:
- if we have a few mbytes/sec https traffic AND
- delay_classes are in place AND
- delay pools are full (I mean the available bandwidth for the customer are used)

-> then squid is trickling https traffic down to the clients in 65-70 byte packets.

Our WAN routers are not designed to handle thousands of 65-70 byte packets per seconds and therefore we have some network stability issues.

I tracked down the following:
- if delay_pools are commented out (clients can go with full speed as they like) -> the problem eliminates, https traffic flows with ~1500 byte packets
- if we use only http traffic, there is no problem: http traffic flows with ~1500 byte packets even if the delay pools are full

Our test URL is www.opengroup.org/infosrv/DCE/dce122.tar.gz, which is available both on http and https protocol.

Resources can be found at http://support.iqsys.hu/logs/

1. squid.conf -> squid configuration file
2. http-delaypool.pcap: 
	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are active
	- http flows with 1500 byte packets
3. http-nodelaypool.pcap: 
	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are INACTIVE
	- http flows with 1500 byte packets
4. https-delaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are active
	- http flows with 69 byte packets -> this is extremely bad
5. https-nodelaypool.pcap:
	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
	- delay pools are INACTIVE
	- http flows with 1500 byte packets

My question is: is it a known bug?

If yes, which version(s) contain the fix (I read through the changelog several times, but I can't found the exact bug)
	- if 3.1 branch fixes this, upgrade is easy
	- upgrading major versions (3.4, 3.5) is not so trivial due to the complex environment (telling you the truth, installing a new squid server and migrating to it would be much easier than in-place upgrade)	

If not, how can I track down the issue?
	- as far as I understand squid configuration, it's not too complex
	- although ICAP is enabled (squidclamav is used), it's not the root of the problem -> when ICAP is commented out, the problem remains

Any ideas are appreciated. 

Thanks for reading this.

Best regards,
  Szabolcs Horvath

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From bpk678 at gmail.com  Wed Jun 17 12:24:59 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Wed, 17 Jun 2015 08:24:59 -0400
Subject: [squid-users] bypass proxy
In-Reply-To: <CAD8CJ-p4RZxS44-eJDzO=wnsU+RcXsk9Eo6vi87K47EGmvfgag@mail.gmail.com>
References: <CAD8CJ-p4RZxS44-eJDzO=wnsU+RcXsk9Eo6vi87K47EGmvfgag@mail.gmail.com>
Message-ID: <CAARxGtgqqAzHa8N9YKV4K+PiBr3Ca-_-WvR7X5A20Dt7FfsMsg@mail.gmail.com>

Look into the pacparser project on github.  It allows you to evaluate a pac
file and test the logic.
Hi All,

I have 2 issues

First one: How can i bypass proxy for an IP in LAN.


Second one:
I am running squid on openwrt and i want to allow some websites to bypass
proxy and want to allow them go direct.
For that i am using wpad with PAC file but the problem is for some websites
it works and for some it doesn't.

Here is my PAC file



function FindProxyForURL(url, host)
{
    // The 1st if function tests if the URI should be by-passed
    // Proxy By-Pass List
    if (
        // ignore RFC 1918 internal addreses
        isInNet(host, "10.0.0.0", "255.0.0.0") ||
        isInNet(host, "172.16.0.0", "255.240.0.0") ||
        isInNet(host, "192.168.0.0", "255.255.0.0") ||

        // is url is like http://server by-pass
        isPlainHostName(host) ||

        // localhost!!
        localHostOrDomainIs(host, "127.0.0.1") ||

        // by-pass internal URLS
        dnsDomainIs(host, ".flipkart.com") ||
        dnsDomainIs(host, ".apple.com") ||
        dnsDomainIs(host, ".linuxbite.com") ||
        dnsDomainIs(host, ".rediff.com") ||

        // by-pass FTP
//        shExpMatch(url, "ftp:*")
        url.substring(0, 4)=="ftp:"
        )

        // If True, tell the browser to go direct
        return "DIRECT";

        // If False, it's not on the by-pass then Proxy the request if you
fail to connect to the proxy, try direct.

return "PROXY 192.168.1.1:3128";
//return "DIRECT";
}



To be precise it works for apple.com but doesn't work for rest of the
websites.
Please enlighten me.

-- 
Regards,
Yashvinder

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150617/6c5f1305/attachment.htm>

From michael.pelletier at palmbeachschools.org  Wed Jun 17 20:53:36 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 17 Jun 2015 16:53:36 -0400
Subject: [squid-users] Recommended Multi-CPU Configuration
Message-ID: <CAEnCSG54TNcRKt4hdAUOHVJVJ=VCKPkZusjq_wJTqP48hv3vWg@mail.gmail.com>

Hello,

I am looking to had some more power to squid. I have seen two different
types of configurations to do this:

1. Adding workers directive equal to the number of cpus. Then adding a
special wrapper around the AUFS disk cache so that the correct worker can
only access the correct cache. Yes, I know rock is multi cpu capable.

2. Using the split configuration from the Squid Web page. This involved a
front end and multiple backend squid servers on the same server.
http://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem

My question is, which one is recommended? What are the pros and cons of
each?

Thanks in advance,
Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150617/88ff90f6/attachment.htm>

From scarboro at envysion.com  Wed Jun 17 21:23:14 2015
From: scarboro at envysion.com (Jeff Scarborough)
Date: Wed, 17 Jun 2015 15:23:14 -0600
Subject: [squid-users] split horizon dns proxy
Message-ID: <CAJomFqHO+1nZKM9-woGSEZ7BQewQd3dXsTf82BdrS5pNc4g=tA@mail.gmail.com>

I am currently using Squid 3.1 that comes packages in RHEL 6.  I have this
line in my config:
  http_port 80 intercept

I have a split horizon dns.  This means if you lookup any address for my
domain from the internet you get the address of the squid proxy server.
However if you lookup the same name from my proxy server you get an
internal RFC1918 IP address for the specific name.

Using squid 3.1 this works great.  A user tries to connect to a URL and by
DNS resolution is sent to the proxy server, the proxy server then does a
DNS lookup of the name in the URL and gets the actual address and sends the
request to the correct place.

When I try and upgrade to anything beyond 3.2 this breaks.  I am finding
references that intercept as of Squid 3.2 NAT is required. Reference from
an email post in 2013:

In Squid since 3.2 if
the original TCP details are not found in the NAT records some
restrictions are placed on what happens with the request and response.


My question is, is there anyway back to the old behavior?  What are the
restrictions mentioned?

You may ask why I am not using the accel mode as this is quite obviously a
reverse proxy.  The reason is I could not get accel to work with the RTSP
server we are using.  I suspect because the Content-length returned by the
RTSP server is invalid as it is unknown since it is streaming video and the
length of the content is not known until a user stops the playback.

When I configure the proxy using accel I can get normal text pages back as
expected but the video fails with TCP_MISS_ABORTED this happens on all
version of squid.

The reason I am trying to upgrade Squid is to be able to do all of this
using HTTPS.

Jeff Scarborough
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150617/36dd20c7/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 18 01:12:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 13:12:39 +1200
Subject: [squid-users] squid 3.1 with https traffic and delay pools is
 flooding network with hundreds of thousands 65-70 bytes packets (and
 killing the routers, anyway)
In-Reply-To: <A3EB676EF70FD04695F1ADED0A79AD8E875323A35F@K-MAIL3.kfki.corp>
References: <A3EB676EF70FD04695F1ADED0A79AD8E875323A35F@K-MAIL3.kfki.corp>
Message-ID: <55821B07.6070209@treenet.co.nz>

On 17/06/2015 10:11 p.m., Horv?th Szabolcs wrote:
> Hello!
> 
> We're having serious problems with a squid proxy server. 
> 
> The good news is the problem can be reproduced at any time in our production squid system.
> 
> Environment:
> - CentOS release 6.5 (Final) with Linux kernel 2.6.32-431.29.2.el6.x86_64
> - squid-3.1.10-22.el6_5.x86_64 (a bit old, CentOS ships this version)
> 
> Problem description:
> - if we have a few mbytes/sec https traffic AND
> - delay_classes are in place AND
> - delay pools are full (I mean the available bandwidth for the customer are used)
> 
> -> then squid is trickling https traffic down to the clients in 65-70 byte packets.
> 
> Our WAN routers are not designed to handle thousands of 65-70 byte packets per seconds and therefore we have some network stability issues.
> 
> I tracked down the following:
> - if delay_pools are commented out (clients can go with full speed as they like) -> the problem eliminates, https traffic flows with ~1500 byte packets
> - if we use only http traffic, there is no problem: http traffic flows with ~1500 byte packets even if the delay pools are full
> 
> Our test URL is www.opengroup.org/infosrv/DCE/dce122.tar.gz, which is available both on http and https protocol.
> 
> Resources can be found at http://support.iqsys.hu/logs/
> 
> 1. squid.conf -> squid configuration file
> 2. http-delaypool.pcap: 
> 	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
> 	- delay pools are active
> 	- http flows with 1500 byte packets
> 3. http-nodelaypool.pcap: 
> 	- wget -c http://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
> 	- delay pools are INACTIVE
> 	- http flows with 1500 byte packets
> 4. https-delaypool.pcap:
> 	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
> 	- delay pools are active
> 	- http flows with 69 byte packets -> this is extremely bad
> 5. https-nodelaypool.pcap:
> 	- wget -c https://www.opengroup.org/infosrv/DCE/dce122.tar.gz, 
> 	- delay pools are INACTIVE
> 	- http flows with 1500 byte packets
> 
> My question is: is it a known bug?

Sounds like http://bugs.squid-cache.org/show_bug.cgi?id=2907,
 which was fixed in Squid-3.5.3.

see comment #16 in the bug report for a 3.1 workaround patch. Though if
your production server has high performance requirements the sleep(1)
workaround is not the best.

Amos


From squid3 at treenet.co.nz  Thu Jun 18 01:31:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 13:31:45 +1200
Subject: [squid-users] Recommended Multi-CPU Configuration
In-Reply-To: <CAEnCSG54TNcRKt4hdAUOHVJVJ=VCKPkZusjq_wJTqP48hv3vWg@mail.gmail.com>
References: <CAEnCSG54TNcRKt4hdAUOHVJVJ=VCKPkZusjq_wJTqP48hv3vWg@mail.gmail.com>
Message-ID: <55821F81.70809@treenet.co.nz>

On 18/06/2015 8:53 a.m., Michael Pelletier wrote:
> Hello,
> 
> I am looking to had some more power to squid. I have seen two different
> types of configurations to do this:
> 
> 1. Adding workers directive equal to the number of cpus. Then adding a
> special wrapper around the AUFS disk cache so that the correct worker can
> only access the correct cache. Yes, I know rock is multi cpu capable.
> 
> 2. Using the split configuration from the Squid Web page. This involved a
> front end and multiple backend squid servers on the same server.
> http://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem
> 
> My question is, which one is recommended? What are the pros and cons of
> each?
> 

Both and neither. #1 improves bandwidth savings. #2 improves raw speed.
Pick your poison.

These are example configurations only. For real high performance mutiple
machines in a mix of the two setups is even better.

Amos



From squid3 at treenet.co.nz  Thu Jun 18 02:22:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 14:22:13 +1200
Subject: [squid-users] split horizon dns proxy
In-Reply-To: <CAJomFqHO+1nZKM9-woGSEZ7BQewQd3dXsTf82BdrS5pNc4g=tA@mail.gmail.com>
References: <CAJomFqHO+1nZKM9-woGSEZ7BQewQd3dXsTf82BdrS5pNc4g=tA@mail.gmail.com>
Message-ID: <55822B55.4010101@treenet.co.nz>

On 18/06/2015 9:23 a.m., Jeff Scarborough wrote:
> I am currently using Squid 3.1 that comes packages in RHEL 6.  I have this
> line in my config:
>   http_port 80 intercept
> 
> I have a split horizon dns.  This means if you lookup any address for my
> domain from the internet you get the address of the squid proxy server.
> However if you lookup the same name from my proxy server you get an
> internal RFC1918 IP address for the specific name.
> 
> Using squid 3.1 this works great.  A user tries to connect to a URL and by
> DNS resolution is sent to the proxy server, the proxy server then does a
> DNS lookup of the name in the URL and gets the actual address and sends the
> request to the correct place.

Sigh. Dont do that.

> 
> When I try and upgrade to anything beyond 3.2 this breaks.  I am finding
> references that intercept as of Squid 3.2 NAT is required. Reference from
> an email post in 2013:
> 
> In Squid since 3.2 if
> the original TCP details are not found in the NAT records some
> restrictions are placed on what happens with the request and response.
> 
> 
> My question is, is there anyway back to the old behavior?

No.

>  What are the restrictions mentioned?

CVE-2009-0801
 Remote attacker able to inject into proxy cache arbitrary content for
arbitrary URLs. This is then delivered by the corrupted proxy to any
client fetching that URL with full assurance that it is the original
content.

The restrictions placed on Squid are:

a) NAT errors are no longer silently ignored.

b) The IP address of the server being contacted by the client is used as
origin unstead of DNS lookup.

> 
> You may ask why I am not using the accel mode as this is quite obviously a
> reverse proxy.

Indeed.

>  The reason is I could not get accel to work with the RTSP
> server we are using.  I suspect because the Content-length returned by the
> RTSP server is invalid as it is unknown since it is streaming video and the
> length of the content is not known until a user stops the playback.

RTSP != HTTP. Squid-3.1 and older are corrupting the RTSP traffic
messages as they travel through the proxy, by injecting HTTP mandatory
header values. The RTSP software may be ignoring that or coping with it
somehow.

Squid-3.2 and later will take such unknown-length content and
Transfer-Encode it. Which will screw with RTSP in a different way.

RFC 2326 section 4.4 "Note that RTSP does not (at present) support the
HTTP/1.1 "chunked" transfer coding(see [H3.6]) and requires the presence
of the Content-Length header field."


We used to see this with ICY protocol (abuses port 80) where strange
popping sounds would be injected into the radio stream by a proxy. That
was actually the chunked encoding headers every few KB counting and
checksum'ing the payload data.

> 
> When I configure the proxy using accel I can get normal text pages back as
> expected but the video fails with TCP_MISS_ABORTED this happens on all
> version of squid.

If intercept works but accel doesn't in 3.1 and older I suspect that had
more to do with squid listening on port 80. RTSP uses port 554 and
reverse proxies by default preserve the port information.

> 
> The reason I am trying to upgrade Squid is to be able to do all of this
> using HTTPS.
> 

Now thats just jumping from the hotplate into the fire. The security
protections addded in 3.2 for plain-text messages are a tame sub-set of
the restrictions on TLS connections.


There is no hope but to convert this to an actual reverse-proxy, an
actual intercept proxy, and either way to stop RTSP going through it.
Squid supports natively HTTP/1.x, HTTPS, ICY/SHOUTcast, and FTP - all
other protocols must be transfered via HTTP CONNECT tunnels.


PS. if you want to sponsor RTSP support being added to Squid I/we are
open to it.

Amos



From squid3 at treenet.co.nz  Thu Jun 18 02:25:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 14:25:59 +1200
Subject: [squid-users] problem with some ssl services
In-Reply-To: <5581192F.9060109@trimble.com>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
Message-ID: <55822C37.7070105@treenet.co.nz>

On 17/06/2015 6:52 p.m., Jason Haar wrote:
> On 15/06/15 11:58, Amos Jeffries wrote:
>> Ensure that you are using the very latest Squid version to avoid
>> problems with unsupported TLS mechanisms. The latest Squid will also
>> automatically splice if its determined that the TLS connection cannot be
>> bumped.
> Is that supposed to be in 3.5.5? I just noticed a problem with bumping
> that came down to the
> web server requiring client cert validation and squid-3.5.5 failed to
> splice - so it failed going through bump
> (as you'd expect).
> 
> I guess I'm asking if this new "SSL determination" includes detecting
> client certs, because that would be a
> good one to detect if possible?

It would seem so. AFAIK we are only detecting resumed sessions and
incompatible cipher sets at present. You may want to contact Christos
about the client certs.

FYI: the "ssl_bump peek all" config I have been advising, may not always
be the best. It seems there is some use for the "stare" option during
stage2 bumping instead of peek. But Im not sure yet myself on when its
best to do that over peek. You might awant to try it.

Amos


From michael.pelletier at palmbeachschools.org  Thu Jun 18 03:28:24 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 17 Jun 2015 23:28:24 -0400
Subject: [squid-users] Recommended Multi-CPU Configuration
In-Reply-To: <55821F81.70809@treenet.co.nz>
References: <CAEnCSG54TNcRKt4hdAUOHVJVJ=VCKPkZusjq_wJTqP48hv3vWg@mail.gmail.com>
 <55821F81.70809@treenet.co.nz>
Message-ID: <CAEnCSG4EQ9_4qHoBnJmS3TKG8fURhzQk7t3K+UmorTuJN=bJWA@mail.gmail.com>

Which one would be good for capacity\load? I have a very, very large
environment. I have 220,000 users on 8 Gig to the INTERNET. I am running a
load balancer, ipvsadm (Direct Routing) with 20 proxies behind it. I am
interested in handling load.

Michael

On Wed, Jun 17, 2015 at 9:31 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/06/2015 8:53 a.m., Michael Pelletier wrote:
> > Hello,
> >
> > I am looking to had some more power to squid. I have seen two different
> > types of configurations to do this:
> >
> > 1. Adding workers directive equal to the number of cpus. Then adding a
> > special wrapper around the AUFS disk cache so that the correct worker can
> > only access the correct cache. Yes, I know rock is multi cpu capable.
> >
> > 2. Using the split configuration from the Squid Web page. This involved a
> > front end and multiple backend squid servers on the same server.
> > http://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem
> >
> > My question is, which one is recommended? What are the pros and cons of
> > each?
> >
>
> Both and neither. #1 improves bandwidth savings. #2 improves raw speed.
> Pick your poison.
>
> These are example configurations only. For real high performance mutiple
> machines in a mix of the two setups is even better.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150617/7c9fa282/attachment.htm>

From tomtux007 at gmail.com  Thu Jun 18 08:54:33 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 18 Jun 2015 10:54:33 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h rhythm
Message-ID: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>

Hi

Squid 3.5.5 on a SLES12 box with Rock and SSL-Bump enabled, reloads
itself after every 2 hours (and only if there was initial some low
traffic through it). Squid 3.3.13 on the same box doesn't reload
itself after 2 hours.

In the cache.log are no suspicious entries. Everything looks and feels
normal (the same log is shown at 06:31:xx and 08:31:xx -> every 2
hours):

2015/06/18 10:31:24 kid1| Set Current Directory to /squid-cache01/
2015/06/18 10:31:24 kid1| Starting Squid Cache version 3.5.5 for
x86_64-unknown-linux-gnu...
2015/06/18 10:31:24 kid1| Service Name: squid
2015/06/18 10:31:24 kid1| Process ID 23816
2015/06/18 10:31:24 kid1| Process Roles: worker
2015/06/18 10:31:24 kid1| With 32768 file descriptors available
2015/06/18 10:31:24 kid1| Initializing IP Cache...
2015/06/18 10:31:24 kid1| DNS Socket created at 0.0.0.0, FD 10
2015/06/18 10:31:24 kid1| Adding domain xxxx from /etc/resolv.conf
2015/06/18 10:31:24 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/06/18 10:31:24 kid1| Adding nameserver xxx.xxx.xxx.xxx from
/etc/resolv.conf
2015/06/18 10:31:24 kid1| helperOpenServers: Starting 10/50
'negotiate_kerberos_auth' processes
2015/06/18 10:31:24 kid1| helperOpenServers: Starting 10/50
'ext_kerberos_ldap_group_acl' processes
2015/06/18 10:31:24 kid1| helperOpenServers: Starting 10/50
'ext_kerberos_ldap_group_acl' processes
2015/06/18 10:31:24 kid1| helperOpenServers: Starting 10/50
'ext_kerberos_ldap_group_acl' processes
2015/06/18 10:31:24 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2015/06/18 10:31:24 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2015/06/18 10:31:24 kid1| Store logging disabled
2015/06/18 10:31:24 kid1| Swap maxSize 0 + 4194304 KB, estimated 322638 objects
2015/06/18 10:31:24 kid1| Target number of buckets: 16131
2015/06/18 10:31:24 kid1| Using 16384 Store buckets
2015/06/18 10:31:24 kid1| Max Mem  size: 4194304 KB [shared]
2015/06/18 10:31:24 kid1| Max Swap size: 0 KB
2015/06/18 10:31:24 kid1| Using Least Load store dir selection
2015/06/18 10:31:24 kid1| Set Current Directory to /squid-cache01/
2015/06/18 10:31:24 kid1| Finished loading MIME types and icons.
2015/06/18 10:31:24 kid1| Sending SNMP messages from 0.0.0.0:3401
2015/06/18 10:31:24 kid1| Squid plugin modules loaded: 0
2015/06/18 10:31:24 kid1| Adaptation support is on
2015/06/18 10:31:24 kid1| Finished rebuilding storage from disk.
2015/06/18 10:31:24 kid1|         0 Entries scanned
2015/06/18 10:31:24 kid1|         0 Invalid entries.
2015/06/18 10:31:24 kid1|         0 With invalid flags.
2015/06/18 10:31:24 kid1|         0 Objects loaded.
2015/06/18 10:31:24 kid1|         0 Objects expired.
2015/06/18 10:31:24 kid1|         0 Objects cancelled.
2015/06/18 10:31:24 kid1|         0 Duplicate URLs purged.
2015/06/18 10:31:24 kid1|         0 Swapfile clashes avoided.
2015/06/18 10:31:24 kid1|   Took 0.02 seconds (  0.00 objects/sec).
2015/06/18 10:31:24 kid1| Beginning Validation Procedure
2015/06/18 10:31:24 kid1|   Completed Validation Procedure
2015/06/18 10:31:24 kid1|   Validated 0 Entries
2015/06/18 10:31:24 kid1|   store_swap_size = 0.00 KB
2015/06/18 10:31:24 kid1| Accepting SSL bumped HTTP Socket connections
at local=0.0.0.0:3128 remote=[::] FD 13 flags=1
2015/06/18 10:31:24 kid1| Accepting SNMP messages on 0.0.0.0:3401
2015/06/18 10:31:25 kid1| storeLateRelease: released 0 objects


Any hints for this behaviour?

Thanks a lot.

Kind regards,
Tom


From Ralf.Hildebrandt at charite.de  Thu Jun 18 09:19:58 2015
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 18 Jun 2015 11:19:58 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
 rhythm
In-Reply-To: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
Message-ID: <20150618091957.GK6799@charite.de>

* Tom Tom <tomtux007 at gmail.com>:
> Hi
> 
> Squid 3.5.5 on a SLES12 box with Rock and SSL-Bump enabled, reloads
> itself after every 2 hours (and only if there was initial some low
> traffic through it). Squid 3.3.13 on the same box doesn't reload
> itself after 2 hours.
> 
> In the cache.log are no suspicious entries. Everything looks and feels
> normal (the same log is shown at 06:31:xx and 08:31:xx -> every 2
> hours):

It looks like squid is restarting itself; the logs prior to that should
indicate why. 

Maybe running in gdb yields more info:
http://wiki.squid-cache.org/SquidFaq/BugReporting
 
> 2015/06/18 10:31:24 kid1| Starting Squid Cache version 3.5.5 for
> x86_64-unknown-linux-gnu...

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Thu Jun 18 09:23:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 21:23:18 +1200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
Message-ID: <55828E06.4070005@treenet.co.nz>

On 18/06/2015 8:54 p.m., Tom Tom wrote:
> Hi
> 
> Squid 3.5.5 on a SLES12 box with Rock and SSL-Bump enabled, reloads
> itself after every 2 hours (and only if there was initial some low
> traffic through it). Squid 3.3.13 on the same box doesn't reload
> itself after 2 hours.
> 
> In the cache.log are no suspicious entries. Everything looks and feels
> normal (the same log is shown at 06:31:xx and 08:31:xx -> every 2
> hours):
> 

If it was an assertion failure, or FATAL event, or expception error, or
transaction ERROR there should be at least one line above these ones you
quoted which explains the problem.

If there is no such lines then we can assume its a segmentation fault
and you need to seek down a core file (if any) which was produced. The
error details will be in there.


> 2015/06/18 10:31:24 kid1| Set Current Directory to /squid-cache01/
> 2015/06/18 10:31:24 kid1| Starting Squid Cache version 3.5.5 for
> x86_64-unknown-linux-gnu...
<snip>

> 
> Any hints for this behaviour?

If its regularly every 2 hours, I would look at your cron settings.
There may be some cronjob restarting or reconfiguring Squid every 2 hrs.

Another alternative is resource limits. If it takes more than 2hrs for
rock to load the cache index and the oom killer process of the kernel
kills the "resource intensive" Squid worker on or before completion you
would also see this type of behaviour.

Amos


From tomtux007 at gmail.com  Thu Jun 18 11:08:54 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 18 Jun 2015 13:08:54 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <20150618091957.GK6799@charite.de>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
Message-ID: <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>

No entries before (10:31:24 -> 12:31:27):
2015/06/18 10:31:24 kid1| Beginning Validation Procedure
2015/06/18 10:31:24 kid1|   Completed Validation Procedure
2015/06/18 10:31:24 kid1|   Validated 0 Entries
2015/06/18 10:31:24 kid1|   store_swap_size = 0.00 KB
2015/06/18 10:31:24 kid1| Accepting SSL bumped HTTP Socket connections
at local=0.0.0.0:3128 remote=[::] FD 13 flags=1
2015/06/18 10:31:24 kid1| Accepting SNMP messages on 0.0.0.0:3401
2015/06/18 10:31:25 kid1| storeLateRelease: released 0 objects

2015/06/18 11:30:37 kid1| Error negotiating SSL connection on FD 17:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca
(1/0)
2015/06/18 11:30:37 kid1| Error negotiating SSL connection on FD 17:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca
(1/0)

2015/06/18 12:31:27 kid1| Set Current Directory to /squid-cache01/
2015/06/18 12:31:27 kid1| Starting Squid Cache version 3.5.5 for
x86_64-unknown-linux-gnu...
2015/06/18 12:31:27 kid1| Service Name: squid
2015/06/18 12:31:27 kid1| Process ID 1174
2015/06/18 12:31:27 kid1| Process Roles: worker
2015/06/18 12:31:27 kid1| With 32768 file descriptors available
2015/06/18 12:31:27 kid1| Initializing IP Cache...
...
...


In /var/log/messages just a hint for SIGABRT (and the timestamp is
always in 2h steps...):
2015-06-18T06:31:16.393051+02:00 proxy squid[21123]: Squid Parent:
(squid-1) process 5770 exited due to signal 6 with status 0
2015-06-18T08:31:18.329729+02:00 proxy squid[29257]: Squid Parent:
(squid-1) process 29261 exited due to signal 6 with status 0
2015-06-18T10:31:21.421409+02:00 proxy squid[29257]: Squid Parent:
(squid-1) process 10384 exited due to signal 6 with status 0
2015-06-18T12:31:24.533701+02:00 proxy squid[29257]: Squid Parent:
(squid-1) process 23816 exited due to signal 6 with status 0

But no hints, why squid will be killed with SIGABRT....


On Thu, Jun 18, 2015 at 11:19 AM, Ralf Hildebrandt
<Ralf.Hildebrandt at charite.de> wrote:
> * Tom Tom <tomtux007 at gmail.com>:
>> Hi
>>
>> Squid 3.5.5 on a SLES12 box with Rock and SSL-Bump enabled, reloads
>> itself after every 2 hours (and only if there was initial some low
>> traffic through it). Squid 3.3.13 on the same box doesn't reload
>> itself after 2 hours.
>>
>> In the cache.log are no suspicious entries. Everything looks and feels
>> normal (the same log is shown at 06:31:xx and 08:31:xx -> every 2
>> hours):
>
> It looks like squid is restarting itself; the logs prior to that should
> indicate why.
>
> Maybe running in gdb yields more info:
> http://wiki.squid-cache.org/SquidFaq/BugReporting
>
>> 2015/06/18 10:31:24 kid1| Starting Squid Cache version 3.5.5 for
>> x86_64-unknown-linux-gnu...
>
> --
> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
> ralf.hildebrandt at charite.de        Campus Benjamin Franklin
> http://www.charite.de              Hindenburgdamm 30, 12203 Berlin
> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From hack.back at hotmail.com  Thu Jun 18 11:01:12 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 18 Jun 2015 04:01:12 -0700 (PDT)
Subject: [squid-users] problem with some ssl services
In-Reply-To: <55822C37.7070105@treenet.co.nz>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
 <55822C37.7070105@treenet.co.nz>
Message-ID: <1434625272907-4671777.post@n4.nabble.com>

i upgrade to 3.5.5 
and i use this conf

always_direct allow all
acl step1 at_step  SslBump1
acl step2 at_step  SslBump2
acl step3 at_step  SslBump3

acl exclude_acl ssl::server_name .yahoo.com .gmail.com .googlemail.com
s.yimg.com .yahooapis.com .akamaihd.net .fbcdn.net .facebook.com .google.com
ssl_bump peek step1 all
ssl_bump splice step2 exclude_acl
ssl_bump stare step2 all
ssl_bump bump step3 all
sslproxy_cert_error allow all


but still the same problem



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/problem-with-some-ssl-services-tp4671733p4671777.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun 18 11:42:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 23:42:11 +1200
Subject: [squid-users] TCP_MISS_ABORTED/000 with SIBLING
In-Reply-To: <1434354290052-4671735.post@n4.nabble.com>
References: <1434354290052-4671735.post@n4.nabble.com>
Message-ID: <5582AE93.3020409@treenet.co.nz>

On 15/06/2015 7:44 p.m., Stakres wrote:
> Hi All,
> 
> Weird issue with 2 Squid 3.5.5 in sibling mode, here is the trace:
> ...... TCP_MISS_ABORTED/000 0 GET http://www.greatandhra.com/ -
> SIBLING_HIT/x.x.x.x
> 
> Any idea ?

The client aborted less than a millisecond after requesting that URL. I
doubt even a local cache HIT would have helped.

Amos



From squid3 at treenet.co.nz  Thu Jun 18 11:43:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2015 23:43:53 +1200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
Message-ID: <5582AEF9.5030106@treenet.co.nz>

Then it is a segmentation fault.
You are going to have to run Squid under gdb like Ralf suggested.

Amos



From eliezer at ngtech.co.il  Thu Jun 18 11:46:42 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 18 Jun 2015 14:46:42 +0300
Subject: [squid-users] Recommended Multi-CPU Configuration
In-Reply-To: <CAEnCSG4EQ9_4qHoBnJmS3TKG8fURhzQk7t3K+UmorTuJN=bJWA@mail.gmail.com>
References: <CAEnCSG54TNcRKt4hdAUOHVJVJ=VCKPkZusjq_wJTqP48hv3vWg@mail.gmail.com>
 <55821F81.70809@treenet.co.nz>
 <CAEnCSG4EQ9_4qHoBnJmS3TKG8fURhzQk7t3K+UmorTuJN=bJWA@mail.gmail.com>
Message-ID: <5582AFA2.5000201@ngtech.co.il>

May I ask about the setup?
Is this setup of 20 pxy are running in interceot\transparent mode?

Eliezer

On 18/06/2015 06:28, Michael Pelletier wrote:
> Which one would be good for capacity\load? I have a very, very large
> environment. I have 220,000 users on 8 Gig to the INTERNET. I am running a
> load balancer, ipvsadm (Direct Routing) with 20 proxies behind it. I am
> interested in handling load.
>
> Michael
>
> On Wed, Jun 17, 2015 at 9:31 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>> On 18/06/2015 8:53 a.m., Michael Pelletier wrote:
>>> Hello,
>>>
>>> I am looking to had some more power to squid. I have seen two different
>>> types of configurations to do this:
>>>
>>> 1. Adding workers directive equal to the number of cpus. Then adding a
>>> special wrapper around the AUFS disk cache so that the correct worker can
>>> only access the correct cache. Yes, I know rock is multi cpu capable.
>>>
>>> 2. Using the split configuration from the Squid Web page. This involved a
>>> front end and multiple backend squid servers on the same server.
>>> http://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem
>>>
>>> My question is, which one is recommended? What are the pros and cons of
>>> each?
>>>
>>
>> Both and neither. #1 improves bandwidth savings. #2 improves raw speed.
>> Pick your poison.
>>
>> These are example configurations only. For real high performance mutiple
>> machines in a mix of the two setups is even better.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From hack.back at hotmail.com  Thu Jun 18 11:30:20 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 18 Jun 2015 04:30:20 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
Message-ID: <1434627020894-4671781.post@n4.nabble.com>

2015/06/18 13:47:25 kid1| WARNING: 1 swapin MD5 mismatches
2015/06/18 13:47:25 kid1| Could not parse headers from on disk object
2015/06/18 13:47:25 kid1| BUG 3279: HTTP reply without Date:
2015/06/18 13:47:25 kid1| StoreEntry->key: CD091412B485DCA6E9B1F7BAE5533671
2015/06/18 13:47:25 kid1| StoreEntry->next: 0x112ad5e38
2015/06/18 13:47:25 kid1| StoreEntry->mem_obj: 0x388fef0
2015/06/18 13:47:25 kid1| StoreEntry->timestamp: -1
2015/06/18 13:47:25 kid1| StoreEntry->lastref: 1434649645
2015/06/18 13:47:25 kid1| StoreEntry->expires: -1
2015/06/18 13:47:25 kid1| StoreEntry->lastmod: -1
2015/06/18 13:47:25 kid1| StoreEntry->swap_file_sz: 0
2015/06/18 13:47:25 kid1| StoreEntry->refcount: 1
2015/06/18 13:47:25 kid1| StoreEntry->flags:
DISPATCHED,PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/06/18 13:47:25 kid1| StoreEntry->swap_dirn: -1
2015/06/18 13:47:25 kid1| StoreEntry->swap_filen: -1
2015/06/18 13:47:25 kid1| StoreEntry->lock_count: 3
2015/06/18 13:47:25 kid1| StoreEntry->mem_status: 0
2015/06/18 13:47:25 kid1| StoreEntry->ping_status: 2
2015/06/18 13:47:25 kid1| StoreEntry->store_status: 1
2015/06/18 13:47:25 kid1| StoreEntry->swap_status: 0
2015/06/18 13:47:25 kid1| assertion failed: store.cc:1885: "isEmpty()"
2015/06/18 13:47:29 kid1| Set Current Directory to /var/spool/squid
2015/06/18 13:47:29 kid1| Starting Squid Cache version 3.5.5-20150610-r13846
for x86_64-unknown-linux-gnu...





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Thu Jun 18 11:42:01 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 18 Jun 2015 04:42:01 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434627020894-4671781.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
Message-ID: <1434627721687-4671782.post@n4.nabble.com>

[New LWP 20654]
[New LWP 21085]
[New LWP 21009]
[New LWP 21124]
[New LWP 20653]
[New LWP 21138]
[New LWP 21082]
[New LWP 21007]
[New LWP 21033]
[New LWP 21152]
[New LWP 21002]
[New LWP 21027]
[New LWP 21021]
[New LWP 21006]
[New LWP 21013]
[New LWP 21003]
[New LWP 21005]
[New LWP 21024]
[New LWP 21004]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f3974930165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f3974930165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f39749333e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x0000000000694008 in xassert ()
#3  0x000000000076485c in StoreEntry::startWriting() ()
#4  0x00000000008b77a9 in Client::setFinalReply(HttpReply*) ()
#5  0x00000000008bc432 in Client::adaptOrFinalizeReply() ()
#6  0x00000000006d9eee in HttpStateData::processReply() ()
#7  0x00000000006d9d50 in HttpStateData::readReply(CommIoCbParams const&) ()
#8  0x00000000006e2731 in CommCbMemFunT<HttpStateData,
CommIoCbParams>::doDial() ()
#9  0x00000000006e2a90 in JobDialer<HttpStateData>::dial(AsyncCall&) ()
#10 0x00000000006e2205 in AsyncCallT<CommCbMemFunT&lt;HttpStateData,
CommIoCbParams> >::fire() ()
#11 0x00000000007f5b44 in AsyncCall::make() ()
#12 0x00000000007f906f in AsyncCallQueue::fireNext() ()
#13 0x00000000007f8dfa in AsyncCallQueue::fire() ()
#14 0x00000000006a7631 in EventLoop::dispatchCalls() ()
#15 0x00000000006a74d5 in EventLoop::runOnce() ()
#16 0x00000000006a733c in EventLoop::run() ()
#17 0x0000000000710763 in SquidMain(int, char**) ()
#18 0x000000000070fba5 in SquidMainSafe(int, char**) ()
#19 0x000000000070fb82 in main ()
(gdb)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671782.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun 18 12:26:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2015 00:26:39 +1200
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434627721687-4671782.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com>
Message-ID: <5582B8FF.5060608@treenet.co.nz>

On 18/06/2015 11:42 p.m., HackXBack wrote:
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
> Program terminated with signal 6, Aborted.
> #0  0x00007f3974930165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) backtrace
> #0  0x00007f3974930165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007f39749333e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x0000000000694008 in xassert ()
> #3  0x000000000076485c in StoreEntry::startWriting() ()
> #4  0x00000000008b77a9 in Client::setFinalReply(HttpReply*) ()
> #5  0x00000000008bc432 in Client::adaptOrFinalizeReply() ()
> #6  0x00000000006d9eee in HttpStateData::processReply() ()
> #7  0x00000000006d9d50 in HttpStateData::readReply(CommIoCbParams const&) ()
> #8  0x00000000006e2731 in CommCbMemFunT<HttpStateData,
> CommIoCbParams>::doDial() ()
> #9  0x00000000006e2a90 in JobDialer<HttpStateData>::dial(AsyncCall&) ()
> #10 0x00000000006e2205 in AsyncCallT<CommCbMemFunT&lt;HttpStateData,
> CommIoCbParams> >::fire() ()
> #11 0x00000000007f5b44 in AsyncCall::make() ()
> #12 0x00000000007f906f in AsyncCallQueue::fireNext() ()
> #13 0x00000000007f8dfa in AsyncCallQueue::fire() ()
> #14 0x00000000006a7631 in EventLoop::dispatchCalls() ()
> #15 0x00000000006a74d5 in EventLoop::runOnce() ()
> #16 0x00000000006a733c in EventLoop::run() ()
> #17 0x0000000000710763 in SquidMain(int, char**) ()
> #18 0x000000000070fba5 in SquidMainSafe(int, char**) ()
> #19 0x000000000070fb82 in main ()
> (gdb)
> 

Finally! core to work with :-)

Can you print out the contents of the StoreEntry object that is
apparently non-empty ?
 I might give a hint about whats been playing with it.

Also please:
 frame 3
 print mem_obj->endOffset()


Amos


From eliezer at ngtech.co.il  Thu Jun 18 15:19:25 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 18 Jun 2015 18:19:25 +0300
Subject: [squid-users] Has anyone tried using the attached tutorial for
	kerberus authentication?
Message-ID: <5582E17D.8060708@ngtech.co.il>

Has anyone tried using the attached tutorial for kerberus  authentication?

https://www.dalemacartney.com/2012/07/06/squid-proxy-integration-with-active-directory-the-quick-and-simple-way/

Thanks,
Eliezer



From hack.back at hotmail.com  Thu Jun 18 15:10:34 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 18 Jun 2015 08:10:34 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <5582B8FF.5060608@treenet.co.nz>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
Message-ID: <1434640234836-4671785.post@n4.nabble.com>

[New LWP 21007]
[New LWP 21033]
[New LWP 21152]
[New LWP 21002]
[New LWP 21027]
[New LWP 21021]
[New LWP 21006]
[New LWP 21013]
[New LWP 21003]
[New LWP 21005]
[New LWP 21024]
[New LWP 21004]
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f3974930165 in ?? ()
(gdb) frame 3
#3  0x00007ffe3dbb9710 in ?? ()
(gdb)  print mem_obj->endOffset()
No symbol table is loaded.  Use the "file" command.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671785.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Thu Jun 18 17:11:09 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 18 Jun 2015 19:11:09 +0200 (CEST)
Subject: [squid-users] Recommended Multi-CPU Configuration
In-Reply-To: <CAEnCSG4EQ9_4qHoBnJmS3TKG8fURhzQk7t3K+UmorTuJN=bJWA@mail.gmail.com>
Message-ID: <1343388597.644083624.1434647469665.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> Which one would be good for capacity\load? I have a very, very large
> environment. I have 220,000 users on 8 Gig to the INTERNET. I am
> running a load balancer, ipvsadm (Direct Routing) with 20 proxies
> behind it. I am interested in handling load.
> 
> Michael
> 
> 

I'm working in a close environment + web filtering (e2guardian) - less bandwidth 
Before beginning be aware of SMP limitations, http://wiki.squid-cache.org/Features/SmpScale (look at "What can workers share?" )

I tried multi-CPU, to be honest not very recently, but the limitations were annoying for me and I encountered some issues with high load (crashes)

For the moment I can still manage with hardware over 5 years old, squid core reach 50 % of cpu usage with 700/900 request/s, diskd, basic + digest authentication, delay pool and caches 2 x 300 Go. The web filtering are using the other cores. 
The CPU usage was really reduced but without SMP I do not have any problem at all, I known no pain no gain but ...

My advice, more than CPU buy RAM to support thousand of users you will never have enough RAM ! (64 Go if you can) and really fast disks - dedicated to cache -

Also there are some little things that you can do to help you to improve your capacity, a squid restart each day, tuning the network, tuning the system, etc 

The bandwidth is a point, but the requests by second is THE point 

20 users can get some 4K videos on youtube your bandwidth is consumed but your load average still close to 0.0
20 users with a spyware (or something similar) can push hundreds of requests by seconds, in this case your bandwidth can still stable - with the cache - but your load average increases

It depends   

Fred    


 


From tomtux007 at gmail.com  Thu Jun 18 17:23:26 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Thu, 18 Jun 2015 19:23:26 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <5582AEF9.5030106@treenet.co.nz>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
Message-ID: <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>

Hi

gdb shows the following:


# gdb /usr/local/squid/sbin/squid /root/core
...
...
[New LWP 12812]
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib64/libthread_db.so.1".
Core was generated by `(squid-1) -f /etc/squid/squid.conf'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007ff7ad7da187 in __GI_raise (sig=sig at entry=6) at
../nptl/sysdeps/unix/sysv/linux/raise.c:56
56    ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
...
...
(gdb) backtrace
#0  0x00007ff7ad7da187 in __GI_raise (sig=sig at entry=6) at
../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ff7ad7db538 in __GI_abort () at abort.c:78
#2  0x00007ff7ad7d3126 in __assert_fail_base (fmt=0x7ff7ad908898
"%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
assertion=assertion at entry=0x83314d "0", file=file at entry=0x8114cb
"hash.cc",
    line=line at entry=240, function=function at entry=0x842020
<hash_remove_link::__PRETTY_FUNCTION__> "void
hash_remove_link(hash_table*, hash_link*)") at assert.c:92
#3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
file=0x8114cb "hash.cc", line=240,
    function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
hash_remove_link(hash_table*, hash_link*)") at assert.c:101
#4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
hl=0x204e060) at hash.cc:240
#5  0x0000000000685296 in Auth::User::cacheCleanup
(datanotused=<optimized out>) at User.cc:208
#6  0x00000000006b1269 in AsyncCall::make (this=0x1fb2fa0) at AsyncCall.cc:40
#7  0x00000000006b5055 in AsyncCallQueue::fireNext
(this=this at entry=0x1004a90) at AsyncCallQueue.cc:56
#8  0x00000000006b5450 in AsyncCallQueue::fire (this=0x1004a90) at
AsyncCallQueue.cc:42
#9  0x0000000000582ab1 in dispatchCalls (this=0x7fffff7e1470) at
EventLoop.cc:143
#10 EventLoop::runOnce (this=this at entry=0x7fffff7e1470) at EventLoop.cc:108
#11 0x0000000000582ca0 in EventLoop::run
(this=this at entry=0x7fffff7e1470) at EventLoop.cc:82
#12 0x00000000005dd3a3 in SquidMain (argc=<optimized out>,
argv=<optimized out>) at main.cc:1511
#13 0x00000000004ff0fb in SquidMainSafe (argv=<optimized out>,
argc=<optimized out>) at main.cc:1243
#14 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1236
(gdb)

On Thu, Jun 18, 2015 at 1:43 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> Then it is a segmentation fault.
> You are going to have to run Squid under gdb like Ralf suggested.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From davis_je at efn.org  Thu Jun 18 22:43:04 2015
From: davis_je at efn.org (Jeremy Davis)
Date: Thu, 18 Jun 2015 15:43:04 -0700
Subject: [squid-users] ICAP error handling
Message-ID: <3B85DFCE-05A8-43D1-BACF-49BB065DB285@efn.org>

Hello all,

I am seeking for a way to handle errors returned by the ICAP server. Specifically a '503 Service Overloaded' response. Is there a way to set squid so that this or a similar response is ignored? Or does error handling need to be done somewhere else in squid? When this message is returned, the client sees an error page. 

Thank you!


From hack.back at hotmail.com  Fri Jun 19 07:42:48 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 19 Jun 2015 00:42:48 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <5582B8FF.5060608@treenet.co.nz>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
Message-ID: <1434699768366-4671789.post@n4.nabble.com>

test test waiting you amos



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671789.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jun 19 10:03:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2015 22:03:48 +1200
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434699768366-4671789.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com>
Message-ID: <5583E904.7040807@treenet.co.nz>

On 19/06/2015 7:42 p.m., HackXBack wrote:
> test test waiting you amos
> 

It seems the core is lacking symbols still. So we get the function call
stack, but not the state Squid is dealing with that makes that stack happen.

Which means we are still stuck. Unless you have a copy of the built
binary's symbols available to load into gdb?

Amos



From squid3 at treenet.co.nz  Fri Jun 19 10:04:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2015 22:04:45 +1200
Subject: [squid-users] ICAP error handling
In-Reply-To: <3B85DFCE-05A8-43D1-BACF-49BB065DB285@efn.org>
References: <3B85DFCE-05A8-43D1-BACF-49BB065DB285@efn.org>
Message-ID: <5583E93D.9080508@treenet.co.nz>

On 19/06/2015 10:43 a.m., Jeremy Davis wrote:
> Hello all,
> 
> I am seeking for a way to handle errors returned by the ICAP server.
> Specifically a '503 Service Overloaded' response. Is there a way to
> set squid so that this or a similar response is ignored? Or does
> error handling need to be done somewhere else in squid? When this
> message is returned, the client sees an error page.

ICAP errors or HTTP errors?

Amos


From squid3 at treenet.co.nz  Fri Jun 19 10:06:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2015 22:06:21 +1200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>	<20150618091957.GK6799@charite.de>	<CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>	<5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
Message-ID: <5583E99D.3030306@treenet.co.nz>

On 19/06/2015 5:23 a.m., Tom Tom wrote:
> Hi
> 
> gdb shows the following:
> 
> 

> #3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
> file=0x8114cb "hash.cc", line=240,
>     function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
> hash_remove_link(hash_table*, hash_link*)") at assert.c:101
> #4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
> hl=0x204e060) at hash.cc:240
> #5  0x0000000000685296 in Auth::User::cacheCleanup
> (datanotused=<optimized out>) at User.cc:208

Ah. The auth username cache again.

Amos



From alex at samad.com.au  Fri Jun 19 10:19:07 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 19 Jun 2015 20:19:07 +1000
Subject: [squid-users] Memory usage question
Message-ID: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>

Hi

I recently push my squid VM memory up to 65G
i pushed up squid usage (i thought) to 40G

squid.conf
cache_mem 40960 MB

cache.log
2015/06/18 22:12:33| Max Mem  size: 41943040 KB
2015/06/18 22:12:33| Max Swap size: 177527808 KB



but it doesn't seem like its using it

 free -g
             total       used       free     shared    buffers     cached
Mem:            62          5         57          0          0          1
-/+ buffers/cache:          2         59
Swap:            1          0          1

again from squid.conf
cache_dir aufs /var/spool/squid 29999 16 256
cache_dir aufs /var/spool/squid2 58368 32 256
cache_dir aufs /var/spool/squid3 85000 32 256

Is this just a case that I don't have enough disk to backend the memory usage ?


this is on centos 6.6
still using the redhat build squid !
rpm -q squid
squid-3.1.10-29.el6.x86_64


 ps -u squid -o pid,rss,vsz,cmd | grep squid.con
  PID   RSS    VSZ CMD
 21605 839260 916880 (squid) -f /etc/squid/squid.conf


I have slowly added disk space here and I am actually planning on just
inserting 1T lun (this is VMware) and migrating it all over to there.


What am I missing, also on the cache_dir statement is there any way of
specifying 1G as 1G not 1024000. ie can i specify the units ?

Thanks


From bimal_pandit at rediffmail.com  Fri Jun 19 11:00:41 2015
From: bimal_pandit at rediffmail.com (Bimal  Kumar)
Date: 19 Jun 2015 11:00:41 -0000
Subject: [squid-users] =?utf-8?q?bypass_proxy?=
In-Reply-To: <CAD8CJ-p4RZxS44-eJDzO=wnsU+RcXsk9Eo6vi87K47EGmvfgag@mail.gmail.com>
Message-ID: <1434532576.S.14718.15757.f5-147-221.1434711640.15005@webmail.rediffmail.com>

Dear Yashvinder,

try this....

  ========================================

function FindProxyForURL(url, host)
{

// To bypass proxy for an IP in LAN.

	if (isInNet(myIpAddress(), "100.200.101.201", "255.255.0.0"))
	return "DIRECT";

// To bypass proxy for a website(url) to access directly.

	if (shExpMatch(url, "*flipkart.com") ||
    		shExpMatch(url, "*apple.com") ||
    		shExpMatch(url, "*linuxbite.com") ||
    		shExpMatch(url, "*rediff.com"))
	return "DIRECT";


}

  ========================================

within your code.

hope this will help,

regards

Bimal Kumar

On Wed, 17 Jun 2015 14:46:16 +0530 yashvinder hooda  wrote
>Hi All,

I have 2 issues

First one: How can i bypass proxy for an IP in LAN.


Second one: 
I am running squid on openwrt and i want to allow some websites to bypass proxy and want to allow them go direct.
For that i am using wpad with PAC file but the problem is for some websites it works and for some it doesn';t.

Here is my PAC file



function FindProxyForURL(url, host)
{
??? // The 1st if function tests if the URI should be by-passed
??? // Proxy By-Pass List
??? if (
??????? // ignore RFC 1918 internal addreses
??????? isInNet(host, "10.0.0.0", "255.0.0.0") ||
??????? isInNet(host, "172.16.0.0", "255.240.0.0") ||
??????? isInNet(host, "192.168.0.0", "255.255.0.0") ||

??????? // is url is like http://server by-pass
??????? isPlainHostName(host) ||

??????? // localhost!!
??????? localHostOrDomainIs(host, "127.0.0.1") ||

??????? // by-pass internal URLS
??????? dnsDomainIs(host, ".flipkart.com") ||
??????? dnsDomainIs(host, ".apple.com") ||
??????? dnsDomainIs(host, ".linuxbite.com") ||
??????? dnsDomainIs(host, ".rediff.com") ||

??????? // by-pass FTP
//??????? shExpMatch(url, "ftp:*")
??????? url.substring(0, 4)=="ftp:"
??????? )

??????? // If True, tell the browser to go direct
??????? return "DIRECT";

??????? // If False, it';s not on the by-pass then Proxy the request if you fail to connect to the proxy, try direct.

return "PROXY 192.168.1.1:3128";
//return "DIRECT";
}



To be precise it works for apple.com but doesn';t work for rest of the websites.
Please enlighten me.
-- 
Regards,
Yashvinder?




_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org

http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150619/0b0219f0/attachment.htm>

From eliezer at ngtech.co.il  Fri Jun 19 11:16:17 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 19 Jun 2015 14:16:17 +0300
Subject: [squid-users] Memory usage question
In-Reply-To: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
Message-ID: <5583FA01.2020101@ngtech.co.il>

First goes first...
Upgrade to 3.5 or 3.4 branch.
Then try to use top or htop to get a snapshot of the virtual memory and 
resident memory that squid uses.

Eliezer

On 19/06/2015 13:19, Alex Samad wrote:
> this is on centos 6.6
> still using the redhat build squid !
> rpm -q squid
> squid-3.1.10-29.el6.x86_64




From marcus.kool at urlfilterdb.com  Fri Jun 19 12:11:04 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 19 Jun 2015 09:11:04 -0300
Subject: [squid-users] Memory usage question
In-Reply-To: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
Message-ID: <558406D8.8090901@urlfilterdb.com>

What is the physical memory size ??
You might want to read the faq on memory:
http://wiki.squid-cache.org/SquidFaq/SquidMemory

Marcus


On 06/19/2015 07:19 AM, Alex Samad wrote:
> Hi
>
> I recently push my squid VM memory up to 65G
> i pushed up squid usage (i thought) to 40G
>
> squid.conf
> cache_mem 40960 MB
>
> cache.log
> 2015/06/18 22:12:33| Max Mem  size: 41943040 KB
> 2015/06/18 22:12:33| Max Swap size: 177527808 KB
>
>
>
> but it doesn't seem like its using it
>
>   free -g
>               total       used       free     shared    buffers     cached
> Mem:            62          5         57          0          0          1
> -/+ buffers/cache:          2         59
> Swap:            1          0          1
>
> again from squid.conf
> cache_dir aufs /var/spool/squid 29999 16 256
> cache_dir aufs /var/spool/squid2 58368 32 256
> cache_dir aufs /var/spool/squid3 85000 32 256
>
> Is this just a case that I don't have enough disk to backend the memory usage ?
>
>
> this is on centos 6.6
> still using the redhat build squid !
> rpm -q squid
> squid-3.1.10-29.el6.x86_64
>
>
>   ps -u squid -o pid,rss,vsz,cmd | grep squid.con
>    PID   RSS    VSZ CMD
>   21605 839260 916880 (squid) -f /etc/squid/squid.conf
>
>
> I have slowly added disk space here and I am actually planning on just
> inserting 1T lun (this is VMware) and migrating it all over to there.
>
>
> What am I missing, also on the cache_dir statement is there any way of
> specifying 1G as 1G not 1024000. ie can i specify the units ?
>
> Thanks
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From davis_je at efn.org  Fri Jun 19 17:40:21 2015
From: davis_je at efn.org (Jeremy Davis)
Date: Fri, 19 Jun 2015 10:40:21 -0700
Subject: [squid-users] ICAP error handling
In-Reply-To: <5583E93D.9080508@treenet.co.nz>
References: <3B85DFCE-05A8-43D1-BACF-49BB065DB285@efn.org>
 <5583E93D.9080508@treenet.co.nz>
Message-ID: <41556CB4-F16C-4EAE-849F-451E055E24AF@efn.org>


Hi Amos,

The ICAP server is returning a 503 ICAP error. I am attempting to intercept or ignore those so my users don't see the error message. 


> On Jun 19, 2015, at 3:04 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 19/06/2015 10:43 a.m., Jeremy Davis wrote:
>> Hello all,
>> 
>> I am seeking for a way to handle errors returned by the ICAP server.
>> Specifically a '503 Service Overloaded' response. Is there a way to
>> set squid so that this or a similar response is ignored? Or does
>> error handling need to be done somewhere else in squid? When this
>> message is returned, the client sees an error page.
> 
> ICAP errors or HTTP errors?
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From stan.prescott at gmail.com  Fri Jun 19 21:46:06 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 19 Jun 2015 16:46:06 -0500
Subject: [squid-users] Cache index (swap.state) corruption when
	enabling/disabling SSLBump?
Message-ID: <CANLNtGSvob8sSM8MNo1QKAgvP1SVUNgtRdz+gVk1G0cDkha-hQ@mail.gmail.com>

I have a working SSLBump configuration with Squid 3.5.4. It seems that
sometimes, if switching from HTTPS caching to only HTTP caching the cache
becomes corrupted requiring deleting the swap.state file and recreating it
by rebuilding the cache directories.

Has anyone else seen this? If so, has anyone found a fix that will prevent
this from happening?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150619/1ed9954a/attachment.htm>

From davincy9102 at hotmail.com  Fri Jun 19 21:45:39 2015
From: davincy9102 at hotmail.com (davincy)
Date: Fri, 19 Jun 2015 14:45:39 -0700 (PDT)
Subject: [squid-users] Help with squid 4.5. on centos 6.6. filter Https
In-Reply-To: <55722854.5090607@treenet.co.nz>
References: <1433515533331-4671561.post@n4.nabble.com>
 <1433515911998-4671562.post@n4.nabble.com> <55722854.5090607@treenet.co.nz>
Message-ID: <1434750339272-4671800.post@n4.nabble.com>

Im trying to enable the certificates. I have the certs .pen but I dont know
how get the .key

https_port 3129 intercept ssl-bump
cert=/etc/squid/ssl_cert/sslsplit_ca_cert.pem
cafile=/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE

On Centos This Lines are funcional?

sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

I dont find the directory

/libexec  and the files ssl_crtd and the ssl_db

Thanks for your help AMos, o Jeffrey 





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-with-squid-3-5-on-centos-6-6-filter-Https-tp4671561p4671800.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rusdso at gmail.com  Fri Jun 19 22:30:57 2015
From: rusdso at gmail.com (Russel D'Souza)
Date: Fri, 19 Jun 2015 15:30:57 -0700
Subject: [squid-users] Cache_peer HTTP Digest authentication issue
Message-ID: <CAG3jBP0BXi3CD61s4uY8_ut=kJReEckpWUsgucTr8hktQJLd-Q@mail.gmail.com>

Hi All,

My name is Russel and I am working on a project wherein I encountered a
problem exactly similar to the one posted here:
http://www.squid-cache.org/mail-archive/squid-users/201006/0326.html  for
which I do not see a solution.


[image: Inline image 1]
Basically, as in the above figure the End user has to access the Internet
going through a Squid Proxy (with no auth). Next the squid proxy must add
the DIGEST/NTLM usename:password to the message and pass it on to the
DIGEST/NTLM Server and give the results back to the user.

Please let me know if there is any way to resolve this issue using Squid.
If Squid does not have this feature, what is the best way to achieve this ?

Thanks,
 Russel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150619/583ab4c4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 21725 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150619/583ab4c4/attachment.png>

From squid3 at treenet.co.nz  Fri Jun 19 23:49:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2015 11:49:47 +1200
Subject: [squid-users] Cache_peer HTTP Digest authentication issue
In-Reply-To: <CAG3jBP0BXi3CD61s4uY8_ut=kJReEckpWUsgucTr8hktQJLd-Q@mail.gmail.com>
References: <CAG3jBP0BXi3CD61s4uY8_ut=kJReEckpWUsgucTr8hktQJLd-Q@mail.gmail.com>
Message-ID: <5584AA9B.8090002@treenet.co.nz>

On 20/06/2015 10:30 a.m., Russel D'Souza wrote:
> Hi All,
> 
> My name is Russel and I am working on a project wherein I encountered a
> problem exactly similar to the one posted here:
> http://www.squid-cache.org/mail-archive/squid-users/201006/0326.html  for
> which I do not see a solution.
> 
> 
> [image: Inline image 1]
> Basically, as in the above figure the End user has to access the Internet
> going through a Squid Proxy (with no auth). Next the squid proxy must add
> the DIGEST/NTLM usename:password to the message and pass it on to the
> DIGEST/NTLM Server and give the results back to the user.
> 
> Please let me know if there is any way to resolve this issue using Squid.
> If Squid does not have this feature, what is the best way to achieve this ?

Squid does not support Digest authentication to peers.

Patches or sponsorship are welcome.
<http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>

Amos


From squid3 at treenet.co.nz  Fri Jun 19 23:55:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2015 11:55:31 +1200
Subject: [squid-users] Help with squid 4.5. on centos 6.6. filter Https
In-Reply-To: <1434750339272-4671800.post@n4.nabble.com>
References: <1433515533331-4671561.post@n4.nabble.com>
 <1433515911998-4671562.post@n4.nabble.com> <55722854.5090607@treenet.co.nz>
 <1434750339272-4671800.post@n4.nabble.com>
Message-ID: <5584ABF3.8010106@treenet.co.nz>

On 20/06/2015 9:45 a.m., davincy wrote:
> Im trying to enable the certificates. I have the certs .pen but I dont know
> how get the .key
> 
> https_port 3129 intercept ssl-bump
> cert=/etc/squid/ssl_cert/sslsplit_ca_cert.pem
> cafile=/etc/squid/certs/sslsplit_ca_cert.pem
> key=/opt/etc/squid/certs/sslsplit_ca_key.pem  generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE
> 
> On Centos This Lines are funcional?

There is nothing CentOS specific about them. With the directory paths
existing, and the relevant *.pem files correctly built they should work
on any POSIX based OS.


> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> sslcrtd_children 5
> 
> I dont find the directory
> 
> /libexec  and the files ssl_crtd and the ssl_db
> 

If the /opt/libexec path does not exist then your Squid will definitely
not work. You need to locate where that sl_crtd helper binary was
installed and use the right path.

Also use "squid -v" to confirm the --enable-ssl-crtd was used during the
build.

Amos


From squid3 at treenet.co.nz  Sat Jun 20 00:21:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2015 12:21:26 +1200
Subject: [squid-users] Cache index (swap.state) corruption when
 enabling/disabling SSLBump?
In-Reply-To: <CANLNtGSvob8sSM8MNo1QKAgvP1SVUNgtRdz+gVk1G0cDkha-hQ@mail.gmail.com>
References: <CANLNtGSvob8sSM8MNo1QKAgvP1SVUNgtRdz+gVk1G0cDkha-hQ@mail.gmail.com>
Message-ID: <5584B206.1030704@treenet.co.nz>

On 20/06/2015 9:46 a.m., Stanford Prescott wrote:
> I have a working SSLBump configuration with Squid 3.5.4. It seems that
> sometimes, if switching from HTTPS caching to only HTTP caching the cache
> becomes corrupted requiring deleting the swap.state file and recreating it
> by rebuilding the cache directories.
> 
> Has anyone else seen this? If so, has anyone found a fix that will prevent
> this from happening?

That is very strange. The cache has nothing to do with HTTPS vs HTTP.
All inputs and outputs are just URI+object with a hashe.

It dont matter if its http:// or https:// or ftp:/ or gopher:// or
whois:// ... its just a URI with a binary object attached.


I suspect you are probably encountering object corruption from in
incomplete shutdown/restart process. This can happen if there is a big
object in transit at the time, or if you set shutdown_timeout to a
small value, or used kill -9 to stop Squid.

Amos



From stan.prescott at gmail.com  Sat Jun 20 00:27:23 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 19 Jun 2015 19:27:23 -0500
Subject: [squid-users] Cache index (swap.state) corruption when
 enabling/disabling SSLBump?
In-Reply-To: <5584B206.1030704@treenet.co.nz>
References: <CANLNtGSvob8sSM8MNo1QKAgvP1SVUNgtRdz+gVk1G0cDkha-hQ@mail.gmail.com>
 <5584B206.1030704@treenet.co.nz>
Message-ID: <CANLNtGSrJ+fqBSpcMxNSOEsh1o2sNhng-5kZXyKdOiX4mmrP0A@mail.gmail.com>

Thanks, Amos. I will look into that.

On Fri, Jun 19, 2015 at 7:21 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/06/2015 9:46 a.m., Stanford Prescott wrote:
> > I have a working SSLBump configuration with Squid 3.5.4. It seems that
> > sometimes, if switching from HTTPS caching to only HTTP caching the cache
> > becomes corrupted requiring deleting the swap.state file and recreating
> it
> > by rebuilding the cache directories.
> >
> > Has anyone else seen this? If so, has anyone found a fix that will
> prevent
> > this from happening?
>
> That is very strange. The cache has nothing to do with HTTPS vs HTTP.
> All inputs and outputs are just URI+object with a hashe.
>
> It dont matter if its http:// or https:// or ftp:/ or gopher:// or
> whois:// ... its just a URI with a binary object attached.
>
>
> I suspect you are probably encountering object corruption from in
> incomplete shutdown/restart process. This can happen if there is a big
> object in transit at the time, or if you set shutdown_timeout to a
> small value, or used kill -9 to stop Squid.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150619/e0e5a486/attachment.htm>

From hack.back at hotmail.com  Sat Jun 20 02:55:08 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 19 Jun 2015 19:55:08 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <5583E904.7040807@treenet.co.nz>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
Message-ID: <1434768908730-4671806.post@n4.nabble.com>

i cant understand you , what you want from me to do exactly ?
we need to solve this problem 
am using debian 7


./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--with-pthreads  --with-aufs-threads=512 --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
--enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
--enable-http-violations --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-swapdir=/var/spool/squid --enable-ltdl-convenience
--with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
--enable-referer-log --enable-truncate --disable-unlinkd
--enable-useragent-log --enable-eui --enable-large-cache-files
'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671806.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sat Jun 20 05:24:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2015 17:24:11 +1200
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434768908730-4671806.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
 <1434768908730-4671806.post@n4.nabble.com>
Message-ID: <5584F8FB.20408@treenet.co.nz>

On 20/06/2015 2:55 p.m., HackXBack wrote:
> i cant understand you , what you want from me to do exactly ?
> we need to solve this problem 
> am using debian 7

Aha, okay. You need to install the squid3-dbg package to get the debug
symbols used by gdb.

Then run gdb like so:
 gdb /usr/lib/debug/usr/sbin/squid3 /path/to/squid3.core.file

Amos



From alex at samad.com.au  Sat Jun 20 09:08:50 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 20 Jun 2015 19:08:50 +1000
Subject: [squid-users] Memory usage question
In-Reply-To: <5583FA01.2020101@ngtech.co.il>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
 <5583FA01.2020101@ngtech.co.il>
Message-ID: <CAJ+Q1PVBCkbg-vhMfzVRyc0ZnxWs8W23F-fXSSC6UMXdKyzJsA@mail.gmail.com>

Hi

Are there any gotchas i need to look out for.
Also I have allocated a 1T lun to the VM. Whats the best way to
allocate this do I use 1 cache_dir or multiple cache_dir.

I currently have 3, is there a way to migrate the cache objects in the
3 into 1 or do I just delete them and bear the cost of re downloading
them




On 19 June 2015 at 21:16, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> First goes first...
> Upgrade to 3.5 or 3.4 branch.
> Then try to use top or htop to get a snapshot of the virtual memory and
> resident memory that squid uses.
>
> Eliezer
>
> On 19/06/2015 13:19, Alex Samad wrote:
>>
>> this is on centos 6.6
>> still using the redhat build squid !
>> rpm -q squid
>> squid-3.1.10-29.el6.x86_64
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Jason_Haar at trimble.com  Sat Jun 20 22:31:45 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Sun, 21 Jun 2015 10:31:45 +1200
Subject: [squid-users] confused about ICAP and who's downloading what
Message-ID: <5585E9D1.5080208@trimble.com>

Hi there

I'm starting to use ICAP as an AV content filter, having moved away from
using the  havp antivirus proxy as a parent proxy

Part of the problem with havp was that it stopped being developed years
ago and HTTP trickery had moved on in ways that basically it
couldn't support - but squid - being the wonderful piece of loved
software it is - was keeping up with the times :-)

Anyway, now that I'm trialing ICAP, I'm concerned about the same issue.
When a web page is requested by a client, what component does what? Does
squid do the download, pass the content to ICAP, or does it (like with
parent proxies), just tell the ICAP software to do the download itself?
You can see where I'm going, the latter would mean "odd" HTTP
applications which might work fine through squid might fail if the ICAP
software does things differently

(btw: "odd" can mean many things: even how dns lookups occur, ipv6
support,etc)

Thanks

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From Antony.Stone at squid.open.source.it  Sat Jun 20 22:45:11 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Jun 2015 00:45:11 +0200
Subject: [squid-users] confused about ICAP and who's downloading what
In-Reply-To: <5585E9D1.5080208@trimble.com>
References: <5585E9D1.5080208@trimble.com>
Message-ID: <201506210045.11647.Antony.Stone@squid.open.source.it>

On Sunday 21 June 2015 at 00:31:45 (EU time), Jason Haar wrote:

> When a web page is requested by a client, what component does what? Does
> squid do the download, pass the content to ICAP, or does it (like with
> parent proxies), just tell the ICAP software to do the download itself?

The former - squid does the download and passes the content to ICAP.

ICAP is simply a means for modifying, or providing information about, HTTP 
requests and responses which are fed to the ICAP server.  The ICAP server 
itself doesn't perform any HTTP requests.

See https://tools.ietf.org/html/rfc3507 for more details (the diagrams in 3.1 
and 3.2 show how the dataflow works).

As that RFC says, "ICAP is, in essence, a lightweight protocol for executing a 
'remote procedure call' on HTTP messages.  It allows ICAP clients to pass HTTP 
messages to ICAP servers for some sort of transformation or other processing".

Squid is the ICAP client in your setup.


Regards,


Antony.

-- 
"I estimate there's a world market for about five computers."

 - Thomas J Watson, Chairman of IBM

                                                   Please reply to the list;
                                                         please *don't* CC me.


From hack.back at hotmail.com  Sun Jun 21 01:27:57 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 20 Jun 2015 18:27:57 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <5584F8FB.20408@treenet.co.nz>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
 <1434768908730-4671806.post@n4.nabble.com> <5584F8FB.20408@treenet.co.nz>
Message-ID: <1434850077862-4671812.post@n4.nabble.com>

New LWP 524]
[New LWP 766]
[New LWP 676]
[New LWP 507]
[New LWP 819]
[New LWP 849]
[New LWP 730]
[New LWP 641]
[New LWP 651]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f9251235165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f9251235165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f92512383e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007f925122e311 in __assert_fail () from
/lib/x86_64-linux-gnu/libc.so.6
#3  0x0000000000778559 in ?? ()
#4  0x00007ffebcde45a0 in ?? ()
#5  0x00000000005e5cd9 in CountHist ()
#6  0x00007ffebcde4600 in ?? ()
#7  0x00000001d223f458 in ?? ()
#8  0x00007ffebcde47e0 in ?? ()
#9  0x000000100066cb42 in ?? ()
#10 0x00007ffebcde4610 in ?? ()
#11 0x0000000000000001 in ?? ()
#12 0x00007ffebcde4660 in ?? ()
#13 0x000000000076c504 in ?? ()
#14 0x000000007d000000 in ?? ()
#15 0x0000000100000800 in ?? ()
#16 0x00000001d223f458 in ?? ()
#17 0x0000000100000006 in ?? ()
#18 0x0000000000000000 in ?? ()
(gdb) frame 3
#3  0x0000000000778559 in ?? ()
(gdb) print mem_obj->endOffset()
No symbol "mem_obj" in current context.
(gdb)


THIS IS THE OUTPUT OF 
gdb /usr/lib/debug/usr/sbin/squid3 /var/spool/squid/core




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671812.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Jun 21 01:57:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jun 2015 13:57:59 +1200
Subject: [squid-users] Memory usage question
In-Reply-To: <CAJ+Q1PVBCkbg-vhMfzVRyc0ZnxWs8W23F-fXSSC6UMXdKyzJsA@mail.gmail.com>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
 <5583FA01.2020101@ngtech.co.il>
 <CAJ+Q1PVBCkbg-vhMfzVRyc0ZnxWs8W23F-fXSSC6UMXdKyzJsA@mail.gmail.com>
Message-ID: <55861A27.4050507@treenet.co.nz>

On 20/06/2015 9:08 p.m., Alex Samad wrote:
> Hi
> 
> Are there any gotchas i need to look out for.
> Also I have allocated a 1T lun to the VM. Whats the best way to
> allocate this do I use 1 cache_dir or multiple cache_dir.

The usual one UFS based dir per physical drive and no RAID. That can be
tricky with SAN/NAS based disk and VMs.

> 
> I currently have 3, is there a way to migrate the cache objects in the
> 3 into 1 or do I just delete them and bear the cost of re downloading
> them

If you need to merge them you can set cache_dir to read-only for a
period. That way Squid will use their content until it becomes too far
out of date, whiel storing new objects into the witable cache_dir.


It should not be a big problem/cost to drop the cache anyway. The
bandwidth to rebuild a cache is far smaller than most people expect.
Much of the content in a large cache is stale objects waiting
revalidation or replacement, and all HITs are duplicates by definition.
The cache fill rate is an exponential/polynomial growth curve with the
bulk of it being a few seconds/minutes worth of traffic.

Amos



From squid3 at treenet.co.nz  Sun Jun 21 03:48:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jun 2015 15:48:22 +1200
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434850077862-4671812.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
 <1434768908730-4671806.post@n4.nabble.com> <5584F8FB.20408@treenet.co.nz>
 <1434850077862-4671812.post@n4.nabble.com>
Message-ID: <55863406.3020305@treenet.co.nz>

On 21/06/2015 1:27 p.m., HackXBack wrote:
> New LWP 524]
> [New LWP 766]
> [New LWP 676]
> [New LWP 507]
> [New LWP 819]
> [New LWP 849]
> [New LWP 730]
> [New LWP 641]
> [New LWP 651]
> 
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
> Program terminated with signal 6, Aborted.
> #0  0x00007f9251235165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) backtrace
> #0  0x00007f9251235165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007f92512383e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x00007f925122e311 in __assert_fail () from
> /lib/x86_64-linux-gnu/libc.so.6
> #3  0x0000000000778559 in ?? ()
> #4  0x00007ffebcde45a0 in ?? ()
> #5  0x00000000005e5cd9 in CountHist ()
> #6  0x00007ffebcde4600 in ?? ()
> #7  0x00000001d223f458 in ?? ()
> #8  0x00007ffebcde47e0 in ?? ()
> #9  0x000000100066cb42 in ?? ()
> #10 0x00007ffebcde4610 in ?? ()
> #11 0x0000000000000001 in ?? ()
> #12 0x00007ffebcde4660 in ?? ()
> #13 0x000000000076c504 in ?? ()
> #14 0x000000007d000000 in ?? ()
> #15 0x0000000100000800 in ?? ()
> #16 0x00000001d223f458 in ?? ()
> #17 0x0000000100000006 in ?? ()
> #18 0x0000000000000000 in ?? ()
> (gdb) frame 3
> #3  0x0000000000778559 in ?? ()
> (gdb) print mem_obj->endOffset()
> No symbol "mem_obj" in current context.
> (gdb)
> 
> 
> THIS IS THE OUTPUT OF 
> gdb /usr/lib/debug/usr/sbin/squid3 /var/spool/squid/core
> 

Darn. Oh well. Chudy seems to be making some progress.
Can you try the patch he submitted?

Amos



From al_luhaybi at yahoo.com  Sun Jun 21 08:18:59 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Sun, 21 Jun 2015 01:18:59 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
Message-ID: <1434874739081-4671815.post@n4.nabble.com>

Hello,

couple of squids servers; in a parent child peer relationship. both are on
centos 6.

the parent is doing url rewrites, parent hit and actually serve traffic from
local cache; while child see's it as miss on both ICP and HTCP, and as such; 
Qos_flows tos parent-hit=0xXX doesn't work.

i've used the parent tos local cache command and it works, I can actually
see the tos tag being delivered from parent to child via tcpdump.


problem is, for the child; if i set the qos_flows miss command for the
parent, child squid will actually tag all hit and miss traffic of parent
with the speified tos.

but if i use the miss-mask=0xXX command; tos is always 0ut with 0x00 for the
parent; so basically, child squid (3.5.5) is not passing the traffic with
the tos being send from parent;  

according to documentation; tos preserve is supposed to be default; provided
that you also do the ZPH patch on linux as the specified link on the Doc
page of qos_flows.

tried everything, nothing worked; please help



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From al_luhaybi at yahoo.com  Sun Jun 21 08:21:56 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Sun, 21 Jun 2015 01:21:56 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <1434874739081-4671815.post@n4.nabble.com>
References: <1434874739081-4671815.post@n4.nabble.com>
Message-ID: <1434874916708-4671816.post@n4.nabble.com>

just a correction, tos is always being reset to 0x00 at the child.

local hits on both parent and child works.

child actually add tos value for local hit.

on the child the miss command marks all parent (misses)

but child never preserves the parent tos



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671816.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jun 21 09:57:44 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 02:57:44 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <55863406.3020305@treenet.co.nz>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
 <1434768908730-4671806.post@n4.nabble.com> <5584F8FB.20408@treenet.co.nz>
 <1434850077862-4671812.post@n4.nabble.com> <55863406.3020305@treenet.co.nz>
Message-ID: <1434880664960-4671817.post@n4.nabble.com>

Yes sure,
can you give me the link to download chudy patch ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671817.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Jun 21 10:41:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 21 Jun 2015 16:41:48 +0600
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <1434880664960-4671817.post@n4.nabble.com>
References: <1434627020894-4671781.post@n4.nabble.com>
 <1434627721687-4671782.post@n4.nabble.com> <5582B8FF.5060608@treenet.co.nz>
 <1434699768366-4671789.post@n4.nabble.com> <5583E904.7040807@treenet.co.nz>
 <1434768908730-4671806.post@n4.nabble.com> <5584F8FB.20408@treenet.co.nz>
 <1434850077862-4671812.post@n4.nabble.com> <55863406.3020305@treenet.co.nz>
 <1434880664960-4671817.post@n4.nabble.com>
Message-ID: <558694EC.1080403@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://bugs.squid-cache.org/attachment.cgi?id=3162

21.06.15 15:57, HackXBack ?????:
> Yes sure,
> can you give me the link to download chudy patch ?
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671817.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVhpTsAAoJENNXIZxhPexGUOsH/iJyBoW/wtp1TabsJIGrKZKD
LMr8+L6u9QS56n72LRrKPqJXl8IOSzhpblNhQzDqd+U8b8kL0XNGaokISdthgxTY
SCAMZpUESnAmEquNLPGivZgUExBXfbJGkoTPWwLuY6LY321Zn62T5dJCDfmEZtQF
fY4h9rzsgWLfTWbS5lBbEUjp694MLJHi2YZ86jlaB5e+MATfxI63bI7Xc+BSOqOk
wBgaNH5U9yB7bk03tkYhKKLmbDvmPqJf1NeFwC2BH6zvJqYaBWjf/JZMC5zwbIFn
7SbQXoXgFmgV4h7qHHRtnLaoX+qrrZoI+IgdVMymEg2ZRUb359mYyvm9GZSQ6eY=
=0EPK
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Sun Jun 21 11:22:08 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 04:22:08 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <558694EC.1080403@gmail.com>
References: <1434627721687-4671782.post@n4.nabble.com>
 <5582B8FF.5060608@treenet.co.nz> <1434699768366-4671789.post@n4.nabble.com>
 <5583E904.7040807@treenet.co.nz> <1434768908730-4671806.post@n4.nabble.com>
 <5584F8FB.20408@treenet.co.nz> <1434850077862-4671812.post@n4.nabble.com>
 <55863406.3020305@treenet.co.nz> <1434880664960-4671817.post@n4.nabble.com>
 <558694EC.1080403@gmail.com>
Message-ID: <1434885728123-4671819.post@n4.nabble.com>

hmmmmmm well this patch seems it solve the problem, 
squid have 15 min run till now ...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671819.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jun 21 11:55:58 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 04:55:58 -0700 (PDT)
Subject: [squid-users] problem with some ssl services
In-Reply-To: <55822C37.7070105@treenet.co.nz>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
 <55822C37.7070105@treenet.co.nz>
Message-ID: <1434887758720-4671820.post@n4.nabble.com>

for example the problem is in facebook app on iphone, 
i need to trace the ip's then none ssl bump to this ip to make the facebook
app work,
now am using 3.5, you said that it can be make this automatically ?
but with which peak and splice conf ?
need to give a try .
Thanks amos



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/problem-with-some-ssl-services-tp4671733p4671820.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sun Jun 21 12:02:50 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 05:02:50 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <554776B1.2080401@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz>
Message-ID: <1434888170081-4671821.post@n4.nabble.com>

i install 3.5 and still the same problem ,
this assertion error exist when i use 

acl partial dstdomain .googlevideo.com
acl partial dstdomain .youtube.com
acl partial dstdomain .mgccw.com
range_offset_limit none partial



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671821.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gcsquid-users at crowie.net  Sun Jun 21 12:24:57 2015
From: gcsquid-users at crowie.net (Graham)
Date: Sun, 21 Jun 2015 22:24:57 +1000
Subject: [squid-users] Squid to ask for, but not require, authentication.
Message-ID: <5586AD19.4060505@crowie.net>

I am looking for a way to configure Squid to ask for (and check) 
authentication using LDAP, but to proceed if there is no auth 
information provided.

I have been using DansGuardian for a while with Squid authenticating and 
then getting DansGuardian to filter based on the username that Squid has 
authenticated. The browsers talk directly to DansGuardian, which talks 
to Squid, which does the work over the 'net.

I am now trying to add an android device - which has some apps that 
don't ask the user for a login/password (although they do talk to the 
proxy) and therefore they fail to connect with a 407 error. I have 
modified DansGuardian to allow just this one IP to work without 
authentication, but Squid requires the auth and denies the requests. If 
I make Squid more permissive (remove the auth config) then DansGuardian 
works with that IP address, but will then block all other IP addresses 
as Squid hasn't authenticated anyone. Note that I can't do IP 
authentication from Squid because all requests come from the 
DansGuardian IP (which happens to be localhost) and it can't tell which 
ones to authenticate and which to allow.

Basically what I think I want is for DansGuardian to make the decisions 
on whether to allow the connection, and Squid to perform the check of 
the authentication via LDAP and to allow the connection if the auth is 
OK, or is not present... and to deny the connection if the auth is 
present but incorrect.


Is this possible?

Or am I going about this in the wrong way?


Thanks

GC


From squid3 at treenet.co.nz  Sun Jun 21 12:35:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 00:35:07 +1200
Subject: [squid-users] Squid to ask for, but not require, authentication.
In-Reply-To: <5586AD19.4060505@crowie.net>
References: <5586AD19.4060505@crowie.net>
Message-ID: <5586AF7B.30505@treenet.co.nz>

On 22/06/2015 12:24 a.m., Graham wrote:
> I am looking for a way to configure Squid to ask for (and check)
> authentication using LDAP, but to proceed if there is no auth
> information provided.

Not possible. The process of asking for auth sends a reply to the client
request. Once that happens there is nothing further possible.

You can check for auth and continue if its missing, but when doing so
cannot ask the client to send any credentials. A secure client will not
send credentials unless asked...

> 
> I have been using DansGuardian for a while with Squid authenticating and
> then getting DansGuardian to filter based on the username that Squid has
> authenticated. The browsers talk directly to DansGuardian, which talks
> to Squid, which does the work over the 'net.
> 
> I am now trying to add an android device - which has some apps that
> don't ask the user for a login/password (although they do talk to the
> proxy) and therefore they fail to connect with a 407 error. I have
> modified DansGuardian to allow just this one IP to work without
> authentication, but Squid requires the auth and denies the requests. If
> I make Squid more permissive (remove the auth config) then DansGuardian
> works with that IP address, but will then block all other IP addresses
> as Squid hasn't authenticated anyone. Note that I can't do IP
> authentication from Squid because all requests come from the
> DansGuardian IP (which happens to be localhost) and it can't tell which
> ones to authenticate and which to allow.

You should be able to use something like the User-Agent header
("browser" regex ACL type) to bypass the auth requirement on a
per-request basis. This has to be done for many Java applications, for
example.

Amos


From squid3 at treenet.co.nz  Sun Jun 21 12:40:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 00:40:13 +1200
Subject: [squid-users] problem with some ssl services
In-Reply-To: <1434887758720-4671820.post@n4.nabble.com>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
 <55822C37.7070105@treenet.co.nz> <1434887758720-4671820.post@n4.nabble.com>
Message-ID: <5586B0AD.70503@treenet.co.nz>

On 21/06/2015 11:55 p.m., HackXBack wrote:
> for example the problem is in facebook app on iphone, 
> i need to trace the ip's then none ssl bump to this ip to make the facebook
> app work,
> now am using 3.5, you said that it can be make this automatically ?

*some* issues that Squid detects as clearly meaning it can't bump the
latest version(s) will auto-splice. Whether it does so for these
particular apps depends on whether they are breaking because of those
issues or something else.
 Using the latest version will make your users experience better than
older versions. But no guarantees about the problem you are looking into
(its still a little vague what that is caused by).

> but with which peak and splice conf ?

Unsure.

Amos



From harley at thepetersclan.com  Sun Jun 21 13:19:42 2015
From: harley at thepetersclan.com (Harley Peters)
Date: Sun, 21 Jun 2015 08:19:42 -0500
Subject: [squid-users] problem with some ssl services
In-Reply-To: <1434625272907-4671777.post@n4.nabble.com>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
 <55822C37.7070105@treenet.co.nz> <1434625272907-4671777.post@n4.nabble.com>
Message-ID: <5586B9EE.5010702@thepetersclan.com>

On 06/18/2015 06:01 AM, HackXBack wrote:
> acl exclude_acl ssl::server_name .yahoo.com .gmail.com .googlemail.com
> s.yimg.com .yahooapis.com .akamaihd.net .fbcdn.net .facebook.com .google.com
> ssl_bump peek step1 all
> ssl_bump splice step2 exclude_acl
> ssl_bump stare step2 all
> ssl_bump bump step3 all
> sslproxy_cert_error allow all

I tried your configuration in my squid conf just for the heck of it.
Though it worked correctly with the desktop browser's it failed with 
Android apps like Google play, ebay etc.

This is the equivalent of what I use and seems to work with everything I 
have tried so far.

ssl_bump peek step1
ssl_bump bump step2 !exclude_acl




From yvoinov at gmail.com  Sun Jun 21 13:20:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 21 Jun 2015 19:20:35 +0600
Subject: [squid-users] problem with some ssl services
In-Reply-To: <5586B0AD.70503@treenet.co.nz>
References: <1434316882187-4671733.post@n4.nabble.com>
 <557E153C.1030308@treenet.co.nz> <5581192F.9060109@trimble.com>
 <55822C37.7070105@treenet.co.nz> <1434887758720-4671820.post@n4.nabble.com>
 <5586B0AD.70503@treenet.co.nz>
Message-ID: <5586BA23.4080502@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
In other words, Amos, the new version is almost never be able to perform
a bump, I understand you correctly?

And there is no full configuration that will work in the same way as 3.4?

21.06.15 18:40, Amos Jeffries ?????:
> *some* issues that Squid detects as clearly meaning it can't bump the
> latest version(s) will auto-splice. Whether it does so for these
> particular apps depends on whether they are breaking because of those
> issues or something else.
>  Using the latest version will make your users experience better than
> older versions. But no guarantees about the problem you are looking into
> (its still a little vague what that is caused by).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVhroiAAoJENNXIZxhPexGlyYIAKqOiHVs+k5M7ic+QxbwQJCT
PLC7K1CBZerXYneH5JJA7yoS9b4VIq78o2ZyyKu2NRlWrF6o+ojSDOvL1dxPYF+T
0QRotRQW8Cxa07okOnekFVV3oRx8A3/daJH1xC94eQ1giNVF6G7+vWsv4h8PDEKY
W9WCh5YpylVOf10pCTssZnqqWA+A10eUnmjTlxndZLkvbpO0BC1135akhS1UWhyx
9fzJVBLlePAcWr/7mhALKXmdZoxSCLFPF0ZdcwVmnYxDFVhaOt1qhaXksA1jUg7F
WbTbSPuOgrz2yrTxSJHqSu4CX9DmLAsAjzCBt6uaXovxBQ+KCrUvR3DsGFG/IRE=
=QtAM
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150621/e846342b/attachment.htm>

From hack.back at hotmail.com  Sun Jun 21 18:21:01 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 11:21:01 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <554816F2.8050303@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz>
Message-ID: <1434910861109-4671827.post@n4.nabble.com>

used the latest squid 3.5.5 and still the same assertion error
where is the patch for this bug ??????????



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671827.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 22 00:40:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 12:40:16 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1434910861109-4671827.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
Message-ID: <55875970.7060200@treenet.co.nz>

On 22/06/2015 6:21 a.m., HackXBack wrote:
> used the latest squid 3.5.5 and still the same assertion error
> where is the patch for this bug ??????????

In bugzilla attached to the bug report. Awaiting somebody to confirm
that it actually works before a production release behaviour gets
changed in such a deep way.

The problem is that the patch requires design changes made in 3.5
series, and the people so far replicating the bug only used 3.4. If you
are replicating the bug in 3.5 and can confirm the patch works it will
be in the next release.

Amos



From hack.back at hotmail.com  Mon Jun 22 01:36:32 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 18:36:32 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55875970.7060200@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz>
Message-ID: <1434936992183-4671829.post@n4.nabble.com>

give me the link to the patch i will test it on 3.5.5 now 
thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671829.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Jun 22 01:39:45 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 18:39:45 -0700 (PDT)
Subject: [squid-users] squid 3.5.5 bug 3279
In-Reply-To: <558694EC.1080403@gmail.com>
References: <1434627721687-4671782.post@n4.nabble.com>
 <5582B8FF.5060608@treenet.co.nz> <1434699768366-4671789.post@n4.nabble.com>
 <5583E904.7040807@treenet.co.nz> <1434768908730-4671806.post@n4.nabble.com>
 <5584F8FB.20408@treenet.co.nz> <1434850077862-4671812.post@n4.nabble.com>
 <55863406.3020305@treenet.co.nz> <1434880664960-4671817.post@n4.nabble.com>
 <558694EC.1080403@gmail.com>
Message-ID: <1434937185255-4671830.post@n4.nabble.com>

This patch solve the problem, it can be used in next update.
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-5-bug-3279-tp4671781p4671830.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 22 02:08:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 14:08:54 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1434936992183-4671829.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
Message-ID: <55876E36.4050702@treenet.co.nz>

On 22/06/2015 1:36 p.m., HackXBack wrote:
> give me the link to the patch i will test it on 3.5.5 now 

<http://bugs.squid-cache.org/show_bug.cgi?id=3775>
<http://bugs.squid-cache.org/attachment.cgi?id=3145>

Amos



From hack.back at hotmail.com  Mon Jun 22 03:23:12 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 21 Jun 2015 20:23:12 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55876E36.4050702@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz>
Message-ID: <1434943392852-4671832.post@n4.nabble.com>

this patch didnt solve the problem :)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671832.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tomtux007 at gmail.com  Mon Jun 22 05:16:17 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 22 Jun 2015 07:16:17 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <5583E99D.3030306@treenet.co.nz>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
Message-ID: <CACLJR+NYn9+eUjjysHpgRHrjZw_xTWX7E5LJKycWCXuNXv_gzQ@mail.gmail.com>

Seems this is a well known problem....? Is there a patch available?

On Fri, Jun 19, 2015 at 12:06 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 19/06/2015 5:23 a.m., Tom Tom wrote:
>> Hi
>>
>> gdb shows the following:
>>
>>
>
>> #3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
>> file=0x8114cb "hash.cc", line=240,
>>     function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
>> hash_remove_link(hash_table*, hash_link*)") at assert.c:101
>> #4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
>> hl=0x204e060) at hash.cc:240
>> #5  0x0000000000685296 in Auth::User::cacheCleanup
>> (datanotused=<optimized out>) at User.cc:208
>
> Ah. The auth username cache again.
>
> Amos
>


From alex at samad.com.au  Mon Jun 22 05:28:17 2015
From: alex at samad.com.au (Alex Samad)
Date: Mon, 22 Jun 2015 15:28:17 +1000
Subject: [squid-users] Memory usage question
In-Reply-To: <55861A27.4050507@treenet.co.nz>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>
 <5583FA01.2020101@ngtech.co.il>
 <CAJ+Q1PVBCkbg-vhMfzVRyc0ZnxWs8W23F-fXSSC6UMXdKyzJsA@mail.gmail.com>
 <55861A27.4050507@treenet.co.nz>
Message-ID: <CAJ+Q1PXh15t1K17d6A+ozuQ=fvBjX6pY6Twq9mdvK6H8=xTpyg@mail.gmail.com>

Hi

UFS or AUFS ? guessing aufs

Any suggestions on the L1 L2  values, defaults ?


On 21 June 2015 at 11:57, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 20/06/2015 9:08 p.m., Alex Samad wrote:
>> Hi
>>
>> Are there any gotchas i need to look out for.
>> Also I have allocated a 1T lun to the VM. Whats the best way to
>> allocate this do I use 1 cache_dir or multiple cache_dir.
>
> The usual one UFS based dir per physical drive and no RAID. That can be
> tricky with SAN/NAS based disk and VMs.
>
>>
>> I currently have 3, is there a way to migrate the cache objects in the
>> 3 into 1 or do I just delete them and bear the cost of re downloading
>> them
>
> If you need to merge them you can set cache_dir to read-only for a
> period. That way Squid will use their content until it becomes too far
> out of date, whiel storing new objects into the witable cache_dir.
>
>
> It should not be a big problem/cost to drop the cache anyway. The
> bandwidth to rebuild a cache is far smaller than most people expect.
> Much of the content in a large cache is stale objects waiting
> revalidation or replacement, and all HITs are duplicates by definition.
> The cache fill rate is an exponential/polynomial growth curve with the
> bulk of it being a few seconds/minutes worth of traffic.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Jun 22 06:48:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 18:48:35 +1200
Subject: [squid-users] Memory usage question
In-Reply-To: <CAJ+Q1PXh15t1K17d6A+ozuQ=fvBjX6pY6Twq9mdvK6H8=xTpyg@mail.gmail.com>
References: <CAJ+Q1PXNBcDYGnXUZgFhKP1_o8xSHGyVfwa=A3_AC2=Xsaoy-Q@mail.gmail.com>	<5583FA01.2020101@ngtech.co.il>	<CAJ+Q1PVBCkbg-vhMfzVRyc0ZnxWs8W23F-fXSSC6UMXdKyzJsA@mail.gmail.com>	<55861A27.4050507@treenet.co.nz>
 <CAJ+Q1PXh15t1K17d6A+ozuQ=fvBjX6pY6Twq9mdvK6H8=xTpyg@mail.gmail.com>
Message-ID: <5587AFC3.4010409@treenet.co.nz>

On 22/06/2015 5:28 p.m., Alex Samad wrote:
> Hi
> 
> UFS or AUFS ? guessing aufs
> 

UFA, AUFS, diskd - all the same in this regard.

> Any suggestions on the L1 L2  values, defaults ?

Not particularly. Though others may have preferences. AFAIK the FS
directory index issues in ext2 that L1/L2 exist to resolve have not been
a problem in any OS for many years now.

Amos



From Jason_Haar at trimble.com  Mon Jun 22 07:24:49 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 22 Jun 2015 19:24:49 +1200
Subject: [squid-users] confused about ICAP and who's downloading what
In-Reply-To: <201506210045.11647.Antony.Stone@squid.open.source.it>
References: <5585E9D1.5080208@trimble.com>
 <201506210045.11647.Antony.Stone@squid.open.source.it>
Message-ID: <5587B841.5010501@trimble.com>

On 21/06/15 10:45, Antony Stone wrote:
> The former - squid does the download and passes the content to ICAP.

Great. So squid does all the network calls and ICAP simply gets to
review the content (request and/or response) and potentially change it.
Perfect :-)

Thanks!


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From squid3 at treenet.co.nz  Mon Jun 22 10:16:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2015 22:16:45 +1200
Subject: [squid-users] confused about ICAP and who's downloading what
In-Reply-To: <5587B841.5010501@trimble.com>
References: <5585E9D1.5080208@trimble.com>
 <201506210045.11647.Antony.Stone@squid.open.source.it>
 <5587B841.5010501@trimble.com>
Message-ID: <5587E08D.8030607@treenet.co.nz>

On 22/06/2015 7:24 p.m., Jason Haar wrote:
> On 21/06/15 10:45, Antony Stone wrote:
>> The former - squid does the download and passes the content to ICAP.
> 
> Great. So squid does all the network calls and ICAP simply gets to
> review the content (request and/or response) and potentially change it.
> Perfect :-)

Not quite. ICAP is a network service. So the ICAP server still has to do
all the parsing parts for both ICAP and HTTP, then the payload as well.

If you just want data passed to your code, go with eCAP module. Squid
loads it as a library and passes pre-parsed HTTP data through the eCAP
API to it. Then the only parsing necessary is the payload format(s).

Amos



From gcsquid-users at crowie.net  Mon Jun 22 11:56:18 2015
From: gcsquid-users at crowie.net (Graham)
Date: Mon, 22 Jun 2015 21:56:18 +1000
Subject: [squid-users] Squid to ask for, but not require, authentication.
In-Reply-To: <5586AF7B.30505@treenet.co.nz>
References: <5586AD19.4060505@crowie.net> <5586AF7B.30505@treenet.co.nz>
Message-ID: <5587F7E2.6080607@crowie.net>

Thanks for the reply.

I monitored the data between dansguardian and squid and there is 
basically nothing there. Dansguardian seems to only pass on the URL in 
its default config - and strips out everything else, including the 
User-Agent header.

I had a read and found that I can turn on the X-Forwarded-For headers in 
DansGuardian with the following two lines
forwardedfor = on
usexforwardedfor = on

I was then able to see the client's IP in the TCP stream between 
DansGuardian and Squid.

Then, in squid.conf, I added the following two lines (I suspect that the 
second one is not needed)
follow_x_forwarded_for allow localhost
acl_uses_indirect_client on

And then all ACL queries used the original client address, rather than 
the DansGuardian proxy address. This made all my rules work (and has 
enabled a few other things, such as logging in Squid that tells me the 
client's IP address correctly).

So, all is working now. Thank you for pointing me at the HTTP headers, 
it was exactly where I needed to look.


Thanks

GC



On 21/06/2015 10:35 PM, Amos Jeffries wrote:
> On 22/06/2015 12:24 a.m., Graham wrote:
>> I am looking for a way to configure Squid to ask for (and check)
>> authentication using LDAP, but to proceed if there is no auth
>> information provided.
> Not possible. The process of asking for auth sends a reply to the client
> request. Once that happens there is nothing further possible.
>
> You can check for auth and continue if its missing, but when doing so
> cannot ask the client to send any credentials. A secure client will not
> send credentials unless asked...
>
>> I have been using DansGuardian for a while with Squid authenticating and
>> then getting DansGuardian to filter based on the username that Squid has
>> authenticated. The browsers talk directly to DansGuardian, which talks
>> to Squid, which does the work over the 'net.
>>
>> I am now trying to add an android device - which has some apps that
>> don't ask the user for a login/password (although they do talk to the
>> proxy) and therefore they fail to connect with a 407 error. I have
>> modified DansGuardian to allow just this one IP to work without
>> authentication, but Squid requires the auth and denies the requests. If
>> I make Squid more permissive (remove the auth config) then DansGuardian
>> works with that IP address, but will then block all other IP addresses
>> as Squid hasn't authenticated anyone. Note that I can't do IP
>> authentication from Squid because all requests come from the
>> DansGuardian IP (which happens to be localhost) and it can't tell which
>> ones to authenticate and which to allow.
> You should be able to use something like the User-Agent header
> ("browser" regex ACL type) to bypass the auth requirement on a
> per-request basis. This has to be done for many Java applications, for
> example.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From michael.pelletier at palmbeachschools.org  Mon Jun 22 15:48:20 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Mon, 22 Jun 2015 11:48:20 -0400
Subject: [squid-users] How can I change the location of the kerberos cache
	file?
Message-ID: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>

Hello,

Squid is keeping the kerberos cache file in /var/tmp. How can I change the
location?

# ls -al /var/tmp/
total 864
drwxrwxrwt.  3 root  root   36864 Jun 22 11:43 .
drwxr-xr-x. 22 root  root    4096 May  9 23:55 ..
-rw-r--r--   1 root  root       0 Jun 21 20:09 .fsrlast_xfs
drwx------.  2 root  root   16384 May  9 19:01 lost+found
-rw-------   1 squid squid 823779 Jun 22 11:43
SVC-137Proxy-137Kerb-137Auth_23

Thanks in advance,
Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150622/7b75ffe8/attachment.htm>

From mmonette at 2keys.ca  Mon Jun 22 16:59:03 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Mon, 22 Jun 2015 12:59:03 -0400 (EDT)
Subject: [squid-users] How can I change the location of the kerberos
 cache	file?
In-Reply-To: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
References: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
Message-ID: <1109778448.2136630.1434992343137.JavaMail.zimbra@2keys.ca>

Have you tried a symlink?  

I know it's not the best answer, but could work until you figure out a real solution

----- Original Message -----
From: "Michael Pelletier" <michael.pelletier at palmbeachschools.org>
To: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Monday, June 22, 2015 11:48:20 AM
Subject: [squid-users] How can I change the location of the kerberos cache	file?

Hello, 

Squid is keeping the kerberos cache file in /var/tmp. How can I change the location? 

# ls -al /var/tmp/ 
total 864 
drwxrwxrwt. 3 root root 36864 Jun 22 11:43 . 
drwxr-xr-x. 22 root root 4096 May 9 23:55 .. 
-rw-r--r-- 1 root root 0 Jun 21 20:09 .fsrlast_xfs 
drwx------. 2 root root 16384 May 9 19:01 lost+found 
-rw------- 1 squid squid 823779 Jun 22 11:43 SVC-137Proxy-137Kerb-137Auth_23 

Thanks in advance, 
Michael 








Disclaimer: Under Florida law, e-mail addresses are public records. If you do not want your e-mail address released in response to a public records request, do not send electronic mail to this entity. Instead, contact this office by phone or in writing. 








_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From michael.pelletier at palmbeachschools.org  Mon Jun 22 17:46:12 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Mon, 22 Jun 2015 13:46:12 -0400
Subject: [squid-users] How can I change the location of the kerberos
	cache file?
In-Reply-To: <1109778448.2136630.1434992343137.JavaMail.zimbra@2keys.ca>
References: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
 <1109778448.2136630.1434992343137.JavaMail.zimbra@2keys.ca>
Message-ID: <CAEnCSG7Yi6MpN4BNgOkj_y6mguDZAp+6ZrBTW9nL++3-LU3k9w@mail.gmail.com>

Not a bad temp solution. The file name seems to be static as I check
multiple servers and it was all the same.

On Mon, Jun 22, 2015 at 12:59 PM, Michael Monette <mmonette at 2keys.ca> wrote:

> Have you tried a symlink?
>
> I know it's not the best answer, but could work until you figure out a
> real solution
>
> ----- Original Message -----
> From: "Michael Pelletier" <michael.pelletier at palmbeachschools.org>
> To: "squid-users" <squid-users at lists.squid-cache.org>
> Sent: Monday, June 22, 2015 11:48:20 AM
> Subject: [squid-users] How can I change the location of the kerberos
> cache      file?
>
> Hello,
>
> Squid is keeping the kerberos cache file in /var/tmp. How can I change the
> location?
>
> # ls -al /var/tmp/
> total 864
> drwxrwxrwt. 3 root root 36864 Jun 22 11:43 .
> drwxr-xr-x. 22 root root 4096 May 9 23:55 ..
> -rw-r--r-- 1 root root 0 Jun 21 20:09 .fsrlast_xfs
> drwx------. 2 root root 16384 May 9 19:01 lost+found
> -rw------- 1 squid squid 823779 Jun 22 11:43
> SVC-137Proxy-137Kerb-137Auth_23
>
> Thanks in advance,
> Michael
>
>
>
>
>
>
>
>
> Disclaimer: Under Florida law, e-mail addresses are public records. If you
> do not want your e-mail address released in response to a public records
> request, do not send electronic mail to this entity. Instead, contact this
> office by phone or in writing.
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150622/7dcb81d9/attachment.htm>

From tomtux007 at gmail.com  Mon Jun 22 17:54:32 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Mon, 22 Jun 2015 19:54:32 +0200
Subject: [squid-users] How can I change the location of the kerberos
	cache file?
In-Reply-To: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
References: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
Message-ID: <CACLJR+O-y-CGuucb_mt0T0ZbtpjxbiKu5C_ZBkex7mn4pKkK-Q@mail.gmail.com>

You can export the variable KRB5CCNAME, where you can specify the
kerberos cache file location.
For example: $ export KRB5CCNAME=/home/testuser/krb5_cache_file_$(id -u)

Regards,
Tom

On Mon, Jun 22, 2015 at 5:48 PM, Michael Pelletier
<michael.pelletier at palmbeachschools.org> wrote:
> Hello,
>
> Squid is keeping the kerberos cache file in /var/tmp. How can I change the
> location?
>
> # ls -al /var/tmp/
> total 864
> drwxrwxrwt.  3 root  root   36864 Jun 22 11:43 .
> drwxr-xr-x. 22 root  root    4096 May  9 23:55 ..
> -rw-r--r--   1 root  root       0 Jun 21 20:09 .fsrlast_xfs
> drwx------.  2 root  root   16384 May  9 19:01 lost+found
> -rw-------   1 squid squid 823779 Jun 22 11:43
> SVC-137Proxy-137Kerb-137Auth_23
>
> Thanks in advance,
> Michael
>
> Disclaimer: Under Florida law, e-mail addresses are public records. If you
> do not want your e-mail address released in response to a public records
> request, do not send electronic mail to this entity. Instead, contact this
> office by phone or in writing.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From michael.pelletier at palmbeachschools.org  Mon Jun 22 18:27:10 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Mon, 22 Jun 2015 14:27:10 -0400
Subject: [squid-users] How can I change the location of the kerberos
	cache file?
In-Reply-To: <CACLJR+O-y-CGuucb_mt0T0ZbtpjxbiKu5C_ZBkex7mn4pKkK-Q@mail.gmail.com>
References: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
 <CACLJR+O-y-CGuucb_mt0T0ZbtpjxbiKu5C_ZBkex7mn4pKkK-Q@mail.gmail.com>
Message-ID: <CAEnCSG7ZhizOP0wcP9UGQR3WTiLBw9rcSX5nPmpNmGxkktefdg@mail.gmail.com>

It did not work. I exported the variable and started squid but it still
used the old file....:-(

On Mon, Jun 22, 2015 at 1:54 PM, Tom Tom <tomtux007 at gmail.com> wrote:

> You can export the variable KRB5CCNAME, where you can specify the
> kerberos cache file location.
> For example: $ export KRB5CCNAME=/home/testuser/krb5_cache_file_$(id -u)
>
> Regards,
> Tom
>
> On Mon, Jun 22, 2015 at 5:48 PM, Michael Pelletier
> <michael.pelletier at palmbeachschools.org> wrote:
> > Hello,
> >
> > Squid is keeping the kerberos cache file in /var/tmp. How can I change
> the
> > location?
> >
> > # ls -al /var/tmp/
> > total 864
> > drwxrwxrwt.  3 root  root   36864 Jun 22 11:43 .
> > drwxr-xr-x. 22 root  root    4096 May  9 23:55 ..
> > -rw-r--r--   1 root  root       0 Jun 21 20:09 .fsrlast_xfs
> > drwx------.  2 root  root   16384 May  9 19:01 lost+found
> > -rw-------   1 squid squid 823779 Jun 22 11:43
> > SVC-137Proxy-137Kerb-137Auth_23
> >
> > Thanks in advance,
> > Michael
> >
> > Disclaimer: Under Florida law, e-mail addresses are public records. If
> you
> > do not want your e-mail address released in response to a public records
> > request, do not send electronic mail to this entity. Instead, contact
> this
> > office by phone or in writing.
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150622/a4da710d/attachment.htm>

From al_luhaybi at yahoo.com  Mon Jun 22 20:11:18 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Mon, 22 Jun 2015 13:11:18 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <1434874916708-4671816.post@n4.nabble.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
Message-ID: <1435003878979-4671844.post@n4.nabble.com>

why is no-one answering this ?!!

BTW, i tried the kernel patch 2.6.35 from ZPH, it worked intermittently, and
stopped working after a squid re-build.

any help is appreciated



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671844.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From leolistas at solutti.com.br  Tue Jun 23 00:50:06 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 22 Jun 2015 21:50:06 -0300
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <1435003878979-4671844.post@n4.nabble.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
Message-ID: <5588AD3E.7000605@solutti.com.br>

Em 22/06/15 17:11, mohammad escreveu:
> why is no-one answering this ?!!
>
> BTW, i tried the kernel patch 2.6.35 from ZPH, it worked intermittently, and
> stopped working after a squid re-build.
>
> any help is appreciated
>
>

     This list is community-based, there's no guarantee someone will 
answer and be able to help you. Everybody does its best, but there's no 
guarantee at all.

     If you really need someone to help solving your problems, there's 
lots of companies/partners that can offer commercial support on squid:

http://www.squid-cache.org/Support/services.html



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From dan at getbusi.com  Tue Jun 23 03:19:06 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 23 Jun 2015 13:19:06 +1000
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <1435003878979-4671844.post@n4.nabble.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
Message-ID: <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>

It's also worth pointing out that your messages are getting flagged as Spam
by Gmail, which probably isn't helping visibility.

On 23 June 2015 at 06:11, mohammad <al_luhaybi at yahoo.com> wrote:

> why is no-one answering this ?!!
>
> BTW, i tried the kernel patch 2.6.35 from ZPH, it worked intermittently,
> and
> stopped working after a squid re-build.
>
> any help is appreciated
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671844.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150623/c696c479/attachment.htm>

From tomtux007 at gmail.com  Tue Jun 23 04:39:55 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Tue, 23 Jun 2015 06:39:55 +0200
Subject: [squid-users] How can I change the location of the kerberos
	cache file?
In-Reply-To: <CAEnCSG7ZhizOP0wcP9UGQR3WTiLBw9rcSX5nPmpNmGxkktefdg@mail.gmail.com>
References: <CAEnCSG7g_-wJ9+LpPtz6nxu17UqRuf1edf03ZJ5qdGYphhAdsA@mail.gmail.com>
 <CACLJR+O-y-CGuucb_mt0T0ZbtpjxbiKu5C_ZBkex7mn4pKkK-Q@mail.gmail.com>
 <CAEnCSG7ZhizOP0wcP9UGQR3WTiLBw9rcSX5nPmpNmGxkktefdg@mail.gmail.com>
Message-ID: <CACLJR+PQE_-3P4s_1Rvz_SE+CakvVvanY+oimaA2M-qSK6h77g@mail.gmail.com>

OK, I think you mean the RCACHE.
You can change the location with exporting the
KRB5RCACHETYPE-variable. Have a look here:
http://web.mit.edu/kerberos/krb5-1.11/doc/basic/rcache_def.html

For disabling it, you can:
$ export KRB5RCACHETYPE=none


Regards,
Tom


On Mon, Jun 22, 2015 at 8:27 PM, Michael Pelletier
<michael.pelletier at palmbeachschools.org> wrote:
> It did not work. I exported the variable and started squid but it still used
> the old file....:-(
>
> On Mon, Jun 22, 2015 at 1:54 PM, Tom Tom <tomtux007 at gmail.com> wrote:
>>
>> You can export the variable KRB5CCNAME, where you can specify the
>> kerberos cache file location.
>> For example: $ export KRB5CCNAME=/home/testuser/krb5_cache_file_$(id -u)
>>
>> Regards,
>> Tom
>>
>> On Mon, Jun 22, 2015 at 5:48 PM, Michael Pelletier
>> <michael.pelletier at palmbeachschools.org> wrote:
>> > Hello,
>> >
>> > Squid is keeping the kerberos cache file in /var/tmp. How can I change
>> > the
>> > location?
>> >
>> > # ls -al /var/tmp/
>> > total 864
>> > drwxrwxrwt.  3 root  root   36864 Jun 22 11:43 .
>> > drwxr-xr-x. 22 root  root    4096 May  9 23:55 ..
>> > -rw-r--r--   1 root  root       0 Jun 21 20:09 .fsrlast_xfs
>> > drwx------.  2 root  root   16384 May  9 19:01 lost+found
>> > -rw-------   1 squid squid 823779 Jun 22 11:43
>> > SVC-137Proxy-137Kerb-137Auth_23
>> >
>> > Thanks in advance,
>> > Michael
>> >
>> > Disclaimer: Under Florida law, e-mail addresses are public records. If
>> > you
>> > do not want your e-mail address released in response to a public records
>> > request, do not send electronic mail to this entity. Instead, contact
>> > this
>> > office by phone or in writing.
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>
>
>
> Disclaimer: Under Florida law, e-mail addresses are public records. If you
> do not want your e-mail address released in response to a public records
> request, do not send electronic mail to this entity. Instead, contact this
> office by phone or in writing.


From itpc.vivek at gmail.com  Tue Jun 23 07:04:45 2015
From: itpc.vivek at gmail.com (vivek singh)
Date: Tue, 23 Jun 2015 12:34:45 +0530
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
 <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
Message-ID: <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>

what is ZPH

Vivek Kumar Singh
J.T.O./ITPC-KOL
Mobile     08902000538
Landline  033-23211548


On Tue, Jun 23, 2015 at 8:49 AM, Dan Charlesworth <dan at getbusi.com> wrote:

> It's also worth pointing out that your messages are getting flagged as
> Spam by Gmail, which probably isn't helping visibility.
>
> On 23 June 2015 at 06:11, mohammad <al_luhaybi at yahoo.com> wrote:
>
>> why is no-one answering this ?!!
>>
>> BTW, i tried the kernel patch 2.6.35 from ZPH, it worked intermittently,
>> and
>> stopped working after a squid re-build.
>>
>> any help is appreciated
>>
>>
>>
>> --
>> View this message in context:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671844.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150623/c817a622/attachment.htm>

From kl at vsen.dk  Tue Jun 23 07:11:26 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 23 Jun 2015 09:11:26 +0200
Subject: [squid-users] Quick peek-splice clarification
In-Reply-To: <1433981796.4674.2.camel@JamesiMac>
References: <1433981796.4674.2.camel@JamesiMac>
Message-ID: <5589069E.90406@vsen.dk>

Hi James,

Did you ever find an answer for this?

James Lay wrote on 06/11/2015 02:16 AM:
> All,
>
>  From the docs at:
>
> http://wiki.squid-cache.org/Features/SslPeekAndSplice
>
> *peek*
>
>
> 	step1, step2
>
>
> 	Receive SNI and client certificate (step1), or server certificate
> (step2) while preserving the possibility of splicing the connection.
> Peeking at the server certificate usually precludes future bumping of
> the connection (see Limitations). This action is the focus of this project.
>
>
> *stare*
>
>
> 	step1, step2
>
>
> 	Receive SNI and client certificate (step1), or server certificate
> (step2) while preserving the possibility of bumping the connection.
> Staring at the server certificate usually precludes future splicing of
> the connection. Currently, we are not aware of any work being done to
> support this action.
>
>
>
> I see a lot of:
>
> ssl_bump peek all
>
> Does this perform both step1 with SNI and client cert, AND server cert?
> Thank you.
>
> James
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Jun 23 07:45:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Jun 2015 19:45:58 +1200
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
 <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
 <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>
Message-ID: <55890EB6.1050609@treenet.co.nz>

On 23/06/2015 7:04 p.m., vivek singh wrote:
> what is ZPH
> 

This explains <http://zph.bratcheda.org/>

The squid parts were added to Squid-3.x as the qos_flows directive.

BUT, copying a TOS flag from one TCP connection to another only works if
one is using HTTP/1.0 non-persistent connections (HTTP/1.1 pipeline and
persistence make outbound have incorrect TOS values), has a patched
Linux kernel (no other OS support it, nor Linux newer than 2.6.35
apparently).



> Vivek Kumar Singh
> J.T.O./ITPC-KOL
> Mobile     08902000538
> Landline  033-23211548
> 
> 
> On Tue, Jun 23, 2015 at 8:49 AM, Dan Charlesworth wrote:
> 
>> It's also worth pointing out that your messages are getting flagged as
>> Spam by Gmail, which probably isn't helping visibility.
>>
>> On 23 June 2015 at 06:11, mohammad wrote:
>>
>>> why is no-one answering this ?!!
>>>
>>> BTW, i tried the kernel patch 2.6.35 from ZPH, it worked intermittently,
>>> and
>>> stopped working after a squid re-build.

AFAIK, thats the best anyone has had TOS preserve-miss working since
Linux 3.0 came out.

The last attempt I helped with (sometime way back around 2012/2013) had
to translate the TOS value on client connection into a netfilter MASK
value, then use the MASK preserve-miss, and translate from MASk to TOS
on the server connection. Good luck with that though success was also
somewhat intermittent.

As mentioned above HTTP/1.1 high performance features can break it. Even
normal HTTP operations such as revalidation can cause problems because
HTTP is stateless - the inbound and outbound TCP connections are *not
1:1 related*.

Amos


From al_luhaybi at yahoo.com  Tue Jun 23 08:18:44 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Tue, 23 Jun 2015 01:18:44 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <55890EB6.1050609@treenet.co.nz>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
 <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
 <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>
 <55890EB6.1050609@treenet.co.nz>
Message-ID: <1435047524657-4671851.post@n4.nabble.com>

Dear Amos,

thank you for replying; 

if I understand correctly from your reply; this can not be achieved by TOS.

how about Marking and iptables, squid documentation refers to connmark, not
mark.

so, let's say if I mark the packet using iptables, on the INPUT, of the
squid; will it retain it to the output; in which i can use another iptables
rule to swap the mark back to TOS value; so that further network elements
can process the DSCP tags.

thank you for again.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671851.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From al_luhaybi at yahoo.com  Tue Jun 23 08:37:04 2015
From: al_luhaybi at yahoo.com (mohammad)
Date: Tue, 23 Jun 2015 01:37:04 -0700 (PDT)
Subject: [squid-users] tos miss-mask not working at all squid 3.5.5
In-Reply-To: <55890EB6.1050609@treenet.co.nz>
References: <1434874739081-4671815.post@n4.nabble.com>
 <1434874916708-4671816.post@n4.nabble.com>
 <1435003878979-4671844.post@n4.nabble.com>
 <CAN8nrKD-iF8fOryhOftGLLX_7EPTrjsGi=+9BfjS568i6M8Uzw@mail.gmail.com>
 <CACeaud=8WdjfcG0d=jz0ineADoOQho6sYEkyK+c=8CpOxtWoXQ@mail.gmail.com>
 <55890EB6.1050609@treenet.co.nz>
Message-ID: <1435048624774-4671852.post@n4.nabble.com>

dear Amos,

I didn't read the AFAIK part of your post just after replying, i guess i'm
busy trying to make this work ;-).

so, if I understand correctly, even marking will have same problems. !!
strange.

this is strange.

this requirement to start; was issued due to the fact, that the parent
squid; using a wrerite program, is returning a hit, and serving files from
local cache.
however, the child squid, keeps seeing it as a miss, i guess it's due to the
rewrite limitation.

meaning, qos_flows tos parent-hit-0xXX at child, never works !!

i've tried both ICP and HTCP, and all same issue, parent returns hit, child
see's it as a miss; and skip the command above.


is there a way, to make the child squid; to actually, depend on the HIT
returned from the parent, and actually makes the parent-hit command works ?

thank you for your time.






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/tos-miss-mask-not-working-at-all-squid-3-5-5-tp4671815p4671852.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jlay at slave-tothe-box.net  Tue Jun 23 11:16:57 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Tue, 23 Jun 2015 05:16:57 -0600
Subject: [squid-users] Quick peek-splice clarification
In-Reply-To: <5589069E.90406@vsen.dk>
References: <1433981796.4674.2.camel@JamesiMac> <5589069E.90406@vsen.dk>
Message-ID: <1435058217.3652.3.camel@JamesiMac>

On Tue, 2015-06-23 at 09:11 +0200, Klavs Klavsen wrote:

> Hi James,
> 
> Did you ever find an answer for this?
> 
> James Lay wrote on 06/11/2015 02:16 AM:
> > All,
> >
> >  From the docs at:
> >
> > http://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> > *peek*
> >
> >
> > 	step1, step2
> >
> >
> > 	Receive SNI and client certificate (step1), or server certificate
> > (step2) while preserving the possibility of splicing the connection.
> > Peeking at the server certificate usually precludes future bumping of
> > the connection (see Limitations). This action is the focus of this project.
> >
> >
> > *stare*
> >
> >
> > 	step1, step2
> >
> >
> > 	Receive SNI and client certificate (step1), or server certificate
> > (step2) while preserving the possibility of bumping the connection.
> > Staring at the server certificate usually precludes future splicing of
> > the connection. Currently, we are not aware of any work being done to
> > support this action.
> >
> >
> >
> > I see a lot of:
> >
> > ssl_bump peek all
> >
> > Does this perform both step1 with SNI and client cert, AND server cert?
> > Thank you.
> >
> > James
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> 
> 


Hi Klavs,

I did not.  I can tell you in my testing that:

ssl_bump peek step1 all
ssl_bump peek step2 all

versus

ssl_bump peek all

Did not give me the same results, so I'm going to assume a single
statement only performs SNI lookup, but maybe someone else on the list
has a better answer.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150623/bc14087a/attachment.htm>

From canschuetz at besitec.com  Tue Jun 23 12:32:56 2015
From: canschuetz at besitec.com (sqca)
Date: Tue, 23 Jun 2015 05:32:56 -0700 (PDT)
Subject: [squid-users] Reverse Proxy translate public domain to internal path
Message-ID: <1435062776011-4671854.post@n4.nabble.com>

Hi folks,

I have started to use Squid 3.5.5 to implement a reverse proxy for multiple
webservers. Some of them are publishing multiple websites on the same port
so I need to do the following:
Publish "site1.example.com" via Squid which points at 192.168.0.1:8080/test
Publish "site.example.com" via Squid which points at
192.168.0.1:8080/production
and so on.

What do I need to do to accomplish this? Is it really necessary to use the
url_rewrite_program directive?

Thank you all for your input.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-Proxy-translate-public-domain-to-internal-path-tp4671854.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 23 13:28:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jun 2015 01:28:41 +1200
Subject: [squid-users] Reverse Proxy translate public domain to internal
 path
In-Reply-To: <1435062776011-4671854.post@n4.nabble.com>
References: <1435062776011-4671854.post@n4.nabble.com>
Message-ID: <55895F09.5040809@treenet.co.nz>

On 24/06/2015 12:32 a.m., sqca wrote:
> Hi folks,
> 
> I have started to use Squid 3.5.5 to implement a reverse proxy for multiple
> webservers. Some of them are publishing multiple websites on the same port
> so I need to do the following:
> Publish "site1.example.com" via Squid which points at 192.168.0.1:8080/test
> Publish "site.example.com" via Squid which points at
> 192.168.0.1:8080/production
> and so on.
> 
> What do I need to do to accomplish this? Is it really necessary to use the
> url_rewrite_program directive?

For this. Yes. The rewrite (*not* redirect) is necessary to change the
path segment. The rest is normal for virtual hosting.

However, its really a bad idea to do it this way (with /path
differences) in the first place.

Ideally you have the server doing virtual hosting for the two domain
names and using port 80 as well. Squid then just passes the requests
traffic to the right server and the server itself does not then have to
care about getting port and path etc right in every URL reference the
cotent contains, just uses relative URLs. A site like that can even be
proxied for HTTPS quite easily.

Amos



From canschuetz at besitec.com  Tue Jun 23 14:17:50 2015
From: canschuetz at besitec.com (sqca)
Date: Tue, 23 Jun 2015 07:17:50 -0700 (PDT)
Subject: [squid-users] Reverse Proxy translate public domain to internal
	path
In-Reply-To: <55895F09.5040809@treenet.co.nz>
References: <1435062776011-4671854.post@n4.nabble.com>
 <55895F09.5040809@treenet.co.nz>
Message-ID: <1435069070036-4671856.post@n4.nabble.com>

Hi Amos,

thanks for the information. Do I need to rewrite all requests from
"site1.example.com" to "site1.example.com/test" in this case?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-Proxy-translate-public-domain-to-internal-path-tp4671854p4671856.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sebag at vianetcon.com.ar  Tue Jun 23 15:10:45 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Tue, 23 Jun 2015 12:10:45 -0300
Subject: [squid-users] Time out where? TCP_MISS_TIMEDOUT
Message-ID: <558976F5.8060104@vianetcon.com.ar>

I've found several of these in my access.log

1435009516.011 899906 10.60.3.221 TCP_MISS_TIMEDOUT/200 8790 GET 
http://t4.kn3.net/taringa/7/5/4/5/0/5/blackz89/236x177_1F2.jpg - 
ORIGINAL_DST/104.18.42.237 image/jpeg
1435009516.011 899840 10.63.6.215 TCP_MISS_TIMEDOUT/200 8742 GET 
http://pagead2.googlesyndication.com/pagead/imgad?id=CICAgKDT-NKwfhCsAhjIATIIcTeEVWIn93c 
- ORIGINAL_DST/173.194.42.26 application/x-javascript


I don't quite understand it. Which connection is timing out? Squid to 
webserver? Squid to client?
Couldn't find much on Google


Thanks,
Sebastian


From tomtux007 at gmail.com  Tue Jun 23 17:33:35 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Tue, 23 Jun 2015 19:33:35 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <CACLJR+NYn9+eUjjysHpgRHrjZw_xTWX7E5LJKycWCXuNXv_gzQ@mail.gmail.com>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
 <CACLJR+NYn9+eUjjysHpgRHrjZw_xTWX7E5LJKycWCXuNXv_gzQ@mail.gmail.com>
Message-ID: <CACLJR+O4KbgJhphT8-VKfd6F3ajL2AafjsVfF5LO2B7=1a=jSA@mail.gmail.com>

...or something else I can configure to prevent restarting after every 2h?

Thanks.
Tom

On Mon, Jun 22, 2015 at 7:16 AM, Tom Tom <tomtux007 at gmail.com> wrote:
> Seems this is a well known problem....? Is there a patch available?
>
> On Fri, Jun 19, 2015 at 12:06 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 19/06/2015 5:23 a.m., Tom Tom wrote:
>>> Hi
>>>
>>> gdb shows the following:
>>>
>>>
>>
>>> #3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
>>> file=0x8114cb "hash.cc", line=240,
>>>     function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
>>> hash_remove_link(hash_table*, hash_link*)") at assert.c:101
>>> #4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
>>> hl=0x204e060) at hash.cc:240
>>> #5  0x0000000000685296 in Auth::User::cacheCleanup
>>> (datanotused=<optimized out>) at User.cc:208
>>
>> Ah. The auth username cache again.
>>
>> Amos
>>


From jaumshock at gmail.com  Tue Jun 23 21:42:07 2015
From: jaumshock at gmail.com (Joao Paulo Monticelli Gaspar)
Date: Tue, 23 Jun 2015 18:42:07 -0300
Subject: [squid-users] Doubt about private ip "leaking"
Message-ID: <CAFjXhxntzj4MoA4iwyrbJE_MqoOfq2cBYD-g6toQQQ4Keq6AUw@mail.gmail.com>

Hello guys,

Usually I check open ports using a site http://canyouseeme.org when I'm on
a client and need to do some kind of NAT etc.

On one of my clients I'm running a SQUID v3.1.10 as a transparent proxy,
and when I open the page the browser shows my private IP, after a little
test, I disabled the proxy and made the same test and the site started
showing my WAN IP as it should. Is that somekind of bug or misconfiguration
on my squid.conf?

-- 
Att,


Jo?o Paulo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150623/be5d4c34/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 23 21:52:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jun 2015 09:52:43 +1200
Subject: [squid-users] Doubt about private ip "leaking"
In-Reply-To: <CAFjXhxntzj4MoA4iwyrbJE_MqoOfq2cBYD-g6toQQQ4Keq6AUw@mail.gmail.com>
References: <CAFjXhxntzj4MoA4iwyrbJE_MqoOfq2cBYD-g6toQQQ4Keq6AUw@mail.gmail.com>
Message-ID: <5589D52B.7060109@treenet.co.nz>

On 24/06/2015 9:42 a.m., Joao Paulo Monticelli Gaspar wrote:
> Hello guys,
> 
> Usually I check open ports using a site http://canyouseeme.org when I'm on
> a client and need to do some kind of NAT etc.
> 
> On one of my clients I'm running a SQUID v3.1.10 as a transparent proxy,
> and when I open the page the browser shows my private IP, after a little
> test, I disabled the proxy and made the same test and the site started
> showing my WAN IP as it should. Is that somekind of bug or misconfiguration
> on my squid.conf?

Nope. Its what HTTP is designed to do. Doubtless your client is
contacting Squid with its internal IP, and that is the one Squid is
proxying for.

Amos



From squid3 at treenet.co.nz  Tue Jun 23 21:57:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jun 2015 09:57:36 +1200
Subject: [squid-users] Time out where? TCP_MISS_TIMEDOUT
In-Reply-To: <558976F5.8060104@vianetcon.com.ar>
References: <558976F5.8060104@vianetcon.com.ar>
Message-ID: <5589D650.3060408@treenet.co.nz>

On 24/06/2015 3:10 a.m., Sebastian Goicochea wrote:
> I've found several of these in my access.log
> 
> 1435009516.011 899906 10.60.3.221 TCP_MISS_TIMEDOUT/200 8790 GET
> http://t4.kn3.net/taringa/7/5/4/5/0/5/blackz89/236x177_1F2.jpg -
> ORIGINAL_DST/104.18.42.237 image/jpeg
> 1435009516.011 899840 10.63.6.215 TCP_MISS_TIMEDOUT/200 8742 GET
> http://pagead2.googlesyndication.com/pagead/imgad?id=CICAgKDT-NKwfhCsAhjIATIIcTeEVWIn93c
> - ORIGINAL_DST/173.194.42.26 application/x-javascript
> 
> 
> I don't quite understand it. Which connection is timing out? Squid to
> webserver? Squid to client?

The request took ~15 minutes to transfer less than 9KB of data.
I guess its <http://www.squid-cache.org/Doc/config/read_timeout/>
(waiting on server to present more data) or
<http://www.squid-cache.org/Doc/config/write_timeout/> (waiting for
client to ack the data relayed).

redbot.org tells me that first one / JPG is 39KB big.

Amos



From mcsnv96 at afo.net  Tue Jun 23 23:03:28 2015
From: mcsnv96 at afo.net (Mike)
Date: Tue, 23 Jun 2015 18:03:28 -0500
Subject: [squid-users] acl for redirect
Message-ID: <5589E5C0.4070307@afo.net>

We have a server setup using squid 3.5 and e2guardian (newer branch of 
dansguardian), the issue is now google has changed a few things around 
and google is no longer filtered which is not acceptable. We already 
have the browser settings for SSL Proxy set to our server, and squid has 
ssl-bump enabled and working. Previously there was enough unsecure 
content on Google that the filtering was still working, but now google 
has gone 100% encrypted meaning it is 100% unfiltered. What is happening 
is it is creating an ssl tunnel (for lack of a better term) between 
their server and the browser, so all squid sees is the connection to 
www.google.com, and after that it is tunneled and not recognized by 
squid or e2guardian at all.

I found a few options online that was used with older squid versions but 
nothing is working with squid 3.5... Looking for something like this:

acl google dstdomain .google.com
deny_info http://www.google.com/webhp?nord=1 google
http_access deny google

Essentially want to have squid take all regular requests for google.com 
and send/relay it to the unsecured page at 
http://www.google.com/webhp?nord=1 which allows e2guardian to properly 
filter. With the current settings though, it goes to the squid access 
denied page.

Mike


From hack.back at hotmail.com  Wed Jun 24 02:24:52 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 23 Jun 2015 19:24:52 -0700 (PDT)
Subject: [squid-users] TCP_MISS/503
Message-ID: <1435112692501-4671863.post@n4.nabble.com>

some times http pages give squid error page
in access.log i see TCP_MISS/503
what should be the problem?
i checked iptables and squid.conf but seems every thing look fine ..!!
thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-503-tp4671863.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Wed Jun 24 02:25:13 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 23 Jun 2015 19:25:13 -0700 (PDT)
Subject: [squid-users] TCP_MISS/503
In-Reply-To: <1435112692501-4671863.post@n4.nabble.com>
References: <1435112692501-4671863.post@n4.nabble.com>
Message-ID: <1435112713523-4671864.post@n4.nabble.com>

The requested URL could not be retrieved



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-503-tp4671863p4671864.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From reet.vyas28 at gmail.com  Wed Jun 24 06:04:37 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Wed, 24 Jun 2015 11:34:37 +0530
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV8==pthmX4X4r3RZ9JTjSCyaTv1ZJUp0adr4cJnaR6kDA@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
 <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
 <55705009.9080200@treenet.co.nz>
 <CAA8ViV8==pthmX4X4r3RZ9JTjSCyaTv1ZJUp0adr4cJnaR6kDA@mail.gmail.com>
Message-ID: <CAA8ViV_oHsmmtCO_5hogrsHtZJQSTZ4w6+e0LwCHz2LyaPeqhQ@mail.gmail.com>

Hi
 Below is my squid file , I have configured squid 3.5.3 with ssl, but I
cant filter https traffic and also in access log I cant see https in access
logs.


#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 116.72.152.37 192.168.0.0/24 # Sesuaikan dengan ip
client/local

acl SSL_ports port 443
acl Safe_ports port 80  # http
acl Safe_ports port 21  # ftp
acl Safe_ports port 443  # https
acl Safe_ports port 70  # gopher
acl Safe_ports port 210  # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280  # http-mgmt
acl Safe_ports port 488  # gss-http
acl Safe_ports port 591  # filemaker
acl Safe_ports port 777  # multiling http
# storeid *test*
acl urlrewrite dstdomain .fbcdn.net .akamaihd.net
acl speedtest url_regex -i speedtest\/.*\.(jpg|txt)\?.*
acl reverbnation url_regex -i reverbnation.*audio_player.*ec_stream_song.*$
acl utmgif url_regex -i utm.gif.*
acl playstoreandroid url_regex -i
c.android.clients.google.com.market.GetBinary.GetBinary.*
acl idyoutube url_regex -i
youtube.*(ptracking|stream_204|player_204).*(v\=|docid\=|video_id\=).*$
acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
acl CONNECT method CONNECT
acl getmethod method GET
acl loop_302 http_status 302
acl step1 at_step SslBump1
acl youtube dstdomain .youtube.com
acl blocksites dstdomain "/etc/squid/restricted-sites.squid"
# TAG: QUERY
#
-----------------------------------------------------------------------------
acl QUERY urlpath_regex -i
(hackshield|blank.html|infinity.js|hshield.da|renew_session_token.php|recaptcha.js|dat.asp|notice.swf|patchlist.txt|hackshield|captcha|reset.css|update.ver|notice.html|updates.txt|gamenotice|images.kom|patchinfo.xml|noupdate.ui|\.Xtp|\.htc|\.txt)
acl QUERY urlpath_regex -i
(patch.conf|uiimageset.xml.iop|gashaponwnd.xml.iop|loading.swf|download.swf|version.list|version.ini|launch.jnlp|server_patch.cfg.iop|core.swf|Loading.swf|resouececheck.sq|mainloading.swf|config.xml|gemmaze.swf|xml.png|size.xml|resourcesbar.swf|version.xml|version.list|delete.ini)
acl QUERY urlpath_regex -i \.(jsp|asp|aspx|cfg|iop|zip|php|xml|html)(\?|$)
cache deny QUERY
cache deny youtube

#
acl dontstore url_regex
^http:\/\/(([\d\w-]*(\.[^\.\-]*?\..*?))(\/\mosalsal\/[\d]{4}\/.*\/)(.*\.flv))\?start.*
acl dontstore url_regex redbot\.org \.php
acl dontstore url_regex -i ^http:\/\/.*gemscool\.com\/.*
acl dontstore url_regex \.(aspx|php)\?
acl dontstore url_regex goldprice\.org\/NewCharts\/gold\/images\/.*\.png
acl dontstore url_regex google\.co(m|\.[a-z]{2})\/complete\/search\?
acl dontstore url_regex
redirector\.([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id|get_video_info\?|ptracking\?|player_204\?|stream_204\?).*

acl store_yt_id url_regex -i
youtube.*(ptracking|stream_204|playback|player_204|watchtime|set_awesome|s\?|ads).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*$
acl store_id_list_yt url_regex -i (youtube|googlevideo).*videoplayback.*$
acl store_id_list_yt url_regex
^https?\:\/\/([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id).*

acl store-id_list urlpath_regex -i dl\.sourceforge\.net
acl store-id_list urlpath_regex -i \.ytimg\.com
acl store-id_list urlpath_regex -i \.(akamaihd|fbcdn)\.net
acl store_id_list urlpath_regex -i
[a-zA-Z]{2}[0-9]*\.4shared\.com\/download\/

acl store_id_list_url url_regex
^http:\/\/[0-9]\.bp\.blogspot\.com.*\.(jpeg|jpg|png|gif|ico)
acl store_id_list_url url_regex
^http[s]?:\/\/.*\.twimg\.com\/(.*)\.(gif|jpeg|jpg|png|js|css)
acl store_id_list_url url_regex
^http[s]?:\/\/(media|static)\.licdn\.com\/.*\.(png|jpg|gif|woff)
acl store_id_list_url url_regex ^https:\/\/fb(static|cdn)\-.*\-
a.akamaihd.net\/(.*)\.(gif|jpeg|jpg|png|js|css|mp4)
acl store_id_list_url url_regex
^http:\/\/.*\.ak\.fbcdn\.net\/.*\.(gif|jpg|png|js|mp4)

# pass requests
url_rewrite_program /etc/squid/phpredir.php
url_rewrite_access allow youtube

request_header_access Range deny store_id_list_yt
range_offset_limit 10 KB store_id_list_yt


###############################################################################
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
###############################################################################
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny blocksites
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

###############################################################################
# squid ssl_bump option
###############################################################################
always_direct allow all
ssl_bump server-first all
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
#ssl_bump peek step1
#ssl_bump bump all
###############################################################################
# Squid normally listens to port 3128
###############################################################################
https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt
key=/etc/squid/ssl_certs/squid.key
http_port 3129 intercept
http_port 3128

# TAG: Store-id Program
#
-----------------------------------------------------------------------------
store_id_program /usr/bin/perl /etc/squid/store-id.pl
store_id_children 100 startup=0 idle=1 concurrency=1000

# TAG: Store-id Access
#
-----------------------------------------------------------------------------
store_id_access allow urlrewrite
store_id_access allow speedtest
store_id_access allow reverbnation
store_id_access allow utmgif
store_id_access allow playstoreandroid
store_id_access allow idyoutube
store_id_access allow videoyoutube
store_id_access deny dontstore
store_id_access deny !getmethod
store_id_access allow store_id_list_yt
store_id_access allow store_yt_id
store_id_access allow store-id_list
store_id_access deny all
store_id_bypass on

# TAG: Youtube 302
#
-----------------------------------------------------------------------------
store_miss deny store_id_list_yt loop_302
send_hit deny store_id_list_yt loop_302

###############################################################################
## MEMORY CACHE OPTIONS
###############################################################################
client_dst_passthru on
cache_mem 1024 MB
maximum_object_size_in_memory 1024 KB
memory_cache_shared off
memory_cache_mode disk
memory_replacement_policy heap GDSF

###############################################################################
## DISK CACHE OPTIONS
###############################################################################
cache_replacement_policy heap LFUDA
minimum_object_size 1 bytes
maximum_object_size 10 GB

###############################################################################
# Uncomment and adjust the following to add a disk cache directory.
###############################################################################
cache_dir aufs /usr/local/cache_proxy 25000 16 256 # sesuaikan dengan drive
penyimpanan cache
store_dir_select_algorithm round-robin
cache_swap_low 90
cache_swap_high 95

###############################################################################
# Leave coredumps in the first cache dir
###############################################################################
coredump_dir /var/spool/squid

###############################################################################
## LOGFILE OPTIONS
###############################################################################
#access_log daemon:/tmp/access.log !log
#logfile_daemon /usr/lib/squid/log_file_daemon
cache_store_log none
logfile_rotate 1
mime_table /etc/squid/mime.conf
pid_filename /var/run/squid.pid
strip_query_terms off
buffered_logs off

###############################################################################
## OPTIONS FOR TROUBLESHOOTING
###############################################################################
#cache_log /tmp/cache.log
cache_log /dev/null
#debug_options ALL,1 22,3
coredump_dir /var/spool/squid

###############################################################################
## OPTIONS FOR TUNING THE CACHE
###############################################################################
max_stale 1 years
vary_ignore_expire on
shutdown_lifetime 10 seconds

###############################################################################
# Add any of your own refresh_pattern entries above these.
###############################################################################
refresh_pattern ^ftp:  1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
# Youtube Video
refresh_pattern -i
(get_video\?|videoplayback\?|videodownload\?|\.mp4|\.webm|\.flv|((audio|video)\/(webm|mp4)))
241920 100% 241920 override-expire ignore-reload ignore-private
ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
store-stale
refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*
10080 99% 43200 override-lastmod override-expire ignore-reload
reload-into-ims ignore-private reload-into-ims ignore-auth store-stale
refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*$
241920 100% 241920 override-expire ignore-reload ignore-private
ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
store-stale

refresh_pattern (akamaihd|fbcdn)\.net 14400 99% 518400  ignore-no-store
ignore-private ignore-reload ignore-must-revalidate store-stale
refresh_pattern -i squid\.internal 14400 99% 518400  ignore-no-store
ignore-private ignore-reload ignore-must-revalidate store-stale
refresh_pattern \.(jpg|png|gif|css|ico)($|\?) 14400 99% 518400
ignore-no-store ignore-private reload-into-ims ignore-must-revalidate
store-stale
refresh_pattern . 0 99% 518400  ignore-no-store ignore-private
reload-into-ims store-stale
# Image Youtube
refresh_pattern -i (yimg|twimg)\.com\.*         1440 100% 129600
override-expire ignore-reload reload-into-ims
refresh_pattern -i (ytimg|ggpht)\.com\.*        1440 80% 129600
override-expire override-lastmod ignore-auth ignore-reload reload-into-ims

#images facebook
refresh_pattern -i
fbcdn.*net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
241920 99% 241920 ignore-no-store ignore-private override-expire
override-lastmod reload-into-ims ignore-auth
refresh_pattern -i pixel\.facebook\.com.*\.(jpg|png|gif|ico|css|js) 241920
80% 241920 override-expire ignore-reload reload-into-ims ignore-auth
refresh_pattern -i \.akamaihd\.net.*\.(jpg|png|gif|ico|css|js) 241920 80%
241920 override-expire ignore-reload reload-into-ims ignore-auth
refresh_pattern -i ((facebook.com)|(85.131.151.39))\.(jpg|png|gif) 241920
99% 241920 ignore-reload override-expire ignore-no-store store-stale
refresh_pattern -i
fbcdn\.net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
241920 99% 241920 ignore-no-store ignore-private override-expire
override-lastmod reload-into-ims ignore-auth
refresh_pattern static\.(xx|ak)\.fbcdn\.net*\.(jpg|gif|png) 241920 99%
241920 ignore-reload override-expire ignore-no-store
refresh_pattern ^https?\:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png) 241920
99% 241920 ignore-reload override-expire ignore-no-store

# Video Facebook
refresh_pattern -i
\.video.ak.fbcdn.net.*\.(mp4|flv|mp3|amf)                    10080 80%
43200 override-expire ignore-reload reload-into-ims ignore-private
ignore-no-store ignore-must-revalidate
refresh_pattern (audio|video)\/(webm|mp4) 129600 99% 129600 ignore-reload
override-expire override-lastmod ignore-must-revalidate  ignore-private
ignore-no-store ignore-auth store-stale
refresh_pattern -i ^http://.*squid\.internal.*  241920 100% 241920
override-lastmod override-expire ignore-reload ignore-must-revalidate
ignore-private ignore-no-store ignore-auth store-stale

# All File
refresh_pattern -i
\.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt) 10080 80%
10080 override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar|iop|nzp|pak|mar|msp)
10080 80% 10080 override-expire override-lastmod reload-into-ims
ignore-reload
refresh_pattern -i
\.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll) 10080 80% 10080
override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob|webm)
10080 80% 10080 override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)
10080 80% 10080 override-expire override-lastmod reload-into-ims
refresh_pattern .  0 20% 4320

###############################################################################
## ADMINISTRATIVE PARAMETERS
###############################################################################
cache_mgr reetika at foxymoron.org
cache_effective_user proxy
cache_effective_group proxy
visible_hostname foxysquid.foxymoron.tv
unique_hostname foxysquid.foxymoron.tv

###############################################################################
## PERSISTENT CONNECTION HANDLING
###############################################################################
detect_broken_pconn on
client_persistent_connections off
server_persistent_connections on

###############################################################################
## ERROR PAGE OPTIONS
###############################################################################
error_directory /usr/share/squid/errors/en
error_log_languages off

###############################################################################
## DNS OPTIONS
###############################################################################
check_hostnames off
hosts_file /etc/hosts
connect_retries 2
ipcache_low 90
ipcache_high 95
ipcache_size 84024                        # 2x Besar RAM
fqdncache_size 64024                        # real RAM Hardware
pipeline_prefetch 100

###############################################################################
## MISCELLANEOUS
###############################################################################
memory_pools off
reload_into_ims on
uri_whitespace strip
max_filedescriptors 65536

IPtable rules :

................................................

My IPtables Rules

Chain PREROUTING (policy ACCEPT 27405 packets, 1872K bytes)
 pkts bytes target     prot opt in     out     source
destination
76873 4457K DNAT       tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 to:192.168.0.200:3129
   26  1184 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3129
    0     0 DNAT       tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 to:192.168.0.200:3130

Chain INPUT (policy ACCEPT 9321 packets, 543K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 1426 packets, 85560 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 1426 packets, 85560 bytes)
 pkts bytes target     prot opt in     out     source
destination
81432   14M MASQUERADE  all  --  *      eth0    192.168.0.0/24
0.0.0.0/0

On Fri, Jun 5, 2015 at 1:43 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:

> Hi
>
> Thanks for reply. I am trying to cache youtube using this wiki
> http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube but I
> cant cache youtube.
>
> I want to cache facebook and youtube. SSl certificate installation that I
> have to do . Please suggest some links.
>
> On Thu, Jun 4, 2015 at 6:48 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 5/06/2015 12:55 a.m., Reet Vyas wrote:
>> > Thank you everyone for helping me to setup squid , Now its working but
>> in
>> > access.logs  I only see tcp_miss if m using same website. I mean squid
>> is
>> > not caching
>>
>> You will get MISS a fair bit more with intercepted traffic than with
>> normal proxied traffic. Particularly on certain major CDN who play
>> tricks with DNS.
>>
>> The reasons and some workarounds to need to be doing are explained in
>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/4aa3fba1/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 24 11:36:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jun 2015 23:36:44 +1200
Subject: [squid-users] acl for redirect
In-Reply-To: <5589E5C0.4070307@afo.net>
References: <5589E5C0.4070307@afo.net>
Message-ID: <558A964C.9070208@treenet.co.nz>

On 24/06/2015 11:03 a.m., Mike wrote:
> We have a server setup using squid 3.5 and e2guardian (newer branch of
> dansguardian), the issue is now google has changed a few things around
> and google is no longer filtered which is not acceptable. We already
> have the browser settings for SSL Proxy set to our server, and squid has
> ssl-bump enabled and working. Previously there was enough unsecure
> content on Google that the filtering was still working, but now google
> has gone 100% encrypted meaning it is 100% unfiltered.

Maybe, maybe not.

> What is happening
> is it is creating an ssl tunnel (for lack of a better term) between

No. That is the correct and official term for what they are doing. And
"CONNECT tunnel" is the full phrase / name for the particular method of
tunnel creation.


> their server and the browser, so all squid sees is the connection to
> www.google.com, and after that it is tunneled and not recognized by
> squid or e2guardian at all.

BUT ... you said you were SSL-Bump'ing. Which means you are decrypting
such tunnels to filter the content inside them.

So what is the problem? is your method of bumping not decrypting the
Google traffic for Squid access controls and helpers to filter?

Note that DansGuardian and e2guardian being independent HTTP proxies are
not party to that SSL-Bump decrypted content inside Squid. ONly Squid
internals and ICAP/eCAP services have access to it.

> 
> I found a few options online that was used with older squid versions but
> nothing is working with squid 3.5... Looking for something like this:
> 
> acl google dstdomain .google.com
> deny_info http://www.google.com/webhp?nord=1 google

As you said Google have gone 100% HTTPS. URLs beginning with http:// are
not HTTPS nor accepted there anymore. If used they just get a 30x
redirect to an https:// URL.

Amos



From squid3 at treenet.co.nz  Wed Jun 24 11:39:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jun 2015 23:39:40 +1200
Subject: [squid-users] TCP_MISS/503
In-Reply-To: <1435112713523-4671864.post@n4.nabble.com>
References: <1435112692501-4671863.post@n4.nabble.com>
 <1435112713523-4671864.post@n4.nabble.com>
Message-ID: <558A96FC.2010501@treenet.co.nz>

On 24/06/2015 2:25 p.m., HackXBack wrote:
> The requested URL could not be retrieved
> 

Which means exactly what it says. That is the category of problem at
least. The page should also contain a set of possible reasons and
details about the particular transaction message(s) or action failing.

Amos



From maamule10 at gmail.com  Wed Jun 24 11:59:53 2015
From: maamule10 at gmail.com (Dalmar)
Date: Wed, 24 Jun 2015 14:59:53 +0300
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CAFUu-Gv6XqzwNAR8G6-w59J8PGOAnnUs8-69+y7d5bQw3S3wmQ@mail.gmail.com>

Hi,
For over two weeks i am having a really headache in configuring squid
transparent/intercept.
I have tried different options and configurations but i couldn't get it to
work.
i think the problems lies in the Iptables / NAT but i really couldn't solve
it.
I have tried different iptable rules including the intercept linuxDnat -
sysctl configuration, but didnt work.

# your proxy IP
SQUIDIP=X.X.X.X

# your proxy listening port
SQUIDPORT=XXXX


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
$SQUIDIP:$SQUIDPORT
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP


i have to say that squid works well when i configure in the client browsers.

at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action
DST-NAT to address squidIP and Port

i am using ubuntu server 15.04 using squid 3.3.8 and this is my
configuration and the errors i get:


                       ------ eth0 WAN <----- MAIN WAN Public IP Internet
                 MK---|
                           ------ eth1 LAN
                          |
                   ------ eth2 Proxy


         ------ eth0 WAN ---> Public IP --> Internet --> gets internet from
24online / another Mikrotik
      Squid---|
                        ------ eth1 Proxy
       |
        ------ eth2 webmin --> For server Management


-error1: if no intercept/transparent and no iptables is configured
-Invalid URL -  The requested url could not be retrieved
-but if proxy is configured in the user browser - it works!


-error2:if intercept and iptable DNAT is configured
-Access Denied and in the access log TCP-MISS/403
-no forward proxy port configured
        -security alert : host header forgery detected on local=
SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
        -warning : forwarding loop detected (x-Forwarded-for mikrotik lan
IP)

squid.conf

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 8080
http_port 8181
cache_mem 2000 MB
cache_dir ufs /var/spool/squid3 100000 16 256
coredump_dir /var/spool/squid3
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320
cache_effective_user proxy
cache_effective_group proxy

----------------------------------------
I am really confused, can anyone guide me please.
Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/c78237ae/attachment.htm>

From maamule10 at gmail.com  Wed Jun 24 12:03:14 2015
From: maamule10 at gmail.com (Dalmar)
Date: Wed, 24 Jun 2015 15:03:14 +0300
Subject: [squid-users] Mikrotik and Squid Transparent
Message-ID: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>

Hi,
For over two weeks i am having a really headache in configuring squid
transparent/intercept.
I have tried different options and configurations but i couldn't get it to
work.
i think the problems lies in the Iptables / NAT but i really couldn't solve
it.
I have tried different iptable rules including the intercept linuxDnat -
sysctl configuration, but didnt work.

# your proxy IP
SQUIDIP=X.X.X.X

# your proxy listening port
SQUIDPORT=XXXX


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
$SQUIDIP:$SQUIDPORT
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP


i have to say that squid works well when i configure in the client browsers.

at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action
DST-NAT to address squidIP and Port

i am using ubuntu server 15.04 using squid 3.3.8 and this is my
configuration and the errors i get:


                        ------ eth0 WAN <----- MAIN WAN Public IP Internet
                 MK---|
                           ------ eth1 LAN
                          |
                   ------ eth2 Proxy


         ------ eth0 WAN ---> Public IP --> Internet --> gets internet from
24online / another Mikrotik
       Squid---|
                        ------ eth1 Proxy
       |
        ------ eth2 webmin --> For server Management


-error1: if no intercept/transparent and no iptables is configured
-Invalid URL -  The requested url could not be retrieved
-but if proxy is configured in the user browser - it works!


-error2:if intercept and iptable DNAT is configured
-Access Denied and in the access log TCP-MISS/403
-no forward proxy port configured
        -security alert : host header forgery detected on local=
SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
        -warning : forwarding loop detected (x-Forwarded-for mikrotik lan
IP)

squid.conf

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 8080
http_port 8181
cache_mem 2000 MB
cache_dir ufs /var/spool/squid3 100000 16 256
coredump_dir /var/spool/squid3
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320
cache_effective_user proxy
cache_effective_group proxy

----------------------------------------
I am really confused, can anyone guide me please.
Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/af9ad583/attachment.htm>

From yvoinov at gmail.com  Wed Jun 24 12:03:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Jun 2015 18:03:40 +0600
Subject: [squid-users] (no subject)
In-Reply-To: <CAFUu-Gv6XqzwNAR8G6-w59J8PGOAnnUs8-69+y7d5bQw3S3wmQ@mail.gmail.com>
References: <CAFUu-Gv6XqzwNAR8G6-w59J8PGOAnnUs8-69+y7d5bQw3S3wmQ@mail.gmail.com>
Message-ID: <558A9C9C.2090809@gmail.com>

Squid 3.5.x?

24.06.15 17:59, Dalmar ?????:
> Hi,
> For over two weeks i am having a really headache in configuring squid 
> transparent/intercept.
> I have tried different options and configurations but i couldn't get 
> it to work.
> i think the problems lies in the Iptables / NAT but i really couldn't 
> solve it.
> I have tried different iptable rules including the intercept linuxDnat 
> - sysctl configuration, but didnt work.
>
> # your proxy IP
> SQUIDIP=X.X.X.X
>
> # your proxy listening port
> SQUIDPORT=XXXX
>
>
> iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT 
> --to-destination $SQUIDIP:$SQUIDPORT
> iptables -t nat -A POSTROUTING -j MASQUERADE
> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>
>
> i have to say that squid works well when i configure in the client 
> browsers.
>
> at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action 
> DST-NAT to address squidIP and Port
>
> i am using ubuntu server 15.04 using squid 3.3.8 and this is my 
> configuration and the errors i get:
>
>
>          ------ eth0 WAN <----- MAIN WAN Public IP Internet
>                  MK---|
>              ------ eth1 LAN
>                           |
>        ------ eth2 Proxy
>
>  ------ eth0 WAN ---> Public IP --> Internet --> gets internet from 
> 24online / another Mikrotik
>  Squid---|
>           ------ eth1 Proxy
>        |
> ------ eth2 webmin --> For server Management
>
>
> -error1: if no intercept/transparent and no iptables is configured
> -Invalid URL -  The requested url could not be retrieved
> -but if proxy is configured in the user browser - it works!
>
>
> -error2:if intercept and iptable DNAT is configured
> -Access Denied and in the access log TCP-MISS/403
> -no forward proxy port configured
>         -security alert : host header forgery detected on local= 
> SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
>         -warning : forwarding loop detected (x-Forwarded-for mikrotik 
> lan IP)
>
> squid.conf
>
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8># RFC1918 possible 
> internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16># RFC1918 
> possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80# http
> acl Safe_ports port 21# ftp
> acl Safe_ports port 443# https
> acl Safe_ports port 70# gopher
> acl Safe_ports port 210# wais
> acl Safe_ports port 1025-65535# unregistered ports
> acl Safe_ports port 280# http-mgmt
> acl Safe_ports port 488# gss-http
> acl Safe_ports port 591# filemaker
> acl Safe_ports port 777# multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 8080
> http_port 8181
> cache_mem 2000 MB
> cache_dir ufs /var/spool/squid3 100000 16 256
> coredump_dir /var/spool/squid3
> refresh_pattern ^ftp:144020%10080
> refresh_pattern ^gopher:14400%1440
> refresh_pattern -i (/cgi-bin/|\?) 00%0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .020%4320
> cache_effective_user proxy
> cache_effective_group proxy
>
> ----------------------------------------
> I am really confused, can anyone guide me please.
> Thanks in advance
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/c51dadea/attachment.htm>

From yvoinov at gmail.com  Wed Jun 24 12:04:06 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Jun 2015 18:04:06 +0600
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
Message-ID: <558A9CB6.9000403@gmail.com>

Squid 3.5.x?

24.06.15 18:03, Dalmar ?????:
> Hi,
> For over two weeks i am having a really headache in configuring squid 
> transparent/intercept.
> I have tried different options and configurations but i couldn't get 
> it to work.
> i think the problems lies in the Iptables / NAT but i really couldn't 
> solve it.
> I have tried different iptable rules including the intercept linuxDnat 
> - sysctl configuration, but didnt work.
>
> # your proxy IP
> SQUIDIP=X.X.X.X
>
> # your proxy listening port
> SQUIDPORT=XXXX
>
>
> iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT 
> --to-destination $SQUIDIP:$SQUIDPORT
> iptables -t nat -A POSTROUTING -j MASQUERADE
> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>
>
> i have to say that squid works well when i configure in the client 
> browsers.
>
> at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action 
> DST-NAT to address squidIP and Port
>
> i am using ubuntu server 15.04 using squid 3.3.8 and this is my 
> configuration and the errors i get:
>
>
>          ------ eth0 WAN <----- MAIN WAN Public IP Internet
>  MK---|
>                  ------ eth1 LAN
>     |
>                  ------ eth2 Proxy
>
>        ------ eth0 WAN ---> Public IP --> Internet --> gets internet 
> from 24online / another Mikrotik
>  Squid---|
>               ------ eth1 Proxy
>      |
>       ------ eth2 webmin --> For server Management
>
>
> -error1: if no intercept/transparent and no iptables is configured
> -Invalid URL -  The requested url could not be retrieved
> -but if proxy is configured in the user browser - it works!
>
>
> -error2:if intercept and iptable DNAT is configured
> -Access Denied and in the access log TCP-MISS/403
> -no forward proxy port configured
>         -security alert : host header forgery detected on local= 
> SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
>         -warning : forwarding loop detected (x-Forwarded-for mikrotik 
> lan IP)
>
> squid.conf
>
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8># RFC1918 possible 
> internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16># RFC1918 
> possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80# http
> acl Safe_ports port 21# ftp
> acl Safe_ports port 443# https
> acl Safe_ports port 70# gopher
> acl Safe_ports port 210# wais
> acl Safe_ports port 1025-65535# unregistered ports
> acl Safe_ports port 280# http-mgmt
> acl Safe_ports port 488# gss-http
> acl Safe_ports port 591# filemaker
> acl Safe_ports port 777# multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 8080
> http_port 8181
> cache_mem 2000 MB
> cache_dir ufs /var/spool/squid3 100000 16 256
> coredump_dir /var/spool/squid3
> refresh_pattern ^ftp:144020%10080
> refresh_pattern ^gopher:14400%1440
> refresh_pattern -i (/cgi-bin/|\?) 00%0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .020%4320
> cache_effective_user proxy
> cache_effective_group proxy
>
> ----------------------------------------
> I am really confused, can anyone guide me please.
> Thanks in advance
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/4494ca42/attachment.htm>

From reet.vyas28 at gmail.com  Wed Jun 24 12:28:23 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Wed, 24 Jun 2015 17:58:23 +0530
Subject: [squid-users] Squid 3.5.3 with SSL not working
Message-ID: <CAA8ViV9E41Kmt9SO_9L1j5yndJJK1E=7WKSzyWpMuV=jdV3v+g@mail.gmail.com>

Hi
 Below is my squid file , I have configured squid 3.5.3 with ssl, but I
cant filter https traffic and also in access log I cant see https in access
logs.


#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 116.72.152.37 192.168.0.0/24 # Sesuaikan dengan ip
client/local

acl SSL_ports port 443
acl Safe_ports port 80  # http
acl Safe_ports port 21  # ftp
acl Safe_ports port 443  # https
acl Safe_ports port 70  # gopher
acl Safe_ports port 210  # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280  # http-mgmt
acl Safe_ports port 488  # gss-http
acl Safe_ports port 591  # filemaker
acl Safe_ports port 777  # multiling http
# storeid *test*
acl urlrewrite dstdomain .fbcdn.net .akamaihd.net
acl speedtest url_regex -i speedtest\/.*\.(jpg|txt)\?.*
acl reverbnation url_regex -i reverbnation.*audio_player.*ec_stream_song.*$
acl utmgif url_regex -i utm.gif.*
acl playstoreandroid url_regex -i
c.android.clients.google.com.market.GetBinary.GetBinary.*
acl idyoutube url_regex -i
youtube.*(ptracking|stream_204|player_204).*(v\=|docid\=|video_id\=).*$
acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
acl CONNECT method CONNECT
acl getmethod method GET
acl loop_302 http_status 302
acl step1 at_step SslBump1
acl youtube dstdomain .youtube.com
acl blocksites dstdomain "/etc/squid/restricted-sites.squid"
# TAG: QUERY
#
-----------------------------------------------------------------------------
acl QUERY urlpath_regex -i
(hackshield|blank.html|infinity.js|hshield.da|renew_session_token.php|recaptcha.js|dat.asp|notice.swf|patchlist.txt|hackshield|captcha|reset.css|update.ver|notice.html|updates.txt|gamenotice|images.kom|patchinfo.xml|noupdate.ui|\.Xtp|\.htc|\.txt)
acl QUERY urlpath_regex -i
(patch.conf|uiimageset.xml.iop|gashaponwnd.xml.iop|loading.swf|download.swf|version.list|version.ini|launch.jnlp|server_patch.cfg.iop|core.swf|Loading.swf|resouececheck.sq|mainloading.swf|config.xml|gemmaze.swf|xml.png|size.xml|resourcesbar.swf|version.xml|version.list|delete.ini)
acl QUERY urlpath_regex -i \.(jsp|asp|aspx|cfg|iop|zip|php|xml|html)(\?|$)
cache deny QUERY
cache deny youtube

#
acl dontstore url_regex
^http:\/\/(([\d\w-]*(\.[^\.\-]*?\..*?))(\/\mosalsal\/[\d]{4}\/.*\/)(.*\.flv))\?start.*
acl dontstore url_regex redbot\.org \.php
acl dontstore url_regex -i ^http:\/\/.*gemscool\.com\/.*
acl dontstore url_regex \.(aspx|php)\?
acl dontstore url_regex goldprice\.org\/NewCharts\/gold\/images\/.*\.png
acl dontstore url_regex google\.co(m|\.[a-z]{2})\/complete\/search\?
acl dontstore url_regex
redirector\.([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id|get_video_info\?|ptracking\?|player_204\?|stream_204\?).*

acl store_yt_id url_regex -i
youtube.*(ptracking|stream_204|playback|player_204|watchtime|set_awesome|s\?|ads).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*$
acl store_id_list_yt url_regex -i (youtube|googlevideo).*videoplayback.*$
acl store_id_list_yt url_regex
^https?\:\/\/([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id).*

acl store-id_list urlpath_regex -i dl\.sourceforge\.net
acl store-id_list urlpath_regex -i \.ytimg\.com
acl store-id_list urlpath_regex -i \.(akamaihd|fbcdn)\.net
acl store_id_list urlpath_regex -i
[a-zA-Z]{2}[0-9]*\.4shared\.com\/download\/

acl store_id_list_url url_regex
^http:\/\/[0-9]\.bp\.blogspot\.com.*\.(jpeg|jpg|png|gif|ico)
acl store_id_list_url url_regex
^http[s]?:\/\/.*\.twimg\.com\/(.*)\.(gif|jpeg|jpg|png|js|css)
acl store_id_list_url url_regex
^http[s]?:\/\/(media|static)\.licdn\.com\/.*\.(png|jpg|gif|woff)
acl store_id_list_url url_regex ^https:\/\/fb(static|cdn)\-.*\-
a.akamaihd.net\/(.*)\.(gif|jpeg|jpg|png|js|css|mp4)
acl store_id_list_url url_regex
^http:\/\/.*\.ak\.fbcdn\.net\/.*\.(gif|jpg|png|js|mp4)

# pass requests
url_rewrite_program /etc/squid/phpredir.php
url_rewrite_access allow youtube

request_header_access Range deny store_id_list_yt
range_offset_limit 10 KB store_id_list_yt


###############################################################################
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
###############################################################################
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny blocksites
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

###############################################################################
# squid ssl_bump option
###############################################################################
always_direct allow all
ssl_bump server-first all
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
#ssl_bump peek step1
#ssl_bump bump all
###############################################################################
# Squid normally listens to port 3128
###############################################################################
https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt
key=/etc/squid/ssl_certs/squid.key
http_port 3129 intercept
http_port 3128

# TAG: Store-id Program
#
-----------------------------------------------------------------------------
store_id_program /usr/bin/perl /etc/squid/store-id.pl
store_id_children 100 startup=0 idle=1 concurrency=1000

# TAG: Store-id Access
#
-----------------------------------------------------------------------------
store_id_access allow urlrewrite
store_id_access allow speedtest
store_id_access allow reverbnation
store_id_access allow utmgif
store_id_access allow playstoreandroid
store_id_access allow idyoutube
store_id_access allow videoyoutube
store_id_access deny dontstore
store_id_access deny !getmethod
store_id_access allow store_id_list_yt
store_id_access allow store_yt_id
store_id_access allow store-id_list
store_id_access deny all
store_id_bypass on

# TAG: Youtube 302
#
-----------------------------------------------------------------------------
store_miss deny store_id_list_yt loop_302
send_hit deny store_id_list_yt loop_302

###############################################################################
## MEMORY CACHE OPTIONS
###############################################################################
client_dst_passthru on
cache_mem 1024 MB
maximum_object_size_in_memory 1024 KB
memory_cache_shared off
memory_cache_mode disk
memory_replacement_policy heap GDSF

###############################################################################
## DISK CACHE OPTIONS
###############################################################################
cache_replacement_policy heap LFUDA
minimum_object_size 1 bytes
maximum_object_size 10 GB

###############################################################################
# Uncomment and adjust the following to add a disk cache directory.
###############################################################################
cache_dir aufs /usr/local/cache_proxy 25000 16 256 # sesuaikan dengan drive
penyimpanan cache
store_dir_select_algorithm round-robin
cache_swap_low 90
cache_swap_high 95

###############################################################################
# Leave coredumps in the first cache dir
###############################################################################
coredump_dir /var/spool/squid

###############################################################################
## LOGFILE OPTIONS
###############################################################################
#access_log daemon:/tmp/access.log !log
#logfile_daemon /usr/lib/squid/log_file_daemon
cache_store_log none
logfile_rotate 1
mime_table /etc/squid/mime.conf
pid_filename /var/run/squid.pid
strip_query_terms off
buffered_logs off

###############################################################################
## OPTIONS FOR TROUBLESHOOTING
###############################################################################
#cache_log /tmp/cache.log
cache_log /dev/null
#debug_options ALL,1 22,3
coredump_dir /var/spool/squid

###############################################################################
## OPTIONS FOR TUNING THE CACHE
###############################################################################
max_stale 1 years
vary_ignore_expire on
shutdown_lifetime 10 seconds

###############################################################################
# Add any of your own refresh_pattern entries above these.
###############################################################################
refresh_pattern ^ftp:  1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
# Youtube Video
refresh_pattern -i
(get_video\?|videoplayback\?|videodownload\?|\.mp4|\.webm|\.flv|((audio|video)\/(webm|mp4)))
241920 100% 241920 override-expire ignore-reload ignore-private
ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
store-stale
refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*
10080 99% 43200 override-lastmod override-expire ignore-reload
reload-into-ims ignore-private reload-into-ims ignore-auth store-stale
refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*$
241920 100% 241920 override-expire ignore-reload ignore-private
ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth
store-stale

refresh_pattern (akamaihd|fbcdn)\.net 14400 99% 518400  ignore-no-store
ignore-private ignore-reload ignore-must-revalidate store-stale
refresh_pattern -i squid\.internal 14400 99% 518400  ignore-no-store
ignore-private ignore-reload ignore-must-revalidate store-stale
refresh_pattern \.(jpg|png|gif|css|ico)($|\?) 14400 99% 518400
ignore-no-store ignore-private reload-into-ims ignore-must-revalidate
store-stale
refresh_pattern . 0 99% 518400  ignore-no-store ignore-private
reload-into-ims store-stale
# Image Youtube
refresh_pattern -i (yimg|twimg)\.com\.*         1440 100% 129600
override-expire ignore-reload reload-into-ims
refresh_pattern -i (ytimg|ggpht)\.com\.*        1440 80% 129600
override-expire override-lastmod ignore-auth ignore-reload reload-into-ims

#images facebook
refresh_pattern -i
fbcdn.*net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
241920 99% 241920 ignore-no-store ignore-private override-expire
override-lastmod reload-into-ims ignore-auth
refresh_pattern -i pixel\.facebook\.com.*\.(jpg|png|gif|ico|css|js) 241920
80% 241920 override-expire ignore-reload reload-into-ims ignore-auth
refresh_pattern -i \.akamaihd\.net.*\.(jpg|png|gif|ico|css|js) 241920 80%
241920 override-expire ignore-reload reload-into-ims ignore-auth
refresh_pattern -i ((facebook.com)|(85.131.151.39))\.(jpg|png|gif) 241920
99% 241920 ignore-reload override-expire ignore-no-store store-stale
refresh_pattern -i
fbcdn\.net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$))
241920 99% 241920 ignore-no-store ignore-private override-expire
override-lastmod reload-into-ims ignore-auth
refresh_pattern static\.(xx|ak)\.fbcdn\.net*\.(jpg|gif|png) 241920 99%
241920 ignore-reload override-expire ignore-no-store
refresh_pattern ^https?\:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png) 241920
99% 241920 ignore-reload override-expire ignore-no-store

# Video Facebook
refresh_pattern -i
\.video.ak.fbcdn.net.*\.(mp4|flv|mp3|amf)                    10080 80%
43200 override-expire ignore-reload reload-into-ims ignore-private
ignore-no-store ignore-must-revalidate
refresh_pattern (audio|video)\/(webm|mp4) 129600 99% 129600 ignore-reload
override-expire override-lastmod ignore-must-revalidate  ignore-private
ignore-no-store ignore-auth store-stale
refresh_pattern -i ^http://.*squid\.internal.*  241920 100% 241920
override-lastmod override-expire ignore-reload ignore-must-revalidate
ignore-private ignore-no-store ignore-auth store-stale

# All File
refresh_pattern -i
\.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt) 10080 80%
10080 override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar|iop|nzp|pak|mar|msp)
10080 80% 10080 override-expire override-lastmod reload-into-ims
ignore-reload
refresh_pattern -i
\.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll) 10080 80% 10080
override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob|webm)
10080 80% 10080 override-expire override-lastmod reload-into-ims
refresh_pattern -i
\.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv)
10080 80% 10080 override-expire override-lastmod reload-into-ims
refresh_pattern .  0 20% 4320

###############################################################################
## ADMINISTRATIVE PARAMETERS
###############################################################################
cache_mgr reetika at foxymoron.org
cache_effective_user proxy
cache_effective_group proxy
visible_hostname foxysquid.foxymoron.tv
unique_hostname foxysquid.foxymoron.tv

###############################################################################
## PERSISTENT CONNECTION HANDLING
###############################################################################
detect_broken_pconn on
client_persistent_connections off
server_persistent_connections on

###############################################################################
## ERROR PAGE OPTIONS
###############################################################################
error_directory /usr/share/squid/errors/en
error_log_languages off

###############################################################################
## DNS OPTIONS
###############################################################################
check_hostnames off
hosts_file /etc/hosts
connect_retries 2
ipcache_low 90
ipcache_high 95
ipcache_size 84024                        # 2x Besar RAM
fqdncache_size 64024                        # real RAM Hardware
pipeline_prefetch 100

###############################################################################
## MISCELLANEOUS
###############################################################################
memory_pools off
reload_into_ims on
uri_whitespace strip
max_filedescriptors 65536

IPtable rules :

................................................

My IPtables Rules

Chain PREROUTING (policy ACCEPT 27405 packets, 1872K bytes)
 pkts bytes target     prot opt in     out     source
destination
76873 4457K DNAT       tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 to:192.168.0.200:3129
   26  1184 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3129
    0     0 DNAT       tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 to:192.168.0.200:3130

Chain INPUT (policy ACCEPT 9321 packets, 543K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 1426 packets, 85560 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 1426 packets, 85560 bytes)
 pkts bytes target     prot opt in     out     source
destination
81432   14M MASQUERADE  all  --  *      eth0    192.168.0.0/24
0.0.0.0/0
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/9ed25ec7/attachment.htm>

From maamule10 at gmail.com  Wed Jun 24 12:30:15 2015
From: maamule10 at gmail.com (Dalmar)
Date: Wed, 24 Jun 2015 15:30:15 +0300
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <558A9CB6.9000403@gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
Message-ID: <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>

squid 3.3.8 and ubuntu 15.04 server

2015-06-24 15:04 GMT+03:00 Yuri Voinov <yvoinov at gmail.com>:

>  Squid 3.5.x?
>
> 24.06.15 18:03, Dalmar ?????:
>
>  Hi,
> For over two weeks i am having a really headache in configuring squid
> transparent/intercept.
> I have tried different options and configurations but i couldn't get it to
> work.
> i think the problems lies in the Iptables / NAT but i really couldn't
> solve it.
> I have tried different iptable rules including the intercept linuxDnat -
> sysctl configuration, but didnt work.
>
>  # your proxy IP
> SQUIDIP=X.X.X.X
>
>  # your proxy listening port
> SQUIDPORT=XXXX
>
>
>  iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
> $SQUIDIP:$SQUIDPORT
> iptables -t nat -A POSTROUTING -j MASQUERADE
> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>
>
>  i have to say that squid works well when i configure in the client
> browsers.
>
>  at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action
> DST-NAT to address squidIP and Port
>
>  i am using ubuntu server 15.04 using squid 3.3.8 and this is my
> configuration and the errors i get:
>
>
>                          ------ eth0 WAN <----- MAIN WAN Public IP
> Internet
>                  MK---|
>                            ------ eth1 LAN
>                           |
>                     ------ eth2 Proxy
>
>
>           ------ eth0 WAN ---> Public IP --> Internet --> gets internet
> from 24online / another Mikrotik
>        Squid---|
>                         ------ eth1 Proxy
>         |
>          ------ eth2 webmin --> For server Management
>
>
>  -error1: if no intercept/transparent and no iptables is configured
>  -Invalid URL -  The requested url could not be retrieved
>  -but if proxy is configured in the user browser - it works!
>
>
>  -error2:if intercept and iptable DNAT is configured
>  -Access Denied and in the access log TCP-MISS/403
>  -no forward proxy port configured
>         -security alert : host header forgery detected on local=
> SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
>         -warning : forwarding loop detected (x-Forwarded-for mikrotik lan
> IP)
>
>  squid.conf
>
>  acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> http_port 8080
> http_port 8181
> cache_mem 2000 MB
> cache_dir ufs /var/spool/squid3 100000 16 256
> coredump_dir /var/spool/squid3
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern . 0 20% 4320
> cache_effective_user proxy
> cache_effective_group proxy
>
>  ----------------------------------------
> I am really confused, can anyone guide me please.
> Thanks in advance
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/4a9fbd38/attachment.htm>

From tmowbray at dalabs.com  Wed Jun 24 15:41:52 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Wed, 24 Jun 2015 11:41:52 -0400
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
	and ssl_bump
Message-ID: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>

Squid 3.5.5

I seem to have some confusion about how acl lists are processed in
squid.conf regarding the handling of SSL (HTTPS) traffic, attempting to use
ssl_bump directives with transparent proxy.

Based on available documentation, I believe my squid.conf is correct,
however it never seems to actually behave as expected.

I define the SSL port, as usual:

acl SSL_ports port 443

But here's where my confusion lies... Many state to place the following
line above the ssl_bump configuration lines:

http_access allow SSL_ports

However when I do this, it appears to simply stop processing any other
rules and allows ALL https traffic through the proxy (which is actually how
I'd expect a standard ACL list to operate, but then how do I actually
filter the traffic though our content-based ACL lists?).  If I put the
above line below the ssl_bump configuration options in my squid.conf, then
it appears to BUMP all, even though I've told the config to SPLICE all
https traffic, which doesn't work for our deployment.

So, does squid actually continue to process the https traffic using the
ssl_bump rules if the "http_access allow SSL_ports" line is placed above it
in the configuration?

I should note that we've been able to get filtering to work correctly when
using our configuration in NON-transparent mode, however our goal is get
this functionality working as a transparent proxy.  We're unable to load
our self-signed cert onto client machines that will be accessing the proxy,
so using the "bump" or man-in-the-middle style https filtering isn't a
viable option for us.

Any help or advice is appreciated!

Thanks,

Tom
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/fe000672/attachment.htm>

From yvoinov at gmail.com  Wed Jun 24 16:00:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Jun 2015 22:00:42 +0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
Message-ID: <558AD42A.30705@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Tom,

one simple question.

Soon, all or almost all the Internet go into HTTPS. Why do you then need
caching proxy? The tunnel connection and process ACLs?

My second question to Amos. Amos, what the hell do we under these
conditions caching proxy?

WBR, Yuri

24.06.15 21:41, Tom Mowbray ?????:
> Squid 3.5.5
>
> I seem to have some confusion about how acl lists are processed in
> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting
to use
> ssl_bump directives with transparent proxy.
>
> Based on available documentation, I believe my squid.conf is correct,
> however it never seems to actually behave as expected.
>
> I define the SSL port, as usual:
>
> acl SSL_ports port 443
>
> But here's where my confusion lies... Many state to place the following
> line above the ssl_bump configuration lines:
>
> http_access allow SSL_ports
>
> However when I do this, it appears to simply stop processing any other
> rules and allows ALL https traffic through the proxy (which is
actually how
> I'd expect a standard ACL list to operate, but then how do I actually
> filter the traffic though our content-based ACL lists?).  If I put the
> above line below the ssl_bump configuration options in my squid.conf, then
> it appears to BUMP all, even though I've told the config to SPLICE all
> https traffic, which doesn't work for our deployment.
>
> So, does squid actually continue to process the https traffic using the
> ssl_bump rules if the "http_access allow SSL_ports" line is placed
above it
> in the configuration?
>
> I should note that we've been able to get filtering to work correctly when
> using our configuration in NON-transparent mode, however our goal is get
> this functionality working as a transparent proxy.  We're unable to load
> our self-signed cert onto client machines that will be accessing the
proxy,
> so using the "bump" or man-in-the-middle style https filtering isn't a
> viable option for us.
>
> Any help or advice is appreciated!
>
> Thanks,
>
> Tom
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVitQqAAoJENNXIZxhPexGDaQIAKtb0MvhmOlS6OpGHNCjvWqd
dYXvdm+gMGE2NSl1FPAUa1sz6zj2gyI21p0nWZZu+BPWRa3Puo2XJDFlujtLtbgq
Tsqf7WeKD/dxSJzK1ooIK4OsxSpXpHchHcPnUTZ4qMPDBaAy5JKnqHK4IaX6Py5u
8AByGDCWkacHOZsjgvWpjlqoPK3bGwHsoQTTp6bs87J1JkpWdrw2eKjQCK4OfCA3
hra/kp38UFIMm/Jy8TPIv1jzx8CJsC72ImovovBSuPn7Aq2QXNyO3ZVC/TtBVHVi
x63zzJ1B599ZOZ2QqeL2fAyzeYr7ZL6MT+J6l8Vk0YvUCCO63b1rwX1Jp4qMyog=
=kTMC
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/c1c97d47/attachment.htm>

From tmowbray at dalabs.com  Wed Jun 24 16:22:51 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Wed, 24 Jun 2015 12:22:51 -0400
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
	and ssl_bump
Message-ID: <CAM3U7EyLbfFw0J-X79Oh_orZ257wakXcstRtzoA_Td1FMW44SQ@mail.gmail.com>

Yuri,

The proxy is being used as a content filter, i.e. domain and URL
whitelisting and blacklisting.

I guess my real question is simply regarding how this traffic is processed
in regards to where I've defined options in my squid.conf?

Also, why does it appear to "bump" all sites when my config says to
"splice" all.

-Tom


Tom,

one simple question.

Soon, all or almost all the Internet go into HTTPS. Why do you then need
caching proxy? The tunnel connection and process ACLs?

My second question to Amos. Amos, what the hell do we under these
conditions caching proxy?

WBR, Yuri

24.06.15 21:41, Tom Mowbray ?????:
> Squid 3.5.5
>
> I seem to have some confusion about how acl lists are processed in
> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting
to use
> ssl_bump directives with transparent proxy.
>
> Based on available documentation, I believe my squid.conf is correct,
> however it never seems to actually behave as expected.
>
> I define the SSL port, as usual:
>
> acl SSL_ports port 443
>
> But here's where my confusion lies... Many state to place the following
> line above the ssl_bump configuration lines:
>
> http_access allow SSL_ports
>
> However when I do this, it appears to simply stop processing any other
> rules and allows ALL https traffic through the proxy (which is
actually how
> I'd expect a standard ACL list to operate, but then how do I actually
> filter the traffic though our content-based ACL lists?).  If I put the
> above line below the ssl_bump configuration options in my squid.conf, then
> it appears to BUMP all, even though I've told the config to SPLICE all
> https traffic, which doesn't work for our deployment.
>
> So, does squid actually continue to process the https traffic using the
> ssl_bump rules if the "http_access allow SSL_ports" line is placed
above it
> in the configuration?
>
> I should note that we've been able to get filtering to work correctly when
> using our configuration in NON-transparent mode, however our goal is get
> this functionality working as a transparent proxy.  We're unable to load
> our self-signed cert onto client machines that will be accessing the
proxy,
> so using the "bump" or man-in-the-middle style https filtering isn't a
> viable option for us.
>
> Any help or advice is appreciated!
>
> Thanks,
>
> Tom
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/a669b386/attachment.htm>

From yvoinov at gmail.com  Wed Jun 24 16:25:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Jun 2015 22:25:29 +0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7EyLbfFw0J-X79Oh_orZ257wakXcstRtzoA_Td1FMW44SQ@mail.gmail.com>
References: <CAM3U7EyLbfFw0J-X79Oh_orZ257wakXcstRtzoA_Td1FMW44SQ@mail.gmail.com>
Message-ID: <558AD9F9.7030606@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Never mind, Tom. I have own cockroaches in my head. Just only for
content filtering, I would not put a caching proxy. Once that's it.

24.06.15 22:22, Tom Mowbray ?????:
> Yuri,
>
> The proxy is being used as a content filter, i.e. domain and URL
> whitelisting and blacklisting.
>
> I guess my real question is simply regarding how this traffic is processed
> in regards to where I've defined options in my squid.conf?
>
> Also, why does it appear to "bump" all sites when my config says to
> "splice" all.
>
> -Tom
>
>
> Tom,
>
> one simple question.
>
> Soon, all or almost all the Internet go into HTTPS. Why do you then need
> caching proxy? The tunnel connection and process ACLs?
>
> My second question to Amos. Amos, what the hell do we under these
> conditions caching proxy?
>
> WBR, Yuri
>
> 24.06.15 21:41, Tom Mowbray ?????:
>> Squid 3.5.5
>>
>> I seem to have some confusion about how acl lists are processed in
>> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting
> to use
>> ssl_bump directives with transparent proxy.
>>
>> Based on available documentation, I believe my squid.conf is correct,
>> however it never seems to actually behave as expected.
>>
>> I define the SSL port, as usual:
>>
>> acl SSL_ports port 443
>>
>> But here's where my confusion lies... Many state to place the following
>> line above the ssl_bump configuration lines:
>>
>> http_access allow SSL_ports
>>
>> However when I do this, it appears to simply stop processing any other
>> rules and allows ALL https traffic through the proxy (which is
> actually how
>> I'd expect a standard ACL list to operate, but then how do I actually
>> filter the traffic though our content-based ACL lists?).  If I put the
>> above line below the ssl_bump configuration options in my squid.conf,
then
>> it appears to BUMP all, even though I've told the config to SPLICE all
>> https traffic, which doesn't work for our deployment.
>>
>> So, does squid actually continue to process the https traffic using the
>> ssl_bump rules if the "http_access allow SSL_ports" line is placed
> above it
>> in the configuration?
>>
>> I should note that we've been able to get filtering to work correctly
when
>> using our configuration in NON-transparent mode, however our goal is get
>> this functionality working as a transparent proxy.  We're unable to load
>> our self-signed cert onto client machines that will be accessing the
> proxy,
>> so using the "bump" or man-in-the-middle style https filtering isn't a
>> viable option for us.
>>
>> Any help or advice is appreciated!
>>
>> Thanks,
>>
>> Tom
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVitn5AAoJENNXIZxhPexGseIH/0Mex6B035vuH5/c/Ui5+az5
glsYSK8AzGGyQNkAvlKQ0xNe+0DrpC96tToafdPee1yyD3mp8U4ftFgb6xOHnfNt
DlFo7oWMJt7xhXyN9oJgwzEDLvfvwQ/YcoPWLmAw0vPcJ9WgIPMLY2Mvpsy/vHnb
dEfBvshk5PvbRwFD/WIbm4dU3x0eIPyHp/M5JG0yi0jVTOmUfbFhqXttGQTnOwl4
d+b8uubNmcOGH5Di2j7wTfT9LFV4o8ijy5oM1WvVRuHNXe/YIY96Gt1v3Hm10Qeu
49PPFTbDiYsJ/39HQ6MfDyhGy3tlWNVY1E5CIV8teVi6P+3ew2nUJw1pQGiawqk=
=SwDm
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/6d92be36/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 24 16:41:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2015 04:41:06 +1200
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
Message-ID: <558ADDA2.1020902@treenet.co.nz>

On 25/06/2015 3:41 a.m., Tom Mowbray wrote:
> Squid 3.5.5
> 
> I seem to have some confusion about how acl lists are processed in
> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting to use
> ssl_bump directives with transparent proxy.
> 
> Based on available documentation, I believe my squid.conf is correct,
> however it never seems to actually behave as expected.
> 
> I define the SSL port, as usual:
> 
> acl SSL_ports port 443
> 
> But here's where my confusion lies... Many state to place the following
> line above the ssl_bump configuration lines:
> 
> http_access allow SSL_ports
> 
> However when I do this, it appears to simply stop processing any other
> rules and allows ALL https traffic through the proxy (which is actually how
> I'd expect a standard ACL list to operate, but then how do I actually
> filter the traffic though our content-based ACL lists?).  If I put the
> above line below the ssl_bump configuration options in my squid.conf, then
> it appears to BUMP all, even though I've told the config to SPLICE all
> https traffic, which doesn't work for our deployment.
> 
> So, does squid actually continue to process the https traffic using the
> ssl_bump rules if the "http_access allow SSL_ports" line is placed above it
> in the configuration?

The order for these only matter for lines of the same directive name.
The http_access set get tested on the initial CONNECT request, then the
ssl_bump on each of the bumping steps, then if bumping decrypts the
http_access is run on the decrypted request(s).


> 
> I should note that we've been able to get filtering to work correctly when
> using our configuration in NON-transparent mode, however our goal is get
> this functionality working as a transparent proxy.  We're unable to load
> our self-signed cert onto client machines that will be accessing the proxy,
> so using the "bump" or man-in-the-middle style https filtering isn't a
> viable option for us.

Then don't bother with any of this. You wont get transparent HTTPS
interception working without that MITM certificate install you already
ruled out.

Just use "ssl_bump none all" and Squid will relay the intercepted TCP
connections to the server they were destined for. Or better yet get rid
of the Squid step and let the traffic out on port 443 without slowing it
down for useless proxying.

Amos


From squid3 at treenet.co.nz  Wed Jun 24 16:55:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2015 04:55:49 +1200
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <558AD42A.30705@gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <558AD42A.30705@gmail.com>
Message-ID: <558AE115.6080205@treenet.co.nz>

On 25/06/2015 4:00 a.m., Yuri Voinov wrote:
> 
> Tom,
> 
> one simple question.
> 
> Soon, all or almost all the Internet go into HTTPS. Why do you then need
> caching proxy?

Because HTTPS is more cacheable than HTTP. A lot of misguided developers
that go needlessly out of their way to prevent caching their http://
content omit the same in https:// (its end-to-end right? ;-). Which is
one of the several reasons HTTPS still works "fast" despite the extra
overheads of MITM decryption.


> The tunnel connection and process ACLs?
> 
> My second question to Amos. Amos, what the hell do we under these
> conditions caching proxy?

Even the experts in the IETF are divided over that question. The only
thing to do right now is rollout MITM across the whole Internet to match
it. The HTTPS bumpign and decryption related threads in here and
elsewhere is a good reflection of that happening as well.

Though efforts are underway to convince the browser people to fix their
lack of TLS-to-proxy for security on http:// and cacheable DRM-style
crypto for just the payload of messages, etc. Once they accept that the
bogus arguments about http:// being "insecure" disappear.

Amos


From tmowbray at dalabs.com  Wed Jun 24 17:27:27 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Wed, 24 Jun 2015 13:27:27 -0400
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
	and ssl_bump
In-Reply-To: <558ADDA2.1020902@treenet.co.nz>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <558ADDA2.1020902@treenet.co.nz>
Message-ID: <CAM3U7EzTXwvDOTWv7z8OXS25M_m9AGYQj4jTp-Np5pS6+XcpZg@mail.gmail.com>

Thanks for the response.  Our understanding was that by using the "peek and
splice" options, we could transparently filter https traffic using the SNI
at the very least (though perhaps the issue lies with our external ACL?),
without having to decrypt the SSL session or use MITM cert.  Our results in
testing, however, have been less than promising.


---------------------------------
Tom Mowbray
*tmowbray at dalabs.com* <tmowbray at dalabs.com>
*703-829-6694*

On Wed, Jun 24, 2015 at 12:41 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 25/06/2015 3:41 a.m., Tom Mowbray wrote:
> > Squid 3.5.5
> >
> > I seem to have some confusion about how acl lists are processed in
> > squid.conf regarding the handling of SSL (HTTPS) traffic, attempting to
> use
> > ssl_bump directives with transparent proxy.
> >
> > Based on available documentation, I believe my squid.conf is correct,
> > however it never seems to actually behave as expected.
> >
> > I define the SSL port, as usual:
> >
> > acl SSL_ports port 443
> >
> > But here's where my confusion lies... Many state to place the following
> > line above the ssl_bump configuration lines:
> >
> > http_access allow SSL_ports
> >
> > However when I do this, it appears to simply stop processing any other
> > rules and allows ALL https traffic through the proxy (which is actually
> how
> > I'd expect a standard ACL list to operate, but then how do I actually
> > filter the traffic though our content-based ACL lists?).  If I put the
> > above line below the ssl_bump configuration options in my squid.conf,
> then
> > it appears to BUMP all, even though I've told the config to SPLICE all
> > https traffic, which doesn't work for our deployment.
> >
> > So, does squid actually continue to process the https traffic using the
> > ssl_bump rules if the "http_access allow SSL_ports" line is placed above
> it
> > in the configuration?
>
> The order for these only matter for lines of the same directive name.
> The http_access set get tested on the initial CONNECT request, then the
> ssl_bump on each of the bumping steps, then if bumping decrypts the
> http_access is run on the decrypted request(s).
>
>
> >
> > I should note that we've been able to get filtering to work correctly
> when
> > using our configuration in NON-transparent mode, however our goal is get
> > this functionality working as a transparent proxy.  We're unable to load
> > our self-signed cert onto client machines that will be accessing the
> proxy,
> > so using the "bump" or man-in-the-middle style https filtering isn't a
> > viable option for us.
>
> Then don't bother with any of this. You wont get transparent HTTPS
> interception working without that MITM certificate install you already
> ruled out.
>
> Just use "ssl_bump none all" and Squid will relay the intercepted TCP
> connections to the server they were destined for. Or better yet get rid
> of the Squid step and let the traffic out on port 443 without slowing it
> down for useless proxying.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/9ea652d2/attachment.htm>

From jlay at slave-tothe-box.net  Wed Jun 24 17:31:05 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 24 Jun 2015 11:31:05 -0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
Message-ID: <f7b351c914319a5b8226ccd73759e05d@localhost>

On 2015-06-24 09:41 AM, Tom Mowbray wrote:
> Squid 3.5.5
> 
> I seem to have some confusion about how acl lists are processed in
> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting
> to use ssl_bump directives with transparent proxy.
> 
> Based on available documentation, I believe my squid.conf is correct,
> however it never seems to actually behave as expected.
> 
> I define the SSL port, as usual:
> 
> acl SSL_ports port 443
> 
> But here's where my confusion lies... Many state to place the
> following line above the ssl_bump configuration lines:
> 
> http_access allow SSL_ports
> 
> However when I do this, it appears to simply stop processing any other
> rules and allows ALL https traffic through the proxy (which is
> actually how I'd expect a standard ACL list to operate, but then how
> do I actually filter the traffic though our content-based ACL lists?).
>  If I put the above line below the ssl_bump configuration options in
> my squid.conf, then it appears to BUMP all, even though I've told the
> config to SPLICE all https traffic, which doesn't work for our
> deployment.
> 
> So, does squid actually continue to process the https traffic using
> the ssl_bump rules if the "http_access allow SSL_ports" line is placed
> above it in the configuration?
> 
> I should note that we've been able to get filtering to work correctly
> when using our configuration in NON-transparent mode, however our goal
> is get this functionality working as a transparent proxy.  We're
> unable to load our self-signed cert onto client machines that will be
> accessing the proxy, so using the "bump" or man-in-the-middle style
> https filtering isn't a viable option for us.
> 
> Any help or advice is appreciated!
> 
> Thanks,
> 
> Tom

Tom,

You kinda have to change the way you think about filtering when it comes 
to Squid 3.5.5 and SSL(TLS).  Normal http traffic is easy....here's 
where we're trying to go and here's a list of place we're alloed to 
go...simple.

Not so with SSL(TLS).  Squid can't filter, since Squid may or may not 
know where we're going...and that's the issue..it's where those ssl_bump 
atStep ACL's come in.  Some sites when you connect to them are 
easy-ish..when you connect your device sends a "Server Name Information" 
(SNI) that says where you're going.  Other sites don't have any 
information until you complete the SSL handshake (how can you filter a 
site name, until squid KNOWS the site or at least domain name?).

If you're still wanting to go through with transparent (intercept) proxy 
with SSL, search through the list for my SSL Deep dive posts...that 
config is working for me so far (granted, not in an enterprise 
environment).  However, as Amos said,....if you choose not to install 
the cert on the client machines, you are either a) going to be out of 
luck on LOT'S of websites because they will fail the SSL handshake, or 
b) teaching your users to ignore the security warnings of their 
browser's....neither of which is a good thing.

Hope that helps.

James



From tmowbray at dalabs.com  Wed Jun 24 17:46:27 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Wed, 24 Jun 2015 13:46:27 -0400
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
	and ssl_bump
In-Reply-To: <f7b351c914319a5b8226ccd73759e05d@localhost>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
Message-ID: <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>

James,

Yes, as a matter of fact I have read through those exact posts and modeled
my config very similarly.  What I have found is that, however, when the
line "http_access allow SSL_ports" is placed above the ssl_bump stuff and
other acl's (as you have it), it seems to simply allow ALL https without
doing any filtering whatsoever.


Thanks for the response.


---------------------------------
Tom Mowbray
*tmowbray at dalabs.com* <tmowbray at dalabs.com>
*703-829-6694*

On Wed, Jun 24, 2015 at 1:31 PM, James Lay <jlay at slave-tothe-box.net> wrote:

> On 2015-06-24 09:41 AM, Tom Mowbray wrote:
>
>> Squid 3.5.5
>>
>> I seem to have some confusion about how acl lists are processed in
>> squid.conf regarding the handling of SSL (HTTPS) traffic, attempting
>> to use ssl_bump directives with transparent proxy.
>>
>> Based on available documentation, I believe my squid.conf is correct,
>> however it never seems to actually behave as expected.
>>
>> I define the SSL port, as usual:
>>
>> acl SSL_ports port 443
>>
>> But here's where my confusion lies... Many state to place the
>> following line above the ssl_bump configuration lines:
>>
>> http_access allow SSL_ports
>>
>> However when I do this, it appears to simply stop processing any other
>> rules and allows ALL https traffic through the proxy (which is
>> actually how I'd expect a standard ACL list to operate, but then how
>> do I actually filter the traffic though our content-based ACL lists?).
>>  If I put the above line below the ssl_bump configuration options in
>> my squid.conf, then it appears to BUMP all, even though I've told the
>> config to SPLICE all https traffic, which doesn't work for our
>> deployment.
>>
>> So, does squid actually continue to process the https traffic using
>> the ssl_bump rules if the "http_access allow SSL_ports" line is placed
>> above it in the configuration?
>>
>> I should note that we've been able to get filtering to work correctly
>> when using our configuration in NON-transparent mode, however our goal
>> is get this functionality working as a transparent proxy.  We're
>> unable to load our self-signed cert onto client machines that will be
>> accessing the proxy, so using the "bump" or man-in-the-middle style
>> https filtering isn't a viable option for us.
>>
>> Any help or advice is appreciated!
>>
>> Thanks,
>>
>> Tom
>>
>
> Tom,
>
> You kinda have to change the way you think about filtering when it comes
> to Squid 3.5.5 and SSL(TLS).  Normal http traffic is easy....here's where
> we're trying to go and here's a list of place we're alloed to go...simple.
>
> Not so with SSL(TLS).  Squid can't filter, since Squid may or may not know
> where we're going...and that's the issue..it's where those ssl_bump atStep
> ACL's come in.  Some sites when you connect to them are easy-ish..when you
> connect your device sends a "Server Name Information" (SNI) that says where
> you're going.  Other sites don't have any information until you complete
> the SSL handshake (how can you filter a site name, until squid KNOWS the
> site or at least domain name?).
>
> If you're still wanting to go through with transparent (intercept) proxy
> with SSL, search through the list for my SSL Deep dive posts...that config
> is working for me so far (granted, not in an enterprise environment).
> However, as Amos said,....if you choose not to install the cert on the
> client machines, you are either a) going to be out of luck on LOT'S of
> websites because they will fail the SSL handshake, or b) teaching your
> users to ignore the security warnings of their browser's....neither of
> which is a good thing.
>
> Hope that helps.
>
> James
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/d8699605/attachment.htm>

From jlay at slave-tothe-box.net  Wed Jun 24 18:05:53 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 24 Jun 2015 12:05:53 -0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
Message-ID: <4d81af3a2e72c81d15faf54761bdb1a8@localhost>

On 2015-06-24 11:46 AM, Tom Mowbray wrote:
> James,
> 
> Yes, as a matter of fact I have read through those exact posts and
> modeled my config very similarly.  What I have found is that, however,
> when the line "http_access allow SSL_ports" is placed above the
> ssl_bump stuff and other acl's (as you have it), it seems to simply
> allow ALL https without doing any filtering whatsoever.
> 
> Thanks for the response.
> 
> ---------------------------------Tom Mowbray
> _tmowbray at dalabs.com_
> _703-829-6694_
> 
> On Wed, Jun 24, 2015 at 1:31 PM, James Lay <jlay at slave-tothe-box.net>
> wrote:
> 
>> On 2015-06-24 09:41 AM, Tom Mowbray wrote:
>> 
>>> Squid 3.5.5
>>> 
>>> I seem to have some confusion about how acl lists are processed
>>> in
>>> squid.conf regarding the handling of SSL (HTTPS) traffic,
>>> attempting
>>> to use ssl_bump directives with transparent proxy.
>>> 
>>> Based on available documentation, I believe my squid.conf is
>>> correct,
>>> however it never seems to actually behave as expected.
>>> 
>>> I define the SSL port, as usual:
>>> 
>>> acl SSL_ports port 443
>>> 
>>> But here's where my confusion lies... Many state to place the
>>> following line above the ssl_bump configuration lines:
>>> 
>>> http_access allow SSL_ports
>>> 
>>> However when I do this, it appears to simply stop processing any
>>> other
>>> rules and allows ALL https traffic through the proxy (which is
>>> actually how I'd expect a standard ACL list to operate, but then
>>> how
>>> do I actually filter the traffic though our content-based ACL
>>> lists?).
>>> If I put the above line below the ssl_bump configuration options
>>> in
>>> my squid.conf, then it appears to BUMP all, even though I've told
>>> the
>>> config to SPLICE all https traffic, which doesn't work for our
>>> deployment.
>>> 
>>> So, does squid actually continue to process the https traffic
>>> using
>>> the ssl_bump rules if the "http_access allow SSL_ports" line is
>>> placed
>>> above it in the configuration?
>>> 
>>> I should note that we've been able to get filtering to work
>>> correctly
>>> when using our configuration in NON-transparent mode, however our
>>> goal
>>> is get this functionality working as a transparent proxy. We're
>>> unable to load our self-signed cert onto client machines that
>>> will be
>>> accessing the proxy, so using the "bump" or man-in-the-middle
>>> style
>>> https filtering isn't a viable option for us.
>>> 
>>> Any help or advice is appreciated!
>>> 
>>> Thanks,
>>> 
>>> Tom
>> 
>> Tom,
>> 
>> You kinda have to change the way you think about filtering when it
>> comes to Squid 3.5.5 and SSL(TLS). Normal http traffic is
>> easy....here's where we're trying to go and here's a list of place
>> we're alloed to go...simple.
>> 
>> Not so with SSL(TLS). Squid can't filter, since Squid may or may
>> not know where we're going...and that's the issue..it's where those
>> ssl_bump atStep ACL's come in. Some sites when you connect to them
>> are easy-ish..when you connect your device sends a "Server Name
>> Information" (SNI) that says where you're going. Other sites don't
>> have any information until you complete the SSL handshake (how can
>> you filter a site name, until squid KNOWS the site or at least
>> domain name?).
>> 
>> If you're still wanting to go through with transparent (intercept)
>> proxy with SSL, search through the list for my SSL Deep dive
>> posts...that config is working for me so far (granted, not in an
>> enterprise environment). However, as Amos said,....if you choose
>> not to install the cert on the client machines, you are either a)
>> going to be out of luck on LOT'S of websites because they will fail
>> the SSL handshake, or b) teaching your users to ignore the security
>> warnings of their browser's....neither of which is a good thing.
>> 
>> Hope that helps.
>> 
>> James
>> 

Tom,

You are right...that absolutely will allow all SSL initially...the 
filtering is down lower in the config here:

With single list of regex sites/domains like \.google\.com...peek, 
splice, no bump...I'm currently using this config section.
############################################################################
ssl_bump peek step1 all
ssl_bump peek step2 all
acl allowed_https_sites ssl::server_name_regex 
"/opt/etc/squid/http_url.txt"
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate all


With broken acl list of networks list 208.85.40.0/21
###########################################################################
ssl_bump peek step1 broken
ssl_bump peek step2 broken
ssl_bump splice broken
ssl_bump peek step1 all
ssl_bump peek step2 all
acl allowed_https_sites ssl::server_name_regex 
"/opt/etc/squid/http_url.txt"
ssl_bump bump allowed_https_sites
ssl_bump terminate all

In both configs above, the SNI and server names are checked, bounced off 
the http_url.txt list, and if the site/domain is NOT in the list the ssl 
session is terminated.  The big drag is, you won't be able to see that 
in the squid logs.  I have a bug open ( I don't remember the number :( ) 
to show this in the logs...so far in my setup I only see the first peek, 
nothing after that.  You can test the above setups with:

openssl s_client -connect x.x.x.x:443

The above will test with no SNI...these look like the below in the logs:
Jun 24 11:35:08 gateway (squid-1): 192.168.1.101 - - 
[24/Jun/2015:11:35:08 -0600] "CONNECT 31.13.76.101:443 HTTP/1.1" - - 200 
0 TAG_NONE:ORIGINAL_DST peek

wget -d --ca-certificate=<your.cert.file)

The above WILL send an SNI...which you should see in your logs as:
Jun 24 12:01:44 gateway (squid-1): 192.168.1.101 - - 
[24/Jun/2015:12:01:44 -0600] "CONNECT 172.230.156.79:443 HTTP/1.1" 
device-api.urbanairship.com - 200 0 TAG_NONE:ORIGINAL_DST peek

Hope that helps.

James


From ht at inf.ed.ac.uk  Wed Jun 24 18:28:33 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Wed, 24 Jun 2015 19:28:33 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 (squid-users-request@lists.squid-cache.org's message of "Wed\,
 24 Jun 2015 18\:18\:32 +0000")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
Message-ID: <f5by4j9hsfi.fsf@troutbeck.inf.ed.ac.uk>

I've searched the documentation and mailing list archives w/o success,
and am not competent to read the source, so asking here: what is
logged as the 'remotehost' in Squid logs when a request that has been
encapsulated, as in from a machine on a local network behind a router
implementing NAT, or from a machine accessing the proxy via a VPN
connection?

Thanks,

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From leolistas at solutti.com.br  Wed Jun 24 19:48:28 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 24 Jun 2015 16:48:28 -0300
Subject: [squid-users] Logging of 'indirect' requests,
 e.g. involving NAT or VPN
In-Reply-To: <f5by4j9hsfi.fsf@troutbeck.inf.ed.ac.uk>
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <f5by4j9hsfi.fsf@troutbeck.inf.ed.ac.uk>
Message-ID: <558B098C.8030905@solutti.com.br>

Em 24/06/15 15:28, Henry S. Thompson escreveu:
> I've searched the documentation and mailing list archives w/o success,
> and am not competent to read the source, so asking here: what is
> logged as the 'remotehost' in Squid logs when a request that has been
> encapsulated, as in from a machine on a local network behind a router
> implementing NAT, or from a machine accessing the proxy via a VPN
> connection?
>
>

     logs will show the IP address that reached squid, ie. the source 
address of the connection. If that was NATted, squid will never know 
(and thus is not able to log) the original address before the NAT.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From yvoinov at gmail.com  Wed Jun 24 19:48:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 25 Jun 2015 01:48:16 +0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <558AE115.6080205@treenet.co.nz>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <558AD42A.30705@gmail.com> <558AE115.6080205@treenet.co.nz>
Message-ID: <558B0980.40102@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Amos,

we are don't care about experts in the IETF.

What is the Squid Team position about SSL bumping and caching? Will
Squid be only content filtering proxy or remains caheable? What will be
next milestone?

3.5. now less used to cache SSL, only 3.4 series, you ceased to maintain
and develop. Moreover, 3.5 still can't bump in NAT transparent
interception (for my patform, for example).

My point is that I now have to make a decision on choosing a caching
proxy for industrial platforms. What should I choose? 3.4? Wait for
version 4, or take an active part in the final design 3.5, which is
still not suitable for productive operation? Then to find out that it is
unable to cache 90% of the traffic?

This is key questions and I have no answer yet.

WBR, Yuri
24.06.15 22:55, Amos Jeffries ?????:
> Though efforts are underway to convince the browser people to fix their
> lack of TLS-to-proxy for security on http:// and cacheable DRM-style
> crypto for just the payload of messages, etc. Once they accept that the
> bogus arguments about http:// being "insecure" disappear.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJViwl/AAoJENNXIZxhPexGp84IAITVXB55JpGcEOVJHSozJgdk
YuzZ9Z03zmT7rWdDH7B1203QtrBHLEKt7xjb9Ys8srOifZlvmz/ke3a5pSY8Dnr6
T6D4WYMgcWsNxiQdL1Am55fxqx1/zQD3HW01mWE/s43isc8fN5dYpa2n4pFbEZZE
rXw3y4outYka/+7VtyUg8PzSeCCQZeGK/vM3uMguTs8jwA0RMvhbOJeE8oiXVWJe
Tsticbv/f/VNfUL7HzeZiFBWBLCAAZu7AjEimvNYNpeulS//KBtl8X7QsBSP+vwc
YK4Mg5g68bppOIjtUBOJEF1IIjFC4us7BUyWztdk2hNd3eLTLwPmkRDm1ly/B2s=
=3faa
-----END PGP SIGNATURE-----




From alex at samad.com.au  Thu Jun 25 00:45:20 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 25 Jun 2015 10:45:20 +1000
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
 <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
Message-ID: <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>

Hi

why this, doesn't this block all traffic getting to the squid port.
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP


what I would do to test is run tcpdump on the squid box and capture
all traffic coming to it on the squid listening port, then go to a
test machine on the eth or wireless and do a telnet google.com 80 and
see what you get on the squid box.

make sure you src and dst addresses are right. then check the squid logs.

I presume you get get to the internet from the squid box ?



On 24 June 2015 at 22:30, Dalmar <maamule10 at gmail.com> wrote:
> squid 3.3.8 and ubuntu 15.04 server
>
> 2015-06-24 15:04 GMT+03:00 Yuri Voinov <yvoinov at gmail.com>:
>>
>> Squid 3.5.x?
>>
>> 24.06.15 18:03, Dalmar ?????:
>>
>> Hi,
>> For over two weeks i am having a really headache in configuring squid
>> transparent/intercept.
>> I have tried different options and configurations but i couldn't get it to
>> work.
>> i think the problems lies in the Iptables / NAT but i really couldn't
>> solve it.
>> I have tried different iptable rules including the intercept linuxDnat -
>> sysctl configuration, but didnt work.
>>
>> # your proxy IP
>> SQUIDIP=X.X.X.X
>>
>> # your proxy listening port
>> SQUIDPORT=XXXX
>>
>>
>> iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
>> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination
>> $SQUIDIP:$SQUIDPORT
>> iptables -t nat -A POSTROUTING -j MASQUERADE
>> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>>
>>
>> i have to say that squid works well when i configure in the client
>> browsers.
>>
>> at the mikrotik side, i am using DST-NAT chain port 80 pro TCP action
>> DST-NAT to address squidIP and Port
>>
>> i am using ubuntu server 15.04 using squid 3.3.8 and this is my
>> configuration and the errors i get:
>>
>>
>>                         ------ eth0 WAN <----- MAIN WAN Public IP Internet
>>                  MK---|
>>                            ------ eth1 LAN
>>                           |
>>                    ------ eth2 Proxy
>>
>>
>>          ------ eth0 WAN ---> Public IP --> Internet --> gets internet
>> from 24online / another Mikrotik
>>        Squid---|
>>                         ------ eth1 Proxy
>>        |
>>         ------ eth2 webmin --> For server Management
>>
>>
>> -error1: if no intercept/transparent and no iptables is configured
>> -Invalid URL -  The requested url could not be retrieved
>> -but if proxy is configured in the user browser - it works!
>>
>>
>> -error2:if intercept and iptable DNAT is configured
>> -Access Denied and in the access log TCP-MISS/403
>> -no forward proxy port configured
>>         -security alert : host header forgery detected on local=
>> SquidIP:8080 remote:mikrotikIP (local ip does not match any domain name)
>>         -warning : forwarding loop detected (x-Forwarded-for mikrotik lan
>> IP)
>>
>> squid.conf
>>
>> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl CONNECT method CONNECT
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> http_access allow localnet
>> http_access allow localhost
>> http_access deny all
>> http_port 8080
>> http_port 8181
>> cache_mem 2000 MB
>> cache_dir ufs /var/spool/squid3 100000 16 256
>> coredump_dir /var/spool/squid3
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> refresh_pattern . 0 20% 4320
>> cache_effective_user proxy
>> cache_effective_group proxy
>>
>> ----------------------------------------
>> I am really confused, can anyone guide me please.
>> Thanks in advance
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From Jason_Haar at trimble.com  Thu Jun 25 01:57:38 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 25 Jun 2015 13:57:38 +1200
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
 <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
Message-ID: <558B6012.2020706@trimble.com>

On 25/06/15 06:05, James Lay wrote:
> openssl s_client -connect x.x.x.x:443 
Just a FYI but you can make openssl do SNI which helps debugging (ie
doing it your way and then doing it with SNI)

openssl s_client -connect x.x.x.x:443 -servername www.site.name

(that will allow squid to see www.site.name as the SNI)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From hectorchan at gmail.com  Thu Jun 25 04:48:11 2015
From: hectorchan at gmail.com (Hector Chan)
Date: Wed, 24 Jun 2015 21:48:11 -0700
Subject: [squid-users] TCP_MISS/503
In-Reply-To: <1435112713523-4671864.post@n4.nabble.com>
References: <1435112692501-4671863.post@n4.nabble.com>
 <1435112713523-4671864.post@n4.nabble.com>
Message-ID: <CAEhCwUxwCaFgorApyqVnShiVzq4in0V+b8ubWjfRB1vwYvN_Zg@mail.gmail.com>

Not sure if this will help you, but I saw 503s on my squid when the origin
server has an invalid SSL certificate -- expired cert, self-signed cert,
etc.

On Tue, Jun 23, 2015 at 7:25 PM, HackXBack <hack.back at hotmail.com> wrote:

> The requested URL could not be retrieved
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-503-tp4671863p4671864.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/a991c6af/attachment.htm>

From squid at hwse3.com  Thu Jun 25 06:49:42 2015
From: squid at hwse3.com (YogiBearNL aka Ronald)
Date: Thu, 25 Jun 2015 07:49:42 +0100
Subject: [squid-users] =?utf-8?q?Squid_3=2E1_access=5Flog_and_log_module_s?=
 =?utf-8?q?yslog_sets_program-name_as_=28squid=29?=
Message-ID: <10a659b7d4710df77cbe365405eebd0e@www.4dorp.nl>

 

Dear Squid users, 

I have a problem with Squid 3.1 on Debian
Squeeze. 

squid3 -v
Squid Cache: Version 3.1.6 

When I use the syslog
Log module for access_log the syslog lines have a funky program name
called (squid) i.s.o. squid.
This is different from syslog lines of
Squid v2. ( Squid Cache: Version 2.7.STABLE9 ).
I will provide an
example here:

Squid v2.7: 

Jun 25 08:36:37 proxy SQUID[16271]:
192.168.2.85 - - [25/Jun/2015:08:36:37 +0200] "GET
http://tpc.googlesyndication.com/safeframe/1-0-2/html/container.html
HTTP/1.1" 200 2439 "http://tweakers.net/" "Mozilla/5.0 (Macintosh; Intel
Mac OS X 10_8_0) AppleWebKit/400.5.3 (KHTML, like Gecko) Version/5.2.3
Safari/427.8.5" TCP_MISS:DIRECT 

Squid v3.1.6: 

Jun 24 21:47:56 proxy
(SQUID): 192.168.2.85 - - [24/Jun/2015:21:47:56 +0200] "GET
http://cdn.viglink.com/images/pixel.gif? HTTP/1.1" 200 639
"http://www.zdnet.com/blog/central-europe/" "Mozilla/5.0 (Macintosh;
Intel Mac OS X 10_8_0) AppleWebKit/400.5.3 (KHTML, like Gecko)
Version/5.2.3 Safari/427.8.5" TCP_MISS:DIRECT 

When I try to parse the
syslog lines, the ones with the (squid) as a program name fail because
there are not normal syslog lines.
Why is this happening ? And is this
fixed in a later release ? Or maybe it's some configuration problem
?

squid.conf (interesting parts only) 

logformat combined %>a %ui %un
[%tl] "%rm %ru HTTP/%rv" %>Hs %<st "%{Referer}>h" "%{User-Agent}>h"
%Ss:%Sh
access_log syslog:local7 combined 

I've googled around and some
other guy had the same issue:
http://serverdown.ttwait.com/que/410957


Thanks,

Ronald 

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150625/24515666/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 25 09:07:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2015 21:07:49 +1200
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
 <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
 <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>
Message-ID: <558BC4E5.6070209@treenet.co.nz>

On 25/06/2015 12:45 p.m., Alex Samad wrote:
> Hi
> 
> why this, doesn't this block all traffic getting to the squid port.
> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP

All external traffic yes. The NAT interception happens afterward and works.

The point is that NAT intercept MUST only be done directly on the Squid
machine. A single external connection being accepted will result in a
forwarding loop DoS and the above protects against that.

> 
> 
> what I would do to test is run tcpdump on the squid box and capture
> all traffic coming to it on the squid listening port,

IIRC, you can't do that because tcpdump operates before NAT. It will not
show you the NAT'ed traffic arriving.

Running Squid with -X or "debug_options ALL,9" would be better. You can
see in cache.log what Squid is receiving and what the NAT de-mangling is
actually doing.

Amos


From squid3 at treenet.co.nz  Thu Jun 25 10:08:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2015 22:08:18 +1200
Subject: [squid-users] TCP_MISS/503
In-Reply-To: <CAEhCwUxwCaFgorApyqVnShiVzq4in0V+b8ubWjfRB1vwYvN_Zg@mail.gmail.com>
References: <1435112692501-4671863.post@n4.nabble.com>
 <1435112713523-4671864.post@n4.nabble.com>
 <CAEhCwUxwCaFgorApyqVnShiVzq4in0V+b8ubWjfRB1vwYvN_Zg@mail.gmail.com>
Message-ID: <558BD312.7070904@treenet.co.nz>

On 25/06/2015 4:48 p.m., Hector Chan wrote:
> Not sure if this will help you, but I saw 503s on my squid when the origin
> server has an invalid SSL certificate -- expired cert, self-signed cert,
> etc.
> 

Nod. They show up whenever Squid cannot successfully connect to the
server. Thats what "503 Service Unavailable" means - unable to connect
to server. And there are a huge amount of reasons that may happen.
Anything that breaks the server connection encryption during SSL-bump
would do it.

Amos



From jlay at slave-tothe-box.net  Thu Jun 25 11:00:38 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 25 Jun 2015 05:00:38 -0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <558B6012.2020706@trimble.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
 <4d81af3a2e72c81d15faf54761bdb1a8@localhost> <558B6012.2020706@trimble.com>
Message-ID: <1435230038.3610.0.camel@JamesiMac>

On Thu, 2015-06-25 at 13:57 +1200, Jason Haar wrote:

> On 25/06/15 06:05, James Lay wrote:
> > openssl s_client -connect x.x.x.x:443 
> Just a FYI but you can make openssl do SNI which helps debugging (ie
> doing it your way and then doing it with SNI)
> 
> openssl s_client -connect x.x.x.x:443 -servername www.site.name
> 
> (that will allow squid to see www.site.name as the SNI)
> 


Thanks Jason....appreciate that heads up.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150625/c2864554/attachment.htm>

From tmowbray at dalabs.com  Thu Jun 25 12:06:42 2015
From: tmowbray at dalabs.com (Tom Mowbray)
Date: Thu, 25 Jun 2015 08:06:42 -0400
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
	and ssl_bump
In-Reply-To: <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
 <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
Message-ID: <CAM3U7EyVsRV+Mvnz995q9MXWkAdzug3KPiXDVSzFVqpt1ToEDw@mail.gmail.com>

James,

Thank for for your help.  Now that I have a better understanding of how the
https traffic is handled, I've been able to get things working as intended.


---------------------------------
Tom Mowbray
*tmowbray at dalabs.com* <tmowbray at dalabs.com>
*703-829-6694*

On Wed, Jun 24, 2015 at 2:05 PM, James Lay <jlay at slave-tothe-box.net> wrote:

> On 2015-06-24 11:46 AM, Tom Mowbray wrote:
>
>> James,
>>
>> Yes, as a matter of fact I have read through those exact posts and
>> modeled my config very similarly.  What I have found is that, however,
>> when the line "http_access allow SSL_ports" is placed above the
>> ssl_bump stuff and other acl's (as you have it), it seems to simply
>> allow ALL https without doing any filtering whatsoever.
>>
>> Thanks for the response.
>>
>> ---------------------------------Tom Mowbray
>> _tmowbray at dalabs.com_
>> _703-829-6694_
>>
>>
>> On Wed, Jun 24, 2015 at 1:31 PM, James Lay <jlay at slave-tothe-box.net>
>> wrote:
>>
>>  On 2015-06-24 09:41 AM, Tom Mowbray wrote:
>>>
>>>  Squid 3.5.5
>>>>
>>>> I seem to have some confusion about how acl lists are processed
>>>> in
>>>> squid.conf regarding the handling of SSL (HTTPS) traffic,
>>>> attempting
>>>> to use ssl_bump directives with transparent proxy.
>>>>
>>>> Based on available documentation, I believe my squid.conf is
>>>> correct,
>>>> however it never seems to actually behave as expected.
>>>>
>>>> I define the SSL port, as usual:
>>>>
>>>> acl SSL_ports port 443
>>>>
>>>> But here's where my confusion lies... Many state to place the
>>>> following line above the ssl_bump configuration lines:
>>>>
>>>> http_access allow SSL_ports
>>>>
>>>> However when I do this, it appears to simply stop processing any
>>>> other
>>>> rules and allows ALL https traffic through the proxy (which is
>>>> actually how I'd expect a standard ACL list to operate, but then
>>>> how
>>>> do I actually filter the traffic though our content-based ACL
>>>> lists?).
>>>> If I put the above line below the ssl_bump configuration options
>>>> in
>>>> my squid.conf, then it appears to BUMP all, even though I've told
>>>> the
>>>> config to SPLICE all https traffic, which doesn't work for our
>>>> deployment.
>>>>
>>>> So, does squid actually continue to process the https traffic
>>>> using
>>>> the ssl_bump rules if the "http_access allow SSL_ports" line is
>>>> placed
>>>> above it in the configuration?
>>>>
>>>> I should note that we've been able to get filtering to work
>>>> correctly
>>>> when using our configuration in NON-transparent mode, however our
>>>> goal
>>>> is get this functionality working as a transparent proxy. We're
>>>> unable to load our self-signed cert onto client machines that
>>>> will be
>>>> accessing the proxy, so using the "bump" or man-in-the-middle
>>>> style
>>>> https filtering isn't a viable option for us.
>>>>
>>>> Any help or advice is appreciated!
>>>>
>>>> Thanks,
>>>>
>>>> Tom
>>>>
>>>
>>> Tom,
>>>
>>> You kinda have to change the way you think about filtering when it
>>> comes to Squid 3.5.5 and SSL(TLS). Normal http traffic is
>>> easy....here's where we're trying to go and here's a list of place
>>> we're alloed to go...simple.
>>>
>>> Not so with SSL(TLS). Squid can't filter, since Squid may or may
>>> not know where we're going...and that's the issue..it's where those
>>> ssl_bump atStep ACL's come in. Some sites when you connect to them
>>> are easy-ish..when you connect your device sends a "Server Name
>>> Information" (SNI) that says where you're going. Other sites don't
>>> have any information until you complete the SSL handshake (how can
>>> you filter a site name, until squid KNOWS the site or at least
>>> domain name?).
>>>
>>> If you're still wanting to go through with transparent (intercept)
>>> proxy with SSL, search through the list for my SSL Deep dive
>>> posts...that config is working for me so far (granted, not in an
>>> enterprise environment). However, as Amos said,....if you choose
>>> not to install the cert on the client machines, you are either a)
>>> going to be out of luck on LOT'S of websites because they will fail
>>> the SSL handshake, or b) teaching your users to ignore the security
>>> warnings of their browser's....neither of which is a good thing.
>>>
>>> Hope that helps.
>>>
>>> James
>>>
>>>
> Tom,
>
> You are right...that absolutely will allow all SSL initially...the
> filtering is down lower in the config here:
>
> With single list of regex sites/domains like \.google\.com...peek, splice,
> no bump...I'm currently using this config section.
>
> ############################################################################
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate all
>
>
> With broken acl list of networks list 208.85.40.0/21
> ###########################################################################
> ssl_bump peek step1 broken
> ssl_bump peek step2 broken
> ssl_bump splice broken
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump bump allowed_https_sites
> ssl_bump terminate all
>
> In both configs above, the SNI and server names are checked, bounced off
> the http_url.txt list, and if the site/domain is NOT in the list the ssl
> session is terminated.  The big drag is, you won't be able to see that in
> the squid logs.  I have a bug open ( I don't remember the number :( ) to
> show this in the logs...so far in my setup I only see the first peek,
> nothing after that.  You can test the above setups with:
>
> openssl s_client -connect x.x.x.x:443
>
> The above will test with no SNI...these look like the below in the logs:
> Jun 24 11:35:08 gateway (squid-1): 192.168.1.101 - - [24/Jun/2015:11:35:08
> -0600] "CONNECT 31.13.76.101:443 HTTP/1.1" - - 200 0
> TAG_NONE:ORIGINAL_DST peek
>
> wget -d --ca-certificate=<your.cert.file)
>
> The above WILL send an SNI...which you should see in your logs as:
> Jun 24 12:01:44 gateway (squid-1): 192.168.1.101 - - [24/Jun/2015:12:01:44
> -0600] "CONNECT 172.230.156.79:443 HTTP/1.1" device-api.urbanairship.com
> - 200 0 TAG_NONE:ORIGINAL_DST peek
>
> Hope that helps.
>
> James
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150625/5f671d4a/attachment.htm>

From eliezer at ngtech.co.il  Thu Jun 25 12:09:47 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 25 Jun 2015 15:09:47 +0300
Subject: [squid-users] I was wondering if someone has ever tried to use a
 SAN\NAS as the cache backend?
Message-ID: <558BEF8B.30205@ngtech.co.il>

Hello list,

I was wondering if someone has ever tried to use a SAN\NAS as the cache 
backend?
Since rock cache type\dir changed the file handling way from "lots of 
files db" into a single(and one more) cache db There is surly a way to 
benefit from nas and SAN.

If someone have used san(ISCSI) or nas(NFS) for any of the cahed dirs 
type I would like to run some tests and you can help me not repeat old 
tests resolts.

Thanks,
Eliezer



From jlay at slave-tothe-box.net  Thu Jun 25 12:15:50 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 25 Jun 2015 06:15:50 -0600
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7EyVsRV+Mvnz995q9MXWkAdzug3KPiXDVSzFVqpt1ToEDw@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
 <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
 <CAM3U7EyVsRV+Mvnz995q9MXWkAdzug3KPiXDVSzFVqpt1ToEDw@mail.gmail.com>
Message-ID: <1435234550.3610.6.camel@JamesiMac>

On Thu, 2015-06-25 at 08:06 -0400, Tom Mowbray wrote:
> James,
> 
> 
> 
> Thank for for your help.  Now that I have a better understanding of
> how the https traffic is handled, I've been able to get things working
> as intended.
> 
> 
> 
> 
> 
> ---------------------------------
> 
> Tom Mowbray
> 
> tmowbray at dalabs.com
> 703-829-6694
> 
> 
> 
> On Wed, Jun 24, 2015 at 2:05 PM, James Lay <jlay at slave-tothe-box.net>
> wrote:
> 
>         On 2015-06-24 11:46 AM, Tom Mowbray wrote:
>         
>                 James,
>                 
>                 Yes, as a matter of fact I have read through those
>                 exact posts and
>                 modeled my config very similarly.  What I have found
>                 is that, however,
>                 when the line "http_access allow SSL_ports" is placed
>                 above the
>                 ssl_bump stuff and other acl's (as you have it), it
>                 seems to simply
>                 allow ALL https without doing any filtering
>                 whatsoever.
>                 
>                 Thanks for the response.
>                 
>                 ---------------------------------Tom Mowbray
>                 _tmowbray at dalabs.com_
>                 _703-829-6694_
>                 
>                 
>                 
>                 On Wed, Jun 24, 2015 at 1:31 PM, James Lay
>                 <jlay at slave-tothe-box.net>
>                 wrote:
>                 
>                 
>                         On 2015-06-24 09:41 AM, Tom Mowbray wrote:
>                         
>                         
>                                 Squid 3.5.5
>                                 
>                                 I seem to have some confusion about
>                                 how acl lists are processed
>                                 in
>                                 squid.conf regarding the handling of
>                                 SSL (HTTPS) traffic,
>                                 attempting
>                                 to use ssl_bump directives with
>                                 transparent proxy.
>                                 
>                                 Based on available documentation, I
>                                 believe my squid.conf is
>                                 correct,
>                                 however it never seems to actually
>                                 behave as expected.
>                                 
>                                 I define the SSL port, as usual:
>                                 
>                                 acl SSL_ports port 443
>                                 
>                                 But here's where my confusion lies...
>                                 Many state to place the
>                                 following line above the ssl_bump
>                                 configuration lines:
>                                 
>                                 http_access allow SSL_ports
>                                 
>                                 However when I do this, it appears to
>                                 simply stop processing any
>                                 other
>                                 rules and allows ALL https traffic
>                                 through the proxy (which is
>                                 actually how I'd expect a standard ACL
>                                 list to operate, but then
>                                 how
>                                 do I actually filter the traffic
>                                 though our content-based ACL
>                                 lists?).
>                                 If I put the above line below the
>                                 ssl_bump configuration options
>                                 in
>                                 my squid.conf, then it appears to BUMP
>                                 all, even though I've told
>                                 the
>                                 config to SPLICE all https traffic,
>                                 which doesn't work for our
>                                 deployment.
>                                 
>                                 So, does squid actually continue to
>                                 process the https traffic
>                                 using
>                                 the ssl_bump rules if the "http_access
>                                 allow SSL_ports" line is
>                                 placed
>                                 above it in the configuration?
>                                 
>                                 I should note that we've been able to
>                                 get filtering to work
>                                 correctly
>                                 when using our configuration in
>                                 NON-transparent mode, however our
>                                 goal
>                                 is get this functionality working as a
>                                 transparent proxy. We're
>                                 unable to load our self-signed cert
>                                 onto client machines that
>                                 will be
>                                 accessing the proxy, so using the
>                                 "bump" or man-in-the-middle
>                                 style
>                                 https filtering isn't a viable option
>                                 for us.
>                                 
>                                 Any help or advice is appreciated!
>                                 
>                                 Thanks,
>                                 
>                                 Tom
>                         
>                         
>                         Tom,
>                         
>                         You kinda have to change the way you think
>                         about filtering when it
>                         comes to Squid 3.5.5 and SSL(TLS). Normal http
>                         traffic is
>                         easy....here's where we're trying to go and
>                         here's a list of place
>                         we're alloed to go...simple.
>                         
>                         Not so with SSL(TLS). Squid can't filter,
>                         since Squid may or may
>                         not know where we're going...and that's the
>                         issue..it's where those
>                         ssl_bump atStep ACL's come in. Some sites when
>                         you connect to them
>                         are easy-ish..when you connect your device
>                         sends a "Server Name
>                         Information" (SNI) that says where you're
>                         going. Other sites don't
>                         have any information until you complete the
>                         SSL handshake (how can
>                         you filter a site name, until squid KNOWS the
>                         site or at least
>                         domain name?).
>                         
>                         If you're still wanting to go through with
>                         transparent (intercept)
>                         proxy with SSL, search through the list for my
>                         SSL Deep dive
>                         posts...that config is working for me so far
>                         (granted, not in an
>                         enterprise environment). However, as Amos
>                         said,....if you choose
>                         not to install the cert on the client
>                         machines, you are either a)
>                         going to be out of luck on LOT'S of websites
>                         because they will fail
>                         the SSL handshake, or b) teaching your users
>                         to ignore the security
>                         warnings of their browser's....neither of
>                         which is a good thing.
>                         
>                         Hope that helps.
>                         
>                         James
>                         
>         
>         
>         Tom,
>         
>         You are right...that absolutely will allow all SSL
>         initially...the filtering is down lower in the config here:
>         
>         With single list of regex sites/domains like \.google
>         \.com...peek, splice, no bump...I'm currently using this
>         config section.
>         ############################################################################
>         ssl_bump peek step1 all
>         ssl_bump peek step2 all
>         acl allowed_https_sites ssl::server_name_regex
>         "/opt/etc/squid/http_url.txt"
>         ssl_bump splice step3 allowed_https_sites
>         ssl_bump terminate all
>         
>         
>         With broken acl list of networks list 208.85.40.0/21
>         ###########################################################################
>         ssl_bump peek step1 broken
>         ssl_bump peek step2 broken
>         ssl_bump splice broken
>         ssl_bump peek step1 all
>         ssl_bump peek step2 all
>         acl allowed_https_sites ssl::server_name_regex
>         "/opt/etc/squid/http_url.txt"
>         ssl_bump bump allowed_https_sites
>         ssl_bump terminate all
>         
>         In both configs above, the SNI and server names are checked,
>         bounced off the http_url.txt list, and if the site/domain is
>         NOT in the list the ssl session is terminated.  The big drag
>         is, you won't be able to see that in the squid logs.  I have a
>         bug open ( I don't remember the number :( ) to show this in
>         the logs...so far in my setup I only see the first peek,
>         nothing after that.  You can test the above setups with:
>         
>         openssl s_client -connect x.x.x.x:443
>         
>         The above will test with no SNI...these look like the below in
>         the logs:
>         Jun 24 11:35:08 gateway (squid-1): 192.168.1.101 - -
>         [24/Jun/2015:11:35:08 -0600] "CONNECT 31.13.76.101:443
>         HTTP/1.1" - - 200 0 TAG_NONE:ORIGINAL_DST peek
>         
>         wget -d --ca-certificate=<your.cert.file)
>         
>         The above WILL send an SNI...which you should see in your logs
>         as:
>         Jun 24 12:01:44 gateway (squid-1): 192.168.1.101 - -
>         [24/Jun/2015:12:01:44 -0600] "CONNECT 172.230.156.79:443
>         HTTP/1.1" device-api.urbanairship.com - 200 0
>         TAG_NONE:ORIGINAL_DST peek
>         
>         Hope that helps.
>         
>         James
> 
> 
> 

Excellent!

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150625/79e44813/attachment.htm>

From kl at vsen.dk  Thu Jun 25 12:16:23 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 25 Jun 2015 14:16:23 +0200
Subject: [squid-users] Questions Regarding Transparent Proxy, HTTPS,
 and ssl_bump
In-Reply-To: <CAM3U7EyVsRV+Mvnz995q9MXWkAdzug3KPiXDVSzFVqpt1ToEDw@mail.gmail.com>
References: <CAM3U7Exq-1o+vaQvQYwJUQe+tHuBvyvaSPQ_qnSh4Ypx2PeW4g@mail.gmail.com>
 <f7b351c914319a5b8226ccd73759e05d@localhost>
 <CAM3U7EyAm01oC_4yZeFo5ktfCPFe5UCnnRp66cvDcLvz9yOWiA@mail.gmail.com>
 <4d81af3a2e72c81d15faf54761bdb1a8@localhost>
 <CAM3U7EyVsRV+Mvnz995q9MXWkAdzug3KPiXDVSzFVqpt1ToEDw@mail.gmail.com>
Message-ID: <558BF117.5070105@vsen.dk>

Hi Tom,

How did you succeed in filtering https traffic? using http_access.. or 
the way James did it, using domainname only ?

Tom Mowbray wrote on 06/25/2015 02:06 PM:
> James,
>
> Thank for for your help.  Now that I have a better understanding of how
> the https traffic is handled, I've been able to get things working as
> intended.
>
>
> ---------------------------------
> Tom Mowbray
> /tmowbray at dalabs.com/ <mailto:tmowbray at dalabs.com>
> /703-829-6694/
>
> On Wed, Jun 24, 2015 at 2:05 PM, James Lay <jlay at slave-tothe-box.net
> <mailto:jlay at slave-tothe-box.net>> wrote:
>
>     On 2015-06-24 11:46 AM, Tom Mowbray wrote:
>
>         James,
>
>         Yes, as a matter of fact I have read through those exact posts and
>         modeled my config very similarly.  What I have found is that,
>         however,
>         when the line "http_access allow SSL_ports" is placed above the
>         ssl_bump stuff and other acl's (as you have it), it seems to simply
>         allow ALL https without doing any filtering whatsoever.
>
>         Thanks for the response.
>
>         ---------------------------------Tom Mowbray
>         _tmowbray at dalabs.com_
>         _703-829-6694 <tel:703-829-6694>_
>
>
>         On Wed, Jun 24, 2015 at 1:31 PM, James Lay
>         <jlay at slave-tothe-box.net <mailto:jlay at slave-tothe-box.net>>
>         wrote:
>
>             On 2015-06-24 09:41 AM, Tom Mowbray wrote:
>
>                 Squid 3.5.5
>
>                 I seem to have some confusion about how acl lists are
>                 processed
>                 in
>                 squid.conf regarding the handling of SSL (HTTPS) traffic,
>                 attempting
>                 to use ssl_bump directives with transparent proxy.
>
>                 Based on available documentation, I believe my squid.conf is
>                 correct,
>                 however it never seems to actually behave as expected.
>
>                 I define the SSL port, as usual:
>
>                 acl SSL_ports port 443
>
>                 But here's where my confusion lies... Many state to
>                 place the
>                 following line above the ssl_bump configuration lines:
>
>                 http_access allow SSL_ports
>
>                 However when I do this, it appears to simply stop
>                 processing any
>                 other
>                 rules and allows ALL https traffic through the proxy
>                 (which is
>                 actually how I'd expect a standard ACL list to operate,
>                 but then
>                 how
>                 do I actually filter the traffic though our
>                 content-based ACL
>                 lists?).
>                 If I put the above line below the ssl_bump configuration
>                 options
>                 in
>                 my squid.conf, then it appears to BUMP all, even though
>                 I've told
>                 the
>                 config to SPLICE all https traffic, which doesn't work
>                 for our
>                 deployment.
>
>                 So, does squid actually continue to process the https
>                 traffic
>                 using
>                 the ssl_bump rules if the "http_access allow SSL_ports"
>                 line is
>                 placed
>                 above it in the configuration?
>
>                 I should note that we've been able to get filtering to work
>                 correctly
>                 when using our configuration in NON-transparent mode,
>                 however our
>                 goal
>                 is get this functionality working as a transparent
>                 proxy. We're
>                 unable to load our self-signed cert onto client machines
>                 that
>                 will be
>                 accessing the proxy, so using the "bump" or
>                 man-in-the-middle
>                 style
>                 https filtering isn't a viable option for us.
>
>                 Any help or advice is appreciated!
>
>                 Thanks,
>
>                 Tom
>
>
>             Tom,
>
>             You kinda have to change the way you think about filtering
>             when it
>             comes to Squid 3.5.5 and SSL(TLS). Normal http traffic is
>             easy....here's where we're trying to go and here's a list of
>             place
>             we're alloed to go...simple.
>
>             Not so with SSL(TLS). Squid can't filter, since Squid may or may
>             not know where we're going...and that's the issue..it's
>             where those
>             ssl_bump atStep ACL's come in. Some sites when you connect
>             to them
>             are easy-ish..when you connect your device sends a "Server Name
>             Information" (SNI) that says where you're going. Other sites
>             don't
>             have any information until you complete the SSL handshake
>             (how can
>             you filter a site name, until squid KNOWS the site or at least
>             domain name?).
>
>             If you're still wanting to go through with transparent
>             (intercept)
>             proxy with SSL, search through the list for my SSL Deep dive
>             posts...that config is working for me so far (granted, not in an
>             enterprise environment). However, as Amos said,....if you choose
>             not to install the cert on the client machines, you are
>             either a)
>             going to be out of luck on LOT'S of websites because they
>             will fail
>             the SSL handshake, or b) teaching your users to ignore the
>             security
>             warnings of their browser's....neither of which is a good thing.
>
>             Hope that helps.
>
>             James
>
>
>     Tom,
>
>     You are right...that absolutely will allow all SSL initially...the
>     filtering is down lower in the config here:
>
>     With single list of regex sites/domains like \.google\.com...peek,
>     splice, no bump...I'm currently using this config section.
>     ############################################################################
>     ssl_bump peek step1 all
>     ssl_bump peek step2 all
>     acl allowed_https_sites ssl::server_name_regex
>     "/opt/etc/squid/http_url.txt"
>     ssl_bump splice step3 allowed_https_sites
>     ssl_bump terminate all
>
>
>     With broken acl list of networks list 208.85.40.0/21
>     <http://208.85.40.0/21>
>     ###########################################################################
>     ssl_bump peek step1 broken
>     ssl_bump peek step2 broken
>     ssl_bump splice broken
>     ssl_bump peek step1 all
>     ssl_bump peek step2 all
>     acl allowed_https_sites ssl::server_name_regex
>     "/opt/etc/squid/http_url.txt"
>     ssl_bump bump allowed_https_sites
>     ssl_bump terminate all
>
>     In both configs above, the SNI and server names are checked, bounced
>     off the http_url.txt list, and if the site/domain is NOT in the list
>     the ssl session is terminated.  The big drag is, you won't be able
>     to see that in the squid logs.  I have a bug open ( I don't remember
>     the number :( ) to show this in the logs...so far in my setup I only
>     see the first peek, nothing after that.  You can test the above
>     setups with:
>
>     openssl s_client -connect x.x.x.x:443
>
>     The above will test with no SNI...these look like the below in the logs:
>     Jun 24 11:35:08 gateway (squid-1): 192.168.1.101 - -
>     [24/Jun/2015:11:35:08 -0600] "CONNECT 31.13.76.101:443
>     <http://31.13.76.101:443> HTTP/1.1" - - 200 0 TAG_NONE:ORIGINAL_DST peek
>
>     wget -d --ca-certificate=<your.cert.file)
>
>     The above WILL send an SNI...which you should see in your logs as:
>     Jun 24 12:01:44 gateway (squid-1): 192.168.1.101 - -
>     [24/Jun/2015:12:01:44 -0600] "CONNECT 172.230.156.79:443
>     <http://172.230.156.79:443> HTTP/1.1" device-api.urbanairship.com
>     <http://device-api.urbanairship.com> - 200 0 TAG_NONE:ORIGINAL_DST peek
>
>     Hope that helps.
>
>     James
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From gkinkie at gmail.com  Thu Jun 25 12:37:32 2015
From: gkinkie at gmail.com (Kinkie)
Date: Thu, 25 Jun 2015 14:37:32 +0200
Subject: [squid-users] I was wondering if someone has ever tried to use
 a SAN\NAS as the cache backend?
In-Reply-To: <558BEF8B.30205@ngtech.co.il>
References: <558BEF8B.30205@ngtech.co.il>
Message-ID: <CA+Y8hcMq-20uZOjfFCPxuz8kcL8K41oCXFA40YYH4T=GrYfVHA@mail.gmail.com>

Hi Eliezer,
  it depends.
The problem is not the NAS/SAN per se, but the disk access patterns.
Squid's disk access pattern, regardless the technology, is always
randomly-timed 4kb writes (in case of Rock, they are sequential, in
*ufs scattered).
If the NAS/SAN uses a write-back policy, it is possible that by the
time it decides to flush to disk, squid has written to a full stripe
and everyone will be happy (except for RAID5 or 10); this is
relatively likely in case of Rock, unlikely in case of *ufs.
But every time a write is not stripe-aligned, the NAS/SAN will have to
read and write N stripes (N >= 2 depending on the type of RAID). This
is a bit  suboptimal for the NAS/SAN in case of Rock, but it will
likely hurt the SAN/NAS performance in case of *ufs.

In case the SAN/NAS policy is not write-back but write-through, any
option (including rock) will adversely affect the SAN/NAS performance.

On Thu, Jun 25, 2015 at 2:09 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hello list,
>
> I was wondering if someone has ever tried to use a SAN\NAS as the cache
> backend?
> Since rock cache type\dir changed the file handling way from "lots of files
> db" into a single(and one more) cache db There is surly a way to benefit
> from nas and SAN.
>
> If someone have used san(ISCSI) or nas(NFS) for any of the cahed dirs type I
> would like to run some tests and you can help me not repeat old tests
> resolts.
>
> Thanks,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From mcsnv96 at afo.net  Thu Jun 25 14:36:18 2015
From: mcsnv96 at afo.net (Mike)
Date: Thu, 25 Jun 2015 09:36:18 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <558A964C.9070208@treenet.co.nz>
References: <5589E5C0.4070307@afo.net> <558A964C.9070208@treenet.co.nz>
Message-ID: <558C11E2.7040909@afo.net>

Amos, thanks for info.

The primary settings being used in squid.conf:

http_port 8080
# this port is what will be used for SSL Proxy on client browser
http_port 8081 intercept

https_port 8082 intercept ssl-bump connection-auth=off 
generate-host-certificates=on dynamic_cert_mem_cache_size=16MB 
cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key 
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
sslcrtd_children 50 startup=5 idle=1
ssl_bump server-first all
ssl_bump none localhost


Then e2guardian uses 10101 for the browsers, and uses 8080 for 
connecting to squid on the same server.

Yet what is happening is there is the GET, then CONNECT and the tunnel 
is created, never allowing squid to decrypt and pass the data along to 
e2guardian, I suspect Google has changed their settings denying any 
proxy from intercepting, because we can type the most foul terms which 
are in the "bannedssllist" for e2guardian and literally nothing is 
filtered at all on google, nor youtube. Yet other secure sites like 
wordpress, yahoo, and others are caught and blocked, so it is just 
google owned sites that are not.

More below...


On 6/24/2015 6:36 AM, Amos Jeffries wrote:
> On 24/06/2015 11:03 a.m., Mike wrote:
>> We have a server setup using squid 3.5 and e2guardian (newer branch of
>> dansguardian), the issue is now google has changed a few things around
>> and google is no longer filtered which is not acceptable. We already
>> have the browser settings for SSL Proxy set to our server, and squid has
>> ssl-bump enabled and working. Previously there was enough unsecure
>> content on Google that the filtering was still working, but now google
>> has gone 100% encrypted meaning it is 100% unfiltered.
> Maybe, maybe not.
>
>> What is happening
>> is it is creating an ssl tunnel (for lack of a better term) between
> No. That is the correct and official term for what they are doing. And
> "CONNECT tunnel" is the full phrase / name for the particular method of
> tunnel creation.
>
>
>> their server and the browser, so all squid sees is the connection to
>> www.google.com, and after that it is tunneled and not recognized by
>> squid or e2guardian at all.
> BUT ... you said you were SSL-Bump'ing. Which means you are decrypting
> such tunnels to filter the content inside them.
>
> So what is the problem? is your method of bumping not decrypting the
> Google traffic for Squid access controls and helpers to filter?
>
> Note that DansGuardian and e2guardian being independent HTTP proxies are
> not party to that SSL-Bump decrypted content inside Squid. ONly Squid
> internals and ICAP/eCAP services have access to it.
>
>> I found a few options online that was used with older squid versions but
>> nothing is working with squid 3.5... Looking for something like this:
>>
>> acl google dstdomain .google.com
>> deny_info http://www.google.com/webhp?nord=1 google
> As you said Google have gone 100% HTTPS. URLs beginning with http:// are
> not HTTPS nor accepted there anymore. If used they just get a 30x
> redirect to an https:// URL.
>
> Amos
This is why we are thinking we can force the redirect, if you have ides 
on how to do that. All google pages use the secure aspect, except when 
that http://www.google.com/webhp?nord=1 is used, it forces use of the 
insecure pages, and allows e2guardian filtering to work properly.

Thank you,

Mike

> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150625/d08f8c8c/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 25 14:38:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jun 2015 02:38:14 +1200
Subject: [squid-users] Squid 3.1 access_log and log module syslog sets
 program-name as (squid)
In-Reply-To: <10a659b7d4710df77cbe365405eebd0e@www.4dorp.nl>
References: <10a659b7d4710df77cbe365405eebd0e@www.4dorp.nl>
Message-ID: <558C1256.7000307@treenet.co.nz>

On 25/06/2015 6:49 p.m., YogiBearNL aka Ronald wrote:
> Squid v2.7: 
> 
> Jun 25 08:36:37 proxy SQUID[16271]:
> 192.168.2.85 - - [25/Jun/2015:08:36:37 +0200] "GET
> http://tpc.googlesyndication.com/safeframe/1-0-2/html/container.html
> HTTP/1.1" 200 2439 "http://tweakers.net/" "Mozilla/5.0 (Macintosh; Intel
> Mac OS X 10_8_0) AppleWebKit/400.5.3 (KHTML, like Gecko) Version/5.2.3
> Safari/427.8.5" TCP_MISS:DIRECT 
> 
> Squid v3.1.6: 
> 
> Jun 24 21:47:56 proxy
> (SQUID): 192.168.2.85 - - [24/Jun/2015:21:47:56 +0200] "GET
> http://cdn.viglink.com/images/pixel.gif? HTTP/1.1" 200 639
> "http://www.zdnet.com/blog/central-europe/" "Mozilla/5.0 (Macintosh;
> Intel Mac OS X 10_8_0) AppleWebKit/400.5.3 (KHTML, like Gecko)
> Version/5.2.3 Safari/427.8.5" TCP_MISS:DIRECT 
> 
> When I try to parse the
> syslog lines, the ones with the (squid) as a program name fail because
> there are not normal syslog lines.
> Why is this happening ? And is this
> fixed in a later release ? Or maybe it's some configuration problem
> ?

Squid (both versions) is using the OS syslog() API to deliver these log
entries. The bits up to and inluding the '(SQUID):' and 'SQUID[16271]:'
are all generated by the syslog kernel daemon.

This is weird output, but I think its due to a change in the syslog
application.

Amos



From squid at visolve.com  Fri Jun 26 04:36:05 2015
From: squid at visolve.com (Squid List)
Date: Fri, 26 Jun 2015 10:06:05 +0530
Subject: [squid-users] Reg - Squid can cache the chrome OS updates.
In-Reply-To: <558CCF53.8060500@visolve.com>
References: <558CCF53.8060500@visolve.com>
Message-ID: <558CD6B5.7020009@visolve.com>

Hi,

Is the Squid can cache Microsoft Updates and IOS Updates?

If its cache means, please help me out for cache Chrome OS updates in 
latest squid version that is installed in CentOS 6.6.


Thanks & Regards,
Nithi



From squid at visolve.com  Fri Jun 26 04:36:05 2015
From: squid at visolve.com (Squid List)
Date: Fri, 26 Jun 2015 10:06:05 +0530
Subject: [squid-users] Reg - Squid can cache the chrome OS updates.
In-Reply-To: <558CCF53.8060500@visolve.com>
References: <558CCF53.8060500@visolve.com>
Message-ID: <558CD6B5.7020009@visolve.com>

Hi,

Is the Squid can cache Microsoft Updates and IOS Updates?

If its cache means, please help me out for cache Chrome OS updates in 
latest squid version that is installed in CentOS 6.6.


Thanks & Regards,
Nithi



From squid3 at treenet.co.nz  Fri Jun 26 05:18:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jun 2015 17:18:39 +1200
Subject: [squid-users] Reg - Squid can cache the chrome OS updates.
In-Reply-To: <558CD6B5.7020009@visolve.com>
References: <558CCF53.8060500@visolve.com> <558CD6B5.7020009@visolve.com>
Message-ID: <558CE0AF.1030806@treenet.co.nz>

On 26/06/2015 4:36 p.m., Squid List wrote:
> Hi,
> 
> Is the Squid can cache Microsoft Updates and IOS Updates?
> 
> If its cache means, please help me out for cache Chrome OS updates in
> latest squid version that is installed in CentOS 6.6.

The short answer (FWIW):

Squid can (and does) cache any HTTP content which is cacheable. With the
exception of 206 responses and PUT request payloads.


The long answer:

Whether the cached content is used depends entirely on what the client
requests. It has the power to request that cached content be ignored.

Whether content is cacheable depends entirely on what the server
delivers. It has the power to place limits on cache times up to and
including stating an object is already stale (ie not usefully cached).

There are also some mechanisms which when used MAY make content
completely untrustworthy or and uncacheable:
* connection based authentication (NTLM, Negotiate)
* traffic interception (NAT, TPROXY, SSL-Bump)
* broken Vary headers (though this causes caching when it shouldn't)
*


I hope that explains why you wont get a clear simple answer to your
question.

To help any further we will need information about;
- what Squid version you are using (if its not the latest 3.5 please try
an upgrade),
- how its configured (squid.conf without the comment lines please),
- how its being used (explicit forward-, reverse-, or interception proxy)
- what exactly the request messages you are trying to make into HITs are
("debug_options 11,2" produces a traces of those),
- what response messages the server is delivering on the MISS (the same
11,2 trace)
- what Squid is logging for them (access.log entries)

Amos



From squid3 at treenet.co.nz  Fri Jun 26 05:29:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jun 2015 17:29:56 +1200
Subject: [squid-users] acl for redirect
In-Reply-To: <558C11E2.7040909@afo.net>
References: <5589E5C0.4070307@afo.net> <558A964C.9070208@treenet.co.nz>
 <558C11E2.7040909@afo.net>
Message-ID: <558CE354.9000304@treenet.co.nz>

On 26/06/2015 2:36 a.m., Mike wrote:
> Amos, thanks for info.
> 
> The primary settings being used in squid.conf:
> 
> http_port 8080
> # this port is what will be used for SSL Proxy on client browser
> http_port 8081 intercept
> 
> https_port 8082 intercept ssl-bump connection-auth=off
> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
> 
> 
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
> sslcrtd_children 50 startup=5 idle=1
> ssl_bump server-first all
> ssl_bump none localhost
> 
> 
> Then e2guardian uses 10101 for the browsers, and uses 8080 for
> connecting to squid on the same server.

Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
connection in re-encrypted on outgoing.


I am doubtful eth nord works anymore since Googles own documentation for
schools states that one must install a MITM proxy that does the traffic
filtering - e2guardian is not one of those. IMO you should convert your
e2guardian config into Squid ACL rules that can be applied to the bumped
traffic without forcing http://

But if nord does work, so should the deny_info in Squid. Something like
this probably:

 acl google dstdomain .google.com
 deny_info 301:http://%H%R?nord=1 google

 acl GwithQuery urlpath_regex ?
 deny_info 301:http://%H%R&nord=1 GwithQuery

 http_access deny google Gquery
 http_access deny google


Amos


From fredbmail at free.fr  Fri Jun 26 08:40:10 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 26 Jun 2015 10:40:10 +0200 (CEST)
Subject: [squid-users] acl for redirect
In-Reply-To: <558CE354.9000304@treenet.co.nz>
Message-ID: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>

Mike, you can also to try the dev branch https://github.com/e2guardian/e2guardian/tree/develop 
SSLMITM works now. The request from the client is intercepted, a spoofed certificate supplied for 
the target site and an encrypted connection made back to the client.  
A separate encrypted connection to the target server is set up.  The resulting 
http dencrypted stream is then filtered as normal.

https://github.com/e2guardian/e2guardian/blob/develop/notes/ssl_mitm

Fred


From ht at inf.ed.ac.uk  Fri Jun 26 08:51:23 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Fri, 26 Jun 2015 09:51:23 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <558B098C.8030905@solutti.com.br> (Leonardo Rodrigues's message
 of "Wed\, 24 Jun 2015 16\:48\:28 -0300")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <f5by4j9hsfi.fsf@troutbeck.inf.ed.ac.uk>
 <558B098C.8030905@solutti.com.br>
Message-ID: <f5bbng2ettg.fsf@troutbeck.inf.ed.ac.uk>

Leonardo Rodrigues writes:

> Em 24/06/15 15:28, Henry S. Thompson escreveu:
>> I've searched the documentation and mailing list archives w/o success,
>> and am not competent to read the source, so asking here: what is
>> logged as the 'remotehost' in Squid logs when a request that has been
>> encapsulated, as in from a machine on a local network behind a router
>> implementing NAT, or from a machine accessing the proxy via a VPN
>> connection?
>
>     logs will show the IP address that reached squid, ie. the source
> address of the connection. If that was NATted, squid will never know
> (and thus is not able to log) the original address before the NAT.

That's what I assumed, but in a log I've been working with for
research purposes, 192.168.... turns up -- how is this possible given
what you say?

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From Antony.Stone at squid.open.source.it  Fri Jun 26 08:58:51 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Jun 2015 09:58:51 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <f5bbng2ettg.fsf@troutbeck.inf.ed.ac.uk>
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <558B098C.8030905@solutti.com.br> <f5bbng2ettg.fsf@troutbeck.inf.ed.ac.uk>
Message-ID: <201506260958.51988.Antony.Stone@squid.open.source.it>

On Friday 26 Jun 2015 at 09:51, Henry S. Thompson wrote:

> >     logs will show the IP address that reached squid, ie. the source
> > address of the connection. If that was NATted, squid will never know
> > (and thus is not able to log) the original address before the NAT.
> 
> That's what I assumed, but in a log I've been working with for
> research purposes, 192.168.... turns up -- how is this possible given
> what you say?

It's entirely plausible (I'd even say common) for VPN clients to get 
192.168.... addresses; also if there's a NATting router in the path and Squid 
is logging its address, that could easily be 192.168....

I'd say your best way of working out what's happening is to pick such an 
address you see (frequently?) in the log files, and ask whoever's network this 
is what machine that address belongs to.


Hope that helps,


Antony.

-- 
There are two possible outcomes:

 If the result confirms the hypothesis, then you've made a measurement.
 If the result is contrary to the hypothesis, then you've made a discovery.

 - Enrico Fermi

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ht at inf.ed.ac.uk  Fri Jun 26 09:42:49 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Fri, 26 Jun 2015 10:42:49 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <201506260958.51988.Antony.Stone@squid.open.source.it> (Antony
 Stone's message of "Fri\, 26 Jun 2015 09\:58\:51 +0100")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <558B098C.8030905@solutti.com.br>
 <f5bbng2ettg.fsf@troutbeck.inf.ed.ac.uk>
 <201506260958.51988.Antony.Stone@squid.open.source.it>
Message-ID: <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>

Antony Stone writes:

> On Friday 26 Jun 2015 at 09:51, Henry S. Thompson wrote:
>
>> >     logs will show the IP address that reached squid, ie. the source
>> > address of the connection. If that was NATted, squid will never know
>> > (and thus is not able to log) the original address before the NAT.
>> 
>> That's what I assumed, but in a log I've been working with for
>> research purposes, 192.168.... turns up -- how is this possible given
>> what you say?
>
> It's entirely plausible (I'd even say common) for VPN clients to get
> 192.168.... addresses; also if there's a NATting router in the path
> and Squid is logging its address, that could easily be 192.168....

Thanks for your input, but I'm still confused.  My (perhaps naive)
understanding was that a VPN host or NATting router assigns local
subnet range IPs (e.g. 192.168... or 10.10...) to its clients, but
presents their traffic to the world, including any proxy, as if from
themselves, encapsulated using their own public, static, 'real' IP.
So I don't see how, for example "a NATting router['s] ... address"
could ever be 192.168...

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From ht at inf.ed.ac.uk  Fri Jun 26 09:42:49 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Fri, 26 Jun 2015 10:42:49 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <201506260958.51988.Antony.Stone@squid.open.source.it> (Antony
 Stone's message of "Fri\, 26 Jun 2015 09\:58\:51 +0100")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <558B098C.8030905@solutti.com.br>
 <f5bbng2ettg.fsf@troutbeck.inf.ed.ac.uk>
 <201506260958.51988.Antony.Stone@squid.open.source.it>
Message-ID: <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>

Antony Stone writes:

> On Friday 26 Jun 2015 at 09:51, Henry S. Thompson wrote:
>
>> >     logs will show the IP address that reached squid, ie. the source
>> > address of the connection. If that was NATted, squid will never know
>> > (and thus is not able to log) the original address before the NAT.
>> 
>> That's what I assumed, but in a log I've been working with for
>> research purposes, 192.168.... turns up -- how is this possible given
>> what you say?
>
> It's entirely plausible (I'd even say common) for VPN clients to get
> 192.168.... addresses; also if there's a NATting router in the path
> and Squid is logging its address, that could easily be 192.168....

Thanks for your input, but I'm still confused.  My (perhaps naive)
understanding was that a VPN host or NATting router assigns local
subnet range IPs (e.g. 192.168... or 10.10...) to its clients, but
presents their traffic to the world, including any proxy, as if from
themselves, encapsulated using their own public, static, 'real' IP.
So I don't see how, for example "a NATting router['s] ... address"
could ever be 192.168...

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From Antony.Stone at squid.open.source.it  Fri Jun 26 10:02:06 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Jun 2015 11:02:06 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <201506260958.51988.Antony.Stone@squid.open.source.it>
 <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>
Message-ID: <201506261102.06752.Antony.Stone@squid.open.source.it>

On Friday 26 Jun 2015 at 10:42, Henry S. Thompson wrote:

> Antony Stone writes:
> > 
> > It's entirely plausible (I'd even say common) for VPN clients to get
> > 192.168.... addresses; also if there's a NATting router in the path
> > and Squid is logging its address, that could easily be 192.168....
> 
> Thanks for your input, but I'm still confused.  My (perhaps naive)
> understanding was that a VPN host or NATting router assigns local
> subnet range IPs (e.g. 192.168... or 10.10...) to its clients, but
> presents their traffic to the world, including any proxy, as if from
> themselves, encapsulated using their own public, static, 'real' IP.
> So I don't see how, for example "a NATting router['s] ... address"
> could ever be 192.168...

Imagine the following setup:

Organisation has a bunch of servers (maybe at their office in a server room, 
maybe in a data centre, doesn't matter which), some of which have public IPs, 
but all of which have private IPs on an internal subnet (for system management 
purposes, aside from anything else).  One of these servers is the squid proxy.  
Another server is the VPN endpoint for remote client machines.

Remote client connects to public IP of the VPN server, gets assigned a 
192.168.x.y address.  Remote client is configured to use the Squid proxy 
server.  When it does so, its request (from 192.168.x.y) is routed from the 
VPN endpoint to the Squid server (they can talk directly to each other because 
they're both on the same subnet, no NAT involved) and the Squid server then 
sends the request out to the Internet to fetch a web page.

The client IP logged by the Squid server in this scenario is 192.168.x.y


Alternatively, imagine the organisation has several office locations 
interconnected using MPLS or some similar private connectivity (ie: not over 
the Internet, or tunneled if it is over the Internet - the end result either 
way being that each office has a 192.168.a.0/24 subnet for its clients).

One of the offices has a Squid server and a connection to the Internet; 
connections from clients at the other offices go over the private links to 
this office, via Squid, to the Internet.

Again, in this setup Squid will see the true IP address of the clients, ie: 
192.168.a.b because that's the only address the clients have, and with direct 
interconnects there's no need for NATting to a public IP along the way.


I repeat my recommendation - pick one of the 192.168.m.n addresses you're 
seeing in the log files and ask whoever looks after this network which machine 
has that address (or at least, what that subnet range is used for) - I think 
it's going to turn out to be one of:

a) a real client in something like the second scenario above
b) a VPN client in the first scenario above
c) an internal router in a variation of the second scenario above


Regards,


Antony.

-- 
You can spend the whole of your life trying to be popular,
but at the end of the day the size of the crowd at your funeral
will be largely dictated by the weather.

 - Frank Skinner

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid at visolve.com  Fri Jun 26 05:49:38 2015
From: squid at visolve.com (ViSolve Squid)
Date: Fri, 26 Jun 2015 11:19:38 +0530
Subject: [squid-users] Reg - Squid can cache the chrome OS updates.
In-Reply-To: <558CE0AF.1030806@treenet.co.nz>
References: <558CCF53.8060500@visolve.com> <558CD6B5.7020009@visolve.com>
 <558CE0AF.1030806@treenet.co.nz>
Message-ID: <558CE7F2.1020201@visolve.com>

Thanks for your valuable information Amos.

Regards,
Nithi

On Friday 26 June 2015 10:48 AM, Amos Jeffries wrote:
> On 26/06/2015 4:36 p.m., Squid List wrote:
>> Hi,
>>
>> Is the Squid can cache Microsoft Updates and IOS Updates?
>>
>> If its cache means, please help me out for cache Chrome OS updates in
>> latest squid version that is installed in CentOS 6.6.
> The short answer (FWIW):
>
> Squid can (and does) cache any HTTP content which is cacheable. With the
> exception of 206 responses and PUT request payloads.
>
>
> The long answer:
>
> Whether the cached content is used depends entirely on what the client
> requests. It has the power to request that cached content be ignored.
>
> Whether content is cacheable depends entirely on what the server
> delivers. It has the power to place limits on cache times up to and
> including stating an object is already stale (ie not usefully cached).
>
> There are also some mechanisms which when used MAY make content
> completely untrustworthy or and uncacheable:
> * connection based authentication (NTLM, Negotiate)
> * traffic interception (NAT, TPROXY, SSL-Bump)
> * broken Vary headers (though this causes caching when it shouldn't)
> *
>
>
> I hope that explains why you wont get a clear simple answer to your
> question.
>
> To help any further we will need information about;
> - what Squid version you are using (if its not the latest 3.5 please try
> an upgrade),
> - how its configured (squid.conf without the comment lines please),
> - how its being used (explicit forward-, reverse-, or interception proxy)
> - what exactly the request messages you are trying to make into HITs are
> ("debug_options 11,2" produces a traces of those),
> - what response messages the server is delivering on the MISS (the same
> 11,2 trace)
> - what Squid is logging for them (access.log entries)
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jun 26 10:12:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jun 2015 22:12:59 +1200
Subject: [squid-users] acl for redirect
In-Reply-To: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <558D25AB.8030802@treenet.co.nz>

On 26/06/2015 8:40 p.m., FredB wrote:
> Mike, you can also to try the dev branch https://github.com/e2guardian/e2guardian/tree/develop 
> SSLMITM works now. The request from the client is intercepted, a spoofed certificate supplied for 
> the target site and an encrypted connection made back to the client.  
> A separate encrypted connection to the target server is set up.  The resulting 
> http dencrypted stream is then filtered as normal.

If that order of operations is correct then the e2guardian dev have made
the same mistake we made back in Squid-3.2. client-first bumping opens a
huge security vulnerability - by hiding issues on the server connection
from the client it enables attackers to hijack the server connection
invisibly. This is the reason the more difficult to get working
server-first and peek-n-splice modes exist and are almost mandatory in
Squid today.

Amos



From fredbmail at free.fr  Fri Jun 26 11:10:45 2015
From: fredbmail at free.fr (FredB)
Date: Fri, 26 Jun 2015 13:10:45 +0200 (CEST)
Subject: [squid-users] acl for redirect
In-Reply-To: <558D25AB.8030802@treenet.co.nz>
Message-ID: <158076321.662171579.1435317045620.JavaMail.root@zimbra4-e1.priv.proxad.net>

Thanks Amos, I will discuss this in more details with the dev of SSLMITM in E2

Fred


From ht at inf.ed.ac.uk  Fri Jun 26 11:20:00 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Fri, 26 Jun 2015 12:20:00 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <201506261102.06752.Antony.Stone@squid.open.source.it> (Antony
 Stone's message of "Fri\, 26 Jun 2015 11\:02\:06 +0100")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <201506260958.51988.Antony.Stone@squid.open.source.it>
 <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>
 <201506261102.06752.Antony.Stone@squid.open.source.it>
Message-ID: <f5b8ub6d8db.fsf@troutbeck.inf.ed.ac.uk>

Antony Stone writes:

> Imagine the following setup:
>
> Organisation has a bunch of servers (maybe at their office in a
> server room, maybe in a data centre, doesn't matter which), some of
> which have public IPs, but all of which have private IPs on an
> internal subnet (for system management purposes, aside from anything
> else).  One of these servers is the squid proxy.  Another server is
> the VPN endpoint for remote client machines.

Got it, makes sense, thanks.

> Remote client connects to public IP of the VPN server, gets assigned a 
> 192.168.x.y address.  Remote client is configured to use the Squid proxy 
> server.  When it does so, its request (from 192.168.x.y) is routed from the 
> VPN endpoint to the Squid server (they can talk directly to each other because 
> they're both on the same subnet, no NAT involved) and the Squid server then 
> sends the request out to the Internet to fetch a web page.
>
> The client IP logged by the Squid server in this scenario is 192.168.x.y

Thanks, that helps a lot.

> I repeat my recommendation - pick one of the 192.168.m.n addresses
> you're seeing in the log files and ask whoever looks after this
> network which machine has that address (or at least, what that
> subnet range is used for)

Will do.

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From ht at inf.ed.ac.uk  Fri Jun 26 11:20:00 2015
From: ht at inf.ed.ac.uk (Henry S. Thompson)
Date: Fri, 26 Jun 2015 12:20:00 +0100
Subject: [squid-users] Logging of 'indirect' requests,
	e.g. involving NAT or VPN
In-Reply-To: <201506261102.06752.Antony.Stone@squid.open.source.it> (Antony
 Stone's message of "Fri\, 26 Jun 2015 11\:02\:06 +0100")
References: <mailman.1141.1435169912.2788.squid-users@lists.squid-cache.org>
 <201506260958.51988.Antony.Stone@squid.open.source.it>
 <f5b7fqqerfq.fsf@troutbeck.inf.ed.ac.uk>
 <201506261102.06752.Antony.Stone@squid.open.source.it>
Message-ID: <f5b8ub6d8db.fsf@troutbeck.inf.ed.ac.uk>

Antony Stone writes:

> Imagine the following setup:
>
> Organisation has a bunch of servers (maybe at their office in a
> server room, maybe in a data centre, doesn't matter which), some of
> which have public IPs, but all of which have private IPs on an
> internal subnet (for system management purposes, aside from anything
> else).  One of these servers is the squid proxy.  Another server is
> the VPN endpoint for remote client machines.

Got it, makes sense, thanks.

> Remote client connects to public IP of the VPN server, gets assigned a 
> 192.168.x.y address.  Remote client is configured to use the Squid proxy 
> server.  When it does so, its request (from 192.168.x.y) is routed from the 
> VPN endpoint to the Squid server (they can talk directly to each other because 
> they're both on the same subnet, no NAT involved) and the Squid server then 
> sends the request out to the Internet to fetch a web page.
>
> The client IP logged by the Squid server in this scenario is 192.168.x.y

Thanks, that helps a lot.

> I repeat my recommendation - pick one of the 192.168.m.n addresses
> you're seeing in the log files and ask whoever looks after this
> network which machine has that address (or at least, what that
> subnet range is used for)

Will do.

ht
-- 
       Henry S. Thompson, School of Informatics, University of Edinburgh
      10 Crichton Street, Edinburgh EH8 9AB, SCOTLAND -- (44) 131 650-4440
                Fax: (44) 131 650-4587, e-mail: ht at inf.ed.ac.uk
                       URL: http://www.ltg.ed.ac.uk/~ht/
 [mail from me _always_ has a .sig like this -- mail without it is forged spam]


From marcus.kool at urlfilterdb.com  Fri Jun 26 11:22:50 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 26 Jun 2015 08:22:50 -0300
Subject: [squid-users] ufdbGuard 1.31-13 released
Message-ID: <558D360A.1040808@urlfilterdb.com>

ufdbGuard, the fastest and free URL filter for Squid, has a new patch release.

Patch 13 resolves:
+ new installation procedure for Solaris 10 and 11 - with much appreciated help from Yuri Voinov
+ various overblocking/underblocking issues with complex ACLs
+ redirection of URLs with HTTPS on Squid 3.4+ sometimes failed
+ and 8 more minor fixes
See https://www.urlfilterdb.com/release-1.31.html for more information.

ufdbGuard is Open Source Software and can be downloaded at
http://sourceforge.net and https://www.urlfilterdb.com

ufdbGuard works with URL databases in plaintext format and with
the commercial database of URLfilterDB.

Consider replacing squidGuard with ufdbGuard since
squidGuard has no active development since 2010.

Marcus


From alex at samad.com.au  Fri Jun 26 12:14:52 2015
From: alex at samad.com.au (Alex Samad)
Date: Fri, 26 Jun 2015 22:14:52 +1000
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <558BC4E5.6070209@treenet.co.nz>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
 <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
 <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>
 <558BC4E5.6070209@treenet.co.nz>
Message-ID: <CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>

aren't squid and nat box different ? that was my presumption..

On 25 June 2015 at 19:07, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/06/2015 12:45 p.m., Alex Samad wrote:
>> Hi
>>
>> why this, doesn't this block all traffic getting to the squid port.
>> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>
> All external traffic yes. The NAT interception happens afterward and works.
>
> The point is that NAT intercept MUST only be done directly on the Squid
> machine. A single external connection being accepted will result in a
> forwarding loop DoS and the above protects against that.
>
>>
>>
>> what I would do to test is run tcpdump on the squid box and capture
>> all traffic coming to it on the squid listening port,
>
> IIRC, you can't do that because tcpdump operates before NAT. It will not
> show you the NAT'ed traffic arriving.
>
> Running Squid with -X or "debug_options ALL,9" would be better. You can
> see in cache.log what Squid is receiving and what the NAT de-mangling is
> actually doing.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Jun 26 12:49:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2015 00:49:46 +1200
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>	<558A9CB6.9000403@gmail.com>	<CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>	<CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>	<558BC4E5.6070209@treenet.co.nz>
 <CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>
Message-ID: <558D4A6A.4020704@treenet.co.nz>

On 27/06/2015 12:14 a.m., Alex Samad wrote:
> aren't squid and nat box different ? that was my presumption..
> 

Best not to.

The dst-IP:port on the TCP packets entering the Squid machine is where
Squid will send the outgoing server requests. If that dst-IP is the IP
of the Squid machine itself you get into big DoS-level trouble really fast.

Amos



From mcsnv96 at afo.net  Fri Jun 26 15:38:33 2015
From: mcsnv96 at afo.net (Mike)
Date: Fri, 26 Jun 2015 10:38:33 -0500
Subject: [squid-users] acl for redirect - re Fred
In-Reply-To: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <558D71F9.3060408@afo.net>

Yes we already have that version installed, that is the version having 
these issues.

[root at Server1 ~]# e2guardian -v
e2guardian 3.0.4


On 6/26/2015 3:40 AM, FredB wrote:
> Mike, you can also to try the dev branch https://github.com/e2guardian/e2guardian/tree/develop
> SSLMITM works now. The request from the client is intercepted, a spoofed certificate supplied for
> the target site and an encrypted connection made back to the client.
> A separate encrypted connection to the target server is set up.  The resulting
> http dencrypted stream is then filtered as normal.
>
> https://github.com/e2guardian/e2guardian/blob/develop/notes/ssl_mitm
>
> Fred
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From mcsnv96 at afo.net  Fri Jun 26 16:09:45 2015
From: mcsnv96 at afo.net (Mike)
Date: Fri, 26 Jun 2015 11:09:45 -0500
Subject: [squid-users] acl for redirect - re Amos
In-Reply-To: <558CE354.9000304@treenet.co.nz>
References: <5589E5C0.4070307@afo.net> <558A964C.9070208@treenet.co.nz>
 <558C11E2.7040909@afo.net> <558CE354.9000304@treenet.co.nz>
Message-ID: <558D7949.6090206@afo.net>

Amos,

I would like to use e2guardian if possible, and after checking it out, 
http://www.google.com/webhp?nord=1 does force the insecure, but previous 
entries attempted just cause all searches to loop back to that same url 
instead of passing it along.

We could use a regex option in squid, but since we want the rest of the 
sites to be handled normally through e2guardian, what acl entries would 
we use to set it up to only take effect on google.com? Essentially "if 
dstdomain = google.com then use acl blocklist /etc/squid/badwords".
I have not used a 2 layer or referring acl setup before, but before now 
never needed to.

Thank you so much for the help!

Mike


On 6/26/2015 0:29 AM, Amos Jeffries wrote:
> On 26/06/2015 2:36 a.m., Mike wrote:
>> Amos, thanks for info.
>>
>> The primary settings being used in squid.conf:
>>
>> http_port 8080
>> # this port is what will be used for SSL Proxy on client browser
>> http_port 8081 intercept
>>
>> https_port 8082 intercept ssl-bump connection-auth=off
>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
>> sslcrtd_children 50 startup=5 idle=1
>> ssl_bump server-first all
>> ssl_bump none localhost
>>
>>
>> Then e2guardian uses 10101 for the browsers, and uses 8080 for
>> connecting to squid on the same server.
> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
> connection in re-encrypted on outgoing.
>
>
> I am doubtful eth nord works anymore since Googles own documentation for
> schools states that one must install a MITM proxy that does the traffic
> filtering - e2guardian is not one of those. IMO you should convert your
> e2guardian config into Squid ACL rules that can be applied to the bumped
> traffic without forcing http://
>
> But if nord does work, so should the deny_info in Squid. Something like
> this probably:
>
>   acl google dstdomain .google.com
>   deny_info 301:http://%H%R?nord=1 google
>
>   acl GwithQuery urlpath_regex ?
>   deny_info 301:http://%H%R&nord=1 GwithQuery
>
>   http_access deny google Gquery
>   http_access deny google
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From mcsnv96 at afo.net  Fri Jun 26 18:29:25 2015
From: mcsnv96 at afo.net (Mike)
Date: Fri, 26 Jun 2015 13:29:25 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <558D25AB.8030802@treenet.co.nz>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz>
Message-ID: <558D9A05.60005@afo.net>

Nevermind... I found another fix within e2guardian:

etc/e2guardian/lists/urlregexplist

Added this entry:
# Disable Google SSL Search
# allows e2g to filter searches properly
"^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1"

This means whenever google.com or www.google.com is typed in the address 
bar, it loads the insecure page and allows e2guardian to properly filter 
whatever search terms they type in. This does break other aspects such 
as google toolbars, using the search bar at upper right of many browsers 
with google as the set search engine, and other ways, but that is an 
issue we can live with.


On 6/26/2015 5:12 AM, Amos Jeffries wrote:
> On 26/06/2015 8:40 p.m., FredB wrote:
>> Mike, you can also to try the dev branch https://github.com/e2guardian/e2guardian/tree/develop
>> SSLMITM works now. The request from the client is intercepted, a spoofed certificate supplied for
>> the target site and an encrypted connection made back to the client.
>> A separate encrypted connection to the target server is set up.  The resulting
>> http dencrypted stream is then filtered as normal.
> If that order of operations is correct then the e2guardian dev have made
> the same mistake we made back in Squid-3.2. client-first bumping opens a
> huge security vulnerability - by hiding issues on the server connection
> from the client it enables attackers to hijack the server connection
> invisibly. This is the reason the more difficult to get working
> server-first and peek-n-splice modes exist and are almost mandatory in
> Squid today.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From hack.back at hotmail.com  Fri Jun 26 21:29:35 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 26 Jun 2015 14:29:35 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55876E36.4050702@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz>
Message-ID: <1435354175872-4671919.post@n4.nabble.com>

acl Y-TUBE dstdomain .googlevideo.com
range_offset_limit -1 Y-TUBE

this conf make the assertion bug,
we need a solution 
am still waiting Amos
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671919.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alex at samad.com.au  Fri Jun 26 22:02:44 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 27 Jun 2015 08:02:44 +1000
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <558D4A6A.4020704@treenet.co.nz>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
 <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
 <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>
 <558BC4E5.6070209@treenet.co.nz>
 <CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>
 <558D4A6A.4020704@treenet.co.nz>
Message-ID: <CAJ+Q1PVi0aQzwqj27K_Fmq7WdPQGvrgxnTkgi87+Qx6xg+nxhA@mail.gmail.com>

Hi

Sorry missing something here.

I thought this was a mikrotek rtr , presumably acting as a default
gateway for the local lan to the internet.
it has a DNAT rule to capture all internet traffic that is port 80
(and presumably at some point in time port 443) and it DNATS it to the
SQUID box.

and there needs to be a special rule on the DGW to allow squid access
out to the internet with out resending it back to the squid and
creating a loop.

from memory thats how I used to do this. unless the DGW is large
enough to run squid, then DNAT to the local box and onto squid.

Why would there be a DoS for SQUID on another box, the only resources
I can think of is the NAT table, maybe conntrack

Alex



On 26 June 2015 at 22:49, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 27/06/2015 12:14 a.m., Alex Samad wrote:
>> aren't squid and nat box different ? that was my presumption..
>>
>
> Best not to.
>
> The dst-IP:port on the TCP packets entering the Squid machine is where
> Squid will send the outgoing server requests. If that dst-IP is the IP
> of the Squid machine itself you get into big DoS-level trouble really fast.
>
> Amos
>


From squid3 at treenet.co.nz  Sat Jun 27 06:33:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2015 18:33:03 +1200
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAJ+Q1PVi0aQzwqj27K_Fmq7WdPQGvrgxnTkgi87+Qx6xg+nxhA@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>	<558A9CB6.9000403@gmail.com>	<CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>	<CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>	<558BC4E5.6070209@treenet.co.nz>	<CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>	<558D4A6A.4020704@treenet.co.nz>
 <CAJ+Q1PVi0aQzwqj27K_Fmq7WdPQGvrgxnTkgi87+Qx6xg+nxhA@mail.gmail.com>
Message-ID: <558E439F.4090800@treenet.co.nz>

On 27/06/2015 10:02 a.m., Alex Samad wrote:
> Hi
> 
> Sorry missing something here.
> 
> I thought this was a mikrotek rtr , presumably acting as a default
> gateway for the local lan to the internet.
> it has a DNAT rule to capture all internet traffic that is port 80
> (and presumably at some point in time port 443) and it DNATS it to the
> SQUID box.
> 
> and there needs to be a special rule on the DGW to allow squid access
> out to the internet with out resending it back to the squid and
> creating a loop.
> 
> from memory thats how I used to do this. unless the DGW is large
> enough to run squid, then DNAT to the local box and onto squid.

Yes, a lot of people used to do it that way. The problem was
CVE-2009-0801 vulnerability allowed attackers script to send any request
to Squid claiming an arbitrary server Host: header and get that content
both delivered back as if it was to some other domain the client thought
it was connecting to and injected into Squid cache for other clients to
be affected by in the same way.

That is no longer permitted since Squid-3.2. The DNAT can only happen
once, and that must be on the Squid machine so Squid can lookup the NAT
tables and unmangle the original dst-IP.

You need to use routing rules on the Mikrotik (or tunnel sometimes works
too) to deliver the original client generated packet to the Squid
machine without NAT changing the dst-IP:port details (SNAT is fine, but
will cause lies about client IP in the access.log).

> 
> Why would there be a DoS for SQUID on another box, the only resources
> I can think of is the NAT table, maybe conntrack

Like I said earlier "The dst-IP:port on the TCP packets entering the
Squid machine is where Squid will send the outgoing server requests."

If you block forwarding loops the outbound requests from Squid get an
error page *always* because the outboudn traffic is going from Squid to
be served by Squid (forwarding loop).

If you disable the Via header forwarding loop protection Squid will just
loop until all TCP port numbers on the machine are consumed sending new
"outbound" connections that loop back Squid. Then no network connections
will be available to Squid or any other software. The RAM associated
with each connection may also be too much and cause the OS to
force-shutdown Squid.

So you get to pick between a DoS or a very nasty DoS.

Amos


From yvoinov at gmail.com  Sat Jun 27 06:34:37 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 27 Jun 2015 12:34:37 +0600
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1435354175872-4671919.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435354175872-4671919.post@n4.nabble.com>
Message-ID: <558E43FD.9030003@gmail.com>

Better to use:

# Adobe/Java and other updates
acl adobe_java_updates urlpath_regex "/usr/local/squid/etc/urlregex.updates"

# Youtube & CDN store rewrite ACLs
acl store_rewrite_list urlpath_regex 
\/(watch\?|get_video|videoplayback\?) 
\.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|f4f|mp4)\? \/ads\?
acl store_rewrite_list_web url_regex 
^https?:\/\/([A-Za-z-]+[0-9]+)*\.[A-Za-z]*\.[A-Za-z]* 
^https?:\/\/(khms|mt)[0-9]+\.google\.[a-z\.]+\/.*
acl store_rewrite_list_path urlpath_regex 
\.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|f4f|mp4|ttf)$
acl store_rewrite_list_web_CDN url_regex 
^https?:\/\/[a-z]+[0-9]\.google\.com doubleclick\.net akamaihd\.net

range_offset_limit none adobe_java_updates store_rewrite_list 
store_rewrite_list_web store_rewrite_list_path store_rewrite_list_web_CDN

Also be sure half_closed_clients parameter is default.


27.06.15 3:29, HackXBack ?????:
> acl Y-TUBE dstdomain .googlevideo.com
> range_offset_limit -1 Y-TUBE
>
> this conf make the assertion bug,
> we need a solution
> am still waiting Amos
> Thanks.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671919.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From alex at samad.com.au  Sat Jun 27 07:48:19 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 27 Jun 2015 17:48:19 +1000
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <558E439F.4090800@treenet.co.nz>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>
 <558A9CB6.9000403@gmail.com>
 <CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>
 <CAJ+Q1PV_ptXTJkzwX9=vvh2QPsKoEDAOra9+Nx9YtFM0X6-u8g@mail.gmail.com>
 <558BC4E5.6070209@treenet.co.nz>
 <CAJ+Q1PVrtrznk6brxK8Wbgp6B4Pic2N=gG-GSKL3KiWZ7w49YQ@mail.gmail.com>
 <558D4A6A.4020704@treenet.co.nz>
 <CAJ+Q1PVi0aQzwqj27K_Fmq7WdPQGvrgxnTkgi87+Qx6xg+nxhA@mail.gmail.com>
 <558E439F.4090800@treenet.co.nz>
Message-ID: <CAJ+Q1PVB-rA5d_=ZXMvKd+AFaEGOmLaBT-nZ99iTTUo==DjZoQ@mail.gmail.com>

On 27 June 2015 at 16:33, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 27/06/2015 10:02 a.m., Alex Samad wrote:
>> Hi
>>
>> Sorry missing something here.
>>
>> I thought this was a mikrotek rtr , presumably acting as a default
>> gateway for the local lan to the internet.
>> it has a DNAT rule to capture all internet traffic that is port 80
>> (and presumably at some point in time port 443) and it DNATS it to the
>> SQUID box.
>>
>> and there needs to be a special rule on the DGW to allow squid access
>> out to the internet with out resending it back to the squid and
>> creating a loop.
>>
>> from memory thats how I used to do this. unless the DGW is large
>> enough to run squid, then DNAT to the local box and onto squid.
>
> Yes, a lot of people used to do it that way. The problem was
> CVE-2009-0801 vulnerability allowed attackers script to send any request
> to Squid claiming an arbitrary server Host: header and get that content
> both delivered back as if it was to some other domain the client thought
> it was connecting to and injected into Squid cache for other clients to
> be affected by in the same way.
>
> That is no longer permitted since Squid-3.2. The DNAT can only happen
> once, and that must be on the Squid machine so Squid can lookup the NAT
> tables and unmangle the original dst-IP.
>
> You need to use routing rules on the Mikrotik (or tunnel sometimes works
> too) to deliver the original client generated packet to the Squid
> machine without NAT changing the dst-IP:port details (SNAT is fine, but
> will cause lies about client IP in the access.log).

Okay good to know.

Alex


From hack.back at hotmail.com  Sat Jun 27 08:00:30 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 27 Jun 2015 01:00:30 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <558E43FD.9030003@gmail.com>
References: <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435354175872-4671919.post@n4.nabble.com>
 <558E43FD.9030003@gmail.com>
Message-ID: <1435392030245-4671924.post@n4.nabble.com>

dear friend, your conf make the same problem, and i dont have
half_closed_clients in my conf !
and this is my configure option maybe the problem from it ...

./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--with-pthreads  --with-aufs-threads=512 --enable-storeio=ufs,aufs
--enable-removal-policies=lru,heap --with-aio --with-dl --disable-icmp
--enable-icap-client --disable-wccp --enable-wccpv2 --enable-cache-digests
--enable-http-violations --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-zph-qos --with-default-user=proxy
--with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid
--with-swapdir=/var/spool/squid --enable-ltdl-convenience
--with-filedescriptors=65536 --enable-ssl --enable-ssl-crtd --with-openssl
--enable-snmp --disable-auth --disable-ipv6 --enable-arp-acl --enable-epoll
--enable-referer-log --enable-truncate --disable-unlinkd
--enable-useragent-log --enable-eui --enable-large-cache-files
'CFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'CXXFLAGS=-march=native -mtune=native -pipe -DNUMTHREADS=512'
'LDFLAGS=-Wl,--no-as-needed -ldl' 'CPPFLAGS=-I/usr/include/openssl'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671924.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From truniger at bluewin.ch  Sat Jun 27 12:33:54 2015
From: truniger at bluewin.ch (Othmar Truniger)
Date: Sat, 27 Jun 2015 14:33:54 +0200
Subject: [squid-users] cannot use squid-3.5.x for production
Message-ID: <000001d0b0d5$8b0d4650$a127d2f0$@bluewin.ch>

Hello,
I found I cannot use squid-3.5.x for production because a possible bug I
filed on http://bugs.squid-cache.org/show_bug.cgi?id=4279 recently.
Unfortunately nobody reacted on that one yet.

The problem: when trying to FTP-download a non-existing file the
client-http-connection just hangs forever. The external FTP communication
works fine but squid misses to send back a http-response with code 404.
Instead the connection just stays open.
I have several jobs in production polling for files on external FTP servers
which work fine with 3.4.x but cannot be used anymore on 3.5.x

Would be nice if someone could confirm this problem and even better if
someone has a valid 3.5.x workaround for this.

Regards, Othmar




From packetlord at gmail.com  Sun Jun 28 10:18:21 2015
From: packetlord at gmail.com (JP)
Date: Sun, 28 Jun 2015 10:18:21 +0000
Subject: [squid-users] SSL Bumping CONNECT With A cache_peer
Message-ID: <CAPBWTD3O24spMyDsSDoxhD1HYK9oJMxtg4io_FQekL74Jv5NeQ@mail.gmail.com>

Hello all.

I tried reading all the FAQ's and scoured the rest of the internet for any
configuration examples I can find and I have not seen a working solution
for this.
I have been using squid for a couple of years now to bump SSL traffic with
no issues.
However I have a new environment where an upstream proxy is already in
place and MUST be used.
So I am trying to get squid working with SSL bump where I have to use a
cache_peer.
So here's the environment.

Normal network setup:
Client --> Forefront Threat Manager Gateway/Proxy (TMG) --> Internet
Client is setup to use TMG:8080 to get to internet for all protocols.

Here's my new network chain with squid inserted:
Moving forward, I will abbreviate the Forefront proxy as "TMG"

Client --> Squid 3.5.5 --> TMG --> Internet
And then I set the client to use squid:3128

The problem is the CONNECT tunnel.
Scenario:
Under normal circumstances, the following takes place for a standard
request:
GET http://www.arin.net
This results in a 301 redirect to https://www.arin.net
The client then immediately sends CONNECT www.arin.net:443 to TMG:8080
And the connection is made. Perfectly normal.

Currently, I have configured the client to use squid:3128 for all protocols
How do I bump the CONNECT tunnel?
I have been trying to manipulate the configuration file for days with no
success.
I have settled on the configuration below for now because it allows
unimpeded network traffic.
It does not bump any SSL.
Client is still set to use port 3128 for all protocols, Setting it to use
3129 for https was a failure.

I've tried:
https_port 3128 & 3129
http_port 3128 & 3129
various ssl_bump directives to include peek, stare, bump.
Nothing works.

Here's my usual failure in /var/log/access.log:
1435482419.334    194 192.168.25.2 TCP_MISS/301 616 GET http://www.arin.net/
- FIRSTUP_PARENT/10.210.4.103 text/html
1435482419.337      0 192.168.25.2 TAG_NONE/409 4324 CONNECT
www.arin.net:443 - HIER_NONE/- text/html

After reviewing many packet captures... I think the problem is this:
In order for squid to establish a connection via SSL to arin, squid would
have to send a CONNECT request through the cache_peer.
Squid "never" sends a CONNECT www.arin.net:443 to the cache_peer in any
configuration I have tried, unless, I leave the configuration the way it is
below.
Of course, since the client is only talking to squid on port 3128, no SSL
bumping takes place.
The minute I try to bump port 3128 all SSL stops working.

Again... I can get this to work in 2 minutes with no problems if I didn't
have to also speak to a cache_peer.


Contents of /etc/squid/squid.conf :
acl localnet src 192.168.25.0/24        # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

cache_effective_user proxy
forwarded_for delete

http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost

http_access deny all

never_direct allow all
cache_peer 192.168.1.5 parent 8080 0 no-query default login=redacted
http_port 3128

host_verify_strict off
sslproxy_cert_sign signTrusted
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

http_port 3129 intercept ssl-bump capath=/etc/ssl/certs
cert=/etc/ssl/certs/midca.pem key=/etc/ssl/private/midca.key
generate-host-certificates=on
ssl_bump server-first all
ssl_bump bump all

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 40 startup=7 idle=5

cache_dir ufs /var/cache/squid 100 16 256
coredump_dir /var/cache/squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320



I would send pcaps of the failures but then I would have to sanitize them.

Thanks.
-JP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/8d24fd44/attachment.htm>

From maamule10 at gmail.com  Sun Jun 28 10:37:01 2015
From: maamule10 at gmail.com (Dalmar)
Date: Sun, 28 Jun 2015 13:37:01 +0300
Subject: [squid-users] Mikrotik and Squid Transparent
Message-ID: <CAFUu-Gvzg-4sSHA59jKdD80kqPWzvQuQT-+YsZV4PNhaVr5bUw@mail.gmail.com>

To begin with, thank you Marcel,Alex and Amos for your help guys i am
really so close because of you. I have done exactly what Marcel told me and
now all transparent/intercept errors are gone. It worked nicely when i used
two mikrotiks one for WAN and the other for the LAN connection, however,
when i use one mikrotik it says TCP_MISS_ABORTED and NONE_ABORTED. In this
situation ,squid gets internet from the MK LAN port using a public IP and i
can ping the net, but squid throws the above error in the access.log. The
topo i wanna use is INTERNET >>MK >> SQUID .
i think the iptable rules will change.The Mikrotik have 3 NICS now , but i
can add 1 more so it becomes eth0:WAN eth1:LAN eth2:PROXY-LAN
eth3:PROXY-WAN .

NB: it says Your message to squid-users awaits moderator approval , Message
body is too big ,for all my replays! so sorry for the delay.

Thanks in advance .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/d2078beb/attachment.htm>

From squid3 at treenet.co.nz  Sun Jun 28 11:01:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Jun 2015 23:01:23 +1200
Subject: [squid-users] SSL Bumping CONNECT With A cache_peer
In-Reply-To: <CAPBWTD3O24spMyDsSDoxhD1HYK9oJMxtg4io_FQekL74Jv5NeQ@mail.gmail.com>
References: <CAPBWTD3O24spMyDsSDoxhD1HYK9oJMxtg4io_FQekL74Jv5NeQ@mail.gmail.com>
Message-ID: <558FD403.5030901@treenet.co.nz>

On 28/06/2015 10:18 p.m., JP wrote:
> Hello all.
> 
> I tried reading all the FAQ's and scoured the rest of the internet for any
> configuration examples I can find and I have not seen a working solution
> for this.

Squid does not support does it permit sending the decrypted requests
over an insecure channel, even using CONNECT. We are inching very slowly
towards support for peer CONNECT tunnels, but not quite there yet.

If you want to speed things up please consider getting involved with the
development and/or sponsoring someone to do the remaining pieces.

Amos



From squid3 at treenet.co.nz  Sun Jun 28 11:11:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Jun 2015 23:11:13 +1200
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAFUu-Gvzg-4sSHA59jKdD80kqPWzvQuQT-+YsZV4PNhaVr5bUw@mail.gmail.com>
References: <CAFUu-Gvzg-4sSHA59jKdD80kqPWzvQuQT-+YsZV4PNhaVr5bUw@mail.gmail.com>
Message-ID: <558FD651.8010200@treenet.co.nz>

On 28/06/2015 10:37 p.m., Dalmar wrote:
> To begin with, thank you Marcel,Alex and Amos for your help guys i am
> really so close because of you. I have done exactly what Marcel told me and
> now all transparent/intercept errors are gone. It worked nicely when i used
> two mikrotiks one for WAN and the other for the LAN connection, however,
> when i use one mikrotik it says TCP_MISS_ABORTED and NONE_ABORTED. In this
> situation ,squid gets internet from the MK LAN port using a public IP and i
> can ping the net, but squid throws the above error in the access.log. The
> topo i wanna use is INTERNET >>MK >> SQUID .
> i think the iptable rules will change.The Mikrotik have 3 NICS now , but i
> can add 1 more so it becomes eth0:WAN eth1:LAN eth2:PROXY-LAN
> eth3:PROXY-WAN .

You should not need extra NICs for this. The Mikrotik rules just need to
distinguish the flows clearly.

a) LAN->WAN dst port TCP/80 use gateway eth2
b) *->WAN use gateway eth0
c) *->Squid use gateway eth2
d) *->LAN use gateway eth1

> 
> NB: it says Your message to squid-users awaits moderator approval , Message
> body is too big ,for all my replays! so sorry for the delay.

NP: We have a 40KB size limit on posts to these lists. Moderation for
others and the moderators procrastinate.

Amos


From fredbmail at free.fr  Sun Jun 28 14:57:40 2015
From: fredbmail at free.fr (Fred)
Date: Sun, 28 Jun 2015 16:57:40 +0200
Subject: [squid-users] acl for redirect - re Fred
In-Reply-To: <558D71F9.3060408@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D71F9.3060408@afo.net>
Message-ID: <55900B64.80500@free.fr>



Le 26/06/2015 17:38, Mike a ?crit :
> Yes we already have that version installed, that is the version having 
> these issues.
>
> [root at Server1 ~]# e2guardian -v
> e2guardian 3.0.4

Stable version = 3.0.4 , it's not the develop version


From eliezer at ngtech.co.il  Sun Jun 28 14:59:07 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 28 Jun 2015 17:59:07 +0300
Subject: [squid-users] Squid 3.5.5 CentOS RPMs release
Message-ID: <55900BBB.4070902@ngtech.co.il>

Hey list,

I have created the new RPM's for CentOS 6 and 7 while not mentioning I 
also created the package for OracleLinux.(which was very annoy to find 
out that the download file from Oracle was not matching an ISO but 
something else)

The 3.5.5 and 3.5.4 was published here:
http://www1.ngtech.co.il/wpe/?p=90

Eliezer



From shakirgil at yahoo.com  Sun Jun 28 21:11:23 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Sun, 28 Jun 2015 21:11:23 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
Message-ID: <1718171797.1256346.1435525883190.JavaMail.yahoo@mail.yahoo.com>

We are using squid on centon 6.6 64 bit with this option.

Squid Cache: Version 3.5.5 
Service Name: squid 
configure options:  '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--datadir=/usr/share/squid' '--includedir=/usr/include' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sharedstatedir=/usr/com' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--x-includes=/usr/include' '--x-libraries=/usr/lib' '--enable-storeio=aufs' '--enable-removal-policies=heap,lru' '--disable-icmp' '--disable-delay-pools' '--enable-useragent-log' '--enable-referer-log' '--disable-kill-parent-hack' '--enable-snmp' '--enable-cachemgr-hostname=localhost' '--enable-arp-acl' '--disable-htcp' '--disable-forw-via-db' '--enable-follow-x-forwarded-for' '--disable-cache-digests' '--disable-poll' '--enable-epoll' '--enable-linux-netfilter' '--disable-ident-lookups' '--enable-default-hostsfile=/etc/hosts' '--enable-http-violations' '--enable-gnuregex' '--enable-async-io=64' '--with-aufs-threads=64' '--with-pthreads' '--with-aio' '--enable-err-languages=English' '--disable-wccp' '--disable-wccpv2' '--disable-devpoll' '--with-large-files' '--enable-large-cache-files' '--with-maxfd=65536' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' --enable-ltdl-convenience 



After proper restart the system, squid can not start and showing following errors.


 
2015/06/29 02:01:25 kid1| Starting Squid Cache version 3.5.5 for x86_64-redhat-l                                                             inux-gnu... 
2015/06/29 02:01:25 kid1| Service Name: squid 
2015/06/29 02:01:25 kid1| Process ID 12515 
2015/06/29 02:01:25 kid1| Process Roles: worker 
2015/06/29 02:01:25 kid1| With 65536 file descriptors available 
2015/06/29 02:01:25 kid1| Initializing IP Cache... 
2015/06/29 02:01:25 kid1| DNS Socket created at [::], FD 6 
2015/06/29 02:01:25 kid1| DNS Socket created at 0.0.0.0, FD 8 
2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf 
2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf 
2015/06/29 02:01:25 kid1| helperOpenServers: Starting 10/40 'storeid.pl' process                                                             es 
2015/06/29 02:01:25 kid1| Logfile: opening log /var/log/squid/access.log 
2015/06/29 02:01:25 kid1| WARNING: log name now starts with a module name. Use '                                                             stdio:/var/log/squid/access.log' 
2015/06/29 02:01:25 kid1| Store logging disabled 
2015/06/29 02:01:25 kid1| Swap maxSize 1945600000 + 131072 KB, estimated 6485770                                                              objects 
2015/06/29 02:01:25 kid1| Target number of buckets: 324288 
2015/06/29 02:01:25 kid1| Using 524288 Store buckets 
2015/06/29 02:01:25 kid1| Max Mem  size: 131072 KB 
2015/06/29 02:01:25 kid1| Max Swap size: 1945600000 KB 
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache01 (clean log) 
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache02 (clean log) 
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache03 (clean log) 
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache04 (clean log) 
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache05 (clean log) 
2015/06/29 02:01:25 kid1| Using Least Load store dir selection 
2015/06/29 02:01:25 kid1| Current Directory is /root 
2015/06/29 02:01:25 kid1| Finished loading MIME types and icons. 
2015/06/29 02:01:25 kid1| Sending SNMP messages from [::]:3401 
2015/06/29 02:01:25 kid1| Squid plugin modules loaded: 0 
2015/06/29 02:01:25 kid1| Adaptation support is off. 
2015/06/29 02:01:25 kid1| Accepting HTTP Socket connections at local=127.0.0.1:3                                                             128 remote=[::] FD 40 flags=9 
2015/06/29 02:01:25 kid1| Accepting TPROXY intercepted HTTP Socket connections a                                                             t local=[::]:3129 remote=[::] FD 41 flags=25 
2015/06/29 02:01:25 kid1| Accepting SNMP messages on [::]:3401 
2015/06/29 02:01:25 kid1| Store rebuilding is 94.23% complete 
2015/06/29 02:01:25 kid1| Done reading /cache01/cache01 swaplog (4244 entries) 
2015/06/29 02:01:25 kid1| Done reading /cache01/cache02 swaplog (4477 entries) 
2015/06/29 02:01:25 kid1| Done reading /cache01/cache03 swaplog (4321 entries) 
2015/06/29 02:01:26 kid1| Done reading /cache01/cache05 swaplog (174941 entries) 
2015/06/29 02:01:26 kid1| Done reading /cache01/cache04 swaplog (175638 entries) 
2015/06/29 02:01:26 kid1| Finished rebuilding storage from disk. 
2015/06/29 02:01:26 kid1|    363621 Entries scanned 
2015/06/29 02:01:26 kid1|         0 Invalid entries. 
2015/06/29 02:01:26 kid1|         0 With invalid flags. 
2015/06/29 02:01:26 kid1|    363621 Objects loaded. 
2015/06/29 02:01:26 kid1|         0 Objects expired. 
2015/06/29 02:01:26 kid1|         0 Objects cancelled. 
2015/06/29 02:01:26 kid1|         0 Duplicate URLs purged. 
2015/06/29 02:01:26 kid1|         0 Swapfile clashes avoided. 
2015/06/29 02:01:26 kid1|   Took 1.61 seconds (226469.63 objects/sec). 
2015/06/29 02:01:26 kid1| Beginning Validation Procedure 
2015/06/29 02:01:26 kid1|   262144 Entries Validated so far. 
2015/06/29 02:01:26 kid1|   Completed Validation Procedure 
2015/06/29 02:01:26 kid1|   Validated 363621 Entries 
2015/06/29 02:01:26 kid1|   store_swap_size = 20638920.00 KB 
2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96' 
2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96' 
2015/06/29 02:01:27 kid1| WARNING: 1 swapin MD5 mismatches 
2015/06/29 02:01:27 kid1| Could not parse headers from on disk object 
2015/06/29 02:01:27 kid1| BUG 3279: HTTP reply without Date: 
2015/06/29 02:01:27 kid1| StoreEntry->key: 757A99D29A48EDD9156B4DE92C0D9AB2 
2015/06/29 02:01:27 kid1| StoreEntry->next: 0 
2015/06/29 02:01:27 kid1| StoreEntry->mem_obj: 0x6efe6c0 
2015/06/29 02:01:27 kid1| StoreEntry->timestamp: -1 
2015/06/29 02:01:27 kid1| StoreEntry->lastref: 1435525287 
2015/06/29 02:01:27 kid1| StoreEntry->expires: -1 
2015/06/29 02:01:27 kid1| StoreEntry->lastmod: -1 
2015/06/29 02:01:27 kid1| StoreEntry->swap_file_sz: 0 
2015/06/29 02:01:27 kid1| StoreEntry->refcount: 1 
2015/06/29 02:01:27 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED 
2015/06/29 02:01:27 kid1| StoreEntry->swap_dirn: -1 
2015/06/29 02:01:27 kid1| StoreEntry->swap_filen: -1 
2015/06/29 02:01:27 kid1| StoreEntry->lock_count: 2 
2015/06/29 02:01:27 kid1| StoreEntry->mem_status: 0 
2015/06/29 02:01:27 kid1| StoreEntry->ping_status: 2 
2015/06/29 02:01:27 kid1| StoreEntry->store_status: 1 
2015/06/29 02:01:27 kid1| StoreEntry->swap_status: 0 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/0000007A 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| storeLateRelease: released 0 objects 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/02/1D/00021DA9 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory 
2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8 
2015/06/29 02:01:27 kid1| assertion failed: store.cc:1885: "isEmpty()" 
2015/06/29 02:01:30 kid1| Current Directory is /root


From eliezer at ngtech.co.il  Sun Jun 28 21:47:26 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 29 Jun 2015 00:47:26 +0300
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <1718171797.1256346.1435525883190.JavaMail.yahoo@mail.yahoo.com>
References: <1718171797.1256346.1435525883190.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55906B6E.8050503@ngtech.co.il>

Is there any particular reason you are using ufs\aufs cache_dir? I 
assume this system works with multiple cores and there is a chance that 
ufs cache_dir is being "managed" by two squids.
This is not something I am speculating and might be wrong about it.

Eliezer

On 29/06/2015 00:11, Mohammad Shakir wrote:
> We are using squid on centon 6.6 64 bit with this option.
>
> Squid Cache: Version 3.5.5
> Service Name: squid
> configure options:  '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--datadir=/usr/share/squid' '--includedir=/usr/include' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sharedstatedir=/usr/com' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--x-includes=/usr/include' '--x-libraries=/usr/lib' '--enable-storeio=aufs' '--enable-removal-policies=heap,lru' '--disable-icmp' '--disable-delay-pools' '--enable-useragent-log' '--enable-referer-log' '--disable-kill-parent-hack' '--enable-snmp' '--enable-cachemgr-hostname=localhost' '--enable-arp-acl' '--disable-htcp' '--disable-forw-via-db' '--enable-follow-x-forwarded-for' '--disable-cache-digests' '--disable-poll' '--enable-epoll' '--enable-linux-netfilter' '--disable-ident-lookups' '--enable-default-hostsfile=/etc/hosts' '--enable-http-violations' '--enable-gnuregex' '--enable-async-io=64' '--with-aufs-threads=64' '--with-pthreads' '--with-ai
o' '--enable-err-languages=English' '--disable-wccp' '--disable-wccpv2' '--disable-devpoll' '--with-large-files' '--enable-large-cache-files' '--with-maxfd=65536' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' --enable-ltdl-convenience
>
>
>
> After proper restart the system, squid can not start and showing following errors.
>
>
>
> 2015/06/29 02:01:25 kid1| Starting Squid Cache version 3.5.5 for x86_64-redhat-l                                                             inux-gnu...
> 2015/06/29 02:01:25 kid1| Service Name: squid
> 2015/06/29 02:01:25 kid1| Process ID 12515
> 2015/06/29 02:01:25 kid1| Process Roles: worker
> 2015/06/29 02:01:25 kid1| With 65536 file descriptors available
> 2015/06/29 02:01:25 kid1| Initializing IP Cache...
> 2015/06/29 02:01:25 kid1| DNS Socket created at [::], FD 6
> 2015/06/29 02:01:25 kid1| DNS Socket created at 0.0.0.0, FD 8
> 2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf
> 2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf
> 2015/06/29 02:01:25 kid1| helperOpenServers: Starting 10/40 'storeid.pl' process                                                             es
> 2015/06/29 02:01:25 kid1| Logfile: opening log /var/log/squid/access.log
> 2015/06/29 02:01:25 kid1| WARNING: log name now starts with a module name. Use '                                                             stdio:/var/log/squid/access.log'
> 2015/06/29 02:01:25 kid1| Store logging disabled
> 2015/06/29 02:01:25 kid1| Swap maxSize 1945600000 + 131072 KB, estimated 6485770                                                              objects
> 2015/06/29 02:01:25 kid1| Target number of buckets: 324288
> 2015/06/29 02:01:25 kid1| Using 524288 Store buckets
> 2015/06/29 02:01:25 kid1| Max Mem  size: 131072 KB
> 2015/06/29 02:01:25 kid1| Max Swap size: 1945600000 KB
> 2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache01 (clean log)
> 2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache02 (clean log)
> 2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache03 (clean log)
> 2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache04 (clean log)
> 2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache05 (clean log)
> 2015/06/29 02:01:25 kid1| Using Least Load store dir selection
> 2015/06/29 02:01:25 kid1| Current Directory is /root
> 2015/06/29 02:01:25 kid1| Finished loading MIME types and icons.
> 2015/06/29 02:01:25 kid1| Sending SNMP messages from [::]:3401
> 2015/06/29 02:01:25 kid1| Squid plugin modules loaded: 0
> 2015/06/29 02:01:25 kid1| Adaptation support is off.
> 2015/06/29 02:01:25 kid1| Accepting HTTP Socket connections at local=127.0.0.1:3                                                             128 remote=[::] FD 40 flags=9
> 2015/06/29 02:01:25 kid1| Accepting TPROXY intercepted HTTP Socket connections a                                                             t local=[::]:3129 remote=[::] FD 41 flags=25
> 2015/06/29 02:01:25 kid1| Accepting SNMP messages on [::]:3401
> 2015/06/29 02:01:25 kid1| Store rebuilding is 94.23% complete
> 2015/06/29 02:01:25 kid1| Done reading /cache01/cache01 swaplog (4244 entries)
> 2015/06/29 02:01:25 kid1| Done reading /cache01/cache02 swaplog (4477 entries)
> 2015/06/29 02:01:25 kid1| Done reading /cache01/cache03 swaplog (4321 entries)
> 2015/06/29 02:01:26 kid1| Done reading /cache01/cache05 swaplog (174941 entries)
> 2015/06/29 02:01:26 kid1| Done reading /cache01/cache04 swaplog (175638 entries)
> 2015/06/29 02:01:26 kid1| Finished rebuilding storage from disk.
> 2015/06/29 02:01:26 kid1|    363621 Entries scanned
> 2015/06/29 02:01:26 kid1|         0 Invalid entries.
> 2015/06/29 02:01:26 kid1|         0 With invalid flags.
> 2015/06/29 02:01:26 kid1|    363621 Objects loaded.
> 2015/06/29 02:01:26 kid1|         0 Objects expired.
> 2015/06/29 02:01:26 kid1|         0 Objects cancelled.
> 2015/06/29 02:01:26 kid1|         0 Duplicate URLs purged.
> 2015/06/29 02:01:26 kid1|         0 Swapfile clashes avoided.
> 2015/06/29 02:01:26 kid1|   Took 1.61 seconds (226469.63 objects/sec).
> 2015/06/29 02:01:26 kid1| Beginning Validation Procedure
> 2015/06/29 02:01:26 kid1|   262144 Entries Validated so far.
> 2015/06/29 02:01:26 kid1|   Completed Validation Procedure
> 2015/06/29 02:01:26 kid1|   Validated 363621 Entries
> 2015/06/29 02:01:26 kid1|   store_swap_size = 20638920.00 KB
> 2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96'
> 2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96'
> 2015/06/29 02:01:27 kid1| WARNING: 1 swapin MD5 mismatches
> 2015/06/29 02:01:27 kid1| Could not parse headers from on disk object
> 2015/06/29 02:01:27 kid1| BUG 3279: HTTP reply without Date:
> 2015/06/29 02:01:27 kid1| StoreEntry->key: 757A99D29A48EDD9156B4DE92C0D9AB2
> 2015/06/29 02:01:27 kid1| StoreEntry->next: 0
> 2015/06/29 02:01:27 kid1| StoreEntry->mem_obj: 0x6efe6c0
> 2015/06/29 02:01:27 kid1| StoreEntry->timestamp: -1
> 2015/06/29 02:01:27 kid1| StoreEntry->lastref: 1435525287
> 2015/06/29 02:01:27 kid1| StoreEntry->expires: -1
> 2015/06/29 02:01:27 kid1| StoreEntry->lastmod: -1
> 2015/06/29 02:01:27 kid1| StoreEntry->swap_file_sz: 0
> 2015/06/29 02:01:27 kid1| StoreEntry->refcount: 1
> 2015/06/29 02:01:27 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/06/29 02:01:27 kid1| StoreEntry->swap_dirn: -1
> 2015/06/29 02:01:27 kid1| StoreEntry->swap_filen: -1
> 2015/06/29 02:01:27 kid1| StoreEntry->lock_count: 2
> 2015/06/29 02:01:27 kid1| StoreEntry->mem_status: 0
> 2015/06/29 02:01:27 kid1| StoreEntry->ping_status: 2
> 2015/06/29 02:01:27 kid1| StoreEntry->store_status: 1
> 2015/06/29 02:01:27 kid1| StoreEntry->swap_status: 0
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/0000007A
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| storeLateRelease: released 0 objects
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/02/1D/00021DA9
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                             ectory
> 2015/06/29 02:01:27 kid1|       /cache01/cache04/00/00/000000F8
> 2015/06/29 02:01:27 kid1| assertion failed: store.cc:1885: "isEmpty()"
> 2015/06/29 02:01:30 kid1| Current Directory is /root
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From alex at samad.com.au  Sun Jun 28 23:22:58 2015
From: alex at samad.com.au (Alex Samad)
Date: Mon, 29 Jun 2015 09:22:58 +1000
Subject: [squid-users] Squid 3.5.5 CentOS RPMs release
In-Reply-To: <55900BBB.4070902@ngtech.co.il>
References: <55900BBB.4070902@ngtech.co.il>
Message-ID: <CAJ+Q1PX_7MM=7i5zL=3NqzVjBdbVHXO8Z84BSiNV5euNWC=NfQ@mail.gmail.com>

Thanks

On 29 June 2015 at 00:59, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey list,
>
> I have created the new RPM's for CentOS 6 and 7 while not mentioning I also
> created the package for OracleLinux.(which was very annoy to find out that
> the download file from Oracle was not matching an ISO but something else)
>
> The 3.5.5 and 3.5.4 was published here:
> http://www1.ngtech.co.il/wpe/?p=90
>
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From alex at samad.com.au  Sun Jun 28 23:28:38 2015
From: alex at samad.com.au (Alex Samad)
Date: Mon, 29 Jun 2015 09:28:38 +1000
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <558FD651.8010200@treenet.co.nz>
References: <CAFUu-Gvzg-4sSHA59jKdD80kqPWzvQuQT-+YsZV4PNhaVr5bUw@mail.gmail.com>
 <558FD651.8010200@treenet.co.nz>
Message-ID: <CAJ+Q1PXg2MHnpEfiUYA5Gr8B1PyReUY5bKTPzs+YUhk8iTo66Q@mail.gmail.com>

Hi

Thought I would re word what i got from this, see if I understood.

If squid and router (default gateway) are on the same box
then
DNAT to the SQUID listening port and local ip (Can you use localhost
suppose it doesn't matter)
else
router the packet to the SQUID box (if possible)
DNAT on the SQUID box to the local listening port and ip


Squid is able to look in the NAT table ? to confirm what the
destination would be not what the DNAT'ed ip would be.


Does that sum it up ?


Alex



On 28 June 2015 at 21:11, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 28/06/2015 10:37 p.m., Dalmar wrote:
>> To begin with, thank you Marcel,Alex and Amos for your help guys i am
>> really so close because of you. I have done exactly what Marcel told me and
>> now all transparent/intercept errors are gone. It worked nicely when i used
>> two mikrotiks one for WAN and the other for the LAN connection, however,
>> when i use one mikrotik it says TCP_MISS_ABORTED and NONE_ABORTED. In this
>> situation ,squid gets internet from the MK LAN port using a public IP and i
>> can ping the net, but squid throws the above error in the access.log. The
>> topo i wanna use is INTERNET >>MK >> SQUID .
>> i think the iptable rules will change.The Mikrotik have 3 NICS now , but i
>> can add 1 more so it becomes eth0:WAN eth1:LAN eth2:PROXY-LAN
>> eth3:PROXY-WAN .
>
> You should not need extra NICs for this. The Mikrotik rules just need to
> distinguish the flows clearly.
>
> a) LAN->WAN dst port TCP/80 use gateway eth2
> b) *->WAN use gateway eth0
> c) *->Squid use gateway eth2
> d) *->LAN use gateway eth1
>
>>
>> NB: it says Your message to squid-users awaits moderator approval , Message
>> body is too big ,for all my replays! so sorry for the delay.
>
> NP: We have a 40KB size limit on posts to these lists. Moderation for
> others and the moderators procrastinate.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Jun 29 03:57:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Jun 2015 15:57:13 +1200
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAJ+Q1PXg2MHnpEfiUYA5Gr8B1PyReUY5bKTPzs+YUhk8iTo66Q@mail.gmail.com>
References: <CAFUu-Gvzg-4sSHA59jKdD80kqPWzvQuQT-+YsZV4PNhaVr5bUw@mail.gmail.com>	<558FD651.8010200@treenet.co.nz>
 <CAJ+Q1PXg2MHnpEfiUYA5Gr8B1PyReUY5bKTPzs+YUhk8iTo66Q@mail.gmail.com>
Message-ID: <5590C219.5010805@treenet.co.nz>

On 29/06/2015 11:28 a.m., Alex Samad wrote:
> Hi
> 
> Thought I would re word what i got from this, see if I understood.
> 
> If squid and router (default gateway) are on the same box
> then
> DNAT to the SQUID listening port and local ip (Can you use localhost
> suppose it doesn't matter)

localhost does matter. Most systems have hardware level protection
preventing localhost IP or lo interface being used for non-local packets.

> else
> router the packet to the SQUID box (if possible)
> DNAT on the SQUID box to the local listening port and ip
> 
> 
> Squid is able to look in the NAT table ? to confirm what the
> destination would be not what the DNAT'ed ip would be.
> 

Yes.

> Does that sum it up ?
> 

And yes.

Amos



From hack.back at hotmail.com  Mon Jun 29 03:40:44 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 28 Jun 2015 20:40:44 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55876E36.4050702@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz>
Message-ID: <1435549243614-4671937.post@n4.nabble.com>

what you supposed me to do Amos, !!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671937.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 29 04:16:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Jun 2015 16:16:49 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1435549243614-4671937.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435549243614-4671937.post@n4.nabble.com>
Message-ID: <5590C6B1.1020706@treenet.co.nz>

On 29/06/2015 3:40 p.m., HackXBack wrote:
> what you supposed me to do Amos, !!
> 

Not sure. I have not been able to replicate or identify the source of
the problem you are having.

Amos



From fmeini at esseweb.eu  Mon Jun 29 08:55:27 2015
From: fmeini at esseweb.eu (Fiorenza Meini)
Date: Mon, 29 Jun 2015 10:55:27 +0200
Subject: [squid-users] Strange warning - squid 3.0
Message-ID: <559107FF.6080200@esseweb.eu>

Hi,
I see this error when I restart squid service:

please, what does it mean:
WARNING: because of this '192.168.100.164' is ignored to keep splay tree 
searching predictable ?

Below there is the warning that is reported

Shutting down WWW-proxy squid 2015/06/29 09:19:10| WARNING: 
'192.168.100.164' is a subnetwork of '192.168.100.164'
2015/06/29 09:19:10| WARNING: because of this '192.168.100.164' is 
ignored to keep splay tree searching predictable
2015/06/29 09:19:10| WARNING: You should probably remove 
'192.168.100.164' from the ACL named 'VBC'
2015/06/29 09:19:10| WARNING: '192.168.100.164' is a subnetwork of 
'192.168.100.164'
2015/06/29 09:19:10| WARNING: because of this '192.168.100.164' is 
ignored to keep splay tree searching predictable
2015/06/29 09:19:10| WARNING: You should probably remove 
'192.168.100.164' from the ACL named 'VBC'

Thank and regards

Fiorenza Meini
-- 
Spazio Web S.r.l.
V. Dante, 10
13900 Biella
Tel.: +39 015 2431982
Fax.: +39 015 2522600
Numero d'Iscrizione al Registro Imprese presso CCIAA Biella, Cod.Fisc.e 
P.Iva: 02414430021
Iscriz. REA: BI - 188936 Cap. Soc.: ?. 30.000 i.v.


From Antony.Stone at squid.open.source.it  Mon Jun 29 09:11:02 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 29 Jun 2015 10:11:02 +0100
Subject: [squid-users] Strange warning - squid 3.0
In-Reply-To: <559107FF.6080200@esseweb.eu>
References: <559107FF.6080200@esseweb.eu>
Message-ID: <201506291011.02338.Antony.Stone@squid.open.source.it>

On Monday 29 Jun 2015 at 09:55, Fiorenza Meini wrote:

> Hi,
> I see this error when I restart squid service:
> 
> please, what does it mean:
> WARNING: because of this '192.168.100.164' is ignored to keep splay tree
> searching predictable ?

It means that squid is going to ignore the address 192.168.100.164 in order to 
ensure that searching the splay tree remains a predictable process.

> Below there is the warning that is reported
> 
> Shutting down WWW-proxy squid 2015/06/29 09:19:10| WARNING:
> '192.168.100.164' is a subnetwork of '192.168.100.164'
> 2015/06/29 09:19:10| WARNING: because of this '192.168.100.164' is
> ignored to keep splay tree searching predictable
> 2015/06/29 09:19:10| WARNING: You should probably remove
> '192.168.100.164' from the ACL named 'VBC'

Show us, at the very least, the definition of this ACL, plus any other lines 
which refer to any addresses starting with 192.168.100

Even better, show us your squid.conf, after removing blank lines and comments, 
and possibly obscuring any private details.

The wording of the above warning makes me wonder whether you have defined a 
network range using an address from inside that range rather than the base 
network address (eg: 192.168.100.164/24 instead of 192.168.100.0/24)?


Regards,


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Mon Jun 29 09:12:10 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 29 Jun 2015 15:12:10 +0600
Subject: [squid-users] Strange warning - squid 3.0
In-Reply-To: <559107FF.6080200@esseweb.eu>
References: <559107FF.6080200@esseweb.eu>
Message-ID: <55910BEA.4070103@gmail.com>

Means exactly your seen. You acl contains two lines with this 
subnetwork. Check and correct.

29.06.15 14:55, Fiorenza Meini ?????:
> Hi,
> I see this error when I restart squid service:
>
> please, what does it mean:
> WARNING: because of this '192.168.100.164' is ignored to keep splay 
> tree searching predictable ?
>
> Below there is the warning that is reported
>
> Shutting down WWW-proxy squid 2015/06/29 09:19:10| WARNING: 
> '192.168.100.164' is a subnetwork of '192.168.100.164'
> 2015/06/29 09:19:10| WARNING: because of this '192.168.100.164' is 
> ignored to keep splay tree searching predictable
> 2015/06/29 09:19:10| WARNING: You should probably remove 
> '192.168.100.164' from the ACL named 'VBC'
> 2015/06/29 09:19:10| WARNING: '192.168.100.164' is a subnetwork of 
> '192.168.100.164'
> 2015/06/29 09:19:10| WARNING: because of this '192.168.100.164' is 
> ignored to keep splay tree searching predictable
> 2015/06/29 09:19:10| WARNING: You should probably remove 
> '192.168.100.164' from the ACL named 'VBC'
>
> Thank and regards
>
> Fiorenza Meini



From hack.back at hotmail.com  Mon Jun 29 10:56:58 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 29 Jun 2015 03:56:58 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <5590C6B1.1020706@treenet.co.nz>
References: <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435549243614-4671937.post@n4.nabble.com>
 <5590C6B1.1020706@treenet.co.nz>
Message-ID: <1435575418899-4671942.post@n4.nabble.com>

i gave you backtrace report and i gave you my configure option ?
what else you want from me ?
when i use youtube mobile app that its content is 206 , then the server
crashed immediately ...
this problem need to be solved, 
am ready for any info's you need.
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671942.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 29 12:18:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 00:18:17 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1435575418899-4671942.post@n4.nabble.com>
References: <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435549243614-4671937.post@n4.nabble.com>
 <5590C6B1.1020706@treenet.co.nz> <1435575418899-4671942.post@n4.nabble.com>
Message-ID: <55913789.7040605@treenet.co.nz>

On 29/06/2015 10:56 p.m., HackXBack wrote:
> i gave you backtrace report and i gave you my configure option ?
> what else you want from me ?

The backtrace shows where the assert is from. But without those symbols
the gdb cant seem to find in your builds we cant debug what state the
transaction is in. Thats what gives the best clues about where the fix
needs to be.

> when i use youtube mobile app that its content is 206 , then the server
> crashed immediately ...
> this problem need to be solved, 
> am ready for any info's you need.
> Thanks
> 


All good clues to help inform the debug. But still lacking info about
whether the client disconnected already (or not), why that
halfClosedReader is unset (client still sending request payload?), and
what events are waiting to be processed for the transaction.

Since it seems so replicatable. Are you able to run a test install with
"debug_options ALL,9" and only perform your known failing transaction
through it?
 Perhapse the resulting cache.log will have enough info to make up for
the symbols absence.

Sorry
Amos



From vdoctor at neuf.fr  Mon Jun 29 12:34:08 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 29 Jun 2015 05:34:08 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
Message-ID: <1435581248847-4671944.post@n4.nabble.com>

Hi All,
is someone with an idea why it happens ?
*TCP_MISS/504 708 GET http://www.myexamplecom/images/menu_hover_left.png -
CD_SIBLING_HIT/x.x.x.x*

i can see the TCP_MISS with the SIBLING only...
i'm looking for this issue since several days and it makes me crazy 

thanks in advance.
bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 29 13:06:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 01:06:56 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435581248847-4671944.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
Message-ID: <559142F0.3030004@treenet.co.nz>

On 30/06/2015 12:34 a.m., Stakres wrote:
> Hi All,
> is someone with an idea why it happens ?
> *TCP_MISS/504 708 GET http://www.myexamplecom/images/menu_hover_left.png -
> CD_SIBLING_HIT/x.x.x.x*
> 
> i can see the TCP_MISS with the SIBLING only...
> i'm looking for this issue since several days and it makes me crazy 

Looks to me like a 504 response coming back from the peer. Or you have
set a ~1 second TTL on transactions.

Amos



From vdoctor at neuf.fr  Mon Jun 29 12:50:26 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 29 Jun 2015 05:50:26 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <559142F0.3030004@treenet.co.nz>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz>
Message-ID: <1435582226950-4671946.post@n4.nabble.com>

Hi Amos,

1. What does the 504 mean ?
2. How can i extend the TTL transaction ?

Thanks :o)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671946.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jun 29 13:46:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 01:46:25 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435582226950-4671946.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
Message-ID: <55914C31.3000500@treenet.co.nz>

On 30/06/2015 12:50 a.m., Stakres wrote:
> Hi Amos,
> 
> 1. What does the 504 mean ?

<https://tools.ietf.org/html/rfc7231#section-6.6.5>

One of the proxies timed out while contacting the origin server, OR the
origin server acting as a gateway to a CGI backend timed out waiting for
the object to be generated.

> 2. How can i extend the TTL transaction ?

If you dont know that chances are you haven't touched it from the 15min
default. read_timeout is probably the relevant Squid directive.

AMos


From alex_wu2012 at hotmail.com  Mon Jun 29 17:35:26 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Mon, 29 Jun 2015 10:35:26 -0700
Subject: [squid-users] sslbump and caching of generated cert
Message-ID: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>

So far as I know, hen sslbump is enabled for a port, for each dns name, squid save a cert generated according to dns name and signing key (from http_port configuration). So the next time, the generated cert can be fetched if the same dns host and configured signing key.
Now  have a question on this:









http_port 10045 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10045.pem cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10045.pem
http_port 10046 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10046.pem cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10046.pem
I have two ports configured with SSLBUMP. Each port has its own CA signing key. The desired behavior is that, for the hostname www.foo.com, the certificate generated for the port should use key_10045, and the certificate generated for the port should use key_10046. It seems OK. 
But, if we look at the ssl_db, only the last generated certificate is cached for www.foo.com. Is it possible to cache the generated certificates by the host and signing key?
Alex 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150629/792be9a5/attachment.htm>

From ncrogers at gmail.com  Mon Jun 29 20:54:33 2015
From: ncrogers at gmail.com (Nick Rogers)
Date: Mon, 29 Jun 2015 13:54:33 -0700
Subject: [squid-users] tcp_outgoing_address binds to wrong interface
Message-ID: <CAKOb=YaQyG_hDPR7K0F8Mu3nX7LPM8Gifrc7NC+yYy3RHN_PKg@mail.gmail.com>

Hello,

I am experiencing an issue with squid 3.5.5 and FreeBSD 10.1 where
tcp_outgoing_address correctly rewrites the source address of outgoing
packets, but fails to bind the socket to the correct interface. I've been
using this kind of setup/configuration for quite some time (since the squid
2.7 days), so I believe something between squid 3.3 and squid 3.5 has
broken this behavior, or it is a problem with FreeBSD 10.x that I do not
understand. FWIW squid 3.3.3 on FreeBSD 9.x behaves correctly with the same
config.

I've been able to replicate this on a non-production VM with a simple
config. For example, I have two WAN interfaces: WAN1 (em0/192.168.92.246)
and WAN2 (em1/10.8.8.10), and a LAN interface (em2/192.168.5.1/24). My
default route points to the gateway for WAN1/em0. I have configured a
tcp_outgoing_address for a single host on the LAN (192.168.5.2) to use
outgoing address 10.8.8.10 (WAN2/em1). HTTP request packets from squid end
up going out em0 (the default route interface) with an ip of 10.8.8.10. The
source IP is correct but the packets egress the wrong interface, so they
are obviously dropped by the upstream router on that incorrect interface.

The following is my test squid.conf and some basic network config. While I
normally use squid as a transparent proxy in conjunction with the PF
firewall and routing enabled, for the purposes of replicating the issue in
my VM test environment I have disabled PF, routing, and connected directly
to squid via my browser's proxy config. Attempts to load webpages time out
unless I remove use of the tcp_outgoing_address directive.

# begin test squid.conf

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

http_access allow localhost manager
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow all

acl wan2 src 192.168.5.2
tcp_outgoing_address 10.8.8.10 wan2

follow_x_forwarded_for allow localhost
acl_uses_indirect_client on
log_uses_indirect_client on

http_port 3129

cache_replacement_policy heap LFUDA
maximum_object_size 768 MB
cache_dir aufs /squid/cache 2048 32 512

access_log      daemon:/squid/logs/access.log squid
cache_log       /squid/logs/cache.log
cache_store_log none

logfile_rotate 0

pid_filename /var/run/squid.pid

# end test squid.conf

em0: flags=8843<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> metric 0 mtu 1500
options=9b<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,VLAN_HWCSUM>
ether 00:0c:29:a3:33:93
inet 192.168.92.246 netmask 0xffffff00 broadcast 192.168.92.255
nd6 options=9<PERFORMNUD,IFDISABLED>
media: Ethernet autoselect (1000baseT <full-duplex>)
status: active

em1: flags=8843<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> metric 0 mtu 1500
options=9b<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,VLAN_HWCSUM>
ether 00:0c:29:a3:33:7f
inet 10.8.8.10 netmask 0xffffff00 broadcast 10.8.8.255
inet 10.8.8.11 netmask 0xffffff00 broadcast 10.8.8.255
inet 10.8.8.12 netmask 0xffffff00 broadcast 10.8.8.255
inet 10.8.8.13 netmask 0xffffff00 broadcast 10.8.8.255
inet 10.8.8.14 netmask 0xffffff00 broadcast 10.8.8.255
nd6 options=9<PERFORMNUD,IFDISABLED>
media: Ethernet autoselect (1000baseT <full-duplex>)
status: active

root# netstat -rn | grep default

default            192.168.92.2       UGS         em0

I am hoping someone can shed some light on whether or not this may be a
squid bug or something going on with FreeBSD. Thank you.

-Nick
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150629/1ec9c37a/attachment.htm>

From shakirgil at yahoo.com  Tue Jun 30 04:24:09 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Tue, 30 Jun 2015 04:24:09 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
Message-ID: <1263869064.2188478.1435638249684.JavaMail.yahoo@mail.yahoo.com>

We are using squid on centon 6.6 64 bit with this option.

Squid Cache: Version 3.5.5
Service Name: squid
configure options:  '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--datadir=/usr/share/squid' '--includedir=/usr/include' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sharedstatedir=/usr/com' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--x-includes=/usr/include' '--x-libraries=/usr/lib' '--enable-storeio=aufs' '--enable-removal-policies=heap,lru' '--disable-icmp' '--disable-delay-pools' '--enable-useragent-log' '--enable-referer-log' '--disable-kill-parent-hack' '--enable-snmp' '--enable-cachemgr-hostname=localhost' '--enable-arp-acl' '--disable-htcp' '--disable-forw-via-db' '--enable-follow-x-forwarded-for' '--disable-cache-digests' '--disable-poll' '--enable-epoll' '--enable-linux-netfilter' '--disable-ident-lookups' '--enable-default-hostsfile=/etc/hosts' '--enable-http-violations' '--enable-gnuregex' '--enable-async-io=64' '--with-aufs-threads=64' '--with-pthreads' '--with-aio' '--enable-err-languages=English' '--disable-wccp' '--disable-wccpv2' '--disable-devpoll' '--with-large-files' '--enable-large-cache-files' '--with-maxfd=65536' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' --enable-ltdl-convenience



After proper restart the system, squid can not start and showing following errors.



2015/06/29 02:01:25 kid1| Starting Squid Cache version 3.5.5 for x86_64-redhat-l                                                            inux-gnu...
2015/06/29 02:01:25 kid1| Service Name: squid
2015/06/29 02:01:25 kid1| Process ID 12515
2015/06/29 02:01:25 kid1| Process Roles: worker
2015/06/29 02:01:25 kid1| With 65536 file descriptors available
2015/06/29 02:01:25 kid1| Initializing IP Cache...
2015/06/29 02:01:25 kid1| DNS Socket created at [::], FD 6
2015/06/29 02:01:25 kid1| DNS Socket created at 0.0.0.0, FD 8
2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf
2015/06/29 02:01:25 kid1| Adding nameserver 192.169.1.1 from squid.conf
2015/06/29 02:01:25 kid1| helperOpenServers: Starting 10/40 'storeid.pl' process                                                            es
2015/06/29 02:01:25 kid1| Logfile: opening log /var/log/squid/access.log
2015/06/29 02:01:25 kid1| WARNING: log name now starts with a module name. Use '                                                            stdio:/var/log/squid/access.log'
2015/06/29 02:01:25 kid1| Store logging disabled
2015/06/29 02:01:25 kid1| Swap maxSize 1945600000 + 131072 KB, estimated 6485770                                                              objects
2015/06/29 02:01:25 kid1| Target number of buckets: 324288
2015/06/29 02:01:25 kid1| Using 524288 Store buckets
2015/06/29 02:01:25 kid1| Max Mem  size: 131072 KB
2015/06/29 02:01:25 kid1| Max Swap size: 1945600000 KB
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache01 (clean log)
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache02 (clean log)
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache03 (clean log)
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache04 (clean log)
2015/06/29 02:01:25 kid1| Rebuilding storage in /cache01/cache05 (clean log)
2015/06/29 02:01:25 kid1| Using Least Load store dir selection
2015/06/29 02:01:25 kid1| Current Directory is /root
2015/06/29 02:01:25 kid1| Finished loading MIME types and icons.
2015/06/29 02:01:25 kid1| Sending SNMP messages from [::]:3401
2015/06/29 02:01:25 kid1| Squid plugin modules loaded: 0
2015/06/29 02:01:25 kid1| Adaptation support is off.
2015/06/29 02:01:25 kid1| Accepting HTTP Socket connections at local=127.0.0.1:3                                                            128 remote=[::] FD 40 flags=9
2015/06/29 02:01:25 kid1| Accepting TPROXY intercepted HTTP Socket connections a                                                            t local=[::]:3129 remote=[::] FD 41 flags=25
2015/06/29 02:01:25 kid1| Accepting SNMP messages on [::]:3401
2015/06/29 02:01:25 kid1| Store rebuilding is 94.23% complete
2015/06/29 02:01:25 kid1| Done reading /cache01/cache01 swaplog (4244 entries)
2015/06/29 02:01:25 kid1| Done reading /cache01/cache02 swaplog (4477 entries)
2015/06/29 02:01:25 kid1| Done reading /cache01/cache03 swaplog (4321 entries)
2015/06/29 02:01:26 kid1| Done reading /cache01/cache05 swaplog (174941 entries)
2015/06/29 02:01:26 kid1| Done reading /cache01/cache04 swaplog (175638 entries)
2015/06/29 02:01:26 kid1| Finished rebuilding storage from disk.
2015/06/29 02:01:26 kid1|    363621 Entries scanned
2015/06/29 02:01:26 kid1|        0 Invalid entries.
2015/06/29 02:01:26 kid1|        0 With invalid flags.
2015/06/29 02:01:26 kid1|    363621 Objects loaded.
2015/06/29 02:01:26 kid1|        0 Objects expired.
2015/06/29 02:01:26 kid1|        0 Objects cancelled.
2015/06/29 02:01:26 kid1|        0 Duplicate URLs purged.
2015/06/29 02:01:26 kid1|        0 Swapfile clashes avoided.
2015/06/29 02:01:26 kid1|  Took 1.61 seconds (226469.63 objects/sec).
2015/06/29 02:01:26 kid1| Beginning Validation Procedure
2015/06/29 02:01:26 kid1|  262144 Entries Validated so far.
2015/06/29 02:01:26 kid1|  Completed Validation Procedure
2015/06/29 02:01:26 kid1|  Validated 363621 Entries
2015/06/29 02:01:26 kid1|  store_swap_size = 20638920.00 KB
2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96'
2015/06/29 02:01:26 kid1| fqdncacheParse: No PTR record for '183.195.233.96'
2015/06/29 02:01:27 kid1| WARNING: 1 swapin MD5 mismatches
2015/06/29 02:01:27 kid1| Could not parse headers from on disk object
2015/06/29 02:01:27 kid1| BUG 3279: HTTP reply without Date:
2015/06/29 02:01:27 kid1| StoreEntry->key: 757A99D29A48EDD9156B4DE92C0D9AB2
2015/06/29 02:01:27 kid1| StoreEntry->next: 0
2015/06/29 02:01:27 kid1| StoreEntry->mem_obj: 0x6efe6c0
2015/06/29 02:01:27 kid1| StoreEntry->timestamp: -1
2015/06/29 02:01:27 kid1| StoreEntry->lastref: 1435525287
2015/06/29 02:01:27 kid1| StoreEntry->expires: -1
2015/06/29 02:01:27 kid1| StoreEntry->lastmod: -1
2015/06/29 02:01:27 kid1| StoreEntry->swap_file_sz: 0
2015/06/29 02:01:27 kid1| StoreEntry->refcount: 1
2015/06/29 02:01:27 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/06/29 02:01:27 kid1| StoreEntry->swap_dirn: -1
2015/06/29 02:01:27 kid1| StoreEntry->swap_filen: -1
2015/06/29 02:01:27 kid1| StoreEntry->lock_count: 2
2015/06/29 02:01:27 kid1| StoreEntry->mem_status: 0
2015/06/29 02:01:27 kid1| StoreEntry->ping_status: 2
2015/06/29 02:01:27 kid1| StoreEntry->store_status: 1
2015/06/29 02:01:27 kid1| StoreEntry->swap_status: 0
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/0000007A
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| storeLateRelease: released 0 objects
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/02/1D/00021DA9
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| DiskThreadsDiskFile::openDone: (2) No such file or dir                                                            ectory
2015/06/29 02:01:27 kid1|      /cache01/cache04/00/00/000000F8
2015/06/29 02:01:27 kid1| assertion failed: store.cc:1885: "isEmpty()"
2015/06/29 02:01:30 kid1| Current Directory is /root


From squid3 at treenet.co.nz  Tue Jun 30 04:35:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 16:35:23 +1200
Subject: [squid-users] tcp_outgoing_address binds to wrong interface
In-Reply-To: <CAKOb=YaQyG_hDPR7K0F8Mu3nX7LPM8Gifrc7NC+yYy3RHN_PKg@mail.gmail.com>
References: <CAKOb=YaQyG_hDPR7K0F8Mu3nX7LPM8Gifrc7NC+yYy3RHN_PKg@mail.gmail.com>
Message-ID: <55921C8B.9000706@treenet.co.nz>

On 30/06/2015 8:54 a.m., Nick Rogers wrote:
> Hello,
> 
> I am experiencing an issue with squid 3.5.5 and FreeBSD 10.1 where
> tcp_outgoing_address correctly rewrites the source address of outgoing
> packets, but fails to bind the socket to the correct interface. I've been
> using this kind of setup/configuration for quite some time (since the squid
> 2.7 days), so I believe something between squid 3.3 and squid 3.5 has
> broken this behavior, or it is a problem with FreeBSD 10.x that I do not
> understand. FWIW squid 3.3.3 on FreeBSD 9.x behaves correctly with the same
> config.

FYI: Squid has nothing to do with interface binding. That is 100%
internal to the kernel routing stack.

All Squid is able to do is set the IP and port for the connection as a
hint to the routing. Whether the hint is taken and used is outside our
control.

Amos



From squid3 at treenet.co.nz  Tue Jun 30 04:38:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 16:38:20 +1200
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <1263869064.2188478.1435638249684.JavaMail.yahoo@mail.yahoo.com>
References: <1263869064.2188478.1435638249684.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55921D3C.30808@treenet.co.nz>

On 30/06/2015 4:24 p.m., Mohammad Shakir wrote:
> We are using squid on centon 6.6 64 bit with this option.
> 

<http://bugs.squid-cache.org/show_bug.cgi?id=3483>

Did you see the responses to your post from yesterday?

Amos



From squid3 at treenet.co.nz  Tue Jun 30 04:51:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 16:51:51 +1200
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>
Message-ID: <55922067.2000401@treenet.co.nz>

On 30/06/2015 5:35 a.m., Alex Wu wrote:
> So far as I know, hen sslbump is enabled for a port, for each dns
> name, squid save a cert generated according to dns name and signing
> key (from http_port configuration). So the next time, the generated
> cert can be fetched if the same dns host and configured signing key. 

Signing key is just a validation check on the cert. It has nothing else
to do with the actual cert.

AFAIK generated certs are stored by DN, serial number or hash of the two.

> Now  have a question on this:
> 
> http_port 10045 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10045.pem
> cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10045.pem http_port
> 10046 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10046.pem
> cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10046.pem I have two
> ports configured with SSLBUMP. Each port has its own CA signing key.
> The desired behavior is that, for the hostname www.foo.com, the
> certificate generated for the port should use key_10045, and the
> certificate generated for the port should use key_10046. It seems OK.
>  But, if we look at the ssl_db, only the last generated certificate
> is cached for www.foo.com. Is it possible to cache the generated
> certificates by the host and signing key? Alex

Not in the current design.

You could assign two workers, each with a different http_port and
ssl_crtd helper using different cert databases.


What is the point of this anyway? Why do you want to make your users
face a constant stream of nasty certificate-changed errors?

Amos


From shakirgil at yahoo.com  Tue Jun 30 05:05:03 2015
From: shakirgil at yahoo.com (Mohammad Shakir)
Date: Tue, 30 Jun 2015 05:05:03 +0000 (UTC)
Subject: [squid-users] squid 3.5.5 issue after restart the system
In-Reply-To: <55921D3C.30808@treenet.co.nz>
References: <55921D3C.30808@treenet.co.nz>
Message-ID: <1467191445.2188429.1435640703348.JavaMail.yahoo@mail.yahoo.com>

I checked my email which show me that my email address has been blocked by squid-user list so I could not check it.

Now should I upgrade to higher version ?





On Tuesday, June 30, 2015 9:38 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
On 30/06/2015 4:24 p.m., Mohammad Shakir wrote:

> We are using squid on centon 6.6 64 bit with this option.
> 

<http://bugs.squid-cache.org/show_bug.cgi?id=3483>

Did you see the responses to your post from yesterday?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From vdoctor at neuf.fr  Tue Jun 30 07:24:26 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 00:24:26 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <55914C31.3000500@treenet.co.nz>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz>
Message-ID: <1435649066566-4671955.post@n4.nabble.com>

Hi Amos,
Yep, i did not modify the TTL transaction.
Here, it seems the parent (sibling mode) tries to do the request itself but
faces an error (504 gateway timeout), it should answer to the kid it does
not have the object (TCP_MISS) then the parent should download the object
from internet.
With this error, i suspect the kid accepts the "error" and does nothing
more, replying to the browser (user) something like "sorry, the object is
missing".

How can i force the kid to download the object from internet when the parent
replies a such answer ?

bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671955.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jun 30 07:54:39 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 30 Jun 2015 08:54:39 +0100
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435649066566-4671955.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
Message-ID: <201506300854.39358.Antony.Stone@squid.open.source.it>

On Tuesday 30 Jun 2015 at 08:24, Stakres wrote:

> Here, it seems the parent (sibling mode) tries to do the request itself but
> faces an error (504 gateway timeout), it should answer to the kid it does
> not have the object (TCP_MISS) then the parent should download the object
> from internet.

Surely it is getting the 504 error when it is trying to download the object?

> With this error, i suspect the kid accepts the "error" and does nothing
> more, replying to the browser (user) something like "sorry, the object is
> missing".

Instead of "missing", try "unavailable".

> How can i force the kid to download the object from internet when the
> parent replies a such answer ?

As far as I know, you can't.  You can configure browsers to use the proxy for 
some requests and go direct for others, but once that choice has been made, 
that's the way the browser will do it.

Anyway, what makes you think the "kid" browser would be able to get the 
object, if the "parent" proxy was unable to?

Have you actually tried downloading such an object direct from a browser 
without using Squid, and found that it is available?

If so, please give us a URL to the object so we can investigate...


regards,


Antony.

-- 
Users don't know what they want until they see what they get.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vdoctor at neuf.fr  Tue Jun 30 07:45:46 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 00:45:46 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <201506300854.39358.Antony.Stone@squid.open.source.it>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
Message-ID: <1435650346019-4671957.post@n4.nabble.com>

Hi Antony,

Correct, the kid contacts the parent that is getting a 504 and replies the
same to the kid. That's why I suspect the parent tries to download by itself
instead replying to the kid it does not have the object so the kid should do
a fresh download from internet.

examples:
TCP_MISS/504 708 GET http://w.sharethis.com/button/buttons.js -
CD_SIBLING_HIT/x.x.x.x
TCP_MISS/504 708 GET http://www.googletagmanager.com/gtm.js?id=GTM-NNVXD6 -
CD_SIBLING_HIT/x.x.x.x
TCP_MISS/504 478 GET http://dl4.offercdn.com/2613/styleVideo.css -
HIER_NONE/-
TCP_MISS/504 708 GET http://www.indianrail.gov.in/seat_Avail.html -
CD_SIBLING_HIT/x.x.x.x
etc...

The kid receives these answers and does nothing more, the result is the
browser (client) does not have all the objects for a correct page...
Sometimes, the page is blank because a js script, css, etc... is missing.

There are 2 squid, sibling each of them.
Squid1 (10.1.1.1):
cache_peer 10.1.1.2 sibling 8182 8183 proxy-only no-tproxy
Squid2 (10.1.1.2):
cache_peer 10.1.1.1 sibling 8182 8183 proxy-only no-tproxy

Both are in tproxy (settings from the Squid wiki), ICP is enabled with each
squid.

if you need more details, feel free to ask 

Bye Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671957.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Jun 30 07:56:34 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 30 Jun 2015 00:56:34 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55913789.7040605@treenet.co.nz>
References: <1430777347185-4671104.post@n4.nabble.com>
 <554816F2.8050303@treenet.co.nz> <1434910861109-4671827.post@n4.nabble.com>
 <55875970.7060200@treenet.co.nz> <1434936992183-4671829.post@n4.nabble.com>
 <55876E36.4050702@treenet.co.nz> <1435549243614-4671937.post@n4.nabble.com>
 <5590C6B1.1020706@treenet.co.nz> <1435575418899-4671942.post@n4.nabble.com>
 <55913789.7040605@treenet.co.nz>
Message-ID: <1435650994449-4671958.post@n4.nabble.com>

2015/06/30 10:09:38.432 kid1| Acl.cc(138) matches: checking always_direct
2015/06/30 10:09:38.432 kid1| Acl.cc(138) matches: checking always_direct#1
2015/06/30 10:09:38.432 kid1| Acl.cc(138) matches: checking fakespeed
2015/06/30 10:09:38.432 kid1| RegexData.cc(51) match: aclRegexData::match:
checking
'https://r1---sn-4g57knls.googlevideo.com/videoplayback?mime=video/mp4&key=yt5&ms=au&mt=1435651756&mv=m&upn=vXBl$
2015/06/30 10:09:38.432 kid1| RegexData.cc(62) match: aclRegexData::match:
looking for
'(\.*(speedtest|espeed).*\/((latency|random.*|upload)\.(jpg|txt|php)))'
2015/06/30 10:09:38.432 kid1| Acl.cc(158) matches: checked: fakespeed = 0
2015/06/30 10:09:38.432 kid1| Acl.cc(158) matches: checked: always_direct#1
= 0
2015/06/30 10:09:38.432 kid1| Acl.cc(138) matches: checking always_direct#2
2015/06/30 10:09:38.432 kid1| Acl.cc(138) matches: checking bau1
2015/06/30 10:09:38.432 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking 'r1---sn-4g57knls.googlevideo.com'
2015/06/30 10:09:38.432 kid1| DomainData.cc(113) match: aclMatchDomainList:
'r1---sn-4g57knls.googlevideo.com' NOT found
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: bau1 = 0
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: always_direct#2
= 0
2015/06/30 10:09:38.433 kid1| Acl.cc(138) matches: checking always_direct#3
2015/06/30 10:09:38.433 kid1| Acl.cc(138) matches: checking betty1
2015/06/30 10:09:38.433 kid1| DomainData.cc(108) match: aclMatchDomainList:
checking 'r1---sn-4g57knls.googlevideo.com'
2015/06/30 10:09:38.433 kid1| DomainData.cc(113) match: aclMatchDomainList:
'r1---sn-4g57knls.googlevideo.com' NOT found
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: betty1 = 0
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: always_direct#3
= 0
2015/06/30 10:09:38.433 kid1| Acl.cc(138) matches: checking always_direct#4
2015/06/30 10:09:38.433 kid1| Acl.cc(138) matches: checking all
2015/06/30 10:09:38.433 kid1| Ip.cc(95) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare: 10.11.20.1:15088/[::] ([::]:15088)  vs
[::]-[::]/[::]
2015/06/30 10:09:38.433 kid1| Ip.cc(539) match: aclIpMatchIp:
'10.11.20.1:15088' found
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: all = 1
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: always_direct#4
= 1
2015/06/30 10:09:38.433 kid1| Acl.cc(158) matches: checked: always_direct =
1
2015/06/30 10:09:38.433 kid1| Checklist.cc(61) markFinished: 0x2077cf098
answer ALLOWED for match
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x2038018=0
2015/06/30 10:09:38.433 kid1| Checklist.cc(161) checkCallback:
ACLChecklist::checkCallback: 0x2077cf098 answer=ALLOWED
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x38f7288
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x38f7288=0
2015/06/30 10:09:38.433 kid1| peer_select.cc(194) peerCheckAlwaysDirectDone:
peerCheckAlwaysDirectDone: ALLOWED
2015/06/30 10:09:38.433 kid1| peer_select.cc(200) peerCheckAlwaysDirectDone:
direct = DIRECT_YES (always_direct allow)
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| peer_select.cc(441) peerSelectFoo: GET
r1---sn-4g57knls.googlevideo.com
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| client_side.cc(4974) validatePinnedConnection:
local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132 flags=1
2015/06/30 10:09:38.433 kid1| peer_select.cc(940) peerAddFwdServer:
peerAddFwdServer: adding DIRECT PINNED
2015/06/30 10:09:38.433 kid1| peer_select.cc(940) peerAddFwdServer:
peerAddFwdServer: adding DIRECT HIER_DIRECT
2015/06/30 10:09:38.433 kid1| peer_select.cc(940) peerAddFwdServer:
peerAddFwdServer: adding DIRECT PINNED
2015/06/30 10:09:38.433 kid1| peer_select.cc(940) peerAddFwdServer:
peerAddFwdServer: adding DIRECT HIER_DIRECT
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| peer_select.cc(258) peerSelectDnsPaths: Find
IP destination for:
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4'
via r1--$
2015/06/30 10:09:38.433 kid1| ipcache.cc(501) ipcache_nbgethostbyname:
ipcache_nbgethostbyname: Name 'r1---sn-4g57knls.googlevideo.com'.
2015/06/30 10:09:38.433 kid1| Address.cc(389) lookupHostIP: Given Non-IP
'r1---sn-4g57knls.googlevideo.com': Name or service not known
2015/06/30 10:09:38.433 kid1| ipcache.cc(533) ipcache_nbgethostbyname:
ipcache_nbgethostbyname: HIT for 'r1---sn-4g57knls.googlevideo.com'
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x38f7288=1
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x38f7288
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x38f7288=0
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| peer_select.cc(280) peerSelectDnsPaths: Found
sources for
'http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4'
2015/06/30 10:09:38.433 kid1| peer_select.cc(281) peerSelectDnsPaths:  
always_direct = ALLOWED
2015/06/30 10:09:38.433 kid1| peer_select.cc(282) peerSelectDnsPaths:   
never_direct = DUNNO
2015/06/30 10:09:38.433 kid1| peer_select.cc(290) peerSelectDnsPaths:         
PINNED = local=0.0.0.0 remote=74.125.99.6:443 flags=1
2015/06/30 10:09:38.433 kid1| peer_select.cc(288) peerSelectDnsPaths:   
ORIGINAL_DST = local=0.0.0.0 remote=74.125.99.6:443 flags=1
2015/06/30 10:09:38.433 kid1| peer_select.cc(295) peerSelectDnsPaths:       
timedout = 0
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0x7abd088
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x7abd088=1
2015/06/30 10:09:38.433 kid1| FwdState.cc(383) startConnectionOrFail:
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4
2015/06/30 10:09:38.433 kid1| HttpRequest.cc(486) clearError: old error
details: 0/0
2015/06/30 10:09:38.433 kid1| FwdState.cc(781) connectStart:
fwdConnectStart:
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x207709228
2015/06/30 10:09:38.433 kid1| FwdState.cc(798) connectStart: pinned peer
connection: 0x207709228
2015/06/30 10:09:38.433 kid1| client_side.cc(4999) borrowPinnedConnection:
local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132 flags=1
2015/06/30 10:09:38.433 kid1| client_side.cc(4974) validatePinnedConnection:
local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132 flags=1
2015/06/30 10:09:38.433 kid1| comm.cc(983) comm_add_close_handler:
comm_add_close_handler: FD 132, handler=1, data=0x7abd088
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x7abd088=2
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x7abd088=3
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
SomeCloseHandler constructed, this=0x209c42180 [call226891275]
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x7abd088=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x7abd088=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x7abd088=2
2015/06/30 10:09:38.433 kid1| comm.cc(993) comm_add_close_handler:
comm_add_close_handler: FD 132, AsyncCall=0x209c42180*1
2015/06/30 10:09:38.433 kid1| FwdState.cc(902) dispatch:
local=74.125.99.6:443 remote=10.11.20.1:15088 FD 17 flags=33: Fetching GET
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4sw$
2015/06/30 10:09:38.433 kid1| http.cc(2261) httpStart: GET
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4
2015/06/30 10:09:38.433 kid1| cbdata.cc(299) cbdataInternalAlloc: Allocating
0x1bdb83218
2015/06/30 10:09:38.433 kid1| AsyncJob.cc(34) AsyncJob: AsyncJob
constructed, this=0x1bdb83308 type=HttpStateData [job8249863]
2015/06/30 10:09:38.433 kid1| store.cc(485) lock: Client locked key
C1D7592C129BCF72DDE33D05D8CDEC4F e:=p2DIWV/0x1cb0af600*4
2015/06/30 10:09:38.433 kid1| http.cc(89) HttpStateData: HttpStateData
0x1bdb83218 created
2015/06/30 10:09:38.433 kid1| cbdata.cc(299) cbdataInternalAlloc: Allocating
0x1cf024708
2015/06/30 10:09:38.433 kid1| http.cc(89) HttpStateData: HttpStateData
0x1bdb83218 created
2015/06/30 10:09:38.433 kid1| cbdata.cc(299) cbdataInternalAlloc: Allocating
0x1cf024708
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=1
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=2
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=3
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
HttpStateData::httpStateConnClosed constructed, this=0x208b0d090
[call226891276]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=5
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=2
2015/06/30 10:09:38.433 kid1| comm.cc(993) comm_add_close_handler:
comm_add_close_handler: FD 132, AsyncCall=0x208b0d090*1
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
AsyncJob::start constructed, this=0x1977797e0 [call226891277]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=5
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(93) ScheduleCall: AsyncJob.cc(26)
will call AsyncJob::start() [call226891277]
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=3
2015/06/30 10:09:38.433 kid1| peer_select.cc(79) ~ps_state:
http://cdn.youtube/id=o-ACQ6eJqVKCPZIUmKoUQvsHsrXismY31LLzsOB4swKbq-&itag=135mime=video/mp4
2015/06/30 10:09:38.433 kid1| store.cc(523) unlock: peerSelect unlocking key
C1D7592C129BCF72DDE33D05D8CDEC4F e:=p2DIWV/0x1cb0af600*4
2015/06/30 10:09:38.433 kid1| cbdata.cc(321) cbdataInternalFree: 0x38f7288
2015/06/30 10:09:38.433 kid1| cbdata.cc(338) cbdataInternalFree: Freeing
0x38f7288
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x207709228=12
2015/06/30 10:09:38.433 kid1| FilledChecklist.cc(66) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x2077cf098
2015/06/30 10:09:38.433 kid1| Checklist.cc(195) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x2077cf098
2015/06/30 10:09:38.433 kid1| cbdata.cc(321) cbdataInternalFree: 0x2077cf098
2015/06/30 10:09:38.433 kid1| cbdata.cc(338) cbdataInternalFree: Freeing
0x2077cf098
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x207709228=11
2015/06/30 10:09:38.433 kid1| FilledChecklist.cc(66) ~ACLFilledChecklist:
ACLFilledChecklist destroyed 0x1c3d6fcb8
2015/06/30 10:09:38.433 kid1| Checklist.cc(195) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x1c3d6fcb8
2015/06/30 10:09:38.433 kid1| cbdata.cc(321) cbdataInternalFree: 0x1c3d6fcb8
2015/06/30 10:09:38.433 kid1| cbdata.cc(338) cbdataInternalFree: Freeing
0x1c3d6fcb8
2015/06/30 10:09:38.433 kid1| SBuf.cc(124) ~SBuf: SBuf47491706 destructed
2015/06/30 10:09:38.433 kid1| MemBlob.cc(83) ~MemBlob: destructed,
this=0x207bb6ad0 id=blob18581031 capacity=1024 size=881
2015/06/30 10:09:38.433 kid1| cbdata.cc(321) cbdataInternalFree: 0x1bc392258
2015/06/30 10:09:38.433 kid1| cbdata.cc(338) cbdataInternalFree: Freeing
0x1bc392258
2015/06/30 10:09:38.433 kid1| helper.cc(1167) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x260f758=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x260f758=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x260f758=3
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x260f758=4
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
helperHandleRead constructed, this=0x206469a30 [call226891278]
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0x260f758=5
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x260f758=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x260f758=3
2015/06/30 10:09:38.433 kid1| Read.cc(58) comm_read_base: comm_read,
queueing read for local=[::] remote=[::] FD 113 flags=1; asynCall
0x206469a30*1
2015/06/30 10:09:38.433 kid1| ModEpoll.cc(116) SetSelect: FD 113, type=1,
handler=1, client_data=0x7fbc33d7c510, timeout=0
2015/06/30 10:09:38.433 kid1| AsyncCallQueue.cc(57) fireNext: leaving
helperHandleRead(local=[::] remote=[::] FD 113 flags=1, data=0x260f758,
size=108, buf=0x260f9e0)
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x260f758=2
2015/06/30 10:09:38.433 kid1| AsyncCallQueue.cc(55) fireNext: entering
TunnelBlindCopyWriteHandler(local=216.58.210.10:443 remote=10.11.20.1:15053
FD 24 flags=33, data=0xa3698a8, size=7596, buf=0x$
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(38) make: make call
TunnelBlindCopyWriteHandler [call226891274]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0xa3698a8
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0xa3698a8
2015/06/30 10:09:38.433 kid1| tunnel.cc(636) writeClientDone:
local=216.58.210.10:443 remote=10.11.20.1:15053 FD 24 flags=33, 7596 bytes
written, flag=0
2015/06/30 10:09:38.433 kid1| tunnel.cc(624) dataSent: len=7596 -
amount=7596
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0xa3698a8=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid: 0xa3698a8
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0xa3698a8=8
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock: 0xa3698a8=9
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
TunnelBlindCopyReadHandler constructed, this=0x294e6a0 [call226891279]
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0xa3698a8=10
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0xa3698a8=9
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0xa3698a8=8
2015/06/30 10:09:38.433 kid1| Read.cc(58) comm_read_base: comm_read,
queueing read for local=10.150.15.11:43712 remote=216.58.210.10:443 FD 27
flags=1; asynCall 0x294e6a0*1
2015/06/30 10:09:38.433 kid1| ModEpoll.cc(116) SetSelect: FD 27, type=1,
handler=1, client_data=0x7fbc33d79cc0, timeout=0
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0xa3698a8=7
2015/06/30 10:09:38.433 kid1| AsyncCallQueue.cc(57) fireNext: leaving
TunnelBlindCopyWriteHandler(local=216.58.210.10:443 remote=10.11.20.1:15053
FD 24 flags=33, data=0xa3698a8, size=7596, buf=0x2$
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0xa3698a8=6
2015/06/30 10:09:38.433 kid1| AsyncCallQueue.cc(55) fireNext: entering
AsyncJob::start()
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(38) make: make call
AsyncJob::start [call226891277]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| AsyncJob.cc(123) callStart: HttpStateData
status in: [ job8249863]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| http.cc(2141) sendRequest:
local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132 flags=1, request
0x1f63a2650, this 0x1bdb83218.
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=4
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=5
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=6
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=6
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
HttpStateData::httpTimeout constructed, this=0x1d5cbc4f0 [call226891280]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
HttpStateData::httpTimeout constructed, this=0x1d5cbc4f0 [call226891280]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=8
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=6
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=5
2015/06/30 10:09:38.433 kid1| comm.cc(553) commSetConnTimeout:
local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132 flags=1 timeout 86400
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x11e9df98=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x11e9df98=6
2015/06/30 10:09:38.433 kid1| http.cc(1498) maybeReadVirginBody: may read up
to 16383 bytes from local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132
flags=1
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=6
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=7
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=8
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=9
2015/06/30 10:09:38.433 kid1| cbdata.cc(426) cbdataInternalUnlock:
0x1bdb83218=8
2015/06/30 10:09:38.433 kid1| AsyncCall.cc(26) AsyncCall: The AsyncCall
HttpStateData::readReply constructed, this=0x201f4a450 [call226891281]
2015/06/30 10:09:38.433 kid1| cbdata.cc(492) cbdataReferenceValid:
0x1bdb83218
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=9
2015/06/30 10:09:38.433 kid1| cbdata.cc(394) cbdataInternalLock:
0x1bdb83218=10
2015/06/30 10:09:38.433 kid1| Read.cc(58) comm_read_base: comm_read,
queueing read for local=10.150.15.11:47595 remote=74.125.99.6:443 FD 132
flags=1; asynCall 0x201f4a450*1
2015/06/30 10:09:38.433 kid1| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/06/30 10:09:41.916 kid1| tools.cc(610) enter_suid: enter_suid: PID
11394 taking root privileges
2015/06/30 10:09:41.916 kid1| cache_manager.cc(80) registerProfile:
registering legacy config
2015/06/30 10:09:41.916 kid1| cache_manager.cc(114) findAction:
CacheManager::findAction: looking for action config
2015/06/30 10:09:41.916 kid1| cache_manager.cc(122) findAction: Action not
found.
2015/06/30 10:09:41.916 kid1| cache_manager.cc(65) registerProfile:
registered profile: config
2015/06/30 10:09:41.916 kid1| mem.cc(473) Report: Memory pools are 'on';
limit: 100.000 MB
2015/06/30 10:09:41.916 kid1| main.cc(1427) SquidMain: Doing post-config
initialization

2015/06/30 10:09:41.916 kid1| tools.cc(543) leave_suid: leave_suid: PID
11394 called
2015/06/30 10:09:41.916 kid1| tools.cc(565) leave_suid: leave_suid: PID
11394 giving up root, becoming 'proxy'
2015/06/30 10:09:41.916 kid1| main.cc(1429) SquidMain: running
RegisteredRunner::finalizeConfig
2015/06/30 10:09:41.916 kid1| main.cc(1430) SquidMain: running
RegisteredRunner::claimMemoryNeeds
2015/06/30 10:09:41.916 kid1| main.cc(1431) SquidMain: running
RegisteredRunner::useConfig
2015/06/30 10:09:41.916 kid1| cache_manager.cc(80) registerProfile:
registering legacy client_list
2015/06/30 10:09:41.916 kid1| cache_manager.cc(114) findAction:
CacheManager::findAction: looking for action client_list
2015/06/30 10:09:41.916 kid1| cache_manager.cc(122) findAction: Action not
found.
2015/06/30 10:09:41.916 kid1| cache_manager.cc(65) registerProfile:
registered profile: client_list
2015/06/30 10:09:41.916 kid1| tools.cc(610) enter_suid: enter_suid: PID
11394 taking root privileges
2015/06/30 10:09:41.918 kid1| cache_manager.cc(80) registerProfile:
registering legacy comm_epoll_incoming
2015/06/30 10:09:41.918 kid1| cache_manager.cc(114) findAction:
CacheManager::findAction: looking for action comm_epoll_incoming
2015/06/30 10:09:41.918 kid1| cache_manager.cc(122) findAction: Action not
found.
2015/06/30 10:09:41.918 kid1| cache_manager.cc(65) registerProfile:
registered profile: comm_epoll_incoming
2015/06/30 10:09:41.918 kid1| Set Current Directory to /var/spool/squid
2015/06/30 10:09:41.918 kid1| cache_manager.cc(65) registerProfile:
registered profile: comm_epoll_incoming
2015/06/30 10:09:41.918 kid1| Set Current Directory to /var/spool/squid
2015/06/30 10:09:41.918 kid1| tools.cc(543) leave_suid: leave_suid: PID
11394 called
2015/06/30 10:09:41.918 kid1| tools.cc(565) leave_suid: leave_suid: PID
11394 giving up root, becoming 'proxy'
2015/06/30 10:09:41.918 kid1| fd.cc(198) fd_open: fd_open() FD 4
/var/log/squid/cache.log
2015/06/30 10:09:41.918 kid1| Starting Squid Cache version
3.5.5-20150624-r13848 for x86_64-unknown-linux-gnu...
2015/06/30 10:09:41.918 kid1| Service Name: squid
2015/06/30 10:09:41.918 kid1| Process ID 11394
2015/06/30 10:09:41.918 kid1| Process Roles: worker
2015/06/30 10:09:41.918 kid1| With 65535 file descriptors available
2015/06/30 10:09:41.918 kid1| Initializing IP Cache...
2015/06/30 10:09:41.918 kid1| cache_manager.cc(80) registerProfile:
registering legacy ipcache
2015/06/30 10:09:41.918 kid1| cache_manager.cc(114) findAction:
CacheManager::findAction: looking for action ipcache
2015/06/30 10:09:41.918 kid1| cache_manager.cc(122) findAction: Action not
found.
2015/06/30 10:09:41.918 kid1| cache_manager.cc(65) registerProfile:
registered profile: ipcache
2015/06/30 10:09:41.918 kid1| cache_manager.cc(80) registerProfile:
registering legacy fqdncache
2015/06/30 10:09:41.918 kid1| cache_manager.cc(114) findAction:
CacheManager::findAction: looking for action fqdncache
2015/06/30 10:09:41.919 kid1| cache_manager.cc(122) findAction: Action not
found.
2015/06/30 10:09:41.919 kid1| cache_manager.cc(65) registerProfile:
registered profile: fqdncache
2015/06/30 10:09:41.919 kid1| fqdncache.cc(720) fqdncache_init: Initializing
FQDN Cache...
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is '127.0.0.1       localhost
'
2015/06/30 10:09:41.919 kid1| tools.cc(1063) parseEtcHosts: etc_hosts:
address is '127.0.0.1'
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'localhost'
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is '10.150.15.10    issa
'
2015/06/30 10:09:41.919 kid1| tools.cc(1063) parseEtcHosts: etc_hosts:
address is '10.150.15.10'
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'issa'
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is '
'
2015/06/30 10:09:41.919 kid1| tools.cc(1063) parseEtcHosts: etc_hosts:
address is ''
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is '::1     localhost ip6-localhost ip6-loopback
'
2015/06/30 10:09:41.919 kid1| tools.cc(1063) parseEtcHosts: etc_hosts:
address is '::1'
2015/06/30 10:09:41.919 kid1| tools.cc(1071) parseEtcHosts: etc_hosts:
multiple spaces, skipping
2015/06/30 10:09:41.919 kid1| tools.cc(1071) parseEtcHosts: etc_hosts:
multiple spaces, skipping
2015/06/30 10:09:41.919 kid1| tools.cc(1071) parseEtcHosts: etc_hosts:
multiple spaces, skipping
2015/06/30 10:09:41.919 kid1| tools.cc(1071) parseEtcHosts: etc_hosts:
multiple spaces, skipping
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'localhost'
2015/06/30 10:09:41.919 kid1| ipcache.cc(174) ipcacheRelease:
ipcacheRelease: Releasing entry for 'localhost'
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'ip6-localhost'
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'ip6-loopback'
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is 'ff02::1 ip6-allnodes
'
2015/06/30 10:09:41.919 kid1| tools.cc(1063) parseEtcHosts: etc_hosts:
address is 'ff02::1'
2015/06/30 10:09:41.919 kid1| tools.cc(1077) parseEtcHosts: etc_hosts: got
hostname 'ip6-allnodes'
2015/06/30 10:09:41.919 kid1| tools.cc(1054) parseEtcHosts: etc_hosts: line
is 'ff02::2 ip6-allrouters




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671958.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Tue Jun 30 07:57:36 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 30 Jun 2015 00:57:36 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1435650994449-4671958.post@n4.nabble.com>
References: <554816F2.8050303@treenet.co.nz>
 <1434910861109-4671827.post@n4.nabble.com> <55875970.7060200@treenet.co.nz>
 <1434936992183-4671829.post@n4.nabble.com> <55876E36.4050702@treenet.co.nz>
 <1435549243614-4671937.post@n4.nabble.com> <5590C6B1.1020706@treenet.co.nz>
 <1435575418899-4671942.post@n4.nabble.com> <55913789.7040605@treenet.co.nz>
 <1435650994449-4671958.post@n4.nabble.com>
Message-ID: <1435651056189-4671959.post@n4.nabble.com>

i copy from normal log to the assertion error\
is this enough or you need more ?
Thanks Amos.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671959.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jun 30 08:21:45 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 30 Jun 2015 09:21:45 +0100
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435650346019-4671957.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com>
Message-ID: <201506300921.45537.Antony.Stone@squid.open.source.it>

On Tuesday 30 Jun 2015 at 08:45, Stakres wrote:

> There are 2 squid, sibling each of them.
> Squid1 (10.1.1.1):
> cache_peer 10.1.1.2 sibling 8182 8183 proxy-only no-tproxy
> Squid2 (10.1.1.2):
> cache_peer 10.1.1.1 sibling 8182 8183 proxy-only no-tproxy

> if you need more details, feel free to ask

Okay, so you do not have a child - parent proxy setup, you have two sibling 
proxies, plus the browser.

However, my question still remains - why should one proxy (the one you're 
referring to as the "kid") be able to retreive the content when the other 
(your "parent") failed?

Do they have different routes for connecting to the Internet or something?

Have you tried temporarily removing the sibling proxy relationship, to see 
whether one proxy on its own can fetch all the content you expect?


Have you tried to manually download an object which the parent says is 
unavailable (with the 504 error) and managed to obtain it?

If so, please post the URL of such an object so we can investigate further.


Regards,

Antony.

-- 
There's no such thing as bad weather - only the wrong clothes.

 - Billy Connolly

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Tue Jun 30 08:27:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 20:27:01 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435650346019-4671957.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com>
Message-ID: <559252D5.8040203@treenet.co.nz>

On 30/06/2015 7:45 p.m., Stakres wrote:
> Hi Antony,
> 
> Correct, the kid contacts the parent that is getting a 504 and replies the
> same to the kid. That's why I suspect the parent tries to download by itself
> instead replying to the kid it does not have the object so the kid should do
> a fresh download from internet.

Lets get this clear. There is *no* kid and *no* parent involved in this
transaction. There is a client sibling, and a server sibling.

The client sibling uses CacheDigest to identify that the server sibling
probably has the object in cache so it fetches from the server sibling.
The server sibling responds with 504.

If it is possibel to re-forward the request to another destination the
client sibling *will* retry one of those after a 504.

Enable "debug_options 17,5" to see the re-forwarding operations in
action. Unfortunately not all reasons for non-retry are logged, but it
gives a good idea of whats going on anyway.

Amos



From vdoctor at neuf.fr  Tue Jun 30 08:21:21 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 01:21:21 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <559252D5.8040203@treenet.co.nz>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
Message-ID: <1435652481249-4671962.post@n4.nabble.com>

Anthony, Amos,

The 2 squid are kid/parent each of them (both sibling).
So, when one aks the second, they play the role of kid -> parent, am I right
?

Here is the way:
Squid1 checks the Squid2 and gets that:
... user-ip TCP_MISS/504 708 GET
http://code.jquery.com/ui/1.10.3/jquery-ui.js - CD_SIBLING_HIT/10.1.1.2 ...
Squid2 replies that to the Squid1:
... squid1-ip TCP_MISS/504 478 GET
http://code.jquery.com/ui/1.10.3/jquery-ui.js - HIER_NONE/- text/html

I did many wget with "unavailable" objects from both squid servers, i can
get the objects correctly.
I'll disable the sibling to check if we still have TCP_MISS/504, keep you
posted...

As the 2 squid are in production at the ISP datacenter, not sure I could
apply a debug...
They do have the same route connected to 2 mikrotik (squid1 -> mikrotik1,
squid 2 -> mikrotik2), both mikrotik connected to the same cisco router.
Mangles/DNS/Gateway on mikrotiks are the same.

Fred.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671962.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 08:44:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 20:44:16 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1435651056189-4671959.post@n4.nabble.com>
References: <554816F2.8050303@treenet.co.nz>
 <1434910861109-4671827.post@n4.nabble.com> <55875970.7060200@treenet.co.nz>
 <1434936992183-4671829.post@n4.nabble.com> <55876E36.4050702@treenet.co.nz>
 <1435549243614-4671937.post@n4.nabble.com> <5590C6B1.1020706@treenet.co.nz>
 <1435575418899-4671942.post@n4.nabble.com> <55913789.7040605@treenet.co.nz>
 <1435650994449-4671958.post@n4.nabble.com>
 <1435651056189-4671959.post@n4.nabble.com>
Message-ID: <559256E0.7070506@treenet.co.nz>

On 30/06/2015 7:57 p.m., HackXBack wrote:
> i copy from normal log to the assertion error\
> is this enough or you need more ?

Its leading me to a very horrible conclusion. This appears to be a
pinned connection with two transactions trying to read data out of the
server connection simultaneously. I'm not sure how to fix that.

Amos



From vdoctor at neuf.fr  Tue Jun 30 08:33:22 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 01:33:22 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435652481249-4671962.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com>
Message-ID: <1435653202246-4671964.post@n4.nabble.com>

Hi,
I disabled the sibling on both squid servers, we got one 504:
TCP_MISS/504 361 GET
http://rate.msfsob.com/review?h=www.searchhomeremedy.com -
HIER_DIRECT/8.25.35.129
A wget on this url gives a 404, so here we can say the object does not
exist, the TCP_MISS/504 seems a correct answer.
But no new 504... the ISP is a 500Mbps.
If we enable the sibling, we get one/two 504 every second on both squid.

any idea ?

Fred.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671964.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 09:05:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 21:05:25 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435652481249-4671962.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com>
Message-ID: <55925BD5.10108@treenet.co.nz>

On 30/06/2015 8:21 p.m., Stakres wrote:
> Anthony, Amos,
> 
> The 2 squid are kid/parent each of them (both sibling).

I'm being pedantic about this because the operations possible with a
parent proxy are very different from those done with a sibling proxy.

> So, when one aks the second, they play the role of kid -> parent, am I right
> ?

No. They play the roles of client->server. A kid->parent pair also plays
those client->server roles, but the conditions are different.

In particular parent can be an origin, sibling can only be a proxy or
cache. The cache controls are different, and in your case digesting is
telling the main proxy that its sibling has a cached object available.


> 
> Here is the way:
> Squid1 checks the Squid2 and gets that:
> ... user-ip TCP_MISS/504 708 GET
> http://code.jquery.com/ui/1.10.3/jquery-ui.js - CD_SIBLING_HIT/10.1.1.2 ...
> Squid2 replies that to the Squid1:
> ... squid1-ip TCP_MISS/504 478 GET
> http://code.jquery.com/ui/1.10.3/jquery-ui.js - HIER_NONE/- text/html
> 
> I did many wget with "unavailable" objects from both squid servers, i can
> get the objects correctly.
> I'll disable the sibling to check if we still have TCP_MISS/504, keep you
> posted...
> 
> As the 2 squid are in production at the ISP datacenter, not sure I could
> apply a debug...
> They do have the same route connected to 2 mikrotik (squid1 -> mikrotik1,
> squid 2 -> mikrotik2), both mikrotik connected to the same cisco router.
> Mangles/DNS/Gateway on mikrotiks are the same.

Then it is pointless to connect these proxies in the way you have done.

In particular the cache_peer "proxy-only" setting prevents them sharing
cached objects, so the sibling should only be used if links through the
Mikrotik go down. (any other use is a bug, and probably what you are
hitting).

So the sibling is being treated as a backup route if the upstream routes
are not working. But the sibling is physically wired to use the same
non-working upstream routes...


I'm wondering if the 504 is result of the sibling restrictions:
  Cache-Control:only-if-cached,max-age=0

Amos


From vdoctor at neuf.fr  Tue Jun 30 08:55:49 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 01:55:49 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <55925BD5.10108@treenet.co.nz>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com> <55925BD5.10108@treenet.co.nz>
Message-ID: <1435654549375-4671966.post@n4.nabble.com>

Amos,
We used this example from the wiki:
http://wiki.squid-cache.org/Features/CacheHierarchy
We can see a sibling/sibling archi is possible, right ?

Here, we can not have a "cache_peer parent" archi as the tproxy (original
user ip) will be lost at the parent level, you wrote this in a previous post
I did.
So, we must have a sibling/sibling with the 2 squid servers, both in tproxy.

Now, if there is a better archi and correct settings to build a
tproxy/sibling-sibling archi, please share 
My ISP is nice and I can arrange that with him...

Thanks in advance.

Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671966.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 09:18:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 21:18:40 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435653202246-4671964.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com>
 <1435653202246-4671964.post@n4.nabble.com>
Message-ID: <55925EF0.1060205@treenet.co.nz>

On 30/06/2015 8:33 p.m., Stakres wrote:
> Hi,
> I disabled the sibling on both squid servers, we got one 504:
> TCP_MISS/504 361 GET
> http://rate.msfsob.com/review?h=www.searchhomeremedy.com -
> HIER_DIRECT/8.25.35.129
> A wget on this url gives a 404, so here we can say the object does not
> exist, the TCP_MISS/504 seems a correct answer.
> But no new 504... the ISP is a 500Mbps.
> If we enable the sibling, we get one/two 504 every second on both squid.
> 
> any idea ?
> 

Maybe...

 What do these DNS lookup results provide to you when run from the Squid
box:

  dig AAAA rate.msfsob.com
  dig A rate.msfsob.com

Amos



From squid3 at treenet.co.nz  Tue Jun 30 09:44:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 21:44:17 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435654549375-4671966.post@n4.nabble.com>
References: <1435581248847-4671944.post@n4.nabble.com>
 <559142F0.3030004@treenet.co.nz> <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com> <55925BD5.10108@treenet.co.nz>
 <1435654549375-4671966.post@n4.nabble.com>
Message-ID: <559264F1.6020106@treenet.co.nz>

On 30/06/2015 8:55 p.m., Stakres wrote:
> Amos,
> We used this example from the wiki:
> http://wiki.squid-cache.org/Features/CacheHierarchy
> We can see a sibling/sibling archi is possible, right ?

Possible vs Useful. For you at present its possible, but not
particularly useful.

I've just received an update about bug
(<http://bugs.squid-cache.org/show_bug.cgi?id=4223>) that matching your
symptoms. Chudy seems to have isolated it to the particular mix of
HTTP/1.1 features sibling requests use.
 So a workaround would be to avoid the sibling relatinonship ...

> 
> Here, we can not have a "cache_peer parent" archi as the tproxy (original
> user ip) will be lost at the parent level, you wrote this in a previous post
> I did.
> So, we must have a sibling/sibling with the 2 squid servers, both in tproxy.

The TPROXY is separate. The IP is lost the instant the peer TCP
connection packets leave the Squid box regardess of the peer type.

> 
> Now, if there is a better archi and correct settings to build a
> tproxy/sibling-sibling archi, please share 

To use TPROXY on traffic between peers you need a cable/pathway between
the proxies that does not go through the Mikrotik/Cisco routers feeding
them with intercepted traffic. And routing rules on the Squid boxen to
ensure that TPROXY traffic exits via the same interface that it arrived on.

Alternatively you can use the no-tproxy rule on parent type cache_peer
lines with the same effect as you expected to get out of the siblings
(but not hitting the bug 4223).

Amos



From gecom at tubosider.it  Tue Jun 30 09:30:48 2015
From: gecom at tubosider.it (masterx81)
Date: Tue, 30 Jun 2015 02:30:48 -0700 (PDT)
Subject: [squid-users] Squid 3.5.5, delay pools and external helpers
Message-ID: <1435656648543-4671969.post@n4.nabble.com>

Hi...
I'm trying to limit download bandwidth to some user groups based on AD using
external helpers, using the following command:
delay_pools 1
delay_class 1 1
delay_access 1 allow InternetLimitato InternetLibero InternetCentralino
!CONNECT
delay_parameters 1 500000/500000

"InternetLimitato InternetLibero InternetCentralino" are some AD groups, and
i want to add theyr traffic to the 1 delay pool, but with this configuration
not work. Else, if i apply the delay pool to "all" all works as expected.

It's like this by design?

Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-5-delay-pools-and-external-helpers-tp4671969.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vdoctor at neuf.fr  Tue Jun 30 09:42:52 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 02:42:52 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <559264F1.6020106@treenet.co.nz>
References: <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com> <55925BD5.10108@treenet.co.nz>
 <1435654549375-4671966.post@n4.nabble.com> <559264F1.6020106@treenet.co.nz>
Message-ID: <1435657372098-4671970.post@n4.nabble.com>

Amos,
Yes, similar case here on the 4223.
By reading the case 4223, we can see that part "Non-cacheable objects should
never be added to the digest." from you.
In my squid, there is no restriction, ICP is fully open, squid server
(3.5.5) are compiled with the digest option, so all is done to allow
ICP/digest connection and exchange.
So, why the servers think they got the objects, especially when they are not
cacheable and not cached ?

To me, it seems the servers think they have the object but they don't, so
they reply with a 404 translated by a 504 to the squid client because
sibling archi.
I could understand it could be a bug but the squid client should see the 504
and should request the object from internet, no ?

At the moment, i use a "retry_on_error on" as a workaround but not sure it's
fixing all 504.

Then, having a dedicated cable between squid servers is not a realistic
solution, my ISP will not see that as a serious solution 

Last point, "no-tproxy rule on parent type cache_peer", it does not work, we
tried that.
We applied that option 1 month ago and the internet sees the squid ip, not
the original ip address, so maybe another bug here... 

Fred.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671970.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 10:10:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 22:10:54 +1200
Subject: [squid-users] Squid 3.5.5, delay pools and external helpers
In-Reply-To: <1435656648543-4671969.post@n4.nabble.com>
References: <1435656648543-4671969.post@n4.nabble.com>
Message-ID: <55926B2E.9080507@treenet.co.nz>

On 30/06/2015 9:30 p.m., masterx81 wrote:
> Hi...
> I'm trying to limit download bandwidth to some user groups based on AD using
> external helpers, using the following command:
> delay_pools 1
> delay_class 1 1
> delay_access 1 allow InternetLimitato InternetLibero InternetCentralino
> !CONNECT
> delay_parameters 1 500000/500000
> 
> "InternetLimitato InternetLibero InternetCentralino" are some AD groups, and
> i want to add theyr traffic to the 1 delay pool, but with this configuration
> not work. Else, if i apply the delay pool to "all" all works as expected.
> 
> It's like this by design?

Yes. delay_access is a "fast"/synchroniuous category control.
<http://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>

The way to use group checks in the fast category access controls is with
helper annotations and the "note" type ACL.

NP: The Negotiate/Kerberos helper is the only one currently presenting
group=X annotations. Thats the *auth* helper, not the external ACL group
check. You may need to write a wrapper script around your chosen group
helper to add the kv-pair.

Amos



From vdoctor at neuf.fr  Tue Jun 30 10:05:49 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 30 Jun 2015 03:05:49 -0700 (PDT)
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435657372098-4671970.post@n4.nabble.com>
References: <55914C31.3000500@treenet.co.nz>
 <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com> <55925BD5.10108@treenet.co.nz>
 <1435654549375-4671966.post@n4.nabble.com> <559264F1.6020106@treenet.co.nz>
 <1435657372098-4671970.post@n4.nabble.com>
Message-ID: <1435658749074-4671972.post@n4.nabble.com>

Hi,

Could the issue be related to that ?
TCP_MEM_HIT/200 224442 GET
http://squid1/8b26b519d740afd8ec698b6af06efd8e17c6e5b6:8182/squid-internal-periodic/store_digest
- HIER_NONE/- application/cache-digest

Is it normal to see the store-digest as a MEM_HIT ?
I say to the squid to do not reply with a HIT:
acl cachedigest rep_mime_type application/cache-digest
store_miss deny cachedigest
send_hit deny cachedigest

In my squids, i have 64 kb as the maximum object in memory:
maximum_object_size_in_memory 64 KB
How a 244 kb object can be saved in memory 

More exploring, more weird... 

Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-MISS-504-in-cache-peer-tp4671944p4671972.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 10:44:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jun 2015 22:44:05 +1200
Subject: [squid-users] TCP_MISS/504 in cache_peer
In-Reply-To: <1435657372098-4671970.post@n4.nabble.com>
References: <1435582226950-4671946.post@n4.nabble.com>
 <55914C31.3000500@treenet.co.nz> <1435649066566-4671955.post@n4.nabble.com>
 <201506300854.39358.Antony.Stone@squid.open.source.it>
 <1435650346019-4671957.post@n4.nabble.com> <559252D5.8040203@treenet.co.nz>
 <1435652481249-4671962.post@n4.nabble.com> <55925BD5.10108@treenet.co.nz>
 <1435654549375-4671966.post@n4.nabble.com> <559264F1.6020106@treenet.co.nz>
 <1435657372098-4671970.post@n4.nabble.com>
Message-ID: <559272F5.1080102@treenet.co.nz>

On 30/06/2015 9:42 p.m., Stakres wrote:
> Amos,
> Yes, similar case here on the 4223.
> By reading the case 4223, we can see that part "Non-cacheable objects should
> never be added to the digest." from you.
> In my squid, there is no restriction, ICP is fully open, squid server
> (3.5.5) are compiled with the digest option, so all is done to allow
> ICP/digest connection and exchange.
> So, why the servers think they got the objects, especially when they are not
> cacheable and not cached ?

They have *an* object cached for the URL. But not the one the client is
requesting from that same URL. Or its got some revalidation requirements
attached. Thats what Chudy found. As a result the sibling is unable to
supply its cached object as the answer.

> 
> To me, it seems the servers think they have the object but they don't, so
> they reply with a 404 translated by a 504 to the squid client because
> sibling archi.
> I could understand it could be a bug but the squid client should see the 504
> and should request the object from internet, no ?

For example;
 the client wants object no more than 60 seconds old with unique label
"ETag:blahblah". But the sibling has a 90 second old object labeled
"ETag:oops".

It would have to contact the origin to get a new copy, which is
forbidden by the sibling relationship Cache-Control:only-if-cached criteria.


It's a common problem when using ICP or cache digests which consider
only the URL to identify objects. HTCP was created to resolve that
problem by considering the whole HTTP header when testing the peer for
objects. Though I'm not sure myself if it would resolve the
only-if-cached issue.


> 
> At the moment, i use a "retry_on_error on" as a workaround but not sure it's
> fixing all 504.
> 

Strange if that is working.

The reforwarding happens on 502 and 504 status without any special
configuration. That directive just adds 403, 500, 501 and 503 to the
list of status that can be retried.


> Then, having a dedicated cable between squid servers is not a realistic
> solution, my ISP will not see that as a serious solution 
> 
> Last point, "no-tproxy rule on parent type cache_peer", it does not work, we
> tried that.
> We applied that option 1 month ago and the internet sees the squid ip, not
> the original ip address, so maybe another bug here... 

Nod. Without the dedicated link between proxies and TPROXY the parent
(or sibling) wont be aware of what IP the original client was using.


Amos


From tomtux007 at gmail.com  Tue Jun 30 11:09:07 2015
From: tomtux007 at gmail.com (Tom Tom)
Date: Tue, 30 Jun 2015 13:09:07 +0200
Subject: [squid-users] Squid 3.5.5 automatically reload itself in 2h
	rhythm
In-Reply-To: <5583E99D.3030306@treenet.co.nz>
References: <CACLJR+Osho2qr+TrmBFyQWZszj9ufaaA4o4b7XD2A1+oW1Qtxg@mail.gmail.com>
 <20150618091957.GK6799@charite.de>
 <CACLJR+MX-e1=A86vLYyKU0vyzLYYmFwKvfqZBRLapJ=XVdwZ-w@mail.gmail.com>
 <5582AEF9.5030106@treenet.co.nz>
 <CACLJR+Pn=ufduStHCCRwXSdvSsqkZw5_0yXui0ZmDhwwQq897A@mail.gmail.com>
 <5583E99D.3030306@treenet.co.nz>
Message-ID: <CACLJR+PDcZgheQKM+Og4gyxo_XWm1hGppHdQJPwkmtq3CMsAjA@mail.gmail.com>

Hi Amos

On Fri, Jun 19, 2015 at 12:06 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 19/06/2015 5:23 a.m., Tom Tom wrote:
>> Hi
>>
>> gdb shows the following:
>>
>>
>
>> #3  0x00007ff7ad7d31d2 in __GI___assert_fail (assertion=0x83314d "0",
>> file=0x8114cb "hash.cc", line=240,
>>     function=0x842020 <hash_remove_link::__PRETTY_FUNCTION__> "void
>> hash_remove_link(hash_table*, hash_link*)") at assert.c:101
>> #4  0x00000000007e1586 in hash_remove_link (hid=0x10249c0,
>> hl=0x204e060) at hash.cc:240
>> #5  0x0000000000685296 in Auth::User::cacheCleanup
>> (datanotused=<optimized out>) at User.cc:208
>
> Ah. The auth username cache again.

Seems like a known issue.....? Is there a patch available? Do you need
more information?

Many thanks.
Tom

>
> Amos
>


From hack.back at hotmail.com  Tue Jun 30 13:07:38 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 30 Jun 2015 06:07:38 -0700 (PDT)
Subject: [squid-users] bypass pinned connection
Message-ID: <1435669658091-4671975.post@n4.nabble.com>

most app's in mobiles used pinned connection.
how we can automatically bypass any pinned connection come to squid and
none_bump it ?
is there a way to make that become automatically ?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/bypass-pinned-connection-tp4671975.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 30 15:47:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 01 Jul 2015 03:47:40 +1200
Subject: [squid-users] bypass pinned connection
In-Reply-To: <1435669658091-4671975.post@n4.nabble.com>
References: <1435669658091-4671975.post@n4.nabble.com>
Message-ID: <5592BA1C.5000908@treenet.co.nz>

On 1/07/2015 1:07 a.m., HackXBack wrote:
> most app's in mobiles used pinned connection.
> how we can automatically bypass any pinned connection come to squid and
> none_bump it ?
> is there a way to make that become automatically ?


I assume you mean Squid definition of pinning (and not the Chrome
terminology for TLS pinning, or the mobile phones terminology for radio
link pinning) after my last message.

A pinned connection in Squid terminology is a server connection which
MUST have a 1:1 relationship with some client connection. Closure or
un-pinning of either one terminates both connections.

They are used only when the client needs exclusive use of the server
connection (such as CONNECT tunnel or SSL-Bump decryption), or when some
abuse of protocol is being done which mandates the two TCP connections
not be re-used by other clients transactions (such a NTLM/Negotiate
"connection auth" or SSL-Bump decryption).

Amos



From ncrogers at gmail.com  Tue Jun 30 16:21:54 2015
From: ncrogers at gmail.com (Nick Rogers)
Date: Tue, 30 Jun 2015 09:21:54 -0700
Subject: [squid-users] tcp_outgoing_address binds to wrong interface
In-Reply-To: <55921C8B.9000706@treenet.co.nz>
References: <CAKOb=YaQyG_hDPR7K0F8Mu3nX7LPM8Gifrc7NC+yYy3RHN_PKg@mail.gmail.com>
 <55921C8B.9000706@treenet.co.nz>
Message-ID: <CAKOb=YYhX326Y8H0R8=XSFdBt3HLn__kQHeYNOs54xzLFRpOHw@mail.gmail.com>

On Mon, Jun 29, 2015 at 9:35 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 30/06/2015 8:54 a.m., Nick Rogers wrote:
> > Hello,
> >
> > I am experiencing an issue with squid 3.5.5 and FreeBSD 10.1 where
> > tcp_outgoing_address correctly rewrites the source address of outgoing
> > packets, but fails to bind the socket to the correct interface. I've been
> > using this kind of setup/configuration for quite some time (since the
> squid
> > 2.7 days), so I believe something between squid 3.3 and squid 3.5 has
> > broken this behavior, or it is a problem with FreeBSD 10.x that I do not
> > understand. FWIW squid 3.3.3 on FreeBSD 9.x behaves correctly with the
> same
> > config.
>
> FYI: Squid has nothing to do with interface binding. That is 100%
> internal to the kernel routing stack.
>
> All Squid is able to do is set the IP and port for the connection as a
> hint to the routing. Whether the hint is taken and used is outside our
> control.
>

Yeah, that is what I thought. I was hoping there was a necessary flag that
needed to be set on the socket that is different for FreeBSD versus linux
and what not, that perhaps may have changed or been removed sometime
recently.

I will take try and take it up with the FreeBSD lists. Thank you.



> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150630/8204537a/attachment.htm>

From alex_wu2012 at hotmail.com  Tue Jun 30 17:08:18 2015
From: alex_wu2012 at hotmail.com (Alex Wu)
Date: Tue, 30 Jun 2015 10:08:18 -0700
Subject: [squid-users] sslbump and caching of generated cert
In-Reply-To: <55922067.2000401@treenet.co.nz>
References: <BAY181-W9171C1A428C30E7D1B67E083AA0@phx.gbl>,
 <55922067.2000401@treenet.co.nz>
Message-ID: <BAY181-W580B9DA56461371B34C00B83A90@phx.gbl>

/*
You could assign two workers, each with a different http_port and
ssl_crtd helper using different cert databases.

*/

How to do this? It sounds it might meet our need. 

The reason is that we assign a port for internal, 
so we can use cheap CA (self-generated CA), for the collaboration, we use a diffrent port, 
may need to set up a different CA.

THX

Alex

> Date: Tue, 30 Jun 2015 16:51:51 +1200
> From: squid3 at treenet.co.nz
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] sslbump and caching of generated cert
> 
> On 30/06/2015 5:35 a.m., Alex Wu wrote:
> > So far as I know, hen sslbump is enabled for a port, for each dns
> > name, squid save a cert generated according to dns name and signing
> > key (from http_port configuration). So the next time, the generated
> > cert can be fetched if the same dns host and configured signing key. 
> 
> Signing key is just a validation check on the cert. It has nothing else
> to do with the actual cert.
> 
> AFAIK generated certs are stored by DN, serial number or hash of the two.
> 
> > Now  have a question on this:
> > 
> > http_port 10045 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB
> > key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10045.pem
> > cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10045.pem http_port
> > 10046 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB
> > key=/opt/bg/deploy/squid/etc/mydlp/ssl/key_10046.pem
> > cert=/opt/bg/deploy/squid/etc/mydlp/ssl/cert_10046.pem I have two
> > ports configured with SSLBUMP. Each port has its own CA signing key.
> > The desired behavior is that, for the hostname www.foo.com, the
> > certificate generated for the port should use key_10045, and the
> > certificate generated for the port should use key_10046. It seems OK.
> >  But, if we look at the ssl_db, only the last generated certificate
> > is cached for www.foo.com. Is it possible to cache the generated
> > certificates by the host and signing key? Alex
> 
> Not in the current design.
> 
> You could assign two workers, each with a different http_port and
> ssl_crtd helper using different cert databases.
> 
> 
> What is the point of this anyway? Why do you want to make your users
> face a constant stream of nasty certificate-changed errors?
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150630/e08e393f/attachment.htm>

From chrisgreene at live.com  Tue Jun 30 18:21:49 2015
From: chrisgreene at live.com (Chris Greene)
Date: Tue, 30 Jun 2015 14:21:49 -0400
Subject: [squid-users] Transparent Proxy Configuration
Message-ID: <SNT146-DS26AC8FBFCF64D2697ECB76DEA90@phx.gbl>

I?ve had Squid running on Ubuntu for a few weeks.  I?d configured the proxy 
settings in the browsers.  Everything has been working well and I've been 
pleased with the results.  But now I need to make this a transparent proxy 
and I?m running into trouble & need some help.

I?ve got a Destination NAT rule set up on my router to forward TCP port 80 
traffic to my proxy.  And I removed proxy configuration settings from the 
browsers.  After enabling this DNAT rule, I see requests being logged to 
/var/log/squid3/access.log.

Results when navigating to http://www.google.com:
The following error was encountered while trying to retrieve the URL: /
   Invalid URL
Some aspect of the requested URL is incorrect.
Some possible problems are:
-Missing or incorrect access protocol (should be ?http://? or similar)
-Missing hostname
-Illegal double-escape in the URL-Path
-Illegal character in hostname; underscores are not allowed.


Next, I added "intercept" to http_port like so:
   "http_port  192.166.2.55:3128  intercept"
Results: Access Denied.

My abbreviated /etc/squid3/squid.conf looks like this:
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow all

I'm new to Squid/Ubuntu, so I likely overlooked something.  What am I 
missing?  What troubleshooting step(s) should I take next?
-DG




From acrow at integrafin.co.uk  Tue Jun 30 18:48:50 2015
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 30 Jun 2015 19:48:50 +0100
Subject: [squid-users] Squid 3.5.5 CentOS RPMs release
In-Reply-To: <CAJ+Q1PX_7MM=7i5zL=3NqzVjBdbVHXO8Z84BSiNV5euNWC=NfQ@mail.gmail.com>
References: <55900BBB.4070902@ngtech.co.il>
 <CAJ+Q1PX_7MM=7i5zL=3NqzVjBdbVHXO8Z84BSiNV5euNWC=NfQ@mail.gmail.com>
Message-ID: <5592E492.2000609@integrafin.co.uk>

Thanks for this Eliezer - however I can't rebuild the SRPM on latest CentOS:

configure: Authentication support enabled: yes
checking for ldap.h... (cached) no
checking winldap.h usability... no
checking winldap.h presence... no
checking for winldap.h... no
configure: error: Basic auth helper LDAP ... found but cannot be built
error: Bad exit status from /var/tmp/rpm-tmp.EEzUBx (%build)

And I definitely have openldap-devel installed. I'm not sure where the 
(cached) comes from, but it's the same for both 3.4.4 and 3.4.5 SRPMs.

Best regards

Alex

On 29/06/15 00:22, Alex Samad wrote:
> Thanks
>
> On 29 June 2015 at 00:59, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey list,
>>
>> I have created the new RPM's for CentOS 6 and 7 while not mentioning I also
>> created the package for OracleLinux.(which was very annoy to find out that
>> the download file from Oracle was not matching an ISO but something else)
>>
>> The 3.5.5 and 3.5.4 was published here:
>> http://www1.ngtech.co.il/wpe/?p=90
>>
>> Eliezer
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Tue Jun 30 19:34:33 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 30 Jun 2015 22:34:33 +0300
Subject: [squid-users] Squid 3.5.5 CentOS RPMs release
In-Reply-To: <5592E492.2000609@integrafin.co.uk>
References: <55900BBB.4070902@ngtech.co.il>
 <CAJ+Q1PX_7MM=7i5zL=3NqzVjBdbVHXO8Z84BSiNV5euNWC=NfQ@mail.gmail.com>
 <5592E492.2000609@integrafin.co.uk>
Message-ID: <5592EF49.1060005@ngtech.co.il>

If you will look on the configure options I have used in the RPMs you 
would have seen that I changed\removed a helper or two from the build.
I didn't had time to inspect the issue yet.
How do you rebuild from SRPM?(important)

Eliezer

On 30/06/2015 21:48, Alex Crow wrote:
> Thanks for this Eliezer - however I can't rebuild the SRPM on latest
> CentOS:
>
> configure: Authentication support enabled: yes
> checking for ldap.h... (cached) no
> checking winldap.h usability... no
> checking winldap.h presence... no
> checking for winldap.h... no
> configure: error: Basic auth helper LDAP ... found but cannot be built
> error: Bad exit status from /var/tmp/rpm-tmp.EEzUBx (%build)
>
> And I definitely have openldap-devel installed. I'm not sure where the
> (cached) comes from, but it's the same for both 3.4.4 and 3.4.5 SRPMs.
>
> Best regards
>
> Alex
>
> On 29/06/15 00:22, Alex Samad wrote:
>> Thanks
>>
>> On 29 June 2015 at 00:59, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>> Hey list,
>>>
>>> I have created the new RPM's for CentOS 6 and 7 while not mentioning
>>> I also
>>> created the package for OracleLinux.(which was very annoy to find out
>>> that
>>> the download file from Oracle was not matching an ISO but something
>>> else)
>>>
>>> The 3.5.5 and 3.5.4 was published here:
>>> http://www1.ngtech.co.il/wpe/?p=90
>>>
>>> Eliezer
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




From jlay at slave-tothe-box.net  Tue Jun 30 20:26:33 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Tue, 30 Jun 2015 14:26:33 -0600
Subject: [squid-users] Transparent Proxy Configuration
In-Reply-To: <SNT146-DS26AC8FBFCF64D2697ECB76DEA90@phx.gbl>
References: <SNT146-DS26AC8FBFCF64D2697ECB76DEA90@phx.gbl>
Message-ID: <154b3e485b7615c678f376a501145a82@localhost>

On 2015-06-30 12:21 PM, Chris Greene wrote:
> I?ve had Squid running on Ubuntu for a few weeks.  I?d configured the
> proxy settings in the browsers.  Everything has been working well and
> I've been pleased with the results.  But now I need to make this a
> transparent proxy and I?m running into trouble & need some help.
> 
> I?ve got a Destination NAT rule set up on my router to forward TCP
> port 80 traffic to my proxy.  And I removed proxy configuration
> settings from the browsers.  After enabling this DNAT rule, I see
> requests being logged to /var/log/squid3/access.log.
> 
> Results when navigating to http://www.google.com:
> The following error was encountered while trying to retrieve the URL: /
>   Invalid URL
> Some aspect of the requested URL is incorrect.
> Some possible problems are:
> -Missing or incorrect access protocol (should be ?http://? or similar)
> -Missing hostname
> -Illegal double-escape in the URL-Path
> -Illegal character in hostname; underscores are not allowed.
> 
> 
> Next, I added "intercept" to http_port like so:
>   "http_port  192.166.2.55:3128  intercept"
> Results: Access Denied.
> 
> My abbreviated /etc/squid3/squid.conf looks like this:
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow all
> 
> I'm new to Squid/Ubuntu, so I likely overlooked something.  What am I
> missing?  What troubleshooting step(s) should I take next?
> -DG
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

What's your DNAT line?  Assuming squid is on the box that you're running 
the DNAT line on...here's mine...redirect is all you need if the 
firewall/gateway is on the same box as squid:

$IPTABLES -t nat -A PREROUTING -i eth0 -s 192.168.1.96/28 -p tcp --dport 
80 -j REDIRECT --to-port 3128

And parts of my squid.conf:

acl localnet src 192.168.1.0/24

acl Safe_ports port 80
acl Safe_ports port 443

acl CONNECT method CONNECT
acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"

http_access deny !Safe_ports
http_access deny CONNECT !SSL_Ports

http_access allow SSL_ports
http_access allow localnet
http_access deny all

http_port 3128 intercept


James


From acrow at integrafin.co.uk  Tue Jun 30 20:27:29 2015
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 30 Jun 2015 21:27:29 +0100
Subject: [squid-users] Squid 3.5.5 CentOS RPMs release
In-Reply-To: <5592EF49.1060005@ngtech.co.il>
References: <55900BBB.4070902@ngtech.co.il>
 <CAJ+Q1PX_7MM=7i5zL=3NqzVjBdbVHXO8Z84BSiNV5euNWC=NfQ@mail.gmail.com>
 <5592E492.2000609@integrafin.co.uk> <5592EF49.1060005@ngtech.co.il>
Message-ID: <5592FBB1.8020401@integrafin.co.uk>

Thanks for the quick reply. I managed to fix it, by removing my old 
rpmbuild directory and starting again, and of course making sure that 
gcc-c++ was installed (which it wasn't!)

Cheers

Alex

On 30/06/15 20:34, Eliezer Croitoru wrote:
> If you will look on the configure options I have used in the RPMs you 
> would have seen that I changed\removed a helper or two from the build.
> I didn't had time to inspect the issue yet.
> How do you rebuild from SRPM?(important)
>
> Eliezer
>
> On 30/06/2015 21:48, Alex Crow wrote:
>> Thanks for this Eliezer - however I can't rebuild the SRPM on latest
>> CentOS:
>>
>> configure: Authentication support enabled: yes
>> checking for ldap.h... (cached) no
>> checking winldap.h usability... no
>> checking winldap.h presence... no
>> checking for winldap.h... no
>> configure: error: Basic auth helper LDAP ... found but cannot be built
>> error: Bad exit status from /var/tmp/rpm-tmp.EEzUBx (%build)
>>
>> And I definitely have openldap-devel installed. I'm not sure where the
>> (cached) comes from, but it's the same for both 3.4.4 and 3.4.5 SRPMs.
>>
>> Best regards
>>
>> Alex
>>
>> On 29/06/15 00:22, Alex Samad wrote:
>>> Thanks
>>>
>>> On 29 June 2015 at 00:59, Eliezer Croitoru <eliezer at ngtech.co.il> 
>>> wrote:
>>>> Hey list,
>>>>
>>>> I have created the new RPM's for CentOS 6 and 7 while not mentioning
>>>> I also
>>>> created the package for OracleLinux.(which was very annoy to find out
>>>> that
>>>> the download file from Oracle was not matching an ISO but something
>>>> else)
>>>>
>>>> The 3.5.5 and 3.5.4 was published here:
>>>> http://www1.ngtech.co.il/wpe/?p=90
>>>>
>>>> Eliezer
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From mcsnv96 at afo.net  Tue Jun 30 20:48:57 2015
From: mcsnv96 at afo.net (Mike)
Date: Tue, 30 Jun 2015 15:48:57 -0500
Subject: [squid-users] acl for redirect
In-Reply-To: <558D9A05.60005@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
Message-ID: <559300B9.4020601@afo.net>

Scratch that (my previous email to this list), google disabled their 
insecure sites when used as part of a redirect. We as individual users 
can use that url directly in the browser ( 
http://www.google.com/webhp?nord=1 ) but any google page load starts 
with secure page causing that redirect to fail... The newer 3.1.2 
e2guardian SSL MITM requires options (like a der certificate file) that 
cannot be used with thousands of existing users on our system, so squid 
may be our only option.

Another issue right now is google is using a "VPN-style" internal 
redirect on their server, so e2guardian (shown in log) sees 
https://www.google.com:443  CONNECT, passes along TCP_TUNNEL/200 
www.google.com:443 to squid (shown in squid log), and after that it is 
directly between google and the browser, not allowing e2guardian nor 
squid to see further urls from google (such as search terms) for the 
rest of that specific session. Can click news, maps, images, videos, and 
NONE of these are passed along to the proxy.

So my original question still stands, how to set an acl for google urls 
that references a file with blocked terms/words/phrases, and denies it 
if those terms are found (like a black list)?

Another option I thought of is since the meta content in the code 
including title is passed along, so is there a way to have it can the 
header or title content as part of the acl "content scan" process?


Thanks
Mike


On 6/26/2015 13:29 PM, Mike wrote:
> Nevermind... I found another fix within e2guardian:
>
> etc/e2guardian/lists/urlregexplist
>
> Added this entry:
> # Disable Google SSL Search
> # allows e2g to filter searches properly
> "^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1" 
>
>
> This means whenever google.com or www.google.com is typed in the 
> address bar, it loads the insecure page and allows e2guardian to 
> properly filter whatever search terms they type in. This does break 
> other aspects such as google toolbars, using the search bar at upper 
> right of many browsers with google as the set search engine, and other 
> ways, but that is an issue we can live with.
>
> On 26/06/2015 2:36 a.m., Mike wrote:
>> Amos, thanks for info.
>>
>> The primary settings being used in squid.conf:
>>
>> http_port 8080
>> # this port is what will be used for SSL Proxy on client browser
>> http_port 8081 intercept
>>
>> https_port 8082 intercept ssl-bump connection-auth=off
>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
>> sslcrtd_children 50 startup=5 idle=1
>> ssl_bump server-first all
>> ssl_bump none localhost
>>
>>
>> Then e2guardian uses 10101 for the browsers, and uses 8080 for
>> connecting to squid on the same server.
> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
> connection in re-encrypted on outgoing.
>
>
> I am doubtful eth nord works anymore since Googles own documentation for
> schools states that one must install a MITM proxy that does the traffic
> filtering - e2guardian is not one of those. IMO you should convert your
> e2guardian config into Squid ACL rules that can be applied to the bumped
> traffic without forcinghttp://
>
> But if nord does work, so should the deny_info in Squid. Something like
> this probably:
>
>   acl google dstdomain .google.com
>   deny_info 301:http://%H%R?nord=1  google
>
>   acl GwithQuery urlpath_regex ?
>   deny_info 301:http://%H%R&nord=1  GwithQuery
>
>   http_access deny google Gquery
>   http_access deny google
>
>
> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From randalcowen at gmail.com  Tue Jun 30 20:52:26 2015
From: randalcowen at gmail.com (Randal Cowen)
Date: Tue, 30 Jun 2015 15:52:26 -0500
Subject: [squid-users] TFD-CONNECT (501 errors)
Message-ID: <CAHObhNUw-_Us0gAxyag-_xB7iTsV3Kb9SL06DY9SVMf2otKUOw@mail.gmail.com>

For years I've been successfully running a squid. Last Wednesday the 17th
magically only HTTPS requests fail over only AT&T's cellular network....

Everything still works great on any other land-line provider I've tested
including AT&T's DSL service. Typically my logs show

1435691713.787 240084 <Source IP> TCP_TUNNEL/200 381 CONNECT
www.google.com:443 - HIER_DIRECT/216.239.32.20 -

but now magically over the AT&T cell network they come in..

1435692019.503      0 <Source IP> TAG_NONE/501 4175 TFD-CONNECT
https://iecvlist.microsoft.com/ - HIER_NONE/- text/html

Notice the odd "TFD-CONNECT" which I assume is 501 "Not Implemented" along
with the URL now containing the https:// prefix...

I'm not finding much on the TFD-CONNECT, what I am finding is leading me to
believe AT&T has possibly enabled their "Toll Free Data" in my area and is
messing with my headers/proxy tunnel.

Has anyone else been experiencing this? Or have any helpful clues?

I have even downloaded and recompiled a completely new box for testing with
the same behavior. The new box is

Squid Cache: Version 3.5.5-20150624-r13848
Service Name: squid
configure options:  '--prefix=/usr' '--includedir=/usr/include'
'--datadir=/usr/share' '--bindir=/usr/sbin'
'--libexecdir=/usr/lib64/squid--localstatedir=/var'
'--sysconfdir=/etc/squid' --enable-ltdl-convenience

Thanks,

Randal
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150630/a940e488/attachment.htm>

From rafael.akchurin at diladele.com  Tue Jun 30 21:17:34 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 30 Jun 2015 21:17:34 +0000
Subject: [squid-users] acl for redirect
In-Reply-To: <559300B9.4020601@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
Message-ID: <VI1PR04MB13594DCDC20D7C3916BEE1328FA90@VI1PR04MB1359.eurprd04.prod.outlook.com>

Hello Mike,

May be it is time to take a look at ICAP/eCAP protocol implementations which target specifically this problem - filtering within the *contents* of the stream on Squid?

Best regards,
Rafael

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike
Sent: Tuesday, June 30, 2015 10:49 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] acl for redirect

Scratch that (my previous email to this list), google disabled their insecure sites when used as part of a redirect. We as individual users can use that url directly in the browser (
http://www.google.com/webhp?nord=1 ) but any google page load starts with secure page causing that redirect to fail... The newer 3.1.2 e2guardian SSL MITM requires options (like a der certificate file) that cannot be used with thousands of existing users on our system, so squid may be our only option.

Another issue right now is google is using a "VPN-style" internal redirect on their server, so e2guardian (shown in log) sees
https://www.google.com:443  CONNECT, passes along TCP_TUNNEL/200
www.google.com:443 to squid (shown in squid log), and after that it is directly between google and the browser, not allowing e2guardian nor squid to see further urls from google (such as search terms) for the rest of that specific session. Can click news, maps, images, videos, and NONE of these are passed along to the proxy.

So my original question still stands, how to set an acl for google urls that references a file with blocked terms/words/phrases, and denies it if those terms are found (like a black list)?

Another option I thought of is since the meta content in the code including title is passed along, so is there a way to have it can the header or title content as part of the acl "content scan" process?


Thanks
Mike


On 6/26/2015 13:29 PM, Mike wrote:
> Nevermind... I found another fix within e2guardian:
>
> etc/e2guardian/lists/urlregexplist
>
> Added this entry:
> # Disable Google SSL Search
> # allows e2g to filter searches properly 
> "^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1"
>
>
> This means whenever google.com or www.google.com is typed in the 
> address bar, it loads the insecure page and allows e2guardian to 
> properly filter whatever search terms they type in. This does break 
> other aspects such as google toolbars, using the search bar at upper 
> right of many browsers with google as the set search engine, and other 
> ways, but that is an issue we can live with.
>
> On 26/06/2015 2:36 a.m., Mike wrote:
>> Amos, thanks for info.
>>
>> The primary settings being used in squid.conf:
>>
>> http_port 8080
>> # this port is what will be used for SSL Proxy on client browser 
>> http_port 8081 intercept
>>
>> https_port 8082 intercept ssl-bump connection-auth=off 
>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB 
>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key 
>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-
>> RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 
>> 16MB sslcrtd_children 50 startup=5 idle=1 ssl_bump server-first all 
>> ssl_bump none localhost
>>
>>
>> Then e2guardian uses 10101 for the browsers, and uses 8080 for 
>> connecting to squid on the same server.
> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS 
> connection in re-encrypted on outgoing.
>
>
> I am doubtful eth nord works anymore since Googles own documentation 
> for schools states that one must install a MITM proxy that does the 
> traffic filtering - e2guardian is not one of those. IMO you should 
> convert your e2guardian config into Squid ACL rules that can be 
> applied to the bumped traffic without forcinghttp://
>
> But if nord does work, so should the deny_info in Squid. Something 
> like this probably:
>
>   acl google dstdomain .google.com
>   deny_info 301:http://%H%R?nord=1  google
>
>   acl GwithQuery urlpath_regex ?
>   deny_info 301:http://%H%R&nord=1  GwithQuery
>
>   http_access deny google Gquery
>   http_access deny google
>
>
> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From marcus.kool at urlfilterdb.com  Tue Jun 30 22:30:03 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 30 Jun 2015 19:30:03 -0300
Subject: [squid-users] acl for redirect
In-Reply-To: <559300B9.4020601@afo.net>
References: <680845082.661801514.1435308010746.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <558D25AB.8030802@treenet.co.nz> <558D9A05.60005@afo.net>
 <559300B9.4020601@afo.net>
Message-ID: <5593186B.5050705@urlfilterdb.com>

I suggest to read this:
https://support.google.com/websearch/answer/186669

and look at option 3 of section 'Keep SafeSearch turned on for your network'

Marcus


On 06/30/2015 05:48 PM, Mike wrote:
> Scratch that (my previous email to this list), google disabled their insecure sites when used as part of a redirect. We as individual users can use that url directly in the browser (
> http://www.google.com/webhp?nord=1 ) but any google page load starts with secure page causing that redirect to fail... The newer 3.1.2 e2guardian SSL MITM requires options (like a der certificate
> file) that cannot be used with thousands of existing users on our system, so squid may be our only option.
>
> Another issue right now is google is using a "VPN-style" internal redirect on their server, so e2guardian (shown in log) sees https://www.google.com:443  CONNECT, passes along TCP_TUNNEL/200
> www.google.com:443 to squid (shown in squid log), and after that it is directly between google and the browser, not allowing e2guardian nor squid to see further urls from google (such as search terms)
> for the rest of that specific session. Can click news, maps, images, videos, and NONE of these are passed along to the proxy.
>
> So my original question still stands, how to set an acl for google urls that references a file with blocked terms/words/phrases, and denies it if those terms are found (like a black list)?
>
> Another option I thought of is since the meta content in the code including title is passed along, so is there a way to have it can the header or title content as part of the acl "content scan" process?
>
>
> Thanks
> Mike
>
>
> On 6/26/2015 13:29 PM, Mike wrote:
>> Nevermind... I found another fix within e2guardian:
>>
>> etc/e2guardian/lists/urlregexplist
>>
>> Added this entry:
>> # Disable Google SSL Search
>> # allows e2g to filter searches properly
>> "^https://www.google.[a-z]{2,6}(.*)"->"http://www.google.com/webhp?nord=1"
>>
>> This means whenever google.com or www.google.com is typed in the address bar, it loads the insecure page and allows e2guardian to properly filter whatever search terms they type in. This does break
>> other aspects such as google toolbars, using the search bar at upper right of many browsers with google as the set search engine, and other ways, but that is an issue we can live with.
>>
>> On 26/06/2015 2:36 a.m., Mike wrote:
>>> Amos, thanks for info.
>>>
>>> The primary settings being used in squid.conf:
>>>
>>> http_port 8080
>>> # this port is what will be used for SSL Proxy on client browser
>>> http_port 8081 intercept
>>>
>>> https_port 8082 intercept ssl-bump connection-auth=off
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
>>> cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key
>>> cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH
>>>
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid_ssl_db -M 16MB
>>> sslcrtd_children 50 startup=5 idle=1
>>> ssl_bump server-first all
>>> ssl_bump none localhost
>>>
>>>
>>> Then e2guardian uses 10101 for the browsers, and uses 8080 for
>>> connecting to squid on the same server.
>> Doesn;t matter. Due to TLS security requirements Squid ensures the TLS
>> connection in re-encrypted on outgoing.
>>
>>
>> I am doubtful eth nord works anymore since Googles own documentation for
>> schools states that one must install a MITM proxy that does the traffic
>> filtering - e2guardian is not one of those. IMO you should convert your
>> e2guardian config into Squid ACL rules that can be applied to the bumped
>> traffic without forcinghttp://
>>
>> But if nord does work, so should the deny_info in Squid. Something like
>> this probably:
>>
>>   acl google dstdomain .google.com
>>   deny_info 301:http://%H%R?nord=1  google
>>
>>   acl GwithQuery urlpath_regex ?
>>   deny_info 301:http://%H%R&nord=1  GwithQuery
>>
>>   http_access deny google Gquery
>>   http_access deny google
>>
>>
>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ahmed.zaeem at netstream.ps  Sun Jun  7 07:40:41 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Sun, 07 Jun 2015 07:40:41 -0000
Subject: [squid-users] TCP_MISS/403 353 HEAD text/plain  Error help !!
In-Reply-To: <55722B1F.4050007@treenet.co.nz>
References: <004101d09fc0$04bbd420$0e337c60$@netstream.ps>
 <55722B1F.4050007@treenet.co.nz>
Message-ID: <003a01d0a148$7d1ca190$7755e4b0$@netstream.ps>

Hi Amos thank you so much 
Again , this App IS REMOTE and as a black box .
It works 100  % on other proxies but I need to let it work on my own proxy.

Now what I did is :
I added 2  directives to my squid.conf :

strip_query_terms off
debug_options 11,2 28,3


then restarted squid and monitored the logs .

again I have 2 files monitored when it worked and when it failed.

But the strange that the problem is it work on some youtube vidoes and don?t work on the others

those files were just as an example :
file 1 didn?t work and give error 403 ===> https://www.youtube.com/watch?v=MYSVMgRr6pw

and other file worked ===>
https://www.youtube.com/watch?v=p0g9_osImd0

now I will test the app with those links , note that that 1sst line will fail and  the 2nd Link will success.

I had graped the log files andf attached them because they are big
Name for failed file is =>debug_failed.txt
Name for succeded file ==>debug_worked.txt

Thanks a lot

cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, June 5, 2015 4:05 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP_MISS/403 353 HEAD text/plain Error help !!

On 6/06/2015 6:47 a.m., snakeeyes wrote:
> Hello All
> 
> I want to ask how could I get out the log
> 
> TCP_MISS/403 353 HEAD text/plain  Error ?????
> 
> I have many logs in cach.log about that and I want to figure out this 
> issue !
> 
> Aby help ?
> 


The posts you made earlier (including the private one) do not have sufficient content to be able to help you.

Firstly, we definitely need to see the full URLs in access.log. So your squid.conf needs to contain "strip_query_terms off" to see what stage the YT request sequence is at.

For tracking down why the status is being given you will want "debug_options 11,2 28,3" to log in cache.og the HTTP request/reply headers and ACL proccessing decisions.

If you can run with debugs set to ALL,5 to get a more full trace that would be excellent. But it will definitely produce a lot of data, so thats optional. The above mentioned two sections should be sufficient to identify teh next steps forward, and will produce a lot of info anyway.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: debug_worked.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150607/27af9ed9/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: debug_fail.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150607/27af9ed9/attachment-0001.txt>

From tmblue at gmail.com  Tue Jun  9 19:59:55 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 09 Jun 2015 19:59:55 -0000
Subject: [squid-users] Squid 3.5 CentOS 6.6 Core keeps dying.
Message-ID: <CAEaSS0Y+bEzq+0nGgK747kAUUaw7jGLSOb8jew3TMP-U8SyaoA@mail.gmail.com>

Just tried to put 3.5 into production and it's dying.  This runs fine with
low volume, but once the volume is up there it dies.

squid-3.5.0.2-1.el6.x86_64

2.6.32-504.16.2.el6.x86_64 #1 SMP


Squid is being handled in memory, so this can't be an I/O disk issue. As I
have other systems running 2.7 in this configuration without issue.


FATAL: Received Segment Violation...dying.

2015/06/09 12:51:19 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:19 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:19 kid1|     65536 entries written so far.

2015/06/09 12:51:19 kid1|    131072 entries written so far.

2015/06/09 12:51:19 kid1|    196608 entries written so far.

2015/06/09 12:51:19 kid1|   Finished.  Wrote 243335 entries.

2015/06/09 12:51:19 kid1|   Took 0.06 seconds (4183026.20 entries/sec).

CPU Usage: 30.585 seconds = 20.918 user + 9.668 sys

Maximum Resident Size: 311424 KB

Page faults with physical i/o: 0

2015/06/09 12:51:22 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:22 kid1| Starting Squid Cache version 3.5.0.2 for
x86_64-redhat-linux-gnu...

2015/06/09 12:51:22 kid1| Service Name: squid

2015/06/09 12:51:22 kid1| Process ID 28874

2015/06/09 12:51:22 kid1| Process Roles: worker

2015/06/09 12:51:22 kid1| With 4096 file descriptors available

2015/06/09 12:51:22 kid1| Initializing IP Cache...

2015/06/09 12:51:22 kid1| DNS Socket created at [::], FD 6

2015/06/09 12:51:22 kid1| DNS Socket created at 0.0.0.0, FD 8

2015/06/09 12:51:22 kid1| Adding domain domain.net from /etc/resolv.conf

2015/06/09 12:51:22 kid1| Adding nameserver 1.1.1.15 from /etc/resolv.conf

2015/06/09 12:51:22 kid1| Adding nameserver 10.13.6.56 from /etc/resolv.conf

2015/06/09 12:51:22 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec

2015/06/09 12:51:22 kid1| Store logging disabled

2015/06/09 12:51:22 kid1| Swap maxSize 14336000 + 1536000 KB, estimated
1220923 objects

2015/06/09 12:51:22 kid1| Target number of buckets: 61046

2015/06/09 12:51:22 kid1| Using 65536 Store buckets

2015/06/09 12:51:22 kid1| Max Mem  size: 1536000 KB

2015/06/09 12:51:22 kid1| Max Swap size: 14336000 KB

2015/06/09 12:51:22 kid1| Rebuilding storage in /cache01 (clean log)

2015/06/09 12:51:22 kid1| Using Least Load store dir selection

2015/06/09 12:51:22 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:22 kid1| Finished loading MIME types and icons.

2015/06/09 12:51:22 kid1| HTCP Disabled.

2015/06/09 12:51:22 kid1| Sending SNMP messages from [::]:3401

2015/06/09 12:51:22 kid1| commBind: Cannot bind socket FD 13 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:22 kid1| commBind: Cannot bind socket FD 14 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:22 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

2015/06/09 12:51:22 kid1| Squid plugin modules loaded: 0

2015/06/09 12:51:22 kid1| Adaptation support is off.

2015/06/09 12:51:22 kid1| Accepting reverse-proxy HTTP Socket connections
at local=[::]:80 remote=[::] FD 11 flags=9

2015/06/09 12:51:22 kid1| Accepting SNMP messages on [::]:3401

2015/06/09 12:51:22 kid1| Store rebuilding is 1.64% complete

2015/06/09 12:51:22 kid1| Configuring Parent apps.domain.net/80/0

2015/06/09 12:51:23 kid1| Done reading /cache01 swaplog (243335 entries)

2015/06/09 12:51:23 kid1| Finished rebuilding storage from disk.

2015/06/09 12:51:23 kid1|    243335 Entries scanned

2015/06/09 12:51:23 kid1|         0 Invalid entries.

2015/06/09 12:51:23 kid1|         0 With invalid flags.

2015/06/09 12:51:23 kid1|    243299 Objects loaded.

2015/06/09 12:51:23 kid1|         0 Objects expired.

2015/06/09 12:51:23 kid1|         0 Objects cancelled.

2015/06/09 12:51:23 kid1|        16 Duplicate URLs purged.

2015/06/09 12:51:23 kid1|        20 Swapfile clashes avoided.

2015/06/09 12:51:23 kid1|   Took 1.10 seconds (220926.04 objects/sec).

2015/06/09 12:51:23 kid1| Beginning Validation Procedure

2015/06/09 12:51:23 kid1|   Completed Validation Procedure

2015/06/09 12:51:23 kid1|   Validated 243298 Entries

2015/06/09 12:51:23 kid1|   store_swap_size = 1129880.00 KB

2015/06/09 12:51:23 kid1| internalStart: unknown request:

GET /squid-internal-static/icons/anthony-unknown.gif HTTP/1.0



2015/06/09 12:51:24 kid1| storeLateRelease: released 3 objects

2015/06/09 12:51:24 kid1| internalStart: unknown request:

GET /squid-internal-static/icons/anthony-unknown.gif HTTP/1.0



FATAL: Received Segment Violation...dying.

2015/06/09 12:51:28 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:28 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:28 kid1|     65536 entries written so far.

2015/06/09 12:51:28 kid1|    131072 entries written so far.

2015/06/09 12:51:28 kid1|    196608 entries written so far.

2015/06/09 12:51:28 kid1|   Finished.  Wrote 243394 entries.

2015/06/09 12:51:28 kid1|   Took 0.06 seconds (4208638.82 entries/sec).

CPU Usage: 1.322 seconds = 0.681 user + 0.641 sys

Maximum Resident Size: 256224 KB

Page faults with physical i/o: 0

2015/06/09 12:51:31 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:31 kid1| Starting Squid Cache version 3.5.0.2 for
x86_64-redhat-linux-gnu...

2015/06/09 12:51:31 kid1| Service Name: squid

2015/06/09 12:51:31 kid1| Process ID 28893

2015/06/09 12:51:31 kid1| Process Roles: worker

2015/06/09 12:51:31 kid1| With 4096 file descriptors available

2015/06/09 12:51:31 kid1| Initializing IP Cache...

2015/06/09 12:51:31 kid1| DNS Socket created at [::], FD 6

2015/06/09 12:51:31 kid1| DNS Socket created at 0.0.0.0, FD 8

2015/06/09 12:51:31 kid1| Adding domain domain.net from /etc/resolv.conf

2015/06/09 12:51:31 kid1| Adding nameserver 1.1.1.15 from /etc/resolv.conf

2015/06/09 12:51:31 kid1| Adding nameserver 10.13.6.56 from /etc/resolv.conf

2015/06/09 12:51:31 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec

2015/06/09 12:51:31 kid1| Store logging disabled

2015/06/09 12:51:31 kid1| Swap maxSize 14336000 + 1536000 KB, estimated
1220923 objects

2015/06/09 12:51:31 kid1| Target number of buckets: 61046

2015/06/09 12:51:31 kid1| Using 65536 Store buckets

2015/06/09 12:51:31 kid1| Max Mem  size: 1536000 KB

2015/06/09 12:51:31 kid1| Max Swap size: 14336000 KB

2015/06/09 12:51:31 kid1| Rebuilding storage in /cache01 (clean log)

2015/06/09 12:51:31 kid1| Using Least Load store dir selection

2015/06/09 12:51:31 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:31 kid1| Finished loading MIME types and icons.

2015/06/09 12:51:31 kid1| HTCP Disabled.

2015/06/09 12:51:31 kid1| Sending SNMP messages from [::]:3401

2015/06/09 12:51:31 kid1| commBind: Cannot bind socket FD 13 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:31 kid1| commBind: Cannot bind socket FD 14 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:31 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

2015/06/09 12:51:31 kid1| Squid plugin modules loaded: 0

2015/06/09 12:51:31 kid1| Adaptation support is off.

2015/06/09 12:51:31 kid1| Accepting reverse-proxy HTTP Socket connections
at local=[::]:80 remote=[::] FD 11 flags=9

2015/06/09 12:51:31 kid1| Accepting SNMP messages on [::]:3401

2015/06/09 12:51:31 kid1| Store rebuilding is 1.64% complete

2015/06/09 12:51:31 kid1| Configuring Parent apps.domain.net/80/0

2015/06/09 12:51:32 kid1| Done reading /cache01 swaplog (243394 entries)

2015/06/09 12:51:32 kid1| Finished rebuilding storage from disk.

2015/06/09 12:51:32 kid1|    243394 Entries scanned

2015/06/09 12:51:32 kid1|         0 Invalid entries.

2015/06/09 12:51:32 kid1|         0 With invalid flags.

2015/06/09 12:51:32 kid1|    243342 Objects loaded.

2015/06/09 12:51:32 kid1|         0 Objects expired.

2015/06/09 12:51:32 kid1|         0 Objects cancelled.

2015/06/09 12:51:32 kid1|        22 Duplicate URLs purged.

2015/06/09 12:51:32 kid1|        30 Swapfile clashes avoided.

2015/06/09 12:51:32 kid1|   Took 1.08 seconds (225998.64 objects/sec).

2015/06/09 12:51:32 kid1| Beginning Validation Procedure

2015/06/09 12:51:32 kid1|   Completed Validation Procedure

2015/06/09 12:51:32 kid1|   Validated 243341 Entries

2015/06/09 12:51:32 kid1|   store_swap_size = 1130044.00 KB

2015/06/09 12:51:33 kid1| storeLateRelease: released 1 objects




FATAL: Received Segment Violation...dying.

2015/06/09 12:51:34 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:34 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:34 kid1|     65536 entries written so far.

2015/06/09 12:51:34 kid1|    131072 entries written so far.

2015/06/09 12:51:34 kid1|    196608 entries written so far.

2015/06/09 12:51:34 kid1|   Finished.  Wrote 243391 entries.

2015/06/09 12:51:34 kid1|   Took 0.06 seconds (4226492.09 entries/sec).

CPU Usage: 1.221 seconds = 0.630 user + 0.591 sys

Maximum Resident Size: 250112 KB

Page faults with physical i/o: 0

2015/06/09 12:51:37 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:37 kid1| Starting Squid Cache version 3.5.0.2 for
x86_64-redhat-linux-gnu...

2015/06/09 12:51:37 kid1| Service Name: squid

2015/06/09 12:51:37 kid1| Process ID 28912

2015/06/09 12:51:37 kid1| Process Roles: worker

2015/06/09 12:51:37 kid1| With 4096 file descriptors available

2015/06/09 12:51:37 kid1| Initializing IP Cache...

2015/06/09 12:51:37 kid1| DNS Socket created at [::], FD 6

2015/06/09 12:51:37 kid1| DNS Socket created at 0.0.0.0, FD 8

2015/06/09 12:51:37 kid1| Adding domain domain.net from /etc/resolv.conf

2015/06/09 12:51:37 kid1| Adding nameserver 1.1.1.15 from /etc/resolv.conf

2015/06/09 12:51:37 kid1| Adding nameserver 10.13.6.56 from /etc/resolv.conf

2015/06/09 12:51:37 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec

2015/06/09 12:51:37 kid1| Store logging disabled

2015/06/09 12:51:37 kid1| Swap maxSize 14336000 + 1536000 KB, estimated
1220923 objects

2015/06/09 12:51:37 kid1| Target number of buckets: 61046

2015/06/09 12:51:37 kid1| Using 65536 Store buckets

2015/06/09 12:51:37 kid1| Max Mem  size: 1536000 KB

2015/06/09 12:51:37 kid1| Max Swap size: 14336000 KB

2015/06/09 12:51:37 kid1| Rebuilding storage in /cache01 (clean log)

2015/06/09 12:51:37 kid1| Using Least Load store dir selection

2015/06/09 12:51:37 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:37 kid1| Finished loading MIME types and icons.

2015/06/09 12:51:37 kid1| HTCP Disabled.

2015/06/09 12:51:37 kid1| Sending SNMP messages from [::]:3401

2015/06/09 12:51:37 kid1| commBind: Cannot bind socket FD 13 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:37 kid1| commBind: Cannot bind socket FD 14 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:37 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

2015/06/09 12:51:37 kid1| Squid plugin modules loaded: 0

2015/06/09 12:51:37 kid1| Adaptation support is off.

2015/06/09 12:51:37 kid1| Accepting reverse-proxy HTTP Socket connections
at local=[::]:80 remote=[::] FD 11 flags=9

2015/06/09 12:51:37 kid1| Accepting SNMP messages on [::]:3401

2015/06/09 12:51:37 kid1| Store rebuilding is 1.64% complete

2015/06/09 12:51:37 kid1| Configuring Parent apps.domain.net/80/0




2015/06/09 12:51:38 kid1| Done reading /cache01 swaplog (243391 entries)

2015/06/09 12:51:38 kid1| Finished rebuilding storage from disk.

2015/06/09 12:51:38 kid1|    243391 Entries scanned

2015/06/09 12:51:38 kid1|         0 Invalid entries.

2015/06/09 12:51:38 kid1|         0 With invalid flags.

2015/06/09 12:51:38 kid1|    243341 Objects loaded.

2015/06/09 12:51:38 kid1|         0 Objects expired.

2015/06/09 12:51:38 kid1|         0 Objects cancelled.

2015/06/09 12:51:38 kid1|        19 Duplicate URLs purged.

2015/06/09 12:51:38 kid1|        31 Swapfile clashes avoided.

2015/06/09 12:51:38 kid1|   Took 1.09 seconds (222890.36 objects/sec).

2015/06/09 12:51:38 kid1| Beginning Validation Procedure

2015/06/09 12:51:38 kid1|   Completed Validation Procedure

2015/06/09 12:51:38 kid1|   Validated 243340 Entries

2015/06/09 12:51:38 kid1|   store_swap_size = 1129972.00 KB



2015/06/09 12:51:39 kid1| storeLateRelease: released 1 objects

FATAL: Received Segment Violation...dying.

2015/06/09 12:51:44 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:44 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:44 kid1|     65536 entries written so far.

2015/06/09 12:51:44 kid1|    131072 entries written so far.

2015/06/09 12:51:44 kid1|    196608 entries written so far.

2015/06/09 12:51:44 kid1|   Finished.  Wrote 243435 entries.

2015/06/09 12:51:44 kid1|   Took 0.06 seconds (4261071.24 entries/sec).

CPU Usage: 1.387 seconds = 0.750 user + 0.637 sys

Maximum Resident Size: 267120 KB

Page faults with physical i/o: 0

2015/06/09 12:51:47 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:47 kid1| Starting Squid Cache version 3.5.0.2 for
x86_64-redhat-linux-gnu...

2015/06/09 12:51:47 kid1| Service Name: squid

2015/06/09 12:51:47 kid1| Process ID 28933

2015/06/09 12:51:47 kid1| Process Roles: worker

2015/06/09 12:51:47 kid1| With 4096 file descriptors available

2015/06/09 12:51:47 kid1| Initializing IP Cache...

2015/06/09 12:51:47 kid1| DNS Socket created at [::], FD 6

2015/06/09 12:51:47 kid1| DNS Socket created at 0.0.0.0, FD 8

2015/06/09 12:51:47 kid1| Adding domain domain.net from /etc/resolv.conf

2015/06/09 12:51:47 kid1| Adding nameserver 1.1.1.15 from /etc/resolv.conf

2015/06/09 12:51:47 kid1| Adding nameserver 10.13.6.56 from /etc/resolv.conf

2015/06/09 12:51:47 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec

2015/06/09 12:51:47 kid1| Store logging disabled

2015/06/09 12:51:47 kid1| Swap maxSize 14336000 + 1536000 KB, estimated
1220923 objects

2015/06/09 12:51:47 kid1| Target number of buckets: 61046

2015/06/09 12:51:47 kid1| Using 65536 Store buckets

2015/06/09 12:51:47 kid1| Max Mem  size: 1536000 KB

2015/06/09 12:51:47 kid1| Max Swap size: 14336000 KB

2015/06/09 12:51:47 kid1| Rebuilding storage in /cache01 (clean log)

2015/06/09 12:51:47 kid1| Using Least Load store dir selection

2015/06/09 12:51:47 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:47 kid1| Finished loading MIME types and icons.

2015/06/09 12:51:47 kid1| HTCP Disabled.

2015/06/09 12:51:47 kid1| Sending SNMP messages from [::]:3401

2015/06/09 12:51:47 kid1| commBind: Cannot bind socket FD 13 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:47 kid1| commBind: Cannot bind socket FD 14 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:47 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

2015/06/09 12:51:47 kid1| Squid plugin modules loaded: 0

2015/06/09 12:51:47 kid1| Adaptation support is off.

2015/06/09 12:51:47 kid1| Accepting reverse-proxy HTTP Socket connections
at local=[::]:80 remote=[::] FD 11 flags=9

2015/06/09 12:51:47 kid1| Accepting SNMP messages on [::]:3401

2015/06/09 12:51:47 kid1| Store rebuilding is 1.64% complete

2015/06/09 12:51:48 kid1| Configuring Parent apps.domain.net/80/0



2015/06/09 12:51:49 kid1| Done reading /cache01 swaplog (243435 entries)

2015/06/09 12:51:49 kid1| Finished rebuilding storage from disk.

2015/06/09 12:51:49 kid1|    243435 Entries scanned

2015/06/09 12:51:49 kid1|         0 Invalid entries.

2015/06/09 12:51:49 kid1|         0 With invalid flags.

2015/06/09 12:51:49 kid1|    243316 Objects loaded.

2015/06/09 12:51:49 kid1|         0 Objects expired.

2015/06/09 12:51:49 kid1|         0 Objects cancelled.

2015/06/09 12:51:49 kid1|        42 Duplicate URLs purged.

2015/06/09 12:51:49 kid1|        77 Swapfile clashes avoided.

2015/06/09 12:51:49 kid1|   Took 1.10 seconds (221373.46 objects/sec).

2015/06/09 12:51:49 kid1| Beginning Validation Procedure

2015/06/09 12:51:49 kid1|   Completed Validation Procedure

2015/06/09 12:51:49 kid1|   Validated 243315 Entries

2015/06/09 12:51:49 kid1|   store_swap_size = 1129896.00 KB




2015/06/09 12:51:50 kid1| storeLateRelease: released 6 objects

FATAL: Received Segment Violation...dying.

2015/06/09 12:51:52 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:52 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:52 kid1|     65536 entries written so far.

2015/06/09 12:51:52 kid1|    131072 entries written so far.

2015/06/09 12:51:52 kid1|    196608 entries written so far.

2015/06/09 12:51:52 kid1|   Finished.  Wrote 243473 entries.

2015/06/09 12:51:52 kid1|   Took 0.06 seconds (4134298.96 entries/sec).

CPU Usage: 1.429 seconds = 0.772 user + 0.657 sys

Maximum Resident Size: 270384 KB

Page faults with physical i/o: 0

2015/06/09 12:51:55 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:55 kid1| Starting Squid Cache version 3.5.0.2 for
x86_64-redhat-linux-gnu...

2015/06/09 12:51:55 kid1| Service Name: squid

2015/06/09 12:51:55 kid1| Process ID 28960

2015/06/09 12:51:55 kid1| Process Roles: worker

2015/06/09 12:51:55 kid1| With 4096 file descriptors available

2015/06/09 12:51:55 kid1| Initializing IP Cache...

2015/06/09 12:51:55 kid1| DNS Socket created at [::], FD 6

2015/06/09 12:51:55 kid1| DNS Socket created at 0.0.0.0, FD 8

2015/06/09 12:51:55 kid1| Adding domain domain.net from /etc/resolv.conf

2015/06/09 12:51:55 kid1| Adding nameserver 1.1.1.15 from /etc/resolv.conf

2015/06/09 12:51:55 kid1| Adding nameserver 10.13.6.56 from /etc/resolv.conf

2015/06/09 12:51:55 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec

2015/06/09 12:51:55 kid1| Store logging disabled

2015/06/09 12:51:55 kid1| Swap maxSize 14336000 + 1536000 KB, estimated
1220923 objects

2015/06/09 12:51:55 kid1| Target number of buckets: 61046

2015/06/09 12:51:55 kid1| Using 65536 Store buckets

2015/06/09 12:51:55 kid1| Max Mem  size: 1536000 KB

2015/06/09 12:51:55 kid1| Max Swap size: 14336000 KB

2015/06/09 12:51:55 kid1| Rebuilding storage in /cache01 (clean log)

2015/06/09 12:51:55 kid1| Using Least Load store dir selection

2015/06/09 12:51:55 kid1| Set Current Directory to /var/spool/squid

2015/06/09 12:51:55 kid1| Finished loading MIME types and icons.

2015/06/09 12:51:55 kid1| HTCP Disabled.

2015/06/09 12:51:55 kid1| Sending SNMP messages from [::]:3401

2015/06/09 12:51:55 kid1| commBind: Cannot bind socket FD 13 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:55 kid1| commBind: Cannot bind socket FD 14 to [::1]: (99)
Cannot assign requested address

2015/06/09 12:51:55 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

2015/06/09 12:51:55 kid1| Squid plugin modules loaded: 0

2015/06/09 12:51:55 kid1| Adaptation support is off.

2015/06/09 12:51:55 kid1| Accepting reverse-proxy HTTP Socket connections
at local=[::]:80 remote=[::] FD 11 flags=9

2015/06/09 12:51:55 kid1| Accepting SNMP messages on [::]:3401

2015/06/09 12:51:55 kid1| Store rebuilding is 1.64% complete

2015/06/09 12:51:55 kid1| Configuring Parent apps.domain.net/80/0

2015/06/09 12:51:56 kid1| Done reading /cache01 swaplog (243473 entries)

2015/06/09 12:51:56 kid1| Finished rebuilding storage from disk.

2015/06/09 12:51:56 kid1|    243473 Entries scanned

2015/06/09 12:51:56 kid1|         0 Invalid entries.

2015/06/09 12:51:56 kid1|         0 With invalid flags.

2015/06/09 12:51:56 kid1|    243310 Objects loaded.

2015/06/09 12:51:56 kid1|         0 Objects expired.

2015/06/09 12:51:56 kid1|         0 Objects cancelled.

2015/06/09 12:51:56 kid1|        65 Duplicate URLs purged.

2015/06/09 12:51:56 kid1|        98 Swapfile clashes avoided.

2015/06/09 12:51:56 kid1|   Took 1.10 seconds (221789.54 objects/sec).

2015/06/09 12:51:56 kid1| Beginning Validation Procedure

2015/06/09 12:51:56 kid1|   Completed Validation Procedure

2015/06/09 12:51:56 kid1|   Validated 243309 Entries

2015/06/09 12:51:56 kid1|   store_swap_size = 1129672.00 KB

FATAL: Received Segment Violation...dying.

2015/06/09 12:51:56 kid1| Closing HTTP port [::]:80

2015/06/09 12:51:56 kid1| storeDirWriteCleanLogs: Starting...

2015/06/09 12:51:56 kid1|     65536 entries written so far.

2015/06/09 12:51:56 kid1|    131072 entries written so far.

2015/06/09 12:51:56 kid1|    196608 entries written so far.

2015/06/09 12:51:56 kid1|   Finished.  Wrote 243411 entries.

2015/06/09 12:51:56 kid1|   Took 0.06 seconds (4148744.69 entries/sec).

CPU Usage: 1.252 seconds = 0.642 user + 0.610 sys

Maximum Resident Size: 253952 KB

Page faults with physical i/o: 0

^C

[root at cache03 squid]#


Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150609/c99071cd/attachment.htm>

From jlay at slave-tothe-box.net  Thu Jun 11 16:10:55 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 11 Jun 2015 16:10:55 -0000
Subject: [squid-users] Properly filtering http and https traffic in a
 transparent proxy environment
Message-ID: <1434039041.3610.49.camel@JamesiMac>

Hey All,

So....here's what I have for filtering http and https in the same
instance.  This is using iptables with -j REDIRECT lines.  Below is my
entire squid.conf, documented as well as I can:

#allow local network to connect to squid
acl localnet src 192.168.1.0/24

#safe ports are 80 and 443 in one acl, port 443 is another acl
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

#allow the http CONNECT method
acl CONNECT method CONNECT

#our regex list of sites and domains that we allow ie, www\.apple\.com
and \.google\.com
acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"

#we don't want to allow anything besides port 80 and port 443
http_access deny !Safe_ports

#we don't want CONNECT if we're not going to port 443
http_access deny CONNECT !SSL_Ports

#since we may not know the https site we're going to (ie connect direct
by IP), we must initially allow all https
http_access allow SSL_ports

#we allow http, but only sites and domains in our regex http_url.txt
list above
http_access allow allowed_http_sites

#drop any other http requests that are not in our regex http_url.txt
list above
http_access deny all

#break out the ssl_bump process by steps
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

#look for site or domain name either by SNI in request (step1), or
server subject in certificate in response (step2)
ssl_bump peek step1 all
ssl_bump peek step2 all

#see if the server name we obtained from the previous peek's above are
in our http_url.txt list above
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/http_url.txt"

#if the server name is in our http_url.txt, allow it 
ssl_bump splice step3 allowed_https_sites

#if the server name is not in our http_url.txt terminate the handshake
it
ssl_bump terminate all

#cert path and allow all the ssl options
sslproxy_capath /etc/ssl/certs
sslproxy_options ALL

#standard crtdaemon options
sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

#intercept 3128 for port 80, and 3129 for port 443.  Cert, cacert (these
are the same, read on the list this fixed an issue), and key, generate
ssl certs
http_port 3128 intercept
https_port 3129 intercept ssl-bump
cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE

#normal-ish log format, but we want to see the SNI, server cert subject,
and how we are bumping
logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %
ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode 

#log the above format name (mine) to syslog
access_log syslog:daemon.info mine

refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

coredump_dir /opt/var 

One caveat I've seen so far is that I have only seen is I only ever see
peek in the logs, though I know it's been spliced as shown below:

[09:35:19 jlay at gateway:/opt/etc/squid$] grep textnow http_url.txt 
api\.textnow\.me

[09:34:57 jlay at gateway:~$] dig api.textnow.me
<snip>
;; ANSWER SECTION:
api.textnow.me.		600	IN	A	209.59.180.48

>From the client:
[09:34:27 jlay at analysis:~/dev/squid$] openssl s_client -connect
209.59.180.48:443
CONNECTED(00000003)
depth=1 C = US, ST = Arizona, L = Scottsdale, O = "GoDaddy.com, Inc.",
OU = http://certificates.godaddy.com/repository, CN = Go Daddy Secure
Certification Authority, serialNumber = 07969287
verify error:num=20:unable to get local issuer certificate
verify return:0
---
Certificate chain
 0 s:/OU=Domain Control Validated/CN=*.textnow.me
<snip>

GET / HTTP/1.1

HTTP/1.1 200 OK
Server: nginx
Date: Thu, 11 Jun 2015 15:36:54 GMT
Content-Type: text/plain
Transfer-Encoding: chunked
Connection: keep-alive
X-Powered-By: PHP/5.5.16
Expires: Thu, 01 Jan 1970 00:00:00 GMT
Cache-Control: no-cache, no-store, must-revalidate, max-age=0

Server Squid log entry:
Jun 11 09:36:55 gateway (squid-1): 192.168.1.6 - - [11/Jun/2015:09:36:55
-0600] "CONNECT 209.59.180.48:443 HTTP/1.1" - - 200 364
TCP_TUNNEL:ORIGINAL_DST peek

I also notice that the CN does not show up in the logs...it WAS spliced
however because it matches our http_url.txt.  From the above, it appears
that only the step1 is getting logged.   The below log entry was used
with wget (sends SNI by default):
Jun 11 08:51:05 gateway squid: 192.168.1.6 - - [11/Jun/2015:08:51:05
-0600] "CONNECT 23.211.252.28:443 HTTP/1.1" www.apple.com - 200 14388
TCP_TUNNEL:ORIGINAL_DST peek

The above shows that I logged and retrieved my SNI at step1....the
subsequent splice was not logged.  I still note that terminates do not
get logged ( I have a bug open for that but I think the core bug may be
that when using atStep you only see step1 regardless, but I could be
wrong).  I'm enclosing an image of what a https terminate looks like
from the server and the client (msn.com isn't in the http_url.txt), and
also what an https allow looks like (apple.com is in the list).

That's it.  I can verify that the above works for a single list of
allowed hosts(www.apple.com) and domains (google.com).  If there's
something I missed...something wrong...ways to improve...ANYTHING for
the betterment of Squid users please don't hesitate to jump in here.
Thank you. 

James

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/8fbe3c34/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: terminate.png
Type: image/png
Size: 112350 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/8fbe3c34/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: allowed.png
Type: image/png
Size: 426652 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150611/8fbe3c34/attachment-0001.png>

From raghunathd8 at gmail.com  Fri Jun 12 14:26:10 2015
From: raghunathd8 at gmail.com (Raghunath Deshpande)
Date: Fri, 12 Jun 2015 14:26:10 -0000
Subject: [squid-users] GET Request URI automatically modified (destination
 webserver address is removed) when requests are sent to another proxy
Message-ID: <CAJGQJPVnZAk4GqSX_MLzshAhY5CGOEo7=xFuTWmH6ysJGAhzwg@mail.gmail.com>

Hi,

When I try to send the request from one squid proxy server to another squid
proxy server, the former proxy server is changing the Request URI field in
GET request to " / " (the web server address is removed entirely).

I have a setup something like this:

h1 --- h2 --- h4 --- h3

h1=web client
h3=web server
h2 and h4 are squid proxy servers.

h2 is changing the http GET Request URI to " / " (nothing) and sending to
h4 which is returning 400 bad HTTP request.

Please find attached captures. Kindly help.

-- 

Thank you & Best Regards,

Deshpande, Raghunath
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/dcfe2fe4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dump_h1.pcap
Type: application/octet-stream
Size: 15289 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/dcfe2fe4/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dump_h2.pcap
Type: application/octet-stream
Size: 30152 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/dcfe2fe4/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dump_h3.pcap
Type: application/octet-stream
Size: 770 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/dcfe2fe4/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dump_h4.pcap
Type: application/octet-stream
Size: 14837 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150612/dcfe2fe4/attachment-0003.obj>

From yvoinov at gmail.com  Wed Jun 24 09:56:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 24 Jun 2015 09:56:28 -0000
Subject: [squid-users] Transparent Squid Proxy Server
In-Reply-To: <CAA8ViV_oHsmmtCO_5hogrsHtZJQSTZ4w6+e0LwCHz2LyaPeqhQ@mail.gmail.com>
References: <CAA8ViV88mdeHPkMU+A+gOO59hsbxCb2v-1xh99STkCKGpAp9Ag@mail.gmail.com>
 <556DAD83.2030006@vsen.dk> <556DBF13.4070000@treenet.co.nz>
 <556DC137.10106@vsen.dk>
 <CAA8ViV9xAmC91EVjEy=mrORVcSBJ8vrSnx8=SimDrQ5=r4YcDg@mail.gmail.com>
 <556EAE1D.8020507@vsen.dk>
 <CAA8ViV9zs6DJfA3mR9xF5rZPednsQEfBsu35xjxQTcpx77afsA@mail.gmail.com>
 <55702344.2060703@treenet.co.nz>
 <CAA8ViV98z==e9txFN4r8SMdgLn8fO2RADNZ=C2J3USbygXNWXw@mail.gmail.com>
 <55705009.9080200@treenet.co.nz>
 <CAA8ViV8==pthmX4X4r3RZ9JTjSCyaTv1ZJUp0adr4cJnaR6kDA@mail.gmail.com>
 <CAA8ViV_oHsmmtCO_5hogrsHtZJQSTZ4w6+e0LwCHz2LyaPeqhQ@mail.gmail.com>
Message-ID: <558A7EBD.3090309@gmail.com>

Man,

3.5.x don't work with server-first. It must be for backward 
compatibility - but don't be.

Also, AFAIK, 3.5.x series don't work with transparent NAT interception 
in bump mode. Fake certs are generated, but with IP against hostnames 
(in all my test installations).

So, if you strictly need working bump with transparent interception, 
rollback to 3.4.

WBR, Yuri.

24.06.15 12:04, Reet Vyas ?????:
> Hi
>  Below is my squid file , I have configured squid 3.5.3 with ssl, but 
> I cant filter https traffic and also in access log I cant see https in 
> access logs.
>
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 116.72.152.37 192.168.0.0/24 <http://192.168.0.0/24> 
> # Sesuaikan dengan ip client/local
>
> acl SSL_ports port 443
> acl Safe_ports port 80  # http
> acl Safe_ports port 21  # ftp
> acl Safe_ports port 443  # https
> acl Safe_ports port 70  # gopher
> acl Safe_ports port 210  # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280  # http-mgmt
> acl Safe_ports port 488  # gss-http
> acl Safe_ports port 591  # filemaker
> acl Safe_ports port 777  # multiling http
> # storeid *test*
> acl urlrewrite dstdomain .fbcdn.net <http://fbcdn.net> .akamaihd.net 
> <http://akamaihd.net>
> acl speedtest url_regex -i speedtest\/.*\.(jpg|txt)\?.*
> acl reverbnation url_regex -i 
> reverbnation.*audio_player.*ec_stream_song.*$
> acl utmgif url_regex -i utm.gif.*
> acl playstoreandroid url_regex -i 
> c.android.clients.google.com.market.GetBinary.GetBinary.*
> acl idyoutube url_regex -i 
> youtube.*(ptracking|stream_204|player_204).*(v\=|docid\=|video_id\=).*$
> acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
> acl videoyoutube url_regex -i (youtube|googlevideo).*videoplayback\?
> acl CONNECT method CONNECT
> acl getmethod method GET
> acl loop_302 http_status 302
> acl step1 at_step SslBump1
> acl youtube dstdomain .youtube.com <http://youtube.com>
> acl blocksites dstdomain "/etc/squid/restricted-sites.squid"
> # TAG: QUERY
> # 
> -----------------------------------------------------------------------------
> acl QUERY urlpath_regex -i 
> (hackshield|blank.html|infinity.js|hshield.da|renew_session_token.php|recaptcha.js|dat.asp|notice.swf|patchlist.txt|hackshield|captcha|reset.css|update.ver|notice.html|updates.txt|gamenotice|images.kom|patchinfo.xml|noupdate.ui|\.Xtp|\.htc|\.txt)
> acl QUERY urlpath_regex -i 
> (patch.conf|uiimageset.xml.iop|gashaponwnd.xml.iop|loading.swf|download.swf|version.list|version.ini|launch.jnlp|server_patch.cfg.iop|core.swf|Loading.swf|resouececheck.sq|mainloading.swf|config.xml|gemmaze.swf|xml.png|size.xml|resourcesbar.swf|version.xml|version.list|delete.ini)
> acl QUERY urlpath_regex -i \.(jsp|asp|aspx|cfg|iop|zip|php|xml|html)(\?|$)
> cache deny QUERY
> cache deny youtube
>
> #
> acl dontstore url_regex 
> ^http:\/\/(([\d\w-]*(\.[^\.\-]*?\..*?))(\/\mosalsal\/[\d]{4}\/.*\/)(.*\.flv))\?start.*
> acl dontstore url_regex redbot\.org \.php
> acl dontstore url_regex -i ^http:\/\/.*gemscool\.com\/.*
> acl dontstore url_regex \.(aspx|php)\?
> acl dontstore url_regex goldprice\.org\/NewCharts\/gold\/images\/.*\.png
> acl dontstore url_regex google\.co(m|\.[a-z]{2})\/complete\/search\?
> acl dontstore url_regex 
> redirector\.([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id|get_video_info\?|ptracking\?|player_204\?|stream_204\?).*
>
> acl store_yt_id url_regex -i 
> youtube.*(ptracking|stream_204|playback|player_204|watchtime|set_awesome|s\?|ads).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*$
> acl store_id_list_yt url_regex -i (youtube|googlevideo).*videoplayback.*$
> acl store_id_list_yt url_regex 
> ^https?\:\/\/([0-9.]{4}|.*\.youtube\.com|.*\.googlevideo\.com|.*\.video\.google\.com)\/(get_video\?|videodownload\?|videoplayback.*id).*
>
> acl store-id_list urlpath_regex -i dl\.sourceforge\.net
> acl store-id_list urlpath_regex -i \.ytimg\.com
> acl store-id_list urlpath_regex -i \.(akamaihd|fbcdn)\.net
> acl store_id_list urlpath_regex -i 
> [a-zA-Z]{2}[0-9]*\.4shared\.com\/download\/
>
> acl store_id_list_url url_regex 
> ^http:\/\/[0-9]\.bp\.blogspot\.com.*\.(jpeg|jpg|png|gif|ico)
> acl store_id_list_url url_regex 
> ^http[s]?:\/\/.*\.twimg\.com\/(.*)\.(gif|jpeg|jpg|png|js|css)
> acl store_id_list_url url_regex 
> ^http[s]?:\/\/(media|static)\.licdn\.com\/.*\.(png|jpg|gif|woff)
> acl store_id_list_url url_regex 
> ^https:\/\/fb(static|cdn)\-.*\-a.akamaihd.net 
> <http://a.akamaihd.net>\/(.*)\.(gif|jpeg|jpg|png|js|css|mp4)
> acl store_id_list_url url_regex 
> ^http:\/\/.*\.ak\.fbcdn\.net\/.*\.(gif|jpg|png|js|mp4)
>
> # pass requests
> url_rewrite_program /etc/squid/phpredir.php
> url_rewrite_access allow youtube
>
> request_header_access Range deny store_id_list_yt
> range_offset_limit 10 KB store_id_list_yt
>
>
> ###############################################################################
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> ###############################################################################
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny blocksites
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
>
> ###############################################################################
> # squid ssl_bump option
> ###############################################################################
> always_direct allow all
> ssl_bump server-first all
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> #ssl_bump peek step1
> #ssl_bump bump all
> ###############################################################################
> # Squid normally listens to port 3128
> ###############################################################################
> https_port 3130 intercept ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt 
> key=/etc/squid/ssl_certs/squid.key
> http_port 3129 intercept
> http_port 3128
>
> # TAG: Store-id Program
> # 
> -----------------------------------------------------------------------------
> store_id_program /usr/bin/perl /etc/squid/store-id.pl <http://store-id.pl>
> store_id_children 100 startup=0 idle=1 concurrency=1000
>
> # TAG: Store-id Access
> # 
> -----------------------------------------------------------------------------
> store_id_access allow urlrewrite
> store_id_access allow speedtest
> store_id_access allow reverbnation
> store_id_access allow utmgif
> store_id_access allow playstoreandroid
> store_id_access allow idyoutube
> store_id_access allow videoyoutube
> store_id_access deny dontstore
> store_id_access deny !getmethod
> store_id_access allow store_id_list_yt
> store_id_access allow store_yt_id
> store_id_access allow store-id_list
> store_id_access deny all
> store_id_bypass on
>
> # TAG: Youtube 302
> # 
> -----------------------------------------------------------------------------
> store_miss deny store_id_list_yt loop_302
> send_hit deny store_id_list_yt loop_302
>
> ###############################################################################
> ## MEMORY CACHE OPTIONS
> ###############################################################################
> client_dst_passthru on
> cache_mem 1024 MB
> maximum_object_size_in_memory 1024 KB
> memory_cache_shared off
> memory_cache_mode disk
> memory_replacement_policy heap GDSF
>
> ###############################################################################
> ## DISK CACHE OPTIONS
> ###############################################################################
> cache_replacement_policy heap LFUDA
> minimum_object_size 1 bytes
> maximum_object_size 10 GB
>
> ###############################################################################
> # Uncomment and adjust the following to add a disk cache directory.
> ###############################################################################
> cache_dir aufs /usr/local/cache_proxy 25000 16 256 # sesuaikan dengan 
> drive penyimpanan cache
> store_dir_select_algorithm round-robin
> cache_swap_low 90
> cache_swap_high 95
>
> ###############################################################################
> # Leave coredumps in the first cache dir
> ###############################################################################
> coredump_dir /var/spool/squid
>
> ###############################################################################
> ## LOGFILE OPTIONS
> ###############################################################################
> #access_log daemon:/tmp/access.log !log
> #logfile_daemon /usr/lib/squid/log_file_daemon
> cache_store_log none
> logfile_rotate 1
> mime_table /etc/squid/mime.conf
> pid_filename /var/run/squid.pid
> strip_query_terms off
> buffered_logs off
>
> ###############################################################################
> ## OPTIONS FOR TROUBLESHOOTING
> ###############################################################################
> #cache_log /tmp/cache.log
> cache_log /dev/null
> #debug_options ALL,1 22,3
> coredump_dir /var/spool/squid
>
> ###############################################################################
> ## OPTIONS FOR TUNING THE CACHE
> ###############################################################################
> max_stale 1 years
> vary_ignore_expire on
> shutdown_lifetime 10 seconds
>
> ###############################################################################
> # Add any of your own refresh_pattern entries above these.
> ###############################################################################
> refresh_pattern ^ftp:  1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> # Youtube Video
> refresh_pattern -i 
> (get_video\?|videoplayback\?|videodownload\?|\.mp4|\.webm|\.flv|((audio|video)\/(webm|mp4))) 
> 241920 100% 241920 override-expire ignore-reload ignore-private 
> ignore-no-store ignore-must-revalidate reload-into-ims ignore-auth 
> store-stale
> refresh_pattern -i ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.* 
>     10080 99% 43200 override-lastmod override-expire ignore-reload 
> reload-into-ims ignore-private reload-into-ims ignore-auth store-stale
> refresh_pattern -i 
> ^https?\:\/\/.*\.googlevideo\.com\/videoplayback.*$    241920 100% 
> 241920 override-expire ignore-reload ignore-private ignore-no-store 
> ignore-must-revalidate reload-into-ims ignore-auth store-stale
>
> refresh_pattern (akamaihd|fbcdn)\.net 14400 99% 518400 ignore-no-store 
> ignore-private ignore-reload ignore-must-revalidate store-stale
> refresh_pattern -i squid\.internal 14400 99% 518400 ignore-no-store 
> ignore-private ignore-reload ignore-must-revalidate store-stale
> refresh_pattern \.(jpg|png|gif|css|ico)($|\?) 14400 99% 518400  
> ignore-no-store ignore-private reload-into-ims ignore-must-revalidate 
> store-stale
> refresh_pattern . 0 99% 518400  ignore-no-store ignore-private 
> reload-into-ims store-stale
> # Image Youtube
> refresh_pattern -i (yimg|twimg)\.com\.*         1440 100% 129600 
> override-expire ignore-reload reload-into-ims
> refresh_pattern -i (ytimg|ggpht)\.com\.*        1440 80% 129600 
> override-expire override-lastmod ignore-auth ignore-reload reload-into-ims
>
> #images facebook
> refresh_pattern -i 
> fbcdn.*net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$)) 
> 241920 99% 241920 ignore-no-store ignore-private override-expire 
> override-lastmod reload-into-ims ignore-auth
> refresh_pattern -i pixel\.facebook\.com.*\.(jpg|png|gif|ico|css|js) 
> 241920 80% 241920 override-expire ignore-reload reload-into-ims 
> ignore-auth
> refresh_pattern -i \.akamaihd\.net.*\.(jpg|png|gif|ico|css|js) 241920 
> 80% 241920 override-expire ignore-reload reload-into-ims ignore-auth
> refresh_pattern -i ((facebook.com 
> <http://facebook.com>)|(85.131.151.39))\.(jpg|png|gif) 241920 99% 
> 241920 ignore-reload override-expire ignore-no-store store-stale
> refresh_pattern -i 
> fbcdn\.net\/.*\.((jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)|(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)(\?|.*$)) 
> 241920 99% 241920 ignore-no-store ignore-private override-expire 
> override-lastmod reload-into-ims ignore-auth
> refresh_pattern static\.(xx|ak)\.fbcdn\.net*\.(jpg|gif|png) 241920 99% 
> 241920 ignore-reload override-expire ignore-no-store
> refresh_pattern ^https?\:\/\/profile\.ak\.fbcdn.net 
> <http://fbcdn.net>*\.(jpg|gif|png) 241920 99% 241920 ignore-reload 
> override-expire ignore-no-store
>
> # Video Facebook
> refresh_pattern -i \.video.ak.fbcdn.net.*\.(mp4|flv|mp3|amf) 10080 80% 
> 43200 override-expire ignore-reload reload-into-ims ignore-private 
> ignore-no-store ignore-must-revalidate
> refresh_pattern (audio|video)\/(webm|mp4) 129600 99% 129600 
> ignore-reload override-expire override-lastmod ignore-must-revalidate  
> ignore-private ignore-no-store ignore-auth store-stale
> refresh_pattern -i ^http://.*squid\.internal.*  241920 100% 241920 
> override-lastmod override-expire ignore-reload ignore-must-revalidate 
> ignore-private ignore-no-store ignore-auth store-stale
>
> # All File
> refresh_pattern -i 
> \.(3gp|7z|ace|asx|bin|deb|divx|dvr-ms|ram|rpm|exe|inc|cab|qt) 10080 
> 80% 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern -i 
> \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v)|arj|lha|lzh|zip|tar|iop|nzp|pak|mar|msp) 
> 10080 80% 10080 override-expire override-lastmod reload-into-ims 
> ignore-reload
> refresh_pattern -i 
> \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|dat|ad|txt|dll) 10080 80% 
> 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern -i 
> \.(avi|ac4|mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rm|r(a|p)m|snd|vob|webm) 
> 10080 80% 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern -i 
> \.(pp(t?x)|s|t)|pdf|rtf|wax|wm(a|v)|wmx|wpl|cb(r|z|t)|xl(s?x)|do(c?x)|flv|x-flv) 
> 10080 80% 10080 override-expire override-lastmod reload-into-ims
> refresh_pattern .  0 20% 4320
>
> ###############################################################################
> ## ADMINISTRATIVE PARAMETERS
> ###############################################################################
> cache_mgr reetika at foxymoron.org <mailto:reetika at foxymoron.org>
> cache_effective_user proxy
> cache_effective_group proxy
> visible_hostname foxysquid.foxymoron.tv <http://foxysquid.foxymoron.tv>
> unique_hostname foxysquid.foxymoron.tv <http://foxysquid.foxymoron.tv>
>
> ###############################################################################
> ## PERSISTENT CONNECTION HANDLING
> ###############################################################################
> detect_broken_pconn on
> client_persistent_connections off
> server_persistent_connections on
>
> ###############################################################################
> ## ERROR PAGE OPTIONS
> ###############################################################################
> error_directory /usr/share/squid/errors/en
> error_log_languages off
>
> ###############################################################################
> ## DNS OPTIONS
> ###############################################################################
> check_hostnames off
> hosts_file /etc/hosts
> connect_retries 2
> ipcache_low 90
> ipcache_high 95
> ipcache_size 84024                        # 2x Besar RAM
> fqdncache_size 64024                        # real RAM Hardware
> pipeline_prefetch 100
>
> ###############################################################################
> ## MISCELLANEOUS
> ###############################################################################
> memory_pools off
> reload_into_ims on
> uri_whitespace strip
> max_filedescriptors 65536
>
> IPtable rules :
>
> ................................................
>
> My IPtables Rules
>
> Chain PREROUTING (policy ACCEPT 27405 packets, 1872K bytes)
>  pkts bytes target     prot opt in     out source               
> destination
> 76873 4457K DNAT       tcp  --  eth1   * 0.0.0.0/0 <http://0.0.0.0/0> 
> 0.0.0.0/0 <http://0.0.0.0/0> tcp dpt:80 to:192.168.0.200:3129 
> <http://192.168.0.200:3129>
>    26  1184 REDIRECT   tcp  --  eth0   * 0.0.0.0/0 <http://0.0.0.0/0> 
> 0.0.0.0/0 <http://0.0.0.0/0> tcp dpt:80 redir ports 3129
>     0     0 DNAT       tcp  --  eth0   * 0.0.0.0/0 <http://0.0.0.0/0> 
> 0.0.0.0/0 <http://0.0.0.0/0> tcp dpt:443 to:192.168.0.200:3130 
> <http://192.168.0.200:3130>
>
> Chain INPUT (policy ACCEPT 9321 packets, 543K bytes)
>  pkts bytes target     prot opt in     out source               
> destination
>
> Chain OUTPUT (policy ACCEPT 1426 packets, 85560 bytes)
>  pkts bytes target     prot opt in     out source               
> destination
>
> Chain POSTROUTING (policy ACCEPT 1426 packets, 85560 bytes)
>  pkts bytes target     prot opt in     out source               
> destination
> 81432   14M MASQUERADE  all  --  *      eth0 192.168.0.0/24 
> <http://192.168.0.0/24> 0.0.0.0/0 <http://0.0.0.0/0>
>
> On Fri, Jun 5, 2015 at 1:43 PM, Reet Vyas <reet.vyas28 at gmail.com 
> <mailto:reet.vyas28 at gmail.com>> wrote:
>
>     Hi
>
>     Thanks for reply. I am trying to cache youtube using this wiki
>     http://wiki.squid-cache.org/ConfigExamples/DynamicContent/YouTube
>     but I cant cache youtube.
>
>     I want to cache facebook and youtube. SSl certificate installation
>     that I have to do . Please suggest some links.
>
>     On Thu, Jun 4, 2015 at 6:48 PM, Amos Jeffries
>     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> wrote:
>
>         On 5/06/2015 12:55 a.m., Reet Vyas wrote:
>         > Thank you everyone for helping me to setup squid , Now its
>         working but in
>         > access.logs  I only see tcp_miss if m using same website. I
>         mean squid is
>         > not caching
>
>         You will get MISS a fair bit more with intercepted traffic
>         than with
>         normal proxied traffic. Particularly on certain major CDN who play
>         tricks with DNS.
>
>         The reasons and some workarounds to need to be doing are
>         explained in
>         <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>
>         Amos
>
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150624/bd9cdc9c/attachment.htm>

From maamule10 at gmail.com  Sun Jun 28 10:34:26 2015
From: maamule10 at gmail.com (Dalmar)
Date: Sun, 28 Jun 2015 13:34:26 +0300
Subject: [squid-users] Mikrotik and Squid Transparent
Message-ID: <CAFUu-Gs8xML8tBaQNFKpysD32=GE7M0cFPjxBVUf8a99vkp0mw@mail.gmail.com>

To begin with, thank you Marcel,Alex and Amos for your help guys i am
really so close because of you. I have done exactly what Marcel told me and
now all transparent/intercept errors are gone. It worked nicely when i used
two mikrotiks one for WAN and the other for the LAN connection, however,
when i use one mikrotik it says TCP_MISS_ABORTED and NONE_ABORTED. In this
situation ,squid gets internet from the MK LAN port using a public IP and i
can ping the net, but squid throws the above error in the access.log. The
topo i wanna use is INTERNET >>MK >> SQUID .
.


i think the iptable rules will change.The Mikrotik have 3 NICS now , but i
can add 1 more so it becomes eth0:WAN eth1:LAN eth2:PROXY-LAN
eth3:PROXY-WAN .

NB: it says Your message to squid-users awaits moderator approval , Message
body is too big , for all my replays, so sorry for the delay.

Thanks in advance .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/2d55f04b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Scenario Network Proxy 2.png
Type: image/png
Size: 34695 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/2d55f04b/attachment.png>

From marcel at guineanet.net  Sun Jun 28 11:39:01 2015
From: marcel at guineanet.net (Marcel)
Date: Sun, 28 Jun 2015 12:39:01 +0100
Subject: [squid-users] Mikrotik and Squid Transparent
In-Reply-To: <CAFUu-GuTTMBGbK6sCcoe9BLcHc=cw_N3wZXCenXWGXxbk8pr7Q@mail.gmail.com>
References: <CAFUu-GvSHj=X-_usesoYDY-fe2Vss9zsNFRr9bZnE5ThDdUR2w@mail.gmail.com>	<558A9CB6.9000403@gmail.com>	<CAFUu-GvsfRdO4xksf2myJYgxoUq=T=faA3D0ryfrdktUs945pg@mail.gmail.com>	<558AAF83.1000801@guineanet.net>
 <CAFUu-GuTTMBGbK6sCcoe9BLcHc=cw_N3wZXCenXWGXxbk8pr7Q@mail.gmail.com>
Message-ID: <558FDCD5.5060301@guineanet.net>

Hi Dalmar
one question how many interfaces do your squid box have ?


El 28/6/15 a las 11:26, Dalmar escribi?:
> To begin with, thank you Marcel,Alex and Amos for your help guys i am 
> really so close because of you. I have done exactly what Marcel told 
> me and now all transparent/intercept errors are gone. It worked nicely 
> when i used two mikrotiks one for WAN and the other for the LAN 
> connection, however, when i use one mikrotik it says TCP_MISS_ABORTED 
> and NONE_ABORTED. In this situation ,squid gets internet from the MK 
> LAN port using a public IP and i can ping the net, but squid throws 
> the above error in the access.log. The topo i wanna use is INTERNET 
> >>MK >> SQUID . .
>
>
> i think the iptable rules will change.The Mikrotik have 3 NICS now , 
> but i can add 1 more so it becomes eth0:WAN eth1:LAN eth2:PROXY-LAN 
> eth3:PROXY-WAN .
>
> NB: it says Your message to squid-users awaits moderator approval , 
> Message body is too big , so sorry for the delay.
>
> Thanks in advance .

-- 
Fossua-vcard
	 Marcel Fossua
Unix/Linux Network Administrator
   Tel: 0240 222299448
www.guineanet.net <http://www.guineanet.net>/ www.familyfossua.com 
<http://www.familyfossua.com>








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/3e89cbee/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 34695 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/3e89cbee/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: guineanet.png
Type: image/png
Size: 24663 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150628/3e89cbee/attachment-0001.png>

