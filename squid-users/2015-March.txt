From hack.back at hotmail.com  Sun Mar  1 00:07:41 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 28 Feb 2015 16:07:41 -0800 (PST)
Subject: [squid-users] squid and post method
Message-ID: <1425168461331-4670122.post@n4.nabble.com>

in my squid proxy server, I would like to monitor certain HTTP/HTTPS POST
requests before it goes out of squid. What are my options?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rwheeler at artifact-software.com  Sun Mar  1 00:40:02 2015
From: rwheeler at artifact-software.com (Ron Wheeler)
Date: Sat, 28 Feb 2015 19:40:02 -0500
Subject: [squid-users] squid and post method
In-Reply-To: <1425168461331-4670122.post@n4.nabble.com>
References: <1425168461331-4670122.post@n4.nabble.com>
Message-ID: <54F25FE2.8030107@artifact-software.com>

To get any useful input, you probably need to describe how you are using 
squid.
What is the origin and destination of the POST requests?
What do you want to know about the request?

Ron

On 28/02/2015 7:07 PM, HackXBack wrote:
> in my squid proxy server, I would like to monitor certain HTTP/HTTPS POST
> requests before it goes out of squid. What are my options?
> Thanks.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-- 
Ron Wheeler
President
Artifact Software Inc
email: rwheeler at artifact-software.com
skype: ronaldmwheeler
phone: 866-970-2435, ext 102



From squid3 at treenet.co.nz  Sun Mar  1 01:15:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 01 Mar 2015 14:15:01 +1300
Subject: [squid-users] Logging variable  question
In-Reply-To: <mcsodm$i43$1@ger.gmane.org>
References: <mcsodm$i43$1@ger.gmane.org>
Message-ID: <54F26815.4020300@treenet.co.nz>

On 1/03/2015 4:55 a.m., Markus Moeller wrote:
> Hi,
> 
>   I wonder about the total size variables <st and >st for squid logs
> 
> # <st   Sent reply size including HTTP headers
> # >st   Received request size including HTTP headers. In the
> #       case of chunked requests the chunked encoding metadata
> #       are not included
> 
> I have set the logformat to
> 
> logformat squid_mm      %tg %6tr %>a %Ss/%03>Hs %<st %>st %rm %ru %un
> %Sh/%<A %mt
> 
> and have 2 cases for which I would like to see the request/reply total
> data size.
> 
> Case 1
> 
> Just receiving data.  (44073 and 35754 are local and remote ports
> respectively)
> 
> 28/Feb/2015:15:29:27   5887 192.168.1.17 TCP_TUNNEL/200 8895 45 CONNECT
> opensuse13.suse.home:443 - HIER_DIRECT/opensuse13.suse.home -

http://bugs.squid-cache.org/show_bug.cgi?id=3069

Amos


From squid3 at treenet.co.nz  Sun Mar  1 02:14:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 01 Mar 2015 15:14:20 +1300
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F0DA99.2030600@gmail.com>
References: <54F0DA99.2030600@gmail.com>
Message-ID: <54F275FC.9030204@treenet.co.nz>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 28/02/2015 9:59 a.m., Yuri Voinov wrote:
> Hi gents.
> 
> Can I use log codes in custom error page and how to do this? Some 
> examples will be useful.
> 
> In details, interested in using adapt::<last_h code.
> 
> Just point me on right way.

You just wrote the static text with the code in it at the position you
want the item to appear.

The currently supported error page macro/tag/code's are listed at:
<http://wiki.squid-cache.org/Features/CustomErrors#ERR_.2A_template_codes_for_embedding>
Note most of the logformat codes are not supported yet.

And have a look at the installed templates for plenty of examples of
usage.

Amos
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBAgAGBQJU8nX6AAoJELJo5wb/XPRjAI4IAMaJ1Bm2s77xzMPyMbMK5xoK
3VZJC4hxOxUcSOXuHyoTNNRAfE7gSAWPqf3PgR+kcqs5iSM6REmC1Jalo+Qv6Paa
RSAhKc39w4ZzMkE/kWVrqs4vzJOxRuG1M9F4Kx2faJMb8OECZx8+URBhF9U9UiSU
vVujyy71ooY9mO2nxldjlzqS/oiNWomxY2mKf7H5c/KbDoolGcMNDXsi0kP0HcQ9
ez9qVi2tVItT2Zb9D2gNbqbujlwMWlu761ydxzvrsGbvsuHTuQ7l1Ez5yXhixuAT
dsTUGOoq2A1fR7rmAjVu30C5DNdz/NYeBlGVjbWQC/mKpQ2lmOlTC9YmWIbdPZw=
=jgOh
-----END PGP SIGNATURE-----


From johnzeng2013 at yahoo.com  Sun Mar  1 03:46:32 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sun, 01 Mar 2015 11:46:32 +0800
Subject: [squid-users] i meet a problem ,
 --- Unsupported Request Method and Protocol'' for such connections
 ( non-HTTP connections ) based 80 port ----, if possible ,
 please give me some advisement or help
In-Reply-To: <54F2370B.8020200@ngtech.co.il>
References: <54F07195.3070708@yahoo.com> <54F13388.8090504@yahoo.com>
 <54F2370B.8020200@ngtech.co.il>
Message-ID: <54F28B98.6080302@yahoo.com>

Hello  Eliezer:

                        Thanks for your reply ,  although squid 2.7 
stables 9 don't support Tproxy really , but We realize the function via 
patch third patch .

                        Why we meet the problem (  Unsupported Request 
Method and Protocol ) ?  although some special application communicate 
based 80 port ,

                        they don't use http protocol or they change http 
protocol and method isn't GET OR POST . for example : QQ application and 
some QQ game .

                        But squid don't support other protocol except 
normal http protocol  or don't support other method except GET .. of 
http protocol , for example HEAD.

                        So if it's a transparent proxy , and you will 
access these application , We have to face the problem .

                       Why i don't use highest version ago , include 
squid 3.5.2 ,

                       because  squid 2.7 stable 9 can support coss for 
small http object

                       and We don't confirm whether squid 3.5.2 can 
support small http object via rock or coss .

                      i know these are some function ( 
on_unsupported_protocol ) in 3.HEAD and ,

                      it won't be very stable or we have to use 
trunk-non-HTTP-bypass-v8.patch for squid 3.5.2

http://www.squid-cache.org/Versions/v3/3.HEAD/cfgman/on_unsupported_protocol.html


                     Thanks again for your reply


                        Why



? 2015?03?01? 05:45, Eliezer Croitoru ??:
> Hey,
>
> It is a bit hard to understand the scenario but I assume it's a 
> transparent proxy for port 80? right?
>
> In any case what so ever squid 2.7 is old and preferably should not be 
> used in production unless there is a very specific need for it while 
> taking into account the advantages and disadvantages.
>
> If you search via www.ask.com it should work on any version of squid 
> like it worked for me in the last who knows how many years.
>
> The best start point would be to get couple lines from the access.log 
> and a description of the network infrastructure.
> (consider to replace\remove any confidential information)
>
> All The Bests,
> Eliezer Croitoru
>
> On 28/02/2015 05:18, johnzeng wrote:
>> Hi all :
>>
>>
>>
>> i meet a problem ,Squid cannot currently deal with such connections (
>> non-HTTP connections ) based 80 port , and We get some error ,
>>
>> Unsupported Request Method and Protocol'' for https URLs..
>>
>> i search viawww.ask.com  , but i don't good way .
>>
>> if possible ,i hope to use squid 2.7 stable9 , Maybe it will be 
>> stable version untile now .
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Mar  1 04:55:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 01 Mar 2015 17:55:23 +1300
Subject: [squid-users] i meet a problem ,
 --- Unsupported Request Method and Protocol'' for such connections
 ( non-HTTP connections ) based 80 port ----, if possible ,
 please give me some advisement or help
In-Reply-To: <54F28B98.6080302@yahoo.com>
References: <54F07195.3070708@yahoo.com> <54F13388.8090504@yahoo.com>
 <54F2370B.8020200@ngtech.co.il> <54F28B98.6080302@yahoo.com>
Message-ID: <54F29BBB.5010505@treenet.co.nz>

On 1/03/2015 4:46 p.m., johnzeng wrote:
> Hello  Eliezer:
> 
>                        Thanks for your reply ,  although squid 2.7
> stables 9 don't support Tproxy really , but We realize the function via
> patch third patch .
> 
>                        Why we meet the problem (  Unsupported Request
> Method and Protocol ) ?  although some special application communicate
> based 80 port ,
> 
>                        they don't use http protocol or they change http
> protocol and method isn't GET OR POST . for example : QQ application and
> some QQ game .


In Squid-2.7 you need to use the extension_methods directive to tell
Squid what non-HTTP/1.0 methods to accept.

Squid-3 accepts the full range of HTTP/1.1 and extension methods by
default with no configuration needed.


I seem to remember QQ application working through Squid-3 though, so it
should work after the upgrade. If not I would like to see a copy of what
the message protocol they use.


> 
>                        But squid don't support other protocol except
> normal http protocol  or don't support other method except GET .. of
> http protocol , for example HEAD.
> 
>                        So if it's a transparent proxy , and you will
> access these application , We have to face the problem .
> 
>                       Why i don't use highest version ago , include
> squid 3.5.2 ,
> 
>                       because  squid 2.7 stable 9 can support coss for
> small http object
> 
>                       and We don't confirm whether squid 3.5.2 can
> support small http object via rock or coss .


COSS is obsolete. It was replaced by Rock storage format.

Squid has always been able to store small objects in various ways. The
type of cache storage formats available is not affecting that.



> 
>                      i know these are some function (
> on_unsupported_protocol ) in 3.HEAD and ,
> 
>                      it won't be very stable or we have to use
> trunk-non-HTTP-bypass-v8.patch for squid 3.5.2
> 
> http://www.squid-cache.org/Versions/v3/3.HEAD/cfgman/on_unsupported_protocol.html
> 


That feature change will not help you with this problem.

IF the method is just an HTTP/1.1 extension method, then the protocol is
HTTP/1.1 and *is* supported by Squid-3.2 and later.


IF, the application is being both abusive and broken by sending a
non-HTTP protocol Squid is rejecting abuse. Detecting and preventing
this type of network abuse is one of the major benefits of using a proxy
like Squid.

The QQ application (and others) are responsible for A) using their own
port suitable to the protocol they are sending, and/or B) accepting the
HTTP messages Squid returns to them on port 80.


Consider what would happen if you tried living underwater? does not work
if you stay the way you are right now. You have to have breathing
apparatus etc to make you compatible with the water environment.

Same goes for applications using the registered ports 0-1024 which have
specific protocols assigned to them. Applications need to understand and
use HTTP to use port 80.


Amos


From ahmed.zaeem at netstream.ps  Sun Mar  1 21:31:08 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Sun, 1 Mar 2015 13:31:08 -0800
Subject: [squid-users] cache peer issue
Message-ID: <000001d05467$08e16be0$1aa443a0$@netstream.ps>

Hi , 

I have many account from same provider and I would like to use those
accounts as round robin and each request has different IP as possible

 

The issue is , I open whatismyipaddress.com

 

for  some freshesh I can see my ip is rotating  but after about 1 minute I
see my ip is stuck on same ip and not rotating .

 

Im wondering wt I need to modify the config below so that the round robin
keep working and each time change the ip :

 

cache_peer  vvvv  parent 22225 0 no-query round-robin name=1 no-digest
no-tproxy proxy-only login=xxxxx

cache_peer  vvv  parent 22225 0 no-query round-robin name=2  no-digest
no-tproxy proxy-only login=xxx

cache_peer  vvv  parent 22225 0 no-query round-robin  name=3 no-digest
no-tproxy proxy-only login=xxx

cache_peer  vvv  parent 22225 0 no-query round-robin name=4  no-digest
no-tproxy proxy-only login=xxx

cache_peer  vvvv  parent 22225 0 no-query round-robin  name=5 no-digest
no-tproxy proxy-only login=xxx

 

 

 

 

 

cheers

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/121f3031/attachment.htm>

From lemayiansam at gmail.com  Sun Mar  1 11:35:04 2015
From: lemayiansam at gmail.com (Samwel Lemayian)
Date: Sun, 1 Mar 2015 11:35:04 +0000
Subject: [squid-users] cache peer issue
In-Reply-To: <000001d05467$08e16be0$1aa443a0$@netstream.ps>
References: <000001d05467$08e16be0$1aa443a0$@netstream.ps>
Message-ID: <1274510552-1425209700-cardhu_decombobulator_blackberry.rim.net-1911459974-@b14.c4.bise7.blackberry>

Following this closely. 
Sent from my BlackBerry?

-----Original Message-----
From: "snakeeyes" <ahmed.zaeem at netstream.ps>
Sender: "squid-users" <squid-users-bounces at lists.squid-cache.org>
Date: Sun, 1 Mar 2015 13:31:08 
To: <squid-users at lists.squid-cache.org>
Subject: [squid-users] cache peer issue

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From yvoinov at gmail.com  Sun Mar  1 12:42:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 01 Mar 2015 18:42:39 +0600
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F275FC.9030204@treenet.co.nz>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
Message-ID: <54F3093F.60609@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



01.03.15 8:14, Amos Jeffries ?????:
> On 28/02/2015 9:59 a.m., Yuri Voinov wrote:
>> Hi gents.
> 
>> Can I use log codes in custom error page and how to do this? Some
>>  examples will be useful.
> 
>> In details, interested in using adapt::<last_h code.
> 
>> Just point me on right way.
> 
> You just wrote the static text with the code in it at the position
> you want the item to appear.
Good,

but how to extract adaptation meta into substitution variable or
something? Without patching Squid or eCAP library? Alex pointed me to
you ;) to ask it.

> 
> The currently supported error page macro/tag/code's are listed at: 
> <http://wiki.squid-cache.org/Features/CustomErrors#ERR_.2A_template_codes_for_embedding>
>
> 
Note most of the logformat codes are not supported yet.
Yes, template codes is passed. This is ok.

But how to extract adaptation meta? May be need external wrapper? Or
something like?

I've seen how ICAP externally extract X-Virus-ID. Can I do the similar
with eCAP and within Squid config?


> 
> And have a look at the installed templates for plenty of examples
> of usage.
> 
> Amos _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU8wk/AAoJENNXIZxhPexGBI4H/RL++iZ+gN0+VgX0Y176mE16
6ReAif8ct+23L2Mfh2JAvTG4rLup7net4OAgSgshvTfhx1xfAnlwyPTd6ChoB6/H
/R5sbfidadyH0TpClHTR+E6Vww0ayld+qRdKpuSlLjsOEn3p8tg5vVetFJPNudlZ
1rXg2dHiWXCYW3iudcRweTButUDqY5fTYpUXu2fKRiAsJWhm6IZmCnMluntTaO2R
4rJKFHDc6AvnVwoYlLO1jFUWNYg6Q8mTfvjNnxLPsJnlqm52P4w7I13KiwyQmjsL
M0dDt6Sh/MzbPj3gSfpb/r+N/bG9opk5PGZaaEQppWn1Oert55+0fjLaOyFjQPg=
=IzCF
-----END PGP SIGNATURE-----


From johnzeng2013 at yahoo.com  Sun Mar  1 13:09:34 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sun, 01 Mar 2015 21:09:34 +0800
Subject: [squid-users] i meet a problem ,
 --- Unsupported Request Method and Protocol'' for such connections
 ( non-HTTP connections ) based 80 port ----, if possible ,
 please give me some advisement or help
In-Reply-To: <54F29BBB.5010505@treenet.co.nz>
References: <54F07195.3070708@yahoo.com> <54F13388.8090504@yahoo.com>
 <54F2370B.8020200@ngtech.co.il> <54F28B98.6080302@yahoo.com>
 <54F29BBB.5010505@treenet.co.nz>
Message-ID: <54F30F8E.401@yahoo.com>


Hello Amos Jeffries:

                                 Thanks again ,  and i will deploy squid 
3.5.2 in according to your advisement ,

                                  and test QQ and other application in 
the environment .  if these are any error ,  i will catch access log


                                  Best Regards




? 2015?03?01? 12:55, Amos Jeffries ??:
> On 1/03/2015 4:46 p.m., johnzeng wrote:
>> Hello  Eliezer:
>>
>>                         Thanks for your reply ,  although squid 2.7
>> stables 9 don't support Tproxy really , but We realize the function via
>> patch third patch .
>>
>>                         Why we meet the problem (  Unsupported Request
>> Method and Protocol ) ?  although some special application communicate
>> based 80 port ,
>>
>>                         they don't use http protocol or they change http
>> protocol and method isn't GET OR POST . for example : QQ application and
>> some QQ game .
>
> In Squid-2.7 you need to use the extension_methods directive to tell
> Squid what non-HTTP/1.0 methods to accept.
>
> Squid-3 accepts the full range of HTTP/1.1 and extension methods by
> default with no configuration needed.
>
>
> I seem to remember QQ application working through Squid-3 though, so it
> should work after the upgrade. If not I would like to see a copy of what
> the message protocol they use.
>
>
>>                         But squid don't support other protocol except
>> normal http protocol  or don't support other method except GET .. of
>> http protocol , for example HEAD.
>>
>>                         So if it's a transparent proxy , and you will
>> access these application , We have to face the problem .
>>
>>                        Why i don't use highest version ago , include
>> squid 3.5.2 ,
>>
>>                        because  squid 2.7 stable 9 can support coss for
>> small http object
>>
>>                        and We don't confirm whether squid 3.5.2 can
>> support small http object via rock or coss .
>
> COSS is obsolete. It was replaced by Rock storage format.
>
> Squid has always been able to store small objects in various ways. The
> type of cache storage formats available is not affecting that.
>
>
>
>>                       i know these are some function (
>> on_unsupported_protocol ) in 3.HEAD and ,
>>
>>                       it won't be very stable or we have to use
>> trunk-non-HTTP-bypass-v8.patch for squid 3.5.2
>>
>> http://www.squid-cache.org/Versions/v3/3.HEAD/cfgman/on_unsupported_protocol.html
>>
>
> That feature change will not help you with this problem.
>
> IF the method is just an HTTP/1.1 extension method, then the protocol is
> HTTP/1.1 and *is* supported by Squid-3.2 and later.
>
>
> IF, the application is being both abusive and broken by sending a
> non-HTTP protocol Squid is rejecting abuse. Detecting and preventing
> this type of network abuse is one of the major benefits of using a proxy
> like Squid.
>
> The QQ application (and others) are responsible for A) using their own
> port suitable to the protocol they are sending, and/or B) accepting the
> HTTP messages Squid returns to them on port 80.
>
>
> Consider what would happen if you tried living underwater? does not work
> if you stay the way you are right now. You have to have breathing
> apparatus etc to make you compatible with the water environment.
>
> Same goes for applications using the registered ports 0-1024 which have
> specific protocols assigned to them. Applications need to understand and
> use HTTP to use port 80.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From huaraz at moeller.plus.com  Sun Mar  1 13:36:08 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 1 Mar 2015 13:36:08 -0000
Subject: [squid-users] Logging variable  question
In-Reply-To: <54F26815.4020300@treenet.co.nz>
References: <mcsodm$i43$1@ger.gmane.org> <54F26815.4020300@treenet.co.nz>
Message-ID: <mcv4kb$k3h$1@ger.gmane.org>

Oh pretty old bug.

Thank you
Markus

"Amos Jeffries"  wrote in message news:54F26815.4020300 at treenet.co.nz... 

On 1/03/2015 4:55 a.m., Markus Moeller wrote:
> Hi,
> 
>   I wonder about the total size variables <st and >st for squid logs
> 
> # <st   Sent reply size including HTTP headers
> # >st   Received request size including HTTP headers. In the
> #       case of chunked requests the chunked encoding metadata
> #       are not included
> 
> I have set the logformat to
> 
> logformat squid_mm      %tg %6tr %>a %Ss/%03>Hs %<st %>st %rm %ru %un
> %Sh/%<A %mt
> 
> and have 2 cases for which I would like to see the request/reply total
> data size.
> 
> Case 1
> 
> Just receiving data.  (44073 and 35754 are local and remote ports
> respectively)
> 
> 28/Feb/2015:15:29:27   5887 192.168.1.17 TCP_TUNNEL/200 8895 45 CONNECT
> opensuse13.suse.home:443 - HIER_DIRECT/opensuse13.suse.home -

http://bugs.squid-cache.org/show_bug.cgi?id=3069

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sun Mar  1 14:07:37 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 01 Mar 2015 16:07:37 +0200
Subject: [squid-users] cache peer issue
In-Reply-To: <000001d05467$08e16be0$1aa443a0$@netstream.ps>
References: <000001d05467$08e16be0$1aa443a0$@netstream.ps>
Message-ID: <54F31D29.3040101@ngtech.co.il>

Hey Sanke Eyes,

Can you provide the access logs output for these requests?

Thanks,
Eliezer

On 01/03/2015 23:31, snakeeyes wrote:
> Hi ,
>
> I have many account from same provider and I would like to use those
> accounts as round robin and each request has different IP as possible
>
>
>
> The issue is , I open whatismyipaddress.com
>
>
>
> for  some freshesh I can see my ip is rotating  but after about 1 minute I
> see my ip is stuck on same ip and not rotating .
>
>
>
> Im wondering wt I need to modify the config below so that the round robin
> keep working and each time change the ip :
>
>
>
> cache_peer  vvvv  parent 22225 0 no-query round-robin name=1 no-digest
> no-tproxy proxy-only login=xxxxx
>
> cache_peer  vvv  parent 22225 0 no-query round-robin name=2  no-digest
> no-tproxy proxy-only login=xxx
>
> cache_peer  vvv  parent 22225 0 no-query round-robin  name=3 no-digest
> no-tproxy proxy-only login=xxx
>
> cache_peer  vvv  parent 22225 0 no-query round-robin name=4  no-digest
> no-tproxy proxy-only login=xxx
>
> cache_peer  vvvv  parent 22225 0 no-query round-robin  name=5 no-digest
> no-tproxy proxy-only login=xxx
>
>
>
>
>
>
>
>
>
>
>
> cheers



From bergonz at labs.it  Sun Mar  1 15:55:09 2015
From: bergonz at labs.it (Michele Bergonzoni)
Date: Sun, 1 Mar 2015 16:55:09 +0100 (CET)
Subject: [squid-users] Dual-stack IPv4/IPv6 captive portal
In-Reply-To: <54F0AF9B.2000600@opendium.com>
References: <54F091C5.9090900@opendium.com>
 <1521450889.828153.1425056458279.JavaMail.zimbra@labs.it>
 <54F0AF9B.2000600@opendium.com>
Message-ID: <1785325384.844859.1425225309295.JavaMail.zimbra@labs.it>

> and again, a lot of software just doesn't bother to support
> proxies these days, and it's only getting worse

You're right, it's probably part of the shift from enterprise IT to consumer IT (a.k.a. "consumerization"). We too find it increasingly difficult to support HTTP proxies.

Best wishes,
                        Bergonz

-- 
Ing. Michele Bergonzoni - Laboratori Guglielmo Marconi S.p.a.
Phone:+39-051-6781926 e-mail: bergonz at labs.it
alt.advanced.networks.design.configure.operate


From bielsk at us.ibm.com  Sun Mar  1 17:18:24 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Sun, 1 Mar 2015 12:18:24 -0500
Subject: [squid-users] question about encrypted connection between https
	client and Squid
Message-ID: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>


I have an https client (not a browser) that normally connects to a reverse
proxy. When it needs to go through a forward proxy, it requests a CONNECT
tunnel.
I now have a requirement to also be able to encrypt the connection between
my client and the forward proxy, and I think this is possible using Squid
and the
https_port directive (??)
My question is, will my https client now have to decrypt twice? Once for
the connection with the forward proxy and once for the connection with the
reverse proxy?
Also, must my https client still send a CONNECT message to Squid, or does
it just connect to Squid's https_port at the TCP level, perform the SSL
handshake,
and then open a TCP connection to the reverse proxy?

Thanks,

J. Bielski
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/d1408d24/attachment.htm>

From yvoinov at gmail.com  Sun Mar  1 17:24:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 01 Mar 2015 23:24:48 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
Message-ID: <54F34B60.7090900@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


01.03.15 23:18, Julianne Bielski ?????:
> 
> I have an https client (not a browser) that normally connects to a
> reverse proxy. When it needs to go through a forward proxy, it
> requests a CONNECT tunnel. I now have a requirement to also be able
> to encrypt the connection between my client and the forward proxy,
> and I think this is possible using Squid and the https_port
> directive (??)
Yep.

> My question is, will my https client now have to decrypt twice?
> Once for the connection with the forward proxy and once for the
> connection with the reverse proxy?

Re-encryption will performs only in case SSL-bumped connections.

But now I still can't imagine your infrastructure and how it must work.

> Also, must my https client still send a CONNECT message to Squid,
> or does it just connect to Squid's https_port at the TCP level,
> perform the SSL handshake, and then open a TCP connection to the
> reverse proxy?

Still want to take a look on your infrastructure scheme.

> 
> Thanks,
> 
> J. Bielski
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU80tgAAoJENNXIZxhPexGVOwH/3Q9L9IfDrP02Lj+q5pvuC2B
+oyD8xTbTGkDsitgl7KUMArhQmWVjSQAgtKEia6xTk69HlODAvVvVYuKdKYWxi8p
PW49iThLwV0GTUzpu1VGIT625ENKxf1l08PoZU+MLi+O6ijClcOfsvb09qc/OPFZ
zqNvYv1h8e7d2fL1blo9NkGgYC3B42FPer/HKqgw1KskoOuwd9OrzaQRxK+ErXAK
/SvFpRJXJcDYsz0Iw3PHFLA34rs2pyAysGllkmH8n8QJzhnOhIdy/g3Hkk2sjqjz
JsVCuj0qf3VoWHx+lIN12Zet2eFF59ELpTM52IOohTlzhWcDEVEI+z0dQyGrUpg=
=j/gg
-----END PGP SIGNATURE-----


From bielsk at us.ibm.com  Sun Mar  1 17:45:48 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Sun, 1 Mar 2015 12:45:48 -0500
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F34B60.7090900@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
Message-ID: <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>

Normally my infrastructure looks like:


client  -- HTTP CONNECT (not encrypted)  ---> proxy
client ------ TCP tunnel ---> proxy --- TCP tunnel ---> reverse proxy
client --- HTTPS application payload ---------------> reverse proxy

Now I need it to look like:

client -------- HTTPS application payload ----> proxy  ---- HTTPS
application payload  ----> reverse proxy





From:	Yuri Voinov <yvoinov at gmail.com>
To:	squid-users at lists.squid-cache.org
Date:	03/01/2015 12:26 PM
Subject:	Re: [squid-users] question about encrypted connection between
            https client and Squid
Sent by:	"squid-users" <squid-users-bounces at lists.squid-cache.org>



-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


01.03.15 23:18, Julianne Bielski ?????:
>
> I have an https client (not a browser) that normally connects to a
> reverse proxy. When it needs to go through a forward proxy, it
> requests a CONNECT tunnel. I now have a requirement to also be able
> to encrypt the connection between my client and the forward proxy,
> and I think this is possible using Squid and the https_port
> directive (??)
Yep.

> My question is, will my https client now have to decrypt twice?
> Once for the connection with the forward proxy and once for the
> connection with the reverse proxy?

Re-encryption will performs only in case SSL-bumped connections.

But now I still can't imagine your infrastructure and how it must work.

> Also, must my https client still send a CONNECT message to Squid,
> or does it just connect to Squid's https_port at the TCP level,
> perform the SSL handshake, and then open a TCP connection to the
> reverse proxy?

Still want to take a look on your infrastructure scheme.

>
> Thanks,
>
> J. Bielski
>
>
>
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU80tgAAoJENNXIZxhPexGVOwH/3Q9L9IfDrP02Lj+q5pvuC2B
+oyD8xTbTGkDsitgl7KUMArhQmWVjSQAgtKEia6xTk69HlODAvVvVYuKdKYWxi8p
PW49iThLwV0GTUzpu1VGIT625ENKxf1l08PoZU+MLi+O6ijClcOfsvb09qc/OPFZ
zqNvYv1h8e7d2fL1blo9NkGgYC3B42FPer/HKqgw1KskoOuwd9OrzaQRxK+ErXAK
/SvFpRJXJcDYsz0Iw3PHFLA34rs2pyAysGllkmH8n8QJzhnOhIdy/g3Hkk2sjqjz
JsVCuj0qf3VoWHx+lIN12Zet2eFF59ELpTM52IOohTlzhWcDEVEI+z0dQyGrUpg=
=j/gg
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/4a9c6c2d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/4a9c6c2d/attachment.gif>

From yvoinov at gmail.com  Sun Mar  1 17:51:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 01 Mar 2015 23:51:39 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
Message-ID: <54F351AB.7070302@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



01.03.15 23:45, Julianne Bielski ?????:
> Normally my infrastructure looks like:
> 
> 
> client  -- HTTP CONNECT (not encrypted)  ---> proxy client ------
> TCP tunnel ---> proxy --- TCP tunnel ---> reverse proxy client ---
> HTTPS application payload ---------------> reverse proxy
> 
> Now I need it to look like:
> 
> client -------- HTTPS application payload ----> proxy  ---- HTTPS 
> application payload  ----> reverse proxy

No problem. This will work - and with only one encryption on every
stage. Proxy can pass both - CONNECT with tunneling to reverse proxy,
or bumped HTTPS connection.

In my installation this scheme is works with most Web-sites uses
reverse proxies. I use transparent interception SSL-bump enabled Squid.


> 
> 
> 
> 
> 
> From:	Yuri Voinov <yvoinov at gmail.com> To:
> squid-users at lists.squid-cache.org Date:	03/01/2015 12:26 PM 
> Subject:	Re: [squid-users] question about encrypted connection
> between https client and Squid Sent by:	"squid-users"
> <squid-users-bounces at lists.squid-cache.org>
> 
> 
> 
> 
> 01.03.15 23:18, Julianne Bielski ?????:
> 
>> I have an https client (not a browser) that normally connects to
>> a reverse proxy. When it needs to go through a forward proxy, it 
>> requests a CONNECT tunnel. I now have a requirement to also be
>> able to encrypt the connection between my client and the forward
>> proxy, and I think this is possible using Squid and the
>> https_port directive (??)
> Yep.
> 
>> My question is, will my https client now have to decrypt twice? 
>> Once for the connection with the forward proxy and once for the 
>> connection with the reverse proxy?
> 
> Re-encryption will performs only in case SSL-bumped connections.
> 
> But now I still can't imagine your infrastructure and how it must
> work.
> 
>> Also, must my https client still send a CONNECT message to
>> Squid, or does it just connect to Squid's https_port at the TCP
>> level, perform the SSL handshake, and then open a TCP connection
>> to the reverse proxy?
> 
> Still want to take a look on your infrastructure scheme.
> 
> 
>> Thanks,
> 
>> J. Bielski
> 
> 
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU81GrAAoJENNXIZxhPexGPwkIAJrQAngPDCkylOCIb/IqYlkp
JmCW/lr2JFcH48Zr954hi7six/uduwfNeTtZsd2Cz8JVW3pqQSIrleuF0B7/7C5H
K+mDN6fQ3yQv9EjWTP1cRRdr+/OXQyWOPLoACUCz52SRvwAt1SnY9malavmnJPHS
Aoj+vGTKSM4IasULA0Vnjm3gRjN6BWrUqoXZm1ODygflGXSJnqdm+8t9RhZIHcsl
E1p9Q/hB1IJPrZU67YtgLHgg0MkOcQQzcJ/jzlPnlfOAFt0MPy8mC03YkcV4888a
KHKXElzUbCDziSbG+L5Fz2zuLlQXoDc0ZqHSSB8iNYuB5UWpSZLXWXJ55yiDUBI=
=xwxI
-----END PGP SIGNATURE-----


From bielsk at us.ibm.com  Sun Mar  1 18:07:51 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Sun, 1 Mar 2015 13:07:51 -0500
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F351AB.7070302@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
Message-ID: <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>

That's good to know.

With a transparent interception SSL-bump enabled Squid, I suppose I do not
have to explicitly
configure anything in my https client, and that Squid must listen on the
port my client is trying to
connect to (443) and that my squid.conf file must look something like this:

http_port 443 ssl-bump cert=/usr/local/squid3/etc/site_priv+pub.pem

where cert points to the location of a certificate designed to look like
the certificate of the actual destination server (my reverse proxy).

In this case there is no http and no HTTP CONNECT required?




From:	Yuri Voinov <yvoinov at gmail.com>
To:	Julianne Bielski/Raleigh/IBM at IBMUS
Cc:	squid-users at lists.squid-cache.org, squid-users
            <squid-users-bounces at lists.squid-cache.org>
Date:	03/01/2015 12:52 PM
Subject:	Re: [squid-users] question about encrypted connection between
            https client and Squid



-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



01.03.15 23:45, Julianne Bielski ?????:
> Normally my infrastructure looks like:
>
>
> client  -- HTTP CONNECT (not encrypted)  ---> proxy client ------
> TCP tunnel ---> proxy --- TCP tunnel ---> reverse proxy client ---
> HTTPS application payload ---------------> reverse proxy
>
> Now I need it to look like:
>
> client -------- HTTPS application payload ----> proxy  ---- HTTPS
> application payload  ----> reverse proxy

No problem. This will work - and with only one encryption on every
stage. Proxy can pass both - CONNECT with tunneling to reverse proxy,
or bumped HTTPS connection.

In my installation this scheme is works with most Web-sites uses
reverse proxies. I use transparent interception SSL-bump enabled Squid.


>
>
>
>
>
> From:		 Yuri Voinov <yvoinov at gmail.com> To:
> squid-users at lists.squid-cache.org Date:		 03/01/2015 12:26 PM
> Subject:		 Re: [squid-users] question about encrypted connection
> between https client and Squid Sent by:		 "squid-users"
> <squid-users-bounces at lists.squid-cache.org>
>
>
>
>
> 01.03.15 23:18, Julianne Bielski ?????:
>
>> I have an https client (not a browser) that normally connects to
>> a reverse proxy. When it needs to go through a forward proxy, it
>> requests a CONNECT tunnel. I now have a requirement to also be
>> able to encrypt the connection between my client and the forward
>> proxy, and I think this is possible using Squid and the
>> https_port directive (??)
> Yep.
>
>> My question is, will my https client now have to decrypt twice?
>> Once for the connection with the forward proxy and once for the
>> connection with the reverse proxy?
>
> Re-encryption will performs only in case SSL-bumped connections.
>
> But now I still can't imagine your infrastructure and how it must
> work.
>
>> Also, must my https client still send a CONNECT message to
>> Squid, or does it just connect to Squid's https_port at the TCP
>> level, perform the SSL handshake, and then open a TCP connection
>> to the reverse proxy?
>
> Still want to take a look on your infrastructure scheme.
>
>
>> Thanks,
>
>> J. Bielski
>
>
>
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU81GrAAoJENNXIZxhPexGPwkIAJrQAngPDCkylOCIb/IqYlkp
JmCW/lr2JFcH48Zr954hi7six/uduwfNeTtZsd2Cz8JVW3pqQSIrleuF0B7/7C5H
K+mDN6fQ3yQv9EjWTP1cRRdr+/OXQyWOPLoACUCz52SRvwAt1SnY9malavmnJPHS
Aoj+vGTKSM4IasULA0Vnjm3gRjN6BWrUqoXZm1ODygflGXSJnqdm+8t9RhZIHcsl
E1p9Q/hB1IJPrZU67YtgLHgg0MkOcQQzcJ/jzlPnlfOAFt0MPy8mC03YkcV4888a
KHKXElzUbCDziSbG+L5Fz2zuLlQXoDc0ZqHSSB8iNYuB5UWpSZLXWXJ55yiDUBI=
=xwxI
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/7a2f23c9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150301/7a2f23c9/attachment.gif>

From yvoinov at gmail.com  Sun Mar  1 18:17:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 00:17:22 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
Message-ID: <54F357B2.3040809@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



02.03.15 0:07, Julianne Bielski ?????:
> That's good to know.
> 
> With a transparent interception SSL-bump enabled Squid, I suppose I
> do not have to explicitly configure anything in my https client,
> and that Squid must listen on the port my client is trying to 
> connect to (443) and that my squid.conf file must look something
> like this:
> 
> http_port 443 ssl-bump
> cert=/usr/local/squid3/etc/site_priv+pub.pem
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
key=/usr/local/squid/etc/rootCA.key

443->3129 port mappind does with NAT.

> 
> where cert points to the location of a certificate designed to look
> like the certificate of the actual destination server (my reverse
> proxy).
With config snippet above. No, cert must be self-signed and different
from reverse proxy.

> 
> In this case there is no http and no HTTP CONNECT required?
Normally you never use CONNECT method over HTTP ports. This is
prohibited by squid basic security requirements.

This is must be in squid.conf:

# Deny requests to unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

due to security reasons.

In general, ever non-HTTPS enabled squid can forward CONNECT over 443
to server (in forwarding mode).

To do that in transparent mode it must be configured with https_port
intercept keywords.

To know more about explicit bump look at this:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

> 
> 
> 
> 
> From:	Yuri Voinov <yvoinov at gmail.com> To:	Julianne
> Bielski/Raleigh/IBM at IBMUS Cc:	squid-users at lists.squid-cache.org,
> squid-users <squid-users-bounces at lists.squid-cache.org> Date:
> 03/01/2015 12:52 PM Subject:	Re: [squid-users] question about
> encrypted connection between https client and Squid
> 
> 
> 
> 
> 
> 01.03.15 23:45, Julianne Bielski ?????:
>> Normally my infrastructure looks like:
> 
> 
>> client  -- HTTP CONNECT (not encrypted)  ---> proxy client
>> ------ TCP tunnel ---> proxy --- TCP tunnel ---> reverse proxy
>> client --- HTTPS application payload ---------------> reverse
>> proxy
> 
>> Now I need it to look like:
> 
>> client -------- HTTPS application payload ----> proxy  ----
>> HTTPS application payload  ----> reverse proxy
> 
> No problem. This will work - and with only one encryption on every 
> stage. Proxy can pass both - CONNECT with tunneling to reverse
> proxy, or bumped HTTPS connection.
> 
> In my installation this scheme is works with most Web-sites uses 
> reverse proxies. I use transparent interception SSL-bump enabled
> Squid.
> 
> 
> 
> 
> 
> 
> 
>> From:		 Yuri Voinov <yvoinov at gmail.com> To: 
>> squid-users at lists.squid-cache.org Date:		 03/01/2015 12:26 PM 
>> Subject:		 Re: [squid-users] question about encrypted connection 
>> between https client and Squid Sent by:		 "squid-users" 
>> <squid-users-bounces at lists.squid-cache.org>
> 
> 
> 
> 
>> 01.03.15 23:18, Julianne Bielski ?????:
> 
>>> I have an https client (not a browser) that normally connects
>>> to a reverse proxy. When it needs to go through a forward
>>> proxy, it requests a CONNECT tunnel. I now have a requirement
>>> to also be able to encrypt the connection between my client and
>>> the forward proxy, and I think this is possible using Squid and
>>> the https_port directive (??)
>> Yep.
> 
>>> My question is, will my https client now have to decrypt
>>> twice? Once for the connection with the forward proxy and once
>>> for the connection with the reverse proxy?
> 
>> Re-encryption will performs only in case SSL-bumped connections.
> 
>> But now I still can't imagine your infrastructure and how it
>> must work.
> 
>>> Also, must my https client still send a CONNECT message to 
>>> Squid, or does it just connect to Squid's https_port at the
>>> TCP level, perform the SSL handshake, and then open a TCP
>>> connection to the reverse proxy?
> 
>> Still want to take a look on your infrastructure scheme.
> 
> 
>>> Thanks,
> 
>>> J. Bielski
> 
> 
> 
>>> _______________________________________________ squid-users 
>>> mailing list squid-users at lists.squid-cache.org 
>>> http://lists.squid-cache.org/listinfo/squid-users
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU81eyAAoJENNXIZxhPexGO58IALLAtQZtg95Dh+82MaccSCho
cVq2Bt5sOTdnDMB/fbYlor5aNrFPvANWoNg8mrsOqssg5S4CXR2RcyNzj97LrHUI
SI3cnpk52xQXZZg88DMl303sijHp/vSH6qFtLKdWKCP/kcNqGOo9J9VYrKlnD8xL
Q7p8xwf/x9jA3u3OyOknp7PokB3NLv9A8+G30unkgZw0JUGdF6to8meS9oH8neRH
mF46EkzXcx5AdITLDHpY6ktRR1+H0rNZ2xnFBE3ESUot2dokf9ohoDS2jDrrRieR
d/CwqpBoy7Ukb1TWJYD67+aezBFUerS7m7j0+AWs/fQaLUKQUHyoOf9AKPWolkI=
=gp1a
-----END PGP SIGNATURE-----


From Antony.Stone at squid.open.source.it  Sun Mar  1 20:03:37 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 1 Mar 2015 21:03:37 +0100
Subject: [squid-users] question about encrypted connection between https
	client and Squid
In-Reply-To: <54F357B2.3040809@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com>
Message-ID: <201503012103.38150.Antony.Stone@squid.open.source.it>

On Sunday 01 March 2015 at 19:17:22 (EU time), Yuri Voinov wrote:

> 02.03.15 0:07, Julianne Bielski ?????:
> > 
> > http_port 443 ssl-bump
> > cert=/usr/local/squid3/etc/site_priv+pub.pem
> 
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt
> key=/usr/local/squid/etc/rootCA.key
> 
> 443->3129 port mappind does with NAT.

Just out of interest, is there any functional difference between:

 - Squid listening (in intercept mode) on port 3129, and NAT redirecting 
packets on port 443 to port 3129

and

 - Squid listening (in intercept mode) on port 443 ?

It seems to me from a networking perspective the two should be identical, so I 
wonder whether there really is any fundctional reason for doing the NAT and 
listening on the redirected port?


Thanks,


Antony.

-- 
It is also possible that putting the birds in a laboratory setting 
inadvertently renders them relatively incompetent.

 - Daniel C Dennett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Sun Mar  1 20:33:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 02:33:22 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <201503012103.38150.Antony.Stone@squid.open.source.it>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com>
 <201503012103.38150.Antony.Stone@squid.open.source.it>
Message-ID: <54F37792.3080302@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



02.03.15 2:03, Antony Stone ?????:
> On Sunday 01 March 2015 at 19:17:22 (EU time), Yuri Voinov wrote:
> 
>> 02.03.15 0:07, Julianne Bielski ?????:
>>> 
>>> http_port 443 ssl-bump 
>>> cert=/usr/local/squid3/etc/site_priv+pub.pem
>> 
>> http_port 3128 intercept https_port 3129 intercept ssl-bump
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> cert=/usr/local/squid/etc/rootCA.crt 
>> key=/usr/local/squid/etc/rootCA.key
>> 
>> 443->3129 port mappind does with NAT.
> 
> Just out of interest, is there any functional difference between:
> 
> - Squid listening (in intercept mode) on port 3129, and NAT
> redirecting packets on port 443 to port 3129
> 
> and
> 
> - Squid listening (in intercept mode) on port 443 ?

Yes. Second will not work. Two days ago one man here tries to do
something like this without nat. With expected result. :)

> 
> It seems to me from a networking perspective the two should be
> identical, so I wonder whether there really is any fundctional
> reason for doing the NAT and listening on the redirected port?
> 
> 
> Thanks,
> 
> 
> Antony.
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU83eSAAoJENNXIZxhPexGWpMH/A320v/Qvyceh8OTyfxNrwRR
s5NntLGL41gF9a0Ie1sgKtNBvmGDgCjBEImBqw3JrK6rIsgReeE7zJ24mUFe97zz
qFOF2OZJVtKzGKDp6qfSqbfXQ5qO1G1nC2oAbB9WUrLRCMrqoMAc7h52MAZUhP1w
CnP8SxQy6rc1UrPs1UiUyWcVHmycNgW3WeUdGQ/14otZ1OrebJxGbVhMkM0OB+Ku
JvxAVg3KnvL0rS8C+qGH0cGVCpvPhkZcgKJrggFCZ0tTQQcR2h73UEyNHnmnt4EN
15A+ZhZqv2LrkKHofV1ZAVtUb74B77ilg6rH9Bb8DJARvBJATZxx9VLkgLAeECY=
=OBz5
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Sun Mar  1 20:34:36 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 02:34:36 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <201503012103.38150.Antony.Stone@squid.open.source.it>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com>
 <201503012103.38150.Antony.Stone@squid.open.source.it>
Message-ID: <54F377DC.5020303@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

If it was possible, all of this simply would not be necessary:

http://wiki.squid-cache.org/ConfigExamples/Intercept#Traffic_Interception_capture_into_Squid

02.03.15 2:03, Antony Stone ?????:
> On Sunday 01 March 2015 at 19:17:22 (EU time), Yuri Voinov wrote:
> 
>> 02.03.15 0:07, Julianne Bielski ?????:
>>> 
>>> http_port 443 ssl-bump 
>>> cert=/usr/local/squid3/etc/site_priv+pub.pem
>> 
>> http_port 3128 intercept https_port 3129 intercept ssl-bump
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> cert=/usr/local/squid/etc/rootCA.crt 
>> key=/usr/local/squid/etc/rootCA.key
>> 
>> 443->3129 port mappind does with NAT.
> 
> Just out of interest, is there any functional difference between:
> 
> - Squid listening (in intercept mode) on port 3129, and NAT
> redirecting packets on port 443 to port 3129
> 
> and
> 
> - Squid listening (in intercept mode) on port 443 ?
> 
> It seems to me from a networking perspective the two should be
> identical, so I wonder whether there really is any fundctional
> reason for doing the NAT and listening on the redirected port?
> 
> 
> Thanks,
> 
> 
> Antony.
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU83fcAAoJENNXIZxhPexGlkcIAJthx+5/TPFHPB8Q1e7sYHIw
OfA6UWv5GxquRuXRAHzpwsYmHWU7FAwMXQjBwcA4XQ5XuX+Pazro45AYsDZIOkNw
ljKCTEKDR1Aq8S553g89dL4aVidkxeb56qxCMDnNVe4Gd11E4c2dPrFEphJ1R29o
hvqy19+9fJt6NkXmgdSsVUC9+K8zwp3kxRyUxXiZAUSsZwbJ843Zn9jO0RPJ0o//
L5c07DuI/+Skq5mYWgUPcaAONrLpHd49jnYw98j+O4bee1wex5ZwPkpNEYXVd/e/
cXCDkovtOJA95jZom7eJxuawh2WPgViyBIWGBVFwUKvFYeVdPwlZK3frPlr7Quo=
=1hO7
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Sun Mar  1 20:55:45 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 01 Mar 2015 22:55:45 +0200
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F357B2.3040809@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com>
Message-ID: <54F37CD1.3040104@ngtech.co.il>

Hey Yuri,

On 01/03/2015 20:17, Yuri Voinov wrote:
> Normally you never use CONNECT method over HTTP ports. This is
> prohibited by squid basic security requirements.

The above statement is true only if the proxy admin prohibit this.
A CONNECT method can be allowed and can be used for any purpose what so 
ever the admin of the server sees right.
There are basic default settings which allows the usage of a CONNECT 
method only to access specific "ssl safe ports".

The "right" way (if these one) to access squid using an encrypted 
channel would be throw either a tunnel or another proxy which can 
forward the request into squid.
If the client supports encrypted proxy connection you can try to use 
squid 3.5.2 and a combination of haproxy in-front.
On the haproxy use a ssl based listening port while between haproxy to 
the squid service you would need to use an unencrypted channel.
Then you can use haproxy PROXY protocol to let squid know what is the 
client src IP address.

All The Bests,
Eliezer

* I did not tested this feature yet but it is on my todo list, for now 
3.5.2 seems very stable.


From yvoinov at gmail.com  Sun Mar  1 21:01:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 03:01:04 +0600
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F37CD1.3040104@ngtech.co.il>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com> <54F37CD1.3040104@ngtech.co.il>
Message-ID: <54F37E10.7090405@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



02.03.15 2:55, Eliezer Croitoru ?????:
> Hey Yuri,
> 
> On 01/03/2015 20:17, Yuri Voinov wrote:
>> Normally you never use CONNECT method over HTTP ports. This is 
>> prohibited by squid basic security requirements.
> 
> The above statement is true only if the proxy admin prohibit this. 
> A CONNECT method can be allowed and can be used for any purpose
> what so ever the admin of the server sees right. There are basic
> default settings which allows the usage of a CONNECT method only to
> access specific "ssl safe ports".

Sure. But this is best option for newbies.

> 
> The "right" way (if these one) to access squid using an encrypted 
> channel would be throw either a tunnel or another proxy which can 
> forward the request into squid. If the client supports encrypted
> proxy connection you can try to use squid 3.5.2 and a combination
> of haproxy in-front.

Will can. When it will completely functional with interception bumping.

> On the haproxy use a ssl based listening port while between haproxy
> to the squid service you would need to use an unencrypted channel. 
> Then you can use haproxy PROXY protocol to let squid know what is
> the client src IP address.

This is environment-specific and non-common.

> 
> All The Bests, Eliezer
> 
> * I did not tested this feature yet but it is on my todo list, for
> now 3.5.2 seems very stable. 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU834QAAoJENNXIZxhPexGiaoIAK2QPyX8iCoSqUyDtBE6j6wN
hx/3gtSYtC697YmqQjti/U1X1F++eSjh8xgLi0Qna9jAyRkN7P9VkJHDqM1CL09g
VXqB8sLBxSFH7RBWOl0ytVHtvyiIC0FSafNxlXONJb1lRXxo5cH6zHw4CK+vrdvM
ZUZIBrfzAvK69yMw97mPwl+RdZxFrAQHSFbu4TrycAr0zaxHM8BGZwhCBjNluJ1M
rGVqLDiu0wi9EMdiYNpn6pvCFzc77+Lsui2XdHxN0ztcumOLgveLVq8dMsm6KcGc
yKwchfW/ATg/krCO4pgdpkX59ttBRKT1WFTpE8IDA16cg/olOCaDGvMTMWYpSsU=
=31rK
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Sun Mar  1 22:47:12 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 02 Mar 2015 00:47:12 +0200
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F37E10.7090405@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com> <54F37CD1.3040104@ngtech.co.il>
 <54F37E10.7090405@gmail.com>
Message-ID: <54F396F0.6030201@ngtech.co.il>

On 01/03/2015 23:01, Yuri Voinov wrote:
> Will can. When it will completely functional with interception bumping.

You don't need it in interception mode.

Eliezer


From squid3 at treenet.co.nz  Mon Mar  2 01:24:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 02 Mar 2015 14:24:32 +1300
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F34B60.7090900@gmail.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
Message-ID: <54F3BBD0.50502@treenet.co.nz>

Hold up there guys. yuri is answering a question that was not asked.



On 2/03/2015 6:24 a.m., Yuri Voinov wrote:
> 
> 01.03.15 23:18, Julianne Bielski ?????:
> 
>> I have an https client (not a browser) that normally connects to
>> a reverse proxy. When it needs to go through a forward proxy, it 
>> requests a CONNECT tunnel. I now have a requirement to also be
>> able to encrypt the connection between my client and the forward
>> proxy, and I think this is possible using Squid and the
>> https_port directive (??)
> Yep.
> 

Yes.

>> My question is, will my https client now have to decrypt twice?

That depends on how much you trust the proxy.

If you trust it fully, then you only have to encrypt the client->proxy
connection. The client sends traffic inside that encrypted connection
as if it were normal HTTP to a proxy.

To cause the outbound connection to be encrypted the client sends URL
https:// scheme. The proxy will separately encrypt the outbound
connection. When the proxy is built with SSL/TLS support this works
regardless of whether the client is connecting via HTTP or TLS to the
proxy.


If you *DONT* trust the proxy at all then use CONNECT tunnel with
second layer of TLS. You also have to use DANE/TLSA to validate the
server certificate using inside the CONNECT tunnel at the client end
since regular trusted-CA validation does not protect agaist proxies
bumping the CONNECT anyway.


>> Once for the connection with the forward proxy and once for the 
>> connection with the reverse proxy?
> 
> Re-encryption will performs only in case SSL-bumped connections.
> 

*NO*.

A forward proxy receiving an https:// scheme URL will always ensure
outbound DIRECT connections are encrypted between the proxy and origin
server.

A cache_peer connection can be un-encrypted, but they must be
explicitly configured by the admin. The peer will receive the https://
URL and try to encrypt its outbound under the same conditions.

Amos


From squid3 at treenet.co.nz  Mon Mar  2 01:38:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 02 Mar 2015 14:38:43 +1300
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F37CD1.3040104@ngtech.co.il>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com> <54F37CD1.3040104@ngtech.co.il>
Message-ID: <54F3BF23.4050307@treenet.co.nz>

On 2/03/2015 9:55 a.m., Eliezer Croitoru wrote:
> Hey Yuri,
> 
> On 01/03/2015 20:17, Yuri Voinov wrote:
>> Normally you never use CONNECT method over HTTP ports. This is
>> prohibited by squid basic security requirements.
> 
> The above statement is true only if the proxy admin prohibit this.
> A CONNECT method can be allowed and can be used for any purpose what so
> ever the admin of the server sees right.
> There are basic default settings which allows the usage of a CONNECT
> method only to access specific "ssl safe ports".
> 
> The "right" way (if these one) to access squid using an encrypted
> channel would be throw either a tunnel or another proxy which can
> forward the request into squid.

There *is* a Right Way.

It is this:

1) using this in squid.conf:
     https_port 3129 cert=/path/to/proxy.pem

2) client connects to 3129 using TCP, then performs TLS handshake.

3) client sends requests inside the encrypted connection as if they were
HTTP to a proxy but using https:// URL scheme.

Thats is *all*.

It is very simple. It works well with SSL-enabled Squid.

It avoids both the page-long list of NAT/TPROXY interception problems
and the other half-page list of SSL-bump hijacking related prblems.

Amos



From squid3 at treenet.co.nz  Mon Mar  2 02:33:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 02 Mar 2015 15:33:29 +1300
Subject: [squid-users] Dual-stack IPv4/IPv6 captive portal
In-Reply-To: <1785325384.844859.1425225309295.JavaMail.zimbra@labs.it>
References: <54F091C5.9090900@opendium.com>
 <1521450889.828153.1425056458279.JavaMail.zimbra@labs.it>
 <54F0AF9B.2000600@opendium.com>
 <1785325384.844859.1425225309295.JavaMail.zimbra@labs.it>
Message-ID: <54F3CBF9.7050804@treenet.co.nz>

On 2/03/2015 4:55 a.m., Michele Bergonzoni wrote:
>> and again, a lot of software just doesn't bother to support proxies
>> these days, and it's only getting worse
> 
> You're right, it's probably part of the shift from enterprise IT to
> consumer IT (a.k.a. "consumerization"). We too find it increasingly
> difficult to support HTTP proxies.

I find its due to two factors:

1) a large number of developers who dont even consider that proxies
might exist in HTTP when they start coding to use it as if it were an
end-to-end *transport* instead of a multi-hop Transfer protocol.

 These people are plain wrong about how the basic protocol works and yet
they are treated with must-accept policies by so many networks.
 Imagine what would happen if you MUST-accept all emails delivered? or
any kind of DNS response they chose to send you? those are two other
major protcols with proxies that work just fine by rejecting bad
messages wholesale.


2) a side effect of the arms race in the past 2 decades between proxy
admin forcing proxy caches to disobey HTTP standards (all those
refresh_pattern overrides etc trying to force, badly, HTTP/1.0 proxies
to do HTTP/1.1 things).
 This widespread behaviour has turned a group of influential people in
the browser community against the idea of HTTP proxies in general and
they are actively fighting to eradicate proxies in HTTP. Even if it
completely breaks users browsing experience.



The more people intercept and accept bad traffic the more it gets worse.
The group #1 developers see their HTTP/1.x crap "working" with many
proxies (so it just _must_ be the standards compliant proxies which are
broken right? yeah No.). The group #2 people see just another evil being
perpetrated by proxies.


Now the point of this rant :-)   HTTP/2 !!

Part of the HTTP/2 design goals was to remove the port difference
barrier between proxies and origin servers. The protocol syntax oddities
are now gone and port 80 native HTTP/2 traffic can be intercepted
without needing the NAT destination tricks that cause so many problems
in HTTP/1.x. Just like email is routinely intercepted by ISPs for spam
filtering.

HTTP/2 support is already in development, hopefully for Squid-3.6 later
this year. Sponsorship is very welcome to assist speeding that up and
with testing when its a bit closer to ready.

Cheers
Amos


From yvoinov at gmail.com  Mon Mar  2 10:25:00 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 16:25:00 +0600
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F275FC.9030204@treenet.co.nz>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
Message-ID: <54F43A7C.1030907@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Not clear:

I can use %adapt directly? This completely undocumented.

I.e.

deny_info
http://192.168.100.251:8080/cgi-bin/virus_warn.cgi?clientaddr=%a&url=%u&virus=%adapt
clamav_service_resp

? This works with %u and %a, but can't get adapt value.

Not obvious how to use this code.

01.03.15 8:14, Amos Jeffries ?????:
> On 28/02/2015 9:59 a.m., Yuri Voinov wrote:
>> Hi gents.
> 
>> Can I use log codes in custom error page and how to do this? Some
>>  examples will be useful.
> 
>> In details, interested in using adapt::<last_h code.
> 
>> Just point me on right way.
> 
> You just wrote the static text with the code in it at the position
> you want the item to appear.
> 
> The currently supported error page macro/tag/code's are listed at: 
> <http://wiki.squid-cache.org/Features/CustomErrors#ERR_.2A_template_codes_for_embedding>
>
> 
Note most of the logformat codes are not supported yet.
> 
> And have a look at the installed templates for plenty of examples
> of usage.
> 
> Amos _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9Dp8AAoJENNXIZxhPexGnT8IAI+d/NvTddldwO/LBzNtTJYn
0RQgtSSZIUrhkjLwoNscDUAq8vt5kYNIg1aWjQixP0FECRZe55xCET+8xdhZN1qK
oate4rG584/rQWTnwntiuz9f90X9QLWPpVpQWUbKmA9MAl1ULrMZmeTLrOKi+UrI
H3QD1pO0Bh750H46DpmYntBdTDOKRbVoNBfycA5llm2Lz7fZZygI/J6lkL3nEmWq
xOcR5ZyimX5qEZ5lPBXQcfjQoysT9dgTXI7a8B69/0s1JYpsWSNt1p2SyIQFheZ9
j8OWiiwYWrMx95Y8Vl2ebvYpQGLBxby9cFP5ij0tbqr7VAY7vxy+ESHT2ruPxt4=
=w2Am
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Mar  2 10:43:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 02 Mar 2015 23:43:03 +1300
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F43A7C.1030907@gmail.com>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
 <54F43A7C.1030907@gmail.com>
Message-ID: <54F43EB7.5030004@treenet.co.nz>

On 2/03/2015 11:25 p.m., Yuri Voinov wrote:
> Not clear:
> 
> I can use %adapt directly? This completely undocumented.

The documented ones are the only ones that work at present.
Patching is required for anything else.

Amos


From yvoinov at gmail.com  Mon Mar  2 10:48:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 16:48:35 +0600
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F43EB7.5030004@treenet.co.nz>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
 <54F43A7C.1030907@gmail.com> <54F43EB7.5030004@treenet.co.nz>
Message-ID: <54F44003.3000505@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Eh....... :(

No way to get metadata from adaptation module?

02.03.15 16:43, Amos Jeffries ?????:
> On 2/03/2015 11:25 p.m., Yuri Voinov wrote:
>> Not clear:
>> 
>> I can use %adapt directly? This completely undocumented.
> 
> The documented ones are the only ones that work at present. 
> Patching is required for anything else.
> 
> Amos _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9EADAAoJENNXIZxhPexGz+4IAKA5d6gxbSfW/hsSHaJgpRrZ
14YTuNtRDddfurNoRLDKDELfkwGzr+gkcIeJHp4ZDxC19icf6g/+DLleo/y/tw7O
7HQ0cLfN/m1zCkziE+vtgraqj6TuObE98Yr9DkRKZ8APnhRdWPxBRRQRqUBW9I5N
wx6Wx2pxP2uYvYniTt2Ogb4v6Mgo7QnCGs6GnHfVDxWYqHmP4pX76yB8BZvrUaDZ
2bj3vRxzqbv4xKNCaRfrp+Jq4CqAtC9h7C6QvWrB31sg/9WaGr8LUw1u3/KVu0PY
b2FkErI+skUcyHaFzWje+qJD3mwDrZjM18uH8e2F9grZJxYwcK98xapH71z/rkU=
=BfrO
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Mon Mar  2 10:56:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 16:56:50 +0600
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F43EB7.5030004@treenet.co.nz>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
 <54F43A7C.1030907@gmail.com> <54F43EB7.5030004@treenet.co.nz>
Message-ID: <54F441F2.3000204@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

But Amos,

wait:

root @ cthulhu / # /usr/local/squid/bin/squidclient -p 3127 -r
http://www.eicar.org/download/eicar_com.zip
Sending HTTP request ... done.
HTTP/1.1 301 Moved Permanently
Date: Mon, 02 Mar 2015 10:55:55 GMT
Location:
http://cthulhu.localdomain:8080/cgi-bin/clwarn.cgi.ru_RU?url=http://www.eicar.org/download/eicar_com.zip&source=127.0.0.1&user=-&virus=stream:
Eicar-Test-Signature FOUND
Server: C-ICAP
Content-Type: text/html
Content-Language: en
X-Virus-ID: Eicar-Test-Signature
X-Infection-Found: Type=0; Resolution=2; Threat=Eicar-Test-Signature;
Via: ICAP/1.0 cthulhu (C-ICAP/0.3.5 SquidClamav/Antivirus service )
X-Cache: MISS from cthulhu
X-Cache-Lookup: MISS from cthulhu:3127
Connection: close

How ICAP does it?

02.03.15 16:43, Amos Jeffries ?????:
> On 2/03/2015 11:25 p.m., Yuri Voinov wrote:
>> Not clear:
>> 
>> I can use %adapt directly? This completely undocumented.
> 
> The documented ones are the only ones that work at present. 
> Patching is required for anything else.
> 
> Amos _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9EHyAAoJENNXIZxhPexGepMIAKLAniURC+wrBuJY7uDXlqvB
oT6YGa2tQWWapTN+1koqR4pNvuuuPmxLBl5VpSEjXOkDYVgPD0oAL/o8UPWIMvuX
wwGx8C9dbpGhfxQijDDNCpIiXsia/1wWB6Pgtbv72x/FmN3UyoEa6i0Bup4FjrBv
Xd2RyINaf6QwwKM13RN0Rinowtk78nkPKgVFOjoy1WeiUiBG3TW4QKyS0DWpTdwa
zVqB9PgM2Iz3mgJsez7qzrD01XEOQY5wkPBxD3QMzxAOkhxTmCynCp631Wekmj33
KYIuT/uto56Sg3RM5xTUf9zmyz+h+jwuNgGxsZMpNJkeqVyPUq7KupUfuQVwmyM=
=6YCq
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Mon Mar  2 11:09:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Mar 2015 00:09:05 +1300
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F441F2.3000204@gmail.com>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
 <54F43A7C.1030907@gmail.com> <54F43EB7.5030004@treenet.co.nz>
 <54F441F2.3000204@gmail.com>
Message-ID: <54F444D1.4050304@treenet.co.nz>

On 2/03/2015 11:56 p.m., Yuri Voinov wrote:
> But Amos,
> 
> wait:
> 
> root @ cthulhu / # /usr/local/squid/bin/squidclient -p 3127 -r 
> http://www.eicar.org/download/eicar_com.zip Sending HTTP request
> ... done. HTTP/1.1 301 Moved Permanently Date: Mon, 02 Mar 2015
> 10:55:55 GMT Location: 
> http://cthulhu.localdomain:8080/cgi-bin/clwarn.cgi.ru_RU?url=http://www.eicar.org/download/eicar_com.zip&source=127.0.0.1&user=-&virus=stream:
>
> 
Eicar-Test-Signature FOUND
> Server: C-ICAP Content-Type: text/html Content-Language: en 
> X-Virus-ID: Eicar-Test-Signature X-Infection-Found: Type=0;
> Resolution=2; Threat=Eicar-Test-Signature; Via: ICAP/1.0 cthulhu
> (C-ICAP/0.3.5 SquidClamav/Antivirus service ) X-Cache: MISS from
> cthulhu X-Cache-Lookup: MISS from cthulhu:3127 Connection: close
> 
> How ICAP does it?

ICAP has full access to the HTTP messages and payloads.

Error page generator ... does not.

Amos


From yvoinov at gmail.com  Mon Mar  2 11:12:24 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 17:12:24 +0600
Subject: [squid-users] How to use access.log codes in custom error page
In-Reply-To: <54F444D1.4050304@treenet.co.nz>
References: <54F0DA99.2030600@gmail.com> <54F275FC.9030204@treenet.co.nz>
 <54F43A7C.1030907@gmail.com> <54F43EB7.5030004@treenet.co.nz>
 <54F441F2.3000204@gmail.com> <54F444D1.4050304@treenet.co.nz>
Message-ID: <54F44598.3090304@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

A-ha!

Can I use ICAP - for example, with ecap to extract metadata from ecap
adapters?

02.03.15 17:09, Amos Jeffries ?????:
> On 2/03/2015 11:56 p.m., Yuri Voinov wrote:
>> But Amos,
>> 
>> wait:
>> 
>> root @ cthulhu / # /usr/local/squid/bin/squidclient -p 3127 -r 
>> http://www.eicar.org/download/eicar_com.zip Sending HTTP request 
>> ... done. HTTP/1.1 301 Moved Permanently Date: Mon, 02 Mar 2015 
>> 10:55:55 GMT Location: 
>> http://cthulhu.localdomain:8080/cgi-bin/clwarn.cgi.ru_RU?url=http://www.eicar.org/download/eicar_com.zip&source=127.0.0.1&user=-&virus=stream:
>>
>>
>
>> 
Eicar-Test-Signature FOUND
>> Server: C-ICAP Content-Type: text/html Content-Language: en 
>> X-Virus-ID: Eicar-Test-Signature X-Infection-Found: Type=0; 
>> Resolution=2; Threat=Eicar-Test-Signature; Via: ICAP/1.0 cthulhu 
>> (C-ICAP/0.3.5 SquidClamav/Antivirus service ) X-Cache: MISS from 
>> cthulhu X-Cache-Lookup: MISS from cthulhu:3127 Connection: close
>> 
>> How ICAP does it?
> 
> ICAP has full access to the HTTP messages and payloads.
> 
> Error page generator ... does not.
> 
> Amos _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9EWYAAoJENNXIZxhPexGaugH/i6UeMallNNDB3Lr+c8KyOyN
vYiiVk5Bs6/HhSdTg4b7EhT/KThqM4ax2SkfBSGeD9bQgivqbGHQHxiy1g11amBP
GW0DoL5JkBgyl5X/OS+1xDWxRWaim73kCbR4NqR+i1sqV2vvGJYijNwme2j7OEgH
OtyfGMWtYRdbofDc3LMvUhc4whYORmLhwFCtCtfKhH6/MnFaRmoimq8Eeaapdcvd
F3+Riy6Ve4VPDbWbx67cgAyElNaH5RHbzswzYzfnZDW9ZQ5RcgQI4awrN79jqnvt
+vcvFmf8KmK7rUgtlXwTcJZRM4lFBg9R2P0fVmUfdVojQUGYTVtyR9wP8r7WxCk=
=qocC
-----END PGP SIGNATURE-----


From hack.back at hotmail.com  Mon Mar  2 11:24:20 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 2 Mar 2015 03:24:20 -0800 (PST)
Subject: [squid-users] squid and post method
In-Reply-To: <54F25FE2.8030107@artifact-software.com>
References: <1425168461331-4670122.post@n4.nabble.com>
 <54F25FE2.8030107@artifact-software.com>
Message-ID: <1425295460608-4670156.post@n4.nabble.com>

POST request come from input form .
i want to read this data in squid .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122p4670156.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Mar  2 11:47:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 02 Mar 2015 17:47:39 +0600
Subject: [squid-users] squid and post method
In-Reply-To: <1425295460608-4670156.post@n4.nabble.com>
References: <1425168461331-4670122.post@n4.nabble.com>
 <54F25FE2.8030107@artifact-software.com>
 <1425295460608-4670156.post@n4.nabble.com>
Message-ID: <54F44DDB.5090503@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Logins/passwords? ;)

02.03.15 17:24, HackXBack ?????:
> POST request come from input form . i want to read this data in
> squid .
> 
> 
> 
> -- View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122p4670156.html
>
> 
Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9E3aAAoJENNXIZxhPexGHAYH/1egDfaF2S5J9egPUFr51K+n
SBT3UdGP7vpXHArSmFUb5irru+w/ree0eRj5Pe+S8f8WXa1Tgge3XN9KBWjvtOc8
/eqwr3qXrqtaF7BJYPzikOUM1IR3h83t8wcRnmzkzokkJCge6KBDziCvADsCHXt6
Nhjcraiem53qolZPiOAEWV54GXJd/aYgDjTcX7NJQG/Sih6+W25G0OSSexvCoHp8
73+qO/k1zj19xVPrZc39opk26JM9SWvQFxi/ySuCHpe5wFLj7fG3V0qsoaRd4Y+o
65aMtFhhRESk3jqJH3jQ2nkGEcp/xlXCAXwh+7hiaVS7BD2Dw0B7FSAP1tcX6Tc=
=AnuN
-----END PGP SIGNATURE-----


From bielsk at us.ibm.com  Mon Mar  2 12:15:18 2015
From: bielsk at us.ibm.com (Julianne Bielski)
Date: Mon, 2 Mar 2015 07:15:18 -0500
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <54F3BF23.4050307@treenet.co.nz>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com> <54F37CD1.3040104@ngtech.co.il>
 <54F3BF23.4050307@treenet.co.nz>
Message-ID: <OF3556AD9E.631767F9-ON87257DFC.0043115D-85257DFC.004352D5@us.ibm.com>


Amos,

Per:
There *is* a Right Way.

It is this:

1) using this in squid.conf:
     https_port 3129 cert=/path/to/proxy.pem

2) client connects to 3129 using TCP, then performs TLS handshake.

3) client sends requests inside the encrypted connection as if they were
HTTP to a proxy but using https:// URL scheme.


If my client (it's not a browser) is an https client ultimately attempting
to send its payload to a reverse proxy listening on 443, does this mean
that I will have an encrypted payload inside of another encrypted payload?
Also, if I configure my client to send traffic to Squid at port 3129,
then doesn't this mean I'm using Squid explicitly and not transparently?




From:	Amos Jeffries <squid3 at treenet.co.nz>
To:	squid-users at lists.squid-cache.org
Date:	03/01/2015 08:39 PM
Subject:	Re: [squid-users] question about encrypted connection between
            https client and Squid
Sent by:	"squid-users" <squid-users-bounces at lists.squid-cache.org>



On 2/03/2015 9:55 a.m., Eliezer Croitoru wrote:
> Hey Yuri,
>
> On 01/03/2015 20:17, Yuri Voinov wrote:
>> Normally you never use CONNECT method over HTTP ports. This is
>> prohibited by squid basic security requirements.
>
> The above statement is true only if the proxy admin prohibit this.
> A CONNECT method can be allowed and can be used for any purpose what so
> ever the admin of the server sees right.
> There are basic default settings which allows the usage of a CONNECT
> method only to access specific "ssl safe ports".
>
> The "right" way (if these one) to access squid using an encrypted
> channel would be throw either a tunnel or another proxy which can
> forward the request into squid.

There *is* a Right Way.

It is this:

1) using this in squid.conf:
     https_port 3129 cert=/path/to/proxy.pem

2) client connects to 3129 using TCP, then performs TLS handshake.

3) client sends requests inside the encrypted connection as if they were
HTTP to a proxy but using https:// URL scheme.

Thats is *all*.

It is very simple. It works well with SSL-enabled Squid.

It avoids both the page-long list of NAT/TPROXY interception problems
and the other half-page list of SSL-bump hijacking related prblems.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150302/e5f18f48/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150302/e5f18f48/attachment.gif>

From alanpalmer72 at yahoo.com  Mon Mar  2 12:29:08 2015
From: alanpalmer72 at yahoo.com (Alan Palmer)
Date: Mon, 02 Mar 2015 07:29:08 -0500
Subject: [squid-users] Squid 3.5.2 and Avast free anti-virus
Message-ID: <54F45794.8070800@yahoo.com>

Squid 3.5.2 intercept mode and Avast free antivirus 2015 on windows 7
aren't playing well together.  Chrome returns a ca invalid error, 
details reveal
its the avast web/mail shield cert that its not being trusted. 
Everything works if
I turn the webshield off, or on a very strange note, works fine on a 
Windows XP
(I know, old/bad, upgrade blah blah) machine also running avast 2015.  The
windows XP version does have a difference cert than the windows 7 version
however.  Avast seems to be doing a sslbump on its own between the client
and the squid proxy.  Does anyone else have a similar setup working, and 
if so
whats the magic incantation to make it play nice?

  squid -v
Squid Cache: Version 3.5.2
Service Name: squid
configure options:  '--disable-strict-error-checking' 
'--disable-arch-native' '--enable-shared' 
'--datadir=/usr/local/share/squid' 
'--libexecdir=/usr/local/libexec/squid' '--disable-loadable-modules' 
'--enable-arp-acl' '--enable-auth' '--enable-delay-pools' 
'--enable-follow-x-forwarded-for' '--enable-forw-via-db' 
'--enable-http-violations' '--enable-icap-client' '--enable-ipv6' 
'--enable-referer-log' '--enable-removal-policies=lru heap' 
'--enable-ssl' '--with-openssl=/usr/local/ssl' '--enable-storeio=aufs 
ufs diskd' '--with-default-user=_squid' '--with-filedescriptors=8192' 
'--with-krb5-config=no' '--with-pidfile=/var/run/squid.pid' 
'--with-pthreads' '--with-swapdir=/var/squid/cache' 
'--disable-pf-transparent' '--enable-ipfw-transparent' 
'--enable-external-acl-helpers=LDAP_group SQL_session file_userip 
time_quota session  unix_group wbinfo_group LDAP_group 
eDirectory_userip' '--prefix=/usr/local' '--sysconfdir=/etc/squid' 
'--mandir=/usr/local/man' '--infodir=/usr/local/info' 
'--localstatedir=/var/squid' '--disable-silent-rules' 'CC=cc' 
'CFLAGS=-O2 -pipe' 'LDFLAGS=-L/usr/local/lib' 
'CPPFLAGS=-I/usr/local/include' 'CXX=c++' 'CXXFLAGS=-O2 -pipe' 
'--enable-ssl-crtd' '--enable-ltdl-convenience'

  uname -a
OpenBSD jarosz-fw 5.6 GENERIC.MP#299 i386

squid.conf
...
https_port [::1]:3127 intercept ssl-bump \

         generate-host-certificates=on \
         dynamic_cert_mem_cache_size=16MB \
         cert=/etc/squid/ssl_cert/Test2.pem
#
#       SSL intercept configuration
#
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /data/squid/ssl_db 
-M 16MB
sslcrtd_children 10
always_direct allow all
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
sslproxy_cafile /etc/ssl/ca-bundle.crt

https_port[127.0.0.1]:3127 same config lines as the IPv6 port.



From squid3 at treenet.co.nz  Mon Mar  2 12:39:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Mar 2015 01:39:55 +1300
Subject: [squid-users] question about encrypted connection between https
 client and Squid
In-Reply-To: <OF3556AD9E.631767F9-ON87257DFC.0043115D-85257DFC.004352D5@us.ibm.com>
References: <OF85718608.39711BFB-ON87257DFB.005DC1C0-85257DFB.005F12CA@us.ibm.com>
 <54F34B60.7090900@gmail.com>
 <OFF1D8F1B0.1194D472-ON87257DFB.006101A3-85257DFB.006194F9@us.ibm.com>
 <54F351AB.7070302@gmail.com>
 <OF98D79C26.E03F99AD-ON87257DFB.0062EF5E-85257DFB.006399A4@us.ibm.com>
 <54F357B2.3040809@gmail.com> <54F37CD1.3040104@ngtech.co.il>
 <54F3BF23.4050307@treenet.co.nz>
 <OF3556AD9E.631767F9-ON87257DFC.0043115D-85257DFC.004352D5@us.ibm.com>
Message-ID: <54F45A1B.5030100@treenet.co.nz>

On 3/03/2015 1:15 a.m., Julianne Bielski wrote:
> 
> Amos,
> 
> Per:
> There *is* a Right Way.
> 
> It is this:
> 
> 1) using this in squid.conf:
>      https_port 3129 cert=/path/to/proxy.pem
> 
> 2) client connects to 3129 using TCP, then performs TLS handshake.
> 
> 3) client sends requests inside the encrypted connection as if they were
> HTTP to a proxy but using https:// URL scheme.
> 
> 
> If my client (it's not a browser) is an https client ultimately attempting
> to send its payload to a reverse proxy listening on 443, does this mean
> that I will have an encrypted payload inside of another encrypted payload?

No. You have one encryption layer, the TLS between the client and proxy.

The https:// scheme tells the proxy what to do with the requests,
including that the need to be kept secure on the outbound connection.


> Also, if I configure my client to send traffic to Squid at port 3129,
> then doesn't this mean I'm using Squid explicitly and not transparently?

That depends on what the other word in the phrase "transparent ..." is.

* Squid always performs "transparent HTTP" as much as it can with the
configuration you give it.

* Its up to you if the network performs "transparent autoconfiguration"
to deliver the proxy IP:port details to the client.


If by "transparently" you mean "transparent interception" then yes, its
not that. The Right Way to use a proxy is explicitly.

Amos



From squid3 at treenet.co.nz  Mon Mar  2 12:55:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Mar 2015 01:55:06 +1300
Subject: [squid-users] Squid 3.5.2 and Avast free anti-virus
In-Reply-To: <54F45794.8070800@yahoo.com>
References: <54F45794.8070800@yahoo.com>
Message-ID: <54F45DAA.2010807@treenet.co.nz>

On 3/03/2015 1:29 a.m., Alan Palmer wrote:
> Squid 3.5.2 intercept mode and Avast free antivirus 2015 on windows 7
> aren't playing well together.  Chrome returns a ca invalid error,
> details reveal
> its the avast web/mail shield cert that its not being trusted.
> Everything works if
> I turn the webshield off, or on a very strange note, works fine on a
> Windows XP
> (I know, old/bad, upgrade blah blah) machine also running avast 2015.  The
> windows XP version does have a difference cert than the windows 7 version
> however.  Avast seems to be doing a sslbump on its own between the client
> and the squid proxy.  Does anyone else have a similar setup working, and
> if so
> whats the magic incantation to make it play nice?


This is roughly what the inter-tubes usually look like:

 browser -> AV ===> router ===> NAT -> Squid ===> Internet


As you can see the browser -> AV link has to be working first before
traffic has a chance of even getting near Squid. So config in Squid is
not going to help there.


You may be able to get the Avast CA cert to be trusted by Chrome.
Otherwise a reasonable backup is to drop the WebShield on-machine and
using ICAP in Squid to pass traffic to an AV scanner instead.

Amos



From alanpalmer72 at yahoo.com  Mon Mar  2 13:23:45 2015
From: alanpalmer72 at yahoo.com (Alan Palmer)
Date: Mon, 2 Mar 2015 08:23:45 -0500
Subject: [squid-users] Squid 3.5.2 and Avast free anti-virus
In-Reply-To: <54F45DAA.2010807@treenet.co.nz>
References: <54F45794.8070800@yahoo.com> <54F45DAA.2010807@treenet.co.nz>
Message-ID: <B3CC43C6-5561-4C6C-96C2-123B8AD41235@yahoo.com>


> This is roughly what the inter-tubes usually look like:
> 
> browser -> AV ===> router ===> NAT -> Squid ===> Internet
> 
In this configuration, chrome gives the error

In
Browser->AV->router->NAT->Inet
Or
Browser->router->NAT(redirect)->squid->Inet

Things work just fine, chrome will trust the av or squid on their own but not the av when both are inline. That's the oddness. Even odder is winXP working with both inline. 

> You may be able to get the Avast CA cert to be trusted by Chrome.
> Otherwise a reasonable backup is to drop the WebShield on-machine and
> using ICAP in Squid to pass traffic to an AV scanner instead.

Maybe I'll go that route. Have to find an AV Scanner. 

Alan

From stu at spacehopper.org  Mon Mar  2 13:36:58 2015
From: stu at spacehopper.org (Stuart Henderson)
Date: Mon, 2 Mar 2015 13:36:58 +0000 (UTC)
Subject: [squid-users] Different squid-3.5.2 compile error on OpenBSD 5.6
References: <54EF6DED.4030604@yahoo.com>
Message-ID: <slrnmf8prq.8gs.stu@naiad.spacehopper.org>

On 2015-02-26, Alan Palmer <alanpalmer72 at yahoo.com> wrote:
> While waiting with baited breath for --with-libressl support, I 
> installed openssl-1.02 on openbsd-5.6 to
> get squid to compile, but got this error in the final linking:
>
> MemStore.o(.text+0x4fe0): In function 
> `MemStore::copyFromShm(StoreEntry&, int, Ipc::StoreMapAnchor const&)':
>: undefined reference to `__sync_fetch_and_add_8'
> MemStore.o(.text+0x5197): more undefined references to 
> `__sync_fetch_and_add_8' follow
>
> Now this is a bit odd because, from the config.log:
>
> configure:20105: inlining optimizations enabled: yes
> configure:20124: checking for GNU atomic operations support
> configure:20151: c++ -o conftest -O2 -pipe -I/usr/local/include 
> -L/usr/local/lib
>   conftest.cpp  >&5
> configure:20151: $? = 0
> configure:20151: ./conftest
> configure:20151: $? = 0
> configure:20156: result: yes
>
> now configure only checks for __sync_fetch_and_add not 
> __sync_fetch_and_add_8.

64-bit atomics are supported on some but not all OpenBSD machine
architectures. Squid's autoconf test only tries to use 32-bit atomic
operations; success of this does not imply that 64-bit operations will
also work. In the squid port, I've modified the test, patch below.

> The issue, I believe, is libc++ hasnt been ported to openbsd so it uses 
> libstdc++

Correct that libc++ hasn't been ported to OpenBSD, but that's not a show-
stopper. In ports, we are using gcc 4.8 with its newer libstdc++ to build
squid-3.5.2.

You will also run into some issues with libressl if you are building
with SSL inspection support:

- SSLv2 and compression (which AIUI only ever applied to SSLv2?) were
completely removed from libressl (same for google's boringssl).
Patch for this below.

- In the version of libressl in OpenBSD 5.6 (and in boringssl),
the *_cipher_by_char functions were removed; they've been re-added in
newer libressl releases and in -current OpenBSD, Squid uses these so
you'll probably want to run -current for now. (This will be in
OpenBSD 5.7 as well).

3.4-style bumping works for me, though I occasionally run into issues
with EADDRINUSE when binding to sockets (especially when restarting).
I haven't tested peek/splice yet.

There's one other known issue with squid on OpenBSD, rock doesn't work
(creates the initial db, but fails with EMSGSIZE when it tries to open
it at startup, I haven't had chance to investigate this further).
Sorry I haven't got round to opening tickets on the tracker for any
of these yet.


--- configure.ac.orig	Sun Dec 21 05:15:31 2014
+++ configure.ac	Tue Jan  6 22:32:47 2015
@@ -426,7 +426,7 @@ dnl Check for atomic operations support in the compile
 dnl
 AC_MSG_CHECKING([for GNU atomic operations support])
 AC_RUN_IFELSE([AC_LANG_PROGRAM([[
-    int n = 0;
+    long long n = 0;
 ]],[[
     __sync_add_and_fetch(&n, 10); // n becomes 10
     __sync_fetch_and_add(&n, 20); // n becomes 30

--- src/ssl/bio.cc.orig	Fri Feb  6 15:36:07 2015
+++ src/ssl/bio.cc	Fri Feb  6 15:47:15 2015
@@ -151,7 +151,10 @@ Ssl::Bio::stateChanged(const SSL *ssl, int where, int 
 bool
 Ssl::ClientBio::isClientHello(int state)
 {
-    return (state == SSL2_ST_GET_CLIENT_HELLO_A ||
+    return (
+#ifdef SSL2_ST_GET_CLIENT_HELLO_A
+            state == SSL2_ST_GET_CLIENT_HELLO_A ||
+#endif
             state == SSL3_ST_SR_CLNT_HELLO_A ||
             state == SSL23_ST_SR_CLNT_HELLO_A ||
             state == SSL23_ST_SR_CLNT_HELLO_B ||
@@ -325,7 +328,11 @@ adjustSSL(SSL *ssl, Ssl::Bio::sslFeatures &features)
 
     // If the client supports compression but our context does not support
     // we can not adjust.
+#ifdef OPENSSL_NO_COMP
+    if (features.compressMethod) {
+#else
     if (features.compressMethod && ssl->ctx->comp_methods == NULL) {
+#endif
         debugs(83, 5, "Client Hello Data supports compression, but we do not!");
         return false;
     }
@@ -669,9 +676,11 @@ Ssl::Bio::sslFeatures::get(const SSL *ssl)
     debugs(83, 7, "SNI server name: " << serverName);
 #endif
 
+#ifndef OPENSSL_NO_COMP
     if (ssl->session->compress_meth)
         compressMethod = ssl->session->compress_meth;
     else if (sslVersion >= 3) //if it is 3 or newer version then compression is disabled
+#endif
         compressMethod = 0;
     debugs(83, 7, "SSL compression: " << compressMethod);
 



From squid3 at treenet.co.nz  Mon Mar  2 15:21:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Mar 2015 04:21:05 +1300
Subject: [squid-users] Different squid-3.5.2 compile error on OpenBSD 5.6
In-Reply-To: <slrnmf8prq.8gs.stu@naiad.spacehopper.org>
References: <54EF6DED.4030604@yahoo.com>
 <slrnmf8prq.8gs.stu@naiad.spacehopper.org>
Message-ID: <54F47FE1.7090009@treenet.co.nz>

Thank you Stuart!

We have officially dropped support for SSLv2 in Squid-3.6, so the
offending line there is now just erased, but your patch is applied to
Squid-3.5.

The SSL compression part is in Squid-3, will be in 3.5 in a few days.

I'm not going to merge the atomics part quite yet. I'd like the the rock
and IPC authors to review it first.


If you have a patch like this in future you can just submit it to the
squid-dev mailing list for review and inclusion.

Cheers
Amos


From carvakaguru at gmail.com  Mon Mar  2 17:41:46 2015
From: carvakaguru at gmail.com (Carvaka Guru)
Date: Mon, 2 Mar 2015 12:41:46 -0500
Subject: [squid-users] Unable to get TPROXY working with squid
In-Reply-To: <54EFC89E.4070908@treenet.co.nz>
References: <CAKL8bP5fniDHdBDzdKn_vhafZgKfz8inqwXtCQihx66B0N2BFg@mail.gmail.com>
 <54EFC89E.4070908@treenet.co.nz>
Message-ID: <CAKL8bP4sR41wjAdm9T9XhUKwaJeTbOefuTj7Wfdfii9U+HXD4A@mail.gmail.com>

On Thu, Feb 26, 2015 at 8:30 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 27/02/2015 12:41 p.m., Carvaka Guru wrote:
> > I am building a simple linux firewall router with eth1 LAN port and eth0
> > WAN port. I have squid3 running on it that I have built with netfilter
> > enabled. The linux version running on the firewall is debian wheezy which
> > has iptables with TPROXY and socket support.
> >
> > By setting up the iptables to send traffic to squid3 using the original
> nat
> > prerouting REDIRECT method everything works fine but I can't get the
> TPROXY
> > method to work. I followed all the steps outlined in
> > http://wiki.squid-cache.org/Features/Tproxy4
>
> Uhm... no. You ran a *completely* different command line.
>
>
Errr ... I didn't mention what command line I ran, just that I tried to
follow the instructions from the link, so I don't understand why you would
say that I ran a completely different command line??


>
> > but no traffic gets to squid3.
> > In fact all HTTP traffic goes into some hole as soon as I issue the
> > following two routing commands -
> >
> > ip rule add fwmark 1 lookup 100
> > ip route add local 0.0.0.0/0 dev lo table 100
> >
> > Without these two commands the HTTP traffic goes through but never gets
> > routed to squid3.
> >
> > I think the "ip route" command is the culprit but I don't know why or how
> > to change it?
>
> That is explained in the "/!\" notes directly following the example
> configuration you "followed".
>
> It even has a whole section "Some routing problems to be aware of" just
> to repeat the message about this problem and what to do about it.
>
> <http://wiki.squid-cache.org/Features/Tproxy4#Routing_configuration>
>
>
I had already gone through these sections (which have very scarce info as
it is) and tried to understand the caveats but since you explicitly pointed
it out as something to look into, I thought I'd go through it again and try
a few more things but nothing really panned out.

I admit that I am a noob to this so I am probably missing something
elemental but one thing I am certain of is that I need to change the "ip
route add local" command to something that will work for my setup. Not sure
what that would be because I tried various combinations of parameters for
this command and the result is the same, i.e. I lose web-connectivity as
soon as I issue the command.

Perhaps someone will humor me and explain what the "ip route add local"
command is exactly suppose to achieve in the context of TPROXY then perhaps
I may be able to morph it to fit my setup.


> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150302/4dcc78fa/attachment.htm>

From hack.back at hotmail.com  Mon Mar  2 17:48:08 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 2 Mar 2015 09:48:08 -0800 (PST)
Subject: [squid-users] squid and post method
In-Reply-To: <54F44DDB.5090503@gmail.com>
References: <1425168461331-4670122.post@n4.nabble.com>
 <54F25FE2.8030107@artifact-software.com>
 <1425295460608-4670156.post@n4.nabble.com> <54F44DDB.5090503@gmail.com>
Message-ID: <1425318488993-4670166.post@n4.nabble.com>

Yuri :P Why you are asking 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122p4670166.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Mar  2 18:52:23 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 03 Mar 2015 00:52:23 +0600
Subject: [squid-users] squid and post method
In-Reply-To: <1425318488993-4670166.post@n4.nabble.com>
References: <1425168461331-4670122.post@n4.nabble.com>
 <54F25FE2.8030107@artifact-software.com>
 <1425295460608-4670156.post@n4.nabble.com> <54F44DDB.5090503@gmail.com>
 <1425318488993-4670166.post@n4.nabble.com>
Message-ID: <54F4B167.70506@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

So what else can be interesting in POST..... ;)

02.03.15 23:48, HackXBack ?????:
> Yuri :P Why you are asking
> 
> 
> 
> -- View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-and-post-method-tp4670122p4670166.html
>
> 
Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9LFnAAoJENNXIZxhPexG624IAIg88DsAdEfGDvLx+beWTDRA
1XHlakGe0nn1V0UkbSZzUrkaVcStIoIqqPD2hvE4E/sZ9AFoUYSJ5i8877Kn5cx1
IB7YiEOqrENsVJ1BHRTQPSMptWgcoCrC0qzcH3KRprzPGzfnhBkEMmlFDs9Hfl/I
TIfCyhl+TesEGa/LUseN31kDTzsn2VbO1+E8tX5bXsK+HR7hy6IdZ1CEsRaTVFGl
yKyQtoWqesAfI5E3x2yqP2Bb9EEtfSB++7ZWnc3moptSNwQEQDyMoOWvQszK17pv
JRI+GSAvDOahHhz31kekuYEAnd6whnZGAydzXxi7qB+1dahTZM9zs73PvpCukQY=
=83fs
-----END PGP SIGNATURE-----


From dan at getbusi.com  Tue Mar  3 03:17:59 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 3 Mar 2015 14:17:59 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <54F06091.5010806@treenet.co.nz>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
Message-ID: <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>

Hey Amos

Sorry for taking a while to get back to you with this. I?ve attached the output of the gdb commands you listed.

Not sure if the errors are from the trace or from my print commands themselves?anyway, see attached.

Cheers!

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: connIsUsable-frame3.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150303/126071e7/attachment.txt>
-------------- next part --------------


P.S. I tried to upload the traces to the bug but I get kept getting weird validation errors on my text files.

> On 27 Feb 2015, at 11:18 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 27/02/2015 12:25 p.m., Dan Charlesworth wrote:
>> Alright I got abrtd on board, finally.
>> 
>> Here?s a a backtrace from this morning (bt and bt full versions included 
>> separately):
>> 
> 
> Wonderful.
> 
> Can you get a print from frame 3 please of which of the connIsUsable()
> checks if failing?
> 
> 
> gdb commands:
> frame 3
> print http->getConn()
> print cbdataReferenceValid(http->getConn())
> print Comm::IsConnOpen(http->getConn()->clientConnection)
> 
> Amos
> 


From squid3 at treenet.co.nz  Tue Mar  3 03:27:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Mar 2015 16:27:01 +1300
Subject: [squid-users] Unable to get TPROXY working with squid
In-Reply-To: <CAKL8bP4sR41wjAdm9T9XhUKwaJeTbOefuTj7Wfdfii9U+HXD4A@mail.gmail.com>
References: <CAKL8bP5fniDHdBDzdKn_vhafZgKfz8inqwXtCQihx66B0N2BFg@mail.gmail.com>
 <54EFC89E.4070908@treenet.co.nz>
 <CAKL8bP4sR41wjAdm9T9XhUKwaJeTbOefuTj7Wfdfii9U+HXD4A@mail.gmail.com>
Message-ID: <54F52A05.1010806@treenet.co.nz>

On 3/03/2015 6:41 a.m., Carvaka Guru wrote:
> On Thu, Feb 26, 2015 at 8:30 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 27/02/2015 12:41 p.m., Carvaka Guru wrote:
>>> I am building a simple linux firewall router with eth1 LAN port and eth0
>>> WAN port. I have squid3 running on it that I have built with netfilter
>>> enabled. The linux version running on the firewall is debian wheezy which
>>> has iptables with TPROXY and socket support.
>>>
>>> By setting up the iptables to send traffic to squid3 using the original
>> nat
>>> prerouting REDIRECT method everything works fine but I can't get the
>> TPROXY
>>> method to work. I followed all the steps outlined in
>>> http://wiki.squid-cache.org/Features/Tproxy4
>>
>> Uhm... no. You ran a *completely* different command line.
>>
>>
> Errr ... I didn't mention what command line I ran, just that I tried to
> follow the instructions from the link, so I don't understand why you would
> say that I ran a completely different command line??


You said:
"
as soon as I issue the following two routing commands -

 ip rule add fwmark 1 lookup 100
 ip route add local 0.0.0.0/0 dev lo table 100
"

the tutorial uses two commands for IPv4 and IPv6 routing and indicates
an ethN interface.

> 
> 
>>
>>> but no traffic gets to squid3.
>>> In fact all HTTP traffic goes into some hole as soon as I issue the
>>> following two routing commands -
>>>
>>> ip rule add fwmark 1 lookup 100
>>> ip route add local 0.0.0.0/0 dev lo table 100
>>>
>>> Without these two commands the HTTP traffic goes through but never gets
>>> routed to squid3.
>>>
>>> I think the "ip route" command is the culprit but I don't know why or how
>>> to change it?
>>
>> That is explained in the "/!\" notes directly following the example
>> configuration you "followed".
>>
>> It even has a whole section "Some routing problems to be aware of" just
>> to repeat the message about this problem and what to do about it.
>>
>> <http://wiki.squid-cache.org/Features/Tproxy4#Routing_configuration>
>>
>>
> I had already gone through these sections (which have very scarce info as
> it is) and tried to understand the caveats but since you explicitly pointed
> it out as something to look into, I thought I'd go through it again and try
> a few more things but nothing really panned out.
> 
> I admit that I am a noob to this so I am probably missing something
> elemental but one thing I am certain of is that I need to change the "ip
> route add local" command to something that will work for my setup. Not sure
> what that would be because I tried various combinations of parameters for
> this command and the result is the same, i.e. I lose web-connectivity as
> soon as I issue the command.
> 
> Perhaps someone will humor me and explain what the "ip route add local"
> command is exactly suppose to achieve in the context of TPROXY then perhaps
> I may be able to morph it to fit my setup.

It is the type of route being created. One for traffic within the local
machine as opposed to other types like unicast (Internet) multicast,
boradcast, or blackholes.

The tuneable parameters of those rules AFAIK are:
 the table ID number - which should be unique for the TPROXY usage, and
 the device name - which should be either "lo" or the eth* physical NIC
the traffic arrives on (not an alias) depending on your OS security
setup, and
 the IP-range you want captured - normally everything and the DIVERT
iptables chain takes care of exceptions.

Amos



From travis.skeel at gmail.com  Tue Mar  3 13:30:18 2015
From: travis.skeel at gmail.com (laxcat)
Date: Tue, 3 Mar 2015 05:30:18 -0800 (PST)
Subject: [squid-users] Help with Squid Proxy on AWS Nat Instance.
Message-ID: <1425389418404-4670170.post@n4.nabble.com>

I have squid installed on a NAT instance in AWS.  I installed squid using
yum.  The OS is amazon linux.  When squid is not running I am able to send
traffic through the nat box from private subnets but when I start squid I am
not.  

This is the default iptables rules:

[admin at box1 ~]# iptables -t nat --line-numbers -L
iptables -t nat --line-numbers -L
Chain PREROUTING (policy ACCEPT)
num  target     prot opt source               destination         

Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination         

Chain POSTROUTING (policy ACCEPT)
num  target     prot opt source               destination         
1    MASQUERADE  all  --  10.3.0.0/16          anywhere         


I start squid and add the below rule to iptables I get a squid error page:
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT
--to-port 3128

Error pages says:
ERROR
The requested URL could not be retrieved
The following error encountered while trying to retrieve the URL: /
Invalid URL

Current config I have tried a few different ones.

#
# Recommended minimum configuration:
#
acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
#http_access allow manager localhost
#http_access allow all
acl whitelist dstdomain "/etc/squid/whitelist"
http_access allow whitelist
http_access allow CONNECT whitelist
http_access deny !whitelist 

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-with-Squid-Proxy-on-AWS-Nat-Instance-tp4670170.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From johnzeng2013 at yahoo.com  Tue Mar  3 14:12:19 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Tue, 03 Mar 2015 22:12:19 +0800
Subject: [squid-users] WARNING: disk-cache maximum object size is too large
 for mem-cache: 102400.00 KB > 90.00 KB
Message-ID: <54F5C143.5010405@yahoo.com>


Hi :

Thanks Amos Jeffries, i updated to squid 3.5.2 and

tested QQ and other application at two different environment ( between
intercept and tproxy ) in according to your advisement .

result is ok as your saying .

Thanks again .

but i checked cache log and found ( WARNING: disk-cache maximum object
size is too large for mem-cache: 102400.00 KB > 90.00 KB

Whether there are some error at my config ??


------------------------------------------------------------

this is my config

-------------------------------------------------------------


minimum_object_size 1 KB
maximum_object_size 100 MB
maximum_object_size_in_memory 90 KB

cache_swap_low 80
cache_swap_high 95


#Smp setting

workers 2

cpu_affinity_map process_numbers=1,2 cores=1,2

if ${process_number} = 1

pid_filename /accerater/logs/webcache3/opmizer1/cachea.pid
cache_swap_state /accerater/logs/webcache3/cachea_swap_log1
access_log stdio:/accerater/logs/webcache3/accessa.log squid
cache_log /accerater/logs/webcache3/cachea.log
cache_store_log stdio:/accerater/logs/webcache3/storea.log1
cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number}
10200 16 64 min-size=262145
coredump_dir /accerater/logs/webcache3/opmizer1
unlinkd_program /accerater/webcache3/libexec/unlinkd
unique_hostname maraservice1
snmp_port 3401

endif

if ${process_number} = 2
pid_filename /accerater/logs/webcache3/opmizer2/cacheb.pid
cache_swap_state /accerater/logs/webcache3/cacheb_swap_log
access_log stdio:/accerater/logs/webcache3/accessb.log squid
cache_log /accerater/logs/webcache3/cacheb.log1
cache_store_log stdio:/accerater/logs/webcache3/storeb.log1
cache_dir aufs /accerater/webcache3/storage/aufs2/${process_number}
10200 16 64 min-size=262145
coredump_dir /accerater/logs/webcache3/opmizer2
unlinkd_program /accerater/webcache3/libexec/unlinka
unique_hostname maraservice2
snmp_port 3402


endif



From steve at opendium.com  Tue Mar  3 14:39:13 2015
From: steve at opendium.com (Steve Hill)
Date: Tue, 03 Mar 2015 14:39:13 +0000
Subject: [squid-users] Dual-stack IPv4/IPv6 captive portal
In-Reply-To: <54F3CBF9.7050804@treenet.co.nz>
References: <54F091C5.9090900@opendium.com>
 <1521450889.828153.1425056458279.JavaMail.zimbra@labs.it>
 <54F0AF9B.2000600@opendium.com>
 <1785325384.844859.1425225309295.JavaMail.zimbra@labs.it>
 <54F3CBF9.7050804@treenet.co.nz>
Message-ID: <54F5C791.8060001@opendium.com>

On 02.03.15 02:33, Amos Jeffries wrote:

>   These people are plain wrong about how the basic protocol works and yet
> they are treated with must-accept policies by so many networks.

Yep, one of the really big problems we have is the "it works when we're 
not using the proxy, so the proxy must be broken" attitude, when almost 
universally the proxy is working fine and the other software is just 
plain broken.  It's really hard to convince a customer that it really 
isn't our fault when some app breaks, especially when that app is made 
by someone like Apple or Google (who, of course, can *never* be wrong!)

The vast majority of our support time is spent figuring out ways to work 
around busted end-user software, because we know saying "Apple's 
software is broken, go and talk to Apple" isn't going to work because 
the likes of Apple have no interest in actually supporting their own 
customers and somehow this ends up being "our fault".  (Not just Apple - 
lots of other companies are equally bad, although Apple have currently 
hit a nerve with me due to a lot of debugging I recently had to do with 
their appstore because they didn't bother to log any errors when things 
broke, which also seems to be par for the course these days).

>   Imagine what would happen if you MUST-accept all emails delivered? or
> any kind of DNS response they chose to send you? those are two other
> major protcols with proxies that work just fine by rejecting bad
> messages wholesale.

Well, you say that, but we also get "it works at home but not at work" 
complaints when DNS servers start returning broken data.  Admittedly we 
usually seem to be able to not catch quite so much blame for that one, 
although I'm not sure how. :)

Basically, in my experience, if it works in situation A and not in 
situation B people will assume that the problem is whatever is different 
in situation B rather than that both situations are completely valid but 
their application is broken and can't handle one of them.  This becomes 
a big problem when situation A is the more prevalent one - at that point 
you either start working around the buggy software, or you lose a 
customer and get a reputation for selling "broken" stuff.

So whilst I agree with you that in an ideal world we wouldn't work 
around stuff, we would just report bugs and the broken software would be 
fixed, in the real world the big mainstream businesses aren't interested 
in supporting their customers and yet somehow the rest of us end up 
having to do it for them or it reflects badly on *us*. <boggle>


FWIW, I am always happy to work with other people/companies to help them 
fix their broken stuff.  This has been met with a mix of responses - 
sometimes they are happy to work with me to fix things, which is great, 
but sadly not the most common experience.  Often I send a detailed bug 
report, explaining what's going wrong, referencing standards, etc. and 
get a "you're wrong, we're right, we're not going to change anything" 
response, which would be fine if they referenced anything to back up 
their position, but they never do.  Many simply ignore the reports 
altogether.  Then we have people like Microsoft, who I've tried to 
contact on several occasions to report bugs in their public-facing web 
servers - there are no suitable contact details ever published and I've 
been bounced from department to department with no one quite sure what 
to do with someone reporting problems with their _public_ servers and 
not having some kind of support contract with them (I've got no 
resolution to any of the problems I reported to them because I've never 
actually managed to get my report to anyone responsible).  I've given up 
reporting bugs to Apple because they always demand that I spend a lot of 
my time collecting debug logs, but then they sit on the report and never 
actually fix it (again, I've never had a resolution to a bug I've 
reported to Apple, despite supplying them with extensive debugging).


/rant :)

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From squid3 at treenet.co.nz  Tue Mar  3 15:13:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Mar 2015 04:13:21 +1300
Subject: [squid-users] Help with Squid Proxy on AWS Nat Instance.
In-Reply-To: <1425389418404-4670170.post@n4.nabble.com>
References: <1425389418404-4670170.post@n4.nabble.com>
Message-ID: <54F5CF91.30305@treenet.co.nz>

On 4/03/2015 2:30 a.m., laxcat wrote:
> I have squid installed on a NAT instance in AWS.  I installed squid using
> yum.  The OS is amazon linux.  When squid is not running I am able to send
> traffic through the nat box from private subnets but when I start squid I am
> not.  

Please follow this config example.
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

It contains everything you need to *add* to the normal iptables and
squid.conf to get NAT intercept working

Amos



From yvoinov at gmail.com  Tue Mar  3 15:14:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 03 Mar 2015 21:14:48 +0600
Subject: [squid-users] Help with Squid Proxy on AWS Nat Instance.
In-Reply-To: <1425389418404-4670170.post@n4.nabble.com>
References: <1425389418404-4670170.post@n4.nabble.com>
Message-ID: <54F5CFE8.3050608@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Feel free to use Squid Wiki:

http://wiki.squid-cache.org/ConfigExamples/Intercept


03.03.15 19:30, laxcat ?????:
> I have squid installed on a NAT instance in AWS.  I installed squid
> using yum.  The OS is amazon linux.  When squid is not running I am
> able to send traffic through the nat box from private subnets but
> when I start squid I am not.
> 
> This is the default iptables rules:
> 
> [admin at box1 ~]# iptables -t nat --line-numbers -L iptables -t nat
> --line-numbers -L Chain PREROUTING (policy ACCEPT) num  target
> prot opt source               destination
> 
> Chain INPUT (policy ACCEPT) num  target     prot opt source
> destination
> 
> Chain OUTPUT (policy ACCEPT) num  target     prot opt source
> destination
> 
> Chain POSTROUTING (policy ACCEPT) num  target     prot opt source
> destination 1    MASQUERADE  all  --  10.3.0.0/16          anywhere
> 
> 
> 
> I start squid and add the below rule to iptables I get a squid
> error page: iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80
> -j REDIRECT --to-port 3128
> 
> Error pages says: ERROR The requested URL could not be retrieved 
> The following error encountered while trying to retrieve the URL:
> / Invalid URL
> 
> Current config I have tried a few different ones.
> 
> # # Recommended minimum configuration: # acl manager proto
> cache_object acl localhost src 127.0.0.1/32 ::1 acl to_localhost
> dst 127.0.0.0/8 0.0.0.0/32 ::1
> 
> # Example rule allowing access from your local networks. # Adapt to
> list your (internal) IP networks from where browsing # should be
> allowed acl localnet src 10.0.0.0/8	# RFC1918 possible internal
> network acl localnet src 172.16.0.0/12	# RFC1918 possible internal
> network acl localnet src 192.168.0.0/16	# RFC1918 possible internal
> network acl localnet src fc00::/7       # RFC 4193 local private
> network range acl localnet src fe80::/10      # RFC 4291 link-local
> (directly plugged) machines
> 
> acl SSL_ports port 443 acl Safe_ports port 80		# http acl
> Safe_ports port 21		# ftp acl Safe_ports port 443		# https acl
> Safe_ports port 70		# gopher acl Safe_ports port 210		# wais acl
> Safe_ports port 1025-65535	# unregistered ports acl Safe_ports port
> 280		# http-mgmt acl Safe_ports port 488		# gss-http acl Safe_ports
> port 591		# filemaker acl Safe_ports port 777		# multiling http acl
> CONNECT method CONNECT
> 
> # # Recommended minimum Access Permission configuration: # # Only
> allow cachemgr access from localhost #http_access allow manager
> localhost #http_access allow all acl whitelist dstdomain
> "/etc/squid/whitelist" http_access allow whitelist http_access
> allow CONNECT whitelist http_access deny !whitelist
> 
> # Deny requests to certain unsafe ports http_access deny
> !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports http_access deny
> CONNECT !SSL_ports
> 
> # We strongly recommend the following be uncommented to protect
> innocent # web applications running on the proxy server who think
> the only # one who can access services on "localhost" is a local
> user #http_access deny to_localhost
> 
> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
> #
> 
> # Example rule allowing access from your local networks. # Adapt
> localnet in the ACL section to list your (internal) IP networks #
> from where browsing should be allowed http_access allow localnet 
> http_access allow localhost
> 
> # And finally deny all other access to this proxy http_access deny
> all
> 
> # Squid normally listens to port 3128 http_port 3128
> 
> # We recommend you to use at least the following line. 
> hierarchy_stoplist cgi-bin ?
> 
> # Uncomment and adjust the following to add a disk cache
> directory. #cache_dir ufs /var/spool/squid 100 16 256
> 
> # Leave coredumps in the first cache dir coredump_dir
> /var/spool/squid
> 
> # Add any of your own refresh_pattern entries above these. 
> refresh_pattern ^ftp:		1440	20%	10080 refresh_pattern ^gopher:	1440
> 0%	1440 refresh_pattern -i (/cgi-bin/|\?) 0	0%	0 refresh_pattern .
> 0	20%	4320
> 
> 
> 
> 
> 
> -- View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Help-with-Squid-Proxy-on-AWS-Nat-Instance-tp4670170.html
>
> 
Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9c/oAAoJENNXIZxhPexGm2IIAKJJt3hxdfzOHsUNt8y126gH
xIbwxDvl2DOfVxSRFqHOWRYEO/72mGPU97sQJaktbs1FTo/pU1gf1zFvNNGo8E7/
+N5xyNJ5KSs0a8SH3elS6YIqsfQ9StWBTCY8ft2B0lsM2/HJakpurOf0c455D8VG
bRHH2vIH+I9iWa2CijfZoIgX2bDieUmn26yFof/8rbjbSf8OBzoPaxOs5dUy8Yme
7uWQARVt3BoH4d1k992pyqcNobzB3t45fRUImIvzHcLBMIywJMcP9M/hPAwnFLex
nWKXEO20M2qV9jp1iTG7RNXou8JN2vZbJGKkeAYVD7yIucxUM3nP5nDBf5fc+Eg=
=psFg
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Tue Mar  3 15:36:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Mar 2015 04:36:20 +1300
Subject: [squid-users] WARNING: disk-cache maximum object size is too
 large for mem-cache: 102400.00 KB > 90.00 KB
In-Reply-To: <54F5C143.5010405@yahoo.com>
References: <54F5C143.5010405@yahoo.com>
Message-ID: <54F5D4F4.7010309@treenet.co.nz>

On 4/03/2015 3:12 a.m., johnzeng wrote:
> 
> Hi :
> 
> Thanks Amos Jeffries, i updated to squid 3.5.2 and
> 
> tested QQ and other application at two different environment ( between
> intercept and tproxy ) in according to your advisement .
> 
> result is ok as your saying .
> 
> Thanks again .
> 
> but i checked cache log and found ( WARNING: disk-cache maximum object
> size is too large for mem-cache: 102400.00 KB > 90.00 KB
> 
> Whether there are some error at my config ??
> 

I'm not sure exactly why that is being warned about. Its reasonable to
have a smaller object size for memory-only objects. I think not to worry
about it.


The size ranges in your config look a little strange though.

* objects between 1KB and 90KB are allowed to be stored only in memory

* objects between 262KB and 100MB are allowed to be stored only on disk

* objects under 1KB, between 90KB and 262KB, or over 100MB are not
allowed to be stored anywhere at all.



Also, how is the multiple PID file setup working out?
 The PID file is supposed to contain the ID number of the master process
in charge of the whole process tree and managing signals received from
the init system and "squid -k" commands.

Amos

> 
> ------------------------------------------------------------
> 
> this is my config
> 
> -------------------------------------------------------------
> 
> 
> minimum_object_size 1 KB
> maximum_object_size 100 MB
> maximum_object_size_in_memory 90 KB
> 
> cache_swap_low 80
> cache_swap_high 95
> 
> 
> #Smp setting
> 
> workers 2
> 
> cpu_affinity_map process_numbers=1,2 cores=1,2
> 
> if ${process_number} = 1
> 
> pid_filename /accerater/logs/webcache3/opmizer1/cachea.pid
> cache_swap_state /accerater/logs/webcache3/cachea_swap_log1
> access_log stdio:/accerater/logs/webcache3/accessa.log squid
> cache_log /accerater/logs/webcache3/cachea.log
> cache_store_log stdio:/accerater/logs/webcache3/storea.log1
> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number}
> 10200 16 64 min-size=262145
> coredump_dir /accerater/logs/webcache3/opmizer1
> unlinkd_program /accerater/webcache3/libexec/unlinkd
> unique_hostname maraservice1
> snmp_port 3401
> 
> endif
> 
> if ${process_number} = 2
> pid_filename /accerater/logs/webcache3/opmizer2/cacheb.pid
> cache_swap_state /accerater/logs/webcache3/cacheb_swap_log
> access_log stdio:/accerater/logs/webcache3/accessb.log squid
> cache_log /accerater/logs/webcache3/cacheb.log1
> cache_store_log stdio:/accerater/logs/webcache3/storeb.log1
> cache_dir aufs /accerater/webcache3/storage/aufs2/${process_number}
> 10200 16 64 min-size=262145
> coredump_dir /accerater/logs/webcache3/opmizer2
> unlinkd_program /accerater/webcache3/libexec/unlinka
> unique_hostname maraservice2
> snmp_port 3402
> 
> 
> endif
> 



From guy.helmer at gmail.com  Tue Mar  3 17:04:49 2015
From: guy.helmer at gmail.com (Guy Helmer)
Date: Tue, 3 Mar 2015 11:04:49 -0600
Subject: [squid-users] wccp2_service_info fails on more than one port number
Message-ID: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>

This used to work in 3.3.x:

wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source priority=240 ports=80,81,83,591,8008,8080,443

squid 3.4.12 fails:
2015/03/03 11:02:33.109| cache_cf.cc(556) parseOneConfigFile: Processing: wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source priority=240 ports=80,81,83,591,8008,8080,443
2015/03/03 11:02:33.109| wccp2.cc(2298) parse_wccp2_service_info: parse_wccp2_service_info: called
2015/03/03 11:02:33.109| ERROR: Invalid value: '80,81,83,591,8008,8080,443' is supposed to be a number.

Any help?

Thanks,
Guy

From yvoinov at gmail.com  Tue Mar  3 17:06:45 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 03 Mar 2015 23:06:45 +0600
Subject: [squid-users] wccp2_service_info fails on more than one port
 number
In-Reply-To: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
Message-ID: <54F5EA25.6080901@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

You cannot combine HTTP and HTTPS in one WCCP service.

http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2


03.03.15 23:04, Guy Helmer ?????:
> This used to work in 3.3.x:
> 
> wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source
> priority=240 ports=80,81,83,591,8008,8080,443
> 
> squid 3.4.12 fails: 2015/03/03 11:02:33.109| cache_cf.cc(556)
> parseOneConfigFile: Processing: wccp2_service_info 94 protocol=tcp
> flags=dst_ip_hash,ports_source priority=240
> ports=80,81,83,591,8008,8080,443 2015/03/03 11:02:33.109|
> wccp2.cc(2298) parse_wccp2_service_info: parse_wccp2_service_info:
> called 2015/03/03 11:02:33.109| ERROR: Invalid value:
> '80,81,83,591,8008,8080,443' is supposed to be a number.
> 
> Any help?
> 
> Thanks, Guy _______________________________________________ 
> squid-users mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9eolAAoJENNXIZxhPexGns0IAKgpDAS7pFxCArCUURUOOt5M
sORC/zvy9VpJjuPlMsnhXuT0vOx8MxgzH/c8kDbWKZNzvvG/NUuhGrPhBzN483x1
uuWB+sTP3dMmPIQobDGnJ1fwxwcoQVIdYyI3BSf5OtL0sE3Lj8O9eeOUbsmNey2M
vZ8uQ+CX8XivCHzS1VUGb3z5gFYTPdYiP23lR5Ltl5d/6UHRz6HhNfwFzJWjhY95
14gasdVMbBjDziTc5QyJbbRvbhOwVgUAHO/FaJ8WChgmQsPmgLFP+pOM5GQ8bLaG
ZCBs42Dvw6ZsFAuuZo5K++FVyzA3uF91Ec8wLUM+SZT7dvJTeSKeVG6ymQ4UygI=
=OXyP
-----END PGP SIGNATURE-----


From sp.kgm at hotmail.com  Tue Mar  3 17:15:58 2015
From: sp.kgm at hotmail.com (siva prakash)
Date: Tue, 3 Mar 2015 17:15:58 +0000
Subject: [squid-users] Can single squid server handle 1Gbps traffic?
Message-ID: <SNT152-W125B916523317FBB8EF9AB83110@phx.gbl>

Hi,
I would like to know whether a single squid server can handle 1Gbps traffic?
Consider I have hardware configuration of 64 GB RAM, 12 Core processor and 10 GB NIC. Is it possible?
- SP 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150303/e2d78276/attachment.htm>

From yvoinov at gmail.com  Tue Mar  3 17:18:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 03 Mar 2015 23:18:40 +0600
Subject: [squid-users] Can single squid server handle 1Gbps traffic?
In-Reply-To: <SNT152-W125B916523317FBB8EF9AB83110@phx.gbl>
References: <SNT152-W125B916523317FBB8EF9AB83110@phx.gbl>
Message-ID: <54F5ECF0.4080708@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

There is no slow servers. There are slow brains system administrators.

IMO - with an excess of.

03.03.15 23:15, siva prakash ?????:
> Hi, I would like to know whether a single squid server can handle
> 1Gbps traffic? Consider I have hardware configuration of 64 GB RAM,
> 12 Core processor and 10 GB NIC. Is it possible? - SP
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9ezwAAoJENNXIZxhPexG1qEIAI1A77VfAbnM7T/e2MRP2SzD
H+PYYmiFC7F1mJXwSxV9lX+mdmPM1XD5haQgKhN5MTNoXN6ZrP/Vm6hgopX0MX8E
cszgATClPzca3LT98giQUTNEuMHd95gpbqHDlqgfsKBjqVnWss83UUTwlm1mOPhn
0qFrprOmMsYnk92DyRyCAxBwfoE/Kigoj1JttauTLLulDFY8tYZ/ATOyeCIw3KE6
TdaqqwImECmigXiHSEz+uFGR13DcwXiZaic5dkkcKfckAruxioQdTkRDGX+5sSzZ
18GDa5STz/7JA2OPK9L+e0KYLzJZMfGv3UIrkxxaSD/fCZWvQE+Cx/6g7ZZq/Cs=
=M+E7
-----END PGP SIGNATURE-----


From guy.helmer at gmail.com  Tue Mar  3 17:21:43 2015
From: guy.helmer at gmail.com (Guy Helmer)
Date: Tue, 3 Mar 2015 11:21:43 -0600
Subject: [squid-users] wccp2_service_info fails on more than one port
	number
In-Reply-To: <54F5EA25.6080901@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com>
Message-ID: <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>

It has worked in the past with WCCP2 dynamic services at multiple sites.

I?ve uncovered the wccp2_service_info ports parsing error:

--- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000 -0600
+++ src/wccp2.cc	2015-03-03 11:10:37.000000000 -0600
@@ -2264,7 +2264,10 @@
         if (i >= WCCP2_NUMPORTS) {
             fatalf("parse_wccp2_service_ports: too many ports (maximum: 8) in list '%s'\n", options);
         }
-        int p = xatoi(tmp);
+        char copy[len + 1];
+        memcpy(copy, tmp, len);
+        copy[len] = '\0';
+        int p = xatoi(copy);
 
         if (p < 1 || p > 65535) {
             fatalf("parse_wccp2_service_ports: port value '%s' isn't valid (1..65535)\n", tmp);


> On Mar 3, 2015, at 11:06 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> You cannot combine HTTP and HTTPS in one WCCP service.
> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
> 
> 
> 03.03.15 23:04, Guy Helmer ?????:
>> This used to work in 3.3.x:
>> 
>> wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source
>> priority=240 ports=80,81,83,591,8008,8080,443
>> 
>> squid 3.4.12 fails: 2015/03/03 11:02:33.109| cache_cf.cc(556)
>> parseOneConfigFile: Processing: wccp2_service_info 94 protocol=tcp
>> flags=dst_ip_hash,ports_source priority=240
>> ports=80,81,83,591,8008,8080,443 2015/03/03 11:02:33.109|
>> wccp2.cc(2298) parse_wccp2_service_info: parse_wccp2_service_info:
>> called 2015/03/03 11:02:33.109| ERROR: Invalid value:
>> '80,81,83,591,8008,8080,443' is supposed to be a number.
>> 
>> Any help?
>> 
>> Thanks, Guy _______________________________________________ 
>> squid-users mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
> 
> iQEcBAEBAgAGBQJU9eolAAoJENNXIZxhPexGns0IAKgpDAS7pFxCArCUURUOOt5M
> sORC/zvy9VpJjuPlMsnhXuT0vOx8MxgzH/c8kDbWKZNzvvG/NUuhGrPhBzN483x1
> uuWB+sTP3dMmPIQobDGnJ1fwxwcoQVIdYyI3BSf5OtL0sE3Lj8O9eeOUbsmNey2M
> vZ8uQ+CX8XivCHzS1VUGb3z5gFYTPdYiP23lR5Ltl5d/6UHRz6HhNfwFzJWjhY95
> 14gasdVMbBjDziTc5QyJbbRvbhOwVgUAHO/FaJ8WChgmQsPmgLFP+pOM5GQ8bLaG
> ZCBs42Dvw6ZsFAuuZo5K++FVyzA3uF91Ec8wLUM+SZT7dvJTeSKeVG6ymQ4UygI=
> =OXyP
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Tue Mar  3 17:29:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 03 Mar 2015 23:29:54 +0600
Subject: [squid-users] wccp2_service_info fails on more than one port
 number
In-Reply-To: <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com> <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
Message-ID: <54F5EF92.80407@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Please, read Cisco iOS WCCPv2 manual first.

This one:

http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html

and this one:

http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955

wccp web-cache uses port 80 by default. Other http ports is less than
percentile on web-traffic.

https requires another dynamic wccp service.

Also, take a look on config example again.

This is working configuration.

03.03.15 23:21, Guy Helmer ?????:
> It has worked in the past with WCCP2 dynamic services at multiple
> sites.
> 
> I?ve uncovered the wccp2_service_info ports parsing error:
> 
> --- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000 -0600 +++
> src/wccp2.cc	2015-03-03 11:10:37.000000000 -0600 @@ -2264,7
> +2264,10 @@ if (i >= WCCP2_NUMPORTS) { 
> fatalf("parse_wccp2_service_ports: too many ports (maximum: 8) in
> list '%s'\n", options); } -        int p = xatoi(tmp); +
> char copy[len + 1]; +        memcpy(copy, tmp, len); +
> copy[len] = '\0'; +        int p = xatoi(copy);
> 
> if (p < 1 || p > 65535) { fatalf("parse_wccp2_service_ports: port
> value '%s' isn't valid (1..65535)\n", tmp);
> 
> 
>> On Mar 3, 2015, at 11:06 AM, Yuri Voinov <yvoinov at gmail.com>
>> wrote:
>> 
> You cannot combine HTTP and HTTPS in one WCCP service.
> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>
> 
> 
> 03.03.15 23:04, Guy Helmer ?????:
>>>> This used to work in 3.3.x:
>>>> 
>>>> wccp2_service_info 94 protocol=tcp
>>>> flags=dst_ip_hash,ports_source priority=240
>>>> ports=80,81,83,591,8008,8080,443
>>>> 
>>>> squid 3.4.12 fails: 2015/03/03 11:02:33.109|
>>>> cache_cf.cc(556) parseOneConfigFile: Processing:
>>>> wccp2_service_info 94 protocol=tcp 
>>>> flags=dst_ip_hash,ports_source priority=240 
>>>> ports=80,81,83,591,8008,8080,443 2015/03/03 11:02:33.109| 
>>>> wccp2.cc(2298) parse_wccp2_service_info:
>>>> parse_wccp2_service_info: called 2015/03/03 11:02:33.109|
>>>> ERROR: Invalid value: '80,81,83,591,8008,8080,443' is
>>>> supposed to be a number.
>>>> 
>>>> Any help?
>>>> 
>>>> Thanks, Guy _______________________________________________ 
>>>> squid-users mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9e+SAAoJENNXIZxhPexGgN8H/iu2SiLdNehFyrVlK/1X2J4X
qa0cZMoJBYcmQYY2VhjaIGVSdO1RfHt1RP5RSEFgkAkdSH10zCSK6KrzK4eh6jrU
Fuze0mvACuQsi0CfwEQ2KO9gREeOpP7+QcqZ304zEdw2fFDT4YEDRBKhiYXtAYtK
zR4oiNAlut5TWhwjHkmbvTADLWjp64gP0FggVEYIPvIBGYQqSKjE+JVRruiCSTtW
TrbXjcwGNX7SA2gN3pl7pO+QcCwLdErzXnzc3E3ExCKW9uTB3VLKkK4BbK7uRBQX
iKUsB/G9Ps8nAt08xK/puKL0fXJ1t//N8BXSrSkztTODQ/ckTBn772Dr2LJNv7Q=
=j/dZ
-----END PGP SIGNATURE-----


From fredbmail at free.fr  Tue Mar  3 17:44:55 2015
From: fredbmail at free.fr (FredB)
Date: Tue, 3 Mar 2015 18:44:55 +0100 (CET)
Subject: [squid-users] Can single squid server handle 1Gbps traffic?
In-Reply-To: <SNT152-W125B916523317FBB8EF9AB83110@phx.gbl>
Message-ID: <1108080049.394590923.1425404695132.JavaMail.root@zimbra4-e1.priv.proxad.net>




> I would like to know whether a single squid server can handle 1Gbps
> traffic?
> 
> 
> Consider I have hardware configuration of 64 GB RAM, 12 Core
> processor and 10 GB NIC. Is it possible?
> 
> 

Depends on what the users are doing, there is a big difference between 

A) One user is downloading an ISO file = 1 Gbps (max bandwidth)
B) 10 000 users are downloading many different ISO files = 1 Gbps (max bandwidth) 

12 Cores are only useful with workers 

So a single server can handle 1 Gbps yes, but ...

----

Regards,

Fred

http://numsys.eu
http://e2guardian.org


From guy.helmer at gmail.com  Tue Mar  3 17:57:47 2015
From: guy.helmer at gmail.com (Guy Helmer)
Date: Tue, 3 Mar 2015 11:57:47 -0600
Subject: [squid-users] wccp2_service_info fails on more than one port
	number
In-Reply-To: <54F5EF92.80407@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com> <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
 <54F5EF92.80407@gmail.com>
Message-ID: <2DCF7238-EAAC-452B-B76D-4F355743ADFE@gmail.com>

Thanks Yuri -- I have thoroughly read Cisco IOS configuration manuals pertaining to WCCP2. From what I have read, there is no strict requirement for separate configurations for standard web-cache port 80 and dynamic service for non-port 80 ? wccp2 dynamic services allow redirection of any ports (up to a total of 8), including port 80. As I?ve stated before, this was a working configuration in squid 3.3 at multiple sites. If there is a rationale for distinct wccp2 service configurations for port 80 vs other ports, I sure could use a reference that explains it.

As I?ve pointed out, there is a bug in Squid 3.4 that prevents specifying multiple TCP ports in the wccp2_service_info line. I?ve corrected that now in my sources. After fixing that bug, squid 3.4.12 is functioning with WCCP2 interception for port 80, 443, and others as it did in version 3.3.x.

Regards,
Guy

> On Mar 3, 2015, at 11:29 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Please, read Cisco iOS WCCPv2 manual first.
> 
> This one:
> 
> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>
> 
> and this one:
> 
> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955 <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>
> 
> wccp web-cache uses port 80 by default. Other http ports is less than
> percentile on web-traffic.
> 
> https requires another dynamic wccp service.
> 
> Also, take a look on config example again.
> 
> This is working configuration.
> 
> 03.03.15 23:21, Guy Helmer ?????:
>> It has worked in the past with WCCP2 dynamic services at multiple
>> sites.
>> 
>> I?ve uncovered the wccp2_service_info ports parsing error:
>> 
>> --- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000 -0600 +++
>> src/wccp2.cc	2015-03-03 11:10:37.000000000 -0600 @@ -2264,7
>> +2264,10 @@ if (i >= WCCP2_NUMPORTS) { 
>> fatalf("parse_wccp2_service_ports: too many ports (maximum: 8) in
>> list '%s'\n", options); } -        int p = xatoi(tmp); +
>> char copy[len + 1]; +        memcpy(copy, tmp, len); +
>> copy[len] = '\0'; +        int p = xatoi(copy);
>> 
>> if (p < 1 || p > 65535) { fatalf("parse_wccp2_service_ports: port
>> value '%s' isn't valid (1..65535)\n", tmp);
>> 
>> 
>>> On Mar 3, 2015, at 11:06 AM, Yuri Voinov <yvoinov at gmail.com>
>>> wrote:
>>> 
>> You cannot combine HTTP and HTTPS in one WCCP service.
>> 
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2 <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>
>> 
>> 
>> 
>> 03.03.15 23:04, Guy Helmer ?????:
>>>>> This used to work in 3.3.x:
>>>>> 
>>>>> wccp2_service_info 94 protocol=tcp
>>>>> flags=dst_ip_hash,ports_source priority=240
>>>>> ports=80,81,83,591,8008,8080,443
>>>>> 
>>>>> squid 3.4.12 fails: 2015/03/03 11:02:33.109|
>>>>> cache_cf.cc(556) parseOneConfigFile: Processing:
>>>>> wccp2_service_info 94 protocol=tcp 
>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>> ports=80,81,83,591,8008,8080,443 2015/03/03 11:02:33.109| 
>>>>> wccp2.cc <http://wccp2.cc/>(2298) parse_wccp2_service_info:
>>>>> parse_wccp2_service_info: called 2015/03/03 11:02:33.109|
>>>>> ERROR: Invalid value: '80,81,83,591,8008,8080,443' is
>>>>> supposed to be a number.
>>>>> 
>>>>> Any help?
>>>>> 
>>>>> Thanks, Guy _______________________________________________ 
>>>>> squid-users mailing list squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
>>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>>>>> 
>>> _______________________________________________ squid-users
>>> mailing list squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
> 
> iQEcBAEBAgAGBQJU9e+SAAoJENNXIZxhPexGgN8H/iu2SiLdNehFyrVlK/1X2J4X
> qa0cZMoJBYcmQYY2VhjaIGVSdO1RfHt1RP5RSEFgkAkdSH10zCSK6KrzK4eh6jrU
> Fuze0mvACuQsi0CfwEQ2KO9gREeOpP7+QcqZ304zEdw2fFDT4YEDRBKhiYXtAYtK
> zR4oiNAlut5TWhwjHkmbvTADLWjp64gP0FggVEYIPvIBGYQqSKjE+JVRruiCSTtW
> TrbXjcwGNX7SA2gN3pl7pO+QcCwLdErzXnzc3E3ExCKW9uTB3VLKkK4BbK7uRBQX
> iKUsB/G9Ps8nAt08xK/puKL0fXJ1t//N8BXSrSkztTODQ/ckTBn772Dr2LJNv7Q=
> =j/dZ
> -----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150303/12c48679/attachment.htm>

From yvoinov at gmail.com  Tue Mar  3 18:15:41 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 04 Mar 2015 00:15:41 +0600
Subject: [squid-users] wccp2_service_info fails on more than one port
 number
In-Reply-To: <2DCF7238-EAAC-452B-B76D-4F355743ADFE@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com> <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
 <54F5EF92.80407@gmail.com> <2DCF7238-EAAC-452B-B76D-4F355743ADFE@gmail.com>
Message-ID: <54F5FA4D.7020704@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Don't think this is bug.

You got very custom configuration, which can not be common.

As I said, HTTP ports other than 80 is very rarely used in WAN's. And
be better to separate HTTPS port from HTTP.

Modern iOS can not accept your configuration. Beware.

03.03.15 23:57, Guy Helmer ?????:
> Thanks Yuri -- I have thoroughly read Cisco IOS configuration
> manuals pertaining to WCCP2. From what I have read, there is no
> strict requirement for separate configurations for standard
> web-cache port 80 and dynamic service for non-port 80 ? wccp2
> dynamic services allow redirection of any ports (up to a total of
> 8), including port 80. As I?ve stated before, this was a working
> configuration in squid 3.3 at multiple sites. If there is a
> rationale for distinct wccp2 service configurations for port 80 vs
> other ports, I sure could use a reference that explains it.
> 
> As I?ve pointed out, there is a bug in Squid 3.4 that prevents
> specifying multiple TCP ports in the wccp2_service_info line. I?ve
> corrected that now in my sources. After fixing that bug, squid
> 3.4.12 is functioning with WCCP2 interception for port 80, 443, and
> others as it did in version 3.3.x.
> 
> Regards, Guy
> 
>> On Mar 3, 2015, at 11:29 AM, Yuri Voinov <yvoinov at gmail.com>
>> wrote:
>> 
> Please, read Cisco iOS WCCPv2 manual first.
> 
> This one:
> 
> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html
> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>
>
>  and this one:
> 
> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955
> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>
>
>  wccp web-cache uses port 80 by default. Other http ports is less
> than percentile on web-traffic.
> 
> https requires another dynamic wccp service.
> 
> Also, take a look on config example again.
> 
> This is working configuration.
> 
> 03.03.15 23:21, Guy Helmer ?????:
>>>> It has worked in the past with WCCP2 dynamic services at
>>>> multiple sites.
>>>> 
>>>> I?ve uncovered the wccp2_service_info ports parsing error:
>>>> 
>>>> --- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000 -0600
>>>> +++ src/wccp2.cc	2015-03-03 11:10:37.000000000 -0600 @@
>>>> -2264,7 +2264,10 @@ if (i >= WCCP2_NUMPORTS) { 
>>>> fatalf("parse_wccp2_service_ports: too many ports (maximum:
>>>> 8) in list '%s'\n", options); } -        int p = xatoi(tmp);
>>>> + char copy[len + 1]; +        memcpy(copy, tmp, len); + 
>>>> copy[len] = '\0'; +        int p = xatoi(copy);
>>>> 
>>>> if (p < 1 || p > 65535) { fatalf("parse_wccp2_service_ports:
>>>> port value '%s' isn't valid (1..65535)\n", tmp);
>>>> 
>>>> 
>>>>> On Mar 3, 2015, at 11:06 AM, Yuri Voinov
>>>>> <yvoinov at gmail.com> wrote:
>>>>> 
>>>> You cannot combine HTTP and HTTPS in one WCCP service.
>>>> 
>>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>>>> <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>
>>>>
>>>>
>>>>
>>>>
>>>> 
03.03.15 23:04, Guy Helmer ?????:
>>>>>>> This used to work in 3.3.x:
>>>>>>> 
>>>>>>> wccp2_service_info 94 protocol=tcp 
>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>> ports=80,81,83,591,8008,8080,443
>>>>>>> 
>>>>>>> squid 3.4.12 fails: 2015/03/03 11:02:33.109| 
>>>>>>> cache_cf.cc(556) parseOneConfigFile: Processing: 
>>>>>>> wccp2_service_info 94 protocol=tcp 
>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>> ports=80,81,83,591,8008,8080,443 2015/03/03
>>>>>>> 11:02:33.109| wccp2.cc <http://wccp2.cc/>(2298)
>>>>>>> parse_wccp2_service_info: parse_wccp2_service_info:
>>>>>>> called 2015/03/03 11:02:33.109| ERROR: Invalid value:
>>>>>>> '80,81,83,591,8008,8080,443' is supposed to be a
>>>>>>> number.
>>>>>>> 
>>>>>>> Any help?
>>>>>>> 
>>>>>>> Thanks, Guy
>>>>>>> _______________________________________________ 
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> <mailto:squid-users at lists.squid-cache.org> 
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>> 
>>>>> _______________________________________________
>>>>> squid-users mailing list squid-users at lists.squid-cache.org
>>>>> <mailto:squid-users at lists.squid-cache.org> 
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>> <http://lists.squid-cache.org/listinfo/squid-users>
>>>> 
> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9fpNAAoJENNXIZxhPexG+hoH/33NudcBRvS5Z/D/8wCN8Hwq
4+efhgF3dCfimb2kQjQFq3JsK6JvPUJO7TJm2h8tzqlAMOnBUuoM/MHEg48skcL2
+udKwPUC/WO0GXVlNFdln3a4ozYotDmpwZIvwzBTPm5dpcwMi/bCX2oOyzq4y8yr
mHzYEYmaP1tKF4bzEqlRsOIkrKUFvTvw6HlMoZ1EDE3Qp6WlM6WWiaf8rOoMKVRn
dGBPvmvRo79OEMXkvc167BH2j8phOFs7XPUw9mWw7nY93jKEcBxxKl4PpwkK7km/
am7DeV7uLQtnA5nuPs4QC063YRBEAu/8mjONRq5ytJurP8nUUsV46+sdl0EMY44=
=S5O/
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Tue Mar  3 18:20:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 04 Mar 2015 00:20:22 +0600
Subject: [squid-users] Can single squid server handle 1Gbps traffic?
In-Reply-To: <1108080049.394590923.1425404695132.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1108080049.394590923.1425404695132.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <54F5FB66.2070902@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Fred,

don't think so.

This is big enough box.

And files above 1 Gb is not downloaded all time.

Main question - what loads planned?

Medium office generates this throughtput easily, and this box
processed it all without warms up.

Medium ISP is another deal. And must have overcome capacity.

Also, performance tuning exists yet....

03.03.15 23:44, FredB ?????:
> 
> 
> 
>> I would like to know whether a single squid server can handle
>> 1Gbps traffic?
>> 
>> 
>> Consider I have hardware configuration of 64 GB RAM, 12 Core 
>> processor and 10 GB NIC. Is it possible?
>> 
>> 
> 
> Depends on what the users are doing, there is a big difference
> between
> 
> A) One user is downloading an ISO file = 1 Gbps (max bandwidth) B)
> 10 000 users are downloading many different ISO files = 1 Gbps (max
> bandwidth)
> 
> 12 Cores are only useful with workers
> 
> So a single server can handle 1 Gbps yes, but ...
> 
> ----
> 
> Regards,
> 
> Fred
> 
> http://numsys.eu http://e2guardian.org 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9ftmAAoJENNXIZxhPexGZXYH/Rf9fZKibcVx9f4OpQ/XPlZ5
5uqDy+hzGV6+YOmtpP3p8mmyzj3LHf9B0HjhuMVZR003UjMXbw602UGbX07iMbwK
oq7d9fD0gsdZbcVeS767XRiX1j7TSz5haoAs19ES9vrrQzPPPU1JVFAySA18EmXh
bvv1lnYKD3lvK8787+1/YUmttJeC+ezLTYrPstsvsfTKrmNAiry3zCUbBiIIUg6K
vKd0ZzZFPh1QLtAbiViU4wLg1zw9ZB9cTf2iAeMVO7UScO49wOHfTFmpfJrNzUfA
i4MoOMy8dm0PoExof9q+AFqn8nDytG8g9Aichy1M9xf4xLvMFVArd+8qnUrXoPE=
=puSH
-----END PGP SIGNATURE-----


From guy.helmer at gmail.com  Tue Mar  3 18:46:01 2015
From: guy.helmer at gmail.com (Guy Helmer)
Date: Tue, 3 Mar 2015 12:46:01 -0600
Subject: [squid-users] wccp2_service_info fails on more than one port
	number
In-Reply-To: <54F5FA4D.7020704@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com> <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
 <54F5EF92.80407@gmail.com> <2DCF7238-EAAC-452B-B76D-4F355743ADFE@gmail.com>
 <54F5FA4D.7020704@gmail.com>
Message-ID: <FB89CE99-232A-4100-AC8B-99B387A73254@gmail.com>

Disregarding the complaints about Cisco configuration, here is a simple example: If I want to intercept plain HTTP on ports 8008 and 8080:

wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source priority=240 ports=8008,8080

Squid 3.4.12 will still give this incorrect error:

FATAL: Bungled /usr/local/etc/squid/squid.conf line 55: wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source priority=240 ports=8008,8080

WCCP2 is documented as allowing up to 8 ports per service. This is broken by parsing changes in Squid 3.4.

Regards,
Guy

> On Mar 3, 2015, at 12:15 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Don't think this is bug.
> 
> You got very custom configuration, which can not be common.
> 
> As I said, HTTP ports other than 80 is very rarely used in WAN's. And
> be better to separate HTTPS port from HTTP.
> 
> Modern iOS can not accept your configuration. Beware.
> 
> 03.03.15 23:57, Guy Helmer ?????:
>> Thanks Yuri -- I have thoroughly read Cisco IOS configuration
>> manuals pertaining to WCCP2. From what I have read, there is no
>> strict requirement for separate configurations for standard
>> web-cache port 80 and dynamic service for non-port 80 ? wccp2
>> dynamic services allow redirection of any ports (up to a total of
>> 8), including port 80. As I?ve stated before, this was a working
>> configuration in squid 3.3 at multiple sites. If there is a
>> rationale for distinct wccp2 service configurations for port 80 vs
>> other ports, I sure could use a reference that explains it.
>> 
>> As I?ve pointed out, there is a bug in Squid 3.4 that prevents
>> specifying multiple TCP ports in the wccp2_service_info line. I?ve
>> corrected that now in my sources. After fixing that bug, squid
>> 3.4.12 is functioning with WCCP2 interception for port 80, 443, and
>> others as it did in version 3.3.x.
>> 
>> Regards, Guy
>> 
>>> On Mar 3, 2015, at 11:29 AM, Yuri Voinov <yvoinov at gmail.com>
>>> wrote:
>>> 
>> Please, read Cisco iOS WCCPv2 manual first.
>> 
>> This one:
>> 
>> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>
>> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>>
>> 
>> and this one:
>> 
>> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955 <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>
>> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955 <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>>
>> 
>> wccp web-cache uses port 80 by default. Other http ports is less
>> than percentile on web-traffic.
>> 
>> https requires another dynamic wccp service.
>> 
>> Also, take a look on config example again.
>> 
>> This is working configuration.
>> 
>> 03.03.15 23:21, Guy Helmer ?????:
>>>>> It has worked in the past with WCCP2 dynamic services at
>>>>> multiple sites.
>>>>> 
>>>>> I?ve uncovered the wccp2_service_info ports parsing error:
>>>>> 
>>>>> --- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000 -0600
>>>>> +++ src/wccp2.cc <http://wccp2.cc/>	2015-03-03 11:10:37.000000000 -0600 @@
>>>>> -2264,7 +2264,10 @@ if (i >= WCCP2_NUMPORTS) { 
>>>>> fatalf("parse_wccp2_service_ports: too many ports (maximum:
>>>>> 8) in list '%s'\n", options); } -        int p = xatoi(tmp);
>>>>> + char copy[len + 1]; +        memcpy(copy, tmp, len); + 
>>>>> copy[len] = '\0'; +        int p = xatoi(copy);
>>>>> 
>>>>> if (p < 1 || p > 65535) { fatalf("parse_wccp2_service_ports:
>>>>> port value '%s' isn't valid (1..65535)\n", tmp);
>>>>> 
>>>>> 
>>>>>> On Mar 3, 2015, at 11:06 AM, Yuri Voinov
>>>>>> <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>>> 
>>>>> You cannot combine HTTP and HTTPS in one WCCP service.
>>>>> 
>>>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2 <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>
>>>>> <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2 <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>>
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
> 03.03.15 23:04, Guy Helmer ?????:
>>>>>>>> This used to work in 3.3.x:
>>>>>>>> 
>>>>>>>> wccp2_service_info 94 protocol=tcp 
>>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>>> ports=80,81,83,591,8008,8080,443
>>>>>>>> 
>>>>>>>> squid 3.4.12 fails: 2015/03/03 11:02:33.109| 
>>>>>>>> cache_cf.cc(556) parseOneConfigFile: Processing: 
>>>>>>>> wccp2_service_info 94 protocol=tcp 
>>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>>> ports=80,81,83,591,8008,8080,443 2015/03/03
>>>>>>>> 11:02:33.109| wccp2.cc <http://wccp2.cc/> <http://wccp2.cc/ <http://wccp2.cc/>>(2298)
>>>>>>>> parse_wccp2_service_info: parse_wccp2_service_info:
>>>>>>>> called 2015/03/03 11:02:33.109| ERROR: Invalid value:
>>>>>>>> '80,81,83,591,8008,8080,443' is supposed to be a
>>>>>>>> number.
>>>>>>>> 
>>>>>>>> Any help?
>>>>>>>> 
>>>>>>>> Thanks, Guy
>>>>>>>> _______________________________________________ 
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>>>> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>> 
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>>
>>>>>>>> 
>>>>>> _______________________________________________
>>>>>> squid-users mailing list squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>>>> <mailto:squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>> 
>>>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>> <http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>>
>>>>> 
>> 
>> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
> 
> iQEcBAEBAgAGBQJU9fpNAAoJENNXIZxhPexG+hoH/33NudcBRvS5Z/D/8wCN8Hwq
> 4+efhgF3dCfimb2kQjQFq3JsK6JvPUJO7TJm2h8tzqlAMOnBUuoM/MHEg48skcL2
> +udKwPUC/WO0GXVlNFdln3a4ozYotDmpwZIvwzBTPm5dpcwMi/bCX2oOyzq4y8yr
> mHzYEYmaP1tKF4bzEqlRsOIkrKUFvTvw6HlMoZ1EDE3Qp6WlM6WWiaf8rOoMKVRn
> dGBPvmvRo79OEMXkvc167BH2j8phOFs7XPUw9mWw7nY93jKEcBxxKl4PpwkK7km/
> am7DeV7uLQtnA5nuPs4QC063YRBEAu/8mjONRq5ytJurP8nUUsV46+sdl0EMY44=
> =S5O/
> -----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150303/6607b329/attachment.htm>

From yvoinov at gmail.com  Tue Mar  3 19:11:11 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 04 Mar 2015 01:11:11 +0600
Subject: [squid-users] wccp2_service_info fails on more than one port
 number
In-Reply-To: <FB89CE99-232A-4100-AC8B-99B387A73254@gmail.com>
References: <8681A17A-3A30-4DFC-A012-2EEEB97ACBB5@gmail.com>
 <54F5EA25.6080901@gmail.com> <FCD432D9-11E6-448A-A60B-3312257BAA6A@gmail.com>
 <54F5EF92.80407@gmail.com> <2DCF7238-EAAC-452B-B76D-4F355743ADFE@gmail.com>
 <54F5FA4D.7020704@gmail.com> <FB89CE99-232A-4100-AC8B-99B387A73254@gmail.com>
Message-ID: <54F6074F.7000200@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

In this case you can simple open bug on Bugzilla.

04.03.15 0:46, Guy Helmer ?????:
> Disregarding the complaints about Cisco configuration, here is a
> simple example: If I want to intercept plain HTTP on ports 8008 and
> 8080:
> 
> wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source
> priority=240 ports=8008,8080
> 
> Squid 3.4.12 will still give this incorrect error:
> 
> FATAL: Bungled /usr/local/etc/squid/squid.conf line 55:
> wccp2_service_info 94 protocol=tcp flags=dst_ip_hash,ports_source
> priority=240 ports=8008,8080
> 
> WCCP2 is documented as allowing up to 8 ports per service. This is
> broken by parsing changes in Squid 3.4.
> 
> Regards, Guy
> 
>> On Mar 3, 2015, at 12:15 PM, Yuri Voinov <yvoinov at gmail.com>
>> wrote:
>> 
> Don't think this is bug.
> 
> You got very custom configuration, which can not be common.
> 
> As I said, HTTP ports other than 80 is very rarely used in WAN's.
> And be better to separate HTTPS port from HTTP.
> 
> Modern iOS can not accept your configuration. Beware.
> 
> 03.03.15 23:57, Guy Helmer ?????:
>>>> Thanks Yuri -- I have thoroughly read Cisco IOS
>>>> configuration manuals pertaining to WCCP2. From what I have
>>>> read, there is no strict requirement for separate
>>>> configurations for standard web-cache port 80 and dynamic
>>>> service for non-port 80 ? wccp2 dynamic services allow
>>>> redirection of any ports (up to a total of 8), including port
>>>> 80. As I?ve stated before, this was a working configuration
>>>> in squid 3.3 at multiple sites. If there is a rationale for
>>>> distinct wccp2 service configurations for port 80 vs other
>>>> ports, I sure could use a reference that explains it.
>>>> 
>>>> As I?ve pointed out, there is a bug in Squid 3.4 that
>>>> prevents specifying multiple TCP ports in the
>>>> wccp2_service_info line. I?ve corrected that now in my
>>>> sources. After fixing that bug, squid 3.4.12 is functioning
>>>> with WCCP2 interception for port 80, 443, and others as it
>>>> did in version 3.3.x.
>>>> 
>>>> Regards, Guy
>>>> 
>>>>> On Mar 3, 2015, at 11:29 AM, Yuri Voinov
>>>>> <yvoinov at gmail.com> wrote:
>>>>> 
>>>> Please, read Cisco iOS WCCPv2 manual first.
>>>> 
>>>> This one:
>>>> 
>>>> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html
>>>> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>
>>>>
>>>> 
<http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html
<http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html>>
>>>> 
>>>> and this one:
>>>> 
>>>> http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955
>>>> <http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>
>>>>
>>>> 
<http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955
<http://www.cisco.com/c/en/us/td/docs/ios/12_2/configfun/configuration/guide/ffun_c/fcf018.html#wp1000955>>
>>>> 
>>>> wccp web-cache uses port 80 by default. Other http ports is
>>>> less than percentile on web-traffic.
>>>> 
>>>> https requires another dynamic wccp service.
>>>> 
>>>> Also, take a look on config example again.
>>>> 
>>>> This is working configuration.
>>>> 
>>>> 03.03.15 23:21, Guy Helmer ?????:
>>>>>>> It has worked in the past with WCCP2 dynamic services
>>>>>>> at multiple sites.
>>>>>>> 
>>>>>>> I?ve uncovered the wccp2_service_info ports parsing
>>>>>>> error:
>>>>>>> 
>>>>>>> --- src/wccp2.cc.ORIG	2015-03-03 11:08:18.000000000
>>>>>>> -0600 +++ src/wccp2.cc <http://wccp2.cc/>	2015-03-03
>>>>>>> 11:10:37.000000000 -0600 @@ -2264,7 +2264,10 @@ if (i
>>>>>>> >= WCCP2_NUMPORTS) { fatalf("parse_wccp2_service_ports:
>>>>>>> too many ports (maximum: 8) in list '%s'\n", options);
>>>>>>> } -        int p = xatoi(tmp); + char copy[len + 1]; +
>>>>>>> memcpy(copy, tmp, len); + copy[len] = '\0'; +
>>>>>>> int p = xatoi(copy);
>>>>>>> 
>>>>>>> if (p < 1 || p > 65535) {
>>>>>>> fatalf("parse_wccp2_service_ports: port value '%s'
>>>>>>> isn't valid (1..65535)\n", tmp);
>>>>>>> 
>>>>>>> 
>>>>>>>> On Mar 3, 2015, at 11:06 AM, Yuri Voinov 
>>>>>>>> <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>>>> wrote:
>>>>>>>> 
>>>>>>> You cannot combine HTTP and HTTPS in one WCCP service.
>>>>>>> 
>>>>>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
>>>>>>> <http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>
>>>>>>>
>>>>>>> 
<http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2
<http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2>>
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
> 03.03.15 23:04, Guy Helmer ?????:
>>>>>>>>>> This used to work in 3.3.x:
>>>>>>>>>> 
>>>>>>>>>> wccp2_service_info 94 protocol=tcp 
>>>>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>>>>> ports=80,81,83,591,8008,8080,443
>>>>>>>>>> 
>>>>>>>>>> squid 3.4.12 fails: 2015/03/03 11:02:33.109| 
>>>>>>>>>> cache_cf.cc(556) parseOneConfigFile: Processing:
>>>>>>>>>>  wccp2_service_info 94 protocol=tcp 
>>>>>>>>>> flags=dst_ip_hash,ports_source priority=240 
>>>>>>>>>> ports=80,81,83,591,8008,8080,443 2015/03/03 
>>>>>>>>>> 11:02:33.109| wccp2.cc <http://wccp2.cc/>
>>>>>>>>>> <http://wccp2.cc/ <http://wccp2.cc/>>(2298) 
>>>>>>>>>> parse_wccp2_service_info:
>>>>>>>>>> parse_wccp2_service_info: called 2015/03/03
>>>>>>>>>> 11:02:33.109| ERROR: Invalid value: 
>>>>>>>>>> '80,81,83,591,8008,8080,443' is supposed to be a 
>>>>>>>>>> number.
>>>>>>>>>> 
>>>>>>>>>> Any help?
>>>>>>>>>> 
>>>>>>>>>> Thanks, Guy 
>>>>>>>>>> _______________________________________________ 
>>>>>>>>>> squid-users mailing list 
>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>> <mailto:squid-users at lists.squid-cache.org> 
>>>>>>>>>> <mailto:squid-users at lists.squid-cache.org
>>>>>>>>>> <mailto:squid-users at lists.squid-cache.org>> 
>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>>>>
>>>>>>>>>> 
<http://lists.squid-cache.org/listinfo/squid-users
<http://lists.squid-cache.org/listinfo/squid-users>>
>>>>>>>>>> 
>>>>>>>> _______________________________________________ 
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>> <mailto:squid-users at lists.squid-cache.org> 
>>>>>>>> <mailto:squid-users at lists.squid-cache.org
>>>>>>>> <mailto:squid-users at lists.squid-cache.org>> 
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users> 
>>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>> <http://lists.squid-cache.org/listinfo/squid-users>>
>>>>>>> 
>>>> 
>>>> 
> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU9gdPAAoJENNXIZxhPexGXSIIAIS7GWUxK9Do7WXVBc+K78tW
muArO50AbqK4EhcimZEp1pNm7+S7eLMcC1tk535l5IlXWzt3y4Y66bweCnmmm1HC
1JbpSYfFHB161L43ltxQ1MDrTZP5IL+VwkCzoV4f4MUkLvwXevZYq3MP5HaWDQim
BjGrp3D2QTvcYxzky+9fOko9cYpkd10aohfSIEDeTWq8QshNYZiqZhEYrh6pnkWR
6QfLX/ie8An80Y+gWfZWk6rOQaKGaD3DH8wHs0Dz1wO7sNBfyt/nLcyoridX17En
T80g2tUErO1OxOcngf+q75VbVEW4IfuiV3WWb61vxu0+iuWOhEevU7/qMmPSOfs=
=l50x
-----END PGP SIGNATURE-----


From sebag at vianetcon.com.ar  Tue Mar  3 20:35:05 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Tue, 03 Mar 2015 17:35:05 -0300
Subject: [squid-users] Redirecting traffic to fake parent
Message-ID: <54F61AF9.4090300@vianetcon.com.ar>

Hello everyone, I'm experimenting with cache_peer directive and node.js:

cache_peer 10.0.0.90 parent 8888 0 no-query no-digest proxy-only name=test

in that port I have a node.js Proxy receiveing connections in the same 
host, it extracts some information I need and saves it to a DB, then 
redirects Squid with a 302 response with some garbage added to the url. 
I use that garbage to match an access list so I can prevent looping.

Squid is working in transparent mode, the problem I'm facing is that if 
I don't configure a tcp_outgoing_address Squid does not reach port 8888 
on localhost. If I set a tcp_outgoing_address Squid can reach 
localhost:8888 but with his own IP address and I need it to be 
transparent, I need the real client IP address.

Is there a way to configure tcp_outgoing_address to use the client's IP 
when fetching something?

Some config lines that might help:

acl donotredirect url_regex .*56498765123168.*
cache_peer_access test deny donotredirect

acl kk url_regex .*redirectthisstuff.*
cache_peer_access test allow kk
cache_peer_access test deny all
never_direct deny donotredirect
never_direct allow kk



Thanks for your time

Sebastian


From vdoctor at neuf.fr  Tue Mar  3 22:14:20 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 3 Mar 2015 14:14:20 -0800 (PST)
Subject: [squid-users] TProxy and client_dst_passthru
Message-ID: <1425420860442-4670189.post@n4.nabble.com>

Hi All,

Does someone know why the "*client_dst_passthru*" does not work in TProxy
mode ?

>From the Squid wiki, we can read that:
"/Regardless of this option setting, when dealing with intercepted
traffic Squid will verify the Host: header and any traffic which
fails Host verification will be treated as if this option were ON/."

In normal (no intercept) http_port, the option works fine but does not act
on Tproxy...

Thanks in advance for your feedbacks 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Mar  4 01:15:15 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 04 Mar 2015 03:15:15 +0200
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1425420860442-4670189.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
Message-ID: <54F65CA3.5070708@ngtech.co.il>

Hey Fred,

It is unclear what doesn't work for you.
What would you expect to work and how it works or doesn't work from a 
user perspective rather then an admin?
Is there any trouble from the user side about this issue?

Eliezer

On 04/03/2015 00:14, Stakres wrote:
> Hi All,
>
> Does someone know why the "*client_dst_passthru*" does not work in TProxy
> mode ?
>
>  From the Squid wiki, we can read that:
> "/Regardless of this option setting, when dealing with intercepted
> traffic Squid will verify the Host: header and any traffic which
> fails Host verification will be treated as if this option were ON/."
>
> In normal (no intercept) http_port, the option works fine but does not act
> on Tproxy...
>
> Thanks in advance for your feedbacks
>
> Bye Fred



From squid3 at treenet.co.nz  Wed Mar  4 02:32:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Mar 2015 15:32:15 +1300
Subject: [squid-users] Redirecting traffic to fake parent
In-Reply-To: <54F61AF9.4090300@vianetcon.com.ar>
References: <54F61AF9.4090300@vianetcon.com.ar>
Message-ID: <54F66EAF.7070307@treenet.co.nz>

On 4/03/2015 9:35 a.m., Sebastian Goicochea wrote:
> Hello everyone, I'm experimenting with cache_peer directive and node.js:
> 
> cache_peer 10.0.0.90 parent 8888 0 no-query no-digest proxy-only name=test
> 
> in that port I have a node.js Proxy receiveing connections in the same
> host, it extracts some information I need and saves it to a DB, then
> redirects Squid with a 302 response with some garbage added to the url.
> I use that garbage to match an access list so I can prevent looping.
> 
> Squid is working in transparent mode, the problem I'm facing is that if
> I don't configure a tcp_outgoing_address Squid does not reach port 8888
> on localhost. If I set a tcp_outgoing_address Squid can reach
> localhost:8888 but with his own IP address and I need it to be
> transparent, I need the real client IP address.

Why? what is all this for?

HTTP is designed to operate just fine without forging client IPs on
proxy outgoing traffic. Some web applications are seriously broken, but
since its on your localhost you obviously are in a great position to fix
this one.


also, it sounds to me like you are using all this complex 4-party
interaction to replicate something that an ICAP/eCAP service does much
faster and simpler. Or perhapse you are trying to implement Squids
StoreID feature using layers of proxies.


> 
> Is there a way to configure tcp_outgoing_address to use the client's IP
> when fetching something?

No. You can only bind to IP addresses which have been assigned to the
machine Squid is running on. Lookup the "triangular routing" problem if
you want all the gory details.

Amos


From johnzeng2013 at yahoo.com  Wed Mar  4 02:41:13 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 04 Mar 2015 10:41:13 +0800
Subject: [squid-users] Redirecting traffic to fake parent
In-Reply-To: <54F61AF9.4090300@vianetcon.com.ar>
References: <54F61AF9.4090300@vianetcon.com.ar>
Message-ID: <54F670C9.1070506@yahoo.com>

Hello Dear Sebastian:

                                  i am thinking about node.js + squid at 
isp environment for video file cache or big file cache recently ,

                                  Which useful public info will we will 
obtain from internet ?

                                  if you have any helpful website about 
nodejs ( for example: gatejs or other ) , and if you think it will share 
with other people ,

                                  please give me some advisement , maybe 
we can develop the part together for everyone's different goal


                                  Best Regareds

                                  john

? 2015?03?04? 04:35, Sebastian Goicochea ??:
> Hello everyone, I'm experimenting with cache_peer directive and node.js:
>
> cache_peer 10.0.0.90 parent 8888 0 no-query no-digest proxy-only 
> name=test
>
> in that port I have a node.js Proxy receiveing connections in the same 
> host, it extracts some information I need and saves it to a DB, then 
> redirects Squid with a 302 response with some garbage added to the 
> url. I use that garbage to match an access list so I can prevent looping.
>
> Squid is working in transparent mode, the problem I'm facing is that 
> if I don't configure a tcp_outgoing_address Squid does not reach port 
> 8888 on localhost. If I set a tcp_outgoing_address Squid can reach 
> localhost:8888 but with his own IP address and I need it to be 
> transparent, I need the real client IP address.
>
> Is there a way to configure tcp_outgoing_address to use the client's 
> IP when fetching something?
>
> Some config lines that might help:
>
> acl donotredirect url_regex .*56498765123168.*
> cache_peer_access test deny donotredirect
>
> acl kk url_regex .*redirectthisstuff.*
> cache_peer_access test allow kk
> cache_peer_access test deny all
> never_direct deny donotredirect
> never_direct allow kk
>
>
>
> Thanks for your time
>
> Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Mar  4 03:02:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Mar 2015 16:02:20 +1300
Subject: [squid-users] Can single squid server handle 1Gbps traffic?
In-Reply-To: <54F5FB66.2070902@gmail.com>
References: <1108080049.394590923.1425404695132.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <54F5FB66.2070902@gmail.com>
Message-ID: <54F675BC.5040200@treenet.co.nz>

On 4/03/2015 7:20 a.m., Yuri Voinov wrote:
> Fred,
> 
> don't think so.
> 
> This is big enough box.
> 
> And files above 1 Gb is not downloaded all time.
> 
> Main question - what loads planned?

That was Fred's point.

The question as asked was meaningless. Yes a single Squid can reach
that speed, but probably only for a uncommon type of traffic.

... and with SMP support nowdays a "single Squid" is no longer a
single *process*. Which raises the throughput capacity by a factor of
up to 10 on that box.

Amos


From vdoctor at neuf.fr  Wed Mar  4 07:19:30 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 3 Mar 2015 23:19:30 -0800 (PST)
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <54F65CA3.5070708@ngtech.co.il>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il>
Message-ID: <1425453570201-4670194.post@n4.nabble.com>

Hi Eliezer,

Well, we have done many tests with Squid (3.1 to 3.5.x), disabling
"client_dst_passthru" (off) will stop the DNS entry as explained in the
wiki, the option directly acts on the flag "ORIGINAL_DST".
As you know, ORIGINAL_DST switches the optimization off (ex: StoreID) then
it's not possible to cache the URL (ex: http://cdn2.example.com/mypic.png).

In no tproxy/NAT mode, the client_dst_passthru works perfectly by disabling
the DNS entry control, so optimization is done correctly.
But in tproxy/NAT, the client_dst_passthru has no effect, we see
ORIGINAL_DST in logs.

So, maybe I'm totaly wrong here the client_dst_passthru is not related to
the ORIGINAL_DST, or there is an explaination why the client_dst_passthru
does not act in tproxy/NAT...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TProxy-and-client-dst-passthru-tp4670189p4670194.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  4 09:38:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Mar 2015 22:38:52 +1300
Subject: [squid-users] TProxy and client_dst_passthru
In-Reply-To: <1425453570201-4670194.post@n4.nabble.com>
References: <1425420860442-4670189.post@n4.nabble.com>
 <54F65CA3.5070708@ngtech.co.il> <1425453570201-4670194.post@n4.nabble.com>
Message-ID: <54F6D2AC.6050203@treenet.co.nz>

On 4/03/2015 8:19 p.m., Stakres wrote:
> Hi Eliezer,
> 
> Well, we have done many tests with Squid (3.1 to 3.5.x), disabling
> "client_dst_passthru" (off) will stop the DNS entry as explained in the
> wiki, the option directly acts on the flag "ORIGINAL_DST".

You literally have that backwards.

The cause is NAT/TPROXY.
The directive is acting on the NAT mangled dst-IP address (replace it?
yes/no).
The log flag is an effect telling you what was chosen (yes or no answer).


> As you know, ORIGINAL_DST switches the optimization off (ex: StoreID) then
> it's not possible to cache the URL (ex: http://cdn2.example.com/mypic.png).
> 
> In no tproxy/NAT mode, the client_dst_passthru works perfectly by disabling
> the DNS entry control, so optimization is done correctly.

WTF? no (and FWIW I wrote that pass-thru feature).

In "no TPROXY/NAT mode" the client *explicitly* requests Squid to fetch
a URL. Squid looks up DNS for the domain in that URL and contacts any
one of the IPs available.
 There is no client DNS lookup to do pass-thru for. The directive has no
effect whatsoever.


> But in tproxy/NAT, the client_dst_passthru has no effect, we see
> ORIGINAL_DST in logs.

In TPROXY/NAT intercept mode, the client does the DNS lookup and selects
which IP to contact. The NAT/TPROXY diverts the traffic into Squid.
Squid grabs the original client dst-IP from the kernel records. Does a
second DNS lookup to see where the URL was supposed to be going
according to more reliable sources.

 *IF* (and only if) Squid can verify the IP the client selected actually
belongs to the domain in the URL does Squid check the pass-thru directive:
 ON (default) - Squid uses the IP the client was connecting to (logs
ORIGINAL_DST);
 OFF - Squid selects a (possibly new, or not) IP to be used as the
server (logs DIRECT).

 Otherwise, when that dst-IP validation fails, Squid uses the IP the
client was connecting to. The directive has no effect whatsoever.


> 
> So, maybe I'm totaly wrong here the client_dst_passthru is not related to
> the ORIGINAL_DST, or there is an explaination why the client_dst_passthru
> does not act in tproxy/NAT...

client_dst_passthru is simply whether or not to use ORIGINAL_DST in the
cases where it is both available AND optional.
 There are other cases where it is mandatory to use, or is unavailable.


For users the key information is that:

Setting OFF enables HTTP routing optimizations and network error
recovery to take place, but severly confuses people/applications who
dont understand much about HTTP.


Setting ON reduces problems with broken web applications that (wrongly)
assume end-to-end connectivity within HTTP, or that (wrongly) assume all
connections to a given IP are going to end up at the same origin server.
Neither of those assumptions are true for HTTP whether it goes over port
80 or port 443 or a proxy.


Amos


From johnzeng2013 at yahoo.com  Wed Mar  4 15:32:15 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 04 Mar 2015 23:32:15 +0800
Subject: [squid-users] WARNING: disk-cache maximum object size is too
 large for mem-cache: 102400.00 KB > 90.00 KB
In-Reply-To: <54F5D4F4.7010309@treenet.co.nz>
References: <54F5C143.5010405@yahoo.com> <54F5D4F4.7010309@treenet.co.nz>
Message-ID: <54F7257F.9010606@yahoo.com>


Hello  Amos :

                      Sorry , previous config is a simple config file 
for testing tproxy.

                      But i checked again and switch different part of 
config for disabling the warning info and find real reason .

                      first config ( it will cause warning info)  :

                      maximum_object_size 100 MB
                      maximum_object_size_in_memory 4095 bytes

                      WARNING: disk-cache maximum object size is too 
large for mem-cache: 102400.00 KB > 4.00 KB


                      Second config ( no any warning info ) :

                      maximum_object_size 10 MB
                      maximum_object_size_in_memory 10 MB

                      or

                      maximum_object_size 4095 bytes
                      maximum_object_size_in_memory 4095 bytes

                      or

                      maximum_object_size 10 MB
                      maximum_object_size_in_memory 11 MB


                      if condition is   maximum_object_size  < =  
maximum_object_size_in_memory at our config , We won't find warning info ,

                      But i think it will be error direction .


                      Whether it is a bug ,  if possible , please give 
me some advisement .


                      Best Regards

                      john

? 2015?03?03? 23:36, Amos Jeffries ??:
> On 4/03/2015 3:12 a.m., johnzeng wrote:
>> Hi :
>>
>> Thanks Amos Jeffries, i updated to squid 3.5.2 and
>>
>> tested QQ and other application at two different environment ( between
>> intercept and tproxy ) in according to your advisement .
>>
>> result is ok as your saying .
>>
>> Thanks again .
>>
>> but i checked cache log and found ( WARNING: disk-cache maximum object
>> size is too large for mem-cache: 102400.00 KB > 90.00 KB
>>
>> Whether there are some error at my config ??
>>
> I'm not sure exactly why that is being warned about. Its reasonable to
> have a smaller object size for memory-only objects. I think not to worry
> about it.
>
>
> The size ranges in your config look a little strange though.
>
> * objects between 1KB and 90KB are allowed to be stored only in memory
>
> * objects between 262KB and 100MB are allowed to be stored only on disk
>
> * objects under 1KB, between 90KB and 262KB, or over 100MB are not
> allowed to be stored anywhere at all.
>
>
>
> Also, how is the multiple PID file setup working out?
>   The PID file is supposed to contain the ID number of the master process
> in charge of the whole process tree and managing signals received from
> the init system and "squid -k" commands.
>
> Amos
>
>> ------------------------------------------------------------
>>
>> this is my config
>>
>> -------------------------------------------------------------
>>
>>
>> minimum_object_size 1 KB
>> maximum_object_size 100 MB
>> maximum_object_size_in_memory 90 KB
>>
>> cache_swap_low 80
>> cache_swap_high 95
>>
>>
>> #Smp setting
>>
>> workers 2
>>
>> cpu_affinity_map process_numbers=1,2 cores=1,2
>>
>> if ${process_number} = 1
>>
>> pid_filename /accerater/logs/webcache3/opmizer1/cachea.pid
>> cache_swap_state /accerater/logs/webcache3/cachea_swap_log1
>> access_log stdio:/accerater/logs/webcache3/accessa.log squid
>> cache_log /accerater/logs/webcache3/cachea.log
>> cache_store_log stdio:/accerater/logs/webcache3/storea.log1
>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number}
>> 10200 16 64 min-size=262145
>> coredump_dir /accerater/logs/webcache3/opmizer1
>> unlinkd_program /accerater/webcache3/libexec/unlinkd
>> unique_hostname maraservice1
>> snmp_port 3401
>>
>> endif
>>
>> if ${process_number} = 2
>> pid_filename /accerater/logs/webcache3/opmizer2/cacheb.pid
>> cache_swap_state /accerater/logs/webcache3/cacheb_swap_log
>> access_log stdio:/accerater/logs/webcache3/accessb.log squid
>> cache_log /accerater/logs/webcache3/cacheb.log1
>> cache_store_log stdio:/accerater/logs/webcache3/storeb.log1
>> cache_dir aufs /accerater/webcache3/storage/aufs2/${process_number}
>> 10200 16 64 min-size=262145
>> coredump_dir /accerater/logs/webcache3/opmizer2
>> unlinkd_program /accerater/webcache3/libexec/unlinka
>> unique_hostname maraservice2
>> snmp_port 3402
>>
>>
>> endif
>>



From sebag at vianetcon.com.ar  Wed Mar  4 15:35:29 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 04 Mar 2015 12:35:29 -0300
Subject: [squid-users] Redirecting traffic to fake parent
In-Reply-To: <54F66EAF.7070307@treenet.co.nz>
References: <54F61AF9.4090300@vianetcon.com.ar>
 <54F66EAF.7070307@treenet.co.nz>
Message-ID: <54F72641.3000705@vianetcon.com.ar>

The problem is that I'm using Squid 2.7 (I know how behind I am), and I 
need to use the referrer of certain requests and give those url + 
referer to a rewriter program (Posible in v 3.5 as far as I know). So in 
order to achieve this using 2.7 I redirect the traffic to a fake node.js 
proxy, extract what I need, store it on a DB and then the rewriter con 
make a query there and make the proper rewrite. I have that working 
already, but I don't know how to mantain the original IP of the client 
making the request to the very end of the transaction so to the outside 
world it doesn't look that all the requests come from the same client.


Thanks
Sebastian


El 03/03/15 a las 23:32, Amos Jeffries escribi?:
> On 4/03/2015 9:35 a.m., Sebastian Goicochea wrote:
>> Hello everyone, I'm experimenting with cache_peer directive and node.js:
>>
>> cache_peer 10.0.0.90 parent 8888 0 no-query no-digest proxy-only name=test
>>
>> in that port I have a node.js Proxy receiveing connections in the same
>> host, it extracts some information I need and saves it to a DB, then
>> redirects Squid with a 302 response with some garbage added to the url.
>> I use that garbage to match an access list so I can prevent looping.
>>
>> Squid is working in transparent mode, the problem I'm facing is that if
>> I don't configure a tcp_outgoing_address Squid does not reach port 8888
>> on localhost. If I set a tcp_outgoing_address Squid can reach
>> localhost:8888 but with his own IP address and I need it to be
>> transparent, I need the real client IP address.
> Why? what is all this for?
>
> HTTP is designed to operate just fine without forging client IPs on
> proxy outgoing traffic. Some web applications are seriously broken, but
> since its on your localhost you obviously are in a great position to fix
> this one.
>
>
> also, it sounds to me like you are using all this complex 4-party
> interaction to replicate something that an ICAP/eCAP service does much
> faster and simpler. Or perhapse you are trying to implement Squids
> StoreID feature using layers of proxies.
>
>
>> Is there a way to configure tcp_outgoing_address to use the client's IP
>> when fetching something?
> No. You can only bind to IP addresses which have been assigned to the
> machine Squid is running on. Lookup the "triangular routing" problem if
> you want all the gory details.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From michel.petterson at gmail.com  Wed Mar  4 23:54:59 2015
From: michel.petterson at gmail.com (Michel Peterson)
Date: Wed, 4 Mar 2015 20:54:59 -0300
Subject: [squid-users] Squid 3.5.2 Compile Error
Message-ID: <CA+oP0hn5Ppk=P1CckTZdtvmXk2Ty5VdHvXWe_c3RLXQbXZ+imQ@mail.gmail.com>

Hi friends,

I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
the following error after running the command "make all":

Making all in compat
make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
        /bin/bash ../libtool  --tag=CXX   --mode=compile g++
-DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
-Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
-pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
-O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
-o assert.lo assert.cc &&\
        mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
-I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
-D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
-MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
.libs/assert.o
In file included from ../include/squid.h:43:0,
                 from assert.cc:9:
../compat/compat.h:49:57: error: operator '&&' has no right operand
make[1]: ** [assert.lo] Erro 1
make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
make: ** [all-recursive] Erro 1


Michel Peterson
Analista de Sistemas Linux
Linux Certifications (LPI-101, LPI-102, LPI-201, LPI-202)
Cel: 71 9952-0767


From michel.petterson at gmail.com  Thu Mar  5 00:07:34 2015
From: michel.petterson at gmail.com (Michel Peterson)
Date: Wed, 4 Mar 2015 21:07:34 -0300
Subject: [squid-users] Squid 3.5.2 Compile Error
Message-ID: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>

Hi friends,

I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
the following error after running the command "make all":

Making all in compat
make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
        /bin/bash ../libtool  --tag=CXX   --mode=compile g++
-DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
-Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
-pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
-O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
-o assert.lo assert.cc &&\
        mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
-I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
-D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
-MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
.libs/assert.o
In file included from ../include/squid.h:43:0,
                 from assert.cc:9:
../compat/compat.h:49:57: error: operator '&&' has no right operand
make[1]: ** [assert.lo] Erro 1
make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
make: ** [all-recursive] Erro 1

-- 
Michel Peterson
Analista de Sistemas Linux
Linux Certifications (LPI-101, LPI-102, LPI-201, LPI-202)
Cel: 71 9952-0767


From squid3 at treenet.co.nz  Thu Mar  5 03:10:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 05 Mar 2015 16:10:05 +1300
Subject: [squid-users] Redirecting traffic to fake parent
In-Reply-To: <54F72641.3000705@vianetcon.com.ar>
References: <54F61AF9.4090300@vianetcon.com.ar>
 <54F66EAF.7070307@treenet.co.nz> <54F72641.3000705@vianetcon.com.ar>
Message-ID: <54F7C90D.5000900@treenet.co.nz>

On 5/03/2015 4:35 a.m., Sebastian Goicochea wrote:
> The problem is that I'm using Squid 2.7 (I know how behind I am), and I
> need to use the referrer of certain requests and give those url +
> referer to a rewriter program (Posible in v 3.5 as far as I know). So in
> order to achieve this using 2.7 I redirect the traffic to a fake node.js
> proxy, extract what I need, store it on a DB and then the rewriter con
> make a query there and make the proper rewrite. I have that working
> already, but I don't know how to mantain the original IP of the client
> making the request to the very end of the transaction so to the outside
> world it doesn't look that all the requests come from the same client.

In 2.7 the only way is "forwarded_for on" in squid.conf.

Amos




From admin at hostknight.net  Thu Mar  5 03:18:30 2015
From: admin at hostknight.net (HostKnight)
Date: Thu, 5 Mar 2015 03:18:30 -0000
Subject: [squid-users] Selling Squid Proxies Module?
Message-ID: <001a01d056f3$0e650ed0$2b2f2c70$@net>

Hello,

 

I am looking to setup a proxy business and am hoping someone can help point
me in the direction of some sort of module I can use to automate the
process? I have found one for WHMCS billing platform however I am not happy
with it (http://whmcsproxymanager.com)

 

Any help will be greatly appreciated.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/3216aafe/attachment.htm>

From alberto2perez at gmail.com  Thu Mar  5 03:25:32 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Wed, 4 Mar 2015 22:25:32 -0500
Subject: [squid-users] issue with tcp_outgoing_address and external acl
Message-ID: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>

Hi and thanks to all members in this community for the great support.


Im trying to send traffic for some users through a faster link using
tcp_outgoing_address, I found this directive very usefull and suitable
for this need.

I use a captive portal and my squid only use autorization, so users
are not available with standard ident method.

I wrote an acl to simulate ident process by reading to memory a list
of users from a file and matching external user returned by active
session external acl. For general purposes of autorization this work
fine.

I found that tcp_outgoing_address works good with src ACLs but doesn't
work with this external acl, by the way, has huge ttl so squid doesn't
evaluate it on each request.

Can anyone please tell me if this is possible, or point the right
direction to solve this.

I will be glad to provide more information about my case if is needed.

Thanks a lot in advance

Alberto


From johnzeng2013 at yahoo.com  Thu Mar  5 03:42:43 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 05 Mar 2015 11:42:43 +0800
Subject: [squid-users] i config rock at smp mode just now ,
 but i find some error. FATAL: Rock cache_dir at
 /squid/storage/rock1/rock failed to open db file: (11) Resource temporarily
 unavailable
Message-ID: <54F7D0B3.9000706@yahoo.com>


Hi all

i config rock at smp mode just now , but i find some error.

if possible , please help me to analyze the error info , and give me
some advisement

show error info via squid -z


2015/03/05 11:35:46 kid1| Creating missing swap directories
2015/03/05 11:35:46 kid3| Skipping existing Rock db:
/squid/storage/rock1/rock

show error info squid -d1

2015/03/05 11:30:28 kid1| ERROR: /squid/storage/rock1/rock communication
channel establishment timeout
FATAL: Rock cache_dir at /squid/storage/rock1/rock failed to open db
file: (11) Resource temporarily unavailable



this is my config about rock part

workers 2
cpu_affinity_map process_numbers=1,2 cores=1,2

cache_dir rock /squid/storage/rock1 2646 min-size=4096 max-size=262144
max-swap-rate=250 swap-timeout=350




From johnzeng2013 at yahoo.com  Thu Mar  5 03:44:12 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 05 Mar 2015 11:44:12 +0800
Subject: [squid-users] i config rock at smp mode just now ,
 but i find some error. FATAL: Rock cache_dir at
 /squid/storage/rock1/rock failed to open db file: (11) Resource temporarily
 unavailable
In-Reply-To: <54F7D0B3.9000706@yahoo.com>
References: <54F7D0B3.9000706@yahoo.com>
Message-ID: <54F7D10C.1000103@yahoo.com>





Hi all

i config rock at smp mode just now , but i find some error.

if possible , please help me to analyze the error info , and give me
some advisement

show error info via squid -z


2015/03/05 11:35:46 kid1| Creating missing swap directories
2015/03/05 11:35:46 kid3| Skipping existing Rock db:
/squid/storage/rock1/rock

show error info squid -d1

2015/03/05 11:30:28 kid1| ERROR: /squid/storage/rock1/rock communication
channel establishment timeout
FATAL: Rock cache_dir at /squid/storage/rock1/rock failed to open db
file: (11) Resource temporarily unavailable



this is my config about rock part

workers 2
cpu_affinity_map process_numbers=1,2 cores=1,2

cache_dir rock /squid/storage/rock1 2646 min-size=4096 max-size=262144
max-swap-rate=250 swap-timeout=350






From johnzeng2013 at yahoo.com  Thu Mar  5 06:55:40 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 05 Mar 2015 14:55:40 +0800
Subject: [squid-users] (sloved)Re: i config rock at smp mode just now ,
 but i find some error. FATAL: Rock cache_dir at
 /squid/storage/rock1/rock failed to open db file: (11) Resource temporarily
 unavailable
In-Reply-To: <54F7D10C.1000103@yahoo.com>
References: <54F7D0B3.9000706@yahoo.com> <54F7D10C.1000103@yahoo.com>
Message-ID: <54F7FDEC.1040006@yahoo.com>

Hello Dear all

i sloved the problem , master reason is http_port Conflict with other
application port , it will cause to show these error info .

ERROR: /squid/storage/rock1/rock communication
channel establishment timeout


            



>
>
>
> Hi all
>
> i config rock at smp mode just now , but i find some error.
>
> if possible , please help me to analyze the error info , and give me
> some advisement
>
> show error info via squid -z
>
>
> 2015/03/05 11:35:46 kid1| Creating missing swap directories
> 2015/03/05 11:35:46 kid3| Skipping existing Rock db:
> /squid/storage/rock1/rock
>
> show error info squid -d1
>
> 2015/03/05 11:30:28 kid1| ERROR: /squid/storage/rock1/rock communication
> channel establishment timeout
> FATAL: Rock cache_dir at /squid/storage/rock1/rock failed to open db
> file: (11) Resource temporarily unavailable
>
>
>
> this is my config about rock part
>
> workers 2
> cpu_affinity_map process_numbers=1,2 cores=1,2
>
> cache_dir rock /squid/storage/rock1 2646 min-size=4096 max-size=262144
> max-swap-rate=250 swap-timeout=350
>
>
>
>



From squid3 at treenet.co.nz  Thu Mar  5 09:36:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 05 Mar 2015 22:36:42 +1300
Subject: [squid-users] issue with tcp_outgoing_address and external acl
In-Reply-To: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>
References: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>
Message-ID: <54F823AA.1030407@treenet.co.nz>

On 5/03/2015 4:25 p.m., Alberto Perez wrote:
> Hi and thanks to all members in this community for the great support.
> 
> 
> Im trying to send traffic for some users through a faster link using
> tcp_outgoing_address, I found this directive very usefull and suitable
> for this need.
> 
> I use a captive portal and my squid only use autorization, so users
> are not available with standard ident method.
> 
> I wrote an acl to simulate ident process by reading to memory a list
> of users from a file and matching external user returned by active
> session external acl. For general purposes of autorization this work
> fine.
> 
> I found that tcp_outgoing_address works good with src ACLs but doesn't
> work with this external acl, by the way, has huge ttl so squid doesn't
> evaluate it on each request.
> 
> Can anyone please tell me if this is possible, or point the right
> direction to solve this.
> 
> I will be glad to provide more information about my case if is needed.
> 

Are you asking how to set this up? or complaining about a problem you
found with it?

The answer to both is in the FAQ:
<http://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>


Amos



From apani at yandex.ru  Thu Mar  5 11:53:13 2015
From: apani at yandex.ru (Sergey Pronin)
Date: Thu, 05 Mar 2015 14:53:13 +0300
Subject: [squid-users] ssl_bump and SNI
Message-ID: <318411425556393@web10j.yandex.ru>

Hello guys,

I have a question about bumping and SNI. Is it supported now in squid 3.5?

What do I have:
Debian Linux
squid 3.5.2

Config for SSL transparent interception is the following:

https_port 10.10.115.7:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/squidCA
always_direct allow all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
ssl_bump none localhost
ssl_bump peek all
ssl_bump bump all

sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

With this configuration access log looks like this for HTTPS traffic:

192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT 177.71.251.241:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE
192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT 223.25.233.66:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE
192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT 103.16.26.232:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE
192.168.78.6 - - [05/Mar/2015:13:44:54 +0200] "CONNECT 65.55.163.221:443 HTTP/1.1" 200 895 "-" "-" TCP_TUNNEL:ORIGINAL_DST

Certificates are generated for IP's as well, not CNs.
Clients are redirected via IPtables.

I have tried to modify ssl_bump options:

1) 
ssl_bump stare all
2) 
ssl_bump peek all
3)
ssl_bump bump all

etc., but still only IPs are shown.

Could you please tell, where it is I'm mistaken?

--
Regards


From monahbaki at gmail.com  Thu Mar  5 12:19:33 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 07:19:33 -0500
Subject: [squid-users] squid intercept config
Message-ID: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>

Hi all, can anyone verify if this is correct, need to make ure that users
will be able to access the internet via the squid.

Running FreeBSD with a single interface with Squid-3.5.2

Policy based routing on Cisco with the following:


interface GigabitEthernet0/0/1.1

encapsulation dot1Q 1 native

ip address 10.0.0.9 255.255.255.0

no ip redirects

no ip unreachables

ip nat inside

standby 1 ip 10.0.0.10

standby 1 priority 120

standby 1 preempt

standby 1 name HSRP

ip policy route-map CFLOW



ip access-list extended REDIRECT

deny   tcp host 10.0.0.24 any eq www

permit tcp host 10.0.0.23 any eq www



route-map CFLOW permit 10

match ip address REDIRECT
set ip next-hop 10.0.0.24

In my /etc/pf.conf
rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
3129

# block in
pass in log quick on bge0
pass out log quick on bge0
pass out keep state

and finally in my squid.conf:
http_port 3128
http_port 3129 intercept



And for testing purposes from the squid server:
 ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/

If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
-p 3128 completely, I can access the websites.

tcpdump with (-p 3128)

13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http: Flags
[.], ack 17377, win 1018, options [nop,nop,TS val 985588797 ecr
1054387720], length 0
13:15:02.681421 IP wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags
[.], seq 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720
ecr 985588501], length 1448
13:15:02.681575 IP wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags
[.], seq 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720
ecr 985588501], length 1448



Did I miss anything?

Thanks
Monah
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/1f6521c7/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 13:18:59 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 19:18:59 +0600
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <318411425556393@web10j.yandex.ru>
References: <318411425556393@web10j.yandex.ru>
Message-ID: <54F857C3.6050200@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Transparent interception in 3.5 still not completely supports SNI.
Only in 3.4.x branch.

And yes - you do it wrong in your config:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

05.03.15 17:53, Sergey Pronin ?????:
> Hello guys,
> 
> I have a question about bumping and SNI. Is it supported now in
> squid 3.5?
> 
> What do I have: Debian Linux squid 3.5.2
> 
> Config for SSL transparent interception is the following:
> 
> https_port 10.10.115.7:3129 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/etc/squid3/squidCA always_direct allow all 
> sslproxy_cert_error allow all sslproxy_flags DONT_VERIFY_PEER 
> ssl_bump none localhost ssl_bump peek all ssl_bump bump all
> 
> sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/ssl_db -M 4MB 
> sslcrtd_children 5
> 
> With this configuration access log looks like this for HTTPS
> traffic:
> 
> 192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT
> 177.71.251.241:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE 
> 192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT
> 223.25.233.66:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE 
> 192.168.78.31 - - [05/Mar/2015:13:44:50 +0200] "CONNECT
> 103.16.26.232:443 HTTP/1.1" 200 0 "-" "-" TCP_DENIED:HIER_NONE 
> 192.168.78.6 - - [05/Mar/2015:13:44:54 +0200] "CONNECT
> 65.55.163.221:443 HTTP/1.1" 200 895 "-" "-"
> TCP_TUNNEL:ORIGINAL_DST
> 
> Certificates are generated for IP's as well, not CNs. Clients are
> redirected via IPtables.
> 
> I have tried to modify ssl_bump options:
> 
> 1) ssl_bump stare all 2) ssl_bump peek all 3) ssl_bump bump all
> 
> etc., but still only IPs are shown.
> 
> Could you please tell, where it is I'm mistaken?
> 
> -- Regards _______________________________________________ 
> squid-users mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+FfDAAoJENNXIZxhPexGLmMH/3MjjYeePFyclBUoiGUtDzni
H2FIyG094emo3q+kLFEHPdBgd923WdCpieG68E+8JThEuXtaYM7p4yp58kFfS4d1
1DZ4sWOwIesWWDq24fUpix8sBnQEmLQ8bMfLuwB5dyqmxQUaIhJuFkb3AmbTDR3y
kxZj71RPsajuKjDhLFWOoK6PNNwf0jITlXYck/TQDYZR0icsihlIHKNN+XqhaLBR
oASarWj9WorXT3LrEBzD+Q9EKtAI4FgPFh1L++oKT1K6Cnbst9KkRlDLDVvqE7Jl
Pa8VJvFTvkHN1Lm1Uhz1308h0AWIV9VCAXwYABywMVeKO0wkwp9vibNNcxjyhvU=
=zWC0
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Thu Mar  5 13:22:06 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 19:22:06 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
Message-ID: <54F8587E.5030401@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf

05.03.15 18:19, Monah Baki ?????:
> Hi all, can anyone verify if this is correct, need to make ure that
> users will be able to access the internet via the squid.
> 
> Running FreeBSD with a single interface with Squid-3.5.2
> 
> Policy based routing on Cisco with the following:
> 
> 
> interface GigabitEthernet0/0/1.1
> 
> encapsulation dot1Q 1 native
> 
> ip address 10.0.0.9 255.255.255.0
> 
> no ip redirects
> 
> no ip unreachables
> 
> ip nat inside
> 
> standby 1 ip 10.0.0.10
> 
> standby 1 priority 120
> 
> standby 1 preempt
> 
> standby 1 name HSRP
> 
> ip policy route-map CFLOW
> 
> 
> 
> ip access-list extended REDIRECT
> 
> deny   tcp host 10.0.0.24 any eq www
> 
> permit tcp host 10.0.0.23 any eq www
> 
> 
> 
> route-map CFLOW permit 10
> 
> match ip address REDIRECT set ip next-hop 10.0.0.24
> 
> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> port 80 -> 10.0.0.24 port 3129
> 
> # block in pass in log quick on bge0 pass out log quick on bge0 
> pass out keep state
> 
> and finally in my squid.conf: http_port 3128 http_port 3129
> intercept
> 
> 
> 
> And for testing purposes from the squid server: ./squidclient -h
> 10.0.0.24 -p 3128 http://www.freebsd.org/
> 
> If I replace -p 3128 with -p 80, I get a access denied, and if I
> omit the -p 3128 completely, I can access the websites.
> 
> tcpdump with (-p 3128)
> 
> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http:
> Flags [.], ack 17377, win 1018, options [nop,nop,TS val 985588797
> ecr 1054387720], length 0 13:15:02.681421 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448 13:15:02.681575 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448
> 
> 
> 
> Did I miss anything?
> 
> Thanks Monah
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+Fh+AAoJENNXIZxhPexGs9gH/jBGW/mUBLI4AYYVdm49CNIt
axPo0KREAP506ONNVpFqrCSoMCs1zxNR55g2S6XCi/BKAf/dgcv/AB7VAjTPBVeM
iw1lgZ9a1ki5xRpJruYWJIobswjqVckk5BPDZCodI5d2tUeO8G1AzIpiZvIYazJC
ykfA+tEFMR/SUjV3VZ8xB8kOhsJ1jZR08k+WtaWR5weweLLjiSlsoJ2yHqKSoPhu
rpuC+WQ52z6y+4by7FH0wrGKcgag97b2ZFrVClP6AH+4D3hi1yvtuWCmmTMsCfi0
MgrvesyjnkFCV6lUV83LZTbamtXejAVLlpYQ/kt9+KEUsF7Ee3zf/lMKjJIJUrs=
=KYeZ
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 13:36:26 2015
From: monahbaki at gmail.com (monahbaki at gmail.com)
Date: Thu, 05 Mar 2015 08:36:26 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F8587E.5030401@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com>
Message-ID: <20150305133626.6066327.33117.30595@gmail.com>

Yes that's what I followed and user is getting a "access denied" from the squid when he tries www.cnn.com

Sent from my BlackBerry 10 smartphone on the Verizon Wireless 4G LTE network.
? Original Message ?
From: Yuri Voinov
Sent: Thursday, March 5, 2015 8:22 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid intercept config

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf

05.03.15 18:19, Monah Baki ?????:
> Hi all, can anyone verify if this is correct, need to make ure that
> users will be able to access the internet via the squid.
> 
> Running FreeBSD with a single interface with Squid-3.5.2
> 
> Policy based routing on Cisco with the following:
> 
> 
> interface GigabitEthernet0/0/1.1
> 
> encapsulation dot1Q 1 native
> 
> ip address 10.0.0.9 255.255.255.0
> 
> no ip redirects
> 
> no ip unreachables
> 
> ip nat inside
> 
> standby 1 ip 10.0.0.10
> 
> standby 1 priority 120
> 
> standby 1 preempt
> 
> standby 1 name HSRP
> 
> ip policy route-map CFLOW
> 
> 
> 
> ip access-list extended REDIRECT
> 
> deny tcp host 10.0.0.24 any eq www
> 
> permit tcp host 10.0.0.23 any eq www
> 
> 
> 
> route-map CFLOW permit 10
> 
> match ip address REDIRECT set ip next-hop 10.0.0.24
> 
> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> port 80 -> 10.0.0.24 port 3129
> 
> # block in pass in log quick on bge0 pass out log quick on bge0 
> pass out keep state
> 
> and finally in my squid.conf: http_port 3128 http_port 3129
> intercept
> 
> 
> 
> And for testing purposes from the squid server: ./squidclient -h
> 10.0.0.24 -p 3128 http://www.freebsd.org/
> 
> If I replace -p 3128 with -p 80, I get a access denied, and if I
> omit the -p 3128 completely, I can access the websites.
> 
> tcpdump with (-p 3128)
> 
> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http:
> Flags [.], ack 17377, win 1018, options [nop,nop,TS val 985588797
> ecr 1054387720], length 0 13:15:02.681421 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448 13:15:02.681575 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448
> 
> 
> 
> Did I miss anything?
> 
> Thanks Monah
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+Fh+AAoJENNXIZxhPexGs9gH/jBGW/mUBLI4AYYVdm49CNIt
axPo0KREAP506ONNVpFqrCSoMCs1zxNR55g2S6XCi/BKAf/dgcv/AB7VAjTPBVeM
iw1lgZ9a1ki5xRpJruYWJIobswjqVckk5BPDZCodI5d2tUeO8G1AzIpiZvIYazJC
ykfA+tEFMR/SUjV3VZ8xB8kOhsJ1jZR08k+WtaWR5weweLLjiSlsoJ2yHqKSoPhu
rpuC+WQ52z6y+4by7FH0wrGKcgag97b2ZFrVClP6AH+4D3hi1yvtuWCmmTMsCfi0
MgrvesyjnkFCV6lUV83LZTbamtXejAVLlpYQ/kt9+KEUsF7Ee3zf/lMKjJIJUrs=
=KYeZ
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Thu Mar  5 13:44:01 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 19:44:01 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <20150305133626.6066327.33117.30595@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
Message-ID: <54F85DA1.90403@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Squid access denied?

Look at this:

In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
>> port 80 -> 10.0.0.24 port 3129

Which port configured in Squid as intercept?

3129?

and 3128 is forwarding?

05.03.15 19:36, monahbaki at gmail.com ?????:
> Yes that's what I followed and user is getting a "access denied"
> from the squid when he tries www.cnn.com
> 
> Sent from my BlackBerry 10 smartphone on the Verizon Wireless 4G
> LTE network. Original Message From: Yuri Voinov Sent: Thursday,
> March 5, 2015 8:22 AM To: squid-users at lists.squid-cache.org 
> Subject: Re: [squid-users] squid intercept config
> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
>
> 
http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> 
> 05.03.15 18:19, Monah Baki ?????:
>> Hi all, can anyone verify if this is correct, need to make ure
>> that users will be able to access the internet via the squid.
> 
>> Running FreeBSD with a single interface with Squid-3.5.2
> 
>> Policy based routing on Cisco with the following:
> 
> 
>> interface GigabitEthernet0/0/1.1
> 
>> encapsulation dot1Q 1 native
> 
>> ip address 10.0.0.9 255.255.255.0
> 
>> no ip redirects
> 
>> no ip unreachables
> 
>> ip nat inside
> 
>> standby 1 ip 10.0.0.10
> 
>> standby 1 priority 120
> 
>> standby 1 preempt
> 
>> standby 1 name HSRP
> 
>> ip policy route-map CFLOW
> 
> 
> 
>> ip access-list extended REDIRECT
> 
>> deny tcp host 10.0.0.24 any eq www
> 
>> permit tcp host 10.0.0.23 any eq www
> 
> 
> 
>> route-map CFLOW permit 10
> 
>> match ip address REDIRECT set ip next-hop 10.0.0.24
> 
>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
>> any port 80 -> 10.0.0.24 port 3129
> 
>> # block in pass in log quick on bge0 pass out log quick on bge0 
>> pass out keep state
> 
>> and finally in my squid.conf: http_port 3128 http_port 3129 
>> intercept
> 
> 
> 
>> And for testing purposes from the squid server: ./squidclient -h 
>> 10.0.0.24 -p 3128 http://www.freebsd.org/
> 
>> If I replace -p 3128 with -p 80, I get a access denied, and if I 
>> omit the -p 3128 completely, I can access the websites.
> 
>> tcpdump with (-p 3128)
> 
>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win 1018,
>> options [nop,nop,TS val 985588797 ecr 1054387720], length 0
>> 13:15:02.681421 IP wfe0.ysv.freebsd.org.http >
>> ISN-PHC-CACHE.44017: Flags [.], seq 17377:18825, ack 289, win
>> 1040, options [nop,nop,TS val 1054387720 ecr 985588501], length
>> 1448 13:15:02.681575 IP wfe0.ysv.freebsd.org.http >
>> ISN-PHC-CACHE.44017: Flags [.], seq 18825:20273, ack 289, win
>> 1040, options [nop,nop,TS val 1054387720 ecr 985588501], length
>> 1448
> 
> 
> 
>> Did I miss anything?
> 
>> Thanks Monah
> 
> 
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+F2gAAoJENNXIZxhPexGivEH/jh0uoMFUNiqROuSVfnCbd4F
pzcgm//4M3CRFCCGYT+u7VA14Uw5EPz/3vIiOQZFWrZLt9zZdtIlHqPA0ucBi5U5
cfHwlOhAXWMihM0gUYCATWit6c+cY9bvFS9wHzav9RJK8aRFWGczBhPLfFMGV8/y
WTgnCh3ViR3ZjilLhM3MB1nd4pNzn01BM9X3rteGu5d1zh6hznyEIqMAzUXFcBeF
cnsWPnXkhU/r13X7zk0K6nF9tSaSIvbYJQaTWRl5DvkYVwQgCcPUwQ5yleWh70Ex
MycgylzjEqCAO4rqpYwV/v8/meb8+QzgK3e1KFRXDz91/zUz8LGO0ns7LzhAKFM=
=ZRtj
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 13:45:18 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 08:45:18 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F85DA1.90403@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
 <54F85DA1.90403@gmail.com>
Message-ID: <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>

In my squid.conf

http_port 3128
http_port 3129 intercept

Thanks

On Thu, Mar 5, 2015 at 8:44 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Squid access denied?
>
> Look at this:
>
> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> >> port 80 -> 10.0.0.24 port 3129
>
> Which port configured in Squid as intercept?
>
> 3129?
>
> and 3128 is forwarding?
>
> 05.03.15 19:36, monahbaki at gmail.com ?????:
> > Yes that's what I followed and user is getting a "access denied"
> > from the squid when he tries www.cnn.com
> >
> > Sent from my BlackBerry 10 smartphone on the Verizon Wireless 4G
> > LTE network. Original Message From: Yuri Voinov Sent: Thursday,
> > March 5, 2015 8:22 AM To: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] squid intercept config
> >
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
> >
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> >
> > 05.03.15 18:19, Monah Baki ?????:
> >> Hi all, can anyone verify if this is correct, need to make ure
> >> that users will be able to access the internet via the squid.
> >
> >> Running FreeBSD with a single interface with Squid-3.5.2
> >
> >> Policy based routing on Cisco with the following:
> >
> >
> >> interface GigabitEthernet0/0/1.1
> >
> >> encapsulation dot1Q 1 native
> >
> >> ip address 10.0.0.9 255.255.255.0
> >
> >> no ip redirects
> >
> >> no ip unreachables
> >
> >> ip nat inside
> >
> >> standby 1 ip 10.0.0.10
> >
> >> standby 1 priority 120
> >
> >> standby 1 preempt
> >
> >> standby 1 name HSRP
> >
> >> ip policy route-map CFLOW
> >
> >
> >
> >> ip access-list extended REDIRECT
> >
> >> deny tcp host 10.0.0.24 any eq www
> >
> >> permit tcp host 10.0.0.23 any eq www
> >
> >
> >
> >> route-map CFLOW permit 10
> >
> >> match ip address REDIRECT set ip next-hop 10.0.0.24
> >
> >> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
> >> any port 80 -> 10.0.0.24 port 3129
> >
> >> # block in pass in log quick on bge0 pass out log quick on bge0
> >> pass out keep state
> >
> >> and finally in my squid.conf: http_port 3128 http_port 3129
> >> intercept
> >
> >
> >
> >> And for testing purposes from the squid server: ./squidclient -h
> >> 10.0.0.24 -p 3128 http://www.freebsd.org/
> >
> >> If I replace -p 3128 with -p 80, I get a access denied, and if I
> >> omit the -p 3128 completely, I can access the websites.
> >
> >> tcpdump with (-p 3128)
> >
> >> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win 1018,
> >> options [nop,nop,TS val 985588797 ecr 1054387720], length 0
> >> 13:15:02.681421 IP wfe0.ysv.freebsd.org.http >
> >> ISN-PHC-CACHE.44017: Flags [.], seq 17377:18825, ack 289, win
> >> 1040, options [nop,nop,TS val 1054387720 ecr 985588501], length
> >> 1448 13:15:02.681575 IP wfe0.ysv.freebsd.org.http >
> >> ISN-PHC-CACHE.44017: Flags [.], seq 18825:20273, ack 289, win
> >> 1040, options [nop,nop,TS val 1054387720 ecr 985588501], length
> >> 1448
> >
> >
> >
> >> Did I miss anything?
> >
> >> Thanks Monah
> >
> >
> >
> >> _______________________________________________ squid-users
> >> mailing list squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >
> > _______________________________________________ squid-users mailing
> > list squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+F2gAAoJENNXIZxhPexGivEH/jh0uoMFUNiqROuSVfnCbd4F
> pzcgm//4M3CRFCCGYT+u7VA14Uw5EPz/3vIiOQZFWrZLt9zZdtIlHqPA0ucBi5U5
> cfHwlOhAXWMihM0gUYCATWit6c+cY9bvFS9wHzav9RJK8aRFWGczBhPLfFMGV8/y
> WTgnCh3ViR3ZjilLhM3MB1nd4pNzn01BM9X3rteGu5d1zh6hznyEIqMAzUXFcBeF
> cnsWPnXkhU/r13X7zk0K6nF9tSaSIvbYJQaTWRl5DvkYVwQgCcPUwQ5yleWh70Ex
> MycgylzjEqCAO4rqpYwV/v8/meb8+QzgK3e1KFRXDz91/zUz8LGO0ns7LzhAKFM=
> =ZRtj
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/83d5a5af/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 13:50:12 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 19:50:12 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
 <54F85DA1.90403@gmail.com>
 <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>
Message-ID: <54F85F14.2070100@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Show complete pf.conf, please.

05.03.15 19:45, Monah Baki ?????:
> In my squid.conf
> 
> http_port 3128 http_port 3129 intercept
> 
> Thanks
> 
> On Thu, Mar 5, 2015 at 8:44 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Squid access denied?
> 
> Look at this:
> 
> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
>>>>> port 80 -> 10.0.0.24 port 3129
> 
> Which port configured in Squid as intercept?
> 
> 3129?
> 
> and 3128 is forwarding?
> 
> 05.03.15 19:36, monahbaki at gmail.com ?????:
>>>> Yes that's what I followed and user is getting a "access
>>>> denied" from the squid when he tries www.cnn.com
>>>> 
>>>> Sent from my BlackBerry 10 smartphone on the Verizon Wireless
>>>> 4G LTE network. Original Message From: Yuri Voinov Sent:
>>>> Thursday, March 5, 2015 8:22 AM To:
>>>> squid-users at lists.squid-cache.org Subject: Re: [squid-users]
>>>> squid intercept config
>>>> 
>>>> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
>>>>
>>>>
>
> 
http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
>>>> 
>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>> Hi all, can anyone verify if this is correct, need to make
>>>>> ure that users will be able to access the internet via the
>>>>> squid.
>>>> 
>>>>> Running FreeBSD with a single interface with Squid-3.5.2
>>>> 
>>>>> Policy based routing on Cisco with the following:
>>>> 
>>>> 
>>>>> interface GigabitEthernet0/0/1.1
>>>> 
>>>>> encapsulation dot1Q 1 native
>>>> 
>>>>> ip address 10.0.0.9 255.255.255.0
>>>> 
>>>>> no ip redirects
>>>> 
>>>>> no ip unreachables
>>>> 
>>>>> ip nat inside
>>>> 
>>>>> standby 1 ip 10.0.0.10
>>>> 
>>>>> standby 1 priority 120
>>>> 
>>>>> standby 1 preempt
>>>> 
>>>>> standby 1 name HSRP
>>>> 
>>>>> ip policy route-map CFLOW
>>>> 
>>>> 
>>>> 
>>>>> ip access-list extended REDIRECT
>>>> 
>>>>> deny tcp host 10.0.0.24 any eq www
>>>> 
>>>>> permit tcp host 10.0.0.23 any eq www
>>>> 
>>>> 
>>>> 
>>>>> route-map CFLOW permit 10
>>>> 
>>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
>>>> 
>>>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8
>>>>> to any port 80 -> 10.0.0.24 port 3129
>>>> 
>>>>> # block in pass in log quick on bge0 pass out log quick on
>>>>> bge0 pass out keep state
>>>> 
>>>>> and finally in my squid.conf: http_port 3128 http_port
>>>>> 3129 intercept
>>>> 
>>>> 
>>>> 
>>>>> And for testing purposes from the squid server:
>>>>> ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
>>>> 
>>>>> If I replace -p 3128 with -p 80, I get a access denied, and
>>>>> if I omit the -p 3128 completely, I can access the
>>>>> websites.
>>>> 
>>>>> tcpdump with (-p 3128)
>>>> 
>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > 
>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win 1018, 
>>>>> options [nop,nop,TS val 985588797 ecr 1054387720], length
>>>>> 0 13:15:02.681421 IP wfe0.ysv.freebsd.org.http > 
>>>>> ISN-PHC-CACHE.44017: Flags [.], seq 17377:18825, ack 289,
>>>>> win 1040, options [nop,nop,TS val 1054387720 ecr
>>>>> 985588501], length 1448 13:15:02.681575 IP
>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.],
>>>>> seq 18825:20273, ack 289, win 1040, options [nop,nop,TS val
>>>>> 1054387720 ecr 985588501], length 1448
>>>> 
>>>> 
>>>> 
>>>>> Did I miss anything?
>>>> 
>>>>> Thanks Monah
>>>> 
>>>> 
>>>> 
>>>>> _______________________________________________
>>>>> squid-users mailing list squid-users at lists.squid-cache.org 
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>>>> _______________________________________________ squid-users
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+F8UAAoJENNXIZxhPexGUd0H/ikmReyo7lGbuMVZelLLdawa
mtKS3N+dfyVWDT6LCGlgJaWFYV8N0Xqvf3dUv73xkrr3Gqoh6pQIVDdUJOObOC/7
/yX9qIPfHxz8pic18Hm3/RCwoeSzXp75JgD8LMy2xkOxto+Gvx3pFBBfMyViBYz9
VTCumGjDvx7pVlcO8MlmZ86jdSvBoEpLYi8J9rjD+11UKhA5mzy8gqzC8OCCTLvc
mP9NcUfvIFPFIW//SyzS+X1DiM/fGJ/jFsJ6QVxU8oY///zpHWyXE9oYZzZ62DqA
2VtPKduIu2NVZ/ibbnPI4CEU52Ct0uz2scbC1ZEvSqrkfYklg+RGDPj3ckcwGMU=
=xmOu
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 13:51:34 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 08:51:34 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F85F14.2070100@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
 <54F85DA1.90403@gmail.com>
 <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>
 <54F85F14.2070100@gmail.com>
Message-ID: <CALP3=x8LiHhYuUu0Zdf90uB9nvtqxUfE9NJxEu7MfnKYpifNLg@mail.gmail.com>

rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
3129

# block in
pass in log quick on bge0
pass out log quick on bge0
pass out keep state


Thanks

On Thu, Mar 5, 2015 at 8:50 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Show complete pf.conf, please.
>
> 05.03.15 19:45, Monah Baki ?????:
> > In my squid.conf
> >
> > http_port 3128 http_port 3129 intercept
> >
> > Thanks
> >
> > On Thu, Mar 5, 2015 at 8:44 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Squid access denied?
> >
> > Look at this:
> >
> > In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> >>>>> port 80 -> 10.0.0.24 port 3129
> >
> > Which port configured in Squid as intercept?
> >
> > 3129?
> >
> > and 3128 is forwarding?
> >
> > 05.03.15 19:36, monahbaki at gmail.com ?????:
> >>>> Yes that's what I followed and user is getting a "access
> >>>> denied" from the squid when he tries www.cnn.com
> >>>>
> >>>> Sent from my BlackBerry 10 smartphone on the Verizon Wireless
> >>>> 4G LTE network. Original Message From: Yuri Voinov Sent:
> >>>> Thursday, March 5, 2015 8:22 AM To:
> >>>> squid-users at lists.squid-cache.org Subject: Re: [squid-users]
> >>>> squid intercept config
> >>>>
> >>>>
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
> >>>>
> >>>>
> >
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> >>>>
> >>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>> Hi all, can anyone verify if this is correct, need to make
> >>>>> ure that users will be able to access the internet via the
> >>>>> squid.
> >>>>
> >>>>> Running FreeBSD with a single interface with Squid-3.5.2
> >>>>
> >>>>> Policy based routing on Cisco with the following:
> >>>>
> >>>>
> >>>>> interface GigabitEthernet0/0/1.1
> >>>>
> >>>>> encapsulation dot1Q 1 native
> >>>>
> >>>>> ip address 10.0.0.9 255.255.255.0
> >>>>
> >>>>> no ip redirects
> >>>>
> >>>>> no ip unreachables
> >>>>
> >>>>> ip nat inside
> >>>>
> >>>>> standby 1 ip 10.0.0.10
> >>>>
> >>>>> standby 1 priority 120
> >>>>
> >>>>> standby 1 preempt
> >>>>
> >>>>> standby 1 name HSRP
> >>>>
> >>>>> ip policy route-map CFLOW
> >>>>
> >>>>
> >>>>
> >>>>> ip access-list extended REDIRECT
> >>>>
> >>>>> deny tcp host 10.0.0.24 any eq www
> >>>>
> >>>>> permit tcp host 10.0.0.23 any eq www
> >>>>
> >>>>
> >>>>
> >>>>> route-map CFLOW permit 10
> >>>>
> >>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
> >>>>
> >>>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8
> >>>>> to any port 80 -> 10.0.0.24 port 3129
> >>>>
> >>>>> # block in pass in log quick on bge0 pass out log quick on
> >>>>> bge0 pass out keep state
> >>>>
> >>>>> and finally in my squid.conf: http_port 3128 http_port
> >>>>> 3129 intercept
> >>>>
> >>>>
> >>>>
> >>>>> And for testing purposes from the squid server:
> >>>>> ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
> >>>>
> >>>>> If I replace -p 3128 with -p 80, I get a access denied, and
> >>>>> if I omit the -p 3128 completely, I can access the
> >>>>> websites.
> >>>>
> >>>>> tcpdump with (-p 3128)
> >>>>
> >>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win 1018,
> >>>>> options [nop,nop,TS val 985588797 ecr 1054387720], length
> >>>>> 0 13:15:02.681421 IP wfe0.ysv.freebsd.org.http >
> >>>>> ISN-PHC-CACHE.44017: Flags [.], seq 17377:18825, ack 289,
> >>>>> win 1040, options [nop,nop,TS val 1054387720 ecr
> >>>>> 985588501], length 1448 13:15:02.681575 IP
> >>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.],
> >>>>> seq 18825:20273, ack 289, win 1040, options [nop,nop,TS val
> >>>>> 1054387720 ecr 985588501], length 1448
> >>>>
> >>>>
> >>>>
> >>>>> Did I miss anything?
> >>>>
> >>>>> Thanks Monah
> >>>>
> >>>>
> >>>>
> >>>>> _______________________________________________
> >>>>> squid-users mailing list squid-users at lists.squid-cache.org
> >>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>>> _______________________________________________ squid-users
> >>>> mailing list squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+F8UAAoJENNXIZxhPexGUd0H/ikmReyo7lGbuMVZelLLdawa
> mtKS3N+dfyVWDT6LCGlgJaWFYV8N0Xqvf3dUv73xkrr3Gqoh6pQIVDdUJOObOC/7
> /yX9qIPfHxz8pic18Hm3/RCwoeSzXp75JgD8LMy2xkOxto+Gvx3pFBBfMyViBYz9
> VTCumGjDvx7pVlcO8MlmZ86jdSvBoEpLYi8J9rjD+11UKhA5mzy8gqzC8OCCTLvc
> mP9NcUfvIFPFIW//SyzS+X1DiM/fGJ/jFsJ6QVxU8oY///zpHWyXE9oYZzZ62DqA
> 2VtPKduIu2NVZ/ibbnPI4CEU52Ct0uz2scbC1ZEvSqrkfYklg+RGDPj3ckcwGMU=
> =xmOu
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/e28bf2d6/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 13:54:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 19:54:29 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x8LiHhYuUu0Zdf90uB9nvtqxUfE9NJxEu7MfnKYpifNLg@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
 <54F85DA1.90403@gmail.com>
 <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>
 <54F85F14.2070100@gmail.com>
 <CALP3=x8LiHhYuUu0Zdf90uB9nvtqxUfE9NJxEu7MfnKYpifNLg@mail.gmail.com>
Message-ID: <54F86015.3080002@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Looking good.

Can I take look onto your squid.conf? Without comment lines and
sensitive info?

05.03.15 19:51, Monah Baki ?????:
> rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24
> port 3129
> 
> # block in pass in log quick on bge0 pass out log quick on bge0 
> pass out keep state
> 
> 
> Thanks
> 
> On Thu, Mar 5, 2015 at 8:50 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Show complete pf.conf, please.
> 
> 05.03.15 19:45, Monah Baki ?????:
>>>> In my squid.conf
>>>> 
>>>> http_port 3128 http_port 3129 intercept
>>>> 
>>>> Thanks
>>>> 
>>>> On Thu, Mar 5, 2015 at 8:44 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Squid access denied?
>>>> 
>>>> Look at this:
>>>> 
>>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
>>>> any
>>>>>>>> port 80 -> 10.0.0.24 port 3129
>>>> 
>>>> Which port configured in Squid as intercept?
>>>> 
>>>> 3129?
>>>> 
>>>> and 3128 is forwarding?
>>>> 
>>>> 05.03.15 19:36, monahbaki at gmail.com ?????:
>>>>>>> Yes that's what I followed and user is getting a
>>>>>>> "access denied" from the squid when he tries
>>>>>>> www.cnn.com
>>>>>>> 
>>>>>>> Sent from my BlackBerry 10 smartphone on the Verizon
>>>>>>> Wireless 4G LTE network. Original Message From: Yuri
>>>>>>> Voinov Sent: Thursday, March 5, 2015 8:22 AM To: 
>>>>>>> squid-users at lists.squid-cache.org Subject: Re:
>>>>>>> [squid-users] squid intercept config
>>>>>>> 
>>>>>>> 
>>>> 
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
>>>>>>>
>>>>>>>
>>>>
>>>>
>
> 
http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
>>>>>>> 
>>>>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>>>>> Hi all, can anyone verify if this is correct, need to
>>>>>>>> make ure that users will be able to access the
>>>>>>>> internet via the squid.
>>>>>>> 
>>>>>>>> Running FreeBSD with a single interface with
>>>>>>>> Squid-3.5.2
>>>>>>> 
>>>>>>>> Policy based routing on Cisco with the following:
>>>>>>> 
>>>>>>> 
>>>>>>>> interface GigabitEthernet0/0/1.1
>>>>>>> 
>>>>>>>> encapsulation dot1Q 1 native
>>>>>>> 
>>>>>>>> ip address 10.0.0.9 255.255.255.0
>>>>>>> 
>>>>>>>> no ip redirects
>>>>>>> 
>>>>>>>> no ip unreachables
>>>>>>> 
>>>>>>>> ip nat inside
>>>>>>> 
>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>> 
>>>>>>>> standby 1 priority 120
>>>>>>> 
>>>>>>>> standby 1 preempt
>>>>>>> 
>>>>>>>> standby 1 name HSRP
>>>>>>> 
>>>>>>>> ip policy route-map CFLOW
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>> ip access-list extended REDIRECT
>>>>>>> 
>>>>>>>> deny tcp host 10.0.0.24 any eq www
>>>>>>> 
>>>>>>>> permit tcp host 10.0.0.23 any eq www
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>> route-map CFLOW permit 10
>>>>>>> 
>>>>>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
>>>>>>> 
>>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp from
>>>>>>>> 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129
>>>>>>> 
>>>>>>>> # block in pass in log quick on bge0 pass out log
>>>>>>>> quick on bge0 pass out keep state
>>>>>>> 
>>>>>>>> and finally in my squid.conf: http_port 3128
>>>>>>>> http_port 3129 intercept
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>> And for testing purposes from the squid server: 
>>>>>>>> ./squidclient -h 10.0.0.24 -p 3128
>>>>>>>> http://www.freebsd.org/
>>>>>>> 
>>>>>>>> If I replace -p 3128 with -p 80, I get a access
>>>>>>>> denied, and if I omit the -p 3128 completely, I can
>>>>>>>> access the websites.
>>>>>>> 
>>>>>>>> tcpdump with (-p 3128)
>>>>>>> 
>>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > 
>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win
>>>>>>>> 1018, options [nop,nop,TS val 985588797 ecr
>>>>>>>> 1054387720], length 0 13:15:02.681421 IP
>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
>>>>>>>> Flags [.], seq 17377:18825, ack 289, win 1040,
>>>>>>>> options [nop,nop,TS val 1054387720 ecr 985588501],
>>>>>>>> length 1448 13:15:02.681575 IP 
>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
>>>>>>>> Flags [.], seq 18825:20273, ack 289, win 1040,
>>>>>>>> options [nop,nop,TS val 1054387720 ecr 985588501],
>>>>>>>> length 1448
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>> Did I miss anything?
>>>>>>> 
>>>>>>>> Thanks Monah
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>> _______________________________________________ 
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+GAUAAoJENNXIZxhPexGCrkH/11tb2r+PvgODC7XyDfA1WUE
zyHTj3ZJ3HU+i9cpGZ8d/n+xWv6R09y+opC6WG0KVNlKIpqzNBSBjp4xKuMB1mAh
M83J38n8Mm38AoOKtNmFq4jipsEkWCo4m/PAWu0h0rRty9HGB+CV8ZSSAQyl4TJg
FY7vembnCRxJT6lDwE5QSWDxeCZUOEPNakonBblvQ6cAcUnhjOHpTVSICBkraNA+
u8jcS1mHST9d64YzVrssGSd1yrVKEVHJPylyXiftGi9hEwhKWivmv2fsJ6LgRMlM
7cXtnxPPiLe0/C4uwnLVdTSJGO6njZ61r8LRHaOT5qrM32aZbqZzDyG2yrXopXk=
=n7R1
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 14:03:28 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 09:03:28 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F86015.3080002@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F8587E.5030401@gmail.com> <20150305133626.6066327.33117.30595@gmail.com>
 <54F85DA1.90403@gmail.com>
 <CALP3=x-JAWdfyFHVpAfBGf=5LZDoNNGAVaag59By2QqfEM1MnQ@mail.gmail.com>
 <54F85F14.2070100@gmail.com>
 <CALP3=x8LiHhYuUu0Zdf90uB9nvtqxUfE9NJxEu7MfnKYpifNLg@mail.gmail.com>
 <54F86015.3080002@gmail.com>
Message-ID: <CALP3=x_UOWNx6pPEa1-BqQXTMbZEMDXmpXeLSkcd+jEkP45A1g@mail.gmail.com>

Sure, here it is, very simple


#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl snmpcheck snmp_community public

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access allow manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
acl manager url_regex -i ^cache_object:// /squid-internal-mgr/

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

snmp_access allow snmpcheck localhost


# And finally deny all other access to this proxy
http_access deny all
snmp_access deny all

# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept
snmp_port 3401

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /cache/squid/var/cache/squid 350000 16 256

# Leave coredumps in the first cache dir
coredump_dir /cache/squid/var/cache/squid

strip_query_terms off


#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

half_closed_clients off
quick_abort_min 0 KB
quick_abort_max 0 KB
vary_ignore_expire on
reload_into_ims on
memory_pools off
cache_mem 4096 MB
memory_cache_shared on
minimum_object_size 0 bytes
maximum_object_size 512 MB
maximum_object_size 512 KB
ipcache_size 1024
ipcache_low 90
ipcache_high 95
cache_swap_low 98
cache_swap_high 100
fqdncache_size 16384
retry_on_error on
offline_mode off
pipeline_prefetch on
logfile_rotate 10
dns_nameservers 8.8.8.8 41.78.211.30


On Thu, Mar 5, 2015 at 8:54 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Looking good.
>
> Can I take look onto your squid.conf? Without comment lines and
> sensitive info?
>
> 05.03.15 19:51, Monah Baki ?????:
> > rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24
> > port 3129
> >
> > # block in pass in log quick on bge0 pass out log quick on bge0
> > pass out keep state
> >
> >
> > Thanks
> >
> > On Thu, Mar 5, 2015 at 8:50 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Show complete pf.conf, please.
> >
> > 05.03.15 19:45, Monah Baki ?????:
> >>>> In my squid.conf
> >>>>
> >>>> http_port 3128 http_port 3129 intercept
> >>>>
> >>>> Thanks
> >>>>
> >>>> On Thu, Mar 5, 2015 at 8:44 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> Squid access denied?
> >>>>
> >>>> Look at this:
> >>>>
> >>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
> >>>> any
> >>>>>>>> port 80 -> 10.0.0.24 port 3129
> >>>>
> >>>> Which port configured in Squid as intercept?
> >>>>
> >>>> 3129?
> >>>>
> >>>> and 3128 is forwarding?
> >>>>
> >>>> 05.03.15 19:36, monahbaki at gmail.com ?????:
> >>>>>>> Yes that's what I followed and user is getting a
> >>>>>>> "access denied" from the squid when he tries
> >>>>>>> www.cnn.com
> >>>>>>>
> >>>>>>> Sent from my BlackBerry 10 smartphone on the Verizon
> >>>>>>> Wireless 4G LTE network. Original Message From: Yuri
> >>>>>>> Voinov Sent: Thursday, March 5, 2015 8:22 AM To:
> >>>>>>> squid-users at lists.squid-cache.org Subject: Re:
> >>>>>>> [squid-users] squid intercept config
> >>>>>>>
> >>>>>>>
> >>>>
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/Cisco2501PolicyRoute
> >>>>>>>
> >>>>>>>
> >>>>
> >>>>
> >
> >
> http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> >>>>>>>
> >>>>>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>>> Hi all, can anyone verify if this is correct, need to
> >>>>>>>> make ure that users will be able to access the
> >>>>>>>> internet via the squid.
> >>>>>>>
> >>>>>>>> Running FreeBSD with a single interface with
> >>>>>>>> Squid-3.5.2
> >>>>>>>
> >>>>>>>> Policy based routing on Cisco with the following:
> >>>>>>>
> >>>>>>>
> >>>>>>>> interface GigabitEthernet0/0/1.1
> >>>>>>>
> >>>>>>>> encapsulation dot1Q 1 native
> >>>>>>>
> >>>>>>>> ip address 10.0.0.9 255.255.255.0
> >>>>>>>
> >>>>>>>> no ip redirects
> >>>>>>>
> >>>>>>>> no ip unreachables
> >>>>>>>
> >>>>>>>> ip nat inside
> >>>>>>>
> >>>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>
> >>>>>>>> standby 1 priority 120
> >>>>>>>
> >>>>>>>> standby 1 preempt
> >>>>>>>
> >>>>>>>> standby 1 name HSRP
> >>>>>>>
> >>>>>>>> ip policy route-map CFLOW
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> ip access-list extended REDIRECT
> >>>>>>>
> >>>>>>>> deny tcp host 10.0.0.24 any eq www
> >>>>>>>
> >>>>>>>> permit tcp host 10.0.0.23 any eq www
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> route-map CFLOW permit 10
> >>>>>>>
> >>>>>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
> >>>>>>>
> >>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp from
> >>>>>>>> 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129
> >>>>>>>
> >>>>>>>> # block in pass in log quick on bge0 pass out log
> >>>>>>>> quick on bge0 pass out keep state
> >>>>>>>
> >>>>>>>> and finally in my squid.conf: http_port 3128
> >>>>>>>> http_port 3129 intercept
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> And for testing purposes from the squid server:
> >>>>>>>> ./squidclient -h 10.0.0.24 -p 3128
> >>>>>>>> http://www.freebsd.org/
> >>>>>>>
> >>>>>>>> If I replace -p 3128 with -p 80, I get a access
> >>>>>>>> denied, and if I omit the -p 3128 completely, I can
> >>>>>>>> access the websites.
> >>>>>>>
> >>>>>>>> tcpdump with (-p 3128)
> >>>>>>>
> >>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win
> >>>>>>>> 1018, options [nop,nop,TS val 985588797 ecr
> >>>>>>>> 1054387720], length 0 13:15:02.681421 IP
> >>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
> >>>>>>>> Flags [.], seq 17377:18825, ack 289, win 1040,
> >>>>>>>> options [nop,nop,TS val 1054387720 ecr 985588501],
> >>>>>>>> length 1448 13:15:02.681575 IP
> >>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
> >>>>>>>> Flags [.], seq 18825:20273, ack 289, win 1040,
> >>>>>>>> options [nop,nop,TS val 1054387720 ecr 985588501],
> >>>>>>>> length 1448
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> Did I miss anything?
> >>>>>>>
> >>>>>>>> Thanks Monah
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> _______________________________________________
> >>>>>>>> squid-users mailing list
> >>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>
> >>>>>>> _______________________________________________
> >>>>>>> squid-users mailing list
> >>>>>>> squid-users at lists.squid-cache.org
> >>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+GAUAAoJENNXIZxhPexGCrkH/11tb2r+PvgODC7XyDfA1WUE
> zyHTj3ZJ3HU+i9cpGZ8d/n+xWv6R09y+opC6WG0KVNlKIpqzNBSBjp4xKuMB1mAh
> M83J38n8Mm38AoOKtNmFq4jipsEkWCo4m/PAWu0h0rRty9HGB+CV8ZSSAQyl4TJg
> FY7vembnCRxJT6lDwE5QSWDxeCZUOEPNakonBblvQ6cAcUnhjOHpTVSICBkraNA+
> u8jcS1mHST9d64YzVrssGSd1yrVKEVHJPylyXiftGi9hEwhKWivmv2fsJ6LgRMlM
> 7cXtnxPPiLe0/C4uwnLVdTSJGO6njZ61r8LRHaOT5qrM32aZbqZzDyG2yrXopXk=
> =n7R1
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/9ebc3af8/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 14:14:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 20:14:22 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
Message-ID: <54F864BE.4010901@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

This looking good too.

Stupid question:

With witch interception option squid builed?

I.e, squid -v?

05.03.15 18:19, Monah Baki ?????:
> Hi all, can anyone verify if this is correct, need to make ure that
> users will be able to access the internet via the squid.
> 
> Running FreeBSD with a single interface with Squid-3.5.2
> 
> Policy based routing on Cisco with the following:
> 
> 
> interface GigabitEthernet0/0/1.1
> 
> encapsulation dot1Q 1 native
> 
> ip address 10.0.0.9 255.255.255.0
> 
> no ip redirects
> 
> no ip unreachables
> 
> ip nat inside
> 
> standby 1 ip 10.0.0.10
> 
> standby 1 priority 120
> 
> standby 1 preempt
> 
> standby 1 name HSRP
> 
> ip policy route-map CFLOW
> 
> 
> 
> ip access-list extended REDIRECT
> 
> deny   tcp host 10.0.0.24 any eq www
> 
> permit tcp host 10.0.0.23 any eq www
> 
> 
> 
> route-map CFLOW permit 10
> 
> match ip address REDIRECT set ip next-hop 10.0.0.24
> 
> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> port 80 -> 10.0.0.24 port 3129
> 
> # block in pass in log quick on bge0 pass out log quick on bge0 
> pass out keep state
> 
> and finally in my squid.conf: http_port 3128 http_port 3129
> intercept
> 
> 
> 
> And for testing purposes from the squid server: ./squidclient -h
> 10.0.0.24 -p 3128 http://www.freebsd.org/
> 
> If I replace -p 3128 with -p 80, I get a access denied, and if I
> omit the -p 3128 completely, I can access the websites.
> 
> tcpdump with (-p 3128)
> 
> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http:
> Flags [.], ack 17377, win 1018, options [nop,nop,TS val 985588797
> ecr 1054387720], length 0 13:15:02.681421 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448 13:15:02.681575 IP
> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720 
> ecr 985588501], length 1448
> 
> 
> 
> Did I miss anything?
> 
> Thanks Monah
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+GS+AAoJENNXIZxhPexGb+8H/R/S58piXzwHUnfmDWEiBD1H
8qID7tliv+MaY2AEGKwr/vCU5d6z2wknXGL/kTk5QV+O4fvdVW9iftSDLfu+jL4F
FKXn38yT+ALUiKeb3239Pd16Z1c/sdhjELDuY6zN7EmQ1Bhw2hW+48UUFptASNJ4
RDAGrKhhwj5l5j8TFn9U25PKgAr7+W4PWgVcQiYW+sYaKTjmr5YYBhOkH7zLIB3G
ZRYb6pJFzLzDTX3NSrwVip1i1k4yRtxVvVjkoEkG042f+q8hX4CI4hGC7NloIuoa
qTIGXVJTzD912p9UBsBJsDgG/tyb/MlTrC0SWcrDOp2SZcfo29bNExSYxeQATQI=
=MZ5a
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 14:15:51 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 09:15:51 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F864BE.4010901@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
Message-ID: <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>

 '--prefix=/cache/squid' '--enable-follow-x-forwarded-for'
'--with-large-files' '--enable-ssl' '--disable-ipv6' '--enable-esi'
'--enable-kill-parent-hack' '--enable-snmp' '--with-pthreads'
'--with-filedescriptors=65535' '--enable-cachemgr-hostname=hostname'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-ipfw-transparent'
'--enable-pf-transparent' '--with-nat-devpf' --enable-ltdl-convenience




On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> This looking good too.
>
> Stupid question:
>
> With witch interception option squid builed?
>
> I.e, squid -v?
>
> 05.03.15 18:19, Monah Baki ?????:
> > Hi all, can anyone verify if this is correct, need to make ure that
> > users will be able to access the internet via the squid.
> >
> > Running FreeBSD with a single interface with Squid-3.5.2
> >
> > Policy based routing on Cisco with the following:
> >
> >
> > interface GigabitEthernet0/0/1.1
> >
> > encapsulation dot1Q 1 native
> >
> > ip address 10.0.0.9 255.255.255.0
> >
> > no ip redirects
> >
> > no ip unreachables
> >
> > ip nat inside
> >
> > standby 1 ip 10.0.0.10
> >
> > standby 1 priority 120
> >
> > standby 1 preempt
> >
> > standby 1 name HSRP
> >
> > ip policy route-map CFLOW
> >
> >
> >
> > ip access-list extended REDIRECT
> >
> > deny   tcp host 10.0.0.24 any eq www
> >
> > permit tcp host 10.0.0.23 any eq www
> >
> >
> >
> > route-map CFLOW permit 10
> >
> > match ip address REDIRECT set ip next-hop 10.0.0.24
> >
> > In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to any
> > port 80 -> 10.0.0.24 port 3129
> >
> > # block in pass in log quick on bge0 pass out log quick on bge0
> > pass out keep state
> >
> > and finally in my squid.conf: http_port 3128 http_port 3129
> > intercept
> >
> >
> >
> > And for testing purposes from the squid server: ./squidclient -h
> > 10.0.0.24 -p 3128 http://www.freebsd.org/
> >
> > If I replace -p 3128 with -p 80, I get a access denied, and if I
> > omit the -p 3128 completely, I can access the websites.
> >
> > tcpdump with (-p 3128)
> >
> > 13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http:
> > Flags [.], ack 17377, win 1018, options [nop,nop,TS val 985588797
> > ecr 1054387720], length 0 13:15:02.681421 IP
> > wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> > 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720
> > ecr 985588501], length 1448 13:15:02.681575 IP
> > wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags [.], seq
> > 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720
> > ecr 985588501], length 1448
> >
> >
> >
> > Did I miss anything?
> >
> > Thanks Monah
> >
> >
> >
> > _______________________________________________ squid-users mailing
> > list squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+GS+AAoJENNXIZxhPexGb+8H/R/S58piXzwHUnfmDWEiBD1H
> 8qID7tliv+MaY2AEGKwr/vCU5d6z2wknXGL/kTk5QV+O4fvdVW9iftSDLfu+jL4F
> FKXn38yT+ALUiKeb3239Pd16Z1c/sdhjELDuY6zN7EmQ1Bhw2hW+48UUFptASNJ4
> RDAGrKhhwj5l5j8TFn9U25PKgAr7+W4PWgVcQiYW+sYaKTjmr5YYBhOkH7zLIB3G
> ZRYb6pJFzLzDTX3NSrwVip1i1k4yRtxVvVjkoEkG042f+q8hX4CI4hGC7NloIuoa
> qTIGXVJTzD912p9UBsBJsDgG/tyb/MlTrC0SWcrDOp2SZcfo29bNExSYxeQATQI=
> =MZ5a
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/01e45b9b/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 14:23:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 20:23:54 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
Message-ID: <54F866FA.2030809@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

10.0.0.23 is your host? And 10.0.0.24 is proxy box?

05.03.15 20:15, Monah Baki ?????:
> '--prefix=/cache/squid' '--enable-follow-x-forwarded-for' 
> '--with-large-files' '--enable-ssl' '--disable-ipv6'
> '--enable-esi' '--enable-kill-parent-hack' '--enable-snmp'
> '--with-pthreads' '--with-filedescriptors=65535'
> '--enable-cachemgr-hostname=hostname' 
> '--enable-storeio=ufs,aufs,diskd,rock' '--enable-ipfw-transparent' 
> '--enable-pf-transparent' '--with-nat-devpf'
> --enable-ltdl-convenience
> 
> 
> 
> 
> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> This looking good too.
> 
> Stupid question:
> 
> With witch interception option squid builed?
> 
> I.e, squid -v?
> 
> 05.03.15 18:19, Monah Baki ?????:
>>>> Hi all, can anyone verify if this is correct, need to make
>>>> ure that users will be able to access the internet via the
>>>> squid.
>>>> 
>>>> Running FreeBSD with a single interface with Squid-3.5.2
>>>> 
>>>> Policy based routing on Cisco with the following:
>>>> 
>>>> 
>>>> interface GigabitEthernet0/0/1.1
>>>> 
>>>> encapsulation dot1Q 1 native
>>>> 
>>>> ip address 10.0.0.9 255.255.255.0
>>>> 
>>>> no ip redirects
>>>> 
>>>> no ip unreachables
>>>> 
>>>> ip nat inside
>>>> 
>>>> standby 1 ip 10.0.0.10
>>>> 
>>>> standby 1 priority 120
>>>> 
>>>> standby 1 preempt
>>>> 
>>>> standby 1 name HSRP
>>>> 
>>>> ip policy route-map CFLOW
>>>> 
>>>> 
>>>> 
>>>> ip access-list extended REDIRECT
>>>> 
>>>> deny   tcp host 10.0.0.24 any eq www
>>>> 
>>>> permit tcp host 10.0.0.23 any eq www
>>>> 
>>>> 
>>>> 
>>>> route-map CFLOW permit 10
>>>> 
>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
>>>> 
>>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
>>>> any port 80 -> 10.0.0.24 port 3129
>>>> 
>>>> # block in pass in log quick on bge0 pass out log quick on
>>>> bge0 pass out keep state
>>>> 
>>>> and finally in my squid.conf: http_port 3128 http_port 3129 
>>>> intercept
>>>> 
>>>> 
>>>> 
>>>> And for testing purposes from the squid server: ./squidclient
>>>> -h 10.0.0.24 -p 3128 http://www.freebsd.org/
>>>> 
>>>> If I replace -p 3128 with -p 80, I get a access denied, and
>>>> if I omit the -p 3128 completely, I can access the websites.
>>>> 
>>>> tcpdump with (-p 3128)
>>>> 
>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win 1018,
>>>> options [nop,nop,TS val 985588797 ecr 1054387720], length 0
>>>> 13:15:02.681421 IP wfe0.ysv.freebsd.org.http >
>>>> ISN-PHC-CACHE.44017: Flags [.], seq 17377:18825, ack 289, win
>>>> 1040, options [nop,nop,TS val 1054387720 ecr 985588501],
>>>> length 1448 13:15:02.681575 IP wfe0.ysv.freebsd.org.http >
>>>> ISN-PHC-CACHE.44017: Flags [.], seq 18825:20273, ack 289, win
>>>> 1040, options [nop,nop,TS val 1054387720 ecr 985588501],
>>>> length 1448
>>>> 
>>>> 
>>>> 
>>>> Did I miss anything?
>>>> 
>>>> Thanks Monah
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________ squid-users
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+Gb6AAoJENNXIZxhPexGZ0sIAKg4iDx7Vm4imHddvGYss5su
AKb0wk0E5tJBRXDH+Mlv+rRAe5CKCqFmNQHe4CMcm5XF3PBSlSKwD6Ih/Mnjtn4m
+6qk/GOWYACyb7NhGsif57VL6b4AHkqVF3gBZjuNiR/9gMhUYcOHGIdvGX/RLn+z
m/gUjA4Ef0JNaflgy48z12ECSvs6RMQzB186i4zm6KoEzFethL/3UhHiLrrDjry+
wB1Rwr8wx3pzbu53WQAS57aGpcp7n0gI7VLwvjh2M6wIetlVLwqWUQu87r0HmvQ5
duoaGplxlCYx7QKZ4L3Q74HH/8taojWxLakCQump1PCTUofWCUy0sAgkxKPCdHw=
=HWEF
-----END PGP SIGNATURE-----


From johnzeng2013 at yahoo.com  Thu Mar  5 14:56:27 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 05 Mar 2015 22:56:27 +0800
Subject: [squid-users] When i browse some webpage ( www.ifeng.com) via proxy
 ( squid.3.5.2) .There are some error info ( Content Encoding Error )
Message-ID: <54F86E9B.4080901@yahoo.com>


Hi all :

When i browse some webpage ( www.ifeng.com) via proxy ( squid.3.5.2) ,

There are some error info from firefox browser .

if possible , please give some advisement .

---------------------------------------------------

Content Encoding Error

The page you are trying to view cannot be shown because it uses an
invalid or unsupported form of compression.

* Please contact the website owners to inform them of this problem.

-----------------------------------------------------


From monahbaki at gmail.com  Thu Mar  5 15:09:04 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 10:09:04 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F868B6.4000208@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
Message-ID: <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>

PORT   STATE SERVICE VERSION
23/tcp open  telnet  Cisco IOS telnetd
MAC Address: 88:5A:92:63:77:81 (Cisco)
Device type: router
Running: Cisco IOS 12.X
OS CPE: cpe:/h:cisco:7600_router cpe:/o:cisco:ios:12.2
OS details: Cisco 7600 router (IOS 12.2)
Network Distance: 1 hop
TCP Sequence Prediction: Difficulty=258 (Good luck!)
IP ID Sequence Generation: Randomized
Service Info: OS: IOS; Device: switch; CPE: cpe:/o:cisco:ios


On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> What is Cisco model and iOS version?
>
> 05.03.15 20:25, Monah Baki ?????:
> > Yes, correct
> >
> > On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > 10.0.0.23 is your host? And 10.0.0.24 is proxy box?
> >
> > 05.03.15 20:15, Monah Baki ?????:
> >>>> '--prefix=/cache/squid' '--enable-follow-x-forwarded-for'
> >>>> '--with-large-files' '--enable-ssl' '--disable-ipv6'
> >>>> '--enable-esi' '--enable-kill-parent-hack' '--enable-snmp'
> >>>> '--with-pthreads' '--with-filedescriptors=65535'
> >>>> '--enable-cachemgr-hostname=hostname'
> >>>> '--enable-storeio=ufs,aufs,diskd,rock'
> >>>> '--enable-ipfw-transparent' '--enable-pf-transparent'
> >>>> '--with-nat-devpf' --enable-ltdl-convenience
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> This looking good too.
> >>>>
> >>>> Stupid question:
> >>>>
> >>>> With witch interception option squid builed?
> >>>>
> >>>> I.e, squid -v?
> >>>>
> >>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>> Hi all, can anyone verify if this is correct, need to
> >>>>>>> make ure that users will be able to access the internet
> >>>>>>> via the squid.
> >>>>>>>
> >>>>>>> Running FreeBSD with a single interface with
> >>>>>>> Squid-3.5.2
> >>>>>>>
> >>>>>>> Policy based routing on Cisco with the following:
> >>>>>>>
> >>>>>>>
> >>>>>>> interface GigabitEthernet0/0/1.1
> >>>>>>>
> >>>>>>> encapsulation dot1Q 1 native
> >>>>>>>
> >>>>>>> ip address 10.0.0.9 255.255.255.0
> >>>>>>>
> >>>>>>> no ip redirects
> >>>>>>>
> >>>>>>> no ip unreachables
> >>>>>>>
> >>>>>>> ip nat inside
> >>>>>>>
> >>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>
> >>>>>>> standby 1 priority 120
> >>>>>>>
> >>>>>>> standby 1 preempt
> >>>>>>>
> >>>>>>> standby 1 name HSRP
> >>>>>>>
> >>>>>>> ip policy route-map CFLOW
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> ip access-list extended REDIRECT
> >>>>>>>
> >>>>>>> deny   tcp host 10.0.0.24 any eq www
> >>>>>>>
> >>>>>>> permit tcp host 10.0.0.23 any eq www
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> route-map CFLOW permit 10
> >>>>>>>
> >>>>>>> match ip address REDIRECT set ip next-hop 10.0.0.24
> >>>>>>>
> >>>>>>> In my /etc/pf.conf rdr pass inet proto tcp from
> >>>>>>> 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129
> >>>>>>>
> >>>>>>> # block in pass in log quick on bge0 pass out log quick
> >>>>>>> on bge0 pass out keep state
> >>>>>>>
> >>>>>>> and finally in my squid.conf: http_port 3128 http_port
> >>>>>>> 3129 intercept
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> And for testing purposes from the squid server:
> >>>>>>> ./squidclient -h 10.0.0.24 -p 3128
> >>>>>>> http://www.freebsd.org/
> >>>>>>>
> >>>>>>> If I replace -p 3128 with -p 80, I get a access denied,
> >>>>>>> and if I omit the -p 3128 completely, I can access the
> >>>>>>> websites.
> >>>>>>>
> >>>>>>> tcpdump with (-p 3128)
> >>>>>>>
> >>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377, win
> >>>>>>> 1018, options [nop,nop,TS val 985588797 ecr
> >>>>>>> 1054387720], length 0 13:15:02.681421 IP
> >>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags
> >>>>>>> [.], seq 17377:18825, ack 289, win 1040, options
> >>>>>>> [nop,nop,TS val 1054387720 ecr 985588501], length 1448
> >>>>>>> 13:15:02.681575 IP wfe0.ysv.freebsd.org.http >
> >>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq 18825:20273, ack
> >>>>>>> 289, win 1040, options [nop,nop,TS val 1054387720 ecr
> >>>>>>> 985588501], length 1448
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> Did I miss anything?
> >>>>>>>
> >>>>>>> Thanks Monah
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> _______________________________________________
> >>>>>>> squid-users mailing list
> >>>>>>> squid-users at lists.squid-cache.org
> >>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>
> >>>>> _______________________________________________
> >>>>> squid-users mailing list squid-users at lists.squid-cache.org
> >>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+Gi2AAoJENNXIZxhPexG5ZsH/jmkZq8gHsA0LSfGBS4PcIb8
> fghHqg00FH48WqCWo4WtagehI1MFU7wdtxERhDJWyO2a3kaOg6i8BfWgC0Cnu3WX
> AMRMs8z2f2Foz9o1UgLMb3LLLQOuXjioCHq0RKFaW5TD70Fqk14CHNCDOSTaHEMv
> e/65hh4aGek1f5wxAGCfPGoX7tTtiH0DZ/XBZ7YOlAqrl0wcAYLI+1gTq2zOk98Y
> Q6zu9whBqKeZvBXMxu8aLyxan3RZcGDeSDlF89l+jwNw+6enHgVElsU22+1eQ54P
> IxmIAaJEbNmSFisj1wb+XRV5SsdjN8etQcBgh5rCz4TWqh9UgsW1K2BonLH4yns=
> =hEYd
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/d91a8ae0/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 15:14:21 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 21:14:21 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
Message-ID: <54F872CD.4030403@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Wow, 7600!

But why is so antique iOS?! Current is 15.4

05.03.15 21:09, Monah Baki ?????:
> PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco IOS
> telnetd MAC Address: 88:5A:92:63:77:81 (Cisco) Device type: router 
> Running: Cisco IOS 12.X OS CPE: cpe:/h:cisco:7600_router
> cpe:/o:cisco:ios:12.2 OS details: Cisco 7600 router (IOS 12.2) 
> Network Distance: 1 hop TCP Sequence Prediction: Difficulty=258
> (Good luck!) IP ID Sequence Generation: Randomized Service Info:
> OS: IOS; Device: switch; CPE: cpe:/o:cisco:ios
> 
> 
> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> What is Cisco model and iOS version?
> 
> 05.03.15 20:25, Monah Baki ?????:
>>>> Yes, correct
>>>> 
>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy box?
>>>> 
>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>> '--prefix=/cache/squid'
>>>>>>> '--enable-follow-x-forwarded-for' '--with-large-files'
>>>>>>> '--enable-ssl' '--disable-ipv6' '--enable-esi'
>>>>>>> '--enable-kill-parent-hack' '--enable-snmp' 
>>>>>>> '--with-pthreads' '--with-filedescriptors=65535' 
>>>>>>> '--enable-cachemgr-hostname=hostname' 
>>>>>>> '--enable-storeio=ufs,aufs,diskd,rock' 
>>>>>>> '--enable-ipfw-transparent' '--enable-pf-transparent' 
>>>>>>> '--with-nat-devpf' --enable-ltdl-convenience
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> This looking good too.
>>>>>>> 
>>>>>>> Stupid question:
>>>>>>> 
>>>>>>> With witch interception option squid builed?
>>>>>>> 
>>>>>>> I.e, squid -v?
>>>>>>> 
>>>>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>>>>>>> Hi all, can anyone verify if this is correct,
>>>>>>>>>> need to make ure that users will be able to
>>>>>>>>>> access the internet via the squid.
>>>>>>>>>> 
>>>>>>>>>> Running FreeBSD with a single interface with 
>>>>>>>>>> Squid-3.5.2
>>>>>>>>>> 
>>>>>>>>>> Policy based routing on Cisco with the
>>>>>>>>>> following:
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> interface GigabitEthernet0/0/1.1
>>>>>>>>>> 
>>>>>>>>>> encapsulation dot1Q 1 native
>>>>>>>>>> 
>>>>>>>>>> ip address 10.0.0.9 255.255.255.0
>>>>>>>>>> 
>>>>>>>>>> no ip redirects
>>>>>>>>>> 
>>>>>>>>>> no ip unreachables
>>>>>>>>>> 
>>>>>>>>>> ip nat inside
>>>>>>>>>> 
>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>> 
>>>>>>>>>> standby 1 priority 120
>>>>>>>>>> 
>>>>>>>>>> standby 1 preempt
>>>>>>>>>> 
>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>> 
>>>>>>>>>> ip policy route-map CFLOW
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> ip access-list extended REDIRECT
>>>>>>>>>> 
>>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
>>>>>>>>>> 
>>>>>>>>>> permit tcp host 10.0.0.23 any eq www
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> route-map CFLOW permit 10
>>>>>>>>>> 
>>>>>>>>>> match ip address REDIRECT set ip next-hop
>>>>>>>>>> 10.0.0.24
>>>>>>>>>> 
>>>>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp from 
>>>>>>>>>> 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129
>>>>>>>>>> 
>>>>>>>>>> # block in pass in log quick on bge0 pass out log
>>>>>>>>>> quick on bge0 pass out keep state
>>>>>>>>>> 
>>>>>>>>>> and finally in my squid.conf: http_port 3128
>>>>>>>>>> http_port 3129 intercept
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> And for testing purposes from the squid server: 
>>>>>>>>>> ./squidclient -h 10.0.0.24 -p 3128 
>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>> 
>>>>>>>>>> If I replace -p 3128 with -p 80, I get a access
>>>>>>>>>> denied, and if I omit the -p 3128 completely, I
>>>>>>>>>> can access the websites.
>>>>>>>>>> 
>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>> 
>>>>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > 
>>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377,
>>>>>>>>>> win 1018, options [nop,nop,TS val 985588797 ecr 
>>>>>>>>>> 1054387720], length 0 13:15:02.681421 IP 
>>>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
>>>>>>>>>> Flags [.], seq 17377:18825, ack 289, win 1040,
>>>>>>>>>> options [nop,nop,TS val 1054387720 ecr
>>>>>>>>>> 985588501], length 1448 13:15:02.681575 IP
>>>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
>>>>>>>>>> Flags [.], seq 18825:20273, ack 289, win 1040,
>>>>>>>>>> options [nop,nop,TS val 1054387720 ecr 
>>>>>>>>>> 985588501], length 1448
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Did I miss anything?
>>>>>>>>>> 
>>>>>>>>>> Thanks Monah
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> _______________________________________________ 
>>>>>>>>>> squid-users mailing list 
>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>
>>>>>>>>
>>>>>>>>>> 
_______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+HLNAAoJENNXIZxhPexGQi8IAIfEtSR4e/FsHLwRqf7ynMMq
tU6HhEyn1sce/YI+WfJ8gTGwBw5mbQr5WklK+3Rnkuq86ZFokVPahOXJg3lILD5I
D+VfWc0rNNP3VLWJeC9OnacwVMzT52Ij7YeNLR0KJPpWzCLjOGf0PyqaXJftnWjx
iT6CfeT+awwvKnogr1h3Cp/T4EDCwgTSSnjViaQjvFbFZa4MtJ1vWdCWoF0bSZic
5lmIc59Rb8VYTyFFjG4ZxSmZIK/xH+HDO4/IZhkL0CN1GmleALGiPFQ5szDCzIoB
7lXlN2M0usbXgREhu26gbHUV8716EN+Kgx2RsiFPREDPrqJmZLDSA0zv4FpN/n4=
=a+x7
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 15:16:59 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 10:16:59 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F872CD.4030403@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
Message-ID: <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>

Not sure why the client is running old hard/soft ware, could it be cause of
the hardware? Is FreeBSD an issue, should I switch to linux?

On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Wow, 7600!
>
> But why is so antique iOS?! Current is 15.4
>
> 05.03.15 21:09, Monah Baki ?????:
> > PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco IOS
> > telnetd MAC Address: 88:5A:92:63:77:81 (Cisco) Device type: router
> > Running: Cisco IOS 12.X OS CPE: cpe:/h:cisco:7600_router
> > cpe:/o:cisco:ios:12.2 OS details: Cisco 7600 router (IOS 12.2)
> > Network Distance: 1 hop TCP Sequence Prediction: Difficulty=258
> > (Good luck!) IP ID Sequence Generation: Randomized Service Info:
> > OS: IOS; Device: switch; CPE: cpe:/o:cisco:ios
> >
> >
> > On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > What is Cisco model and iOS version?
> >
> > 05.03.15 20:25, Monah Baki ?????:
> >>>> Yes, correct
> >>>>
> >>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy box?
> >>>>
> >>>> 05.03.15 20:15, Monah Baki ?????:
> >>>>>>> '--prefix=/cache/squid'
> >>>>>>> '--enable-follow-x-forwarded-for' '--with-large-files'
> >>>>>>> '--enable-ssl' '--disable-ipv6' '--enable-esi'
> >>>>>>> '--enable-kill-parent-hack' '--enable-snmp'
> >>>>>>> '--with-pthreads' '--with-filedescriptors=65535'
> >>>>>>> '--enable-cachemgr-hostname=hostname'
> >>>>>>> '--enable-storeio=ufs,aufs,diskd,rock'
> >>>>>>> '--enable-ipfw-transparent' '--enable-pf-transparent'
> >>>>>>> '--with-nat-devpf' --enable-ltdl-convenience
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov
> >>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>
> >>>>>>> This looking good too.
> >>>>>>>
> >>>>>>> Stupid question:
> >>>>>>>
> >>>>>>> With witch interception option squid builed?
> >>>>>>>
> >>>>>>> I.e, squid -v?
> >>>>>>>
> >>>>>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>>>>> Hi all, can anyone verify if this is correct,
> >>>>>>>>>> need to make ure that users will be able to
> >>>>>>>>>> access the internet via the squid.
> >>>>>>>>>>
> >>>>>>>>>> Running FreeBSD with a single interface with
> >>>>>>>>>> Squid-3.5.2
> >>>>>>>>>>
> >>>>>>>>>> Policy based routing on Cisco with the
> >>>>>>>>>> following:
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> interface GigabitEthernet0/0/1.1
> >>>>>>>>>>
> >>>>>>>>>> encapsulation dot1Q 1 native
> >>>>>>>>>>
> >>>>>>>>>> ip address 10.0.0.9 255.255.255.0
> >>>>>>>>>>
> >>>>>>>>>> no ip redirects
> >>>>>>>>>>
> >>>>>>>>>> no ip unreachables
> >>>>>>>>>>
> >>>>>>>>>> ip nat inside
> >>>>>>>>>>
> >>>>>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>>>>
> >>>>>>>>>> standby 1 priority 120
> >>>>>>>>>>
> >>>>>>>>>> standby 1 preempt
> >>>>>>>>>>
> >>>>>>>>>> standby 1 name HSRP
> >>>>>>>>>>
> >>>>>>>>>> ip policy route-map CFLOW
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> ip access-list extended REDIRECT
> >>>>>>>>>>
> >>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
> >>>>>>>>>>
> >>>>>>>>>> permit tcp host 10.0.0.23 any eq www
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> route-map CFLOW permit 10
> >>>>>>>>>>
> >>>>>>>>>> match ip address REDIRECT set ip next-hop
> >>>>>>>>>> 10.0.0.24
> >>>>>>>>>>
> >>>>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp from
> >>>>>>>>>> 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129
> >>>>>>>>>>
> >>>>>>>>>> # block in pass in log quick on bge0 pass out log
> >>>>>>>>>> quick on bge0 pass out keep state
> >>>>>>>>>>
> >>>>>>>>>> and finally in my squid.conf: http_port 3128
> >>>>>>>>>> http_port 3129 intercept
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> And for testing purposes from the squid server:
> >>>>>>>>>> ./squidclient -h 10.0.0.24 -p 3128
> >>>>>>>>>> http://www.freebsd.org/
> >>>>>>>>>>
> >>>>>>>>>> If I replace -p 3128 with -p 80, I get a access
> >>>>>>>>>> denied, and if I omit the -p 3128 completely, I
> >>>>>>>>>> can access the websites.
> >>>>>>>>>>
> >>>>>>>>>> tcpdump with (-p 3128)
> >>>>>>>>>>
> >>>>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack 17377,
> >>>>>>>>>> win 1018, options [nop,nop,TS val 985588797 ecr
> >>>>>>>>>> 1054387720], length 0 13:15:02.681421 IP
> >>>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
> >>>>>>>>>> Flags [.], seq 17377:18825, ack 289, win 1040,
> >>>>>>>>>> options [nop,nop,TS val 1054387720 ecr
> >>>>>>>>>> 985588501], length 1448 13:15:02.681575 IP
> >>>>>>>>>> wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017:
> >>>>>>>>>> Flags [.], seq 18825:20273, ack 289, win 1040,
> >>>>>>>>>> options [nop,nop,TS val 1054387720 ecr
> >>>>>>>>>> 985588501], length 1448
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> Did I miss anything?
> >>>>>>>>>>
> >>>>>>>>>> Thanks Monah
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> squid-users mailing list
> >>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>>>>>
> _______________________________________________
> >>>>>>>> squid-users mailing list
> >>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+HLNAAoJENNXIZxhPexGQi8IAIfEtSR4e/FsHLwRqf7ynMMq
> tU6HhEyn1sce/YI+WfJ8gTGwBw5mbQr5WklK+3Rnkuq86ZFokVPahOXJg3lILD5I
> D+VfWc0rNNP3VLWJeC9OnacwVMzT52Ij7YeNLR0KJPpWzCLjOGf0PyqaXJftnWjx
> iT6CfeT+awwvKnogr1h3Cp/T4EDCwgTSSnjViaQjvFbFZa4MtJ1vWdCWoF0bSZic
> 5lmIc59Rb8VYTyFFjG4ZxSmZIK/xH+HDO4/IZhkL0CN1GmleALGiPFQ5szDCzIoB
> 7lXlN2M0usbXgREhu26gbHUV8716EN+Kgx2RsiFPREDPrqJmZLDSA0zv4FpN/n4=
> =a+x7
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/a4e3035c/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 15:20:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 21:20:29 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
Message-ID: <54F8743D.8080105@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hm. No.

We not checked only OS.

Does your BSD really loads PF module?

05.03.15 21:16, Monah Baki ?????:
> Not sure why the client is running old hard/soft ware, could it be
> cause of the hardware? Is FreeBSD an issue, should I switch to
> linux?
> 
> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Wow, 7600!
> 
> But why is so antique iOS?! Current is 15.4
> 
> 05.03.15 21:09, Monah Baki ?????:
>>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco IOS 
>>>> telnetd MAC Address: 88:5A:92:63:77:81 (Cisco) Device type:
>>>> router Running: Cisco IOS 12.X OS CPE:
>>>> cpe:/h:cisco:7600_router cpe:/o:cisco:ios:12.2 OS details:
>>>> Cisco 7600 router (IOS 12.2) Network Distance: 1 hop TCP
>>>> Sequence Prediction: Difficulty=258 (Good luck!) IP ID
>>>> Sequence Generation: Randomized Service Info: OS: IOS;
>>>> Device: switch; CPE: cpe:/o:cisco:ios
>>>> 
>>>> 
>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> What is Cisco model and iOS version?
>>>> 
>>>> 05.03.15 20:25, Monah Baki ?????:
>>>>>>> Yes, correct
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy box?
>>>>>>> 
>>>>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>>>>> '--prefix=/cache/squid' 
>>>>>>>>>> '--enable-follow-x-forwarded-for'
>>>>>>>>>> '--with-large-files' '--enable-ssl'
>>>>>>>>>> '--disable-ipv6' '--enable-esi' 
>>>>>>>>>> '--enable-kill-parent-hack' '--enable-snmp' 
>>>>>>>>>> '--with-pthreads' '--with-filedescriptors=65535' 
>>>>>>>>>> '--enable-cachemgr-hostname=hostname' 
>>>>>>>>>> '--enable-storeio=ufs,aufs,diskd,rock' 
>>>>>>>>>> '--enable-ipfw-transparent'
>>>>>>>>>> '--enable-pf-transparent' '--with-nat-devpf'
>>>>>>>>>> --enable-ltdl-convenience
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov 
>>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> This looking good too.
>>>>>>>>>> 
>>>>>>>>>> Stupid question:
>>>>>>>>>> 
>>>>>>>>>> With witch interception option squid builed?
>>>>>>>>>> 
>>>>>>>>>> I.e, squid -v?
>>>>>>>>>> 
>>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>>>>>>>>>> Hi all, can anyone verify if this is
>>>>>>>>>>>>> correct, need to make ure that users will
>>>>>>>>>>>>> be able to access the internet via the
>>>>>>>>>>>>> squid.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Running FreeBSD with a single interface
>>>>>>>>>>>>> with Squid-3.5.2
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Policy based routing on Cisco with the 
>>>>>>>>>>>>> following:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> interface GigabitEthernet0/0/1.1
>>>>>>>>>>>>> 
>>>>>>>>>>>>> encapsulation dot1Q 1 native
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ip address 10.0.0.9 255.255.255.0
>>>>>>>>>>>>> 
>>>>>>>>>>>>> no ip redirects
>>>>>>>>>>>>> 
>>>>>>>>>>>>> no ip unreachables
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ip nat inside
>>>>>>>>>>>>> 
>>>>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>>>>> 
>>>>>>>>>>>>> standby 1 priority 120
>>>>>>>>>>>>> 
>>>>>>>>>>>>> standby 1 preempt
>>>>>>>>>>>>> 
>>>>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ip policy route-map CFLOW
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ip access-list extended REDIRECT
>>>>>>>>>>>>> 
>>>>>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
>>>>>>>>>>>>> 
>>>>>>>>>>>>> permit tcp host 10.0.0.23 any eq www
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> route-map CFLOW permit 10
>>>>>>>>>>>>> 
>>>>>>>>>>>>> match ip address REDIRECT set ip next-hop 
>>>>>>>>>>>>> 10.0.0.24
>>>>>>>>>>>>> 
>>>>>>>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp
>>>>>>>>>>>>> from 10.0.0.0/8 to any port 80 -> 10.0.0.24
>>>>>>>>>>>>> port 3129
>>>>>>>>>>>>> 
>>>>>>>>>>>>> # block in pass in log quick on bge0 pass
>>>>>>>>>>>>> out log quick on bge0 pass out keep state
>>>>>>>>>>>>> 
>>>>>>>>>>>>> and finally in my squid.conf: http_port
>>>>>>>>>>>>> 3128 http_port 3129 intercept
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> And for testing purposes from the squid
>>>>>>>>>>>>> server: ./squidclient -h 10.0.0.24 -p 3128 
>>>>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>>>>> 
>>>>>>>>>>>>> If I replace -p 3128 with -p 80, I get a
>>>>>>>>>>>>> access denied, and if I omit the -p 3128
>>>>>>>>>>>>> completely, I can access the websites.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 > 
>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack
>>>>>>>>>>>>> 17377, win 1018, options [nop,nop,TS val
>>>>>>>>>>>>> 985588797 ecr 1054387720], length 0
>>>>>>>>>>>>> 13:15:02.681421 IP 
>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
>>>>>>>>>>>>> 17377:18825, ack 289, win 1040, options
>>>>>>>>>>>>> [nop,nop,TS val 1054387720 ecr 985588501],
>>>>>>>>>>>>> length 1448 13:15:02.681575 IP 
>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
>>>>>>>>>>>>> 18825:20273, ack 289, win 1040, options
>>>>>>>>>>>>> [nop,nop,TS val 1054387720 ecr 985588501],
>>>>>>>>>>>>> length 1448
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Did I miss anything?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Thanks Monah
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>>
>>>>>>>>>>>>> 
squid-users mailing list
>>>>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>>>
>
>>>>>>>>>>>>> 
_______________________________________________
>>>>>>>>>>> squid-users mailing list 
>>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>
>>>>
>>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+HQ9AAoJENNXIZxhPexGzdkIAJKlI1/eYSwBD/vqY1nk+nRr
6PsBHLm8fVFU9iEOq+PXFOsm9F1L6Ak86+XI24BB/LYEzbrFGAIalu+BbZiS3dsR
Ym9MkaiKfJEqNz7ajKzyydRB4t6MFULGrqQXJeYUH0zUraTB1iJwn1fyXEoyssKW
4wet9JbFIbCZyNNzeXMHhpZ3bNy1/bbgHk+paBwJCCgvriiHW+Jn3L6Ppd0HvNXY
QD5P3YQrUuPVYxjZtkMCEwigv5cqV3aOyn9NzMwwjLrGSfogK529gA22QK5NJiMV
65yGags/Z++SfVTL+7MYnVOWkmYFExmMdIXHw9T4WZ8cMZQd2j3EkuiRw8EHf5M=
=0UyO
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 15:25:04 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 10:25:04 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F8743D.8080105@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
Message-ID: <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>

root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e -ttt -i pflog0
tcpdump: WARNING: pflog0: no IPv4 address assigned
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on pflog0, link-type PFLOG (OpenBSD pflog file), capture size
65535 bytes
capability mode sandbox enabled
00:00:00.000000 rule 0..16777216/0(match): pass in on bge0: 10.0.0.106.5678
> 255.255.255.255.5678: UDP, length 88
00:00:08.342860 rule 0..16777216/0(match): pass in on bge0: 10.0.0.14.54264
> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192, options [mss
1460,nop,wscale 2,nop,nop,sackOK], length 0



On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hm. No.
>
> We not checked only OS.
>
> Does your BSD really loads PF module?
>
> 05.03.15 21:16, Monah Baki ?????:
> > Not sure why the client is running old hard/soft ware, could it be
> > cause of the hardware? Is FreeBSD an issue, should I switch to
> > linux?
> >
> > On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Wow, 7600!
> >
> > But why is so antique iOS?! Current is 15.4
> >
> > 05.03.15 21:09, Monah Baki ?????:
> >>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco IOS
> >>>> telnetd MAC Address: 88:5A:92:63:77:81 (Cisco) Device type:
> >>>> router Running: Cisco IOS 12.X OS CPE:
> >>>> cpe:/h:cisco:7600_router cpe:/o:cisco:ios:12.2 OS details:
> >>>> Cisco 7600 router (IOS 12.2) Network Distance: 1 hop TCP
> >>>> Sequence Prediction: Difficulty=258 (Good luck!) IP ID
> >>>> Sequence Generation: Randomized Service Info: OS: IOS;
> >>>> Device: switch; CPE: cpe:/o:cisco:ios
> >>>>
> >>>>
> >>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> What is Cisco model and iOS version?
> >>>>
> >>>> 05.03.15 20:25, Monah Baki ?????:
> >>>>>>> Yes, correct
> >>>>>>>
> >>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov
> >>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>
> >>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy box?
> >>>>>>>
> >>>>>>> 05.03.15 20:15, Monah Baki ?????:
> >>>>>>>>>> '--prefix=/cache/squid'
> >>>>>>>>>> '--enable-follow-x-forwarded-for'
> >>>>>>>>>> '--with-large-files' '--enable-ssl'
> >>>>>>>>>> '--disable-ipv6' '--enable-esi'
> >>>>>>>>>> '--enable-kill-parent-hack' '--enable-snmp'
> >>>>>>>>>> '--with-pthreads' '--with-filedescriptors=65535'
> >>>>>>>>>> '--enable-cachemgr-hostname=hostname'
> >>>>>>>>>> '--enable-storeio=ufs,aufs,diskd,rock'
> >>>>>>>>>> '--enable-ipfw-transparent'
> >>>>>>>>>> '--enable-pf-transparent' '--with-nat-devpf'
> >>>>>>>>>> --enable-ltdl-convenience
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri Voinov
> >>>>>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> This looking good too.
> >>>>>>>>>>
> >>>>>>>>>> Stupid question:
> >>>>>>>>>>
> >>>>>>>>>> With witch interception option squid builed?
> >>>>>>>>>>
> >>>>>>>>>> I.e, squid -v?
> >>>>>>>>>>
> >>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>>>>>>>> Hi all, can anyone verify if this is
> >>>>>>>>>>>>> correct, need to make ure that users will
> >>>>>>>>>>>>> be able to access the internet via the
> >>>>>>>>>>>>> squid.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Running FreeBSD with a single interface
> >>>>>>>>>>>>> with Squid-3.5.2
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Policy based routing on Cisco with the
> >>>>>>>>>>>>> following:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> interface GigabitEthernet0/0/1.1
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> encapsulation dot1Q 1 native
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> ip address 10.0.0.9 255.255.255.0
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> no ip redirects
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> no ip unreachables
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> ip nat inside
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> standby 1 priority 120
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> standby 1 preempt
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> standby 1 name HSRP
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> ip policy route-map CFLOW
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> ip access-list extended REDIRECT
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> permit tcp host 10.0.0.23 any eq www
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> route-map CFLOW permit 10
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> match ip address REDIRECT set ip next-hop
> >>>>>>>>>>>>> 10.0.0.24
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> In my /etc/pf.conf rdr pass inet proto tcp
> >>>>>>>>>>>>> from 10.0.0.0/8 to any port 80 -> 10.0.0.24
> >>>>>>>>>>>>> port 3129
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> # block in pass in log quick on bge0 pass
> >>>>>>>>>>>>> out log quick on bge0 pass out keep state
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> and finally in my squid.conf: http_port
> >>>>>>>>>>>>> 3128 http_port 3129 intercept
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> And for testing purposes from the squid
> >>>>>>>>>>>>> server: ./squidclient -h 10.0.0.24 -p 3128
> >>>>>>>>>>>>> http://www.freebsd.org/
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> If I replace -p 3128 with -p 80, I get a
> >>>>>>>>>>>>> access denied, and if I omit the -p 3128
> >>>>>>>>>>>>> completely, I can access the websites.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> tcpdump with (-p 3128)
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 13:15:02.681106 IP ISN-PHC-CACHE.44017 >
> >>>>>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.], ack
> >>>>>>>>>>>>> 17377, win 1018, options [nop,nop,TS val
> >>>>>>>>>>>>> 985588797 ecr 1054387720], length 0
> >>>>>>>>>>>>> 13:15:02.681421 IP
> >>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
> >>>>>>>>>>>>> 17377:18825, ack 289, win 1040, options
> >>>>>>>>>>>>> [nop,nop,TS val 1054387720 ecr 985588501],
> >>>>>>>>>>>>> length 1448 13:15:02.681575 IP
> >>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
> >>>>>>>>>>>>> 18825:20273, ack 289, win 1040, options
> >>>>>>>>>>>>> [nop,nop,TS val 1054387720 ecr 985588501],
> >>>>>>>>>>>>> length 1448
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Did I miss anything?
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Thanks Monah
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> _______________________________________________
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> squid-users mailing list
> >>>>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>>>
> >
> >>>>>>>>>>>>>
> _______________________________________________
> >>>>>>>>>>> squid-users mailing list
> >>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+HQ9AAoJENNXIZxhPexGzdkIAJKlI1/eYSwBD/vqY1nk+nRr
> 6PsBHLm8fVFU9iEOq+PXFOsm9F1L6Ak86+XI24BB/LYEzbrFGAIalu+BbZiS3dsR
> Ym9MkaiKfJEqNz7ajKzyydRB4t6MFULGrqQXJeYUH0zUraTB1iJwn1fyXEoyssKW
> 4wet9JbFIbCZyNNzeXMHhpZ3bNy1/bbgHk+paBwJCCgvriiHW+Jn3L6Ppd0HvNXY
> QD5P3YQrUuPVYxjZtkMCEwigv5cqV3aOyn9NzMwwjLrGSfogK529gA22QK5NJiMV
> 65yGags/Z++SfVTL+7MYnVOWkmYFExmMdIXHw9T4WZ8cMZQd2j3EkuiRw8EHf5M=
> =0UyO
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/1860491d/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 16:12:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 22:12:55 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
Message-ID: <54F88087.5020907@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Does 80 port outside BSD-box listens?

05.03.15 21:25, Monah Baki ?????:
> root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e -ttt -i pflog0 
> tcpdump: WARNING: pflog0: no IPv4 address assigned tcpdump: verbose
> output suppressed, use -v or -vv for full protocol decode listening
> on pflog0, link-type PFLOG (OpenBSD pflog file), capture size 65535
> bytes capability mode sandbox enabled 00:00:00.000000 rule
> 0..16777216/0(match): pass in on bge0: 10.0.0.106.5678
>> 255.255.255.255.5678: UDP, length 88
> 00:00:08.342860 rule 0..16777216/0(match): pass in on bge0:
> 10.0.0.14.54264
>> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192, options [mss
> 1460,nop,wscale 2,nop,nop,sackOK], length 0
> 
> 
> 
> On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Hm. No.
> 
> We not checked only OS.
> 
> Does your BSD really loads PF module?
> 
> 05.03.15 21:16, Monah Baki ?????:
>>>> Not sure why the client is running old hard/soft ware, could
>>>> it be cause of the hardware? Is FreeBSD an issue, should I
>>>> switch to linux?
>>>> 
>>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Wow, 7600!
>>>> 
>>>> But why is so antique iOS?! Current is 15.4
>>>> 
>>>> 05.03.15 21:09, Monah Baki ?????:
>>>>>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco
>>>>>>> IOS telnetd MAC Address: 88:5A:92:63:77:81 (Cisco)
>>>>>>> Device type: router Running: Cisco IOS 12.X OS CPE: 
>>>>>>> cpe:/h:cisco:7600_router cpe:/o:cisco:ios:12.2 OS
>>>>>>> details: Cisco 7600 router (IOS 12.2) Network Distance:
>>>>>>> 1 hop TCP Sequence Prediction: Difficulty=258 (Good
>>>>>>> luck!) IP ID Sequence Generation: Randomized Service
>>>>>>> Info: OS: IOS; Device: switch; CPE: cpe:/o:cisco:ios
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> What is Cisco model and iOS version?
>>>>>>> 
>>>>>>> 05.03.15 20:25, Monah Baki ?????:
>>>>>>>>>> Yes, correct
>>>>>>>>>> 
>>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov 
>>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy
>>>>>>>>>> box?
>>>>>>>>>> 
>>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>>>>>>>> '--prefix=/cache/squid' 
>>>>>>>>>>>>> '--enable-follow-x-forwarded-for' 
>>>>>>>>>>>>> '--with-large-files' '--enable-ssl' 
>>>>>>>>>>>>> '--disable-ipv6' '--enable-esi' 
>>>>>>>>>>>>> '--enable-kill-parent-hack'
>>>>>>>>>>>>> '--enable-snmp' '--with-pthreads'
>>>>>>>>>>>>> '--with-filedescriptors=65535' 
>>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname' 
>>>>>>>>>>>>> '--enable-storeio=ufs,aufs,diskd,rock' 
>>>>>>>>>>>>> '--enable-ipfw-transparent' 
>>>>>>>>>>>>> '--enable-pf-transparent'
>>>>>>>>>>>>> '--with-nat-devpf' 
>>>>>>>>>>>>> --enable-ltdl-convenience
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri
>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> This looking good too.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Stupid question:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> With witch interception option squid
>>>>>>>>>>>>> builed?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> I.e, squid -v?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>>>>>>>>>>>>> Hi all, can anyone verify if this is 
>>>>>>>>>>>>>>>> correct, need to make ure that users
>>>>>>>>>>>>>>>> will be able to access the internet
>>>>>>>>>>>>>>>> via the squid.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Running FreeBSD with a single
>>>>>>>>>>>>>>>> interface with Squid-3.5.2
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Policy based routing on Cisco with
>>>>>>>>>>>>>>>> the following:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> interface GigabitEthernet0/0/1.1
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> encapsulation dot1Q 1 native
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> ip address 10.0.0.9 255.255.255.0
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> no ip redirects
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> no ip unreachables
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> ip nat inside
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> standby 1 priority 120
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> standby 1 preempt
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> ip policy route-map CFLOW
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> ip access-list extended REDIRECT
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> permit tcp host 10.0.0.23 any eq www
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> route-map CFLOW permit 10
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> match ip address REDIRECT set ip
>>>>>>>>>>>>>>>> next-hop 10.0.0.24
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> In my /etc/pf.conf rdr pass inet
>>>>>>>>>>>>>>>> proto tcp from 10.0.0.0/8 to any port
>>>>>>>>>>>>>>>> 80 -> 10.0.0.24 port 3129
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> # block in pass in log quick on bge0
>>>>>>>>>>>>>>>> pass out log quick on bge0 pass out
>>>>>>>>>>>>>>>> keep state
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> and finally in my squid.conf:
>>>>>>>>>>>>>>>> http_port 3128 http_port 3129
>>>>>>>>>>>>>>>> intercept
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> And for testing purposes from the
>>>>>>>>>>>>>>>> squid server: ./squidclient -h
>>>>>>>>>>>>>>>> 10.0.0.24 -p 3128 
>>>>>>>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> If I replace -p 3128 with -p 80, I
>>>>>>>>>>>>>>>> get a access denied, and if I omit
>>>>>>>>>>>>>>>> the -p 3128 completely, I can access
>>>>>>>>>>>>>>>> the websites.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 13:15:02.681106 IP
>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 > 
>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.],
>>>>>>>>>>>>>>>> ack 17377, win 1018, options
>>>>>>>>>>>>>>>> [nop,nop,TS val 985588797 ecr
>>>>>>>>>>>>>>>> 1054387720], length 0 13:15:02.681421
>>>>>>>>>>>>>>>> IP wfe0.ysv.freebsd.org.http > 
>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq 
>>>>>>>>>>>>>>>> 17377:18825, ack 289, win 1040,
>>>>>>>>>>>>>>>> options [nop,nop,TS val 1054387720
>>>>>>>>>>>>>>>> ecr 985588501], length 1448
>>>>>>>>>>>>>>>> 13:15:02.681575 IP 
>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http > 
>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq 
>>>>>>>>>>>>>>>> 18825:20273, ack 289, win 1040,
>>>>>>>>>>>>>>>> options [nop,nop,TS val 1054387720
>>>>>>>>>>>>>>>> ecr 985588501], length 1448
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Did I miss anything?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Thanks Monah
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>> 
squid-users mailing list
>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>
>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>> 
_______________________________________________
>>>>>>>>>>>>>> squid-users mailing list 
>>>>>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>
>>>>
>>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+ICHAAoJENNXIZxhPexGX9oH/ibNwg5eWJg+hfTfbNmxHNKb
D5b08bvJsbtbP8N5RVZQTnzKvLNOEnoVLHU22Zh9HDSy1OJ4fFyggNLFZPrAb09M
BH8URJSCjlMQudKXC5heZMaOu2V4idO70Cvif8I+pB1DcManxT3jldqYDFOClqjc
AV+NinCwEwxpcLjE1TL/a8MfVPjFw2EcI3P0Ne2IOwz1CwIsiTNcQr2kkX3MZq6b
S/ZbMygeHQvdNqMNxnv4o9ZdW0G/J7+2BCYly9Ks9mwOn7tA9qkueNTMXB/WAYXs
3355SSl69GVSMMJRCiAiXdm9x7flnPgN/kJUVgJ0sfWvhN+ahkbgDHph4qJWP3I=
=5Obg
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 17:10:23 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 12:10:23 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F88087.5020907@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
Message-ID: <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>

How can I confirm, I have access only to the BSD box

Thanks

On Thu, Mar 5, 2015 at 11:12 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Does 80 port outside BSD-box listens?
>
> 05.03.15 21:25, Monah Baki ?????:
> > root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e -ttt -i pflog0
> > tcpdump: WARNING: pflog0: no IPv4 address assigned tcpdump: verbose
> > output suppressed, use -v or -vv for full protocol decode listening
> > on pflog0, link-type PFLOG (OpenBSD pflog file), capture size 65535
> > bytes capability mode sandbox enabled 00:00:00.000000 rule
> > 0..16777216/0(match): pass in on bge0: 10.0.0.106.5678
> >> 255.255.255.255.5678: UDP, length 88
> > 00:00:08.342860 rule 0..16777216/0(match): pass in on bge0:
> > 10.0.0.14.54264
> >> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192, options [mss
> > 1460,nop,wscale 2,nop,nop,sackOK], length 0
> >
> >
> >
> > On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Hm. No.
> >
> > We not checked only OS.
> >
> > Does your BSD really loads PF module?
> >
> > 05.03.15 21:16, Monah Baki ?????:
> >>>> Not sure why the client is running old hard/soft ware, could
> >>>> it be cause of the hardware? Is FreeBSD an issue, should I
> >>>> switch to linux?
> >>>>
> >>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> Wow, 7600!
> >>>>
> >>>> But why is so antique iOS?! Current is 15.4
> >>>>
> >>>> 05.03.15 21:09, Monah Baki ?????:
> >>>>>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet  Cisco
> >>>>>>> IOS telnetd MAC Address: 88:5A:92:63:77:81 (Cisco)
> >>>>>>> Device type: router Running: Cisco IOS 12.X OS CPE:
> >>>>>>> cpe:/h:cisco:7600_router cpe:/o:cisco:ios:12.2 OS
> >>>>>>> details: Cisco 7600 router (IOS 12.2) Network Distance:
> >>>>>>> 1 hop TCP Sequence Prediction: Difficulty=258 (Good
> >>>>>>> luck!) IP ID Sequence Generation: Randomized Service
> >>>>>>> Info: OS: IOS; Device: switch; CPE: cpe:/o:cisco:ios
> >>>>>>>
> >>>>>>>
> >>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov
> >>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>
> >>>>>>> What is Cisco model and iOS version?
> >>>>>>>
> >>>>>>> 05.03.15 20:25, Monah Baki ?????:
> >>>>>>>>>> Yes, correct
> >>>>>>>>>>
> >>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri Voinov
> >>>>>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is proxy
> >>>>>>>>>> box?
> >>>>>>>>>>
> >>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
> >>>>>>>>>>>>> '--prefix=/cache/squid'
> >>>>>>>>>>>>> '--enable-follow-x-forwarded-for'
> >>>>>>>>>>>>> '--with-large-files' '--enable-ssl'
> >>>>>>>>>>>>> '--disable-ipv6' '--enable-esi'
> >>>>>>>>>>>>> '--enable-kill-parent-hack'
> >>>>>>>>>>>>> '--enable-snmp' '--with-pthreads'
> >>>>>>>>>>>>> '--with-filedescriptors=65535'
> >>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname'
> >>>>>>>>>>>>> '--enable-storeio=ufs,aufs,diskd,rock'
> >>>>>>>>>>>>> '--enable-ipfw-transparent'
> >>>>>>>>>>>>> '--enable-pf-transparent'
> >>>>>>>>>>>>> '--with-nat-devpf'
> >>>>>>>>>>>>> --enable-ltdl-convenience
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri
> >>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> This looking good too.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Stupid question:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> With witch interception option squid
> >>>>>>>>>>>>> builed?
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> I.e, squid -v?
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>>>>>>>>>>> Hi all, can anyone verify if this is
> >>>>>>>>>>>>>>>> correct, need to make ure that users
> >>>>>>>>>>>>>>>> will be able to access the internet
> >>>>>>>>>>>>>>>> via the squid.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Running FreeBSD with a single
> >>>>>>>>>>>>>>>> interface with Squid-3.5.2
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Policy based routing on Cisco with
> >>>>>>>>>>>>>>>> the following:
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> interface GigabitEthernet0/0/1.1
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> encapsulation dot1Q 1 native
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> ip address 10.0.0.9 255.255.255.0
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> no ip redirects
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> no ip unreachables
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> ip nat inside
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> standby 1 priority 120
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> standby 1 preempt
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> standby 1 name HSRP
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> ip policy route-map CFLOW
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> ip access-list extended REDIRECT
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24 any eq www
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> permit tcp host 10.0.0.23 any eq www
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> route-map CFLOW permit 10
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> match ip address REDIRECT set ip
> >>>>>>>>>>>>>>>> next-hop 10.0.0.24
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> In my /etc/pf.conf rdr pass inet
> >>>>>>>>>>>>>>>> proto tcp from 10.0.0.0/8 to any port
> >>>>>>>>>>>>>>>> 80 -> 10.0.0.24 port 3129
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> # block in pass in log quick on bge0
> >>>>>>>>>>>>>>>> pass out log quick on bge0 pass out
> >>>>>>>>>>>>>>>> keep state
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> and finally in my squid.conf:
> >>>>>>>>>>>>>>>> http_port 3128 http_port 3129
> >>>>>>>>>>>>>>>> intercept
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> And for testing purposes from the
> >>>>>>>>>>>>>>>> squid server: ./squidclient -h
> >>>>>>>>>>>>>>>> 10.0.0.24 -p 3128
> >>>>>>>>>>>>>>>> http://www.freebsd.org/
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> If I replace -p 3128 with -p 80, I
> >>>>>>>>>>>>>>>> get a access denied, and if I omit
> >>>>>>>>>>>>>>>> the -p 3128 completely, I can access
> >>>>>>>>>>>>>>>> the websites.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> tcpdump with (-p 3128)
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> 13:15:02.681106 IP
> >>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 >
> >>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http: Flags [.],
> >>>>>>>>>>>>>>>> ack 17377, win 1018, options
> >>>>>>>>>>>>>>>> [nop,nop,TS val 985588797 ecr
> >>>>>>>>>>>>>>>> 1054387720], length 0 13:15:02.681421
> >>>>>>>>>>>>>>>> IP wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
> >>>>>>>>>>>>>>>> 17377:18825, ack 289, win 1040,
> >>>>>>>>>>>>>>>> options [nop,nop,TS val 1054387720
> >>>>>>>>>>>>>>>> ecr 985588501], length 1448
> >>>>>>>>>>>>>>>> 13:15:02.681575 IP
> >>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.], seq
> >>>>>>>>>>>>>>>> 18825:20273, ack 289, win 1040,
> >>>>>>>>>>>>>>>> options [nop,nop,TS val 1054387720
> >>>>>>>>>>>>>>>> ecr 985588501], length 1448
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Did I miss anything?
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Thanks Monah
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> _______________________________________________
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >
> >>>>>>>>>>>>>>>>
> squid-users mailing list
> >>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>
> >>>>>>>>>>>>>>>>
> >
> >>>>>>>>>>>>>>>>
> _______________________________________________
> >>>>>>>>>>>>>> squid-users mailing list
> >>>>>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+ICHAAoJENNXIZxhPexGX9oH/ibNwg5eWJg+hfTfbNmxHNKb
> D5b08bvJsbtbP8N5RVZQTnzKvLNOEnoVLHU22Zh9HDSy1OJ4fFyggNLFZPrAb09M
> BH8URJSCjlMQudKXC5heZMaOu2V4idO70Cvif8I+pB1DcManxT3jldqYDFOClqjc
> AV+NinCwEwxpcLjE1TL/a8MfVPjFw2EcI3P0Ne2IOwz1CwIsiTNcQr2kkX3MZq6b
> S/ZbMygeHQvdNqMNxnv4o9ZdW0G/J7+2BCYly9Ks9mwOn7tA9qkueNTMXB/WAYXs
> 3355SSl69GVSMMJRCiAiXdm9x7flnPgN/kJUVgJ0sfWvhN+ahkbgDHph4qJWP3I=
> =5Obg
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/0efaa89e/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 17:12:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 05 Mar 2015 23:12:09 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
Message-ID: <54F88E69.1050804@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

- From your PC run telnet 10.0.0.24 80. You've seen if TCP socket opens.

05.03.15 23:10, Monah Baki ?????:
> How can I confirm, I have access only to the BSD box
> 
> Thanks
> 
> On Thu, Mar 5, 2015 at 11:12 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Does 80 port outside BSD-box listens?
> 
> 05.03.15 21:25, Monah Baki ?????:
>>>> root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e -ttt -i
>>>> pflog0 tcpdump: WARNING: pflog0: no IPv4 address assigned
>>>> tcpdump: verbose output suppressed, use -v or -vv for full
>>>> protocol decode listening on pflog0, link-type PFLOG (OpenBSD
>>>> pflog file), capture size 65535 bytes capability mode sandbox
>>>> enabled 00:00:00.000000 rule 0..16777216/0(match): pass in on
>>>> bge0: 10.0.0.106.5678
>>>>> 255.255.255.255.5678: UDP, length 88
>>>> 00:00:08.342860 rule 0..16777216/0(match): pass in on bge0: 
>>>> 10.0.0.14.54264
>>>>> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192, options
>>>>> [mss
>>>> 1460,nop,wscale 2,nop,nop,sackOK], length 0
>>>> 
>>>> 
>>>> 
>>>> On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Hm. No.
>>>> 
>>>> We not checked only OS.
>>>> 
>>>> Does your BSD really loads PF module?
>>>> 
>>>> 05.03.15 21:16, Monah Baki ?????:
>>>>>>> Not sure why the client is running old hard/soft ware,
>>>>>>> could it be cause of the hardware? Is FreeBSD an issue,
>>>>>>> should I switch to linux?
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> Wow, 7600!
>>>>>>> 
>>>>>>> But why is so antique iOS?! Current is 15.4
>>>>>>> 
>>>>>>> 05.03.15 21:09, Monah Baki ?????:
>>>>>>>>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet
>>>>>>>>>> Cisco IOS telnetd MAC Address: 88:5A:92:63:77:81
>>>>>>>>>> (Cisco) Device type: router Running: Cisco IOS
>>>>>>>>>> 12.X OS CPE: cpe:/h:cisco:7600_router
>>>>>>>>>> cpe:/o:cisco:ios:12.2 OS details: Cisco 7600
>>>>>>>>>> router (IOS 12.2) Network Distance: 1 hop TCP
>>>>>>>>>> Sequence Prediction: Difficulty=258 (Good luck!)
>>>>>>>>>> IP ID Sequence Generation: Randomized Service 
>>>>>>>>>> Info: OS: IOS; Device: switch; CPE:
>>>>>>>>>> cpe:/o:cisco:ios
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov 
>>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> What is Cisco model and iOS version?
>>>>>>>>>> 
>>>>>>>>>> 05.03.15 20:25, Monah Baki ?????:
>>>>>>>>>>>>> Yes, correct
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri
>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is
>>>>>>>>>>>>> proxy box?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>>>>>>>>>>> '--prefix=/cache/squid' 
>>>>>>>>>>>>>>>> '--enable-follow-x-forwarded-for' 
>>>>>>>>>>>>>>>> '--with-large-files' '--enable-ssl' 
>>>>>>>>>>>>>>>> '--disable-ipv6' '--enable-esi' 
>>>>>>>>>>>>>>>> '--enable-kill-parent-hack' 
>>>>>>>>>>>>>>>> '--enable-snmp' '--with-pthreads' 
>>>>>>>>>>>>>>>> '--with-filedescriptors=65535' 
>>>>>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname'
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>> 
'--enable-storeio=ufs,aufs,diskd,rock'
>>>>>>>>>>>>>>>> '--enable-ipfw-transparent' 
>>>>>>>>>>>>>>>> '--enable-pf-transparent' 
>>>>>>>>>>>>>>>> '--with-nat-devpf' 
>>>>>>>>>>>>>>>> --enable-ltdl-convenience
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri 
>>>>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> This looking good too.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Stupid question:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> With witch interception option squid 
>>>>>>>>>>>>>>>> builed?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> I.e, squid -v?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
>>>>>>>>>>>>>>>>>>> Hi all, can anyone verify if
>>>>>>>>>>>>>>>>>>> this is correct, need to make
>>>>>>>>>>>>>>>>>>> ure that users will be able to
>>>>>>>>>>>>>>>>>>> access the internet via the
>>>>>>>>>>>>>>>>>>> squid.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Running FreeBSD with a single 
>>>>>>>>>>>>>>>>>>> interface with Squid-3.5.2
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Policy based routing on Cisco
>>>>>>>>>>>>>>>>>>> with the following:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> interface
>>>>>>>>>>>>>>>>>>> GigabitEthernet0/0/1.1
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> encapsulation dot1Q 1 native
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> ip address 10.0.0.9
>>>>>>>>>>>>>>>>>>> 255.255.255.0
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> no ip redirects
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> no ip unreachables
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> ip nat inside
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> standby 1 priority 120
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> standby 1 preempt
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> ip policy route-map CFLOW
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> ip access-list extended
>>>>>>>>>>>>>>>>>>> REDIRECT
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24 any
>>>>>>>>>>>>>>>>>>> eq www
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> permit tcp host 10.0.0.23 any
>>>>>>>>>>>>>>>>>>> eq www
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> route-map CFLOW permit 10
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> match ip address REDIRECT set
>>>>>>>>>>>>>>>>>>> ip next-hop 10.0.0.24
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> In my /etc/pf.conf rdr pass
>>>>>>>>>>>>>>>>>>> inet proto tcp from 10.0.0.0/8
>>>>>>>>>>>>>>>>>>> to any port 80 -> 10.0.0.24
>>>>>>>>>>>>>>>>>>> port 3129
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> # block in pass in log quick on
>>>>>>>>>>>>>>>>>>> bge0 pass out log quick on bge0
>>>>>>>>>>>>>>>>>>> pass out keep state
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> and finally in my squid.conf: 
>>>>>>>>>>>>>>>>>>> http_port 3128 http_port 3129 
>>>>>>>>>>>>>>>>>>> intercept
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> And for testing purposes from
>>>>>>>>>>>>>>>>>>> the squid server: ./squidclient
>>>>>>>>>>>>>>>>>>> -h 10.0.0.24 -p 3128 
>>>>>>>>>>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> If I replace -p 3128 with -p
>>>>>>>>>>>>>>>>>>> 80, I get a access denied, and
>>>>>>>>>>>>>>>>>>> if I omit the -p 3128
>>>>>>>>>>>>>>>>>>> completely, I can access the
>>>>>>>>>>>>>>>>>>> websites.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 13:15:02.681106 IP 
>>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 > 
>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http:
>>>>>>>>>>>>>>>>>>> Flags [.], ack 17377, win 1018,
>>>>>>>>>>>>>>>>>>> options [nop,nop,TS val
>>>>>>>>>>>>>>>>>>> 985588797 ecr 1054387720],
>>>>>>>>>>>>>>>>>>> length 0 13:15:02.681421 IP
>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http > 
>>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.],
>>>>>>>>>>>>>>>>>>> seq 17377:18825, ack 289, win
>>>>>>>>>>>>>>>>>>> 1040, options [nop,nop,TS val
>>>>>>>>>>>>>>>>>>> 1054387720 ecr 985588501],
>>>>>>>>>>>>>>>>>>> length 1448 13:15:02.681575 IP 
>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http > 
>>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.],
>>>>>>>>>>>>>>>>>>> seq 18825:20273, ack 289, win
>>>>>>>>>>>>>>>>>>> 1040, options [nop,nop,TS val
>>>>>>>>>>>>>>>>>>> 1054387720 ecr 985588501],
>>>>>>>>>>>>>>>>>>> length 1448
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Did I miss anything?
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Thanks Monah
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>>>>
>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>> 
squid-users mailing list
>>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> 
http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>> 
>>>>>>>>>>>>>>>>>>> 
> _______________________________________________
>>>>>>>>>>>>>>>>> squid-users mailing list 
>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>
>>>>
>>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+I5pAAoJENNXIZxhPexG68cIAIjP0CaVSgjlexyNc4xATrws
SGS2hfPSOwPNJ32YVL43OJKtGLAMlumeo2oxJyZjdUFKftT5g7h8ldRKY3tPcV1m
lYo+lbS4FOA0MspbK9oGP+xiZhvB8jnxmLxM13+Rnmf1309rYhhIyL7y7yih02i/
KIulMyYwjZC8BvsCHRAjhxIiMQ0AJEO17hzs/6OCIkIjYNiswl9D8MbH0KOuKJMy
4BpYMpYnzDjfJAehwD7n1xnta3GvLhHbUL7fcYO9hvCX1mzZW/06nFpKQL/5pF06
CVmtTAtg85JlFESBlA9JHbDOV3AGn0bOC5KFtIqX3osFz46Ac+HqOV8P0kXm85k=
=eSQL
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Thu Mar  5 18:01:20 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 00:01:20 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
Message-ID: <54F899F0.8070909@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Good.

I don't see any 80 port listens.

This is root of problem.

PF does not work.

05.03.15 23:59, Monah Baki ?????:
> On 10.0.0.24
> 
> root at ISN-PHC-CACHE:/home/support # netstat -an Active Internet
> connections (including servers) Proto Recv-Q Send-Q Local Address
> Foreign Address        (state) tcp4       0     52 10.0.0.24.22
> 96.255.8.226.50911 ESTABLISHED tcp4       0      0 *.3129
> *.*                    LISTEN tcp4       0      0 *.3128
> *.*                    LISTEN tcp4       0      0 *.81
> *.*                    LISTEN tcp6       0      0 *.81
> *.*                    LISTEN tcp4       0      0 *.22
> *.*                    LISTEN tcp6       0      0 *.22
> *.*                    LISTEN tcp6       0      0 ::1.562
> ::1.40066 ESTABLISHED tcp6       0      0 ::1.40066
> ::1.562 ESTABLISHED tcp6       0      0 *.561                  *.*
> LISTEN tcp6       0      0 *.562                  *.*
> LISTEN tcp4       0      0 *.199                  *.*
> LISTEN tcp4       0      0 *.10000                *.*
> LISTEN udp4       0      0 *.3401                 *.* udp4       0
> 0 *.34985                *.* udp4       0      0 *.*
> *.* udp4       0      0 *.161                  *.* udp4       0
> 0 *.162                  *.* udp4       0      0 *.10000
> *.* udp4       0      0 127.0.0.1.123          *.* udp6       0
> 0 fe80::1%lo0.123        *.* udp6       0      0 ::1.123
> *.* udp4       0      0 10.0.0.24.123          *.* udp6       0
> 0 *.123                  *.* udp4       0      0 *.123
> *.* udp4       0      0 *.514                  *.* udp6       0
> 0 *.514                  *.*
> 
> 
> 
> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> - From your PC run telnet 10.0.0.24 80. You've seen if TCP socket
> opens.
> 
> 05.03.15 23:10, Monah Baki ?????:
>>>> How can I confirm, I have access only to the BSD box
>>>> 
>>>> Thanks
>>>> 
>>>> On Thu, Mar 5, 2015 at 11:12 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Does 80 port outside BSD-box listens?
>>>> 
>>>> 05.03.15 21:25, Monah Baki ?????:
>>>>>>> root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e
>>>>>>> -ttt -i pflog0 tcpdump: WARNING: pflog0: no IPv4
>>>>>>> address assigned tcpdump: verbose output suppressed,
>>>>>>> use -v or -vv for full protocol decode listening on
>>>>>>> pflog0, link-type PFLOG (OpenBSD pflog file), capture
>>>>>>> size 65535 bytes capability mode sandbox enabled
>>>>>>> 00:00:00.000000 rule 0..16777216/0(match): pass in on 
>>>>>>> bge0: 10.0.0.106.5678
>>>>>>>> 255.255.255.255.5678: UDP, length 88
>>>>>>> 00:00:08.342860 rule 0..16777216/0(match): pass in on
>>>>>>> bge0: 10.0.0.14.54264
>>>>>>>> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192,
>>>>>>>> options [mss
>>>>>>> 1460,nop,wscale 2,nop,nop,sackOK], length 0
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> Hm. No.
>>>>>>> 
>>>>>>> We not checked only OS.
>>>>>>> 
>>>>>>> Does your BSD really loads PF module?
>>>>>>> 
>>>>>>> 05.03.15 21:16, Monah Baki ?????:
>>>>>>>>>> Not sure why the client is running old hard/soft
>>>>>>>>>> ware, could it be cause of the hardware? Is
>>>>>>>>>> FreeBSD an issue, should I switch to linux?
>>>>>>>>>> 
>>>>>>>>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov 
>>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> Wow, 7600!
>>>>>>>>>> 
>>>>>>>>>> But why is so antique iOS?! Current is 15.4
>>>>>>>>>> 
>>>>>>>>>> 05.03.15 21:09, Monah Baki ?????:
>>>>>>>>>>>>> PORT   STATE SERVICE VERSION 23/tcp open
>>>>>>>>>>>>> telnet Cisco IOS telnetd MAC Address:
>>>>>>>>>>>>> 88:5A:92:63:77:81 (Cisco) Device type:
>>>>>>>>>>>>> router Running: Cisco IOS 12.X OS CPE:
>>>>>>>>>>>>> cpe:/h:cisco:7600_router 
>>>>>>>>>>>>> cpe:/o:cisco:ios:12.2 OS details: Cisco
>>>>>>>>>>>>> 7600 router (IOS 12.2) Network Distance: 1
>>>>>>>>>>>>> hop TCP Sequence Prediction: Difficulty=258
>>>>>>>>>>>>> (Good luck!) IP ID Sequence Generation:
>>>>>>>>>>>>> Randomized Service Info: OS: IOS; Device:
>>>>>>>>>>>>> switch; CPE: cpe:/o:cisco:ios
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri
>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> What is Cisco model and iOS version?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 05.03.15 20:25, Monah Baki ?????:
>>>>>>>>>>>>>>>> Yes, correct
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri 
>>>>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24
>>>>>>>>>>>>>>>> is proxy box?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>>>>>>>>>>>>>> '--prefix=/cache/squid' 
>>>>>>>>>>>>>>>>>>> '--enable-follow-x-forwarded-for'
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> 
'--with-large-files' '--enable-ssl'
>>>>>>>>>>>>>>>>>>> '--disable-ipv6'
>>>>>>>>>>>>>>>>>>> '--enable-esi' 
>>>>>>>>>>>>>>>>>>> '--enable-kill-parent-hack' 
>>>>>>>>>>>>>>>>>>> '--enable-snmp'
>>>>>>>>>>>>>>>>>>> '--with-pthreads' 
>>>>>>>>>>>>>>>>>>> '--with-filedescriptors=65535' 
>>>>>>>>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname'
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>> 
'--enable-storeio=ufs,aufs,diskd,rock'
>>>>>>>>>>>>>>>>>>> '--enable-ipfw-transparent' 
>>>>>>>>>>>>>>>>>>> '--enable-pf-transparent' 
>>>>>>>>>>>>>>>>>>> '--with-nat-devpf' 
>>>>>>>>>>>>>>>>>>> --enable-ltdl-convenience
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM,
>>>>>>>>>>>>>>>>>>> Yuri Voinov <yvoinov at gmail.com>
>>>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> This looking good too.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Stupid question:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> With witch interception option
>>>>>>>>>>>>>>>>>>> squid builed?
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> I.e, squid -v?
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 05.03.15 18:19, Monah Baki
>>>>>>>>>>>>>>>>>>> ?????:
>>>>>>>>>>>>>>>>>>>>>> Hi all, can anyone verify
>>>>>>>>>>>>>>>>>>>>>> if this is correct, need
>>>>>>>>>>>>>>>>>>>>>> to make ure that users
>>>>>>>>>>>>>>>>>>>>>> will be able to access
>>>>>>>>>>>>>>>>>>>>>> the internet via the 
>>>>>>>>>>>>>>>>>>>>>> squid.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Running FreeBSD with a
>>>>>>>>>>>>>>>>>>>>>> single interface with
>>>>>>>>>>>>>>>>>>>>>> Squid-3.5.2
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Policy based routing on
>>>>>>>>>>>>>>>>>>>>>> Cisco with the
>>>>>>>>>>>>>>>>>>>>>> following:
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> interface 
>>>>>>>>>>>>>>>>>>>>>> GigabitEthernet0/0/1.1
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> encapsulation dot1Q 1
>>>>>>>>>>>>>>>>>>>>>> native
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip address 10.0.0.9 
>>>>>>>>>>>>>>>>>>>>>> 255.255.255.0
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> no ip redirects
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> no ip unreachables
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip nat inside
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 priority 120
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 preempt
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip policy route-map
>>>>>>>>>>>>>>>>>>>>>> CFLOW
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip access-list extended 
>>>>>>>>>>>>>>>>>>>>>> REDIRECT
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24
>>>>>>>>>>>>>>>>>>>>>> any eq www
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> permit tcp host 10.0.0.23
>>>>>>>>>>>>>>>>>>>>>> any eq www
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> route-map CFLOW permit
>>>>>>>>>>>>>>>>>>>>>> 10
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> match ip address REDIRECT
>>>>>>>>>>>>>>>>>>>>>> set ip next-hop
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.24
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> In my /etc/pf.conf rdr
>>>>>>>>>>>>>>>>>>>>>> pass inet proto tcp from
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.0/8 to any port 80
>>>>>>>>>>>>>>>>>>>>>> -> 10.0.0.24 port 3129
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> # block in pass in log
>>>>>>>>>>>>>>>>>>>>>> quick on bge0 pass out
>>>>>>>>>>>>>>>>>>>>>> log quick on bge0 pass
>>>>>>>>>>>>>>>>>>>>>> out keep state
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> and finally in my
>>>>>>>>>>>>>>>>>>>>>> squid.conf: http_port
>>>>>>>>>>>>>>>>>>>>>> 3128 http_port 3129 
>>>>>>>>>>>>>>>>>>>>>> intercept
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> And for testing purposes
>>>>>>>>>>>>>>>>>>>>>> from the squid server:
>>>>>>>>>>>>>>>>>>>>>> ./squidclient -h
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.24 -p 3128 
>>>>>>>>>>>>>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> If I replace -p 3128 with
>>>>>>>>>>>>>>>>>>>>>> -p 80, I get a access
>>>>>>>>>>>>>>>>>>>>>> denied, and if I omit the
>>>>>>>>>>>>>>>>>>>>>> -p 3128 completely, I can
>>>>>>>>>>>>>>>>>>>>>> access the websites.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681106 IP 
>>>>>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 > 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http:
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>> 
Flags [.], ack 17377, win 1018,
>>>>>>>>>>>>>>>>>>>>>> options [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 985588797 ecr
>>>>>>>>>>>>>>>>>>>>>> 1054387720], length 0
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681421 IP 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http
>>>>>>>>>>>>>>>>>>>>>> > ISN-PHC-CACHE.44017:
>>>>>>>>>>>>>>>>>>>>>> Flags [.], seq
>>>>>>>>>>>>>>>>>>>>>> 17377:18825, ack 289,
>>>>>>>>>>>>>>>>>>>>>> win 1040, options
>>>>>>>>>>>>>>>>>>>>>> [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 1054387720 ecr
>>>>>>>>>>>>>>>>>>>>>> 985588501], length 1448
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681575 IP 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http
>>>>>>>>>>>>>>>>>>>>>> > ISN-PHC-CACHE.44017:
>>>>>>>>>>>>>>>>>>>>>> Flags [.], seq
>>>>>>>>>>>>>>>>>>>>>> 18825:20273, ack 289,
>>>>>>>>>>>>>>>>>>>>>> win 1040, options
>>>>>>>>>>>>>>>>>>>>>> [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 1054387720 ecr
>>>>>>>>>>>>>>>>>>>>>> 985588501], length 1448
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Did I miss anything?
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Thanks Monah
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>>>>
>>>>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>>>>> 
squid-users mailing list
>>>>>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>>>>> 
http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>> _______________________________________________
>>>>>>>>>>>>>>>>>>>> squid-users mailing list 
>>>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> 
http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+JnwAAoJENNXIZxhPexGZyAIALntLpMhoacCb/BEpimuJtq/
9lddOw02qrA9imxJvXfGRmqz5cosbecQcCAeZoXaY2XRtvWF0jM5y+GgeHeLDnjA
3n0mXORBS9LkWNgaOmP6CTyvs4InDEHJExk1SPF1su+4bvDUXwv5XuCJEQERDy3E
r80/p4bmtNTg4WR30abxICBIzffDodzAVg81wt5IvDrkKUdX6c5CYt25ASDBFqIU
gJOKMcAFgUgeyQzjrnTxNt+wxDGLRoIGXitYPy1h+EZ+XnNfrUDxx3DFfoFXmZyQ
P/IPE3XbBZXkRw1sDBnz2jgZddUBknqsGgl1o5Bl8UiLtSPqEDQrLD003GB8krI=
=TnLr
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Thu Mar  5 18:02:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 00:02:42 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
Message-ID: <54F89A42.6070501@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Sorry, I'm wrong. Netstat on host can't show redirected listeners.

Need to check it externally.

05.03.15 23:59, Monah Baki ?????:
> On 10.0.0.24
> 
> root at ISN-PHC-CACHE:/home/support # netstat -an Active Internet
> connections (including servers) Proto Recv-Q Send-Q Local Address
> Foreign Address        (state) tcp4       0     52 10.0.0.24.22
> 96.255.8.226.50911 ESTABLISHED tcp4       0      0 *.3129
> *.*                    LISTEN tcp4       0      0 *.3128
> *.*                    LISTEN tcp4       0      0 *.81
> *.*                    LISTEN tcp6       0      0 *.81
> *.*                    LISTEN tcp4       0      0 *.22
> *.*                    LISTEN tcp6       0      0 *.22
> *.*                    LISTEN tcp6       0      0 ::1.562
> ::1.40066 ESTABLISHED tcp6       0      0 ::1.40066
> ::1.562 ESTABLISHED tcp6       0      0 *.561                  *.*
> LISTEN tcp6       0      0 *.562                  *.*
> LISTEN tcp4       0      0 *.199                  *.*
> LISTEN tcp4       0      0 *.10000                *.*
> LISTEN udp4       0      0 *.3401                 *.* udp4       0
> 0 *.34985                *.* udp4       0      0 *.*
> *.* udp4       0      0 *.161                  *.* udp4       0
> 0 *.162                  *.* udp4       0      0 *.10000
> *.* udp4       0      0 127.0.0.1.123          *.* udp6       0
> 0 fe80::1%lo0.123        *.* udp6       0      0 ::1.123
> *.* udp4       0      0 10.0.0.24.123          *.* udp6       0
> 0 *.123                  *.* udp4       0      0 *.123
> *.* udp4       0      0 *.514                  *.* udp6       0
> 0 *.514                  *.*
> 
> 
> 
> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> - From your PC run telnet 10.0.0.24 80. You've seen if TCP socket
> opens.
> 
> 05.03.15 23:10, Monah Baki ?????:
>>>> How can I confirm, I have access only to the BSD box
>>>> 
>>>> Thanks
>>>> 
>>>> On Thu, Mar 5, 2015 at 11:12 AM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Does 80 port outside BSD-box listens?
>>>> 
>>>> 05.03.15 21:25, Monah Baki ?????:
>>>>>>> root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e
>>>>>>> -ttt -i pflog0 tcpdump: WARNING: pflog0: no IPv4
>>>>>>> address assigned tcpdump: verbose output suppressed,
>>>>>>> use -v or -vv for full protocol decode listening on
>>>>>>> pflog0, link-type PFLOG (OpenBSD pflog file), capture
>>>>>>> size 65535 bytes capability mode sandbox enabled
>>>>>>> 00:00:00.000000 rule 0..16777216/0(match): pass in on 
>>>>>>> bge0: 10.0.0.106.5678
>>>>>>>> 255.255.255.255.5678: UDP, length 88
>>>>>>> 00:00:08.342860 rule 0..16777216/0(match): pass in on
>>>>>>> bge0: 10.0.0.14.54264
>>>>>>>> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192,
>>>>>>>> options [mss
>>>>>>> 1460,nop,wscale 2,nop,nop,sackOK], length 0
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>> Hm. No.
>>>>>>> 
>>>>>>> We not checked only OS.
>>>>>>> 
>>>>>>> Does your BSD really loads PF module?
>>>>>>> 
>>>>>>> 05.03.15 21:16, Monah Baki ?????:
>>>>>>>>>> Not sure why the client is running old hard/soft
>>>>>>>>>> ware, could it be cause of the hardware? Is
>>>>>>>>>> FreeBSD an issue, should I switch to linux?
>>>>>>>>>> 
>>>>>>>>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov 
>>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> Wow, 7600!
>>>>>>>>>> 
>>>>>>>>>> But why is so antique iOS?! Current is 15.4
>>>>>>>>>> 
>>>>>>>>>> 05.03.15 21:09, Monah Baki ?????:
>>>>>>>>>>>>> PORT   STATE SERVICE VERSION 23/tcp open
>>>>>>>>>>>>> telnet Cisco IOS telnetd MAC Address:
>>>>>>>>>>>>> 88:5A:92:63:77:81 (Cisco) Device type:
>>>>>>>>>>>>> router Running: Cisco IOS 12.X OS CPE:
>>>>>>>>>>>>> cpe:/h:cisco:7600_router 
>>>>>>>>>>>>> cpe:/o:cisco:ios:12.2 OS details: Cisco
>>>>>>>>>>>>> 7600 router (IOS 12.2) Network Distance: 1
>>>>>>>>>>>>> hop TCP Sequence Prediction: Difficulty=258
>>>>>>>>>>>>> (Good luck!) IP ID Sequence Generation:
>>>>>>>>>>>>> Randomized Service Info: OS: IOS; Device:
>>>>>>>>>>>>> switch; CPE: cpe:/o:cisco:ios
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri
>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> What is Cisco model and iOS version?
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 05.03.15 20:25, Monah Baki ?????:
>>>>>>>>>>>>>>>> Yes, correct
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri 
>>>>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24
>>>>>>>>>>>>>>>> is proxy box?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
>>>>>>>>>>>>>>>>>>> '--prefix=/cache/squid' 
>>>>>>>>>>>>>>>>>>> '--enable-follow-x-forwarded-for'
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>> 
'--with-large-files' '--enable-ssl'
>>>>>>>>>>>>>>>>>>> '--disable-ipv6'
>>>>>>>>>>>>>>>>>>> '--enable-esi' 
>>>>>>>>>>>>>>>>>>> '--enable-kill-parent-hack' 
>>>>>>>>>>>>>>>>>>> '--enable-snmp'
>>>>>>>>>>>>>>>>>>> '--with-pthreads' 
>>>>>>>>>>>>>>>>>>> '--with-filedescriptors=65535' 
>>>>>>>>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname'
>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>> 
'--enable-storeio=ufs,aufs,diskd,rock'
>>>>>>>>>>>>>>>>>>> '--enable-ipfw-transparent' 
>>>>>>>>>>>>>>>>>>> '--enable-pf-transparent' 
>>>>>>>>>>>>>>>>>>> '--with-nat-devpf' 
>>>>>>>>>>>>>>>>>>> --enable-ltdl-convenience
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM,
>>>>>>>>>>>>>>>>>>> Yuri Voinov <yvoinov at gmail.com>
>>>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> This looking good too.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Stupid question:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> With witch interception option
>>>>>>>>>>>>>>>>>>> squid builed?
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> I.e, squid -v?
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 05.03.15 18:19, Monah Baki
>>>>>>>>>>>>>>>>>>> ?????:
>>>>>>>>>>>>>>>>>>>>>> Hi all, can anyone verify
>>>>>>>>>>>>>>>>>>>>>> if this is correct, need
>>>>>>>>>>>>>>>>>>>>>> to make ure that users
>>>>>>>>>>>>>>>>>>>>>> will be able to access
>>>>>>>>>>>>>>>>>>>>>> the internet via the 
>>>>>>>>>>>>>>>>>>>>>> squid.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Running FreeBSD with a
>>>>>>>>>>>>>>>>>>>>>> single interface with
>>>>>>>>>>>>>>>>>>>>>> Squid-3.5.2
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Policy based routing on
>>>>>>>>>>>>>>>>>>>>>> Cisco with the
>>>>>>>>>>>>>>>>>>>>>> following:
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> interface 
>>>>>>>>>>>>>>>>>>>>>> GigabitEthernet0/0/1.1
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> encapsulation dot1Q 1
>>>>>>>>>>>>>>>>>>>>>> native
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip address 10.0.0.9 
>>>>>>>>>>>>>>>>>>>>>> 255.255.255.0
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> no ip redirects
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> no ip unreachables
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip nat inside
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 priority 120
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 preempt
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> standby 1 name HSRP
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip policy route-map
>>>>>>>>>>>>>>>>>>>>>> CFLOW
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> ip access-list extended 
>>>>>>>>>>>>>>>>>>>>>> REDIRECT
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24
>>>>>>>>>>>>>>>>>>>>>> any eq www
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> permit tcp host 10.0.0.23
>>>>>>>>>>>>>>>>>>>>>> any eq www
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> route-map CFLOW permit
>>>>>>>>>>>>>>>>>>>>>> 10
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> match ip address REDIRECT
>>>>>>>>>>>>>>>>>>>>>> set ip next-hop
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.24
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> In my /etc/pf.conf rdr
>>>>>>>>>>>>>>>>>>>>>> pass inet proto tcp from
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.0/8 to any port 80
>>>>>>>>>>>>>>>>>>>>>> -> 10.0.0.24 port 3129
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> # block in pass in log
>>>>>>>>>>>>>>>>>>>>>> quick on bge0 pass out
>>>>>>>>>>>>>>>>>>>>>> log quick on bge0 pass
>>>>>>>>>>>>>>>>>>>>>> out keep state
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> and finally in my
>>>>>>>>>>>>>>>>>>>>>> squid.conf: http_port
>>>>>>>>>>>>>>>>>>>>>> 3128 http_port 3129 
>>>>>>>>>>>>>>>>>>>>>> intercept
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> And for testing purposes
>>>>>>>>>>>>>>>>>>>>>> from the squid server:
>>>>>>>>>>>>>>>>>>>>>> ./squidclient -h
>>>>>>>>>>>>>>>>>>>>>> 10.0.0.24 -p 3128 
>>>>>>>>>>>>>>>>>>>>>> http://www.freebsd.org/
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> If I replace -p 3128 with
>>>>>>>>>>>>>>>>>>>>>> -p 80, I get a access
>>>>>>>>>>>>>>>>>>>>>> denied, and if I omit the
>>>>>>>>>>>>>>>>>>>>>> -p 3128 completely, I can
>>>>>>>>>>>>>>>>>>>>>> access the websites.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> tcpdump with (-p 3128)
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681106 IP 
>>>>>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 > 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http:
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>> 
Flags [.], ack 17377, win 1018,
>>>>>>>>>>>>>>>>>>>>>> options [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 985588797 ecr
>>>>>>>>>>>>>>>>>>>>>> 1054387720], length 0
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681421 IP 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http
>>>>>>>>>>>>>>>>>>>>>> > ISN-PHC-CACHE.44017:
>>>>>>>>>>>>>>>>>>>>>> Flags [.], seq
>>>>>>>>>>>>>>>>>>>>>> 17377:18825, ack 289,
>>>>>>>>>>>>>>>>>>>>>> win 1040, options
>>>>>>>>>>>>>>>>>>>>>> [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 1054387720 ecr
>>>>>>>>>>>>>>>>>>>>>> 985588501], length 1448
>>>>>>>>>>>>>>>>>>>>>> 13:15:02.681575 IP 
>>>>>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http
>>>>>>>>>>>>>>>>>>>>>> > ISN-PHC-CACHE.44017:
>>>>>>>>>>>>>>>>>>>>>> Flags [.], seq
>>>>>>>>>>>>>>>>>>>>>> 18825:20273, ack 289,
>>>>>>>>>>>>>>>>>>>>>> win 1040, options
>>>>>>>>>>>>>>>>>>>>>> [nop,nop,TS val 
>>>>>>>>>>>>>>>>>>>>>> 1054387720 ecr
>>>>>>>>>>>>>>>>>>>>>> 985588501], length 1448
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Did I miss anything?
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Thanks Monah
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>>>>
>>>>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>>>>> 
squid-users mailing list
>>>>>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>
>
>>>>>>>>>>>>>>>>>>>>>> 
http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>> _______________________________________________
>>>>>>>>>>>>>>>>>>>> squid-users mailing list 
>>>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>> 
http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+JpCAAoJENNXIZxhPexGVD0H+QGY5D/awGhmSQmtwQ92qhEu
uGxoqT6Ejc62LZQxQvILRLMQeOjjm5i3ev/hXmxs0OFPBKQupTgataCGB7uUXU39
BEozwd6dSl+w8pWMcJYsuqw1KJrHpP4FCNF90qosdwx9O8W7nTZUQuLpqYvbwaMQ
6YHh4tW1nq13AGZTYqwZX6sSl1vxHfSmq78UcCbbWMPp2Cggc2HpK+Z9EFsty8ks
eXTnLceHpOV9CUz46/KNh/tUamaeVG1PHC48f3F7mW27VQDCHtYc4UyFTAJ8UQ9s
sMdeIu2B0P/Adi7y75fDROwH7+2NohnDmcfhuCRQdkDOvB0I8X26EeLWpswmEsY=
=1AyW
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 18:05:07 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 13:05:07 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F89A42.6070501@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
Message-ID: <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>

Ok let me ask the client tomorrow to run telnet 10.0.0.24 80 from a
workstation

Thanks for he help Yuri

On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Sorry, I'm wrong. Netstat on host can't show redirected listeners.
>
> Need to check it externally.
>
> 05.03.15 23:59, Monah Baki ?????:
> > On 10.0.0.24
> >
> > root at ISN-PHC-CACHE:/home/support # netstat -an Active Internet
> > connections (including servers) Proto Recv-Q Send-Q Local Address
> > Foreign Address        (state) tcp4       0     52 10.0.0.24.22
> > 96.255.8.226.50911 ESTABLISHED tcp4       0      0 *.3129
> > *.*                    LISTEN tcp4       0      0 *.3128
> > *.*                    LISTEN tcp4       0      0 *.81
> > *.*                    LISTEN tcp6       0      0 *.81
> > *.*                    LISTEN tcp4       0      0 *.22
> > *.*                    LISTEN tcp6       0      0 *.22
> > *.*                    LISTEN tcp6       0      0 ::1.562
> > ::1.40066 ESTABLISHED tcp6       0      0 ::1.40066
> > ::1.562 ESTABLISHED tcp6       0      0 *.561                  *.*
> > LISTEN tcp6       0      0 *.562                  *.*
> > LISTEN tcp4       0      0 *.199                  *.*
> > LISTEN tcp4       0      0 *.10000                *.*
> > LISTEN udp4       0      0 *.3401                 *.* udp4       0
> > 0 *.34985                *.* udp4       0      0 *.*
> > *.* udp4       0      0 *.161                  *.* udp4       0
> > 0 *.162                  *.* udp4       0      0 *.10000
> > *.* udp4       0      0 127.0.0.1.123          *.* udp6       0
> > 0 fe80::1%lo0.123        *.* udp6       0      0 ::1.123
> > *.* udp4       0      0 10.0.0.24.123          *.* udp6       0
> > 0 *.123                  *.* udp4       0      0 *.123
> > *.* udp4       0      0 *.514                  *.* udp6       0
> > 0 *.514                  *.*
> >
> >
> >
> > On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > - From your PC run telnet 10.0.0.24 80. You've seen if TCP socket
> > opens.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/12b083e4/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 18:08:19 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 00:08:19 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
 <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
Message-ID: <54F89B93.3080105@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Can you run pfctl -s nat state on proxy box?

06.03.15 0:05, Monah Baki ?????:
> Ok let me ask the client tomorrow to run telnet 10.0.0.24 80 from
> a workstation
> 
> Thanks for he help Yuri
> 
> On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>> 
>> Sorry, I'm wrong. Netstat on host can't show redirected
>> listeners.
>> 
>> Need to check it externally.
>> 
>> 05.03.15 23:59, Monah Baki ?????:
>>> On 10.0.0.24
>>> 
>>> root at ISN-PHC-CACHE:/home/support # netstat -an Active Internet 
>>> connections (including servers) Proto Recv-Q Send-Q Local
>>> Address Foreign Address        (state) tcp4       0     52
>>> 10.0.0.24.22 96.255.8.226.50911 ESTABLISHED tcp4       0      0
>>> *.3129 *.*                    LISTEN tcp4       0      0
>>> *.3128 *.*                    LISTEN tcp4       0      0 *.81 
>>> *.*                    LISTEN tcp6       0      0 *.81 *.*
>>> LISTEN tcp4       0      0 *.22 *.*                    LISTEN
>>> tcp6       0      0 *.22 *.*                    LISTEN tcp6
>>> 0      0 ::1.562 ::1.40066 ESTABLISHED tcp6       0      0
>>> ::1.40066 ::1.562 ESTABLISHED tcp6       0      0 *.561
>>> *.* LISTEN tcp6       0      0 *.562                  *.* 
>>> LISTEN tcp4       0      0 *.199                  *.* LISTEN
>>> tcp4       0      0 *.10000                *.* LISTEN udp4
>>> 0      0 *.3401                 *.* udp4       0 0 *.34985
>>> *.* udp4       0      0 *.* *.* udp4       0      0 *.161
>>> *.* udp4       0 0 *.162                  *.* udp4       0
>>> 0 *.10000 *.* udp4       0      0 127.0.0.1.123          *.*
>>> udp6       0 0 fe80::1%lo0.123        *.* udp6       0      0
>>> ::1.123 *.* udp4       0      0 10.0.0.24.123          *.* udp6
>>> 0 0 *.123                  *.* udp4       0      0 *.123 *.*
>>> udp4       0      0 *.514                  *.* udp6       0 0
>>> *.514                  *.*
>>> 
>>> 
>>> 
>>> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov
>>> <yvoinov at gmail.com> wrote:
>>> 
>>> - From your PC run telnet 10.0.0.24 80. You've seen if TCP
>>> socket opens.
>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+JuSAAoJENNXIZxhPexGmkMIAJQNjE4TwhTnO1hI+jqWgG06
XmPpDOkpv6xiMezh4NrgMNA3YRDysjEXHBywMlBop92/iLAehv1nyadMhIdWmPj0
im9RFuHVgTiEz9Fr7llc6jyz6oEkp4Ne3//FilvII0X2M8tnSknhiMhYliuloX8p
98IjTJPWDsYeEqURcwbxtGCz431GrpLmKTZkxQuw43a1hIQha4570prmbvcwU1xP
TLgv/WhltGRJyXszr3pwh1R/6cM8UYCK8iNgxn6KJvh2x+8hyc5avyttEbmyQDz+
JfuZoOyCyNU321yiONFS4EaPRWZsoUv+s59mS37m8gSuGIED6aKWlgceRE4OOgQ=
=t9CZ
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 18:10:31 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 13:10:31 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F89B93.3080105@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
 <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
 <54F89B93.3080105@gmail.com>
Message-ID: <CALP3=x-4nSqyijjStUVphsj=mXY6esRxhgKC2CTuASLLox_RGw@mail.gmail.com>

root at ISN-PHC-CACHE:/home/support # pfctl -s nat
No ALTQ support in kernel
ALTQ related functions disabled
rdr pass inet proto tcp from 10.0.0.0/8 to any port = http -> 10.0.0.24
port 3129

On Thu, Mar 5, 2015 at 1:08 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Can you run pfctl -s nat state on proxy box?
>
> 06.03.15 0:05, Monah Baki ?????:
> > Ok let me ask the client tomorrow to run telnet 10.0.0.24 80 from
> > a workstation
> >
> > Thanks for he help Yuri
> >
> > On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> >> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
> >>
> >> Sorry, I'm wrong. Netstat on host can't show redirected
> >> listeners.
> >>
> >> Need to check it externally.
> >>
> >> 05.03.15 23:59, Monah Baki ?????:
> >>> On 10.0.0.24
> >>>
> >>> root at ISN-PHC-CACHE:/home/support # netstat -an Active Internet
> >>> connections (including servers) Proto Recv-Q Send-Q Local
> >>> Address Foreign Address        (state) tcp4       0     52
> >>> 10.0.0.24.22 96.255.8.226.50911 ESTABLISHED tcp4       0      0
> >>> *.3129 *.*                    LISTEN tcp4       0      0
> >>> *.3128 *.*                    LISTEN tcp4       0      0 *.81
> >>> *.*                    LISTEN tcp6       0      0 *.81 *.*
> >>> LISTEN tcp4       0      0 *.22 *.*                    LISTEN
> >>> tcp6       0      0 *.22 *.*                    LISTEN tcp6
> >>> 0      0 ::1.562 ::1.40066 ESTABLISHED tcp6       0      0
> >>> ::1.40066 ::1.562 ESTABLISHED tcp6       0      0 *.561
> >>> *.* LISTEN tcp6       0      0 *.562                  *.*
> >>> LISTEN tcp4       0      0 *.199                  *.* LISTEN
> >>> tcp4       0      0 *.10000                *.* LISTEN udp4
> >>> 0      0 *.3401                 *.* udp4       0 0 *.34985
> >>> *.* udp4       0      0 *.* *.* udp4       0      0 *.161
> >>> *.* udp4       0 0 *.162                  *.* udp4       0
> >>> 0 *.10000 *.* udp4       0      0 127.0.0.1.123          *.*
> >>> udp6       0 0 fe80::1%lo0.123        *.* udp6       0      0
> >>> ::1.123 *.* udp4       0      0 10.0.0.24.123          *.* udp6
> >>> 0 0 *.123                  *.* udp4       0      0 *.123 *.*
> >>> udp4       0      0 *.514                  *.* udp6       0 0
> >>> *.514                  *.*
> >>>
> >>>
> >>>
> >>> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov
> >>> <yvoinov at gmail.com> wrote:
> >>>
> >>> - From your PC run telnet 10.0.0.24 80. You've seen if TCP
> >>> socket opens.
> >>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+JuSAAoJENNXIZxhPexGmkMIAJQNjE4TwhTnO1hI+jqWgG06
> XmPpDOkpv6xiMezh4NrgMNA3YRDysjEXHBywMlBop92/iLAehv1nyadMhIdWmPj0
> im9RFuHVgTiEz9Fr7llc6jyz6oEkp4Ne3//FilvII0X2M8tnSknhiMhYliuloX8p
> 98IjTJPWDsYeEqURcwbxtGCz431GrpLmKTZkxQuw43a1hIQha4570prmbvcwU1xP
> TLgv/WhltGRJyXszr3pwh1R/6cM8UYCK8iNgxn6KJvh2x+8hyc5avyttEbmyQDz+
> JfuZoOyCyNU321yiONFS4EaPRWZsoUv+s59mS37m8gSuGIED6aKWlgceRE4OOgQ=
> =t9CZ
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/631ea27d/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 18:12:38 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 00:12:38 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-4nSqyijjStUVphsj=mXY6esRxhgKC2CTuASLLox_RGw@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
 <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
 <54F89B93.3080105@gmail.com>
 <CALP3=x-4nSqyijjStUVphsj=mXY6esRxhgKC2CTuASLLox_RGw@mail.gmail.com>
Message-ID: <54F89C96.7020509@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Looks good too.

Damn.

Will think.

Need to run some external checks.

06.03.15 0:10, Monah Baki ?????:
> root at ISN-PHC-CACHE:/home/support # pfctl -s nat No ALTQ support in
> kernel ALTQ related functions disabled rdr pass inet proto tcp from
> 10.0.0.0/8 to any port = http -> 10.0.0.24 port 3129
> 
> On Thu, Mar 5, 2015 at 1:08 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Can you run pfctl -s nat state on proxy box?
> 
> 06.03.15 0:05, Monah Baki ?????:
>>>> Ok let me ask the client tomorrow to run telnet 10.0.0.24 80
>>>> from a workstation
>>>> 
>>>> Thanks for he help Yuri
>>>> 
>>>> On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>>>>> 
>>>>> Sorry, I'm wrong. Netstat on host can't show redirected 
>>>>> listeners.
>>>>> 
>>>>> Need to check it externally.
>>>>> 
>>>>> 05.03.15 23:59, Monah Baki ?????:
>>>>>> On 10.0.0.24
>>>>>> 
>>>>>> root at ISN-PHC-CACHE:/home/support # netstat -an Active
>>>>>> Internet connections (including servers) Proto Recv-Q
>>>>>> Send-Q Local Address Foreign Address        (state) tcp4
>>>>>> 0     52 10.0.0.24.22 96.255.8.226.50911 ESTABLISHED tcp4
>>>>>> 0      0 *.3129 *.*                    LISTEN tcp4
>>>>>> 0      0 *.3128 *.*                    LISTEN tcp4
>>>>>> 0      0 *.81 *.*                    LISTEN tcp6       0
>>>>>> 0 *.81 *.* LISTEN tcp4       0      0 *.22 *.*
>>>>>> LISTEN tcp6       0      0 *.22 *.*
>>>>>> LISTEN tcp6 0      0 ::1.562 ::1.40066 ESTABLISHED tcp6
>>>>>> 0      0 ::1.40066 ::1.562 ESTABLISHED tcp6       0
>>>>>> 0 *.561 *.* LISTEN tcp6       0      0 *.562
>>>>>> *.* LISTEN tcp4       0      0 *.199                  *.*
>>>>>> LISTEN tcp4       0      0 *.10000                *.*
>>>>>> LISTEN udp4 0      0 *.3401                 *.* udp4
>>>>>> 0 0 *.34985 *.* udp4       0      0 *.* *.* udp4       0
>>>>>> 0 *.161 *.* udp4       0 0 *.162                  *.*
>>>>>> udp4       0 0 *.10000 *.* udp4       0      0
>>>>>> 127.0.0.1.123          *.* udp6       0 0 fe80::1%lo0.123
>>>>>> *.* udp6       0      0 ::1.123 *.* udp4       0      0
>>>>>> 10.0.0.24.123          *.* udp6 0 0 *.123
>>>>>> *.* udp4       0      0 *.123 *.* udp4       0      0
>>>>>> *.514                  *.* udp6       0 0 *.514
>>>>>> *.*
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov 
>>>>>> <yvoinov at gmail.com> wrote:
>>>>>> 
>>>>>> - From your PC run telnet 10.0.0.24 80. You've seen if
>>>>>> TCP socket opens.
>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+JyWAAoJENNXIZxhPexGUc4IAJmS4DMs6Kf2D8Klm2GsKiDD
pHJsAk7XKPJ2oL97lQwPZs8vfDPB5AFJRSHS9BMxT5Y5q2tMbkuC8vh8w1uxG1rD
QercldJCcw4Rwxlq4nJUxEp8Hj82tPrCoMIiedSwCPBzka3OBEZfGHXMJAsGsvO0
FnmPJ5PXyen9OycBbe/bWVmt3aypi3ZA5/T+5yTS2dU49jDY2Wg47RJEsWmd3DsV
DU9js4Wz5woqzZerSkGizXSG9IZMBE8svR5X3l3nejy8NPwVc1ku2I7dAPcfCe9C
Fcuww85x2PpYfMNEnzgzzSdXx2oxfeeUMtO++zK3CaNCQxm1veTrwbrlu5sY8z4=
=diIu
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Thu Mar  5 18:14:21 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 5 Mar 2015 13:14:21 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54F89C96.7020509@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
 <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
 <54F89B93.3080105@gmail.com>
 <CALP3=x-4nSqyijjStUVphsj=mXY6esRxhgKC2CTuASLLox_RGw@mail.gmail.com>
 <54F89C96.7020509@gmail.com>
Message-ID: <CALP3=x_OkGFVien1xttpM78-TCNzJj=XMG5e5zh8oK+n92qJGg@mail.gmail.com>

So from my proxy server, everything looks good?



On Thu, Mar 5, 2015 at 1:12 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Looks good too.
>
> Damn.
>
> Will think.
>
> Need to run some external checks.
>
> 06.03.15 0:10, Monah Baki ?????:
> > root at ISN-PHC-CACHE:/home/support # pfctl -s nat No ALTQ support in
> > kernel ALTQ related functions disabled rdr pass inet proto tcp from
> > 10.0.0.0/8 to any port = http -> 10.0.0.24 port 3129
> >
> > On Thu, Mar 5, 2015 at 1:08 PM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Can you run pfctl -s nat state on proxy box?
> >
> > 06.03.15 0:05, Monah Baki ?????:
> >>>> Ok let me ask the client tomorrow to run telnet 10.0.0.24 80
> >>>> from a workstation
> >>>>
> >>>> Thanks for he help Yuri
> >>>>
> >>>> On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
> >>>>>
> >>>>> Sorry, I'm wrong. Netstat on host can't show redirected
> >>>>> listeners.
> >>>>>
> >>>>> Need to check it externally.
> >>>>>
> >>>>> 05.03.15 23:59, Monah Baki ?????:
> >>>>>> On 10.0.0.24
> >>>>>>
> >>>>>> root at ISN-PHC-CACHE:/home/support # netstat -an Active
> >>>>>> Internet connections (including servers) Proto Recv-Q
> >>>>>> Send-Q Local Address Foreign Address        (state) tcp4
> >>>>>> 0     52 10.0.0.24.22 96.255.8.226.50911 ESTABLISHED tcp4
> >>>>>> 0      0 *.3129 *.*                    LISTEN tcp4
> >>>>>> 0      0 *.3128 *.*                    LISTEN tcp4
> >>>>>> 0      0 *.81 *.*                    LISTEN tcp6       0
> >>>>>> 0 *.81 *.* LISTEN tcp4       0      0 *.22 *.*
> >>>>>> LISTEN tcp6       0      0 *.22 *.*
> >>>>>> LISTEN tcp6 0      0 ::1.562 ::1.40066 ESTABLISHED tcp6
> >>>>>> 0      0 ::1.40066 ::1.562 ESTABLISHED tcp6       0
> >>>>>> 0 *.561 *.* LISTEN tcp6       0      0 *.562
> >>>>>> *.* LISTEN tcp4       0      0 *.199                  *.*
> >>>>>> LISTEN tcp4       0      0 *.10000                *.*
> >>>>>> LISTEN udp4 0      0 *.3401                 *.* udp4
> >>>>>> 0 0 *.34985 *.* udp4       0      0 *.* *.* udp4       0
> >>>>>> 0 *.161 *.* udp4       0 0 *.162                  *.*
> >>>>>> udp4       0 0 *.10000 *.* udp4       0      0
> >>>>>> 127.0.0.1.123          *.* udp6       0 0 fe80::1%lo0.123
> >>>>>> *.* udp6       0      0 ::1.123 *.* udp4       0      0
> >>>>>> 10.0.0.24.123          *.* udp6 0 0 *.123
> >>>>>> *.* udp4       0      0 *.123 *.* udp4       0      0
> >>>>>> *.514                  *.* udp6       0 0 *.514
> >>>>>> *.*
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov
> >>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>
> >>>>>> - From your PC run telnet 10.0.0.24 80. You've seen if
> >>>>>> TCP socket opens.
> >>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+JyWAAoJENNXIZxhPexGUc4IAJmS4DMs6Kf2D8Klm2GsKiDD
> pHJsAk7XKPJ2oL97lQwPZs8vfDPB5AFJRSHS9BMxT5Y5q2tMbkuC8vh8w1uxG1rD
> QercldJCcw4Rwxlq4nJUxEp8Hj82tPrCoMIiedSwCPBzka3OBEZfGHXMJAsGsvO0
> FnmPJ5PXyen9OycBbe/bWVmt3aypi3ZA5/T+5yTS2dU49jDY2Wg47RJEsWmd3DsV
> DU9js4Wz5woqzZerSkGizXSG9IZMBE8svR5X3l3nejy8NPwVc1ku2I7dAPcfCe9C
> Fcuww85x2PpYfMNEnzgzzSdXx2oxfeeUMtO++zK3CaNCQxm1veTrwbrlu5sY8z4=
> =diIu
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/179ab28f/attachment.htm>

From yvoinov at gmail.com  Thu Mar  5 18:56:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 00:56:27 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_OkGFVien1xttpM78-TCNzJj=XMG5e5zh8oK+n92qJGg@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
 <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>
 <54F89A42.6070501@gmail.com>
 <CALP3=x_KpfWyy1r76-gVmy3Vwb4WkjO+b_x8eOT-Ysuerw5tFQ@mail.gmail.com>
 <54F89B93.3080105@gmail.com>
 <CALP3=x-4nSqyijjStUVphsj=mXY6esRxhgKC2CTuASLLox_RGw@mail.gmail.com>
 <54F89C96.7020509@gmail.com>
 <CALP3=x_OkGFVien1xttpM78-TCNzJj=XMG5e5zh8oK+n92qJGg@mail.gmail.com>
Message-ID: <54F8A6DB.2010802@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Yep.

I don't see any inconsistencies.

06.03.15 0:14, Monah Baki ?????:
> So from my proxy server, everything looks good?
> 
> 
> 
> On Thu, Mar 5, 2015 at 1:12 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Looks good too.
> 
> Damn.
> 
> Will think.
> 
> Need to run some external checks.
> 
> 06.03.15 0:10, Monah Baki ?????:
>>>> root at ISN-PHC-CACHE:/home/support # pfctl -s nat No ALTQ
>>>> support in kernel ALTQ related functions disabled rdr pass
>>>> inet proto tcp from 10.0.0.0/8 to any port = http ->
>>>> 10.0.0.24 port 3129
>>>> 
>>>> On Thu, Mar 5, 2015 at 1:08 PM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote:
>>>> 
>>>> Can you run pfctl -s nat state on proxy box?
>>>> 
>>>> 06.03.15 0:05, Monah Baki ?????:
>>>>>>> Ok let me ask the client tomorrow to run telnet
>>>>>>> 10.0.0.24 80 from a workstation
>>>>>>> 
>>>>>>> Thanks for he help Yuri
>>>>>>> 
>>>>>>> On Thu, Mar 5, 2015 at 1:02 PM, Yuri Voinov 
>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>> 
>>>>>>>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>>>>>>>> 
>>>>>>>> Sorry, I'm wrong. Netstat on host can't show
>>>>>>>> redirected listeners.
>>>>>>>> 
>>>>>>>> Need to check it externally.
>>>>>>>> 
>>>>>>>> 05.03.15 23:59, Monah Baki ?????:
>>>>>>>>> On 10.0.0.24
>>>>>>>>> 
>>>>>>>>> root at ISN-PHC-CACHE:/home/support # netstat -an
>>>>>>>>> Active Internet connections (including servers)
>>>>>>>>> Proto Recv-Q Send-Q Local Address Foreign Address
>>>>>>>>> (state) tcp4 0     52 10.0.0.24.22
>>>>>>>>> 96.255.8.226.50911 ESTABLISHED tcp4 0      0 *.3129
>>>>>>>>> *.*                    LISTEN tcp4 0      0 *.3128
>>>>>>>>> *.*                    LISTEN tcp4 0      0 *.81
>>>>>>>>> *.*                    LISTEN tcp6       0 0 *.81
>>>>>>>>> *.* LISTEN tcp4       0      0 *.22 *.* LISTEN tcp6
>>>>>>>>> 0      0 *.22 *.* LISTEN tcp6 0      0 ::1.562
>>>>>>>>> ::1.40066 ESTABLISHED tcp6 0      0 ::1.40066
>>>>>>>>> ::1.562 ESTABLISHED tcp6       0 0 *.561 *.* LISTEN
>>>>>>>>> tcp6       0      0 *.562 *.* LISTEN tcp4       0
>>>>>>>>> 0 *.199                  *.* LISTEN tcp4       0
>>>>>>>>> 0 *.10000                *.* LISTEN udp4 0      0
>>>>>>>>> *.3401                 *.* udp4 0 0 *.34985 *.*
>>>>>>>>> udp4       0      0 *.* *.* udp4       0 0 *.161
>>>>>>>>> *.* udp4       0 0 *.162                  *.* udp4
>>>>>>>>> 0 0 *.10000 *.* udp4       0      0 127.0.0.1.123
>>>>>>>>> *.* udp6       0 0 fe80::1%lo0.123 *.* udp6       0
>>>>>>>>> 0 ::1.123 *.* udp4       0      0 10.0.0.24.123
>>>>>>>>> *.* udp6 0 0 *.123 *.* udp4       0      0 *.123
>>>>>>>>> *.* udp4       0      0 *.514                  *.*
>>>>>>>>> udp6       0 0 *.514 *.*
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov 
>>>>>>>>> <yvoinov at gmail.com> wrote:
>>>>>>>>> 
>>>>>>>>> - From your PC run telnet 10.0.0.24 80. You've seen
>>>>>>>>> if TCP socket opens.
>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJU+KbbAAoJENNXIZxhPexGpa4IAJ2TC8byBNtJEEa8CfHSnYjd
3RPvzUzvzwaO2FWi0ZajDq0CYEJ8uTjdTiopZOVwGGAr5NdDk62Q7ZCwwZXgzEnD
SSve5Yl1lWpM6Zj9tDanCN3sw245te1MXer1CKLXR2bbd68WYQ0FT05fgCG9RrT7
XHZPvQRnE4BN5qIuu9pS3C9qvToYIM+C/jMqbT3MuDqrzPWksOnQ35LDf4mQAEgS
iH1XNM5bNVqrPIKTuZ8z8yJvZ+yuqkKde/6Y1k2QigocWre3jdDW0AdxXBZIo9QP
yCqkl9UmM/UefvvVgt44BftoJlUvaDYSoy2PAZ23khVHmDYY2bq0XJinKZwOHDQ=
=Z2XY
-----END PGP SIGNATURE-----


From infneurodcr.mtz at infomed.sld.cu  Thu Mar  5 20:49:49 2015
From: infneurodcr.mtz at infomed.sld.cu (Informatico Neurodesarrollo)
Date: Thu, 05 Mar 2015 15:49:49 -0500
Subject: [squid-users] Authentication help
Message-ID: <54F8C16D.1060908@infomed.sld.cu>

Hi list,
I am new in the list and I want to solve a problem with the 
authentication process in the factory that I worked some years ago and 
in this place I began work with Linux.
They use openSuSE 13.2 (64bits) with squid 3.4.4, the specification are:
- the authentication is local, Unix users
- two groups created :intranet (only can access to domain ".cu" ),internet

What is the deal?:

When I try to access, in the surfer arise a windows ask me the user and 
password, but when I push Enter key, this windows arise again and I have 
to press several times the "ESC" key to can navigate.

I attach bellow squid.conf file.

My best regards.

PD Apologist my english, but if any body else understand Spanish 
language I can explain better.

-- 

Jes?s Reyes Piedra
Admin Red Neurodearrollo,C?rdenas
La caja dec?a:"Requiere windows 95 o superior"...
Entonces instal? LINUX.



squid.conf:

# Squid normally listens to port 3128
http_port 3128

###################################################
#Memoria destinada para mantener el cache en la RAM
cache_mem 1024 MB

# Uncomment and adjust the following to add a disk cache directory.
cache_dir aufs /var/cache/squid 99999 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mgr juanc at jvr.cu
cache_replacement_policy lru
cache_swap_high 95
cache_swap_low 90
client_lifetime 1 days
connect_timeout 2 minutes
#emulate_httpd_log off
debug_options ALL,1 33,2

ftp_passive on
maximum_object_size 4096 KB
memory_replacement_policy lru
minimum_object_size 0 KB

########################################################
#Autenticaci?n

auth_param basic program /usr/sbin/basic_getpwnam_auth 
--helper-protocol=squid-2.5-basic
auth_param basic children 20
auth_param basic realm Servidor Proxy JVR
auth_param basic credentialsttl 1 hours
auth_param basic casesensitive off

############
#Grupos Unix

external_acl_type groupo_linux %LOGIN /usr/sbin/ext_unix_group_acl -p

acl nav_nac external groupo_linux intranet
acl nav_int external groupo_linux internet

acl nav_full proxy_auth nav_int
acl nav_cuba proxy_auth nav_nac

acl Auth_jvr proxy_auth REQUIRED

#########################################################
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320


cache_log /var/log/squid/cache.log
access_log /var/log/squid/access.log
cache_store_log /var/log/squid/store.log
error_directory /usr/share/squid/errors/es


acl localnet src 10.44.1.0/24
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT

acl restricted_sites dstdomain "/etc/squid/listas/blocked_sites.acl"
acl restricted_dst dst "/etc/squid/listas/blocked_src"
acl nacional dstdomain .cu

# Regla para denegar palabras indebidas
acl palabras url_regex -i "/etc/squid/deneg"


#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
# Deny CONNECT to other than secure SSL ports
# Only allow cachemgr access from localhost
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
# Allow localhost always proxy functionality
# And finally deny all other access to this proxy

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

http_access allow localnet !restricted_sites !restricted_dst !palabras
http_access allow Auth_jvr nav_full !nav_nac
http_access allow Auth_jvr nav_cuba nacional


http_access deny all


--
Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas

Infomed: http://www.sld.cu/



From alberto2perez at gmail.com  Thu Mar  5 21:35:22 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Thu, 5 Mar 2015 16:35:22 -0500
Subject: [squid-users] issue with tcp_outgoing_address and external acl
In-Reply-To: <54F823AA.1030407@treenet.co.nz>
References: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>
 <54F823AA.1030407@treenet.co.nz>
Message-ID: <CAMZauGp1w3yFQH+R4xYUpMAUvJX2PBKgJZ9mfX6m0kWNE7OfOQ@mail.gmail.com>

Thanks Amos  for the link, I understand tcp_outgoing_address only
works with fast acl and external acls are slow.

In order to mitigate this fact and achieve my purpose of share traffic
among two links depending only of username, who can recommend me a
workaround?

I was working in mantaining of IPs for those users special, once the
user login or logout from captive portal I update the list if IPs to
be used as SRC acl combined with tcp_outgoing_address and worked like
a charm except for the fact that squid only notice this change if I
reload configuration, which is a heavy reason to consider another
solutions.

It is possible to setup a ttl for this SRC acl, how can I make squid
note the change in this list without reloading configuration.

Thanks a lot for the help.



Well, my only complaint about squid would be about the fact

On 3/5/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 5/03/2015 4:25 p.m., Alberto Perez wrote:
>> Hi and thanks to all members in this community for the great support.
>>
>>
>> Im trying to send traffic for some users through a faster link using
>> tcp_outgoing_address, I found this directive very usefull and suitable
>> for this need.
>>
>> I use a captive portal and my squid only use autorization, so users
>> are not available with standard ident method.
>>
>> I wrote an acl to simulate ident process by reading to memory a list
>> of users from a file and matching external user returned by active
>> session external acl. For general purposes of autorization this work
>> fine.
>>
>> I found that tcp_outgoing_address works good with src ACLs but doesn't
>> work with this external acl, by the way, has huge ttl so squid doesn't
>> evaluate it on each request.
>>
>> Can anyone please tell me if this is possible, or point the right
>> direction to solve this.
>>
>> I will be glad to provide more information about my case if is needed.
>>
>
> Are you asking how to set this up? or complaining about a problem you
> found with it?
>
> The answer to both is in the FAQ:
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Fri Mar  6 00:25:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Mar 2015 13:25:57 +1300
Subject: [squid-users] When i browse some webpage ( www.ifeng.com) via
 proxy ( squid.3.5.2) .There are some error info ( Content Encoding Error )
In-Reply-To: <54F86E9B.4080901@yahoo.com>
References: <54F86E9B.4080901@yahoo.com>
Message-ID: <54F8F415.8090507@treenet.co.nz>

On 6/03/2015 3:56 a.m., johnzeng wrote:
> 
> Hi all :
> 
> When i browse some webpage ( www.ifeng.com) via proxy ( squid.3.5.2) ,
> 
> There are some error info from firefox browser .
> 
> if possible , please give some advisement .
> 
> ---------------------------------------------------
> 
> Content Encoding Error
> 
> The page you are trying to view cannot be shown because it uses an
> invalid or unsupported form of compression.
> 
> * Please contact the website owners to inform them of this problem.

Are we the website owners ?
and what type of encoding is it that your browser is not supporting ?

Amos


From squid3 at treenet.co.nz  Fri Mar  6 00:39:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Mar 2015 13:39:41 +1300
Subject: [squid-users] issue with tcp_outgoing_address and external acl
In-Reply-To: <CAMZauGp1w3yFQH+R4xYUpMAUvJX2PBKgJZ9mfX6m0kWNE7OfOQ@mail.gmail.com>
References: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>	<54F823AA.1030407@treenet.co.nz>
 <CAMZauGp1w3yFQH+R4xYUpMAUvJX2PBKgJZ9mfX6m0kWNE7OfOQ@mail.gmail.com>
Message-ID: <54F8F74D.7040800@treenet.co.nz>

On 6/03/2015 10:35 a.m., Alberto Perez wrote:
> Thanks Amos  for the link, I understand tcp_outgoing_address only
> works with fast acl and external acls are slow.
> 
> In order to mitigate this fact and achieve my purpose of share traffic
> among two links depending only of username, who can recommend me a
> workaround?
> 

In the current (3.4+) Squid you can use the note ACL to check user=
exists. It is a fast ACL check and does not do anything to trigger auth
when its absent. It will match usernames added by non-auth helpers as well.

To specifically limit it to HTTP authenticated users you can also check
the raw HTTP "Proxy-Authentication" header contents with req_header ACL
type.

Amos


> I was working in mantaining of IPs for those users special, once the
> user login or logout from captive portal I update the list if IPs to
> be used as SRC acl combined with tcp_outgoing_address and worked like
> a charm except for the fact that squid only notice this change if I
> reload configuration, which is a heavy reason to consider another
> solutions.
> 
> It is possible to setup a ttl for this SRC acl, how can I make squid
> note the change in this list without reloading configuration.

SRC is the client IP the request message was received from. Its part of
the mesage, there is nothing stored to have a TTL.

Amos



From squid3 at treenet.co.nz  Fri Mar  6 00:50:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Mar 2015 13:50:15 +1300
Subject: [squid-users] When i browse some webpage ( www.ifeng.com) via
 proxy ( squid.3.5.2) .There are some error info ( Content Encoding Error )
In-Reply-To: <54F8F415.8090507@treenet.co.nz>
References: <54F86E9B.4080901@yahoo.com> <54F8F415.8090507@treenet.co.nz>
Message-ID: <54F8F9C7.1090306@treenet.co.nz>

On 6/03/2015 1:25 p.m., Amos Jeffries wrote:
> On 6/03/2015 3:56 a.m., johnzeng wrote:
>>
>> Hi all :
>>
>> When i browse some webpage ( www.ifeng.com) via proxy ( squid.3.5.2) ,
>>
>> There are some error info from firefox browser .
>>
>> if possible , please give some advisement .
>>
>> ---------------------------------------------------
>>
>> Content Encoding Error
>>
>> The page you are trying to view cannot be shown because it uses an
>> invalid or unsupported form of compression.
>>
>> * Please contact the website owners to inform them of this problem.
> 
> Are we the website owners ?
> and what type of encoding is it that your browser is not supporting ?


Ah, redbot.org shows that the server is outputting unknown-length gzip
compressed content on some requests but omitting the required Vary: header.

https://redbot.org/?uri=http%3A%2F%2Fwww.ifeng.com%2F


Also, whoever is in charge of bjtel-cache02.ifeng.com really needs to do
some upgrades. It is running very obsolete software.

Amos



From johnzeng2013 at yahoo.com  Fri Mar  6 03:51:38 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 06 Mar 2015 11:51:38 +0800
Subject: [squid-users] When i browse some webpage ( www.ifeng.com) via
 proxy ( squid.3.5.2) .There are some error info ( Content Encoding Error )
In-Reply-To: <54F8F415.8090507@treenet.co.nz>
References: <54F86E9B.4080901@yahoo.com> <54F8F415.8090507@treenet.co.nz>
Message-ID: <54F9244A.5080505@yahoo.com>

Hello Amos :

                     Thanks for your reply , and i sloved the probme  
just now,

                     our error config( reply_header_access ) caused the 
problem .

                     when i disable  the part via #reply_header_access 
All deny all  , and everything is fine.





reply_header_access Allow allow all
reply_header_access Authorization allow all
reply_header_access WWW-Authenticate allow all
reply_header_access Proxy-Authorization allow all
reply_header_access Proxy-Authenticate allow all
reply_header_access Cache-Control allow all
reply_header_access Content-Encoding allow all
reply_header_access Content-Length allow all
reply_header_access Content-Type allow all
reply_header_access Date allow all
reply_header_access Expires allow all
reply_header_access Host allow all
reply_header_access If-Modified-Since allow all
reply_header_access Last-Modified allow all
reply_header_access Location allow all
reply_header_access Pragma allow all
reply_header_access Accept allow all
reply_header_access Accept-Charset allow all
reply_header_access Accept-Encoding allow all
reply_header_access Accept-Language allow all
reply_header_access Content-Language allow all
reply_header_access Mime-Version allow all
reply_header_access Retry-After allow all
reply_header_access Title allow all
reply_header_access Connection allow all
reply_header_access Proxy-Connection allow all
request_header_access Cookie allow all
reply_header_access All deny all



? 2015?03?06? 08:25, Amos Jeffries ??:
> On 6/03/2015 3:56 a.m., johnzeng wrote:
>> Hi all :
>>
>> When i browse some webpage ( www.ifeng.com) via proxy ( squid.3.5.2) ,
>>
>> There are some error info from firefox browser .
>>
>> if possible , please give some advisement .
>>
>> ---------------------------------------------------
>>
>> Content Encoding Error
>>
>> The page you are trying to view cannot be shown because it uses an
>> invalid or unsupported form of compression.
>>
>> * Please contact the website owners to inform them of this problem.
> Are we the website owners ?
> and what type of encoding is it that your browser is not supporting ?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From vin.krish25 at gmail.com  Fri Mar  6 04:11:18 2015
From: vin.krish25 at gmail.com (vin_krish)
Date: Thu, 5 Mar 2015 20:11:18 -0800 (PST)
Subject: [squid-users] how to avoid browser finger printing using squid3
Message-ID: <1425615078229-4670244.post@n4.nabble.com>

Hi all,

        Can we avoid browser finger printing using squid 3..? Please help
me.


Regards,
krish



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/how-to-avoid-browser-finger-printing-using-squid3-tp4670244.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Mar  6 05:06:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Mar 2015 18:06:19 +1300
Subject: [squid-users] how to avoid browser finger printing using squid3
In-Reply-To: <1425615078229-4670244.post@n4.nabble.com>
References: <1425615078229-4670244.post@n4.nabble.com>
Message-ID: <54F935CB.3000205@treenet.co.nz>

On 6/03/2015 5:11 p.m., vin_krish wrote:
> Hi all,
> 
>         Can we avoid browser finger printing using squid 3..? Please help
> me.

Yes and no. But mostly no.

Yes in that Squid adds/removes headers for its normal operation and that
can be extended to remove or change browser details.


No because;
 Firstly, the very act of avoiding fingerprinting is itself a
fingerprint signature.
 Secondly, even if you manage to completely mangle all fingerprint
details from the HTTP layer the browser AJAX/XHR or WebSockets
functionality has the ability to send browser info directly to the
server using encoded payloads - and is not limited to using HTTP for
that action either.
 Thirdly, the browser signature is not just what can be found in one
message but the collection of things it can (or not) do over teh whole
period in which its being fingerprinted - which brings you back to the
first problem.

If you have a set of particular things you want to do to alter your
fingerprint we can probably help wth that. But advice beyond that is
tricky - your situation WILL be different from others.

Good Luck.

Amos



From alberto2perez at gmail.com  Fri Mar  6 05:13:01 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Fri, 6 Mar 2015 00:13:01 -0500
Subject: [squid-users] issue with tcp_outgoing_address and external acl
In-Reply-To: <54F8F74D.7040800@treenet.co.nz>
References: <CAMZauGpevazb=VQYV=fA-oDtD4-p7xtLE-jybwHmAVE1WnWyLQ@mail.gmail.com>
 <54F823AA.1030407@treenet.co.nz>
 <CAMZauGp1w3yFQH+R4xYUpMAUvJX2PBKgJZ9mfX6m0kWNE7OfOQ@mail.gmail.com>
 <54F8F74D.7040800@treenet.co.nz>
Message-ID: <CAMZauGo4ueru==gZKc1oftP85A0-+67hRm75_CWHaV8SZyG2pQ@mail.gmail.com>

Thank you Amos,

As always you hit it, it worked fine with note acl.

I really appreciate your time to support this community, great help
today. Thanks a lot

God Bless you

Alberto


On 3/5/15, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 6/03/2015 10:35 a.m., Alberto Perez wrote:
>> Thanks Amos  for the link, I understand tcp_outgoing_address only
>> works with fast acl and external acls are slow.
>>
>> In order to mitigate this fact and achieve my purpose of share traffic
>> among two links depending only of username, who can recommend me a
>> workaround?
>>
>
> In the current (3.4+) Squid you can use the note ACL to check user=
> exists. It is a fast ACL check and does not do anything to trigger auth
> when its absent. It will match usernames added by non-auth helpers as well.
>
> To specifically limit it to HTTP authenticated users you can also check
> the raw HTTP "Proxy-Authentication" header contents with req_header ACL
> type.
>
> Amos
>
>
>> I was working in mantaining of IPs for those users special, once the
>> user login or logout from captive portal I update the list if IPs to
>> be used as SRC acl combined with tcp_outgoing_address and worked like
>> a charm except for the fact that squid only notice this change if I
>> reload configuration, which is a heavy reason to consider another
>> solutions.
>>
>> It is possible to setup a ttl for this SRC acl, how can I make squid
>> note the change in this list without reloading configuration.
>
> SRC is the client IP the request message was received from. Its part of
> the mesage, there is nothing stored to have a TTL.
>
> Amos
>
>


From monahbaki at gmail.com  Fri Mar  6 13:03:28 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 6 Mar 2015 08:03:28 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
Message-ID: <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>

Hi All,

As an addition to my yesterday's issue,

Tail -f cache.log, I am getting the following:

015/03/06 13:54:02| WARNING: Forwarding loop detected for:
GET /Artwork/SN.png HTTP/1.1
Host: www.squid-cache.org
Accept: image/webp,*/*;q=0.8
User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/40.0.2214.115 Safari/537.36
Referer: http://www.openbsd.org/
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,ar;q=0.6
Via: 1.1 ISN-PHC-CACHE (squid/3.5.2)
X-Forwarded-For: 10.0.0.23
Cache-Control: max-age=0
Connection: keep-alive


2015/03/06 13:54:02| WARNING: Forwarding loop detected for:
GET /favicon.ico HTTP/1.1
Host: www.openbsd.org
Accept: */*
User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/40.0.2214.115 Safari/537.36
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,ar;q=0.6
Via: 1.1 ISN-PHC-CACHE (squid/3.5.2)
X-Forwarded-For: 10.0.0.23
Cache-Control: max-age=259200
Connection: keep-alive

Any ideas?

---------- Forwarded message ----------
From: Monah Baki <monahbaki at gmail.com>
Date: Thu, Mar 5, 2015 at 7:19 AM
Subject: squid intercept config
To: Squid Users <squid-users at squid-cache.org>


Hi all, can anyone verify if this is correct, need to make ure that users
will be able to access the internet via the squid.

Running FreeBSD with a single interface with Squid-3.5.2

Policy based routing on Cisco with the following:


interface GigabitEthernet0/0/1.1

encapsulation dot1Q 1 native

ip address 10.0.0.9 255.255.255.0

no ip redirects

no ip unreachables

ip nat inside

standby 1 ip 10.0.0.10

standby 1 priority 120

standby 1 preempt

standby 1 name HSRP

ip policy route-map CFLOW



ip access-list extended REDIRECT

deny   tcp host 10.0.0.24 any eq www

permit tcp host 10.0.0.23 any eq www



route-map CFLOW permit 10

match ip address REDIRECT
set ip next-hop 10.0.0.24

In my /etc/pf.conf
rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
3129

# block in
pass in log quick on bge0
pass out log quick on bge0
pass out keep state

and finally in my squid.conf:
http_port 3128
http_port 3129 intercept



And for testing purposes from the squid server:
 ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/

If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
-p 3128 completely, I can access the websites.

tcpdump with (-p 3128)

13:15:02.681106 IP ISN-PHC-CACHE.44017 > wfe0.ysv.freebsd.org.http: Flags
[.], ack 17377, win 1018, options [nop,nop,TS val 985588797 ecr
1054387720], length 0
13:15:02.681421 IP wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags
[.], seq 17377:18825, ack 289, win 1040, options [nop,nop,TS val 1054387720
ecr 985588501], length 1448
13:15:02.681575 IP wfe0.ysv.freebsd.org.http > ISN-PHC-CACHE.44017: Flags
[.], seq 18825:20273, ack 289, win 1040, options [nop,nop,TS val 1054387720
ecr 985588501], length 1448



Did I miss anything?

Thanks
Monah
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/b2d8dabf/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Mar  6 13:14:03 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 6 Mar 2015 14:14:03 +0100
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
Message-ID: <201503061414.03329.Antony.Stone@squid.open.source.it>

On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:

> Hi All,
> 
> As an addition to my yesterday's issue,
> 
> Tail -f cache.log, I am getting the following:
> 
> 015/03/06 13:54:02| WARNING: Forwarding loop detected for:

> Any ideas?

Is your NAT rule catching the HTTP requests from the proxy itself (as well as 
the requests from the clients) and sending *everything* to the proxy 
(including the requests the proxy is trying to make out to the Internet)?

I'm not an expert on Cisco or BSD, but it does strike me that your rule:

rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port 3129

looks like it will match requests from the proxy's address 10.0.0.24 as well 
as all the clients...

Try adding an exception in before the NAT rule, saying "traffic from 10.0.0.24 
should not be NATted".


Regards,


Antony.

-- 
"Once you have a panic, things tend to become rather undefined."

 - murble

                                                   Please reply to the list;
                                                         please *don't* CC me.


From monahbaki at gmail.com  Fri Mar  6 13:26:46 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 6 Mar 2015 08:26:46 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <201503061414.03329.Antony.Stone@squid.open.source.it>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
Message-ID: <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>

I went and changed the 10.0.0.0/8 to 10.0.0.23, which is the client station
we are testing on, same results. Forward loop detected

Thanks

On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>
> > Hi All,
> >
> > As an addition to my yesterday's issue,
> >
> > Tail -f cache.log, I am getting the following:
> >
> > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>
> > Any ideas?
>
> Is your NAT rule catching the HTTP requests from the proxy itself (as well
> as
> the requests from the clients) and sending *everything* to the proxy
> (including the requests the proxy is trying to make out to the Internet)?
>
> I'm not an expert on Cisco or BSD, but it does strike me that your rule:
>
> rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
> 3129
>
> looks like it will match requests from the proxy's address 10.0.0.24 as
> well
> as all the clients...
>
> Try adding an exception in before the NAT rule, saying "traffic from
> 10.0.0.24
> should not be NATted".
>
>
> Regards,
>
>
> Antony.
>
> --
> "Once you have a panic, things tend to become rather undefined."
>
>  - murble
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/6835df70/attachment.htm>

From yvoinov at gmail.com  Fri Mar  6 13:28:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 19:28:50 +0600
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
Message-ID: <54F9AB92.1070400@gmail.com>

Did you have another listening process on 80 port on your proxy box?

I.e., web-server?

06.03.15 19:26, Monah Baki ?????:
> I went and changed the 10.0.0.0/8 <http://10.0.0.0/8> to 10.0.0.23, 
> which is the client station we are testing on, same results. Forward 
> loop detected
>
> Thanks
>
> On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone 
> <Antony.Stone at squid.open.source.it 
> <mailto:Antony.Stone at squid.open.source.it>> wrote:
>
>     On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>
>     > Hi All,
>     >
>     > As an addition to my yesterday's issue,
>     >
>     > Tail -f cache.log, I am getting the following:
>     >
>     > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>
>     > Any ideas?
>
>     Is your NAT rule catching the HTTP requests from the proxy itself
>     (as well as
>     the requests from the clients) and sending *everything* to the proxy
>     (including the requests the proxy is trying to make out to the
>     Internet)?
>
>     I'm not an expert on Cisco or BSD, but it does strike me that your
>     rule:
>
>     rdr pass inet proto tcp from 10.0.0.0/8 <http://10.0.0.0/8> to any
>     port 80 -> 10.0.0.24 port 3129
>
>     looks like it will match requests from the proxy's address
>     10.0.0.24 as well
>     as all the clients...
>
>     Try adding an exception in before the NAT rule, saying "traffic
>     from 10.0.0.24
>     should not be NATted".
>
>
>     Regards,
>
>
>     Antony.
>
>     --
>     "Once you have a panic, things tend to become rather undefined."
>
>      - murble
>
>      Please reply to the list;
>      please *don't* CC me.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/bc59f896/attachment.htm>

From monahbaki at gmail.com  Fri Mar  6 13:43:04 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 6 Mar 2015 08:43:04 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <54F9AB92.1070400@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
 <54F9AB92.1070400@gmail.com>
Message-ID: <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>

No other process on 80 is on the server. I also confirmed from the client
side if he runs "telnet www.openbsd.org 80" on his desktop, he gets a
response.

Thanks

On Fri, Mar 6, 2015 at 8:28 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

>  Did you have another listening process on 80 port on your proxy box?
>
> I.e., web-server?
>
> 06.03.15 19:26, Monah Baki ?????:
>
>  I went and changed the 10.0.0.0/8 to 10.0.0.23, which is the client
> station we are testing on, same results. Forward loop detected
>
>  Thanks
>
> On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone <
> Antony.Stone at squid.open.source.it> wrote:
>
>> On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>>
>> > Hi All,
>> >
>> > As an addition to my yesterday's issue,
>> >
>> > Tail -f cache.log, I am getting the following:
>> >
>> > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>>
>> > Any ideas?
>>
>> Is your NAT rule catching the HTTP requests from the proxy itself (as
>> well as
>> the requests from the clients) and sending *everything* to the proxy
>> (including the requests the proxy is trying to make out to the Internet)?
>>
>> I'm not an expert on Cisco or BSD, but it does strike me that your rule:
>>
>> rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
>> 3129
>>
>> looks like it will match requests from the proxy's address 10.0.0.24 as
>> well
>> as all the clients...
>>
>> Try adding an exception in before the NAT rule, saying "traffic from
>> 10.0.0.24
>> should not be NATted".
>>
>>
>> Regards,
>>
>>
>> Antony.
>>
>> --
>> "Once you have a panic, things tend to become rather undefined."
>>
>>  - murble
>>
>>                                                    Please reply to the
>> list;
>>                                                          please *don't*
>> CC me.
>>  _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/b42d5c29/attachment.htm>

From yvoinov at gmail.com  Fri Mar  6 13:44:38 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 19:44:38 +0600
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
 <54F9AB92.1070400@gmail.com>
 <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>
Message-ID: <54F9AF46.4040005@gmail.com>

Ok.

In this case this is NAT misconfiguration.

You need to check it carefully.

06.03.15 19:43, Monah Baki ?????:
> No other process on 80 is on the server. I also confirmed from the 
> client side if he runs "telnet www.openbsd.org 
> <http://www.openbsd.org> 80" on his desktop, he gets a response.
>
> Thanks
>
> On Fri, Mar 6, 2015 at 8:28 AM, Yuri Voinov <yvoinov at gmail.com 
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Did you have another listening process on 80 port on your proxy box?
>
>     I.e., web-server?
>
>     06.03.15 19:26, Monah Baki ?????:
>>     I went and changed the 10.0.0.0/8 <http://10.0.0.0/8> to
>>     10.0.0.23, which is the client station we are testing on, same
>>     results. Forward loop detected
>>
>>     Thanks
>>
>>     On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone
>>     <Antony.Stone at squid.open.source.it
>>     <mailto:Antony.Stone at squid.open.source.it>> wrote:
>>
>>         On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>>
>>         > Hi All,
>>         >
>>         > As an addition to my yesterday's issue,
>>         >
>>         > Tail -f cache.log, I am getting the following:
>>         >
>>         > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>>
>>         > Any ideas?
>>
>>         Is your NAT rule catching the HTTP requests from the proxy
>>         itself (as well as
>>         the requests from the clients) and sending *everything* to
>>         the proxy
>>         (including the requests the proxy is trying to make out to
>>         the Internet)?
>>
>>         I'm not an expert on Cisco or BSD, but it does strike me that
>>         your rule:
>>
>>         rdr pass inet proto tcp from 10.0.0.0/8 <http://10.0.0.0/8>
>>         to any port 80 -> 10.0.0.24 port 3129
>>
>>         looks like it will match requests from the proxy's address
>>         10.0.0.24 as well
>>         as all the clients...
>>
>>         Try adding an exception in before the NAT rule, saying
>>         "traffic from 10.0.0.24
>>         should not be NATted".
>>
>>
>>         Regards,
>>
>>
>>         Antony.
>>
>>         --
>>         "Once you have a panic, things tend to become rather undefined."
>>
>>          - murble
>>
>>                  Please reply to the list;
>>                        please *don't* CC me.
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org
>>         <mailto:squid-users at lists.squid-cache.org>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org  <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/4dfd96b4/attachment.htm>

From monahbaki at gmail.com  Fri Mar  6 13:47:31 2015
From: monahbaki at gmail.com (monahbaki at gmail.com)
Date: Fri, 06 Mar 2015 08:47:31 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <54F9AF46.4040005@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
 <54F9AB92.1070400@gmail.com>
 <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>
 <54F9AF46.4040005@gmail.com>
Message-ID: <20150306134731.6066327.48618.30739@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/4ed59c4c/attachment.htm>

From yvoinov at gmail.com  Fri Mar  6 13:47:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 06 Mar 2015 19:47:54 +0600
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <20150306134731.6066327.48618.30739@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
 <54F9AB92.1070400@gmail.com>
 <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>
 <54F9AF46.4040005@gmail.com> <20150306134731.6066327.48618.30739@gmail.com>
Message-ID: <54F9B00A.8050302@gmail.com>

On proxy box.

06.03.15 19:47, monahbaki at gmail.com ?????:
> From squid or router?
>
> Thanks
>
> Sent from my BlackBerry 10 smartphone on the Verizon Wireless 4G LTE 
> network.
> *From: *Yuri Voinov
> *Sent: *Friday, March 6, 2015 8:44 AM
> *To: *Monah Baki
> *Cc: *squid-users at lists.squid-cache.org
> *Subject: *Re: [squid-users] Fwd: squid intercept config
>
>
> Ok.
>
> In this case this is NAT misconfiguration.
>
> You need to check it carefully.
>
> 06.03.15 19:43, Monah Baki ?????:
>> No other process on 80 is on the server. I also confirmed from the 
>> client side if he runs "telnet www.openbsd.org 
>> <http://www.openbsd.org> 80" on his desktop, he gets a response.
>>
>> Thanks
>>
>> On Fri, Mar 6, 2015 at 8:28 AM, Yuri Voinov <yvoinov at gmail.com 
>> <mailto:yvoinov at gmail.com>> wrote:
>>
>>     Did you have another listening process on 80 port on your proxy box?
>>
>>     I.e., web-server?
>>
>>     06.03.15 19:26, Monah Baki ?????:
>>>     I went and changed the 10.0.0.0/8 <http://10.0.0.0/8> to
>>>     10.0.0.23, which is the client station we are testing on, same
>>>     results. Forward loop detected
>>>
>>>     Thanks
>>>
>>>     On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone
>>>     <Antony.Stone at squid.open.source.it
>>>     <mailto:Antony.Stone at squid.open.source.it>> wrote:
>>>
>>>         On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>>>
>>>         > Hi All,
>>>         >
>>>         > As an addition to my yesterday's issue,
>>>         >
>>>         > Tail -f cache.log, I am getting the following:
>>>         >
>>>         > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>>>
>>>         > Any ideas?
>>>
>>>         Is your NAT rule catching the HTTP requests from the proxy
>>>         itself (as well as
>>>         the requests from the clients) and sending *everything* to
>>>         the proxy
>>>         (including the requests the proxy is trying to make out to
>>>         the Internet)?
>>>
>>>         I'm not an expert on Cisco or BSD, but it does strike me
>>>         that your rule:
>>>
>>>         rdr pass inet proto tcp from 10.0.0.0/8 <http://10.0.0.0/8>
>>>         to any port 80 -> 10.0.0.24 port 3129
>>>
>>>         looks like it will match requests from the proxy's address
>>>         10.0.0.24 as well
>>>         as all the clients...
>>>
>>>         Try adding an exception in before the NAT rule, saying
>>>         "traffic from 10.0.0.24
>>>         should not be NATted".
>>>
>>>
>>>         Regards,
>>>
>>>
>>>         Antony.
>>>
>>>         --
>>>         "Once you have a panic, things tend to become rather undefined."
>>>
>>>          - murble
>>>
>>>                      Please reply to the list;
>>>                            please *don't* CC me.
>>>         _______________________________________________
>>>         squid-users mailing list
>>>         squid-users at lists.squid-cache.org
>>>         <mailto:squid-users at lists.squid-cache.org>
>>>         http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>>
>>>
>>>     _______________________________________________
>>>     squid-users mailing list
>>>     squid-users at lists.squid-cache.org  <mailto:squid-users at lists.squid-cache.org>
>>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/867da2e4/attachment.htm>

From monahbaki at gmail.com  Fri Mar  6 13:50:50 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 6 Mar 2015 08:50:50 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <54F9B00A.8050302@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <CALP3=x8=qdbndS+yOWJ0284EKoxk_5S3-wdCyaPTq+KwtDB9Zg@mail.gmail.com>
 <201503061414.03329.Antony.Stone@squid.open.source.it>
 <CALP3=x9ZdxRYfDMXLCL4auZ-MaSp0a-_=g52-vcx+ULd4FTP3A@mail.gmail.com>
 <54F9AB92.1070400@gmail.com>
 <CALP3=x8q7XgFqCmBevBNug_QQmBE+GKUq=pAtajajVfydqApUQ@mail.gmail.com>
 <54F9AF46.4040005@gmail.com> <20150306134731.6066327.48618.30739@gmail.com>
 <54F9B00A.8050302@gmail.com>
Message-ID: <CALP3=x94dGKUm29hzDDdHKYgUKTv6NoMZJ1BaKusrfL=FU8bjQ@mail.gmail.com>

http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf

So something else is missing?

On Fri, Mar 6, 2015 at 8:47 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

>  On proxy box.
>
> 06.03.15 19:47, monahbaki at gmail.com ?????:
>
> From squid or router?
>
>  Thanks
>
>  Sent from my BlackBerry 10 smartphone on the Verizon Wireless 4G LTE
> network.
>    *From: *Yuri Voinov
> *Sent: *Friday, March 6, 2015 8:44 AM
> *To: *Monah Baki
> *Cc: *squid-users at lists.squid-cache.org
> *Subject: *Re: [squid-users] Fwd: squid intercept config
>
>  Ok.
>
> In this case this is NAT misconfiguration.
>
> You need to check it carefully.
>
> 06.03.15 19:43, Monah Baki ?????:
>
>  No other process on 80 is on the server. I also confirmed from the
> client side if he runs "telnet www.openbsd.org 80" on his desktop, he
> gets a response.
>
>  Thanks
>
> On Fri, Mar 6, 2015 at 8:28 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
>>  Did you have another listening process on 80 port on your proxy box?
>>
>> I.e., web-server?
>>
>> 06.03.15 19:26, Monah Baki ?????:
>>
>>  I went and changed the 10.0.0.0/8 to 10.0.0.23, which is the client
>> station we are testing on, same results. Forward loop detected
>>
>>  Thanks
>>
>> On Fri, Mar 6, 2015 at 8:14 AM, Antony Stone <
>> Antony.Stone at squid.open.source.it> wrote:
>>
>>> On Friday 06 March 2015 at 14:03:28 (EU time), Monah Baki wrote:
>>>
>>> > Hi All,
>>> >
>>> > As an addition to my yesterday's issue,
>>> >
>>> > Tail -f cache.log, I am getting the following:
>>> >
>>> > 015/03/06 13:54:02| WARNING: Forwarding loop detected for:
>>>
>>> > Any ideas?
>>>
>>> Is your NAT rule catching the HTTP requests from the proxy itself (as
>>> well as
>>> the requests from the clients) and sending *everything* to the proxy
>>> (including the requests the proxy is trying to make out to the Internet)?
>>>
>>> I'm not an expert on Cisco or BSD, but it does strike me that your rule:
>>>
>>> rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24
>>> port 3129
>>>
>>> looks like it will match requests from the proxy's address 10.0.0.24 as
>>> well
>>> as all the clients...
>>>
>>> Try adding an exception in before the NAT rule, saying "traffic from
>>> 10.0.0.24
>>> should not be NATted".
>>>
>>>
>>> Regards,
>>>
>>>
>>> Antony.
>>>
>>> --
>>> "Once you have a panic, things tend to become rather undefined."
>>>
>>>  - murble
>>>
>>>                                                    Please reply to the
>>> list;
>>>                                                          please *don't*
>>> CC me.
>>>  _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/56777ad0/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Mar  6 13:57:37 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 6 Mar 2015 14:57:37 +0100
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <CALP3=x94dGKUm29hzDDdHKYgUKTv6NoMZJ1BaKusrfL=FU8bjQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F9B00A.8050302@gmail.com>
 <CALP3=x94dGKUm29hzDDdHKYgUKTv6NoMZJ1BaKusrfL=FU8bjQ@mail.gmail.com>
Message-ID: <201503061457.37563.Antony.Stone@squid.open.source.it>

On Friday 06 March 2015 at 14:50:50 (EU time), Monah Baki wrote:

> http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> 
> So something else is missing?

Can you run a packet sniffer on the proxy, to see what packets come in (noting 
the MAC address of the previous hop), what packets go out (to what 
address/es), and whether they then seem to come back in again (and if so, from 
which MAC address)?

That might give you a clue as to where the forwarding loop is being created.


Regards,


Antony.

-- 
How I want a drink, alcoholic of course, after the heavy chapters involving 
quantum mechanics.

 - mnemonic for 3.14159265358979

                                                   Please reply to the list;
                                                         please *don't* CC me.


From monahbaki at gmail.com  Sat Mar  7 01:11:14 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 6 Mar 2015 20:11:14 -0500
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <201503061457.37563.Antony.Stone@squid.open.source.it>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F9B00A.8050302@gmail.com>
 <CALP3=x94dGKUm29hzDDdHKYgUKTv6NoMZJ1BaKusrfL=FU8bjQ@mail.gmail.com>
 <201503061457.37563.Antony.Stone@squid.open.source.it>
Message-ID: <CALP3=x9F-244Z6Csi7iq30R5UJZ+4+o8X2z3W_EybbMwkj1gRw@mail.gmail.com>

Windows Client - 10.0.0.23 MAC (9d:3a:96)

root at ISN-PHC-CACHE:/home/support # arp -a
 (10.0.0.9) at 00:00:0c:07:ac:01 on bge0 THIS IS THE PHYSICAL INTERFACE ON
THE ROUTER
 (10.0.0.10) at 88:5a:92:63:77:81 on bge0  THIS IS THE GATEWAY IP ON THE
DESKTOP AND SQUID SERVER
 (10.0.0.24) at a0:d3:c1:06:a5:c4 on bge0 THIS IS THE SQUID SERVER


User was trying to access www.espn.com

Frame 8 and 9 is where I get my access denied.

No.     Time        Source                Destination           Protocol
Length Info
      7 0.508041    68.71.212.158         10.0.0.23             TCP
3902   80?42794 [PSH, ACK] Seq=412 Ack=401 Win=65664 Len=1460

Frame 7: 3902 bytes on wire (31216 bits), 1500 bytes captured (12000 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453922000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453922000 seconds
    [Time delta from previous captured frame: 0.000118000 seconds]
    [Time delta from previous displayed frame: 0.000118000 seconds]
    [Time since reference or first frame: 0.508041000 seconds]
    Frame Number: 7
    Frame Length: 3902 bytes (31216 bits)
    Capture Length: 1500 bytes (12000 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst:
CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst:
10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 1500
    Identification: 0x2222 (8738)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794),
Seq: 412, Ack: 401, Len: 1460

No.     Time        Source                Destination           Protocol
Length Info
      8 0.508073    68.71.212.158         10.0.0.23             TCP
170    [TCP Previous segment not captured] [TCP segment of a reassembled
PDU]

Frame 8: 170 bytes on wire (1360 bits), 170 bytes captured (1360 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453954000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453954000 seconds
    [Time delta from previous captured frame: 0.000032000 seconds]
    [Time delta from previous displayed frame: 0.000032000 seconds]
    [Time since reference or first frame: 0.508073000 seconds]
    Frame Number: 8
    Frame Length: 170 bytes (1360 bits)
    Capture Length: 170 bytes (1360 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags &&
!tcp.analysis.window_update]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst:
CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst:
10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 156
    Identification: 0x2223 (8739)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794),
Seq: 4260, Ack: 401, Len: 116

No.     Time        Source                Destination           Protocol
Length Info
      9 0.508835    10.0.0.23             68.71.212.158         TCP
60     [TCP ACKed unseen segment] 42794?80 [ACK] Seq=401 Ack=3332 Win=65536
Len=0

Frame 9: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.454716000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.454716000 seconds
    [Time delta from previous captured frame: 0.000762000 seconds]
    [Time delta from previous displayed frame: 0.000762000 seconds]
    [Time since reference or first frame: 0.508835000 seconds]
    Frame Number: 9
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags &&
!tcp.analysis.window_update]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst:
HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158
(68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x572a (22314)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a9 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80),
Seq: 401, Ack: 3332, Len: 0

On Fri, Mar 6, 2015 at 8:57 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Friday 06 March 2015 at 14:50:50 (EU time), Monah Baki wrote:
>
> > http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> >
> > So something else is missing?
>
> Can you run a packet sniffer on the proxy, to see what packets come in
> (noting
> the MAC address of the previous hop), what packets go out (to what
> address/es), and whether they then seem to come back in again (and if so,
> from
> which MAC address)?
>
> That might give you a clue as to where the forwarding loop is being
> created.
>
>
> Regards,
>
>
> Antony.
>
> --
> How I want a drink, alcoholic of course, after the heavy chapters involving
> quantum mechanics.
>
>  - mnemonic for 3.14159265358979
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/f3cd1f51/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar  7 04:26:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 07 Mar 2015 17:26:21 +1300
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
Message-ID: <54FA7DED.9090902@treenet.co.nz>

On 6/03/2015 1:19 a.m., Monah Baki wrote:
> Hi all, can anyone verify if this is correct, need to make ure that users
> will be able to access the internet via the squid.
> 
> Running FreeBSD with a single interface with Squid-3.5.2
> 
> Policy based routing on Cisco with the following:
> 
> 
> interface GigabitEthernet0/0/1.1
> 
> encapsulation dot1Q 1 native
> 
> ip address 10.0.0.9 255.255.255.0
> 
> no ip redirects
> 
> no ip unreachables
> 
> ip nat inside
> 
> standby 1 ip 10.0.0.10
> 
> standby 1 priority 120
> 
> standby 1 preempt
> 
> standby 1 name HSRP
> 
> ip policy route-map CFLOW
> 
> 
> 
> ip access-list extended REDIRECT
> 
> deny   tcp host 10.0.0.24 any eq www
> 
> permit tcp host 10.0.0.23 any eq www
> 
> 
> 
> route-map CFLOW permit 10
> 
> match ip address REDIRECT
> set ip next-hop 10.0.0.24
> 
> In my /etc/pf.conf
> rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
> 3129
> 
> # block in
> pass in log quick on bge0
> pass out log quick on bge0
> pass out keep state
> 
> and finally in my squid.conf:
> http_port 3128
> http_port 3129 intercept
> 
> 
> 
> And for testing purposes from the squid server:
>  ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
> 
> If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
> -p 3128 completely, I can access the websites.

If you omit the -p entirely squidclient assumes "-p 3128" (the proxy
default listening port), so it works exactly the same as if you had used
-p 3128 explicitly.

If you use -p 80 you also need to change the pther parameters so they
generate port-80 syntax message:
 - the -h with IP or hostname of the remote web server, and
 - the URL parameters being a relative URL, and
 - the -j parameter with Host: header domain name of the server
...
 eg.
 squidclient -h www.freebsd.org -j www.freebsd.org -p 80 /

NP: if your squidclient is too old to support -j, use this instead:
  -H 'Host: www.freebsd.org\n'

 ** this test should work from the squid box without having gone through
the proxy. Only from the client machine should it work *with* NAT
passing it through the proxy.



Using a proxy syntax message sent directly to the proxy receiving port,
or with the proxy as receiving IP on port 80 (NAT'ed to Squid) is a
guaranted forwarding loop failure.


That doesn't fix your clients issue, but hopefully makes it clear that
the above desribed test is broken enough to prevent you identifying when
the client issue is fixed if that happens on some change.

Amos


From michel.petterson at gmail.com  Sat Mar  7 04:27:12 2015
From: michel.petterson at gmail.com (Michel Peterson)
Date: Sat, 7 Mar 2015 01:27:12 -0300
Subject: [squid-users] Fwd: Squid 3.5.2 Compile Error
In-Reply-To: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>
References: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>
Message-ID: <CA+oP0hk1aoEWQjKh80_y4RGeUKRVwhgzNxxtnZ+vuL7Ypo=wsw@mail.gmail.com>

Hi friends,

I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
the following error after running the command "make all":

Making all in compat
make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
        /bin/bash ../libtool  --tag=CXX   --mode=compile g++
-DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
-Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
-pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
-O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
-o assert.lo assert.cc &&\
        mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
-I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
-D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
-MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
.libs/assert.o
In file included from ../include/squid.h:43:0,
                 from assert.cc:9:
../compat/compat.h:49:57: error: operator '&&' has no right operand
make[1]: ** [assert.lo] Erro 1
make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
make: ** [all-recursive] Erro 1



Help me plz.


From monahbaki at gmail.com  Sat Mar  7 11:33:12 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Sat, 7 Mar 2015 06:33:12 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54FA7DED.9090902@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
Message-ID: <CALP3=x_2Zdw3OrV+kaROTcgcm67UfbfwcVTC0O30L+dcku_s4g@mail.gmail.com>

Hi Amos,

Thanks for the assist. So basically from my end, the squid proxy which I am
responsible for, I shouldn't concentrate on changing any of it's
configuration, but instead tell them to try to solve on their end?
If yes, what are we looking at, their router setup?

Thanks

On Fri, Mar 6, 2015 at 11:26 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/03/2015 1:19 a.m., Monah Baki wrote:
> > Hi all, can anyone verify if this is correct, need to make ure that users
> > will be able to access the internet via the squid.
> >
> > Running FreeBSD with a single interface with Squid-3.5.2
> >
> > Policy based routing on Cisco with the following:
> >
> >
> > interface GigabitEthernet0/0/1.1
> >
> > encapsulation dot1Q 1 native
> >
> > ip address 10.0.0.9 255.255.255.0
> >
> > no ip redirects
> >
> > no ip unreachables
> >
> > ip nat inside
> >
> > standby 1 ip 10.0.0.10
> >
> > standby 1 priority 120
> >
> > standby 1 preempt
> >
> > standby 1 name HSRP
> >
> > ip policy route-map CFLOW
> >
> >
> >
> > ip access-list extended REDIRECT
> >
> > deny   tcp host 10.0.0.24 any eq www
> >
> > permit tcp host 10.0.0.23 any eq www
> >
> >
> >
> > route-map CFLOW permit 10
> >
> > match ip address REDIRECT
> > set ip next-hop 10.0.0.24
> >
> > In my /etc/pf.conf
> > rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
> > 3129
> >
> > # block in
> > pass in log quick on bge0
> > pass out log quick on bge0
> > pass out keep state
> >
> > and finally in my squid.conf:
> > http_port 3128
> > http_port 3129 intercept
> >
> >
> >
> > And for testing purposes from the squid server:
> >  ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
> >
> > If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
> > -p 3128 completely, I can access the websites.
>
> If you omit the -p entirely squidclient assumes "-p 3128" (the proxy
> default listening port), so it works exactly the same as if you had used
> -p 3128 explicitly.
>
> If you use -p 80 you also need to change the pther parameters so they
> generate port-80 syntax message:
>  - the -h with IP or hostname of the remote web server, and
>  - the URL parameters being a relative URL, and
>  - the -j parameter with Host: header domain name of the server
> ...
>  eg.
>  squidclient -h www.freebsd.org -j www.freebsd.org -p 80 /
>
> NP: if your squidclient is too old to support -j, use this instead:
>   -H 'Host: www.freebsd.org\n'
>
>  ** this test should work from the squid box without having gone through
> the proxy. Only from the client machine should it work *with* NAT
> passing it through the proxy.
>
>
>
> Using a proxy syntax message sent directly to the proxy receiving port,
> or with the proxy as receiving IP on port 80 (NAT'ed to Squid) is a
> guaranted forwarding loop failure.
>
>
> That doesn't fix your clients issue, but hopefully makes it clear that
> the above desribed test is broken enough to prevent you identifying when
> the client issue is fixed if that happens on some change.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150307/fd03f52a/attachment.htm>

From monahbaki at gmail.com  Sat Mar  7 12:09:57 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Sat, 7 Mar 2015 07:09:57 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54FA7DED.9090902@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
Message-ID: <CALP3=x_f_9mBrUZJuQ8fR128DK5jZ4m_Npd8PM+RfxeSo-Bbrg@mail.gmail.com>

Forgot to paste my test.

Basically from my squid server:
root at ISN-PHC-CACHE:/cache/squid/bin # ./squidclient -h www.cnn.com -H
'Host: www.cnn.com\n' -p 80
HTTP/1.1 302 Found
Server: Varnish
Retry-After: 0
Content-Length: 0
Location: http://edition.cnn.com80
Accept-Ranges: bytes
Date: Sat, 07 Mar 2015 12:08:21 GMT
Via: 1.1 varnish
Connection: close
X-Served-By: cache-lhr6328-LHR
X-Cache: MISS
X-Cache-Hits: 0


Thanks
Monah

On Fri, Mar 6, 2015 at 11:26 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/03/2015 1:19 a.m., Monah Baki wrote:
> > Hi all, can anyone verify if this is correct, need to make ure that users
> > will be able to access the internet via the squid.
> >
> > Running FreeBSD with a single interface with Squid-3.5.2
> >
> > Policy based routing on Cisco with the following:
> >
> >
> > interface GigabitEthernet0/0/1.1
> >
> > encapsulation dot1Q 1 native
> >
> > ip address 10.0.0.9 255.255.255.0
> >
> > no ip redirects
> >
> > no ip unreachables
> >
> > ip nat inside
> >
> > standby 1 ip 10.0.0.10
> >
> > standby 1 priority 120
> >
> > standby 1 preempt
> >
> > standby 1 name HSRP
> >
> > ip policy route-map CFLOW
> >
> >
> >
> > ip access-list extended REDIRECT
> >
> > deny   tcp host 10.0.0.24 any eq www
> >
> > permit tcp host 10.0.0.23 any eq www
> >
> >
> >
> > route-map CFLOW permit 10
> >
> > match ip address REDIRECT
> > set ip next-hop 10.0.0.24
> >
> > In my /etc/pf.conf
> > rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
> > 3129
> >
> > # block in
> > pass in log quick on bge0
> > pass out log quick on bge0
> > pass out keep state
> >
> > and finally in my squid.conf:
> > http_port 3128
> > http_port 3129 intercept
> >
> >
> >
> > And for testing purposes from the squid server:
> >  ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
> >
> > If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
> > -p 3128 completely, I can access the websites.
>
> If you omit the -p entirely squidclient assumes "-p 3128" (the proxy
> default listening port), so it works exactly the same as if you had used
> -p 3128 explicitly.
>
> If you use -p 80 you also need to change the pther parameters so they
> generate port-80 syntax message:
>  - the -h with IP or hostname of the remote web server, and
>  - the URL parameters being a relative URL, and
>  - the -j parameter with Host: header domain name of the server
> ...
>  eg.
>  squidclient -h www.freebsd.org -j www.freebsd.org -p 80 /
>
> NP: if your squidclient is too old to support -j, use this instead:
>   -H 'Host: www.freebsd.org\n'
>
>  ** this test should work from the squid box without having gone through
> the proxy. Only from the client machine should it work *with* NAT
> passing it through the proxy.
>
>
>
> Using a proxy syntax message sent directly to the proxy receiving port,
> or with the proxy as receiving IP on port 80 (NAT'ed to Squid) is a
> guaranted forwarding loop failure.
>
>
> That doesn't fix your clients issue, but hopefully makes it clear that
> the above desribed test is broken enough to prevent you identifying when
> the client issue is fixed if that happens on some change.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150307/9e987689/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar  7 13:24:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 08 Mar 2015 02:24:55 +1300
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_f_9mBrUZJuQ8fR128DK5jZ4m_Npd8PM+RfxeSo-Bbrg@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_f_9mBrUZJuQ8fR128DK5jZ4m_Npd8PM+RfxeSo-Bbrg@mail.gmail.com>
Message-ID: <54FAFC27.50909@treenet.co.nz>

On 8/03/2015 1:09 a.m., Monah Baki wrote:
> Forgot to paste my test.
> 
> Basically from my squid server:
> root at ISN-PHC-CACHE:/cache/squid/bin # ./squidclient -h www.cnn.com -H
> 'Host: www.cnn.com\n' -p 80
> HTTP/1.1 302 Found
> Server: Varnish
> Retry-After: 0
> Content-Length: 0
> Location: http://edition.cnn.com80

Um, that redirect URL is invalid. This Varnish is outputting garbage.


However, this test result does prove that output traffic from your Squid
should be fine. The test connecting to your port 3128 should confirm
that by getting the same or very similar result for normal traffic.


So the problem is on the input. It could still be at the client end, or
in the NAT redirection.

One thing I've not seen clarified in the discussion is which machine the
NAT rules have been placed (Squid box? or router?). Sorry if I missed that.
 The NAT operation MUST be done on the Squid box or the local machines
NAT system tells it the client was connecting to connect to
itself/Squid:3129 (which is the forwarding loop).

The router looks liek a Cisco device, so it must do L2 routing
redirection or WCCP to deliver packets to the Squid machine without
having altered their IP:port details in any way.

Amos



From monahbaki at gmail.com  Sat Mar  7 13:46:45 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Sat, 7 Mar 2015 08:46:45 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54FAFC27.50909@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_f_9mBrUZJuQ8fR128DK5jZ4m_Npd8PM+RfxeSo-Bbrg@mail.gmail.com>
 <54FAFC27.50909@treenet.co.nz>
Message-ID: <CALP3=x8_k-OKy+X3ZF4ven-6TUYpKD37P_nc60+G0sUdex2_0Q@mail.gmail.com>

Thanks Amos and everyone who helped me,

Will revert to client to check his Cisco device, I been banging my head for
days now troubleshooting the proxy.
He's running an old cisco hardware and IOS too.



On Sat, Mar 7, 2015 at 8:24 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/03/2015 1:09 a.m., Monah Baki wrote:
> > Forgot to paste my test.
> >
> > Basically from my squid server:
> > root at ISN-PHC-CACHE:/cache/squid/bin # ./squidclient -h www.cnn.com -H
> > 'Host: www.cnn.com\n' -p 80
> > HTTP/1.1 302 Found
> > Server: Varnish
> > Retry-After: 0
> > Content-Length: 0
> > Location: http://edition.cnn.com80
>
> Um, that redirect URL is invalid. This Varnish is outputting garbage.
>
>
> However, this test result does prove that output traffic from your Squid
> should be fine. The test connecting to your port 3128 should confirm
> that by getting the same or very similar result for normal traffic.
>
>
> So the problem is on the input. It could still be at the client end, or
> in the NAT redirection.
>
> One thing I've not seen clarified in the discussion is which machine the
> NAT rules have been placed (Squid box? or router?). Sorry if I missed that.
>  The NAT operation MUST be done on the Squid box or the local machines
> NAT system tells it the client was connecting to connect to
> itself/Squid:3129 (which is the forwarding loop).
>
> The router looks liek a Cisco device, so it must do L2 routing
> redirection or WCCP to deliver packets to the Squid machine without
> having altered their IP:port details in any way.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150307/739643ef/attachment.htm>

From monahbaki at gmail.com  Sat Mar  7 13:52:18 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Sat, 7 Mar 2015 08:52:18 -0500
Subject: [squid-users] squid intercept config
In-Reply-To: <54FAFC27.50909@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_f_9mBrUZJuQ8fR128DK5jZ4m_Npd8PM+RfxeSo-Bbrg@mail.gmail.com>
 <54FAFC27.50909@treenet.co.nz>
Message-ID: <CALP3=x-MbDzNFzM0yC79ia1K6bPNQHxa7HgFBSzqtGfoPTy_hw@mail.gmail.com>

I forgot to paste my pf.conf

# rdr pass inet proto tcp from 10.0.0.9/32 to any port 80 -> 10.0.0.24 port
3128
# nat on bge0 inet from any to port 80 -> bge0
rdr pass inet proto tcp from 10.0.0.23 to any port 80 -> 10.0.0.24 port 3129
# pass on bge0 inet proto tcp from bge0 to bge0 port 3128

# block in
pass in log quick on bge0
pass out log quick on bge0
pass out keep state


On Sat, Mar 7, 2015 at 8:24 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/03/2015 1:09 a.m., Monah Baki wrote:
> > Forgot to paste my test.
> >
> > Basically from my squid server:
> > root at ISN-PHC-CACHE:/cache/squid/bin # ./squidclient -h www.cnn.com -H
> > 'Host: www.cnn.com\n' -p 80
> > HTTP/1.1 302 Found
> > Server: Varnish
> > Retry-After: 0
> > Content-Length: 0
> > Location: http://edition.cnn.com80
>
> Um, that redirect URL is invalid. This Varnish is outputting garbage.
>
>
> However, this test result does prove that output traffic from your Squid
> should be fine. The test connecting to your port 3128 should confirm
> that by getting the same or very similar result for normal traffic.
>
>
> So the problem is on the input. It could still be at the client end, or
> in the NAT redirection.
>
> One thing I've not seen clarified in the discussion is which machine the
> NAT rules have been placed (Squid box? or router?). Sorry if I missed that.
>  The NAT operation MUST be done on the Squid box or the local machines
> NAT system tells it the client was connecting to connect to
> itself/Squid:3129 (which is the forwarding loop).
>
> The router looks liek a Cisco device, so it must do L2 routing
> redirection or WCCP to deliver packets to the Squid machine without
> having altered their IP:port details in any way.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150307/3d4f8db2/attachment.htm>

From donny.vibianto at gmail.com  Sat Mar  7 15:02:58 2015
From: donny.vibianto at gmail.com (Donny Vibianto)
Date: Sat, 7 Mar 2015 22:02:58 +0700
Subject: [squid-users] negotiate_wrapper: fgets() failed! dying..
Message-ID: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>

Hi Guys,

After two weeks successful running several authentication in my development
environment with average 10-20 users, i encourage myself to put in my
production. it was up and ran with +-1000 users but only took 3-5 hours
then squid suddenly stopped with error:

2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
(Operation not permitted)
2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
(Operation not permitted)
2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
(Operation not permitted)
2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARupdwIysaz6zjRSqsI8V4K0X67z4t5a9aOT7WPlyWRrp+1ol2zL6CYTcfZIyAq8q3D00mf+vpIeoiDDmkUkr+vXN+xkpXkWdX5pMD1hBrF4EDOL1RIp9XjpkdfIcEgg8Oia0Ay153sPK3+Tif4bGE=
RickyC at company.local
'
2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
oYG1MIGyoAMKAQChCwYJKoZIhvcSAQICooGdBIGaYIGXBgkqhkiG9xIBAgICAG+BhzCBhKADAgEFoQMCAQ+ieDB2oAMCARKibwRtX5xuxTxrgsKQpg3Y+kUXLOng15XJ7eDByao5YtNPZByv/zRtrz13QgKkCuk+VkXnCAzaii0ri4Mxvd+4BoskIrjf5FuPP3W59wMTCtkPJD85igR/OmQ4Ch09DJ51WGwnOizMuCW+9jg6EsFa1Q==
JanTS at company.local

i use ubuntu server 14.04 with newest squid 3.5.2

Squid Cache: Version 3.5.2
Service Name: squid
configure options:  '--enable-build-info'
'--enable-removal-policies=lru,heap' '--enable-ltdl-install'
'--enable-storeio=ufs,aufs,rock' '--enable-auth-basic=LDAP'
'--enable-auth-negotiate=wrapper,kerberos'
'--enable-external-acl-helpers=LDAP_group' '--enable-translation'
'--enable-ssl-crtd' '--enable-gnuregex' '--enable-xmalloc-debug'
'--enable-xmalloc-debug-trace' '--enable-xmalloc-statistics'
'--enable-async-io' '--enable-icmp' '--enable-delay-pools'
'--enable-useragent-log' '--enable-kill-parent-hack' '--enable-htpc'
'--enable-forw-via-db' '--enable-cache-digests' '--enable-underscores'
'--enable-x-accelerator-vary' '--enable-esi' '--enable-inline'
'--enable-linux-netfilter' '--with-openssl' '--with-large-files'

here is my squid.conf:

# ===================== ACL Cachemgr
============================================
acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
acl managerAdmin src "/usr/local/squid/etc/mgradmin.txt"
acl stream url_regex -i "/usr/local/squid/etc/stream"

acl download url_regex -i "/usr/local/squid/etc/download"
acl whitelist url_regex -i "/usr/local/squid/etc/whitelist"
acl blacklist url_regex -i "/usr/local/squid/etc/blacklist"

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl http proto http
acl CONNECT method CONNECT

# ==================== Authenticate using negotiate_wrapper
=====================
auth_param negotiate program
/usr/local/squid/libexec/negotiate_wrapper_auth -d --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--kerberos /usr/local/squid/libexec/negotiate_kerberos_auth -s GSS_C_NO_NAME
auth_param negotiate children 50 startup=0 idle=1
auth_param negotiate keep_alive off
# ==================== Authenticate using NTLM
==================================
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 20 startup=0 idle=1
auth_param ntlm keep_alive off
# ==================== Authenticate using Basic LDAP
============================
auth_param basic program /usr/local/squid/libexec/basic_ldap_auth -R -b
"dc=company,dc=local" -D squid at company.local -w "password" -f
sAMAccountName=%s -h idhqvdc01.company.local,idhqvdc02.company.local
auth_param basic children 5 startup=0 idle=1
auth_param basic realm AGDS Proxy: Please enter your username and password
domain
auth_param basic credentialsttl 1 minute
# ==================== Authenticate to Group Security Actice Directory
==========
external_acl_type memberof ipv4 children-max=10 children-startup=1 %LOGIN
/usr/local/squid/libexec/ext_ldap_group_acl -R -K -S -b
"dc=company,dc=local" -D squid at company.local -w "password" -f
"(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%g,ou=groups,ou=resources,dc=company,dc=local))"
-h idhqvdc01.company.local,idhqvdc02.company.local

acl auth proxy_auth REQUIRED
# ==================== ACL Access hour user
=====================================
acl ach1 external memberof "/usr/local/squid/etc/ach1.txt" # access hour 1
acl ach2 external memberof "/usr/local/squid/etc/ach2.txt" # access hour 2
acl ach3 external memberof "/usr/local/squid/etc/ach3.txt" # access hour 3
acl ach4 external memberof "/usr/local/squid/etc/ach4.txt" # access hour 4
acl ach2time time D 10:00-11:59
acl ach2time time D 13:00-14:59
acl ach3time time D 08:00-09:59
acl ach3time time D 15:00-16:59
acl ach4time time D 08:00-16:59
acl bebastime time D 00:01-07:59 12:00-13:59 17:00-23:59

#
==============================================================================
http_access deny !Safe_ports # Deny requests to certain unsafe ports
http_access deny CONNECT !SSL_ports # Deny CONNECT to other than secure SSL
ports
http_access allow manager localhost # Only allow cachemgr access from
localhost
http_access allow manager managerAdmin
http_access deny manager

#
==============================================================================
#http_access allow localnet
http_access allow localhost
http_access deny blacklist !bebastime

http_access allow http Safe_ports whitelist
http_access allow CONNECT SSL_ports whitelist
#http_access deny all !auth

#http_access allow http Safe_ports ach1
#http_access allow CONNECT SSL_ports ach1
#http_access allow http Safe_ports ach2 !ach2time
#http_access allow CONNECT SSL_ports ach2 !ach2time
#http_access allow http Safe_ports ach3 !ach3time
#http_access allow CONNECT SSL_ports ach3 !ach3time
#http_access allow http Safe_ports ach4 !ach4time
#http_access allow CONNECT SSL_ports ach4 !ach4time

#http_access allow accesshours1
#http_access allow accesshours2 !ach2time
#http_access allow accesshours3 !ach3time
#http_access allow accesshours4 !ach3time

http_access allow ach1
http_access allow ach2 !ach2time
http_access allow ach3 !ach3time
http_access allow ach4 !ach4time

http_access deny all # Deny all other access to this proxy
#
==============================================================================

cache_dir rock /cache1/squid 97485 max-swap-rate=200 swap-timeout=300
cache_dir rock /cache2/squid 97485 max-swap-rate=200 swap-timeout=300
coredump_dir /usr/local/squid/var/cache/squid
# =============================== Refresh Pattern
==============================
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache_effective_user proxy
visible_hostname proxy.company.local
cache_mgr proxyastragraphia
cachemgr_passwd secret all
#err_page_stylesheet /usr/local/squid/etc/default.css
pinger_enable off
#workers 2

i was tried to put different acl and put my auth_param at the top of my
conf but still dying error. what should i do?
any assistant or hint would be very appreciate. thanks


Donny Vibianto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150307/5189c7a0/attachment.htm>

From gkinkie at gmail.com  Sat Mar  7 18:33:31 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sat, 7 Mar 2015 19:33:31 +0100
Subject: [squid-users] Fwd: Squid 3.5.2 Compile Error
In-Reply-To: <CA+oP0hk1aoEWQjKh80_y4RGeUKRVwhgzNxxtnZ+vuL7Ypo=wsw@mail.gmail.com>
References: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>
 <CA+oP0hk1aoEWQjKh80_y4RGeUKRVwhgzNxxtnZ+vuL7Ypo=wsw@mail.gmail.com>
Message-ID: <CA+Y8hcMxn0NFJOsN6fBC8dYVXArAheCu-C6mQroBOCVf_7oBjw@mail.gmail.com>

Could you please attach your config.log file?

Thanks

On Sat, Mar 7, 2015 at 5:27 AM, Michel Peterson
<michel.petterson at gmail.com> wrote:
> Hi friends,
>
> I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
> the following error after running the command "make all":
>
> Making all in compat
> make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
> depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
>         /bin/bash ../libtool  --tag=CXX   --mode=compile g++
> -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
> -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
> -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
> -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
> -o assert.lo assert.cc &&\
>         mv -f $depbase.Tpo $depbase.Plo
> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
> -I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
> -D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
> -MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
> .libs/assert.o
> In file included from ../include/squid.h:43:0,
>                  from assert.cc:9:
> ../compat/compat.h:49:57: error: operator '&&' has no right operand
> make[1]: ** [assert.lo] Erro 1
> make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
> make: ** [all-recursive] Erro 1
>
>
>
> Help me plz.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From squid3 at treenet.co.nz  Sun Mar  8 05:26:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 08 Mar 2015 18:26:12 +1300
Subject: [squid-users] Fwd: Squid 3.5.2 Compile Error
In-Reply-To: <CA+Y8hcMxn0NFJOsN6fBC8dYVXArAheCu-C6mQroBOCVf_7oBjw@mail.gmail.com>
References: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>
 <CA+oP0hk1aoEWQjKh80_y4RGeUKRVwhgzNxxtnZ+vuL7Ypo=wsw@mail.gmail.com>
 <CA+Y8hcMxn0NFJOsN6fBC8dYVXArAheCu-C6mQroBOCVf_7oBjw@mail.gmail.com>
Message-ID: <54FBDD74.5040803@treenet.co.nz>

On 8/03/2015 7:33 a.m., Kinkie wrote:
> Could you please attach your config.log file?

FYI: This appears to be the precompiler not treating undefined macros as
0/false. Which is kind of weird in Wheezy since that compiler version
was in use during the test development.

Amos

> 
> Thanks
> 
> On Sat, Mar 7, 2015 at 5:27 AM, Michel Peterson wrote:
>> Hi friends,
>>
>> I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
>> the following error after running the command "make all":
>>
>> Making all in compat
>> make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
>> depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
>>         /bin/bash ../libtool  --tag=CXX   --mode=compile g++
>> -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
>> -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
>> -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
>> -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
>> -o assert.lo assert.cc &&\
>>         mv -f $depbase.Tpo $depbase.Plo
>> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
>> -I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
>> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
>> -D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
>> -MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
>> .libs/assert.o
>> In file included from ../include/squid.h:43:0,
>>                  from assert.cc:9:
>> ../compat/compat.h:49:57: error: operator '&&' has no right operand
>> make[1]: ** [assert.lo] Erro 1
>> make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
>> make: ** [all-recursive] Erro 1
>>
>>
>>
>> Help me plz.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 



From gkinkie at gmail.com  Sun Mar  8 07:02:52 2015
From: gkinkie at gmail.com (Kinkie)
Date: Sun, 8 Mar 2015 08:02:52 +0100
Subject: [squid-users] Fwd: Squid 3.5.2 Compile Error
In-Reply-To: <54FBDD74.5040803@treenet.co.nz>
References: <CA+oP0h=aQ2UWzTZL0PGAdF5sNtBbtr1KN9ZmLy0tTNGo_g0t9Q@mail.gmail.com>
 <CA+oP0hk1aoEWQjKh80_y4RGeUKRVwhgzNxxtnZ+vuL7Ypo=wsw@mail.gmail.com>
 <CA+Y8hcMxn0NFJOsN6fBC8dYVXArAheCu-C6mQroBOCVf_7oBjw@mail.gmail.com>
 <54FBDD74.5040803@treenet.co.nz>
Message-ID: <CA+Y8hcOK0Ksv0NXuk3pDXakga85QjKHAQ4bgF1hMikUgQa7fEQ@mail.gmail.com>

Yes, and I thought that as well. My reaction would be to make sure
that the secondo variable is guaranteed defined as 0.

On Sun, Mar 8, 2015 at 6:26 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 8/03/2015 7:33 a.m., Kinkie wrote:
>> Could you please attach your config.log file?
>
> FYI: This appears to be the precompiler not treating undefined macros as
> 0/false. Which is kind of weird in Wheezy since that compiler version
> was in use during the test development.
>
> Amos
>
>>
>> Thanks
>>
>> On Sat, Mar 7, 2015 at 5:27 AM, Michel Peterson wrote:
>>> Hi friends,
>>>
>>> I'm trying to compile squid 3.5.2 on debian wheezy and I am getting
>>> the following error after running the command "make all":
>>>
>>> Making all in compat
>>> make[1]: Entrando no diret?rio `/root/squid-3.5.2/compat'
>>> depbase=`echo assert.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
>>>         /bin/bash ../libtool  --tag=CXX   --mode=compile g++
>>> -DHAVE_CONFIG_H   -I.. -I../include -I../lib -I../src -I../include
>>> -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror
>>> -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  -g
>>> -O2 -march=native -std=c++11 -MT assert.lo -MD -MP -MF $depbase.Tpo -c
>>> -o assert.lo assert.cc &&\
>>>         mv -f $depbase.Tpo $depbase.Plo
>>> libtool: compile:  g++ -DHAVE_CONFIG_H -I.. -I../include -I../lib
>>> -I../src -I../include -Wall -Wpointer-arith -Wwrite-strings -Wcomments
>>> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
>>> -D_FILE_OFFSET_BITS=64 -g -O2 -march=native -std=c++11 -MT assert.lo
>>> -MD -MP -MF .deps/assert.Tpo -c assert.cc  -fPIC -DPIC -o
>>> .libs/assert.o
>>> In file included from ../include/squid.h:43:0,
>>>                  from assert.cc:9:
>>> ../compat/compat.h:49:57: error: operator '&&' has no right operand
>>> make[1]: ** [assert.lo] Erro 1
>>> make[1]: Saindo do diret?rio `/root/squid-3.5.2/compat'
>>> make: ** [all-recursive] Erro 1
>>>
>>>
>>>
>>> Help me plz.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco


From hack.back at hotmail.com  Sun Mar  8 23:19:20 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 8 Mar 2015 16:19:20 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
Message-ID: <1425856760429-4670271.post@n4.nabble.com>

2015/03/09 03:27:28 kid1| cannot change current directory to
/var/spool/squid/cache/squid: (2) No such file or directory
2015/03/09 03:27:28 kid1| Current Directory is /
2015/03/09 03:27:28 kid1| Starting Squid Cache version 3.4.12 for
x86_64-unknown-linux-gnu...
2015/03/09 03:27:28 kid1| Process ID 2993
2015/03/09 03:27:28 kid1| Process Roles: worker
2015/03/09 03:27:28 kid1| With 65536 file descriptors available
2015/03/09 03:27:28 kid1| Initializing IP Cache...
2015/03/09 03:27:28 kid1| DNS Socket created at 0.0.0.0, FD 7
2015/03/09 03:27:28 kid1| Adding nameserver 127.0.0.1 from squid.conf
2015/03/09 03:27:28 kid1| helperOpenServers: Starting 40/50 'ssl_crtd'
processes
2015/03/09 03:27:28 kid1| helperOpenServers: Starting 1/1 'storeid.pl'
processes
2015/03/09 03:27:28 kid1| Logfile: opening log /var/log/squid/access.log
2015/03/09 03:27:28 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/access.log'
FATAL: xcalloc: Unable to allocate 18446744073487757627 blocks of 1 bytes!

Squid Cache (Version 3.4.12): Terminated abnormally.
CPU Usage: 0.036 seconds = 0.016 user + 0.020 sys
Maximum Resident Size: 78368 KB
Page faults with physical i/o: 0





when i remove hard drive it work !! why ??




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From johnzeng2013 at yahoo.com  Mon Mar  9 03:38:48 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 11:38:48 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp tproxy
 environment really ?
Message-ID: <54FD15C8.20801@yahoo.com>



Hello Dear All :

I face a problem recently , When i config wccp ( tproxy ) environment (
via using squid 3.5.2 ) ,

if i disable cache_dir rock part ,and it will be success for wccp(
tproxy) , and enable cache_dir aufs

#cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

but if i enable cache_dir rock part ,and it will be failure for wccp(
tproxy) and enable cache_dir aufs

cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350


Whether some of my config is error , if possible , please give me some
advisement


This is my config


thanks

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

coredump_dir /accerater/logs/webcache3/
unlinkd_program /accerater/webcache3/libexec/unlinkd
pid_filename /accerater/logs/webcache3/opmizer1/cache.pid


workers 2
cpu_affinity_map process_numbers=1,2 cores=1,3

cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350


if ${process_number} = 1

cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
cache_log /accerater/logs/webcache3/opmizer1_cache.log
cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
url_rewrite_program /accerater/webcache3/media/mediatool/media2
store_id_program /accerater/webcache3/media/mediatool/media1
unique_hostname fast_opmizer1
snmp_port 3401

#cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
16 64 min-size=262145

else

#endif


if ${process_number} = 2


cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
cache_log /accerater/logs/webcache3/opmizer2_cache.log
cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
url_rewrite_program /accerater/webcache3/media/mediatool/media4
store_id_program /accerater/webcache3/media/mediatool/media3
unique_hostname fast_opmizer2
snmp_port 3402

#cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
16 64 min-size=262145

endif

endif



http_port 127.0.0.1:3220
http_port 3221 tproxy

wccp_version 2
wccp2_router 192.168.2.1
wccp2_forwarding_method 1
wccp2_return_method 1
wccp2_assignment_method 1
wccp2_service dynamic 80
wccp2_service dynamic 90
wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
priority=240 ports=80

tcp_outgoing_address 192.168.2.2



From johnzeng2013 at yahoo.com  Mon Mar  9 03:39:00 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 11:39:00 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp tproxy
 environment really ?
Message-ID: <54FD15D4.4030109@yahoo.com>



Hello Dear All :

I face a problem recently , When i config wccp ( tproxy ) environment (
via using squid 3.5.2 ) ,

if i disable cache_dir rock part ,and it will be success for wccp(
tproxy) , and enable cache_dir aufs

#cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

but if i enable cache_dir rock part ,and it will be failure for wccp(
tproxy) and enable cache_dir aufs

cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350


Whether some of my config is error , if possible , please give me some
advisement


This is my config


thanks

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

coredump_dir /accerater/logs/webcache3/
unlinkd_program /accerater/webcache3/libexec/unlinkd
pid_filename /accerater/logs/webcache3/opmizer1/cache.pid


workers 2
cpu_affinity_map process_numbers=1,2 cores=1,3

cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350


if ${process_number} = 1

cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
cache_log /accerater/logs/webcache3/opmizer1_cache.log
cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
url_rewrite_program /accerater/webcache3/media/mediatool/media2
store_id_program /accerater/webcache3/media/mediatool/media1
unique_hostname fast_opmizer1
snmp_port 3401

#cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
16 64 min-size=262145

else

#endif


if ${process_number} = 2


cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
cache_log /accerater/logs/webcache3/opmizer2_cache.log
cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
url_rewrite_program /accerater/webcache3/media/mediatool/media4
store_id_program /accerater/webcache3/media/mediatool/media3
unique_hostname fast_opmizer2
snmp_port 3402

#cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
16 64 min-size=262145

endif

endif



http_port 127.0.0.1:3220
http_port 3221 tproxy

wccp_version 2
wccp2_router 192.168.2.1
wccp2_forwarding_method 1
wccp2_return_method 1
wccp2_assignment_method 1
wccp2_service dynamic 80
wccp2_service dynamic 90
wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
priority=240 ports=80

tcp_outgoing_address 192.168.2.2



From squid3 at treenet.co.nz  Mon Mar  9 03:57:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 09 Mar 2015 16:57:00 +1300
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1425856760429-4670271.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
Message-ID: <54FD1A0C.8060209@treenet.co.nz>

On 9/03/2015 12:19 p.m., HackXBack wrote:
> 2015/03/09 03:27:28 kid1| cannot change current directory to
> /var/spool/squid/cache/squid: (2) No such file or directory
> 2015/03/09 03:27:28 kid1| Current Directory is /
> 2015/03/09 03:27:28 kid1| Starting Squid Cache version 3.4.12 for
> x86_64-unknown-linux-gnu...
> 2015/03/09 03:27:28 kid1| Process ID 2993
> 2015/03/09 03:27:28 kid1| Process Roles: worker
> 2015/03/09 03:27:28 kid1| With 65536 file descriptors available
> 2015/03/09 03:27:28 kid1| Initializing IP Cache...
> 2015/03/09 03:27:28 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2015/03/09 03:27:28 kid1| Adding nameserver 127.0.0.1 from squid.conf
> 2015/03/09 03:27:28 kid1| helperOpenServers: Starting 40/50 'ssl_crtd'
> processes
> 2015/03/09 03:27:28 kid1| helperOpenServers: Starting 1/1 'storeid.pl'
> processes
> 2015/03/09 03:27:28 kid1| Logfile: opening log /var/log/squid/access.log
> 2015/03/09 03:27:28 kid1| WARNING: log name now starts with a module name.
> Use 'stdio:/var/log/squid/access.log'
> FATAL: xcalloc: Unable to allocate 18446744073487757627 blocks of 1 bytes!
> 
> Squid Cache (Version 3.4.12): Terminated abnormally.
> CPU Usage: 0.036 seconds = 0.016 user + 0.020 sys
> Maximum Resident Size: 78368 KB
> Page faults with physical i/o: 0
> 
> 
> when i remove hard drive it work !! why ??
> 

You have something of size 15YB being loaded from disk into memory.

A backtrace is needed to find out how.

Amos


From johnzeng2013 at yahoo.com  Mon Mar  9 04:14:16 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 12:14:16 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp tproxy
 environment really ?
In-Reply-To: <54FD15D4.4030109@yahoo.com>
References: <54FD15D4.4030109@yahoo.com>
Message-ID: <54FD1E18.30509@yahoo.com>

:
>
> Hello Dear All :
>
> I face a problem recently , When i config wccp ( tproxy ) environment (
> via using squid 3.5.2 ) ,
>
> if i disable cache_dir rock part ,and it will be success for wccp(
> tproxy) , and enable cache_dir aufs
>
> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
>
> but if i enable cache_dir rock part ,and it will be failure for wccp(
> tproxy) and enable cache_dir aufs
>
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
>
>
> Whether some of my config is error , if possible , please give me some
> advisement
>
>
> This is my config
>
>
> thanks
>
> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> coredump_dir /accerater/logs/webcache3/
> unlinkd_program /accerater/webcache3/libexec/unlinkd
> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
>
>
> workers 2
> cpu_affinity_map process_numbers=1,2 cores=1,3
>
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
>
>
> if ${process_number} = 1
>
> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
> cache_log /accerater/logs/webcache3/opmizer1_cache.log
> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
> url_rewrite_program /accerater/webcache3/media/mediatool/media2
> store_id_program /accerater/webcache3/media/mediatool/media1
> unique_hostname fast_opmizer1
> snmp_port 3401
>
> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
>
> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
> 16 64 min-size=262145
>
> else
>
> #endif
>
>
> if ${process_number} = 2
>
>
> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
> cache_log /accerater/logs/webcache3/opmizer2_cache.log
> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
> url_rewrite_program /accerater/webcache3/media/mediatool/media4
> store_id_program /accerater/webcache3/media/mediatool/media3
> unique_hostname fast_opmizer2
> snmp_port 3402
>
> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
>
> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
> 16 64 min-size=262145
>
> endif
>
> endif
>
>
>
> http_port 127.0.0.1:3220
> http_port 3221 tproxy
>
> wccp_version 2
> wccp2_router 192.168.2.1
> wccp2_forwarding_method 1
> wccp2_return_method 1
> wccp2_assignment_method 1
> wccp2_service dynamic 80
> wccp2_service dynamic 90
> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
> priority=240 ports=80
>
> tcp_outgoing_address 192.168.2.2
>



From nathan at getoffmalawn.com  Mon Mar  9 04:52:31 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 9 Mar 2015 15:52:31 +1100
Subject: [squid-users] SSL Peek-n-Splice and exclusions by SNI
Message-ID: <CAGUJm7bZs4eYJOesKpid_rodNCUB=-U41jUyWvnzGbcgd19KJQ@mail.gmail.com>

Hi folks,

I'm playing with 3.5.2 and Peek-n-Splice, I was wondering if it's
actually possible to exclude requests based on the SNI host and have
Squid still bump correcty. I've been trying with this configuration,
using a simple external acl:

https_port 60443 intercept ssl-bump cert=/path/to/inspectcert.pem
key=/path/to/inspectkey.pem generate-host-certificates=on
external_acl_type sni ttl=30 concurrency=60 children-max=3
children-startup=1 %ssl::>sni /usr/libexec/bumphelper

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

acl sslbump_exclusions external sni

ssl_bump peek step1 all
ssl_bump splice step2 sslbump_exclusions
ssl_bump bump all

Where bumphelper is a very simple Python script:

while True:
    req = sys.stdin.readline()

    if not req:
        break

    id, sni = req.split()

    sys.stderr.write('request %r\n' % req)
    sys.stderr.flush()

    if sni == 'google.com':  # bypass
        sys.stdout.write('{} OK\n'.format(id))
        sys.stdout.flush()
    else:
        sys.stdout.write('{} ERR\n'.format(id))
        sys.stdout.flush()

The result I'm seeing is that requesting "https://youtube.com" gets
inspected as it should, but "https://google.com" results in a
handshake error, when it should have been bypassed, like so:

[~]# openssl s_client -connect google.com:443 -servername google.com
CONNECTED(00000003)
140086124283808:error:140790E5:SSL routines:SSL23_WRITE:ssl handshake
failure:s23_lib.c:177:
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 268 bytes
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
---

So what am I missing? It's very hard to find documentation about this,
so I might put this up on the wiki as an example once it's sorted.

Cheers,

Nathan.


From squid3 at treenet.co.nz  Mon Mar  9 05:01:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 09 Mar 2015 18:01:00 +1300
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FD15C8.20801@yahoo.com>
References: <54FD15C8.20801@yahoo.com>
Message-ID: <54FD290C.3080201@treenet.co.nz>

On 9/03/2015 4:38 p.m., johnzeng wrote:
> 
> 
> Hello Dear All :
> 
> I face a problem recently , When i config wccp ( tproxy ) environment (
> via using squid 3.5.2 ) ,
> 
> if i disable cache_dir rock part ,and it will be success for wccp(
> tproxy) , and enable cache_dir aufs
> 
> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> 
> but if i enable cache_dir rock part ,and it will be failure for wccp(
> tproxy) and enable cache_dir aufs
> 
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> 
> 
> Whether some of my config is error , if possible , please give me some
> advisement
> 

For starters,
 WCCP is a network protocol Squid uses to inform remote routers that it
is active and what traffic it can receive.
 rock is a layout format for bits stored on a disk.
 ... they are *completely* unrelated.



> 
> This is my config
> 
> 
> thanks
> 
> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> coredump_dir /accerater/logs/webcache3/
> unlinkd_program /accerater/webcache3/libexec/unlinkd
> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
> 
> 
> workers 2
> cpu_affinity_map process_numbers=1,2 cores=1,3
> 
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350

You are telling Squid to start two controllers to the database file
/accerater/webcache3/storage/rock1 from *each* worker. There is zero
benefit from this and the two controllers may enounter collisions as
they compete for acces to the disk without sharing atomic locks. That
leads to cache corruption.

Remove one of those two lines.


> 
> if ${process_number} = 1
> 
> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1

Dont use cache_swap_state.

> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid

Use this instead (mind the wrap):

access_log
stdio:/accerater/logs/webcache/opmizer${process_number}_access.log squid

> cache_log /accerater/logs/webcache3/opmizer1_cache.log


Use this instead:

cache_log /accerater/logs/webcache3/opmizer${process_number}_cache.log

> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log

You should not need cache_store_log at all.

Either remove it or use this instead (mind the wrap):

cache_store_log
stdio:/accerater/logs/webcache3/opmizer${process_number}_store.log


> url_rewrite_program /accerater/webcache3/media/mediatool/media2
> store_id_program /accerater/webcache3/media/mediatool/media1

Why do you have different binary executable names for the two workers
helpers?

If they are actually different, then the traffic will have different
things applied randomly depending on which worker happened to accept the
TCP connection. If they are the same, then you only need to define them
once and workers will start their own sets as needed.


> unique_hostname fast_opmizer1
> snmp_port 3401

Use this instead:

 unique_hostname fast_opmizer${process_number}
 snmp_port 340${process_number}


All of the above details can move up out of the per-worker area.


> 
> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> 
> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
> 16 64 min-size=262145
> 
> else
> 
> #endif
> 
> 
> if ${process_number} = 2
> 
> 
> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
> cache_log /accerater/logs/webcache3/opmizer2_cache.log
> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
> url_rewrite_program /accerater/webcache3/media/mediatool/media4
> store_id_program /accerater/webcache3/media/mediatool/media3
> unique_hostname fast_opmizer2
> snmp_port 3402
> 

Same notes as for worker 1.


> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
> max-size=262144 max-swap-rate=250 swap-timeout=350
> 
> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
> 16 64 min-size=262145
> 
> endif
> 
> endif
> 
> 
> 
> http_port 127.0.0.1:3220
> http_port 3221 tproxy
> 
> wccp_version 2
> wccp2_router 192.168.2.1
> wccp2_forwarding_method 1
> wccp2_return_method 1
> wccp2_assignment_method 1
> wccp2_service dynamic 80
> wccp2_service dynamic 90
> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
> priority=240 ports=80

Both workers are telling the WCCP router in different packets that they
are available on the same IP:port. In theory that should work fine,
since the router is just getting twice as many updates as it needs to
keep the proxy registered. In practice some people are finding that
certain routers cant cope with the extra registration operations.


> 
> tcp_outgoing_address 192.168.2.2
> 

Be aware that TPROXY spoof the client outgong address. This line has no
effect on the TPROXY intercepted traffic. Only the traffic received on
port 3220 will be using this outgoing address.
 Your WCCP rules need to account for that by not depending on the packet
IP addresses.


Amos


From squid3 at treenet.co.nz  Mon Mar  9 05:06:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 09 Mar 2015 18:06:53 +1300
Subject: [squid-users] SSL Peek-n-Splice and exclusions by SNI
In-Reply-To: <CAGUJm7bZs4eYJOesKpid_rodNCUB=-U41jUyWvnzGbcgd19KJQ@mail.gmail.com>
References: <CAGUJm7bZs4eYJOesKpid_rodNCUB=-U41jUyWvnzGbcgd19KJQ@mail.gmail.com>
Message-ID: <54FD2A6D.6080007@treenet.co.nz>

On 9/03/2015 5:52 p.m., Nathan Hoad wrote:
> Hi folks,
> 
> I'm playing with 3.5.2 and Peek-n-Splice, I was wondering if it's
> actually possible to exclude requests based on the SNI host and have
> Squid still bump correcty.

It is supposed to work, but there have been troubles. So YMMV.

> I've been trying with this configuration,
> using a simple external acl:
> 
> https_port 60443 intercept ssl-bump cert=/path/to/inspectcert.pem
> key=/path/to/inspectkey.pem generate-host-certificates=on
> external_acl_type sni ttl=30 concurrency=60 children-max=3
> children-startup=1 %ssl::>sni /usr/libexec/bumphelper
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> acl sslbump_exclusions external sni
> 
> ssl_bump peek step1 all
> ssl_bump splice step2 sslbump_exclusions
<snip>

> 
> So what am I missing? It's very hard to find documentation about this,
> so I might put this up on the wiki as an example once it's sorted.

The big issue here is ssl_bump being a fast-type access check. external
ACL helpers do not work reliably.

Amos



From nathan at getoffmalawn.com  Mon Mar  9 07:16:18 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Mon, 9 Mar 2015 18:16:18 +1100
Subject: [squid-users] SSL Peek-n-Splice and exclusions by SNI
In-Reply-To: <54FD2A6D.6080007@treenet.co.nz>
References: <CAGUJm7bZs4eYJOesKpid_rodNCUB=-U41jUyWvnzGbcgd19KJQ@mail.gmail.com>
 <54FD2A6D.6080007@treenet.co.nz>
Message-ID: <CAGUJm7bPBEFWUfJ7r_MJ8b7nxqcZ+EC6wnayuE3jMn8Xst=F5g@mail.gmail.com>

Hi Amos,

After digging through debug logs, I noticed this:

2015/03/09 14:40:12.467 | client_side.cc(2902)
concurrentRequestQueueFilled: local=74.125.23.95:443
remote=10.3.20.249:40083 FD 11 flags=33 max concurrent requests
reached (1)
2015/03/09 14:40:12.467 | client_side.cc(2903)
concurrentRequestQueueFilled: local=74.125.23.95:443
remote=10.3.20.249:40083 FD 11 flags=33 deferring new request until
one is done
2015/03/09 14:40:12.467 | client_side.cc(4365)
httpsSslBumpStep2AccessCheckDone: Failed to start fake CONNECT request
for ssl spliced connection: local=74.125.23.95:443
remote=10.3.20.249:40083 FD 11 flags=33

Which sparked my memory about a patch that Christos has for 3.5.3:
http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13766.patch

After applying this patch and rebuilding, everything works now, so
that's good. I tried using dstdomain as opposed to an external ACL and
it did not work - I suspect this is because dstdomain doesn't cover
the SNI server name, but it should be fine with Christos' server_name
ACL patch I would expect. If I get time I might try applying that to
3.5.x to see if it covers my use case, but for the time being I'll
stick with the external ACL helper.

Cheers,

Nathan.

On 9 March 2015 at 16:06, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 9/03/2015 5:52 p.m., Nathan Hoad wrote:
>> Hi folks,
>>
>> I'm playing with 3.5.2 and Peek-n-Splice, I was wondering if it's
>> actually possible to exclude requests based on the SNI host and have
>> Squid still bump correcty.
>
> It is supposed to work, but there have been troubles. So YMMV.
>
>> I've been trying with this configuration,
>> using a simple external acl:
>>
>> https_port 60443 intercept ssl-bump cert=/path/to/inspectcert.pem
>> key=/path/to/inspectkey.pem generate-host-certificates=on
>> external_acl_type sni ttl=30 concurrency=60 children-max=3
>> children-startup=1 %ssl::>sni /usr/libexec/bumphelper
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>>
>> acl sslbump_exclusions external sni
>>
>> ssl_bump peek step1 all
>> ssl_bump splice step2 sslbump_exclusions
> <snip>
>
>>
>> So what am I missing? It's very hard to find documentation about this,
>> so I might put this up on the wiki as an example once it's sorted.
>
> The big issue here is ssl_bump being a fast-type access check. external
> ACL helpers do not work reliably.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From johnzeng2013 at yahoo.com  Mon Mar  9 07:31:10 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 15:31:10 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FD290C.3080201@treenet.co.nz>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
Message-ID: <54FD4C3E.5070107@yahoo.com>

Hello Dear Amos:

                              Thanks for your reply ,  i updated my 
config in according to your advisement .

                              i do more testing for the part .  i face 
same problem still , although i understand your saying ( they are 
*completely* unrelated. ")

but it is real result via more testing still !

                               if i disable cache_dir rock part ,and it 
will be success for wccp( tproxy) connection

                              if i enable cache_dir rock part ,and it 
will be failure for wccp( tproxy)

                              it is very strange really  ,

                              Maybe there are some error  at Cache_dir 
rock , and Wccp don't running at error status ,

                               but i don't find any error logs info  
after running squid .


                 this is a config for rock

	cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096 max-size=262144 max-swap-rate=250 swap-timeout=350


this is status info

squid -z

2015/03/09 15:22:45 kid3| Creating Rock db: /accerater/webcache3/storage/rock1/rock


squid -d1

root at fastopmizer:/accerater/webcache3/sbin# 2015/03/09 15:23:34 kid3| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid4| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid3| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2015/03/09 15:23:34 kid3| Service Name: squid
2015/03/09 15:23:34 kid4| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2015/03/09 15:23:34 kid3| Process ID 12049
2015/03/09 15:23:34 kid2| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid4| Service Name: squid
2015/03/09 15:23:34 kid3| Process Roles: disker
2015/03/09 15:23:34 kid4| Process ID 12048
2015/03/09 15:23:34 kid2| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2015/03/09 15:23:34 kid3| With 4096 file descriptors available
2015/03/09 15:23:34 kid2| Service Name: squid
2015/03/09 15:23:34 kid2| Process ID 12050
2015/03/09 15:23:34 kid4| Process Roles: coordinator
2015/03/09 15:23:34 kid3| Initializing IP Cache...
2015/03/09 15:23:34 kid2| Process Roles: worker
2015/03/09 15:23:34 kid4| With 4096 file descriptors available
2015/03/09 15:23:34 kid2| With 4096 file descriptors available
2015/03/09 15:23:34 kid4| Initializing IP Cache...
2015/03/09 15:23:34 kid2| Initializing IP Cache...
2015/03/09 15:23:34 kid3| DNS Socket created at [::], FD 7
2015/03/09 15:23:34 kid3| DNS Socket created at 0.0.0.0, FD 8
2015/03/09 15:23:34 kid4| DNS Socket created at [::], FD 7
2015/03/09 15:23:34 kid4| DNS Socket created at 0.0.0.0, FD 8
2015/03/09 15:23:34 kid3| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/03/09 15:23:34 kid2| DNS Socket created at [::], FD 10
2015/03/09 15:23:34 kid4| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/03/09 15:23:34 kid1| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid2| DNS Socket created at 0.0.0.0, FD 11
2015/03/09 15:23:34 kid2| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/03/09 15:23:34 kid1| Starting Squid Cache version 3.5.2 for x86_64-unknown-linux-gnu...
2015/03/09 15:23:34 kid1| Service Name: squid
2015/03/09 15:23:34 kid1| Process ID 12051
2015/03/09 15:23:34 kid1| Process Roles: worker
2015/03/09 15:23:34 kid1| With 4096 file descriptors available
2015/03/09 15:23:34 kid1| Initializing IP Cache...
2015/03/09 15:23:34 kid1| DNS Socket created at [::], FD 10
2015/03/09 15:23:34 kid1| DNS Socket created at 0.0.0.0, FD 11
2015/03/09 15:23:34 kid1| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/03/09 15:23:34 kid3| Logfile: opening log daemon:/accerater/webcache3/var/logs/access.log
2015/03/09 15:23:34 kid4| Logfile: opening log daemon:/accerater/webcache3/var/logs/access.log
2015/03/09 15:23:34 kid3| Logfile Daemon: opening log /accerater/webcache3/var/logs/access.log

2015/03/09 15:23:34 kid4| Logfile Daemon: opening log /accerater/webcache3/var/logs/access.log
2015/03/09 15:23:34 kid2| Logfile: opening log stdio:/accerater/logs/webcache3/accessb.log
2015/03/09 15:23:34 kid1| Logfile: opening log stdio:/accerater/logs/webcache3/accessa.log
2015/03/09 15:23:34 kid2| Logfile: opening log stdio:/accerater/logs/webcache3/storeb.log1
2015/03/09 15:23:34 kid2| WARNING: disk-cache maximum object size is too large for mem-cache: 102400.00 KB > 90.00 KB
2015/03/09 15:23:34 kid2| Swap maxSize 10444800 + 1024000 KB, estimated 882215 objects
2015/03/09 15:23:34 kid2| Target number of buckets: 44110
2015/03/09 15:23:34 kid2| Using 65536 Store buckets
2015/03/09 15:23:34 kid2| Max Mem  size: 1024000 KB [shared]
2015/03/09 15:23:34 kid2| Max Swap size: 10444800 KB
2015/03/09 15:23:34 kid1| Logfile: opening log stdio:/accerater/logs/webcache3/storea.log1
2015/03/09 15:23:34 kid1| WARNING: disk-cache maximum object size is too large for mem-cache: 102400.00 KB > 90.00 KB
2015/03/09 15:23:34 kid1| Swap maxSize 10444800 + 1024000 KB, estimated 882215 objects
2015/03/09 15:23:34 kid1| Target number of buckets: 44110
2015/03/09 15:23:34 kid1| Using 65536 Store buckets
2015/03/09 15:23:34 kid1| Max Mem  size: 1024000 KB [shared]
2015/03/09 15:23:34 kid1| Max Swap size: 10444800 KB
2015/03/09 15:23:34 kid2| Rebuilding storage in /accerater/webcache3/storage/aufs2/2 (dirty log)
2015/03/09 15:23:34 kid2| Using Least Load store dir selection
2015/03/09 15:23:34 kid2| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid1| Rebuilding storage in /accerater/webcache3/storage/aufs1/1 (dirty log)
2015/03/09 15:23:34 kid1| Using Least Load store dir selection
2015/03/09 15:23:34 kid1| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid3| Store logging disabled
2015/03/09 15:23:34 kid3| Swap maxSize 2709504 + 1024000 KB, estimated 287192 objects
2015/03/09 15:23:34 kid3| Target number of buckets: 14359
2015/03/09 15:23:34 kid4| Store logging disabled
2015/03/09 15:23:34 kid3| Using 16384 Store buckets
2015/03/09 15:23:34 kid3| Max Mem  size: 1024000 KB [shared]
2015/03/09 15:23:34 kid4| Swap maxSize 0 + 1024000 KB, estimated 78769 objects
2015/03/09 15:23:34 kid3| Max Swap size: 2709504 KB
2015/03/09 15:23:34 kid4| Target number of buckets: 3938
2015/03/09 15:23:34 kid4| Using 8192 Store buckets
2015/03/09 15:23:34 kid4| Max Mem  size: 1024000 KB [shared]
2015/03/09 15:23:34 kid4| Max Swap size: 0 KB
2015/03/09 15:23:34 kid4| Using Least Load store dir selection
2015/03/09 15:23:34 kid4| Set Current Directory to /accerater/logs/webcache3/opmizer1

2015/03/09 15:23:34 kid3| Using Least Load store dir selection
2015/03/09 15:23:34 kid3| Set Current Directory to /accerater/logs/webcache3/opmizer1
2015/03/09 15:23:34 kid1| Finished loading MIME types and icons.
2015/03/09 15:23:34 kid1| HTCP Disabled.
2015/03/09 15:23:34 kid1| Sending SNMP messages from [::]:3401
2015/03/09 15:23:34 kid2| Finished loading MIME types and icons.
2015/03/09 15:23:34 kid2| HTCP Disabled.
2015/03/09 15:23:34 kid2| Sending SNMP messages from [::]:3402
2015/03/09 15:23:34 kid1| Squid plugin modules loaded: 0
2015/03/09 15:23:34 kid1| Adaptation support is off.
2015/03/09 15:23:34 kid2| Squid plugin modules loaded: 0
2015/03/09 15:23:34 kid2| Adaptation support is off.
2015/03/09 15:23:34 kid1| Done reading /accerater/webcache3/storage/aufs1/1 swaplog (10 entries)
2015/03/09 15:23:34 kid2| Done reading /accerater/webcache3/storage/aufs2/2 swaplog (13 entries)
2015/03/09 15:23:34 kid4| Finished loading MIME types and icons.
2015/03/09 15:23:34 kid3| Finished loading MIME types and icons.
2015/03/09 15:23:34 kid4| Accepting WCCPv2 messages on port 2048, FD 11.
2015/03/09 15:23:34 kid4| Initialising all WCCPv2 lists
2015/03/09 15:23:34 kid3| Squid plugin modules loaded: 0
2015/03/09 15:23:34 kid3| Adaptation support is off.
2015/03/09 15:23:34 kid3| Loading cache_dir #0 from /accerater/webcache3/storage/rock1/rock
2015/03/09 15:23:34 kid4| Squid plugin modules loaded: 0
2015/03/09 15:23:34 kid4| Adaptation support is off.
2015/03/09 15:23:34 kid3| Store rebuilding is 0.59% complete
2015/03/09 15:23:35 kid1| Finished rebuilding storage from disk.
2015/03/09 15:23:35 kid1|        10 Entries scanned
2015/03/09 15:23:35 kid1|         0 Invalid entries.
2015/03/09 15:23:35 kid1|         0 With invalid flags.
2015/03/09 15:23:35 kid1|        10 Objects loaded.
2015/03/09 15:23:35 kid1|         0 Objects expired.
2015/03/09 15:23:35 kid1|         0 Objects cancelled.
2015/03/09 15:23:35 kid1|         0 Duplicate URLs purged.
2015/03/09 15:23:35 kid1|         0 Swapfile clashes avoided.
2015/03/09 15:23:35 kid1|   Took 1.01 seconds (  9.88 objects/sec).
2015/03/09 15:23:35 kid2| Finished rebuilding storage from disk.
2015/03/09 15:23:35 kid1| Beginning Validation Procedure
2015/03/09 15:23:35 kid2|        13 Entries scanned
2015/03/09 15:23:35 kid2|         0 Invalid entries.

2015/03/09 15:23:35 kid2|         0 With invalid flags.
2015/03/09 15:23:35 kid2|        13 Objects loaded.
2015/03/09 15:23:35 kid2|         0 Objects expired.
2015/03/09 15:23:35 kid2|         0 Objects cancelled.
2015/03/09 15:23:35 kid2|         0 Duplicate URLs purged.
2015/03/09 15:23:35 kid2|         0 Swapfile clashes avoided.
2015/03/09 15:23:35 kid2|   Took 1.01 seconds ( 12.84 objects/sec).
2015/03/09 15:23:35 kid2| Beginning Validation Procedure
2015/03/09 15:23:35 kid1|   Completed Validation Procedure
2015/03/09 15:23:35 kid1|   Validated 10 Entries
2015/03/09 15:23:35 kid1|   store_swap_size = 8840.00 KB
2015/03/09 15:23:35 kid1| Accepting HTTP Socket connections at local=127.0.0.1:3220 remote=[::] FD 26 flags=1
2015/03/09 15:23:35 kid1| Accepting TPROXY intercepted HTTP Socket connections at local=[::]:3221 remote=[::] FD 27 flags=17
2015/03/09 15:23:35 kid1| Accepting SNMP messages on [::]:3401
2015/03/09 15:23:35 kid2|   Completed Validation Procedure
2015/03/09 15:23:35 kid2|   Validated 13 Entries
2015/03/09 15:23:35 kid2|   store_swap_size = 16852.00 KB
2015/03/09 15:23:35 kid2| Accepting HTTP Socket connections at local=127.0.0.1:3220 remote=[::] FD 26 flags=1
2015/03/09 15:23:35 kid2| Accepting TPROXY intercepted HTTP Socket connections at local=[::]:3221 remote=[::] FD 27 flags=17
2015/03/09 15:23:35 kid2| Accepting SNMP messages on [::]:3402
2015/03/09 15:23:36 kid3| Finished rebuilding storage from disk.
2015/03/09 15:23:36 kid3|    169343 Entries scanned
2015/03/09 15:23:36 kid3|         0 Invalid entries.
2015/03/09 15:23:36 kid3|         0 With invalid flags.
2015/03/09 15:23:36 kid3|         0 Objects loaded.
2015/03/09 15:23:36 kid3|         0 Objects expired.
2015/03/09 15:23:36 kid3|         0 Objects cancelled.
2015/03/09 15:23:36 kid3|         0 Duplicate URLs purged.
2015/03/09 15:23:36 kid3|         0 Swapfile clashes avoided.
2015/03/09 15:23:36 kid3|   Took 1.40 seconds (  0.00 objects/sec).
2015/03/09 15:23:36 kid3| Beginning Validation Procedure
2015/03/09 15:23:36 kid3|   Completed Validation Procedure
2015/03/09 15:23:36 kid3|   Validated 0 Entries
2015/03/09 15:23:36 kid3|   store_swap_size = 16.00 KB
2015/03/09 15:23:36 kid3| storeLateRelease: released 0 objects
2015/03/09 15:23:36 kid1| storeLateRelease: released 0 objects
2015/03/09 15:23:36 kid2| storeLateRelease: released 0 objects






? 2015?03?09? 13:01, Amos Jeffries ??:
> On 9/03/2015 4:38 p.m., johnzeng wrote:
>>
>> Hello Dear All :
>>
>> I face a problem recently , When i config wccp ( tproxy ) environment (
>> via using squid 3.5.2 ) ,
>>
>> if i disable cache_dir rock part ,and it will be success for wccp(
>> tproxy) , and enable cache_dir aufs
>>
>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> but if i enable cache_dir rock part ,and it will be failure for wccp(
>> tproxy) and enable cache_dir aufs
>>
>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>>
>> Whether some of my config is error , if possible , please give me some
>> advisement
>>
> For starters,
>   WCCP is a network protocol Squid uses to inform remote routers that it
> is active and what traffic it can receive.
>   rock is a layout format for bits stored on a disk.
>   ... they are *completely* unrelated.
>
>
>
>> This is my config
>>
>>
>> thanks
>>
>> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>
>> coredump_dir /accerater/logs/webcache3/
>> unlinkd_program /accerater/webcache3/libexec/unlinkd
>> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
>>
>>
>> workers 2
>> cpu_affinity_map process_numbers=1,2 cores=1,3
>>
>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
> You are telling Squid to start two controllers to the database file
> /accerater/webcache3/storage/rock1 from *each* worker. There is zero
> benefit from this and the two controllers may enounter collisions as
> they compete for acces to the disk without sharing atomic locks. That
> leads to cache corruption.
>
> Remove one of those two lines.
>
>
>> if ${process_number} = 1
>>
>> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
> Dont use cache_swap_state.
>
>> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
> Use this instead (mind the wrap):
>
> access_log
> stdio:/accerater/logs/webcache/opmizer${process_number}_access.log squid
>
>> cache_log /accerater/logs/webcache3/opmizer1_cache.log
>
> Use this instead:
>
> cache_log /accerater/logs/webcache3/opmizer${process_number}_cache.log
>
>> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
> You should not need cache_store_log at all.
>
> Either remove it or use this instead (mind the wrap):
>
> cache_store_log
> stdio:/accerater/logs/webcache3/opmizer${process_number}_store.log
>
>
>> url_rewrite_program /accerater/webcache3/media/mediatool/media2
>> store_id_program /accerater/webcache3/media/mediatool/media1
> Why do you have different binary executable names for the two workers
> helpers?
>
> If they are actually different, then the traffic will have different
> things applied randomly depending on which worker happened to accept the
> TCP connection. If they are the same, then you only need to define them
> once and workers will start their own sets as needed.
>
>
>> unique_hostname fast_opmizer1
>> snmp_port 3401
> Use this instead:
>
>   unique_hostname fast_opmizer${process_number}
>   snmp_port 340${process_number}
>
>
> All of the above details can move up out of the per-worker area.
>
>
>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
>> 16 64 min-size=262145
>>
>> else
>>
>> #endif
>>
>>
>> if ${process_number} = 2
>>
>>
>> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
>> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
>> cache_log /accerater/logs/webcache3/opmizer2_cache.log
>> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
>> url_rewrite_program /accerater/webcache3/media/mediatool/media4
>> store_id_program /accerater/webcache3/media/mediatool/media3
>> unique_hostname fast_opmizer2
>> snmp_port 3402
>>
> Same notes as for worker 1.
>
>
>> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
>> 16 64 min-size=262145
>>
>> endif
>>
>> endif
>>
>>
>>
>> http_port 127.0.0.1:3220
>> http_port 3221 tproxy
>>
>> wccp_version 2
>> wccp2_router 192.168.2.1
>> wccp2_forwarding_method 1
>> wccp2_return_method 1
>> wccp2_assignment_method 1
>> wccp2_service dynamic 80
>> wccp2_service dynamic 90
>> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
>> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
>> priority=240 ports=80
> Both workers are telling the WCCP router in different packets that they
> are available on the same IP:port. In theory that should work fine,
> since the router is just getting twice as many updates as it needs to
> keep the proxy registered. In practice some people are finding that
> certain routers cant cope with the extra registration operations.
>
>
>> tcp_outgoing_address 192.168.2.2
>>
> Be aware that TPROXY spoof the client outgong address. This line has no
> effect on the TPROXY intercepted traffic. Only the traffic received on
> port 3220 will be using this outgoing address.
>   Your WCCP rules need to account for that by not depending on the packet
> IP addresses.
>
>
> Amos



From johnzeng2013 at yahoo.com  Mon Mar  9 09:28:28 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 17:28:28 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FD4C3E.5070107@yahoo.com>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD4C3E.5070107@yahoo.com>
Message-ID: <54FD67BC.2030407@yahoo.com>


> Hello Dear Amos:
>
>                              Thanks for your reply ,  i updated my 
> config in according to your advisement .
>
>                              i do more testing for the part .  i face 
> same problem still , although i understand your saying ( they are 
> *completely* unrelated. ")
>
> but it is real result via more testing still !
>
>                               if i disable cache_dir rock part ,and it 
> will be success for wccp( tproxy) connection
>
>                              if i enable cache_dir rock part ,and it 
> will be failure for wccp( tproxy)
>
>                              it is very strange really  ,
>
>                              Maybe there are some error  at Cache_dir 
> rock , and Wccp don't running at error status ,
>
>                               but i don't find any error logs info  
> after running squid .
>
>
>                 this is a config for rock
>
>     cache_dir rock /accerater/webcache3/storage/rock1 2646 
> min-size=4096 max-size=262144 max-swap-rate=250 swap-timeout=350
>
>
> this is status info
>
> squid -z
>
> 2015/03/09 15:22:45 kid3| Creating Rock db: 
> /accerater/webcache3/storage/rock1/rock
>
>
> squid -d1
>
> root at fastopmizer:/accerater/webcache3/sbin# 2015/03/09 15:23:34 kid3| 
> Set Current Directory to /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid4| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid3| Starting Squid Cache version 3.5.2 for 
> x86_64-unknown-linux-gnu...
> 2015/03/09 15:23:34 kid3| Service Name: squid
> 2015/03/09 15:23:34 kid4| Starting Squid Cache version 3.5.2 for 
> x86_64-unknown-linux-gnu...
> 2015/03/09 15:23:34 kid3| Process ID 12049
> 2015/03/09 15:23:34 kid2| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid4| Service Name: squid
> 2015/03/09 15:23:34 kid3| Process Roles: disker
> 2015/03/09 15:23:34 kid4| Process ID 12048
> 2015/03/09 15:23:34 kid2| Starting Squid Cache version 3.5.2 for 
> x86_64-unknown-linux-gnu...
> 2015/03/09 15:23:34 kid3| With 4096 file descriptors available
> 2015/03/09 15:23:34 kid2| Service Name: squid
> 2015/03/09 15:23:34 kid2| Process ID 12050
> 2015/03/09 15:23:34 kid4| Process Roles: coordinator
> 2015/03/09 15:23:34 kid3| Initializing IP Cache...
> 2015/03/09 15:23:34 kid2| Process Roles: worker
> 2015/03/09 15:23:34 kid4| With 4096 file descriptors available
> 2015/03/09 15:23:34 kid2| With 4096 file descriptors available
> 2015/03/09 15:23:34 kid4| Initializing IP Cache...
> 2015/03/09 15:23:34 kid2| Initializing IP Cache...
> 2015/03/09 15:23:34 kid3| DNS Socket created at [::], FD 7
> 2015/03/09 15:23:34 kid3| DNS Socket created at 0.0.0.0, FD 8
> 2015/03/09 15:23:34 kid4| DNS Socket created at [::], FD 7
> 2015/03/09 15:23:34 kid4| DNS Socket created at 0.0.0.0, FD 8
> 2015/03/09 15:23:34 kid3| Adding nameserver 127.0.0.1 from 
> /etc/resolv.conf
> 2015/03/09 15:23:34 kid2| DNS Socket created at [::], FD 10
> 2015/03/09 15:23:34 kid4| Adding nameserver 127.0.0.1 from 
> /etc/resolv.conf
> 2015/03/09 15:23:34 kid1| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid2| DNS Socket created at 0.0.0.0, FD 11
> 2015/03/09 15:23:34 kid2| Adding nameserver 127.0.0.1 from 
> /etc/resolv.conf
> 2015/03/09 15:23:34 kid1| Starting Squid Cache version 3.5.2 for 
> x86_64-unknown-linux-gnu...
> 2015/03/09 15:23:34 kid1| Service Name: squid
> 2015/03/09 15:23:34 kid1| Process ID 12051
> 2015/03/09 15:23:34 kid1| Process Roles: worker
> 2015/03/09 15:23:34 kid1| With 4096 file descriptors available
> 2015/03/09 15:23:34 kid1| Initializing IP Cache...
> 2015/03/09 15:23:34 kid1| DNS Socket created at [::], FD 10
> 2015/03/09 15:23:34 kid1| DNS Socket created at 0.0.0.0, FD 11
> 2015/03/09 15:23:34 kid1| Adding nameserver 127.0.0.1 from 
> /etc/resolv.conf
> 2015/03/09 15:23:34 kid3| Logfile: opening log 
> daemon:/accerater/webcache3/var/logs/access.log
> 2015/03/09 15:23:34 kid4| Logfile: opening log 
> daemon:/accerater/webcache3/var/logs/access.log
> 2015/03/09 15:23:34 kid3| Logfile Daemon: opening log 
> /accerater/webcache3/var/logs/access.log
>
> 2015/03/09 15:23:34 kid4| Logfile Daemon: opening log 
> /accerater/webcache3/var/logs/access.log
> 2015/03/09 15:23:34 kid2| Logfile: opening log 
> stdio:/accerater/logs/webcache3/accessb.log
> 2015/03/09 15:23:34 kid1| Logfile: opening log 
> stdio:/accerater/logs/webcache3/accessa.log
> 2015/03/09 15:23:34 kid2| Logfile: opening log 
> stdio:/accerater/logs/webcache3/storeb.log1
> 2015/03/09 15:23:34 kid2| WARNING: disk-cache maximum object size is 
> too large for mem-cache: 102400.00 KB > 90.00 KB
> 2015/03/09 15:23:34 kid2| Swap maxSize 10444800 + 1024000 KB, 
> estimated 882215 objects
> 2015/03/09 15:23:34 kid2| Target number of buckets: 44110
> 2015/03/09 15:23:34 kid2| Using 65536 Store buckets
> 2015/03/09 15:23:34 kid2| Max Mem  size: 1024000 KB [shared]
> 2015/03/09 15:23:34 kid2| Max Swap size: 10444800 KB
> 2015/03/09 15:23:34 kid1| Logfile: opening log 
> stdio:/accerater/logs/webcache3/storea.log1
> 2015/03/09 15:23:34 kid1| WARNING: disk-cache maximum object size is 
> too large for mem-cache: 102400.00 KB > 90.00 KB
> 2015/03/09 15:23:34 kid1| Swap maxSize 10444800 + 1024000 KB, 
> estimated 882215 objects
> 2015/03/09 15:23:34 kid1| Target number of buckets: 44110
> 2015/03/09 15:23:34 kid1| Using 65536 Store buckets
> 2015/03/09 15:23:34 kid1| Max Mem  size: 1024000 KB [shared]
> 2015/03/09 15:23:34 kid1| Max Swap size: 10444800 KB
> 2015/03/09 15:23:34 kid2| Rebuilding storage in 
> /accerater/webcache3/storage/aufs2/2 (dirty log)
> 2015/03/09 15:23:34 kid2| Using Least Load store dir selection
> 2015/03/09 15:23:34 kid2| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid1| Rebuilding storage in 
> /accerater/webcache3/storage/aufs1/1 (dirty log)
> 2015/03/09 15:23:34 kid1| Using Least Load store dir selection
> 2015/03/09 15:23:34 kid1| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid3| Store logging disabled
> 2015/03/09 15:23:34 kid3| Swap maxSize 2709504 + 1024000 KB, estimated 
> 287192 objects
> 2015/03/09 15:23:34 kid3| Target number of buckets: 14359
> 2015/03/09 15:23:34 kid4| Store logging disabled
> 2015/03/09 15:23:34 kid3| Using 16384 Store buckets
> 2015/03/09 15:23:34 kid3| Max Mem  size: 1024000 KB [shared]
> 2015/03/09 15:23:34 kid4| Swap maxSize 0 + 1024000 KB, estimated 78769 
> objects
> 2015/03/09 15:23:34 kid3| Max Swap size: 2709504 KB
> 2015/03/09 15:23:34 kid4| Target number of buckets: 3938
> 2015/03/09 15:23:34 kid4| Using 8192 Store buckets
> 2015/03/09 15:23:34 kid4| Max Mem  size: 1024000 KB [shared]
> 2015/03/09 15:23:34 kid4| Max Swap size: 0 KB
> 2015/03/09 15:23:34 kid4| Using Least Load store dir selection
> 2015/03/09 15:23:34 kid4| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
>
> 2015/03/09 15:23:34 kid3| Using Least Load store dir selection
> 2015/03/09 15:23:34 kid3| Set Current Directory to 
> /accerater/logs/webcache3/opmizer1
> 2015/03/09 15:23:34 kid1| Finished loading MIME types and icons.
> 2015/03/09 15:23:34 kid1| HTCP Disabled.
> 2015/03/09 15:23:34 kid1| Sending SNMP messages from [::]:3401
> 2015/03/09 15:23:34 kid2| Finished loading MIME types and icons.
> 2015/03/09 15:23:34 kid2| HTCP Disabled.
> 2015/03/09 15:23:34 kid2| Sending SNMP messages from [::]:3402
> 2015/03/09 15:23:34 kid1| Squid plugin modules loaded: 0
> 2015/03/09 15:23:34 kid1| Adaptation support is off.
> 2015/03/09 15:23:34 kid2| Squid plugin modules loaded: 0
> 2015/03/09 15:23:34 kid2| Adaptation support is off.
> 2015/03/09 15:23:34 kid1| Done reading 
> /accerater/webcache3/storage/aufs1/1 swaplog (10 entries)
> 2015/03/09 15:23:34 kid2| Done reading 
> /accerater/webcache3/storage/aufs2/2 swaplog (13 entries)
> 2015/03/09 15:23:34 kid4| Finished loading MIME types and icons.
> 2015/03/09 15:23:34 kid3| Finished loading MIME types and icons.
> 2015/03/09 15:23:34 kid4| Accepting WCCPv2 messages on port 2048, FD 11.
> 2015/03/09 15:23:34 kid4| Initialising all WCCPv2 lists
> 2015/03/09 15:23:34 kid3| Squid plugin modules loaded: 0
> 2015/03/09 15:23:34 kid3| Adaptation support is off.
> 2015/03/09 15:23:34 kid3| Loading cache_dir #0 from 
> /accerater/webcache3/storage/rock1/rock
> 2015/03/09 15:23:34 kid4| Squid plugin modules loaded: 0
> 2015/03/09 15:23:34 kid4| Adaptation support is off.
> 2015/03/09 15:23:34 kid3| Store rebuilding is 0.59% complete
> 2015/03/09 15:23:35 kid1| Finished rebuilding storage from disk.
> 2015/03/09 15:23:35 kid1|        10 Entries scanned
> 2015/03/09 15:23:35 kid1|         0 Invalid entries.
> 2015/03/09 15:23:35 kid1|         0 With invalid flags.
> 2015/03/09 15:23:35 kid1|        10 Objects loaded.
> 2015/03/09 15:23:35 kid1|         0 Objects expired.
> 2015/03/09 15:23:35 kid1|         0 Objects cancelled.
> 2015/03/09 15:23:35 kid1|         0 Duplicate URLs purged.
> 2015/03/09 15:23:35 kid1|         0 Swapfile clashes avoided.
> 2015/03/09 15:23:35 kid1|   Took 1.01 seconds (  9.88 objects/sec).
> 2015/03/09 15:23:35 kid2| Finished rebuilding storage from disk.
> 2015/03/09 15:23:35 kid1| Beginning Validation Procedure
> 2015/03/09 15:23:35 kid2|        13 Entries scanned
> 2015/03/09 15:23:35 kid2|         0 Invalid entries.
>
> 2015/03/09 15:23:35 kid2|         0 With invalid flags.
> 2015/03/09 15:23:35 kid2|        13 Objects loaded.
> 2015/03/09 15:23:35 kid2|         0 Objects expired.
> 2015/03/09 15:23:35 kid2|         0 Objects cancelled.
> 2015/03/09 15:23:35 kid2|         0 Duplicate URLs purged.
> 2015/03/09 15:23:35 kid2|         0 Swapfile clashes avoided.
> 2015/03/09 15:23:35 kid2|   Took 1.01 seconds ( 12.84 objects/sec).
> 2015/03/09 15:23:35 kid2| Beginning Validation Procedure
> 2015/03/09 15:23:35 kid1|   Completed Validation Procedure
> 2015/03/09 15:23:35 kid1|   Validated 10 Entries
> 2015/03/09 15:23:35 kid1|   store_swap_size = 8840.00 KB
> 2015/03/09 15:23:35 kid1| Accepting HTTP Socket connections at 
> local=127.0.0.1:3220 remote=[::] FD 26 flags=1
> 2015/03/09 15:23:35 kid1| Accepting TPROXY intercepted HTTP Socket 
> connections at local=[::]:3221 remote=[::] FD 27 flags=17
> 2015/03/09 15:23:35 kid1| Accepting SNMP messages on [::]:3401
> 2015/03/09 15:23:35 kid2|   Completed Validation Procedure
> 2015/03/09 15:23:35 kid2|   Validated 13 Entries
> 2015/03/09 15:23:35 kid2|   store_swap_size = 16852.00 KB
> 2015/03/09 15:23:35 kid2| Accepting HTTP Socket connections at 
> local=127.0.0.1:3220 remote=[::] FD 26 flags=1
> 2015/03/09 15:23:35 kid2| Accepting TPROXY intercepted HTTP Socket 
> connections at local=[::]:3221 remote=[::] FD 27 flags=17
> 2015/03/09 15:23:35 kid2| Accepting SNMP messages on [::]:3402
> 2015/03/09 15:23:36 kid3| Finished rebuilding storage from disk.
> 2015/03/09 15:23:36 kid3|    169343 Entries scanned
> 2015/03/09 15:23:36 kid3|         0 Invalid entries.
> 2015/03/09 15:23:36 kid3|         0 With invalid flags.
> 2015/03/09 15:23:36 kid3|         0 Objects loaded.
> 2015/03/09 15:23:36 kid3|         0 Objects expired.
> 2015/03/09 15:23:36 kid3|         0 Objects cancelled.
> 2015/03/09 15:23:36 kid3|         0 Duplicate URLs purged.
> 2015/03/09 15:23:36 kid3|         0 Swapfile clashes avoided.
> 2015/03/09 15:23:36 kid3|   Took 1.40 seconds (  0.00 objects/sec).
> 2015/03/09 15:23:36 kid3| Beginning Validation Procedure
> 2015/03/09 15:23:36 kid3|   Completed Validation Procedure
> 2015/03/09 15:23:36 kid3|   Validated 0 Entries
> 2015/03/09 15:23:36 kid3|   store_swap_size = 16.00 KB
> 2015/03/09 15:23:36 kid3| storeLateRelease: released 0 objects
> 2015/03/09 15:23:36 kid1| storeLateRelease: released 0 objects
> 2015/03/09 15:23:36 kid2| storeLateRelease: released 0 objects
>
>
>
>
>
>
> ? 2015?03?09? 13:01, Amos Jeffries ??:
>> On 9/03/2015 4:38 p.m., johnzeng wrote:
>>>
>>> Hello Dear All :
>>>
>>> I face a problem recently , When i config wccp ( tproxy ) environment (
>>> via using squid 3.5.2 ) ,
>>>
>>> if i disable cache_dir rock part ,and it will be success for wccp(
>>> tproxy) , and enable cache_dir aufs
>>>
>>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>
>>> but if i enable cache_dir rock part ,and it will be failure for wccp(
>>> tproxy) and enable cache_dir aufs
>>>
>>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>
>>>
>>> Whether some of my config is error , if possible , please give me some
>>> advisement
>>>
>> For starters,
>>   WCCP is a network protocol Squid uses to inform remote routers that it
>> is active and what traffic it can receive.
>>   rock is a layout format for bits stored on a disk.
>>   ... they are *completely* unrelated.
>>
>>
>>
>>> This is my config
>>>
>>>
>>> thanks
>>>
>>> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> coredump_dir /accerater/logs/webcache3/
>>> unlinkd_program /accerater/webcache3/libexec/unlinkd
>>> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
>>>
>>>
>>> workers 2
>>> cpu_affinity_map process_numbers=1,2 cores=1,3
>>>
>>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>
>> You are telling Squid to start two controllers to the database file
>> /accerater/webcache3/storage/rock1 from *each* worker. There is zero
>> benefit from this and the two controllers may enounter collisions as
>> they compete for acces to the disk without sharing atomic locks. That
>> leads to cache corruption.
>>
>> Remove one of those two lines.
>>
>>
>>> if ${process_number} = 1
>>>
>>> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
>> Dont use cache_swap_state.
>>
>>> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
>> Use this instead (mind the wrap):
>>
>> access_log
>> stdio:/accerater/logs/webcache/opmizer${process_number}_access.log squid
>>
>>> cache_log /accerater/logs/webcache3/opmizer1_cache.log
>>
>> Use this instead:
>>
>> cache_log /accerater/logs/webcache3/opmizer${process_number}_cache.log
>>
>>> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
>> You should not need cache_store_log at all.
>>
>> Either remove it or use this instead (mind the wrap):
>>
>> cache_store_log
>> stdio:/accerater/logs/webcache3/opmizer${process_number}_store.log
>>
>>
>>> url_rewrite_program /accerater/webcache3/media/mediatool/media2
>>> store_id_program /accerater/webcache3/media/mediatool/media1
>> Why do you have different binary executable names for the two workers
>> helpers?
>>
>> If they are actually different, then the traffic will have different
>> things applied randomly depending on which worker happened to accept the
>> TCP connection. If they are the same, then you only need to define them
>> once and workers will start their own sets as needed.
>>
>>
>>> unique_hostname fast_opmizer1
>>> snmp_port 3401
>> Use this instead:
>>
>>   unique_hostname fast_opmizer${process_number}
>>   snmp_port 340${process_number}
>>
>>
>> All of the above details can move up out of the per-worker area.
>>
>>
>>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>
>>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 
>>> 5200
>>> 16 64 min-size=262145
>>>
>>> else
>>>
>>> #endif
>>>
>>>
>>> if ${process_number} = 2
>>>
>>>
>>> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
>>> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
>>> cache_log /accerater/logs/webcache3/opmizer2_cache.log
>>> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
>>> url_rewrite_program /accerater/webcache3/media/mediatool/media4
>>> store_id_program /accerater/webcache3/media/mediatool/media3
>>> unique_hostname fast_opmizer2
>>> snmp_port 3402
>>>
>> Same notes as for worker 1.
>>
>>
>>> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>
>>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 
>>> 5200
>>> 16 64 min-size=262145
>>>
>>> endif
>>>
>>> endif
>>>
>>>
>>>
>>> http_port 127.0.0.1:3220
>>> http_port 3221 tproxy
>>>
>>> wccp_version 2
>>> wccp2_router 192.168.2.1
>>> wccp2_forwarding_method 1
>>> wccp2_return_method 1
>>> wccp2_assignment_method 1
>>> wccp2_service dynamic 80
>>> wccp2_service dynamic 90
>>> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 
>>> ports=80
>>> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
>>> priority=240 ports=80
>> Both workers are telling the WCCP router in different packets that they
>> are available on the same IP:port. In theory that should work fine,
>> since the router is just getting twice as many updates as it needs to
>> keep the proxy registered. In practice some people are finding that
>> certain routers cant cope with the extra registration operations.
>>
>>
>>> tcp_outgoing_address 192.168.2.2
>>>
>> Be aware that TPROXY spoof the client outgong address. This line has no
>> effect on the TPROXY intercepted traffic. Only the traffic received on
>> port 3220 will be using this outgoing address.
>>   Your WCCP rules need to account for that by not depending on the 
>> packet
>> IP addresses.
>>
>>
>> Amos
>



From johnzeng2013 at yahoo.com  Mon Mar  9 10:53:56 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 18:53:56 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ? ( there are same problem between smp model or
 single model )
In-Reply-To: <54FD67BC.2030407@yahoo.com>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD4C3E.5070107@yahoo.com> <54FD67BC.2030407@yahoo.com>
Message-ID: <54FD7BC4.7000508@yahoo.com>

? 2015?03?09? 17:28, johnzeng ??:
>
>> Hello Dear Amos:
>>
>>                              Thanks for your reply ,  i updated my 
>> config in according to your advisement .
>>
>>                              i do more testing for the part .  i face 
>> same problem still , although i understand your saying ( they are 
>> *completely* unrelated. ")
>>
>> but it is real result via more testing still !
>>
>>                               if i disable cache_dir rock part ,and 
>> it will be success for wccp( tproxy) connection
>>
>>                              if i enable cache_dir rock part ,and it 
>> will be failure for wccp( tproxy)
>>
>>                              it is very strange really  ,
>>
>>                              Maybe there are some error  at Cache_dir 
>> rock , and Wccp don't running at error status ,
>>
>>                               but i don't find any error logs info  
>> after running squid .
>>
>>
>>                 this is a config for rock
>>
>>     cache_dir rock /accerater/webcache3/storage/rock1 2646 
>> min-size=4096 max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>>
>> this is status info
>>
>> squid -z
>>
>> 2015/03/09 15:22:45 kid3| Creating Rock db: 
>> /accerater/webcache3/storage/rock1/rock
>>
>>
>> squid -d1
>>
>> root at fastopmizer:/accerater/webcache3/sbin# 2015/03/09 15:23:34 kid3| 
>> Set Current Directory to /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid4| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid3| Starting Squid Cache version 3.5.2 for 
>> x86_64-unknown-linux-gnu...
>> 2015/03/09 15:23:34 kid3| Service Name: squid
>> 2015/03/09 15:23:34 kid4| Starting Squid Cache version 3.5.2 for 
>> x86_64-unknown-linux-gnu...
>> 2015/03/09 15:23:34 kid3| Process ID 12049
>> 2015/03/09 15:23:34 kid2| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid4| Service Name: squid
>> 2015/03/09 15:23:34 kid3| Process Roles: disker
>> 2015/03/09 15:23:34 kid4| Process ID 12048
>> 2015/03/09 15:23:34 kid2| Starting Squid Cache version 3.5.2 for 
>> x86_64-unknown-linux-gnu...
>> 2015/03/09 15:23:34 kid3| With 4096 file descriptors available
>> 2015/03/09 15:23:34 kid2| Service Name: squid
>> 2015/03/09 15:23:34 kid2| Process ID 12050
>> 2015/03/09 15:23:34 kid4| Process Roles: coordinator
>> 2015/03/09 15:23:34 kid3| Initializing IP Cache...
>> 2015/03/09 15:23:34 kid2| Process Roles: worker
>> 2015/03/09 15:23:34 kid4| With 4096 file descriptors available
>> 2015/03/09 15:23:34 kid2| With 4096 file descriptors available
>> 2015/03/09 15:23:34 kid4| Initializing IP Cache...
>> 2015/03/09 15:23:34 kid2| Initializing IP Cache...
>> 2015/03/09 15:23:34 kid3| DNS Socket created at [::], FD 7
>> 2015/03/09 15:23:34 kid3| DNS Socket created at 0.0.0.0, FD 8
>> 2015/03/09 15:23:34 kid4| DNS Socket created at [::], FD 7
>> 2015/03/09 15:23:34 kid4| DNS Socket created at 0.0.0.0, FD 8
>> 2015/03/09 15:23:34 kid3| Adding nameserver 127.0.0.1 from 
>> /etc/resolv.conf
>> 2015/03/09 15:23:34 kid2| DNS Socket created at [::], FD 10
>> 2015/03/09 15:23:34 kid4| Adding nameserver 127.0.0.1 from 
>> /etc/resolv.conf
>> 2015/03/09 15:23:34 kid1| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid2| DNS Socket created at 0.0.0.0, FD 11
>> 2015/03/09 15:23:34 kid2| Adding nameserver 127.0.0.1 from 
>> /etc/resolv.conf
>> 2015/03/09 15:23:34 kid1| Starting Squid Cache version 3.5.2 for 
>> x86_64-unknown-linux-gnu...
>> 2015/03/09 15:23:34 kid1| Service Name: squid
>> 2015/03/09 15:23:34 kid1| Process ID 12051
>> 2015/03/09 15:23:34 kid1| Process Roles: worker
>> 2015/03/09 15:23:34 kid1| With 4096 file descriptors available
>> 2015/03/09 15:23:34 kid1| Initializing IP Cache...
>> 2015/03/09 15:23:34 kid1| DNS Socket created at [::], FD 10
>> 2015/03/09 15:23:34 kid1| DNS Socket created at 0.0.0.0, FD 11
>> 2015/03/09 15:23:34 kid1| Adding nameserver 127.0.0.1 from 
>> /etc/resolv.conf
>> 2015/03/09 15:23:34 kid3| Logfile: opening log 
>> daemon:/accerater/webcache3/var/logs/access.log
>> 2015/03/09 15:23:34 kid4| Logfile: opening log 
>> daemon:/accerater/webcache3/var/logs/access.log
>> 2015/03/09 15:23:34 kid3| Logfile Daemon: opening log 
>> /accerater/webcache3/var/logs/access.log
>>
>> 2015/03/09 15:23:34 kid4| Logfile Daemon: opening log 
>> /accerater/webcache3/var/logs/access.log
>> 2015/03/09 15:23:34 kid2| Logfile: opening log 
>> stdio:/accerater/logs/webcache3/accessb.log
>> 2015/03/09 15:23:34 kid1| Logfile: opening log 
>> stdio:/accerater/logs/webcache3/accessa.log
>> 2015/03/09 15:23:34 kid2| Logfile: opening log 
>> stdio:/accerater/logs/webcache3/storeb.log1
>> 2015/03/09 15:23:34 kid2| WARNING: disk-cache maximum object size is 
>> too large for mem-cache: 102400.00 KB > 90.00 KB
>> 2015/03/09 15:23:34 kid2| Swap maxSize 10444800 + 1024000 KB, 
>> estimated 882215 objects
>> 2015/03/09 15:23:34 kid2| Target number of buckets: 44110
>> 2015/03/09 15:23:34 kid2| Using 65536 Store buckets
>> 2015/03/09 15:23:34 kid2| Max Mem  size: 1024000 KB [shared]
>> 2015/03/09 15:23:34 kid2| Max Swap size: 10444800 KB
>> 2015/03/09 15:23:34 kid1| Logfile: opening log 
>> stdio:/accerater/logs/webcache3/storea.log1
>> 2015/03/09 15:23:34 kid1| WARNING: disk-cache maximum object size is 
>> too large for mem-cache: 102400.00 KB > 90.00 KB
>> 2015/03/09 15:23:34 kid1| Swap maxSize 10444800 + 1024000 KB, 
>> estimated 882215 objects
>> 2015/03/09 15:23:34 kid1| Target number of buckets: 44110
>> 2015/03/09 15:23:34 kid1| Using 65536 Store buckets
>> 2015/03/09 15:23:34 kid1| Max Mem  size: 1024000 KB [shared]
>> 2015/03/09 15:23:34 kid1| Max Swap size: 10444800 KB
>> 2015/03/09 15:23:34 kid2| Rebuilding storage in 
>> /accerater/webcache3/storage/aufs2/2 (dirty log)
>> 2015/03/09 15:23:34 kid2| Using Least Load store dir selection
>> 2015/03/09 15:23:34 kid2| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid1| Rebuilding storage in 
>> /accerater/webcache3/storage/aufs1/1 (dirty log)
>> 2015/03/09 15:23:34 kid1| Using Least Load store dir selection
>> 2015/03/09 15:23:34 kid1| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid3| Store logging disabled
>> 2015/03/09 15:23:34 kid3| Swap maxSize 2709504 + 1024000 KB, 
>> estimated 287192 objects
>> 2015/03/09 15:23:34 kid3| Target number of buckets: 14359
>> 2015/03/09 15:23:34 kid4| Store logging disabled
>> 2015/03/09 15:23:34 kid3| Using 16384 Store buckets
>> 2015/03/09 15:23:34 kid3| Max Mem  size: 1024000 KB [shared]
>> 2015/03/09 15:23:34 kid4| Swap maxSize 0 + 1024000 KB, estimated 
>> 78769 objects
>> 2015/03/09 15:23:34 kid3| Max Swap size: 2709504 KB
>> 2015/03/09 15:23:34 kid4| Target number of buckets: 3938
>> 2015/03/09 15:23:34 kid4| Using 8192 Store buckets
>> 2015/03/09 15:23:34 kid4| Max Mem  size: 1024000 KB [shared]
>> 2015/03/09 15:23:34 kid4| Max Swap size: 0 KB
>> 2015/03/09 15:23:34 kid4| Using Least Load store dir selection
>> 2015/03/09 15:23:34 kid4| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>>
>> 2015/03/09 15:23:34 kid3| Using Least Load store dir selection
>> 2015/03/09 15:23:34 kid3| Set Current Directory to 
>> /accerater/logs/webcache3/opmizer1
>> 2015/03/09 15:23:34 kid1| Finished loading MIME types and icons.
>> 2015/03/09 15:23:34 kid1| HTCP Disabled.
>> 2015/03/09 15:23:34 kid1| Sending SNMP messages from [::]:3401
>> 2015/03/09 15:23:34 kid2| Finished loading MIME types and icons.
>> 2015/03/09 15:23:34 kid2| HTCP Disabled.
>> 2015/03/09 15:23:34 kid2| Sending SNMP messages from [::]:3402
>> 2015/03/09 15:23:34 kid1| Squid plugin modules loaded: 0
>> 2015/03/09 15:23:34 kid1| Adaptation support is off.
>> 2015/03/09 15:23:34 kid2| Squid plugin modules loaded: 0
>> 2015/03/09 15:23:34 kid2| Adaptation support is off.
>> 2015/03/09 15:23:34 kid1| Done reading 
>> /accerater/webcache3/storage/aufs1/1 swaplog (10 entries)
>> 2015/03/09 15:23:34 kid2| Done reading 
>> /accerater/webcache3/storage/aufs2/2 swaplog (13 entries)
>> 2015/03/09 15:23:34 kid4| Finished loading MIME types and icons.
>> 2015/03/09 15:23:34 kid3| Finished loading MIME types and icons.
>> 2015/03/09 15:23:34 kid4| Accepting WCCPv2 messages on port 2048, FD 11.
>> 2015/03/09 15:23:34 kid4| Initialising all WCCPv2 lists
>> 2015/03/09 15:23:34 kid3| Squid plugin modules loaded: 0
>> 2015/03/09 15:23:34 kid3| Adaptation support is off.
>> 2015/03/09 15:23:34 kid3| Loading cache_dir #0 from 
>> /accerater/webcache3/storage/rock1/rock
>> 2015/03/09 15:23:34 kid4| Squid plugin modules loaded: 0
>> 2015/03/09 15:23:34 kid4| Adaptation support is off.
>> 2015/03/09 15:23:34 kid3| Store rebuilding is 0.59% complete
>> 2015/03/09 15:23:35 kid1| Finished rebuilding storage from disk.
>> 2015/03/09 15:23:35 kid1|        10 Entries scanned
>> 2015/03/09 15:23:35 kid1|         0 Invalid entries.
>> 2015/03/09 15:23:35 kid1|         0 With invalid flags.
>> 2015/03/09 15:23:35 kid1|        10 Objects loaded.
>> 2015/03/09 15:23:35 kid1|         0 Objects expired.
>> 2015/03/09 15:23:35 kid1|         0 Objects cancelled.
>> 2015/03/09 15:23:35 kid1|         0 Duplicate URLs purged.
>> 2015/03/09 15:23:35 kid1|         0 Swapfile clashes avoided.
>> 2015/03/09 15:23:35 kid1|   Took 1.01 seconds (  9.88 objects/sec).
>> 2015/03/09 15:23:35 kid2| Finished rebuilding storage from disk.
>> 2015/03/09 15:23:35 kid1| Beginning Validation Procedure
>> 2015/03/09 15:23:35 kid2|        13 Entries scanned
>> 2015/03/09 15:23:35 kid2|         0 Invalid entries.
>>
>> 2015/03/09 15:23:35 kid2|         0 With invalid flags.
>> 2015/03/09 15:23:35 kid2|        13 Objects loaded.
>> 2015/03/09 15:23:35 kid2|         0 Objects expired.
>> 2015/03/09 15:23:35 kid2|         0 Objects cancelled.
>> 2015/03/09 15:23:35 kid2|         0 Duplicate URLs purged.
>> 2015/03/09 15:23:35 kid2|         0 Swapfile clashes avoided.
>> 2015/03/09 15:23:35 kid2|   Took 1.01 seconds ( 12.84 objects/sec).
>> 2015/03/09 15:23:35 kid2| Beginning Validation Procedure
>> 2015/03/09 15:23:35 kid1|   Completed Validation Procedure
>> 2015/03/09 15:23:35 kid1|   Validated 10 Entries
>> 2015/03/09 15:23:35 kid1|   store_swap_size = 8840.00 KB
>> 2015/03/09 15:23:35 kid1| Accepting HTTP Socket connections at 
>> local=127.0.0.1:3220 remote=[::] FD 26 flags=1
>> 2015/03/09 15:23:35 kid1| Accepting TPROXY intercepted HTTP Socket 
>> connections at local=[::]:3221 remote=[::] FD 27 flags=17
>> 2015/03/09 15:23:35 kid1| Accepting SNMP messages on [::]:3401
>> 2015/03/09 15:23:35 kid2|   Completed Validation Procedure
>> 2015/03/09 15:23:35 kid2|   Validated 13 Entries
>> 2015/03/09 15:23:35 kid2|   store_swap_size = 16852.00 KB
>> 2015/03/09 15:23:35 kid2| Accepting HTTP Socket connections at 
>> local=127.0.0.1:3220 remote=[::] FD 26 flags=1
>> 2015/03/09 15:23:35 kid2| Accepting TPROXY intercepted HTTP Socket 
>> connections at local=[::]:3221 remote=[::] FD 27 flags=17
>> 2015/03/09 15:23:35 kid2| Accepting SNMP messages on [::]:3402
>> 2015/03/09 15:23:36 kid3| Finished rebuilding storage from disk.
>> 2015/03/09 15:23:36 kid3|    169343 Entries scanned
>> 2015/03/09 15:23:36 kid3|         0 Invalid entries.
>> 2015/03/09 15:23:36 kid3|         0 With invalid flags.
>> 2015/03/09 15:23:36 kid3|         0 Objects loaded.
>> 2015/03/09 15:23:36 kid3|         0 Objects expired.
>> 2015/03/09 15:23:36 kid3|         0 Objects cancelled.
>> 2015/03/09 15:23:36 kid3|         0 Duplicate URLs purged.
>> 2015/03/09 15:23:36 kid3|         0 Swapfile clashes avoided.
>> 2015/03/09 15:23:36 kid3|   Took 1.40 seconds (  0.00 objects/sec).
>> 2015/03/09 15:23:36 kid3| Beginning Validation Procedure
>> 2015/03/09 15:23:36 kid3|   Completed Validation Procedure
>> 2015/03/09 15:23:36 kid3|   Validated 0 Entries
>> 2015/03/09 15:23:36 kid3|   store_swap_size = 16.00 KB
>> 2015/03/09 15:23:36 kid3| storeLateRelease: released 0 objects
>> 2015/03/09 15:23:36 kid1| storeLateRelease: released 0 objects
>> 2015/03/09 15:23:36 kid2| storeLateRelease: released 0 objects
>>
>>
>>
>>
>>
>>
>> ? 2015?03?09? 13:01, Amos Jeffries ??:
>>> On 9/03/2015 4:38 p.m., johnzeng wrote:
>>>>
>>>> Hello Dear All :
>>>>
>>>> I face a problem recently , When i config wccp ( tproxy ) 
>>>> environment (
>>>> via using squid 3.5.2 ) ,
>>>>
>>>> if i disable cache_dir rock part ,and it will be success for wccp(
>>>> tproxy) , and enable cache_dir aufs
>>>>
>>>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>>
>>>> but if i enable cache_dir rock part ,and it will be failure for wccp(
>>>> tproxy) and enable cache_dir aufs
>>>>
>>>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>>
>>>>
>>>> Whether some of my config is error , if possible , please give me some
>>>> advisement
>>>>
>>> For starters,
>>>   WCCP is a network protocol Squid uses to inform remote routers 
>>> that it
>>> is active and what traffic it can receive.
>>>   rock is a layout format for bits stored on a disk.
>>>   ... they are *completely* unrelated.
>>>
>>>
>>>
>>>> This is my config
>>>>
>>>>
>>>> thanks
>>>>
>>>> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
>>>>
>>>>
>>>> coredump_dir /accerater/logs/webcache3/
>>>> unlinkd_program /accerater/webcache3/libexec/unlinkd
>>>> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
>>>>
>>>>
>>>> workers 2
>>>> cpu_affinity_map process_numbers=1,2 cores=1,3
>>>>
>>>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>>
>>> You are telling Squid to start two controllers to the database file
>>> /accerater/webcache3/storage/rock1 from *each* worker. There is zero
>>> benefit from this and the two controllers may enounter collisions as
>>> they compete for acces to the disk without sharing atomic locks. That
>>> leads to cache corruption.
>>>
>>> Remove one of those two lines.
>>>
>>>
>>>> if ${process_number} = 1
>>>>
>>>> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
>>> Dont use cache_swap_state.
>>>
>>>> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
>>> Use this instead (mind the wrap):
>>>
>>> access_log
>>> stdio:/accerater/logs/webcache/opmizer${process_number}_access.log 
>>> squid
>>>
>>>> cache_log /accerater/logs/webcache3/opmizer1_cache.log
>>>
>>> Use this instead:
>>>
>>> cache_log /accerater/logs/webcache3/opmizer${process_number}_cache.log
>>>
>>>> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
>>> You should not need cache_store_log at all.
>>>
>>> Either remove it or use this instead (mind the wrap):
>>>
>>> cache_store_log
>>> stdio:/accerater/logs/webcache3/opmizer${process_number}_store.log
>>>
>>>
>>>> url_rewrite_program /accerater/webcache3/media/mediatool/media2
>>>> store_id_program /accerater/webcache3/media/mediatool/media1
>>> Why do you have different binary executable names for the two workers
>>> helpers?
>>>
>>> If they are actually different, then the traffic will have different
>>> things applied randomly depending on which worker happened to accept 
>>> the
>>> TCP connection. If they are the same, then you only need to define them
>>> once and workers will start their own sets as needed.
>>>
>>>
>>>> unique_hostname fast_opmizer1
>>>> snmp_port 3401
>>> Use this instead:
>>>
>>>   unique_hostname fast_opmizer${process_number}
>>>   snmp_port 340${process_number}
>>>
>>>
>>> All of the above details can move up out of the per-worker area.
>>>
>>>
>>>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>>
>>>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 
>>>> 5200
>>>> 16 64 min-size=262145
>>>>
>>>> else
>>>>
>>>> #endif
>>>>
>>>>
>>>> if ${process_number} = 2
>>>>
>>>>
>>>> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
>>>> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
>>>> cache_log /accerater/logs/webcache3/opmizer2_cache.log
>>>> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
>>>> url_rewrite_program /accerater/webcache3/media/mediatool/media4
>>>> store_id_program /accerater/webcache3/media/mediatool/media3
>>>> unique_hostname fast_opmizer2
>>>> snmp_port 3402
>>>>
>>> Same notes as for worker 1.
>>>
>>>
>>>> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
>>>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>>>
>>>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 
>>>> 5200
>>>> 16 64 min-size=262145
>>>>
>>>> endif
>>>>
>>>> endif
>>>>
>>>>
>>>>
>>>> http_port 127.0.0.1:3220
>>>> http_port 3221 tproxy
>>>>
>>>> wccp_version 2
>>>> wccp2_router 192.168.2.1
>>>> wccp2_forwarding_method 1
>>>> wccp2_return_method 1
>>>> wccp2_assignment_method 1
>>>> wccp2_service dynamic 80
>>>> wccp2_service dynamic 90
>>>> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 
>>>> ports=80
>>>> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
>>>> priority=240 ports=80
>>> Both workers are telling the WCCP router in different packets that they
>>> are available on the same IP:port. In theory that should work fine,
>>> since the router is just getting twice as many updates as it needs to
>>> keep the proxy registered. In practice some people are finding that
>>> certain routers cant cope with the extra registration operations.
>>>
>>>
>>>> tcp_outgoing_address 192.168.2.2
>>>>
>>> Be aware that TPROXY spoof the client outgong address. This line has no
>>> effect on the TPROXY intercepted traffic. Only the traffic received on
>>> port 3220 will be using this outgoing address.
>>>   Your WCCP rules need to account for that by not depending on the 
>>> packet
>>> IP addresses.
>>>
>>>
>>> Amos
>>
>



From johnzeng2013 at yahoo.com  Mon Mar  9 12:43:54 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 09 Mar 2015 20:43:54 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FD290C.3080201@treenet.co.nz>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
Message-ID: <54FD958A.8000100@yahoo.com>

Hello Amos:

----------------------------------------------------------------------------------------------------------------------- 


For starters,
  WCCP is a network protocol Squid uses to inform remote routers that it
is active and what traffic it can receive.
  rock is a layout format for bits stored on a disk.
  ... they are *completely* unrelated.

-------------------------------------------------------------------------

Your meaning is running two different process for wccp redirection and Cache operation ?

first process is for wccp redirection

and other process is for Cache operation


  




? 2015?03?09? 13:01, Amos Jeffries ??:
> On 9/03/2015 4:38 p.m., johnzeng wrote:
>>
>> Hello Dear All :
>>
>> I face a problem recently , When i config wccp ( tproxy ) environment (
>> via using squid 3.5.2 ) ,
>>
>> if i disable cache_dir rock part ,and it will be success for wccp(
>> tproxy) , and enable cache_dir aufs
>>
>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> but if i enable cache_dir rock part ,and it will be failure for wccp(
>> tproxy) and enable cache_dir aufs
>>
>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>>
>> Whether some of my config is error , if possible , please give me some
>> advisement
>>
> For starters,
>   WCCP is a network protocol Squid uses to inform remote routers that it
> is active and what traffic it can receive.
>   rock is a layout format for bits stored on a disk.
>   ... they are *completely* unrelated.
>
>
>
>> This is my config
>>
>>
>> thanks
>>
>> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>
>> coredump_dir /accerater/logs/webcache3/
>> unlinkd_program /accerater/webcache3/libexec/unlinkd
>> pid_filename /accerater/logs/webcache3/opmizer1/cache.pid
>>
>>
>> workers 2
>> cpu_affinity_map process_numbers=1,2 cores=1,3
>>
>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>> cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
> You are telling Squid to start two controllers to the database file
> /accerater/webcache3/storage/rock1 from *each* worker. There is zero
> benefit from this and the two controllers may enounter collisions as
> they compete for acces to the disk without sharing atomic locks. That
> leads to cache corruption.
>
> Remove one of those two lines.
>
>
>> if ${process_number} = 1
>>
>> cache_swap_state /accerater/logs/webcache3/opmizer1_swap_log1
> Dont use cache_swap_state.
>
>> access_log stdio:/accerater/logs/webcache3/opmizer1_access.log squid
> Use this instead (mind the wrap):
>
> access_log
> stdio:/accerater/logs/webcache/opmizer${process_number}_access.log squid
>
>> cache_log /accerater/logs/webcache3/opmizer1_cache.log
>
> Use this instead:
>
> cache_log /accerater/logs/webcache3/opmizer${process_number}_cache.log
>
>> cache_store_log stdio:/accerater/logs/webcache3/opmizer1_store.log
> You should not need cache_store_log at all.
>
> Either remove it or use this instead (mind the wrap):
>
> cache_store_log
> stdio:/accerater/logs/webcache3/opmizer${process_number}_store.log
>
>
>> url_rewrite_program /accerater/webcache3/media/mediatool/media2
>> store_id_program /accerater/webcache3/media/mediatool/media1
> Why do you have different binary executable names for the two workers
> helpers?
>
> If they are actually different, then the traffic will have different
> things applied randomly depending on which worker happened to accept the
> TCP connection. If they are the same, then you only need to define them
> once and workers will start their own sets as needed.
>
>
>> unique_hostname fast_opmizer1
>> snmp_port 3401
> Use this instead:
>
>   unique_hostname fast_opmizer${process_number}
>   snmp_port 340${process_number}
>
>
> All of the above details can move up out of the per-worker area.
>
>
>> #cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
>> 16 64 min-size=262145
>>
>> else
>>
>> #endif
>>
>>
>> if ${process_number} = 2
>>
>>
>> cache_swap_state /accerater/logs/webcache3/opmizer2_swap_log
>> access_log stdio:/accerater/logs/webcache3/opmizer2_access.log squid
>> cache_log /accerater/logs/webcache3/opmizer2_cache.log
>> cache_store_log stdio:/accerater/logs/webcache3/opmizer2_store.log
>> url_rewrite_program /accerater/webcache3/media/mediatool/media4
>> store_id_program /accerater/webcache3/media/mediatool/media3
>> unique_hostname fast_opmizer2
>> snmp_port 3402
>>
> Same notes as for worker 1.
>
>
>> #cache_dir rock /accerater/webcache3/storage/rock2 2646 min-size=4096
>> max-size=262144 max-swap-rate=250 swap-timeout=350
>>
>> cache_dir aufs /accerater/webcache3/storage/aufs1/${process_number} 5200
>> 16 64 min-size=262145
>>
>> endif
>>
>> endif
>>
>>
>>
>> http_port 127.0.0.1:3220
>> http_port 3221 tproxy
>>
>> wccp_version 2
>> wccp2_router 192.168.2.1
>> wccp2_forwarding_method 1
>> wccp2_return_method 1
>> wccp2_assignment_method 1
>> wccp2_service dynamic 80
>> wccp2_service dynamic 90
>> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
>> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
>> priority=240 ports=80
> Both workers are telling the WCCP router in different packets that they
> are available on the same IP:port. In theory that should work fine,
> since the router is just getting twice as many updates as it needs to
> keep the proxy registered. In practice some people are finding that
> certain routers cant cope with the extra registration operations.
>
>
>> tcp_outgoing_address 192.168.2.2
>>
> Be aware that TPROXY spoof the client outgong address. This line has no
> effect on the TPROXY intercepted traffic. Only the traffic received on
> port 3220 will be using this outgoing address.
>   Your WCCP rules need to account for that by not depending on the packet
> IP addresses.
>
>
> Amos



From donny.vibianto at gmail.com  Mon Mar  9 14:20:52 2015
From: donny.vibianto at gmail.com (Donny Vibianto)
Date: Mon, 9 Mar 2015 21:20:52 +0700
Subject: [squid-users] negotiate_wrapper: fgets() failed! dying..
In-Reply-To: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>
References: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>
Message-ID: <CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q@mail.gmail.com>

anyone please...?

On Sat, Mar 7, 2015 at 10:02 PM, Donny Vibianto <donny.vibianto at gmail.com>
wrote:

> Hi Guys,
>
> After two weeks successful running several authentication in my
> development environment with average 10-20 users, i encourage myself to put
> in my production. it was up and ran with +-1000 users but only took 3-5
> hours then squid suddenly stopped with error:
>
> 2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
> (Operation not permitted)
> 2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
> (Operation not permitted)
> 2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1
> (Operation not permitted)
> 2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
> oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARupdwIysaz6zjRSqsI8V4K0X67z4t5a9aOT7WPlyWRrp+1ol2zL6CYTcfZIyAq8q3D00mf+vpIeoiDDmkUkr+vXN+xkpXkWdX5pMD1hBrF4EDOL1RIp9XjpkdfIcEgg8Oia0Ay153sPK3+Tif4bGE=
> RickyC at company.local
> '
> 2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
> oYG1MIGyoAMKAQChCwYJKoZIhvcSAQICooGdBIGaYIGXBgkqhkiG9xIBAgICAG+BhzCBhKADAgEFoQMCAQ+ieDB2oAMCARKibwRtX5xuxTxrgsKQpg3Y+kUXLOng15XJ7eDByao5YtNPZByv/zRtrz13QgKkCuk+VkXnCAzaii0ri4Mxvd+4BoskIrjf5FuPP3W59wMTCtkPJD85igR/OmQ4Ch09DJ51WGwnOizMuCW+9jg6EsFa1Q==
> JanTS at company.local
>
> i use ubuntu server 14.04 with newest squid 3.5.2
>
> Squid Cache: Version 3.5.2
> Service Name: squid
> configure options:  '--enable-build-info'
> '--enable-removal-policies=lru,heap' '--enable-ltdl-install'
> '--enable-storeio=ufs,aufs,rock' '--enable-auth-basic=LDAP'
> '--enable-auth-negotiate=wrapper,kerberos'
> '--enable-external-acl-helpers=LDAP_group' '--enable-translation'
> '--enable-ssl-crtd' '--enable-gnuregex' '--enable-xmalloc-debug'
> '--enable-xmalloc-debug-trace' '--enable-xmalloc-statistics'
> '--enable-async-io' '--enable-icmp' '--enable-delay-pools'
> '--enable-useragent-log' '--enable-kill-parent-hack' '--enable-htpc'
> '--enable-forw-via-db' '--enable-cache-digests' '--enable-underscores'
> '--enable-x-accelerator-vary' '--enable-esi' '--enable-inline'
> '--enable-linux-netfilter' '--with-openssl' '--with-large-files'
>
> here is my squid.conf:
>
> # ===================== ACL Cachemgr
> ============================================
> acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
> acl managerAdmin src "/usr/local/squid/etc/mgradmin.txt"
> acl stream url_regex -i "/usr/local/squid/etc/stream"
>
> acl download url_regex -i "/usr/local/squid/etc/download"
> acl whitelist url_regex -i "/usr/local/squid/etc/whitelist"
> acl blacklist url_regex -i "/usr/local/squid/etc/blacklist"
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl http proto http
> acl CONNECT method CONNECT
>
> # ==================== Authenticate using negotiate_wrapper
> =====================
> auth_param negotiate program
> /usr/local/squid/libexec/negotiate_wrapper_auth -d --ntlm
> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
> --kerberos /usr/local/squid/libexec/negotiate_kerberos_auth -s GSS_C_NO_NAME
> auth_param negotiate children 50 startup=0 idle=1
> auth_param negotiate keep_alive off
> # ==================== Authenticate using NTLM
> ==================================
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp
> auth_param ntlm children 20 startup=0 idle=1
> auth_param ntlm keep_alive off
> # ==================== Authenticate using Basic LDAP
> ============================
> auth_param basic program /usr/local/squid/libexec/basic_ldap_auth -R -b
> "dc=company,dc=local" -D squid at company.local -w "password" -f
> sAMAccountName=%s -h idhqvdc01.company.local,idhqvdc02.company.local
> auth_param basic children 5 startup=0 idle=1
> auth_param basic realm AGDS Proxy: Please enter your username and password
> domain
> auth_param basic credentialsttl 1 minute
> # ==================== Authenticate to Group Security Actice Directory
> ==========
> external_acl_type memberof ipv4 children-max=10 children-startup=1 %LOGIN
> /usr/local/squid/libexec/ext_ldap_group_acl -R -K -S -b
> "dc=company,dc=local" -D squid at company.local -w "password" -f
> "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%g,ou=groups,ou=resources,dc=company,dc=local))"
> -h idhqvdc01.company.local,idhqvdc02.company.local
>
> acl auth proxy_auth REQUIRED
> # ==================== ACL Access hour user
> =====================================
> acl ach1 external memberof "/usr/local/squid/etc/ach1.txt" # access hour 1
> acl ach2 external memberof "/usr/local/squid/etc/ach2.txt" # access hour 2
> acl ach3 external memberof "/usr/local/squid/etc/ach3.txt" # access hour 3
> acl ach4 external memberof "/usr/local/squid/etc/ach4.txt" # access hour 4
> acl ach2time time D 10:00-11:59
> acl ach2time time D 13:00-14:59
> acl ach3time time D 08:00-09:59
> acl ach3time time D 15:00-16:59
> acl ach4time time D 08:00-16:59
> acl bebastime time D 00:01-07:59 12:00-13:59 17:00-23:59
>
> #
> ==============================================================================
> http_access deny !Safe_ports # Deny requests to certain unsafe ports
> http_access deny CONNECT !SSL_ports # Deny CONNECT to other than secure
> SSL ports
> http_access allow manager localhost # Only allow cachemgr access from
> localhost
> http_access allow manager managerAdmin
> http_access deny manager
>
> #
> ==============================================================================
> #http_access allow localnet
> http_access allow localhost
> http_access deny blacklist !bebastime
>
> http_access allow http Safe_ports whitelist
> http_access allow CONNECT SSL_ports whitelist
> #http_access deny all !auth
>
> #http_access allow http Safe_ports ach1
> #http_access allow CONNECT SSL_ports ach1
> #http_access allow http Safe_ports ach2 !ach2time
> #http_access allow CONNECT SSL_ports ach2 !ach2time
> #http_access allow http Safe_ports ach3 !ach3time
> #http_access allow CONNECT SSL_ports ach3 !ach3time
> #http_access allow http Safe_ports ach4 !ach4time
> #http_access allow CONNECT SSL_ports ach4 !ach4time
>
> #http_access allow accesshours1
> #http_access allow accesshours2 !ach2time
> #http_access allow accesshours3 !ach3time
> #http_access allow accesshours4 !ach3time
>
> http_access allow ach1
> http_access allow ach2 !ach2time
> http_access allow ach3 !ach3time
> http_access allow ach4 !ach4time
>
> http_access deny all # Deny all other access to this proxy
> #
> ==============================================================================
>
> cache_dir rock /cache1/squid 97485 max-swap-rate=200 swap-timeout=300
> cache_dir rock /cache2/squid 97485 max-swap-rate=200 swap-timeout=300
> coredump_dir /usr/local/squid/var/cache/squid
> # =============================== Refresh Pattern
> ==============================
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>
> cache_effective_user proxy
> visible_hostname proxy.company.local
> cache_mgr proxyastragraphia
> cachemgr_passwd secret all
> #err_page_stylesheet /usr/local/squid/etc/default.css
> pinger_enable off
> #workers 2
>
> i was tried to put different acl and put my auth_param at the top of my
> conf but still dying error. what should i do?
> any assistant or hint would be very appreciate. thanks
>
>
> Donny Vibianto
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150309/d8c8b1b7/attachment.htm>

From hack.back at hotmail.com  Mon Mar  9 19:40:52 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 9 Mar 2015 12:40:52 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <54FD1A0C.8060209@treenet.co.nz>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz>
Message-ID: <1425930052114-4670285.post@n4.nabble.com>

no core dump created after restarting




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271p4670285.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon Mar  9 19:44:00 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 9 Mar 2015 12:44:00 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1425930052114-4670285.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
Message-ID: <1425930240507-4670286.post@n4.nabble.com>

root at debian:/etc/squid# gdb /usr/sbin/squid /var/spool/squid/cache/squid/core
GNU gdb (GDB) 7.4.1-debian
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later
<http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/sbin/squid...done.
[New LWP 7263]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f938c52d165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f938c52d165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f938c5303e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x0000000000573bc5 in fatal_dump (message=0xbaf580 "xcalloc: Unable to
allocate 18446744073645296089 blocks of 1 bytes!\n") at fatal.cc:138
#3  0x00000000007ab976 in xcalloc (n=18446744073645296089, sz=1) at
xalloc.cc:82
#4  0x00000000bfebfbff in ?? ()
#5  0x0000000000000006 in ?? ()
#6  0x0000000000001000 in ?? ()
#7  0x0000000000000011 in ?? ()
#8  0x0000000000000064 in ?? ()
#9  0x0000000000000003 in ?? ()
#10 0x0000000000400040 in ?? ()
#11 0x0000000000000004 in ?? ()
#12 0x0000000000000038 in ?? ()
#13 0x0000000000000005 in ?? ()
#14 0x0000000000000008 in ?? ()
#15 0x0000000000000007 in ?? ()
#16 0x00007f3c7d951000 in ?? ()
#17 0x0000000000000008 in ?? ()
#18 0x0000000000000000 in ?? ()
(gdb)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271p4670286.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From dan at getbusi.com  Tue Mar 10 05:51:21 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Tue, 10 Mar 2015 16:51:21 +1100
Subject: [squid-users] Random SSL bump DB corruption
Message-ID: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>

Hey folks

After having many of our systems running Squid 3.4.12 for a couple of weeks now we had two different deployments fail today due to SSL DB corruption.

Never seen this in almost 9 months of SSL bump being in production and there were no problems in either cache log until the ?wrong number of fields? lines, apparently.

Anyone else?

Deployment #1 log excerpt:
wrong number of fields on line 505 (looking for field 6, got 1, '' left)
(squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
2015/03/10 09:04:24 kid1| WARNING: ssl_crtd #Hlpr0 exited
2015/03/10 09:04:24 kid1| Too few ssl_crtd processes are running (need 1/32)
2015/03/10 09:04:24 kid1| Starting new helpers
2015/03/10 09:04:24 kid1| helperOpenServers: Starting 1/32 'squid_ssl_crtd' processes
2015/03/10 09:04:24 kid1| "ssl_crtd" helper returned <NULL> reply.
wrong number of fields on line 505 (looking for field 6, got 1, '' left)
(squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild

Deployment #2 log excerpt:
wrong number of fields on line 2 (looking for field 6, got 1, '' left)
(squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
2015/03/10 15:29:16 kid1| WARNING: ssl_crtd #Hlpr0 exited
2015/03/10 15:29:16 kid1| Too few ssl_crtd processes are running (need 1/32)
2015/03/10 15:29:16 kid1| Starting new helpers
2015/03/10 15:29:16 kid1| helperOpenServers: Starting 1/32 'squid_ssl_crtd' processes
2015/03/10 15:29:17 kid1| "ssl_crtd" helper returned <NULL> reply.
wrong number of fields on line 2 (looking for field 6, got 1, '' left)
(squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150310/3c194aaa/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 10 08:30:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Mar 2015 21:30:41 +1300
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1425930240507-4670286.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com>
Message-ID: <54FEABB1.20402@treenet.co.nz>

On 10/03/2015 8:44 a.m., HackXBack wrote:
> root at debian:/etc/squid# gdb /usr/sbin/squid /var/spool/squid/cache/squid/core
> GNU gdb (GDB) 7.4.1-debian
> Copyright (C) 2012 Free Software Foundation, Inc.
> License GPLv3+: GNU GPL version 3 or later
> <http://gnu.org/licenses/gpl.html>
> This is free software: you are free to change and redistribute it.
> There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
> and "show warranty" for details.
> This GDB was configured as "x86_64-linux-gnu".
> For bug reporting instructions, please see:
> <http://www.gnu.org/software/gdb/bugs/>...
> Reading symbols from /usr/sbin/squid...done.
> [New LWP 7263]
> 
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
> Program terminated with signal 6, Aborted.
> #0  0x00007f938c52d165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) backtrace
> #0  0x00007f938c52d165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007f938c5303e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x0000000000573bc5 in fatal_dump (message=0xbaf580 "xcalloc: Unable to
> allocate 18446744073645296089 blocks of 1 bytes!\n") at fatal.cc:138
> #3  0x00000000007ab976 in xcalloc (n=18446744073645296089, sz=1) at
> xalloc.cc:82

Oh darn. These bits ...

> #4  0x00000000bfebfbff in ?? ()
> #5  0x0000000000000006 in ?? ()
> #6  0x0000000000001000 in ?? ()
> #7  0x0000000000000011 in ?? ()
> #8  0x0000000000000064 in ?? ()
> #9  0x0000000000000003 in ?? ()
> #10 0x0000000000400040 in ?? ()
> #11 0x0000000000000004 in ?? ()
> #12 0x0000000000000038 in ?? ()
> #13 0x0000000000000005 in ?? ()
> #14 0x0000000000000008 in ?? ()
> #15 0x0000000000000007 in ?? ()
> #16 0x00007f3c7d951000 in ?? ()
> #17 0x0000000000000008 in ?? ()
> #18 0x0000000000000000 in ?? ()
> (gdb)

Are the ones we need to see symbol names from.

If you self-built it from sources it will need to be rebuilt without
symbol stripping, then a new trace gathered.

If you built it using updated sources under the .deb package there
should be a squid-dbg binary with all the symbol names in to be loaded
by gdb instead of the sbin/squid parameter.


Amos


From hack.back at hotmail.com  Tue Mar 10 09:39:31 2015
From: hack.back at hotmail.com (HackXBack)
Date: Tue, 10 Mar 2015 02:39:31 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <54FEABB1.20402@treenet.co.nz>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com> <54FEABB1.20402@treenet.co.nz>
Message-ID: <1425980371350-4670291.post@n4.nabble.com>

this is my configure option , what may cause the problem

./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
--libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
--libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
--infodir=/usr/share/info --mandir=/usr/share/man
--disable-dependency-tracking --disable-strict-error-checking
--enable-async-io=32 --with-aufs-threads=32 --with-pthreads
--enable-storeio=ufs,aufs,diskd --enable-removal-policies=lru,heap
--with-aio --with-dl --disable-icmp --enable-esi --enable-icap-client
--disable-wccp --enable-wccpv2 --enable-kill-parent-hack
--enable-cache-digests --disable-select --enable-http-violations
--enable-linux-netfilter --enable-follow-x-forwarded-for
--disable-ident-lookups --enable-x-accelerator-vary --enable-zph-qos
--with-default-user=proxy --with-logdir=/var/log/squid
--with-pidfile=/var/run/squid.pid --with-swapdir=/var/spool/squid
--with-large-files --enable-ltdl-convenience --with-filedescriptors=100000
--enable-ssl --enable-ssl-crtd --with-openssl --disable-auth --disable-ipv6
--enable-build-info="build by : ANDO_TBLRB && HackXBack"
--with-included-ltdl --enable-arp-acl --enable-epoll --enable-snmp
--enable-referer-log --disable-unlinkd --enable-truncate
--enable-useragent-log --enable-eui --enable-large-cache-files
--with-maxfd=65536 CFLAGS="-Wall -g -O3 -march=native -mtune=native -pipe
-DNUMTHREADS=60 -fomit-frame-pointer -fno-strict-aliasing -funroll-loops
-ffast-math -fno-exceptions" LDFLAGS="-Wl,-Bsymbolic-functions"




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271p4670291.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From roel at 1afa.com  Tue Mar 10 11:17:23 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 12:17:23 +0100
Subject: [squid-users] Fast acl for ip-based url
Message-ID: <cone.1425986243.435473.3977.1000@bollix>

Hi list,

is there a fast acl to match ip-based urls?

I would have thought to use dstdom_regex, but the docs say that a reverse  
lookup is done if no match is found, which means (I think) that it will  
become a slow acl for all regular urls.

Thanks,

Roel


From squid3 at treenet.co.nz  Tue Mar 10 11:28:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 00:28:52 +1300
Subject: [squid-users] Fast acl for ip-based url
In-Reply-To: <cone.1425986243.435473.3977.1000@bollix>
References: <cone.1425986243.435473.3977.1000@bollix>
Message-ID: <54FED574.40100@treenet.co.nz>

On 11/03/2015 12:17 a.m., Roel van Meer wrote:
> Hi list,
> 
> is there a fast acl to match ip-based urls?
> 
> I would have thought to use dstdom_regex, but the docs say that a
> reverse lookup is done if no match is found, which means (I think) that
> it will become a slow acl for all regular urls.

dtsdom* will only do a lookup if the URL content is a raw-IP that fails
to match the pattern. "regular URLs" already have a domain named so no
lookup is needed.

Also, if an asynchronous lookup is required by an ACL when testing in
the non-blocking (fast) access check the ACL is considered a non-match.
The inverted (!) result is also a non-match which can confuse.

Amos



From kl at vsen.dk  Tue Mar 10 12:29:15 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 10 Mar 2015 13:29:15 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat going
	on..?
Message-ID: <54FEE39B.6050908@vsen.dk>

Hi,

I just setup a squid trying to get it to work in intercept mode..

I seem to hit some squid internal loop where it goes haywire internally 
somehow?

When I access it via port 3129 (tried from both localhost and from 
another host - same problem) - using

curl -H "Host: www.bt.dk" http://ip-of-squid-box/

the request just hangs..

I enabled debug logging and can see squid logging this:
----------
2015/03/10 13:07:47.028 kid1| client_side.cc(2407) parseHttpRequest: 
HTTP Client local=127.0.0.1:3129 remote=127.0.0.1:45060 FD 17 flags=33
2015/03/10 13:07:47.028 kid1| client_side.cc(2408) parseHttpRequest: 
HTTP Client REQUEST:
---------
GET / HTTP/1.1
User-Agent: curl/7.29.0
Accept: */*
Host: www.bt.dk
X-Forwarded-For: 127.0.0.1
Cache-Control: max-age=259200
Connection: keep-alive


----------
2015/03/10 13:07:47.028 kid1| client_side_request.cc(759) 
clientAccessCheckDone: The request GET http://www.bt.dk/ is ALLOWED; 
last ACL checked: localhost
2015/03/10 13:07:47.028 kid1| client_side_request.cc(734) 
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2015/03/10 13:07:47.028 kid1| client_side_request.cc(759) 
clientAccessCheckDone: The request GET http://www.bt.dk/ is ALLOWED; 
last ACL checked: localhost
2015/03/10 13:07:47.028 kid1| FwdState.cc(124) FwdState: Forwarding 
client request local=127.0.0.1:3129 remote=127.0.0.1:45060 FD 17 
flags=33, url=http://www.bt.dk/
2015/03/10 13:07:47.028 kid1| peer_select.cc(286) peerSelectDnsPaths: 
Found sources for 'http://www.bt.dk/'
2015/03/10 13:07:47.028 kid1| peer_select.cc(287) peerSelectDnsPaths: 
always_direct = ALLOWED
2015/03/10 13:07:47.028 kid1| peer_select.cc(288) peerSelectDnsPaths: 
  never_direct = DENIED
2015/03/10 13:07:47.028 kid1| peer_select.cc(294) peerSelectDnsPaths: 
  ORIGINAL_DST = local=0.0.0.0 remote=127.0.0.1:3129 flags=1
2015/03/10 13:07:47.028 kid1| peer_select.cc(301) peerSelectDnsPaths: 
      timedout = 0
2015/03/10 13:07:47.028 kid1| TcpAcceptor.cc(220) doAccept: New 
connection on FD 14
2015/03/10 13:07:47.028 kid1| TcpAcceptor.cc(295) acceptNext: connection 
on local=[::]:3129 remote=[::] FD 14 flags=41
2015/03/10 13:07:47.029 kid1| http.cc(2219) sendRequest: HTTP Server 
local=127.0.0.1:45061 remote=127.0.0.1:3129 FD 18 flags=1
2015/03/10 13:07:47.029 kid1| http.cc(2220) sendRequest: HTTP Server 
REQUEST:
---------
GET / HTTP/1.1
User-Agent: curl/7.29.0
Accept: */*
Host: www.bt.dk
X-Forwarded-For: 127.0.0.1, 127.0.0.1
Cache-Control: max-age=259200
Connection: keep-alive

When I then ctrl-c the client request..
it ends with
X-Forwarded-For: error, 127.0.0.1, 127.0.0.1, (and 100+ of , 127.0.0.1)
##########################################
end log output snippet
##########################################

and the logs keep going with X-Forwarded-For getting , 127.0.0.1 added 
ad infinitum..

I tcpdump'ed and saw no output from squid - so it seems to be an 
internal loop in squid.

My config is:
# predefined ACLs
#acl localhost src 127.0.0.1 ::1
#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) 
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

# predefined http_accesses
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# user-defined ACLs

# user-defined http_accesses
http_access allow localnet
http_access allow localhost
http_access deny all

# user-defined icp_access

# user-defined http_port
http_port 3128

# user-defined tcp_outgoing_addresses

# user-defined cache-dirs
# general settings
hierarchy_stoplist             cgi-bin ?
coredump_dir                   /var/spool/squid
maximum_object_size_in_memory  512 KB
maximum_object_size            4096 KB
#ignore_expect_100              off
cache_mgr                      root
client_persistent_connections  on
server_persistent_connections  on
#forwarded_for                  on
via                            off
access_log                     /var/log/squid/access.log squid

cache_mem                      256 MB
cache_store_log                /var/log/squid/store.log
cache_log                      /var/log/squid/cache.log
cache                          deny all

refresh_pattern                ^ftp:              1440    20%    10080
refresh_pattern                ^gopher:           1440     0%     1440
refresh_pattern                -i (/cgi-bin/|\?)     0     0%        0
refresh_pattern                .                     0    20%     4320


# user-defined configuration settings from config_hash
sslcrtd_program                /usr/lib64/squid/ssl_crtd -s 
/etc/ssl/certs/cache/ -M 4MB
sslcrtd_children               8 startup=1 idle=1
https_port                     3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
key=/etc/squid/ca.private cert=/etc/squid/ca.cert
sslproxy_flags                 DONT_VERIFY_PEER
always_direct                  allow all
http_port                      3129 intercept
shutdown_lifetime              3
sslproxy_cert_error            allow all
ssl_bump                       server-first all


p.s. it's a self-compiled squid-3.4.9 on centos 7.

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From roel at 1afa.com  Tue Mar 10 12:29:36 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 13:29:36 +0100
Subject: [squid-users] Fast acl for ip-based url
References: <cone.1425986243.435473.3977.1000@bollix>
 <54FED574.40100@treenet.co.nz>
Message-ID: <cone.1425990576.470566.3977.1000@bollix>

Amos Jeffries writes:

> > is there a fast acl to match ip-based urls?
> >
> > I would have thought to use dstdom_regex, but the docs say that a
> > reverse lookup is done if no match is found, which means (I think) that
> > it will become a slow acl for all regular urls.
>
> dtsdom* will only do a lookup if the URL content is a raw-IP that fails
> to match the pattern. "regular URLs" already have a domain named so no
> lookup is needed.
>
> Also, if an asynchronous lookup is required by an ACL when testing in
> the non-blocking (fast) access check the ACL is considered a non-match.
> The inverted (!) result is also a non-match which can confuse.

I understand. Thanks a lot!

Regards,

Roel


From squid3 at treenet.co.nz  Tue Mar 10 12:31:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 01:31:36 +1300
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FD958A.8000100@yahoo.com>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD958A.8000100@yahoo.com>
Message-ID: <54FEE428.3080308@treenet.co.nz>

On 10/03/2015 1:43 a.m., johnzeng wrote:
> Hello Amos:
> 
> -----------------------------------------------------------------------------------------------------------------------
> 
> 
> For starters,
>  WCCP is a network protocol Squid uses to inform remote routers that it
> is active and what traffic it can receive.
>  rock is a layout format for bits stored on a disk.
>  ... they are *completely* unrelated.
> 
> -------------------------------------------------------------------------
> 
> Your meaning is running two different process for wccp redirection and
> Cache operation ?
> 

I mean they are different like shelves in a cabinet versus a spoken
sentence.

And yes, they are probably also in different processes in your setup.

Amos



From Antony.Stone at squid.open.source.it  Tue Mar 10 12:48:10 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 10 Mar 2015 13:48:10 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
	going on..?
In-Reply-To: <54FEE39B.6050908@vsen.dk>
References: <54FEE39B.6050908@vsen.dk>
Message-ID: <201503101348.10349.Antony.Stone@squid.open.source.it>

On Tuesday 10 March 2015 at 13:29:15 (EU time), Klavs Klavsen wrote:

> Hi,
> 
> I just setup a squid trying to get it to work in intercept mode.

Is it working correctly in non-intercept mode?  It can be helpful to check the 
simple setup first, and then try something more complex...

> When I access it via port 3129 (tried from both localhost and from
> another host - same problem) - using
> 
> curl -H "Host: www.bt.dk" http://ip-of-squid-box/

Um, I don't see where you're telling curl to connect to port 3129 there (and 
your subject line says you have no NAT in place).

> My config is:
> # predefined ACLs
> #acl localhost src 127.0.0.1 ::1
> #acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

So, you've disabled the definition of 'localhost', and then...

> # user-defined http_accesses
> http_access allow localnet
> http_access allow localhost
> http_access deny all

You're allowing from 'localnet' and 'localhost'...

You also say you're getting hundreds of 127.0.0.1 entries in the log output.

> # user-defined http_port
> http_port 3128

So, you're listening in proxy mode on port 3128 - does this work?

> http_port                      3129 intercept

And you're also listening on 3129 in intercept mode - does a tcpdump (on all 
interfaces, including lo) show any packets arriving on that port?


Regards,,

Antony.

-- 
There's no such thing as bad weather - only the wrong clothes.

 - Billy Connolly

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Tue Mar 10 12:50:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 01:50:18 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FEE39B.6050908@vsen.dk>
References: <54FEE39B.6050908@vsen.dk>
Message-ID: <54FEE88A.4000800@treenet.co.nz>

On 11/03/2015 1:29 a.m., Klavs Klavsen wrote:
> Hi,
> 
> I just setup a squid trying to get it to work in intercept mode..
> 
> I seem to hit some squid internal loop where it goes haywire internally
> somehow?

You have explicitly configured Squid instructing it that traffic
arriving on port 3129 has been intercepted.

You then sent Squid a port-80 syntax message with TCP packet destination
IP:port of 127.0.0.1:3129.

It is for this reason that all our interception tutorials state in bold
that its a very good idea to firewall the 3129 port such that no
software, even localhost may send traffic directly into it.

Amos



From kl at vsen.dk  Tue Mar 10 13:19:17 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 10 Mar 2015 14:19:17 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FEE88A.4000800@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEE88A.4000800@treenet.co.nz>
Message-ID: <54FEEF55.6080004@vsen.dk>

Amos Jeffries wrote on 03/10/2015 01:50 PM:
> On 11/03/2015 1:29 a.m., Klavs Klavsen wrote:
>> Hi,
>>
>> I just setup a squid trying to get it to work in intercept mode..
>>
>> I seem to hit some squid internal loop where it goes haywire internally
>> somehow?
>
> You have explicitly configured Squid instructing it that traffic
> arriving on port 3129 has been intercepted.
>
> You then sent Squid a port-80 syntax message with TCP packet destination
> IP:port of 127.0.0.1:3129.
>
port 80 syntax?

> It is for this reason that all our interception tutorials state in bold
> that its a very good idea to firewall the 3129 port such that no
> software, even localhost may send traffic directly into it.
>

ahh.. I was hoping to have a loadbalancer in front of squid (haproxy) - 
to have failover, if squid server should fail..

I'm trying to read and understand:
http://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Concepts_of_Interception_Caching

when nat'ing - doesn't squid just get the rewritten package (which would 
have port 3129 in the tcp dest. port field?)

ie. how can it discern a package send directly to port 3129 - with data 
containing f.ex.:
GET / HTTP/1.1
Host: www.bt.dk

with one just sent directly to that port?

I seem to be failing to understand wherein the difference lies :(

I can see that one can choose to use GRE encapsulation - but that is 
stated to be optional..
-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From roel at 1afa.com  Tue Mar 10 13:46:54 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 14:46:54 +0100
Subject: [squid-users] peek/splice working with lynx but not with firefox or
	chrome
Message-ID: <cone.1425995214.702778.3977.1000@bollix>

Hi list!

I'm trying to get peek/splice working with intercepted https connections.  
The final goal is to accept or reject connections based on the SNI info that  
we get from the first peek. So first, I would like to be able to do  
peek/splice on all requests, and then later I can use an external acl to  
block some of them.

I'm having trouble getting the first step to work. My peek/splice config  
works when I use lynx as a browser, but not (well) with firefox or chrome.  
The latter two sometimes return a result, but often don't. When this happens  
I get diverse errors in the cache log like:

  Error negotiating SSL on FD 20: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol (1/-1/0)
  Error negotiating SSL on FD 41: error:14094085:SSL routines:SSL3_READ_BYTES:ccs received early (1/-1/0)
  Error negotiating SSL on FD 31: error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)

The relevant portions of squid.conf:

  https_port 192.168.13.1:3130 intercept ssl-bump options=ALL cert=/etc/ssl/certs/server.pem

  acl step1 at_step SslBump1
  acl step2 at_step SslBump2
  acl step3 at_step SslBump3

  ssl_bump peek step1
  ssl_bump peek step2
  ssl_bump splice all

  sslproxy_cert_error allow all
  sslproxy_flags DONT_VERIFY_PEER

I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1. Traffic  
is redirected from port 443 top 3130 with iptables.

Any help would be really appreciated.

Thanks a lot,

Roel


From squid3 at treenet.co.nz  Tue Mar 10 13:48:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 02:48:29 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FEEF55.6080004@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEE88A.4000800@treenet.co.nz>
 <54FEEF55.6080004@vsen.dk>
Message-ID: <54FEF62D.2080403@treenet.co.nz>

On 11/03/2015 2:19 a.m., Klavs Klavsen wrote:
> Amos Jeffries wrote on 03/10/2015 01:50 PM:
>> On 11/03/2015 1:29 a.m., Klavs Klavsen wrote:
>>> Hi,
>>>
>>> I just setup a squid trying to get it to work in intercept mode..
>>>
>>> I seem to hit some squid internal loop where it goes haywire internally
>>> somehow?
>>
>> You have explicitly configured Squid instructing it that traffic
>> arriving on port 3129 has been intercepted.
>>
>> You then sent Squid a port-80 syntax message with TCP packet destination
>> IP:port of 127.0.0.1:3129.
>>
> port 80 syntax?
> 
>> It is for this reason that all our interception tutorials state in bold
>> that its a very good idea to firewall the 3129 port such that no
>> software, even localhost may send traffic directly into it.
>>
> 
> ahh.. I was hoping to have a loadbalancer in front of squid (haproxy) -
> to have failover, if squid server should fail..

In which case you would NOT be intercepting by Squid. The LB device
would be doing that. The haproxy would be configured to pass traffic to
Squid port 3128.

Though, what happens if the haproxy device fails? all you've done is
shift the bottleneck from Squid to both Squid and haproxy.

Squid has built in mechanisms for auto-restart if anything goes wrong.
Its sometimes hard to see that anything has happened at all from a
client perspective. The admin will just see some graph spikes in the
service records and (if they look) a log message.


> 
> I'm trying to read and understand:
> http://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Concepts_of_Interception_Caching
> 
> 
> when nat'ing - doesn't squid just get the rewritten package (which would
> have port 3129 in the tcp dest. port field?)

Squid gets a NAT-mangled TCP/IP SYN packet. It then uses the kernel to
undo that mangling in order to contact the original destination IP on
the outgoing connection from Squid.

If the incoming detail (after un-mangling) was Squid itself, things loop.

> 
> ie. how can it discern a package send directly to port 3129 - with data
> containing f.ex.:
> GET / HTTP/1.1
> Host: www.bt.dk
> 
> with one just sent directly to that port?

It can't. Which is why NAT existence is a bunch of security vulnerabilities.


Squid is using the Via header to detect if a request has already been
through this same proxy N times. Which is why Via is a mandatory header
in HTTP/1.1. But all that does is prevent the loop DoS'ing the entire
machine and all other services on it - the bad client has still DoS'ed
itself.

> 
> I seem to be failing to understand wherein the difference lies :(
> 
> I can see that one can choose to use GRE encapsulation - but that is
> stated to be optional..

GRE is just a tunnel that can be used to divert traffic to the Squid
machine for NAT to operate on. Usually used as part of WCCP routing
setups, but its just a tunnel.

Amos


From kl at vsen.dk  Tue Mar 10 14:09:14 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 10 Mar 2015 15:09:14 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FEF62D.2080403@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEE88A.4000800@treenet.co.nz>
 <54FEEF55.6080004@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
Message-ID: <54FEFB0A.8010900@vsen.dk>

Amos Jeffries wrote on 03/10/2015 02:48 PM:
[CUT]
>> ahh.. I was hoping to have a loadbalancer in front of squid (haproxy) -
>> to have failover, if squid server should fail..
>
> In which case you would NOT be intercepting by Squid. The LB device
> would be doing that. The haproxy would be configured to pass traffic to
> Squid port 3128.
>
> Though, what happens if the haproxy device fails? all you've done is
> shift the bottleneck from Squid to both Squid and haproxy.
>
haproxy is performing a much less intensive task than squid.. and having 
haproxy in front, allows me to add multiple squid setups if I want.. and 
f.ex. to test a new setup on one squid - and then quickly fall back if 
there's issues etc.

with haproxy I use keepalived to handle HA - and since haproxy is a HA 
setup we already use many places - it's something we have a fair 
understanding of - making it the simple solution for us :)

Also - we already have data collection setup for haproxy, so we get 
counters for traffic automaticly feed in to our graphite setup :)

> Squid has built in mechanisms for auto-restart if anything goes wrong.
> Its sometimes hard to see that anything has happened at all from a
> client perspective. The admin will just see some graph spikes in the
> service records and (if they look) a log message.
>
nice to know that squid handles this fairly well :)

>
>>
>> I'm trying to read and understand:
>> http://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Concepts_of_Interception_Caching
>>
>>
>> when nat'ing - doesn't squid just get the rewritten package (which would
>> have port 3129 in the tcp dest. port field?)
>
> Squid gets a NAT-mangled TCP/IP SYN packet. It then uses the kernel to
> undo that mangling in order to contact the original destination IP on
> the outgoing connection from Squid.
>
> If the incoming detail (after un-mangling) was Squid itself, things loop.
>
so intercept mode is only used, if you actually do the nat'ing on the 
same server as squid is running..

ie. I should use accel mode instead in my use case?

[CUT]

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Tue Mar 10 14:08:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 03:08:55 +1300
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome
In-Reply-To: <cone.1425995214.702778.3977.1000@bollix>
References: <cone.1425995214.702778.3977.1000@bollix>
Message-ID: <54FEFAF7.70205@treenet.co.nz>

On 11/03/2015 2:46 a.m., Roel van Meer wrote:
> Hi list!
> 
> I'm trying to get peek/splice working with intercepted https
> connections. The final goal is to accept or reject connections based on
> the SNI info that we get from the first peek. So first, I would like to
> be able to do peek/splice on all requests, and then later I can use an
> external acl to block some of them.
> 
> I'm having trouble getting the first step to work. My peek/splice config
> works when I use lynx as a browser, but not (well) with firefox or
> chrome. The latter two sometimes return a result, but often don't. When
> this happens I get diverse errors in the cache log like:
> 
>  Error negotiating SSL on FD 20: error:140770FC:SSL
> routines:SSL23_GET_SERVER_HELLO:unknown protocol (1/-1/0)
>  Error negotiating SSL on FD 41: error:14094085:SSL
> routines:SSL3_READ_BYTES:ccs received early (1/-1/0)
>  Error negotiating SSL on FD 31: error:1407743E:SSL
> routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)
> 
> The relevant portions of squid.conf:
> 
>  https_port 192.168.13.1:3130 intercept ssl-bump options=ALL
> cert=/etc/ssl/certs/server.pem

With "options=ALL" you have enabled all features in the OpenSSL library
including features which can cause the popular modern browsers to view
Squid as a dangerously insecure server.


> 
>  acl step1 at_step SslBump1
>  acl step2 at_step SslBump2
>  acl step3 at_step SslBump3
> 
>  ssl_bump peek step1
>  ssl_bump peek step2
>  ssl_bump splice all
> 

Theres nothing in the above which uses SNI. All that does is cause Squid
to expolicitly look at the TLS handshake that is going on. Then to
splice the two connections together a if it weren't there.


>  sslproxy_cert_error allow all
>  sslproxy_flags DONT_VERIFY_PEER

Then you are explicily disabling the checks to ensure the connections
Squid uses to send the clients private data to servers are secure.


> 
> I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
> Traffic is redirected from port 443 top 3130 with iptables.


... and with an older version of OpenSSL missing many of the last few
years worth of TLS crypto features. IIRC the library releases are now up
to 1.1.* or something. Its best to keep that kind of thing operating the
latest versions.


It may sound silly but, do all those browsers even support SNI on your
OS with the crypto libraries they use?

Amos



From Antony.Stone at squid.open.source.it  Tue Mar 10 14:18:48 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 10 Mar 2015 15:18:48 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
	going on..?
In-Reply-To: <54FEFB0A.8010900@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
Message-ID: <201503101518.48686.Antony.Stone@squid.open.source.it>

On Tuesday 10 March 2015 at 15:09:14 (EU time), Klavs Klavsen wrote:

> so intercept mode is only used, if you actually do the nat'ing on the
> same server as squid is running..

You can do the NATting somewhere else; the important point is that the traffic 
must be NATted, not direct.

> ie. I should use accel mode instead in my use case?

NO.  Accelerator mode is entirely different (from both intercept mode and 
normal Squid usage).  Accelerator mode is for placing squid in front of a 
specific web server (or a bunch of them, but not the entire Internet).  It is 
not for enabling clients to connect to the Internet in general.


Regards,


Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From roel at 1afa.com  Tue Mar 10 14:28:23 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 15:28:23 +0100
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome
References: <cone.1425995214.702778.3977.1000@bollix>
 <54FEFAF7.70205@treenet.co.nz>
Message-ID: <cone.1425997703.735129.3977.1000@bollix>

Amos Jeffries writes:

> > The relevant portions of squid.conf:
> >
> >  https_port 192.168.13.1:3130 intercept ssl-bump options=ALL
> > cert=/etc/ssl/certs/server.pem
>
> With "options=ALL" you have enabled all features in the OpenSSL library
> including features which can cause the popular modern browsers to view
> Squid as a dangerously insecure server.

I also tried it without options=ALL. The reason I enabled it was that the  
documentation says:

  Enable various bug workarounds suggested as "harmless" by OpenSSL
  Be warned that this reduces SSL/TLS strength to some attacks.

This seemed useful to me.

> >  acl step1 at_step SslBump1
> >  acl step2 at_step SslBump2
> >  acl step3 at_step SslBump3
> >
> >  ssl_bump peek step1
> >  ssl_bump peek step2
> >  ssl_bump splice all
> >
>
> Theres nothing in the above which uses SNI. All that does is cause Squid
> to expolicitly look at the TLS handshake that is going on. Then to
> splice the two connections together a if it weren't there.

Yes, I know. I would use something like this for that:

  external_acl_type sg1t %URI %SRC %METHOD %ssl::>sni /tmp/test.sh
  acl sg1 external sg1t
  ssl_bump terminate step3 sg1

which does work. In the test script, I can either allow or reject a  
connection based on the SNI that is passed in (if one is).
I have no trouble there, only with the peek/splice stuff.

I'm trying to get peek/splice to work on all intercepted https connections.  
As soon as that works, I can do fancy stuff with SNI. But when I enable  
peek/splice as in the config above, I can no longer connect to https sites  
from Chrome and Firefox, so that is what I need to solve first. Sorry if I  
was unclear.

> >  sslproxy_cert_error allow all
> >  sslproxy_flags DONT_VERIFY_PEER
>
> Then you are explicily disabling the checks to ensure the connections
> Squid uses to send the clients private data to servers are secure.

Yes, I am, but since I'm only splicing the connection, the browser itself  
should be able to get the original certificate sent by the server, and  
handle it appropriately. Or am I mistaken there?

> > I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
> > Traffic is redirected from port 443 top 3130 with iptables.
>
> ... and with an older version of OpenSSL missing many of the last few
> years worth of TLS crypto features. IIRC the library releases are now up
> to 1.1.* or something. Its best to keep that kind of thing operating the
> latest versions.

I know it missing the latest features, but security patches are backported.  
And I know it is old, but it's what I have to work with now. 
Do you think it might be the cause of the problem I'm having with  
peek/splice, or was it a general recommendation?

> It may sound silly but, do all those browsers even support SNI on your
> OS with the crypto libraries they use?

Ah, I should have said, chrome and FF are on a Windows 7.

Thanks,

Roel


From squid3 at treenet.co.nz  Tue Mar 10 14:28:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 03:28:10 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FEFB0A.8010900@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEE88A.4000800@treenet.co.nz>
 <54FEEF55.6080004@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
Message-ID: <54FEFF7A.30608@treenet.co.nz>

On 11/03/2015 3:09 a.m., Klavs Klavsen wrote:
> Amos Jeffries wrote on 03/10/2015 02:48 PM:
> [CUT]
>>> ahh.. I was hoping to have a loadbalancer in front of squid (haproxy) -
>>> to have failover, if squid server should fail..
>>
>> In which case you would NOT be intercepting by Squid. The LB device
>> would be doing that. The haproxy would be configured to pass traffic to
>> Squid port 3128.
>>
>> Though, what happens if the haproxy device fails? all you've done is
>> shift the bottleneck from Squid to both Squid and haproxy.
>>
> haproxy is performing a much less intensive task than squid.. and having
> haproxy in front, allows me to add multiple squid setups if I want.. and
> f.ex. to test a new setup on one squid - and then quickly fall back if
> there's issues etc.
> 
> with haproxy I use keepalived to handle HA - and since haproxy is a HA
> setup we already use many places - it's something we have a fair
> understanding of - making it the simple solution for us :)
> 
> Also - we already have data collection setup for haproxy, so we get
> counters for traffic automaticly feed in to our graphite setup :)
> 
>> Squid has built in mechanisms for auto-restart if anything goes wrong.
>> Its sometimes hard to see that anything has happened at all from a
>> client perspective. The admin will just see some graph spikes in the
>> service records and (if they look) a log message.
>>
> nice to know that squid handles this fairly well :)
> 
>>
>>>
>>> I'm trying to read and understand:
>>> http://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Concepts_of_Interception_Caching
>>>
>>>
>>>
>>> when nat'ing - doesn't squid just get the rewritten package (which would
>>> have port 3129 in the tcp dest. port field?)
>>
>> Squid gets a NAT-mangled TCP/IP SYN packet. It then uses the kernel to
>> undo that mangling in order to contact the original destination IP on
>> the outgoing connection from Squid.
>>
>> If the incoming detail (after un-mangling) was Squid itself, things loop.
>>
> so intercept mode is only used, if you actually do the nat'ing on the
> same server as squid is running..
> 
> ie. I should use accel mode instead in my use case?

No, in your setup the Squid is a regular forward-proxy servicing traffic
sent to it explicitly by haproxy.

If the overall system happens to be a CDN then also use cache_peer to
configure Squid where to fetch the responses, or use split-DNS to make
Squid resolve the internal server IPs differently from the clients.

Amos


From squid3 at treenet.co.nz  Tue Mar 10 14:32:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 03:32:25 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <201503101518.48686.Antony.Stone@squid.open.source.it>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
Message-ID: <54FF0079.2020400@treenet.co.nz>

On 11/03/2015 3:18 a.m., Antony Stone wrote:
> On Tuesday 10 March 2015 at 15:09:14 (EU time), Klavs Klavsen wrote:
> 
>> so intercept mode is only used, if you actually do the nat'ing on the
>> same server as squid is running..
> 
> You can do the NATting somewhere else; the important point is that the traffic 
> must be NATted, not direct.

Nope. If NAT is being performed, then it must be on the same machine as
the proxy it is NATing *to* - haproxy in this case it seems.

NATing on machine A to port-forwarding deliver the traffic to a port on
machine B is no different to contacting directly to the same port on
machine B.

Amos



From squid3 at treenet.co.nz  Tue Mar 10 14:47:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 03:47:34 +1300
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome
In-Reply-To: <cone.1425997703.735129.3977.1000@bollix>
References: <cone.1425995214.702778.3977.1000@bollix>
 <54FEFAF7.70205@treenet.co.nz> <cone.1425997703.735129.3977.1000@bollix>
Message-ID: <54FF0406.9000203@treenet.co.nz>

On 11/03/2015 3:28 a.m., Roel van Meer wrote:
> Amos Jeffries writes:
> 
>> > The relevant portions of squid.conf:
>> >
>> >  https_port 192.168.13.1:3130 intercept ssl-bump options=ALL
>> > cert=/etc/ssl/certs/server.pem
>>
>> With "options=ALL" you have enabled all features in the OpenSSL library
>> including features which can cause the popular modern browsers to view
>> Squid as a dangerously insecure server.
> 
> I also tried it without options=ALL. The reason I enabled it was that
> the documentation says:
> 
>  Enable various bug workarounds suggested as "harmless" by OpenSSL
>  Be warned that this reduces SSL/TLS strength to some attacks.
> 
> This seemed useful to me.
> 
>> >  acl step1 at_step SslBump1
>> >  acl step2 at_step SslBump2
>> >  acl step3 at_step SslBump3
>> >
>> >  ssl_bump peek step1
>> >  ssl_bump peek step2
>> >  ssl_bump splice all
>> >
>>
>> Theres nothing in the above which uses SNI. All that does is cause Squid
>> to expolicitly look at the TLS handshake that is going on. Then to
>> splice the two connections together a if it weren't there.
> 
> Yes, I know. I would use something like this for that:
> 
>  external_acl_type sg1t %URI %SRC %METHOD %ssl::>sni /tmp/test.sh
>  acl sg1 external sg1t
>  ssl_bump terminate step3 sg1
> 
> which does work. In the test script, I can either allow or reject a
> connection based on the SNI that is passed in (if one is).
> I have no trouble there, only with the peek/splice stuff.

see Nathan Hoads thread just the other day about a setup same as yours
NOT working.

There are two patches that need applying. One already in the 3.5 series
snapshots to fix SNI on some traffic cases, one still in QA review for
adding an ACL "server_name" that can match SNI without the helper.

> 
> I'm trying to get peek/splice to work on all intercepted https
> connections. As soon as that works, I can do fancy stuff with SNI. But
> when I enable peek/splice as in the config above, I can no longer
> connect to https sites from Chrome and Firefox, so that is what I need
> to solve first. Sorry if I was unclear.
> 
>> >  sslproxy_cert_error allow all
>> >  sslproxy_flags DONT_VERIFY_PEER
>>
>> Then you are explicily disabling the checks to ensure the connections
>> Squid uses to send the clients private data to servers are secure.
> 
> Yes, I am, but since I'm only splicing the connection, the browser
> itself should be able to get the original certificate sent by the
> server, and handle it appropriately. Or am I mistaken there?

That is correct. But also they get it through the filter of your OpenSSL
version parsing and re-packing capabilities for the underlying TLS/SSL
protocol syntax.

Those errors hint at things like the SSLv2/SSLv3 syntax being offered
and rejected, ALPN being mangled, or some advanced timing-based feature
being screwed up by the peek operation.


> 
>> > I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
>> > Traffic is redirected from port 443 top 3130 with iptables.
>>
>> ... and with an older version of OpenSSL missing many of the last few
>> years worth of TLS crypto features. IIRC the library releases are now up
>> to 1.1.* or something. Its best to keep that kind of thing operating the
>> latest versions.
> 
> I know it missing the latest features, but security patches are
> backported. And I know it is old, but it's what I have to work with
> now.Do you think it might be the cause of the problem I'm having with
> peek/splice, or was it a general recommendation?

Its a potential source of problems. Chrome is very much on the front
line of the arms race attempting to stop things like SSL-Bump working.
Firefox implement their own crypto library which tracks the latest TLS
features at a similar speed of development.
OpenSSL will be perpetually behind both of them, but at least the latest
one(s) have better chances not to be advertising features they reject in
"considered harmful" grounds.

> 
>> It may sound silly but, do all those browsers even support SNI on your
>> OS with the crypto libraries they use?
> 
> Ah, I should have said, chrome and FF are on a Windows 7.

Oay. They should then.


Amos


From roel at 1afa.com  Tue Mar 10 15:03:29 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 16:03:29 +0100
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome
References: <cone.1425995214.702778.3977.1000@bollix>
 <54FEFAF7.70205@treenet.co.nz> <cone.1425997703.735129.3977.1000@bollix>
 <54FF0406.9000203@treenet.co.nz>
Message-ID: <cone.1425999809.356038.3977.1000@bollix>

Amos Jeffries writes:

> see Nathan Hoads thread just the other day about a setup same as yours
> NOT working.
>
> There are two patches that need applying. One already in the 3.5 series
> snapshots to fix SNI on some traffic cases, one still in QA review for
> adding an ACL "server_name" that can match SNI without the helper.

That's very useful for the SNI matching indeed. Thanks!

> > Yes, I am, but since I'm only splicing the connection, the browser
> > itself should be able to get the original certificate sent by the
> > server, and handle it appropriately. Or am I mistaken there?
>
> That is correct. But also they get it through the filter of your OpenSSL
> version parsing and re-packing capabilities for the underlying TLS/SSL
> protocol syntax.
>
> Those errors hint at things like the SSLv2/SSLv3 syntax being offered
> and rejected, ALPN being mangled, or some advanced timing-based feature
> being screwed up by the peek operation.

It seems so.

> >> > I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
> >> > Traffic is redirected from port 443 top 3130 with iptables.
> >>
> >> ... and with an older version of OpenSSL missing many of the last few
> >> years worth of TLS crypto features. IIRC the library releases are now up
> >> to 1.1.* or something. Its best to keep that kind of thing operating the
> >> latest versions.
> >
> > I know it missing the latest features, but security patches are
> > backported. And I know it is old, but it's what I have to work with
> > now.Do you think it might be the cause of the problem I'm having with
> > peek/splice, or was it a general recommendation?
>
> Its a potential source of problems. Chrome is very much on the front
> line of the arms race attempting to stop things like SSL-Bump working.
> Firefox implement their own crypto library which tracks the latest TLS
> features at a similar speed of development.
> OpenSSL will be perpetually behind both of them, but at least the latest
> one(s) have better chances not to be advertising features they reject in
> "considered harmful" grounds.

I'll have a go then at trying with a newer openssl and the patches from the  
thread you mentioned.

Thanks a lot so far,

Roel



From kl at vsen.dk  Tue Mar 10 15:08:51 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 10 Mar 2015 16:08:51 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <201503101518.48686.Antony.Stone@squid.open.source.it>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
Message-ID: <54FF0903.6000404@vsen.dk>

hmm..

I've read the config examples..

I would very much like to understand how/why it works, if I've setup a 
client to route package to squid (instead of trying to send directly)..

I'm trying to follow this on a test client (haven't gotten it working yet):
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute 
(where squid is amongst the internal clients - actually on it's own vlan 
- but it's not the default route)

meanwhile I tried pointing to the haproxy - which then forwards requests 
in tcp mode, to squid server port 3129.

If I just send to haproxy directly, I get the loop and this in the 
accesslog:
1425998994.271      0 10.43.18.165 TCP_MISS_ABORTED/000 0 GET 
http://www.bt.dk/ - ORIGINAL_DST/10.43.18.165 -

when doing:
curl -H "Host: www.bt.dk" http://proxy-haproxy-ip/

10.43.18.165 is the ip of squid server behind haproxy.



Antony Stone wrote on 03/10/2015 03:18 PM:
> On Tuesday 10 March 2015 at 15:09:14 (EU time), Klavs Klavsen wrote:
>
>> so intercept mode is only used, if you actually do the nat'ing on the
>> same server as squid is running..
>
> You can do the NATting somewhere else; the important point is that the traffic
> must be NATted, not direct.
>
>> ie. I should use accel mode instead in my use case?
>
> NO.  Accelerator mode is entirely different (from both intercept mode and
> normal Squid usage).  Accelerator mode is for placing squid in front of a
> specific web server (or a bunch of them, but not the entire Internet).  It is
> not for enabling clients to connect to the Internet in general.
>
>
> Regards,
>
>
> Antony.
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From Antony.Stone at squid.open.source.it  Tue Mar 10 15:36:31 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 10 Mar 2015 16:36:31 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
	going on..?
In-Reply-To: <54FF0079.2020400@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0079.2020400@treenet.co.nz>
Message-ID: <201503101636.31802.Antony.Stone@squid.open.source.it>

On Tuesday 10 March 2015 at 15:32:25 (EU time), Amos Jeffries wrote:

> On 11/03/2015 3:18 a.m., Antony Stone wrote:
> > On Tuesday 10 March 2015 at 15:09:14 (EU time), Klavs Klavsen wrote:
> >> so intercept mode is only used, if you actually do the nat'ing on the
> >> same server as squid is running..
> > 
> > You can do the NATting somewhere else; the important point is that the
> > traffic must be NATted, not direct.
> 
> Nope. If NAT is being performed, then it must be on the same machine as
> the proxy it is NATing *to* - haproxy in this case it seems.
> 
> NATing on machine A to port-forwarding deliver the traffic to a port on
> machine B is no different to contacting directly to the same port on
> machine B.

Hm, apologies for the misleading advice; I suspect this may be due to changes 
in Squid's handling of things since I last set up a router redirecting traffic 
to a separate Squid box.  Either that, or my brain has simply discarded some 
details in the interim... :(

Anyway, Klavs, please take Amos' advice over mine on this.


Regards,


Antony.

-- 
It is also possible that putting the birds in a laboratory setting 
inadvertently renders them relatively incompetent.

 - Daniel C Dennett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From kl at vsen.dk  Tue Mar 10 15:52:10 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Tue, 10 Mar 2015 16:52:10 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <201503101518.48686.Antony.Stone@squid.open.source.it>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
Message-ID: <54FF132A.3020903@vsen.dk>

hmm..

I'm trying to follow this on a test client (haven't gotten it working yet):
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute 
(where squid is amongst the internal clients - actually on it's own vlan 
- but it's not the default route)

but this won't work:
ip route add default via 10.47.18.181 dev eth0 table 201
RTNETLINK answers: No such process

which seems to be because this way of routing will only work if the 
squid box is on the same network as the clients.. :(

I would like to have the squid box on it's own vlan.. (and ip-segment).. 
do I need to setup some tunneling, or perhaps tproxy can be used..

I was hoping I could simply direct packages to squid (doing NAT on 
clients) - and squid would get hostname and other details from data.. as 
accel mode does :(


Antony Stone wrote on 03/10/2015 03:18 PM:
> On Tuesday 10 March 2015 at 15:09:14 (EU time), Klavs Klavsen wrote:
>
>> so intercept mode is only used, if you actually do the nat'ing on the
>> same server as squid is running..
>
> You can do the NATting somewhere else; the important point is that the traffic
> must be NATted, not direct.
>
>> ie. I should use accel mode instead in my use case?
>
> NO.  Accelerator mode is entirely different (from both intercept mode and
> normal Squid usage).  Accelerator mode is for placing squid in front of a
> specific web server (or a bunch of them, but not the entire Internet).  It is
> not for enabling clients to connect to the Internet in general.
>
>
> Regards,
>
>
> Antony.
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
    --Henry Spencer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Mar 10 16:45:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 05:45:51 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FF0903.6000404@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk>
Message-ID: <54FF1FBF.6000605@treenet.co.nz>

On 11/03/2015 4:08 a.m., Klavs Klavsen wrote:
> hmm..
> 
> I've read the config examples..
> 
> I would very much like to understand how/why it works, if I've setup a
> client to route package to squid (instead of trying to send directly)..

I suggest diagramming your network traffic flow, writing down the TCP
packet src/dst IP:port values on each connection.

"Normal" interception operation showing your loop:

1) curl thinks its talking to Internet server
    src ip-of-curl:*
    dst ip-of-bt.dk:80

2) NAT diverts the packet to haproxy
    src ip-of-curl:*
    dst ip-of-haproxy-machine:*

3) haproxy knows its talking to Squid
    src ip-of-haproxy-machine:*
    dst 10.43.18.165:3129

4) Squid machine reports un-mangled NAT is ...
    src ip-of-haproxy-machine:*
    dst 10.43.18.165:3129

5) Squid contacts dst-IP to fetch the HTTP request
    src 10.43.18.165:*
    dst 10.43.18.165:3129

6) repeat from 4 (but with src now 10.43.18.165:*) until
  a) machine runs out of memory,
  b) machine runs out of TCP sockets,
  c) machine runs out of disk space logging errors, or
  d) Squid notices Via header loop and rejects request.


A "Normal" connection directly to squid port:


1) curl thinks its talking to Internet server on 10.43.18.165:3129
    src ip-of-curl:*
    dst 10.43.18.165:3129

2) Squid machine reports un-mangled NAT is ...
    src ip-of-curl:*
    dst 10.43.18.165:3129

3) Squid contacts dst-IP to fetch the HTTP request
    src 10.43.18.165:*
    dst 10.43.18.165:3129

4) repeat from 2 (but with src now 10.43.18.165:*) until
  a) machine runs out of memory,
  b) machine runs out of TCP sockets,
  c) machine runs out of disk space logging errors, or
  d) Squid notices Via header loop and rejects request.



Confirm it against wireshark packet traces of what is coming *in* to the
Squid machine.

Squid undoes the NAT mangling done on the machine its running. Any NAT
changes prior to that is unknowable. NAT un-mangling can sometimes
appear to work in Squid if haproxy runs on the same machine BUT, some
NAT lookup requires correct TCP socket and some require particular
IP:port ordering so its unrelible.


The Squid access.log lines with ORIGINAL_DST show what client src-IP and
dst-IP Squid received after NAT un-mangling. Though you have to take
care to subtract the duration value from the line timestamp to get the
order right for loops.


> 
> I'm trying to follow this on a test client (haven't gotten it working yet):
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> (where squid is amongst the internal clients - actually on it's own vlan
> - but it's not the default route)

If its on its own VLAN then its not "among the clients".

"among the clients" means a bunch of machines all on the same LAN
subnet, one of which is the Squid proxy. They can talk directly to each
other without involving the router so you can hit the triangular routing
problem and have to account for it in your forwarding and/or NAT rules.

If you have Squid on a separate subnet or VLAN with all packets going
via the router, that is a DMZ subnet. The router sees all packets and
can adjusts them right, so triangular routing problem is avoided.


> 
> meanwhile I tried pointing to the haproxy - which then forwards requests
> in tcp mode, to squid server port 3129.
> 
> If I just send to haproxy directly, I get the loop and this in the
> accesslog:
> 1425998994.271      0 10.43.18.165 TCP_MISS_ABORTED/000 0 GET
> http://www.bt.dk/ - ORIGINAL_DST/10.43.18.165 -
> 
> when doing:
> curl -H "Host: www.bt.dk" http://proxy-haproxy-ip/
> 
> 10.43.18.165 is the ip of squid server behind haproxy.


Methinks that haproxy is still sending to the Squid port configured with
"intercept" option.

A connection between two proxies is explicitly configured. Thus a normal
forward-proxy connection, no special port flags required.


The log line is also showing haproxy to be vulnerable to CVE-2009-0801.

If your Squid and haproxy are recent enough they could use the PROXY
protocol to exchange the original client connections TCP/IP details.
I've not tested it for this myself but that may also allow Squid with an
intercept port to both handle the traffic okay (PROXY protocol override
the NAT lookups), and protect against the CVE-2009-0801 vulnerability in
haproxy ('intercept' flags existence makes Squid do the security checks).


Amos


From roel at 1afa.com  Tue Mar 10 17:12:23 2015
From: roel at 1afa.com (Roel van Meer)
Date: Tue, 10 Mar 2015 18:12:23 +0100
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome [SOLVED]
References: <cone.1425995214.702778.3977.1000@bollix>
 <54FEFAF7.70205@treenet.co.nz> <cone.1425997703.735129.3977.1000@bollix>
 <54FF0406.9000203@treenet.co.nz> <cone.1425999809.356038.3977.1000@bollix>
Message-ID: <cone.1426007543.73762.3977.1000@bollix>

Roel van Meer writes:

>> >> > I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
>> >> > Traffic is redirected from port 443 top 3130 with iptables.
>> >>
>> >> ... and with an older version of OpenSSL missing many of the last few
>> >> years worth of TLS crypto features. IIRC the library releases are now up
>> >> to 1.1.* or something. Its best to keep that kind of thing operating the
>> >> latest versions.
>> >
>> > I know it missing the latest features, but security patches are
>> > backported. And I know it is old, but it's what I have to work with
>> > now.Do you think it might be the cause of the problem I'm having with
>> > peek/splice, or was it a general recommendation?
>>
>> Its a potential source of problems. Chrome is very much on the front
>> line of the arms race attempting to stop things like SSL-Bump working.
>> Firefox implement their own crypto library which tracks the latest TLS
>> features at a similar speed of development.
>> OpenSSL will be perpetually behind both of them, but at least the latest
>> one(s) have better chances not to be advertising features they reject in
>> "considered harmful" grounds.
>
> I'll have a go then at trying with a newer openssl and the patches from the 
> thread you mentioned.

With Squid 3.5.2 built with openssl 1.0.1k I can splice https connections  
with no trouble. Tested with Lync, Chrome, Firefox, and IE.

So you were right. :) Thanks a lot for pointing me in the right direction!

Cheers,

Roel


From dweimer at dweimer.net  Wed Mar 11 04:42:17 2015
From: dweimer at dweimer.net (dweimer)
Date: Tue, 10 Mar 2015 23:42:17 -0500
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
Message-ID: <0458b52461578fe1012818892d032556@dweimer.net>

We have setup Squid as a reverse proxy to Exchange 2010 OWA server we 
thought everything was working OK, but found out that any file 
attachments over 2MB cause a timeout after 5 minutes. I remembered 
having this issue a while back with HTTPS, and it just went away after 
some updates. Some searching found other users posting messages to the 
Squid mailing list that had this issue in particular with OWA. However I 
never found a fix on any of the threads.

Squid is currently running 3.4.11, on FreeBSD 10.1-RELEASE-p5, This 
occurs even when sending the file through the local network passing 
through the reverse proxy. With the slowest link being a 1G.

Below is the relevant parts of the configuration, with some information 
excluded for security
https_port 10.50.20.12:443 accel defaultsite=... \
  cert=... \
  key=... \
  options=NO_SSLv2:NO_TLSv1:CIPHER_SERVER_PREFERENCE \
  cipher=RC4:!MD5:!aNULL:!EDH

cache_peer ... parent 443 0 ssl no-query no-digest no-netdb-exchange 
originserver name=owa2010_parent sslcapath=/usr/local/share/certs 
sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on

We also host sharepoint (certificate is a wildcard certificate) this way 
as well, and I have just verified that it has the same problem. It is 
served by the same https_port line, and a different cache_peer the only 
difference is the IP and it doesn't have the front-end-https option set.

Does anyone have any ideas to check?

is this possibly a cause 
<http://www.squid-cache.org/Doc/config/broken_posts/>?

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From alex at samad.com.au  Wed Mar 11 06:16:19 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 11 Mar 2015 17:16:19 +1100
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <0458b52461578fe1012818892d032556@dweimer.net>
References: <0458b52461578fe1012818892d032556@dweimer.net>
Message-ID: <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>

This is mine against 2008. haven't had any issues with attachments up to 10M


cache_peer 127.0.0.1 parent 443 0 proxy-only no-query no-digest
originserver login=PASS ssl sslflags=DONT_VERIFY_PEER
sslcert=/etc/httpd/conf.d/o.crt sslkey=/etc/httpd/conf.d/o.key
name=webServer
cache_peer 10.32.69.11 parent 443 0 proxy-only no-query no-digest
originserver login=PASS front-end-https=on ssl
sslflags=DONT_VERIFY_PEER sslcert=/etc/httpd/conf.d/o.crt
sslkey=/etc/httpd/conf.d/o.key name=exchangeServer

# List of acceptable URLs to send to the Exchange server
acl exch_url url_regex -i <o>/exchange
acl exch_url url_regex -i <o>/exchweb
acl exch_url url_regex -i <o>/public
acl exch_url url_regex -i <o>/owa
acl exch_url url_regex -i <o>/ecp
acl exch_url url_regex -i <o>/microsoft-server-activesync
acl exch_url url_regex -i <o>/rpc
acl exch_url url_regex -i <o>/rpcwithcert
acl exch_url url_regex -i <o>/exadmin
acl exch_url url_regex -i <o>/oab

# Send the Exchange URLs to the Exchange server
cache_peer_access exchangeServer allow exch_url

# Send everything else to the Apache
cache_peer_access webServer deny exch_url

# This is to protect Squid
never_direct allow exch_url

# Logging Configuration
redirect_rewrites_host_header off
cache_mem 32 MB
maximum_object_size_in_memory 128 KB
cache_log none
cache_store_log none

access_log /var/log/squid/office-access.log squid
#access_log none
cache_log /var/log/squid/office-cache.log
#cache_log none
pid_filename /var/run/squid-office.pid


# Set the hostname so that we can see Squid in the path (Optional)
visible_hostname <o>
deny_info TCP_RESET all

# ACL - required to allow
#acl all src ALL

# Allow everyone through, internal and external connections
http_access allow all
miss_access allow all

icp_port 0
snmp_port 0

via off

On 11 March 2015 at 15:42, dweimer <dweimer at dweimer.net> wrote:
> We have setup Squid as a reverse proxy to Exchange 2010 OWA server we
> thought everything was working OK, but found out that any file attachments
> over 2MB cause a timeout after 5 minutes. I remembered having this issue a
> while back with HTTPS, and it just went away after some updates. Some
> searching found other users posting messages to the Squid mailing list that
> had this issue in particular with OWA. However I never found a fix on any of
> the threads.
>
> Squid is currently running 3.4.11, on FreeBSD 10.1-RELEASE-p5, This occurs
> even when sending the file through the local network passing through the
> reverse proxy. With the slowest link being a 1G.
>
> Below is the relevant parts of the configuration, with some information
> excluded for security
> https_port 10.50.20.12:443 accel defaultsite=... \
>  cert=... \
>  key=... \
>  options=NO_SSLv2:NO_TLSv1:CIPHER_SERVER_PREFERENCE \
>  cipher=RC4:!MD5:!aNULL:!EDH
>
> cache_peer ... parent 443 0 ssl no-query no-digest no-netdb-exchange
> originserver name=owa2010_parent sslcapath=/usr/local/share/certs
> sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on
>
> We also host sharepoint (certificate is a wildcard certificate) this way as
> well, and I have just verified that it has the same problem. It is served by
> the same https_port line, and a different cache_peer the only difference is
> the IP and it doesn't have the front-end-https option set.
>
> Does anyone have any ideas to check?
>
> is this possibly a cause
> <http://www.squid-cache.org/Doc/config/broken_posts/>?
>
> --
> Thanks,
>    Dean E. Weimer
>    http://www.dweimer.net/
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Mar 11 08:09:10 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 21:09:10 +1300
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
References: <0458b52461578fe1012818892d032556@dweimer.net>
 <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
Message-ID: <54FFF826.3080008@treenet.co.nz>

On 11/03/2015 7:16 p.m., Alex Samad wrote:
> This is mine against 2008. haven't had any issues with attachments up to 10M
> 
> 

Small audit with things to look at fixing to improve your security when
you have some time.

> cache_peer 127.0.0.1 parent 443 0 proxy-only no-query no-digest
> originserver login=PASS ssl sslflags=DONT_VERIFY_PEER
> sslcert=/etc/httpd/conf.d/o.crt sslkey=/etc/httpd/conf.d/o.key
> name=webServer
> cache_peer 10.32.69.11 parent 443 0 proxy-only no-query no-digest
> originserver login=PASS front-end-https=on ssl
> sslflags=DONT_VERIFY_PEER sslcert=/etc/httpd/conf.d/o.crt
> sslkey=/etc/httpd/conf.d/o.key name=exchangeServer

Please locate the public cert/key for the exchange servers CA, load it
into Squid (aka ake Squid trust it) with cacert=X.pem and remove the
"DONT_VERIFY_PEER" debug flag.


> 
> # List of acceptable URLs to send to the Exchange server
> acl exch_url url_regex -i <o>/exchange
> acl exch_url url_regex -i <o>/exchweb
> acl exch_url url_regex -i <o>/public
> acl exch_url url_regex -i <o>/owa
> acl exch_url url_regex -i <o>/ecp
> acl exch_url url_regex -i <o>/microsoft-server-activesync
> acl exch_url url_regex -i <o>/rpc
> acl exch_url url_regex -i <o>/rpcwithcert
> acl exch_url url_regex -i <o>/exadmin
> acl exch_url url_regex -i <o>/oab

I suggest you replace the above with ACLs:

 acl exch_domain dstdomain <o>
 acl exch_path urlpath_regex -i /exch(ange|web)
 acl exch_path urlpath_regex -i /public
 acl exch_path urlpath_regex -i /owa
 ...

If you have a current 3.4+ Squid version:
 acl exch_url allof exch_domain exch_path

otherwise for older Squid replace the various access lines like so:
  cache_peer_access exchangeServer allow exch_domain exch_path


> 
> # Send the Exchange URLs to the Exchange server
> cache_peer_access exchangeServer allow exch_url
> 
> # Send everything else to the Apache
> cache_peer_access webServer deny exch_url
> 
> # This is to protect Squid
> never_direct allow exch_url
> 
> # Logging Configuration
> redirect_rewrites_host_header off

Thats begging for abuse of the security hole it opens. If you can
operate without that setting please do so.


> cache_mem 32 MB
> maximum_object_size_in_memory 128 KB
> cache_log none
> cache_store_log none
> 
> access_log /var/log/squid/office-access.log squid
> #access_log none
> cache_log /var/log/squid/office-cache.log
> #cache_log none
> pid_filename /var/run/squid-office.pid
> 
> 
> # Set the hostname so that we can see Squid in the path (Optional)

NOTE: its not optional. You have disabled most of the HTTP features
which use the Squid hostname, but not all of them can be.
 For example "via off" contradicts this comment, by NOT adding Squid to
the relay path (Via header).


> visible_hostname <o>
> deny_info TCP_RESET all
> 
> # ACL - required to allow
> #acl all src ALL
> 
> # Allow everyone through, internal and external connections
> http_access allow all
> miss_access allow all
> 
> icp_port 0
> snmp_port 0
> 
> via off
> 


Amos



From squid3 at treenet.co.nz  Wed Mar 11 08:23:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 21:23:43 +1300
Subject: [squid-users] Squid Bugzilla attachment problems FIXED
Message-ID: <54FFFB8F.6030001@treenet.co.nz>

For those of you trying add attachments to our bugzilla, we believe the
issue that has been coming up over the last few weeks is now fixed.

The problem was a mismatch between recent Perl versions and our ancient
bugzilla code modules. As usual with this type of problem, an upgrade of
the affected software swept away the bugs.

Keep in mind there is a 10MB limit on attachment size though. So large
logs need to be compressed first.

Amos


From squid3 at treenet.co.nz  Wed Mar 11 09:01:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Mar 2015 22:01:25 +1300
Subject: [squid-users] Authentication help
In-Reply-To: <54F8C16D.1060908@infomed.sld.cu>
References: <54F8C16D.1060908@infomed.sld.cu>
Message-ID: <55000465.4020106@treenet.co.nz>

On 6/03/2015 9:49 a.m., Informatico Neurodesarrollo wrote:
> Hi list,
> I am new in the list and I want to solve a problem with the
> authentication process in the factory that I worked some years ago and
> in this place I began work with Linux.
> They use openSuSE 13.2 (64bits) with squid 3.4.4, the specification are:
> - the authentication is local, Unix users
> - two groups created :intranet (only can access to domain ".cu" ),internet
> 
> What is the deal?:
> 
> When I try to access, in the surfer arise a windows ask me the user and
> password, but when I push Enter key, this windows arise again and I have
> to press several times the "ESC" key to can navigate.

The client software (browser?) is responsible for locating suitable
credentials that teh authenticatio system will accept. The popup window
you are seeing is one of several options available to it and most modern
browsers use it as a last resort only if the automated alternatives fail.

If the proxy is offering multiple types of authentication and the client
browser sends the credentials for Type A when they should have been
labeled type B, then you can see the popup happen multiple times. There
is nothing we (the proxy people) can do about this type of problem in
the client browser.


It can also keep popping up if your rules say that the provided
credentials are not adequate for the access desired, but other
credentials might work. Your brower is given a chance to try those other
credentials.

So lets look at the specific config...

> 
> I attach bellow squid.conf file.
> 
> My best regards.
> 
> PD Apologist my english, but if any body else understand Spanish
> language I can explain better.
> 
> 
> squid.conf:
> 
> # Squid normally listens to port 3128
> http_port 3128
>
<snip>
> 
> ########################################################
> #Autenticaci?n
> 
> auth_param basic program /usr/sbin/basic_getpwnam_auth --helper-protocol=squid-2.5-basic
> auth_param basic children 20
> auth_param basic realm Servidor Proxy JVR
> auth_param basic credentialsttl 1 hours
> auth_param basic casesensitive off
> 

NOTE: The basic_getpwnam_auth helper does not take any command line
parameter "--helper-protocol=squid-2.5-basic"

That should not have been causing any issue though. These settings
appear to be fine other than the garbage parameer.


> ############
> #Grupos Unix
> 
> external_acl_type groupo_linux %LOGIN /usr/sbin/ext_unix_group_acl -p
> 
> acl nav_nac external groupo_linux intranet
> acl nav_int external groupo_linux internet

The above two ACls will match the groups.

> 
> acl nav_full proxy_auth nav_int
> acl nav_cuba proxy_auth nav_nac

The above two ACLs will match the *individual user login* name "nav_int"
or "nav_nac".

> 
> acl Auth_jvr proxy_auth REQUIRED
> 

<snip>
> cache_log /var/log/squid/cache.log
> access_log /var/log/squid/access.log
> cache_store_log /var/log/squid/store.log
> error_directory /usr/share/squid/errors/es

I recommend setting this instead:
 default_error_language es

Your Squid will then report errors your users can read (with Espanol as
default), instead of forcing Espanol on all of them.

> 
> acl localnet src 10.44.1.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> 
> acl restricted_sites dstdomain "/etc/squid/listas/blocked_sites.acl"
> acl restricted_dst dst "/etc/squid/listas/blocked_src"
> acl nacional dstdomain .cu
> 
> # Regla para denegar palabras indebidas
> acl palabras url_regex -i "/etc/squid/deneg"
> 
<snip>
> 
> http_access allow localnet !restricted_sites !restricted_dst !palabras
> http_access allow Auth_jvr nav_full !nav_nac

The above rule will require authentication for the single user name
"nav_int" in the group "internet". Otherwise will request new
credentials that can pass the tests (the popup).

> http_access allow Auth_jvr nav_cuba nacional

The above will request authentication, and if it provide and the other
check passes will allow the request. Due to the "nacional" ACL being lat
it will NOT request different credentials.


I suspect your use of individual username ACLs is a mistake. Your policy
description only mentioned restricting access by group.

Which means your custom ACL tests should be:

 # allow access *from* LAN machines unless requesting restricted URLs
 http_access allow localnet !restricted_sites !restricted_dst !palabras

 # require login for restricted URLs
 http_access deny !Auth_jvr

 # group "internet" users can access anywhere
 http_access allow nav_int all

 # group "intranet" users can access restricted .cu domains
 http_access allow nav_cuba nacional


Amos


From johnzeng2013 at yahoo.com  Wed Mar 11 09:09:25 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 11 Mar 2015 17:09:25 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <54FEE428.3080308@treenet.co.nz>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD958A.8000100@yahoo.com> <54FEE428.3080308@treenet.co.nz>
Message-ID: <55000645.5010800@yahoo.com>


       Hello Amos:

                           Ok,  I see

                           Thanks again.

                            Have a good day with you .





>> Hello Amos:
>>
>> -----------------------------------------------------------------------------------------------------------------------
>>
>>
>> For starters,
>>   WCCP is a network protocol Squid uses to inform remote routers that it
>> is active and what traffic it can receive.
>>   rock is a layout format for bits stored on a disk.
>>   ... they are *completely* unrelated.
>>
>> -------------------------------------------------------------------------
>>
>> Your meaning is running two different process for wccp redirection and
>> Cache operation ?
>>
> I mean they are different like shelves in a cabinet versus a spoken
> sentence.
>
> And yes, they are probably also in different processes in your setup.
>
> Amos
>



From johnzeng2013 at yahoo.com  Wed Mar 11 10:22:19 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 11 Mar 2015 18:22:19 +0800
Subject: [squid-users] i hope to build web Authentication portal at Tproxy
 environment recenty , can you give me some advisement .
Message-ID: <5500175B.5040806@yahoo.com>


Hello Dear all:

i hope to build web Authentication portal at Tproxy environment recenty ,

but i don't have experience about the Authentication,

Which Authentication mode will be best direction between Ldap and My sql
or other (ncsa ....) for web Athentication portal ?

my thought is :

When Http traffic pass through Tproxy bridge , if our customer isn't
Authentication user , and

Squid will prevent them and push web Authentication portal via these
ways (acl + deny info or error page ),

and our customer will start input correct acount and password via web
Authentication portal ,

if they success login , and squid will permit them to pass through
Tproxy bridge and access internet normal .

-------------------------------------------------------------------------------------------------------------------------------------------------------------

i can write web Authentication portal program via php or javascript (
jquery or node.js ) .

but if they success login,

whether php or jquery need send user ip address to squid ? otherwise i
worried whether squid can confirm user info

and how to identify and controll http traffic ?









if possible , please give me some useful link or advisement info .




Thanks .



From steve at opendium.com  Wed Mar 11 10:54:22 2015
From: steve at opendium.com (Steve Hill)
Date: Wed, 11 Mar 2015 10:54:22 +0000
Subject: [squid-users] i hope to build web Authentication portal at
 Tproxy environment recenty , can you give me some advisement .
In-Reply-To: <5500175B.5040806@yahoo.com>
References: <5500175B.5040806@yahoo.com>
Message-ID: <55001EDE.700@opendium.com>

On 11.03.15 10:22, johnzeng wrote:

> whether php or jquery need send user ip address to squid ? otherwise i
> worried whether squid can confirm user info
>
> and how to identify and controll http traffic ?

I'd do this with an external ACL - when processing a request, Squid 
would call the external ACL which would do:

1. If the user is not authenticated or their "last seen" timestamp has 
expired, return "ERR"
2. If the user is authenticated, update their "last seen" timestamp and 
return OK.

Obviously if the ACL returns ERR, Squid needs to redirect the user to 
the authentication page.  If the ACL returns OK, Squid needs to service 
the request as normal.

The authentication page would update the database which the external ACL 
refers to.

Identifying the user's traffic would need to be done by MAC address or IP:
  - MAC address requires a flat network with no routers between the 
device and Squid.
  - IP has (probably) unfixable problems in a dual-stacked network.

Beware that:
1. Access to the authentication page must be allowed for unauthenticated 
users (obviously :)
2. Authentication should really be done over HTTPS with a trusted 
certificate.
3. Clients require access to some external servers to validate HTTPS 
certs before they have authenticated.
4. If you want to support WISPr then (2) and (3) are mandatory.
5. External ACL caching

You might be able to do it with internal ACLs, but... pain :)

-- 
  - Steve Hill
    Technical Director
    Opendium Limited     http://www.opendium.com

Direct contacts:
    Instant messager: xmpp:steve at opendium.com
    Email:            steve at opendium.com
    Phone:            sip:steve at opendium.com

Sales / enquiries contacts:
    Email:            sales at opendium.com
    Phone:            +44-1792-824568 / sip:sales at opendium.com

Support contacts:
    Email:            support at opendium.com
    Phone:            +44-1792-825748 / sip:support at opendium.com


From johnzeng2013 at yahoo.com  Wed Mar 11 13:11:42 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 11 Mar 2015 21:11:42 +0800
Subject: [squid-users] i hope to build web Authentication portal at
 Tproxy environment recenty , can you give me some advisement .
In-Reply-To: <55001EDE.700@opendium.com>
References: <5500175B.5040806@yahoo.com> <55001EDE.700@opendium.com>
Message-ID: <55003F0E.4010008@yahoo.com>


Hello Steve:

                     Thanks for your clear detail and advisement .

                      John


> On 11.03.15 10:22, johnzeng wrote:
>
>> whether php or jquery need send user ip address to squid ? otherwise i
>> worried whether squid can confirm user info
>>
>> and how to identify and controll http traffic ?
>
> I'd do this with an external ACL - when processing a request, Squid 
> would call the external ACL which would do:
>
> 1. If the user is not authenticated or their "last seen" timestamp has 
> expired, return "ERR"
> 2. If the user is authenticated, update their "last seen" timestamp 
> and return OK.
>
> Obviously if the ACL returns ERR, Squid needs to redirect the user to 
> the authentication page.  If the ACL returns OK, Squid needs to 
> service the request as normal.
>
> The authentication page would update the database which the external 
> ACL refers to.
>
> Identifying the user's traffic would need to be done by MAC address or 
> IP:
>  - MAC address requires a flat network with no routers between the 
> device and Squid.
>  - IP has (probably) unfixable problems in a dual-stacked network.
>
> Beware that:
> 1. Access to the authentication page must be allowed for 
> unauthenticated users (obviously :)
> 2. Authentication should really be done over HTTPS with a trusted 
> certificate.
> 3. Clients require access to some external servers to validate HTTPS 
> certs before they have authenticated.
> 4. If you want to support WISPr then (2) and (3) are mandatory.
> 5. External ACL caching
>
> You might be able to do it with internal ACLs, but... pain :)
>



From fredbmail at free.fr  Wed Mar 11 14:05:39 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 11 Mar 2015 15:05:39 +0100 (CET)
Subject: [squid-users] Ipc::Mem::Segment::attach failed to
 mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
In-Reply-To: <1179794891.412888208.1426082264954.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1242088312.412924572.1426082739873.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hi,

I'm trying workers and rock store, I missed something ?

workers 2
cache_dir rock /tmp/squid1 13000 max-size=1024
cache_dir rock /tmp/squid2 13000 max-size=1024


Squid Cache: Version 3.5.2-20150304-r13770
Service Name: squid
configure options:  '--prefix=/' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--libexecdir=/usr/lib/squid' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-auth-negotiate--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-basic-auth-helpers=LDAP,digest' '--enable-digest-auth-helpers=ldap,password' '--enable-arp-acl' '--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid' '--with-filedescriptors=65536' '--with-large-files' '--disable-snmp' '--with-default-user=squid'


Linux proxytest 3.2.0-4-686-pae #1 SMP Debian 3.2.54-2 i686 GNU/Linux (Debian Wheezy 32 bits)


squid -z
FATAL: Ipc::Mem::Segment::attach failed to mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
2015 Mar 11 14:55:26 localhost Ipc::Mem::Segment::attach failed to mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory




From squid3 at treenet.co.nz  Wed Mar 11 15:49:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 04:49:39 +1300
Subject: [squid-users] Ipc::Mem::Segment::attach failed to
 mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
In-Reply-To: <1242088312.412924572.1426082739873.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1242088312.412924572.1426082739873.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <55006413.2030002@treenet.co.nz>

On 12/03/2015 3:05 a.m., FredB wrote:
> Hi,
> 
> I'm trying workers and rock store, I missed something ?
> 
> workers 2
> cache_dir rock /tmp/squid1 13000 max-size=1024
> cache_dir rock /tmp/squid2 13000 max-size=1024
> 
> 
> Squid Cache: Version 3.5.2-20150304-r13770
> Service Name: squid
> configure options: '--prefix=/' '--includedir=${prefix}/include'
'--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info'
'--sysconfdir=/etc' '--libexecdir=/usr/lib/squid'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--srcdir=.' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline'
'--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-auth-negotiate--enable-cache-digests' '--enable-underscores'

mising whitespace separator between those options.

'--enable-icap-client' '--enable-follow-x-forwarded-for'
'--enable-basic-auth-helpers=LDAP,digest'
'--enable-digest-auth-helpers=ldap,password'

Syntax: --enable-auth-TYPE=HELPER,LIST

If you dont use that syntax to explicitly limit the helpers built you
get the default list of: everything that will build.

 '--enable-arp-acl'
'--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid'
'--with-filedescriptors=65536' '--with-large-files' '--disable-snmp'
'--with-default-user=squid'
> 
> 
> Linux proxytest 3.2.0-4-686-pae #1 SMP Debian 3.2.54-2 i686 GNU/Linux (Debian Wheezy 32 bits)
> 
> 
> squid -z
> FATAL: Ipc::Mem::Segment::attach failed to mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
> 2015 Mar 11 14:55:26 localhost Ipc::Mem::Segment::attach failed to mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory

You may be missing a library, but I'm not aware what off the top of my
head of any particular one.

Or perhape its the recent issue of workers SUID permissions. I hope
Christos applies the final patch fixing that today, if not the one in
squid-dev is usable enough.

Amos


From jaykbvt at gmail.com  Wed Mar 11 18:41:06 2015
From: jaykbvt at gmail.com (jaykbvt)
Date: Wed, 11 Mar 2015 11:41:06 -0700 (PDT)
Subject: [squid-users] Hostname missing in request URL
Message-ID: <1426099266130-4670326.post@n4.nabble.com>

Hi, I am facing a strange scenario where my users requested URL gets
truncated in request to my squid server. The setup is in transparent proxy
mode with Cisco ISG in between with L4 redirection on www traffic.

 

It works proper only if I configure proxy settings explicitly in user's
browser.

 

Given below are excerpt of TCP request with both the scenario.

  

 



Without proxy setting

 

GET / HTTP/1.1

Host: www.cisco.com

User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:36.0) Gecko/20100101
Firefox/36.0

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8

Accept-Language: en-US,en;q=0.5

Accept-Encoding: gzip, deflate



 


With Proxy setting

 

.GET http://www.cisco.com/favicon.ico HTTP/1.1

Host: www.cisco.com

User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:36.0) Gecko/20100101
Firefox/36.0

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8

Accept-Language: en-US,en;q=0.5

Accept-Encoding: gzip, deflate




--
Thanks & Regards,
jaykbvt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Hostname-missing-in-request-URL-tp4670326.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Mar 11 19:29:18 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 11 Mar 2015 21:29:18 +0200
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <55000645.5010800@yahoo.com>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD958A.8000100@yahoo.com> <54FEE428.3080308@treenet.co.nz>
 <55000645.5010800@yahoo.com>
Message-ID: <5500978E.9040301@ngtech.co.il>

Hey,

I was left in the dark and still unsure what the situation is??
Did you made it work fine?

Eliezer

On 11/03/2015 11:09, johnzeng wrote:
>
>        Hello Amos:
>
>                            Ok,  I see
>
>                            Thanks again.
>
>                             Have a good day with you .
>
>



From dweimer at dweimer.net  Wed Mar 11 19:59:34 2015
From: dweimer at dweimer.net (dweimer)
Date: Wed, 11 Mar 2015 14:59:34 -0500
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
References: <0458b52461578fe1012818892d032556@dweimer.net>
 <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
Message-ID: <b3dc7de79e30d3ee613464d80c3fc709@dweimer.net>

On 03/11/2015 1:16 am, Alex Samad wrote:
> This is mine against 2008. haven't had any issues with attachments up 
> to 10M
> 
> 
> cache_peer 127.0.0.1 parent 443 0 proxy-only no-query no-digest
> originserver login=PASS ssl sslflags=DONT_VERIFY_PEER
> sslcert=/etc/httpd/conf.d/o.crt sslkey=/etc/httpd/conf.d/o.key
> name=webServer
> cache_peer 10.32.69.11 parent 443 0 proxy-only no-query no-digest
> originserver login=PASS front-end-https=on ssl
> sslflags=DONT_VERIFY_PEER sslcert=/etc/httpd/conf.d/o.crt
> sslkey=/etc/httpd/conf.d/o.key name=exchangeServer
> 
> # List of acceptable URLs to send to the Exchange server
> acl exch_url url_regex -i <o>/exchange
> acl exch_url url_regex -i <o>/exchweb
> acl exch_url url_regex -i <o>/public
> acl exch_url url_regex -i <o>/owa
> acl exch_url url_regex -i <o>/ecp
> acl exch_url url_regex -i <o>/microsoft-server-activesync
> acl exch_url url_regex -i <o>/rpc
> acl exch_url url_regex -i <o>/rpcwithcert
> acl exch_url url_regex -i <o>/exadmin
> acl exch_url url_regex -i <o>/oab
> 
> # Send the Exchange URLs to the Exchange server
> cache_peer_access exchangeServer allow exch_url
> 
> # Send everything else to the Apache
> cache_peer_access webServer deny exch_url
> 
> # This is to protect Squid
> never_direct allow exch_url
> 
> # Logging Configuration
> redirect_rewrites_host_header off
> cache_mem 32 MB
> maximum_object_size_in_memory 128 KB
> cache_log none
> cache_store_log none
> 
> access_log /var/log/squid/office-access.log squid
> #access_log none
> cache_log /var/log/squid/office-cache.log
> #cache_log none
> pid_filename /var/run/squid-office.pid
> 
> 
> # Set the hostname so that we can see Squid in the path (Optional)
> visible_hostname <o>
> deny_info TCP_RESET all
> 
> # ACL - required to allow
> #acl all src ALL
> 
> # Allow everyone through, internal and external connections
> http_access allow all
> miss_access allow all
> 
> icp_port 0
> snmp_port 0
> 
> via off
> 
> On 11 March 2015 at 15:42, dweimer <dweimer at dweimer.net> wrote:
>> We have setup Squid as a reverse proxy to Exchange 2010 OWA server we
>> thought everything was working OK, but found out that any file 
>> attachments
>> over 2MB cause a timeout after 5 minutes. I remembered having this 
>> issue a
>> while back with HTTPS, and it just went away after some updates. Some
>> searching found other users posting messages to the Squid mailing list 
>> that
>> had this issue in particular with OWA. However I never found a fix on 
>> any of
>> the threads.
>> 
>> Squid is currently running 3.4.11, on FreeBSD 10.1-RELEASE-p5, This 
>> occurs
>> even when sending the file through the local network passing through 
>> the
>> reverse proxy. With the slowest link being a 1G.
>> 
>> Below is the relevant parts of the configuration, with some 
>> information
>> excluded for security
>> https_port 10.50.20.12:443 accel defaultsite=... \
>>  cert=... \
>>  key=... \
>>  options=NO_SSLv2:NO_TLSv1:CIPHER_SERVER_PREFERENCE \
>>  cipher=RC4:!MD5:!aNULL:!EDH
>> 
>> cache_peer ... parent 443 0 ssl no-query no-digest no-netdb-exchange
>> originserver name=owa2010_parent sslcapath=/usr/local/share/certs
>> sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on
>> 
>> We also host sharepoint (certificate is a wildcard certificate) this 
>> way as
>> well, and I have just verified that it has the same problem. It is 
>> served by
>> the same https_port line, and a different cache_peer the only 
>> difference is
>> the IP and it doesn't have the front-end-https option set.
>> 
>> Does anyone have any ideas to check?
>> 
>> is this possibly a cause
>> <http://www.squid-cache.org/Doc/config/broken_posts/>?
>> 

I don't believe OWA has anything to do with my problem, it appears to be 
related entirely to posting with HTTPS using the reverse proxy. I ran 
into this after the upgrade from squid 3.1 to 3.2 back on FreeBSD 
version 9.0 & 9.1. I never did find the answer to the issue then, it 
finally just went away after the upgrade to squid 3.3. At the time the 
application I was using was using that was affected was running on 
Apache and PHP, that application is still run from this reverse proxy 
server as well but apparently unaffected by the problem now. It's 
current version uses an uploader process that uses java to chunk the 
files into smaller pieces during upload.

What appears to happen is that the client sends the entire file to the 
Squid Reverse Proxy server, but somehow at the end of the file Squid 
never receives an expected end of file from the client. The client 
believes that everything is sent and sits waiting until it times out.

Oddly enough when I put fiddler on my laptop to capture the HTTPS 
traffic in a decrypted method so that I could see more information into 
what was happening. It succeeds if the browser is talking to fiddler, 
which is talking to the reverse proxy which is talking to the end 
server. Why adding fiddler to this chain of options fixes it is well 
beyond my understanding.

I am working on setting up a more simplified VM with mimimal settings in 
that I can run debugging in to get a better logs to help with the issue 
the current system has to much traffic on it. Does anyone have any 
suggestions for which debugging level I should use for squid in order to 
attempt to get useful logs to troubleshoot this?

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From tmblue at gmail.com  Wed Mar 11 20:22:04 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 11 Mar 2015 13:22:04 -0700
Subject: [squid-users] =?utf-8?q?3=2E5_cache_and_=E2=80=9Conly-if-cached?=
	=?utf-8?q?=E2=80=9D_directive_was_specified=2E?=
Message-ID: <CAEaSS0a+Ps+JjgdEnDwWo0d37FZuoY8U5t-R4DWWo+uN5XVHVw@mail.gmail.com>

Wondering why I'm getting this error, what config param am I missing?

I have 1 parent, 2 squid servers configured as siblings for each other

http_port 80 accel vhost

cache_peer apps-preprod.domain.net parent 80 0 no-digest no-query
originserver no-netdb-exchange

cache_peer cache01.pp.sv.domain.net sibling 80 3130 no-digest
no-netdb-exchange

I can't have this error bubble up, regardless of what a client/browser is
looking for. I need to be able to check the sibling and go to the parent if
neither has it..

ERROR:

The following error was encountered while trying to retrieve the URL:
http://view-preprod.admission.net/rimfire/admission/search/v2.0?

*Valid document was not found in the cache and ?only-if-cached? directive
was specified.*

You have issued a request with a ?only-if-cached? cache control directive.
The document was not found in the cache, *or* it required revalidation
prohibited by the ?only-if-cached? directive.


Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150311/6132fb0a/attachment.htm>

From grzeg.falkowski at gmail.com  Wed Mar 11 21:26:33 2015
From: grzeg.falkowski at gmail.com (Grzegorz Falkowski)
Date: Wed, 11 Mar 2015 22:26:33 +0100
Subject: [squid-users] SquidclamAV respons modification
Message-ID: <CAMD-m9MdfkK9BbwrBhRBpL6CTYv3WT7-6MY5g=WQa_BZpoQmHw@mail.gmail.com>

Hello,
I plan to use sclamav with c-icap to secure web app from malware threat.
I prepare whole configuration and it's work fine. Unfortunately in first
stage of implementation it shouldn't make any changes to the respond. Virus
detection must be logged and that it. I was looking for a solution in
documentation of c-icap, clamav, squidclamav but I didn't find any.
My idea is to modify the way in which the c-icap handle feedback from clamav
. C-icap should ignore the information that has been detected threat and
return the original request to squid. I suspect that I will need to modify
the source code to achieve this
Has anyone tried to make such a modification?
Some  other hints?

Thank You in advance
Best Regards
Grzegorz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150311/de57a134/attachment.htm>

From alex at samad.com.au  Wed Mar 11 22:15:22 2015
From: alex at samad.com.au (Alex Samad)
Date: Thu, 12 Mar 2015 09:15:22 +1100
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <54FFF826.3080008@treenet.co.nz>
References: <0458b52461578fe1012818892d032556@dweimer.net>
 <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
 <54FFF826.3080008@treenet.co.nz>
Message-ID: <CAJ+Q1PX3R_5aUXs_HQn3Db4YJUD5Qf-57DAzUFDo6qhjf=DdiA@mail.gmail.com>

I have to admit this was built from a lot of googling for a working config.


On 11 March 2015 at 19:09, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 11/03/2015 7:16 p.m., Alex Samad wrote:
[snip]
>> # List of acceptable URLs to send to the Exchange server
>> acl exch_url url_regex -i <o>/exchange
>> acl exch_url url_regex -i <o>/exchweb
>> acl exch_url url_regex -i <o>/public
>> acl exch_url url_regex -i <o>/owa
>> acl exch_url url_regex -i <o>/ecp
>> acl exch_url url_regex -i <o>/microsoft-server-activesync
>> acl exch_url url_regex -i <o>/rpc
>> acl exch_url url_regex -i <o>/rpcwithcert
>> acl exch_url url_regex -i <o>/exadmin
>> acl exch_url url_regex -i <o>/oab
>
> I suggest you replace the above with ACLs:
>
>  acl exch_domain dstdomain <o>
>  acl exch_path urlpath_regex -i /exch(ange|web)
>  acl exch_path urlpath_regex -i /public
>  acl exch_path urlpath_regex -i /owa
>  ...
I presume you ... means the other paths

and this is for speed ?

>
[snip]

>> # Logging Configuration
>> redirect_rewrites_host_header off
>
> Thats begging for abuse of the security hole it opens. If you can
> operate without that setting please do so.
I believe (and its been a while, that it was need for exchange), I can
find some time and retest.

> NOTE: its not optional. You have disabled most of the HTTP features
> which use the Squid hostname, but not all of them can be.
>  For example "via off" contradicts this comment, by NOT adding Squid to
> the relay path (Via header).
>
>
>> visible_hostname <o>
>> deny_info TCP_RESET all
>>
>> # ACL - required to allow
>> #acl all src ALL
>>
>> # Allow everyone through, internal and external connections
>> http_access allow all
>> miss_access allow all
>>
>> icp_port 0
>> snmp_port 0
>>
>> via off
so you would suggest
visibile <o>
and no via off ?

>>
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Mar 11 23:48:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 12:48:51 +1300
Subject: [squid-users]
 =?utf-8?q?3=2E5_cache_and_=E2=80=9Conly-if-cached?=
 =?utf-8?q?=E2=80=9D_directive_was_specified=2E?=
In-Reply-To: <CAEaSS0a+Ps+JjgdEnDwWo0d37FZuoY8U5t-R4DWWo+uN5XVHVw@mail.gmail.com>
References: <CAEaSS0a+Ps+JjgdEnDwWo0d37FZuoY8U5t-R4DWWo+uN5XVHVw@mail.gmail.com>
Message-ID: <5500D463.3020303@treenet.co.nz>

On 12/03/2015 9:22 a.m., Tory M Blue wrote:
> Wondering why I'm getting this error, what config param am I missing?
> 
> I have 1 parent, 2 squid servers configured as siblings for each other
> 
> http_port 80 accel vhost
> 
> cache_peer apps-preprod.domain.net parent 80 0 no-digest no-query
> originserver no-netdb-exchange
> 
> cache_peer cache01.pp.sv.domain.net sibling 80 3130 no-digest
> no-netdb-exchange


These siblings use ICP protocol to check if the other proxy has the
object. That protocol only uses URL and fails if the response uses Vary
mechanism from HTTP/1.1.

Its better to use HTCP protocol between the siblings so they can cope
with variants and other HTTP/1.1 things better. For that you need the
"htcp" parameter on the cache_peer line, maybe Squid to be built with
--enable-htcp, and htcp_port and access controls to be configured.

Amos



From squid3 at treenet.co.nz  Wed Mar 11 23:54:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 12:54:05 +1300
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <CAJ+Q1PX3R_5aUXs_HQn3Db4YJUD5Qf-57DAzUFDo6qhjf=DdiA@mail.gmail.com>
References: <0458b52461578fe1012818892d032556@dweimer.net>	<CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>	<54FFF826.3080008@treenet.co.nz>
 <CAJ+Q1PX3R_5aUXs_HQn3Db4YJUD5Qf-57DAzUFDo6qhjf=DdiA@mail.gmail.com>
Message-ID: <5500D59C.3070706@treenet.co.nz>

On 12/03/2015 11:15 a.m., Alex Samad wrote:
> I have to admit this was built from a lot of googling for a working config.
> 
> 
> On 11 March 2015 at 19:09, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 11/03/2015 7:16 p.m., Alex Samad wrote:
> [snip]
>>> # List of acceptable URLs to send to the Exchange server
>>> acl exch_url url_regex -i <o>/exchange
>>> acl exch_url url_regex -i <o>/exchweb
>>> acl exch_url url_regex -i <o>/public
>>> acl exch_url url_regex -i <o>/owa
>>> acl exch_url url_regex -i <o>/ecp
>>> acl exch_url url_regex -i <o>/microsoft-server-activesync
>>> acl exch_url url_regex -i <o>/rpc
>>> acl exch_url url_regex -i <o>/rpcwithcert
>>> acl exch_url url_regex -i <o>/exadmin
>>> acl exch_url url_regex -i <o>/oab
>>
>> I suggest you replace the above with ACLs:
>>
>>  acl exch_domain dstdomain <o>
>>  acl exch_path urlpath_regex -i /exch(ange|web)
>>  acl exch_path urlpath_regex -i /public
>>  acl exch_path urlpath_regex -i /owa
>>  ...
> I presume you ... means the other paths

Yes.

> 
> and this is for speed ?

Yes, and safety preventing matching against other non-path parts of the URL.

I forgot to start the paths with "^" characters in the above to make
sure they match only at the beginning of the path.

> 
>>
> [snip]
> 
>>> # Logging Configuration
>>> redirect_rewrites_host_header off
>>
>> Thats begging for abuse of the security hole it opens. If you can
>> operate without that setting please do so.
> I believe (and its been a while, that it was need for exchange), I can
> find some time and retest.
> 
>> NOTE: its not optional. You have disabled most of the HTTP features
>> which use the Squid hostname, but not all of them can be.
>>  For example "via off" contradicts this comment, by NOT adding Squid to
>> the relay path (Via header).
>>
>>
>>> visible_hostname <o>
>>> deny_info TCP_RESET all
>>>
>>> # ACL - required to allow
>>> #acl all src ALL
>>>
>>> # Allow everyone through, internal and external connections
>>> http_access allow all
>>> miss_access allow all
>>>
>>> icp_port 0
>>> snmp_port 0
>>>
>>> via off
> so you would suggest
> visibile <o>
> and no via off ?

Yes if you can. It is more HTTP compliant and helps detect some
potential nasty attacks.

Amos


From squid3 at treenet.co.nz  Thu Mar 12 00:07:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 13:07:36 +1300
Subject: [squid-users] Squid Reverse Proxy to Exchange 2010 OWA
In-Reply-To: <b3dc7de79e30d3ee613464d80c3fc709@dweimer.net>
References: <0458b52461578fe1012818892d032556@dweimer.net>
 <CAJ+Q1PU_mMzzH+a9EF0yoTA7qq-kXwau=iEwCQKAp5C6te2NAg@mail.gmail.com>
 <b3dc7de79e30d3ee613464d80c3fc709@dweimer.net>
Message-ID: <5500D8C8.8030509@treenet.co.nz>

On 12/03/2015 8:59 a.m., dweimer wrote:
> On 03/11/2015 1:16 am, Alex Samad wrote:
>> This is mine against 2008. haven't had any issues with attachments up
>> to 10M
>>
>>
>> cache_peer 127.0.0.1 parent 443 0 proxy-only no-query no-digest
>> originserver login=PASS ssl sslflags=DONT_VERIFY_PEER
>> sslcert=/etc/httpd/conf.d/o.crt sslkey=/etc/httpd/conf.d/o.key
>> name=webServer
>> cache_peer 10.32.69.11 parent 443 0 proxy-only no-query no-digest
>> originserver login=PASS front-end-https=on ssl
>> sslflags=DONT_VERIFY_PEER sslcert=/etc/httpd/conf.d/o.crt
>> sslkey=/etc/httpd/conf.d/o.key name=exchangeServer
>>
>> # List of acceptable URLs to send to the Exchange server
>> acl exch_url url_regex -i <o>/exchange
>> acl exch_url url_regex -i <o>/exchweb
>> acl exch_url url_regex -i <o>/public
>> acl exch_url url_regex -i <o>/owa
>> acl exch_url url_regex -i <o>/ecp
>> acl exch_url url_regex -i <o>/microsoft-server-activesync
>> acl exch_url url_regex -i <o>/rpc
>> acl exch_url url_regex -i <o>/rpcwithcert
>> acl exch_url url_regex -i <o>/exadmin
>> acl exch_url url_regex -i <o>/oab
>>
>> # Send the Exchange URLs to the Exchange server
>> cache_peer_access exchangeServer allow exch_url
>>
>> # Send everything else to the Apache
>> cache_peer_access webServer deny exch_url
>>
>> # This is to protect Squid
>> never_direct allow exch_url
>>
>> # Logging Configuration
>> redirect_rewrites_host_header off
>> cache_mem 32 MB
>> maximum_object_size_in_memory 128 KB
>> cache_log none
>> cache_store_log none
>>
>> access_log /var/log/squid/office-access.log squid
>> #access_log none
>> cache_log /var/log/squid/office-cache.log
>> #cache_log none
>> pid_filename /var/run/squid-office.pid
>>
>>
>> # Set the hostname so that we can see Squid in the path (Optional)
>> visible_hostname <o>
>> deny_info TCP_RESET all
>>
>> # ACL - required to allow
>> #acl all src ALL
>>
>> # Allow everyone through, internal and external connections
>> http_access allow all
>> miss_access allow all
>>
>> icp_port 0
>> snmp_port 0
>>
>> via off
>>
>> On 11 March 2015 at 15:42, dweimer <dweimer at dweimer.net> wrote:
>>> We have setup Squid as a reverse proxy to Exchange 2010 OWA server we
>>> thought everything was working OK, but found out that any file
>>> attachments
>>> over 2MB cause a timeout after 5 minutes. I remembered having this
>>> issue a
>>> while back with HTTPS, and it just went away after some updates. Some
>>> searching found other users posting messages to the Squid mailing
>>> list that
>>> had this issue in particular with OWA. However I never found a fix on
>>> any of
>>> the threads.
>>>
>>> Squid is currently running 3.4.11, on FreeBSD 10.1-RELEASE-p5, This
>>> occurs
>>> even when sending the file through the local network passing through the
>>> reverse proxy. With the slowest link being a 1G.
>>>
>>> Below is the relevant parts of the configuration, with some information
>>> excluded for security
>>> https_port 10.50.20.12:443 accel defaultsite=... \
>>>  cert=... \
>>>  key=... \
>>>  options=NO_SSLv2:NO_TLSv1:CIPHER_SERVER_PREFERENCE \
>>>  cipher=RC4:!MD5:!aNULL:!EDH
>>>
>>> cache_peer ... parent 443 0 ssl no-query no-digest no-netdb-exchange
>>> originserver name=owa2010_parent sslcapath=/usr/local/share/certs
>>> sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on
>>>
>>> We also host sharepoint (certificate is a wildcard certificate) this
>>> way as
>>> well, and I have just verified that it has the same problem. It is
>>> served by
>>> the same https_port line, and a different cache_peer the only
>>> difference is
>>> the IP and it doesn't have the front-end-https option set.
>>>
>>> Does anyone have any ideas to check?
>>>
>>> is this possibly a cause
>>> <http://www.squid-cache.org/Doc/config/broken_posts/>?
>>>
> 
> I don't believe OWA has anything to do with my problem, it appears to be
> related entirely to posting with HTTPS using the reverse proxy. I ran
> into this after the upgrade from squid 3.1 to 3.2 back on FreeBSD
> version 9.0 & 9.1. I never did find the answer to the issue then, it
> finally just went away after the upgrade to squid 3.3. At the time the
> application I was using was using that was affected was running on
> Apache and PHP, that application is still run from this reverse proxy
> server as well but apparently unaffected by the problem now. It's
> current version uses an uploader process that uses java to chunk the
> files into smaller pieces during upload.
> 
> What appears to happen is that the client sends the entire file to the
> Squid Reverse Proxy server, but somehow at the end of the file Squid
> never receives an expected end of file from the client. The client
> believes that everything is sent and sits waiting until it times out.
> 
> Oddly enough when I put fiddler on my laptop to capture the HTTPS
> traffic in a decrypted method so that I could see more information into
> what was happening. It succeeds if the browser is talking to fiddler,
> which is talking to the reverse proxy which is talking to the end
> server. Why adding fiddler to this chain of options fixes it is well
> beyond my understanding.
> 
> I am working on setting up a more simplified VM with mimimal settings in
> that I can run debugging in to get a better logs to help with the issue
> the current system has to much traffic on it. Does anyone have any
> suggestions for which debugging level I should use for squid in order to
> attempt to get useful logs to troubleshoot this?
> 

debug_options ALL,9 if you are using a lab setup like that with easily
repeatable test.


It could be related to the issue for which broken_posts exists, but that
directive will probably only help if Squid is aware that it has the
entire request payload.

Or it could be Squid is stuck with only 1-2 bytes of buffer space
available and waiting for the server to drain it.

Amos


From squid3 at treenet.co.nz  Thu Mar 12 01:09:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 14:09:16 +1300
Subject: [squid-users] SquidclamAV respons modification
In-Reply-To: <CAMD-m9MdfkK9BbwrBhRBpL6CTYv3WT7-6MY5g=WQa_BZpoQmHw@mail.gmail.com>
References: <CAMD-m9MdfkK9BbwrBhRBpL6CTYv3WT7-6MY5g=WQa_BZpoQmHw@mail.gmail.com>
Message-ID: <5500E73C.2050504@treenet.co.nz>

On 12/03/2015 10:26 a.m., Grzegorz Falkowski wrote:
> Hello,
> I plan to use sclamav with c-icap to secure web app from malware threat.
> I prepare whole configuration and it's work fine. Unfortunately in first
> stage of implementation it shouldn't make any changes to the respond. Virus
> detection must be logged and that it.

Bad Idea. You are knowingly allowing your clients to be infected.

> I was looking for a solution in
> documentation of c-icap, clamav, squidclamav but I didn't find any.
> My idea is to modify the way in which the c-icap handle feedback from clamav
> . C-icap should ignore the information that has been detected threat and
> return the original request to squid. I suspect that I will need to modify
> the source code to achieve this
> Has anyone tried to make such a modification?

Why would anyone sane want clients to become infected if they could
prevent it? you will have enough false-negatives occuring anyway.

I recommend you skip this and move on to identifying how clamav records
whats its done and why. Or at least change the planned bypass to make
clamav do less intensive scanning initially. There should be settings in
clamav regarding logging level and level of scan performed.

Amos



From squid3 at treenet.co.nz  Thu Mar 12 01:22:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 14:22:37 +1300
Subject: [squid-users] Hostname missing in request URL
In-Reply-To: <1426099266130-4670326.post@n4.nabble.com>
References: <1426099266130-4670326.post@n4.nabble.com>
Message-ID: <5500EA5D.6080402@treenet.co.nz>

On 12/03/2015 7:41 a.m., jaykbvt wrote:
> Hi, I am facing a strange scenario where my users requested URL gets
> truncated in request to my squid server. The setup is in transparent proxy
> mode with Cisco ISG in between with L4 redirection on www traffic.
> 
>  
> 
> It works proper only if I configure proxy settings explicitly in user's
> browser.
> 

<http://tools.ietf.org/html/rfc7230#section-5.3>

In short; its not truncated. That is the proper/expected message format
on each of the *two different ports*.

Handling the intercepted URL format from port 80 messages is part of
what the "intercept" option does in squid.conf http_port.

If you follow one of the config examples detailed in the section
"Traffic Interception capture into Squid" and the below wiki page you
wont get these issues.
 <http://wiki.squid-cache.org/ConfigExamples/Intercept>

Amos



From eliezer at ngtech.co.il  Thu Mar 12 01:50:32 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 12 Mar 2015 03:50:32 +0200
Subject: [squid-users] One Time Password with squid, exists?
Message-ID: <5500F0E8.1010405@ngtech.co.il>

Hey List,

I was wondering about if OTP was ever implemented officially with squid?
I have seen that most OTP works with some kind of third party system 
such as a radius server.

I had in mind a simple setup with a mysql backend and a php\cgi\other 
frontend that will create a one time password for the user which will be 
valid for some period of time\window.

The options for authentication are:
- basic (via db)
- digest

For me the most simple setup with a basic authentication would be the 
most simple to implement but digest I cannot yet understand so I cannot 
really create a fully blown system for that.

If the list users can help me to compose the sketch for the idea I will 
might be able to implement a basic OTP idea with a mysql DB.

What one time password authentication should contain?
How long should a password be usable?
What would invalidate the password?
How many wrong passwords should be considered a cracking trial?

If you have more ideas about the subject I would be happy to see them here.

Thanks In Advance,
Eliezer Croitoru


From squid3 at treenet.co.nz  Thu Mar 12 02:01:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 15:01:33 +1300
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <5500F0E8.1010405@ngtech.co.il>
References: <5500F0E8.1010405@ngtech.co.il>
Message-ID: <5500F37D.9030800@treenet.co.nz>

On 12/03/2015 2:50 p.m., Eliezer Croitoru wrote:
> Hey List,
> 
> I was wondering about if OTP was ever implemented officially with squid?

To answer that you need to define OTP.

* Basic is the only scheme which delivers a password. So technically the
others are all one-use-password schemes already.

* Digest with nonce count 1 is a one-time-token scheme at the message level.

* Negotiate and NTLM are one-time-token schemes at the TCP connection level.

* Basic auth can be one-time-token but requires supporting logic to be
implemented in the clients, server, and a token asignment mechanism. Its
easier to just use Digest in most cases.

Amos



From eliezer at ngtech.co.il  Thu Mar 12 02:25:52 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 12 Mar 2015 04:25:52 +0200
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <5500F37D.9030800@treenet.co.nz>
References: <5500F0E8.1010405@ngtech.co.il> <5500F37D.9030800@treenet.co.nz>
Message-ID: <5500F930.5010307@ngtech.co.il>

Thanks Amos,

So NTLM has "two steps" authentication which means that there is a basic 
negotiation over the http connection to the proxy which makes it less 
secure then kerberos.

(speculating)
The main reason it's less secure then kerberos is that every part of the 
password negotiation steps is being done in the same channel that the 
proxy is being contacted and there for cannot apply a third party 
"verification" for the authenticity of any of the tokens.
As a matter of fact NTLM http proxy authentication may be intercepted 
and can do lots of bad things to the connections.

I will try to read more about Digest authentication to make sure I will 
not create something when it's not needed.

But in any case that there is an option to make the proxy to client 
connection one level more secure then plain http proxy port it should be 
considered better, right?

Eliezer

On 12/03/2015 04:01, Amos Jeffries wrote:
> To answer that you need to define OTP.
>
> * Basic is the only scheme which delivers a password. So technically the
> others are all one-use-password schemes already.
>
> * Digest with nonce count 1 is a one-time-token scheme at the message level.
>
> * Negotiate and NTLM are one-time-token schemes at the TCP connection level.
>
> * Basic auth can be one-time-token but requires supporting logic to be
> implemented in the clients, server, and a token asignment mechanism. Its
> easier to just use Digest in most cases.
>
> Amos



From hack.back at hotmail.com  Thu Mar 12 02:47:57 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 11 Mar 2015 19:47:57 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:769:
	"Comm::IsConnOpen(conn)"
In-Reply-To: <1424449064181-4670001.post@n4.nabble.com>
References: <1424025171679-4669842.post@n4.nabble.com>
 <1424209440077-4669927.post@n4.nabble.com> <54E3F058.3040200@treenet.co.nz>
 <1424253241949-4669944.post@n4.nabble.com>
 <1424377008695-4669964.post@n4.nabble.com>
 <1424449064181-4670001.post@n4.nabble.com>
Message-ID: <1426128477672-4670340.post@n4.nabble.com>

please i need solution for this am using 3.4.12 and still same problem every
few hour squid restart automatically 
maybe high traffic make this problem ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-769-Comm-IsConnOpen-conn-tp4669842p4670340.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar 12 03:02:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 16:02:31 +1300
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <5500F930.5010307@ngtech.co.il>
References: <5500F0E8.1010405@ngtech.co.il> <5500F37D.9030800@treenet.co.nz>
 <5500F930.5010307@ngtech.co.il>
Message-ID: <550101C7.2000908@treenet.co.nz>

On 12/03/2015 3:25 p.m., Eliezer Croitoru wrote:
> Thanks Amos,
> 
> So NTLM has "two steps" authentication which means that there is a basic
> negotiation over the http connection to the proxy which makes it less
> secure then kerberos.
> 
> (speculating)
> The main reason it's less secure then kerberos is that every part of the
> password negotiation steps is being done in the same channel that the
> proxy is being contacted and there for cannot apply a third party
> "verification" for the authenticity of any of the tokens.
> As a matter of fact NTLM http proxy authentication may be intercepted
> and can do lots of bad things to the connections.

No, quite. The main reason is that NTLM (without NTLMv2 extensions)
contains silent downgrade to several LM protocol versions that either
send the password in username:password form or only encode it with a
8-bit hash that can be decoded as easily as base64 Basic auth. NTLMv1
does that whenever the client/attacker wants it to. NTLMv2 is better,
but still has relatively easily decrypted keys and the interception
problem you are aware of.

The Squid SMB_LM helpers work by using that LM weakness. Which is why
they dont work with Windows systems newer than 2006 when MS started
removing NTLMv1 support at the code level.

> 
> I will try to read more about Digest authentication to make sure I will
> not create something when it's not needed.
> 
> But in any case that there is an option to make the proxy to client
> connection one level more secure then plain http proxy port it should be
> considered better, right?

Perhapse. It depends on whether that step-up is actually better. There
are a lot of attempts and possible ways to do it that leave people worse
off than where they started (ie the LM issues).

The most secure authentication I've seen so far was simply Basic auth
with a per-message client PKI key exchange in the username:password
fields and server response encrypted using the provided key. Thats a bit
over the top for most uses though, it require a lot of CPU time at both
ends and ability to create many keys fast.


PS. I also have that experimental branch containing OAuth 2.0 Bearer
implementation for Squid which can simplify things at least for Squids part.

Amos



From squid3 at treenet.co.nz  Thu Mar 12 05:07:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 18:07:05 +1300
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <CAOsHgtvDPZpsJzjrR32D22B2KzfJf24nNw7KiUZ_ScFDgFG0FQ@mail.gmail.com>
References: <5500F0E8.1010405@ngtech.co.il>	<5500F37D.9030800@treenet.co.nz>	<5500F930.5010307@ngtech.co.il>	<550101C7.2000908@treenet.co.nz>
 <CAOsHgtvDPZpsJzjrR32D22B2KzfJf24nNw7KiUZ_ScFDgFG0FQ@mail.gmail.com>
Message-ID: <55011EF9.3020901@treenet.co.nz>

On 12/03/2015 4:22 p.m., Daniel Greenwald wrote:
> Amos- Where can i get the per message pki authentication you describe.!?

I saw it on a forum somewhere shortly after the Sony DRM/rootkit issue
came out. It was a proposal for non-intrusive DRM in music/video streams
with a custom client and server. Dismissed at the time due to DRM folks
wanting the encryption even the client could not decrypt. I've not seen
anything like it in public availability, but given that YT are now doing
HTML5 video I suspect they or Netflix are the guys to ask there.

Should be easy enough to code up one of your own though. The client
agent had a public key for the server, generated keys get sent as
password salted with something from the cert. The username was used for
a client cert for the actual auth part same as in TLS. Both parts
encrypted with the servers key to prevent MITM on the way.
 All the server has to do is decrypt the received cert+key, validate,
then encrypt the response payload (optionally some headers about it a
well) with the client provided key.


If I was implementing it today I'd use DNS TLSA record to publish the
server key(s) instead of embedding in the client, and list
available/preferred ciphers etc in the username part instead of just a
client cert.

Amos



From hack.back at hotmail.com  Thu Mar 12 05:14:04 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 11 Mar 2015 22:14:04 -0700 (PDT)
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <5500F0E8.1010405@ngtech.co.il>
References: <5500F0E8.1010405@ngtech.co.il>
Message-ID: <1426137244594-4670343.post@n4.nabble.com>

are you talking about radius server like free radius ?
or like dmasoftlab.com ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/One-Time-Password-with-squid-exists-tp4670337p4670343.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Thu Mar 12 05:15:30 2015
From: hack.back at hotmail.com (HackXBack)
Date: Wed, 11 Mar 2015 22:15:30 -0700 (PDT)
Subject: [squid-users] keep data after delete swap.state
Message-ID: <1426137330809-4670344.post@n4.nabble.com>

if there is error in swap.state file and want to recreate it what is the
option to do that without losing data on hdd ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/keep-data-after-delete-swap-state-tp4670344.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar 12 05:37:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 18:37:43 +1300
Subject: [squid-users] keep data after delete swap.state
In-Reply-To: <1426137330809-4670344.post@n4.nabble.com>
References: <1426137330809-4670344.post@n4.nabble.com>
Message-ID: <55012627.1030107@treenet.co.nz>

On 12/03/2015 6:15 p.m., HackXBack wrote:
> if there is error in swap.state file and want to recreate it what is the
> option to do that without losing data on hdd ?

1) stop Squid
2) manually erase the broken swap.state file(s)
3) run squid -z
4) start Squid

NOTE: squid -z is just to ensure the dir format is okay

NOTE 2: when the Squid run at (4) reports done rebuilding all caches its
ready for receiving traffic.

Amos



From grzeg.falkowski at gmail.com  Thu Mar 12 08:01:59 2015
From: grzeg.falkowski at gmail.com (Grzegorz Falkowski)
Date: Thu, 12 Mar 2015 09:01:59 +0100
Subject: [squid-users] SquidclamAV respons modification
In-Reply-To: <5500E73C.2050504@treenet.co.nz>
References: <CAMD-m9MdfkK9BbwrBhRBpL6CTYv3WT7-6MY5g=WQa_BZpoQmHw@mail.gmail.com>
 <5500E73C.2050504@treenet.co.nz>
Message-ID: <CAMD-m9NGrbJeui_bNFdw1BJ0ybpuquWybyzXdevBDmkxWs3pew@mail.gmail.com>

2015-03-12 2:09 GMT+01:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 12/03/2015 10:26 a.m., Grzegorz Falkowski wrote:
> > Hello,
> > I plan to use sclamav with c-icap to secure web app from malware threat.
> > I prepare whole configuration and it's work fine. Unfortunately in first
> > stage of implementation it shouldn't make any changes to the respond.
> Virus
> > detection must be logged and that it.
>
> Bad Idea. You are knowingly allowing your clients to be infected.
>

Like I wrote before this is first stage of implementation. We don't want to
interferer user action but we would like have information about potential
threats in uploaded files

>
> > I was looking for a solution in
> > documentation of c-icap, clamav, squidclamav but I didn't find any.
> > My idea is to modify the way in which the c-icap handle feedback from
> clamav
> > . C-icap should ignore the information that has been detected threat and
> > return the original request to squid. I suspect that I will need to
> modify
> > the source code to achieve this
> > Has anyone tried to make such a modification?
>
> Why would anyone sane want clients to become infected if they could
> prevent it? you will have enough false-negatives occuring anyway.
>
> I recommend you skip this and move on to identifying how clamav records
> whats its done and why. Or at least change the planned bypass to make
> clamav do less intensive scanning initially. There should be settings in
> clamav regarding logging level and level of scan performed.
>
> I did'n find any information in clamav documentation how can I change
scanning level in clamavd.
So I began to consider the above option

> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
Best regards
Grzegorz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150312/99455e1d/attachment.htm>

From kl at vsen.dk  Thu Mar 12 08:13:12 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 09:13:12 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <54FF1FBF.6000605@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
Message-ID: <55014A98.8030705@vsen.dk>

Hi Amos,

Thank you for the walkthrough..

Instead of having to play with tproxy on haproxy currently, I figured 
i'd try a simpler route..

The purpose of this setup, is to "jump around" a firewall issue with a 
sh#! firewall, which in order to filter http and https traffic 
appearently drops 5-15%.. and in 6 months noone has seem willing/able to 
fix it :(

I've put squid directly on the 10.43.18.181 (previously haproxy) ip - 
and changed to: http_port 80 intercept

I tried this on the client (being on other subnet):
iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination 
10.43.18.181:80

and the traffic looped in exactly the same way :(


Amos Jeffries wrote on 03/10/2015 05:45 PM:
> On 11/03/2015 4:08 a.m., Klavs Klavsen wrote:
>> hmm..
>>
>> I've read the config examples..
>>
>> I would very much like to understand how/why it works, if I've setup a
>> client to route package to squid (instead of trying to send directly)..
>
> I suggest diagramming your network traffic flow, writing down the TCP
> packet src/dst IP:port values on each connection.
>
> "Normal" interception operation showing your loop:
>
> 1) curl thinks its talking to Internet server
>      src ip-of-curl:*
>      dst ip-of-bt.dk:80
>
> 2) NAT diverts the packet to haproxy
>      src ip-of-curl:*
>      dst ip-of-haproxy-machine:*
>
> 3) haproxy knows its talking to Squid
>      src ip-of-haproxy-machine:*
>      dst 10.43.18.165:3129
>
> 4) Squid machine reports un-mangled NAT is ...
>      src ip-of-haproxy-machine:*
>      dst 10.43.18.165:3129
>
> 5) Squid contacts dst-IP to fetch the HTTP request
>      src 10.43.18.165:*
>      dst 10.43.18.165:3129
>
> 6) repeat from 4 (but with src now 10.43.18.165:*) until
>    a) machine runs out of memory,
>    b) machine runs out of TCP sockets,
>    c) machine runs out of disk space logging errors, or
>    d) Squid notices Via header loop and rejects request.
>
>
> A "Normal" connection directly to squid port:
>
>
> 1) curl thinks its talking to Internet server on 10.43.18.165:3129
>      src ip-of-curl:*
>      dst 10.43.18.165:3129
>
> 2) Squid machine reports un-mangled NAT is ...
>      src ip-of-curl:*
>      dst 10.43.18.165:3129
>
> 3) Squid contacts dst-IP to fetch the HTTP request
>      src 10.43.18.165:*
>      dst 10.43.18.165:3129
>
> 4) repeat from 2 (but with src now 10.43.18.165:*) until
>    a) machine runs out of memory,
>    b) machine runs out of TCP sockets,
>    c) machine runs out of disk space logging errors, or
>    d) Squid notices Via header loop and rejects request.
>
>
>
> Confirm it against wireshark packet traces of what is coming *in* to the
> Squid machine.
>
> Squid undoes the NAT mangling done on the machine its running. Any NAT
> changes prior to that is unknowable. NAT un-mangling can sometimes
> appear to work in Squid if haproxy runs on the same machine BUT, some
> NAT lookup requires correct TCP socket and some require particular
> IP:port ordering so its unrelible.
>
>
> The Squid access.log lines with ORIGINAL_DST show what client src-IP and
> dst-IP Squid received after NAT un-mangling. Though you have to take
> care to subtract the duration value from the line timestamp to get the
> order right for loops.
>
>
>>
>> I'm trying to follow this on a test client (haven't gotten it working yet):
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>> (where squid is amongst the internal clients - actually on it's own vlan
>> - but it's not the default route)
>
> If its on its own VLAN then its not "among the clients".
>
> "among the clients" means a bunch of machines all on the same LAN
> subnet, one of which is the Squid proxy. They can talk directly to each
> other without involving the router so you can hit the triangular routing
> problem and have to account for it in your forwarding and/or NAT rules.
>
> If you have Squid on a separate subnet or VLAN with all packets going
> via the router, that is a DMZ subnet. The router sees all packets and
> can adjusts them right, so triangular routing problem is avoided.
>
>
>>
>> meanwhile I tried pointing to the haproxy - which then forwards requests
>> in tcp mode, to squid server port 3129.
>>
>> If I just send to haproxy directly, I get the loop and this in the
>> accesslog:
>> 1425998994.271      0 10.43.18.165 TCP_MISS_ABORTED/000 0 GET
>> http://www.bt.dk/ - ORIGINAL_DST/10.43.18.165 -
>>
>> when doing:
>> curl -H "Host: www.bt.dk" http://proxy-haproxy-ip/
>>
>> 10.43.18.165 is the ip of squid server behind haproxy.
>
>
> Methinks that haproxy is still sending to the Squid port configured with
> "intercept" option.
>
> A connection between two proxies is explicitly configured. Thus a normal
> forward-proxy connection, no special port flags required.
>
>
> The log line is also showing haproxy to be vulnerable to CVE-2009-0801.
>
> If your Squid and haproxy are recent enough they could use the PROXY
> protocol to exchange the original client connections TCP/IP details.
> I've not tested it for this myself but that may also allow Squid with an
> intercept port to both handle the traffic okay (PROXY protocol override
> the NAT lookups), and protect against the CVE-2009-0801 vulnerability in
> haproxy ('intercept' flags existence makes Squid do the security checks).
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From james at ejbdigital.com.au  Thu Mar 12 08:14:49 2015
From: james at ejbdigital.com.au (James Harper)
Date: Thu, 12 Mar 2015 08:14:49 +0000
Subject: [squid-users] urlpath_regex
Message-ID: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>

I have just noticed that urlpath_regex isn't doing what I want:

acl wuau_repo dstdomain .download.windowsupdate.com
acl wuau_path urlpath_regex -i \.psf$
acl dst_server dstdomain server
acl apt_cacher browser apt-cacher

cache deny dst_server
cache deny apt_cacher
cache deny wuau_repo
cache allow all

url_rewrite_program /usr/local/squid/libexec/ext_apt_rewrite
url_rewrite_children 5 startup=0 idle=1 concurrency=0
url_rewrite_access deny apt_cacher
url_rewrite_access allow wuau_repo wuau_path
url_rewrite_access deny all

Basically I am using apt-cacher to cache windows updates (apt-cacher appears to handle and cache partial requests nicely while squid doesn't). The main static windows update content files appear to be .psf. So basically, if the destination is windows update, and the path ends in .psf, then rewrite the url to go to my apt-cacher server (called server, conveniently). If the browser string is apt-cacher then don't rewrite, to avoid loops. Also don't cache anything to do with these servers.

It works except for the wuau_path acl. The line:

url_rewrite_access allow wuau_repo wuau_path

matches all paths on the wuau_repo access list, not just those ending in .psf. I get hits for paths ending in .cab, and even worse, paths with a ? in them.

What am I doing wrong?

Thanks

James




From johnzeng2013 at yahoo.com  Thu Mar 12 08:18:52 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 12 Mar 2015 16:18:52 +0800
Subject: [squid-users] Whether squid 3.5.2 can support rock at wccp
 tproxy environment really ?
In-Reply-To: <5500978E.9040301@ngtech.co.il>
References: <54FD15C8.20801@yahoo.com> <54FD290C.3080201@treenet.co.nz>
 <54FD958A.8000100@yahoo.com> <54FEE428.3080308@treenet.co.nz>
 <55000645.5010800@yahoo.com> <5500978E.9040301@ngtech.co.il>
Message-ID: <55014BEC.90805@yahoo.com>


Hello Dear Eliezer:

                              Although

we are running two different process for wccp redirection and
Cache operation.

but There are the problem still  ,   i guess wccp need check rock and 
aufs' health ,

Maybe rock is incompatible with wccp .

  (

problem :

I face a problem recently , When i config wccp ( tproxy ) environment (
via using squid 3.5.2 ) ,

if i disable cache_dir rock part ,and it will be success for wccp(
tproxy) , and enable cache_dir aufs

#cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

but if i enable cache_dir rock part ,and it will be failure for wccp(
tproxy) and enable cache_dir aufs

cache_dir rock /accerater/webcache3/storage/rock1 2646 min-size=4096
max-size=262144 max-swap-rate=250 swap-timeout=350

)




From kl at vsen.dk  Thu Mar 12 09:11:39 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 10:11:39 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55014A98.8030705@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk>
Message-ID: <5501584B.1060402@vsen.dk>

As i Understand intercept - that will only work (as you said) when NAT 
is performed on the box that is to intercept (when I remove haproxy - 
that means the squid box itself).

and I'm going to move the squid box to the same network as the 
webservers - to be able to do it the routing way.

It seems this config should then be applicable "When Squid is in a DMZ 
between the router and Internet" from 
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute

But do I need to enable net.ipv4.ip_forward = 1 ?
But perhaps that is needed, for iptables/linux kernel to even evaluate 
PREROUTING rules (which then dnat's the package into squid port 3129)?

I'd like to only allow packages out of the box, when coming from squid 
(in case of some config issue - I'd like the squid-server NOT to route 
stuff, which didn't go through squid program).

and then I figured I'd could ensure that from happening, by adding:
iptables -t nat -A OUTPUT --match owner --uid-owner squid -p tcp --dport 
80 -j ACCEPT
iptables -t nat -A OUTPUT --match owner --uid-owner squid -p tcp --dport 
80 -j ACCEPT

and then ofcourse allow "state RELATED,ESTABLISHED" and traffic I 
actually "want" and have default policy DROP.

Klavs Klavsen wrote on 03/12/2015 09:13 AM:
> Hi Amos,
>
> Thank you for the walkthrough..
>
> Instead of having to play with tproxy on haproxy currently, I figured
> i'd try a simpler route..
>
> The purpose of this setup, is to "jump around" a firewall issue with a
> sh#! firewall, which in order to filter http and https traffic
> appearently drops 5-15%.. and in 6 months noone has seem willing/able to
> fix it :(
>
> I've put squid directly on the 10.43.18.181 (previously haproxy) ip -
> and changed to: http_port 80 intercept
>
> I tried this on the client (being on other subnet):
> iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination
> 10.43.18.181:80
>
> and the traffic looped in exactly the same way :(
>
>
> Amos Jeffries wrote on 03/10/2015 05:45 PM:
>> On 11/03/2015 4:08 a.m., Klavs Klavsen wrote:
>>> hmm..
>>>
>>> I've read the config examples..
>>>
>>> I would very much like to understand how/why it works, if I've setup a
>>> client to route package to squid (instead of trying to send directly)..
>>
>> I suggest diagramming your network traffic flow, writing down the TCP
>> packet src/dst IP:port values on each connection.
>>
>> "Normal" interception operation showing your loop:
>>
>> 1) curl thinks its talking to Internet server
>>      src ip-of-curl:*
>>      dst ip-of-bt.dk:80
>>
>> 2) NAT diverts the packet to haproxy
>>      src ip-of-curl:*
>>      dst ip-of-haproxy-machine:*
>>
>> 3) haproxy knows its talking to Squid
>>      src ip-of-haproxy-machine:*
>>      dst 10.43.18.165:3129
>>
>> 4) Squid machine reports un-mangled NAT is ...
>>      src ip-of-haproxy-machine:*
>>      dst 10.43.18.165:3129
>>
>> 5) Squid contacts dst-IP to fetch the HTTP request
>>      src 10.43.18.165:*
>>      dst 10.43.18.165:3129
>>
>> 6) repeat from 4 (but with src now 10.43.18.165:*) until
>>    a) machine runs out of memory,
>>    b) machine runs out of TCP sockets,
>>    c) machine runs out of disk space logging errors, or
>>    d) Squid notices Via header loop and rejects request.
>>
>>
>> A "Normal" connection directly to squid port:
>>
>>
>> 1) curl thinks its talking to Internet server on 10.43.18.165:3129
>>      src ip-of-curl:*
>>      dst 10.43.18.165:3129
>>
>> 2) Squid machine reports un-mangled NAT is ...
>>      src ip-of-curl:*
>>      dst 10.43.18.165:3129
>>
>> 3) Squid contacts dst-IP to fetch the HTTP request
>>      src 10.43.18.165:*
>>      dst 10.43.18.165:3129
>>
>> 4) repeat from 2 (but with src now 10.43.18.165:*) until
>>    a) machine runs out of memory,
>>    b) machine runs out of TCP sockets,
>>    c) machine runs out of disk space logging errors, or
>>    d) Squid notices Via header loop and rejects request.
>>
>>
>>
>> Confirm it against wireshark packet traces of what is coming *in* to the
>> Squid machine.
>>
>> Squid undoes the NAT mangling done on the machine its running. Any NAT
>> changes prior to that is unknowable. NAT un-mangling can sometimes
>> appear to work in Squid if haproxy runs on the same machine BUT, some
>> NAT lookup requires correct TCP socket and some require particular
>> IP:port ordering so its unrelible.
>>
>>
>> The Squid access.log lines with ORIGINAL_DST show what client src-IP and
>> dst-IP Squid received after NAT un-mangling. Though you have to take
>> care to subtract the duration value from the line timestamp to get the
>> order right for loops.
>>
>>
>>>
>>> I'm trying to follow this on a test client (haven't gotten it working
>>> yet):
>>> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>>> (where squid is amongst the internal clients - actually on it's own vlan
>>> - but it's not the default route)
>>
>> If its on its own VLAN then its not "among the clients".
>>
>> "among the clients" means a bunch of machines all on the same LAN
>> subnet, one of which is the Squid proxy. They can talk directly to each
>> other without involving the router so you can hit the triangular routing
>> problem and have to account for it in your forwarding and/or NAT rules.
>>
>> If you have Squid on a separate subnet or VLAN with all packets going
>> via the router, that is a DMZ subnet. The router sees all packets and
>> can adjusts them right, so triangular routing problem is avoided.
>>
>>
>>>
>>> meanwhile I tried pointing to the haproxy - which then forwards requests
>>> in tcp mode, to squid server port 3129.
>>>
>>> If I just send to haproxy directly, I get the loop and this in the
>>> accesslog:
>>> 1425998994.271      0 10.43.18.165 TCP_MISS_ABORTED/000 0 GET
>>> http://www.bt.dk/ - ORIGINAL_DST/10.43.18.165 -
>>>
>>> when doing:
>>> curl -H "Host: www.bt.dk" http://proxy-haproxy-ip/
>>>
>>> 10.43.18.165 is the ip of squid server behind haproxy.
>>
>>
>> Methinks that haproxy is still sending to the Squid port configured with
>> "intercept" option.
>>
>> A connection between two proxies is explicitly configured. Thus a normal
>> forward-proxy connection, no special port flags required.
>>
>>
>> The log line is also showing haproxy to be vulnerable to CVE-2009-0801.
>>
>> If your Squid and haproxy are recent enough they could use the PROXY
>> protocol to exchange the original client connections TCP/IP details.
>> I've not tested it for this myself but that may also allow Squid with an
>> intercept port to both handle the traffic okay (PROXY protocol override
>> the NAT lookups), and protect against the CVE-2009-0801 vulnerability in
>> haproxy ('intercept' flags existence makes Squid do the security checks).
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From hack.back at hotmail.com  Thu Mar 12 09:28:44 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 12 Mar 2015 02:28:44 -0700 (PDT)
Subject: [squid-users] keep data after delete swap.state
In-Reply-To: <55012627.1030107@treenet.co.nz>
References: <1426137330809-4670344.post@n4.nabble.com>
 <55012627.1030107@treenet.co.nz>
Message-ID: <1426152524357-4670351.post@n4.nabble.com>

Hmmmm, Thanks Amos , 
Then what [squid -F]  do then ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/keep-data-after-delete-swap-state-tp4670344p4670351.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Thu Mar 12 09:41:30 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 12 Mar 2015 10:41:30 +0100 (CET)
Subject: [squid-users] Ipc::Mem::Segment::attach failed to
 mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
In-Reply-To: <55006413.2030002@treenet.co.nz>
Message-ID: <1982154535.414582382.1426153290459.JavaMail.root@zimbra4-e1.priv.proxad.net>


> 
> mising whitespace separator between those options.
> 
> '--enable-icap-client' '--enable-follow-x-forwarded-for'
> '--enable-basic-auth-helpers=LDAP,digest'
> '--enable-digest-auth-helpers=ldap,password'
> 
> Syntax: --enable-auth-TYPE=HELPER,LIST
> 
> If you dont use that syntax to explicitly limit the helpers built you
> get the default list of: everything that will build.
> 


Fixed thanks, but same problem


> You may be missing a library, but I'm not aware what off the top of
> my
> head of any particular one.
> 
> Or perhape its the recent issue of workers SUID permissions. I hope
> Christos applies the final patch fixing that today, if not the one in
> squid-dev is usable enough.
> 

Ok I will try, perhaps a specific problem with 32Bits OS ?


From squid3 at treenet.co.nz  Thu Mar 12 10:23:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 23:23:36 +1300
Subject: [squid-users] urlpath_regex
In-Reply-To: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <55016928.4070900@treenet.co.nz>

On 12/03/2015 9:14 p.m., James Harper wrote:
> I have just noticed that urlpath_regex isn't doing what I want:
> 
> acl wuau_repo dstdomain .download.windowsupdate.com
> acl wuau_path urlpath_regex -i \.psf$
> acl dst_server dstdomain server
> acl apt_cacher browser apt-cacher
> 
> cache deny dst_server
> cache deny apt_cacher
> cache deny wuau_repo
> cache allow all
> 
> url_rewrite_program /usr/local/squid/libexec/ext_apt_rewrite
> url_rewrite_children 5 startup=0 idle=1 concurrency=0
> url_rewrite_access deny apt_cacher
> url_rewrite_access allow wuau_repo wuau_path
> url_rewrite_access deny all
> 
> 
> Basically I am using apt-cacher to cache windows updates (apt-cacher
> appears to handle and cache partial requests nicely while squid
> doesn't). The main static windows update content files appear to be
> .psf. So basically, if the destination is windows update, and the path
> ends in .psf, then rewrite the url to go to my apt-cacher server (called
> server, conveniently). If the browser string is apt-cacher then don't
> rewrite, to avoid loops. Also don't cache anything to do with these servers.

Three things;

* by re-writing you are generating an entirely new request with the
apt-cacher server URL as the destination. The HTTP message details about
what was originally requested and from where is *gone* when the traffic
leaves for the server. The solution for that is outlined at the end of
this mail.

* the .cab also contain "static" content for the updates installer files
and DLLs etc. particularly for the older Windows versions.

* if you leave Squid as being allowed to cache the content it will do so
for most of the visible page contents people see. And for a moderate
portion of the updates as well. Its just the range request fetches
inside archives that Squid wont cache without tuning.

> 
> It works except for the wuau_path acl. The line:
> 
> url_rewrite_access allow wuau_repo wuau_path
> 
> matches all paths on the wuau_repo access list, not just those ending in
> .psf. I get hits for paths ending in .cab, and even worse, paths with a
> ? in them.
>
> What am I doing wrong?

Strange. It should only be matching for URLs ending in ".psf".

Be aware that regex is quite literaly only looking at the line ending
and does explicitly mean URLs such as these as well:

 http://download.windowsupdate.com/blah.cab?voodoo.psf
 http://download.windowsupdate.com/?BLAH.PSF

If you need it to match only the things that look (to humans) like
on-disk file names I recommend:

   -i \.psf(\?.*)?$


Did you build your Squid normally, or with a specific regex library?

Is there another config line adding to wuau_path somewhere in your
squid.conf?

Is Squid actually running with the squid.conf you think it is?
 (sounds silly, but mistakes happen)



Also, unrelated to this ... have you tried using cache_peer instead of
URL-rewriting ?

 cache_peer server parent 8080 0
 cache_peer_access server allow wuau_repo wuau_path
 cache_peer_access server deny all

I assuming its listening on port 8080. It may need the "originserver"
option as well.
apt-cacher should fetch from the Internet, not looping back through Squid.

Amos



From james at ejbdigital.com.au  Thu Mar 12 10:52:59 2015
From: james at ejbdigital.com.au (James Harper)
Date: Thu, 12 Mar 2015 10:52:59 +0000
Subject: [squid-users] urlpath_regex
In-Reply-To: <55016928.4070900@treenet.co.nz>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55016928.4070900@treenet.co.nz>
Message-ID: <HKNPR04MB1934E2EDFB3930798D5FB37E8060@HKNPR04MB193.apcprd04.prod.outlook.com>

> Three things;
> 
> * by re-writing you are generating an entirely new request with the
> apt-cacher server URL as the destination. The HTTP message details about
> what was originally requested and from where is *gone* when the traffic
> leaves for the server. The solution for that is outlined at the end of
> this mail.
> 
> * the .cab also contain "static" content for the updates installer files
> and DLLs etc. particularly for the older Windows versions.

Long before I started using apt-cacher, I was having a problem with .cab?xxx files being cached when they shouldn't be, so I'm excluding them for now.

> * if you leave Squid as being allowed to cache the content it will do so
> for most of the visible page contents people see. And for a moderate
> portion of the updates as well. Its just the range request fetches
> inside archives that Squid wont cache without tuning.

AFAICT, the "tuning" involves telling squid to grab the whole file, and with some of these updates being >800MB, I think squid is the wrong tool for the job here.

My setup has squid running on an old laptop with a fairly small disk, and apt-cacher running on a server with a very large disk. The server isn't always on though, so I can't put squid there. I'm content with windows updates failing when the server is off, it's just for a home network.

> >
> > It works except for the wuau_path acl. The line:
> >
> > url_rewrite_access allow wuau_repo wuau_path
> >
> > matches all paths on the wuau_repo access list, not just those ending in
> > .psf. I get hits for paths ending in .cab, and even worse, paths with a
> > ? in them.
> >
> > What am I doing wrong?
> 
> Strange. It should only be matching for URLs ending in ".psf".
> 
> Be aware that regex is quite literaly only looking at the line ending
> and does explicitly mean URLs such as these as well:
> 
>  http://download.windowsupdate.com/blah.cab?voodoo.psf
>  http://download.windowsupdate.com/?BLAH.PSF
> 
> If you need it to match only the things that look (to humans) like
> on-disk file names I recommend:
> 
>    -i \.psf(\?.*)?$

I will rewrite as per your suggestion, but as I'm only applying the regex to windowsupdate.com urls I don't think this is the problem I'm having.

> Did you build your Squid normally, or with a specific regex library?

Normally, I guess.

# /usr/local/squid/sbin/squid -v
Squid Cache: Version 3.5.2
Service Name: squid
configure options:  '--enable-storeio=ufs' '--enable-linux-netfilter' '--with-openssl' --enable-ltdl-convenience

> 
> Is there another config line adding to wuau_path somewhere in your
> squid.conf?

No

> 
> Is Squid actually running with the squid.conf you think it is?
>  (sounds silly, but mistakes happen)

Yes

I also tried the same thing with http_access and that works as expected - *.psf files are allowed, non *.psf file are denied. I'm thinking bug at the point... I'll do some more testing and see if I can narrow it doen.

> Also, unrelated to this ... have you tried using cache_peer instead of
> URL-rewriting ?
> 
>  cache_peer server parent 8080 0
>  cache_peer_access server allow wuau_repo wuau_path
>  cache_peer_access server deny all
> 
> I assuming its listening on port 8080. It may need the "originserver"
> option as well.

apt-cacher expects a url such that if you wanted to get:

http://ftp.debian.org/debian/somefile

you would ask apt-cacher for:

http://server:3142/apt-cacher/ftp.debian.org/debian/somefile

And it knows about debian package lists and what it can throw out when etc, and if you specify ftp.debian.org and ftp.au.debian.org as apt sources, it's smart enough to know that a file cached from one can be used in the other. I'm probably pushing the friendship by using it to cache windows packages but so far it seems to do the job okay. Keeping the archive clean might be a problem though.

> apt-cacher should fetch from the Internet, not looping back through Squid.
>

Given that the server might also retrieve other urls, and all the proxying is transparent, the best I could come up with was to match the browser agent string from apt-cacher when it requests a url:

acl apt_cacher browser apt-cacher

and then exclude that from caching and rewriting.

Thanks

James

From squid3 at treenet.co.nz  Thu Mar 12 10:59:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Mar 2015 23:59:55 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <5501584B.1060402@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
Message-ID: <550171AB.9020601@treenet.co.nz>

If your intention here is to get around a broken firewall devices port
80/443 inspection then I expect you require two proxies anyway. The
traffic has to be on a different port entirely which is not being
mangled by the firewall.

One before and one after the firewall, with packets flowing over the
unusual port between them.

Otherwise the traffic will still be going through the firewall on port
80/443 regardless of the proxying.

The proxy closest to the clients should be doing the NAT intercept,
pasing traffic explicitly to the one outside, and the one outside being
a normal forward-proxy accepting traffic only from the internal one.




On 12/03/2015 10:11 p.m., Klavs Klavsen wrote:
> As i Understand intercept - that will only work (as you said) when NAT
> is performed on the box that is to intercept (when I remove haproxy -
> that means the squid box itself).
> 
> and I'm going to move the squid box to the same network as the
> webservers - to be able to do it the routing way.
> 
> It seems this config should then be applicable "When Squid is in a DMZ
> between the router and Internet" from
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> 
> But do I need to enable net.ipv4.ip_forward = 1 ?
> But perhaps that is needed, for iptables/linux kernel to even evaluate
> PREROUTING rules (which then dnat's the package into squid port 3129)?

Yes, in order to receive packets destined for machines other than itself
forwarding is required. Otherwise the TCP stack martian packet detection
kicks in and drops them as invalid.

> 
> I'd like to only allow packages out of the box, when coming from squid
> (in case of some config issue - I'd like the squid-server NOT to route
> stuff, which didn't go through squid program).
> 
> and then I figured I'd could ensure that from happening, by adding:
> iptables -t nat -A OUTPUT --match owner --uid-owner squid -p tcp --dport
> 80 -j ACCEPT
> iptables -t nat -A OUTPUT --match owner --uid-owner squid -p tcp --dport
> 80 -j ACCEPT
> 
> and then ofcourse allow "state RELATED,ESTABLISHED" and traffic I
> actually "want" and have default policy DROP.


I dont think the NAT table is the right place to be doing that.

You need the NAT table rules from here
<http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>

The mangle PREROUTING rule protects against external abuse of the Squid
port.
The nat PREROUNTING rules are to get packets into Squid (or bypass).

The nat POSTROUTING to ensure that all packets bypassed or Squid outputs
all come back through this machine.

To prevent packets going through the machine without going through Squid
add rules in the fiter table FORWARD chain.

Here is the netfilter packet flow diagram:
<http://inai.de/images/nf-packet-flow.svg>
(Squid would be the "local process" at the top center.)


> 
> Klavs Klavsen wrote on 03/12/2015 09:13 AM:
>> Hi Amos,
>>
>> Thank you for the walkthrough..
>>
>> Instead of having to play with tproxy on haproxy currently, I figured
>> i'd try a simpler route..
>>
>> The purpose of this setup, is to "jump around" a firewall issue with a
>> sh#! firewall, which in order to filter http and https traffic
>> appearently drops 5-15%.. and in 6 months noone has seem willing/able to
>> fix it :(
>>
>> I've put squid directly on the 10.43.18.181 (previously haproxy) ip -
>> and changed to: http_port 80 intercept
>>
>> I tried this on the client (being on other subnet):
>> iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination
>> 10.43.18.181:80
>>
>> and the traffic looped in exactly the same way :(

OUTPUT is catching all traffic leaving processes running on the Squid
machine, including from Squid itself.

Amos


From squid3 at treenet.co.nz  Thu Mar 12 11:13:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 00:13:31 +1300
Subject: [squid-users] keep data after delete swap.state
In-Reply-To: <1426152524357-4670351.post@n4.nabble.com>
References: <1426137330809-4670344.post@n4.nabble.com>
 <55012627.1030107@treenet.co.nz> <1426152524357-4670351.post@n4.nabble.com>
Message-ID: <550174DB.2040902@treenet.co.nz>

On 12/03/2015 10:28 p.m., HackXBack wrote:
> Hmmmm, Thanks Amos , 
> Then what [squid -F]  do then ?

It enforces Squid not being able to answer client requests during that
step #4 operation. Its a possibility to avoid manually directing traffic
at the proxy when step #4 is completed. Especially if you have some
external heartbeat or similar monitor making the decision whether Squid
gets traffic.

In your case with TB of cache past experience indicate that a full
rebuild may take some hours. Its sort of equivalent in operations to a
disk defrag. Squid with -F will still be accepting client connections,
just not answering them for that time. Which is unlikely to be desirable
if the full rebuild takes a long time and holding up real clients so I
didnt suggest it.

Amos



From kl at vsen.dk  Thu Mar 12 11:15:17 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 12:15:17 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <550171AB.9020601@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz>
Message-ID: <55017545.4060307@vsen.dk>

Amos Jeffries wrote on 03/12/2015 11:59 AM:
> If your intention here is to get around a broken firewall devices port
> 80/443 inspection then I expect you require two proxies anyway. The
> traffic has to be on a different port entirely which is not being
> mangled by the firewall.
>
I've gotten an OK - for them to not inspect traffic coming from the 
squid server - as long as I verify it's setup is secure and only allows 
traffic for webservers to certain urls. (it's not me they don't trust - 
it's the websites running on those servers :)

As far as I understand the routing example - it only works when I've 
setup a router (that bypasses the normal gateway, and acts as a router), 
or a setup where squid is on the same box.

What setup should I use to route packages from clients (when I want to 
capture/redirect on clients) to squid server on same LAN ?

the routing example didn't seem to work :(

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Thu Mar 12 11:19:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 00:19:39 +1300
Subject: [squid-users] Ipc::Mem::Segment::attach failed to
 mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
In-Reply-To: <1982154535.414582382.1426153290459.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1982154535.414582382.1426153290459.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5501764B.9050408@treenet.co.nz>

On 12/03/2015 10:41 p.m., FredB wrote:
> 
>>
>> mising whitespace separator between those options.
>>
>> '--enable-icap-client' '--enable-follow-x-forwarded-for'
>> '--enable-basic-auth-helpers=LDAP,digest'
>> '--enable-digest-auth-helpers=ldap,password'
>>
>> Syntax: --enable-auth-TYPE=HELPER,LIST
>>
>> If you dont use that syntax to explicitly limit the helpers built you
>> get the default list of: everything that will build.
>>
> 
> 
> Fixed thanks, but same problem
> 
> 
>> You may be missing a library, but I'm not aware what off the top of
>> my
>> head of any particular one.
>>
>> Or perhape its the recent issue of workers SUID permissions. I hope
>> Christos applies the final patch fixing that today, if not the one in
>> squid-dev is usable enough.
>>
> 
> Ok I will try, perhaps a specific problem with 32Bits OS ?

Maybe if you are allocating large cache_mem (I think page-pool.shm was
shared cache_mem). On 32-bit the 2(4?) GB RAM limit needs to cover
everything including virtual memory IIRC.

Amos



From kl at vsen.dk  Thu Mar 12 11:27:13 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 12:27:13 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55017545.4060307@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
Message-ID: <55017811.9060901@vsen.dk>

Klavs Klavsen wrote on 03/12/2015 12:15 PM:
>
> the routing example didn't seem to work :(
>
As I understand it.. I can't use DNAT on client machine to get packages 
to squid box.. and since it's locally generated packages(ie. I want to 
capture on the clients - instead of capturing on their default gateway), 
the packages only traverse POSTROUTING and OUTPUT..

any hints appreciated :)


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From james at ejbdigital.com.au  Thu Mar 12 11:30:29 2015
From: james at ejbdigital.com.au (James Harper)
Date: Thu, 12 Mar 2015 11:30:29 +0000
Subject: [squid-users] urlpath_regex
In-Reply-To: <HKNPR04MB1934E2EDFB3930798D5FB37E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55016928.4070900@treenet.co.nz>
 <HKNPR04MB1934E2EDFB3930798D5FB37E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <HKNPR04MB193129CC19282BA9D570D3EE8060@HKNPR04MB193.apcprd04.prod.outlook.com>

> 
> I also tried the same thing with http_access and that works as expected -
> *.psf files are allowed, non *.psf file are denied. I'm thinking bug at the
> point... I'll do some more testing and see if I can narrow it doen.
> 

Found it. Really stupid mistake. The documentation shows [-i] for case insensitivity, but I hadn't picked up that the [] around the -i indicated that it was optional. I had just cut and pasted from examples. So the .cab thing was irrelevant - it just happened that the .cab files had an "i" in them somewhere.

Thanks for your input!

James




From fredbmail at free.fr  Thu Mar 12 11:36:16 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 12 Mar 2015 12:36:16 +0100 (CET)
Subject: [squid-users] Ipc::Mem::Segment::attach failed to
 mmap(/squid-squid-page-pool.shm): (12) Cannot allocate memory
In-Reply-To: <5501764B.9050408@treenet.co.nz>
Message-ID: <1304216905.414811455.1426160176434.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Maybe if you are allocating large cache_mem (I think page-pool.shm
> was
> shared cache_mem). On 32-bit the 2(4?) GB RAM limit needs to cover
> everything including virtual memory IIRC.
> 
> Amos

Yes you are right it's good now ! 


From squid3 at treenet.co.nz  Thu Mar 12 11:42:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 00:42:40 +1300
Subject: [squid-users] urlpath_regex
In-Reply-To: <HKNPR04MB193129CC19282BA9D570D3EE8060@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55016928.4070900@treenet.co.nz>
 <HKNPR04MB1934E2EDFB3930798D5FB37E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <HKNPR04MB193129CC19282BA9D570D3EE8060@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <55017BB0.4000708@treenet.co.nz>

On 13/03/2015 12:30 a.m., James Harper wrote:
>>
>> I also tried the same thing with http_access and that works as expected -
>> *.psf files are allowed, non *.psf file are denied. I'm thinking bug at the
>> point... I'll do some more testing and see if I can narrow it doen.
>>
> 
> Found it. Really stupid mistake. The documentation shows [-i] for
> case insensitivity, but I hadn't picked up that the [] around the -i
> indicated that it was optional. I had just cut and pasted from
> examples. So the .cab thing was irrelevant - it just happened that
> the .cab files had an "i" in them somewhere.
> 

Ah. That is a bug then. The -i bit is not supposed to be treated as a
pattern.

Thanks.

Amos



From james at ejbdigital.com.au  Thu Mar 12 11:46:36 2015
From: james at ejbdigital.com.au (James Harper)
Date: Thu, 12 Mar 2015 11:46:36 +0000
Subject: [squid-users] urlpath_regex
In-Reply-To: <55017BB0.4000708@treenet.co.nz>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55016928.4070900@treenet.co.nz>
 <HKNPR04MB1934E2EDFB3930798D5FB37E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <HKNPR04MB193129CC19282BA9D570D3EE8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55017BB0.4000708@treenet.co.nz>
Message-ID: <HKNPR04MB1938977C874A16B7D724500E8060@HKNPR04MB193.apcprd04.prod.outlook.com>

> >
> > Found it. Really stupid mistake. The documentation shows [-i] for
> > case insensitivity, but I hadn't picked up that the [] around the -i
> > indicated that it was optional. I had just cut and pasted from
> > examples. So the .cab thing was irrelevant - it just happened that
> > the .cab files had an "i" in them somewhere.
> >
> 
> Ah. That is a bug then. The -i bit is not supposed to be treated as a
> pattern.
> 

Even when I put it in []'s? I think the mistake was mine.

James

From Antony.Stone at squid.open.source.it  Thu Mar 12 11:53:04 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 12 Mar 2015 12:53:04 +0100
Subject: [squid-users] urlpath_regex
In-Reply-To: <HKNPR04MB1938977C874A16B7D724500E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <HKNPR04MB193735C575E1581AB2290C6E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
 <55017BB0.4000708@treenet.co.nz>
 <HKNPR04MB1938977C874A16B7D724500E8060@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <201503121253.05062.Antony.Stone@squid.open.source.it>

On Thursday 12 March 2015 at 12:46:36 (EU time), James Harper wrote:

> > Ah. That is a bug then. The -i bit is not supposed to be treated as a
> > pattern.
> 
> Even when I put it in []'s? I think the mistake was mine.

There was no [] in your original posting of your conf file...

On Thursday 12 March 2015 at 09:14:49 (EU time), James Harper wrote:

> I have just noticed that urlpath_regex isn't doing what I want:
> 
> acl wuau_repo dstdomain .download.windowsupdate.com
> acl wuau_path urlpath_regex -i \.psf$
> acl dst_server dstdomain server
> acl apt_cacher browser apt-cacher


Regards,


Antony.

-- 
Anything that improbable is effectively impossible.

 - Murray Gell-Mann, Nobel Prizewinner in Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Mar 12 12:05:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 01:05:55 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55017811.9060901@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk>
Message-ID: <55018123.3010607@treenet.co.nz>

On 13/03/2015 12:27 a.m., Klavs Klavsen wrote:
> Klavs Klavsen wrote on 03/12/2015 12:15 PM:
>>
>> the routing example didn't seem to work :(
>>
> As I understand it.. I can't use DNAT on client machine to get packages
> to squid box.. and since it's locally generated packages(ie. I want to
> capture on the clients - instead of capturing on their default gateway),
> the packages only traverse POSTROUTING and OUTPUT..
> 
> any hints appreciated :)
> 

You can either, set the clients default gateway to be the Squid machine
which just forwards non-HTTP packets on to the actual gateway router
which is set as Squid machines default gateway.

Or, add policy routing into the gateway router diverting just the port
80 traffic from the real clients (but excluding the Squid machine) to
the Squid machine as its upstream router.

In both those cases both the normal gateway and the Squid machine are
configured as routers with the Squid machine using the real gateway as
its default gateway.

Or, you can run Squid on the main gateway router - provided it has
enough memory for what you want it doing.

You can also physically plug the Squid machine into the network path as
a router before the main gateway router. This is same as the first
option but hard-wired as well as configured.


Capture wont work on client devices because Squid cant make system calls
directly into their remote machines kernel / NAT driver. You end up with
wrong IPs know to Squid and those loops.

So, pick one of the two above options and lets see why the routing is
"not working".


Amos



From hack.back at hotmail.com  Thu Mar 12 12:27:13 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 12 Mar 2015 05:27:13 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1425980371350-4670291.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com> <54FEABB1.20402@treenet.co.nz>
 <1425980371350-4670291.post@n4.nabble.com>
Message-ID: <1426163233203-4670366.post@n4.nabble.com>

how to  rebuilt without symbol stripping  ????????????



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271p4670366.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From kl at vsen.dk  Thu Mar 12 12:52:32 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 13:52:32 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55018123.3010607@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
Message-ID: <55018C10.20303@vsen.dk>

I'd rather not have to route everything (incl. normal ingoing web 
traffic) through the squid box.. and the firewalls are proprietary stuff 
- so can't install squid there :)

It works fine in accel mode.. and I can limit what urls each client ip 
is able to access, and disable caching..

Shouldn't accel mode, for this use case (curl access from websites - all 
using http/1.1 with host header) be good enough - or are there security 
issues I am not aware of?

I realize I move the DNS lookup to the squid box - but that's actually 
what I want in this case.

Amos Jeffries wrote on 03/12/2015 01:05 PM:
> On 13/03/2015 12:27 a.m., Klavs Klavsen wrote:
>> Klavs Klavsen wrote on 03/12/2015 12:15 PM:
>>>
>>> the routing example didn't seem to work :(
>>>
>> As I understand it.. I can't use DNAT on client machine to get packages
>> to squid box.. and since it's locally generated packages(ie. I want to
>> capture on the clients - instead of capturing on their default gateway),
>> the packages only traverse POSTROUTING and OUTPUT..
>>
>> any hints appreciated :)
>>
>
> You can either, set the clients default gateway to be the Squid machine
> which just forwards non-HTTP packets on to the actual gateway router
> which is set as Squid machines default gateway.
>
> Or, add policy routing into the gateway router diverting just the port
> 80 traffic from the real clients (but excluding the Squid machine) to
> the Squid machine as its upstream router.
>
> In both those cases both the normal gateway and the Squid machine are
> configured as routers with the Squid machine using the real gateway as
> its default gateway.
>
> Or, you can run Squid on the main gateway router - provided it has
> enough memory for what you want it doing.
>
> You can also physically plug the Squid machine into the network path as
> a router before the main gateway router. This is same as the first
> option but hard-wired as well as configured.
>
>
> Capture wont work on client devices because Squid cant make system calls
> directly into their remote machines kernel / NAT driver. You end up with
> wrong IPs know to Squid and those loops.
>
> So, pick one of the two above options and lets see why the routing is
> "not working".
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From squid3 at treenet.co.nz  Thu Mar 12 12:57:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 01:57:27 +1300
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1425980371350-4670291.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com> <54FEABB1.20402@treenet.co.nz>
 <1425980371350-4670291.post@n4.nabble.com>
Message-ID: <55018D37.3050502@treenet.co.nz>

On 10/03/2015 10:39 p.m., HackXBack wrote:
> this is my configure option , what may cause the problem
> 
> ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin
> --libexecdir=/usr/lib/squid --sysconfdir=/etc/squid --localstatedir=/var
> --libdir=/usr/lib --includedir=/usr/include --datadir=/usr/share/squid
> --infodir=/usr/share/info --mandir=/usr/share/man
> --disable-dependency-tracking --disable-strict-error-checking
> --enable-async-io=32 --with-aufs-threads=32 --with-pthreads
> --enable-storeio=ufs,aufs,diskd --enable-removal-policies=lru,heap
> --with-aio --with-dl --disable-icmp --enable-esi --enable-icap-client
> --disable-wccp --enable-wccpv2 --enable-kill-parent-hack
> --enable-cache-digests --disable-select --enable-http-violations
> --enable-linux-netfilter --enable-follow-x-forwarded-for
> --disable-ident-lookups --enable-x-accelerator-vary --enable-zph-qos
> --with-default-user=proxy --with-logdir=/var/log/squid
> --with-pidfile=/var/run/squid.pid --with-swapdir=/var/spool/squid
> --with-large-files --enable-ltdl-convenience --with-filedescriptors=100000
> --enable-ssl --enable-ssl-crtd --with-openssl --disable-auth --disable-ipv6
> --enable-build-info="build by : ANDO_TBLRB && HackXBack"
> --with-included-ltdl --enable-arp-acl --enable-epoll --enable-snmp
> --enable-referer-log --disable-unlinkd --enable-truncate
> --enable-useragent-log --enable-eui --enable-large-cache-files
> --with-maxfd=65536 CFLAGS="-Wall -g -O3 -march=native -mtune=native -pipe
> -DNUMTHREADS=60 -fomit-frame-pointer -fno-strict-aliasing -funroll-loops
> -ffast-math -fno-exceptions" LDFLAGS="-Wl,-Bsymbolic-functions"

Remove the -O3 or at least reduce it to -O1 and the -g should make the
symbols.

Its possible the rest fo the toolchain is removing them smoehow but
thats not clear from these options.

Note also that CFLAGS is only used for some old C libraries we have to
build. You are not defining any options for CXXFLAGS which is what the
C++ compiler uses to build Squid, so the default options are used.
 squid -v output will show you what actually got used.

Amos



From squid3 at treenet.co.nz  Thu Mar 12 13:27:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 02:27:35 +1300
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55018C10.20303@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
 <55018C10.20303@vsen.dk>
Message-ID: <55019447.5000304@treenet.co.nz>

On 13/03/2015 1:52 a.m., Klavs Klavsen wrote:
> I'd rather not have to route everything (incl. normal ingoing web
> traffic) through the squid box.. and the firewalls are proprietary stuff
> - so can't install squid there :)

You don't, port 80 TCP is all that *needs* it, and only for the traffic
from clients you want to go through Squid.

If you are passing outgoing web traffic through Squid the responses
(incoming) have to come back through it.

If you have external stuff making requests to internal servers, that can
be left alone in the same way Squid' outgoing traffic is.

Are we talking more or less than 100Mbps of port 80 traffic here?


> 
> It works fine in accel mode.. and I can limit what urls each client ip
> is able to access, and disable caching..
> 
> Shouldn't accel mode, for this use case (curl access from websites - all
> using http/1.1 with host header) be good enough - or are there security
> issues I am not aware of?

You guessed it. CVE-2009-0801 - the Host header is not trustworthy.
accel/reverse-proxy mode has no protection at all since the upstream
servers are expected to be explicitly configured or the allowed domains
restricted to those hosted by the CDN the proxy is part of.

... and the Host header is not always present, though that case has
declined a lot in the past few years.


> 
> I realize I move the DNS lookup to the squid box - but that's actually
> what I want in this case.

Actually you will need two DN lookups to be happening if you use accel.
Only the intercept mode with NAT lookups has ability to avoid the second
one by using ORIGINAL_DST.

accel mode normaly avoids the second DNS lookup by having the upstream
servers explicitly configured. You dont want to do that manually for
every Internet server in existence so forcing a DNS lookup with
"always_direct allow all" is required.


Routings your friend, really :-)

Amos



From kl at vsen.dk  Thu Mar 12 13:37:53 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 14:37:53 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55019447.5000304@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
 <55018C10.20303@vsen.dk> <55019447.5000304@treenet.co.nz>
Message-ID: <550196B1.7020809@vsen.dk>

Amos Jeffries wrote on 03/12/2015 02:27 PM:
> On 13/03/2015 1:52 a.m., Klavs Klavsen wrote:
>> I'd rather not have to route everything (incl. normal ingoing web
>> traffic) through the squid box.. and the firewalls are proprietary stuff
>> - so can't install squid there :)
>
> You don't, port 80 TCP is all that *needs* it, and only for the traffic
> from clients you want to go through Squid.
>
so you're saying that I should set default gateway to point to squid - 
and then setup routes to the firewall for traffic I don't want to go 
through squid (internal stuff.. dns, rpm mirror etc.)

meaning that all traffic (not just port 80) to public internet adresses 
will go through squid. Since I have haproxy in front of the webservers - 
they'll respond to haproxy directly, and traffic to websites on the 
webservers won't go through squid.

> If you are passing outgoing web traffic through Squid the responses
> (incoming) have to come back through it.
>
can't I just masquerade/dnat outgoing traffic from squid server - so 
firewall will route response to it?

> If you have external stuff making requests to internal servers, that can
> be left alone in the same way Squid' outgoing traffic is.
>
> Are we talking more or less than 100Mbps of port 80 traffic here?
>
far far less :)

it's just a few api calls to facebook etc.

-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From kl at vsen.dk  Thu Mar 12 14:48:45 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 15:48:45 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <55019447.5000304@treenet.co.nz>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
 <55018C10.20303@vsen.dk> <55019447.5000304@treenet.co.nz>
Message-ID: <5501A74D.60205@vsen.dk>

I just found the config, stating that ssl-bump is only supported in 
intercept mode.. that invalides accel :)

I setup a client on same LAN as squid, and told it to use squid box as 
default gw. for traffic to public addresses..

intercept on port 80 works fine.

on https however I get an SSL connect error.

This is my config related to that:
sslcrtd_program                /usr/lib64/squid/ssl_crtd -s 
/etc/ssl/certs/cache/ -M 4MB
sslcrtd_children               8 startup=1 idle=1
https_port                     3130 intercept ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
key=/etc/squid/ca.private cert=/etc/squid/ca.cert
sslproxy_flags                 DONT_VERIFY_PEER
always_direct                  allow all
http_port                      3129 intercept
shutdown_lifetime              3
sslproxy_cert_error            allow all
ssl_bump                       server-first all

I'm running squid-3.4.9. (I can easily upgrade to newer if that will 
help any :) - on centos 7.0.

What debug options should/could I set to hopefully enlighten me? squid 
logs nothing in cache.log or access.log except:
1426171540.277      0 10.43.18.168 TAG_NONE/400 4047 NONE 
error:invalid-request - HIER_NONE/- text/html


Amos Jeffries wrote on 03/12/2015 02:27 PM:
> On 13/03/2015 1:52 a.m., Klavs Klavsen wrote:
>> I'd rather not have to route everything (incl. normal ingoing web
>> traffic) through the squid box.. and the firewalls are proprietary stuff
>> - so can't install squid there :)
>
> You don't, port 80 TCP is all that *needs* it, and only for the traffic
> from clients you want to go through Squid.
>
> If you are passing outgoing web traffic through Squid the responses
> (incoming) have to come back through it.
>
> If you have external stuff making requests to internal servers, that can
> be left alone in the same way Squid' outgoing traffic is.
>
> Are we talking more or less than 100Mbps of port 80 traffic here?
>
>
>>
>> It works fine in accel mode.. and I can limit what urls each client ip
>> is able to access, and disable caching..
>>
>> Shouldn't accel mode, for this use case (curl access from websites - all
>> using http/1.1 with host header) be good enough - or are there security
>> issues I am not aware of?
>
> You guessed it. CVE-2009-0801 - the Host header is not trustworthy.
> accel/reverse-proxy mode has no protection at all since the upstream
> servers are expected to be explicitly configured or the allowed domains
> restricted to those hosted by the CDN the proxy is part of.
>
> ... and the Host header is not always present, though that case has
> declined a lot in the past few years.
>
>
>>
>> I realize I move the DNS lookup to the squid box - but that's actually
>> what I want in this case.
>
> Actually you will need two DN lookups to be happening if you use accel.
> Only the intercept mode with NAT lookups has ability to avoid the second
> one by using ORIGINAL_DST.
>
> accel mode normaly avoids the second DNS lookup by having the upstream
> servers explicitly configured. You dont want to do that manually for
> every Internet server in existence so forcing a DNS lookup with
> "always_direct allow all" is required.
>
>
> Routings your friend, really :-)
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From kl at vsen.dk  Thu Mar 12 14:53:22 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 15:53:22 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <5501A74D.60205@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
 <55018C10.20303@vsen.dk> <55019447.5000304@treenet.co.nz>
 <5501A74D.60205@vsen.dk>
Message-ID: <5501A862.2090705@vsen.dk>

I think I found it..

trying to run ssl_crtd myself to issue a cert it says:
  Error while parsing the crtd request: Broken signing certificate!

shouldn't that end up in squid logs as well?

Klavs Klavsen wrote on 03/12/2015 03:48 PM:
> I just found the config, stating that ssl-bump is only supported in
> intercept mode.. that invalides accel :)
>
> I setup a client on same LAN as squid, and told it to use squid box as
> default gw. for traffic to public addresses..
>
> intercept on port 80 works fine.
>
> on https however I get an SSL connect error.
>
> This is my config related to that:
> sslcrtd_program                /usr/lib64/squid/ssl_crtd -s
> /etc/ssl/certs/cache/ -M 4MB
> sslcrtd_children               8 startup=1 idle=1
> https_port                     3130 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> key=/etc/squid/ca.private cert=/etc/squid/ca.cert
> sslproxy_flags                 DONT_VERIFY_PEER
> always_direct                  allow all
> http_port                      3129 intercept
> shutdown_lifetime              3
> sslproxy_cert_error            allow all
> ssl_bump                       server-first all
>
> I'm running squid-3.4.9. (I can easily upgrade to newer if that will
> help any :) - on centos 7.0.
>
> What debug options should/could I set to hopefully enlighten me? squid
> logs nothing in cache.log or access.log except:
> 1426171540.277      0 10.43.18.168 TAG_NONE/400 4047 NONE
> error:invalid-request - HIER_NONE/- text/html
>
>
> Amos Jeffries wrote on 03/12/2015 02:27 PM:
>> On 13/03/2015 1:52 a.m., Klavs Klavsen wrote:
>>> I'd rather not have to route everything (incl. normal ingoing web
>>> traffic) through the squid box.. and the firewalls are proprietary stuff
>>> - so can't install squid there :)
>>
>> You don't, port 80 TCP is all that *needs* it, and only for the traffic
>> from clients you want to go through Squid.
>>
>> If you are passing outgoing web traffic through Squid the responses
>> (incoming) have to come back through it.
>>
>> If you have external stuff making requests to internal servers, that can
>> be left alone in the same way Squid' outgoing traffic is.
>>
>> Are we talking more or less than 100Mbps of port 80 traffic here?
>>
>>
>>>
>>> It works fine in accel mode.. and I can limit what urls each client ip
>>> is able to access, and disable caching..
>>>
>>> Shouldn't accel mode, for this use case (curl access from websites - all
>>> using http/1.1 with host header) be good enough - or are there security
>>> issues I am not aware of?
>>
>> You guessed it. CVE-2009-0801 - the Host header is not trustworthy.
>> accel/reverse-proxy mode has no protection at all since the upstream
>> servers are expected to be explicitly configured or the allowed domains
>> restricted to those hosted by the CDN the proxy is part of.
>>
>> ... and the Host header is not always present, though that case has
>> declined a lot in the past few years.
>>
>>
>>>
>>> I realize I move the DNS lookup to the squid box - but that's actually
>>> what I want in this case.
>>
>> Actually you will need two DN lookups to be happening if you use accel.
>> Only the intercept mode with NAT lookups has ability to avoid the second
>> one by using ORIGINAL_DST.
>>
>> accel mode normaly avoids the second DNS lookup by having the upstream
>> servers explicitly configured. You dont want to do that manually for
>> every Internet server in existence so forcing a DNS lookup with
>> "always_direct allow all" is required.
>>
>>
>> Routings your friend, really :-)
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>


-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
   --Henry Spencer



From mukulg at gwmail.gwu.edu  Thu Mar 12 15:01:51 2015
From: mukulg at gwmail.gwu.edu (Mukul Gandhi)
Date: Thu, 12 Mar 2015 11:01:51 -0400
Subject: [squid-users] ssl_bump for specific dstdomain
Message-ID: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>

I am running squid 3.4.8 and am looking for solutions to ssl_bump for
specific domains only. Going through the archives it is clear that it is
not possible unless the reverse DNS points back to the domain that is to be
ssl bumped.

So then what is the solution to this problem. I just want to create a SSL
whitelist of domains that are to be bumped and the rest should be tunneled
through. What I have is -

ssl_bump none localhost
acl ssl_whitelist dstdomain "/tmp/ssl_whitelist.txt"
ssl_bump server-first ssl_whitelist

The file /tmp/ssl_whitelist.txt contains -

.facebook.com
.twitter.com
.pintrest.com

Of course, this doesn't work because the ip address for these websites
points back to <something>.akamaitechnologies.com.

All I want is to be able to decrypt just the traffic to these three
web-sites, the rest should go through encrypted. But I couldn't find a
solution for this anywhere in the archives. I did see some mention of using
SslBump1/2/3 but it wasn't clear if this was the silver bullet. Also I
would have to upgrade to 3.5 to use these new directives.

Any idea how I can achieve this in 3.4.8 (if possible)? Or if I a solution
exists for this in 3.5?

Thanks,
-Mukul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150312/d743a42b/attachment.htm>

From yvoinov at gmail.com  Thu Mar 12 15:04:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 12 Mar 2015 21:04:22 +0600
Subject: [squid-users] ssl_bump for specific dstdomain
In-Reply-To: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
References: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
Message-ID: <5501AAF6.6040609@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

You only have external helper (which is must wrote yourself) in 3.4.x.

Works with domains in ssl bump fully available at least 3.5.x

12.03.15 21:01, Mukul Gandhi ?????:
> I am running squid 3.4.8 and am looking for solutions to ssl_bump
> for specific domains only. Going through the archives it is clear
> that it is not possible unless the reverse DNS points back to the
> domain that is to be ssl bumped.
> 
> So then what is the solution to this problem. I just want to create
> a SSL whitelist of domains that are to be bumped and the rest
> should be tunneled through. What I have is -
> 
> ssl_bump none localhost acl ssl_whitelist dstdomain
> "/tmp/ssl_whitelist.txt" ssl_bump server-first ssl_whitelist
> 
> The file /tmp/ssl_whitelist.txt contains -
> 
> .facebook.com .twitter.com .pintrest.com
> 
> Of course, this doesn't work because the ip address for these
> websites points back to <something>.akamaitechnologies.com.
> 
> All I want is to be able to decrypt just the traffic to these
> three web-sites, the rest should go through encrypted. But I
> couldn't find a solution for this anywhere in the archives. I did
> see some mention of using SslBump1/2/3 but it wasn't clear if this
> was the silver bullet. Also I would have to upgrade to 3.5 to use
> these new directives.
> 
> Any idea how I can achieve this in 3.4.8 (if possible)? Or if I a
> solution exists for this in 3.5?
> 
> Thanks, -Mukul
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVAar2AAoJENNXIZxhPexGm5MH/0JUWgIjDrNb8+a0b66iyY+x
uWgoNnGqBKL/gzQt3AmKv3P31/3Vc8wCpMlSd3HpOSeyOtJ4pYAqI3kw1o91kkEK
YJ1wGc4FN+8sxUplA9+Kz/XDxpxTFAvS4/9d5AUOmxCoi2PmIhThozl8X8fIMdv/
7shy+Ce9kKj/ozSievVaePxdH+OUd0fmdKtDrv1aenxQpclaZSkuwEflQ3idTYBu
zTpNP3AvEP4+32yb2W+mP4p1JgHwUAi60hEz3kP9pxd+Ym2kuZeFDF5ZV2x2/cKQ
iRpmS++2kOt0nIT074PhV8dzPfD1lZt7atQT+mBJhLvzlD5Sxvxqll7Z/dpQSSI=
=P+8j
-----END PGP SIGNATURE-----


From dweimer at dweimer.net  Thu Mar 12 15:31:01 2015
From: dweimer at dweimer.net (dweimer)
Date: Thu, 12 Mar 2015 10:31:01 -0500
Subject: [squid-users] Reverse Proxy Funny Logging Issue
In-Reply-To: <5100BB15.3030502@treenet.co.nz>
References: <0113b4b74c8c9e0a05cd692641aecf25@dweimer.net>
 <50FF7561.6050301@treenet.co.nz>
 <88bdb4a5d0b47bb9e459b91b079b7425@dweimer.net>
 <8a05b3c589b46021b42d5a59a7b11e8a@dweimer.net>
 <5100BB15.3030502@treenet.co.nz>
Message-ID: <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>

On 01/23/2013 10:39 pm, Amos Jeffries wrote:
> On 24/01/2013 4:13 a.m., dweimer wrote:
>> On 2013-01-23 08:40, dweimer wrote:
>>> On 2013-01-22 23:30, Amos Jeffries wrote:
>>>> On 23/01/2013 5:34 a.m., dweimer wrote:
>>>>> I just upgraded my reverse proxy server last night from 3.1.20 to 
>>>>> 3.2.6, all is working well except one of my log rules, and I can't 
>>>>> figure out why.
>>>> 
>>>> Please run "squid -k parse" and resolve the WARNING or ERROR which 
>>>> are listed.
>>>> 
>>>> There are two possible reasons...
>>>> 
>>>>> 
>>>>> I have a several sites behind the server, with dstdomain access 
>>>>> rules setup.
>>>>> 
>>>>> acl website1 dstdomain www.website1.com
>>>>> acl website2 dstdomain www.website2.com
>>>>> acl website2 dstdomain www.website3.com
>>>> 
>>>> Possible reason #1 (assuming thi is an accurate copy-n-paste from
>>>> yoru config file).....  you have no website3 ACL definition?
>>> 
>>> That was a typo in the email, correct ACL is in the configuration,
>>> squid -k parse outputs no warnings or errors.
>>> 
>>>> 
>>>>> ...
>>>>> 
>>>>> Followed by the access rules
>>>>> 
>>>>> http_access allow website1
>>>>> http_access allow website2
>>>>> http_access allow website3
>>>>> ...
>>>>> http_access deny all
>>>>> 
>>>>> Some are using rewrites
>>>>> url_rewrite_program /usr/local/etc/squid/url_rewrite.py
>>>>> url_rewrite_children 20
>>>>> url_rewrite_access allow website1
>>>>> url_rewrite_access allow website3
>>>>> ...
>>>>> url_rewrite_access deny all
>>>>> 
>>>>> Then my access logs
>>>>> 
>>>>> # First I grab everything in one
>>>>> access_log daemon:/var/log/squid/access.log squid all
>> 
>>>> 
>>>>> access_log daemon:/var/log/squid/website1.log combined website1
>>>>> access_log daemon:/var/log/squid/website2.log combined website2
>>>>> access_log daemon:/var/log/squid/website3.log combined website3
>>>>> ...
>>>>> 
>>>>> everything works, write down to one of the access logs, the data 
>>>>> shows up in the access.log file, the data shows up in the 
>>>>> individual logs for all the others, except that one.  If we use 
>>>>> website3 from the above example like my actual file the access rule 
>>>>> works on the url_rewrite_access allow line, but for some reason is 
>>>>> failing on the log line.  squid -k parse doesn't show any errors, 
>>>>> and shows a Processing: access_log 
>>>>> daemon:/var/log/squid/website3.log combined website3 line in the 
>>>>> output.
>>>>> 
>>>>> The log in question was originally at the end of my access_log list 
>>>>> section, so I changed the order around to see if for some reason it 
>>>>> was only the last one not working, no change still only that one 
>>>>> not working, And the new last one in the list still works as 
>>>>> expected.
>>>>> 
>>>>> I know the ACL is working as it works correctly on the rewrite rule 
>>>>> and the http access just above the log rules, anyone have any ideas 
>>>>> on how I can figure out why the log entry isn't working?
>>> 
>> 
>> Changed lines back to daemon, changed acl on logs to the rewrite side 
>> used on the cache_peer_access lines later in the configuration.  Works 
>> now, and logs even show up with the pre-rewrite rule host 
>> information...
>> 
>> That does make me wonder why some lines were getting logged but not 
>> all, the sites I thought were working do have higher usage, maybe I 
>> was still missing a lot from them, and just not knowing it.  I guess I 
>> will see if my webalizer reports show a huge gain in hit count over 
>> the old records from the the 3.1.20 installation, of if this behavior 
>> is only evident in the 3.2 branch.
>> 
> 
> I think you will find that the lines being logged previously were on
> the requests which were either not rewritten at all or were re-written
> from another requests URL which was being logged.
> 
> Each of the ACL-driven directive labels in squid.conf is effectively
> an event trigger script - deciding whether or not to perform some
> action. This only makes sense testing when that action choice is
> requried.  Squid processing pathway checks http_access first, ... then
> some others, ... then url_rewriting, ... then the destination
> selection (cache_peer and others), ... then when the transaction is
> fully completed access_log output decision are done.
> 
> Amos

Last night I applied the FreeBSD 10.1-RELEASE-p6 Update and Upgraded the 
ports which included Squid 3.4.12, I enabled the LAX HTTP option in the 
ports configuration with adds the --enable-http-violations compile 
option. With the intention to enable broken_posts option in the 
configuration. I will hopefully be able to apply any necessary changes 
to the production system after I test them now.
When doing this update I did have a thought the system is running in a 
FreeBSD jail and not on the base system is there a chance this issue is 
caused by running within a jail? curious if anyone has ran into specific 
issues running Squid in a FreeBSD jail before?

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From eliezer at ngtech.co.il  Thu Mar 12 17:02:13 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 12 Mar 2015 19:02:13 +0200
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <1426137244594-4670343.post@n4.nabble.com>
References: <5500F0E8.1010405@ngtech.co.il>
 <1426137244594-4670343.post@n4.nabble.com>
Message-ID: <5501C695.7000407@ngtech.co.il>

Hey Hack,

I wsa talking about radius server like free radius.
Which by the way dmasoftlab uses in their product\s.

Eliezer

On 12/03/2015 07:14, HackXBack wrote:
> are you talking about radius server like free radius ?
> or like dmasoftlab.com ?
>



From kl at vsen.dk  Thu Mar 12 19:18:55 2015
From: kl at vsen.dk (Klavs Klavsen)
Date: Thu, 12 Mar 2015 20:18:55 +0100
Subject: [squid-users] squid "internal?" loop - with no firewall nat
 going on..?
In-Reply-To: <5501A74D.60205@vsen.dk>
References: <54FEE39B.6050908@vsen.dk> <54FEF62D.2080403@treenet.co.nz>
 <54FEFB0A.8010900@vsen.dk>
 <201503101518.48686.Antony.Stone@squid.open.source.it>
 <54FF0903.6000404@vsen.dk> <54FF1FBF.6000605@treenet.co.nz>
 <55014A98.8030705@vsen.dk> <5501584B.1060402@vsen.dk>
 <550171AB.9020601@treenet.co.nz> <55017545.4060307@vsen.dk>
 <55017811.9060901@vsen.dk> <55018123.3010607@treenet.co.nz>
 <55018C10.20303@vsen.dk> <55019447.5000304@treenet.co.nz>
 <5501A74D.60205@vsen.dk>
Message-ID: <5501E69F.3090802@vsen.dk>

dooh. Solved. iptables forwarded https traffic to 3129 - not 3130.. :)

Thank you for your kind assistance

[CUT]
-- 
Regards,
Klavs Klavsen, GSEC - kl at vsen.dk - http://www.vsen.dk - Tlf. 61281200

"Those who do not understand Unix are condemned to reinvent it, poorly."
  --Henry Spencer



From mukulg at gwmail.gwu.edu  Thu Mar 12 20:37:44 2015
From: mukulg at gwmail.gwu.edu (Mukul Gandhi)
Date: Thu, 12 Mar 2015 16:37:44 -0400
Subject: [squid-users] ssl_bump for specific dstdomain
In-Reply-To: <5501AAF6.6040609@gmail.com>
References: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
 <5501AAF6.6040609@gmail.com>
Message-ID: <CAJUmE_PVfmsVA5Ykd2Gk6PULLEsY2jzF5e0VquZWKeAfK-W3uQ@mail.gmail.com>

On Thu, Mar 12, 2015 at 11:04 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> You only have external helper (which is must wrote yourself) in 3.4.x.
>
>
Are there any examples that I can look at to implemented this external
helper for doing selective ssl_bumps. And what would this helper script do
anyways? All we have is the destination IP address which is not really
going to give us the actual HTTP hostname.



> Works with domains in ssl bump fully available at least 3.5.x
>

Does the 3.5.x implementation decrypt the whole payload and then do the
ssl_bump? The "peek" option seems to imply that only the HTTP headers are
peeked at.

I guess what I am asking is, is there any way we can do this without
actually decrypting the payload?


>
> 12.03.15 21:01, Mukul Gandhi ?????:
> > I am running squid 3.4.8 and am looking for solutions to ssl_bump
> > for specific domains only. Going through the archives it is clear
> > that it is not possible unless the reverse DNS points back to the
> > domain that is to be ssl bumped.
> >
> > So then what is the solution to this problem. I just want to create
> > a SSL whitelist of domains that are to be bumped and the rest
> > should be tunneled through. What I have is -
> >
> > ssl_bump none localhost acl ssl_whitelist dstdomain
> > "/tmp/ssl_whitelist.txt" ssl_bump server-first ssl_whitelist
> >
> > The file /tmp/ssl_whitelist.txt contains -
> >
> > .facebook.com .twitter.com .pintrest.com
> >
> > Of course, this doesn't work because the ip address for these
> > websites points back to <something>.akamaitechnologies.com.
> >
> > All I want is to be able to decrypt just the traffic to these
> > three web-sites, the rest should go through encrypted. But I
> > couldn't find a solution for this anywhere in the archives. I did
> > see some mention of using SslBump1/2/3 but it wasn't clear if this
> > was the silver bullet. Also I would have to upgrade to 3.5 to use
> > these new directives.
> >
> > Any idea how I can achieve this in 3.4.8 (if possible)? Or if I a
> > solution exists for this in 3.5?
> >
> > Thanks, -Mukul
> >
> >
> >
> > _______________________________________________ squid-users mailing
> > list squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJVAar2AAoJENNXIZxhPexGm5MH/0JUWgIjDrNb8+a0b66iyY+x
> uWgoNnGqBKL/gzQt3AmKv3P31/3Vc8wCpMlSd3HpOSeyOtJ4pYAqI3kw1o91kkEK
> YJ1wGc4FN+8sxUplA9+Kz/XDxpxTFAvS4/9d5AUOmxCoi2PmIhThozl8X8fIMdv/
> 7shy+Ce9kKj/ozSievVaePxdH+OUd0fmdKtDrv1aenxQpclaZSkuwEflQ3idTYBu
> zTpNP3AvEP4+32yb2W+mP4p1JgHwUAi60hEz3kP9pxd+Ym2kuZeFDF5ZV2x2/cKQ
> iRpmS++2kOt0nIT074PhV8dzPfD1lZt7atQT+mBJhLvzlD5Sxvxqll7Z/dpQSSI=
> =P+8j
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150312/58bc188a/attachment.htm>

From huaraz at moeller.plus.com  Thu Mar 12 20:43:18 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Thu, 12 Mar 2015 20:43:18 -0000
Subject: [squid-users] negotiate_wrapper: fgets() failed! dying..
In-Reply-To: <CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q@mail.gmail.com>
References: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>
 <CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q@mail.gmail.com>
Message-ID: <mdstpc$h8v$1@ger.gmane.org>

Do you get any more details when you start the wrapper with ?d ? 

Markus

"Donny Vibianto" <donny.vibianto at gmail.com> wrote in message news:CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q at mail.gmail.com...
anyone please...? 

On Sat, Mar 7, 2015 at 10:02 PM, Donny Vibianto <donny.vibianto at gmail.com> wrote:

  Hi Guys,

  After two weeks successful running several authentication in my development environment with average 10-20 users, i encourage myself to put in my production. it was up and ran with +-1000 users but only took 3-5 hours then squid suddenly stopped with error:

  2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1 (Operation not permitted)
  2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1 (Operation not permitted)
  2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying..... errno=1 (Operation not permitted)
  2015/03/06 15:07:59| negotiate_wrapper: Return 'AF oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARupdwIysaz6zjRSqsI8V4K0X67z4t5a9aOT7WPlyWRrp+1ol2zL6CYTcfZIyAq8q3D00mf+vpIeoiDDmkUkr+vXN+xkpXkWdX5pMD1hBrF4EDOL1RIp9XjpkdfIcEgg8Oia0Ay153sPK3+Tif4bGE= RickyC at company.local
  '
  2015/03/06 15:07:59| negotiate_wrapper: Return 'AF oYG1MIGyoAMKAQChCwYJKoZIhvcSAQICooGdBIGaYIGXBgkqhkiG9xIBAgICAG+BhzCBhKADAgEFoQMCAQ+ieDB2oAMCARKibwRtX5xuxTxrgsKQpg3Y+kUXLOng15XJ7eDByao5YtNPZByv/zRtrz13QgKkCuk+VkXnCAzaii0ri4Mxvd+4BoskIrjf5FuPP3W59wMTCtkPJD85igR/OmQ4Ch09DJ51WGwnOizMuCW+9jg6EsFa1Q== JanTS at company.local

  i use ubuntu server 14.04 with newest squid 3.5.2

  Squid Cache: Version 3.5.2
  Service Name: squid
  configure options:  '--enable-build-info' '--enable-removal-policies=lru,heap' '--enable-ltdl-install' '--enable-storeio=ufs,aufs,rock' '--enable-auth-basic=LDAP' '--enable-auth-negotiate=wrapper,kerberos' '--enable-external-acl-helpers=LDAP_group' '--enable-translation' '--enable-ssl-crtd' '--enable-gnuregex' '--enable-xmalloc-debug' '--enable-xmalloc-debug-trace' '--enable-xmalloc-statistics' '--enable-async-io' '--enable-icmp' '--enable-delay-pools' '--enable-useragent-log' '--enable-kill-parent-hack' '--enable-htpc' '--enable-forw-via-db' '--enable-cache-digests' '--enable-underscores' '--enable-x-accelerator-vary' '--enable-esi' '--enable-inline' '--enable-linux-netfilter' '--with-openssl' '--with-large-files'

  here is my squid.conf:

  # ===================== ACL Cachemgr ============================================
  acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
  acl managerAdmin src "/usr/local/squid/etc/mgradmin.txt"
  acl stream url_regex -i "/usr/local/squid/etc/stream"


  acl download url_regex -i "/usr/local/squid/etc/download"
  acl whitelist url_regex -i "/usr/local/squid/etc/whitelist"
  acl blacklist url_regex -i "/usr/local/squid/etc/blacklist"


  acl SSL_ports port 443
  acl Safe_ports port 80 # http
  acl Safe_ports port 21 # ftp
  acl Safe_ports port 443 # https
  acl Safe_ports port 70 # gopher
  acl Safe_ports port 210 # wais
  acl Safe_ports port 1025-65535 # unregistered ports
  acl Safe_ports port 280 # http-mgmt
  acl Safe_ports port 488 # gss-http
  acl Safe_ports port 591 # filemaker
  acl Safe_ports port 777 # multiling http
  acl http proto http
  acl CONNECT method CONNECT


  # ==================== Authenticate using negotiate_wrapper =====================
  auth_param negotiate program /usr/local/squid/libexec/negotiate_wrapper_auth -d --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --kerberos /usr/local/squid/libexec/negotiate_kerberos_auth -s GSS_C_NO_NAME
  auth_param negotiate children 50 startup=0 idle=1
  auth_param negotiate keep_alive off
  # ==================== Authenticate using NTLM ==================================
  auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp 
  auth_param ntlm children 20 startup=0 idle=1
  auth_param ntlm keep_alive off
  # ==================== Authenticate using Basic LDAP ============================
  auth_param basic program /usr/local/squid/libexec/basic_ldap_auth -R -b "dc=company,dc=local" -D squid at company.local -w "password" -f sAMAccountName=%s -h idhqvdc01.company.local,idhqvdc02.company.local
  auth_param basic children 5 startup=0 idle=1
  auth_param basic realm AGDS Proxy: Please enter your username and password domain
  auth_param basic credentialsttl 1 minute
  # ==================== Authenticate to Group Security Actice Directory ==========
  external_acl_type memberof ipv4 children-max=10 children-startup=1 %LOGIN /usr/local/squid/libexec/ext_ldap_group_acl -R -K -S -b "dc=company,dc=local" -D squid at company.local -w "password" -f "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%g,ou=groups,ou=resources,dc=company,dc=local))" -h idhqvdc01.company.local,idhqvdc02.company.local  


  acl auth proxy_auth REQUIRED
  # ==================== ACL Access hour user =====================================
  acl ach1 external memberof "/usr/local/squid/etc/ach1.txt" # access hour 1
  acl ach2 external memberof "/usr/local/squid/etc/ach2.txt" # access hour 2
  acl ach3 external memberof "/usr/local/squid/etc/ach3.txt" # access hour 3
  acl ach4 external memberof "/usr/local/squid/etc/ach4.txt" # access hour 4
  acl ach2time time D 10:00-11:59
  acl ach2time time D 13:00-14:59
  acl ach3time time D 08:00-09:59 
  acl ach3time time D 15:00-16:59
  acl ach4time time D 08:00-16:59
  acl bebastime time D 00:01-07:59 12:00-13:59 17:00-23:59


  # ==============================================================================
  http_access deny !Safe_ports # Deny requests to certain unsafe ports
  http_access deny CONNECT !SSL_ports # Deny CONNECT to other than secure SSL ports
  http_access allow manager localhost # Only allow cachemgr access from localhost
  http_access allow manager managerAdmin
  http_access deny manager


  # ==============================================================================
  #http_access allow localnet
  http_access allow localhost
  http_access deny blacklist !bebastime


  http_access allow http Safe_ports whitelist
  http_access allow CONNECT SSL_ports whitelist
  #http_access deny all !auth


  #http_access allow http Safe_ports ach1
  #http_access allow CONNECT SSL_ports ach1
  #http_access allow http Safe_ports ach2 !ach2time
  #http_access allow CONNECT SSL_ports ach2 !ach2time
  #http_access allow http Safe_ports ach3 !ach3time
  #http_access allow CONNECT SSL_ports ach3 !ach3time
  #http_access allow http Safe_ports ach4 !ach4time
  #http_access allow CONNECT SSL_ports ach4 !ach4time


  #http_access allow accesshours1
  #http_access allow accesshours2 !ach2time
  #http_access allow accesshours3 !ach3time
  #http_access allow accesshours4 !ach3time


  http_access allow ach1
  http_access allow ach2 !ach2time
  http_access allow ach3 !ach3time
  http_access allow ach4 !ach4time


  http_access deny all # Deny all other access to this proxy
  # ==============================================================================


  cache_dir rock /cache1/squid 97485 max-swap-rate=200 swap-timeout=300

  cache_dir rock /cache2/squid 97485 max-swap-rate=200 swap-timeout=300
  coredump_dir /usr/local/squid/var/cache/squid
  # =============================== Refresh Pattern ==============================

  refresh_pattern ^ftp: 1440 20% 10080
  refresh_pattern ^gopher: 1440 0% 1440
  refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
  refresh_pattern . 0 20% 4320


  cache_effective_user proxy
  visible_hostname proxy.company.local
  cache_mgr proxyastragraphia
  cachemgr_passwd secret all
  #err_page_stylesheet /usr/local/squid/etc/default.css
  pinger_enable off
  #workers 2


  i was tried to put different acl and put my auth_param at the top of my conf but still dying error. what should i do?
  any assistant or hint would be very appreciate. thanks



  Donny Vibianto



--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150312/b297667a/attachment.htm>

From ashish.patil at shreshtait.com  Fri Mar 13 05:25:45 2015
From: ashish.patil at shreshtait.com (Ashish Patil)
Date: Fri, 13 Mar 2015 10:55:45 +0530
Subject: [squid-users] Captive Portal authentication in Intercept mode
Message-ID: <CANQnQxSovvzZVod_35+sAFXNpw5TRE6O1Ew96nKaAxz4YmYdJQ@mail.gmail.com>

Hello,

I am trying to set up a Captive Portal with Squid (v.3.5.2) in Intercept
mode and SquidGuard (v.1.5) as URL rewriter. The Captive portal works off
usernames in a database, but Squid + SquidGuard work based off IP's.

The most progress I have had just says Authentication by Squid cannot be
done with Squid acting as a Intercepting Proxy. Is there some helper (even
probably in beta stage) that could help me achieve this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/eafeda15/attachment.htm>

From yvoinov at gmail.com  Fri Mar 13 05:39:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 13 Mar 2015 11:39:35 +0600
Subject: [squid-users] ssl_bump for specific dstdomain
In-Reply-To: <CAJUmE_PVfmsVA5Ykd2Gk6PULLEsY2jzF5e0VquZWKeAfK-W3uQ@mail.gmail.com>
References: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
 <5501AAF6.6040609@gmail.com>
 <CAJUmE_PVfmsVA5Ykd2Gk6PULLEsY2jzF5e0VquZWKeAfK-W3uQ@mail.gmail.com>
Message-ID: <55027817.9070607@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



13.03.15 2:37, Mukul Gandhi ?????:
> On Thu, Mar 12, 2015 at 11:04 AM, Yuri Voinov <yvoinov at gmail.com> 
> wrote:
> 
> You only have external helper (which is must wrote yourself) in 
> 3.4.x.
> 
> 
>> Are there any examples that I can look at to implemented this 
>> external helper for doing selective ssl_bumps. And what would 
>> this helper script do anyways? All we have is the destination IP 
>> address which is not really going to give us the actual HTTP 
>> hostname.
Yes and no. There is one third-party helper in list archives, written
on python. No one of this including in squid distribution.


> 
> 
> 
> Works with domains in ssl bump fully available at least 3.5.x
> 
> 
>> Does the 3.5.x implementation decrypt the whole payload and then 
>> do the ssl_bump? The "peek" option seems to imply that only the 
>> HTTP headers are peeked at.
Of course. As by 3.4.x. The difference is only with mechanisms.

> 
>> I guess what I am asking is, is there any way we can do this 
>> without actually decrypting the payload?
3.5.x peek-and-splise functionality do bump splitted by stages.
Against 3.4.x, which is makes bump in one stage.

> 
> 
> 
> 12.03.15 21:01, Mukul Gandhi ?????:
>>>> I am running squid 3.4.8 and am looking for solutions to 
>>>> ssl_bump for specific domains only. Going through the 
>>>> archives it is clear that it is not possible unless the 
>>>> reverse DNS points back to the domain that is to be ssl 
>>>> bumped.
>>>> 
>>>> So then what is the solution to this problem. I just want to 
>>>> create a SSL whitelist of domains that are to be bumped and 
>>>> the rest should be tunneled through. What I have is -
>>>> 
>>>> ssl_bump none localhost acl ssl_whitelist dstdomain 
>>>> "/tmp/ssl_whitelist.txt" ssl_bump server-first ssl_whitelist
>>>> 
>>>> The file /tmp/ssl_whitelist.txt contains -
>>>> 
>>>> .facebook.com .twitter.com .pintrest.com
>>>> 
>>>> Of course, this doesn't work because the ip address for these
>>>> websites points back to <something>.akamaitechnologies.com.
>>>> 
>>>> All I want is to be able to decrypt just the traffic to these
>>>> three web-sites, the rest should go through encrypted. But I
>>>> couldn't find a solution for this anywhere in the archives. I
>>>> did see some mention of using SslBump1/2/3 but it wasn't
>>>> clear if this was the silver bullet. Also I would have to
>>>> upgrade to 3.5 to use these new directives.
>>>> 
>>>> Any idea how I can achieve this in 3.4.8 (if possible)? Or
>>>> if I a solution exists for this in 3.5?
>>>> 
>>>> Thanks, -Mukul
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________ squid-users 
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users 
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> 
> 
> _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVAngXAAoJENNXIZxhPexGeYwIAIHnxixkc7Giy4EzQXpf+xqa
fqtozs1W2D7D349AURkUkwnNeq1VTNZb22Px6Jya9wpyuqAH0MXHSkeMkjDTtdjF
qUGIXEpjuhfHg0TaOXfnf41N8bdZ/lw4ZOeAgLdkVrfwXOO04oBqrr6ThVQMIjOS
NP1gz0ccxKFaZDgOS32Cg6uZ3fu92+vjobJN6UPVfr+EuN4BtF//aRxZ8BHfKX9C
ztrW1cBwL5IV4fecrFbJbEUSkria1IMezhnNRtrI5RtLVapftIN4jYGXFHwCUPHz
EMTboo1ohi5/WbOWvGQhsQjsm4mqkZ615Tk/CwQFGZ3qsJf1RK7msE2TeBWn8XE=
=7Rxa
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Fri Mar 13 08:46:05 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Mar 2015 10:46:05 +0200
Subject: [squid-users] One Time Password with squid, exists?
In-Reply-To: <CAOsHgttMmwBLfh2TgFBA-xQCmNgH+Vf_MdBm+ps6Ynj2OUdVdg@mail.gmail.com>
References: <5500F0E8.1010405@ngtech.co.il>	<1426137244594-4670343.post@n4.nabble.com>	<5501C695.7000407@ngtech.co.il>
 <CAOsHgttMmwBLfh2TgFBA-xQCmNgH+Vf_MdBm+ps6Ynj2OUdVdg@mail.gmail.com>
Message-ID: <5502A3CD.5020105@ngtech.co.il>

On 13/03/2015 05:22, Daniel Greenwald wrote:
> Ah that would be a clever way to implement pki authentication but i was
> thinking of something more that browser natively support..

Hey Daniel,

What is the direction of what you are thinking about?
I do not know about a browser natively support option.
The options I have seen until now are a local client in java or other 
languages which implements the whole secure level and the browser uses 
this layer to contact either a proxy or using a route as the default GW.

I think that it depends on the security level of the information in most 
cases.
If I do not trust the client\end machine such as in an Internet cafe I 
assume it will be pretty unsmart to assume any such implementation like 
pki will help.
If I do have a basic level of trust on the machine but not the network, 
it will be pretty simple to use a secure client like used in many cases 
with ssh tunnels.

Eliezer


From eliezer at ngtech.co.il  Fri Mar 13 09:03:01 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Mar 2015 11:03:01 +0200
Subject: [squid-users] Captive Portal authentication in Intercept mode
In-Reply-To: <CANQnQxSovvzZVod_35+sAFXNpw5TRE6O1Ew96nKaAxz4YmYdJQ@mail.gmail.com>
References: <CANQnQxSovvzZVod_35+sAFXNpw5TRE6O1Ew96nKaAxz4YmYdJQ@mail.gmail.com>
Message-ID: <5502A7C5.9030702@ngtech.co.il>

Hey,

I have written a basic idea with a php "login portal" that can be seen at:
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Conf
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/PhpLoginExample
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Python
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/SplashPageTemplate

The idea is an IP session based login.
The user actively needs to login and it will login the user IP address.
The helper(s) logic is based on time since the last user login.
This idea can be used as a sketch for a more advanced options with a portal.

There are other better ways to implement this idea and one of them is 
using a radius server.

As you noticed there is no way to directly authenticate a proxy in 
intercept mode.
Maybe someone out-there have been thinking about a way to do such a 
thing but it is yet to be possible with squid.

You can combine the php session login like in dyndns based solutions.
They offer a capability to re-register a domain based on your internet 
faced IP address.
Their client checks if the IP was changed and if so re-register vs the 
main server(with username and password).
So for example any new registration will revoke the old registration and 
any current registration is limited by to the current session life time.

Like in linux tcp_keep_alive there is an option to limit the session for 
5-10 minutes and if it will not be "keeped" alive after 2 hours it will 
be automatically revoked off access to the proxy.

The logic I have written can be implemented but should be carefully 
designed.

All The Bests,
Eliezer Croitoru

On 13/03/2015 07:25, Ashish Patil wrote:
> Hello,
>
> I am trying to set up a Captive Portal with Squid (v.3.5.2) in Intercept
> mode and SquidGuard (v.1.5) as URL rewriter. The Captive portal works off
> usernames in a database, but Squid + SquidGuard work based off IP's.
>
> The most progress I have had just says Authentication by Squid cannot be
> done with Squid acting as a Intercepting Proxy. Is there some helper (even
> probably in beta stage) that could help me achieve this?
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From squid3 at treenet.co.nz  Fri Mar 13 09:04:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 22:04:07 +1300
Subject: [squid-users] ssl_bump for specific dstdomain
In-Reply-To: <55027817.9070607@gmail.com>
References: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
 <5501AAF6.6040609@gmail.com>
 <CAJUmE_PVfmsVA5Ykd2Gk6PULLEsY2jzF5e0VquZWKeAfK-W3uQ@mail.gmail.com>
 <55027817.9070607@gmail.com>
Message-ID: <5502A807.6080700@treenet.co.nz>

On 13/03/2015 6:39 p.m., Yuri Voinov wrote:
> 
> 
> 13.03.15 2:37, Mukul Gandhi ?????:
>> On Thu, Mar 12, 2015 at 11:04 AM, Yuri Voinov <yvoinov at gmail.com> 
>> wrote:
> 
>> You only have external helper (which is must wrote yourself) in 
>> 3.4.x.
> 
> 
>>> Are there any examples that I can look at to implemented this 
>>> external helper for doing selective ssl_bumps. And what would 
>>> this helper script do anyways? All we have is the destination IP 
>>> address which is not really going to give us the actual HTTP 
>>> hostname.
> Yes and no. There is one third-party helper in list archives, written
> on python. No one of this including in squid distribution.
> 
> 
>> Works with domains in ssl bump fully available at least 3.5.x
> 
> 
>>> Does the 3.5.x implementation decrypt the whole payload and then 
>>> do the ssl_bump? The "peek" option seems to imply that only the 
>>> HTTP headers are peeked at.
> Of course. As by 3.4.x. The difference is only with mechanisms.

And no at the same time. HTTP message headers inside the encryption are
encrypted and unavailable until after the decryption is decided (bumped).

What gets peeked at is the TLS ClientHello and TLS ServerHello details.
SNI may become available by peeking when raw-IP was all that was in the
HTTP CONNECT message or intercepted TCP packets.

You can then use those non-private TLS details to decide between reject,
splice (pass-thru) or bump (decrypt) for the encrypted HTTPS data.


> 
>>> I guess what I am asking is, is there any way we can do this 
>>> without actually decrypting the payload?
> 3.5.x peek-and-splise functionality do bump splitted by stages.
> Against 3.4.x, which is makes bump in one stage.
> 

Amos



From james at ejbdigital.com.au  Fri Mar 13 09:10:55 2015
From: james at ejbdigital.com.au (James Harper)
Date: Fri, 13 Mar 2015 09:10:55 +0000
Subject: [squid-users] Captive Portal authentication in Intercept mode
In-Reply-To: <5502A7C5.9030702@ngtech.co.il>
References: <CANQnQxSovvzZVod_35+sAFXNpw5TRE6O1Ew96nKaAxz4YmYdJQ@mail.gmail.com>
 <5502A7C5.9030702@ngtech.co.il>
Message-ID: <HKNPR04MB193C9587D8CAC2620D45E4FE8070@HKNPR04MB193.apcprd04.prod.outlook.com>

> Hey,
> 
> I have written a basic idea with a php "login portal" that can be seen at:
> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/
> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Conf
> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/PhpLoginExample
> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Python
> http://wiki.squid-
> cache.org/EliezerCroitoru/SessionHelper/SplashPageTemplate
> 
> The idea is an IP session based login.
> The user actively needs to login and it will login the user IP address.
> The helper(s) logic is based on time since the last user login.
> This idea can be used as a sketch for a more advanced options with a portal.
> 
> There are other better ways to implement this idea and one of them is
> using a radius server.
> 
> As you noticed there is no way to directly authenticate a proxy in
> intercept mode.
> Maybe someone out-there have been thinking about a way to do such a
> thing but it is yet to be possible with squid.
> 

If you could do ntlm auth at your portal page then the user might never even notice that authentication took place...

You'd need to do some sort of browser detection though - browsers could handle such authentication, but programs phoning home or otherwise using web services would hate it.

James

From priya_agarwal at students.iitmandi.ac.in  Fri Mar 13 09:19:24 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Fri, 13 Mar 2015 14:49:24 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
Message-ID: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>

Hi,

I wanted to link certain static libraries and use them in squid source
code. I added the following lines in Makefile.am of the 'src' directory.

squid_LDFLAGS =
-L/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/lib_powerpc/

squid_LDLIBS = -lusdpaa_dma -lusdpaa_dpa_offload -lusdpaa_of -lusdpaa_ppac
-lusdpaa_qbman -lusdpaa_rmu -lusdpaa_srio -lusdpaa_dma_mem -lusdpaa_fman
-lusdpaa_pme \
               -lusdpaa_process -lusdpaa_rman -lusdpaa_sec -lusdpaa_syscfg

squid_CPPFLAGS =
-I/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/include/

But I think this wrong as libraries aren't getting linked. I am getting
"undefined reference to" errors.
So what variables should I add/change.


Thanks.

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/c7661025/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 13 09:24:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 22:24:55 +1300
Subject: [squid-users] Captive Portal authentication in Intercept mode
In-Reply-To: <HKNPR04MB193C9587D8CAC2620D45E4FE8070@HKNPR04MB193.apcprd04.prod.outlook.com>
References: <CANQnQxSovvzZVod_35+sAFXNpw5TRE6O1Ew96nKaAxz4YmYdJQ@mail.gmail.com>
 <5502A7C5.9030702@ngtech.co.il>
 <HKNPR04MB193C9587D8CAC2620D45E4FE8070@HKNPR04MB193.apcprd04.prod.outlook.com>
Message-ID: <5502ACE7.9010300@treenet.co.nz>

On 13/03/2015 10:10 p.m., James Harper wrote:
>> Hey,
>>
>> I have written a basic idea with a php "login portal" that can be seen at:
>> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/
>> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Conf
>> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/PhpLoginExample
>> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Python
>> http://wiki.squid-
>> cache.org/EliezerCroitoru/SessionHelper/SplashPageTemplate
>>
>> The idea is an IP session based login.
>> The user actively needs to login and it will login the user IP address.
>> The helper(s) logic is based on time since the last user login.
>> This idea can be used as a sketch for a more advanced options with a portal.
>>
>> There are other better ways to implement this idea and one of them is
>> using a radius server.
>>
>> As you noticed there is no way to directly authenticate a proxy in
>> intercept mode.
>> Maybe someone out-there have been thinking about a way to do such a
>> thing but it is yet to be possible with squid.
>>
> 
> If you could do ntlm auth at your portal page then the user might never even notice that authentication took place...
> 
> You'd need to do some sort of browser detection though - browsers could handle such authentication, but programs phoning home or otherwise using web services would hate it.
> 

That auth trick is usable for any kind of HTTP auth the client software
supports. eg. Basic auth for the automated tools usually. It's just
authenticating to the portal web server. As long as the portals not
trying to do auth with the intercepted traffic its fine.

FYI: NTLM is probably amongst the worst ways to do it given all the
nastiness that has to take place for NTLM to "work".

Amos



From squid3 at treenet.co.nz  Fri Mar 13 09:29:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 22:29:30 +1300
Subject: [squid-users] Reverse Proxy Funny Logging Issue
In-Reply-To: <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>
References: <0113b4b74c8c9e0a05cd692641aecf25@dweimer.net>
 <50FF7561.6050301@treenet.co.nz>
 <88bdb4a5d0b47bb9e459b91b079b7425@dweimer.net>
 <8a05b3c589b46021b42d5a59a7b11e8a@dweimer.net>
 <5100BB15.3030502@treenet.co.nz>
 <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>
Message-ID: <5502ADFA.5030303@treenet.co.nz>

On 13/03/2015 4:31 a.m., dweimer wrote:
> On 01/23/2013 10:39 pm, Amos Jeffries wrote:
>> On 24/01/2013 4:13 a.m., dweimer wrote:
>>> On 2013-01-23 08:40, dweimer wrote:
>>>> On 2013-01-22 23:30, Amos Jeffries wrote:
>>>>> On 23/01/2013 5:34 a.m., dweimer wrote:
>>>>>> I just upgraded my reverse proxy server last night from 3.1.20 to
>>>>>> 3.2.6, all is working well except one of my log rules, and I can't
>>>>>> figure out why.
>>>>>
>>>>> Please run "squid -k parse" and resolve the WARNING or ERROR which
>>>>> are listed.
>>>>>
>>>>> There are two possible reasons...
>>>>>
>>>>>>
>>>>>> I have a several sites behind the server, with dstdomain access
>>>>>> rules setup.
>>>>>>
>>>>>> acl website1 dstdomain www.website1.com
>>>>>> acl website2 dstdomain www.website2.com
>>>>>> acl website2 dstdomain www.website3.com
>>>>>
>>>>> Possible reason #1 (assuming thi is an accurate copy-n-paste from
>>>>> yoru config file).....  you have no website3 ACL definition?
>>>>
>>>> That was a typo in the email, correct ACL is in the configuration,
>>>> squid -k parse outputs no warnings or errors.
>>>>
>>>>>
>>>>>> ...
>>>>>>
>>>>>> Followed by the access rules
>>>>>>
>>>>>> http_access allow website1
>>>>>> http_access allow website2
>>>>>> http_access allow website3
>>>>>> ...
>>>>>> http_access deny all
>>>>>>
>>>>>> Some are using rewrites
>>>>>> url_rewrite_program /usr/local/etc/squid/url_rewrite.py
>>>>>> url_rewrite_children 20
>>>>>> url_rewrite_access allow website1
>>>>>> url_rewrite_access allow website3
>>>>>> ...
>>>>>> url_rewrite_access deny all
>>>>>>
>>>>>> Then my access logs
>>>>>>
>>>>>> # First I grab everything in one
>>>>>> access_log daemon:/var/log/squid/access.log squid all
>>>
>>>>>
>>>>>> access_log daemon:/var/log/squid/website1.log combined website1
>>>>>> access_log daemon:/var/log/squid/website2.log combined website2
>>>>>> access_log daemon:/var/log/squid/website3.log combined website3
>>>>>> ...
>>>>>>
>>>>>> everything works, write down to one of the access logs, the data
>>>>>> shows up in the access.log file, the data shows up in the
>>>>>> individual logs for all the others, except that one.  If we use
>>>>>> website3 from the above example like my actual file the access
>>>>>> rule works on the url_rewrite_access allow line, but for some
>>>>>> reason is failing on the log line.  squid -k parse doesn't show
>>>>>> any errors, and shows a Processing: access_log
>>>>>> daemon:/var/log/squid/website3.log combined website3 line in the
>>>>>> output.
>>>>>>
>>>>>> The log in question was originally at the end of my access_log
>>>>>> list section, so I changed the order around to see if for some
>>>>>> reason it was only the last one not working, no change still only
>>>>>> that one not working, And the new last one in the list still works
>>>>>> as expected.
>>>>>>
>>>>>> I know the ACL is working as it works correctly on the rewrite
>>>>>> rule and the http access just above the log rules, anyone have any
>>>>>> ideas on how I can figure out why the log entry isn't working?
>>>>
>>>
>>> Changed lines back to daemon, changed acl on logs to the rewrite side
>>> used on the cache_peer_access lines later in the configuration. 
>>> Works now, and logs even show up with the pre-rewrite rule host
>>> information...
>>>
>>> That does make me wonder why some lines were getting logged but not
>>> all, the sites I thought were working do have higher usage, maybe I
>>> was still missing a lot from them, and just not knowing it.  I guess
>>> I will see if my webalizer reports show a huge gain in hit count over
>>> the old records from the the 3.1.20 installation, of if this behavior
>>> is only evident in the 3.2 branch.
>>>
>>
>> I think you will find that the lines being logged previously were on
>> the requests which were either not rewritten at all or were re-written
>> from another requests URL which was being logged.
>>
>> Each of the ACL-driven directive labels in squid.conf is effectively
>> an event trigger script - deciding whether or not to perform some
>> action. This only makes sense testing when that action choice is
>> requried.  Squid processing pathway checks http_access first, ... then
>> some others, ... then url_rewriting, ... then the destination
>> selection (cache_peer and others), ... then when the transaction is
>> fully completed access_log output decision are done.
>>
>> Amos
> 
> Last night I applied the FreeBSD 10.1-RELEASE-p6 Update and Upgraded the
> ports which included Squid 3.4.12, I enabled the LAX HTTP option in the
> ports configuration with adds the --enable-http-violations compile
> option. With the intention to enable broken_posts option in the
> configuration. I will hopefully be able to apply any necessary changes
> to the production system after I test them now.
> When doing this update I did have a thought the system is running in a
> FreeBSD jail and not on the base system is there a chance this issue is
> caused by running within a jail? curious if anyone has ran into specific
> issues running Squid in a FreeBSD jail before?
> 

That should only matter if you are getting permissions issues or not
able to find system parts that are depended on. I expect that type of
issue to be visible in cache.log or syslog rather than the HTTP access.log.

Amos


From squid3 at treenet.co.nz  Fri Mar 13 09:54:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Mar 2015 22:54:54 +1300
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
Message-ID: <5502B3EE.9040908@treenet.co.nz>

On 13/03/2015 10:19 p.m., Priya Agarwal wrote:
> Hi,
> 
> I wanted to link certain static libraries and use them in squid source
> code. I added the following lines in Makefile.am of the 'src' directory.

Please be aware if you do that your new code and anything built from the
Squid Makefile MUST be GPLv2 compliant.


> 
> squid_LDFLAGS =
> -L/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/lib_powerpc/
> 
> squid_LDLIBS = -lusdpaa_dma -lusdpaa_dpa_offload -lusdpaa_of -lusdpaa_ppac
> -lusdpaa_qbman -lusdpaa_rmu -lusdpaa_srio -lusdpaa_dma_mem -lusdpaa_fman
> -lusdpaa_pme \
>                -lusdpaa_process -lusdpaa_rman -lusdpaa_sec -lusdpaa_syscfg
> 
> squid_CPPFLAGS =
> -I/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/include/
> 
> But I think this wrong as libraries aren't getting linked. I am getting
> "undefined reference to" errors.
> So what variables should I add/change.

Those options are auto-generated by the autotools build chain.

The -L flag should be passed by the ./configure LDFLAGS= parameter or
coded into Makefile.am variable as a relative path. Its unlikely the
exact absolute path will remain the same even within your build
machines. I see you are using some path under ".../tmp" for example.


Libraries are added to squid_LDADD variable. That takes the form:

 squid_LDADD += -lusdpaa_dma
or
 squid_LDADD += -L$(top_srcdir)/../usdpaa/ -lusdpaa_dma


which assumes the squid and usdpaa library code checkouts are sitting
next to each other.


Same deal for the -I value in the ./configure CXXFLAGS= parameter, or in
the Makefile.am as a relative path:

 AM_CPPFLAGS += -I$(top_srcdir)/../usdpaa/include/


Amos



From priya_agarwal at students.iitmandi.ac.in  Fri Mar 13 11:28:09 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Fri, 13 Mar 2015 16:58:09 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <5502B3EE.9040908@treenet.co.nz>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
 <5502B3EE.9040908@treenet.co.nz>
Message-ID: <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>

I tried what you advised. Getting the same error for both methods
(./configure LDFLAGS=-L<../tmp/../lib CXXFLAGS=-I<.../tmp../include or
editing Makefile.am appropriately). autoreconf is failing.
And also I am getting many such warnings:

| src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
(or '*_CPPFLAGS')
| compat/Makefile.am:5:   'src/Common.am' included from here
| src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
(or '*_CPPFLAGS')
| helpers/basic_auth/DB/Makefile.am:1:   'src/Common.am' included from here
| src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
(or '*_CPPFLAGS')
| helpers/basic_auth/LDAP/Makefile.am:1:   'src/Common.am' included from
here
| src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
(or '*_CPPFLAGS')
| helpers/basic_auth/MSNT-multi-domain/Makefile.am:1:   'src/Common.am'
included from here
| src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
(or '*_CPPFLAGS')

Final error:
| autoreconf: automake failed with exit status: 1
| ERROR: autoreconf execution failed.

So is something wrong with the path?

I have attached the logfile as well which shows the detailed output.




On Fri, Mar 13, 2015 at 3:24 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 13/03/2015 10:19 p.m., Priya Agarwal wrote:
> > Hi,
> >
> > I wanted to link certain static libraries and use them in squid source
> > code. I added the following lines in Makefile.am of the 'src' directory.
>
> Please be aware if you do that your new code and anything built from the
> Squid Makefile MUST be GPLv2 compliant.
>
>
> >
> > squid_LDFLAGS =
> >
> -L/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/lib_powerpc/
> >
> > squid_LDLIBS = -lusdpaa_dma -lusdpaa_dpa_offload -lusdpaa_of
> -lusdpaa_ppac
> > -lusdpaa_qbman -lusdpaa_rmu -lusdpaa_srio -lusdpaa_dma_mem -lusdpaa_fman
> > -lusdpaa_pme \
> >                -lusdpaa_process -lusdpaa_rman -lusdpaa_sec
> -lusdpaa_syscfg
> >
> > squid_CPPFLAGS =
> >
> -I/media/NewVolume/yocto/build_t4240qds_release/tmp/work/ppce6500-fsl_networking-linux/usdpaa/git-r5/include/
> >
> > But I think this wrong as libraries aren't getting linked. I am getting
> > "undefined reference to" errors.
> > So what variables should I add/change.
>
> Those options are auto-generated by the autotools build chain.
>
> The -L flag should be passed by the ./configure LDFLAGS= parameter or
> coded into Makefile.am variable as a relative path. Its unlikely the
> exact absolute path will remain the same even within your build
> machines. I see you are using some path under ".../tmp" for example.
>
>
> Libraries are added to squid_LDADD variable. That takes the form:
>
>  squid_LDADD += -lusdpaa_dma
> or
>  squid_LDADD += -L$(top_srcdir)/../usdpaa/ -lusdpaa_dma
>
>
> which assumes the squid and usdpaa library code checkouts are sitting
> next to each other.
>
>
> Same deal for the -I value in the ./configure CXXFLAGS= parameter, or in
> the Makefile.am as a relative path:
>
>  AM_CPPFLAGS += -I$(top_srcdir)/../usdpaa/include/
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/340d7e08/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: log.do_configure.3013
Type: application/octet-stream
Size: 21494 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/340d7e08/attachment.obj>

From squid3 at treenet.co.nz  Fri Mar 13 11:59:37 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 00:59:37 +1300
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>	<5502B3EE.9040908@treenet.co.nz>
 <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
Message-ID: <5502D129.7030809@treenet.co.nz>

On 14/03/2015 12:28 a.m., Priya Agarwal wrote:
> I tried what you advised. Getting the same error for both methods
> (./configure LDFLAGS=-L<../tmp/../lib CXXFLAGS=-I<.../tmp../include or
> editing Makefile.am appropriately). autoreconf is failing.

I see "<" characters in your paths. That is invalid. As is the -I paths
segments "..." and "tmp.." looks like you are missing '/' somewhere.


> And also I am getting many such warnings:
> 
> | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> (or '*_CPPFLAGS')
> | compat/Makefile.am:5:   'src/Common.am' included from here
> | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> (or '*_CPPFLAGS')
> | helpers/basic_auth/DB/Makefile.am:1:   'src/Common.am' included from here
> | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> (or '*_CPPFLAGS')
> | helpers/basic_auth/LDAP/Makefile.am:1:   'src/Common.am' included from
> here
> | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> (or '*_CPPFLAGS')
> | helpers/basic_auth/MSNT-multi-domain/Makefile.am:1:   'src/Common.am'
> included from here
> | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> (or '*_CPPFLAGS')
> 

Those are just warnings because you are working with an old Squid
version and autotools have changed their requirements since. The current
release dont have quite so many warnings (some remain). Those can be ignore.

I does mean that what I wrote as AM_CPPFLAGS needs to instead be written
as INCLUDES in your Squid versions Makefile.am.


> Final error:
> | autoreconf: automake failed with exit status: 1
> | ERROR: autoreconf execution failed.
> 
> So is something wrong with the path?

I see "<" characters in what you

> 
> I have attached the logfile as well which shows the detailed output.
> 

Buried in the warnings I see this:

src/Makefile.am:661: error: '#' comment at start of rule is unportable


automake syntax has two forms of comment.
 ## comments are autoreconf comments and ignored
 # comments are copied through as-is to the final Makefile

If you are using multi-line wrapped lists of things, that can cause
issues. Its easier to just never use comments inside the wrapped lines.


Other things to watch out for auth makefiles:

* indentation for rules needs to be one tab, not spaces. This needs
checking after each copy-paste you do.

* multi-line rules and lists use '\' character ending to explicitly
define the wrapping.
  Be careful that lists of libraries etc use them on each line up to,
but not on, the final line of the list.


Amos


From hack.back at hotmail.com  Fri Mar 13 12:14:19 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 13 Mar 2015 05:14:19 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:769:
	"Comm::IsConnOpen(conn)"
In-Reply-To: <1426128477672-4670340.post@n4.nabble.com>
References: <1424025171679-4669842.post@n4.nabble.com>
 <1424209440077-4669927.post@n4.nabble.com> <54E3F058.3040200@treenet.co.nz>
 <1424253241949-4669944.post@n4.nabble.com>
 <1424377008695-4669964.post@n4.nabble.com>
 <1424449064181-4670001.post@n4.nabble.com>
 <1426128477672-4670340.post@n4.nabble.com>
Message-ID: <1426248859590-4670392.post@n4.nabble.com>

Dear amos i still have the same problem !!
please give advice about this critical problem



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-769-Comm-IsConnOpen-conn-tp4669842p4670392.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sci at posteo.de  Fri Mar 13 13:34:27 2015
From: sci at posteo.de (sci)
Date: Fri, 13 Mar 2015 14:34:27 +0100
Subject: [squid-users] Read Timeout
Message-ID: <ea40eb71a0a98c6f4ace2ec4f5d7740d@posteo.de>

Hi,

I search a solution for the follow "problem":

A website with a simple button (POST) needs more than 15min to send the 
information what the clients needs (a simple excel sheet).
We don't want to change the global settings at the "Read Timeout", so I 
try to find something like a ACL for the squid.

Is it possible to at a ACL to the squid.conf like:

# rule only for the slow website
acl slow-server dstdomain .someserver-slow.com
read_timeout 30 minutes slow-server
# end rule

-- 
regards,
sci


From mattatrmc at gmail.com  Fri Mar 13 13:37:12 2015
From: mattatrmc at gmail.com (mattatrmc)
Date: Fri, 13 Mar 2015 06:37:12 -0700 (PDT)
Subject: [squid-users] https traffic using squid and icap
In-Reply-To: <loom.20130818T133845-351@post.gmane.org>
References: <51C352D9.6000700@nanogherkin.com>
 <51C39B8D.1040205@treenet.co.nz> <1371774072964-4660727.post@n4.nabble.com>
 <51C39EAD.80207@treenet.co.nz> <1371776839469-4660730.post@n4.nabble.com>
 <51C3AE0A.1090207@treenet.co.nz> <1371779363334-4660733.post@n4.nabble.com>
 <51C3F192.1080408@nanogherkin.com> <1371836546140-4660763.post@n4.nabble.com>
 <loom.20130818T133845-351@post.gmane.org>
Message-ID: <1426253832758-4670394.post@n4.nabble.com>

Hi Michael, 

I was just reading this thread, and I've been following the same line of
thought wrt to monitoring the contents of HTTPS payloads that way I can
conduct analysis with an IDS such as Snort, or Bro.  Did you end up having
any luck with ICAP, or is this a rabbit hole that I'm going down?

Cheers,

Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/https-traffic-using-squid-and-icap-tp4660720p4670394.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From john.kj1984 at gmail.com  Fri Mar 13 14:40:35 2015
From: john.kj1984 at gmail.com (john jacob)
Date: Fri, 13 Mar 2015 20:10:35 +0530
Subject: [squid-users] peek/splice working with lynx but not with
 firefox or chrome [SOLVED]
In-Reply-To: <cone.1426007543.73762.3977.1000@bollix>
References: <cone.1425995214.702778.3977.1000@bollix>
 <54FEFAF7.70205@treenet.co.nz>
 <cone.1425997703.735129.3977.1000@bollix>
 <54FF0406.9000203@treenet.co.nz>
 <cone.1425999809.356038.3977.1000@bollix>
 <cone.1426007543.73762.3977.1000@bollix>
Message-ID: <CAOoFbVc6rNgYdK_BwONLMGphaF-7o=7oGr0J=WBZL+J8OY1KcQ@mail.gmail.com>

Hi,

I am also having similar environment with squid (version 3.5.2
-20150218-r13758) and openssl 1.0.1k, but for me only small number of https
sites are working with peek and splice. For eg:- , I can access
https://www.google.com but not https://ssllabs.com and lot of other https
domains, giving "Error negotiating SSL on FD 15: error:140920E3:SSL
routines:SSL3_GET_SERVER_HELLO:parse tlsext (1/-1/0) " in the cache.log
file.

Also I could see a bunch of other error messages in the cache.log files
relating to openssl (like "Error negotiating SSL on FD 21:
error:14094085:SSL routines:SSL3_READ_BYTES:ccs received early (1/-1/0)" ,
"Error verifying certificates " etc)  when tried to access sites like
https://www.facebook.com, https://www.yahoo.com etc

Squid is running on a CentOS 7 x64 box and Workstation is Win7 with Firefox
and Chrome. I tried configuring openssl with disabling certain options with
no-nextprotoneg  and no-ec as well as with recent openssl version1.0.2 ,
but without any success.

Below is my squid config file.

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

ssl_bump peek all
ssl_bump splice all

# Squid normally listens to port 3128
http_port <WAN Interface IP>:3128
http_port <WAN Interface IP>:3129 intercept
https_port <WAN Interface IP>:3130 intercept ssl-bump
cert=/tmp/sslcertificates/server.cert.pem
key=/tmp/sslcertificates/server.key.pem

Does this has to do anything specific to my environment or the config
options? Any help on this is highly appreciated.

Thanks in advance,
John

On Tue, Mar 10, 2015 at 10:42 PM, Roel van Meer <roel at 1afa.com> wrote:

> Roel van Meer writes:
>
>  >> > I'm using squid 3.5.2 built with openssl 0.9.8zc on Slackware 13.1.
>>> >> > Traffic is redirected from port 443 top 3130 with iptables.
>>> >>
>>> >> ... and with an older version of OpenSSL missing many of the last few
>>> >> years worth of TLS crypto features. IIRC the library releases are now
>>> up
>>> >> to 1.1.* or something. Its best to keep that kind of thing operating
>>> the
>>> >> latest versions.
>>> >
>>> > I know it missing the latest features, but security patches are
>>> > backported. And I know it is old, but it's what I have to work with
>>> > now.Do you think it might be the cause of the problem I'm having with
>>> > peek/splice, or was it a general recommendation?
>>>
>>> Its a potential source of problems. Chrome is very much on the front
>>> line of the arms race attempting to stop things like SSL-Bump working.
>>> Firefox implement their own crypto library which tracks the latest TLS
>>> features at a similar speed of development.
>>> OpenSSL will be perpetually behind both of them, but at least the latest
>>> one(s) have better chances not to be advertising features they reject in
>>> "considered harmful" grounds.
>>>
>>
>> I'll have a go then at trying with a newer openssl and the patches from
>> thethread you mentioned.
>>
>
> With Squid 3.5.2 built with openssl 1.0.1k I can splice https connections
> with no trouble. Tested with Lync, Chrome, Firefox, and IE.
>
> So you were right. :) Thanks a lot for pointing me in the right direction!
>
> Cheers,
>
> Roel
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/2da60887/attachment.htm>

From vdoctor at neuf.fr  Fri Mar 13 15:37:44 2015
From: vdoctor at neuf.fr (Stakres)
Date: Fri, 13 Mar 2015 08:37:44 -0700 (PDT)
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraris/CDNs Booster
In-Reply-To: <1424604199892-4670015.post@n4.nabble.com>
References: <1419155198462-4668803.post@n4.nabble.com>
 <1420464046420-4668929.post@n4.nabble.com> <54AA918C.10300@gmail.com>
 <1420465642362-4668933.post@n4.nabble.com> <54AA979B.7060803@gmail.com>
 <1420470567538-4668941.post@n4.nabble.com>
 <1421655817441-4669159.post@n4.nabble.com>
 <1422466278065-4669395.post@n4.nabble.com>
 <1423553249913-4669653.post@n4.nabble.com>
 <1424604199892-4670015.post@n4.nabble.com>
Message-ID: <1426261064321-4670396.post@n4.nabble.com>

Hi All,

Advanced Caching Add-On for Linux Squid Proxy Cache v2.7, v3.4 and v3.5 with
Videos, Music, Images, Libraries and CDNs.

New  version 2.39 <https://sourceforge.net/projects/squidvideosbooster/>   -
March 13th 2015.
- New websites
- Tiny bugs fixed
More details on  https://svb.unveiltech.com <https://svb.unveiltech.com>  

Enjoy 

Bye Fred 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-2-7-3-4-and-3-5-Videos-Music-Images-Libraries-CDNs-Booster-tp4668683p4670396.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From monahbaki at gmail.com  Fri Mar 13 15:58:59 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 13 Mar 2015 11:58:59 -0400
Subject: [squid-users] squid intercept config
In-Reply-To: <54FA7DED.9090902@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
Message-ID: <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>

Hi All,

Installed squid on CentOS 6.6 and it's working, but mY access.log shows all
TCP_MISS and no TCP_HIT. The following config:

squid.conf
# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept



iptables

# Generated by iptables-save v1.4.7 on Fri Mar 13 16:04:02 2015
*nat
:PREROUTING ACCEPT [10:2031]
:POSTROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A PREROUTING -s 147.245.252.13/32 -p tcp -m tcp --dport 80 -j ACCEPT
-A PREROUTING -s 10.0.0.24/32 -p tcp -m tcp --dport 80 -j ACCEPT
-A PREROUTING -s 147.245.252.13/32 -p tcp -m tcp --dport 80 -j ACCEPT
-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
-A POSTROUTING -j MASQUERADE
COMMIT
# Completed on Fri Mar 13 16:04:02 2015
# Generated by iptables-save v1.4.7 on Fri Mar 13 16:04:02 2015
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [1818:649971]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j REJECT --reject-with icmp-port-unreachable
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -i eth0 -p tcp -m tcp --dport 3129 -m state --state
NEW,ESTABLISHED -j ACCEPT
-A INPUT -i eth0 -p tcp -m tcp --dport 80 -m state --state NEW,ESTABLISHED
-j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Fri Mar 13 16:04:02 2015
# Generated by iptables-save v1.4.7 on Fri Mar 13 16:04:02 2015
*mangle
:PREROUTING ACCEPT [68:6199]
:INPUT ACCEPT [68:6199]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [26:3064]
:POSTROUTING ACCEPT [26:3064]
-A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
COMMIT
# Completed on Fri Mar 13 16:04:02 2015


Accessing sites, shows the IP address of the proxy 147.245.252.13.

Am I missing something in IPTables that it is not caching?


Thanks
Monah

On Fri, Mar 6, 2015 at 11:26 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/03/2015 1:19 a.m., Monah Baki wrote:
> > Hi all, can anyone verify if this is correct, need to make ure that users
> > will be able to access the internet via the squid.
> >
> > Running FreeBSD with a single interface with Squid-3.5.2
> >
> > Policy based routing on Cisco with the following:
> >
> >
> > interface GigabitEthernet0/0/1.1
> >
> > encapsulation dot1Q 1 native
> >
> > ip address 10.0.0.9 255.255.255.0
> >
> > no ip redirects
> >
> > no ip unreachables
> >
> > ip nat inside
> >
> > standby 1 ip 10.0.0.10
> >
> > standby 1 priority 120
> >
> > standby 1 preempt
> >
> > standby 1 name HSRP
> >
> > ip policy route-map CFLOW
> >
> >
> >
> > ip access-list extended REDIRECT
> >
> > deny   tcp host 10.0.0.24 any eq www
> >
> > permit tcp host 10.0.0.23 any eq www
> >
> >
> >
> > route-map CFLOW permit 10
> >
> > match ip address REDIRECT
> > set ip next-hop 10.0.0.24
> >
> > In my /etc/pf.conf
> > rdr pass inet proto tcp from 10.0.0.0/8 to any port 80 -> 10.0.0.24 port
> > 3129
> >
> > # block in
> > pass in log quick on bge0
> > pass out log quick on bge0
> > pass out keep state
> >
> > and finally in my squid.conf:
> > http_port 3128
> > http_port 3129 intercept
> >
> >
> >
> > And for testing purposes from the squid server:
> >  ./squidclient -h 10.0.0.24 -p 3128 http://www.freebsd.org/
> >
> > If I replace -p 3128 with -p 80, I get a access denied, and if I omit the
> > -p 3128 completely, I can access the websites.
>
> If you omit the -p entirely squidclient assumes "-p 3128" (the proxy
> default listening port), so it works exactly the same as if you had used
> -p 3128 explicitly.
>
> If you use -p 80 you also need to change the pther parameters so they
> generate port-80 syntax message:
>  - the -h with IP or hostname of the remote web server, and
>  - the URL parameters being a relative URL, and
>  - the -j parameter with Host: header domain name of the server
> ...
>  eg.
>  squidclient -h www.freebsd.org -j www.freebsd.org -p 80 /
>
> NP: if your squidclient is too old to support -j, use this instead:
>   -H 'Host: www.freebsd.org\n'
>
>  ** this test should work from the squid box without having gone through
> the proxy. Only from the client machine should it work *with* NAT
> passing it through the proxy.
>
>
>
> Using a proxy syntax message sent directly to the proxy receiving port,
> or with the proxy as receiving IP on port 80 (NAT'ed to Squid) is a
> guaranted forwarding loop failure.
>
>
> That doesn't fix your clients issue, but hopefully makes it clear that
> the above desribed test is broken enough to prevent you identifying when
> the client issue is fixed if that happens on some change.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/5018e95c/attachment.htm>

From yvoinov at gmail.com  Fri Mar 13 16:18:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 13 Mar 2015 22:18:40 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
Message-ID: <55030DE0.60606@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



13.03.15 21:58, Monah Baki ?????:
> Hi All,
> 
> Installed squid on CentOS 6.6 and it's working, but mY access.log
> shows all TCP_MISS and no TCP_HIT. The following config:
> 
> squid.conf # Squid normally listens to port 3128 http_port 3128 
> http_port 3129 intercept

And that's all????

> 
> 
> 
> iptables
> 
> # Generated by iptables-save v1.4.7 on Fri Mar 13 16:04:02 2015 
> *nat :PREROUTING ACCEPT [10:2031] :POSTROUTING ACCEPT [0:0] :OUTPUT
> ACCEPT [0:0] -A PREROUTING -s 147.245.252.13/32 -p tcp -m tcp
> --dport 80 -j ACCEPT -A PREROUTING -s 10.0.0.24/32 -p tcp -m tcp
> --dport 80 -j ACCEPT -A PREROUTING -s 147.245.252.13/32 -p tcp -m
> tcp --dport 80 -j ACCEPT -A PREROUTING -p tcp -m tcp --dport 80 -j
> REDIRECT --to-ports 3129 -A POSTROUTING -j MASQUERADE COMMIT #
> Completed on Fri Mar 13 16:04:02 2015 # Generated by iptables-save
> v1.4.7 on Fri Mar 13 16:04:02 2015 *filter :INPUT ACCEPT [0:0] 
> :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [1818:649971] -A INPUT -m
> state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j
> REJECT --reject-with icmp-port-unreachable -A INPUT -i lo -j
> ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j
> ACCEPT -A INPUT -i eth0 -p tcp -m tcp --dport 3129 -m state
> --state NEW,ESTABLISHED -j ACCEPT -A INPUT -i eth0 -p tcp -m tcp
> --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT -A INPUT -j
> REJECT --reject-with icmp-host-prohibited -A FORWARD -j REJECT
> --reject-with icmp-host-prohibited COMMIT # Completed on Fri Mar 13
> 16:04:02 2015 # Generated by iptables-save v1.4.7 on Fri Mar 13
> 16:04:02 2015 *mangle :PREROUTING ACCEPT [68:6199] :INPUT ACCEPT
> [68:6199] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [26:3064] 
> :POSTROUTING ACCEPT [26:3064] -A PREROUTING -p tcp -m tcp --dport
> 3129 -j DROP COMMIT # Completed on Fri Mar 13 16:04:02 2015
> 
> 
> Accessing sites, shows the IP address of the proxy 147.245.252.13.
> 
> Am I missing something in IPTables that it is not caching?
> 
> 
> Thanks Monah
> 
> On Fri, Mar 6, 2015 at 11:26 PM, Amos Jeffries
> <squid3 at treenet.co.nz> wrote:
> 
>> On 6/03/2015 1:19 a.m., Monah Baki wrote:
>>> Hi all, can anyone verify if this is correct, need to make ure
>>> that users will be able to access the internet via the squid.
>>> 
>>> Running FreeBSD with a single interface with Squid-3.5.2
>>> 
>>> Policy based routing on Cisco with the following:
>>> 
>>> 
>>> interface GigabitEthernet0/0/1.1
>>> 
>>> encapsulation dot1Q 1 native
>>> 
>>> ip address 10.0.0.9 255.255.255.0
>>> 
>>> no ip redirects
>>> 
>>> no ip unreachables
>>> 
>>> ip nat inside
>>> 
>>> standby 1 ip 10.0.0.10
>>> 
>>> standby 1 priority 120
>>> 
>>> standby 1 preempt
>>> 
>>> standby 1 name HSRP
>>> 
>>> ip policy route-map CFLOW
>>> 
>>> 
>>> 
>>> ip access-list extended REDIRECT
>>> 
>>> deny   tcp host 10.0.0.24 any eq www
>>> 
>>> permit tcp host 10.0.0.23 any eq www
>>> 
>>> 
>>> 
>>> route-map CFLOW permit 10
>>> 
>>> match ip address REDIRECT set ip next-hop 10.0.0.24
>>> 
>>> In my /etc/pf.conf rdr pass inet proto tcp from 10.0.0.0/8 to
>>> any port 80 -> 10.0.0.24 port 3129
>>> 
>>> # block in pass in log quick on bge0 pass out log quick on
>>> bge0 pass out keep state
>>> 
>>> and finally in my squid.conf: http_port 3128 http_port 3129
>>> intercept
>>> 
>>> 
>>> 
>>> And for testing purposes from the squid server: ./squidclient
>>> -h 10.0.0.24 -p 3128 http://www.freebsd.org/
>>> 
>>> If I replace -p 3128 with -p 80, I get a access denied, and if
>>> I omit the -p 3128 completely, I can access the websites.
>> 
>> If you omit the -p entirely squidclient assumes "-p 3128" (the
>> proxy default listening port), so it works exactly the same as if
>> you had used -p 3128 explicitly.
>> 
>> If you use -p 80 you also need to change the pther parameters so
>> they generate port-80 syntax message: - the -h with IP or
>> hostname of the remote web server, and - the URL parameters being
>> a relative URL, and - the -j parameter with Host: header domain
>> name of the server ... eg. squidclient -h www.freebsd.org -j
>> www.freebsd.org -p 80 /
>> 
>> NP: if your squidclient is too old to support -j, use this
>> instead: -H 'Host: www.freebsd.org\n'
>> 
>> ** this test should work from the squid box without having gone
>> through the proxy. Only from the client machine should it work
>> *with* NAT passing it through the proxy.
>> 
>> 
>> 
>> Using a proxy syntax message sent directly to the proxy receiving
>> port, or with the proxy as receiving IP on port 80 (NAT'ed to
>> Squid) is a guaranted forwarding loop failure.
>> 
>> 
>> That doesn't fix your clients issue, but hopefully makes it clear
>> that the above desribed test is broken enough to prevent you
>> identifying when the client issue is fixed if that happens on
>> some change.
>> 
>> Amos _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVAw3gAAoJENNXIZxhPexGk4EH/2YErYeV3IcEOyngCUHZJbyk
5sY2bMrA+0kpxTa9YQsVzC9QuULvh7NLbT/1J1Tx7k0CYkM+9T1qTjx6WPmHHE4x
GrsrW6qBVM2t0zgHcd4d5BQqDqs03F5fNkEINgufdMaBAkazr7YMWNciaX6j/36Y
BWFKFPB8BJNRbbamEqTrtL0r0qntNRXrBQjlP52PzXpqnnREn8n/mwLPn3wlTQl5
17HbvXBhgliyypIoitNGGWbM2SNdJSkrR0DqrM5SNfjPX9Ffm6FBRM2obA+TNl/q
j3elyeu/QHZhbxfJJmZYsJB+B2Q7dQXVvm37LVpRc2wHF6nUNEgmsjcG9Y98xqc=
=Dg4r
-----END PGP SIGNATURE-----


From dweimer at dweimer.net  Fri Mar 13 16:19:07 2015
From: dweimer at dweimer.net (dweimer)
Date: Fri, 13 Mar 2015 11:19:07 -0500
Subject: [squid-users] Reverse Proxy Funny Logging Issue
In-Reply-To: <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>
References: <0113b4b74c8c9e0a05cd692641aecf25@dweimer.net>
 <50FF7561.6050301@treenet.co.nz>
 <88bdb4a5d0b47bb9e459b91b079b7425@dweimer.net>
 <8a05b3c589b46021b42d5a59a7b11e8a@dweimer.net>
 <5100BB15.3030502@treenet.co.nz>
 <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>
Message-ID: <2be09011d9c5f16220f0c781c793e242@dweimer.net>

On 03/12/2015 10:31 am, dweimer wrote:
> On 01/23/2013 10:39 pm, Amos Jeffries wrote:
>> On 24/01/2013 4:13 a.m., dweimer wrote:
>>> On 2013-01-23 08:40, dweimer wrote:
>>>> On 2013-01-22 23:30, Amos Jeffries wrote:
>>>>> On 23/01/2013 5:34 a.m., dweimer wrote:
>>>>>> I just upgraded my reverse proxy server last night from 3.1.20 to 
>>>>>> 3.2.6, all is working well except one of my log rules, and I can't 
>>>>>> figure out why.
>>>>> 
>>>>> Please run "squid -k parse" and resolve the WARNING or ERROR which 
>>>>> are listed.
>>>>> 
>>>>> There are two possible reasons...
>>>>> 
>>>>>> 
>>>>>> I have a several sites behind the server, with dstdomain access 
>>>>>> rules setup.
>>>>>> 
>>>>>> acl website1 dstdomain www.website1.com
>>>>>> acl website2 dstdomain www.website2.com
>>>>>> acl website2 dstdomain www.website3.com
>>>>> 
>>>>> Possible reason #1 (assuming thi is an accurate copy-n-paste from
>>>>> yoru config file).....  you have no website3 ACL definition?
>>>> 
>>>> That was a typo in the email, correct ACL is in the configuration,
>>>> squid -k parse outputs no warnings or errors.
>>>> 
>>>>> 
>>>>>> ...
>>>>>> 
>>>>>> Followed by the access rules
>>>>>> 
>>>>>> http_access allow website1
>>>>>> http_access allow website2
>>>>>> http_access allow website3
>>>>>> ...
>>>>>> http_access deny all
>>>>>> 
>>>>>> Some are using rewrites
>>>>>> url_rewrite_program /usr/local/etc/squid/url_rewrite.py
>>>>>> url_rewrite_children 20
>>>>>> url_rewrite_access allow website1
>>>>>> url_rewrite_access allow website3
>>>>>> ...
>>>>>> url_rewrite_access deny all
>>>>>> 
>>>>>> Then my access logs
>>>>>> 
>>>>>> # First I grab everything in one
>>>>>> access_log daemon:/var/log/squid/access.log squid all
>>> 
>>>>> 
>>>>>> access_log daemon:/var/log/squid/website1.log combined website1
>>>>>> access_log daemon:/var/log/squid/website2.log combined website2
>>>>>> access_log daemon:/var/log/squid/website3.log combined website3
>>>>>> ...
>>>>>> 
>>>>>> everything works, write down to one of the access logs, the data 
>>>>>> shows up in the access.log file, the data shows up in the 
>>>>>> individual logs for all the others, except that one.  If we use 
>>>>>> website3 from the above example like my actual file the access 
>>>>>> rule works on the url_rewrite_access allow line, but for some 
>>>>>> reason is failing on the log line.  squid -k parse doesn't show 
>>>>>> any errors, and shows a Processing: access_log 
>>>>>> daemon:/var/log/squid/website3.log combined website3 line in the 
>>>>>> output.
>>>>>> 
>>>>>> The log in question was originally at the end of my access_log 
>>>>>> list section, so I changed the order around to see if for some 
>>>>>> reason it was only the last one not working, no change still only 
>>>>>> that one not working, And the new last one in the list still works 
>>>>>> as expected.
>>>>>> 
>>>>>> I know the ACL is working as it works correctly on the rewrite 
>>>>>> rule and the http access just above the log rules, anyone have any 
>>>>>> ideas on how I can figure out why the log entry isn't working?
>>>> 
>>> 
>>> Changed lines back to daemon, changed acl on logs to the rewrite side 
>>> used on the cache_peer_access lines later in the configuration.  
>>> Works now, and logs even show up with the pre-rewrite rule host 
>>> information...
>>> 
>>> That does make me wonder why some lines were getting logged but not 
>>> all, the sites I thought were working do have higher usage, maybe I 
>>> was still missing a lot from them, and just not knowing it.  I guess 
>>> I will see if my webalizer reports show a huge gain in hit count over 
>>> the old records from the the 3.1.20 installation, of if this behavior 
>>> is only evident in the 3.2 branch.
>>> 
>> 
>> I think you will find that the lines being logged previously were on
>> the requests which were either not rewritten at all or were re-written
>> from another requests URL which was being logged.
>> 
>> Each of the ACL-driven directive labels in squid.conf is effectively
>> an event trigger script - deciding whether or not to perform some
>> action. This only makes sense testing when that action choice is
>> requried.  Squid processing pathway checks http_access first, ... then
>> some others, ... then url_rewriting, ... then the destination
>> selection (cache_peer and others), ... then when the transaction is
>> fully completed access_log output decision are done.
>> 
>> Amos
> 
> Last night I applied the FreeBSD 10.1-RELEASE-p6 Update and Upgraded
> the ports which included Squid 3.4.12, I enabled the LAX HTTP option
> in the ports configuration with adds the --enable-http-violations
> compile option. With the intention to enable broken_posts option in
> the configuration. I will hopefully be able to apply any necessary
> changes to the production system after I test them now.
> When doing this update I did have a thought the system is running in a
> FreeBSD jail and not on the base system is there a chance this issue
> is caused by running within a jail? curious if anyone has ran into
> specific issues running Squid in a FreeBSD jail before?

Well I am at a loss, debugging hasn't led to anything more than a 
timeout occurs. I was able to create a test PHP form to upload files on 
an Apache server and upload up to a 264MB file. I didn't try any larger 
files though I suspect it would work up to the configured 1024MB that I 
had Apache configured for. So its not all HTTPS only those files going 
to our OWA and Sharepoint servers. The only settings I can find that 
changes the behavior at all is to change the "write_timeout" to 
something smaller, like 45 seconds and then it errors sooner instead of 
taking forever to give up.

I tried uninstalling the Squid 3.4 FreeBSD port and using the 3.3 port 
instead on the test system, no change. I also tried installing 3.5 from 
source using the same configure options that my 3.4 port returned with 
squid -v, again no change.

I have verified that the IIS logs show a client request timeout has 
occurred, the broken_posts allow didn't create any change in behavior. I 
do know that if I point the browser directly to the Exchange server it 
works, so its only broken going through the reverse the proxy. If I 
point the browser through a forwarding Squid proxy that knows how to 
talk directly to the exchange server instead of the reverse proxy it 
works with no special settings. If I post a large debugging file to a 
website do I have any volunteers to look at it and see if they can see 
what's going on?


-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From johnzeng2013 at yahoo.com  Fri Mar 13 16:37:28 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 00:37:28 +0800
Subject: [squid-users] i hope to use external ACL + ldap at squid 3.5.2,
 but i don't find ext_ldap_group_acl and basic_ldap_auth from
 /squid/libexec/
Message-ID: <55031248.8080700@yahoo.com>


Hello All:

i hope to use external ACL + ldap at squid 3.5.2, but i don't find
ext_ldap_group_acl and basic_ldap_auth from /squid/libexec/

if possible , please give me some advisement . Thanks

This is my config


configure options: '--prefix=/accerater/webcache3'
'--enable-follow-x-forwarded-for' '--enable-snmp'
'--enable-linux-netfilter' '--enable-storeio=aufs,rock'
'--enable-wccpv2' '--with-large-files'
'--enable-removal-policies=lru,heap' '--enable-async-io=128'
'--enable-http-violations' '--enable-default-err-language=English'
'--enable-err-languages=English' '--enable-referer-log'
'--enable-useragent-log' '--with-maxfd=65536'
'--enable-large-cache-files' '--enable-delay-pools'
'--enable-forward-log' '--with-pthreads' 'LIBS=-ltcmalloc'
'--disable-internal-dns' '--enable-url-rewrite-helpers'
'--enable-log-daemon-helpers' '--enable-epoll'
'--enable-ltdl-convenience' '--with-included-ltdl'
'--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped'

This is full file at /squid/libexec


basic_db_auth basic_ncsa_auth basic_smb_auth digest_file_auth
ext_unix_group_acl log_file_daemon storeid_file_rewrite
basic_fake_auth basic_nis_auth basic_smb_auth.sh ext_delayer_acl
ext_wbinfo_group_acl negotiate_wrapper_auth unlinkd
basic_getpwnam_auth basic_pop3_auth basic_smb_lm_auth
ext_file_userip_acl helper-mux.pl ntlm_fake_auth url_fake_rewrite
basic_msnt_multi_domain_auth basic_radius_auth cachemgr.cgi
ext_sql_session_acl log_db_daemon ntlm_smb_lm_auth url_fake_rewrite.sh


From monahbaki at gmail.com  Fri Mar 13 16:47:44 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 13 Mar 2015 12:47:44 -0400
Subject: [squid-users] squid intercept config
In-Reply-To: <55030DE0.60606@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
 <55030DE0.60606@gmail.com>
Message-ID: <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /usr/local/squid/var/cache/squid 350000 16 256


#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

half_closed_clients off
quick_abort_min 0 KB
quick_abort_max 0 KB
vary_ignore_expire on
reload_into_ims on
memory_pools off
cache_mem 4096 MB
visible_hostname isn-phc-cache
minimum_object_size 0 bytes
maximum_object_size 512 MB
maximum_object_size 512 KB
ipcache_size 1024
ipcache_low 90
ipcache_high 95
cache_swap_low 98
cache_swap_high 100
fqdncache_size 16384
retry_on_error on
offline_mode off
logfile_rotate 10
dns_nameservers 8.8.8.8 41.78.211.30




access.log:

1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.211    198 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.211    198 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.223    301 10.0.0.23 TCP_MISS/200 222 GET
http://rma-api.gravity.com/v1/beacons/log? - ORIGINAL_DST/80.239.148.18
text/html
1426267535.244    195 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET
http://hpr.outbrain.com/utils/get? - ORIGINAL_DST/50.31.185.42 text/x-json
1426267535.345    412 10.0.0.23 TCP_MISS/200 11179 GET
http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40 text/javascript
1426267535.346    411 10.0.0.23 TCP_MISS/200 423 GET
http://t1.visualrevenue.com/? - ORIGINAL_DST/64.74.232.44 image/gif
1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327 GET
http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
- ORIGINAL_DST/80.239.152.153 application/x-javascript
1426267535.381    193 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.406    189 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.408    190 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.408    191 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.418    200 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.437    188 10.0.0.23 TCP_MISS/200 431 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.464    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327 GET
http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/1.3.4/js/player/CNNAPIVideoPlayer.js
- ORIGINAL_DST/80.239.152.153 application/x-javascript
1426267535.494    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327 GET
http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/1.3.4/js/legacy/CNNVideoPlayer.js
- ORIGINAL_DST/80.239.152.153 application/x-javascript
1426267535.604    217 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.609    256 10.0.0.23 TCP_REFRESH_UNMODIFIED/200 41017 GET
http://cdn.gigya.com/js/gigya.js? - ORIGINAL_DST/80.239.148.17
text/javascript
1426267535.619    206 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.622    208 10.0.0.23 TCP_MISS/200 412 GET
http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
image/gif
1426267535.696    129 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 312 GET
http://z.cdn.turner.com/cnn/.element/img/3.0/video/cnn_embedDefault.png -
ORIGINAL_DST/80.239.152.153 image/png
1426267536.071    656 10.0.0.23 TCP_MISS/302 849 GET
http://metrics.cnn.com/b/ss/cnn-adbp-domestic/1/H.26.1/s11300422861240? -
ORIGINAL_DST/66.235.141.144 text/plain
1426267536.075    257 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 348 GET
http://cdn.gigya.com/js/gigya.services.plugins.base.min.js? - ORIGINAL_DST/
80.239.148.17 text/javascript
1426267536.203    128 10.0.0.23 TCP_MISS/200 381 GET
http://b.scorecardresearch.com/r? - ORIGINAL_DST/80.239.148.16 image/gif
1426267536.570    393 10.0.0.23 TCP_MISS/304 338 GET
http://cdn3.gigya.com/js/gigya.services.socialize.plugins.simpleshare.min.js
- ORIGINAL_DST/80.239.148.32 text/javascript
1426267536.746    125 10.0.0.23 TCP_MISS/304 340 GET
http://static.chartbeat.com/js/chartbeat.js - ORIGINAL_DST/23.67.1.243
application/x-javascript
1426267536.819    199 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 233 GET
http://data.cnn.com/jsonp/video/nowPlayingSchedule.json? - ORIGINAL_DST/
157.166.238.237 -
1426267536.942    260 10.0.0.23 TCP_MISS/200 677 GET
http://beacon.krxd.net/optout_check? - ORIGINAL_DST/176.34.190.30
text/javascript
1426267537.027    236 10.0.0.23 TCP_MISS/200 758 GET http://t.co/i/adsct? -
ORIGINAL_DST/199.16.156.11 image/gif
1426267537.146    362 10.0.0.23 TCP_MISS/200 758 GET http://t.co/i/adsct? -
ORIGINAL_DST/199.16.156.11 image/gif
1426267537.171    388 10.0.0.23 TCP_MISS/200 758 GET http://t.co/i/adsct? -
ORIGINAL_DST/199.16.156.11 image/gif
1426267537.230    432 10.0.0.23 TCP_MISS/302 481 GET
http://apiservices.krxd.net/um? - ORIGINAL_DST/54.243.83.18 text/html
1426267537.603    173 10.0.0.23 TCP_MISS/204 676 GET
http://beacon.krxd.net/pixel.gif? - ORIGINAL_DST/176.34.190.30 image/gif
1426267537.618    247 10.0.0.23 TCP_MISS/200 322 GET
http://ping.chartbeat.net/ping? - ORIGINAL_DST/54.235.85.218 image/gif
1426267537.892    388 10.0.0.23 TCP_MISS/200 68649 GET
http://z.cdn.turner.com/xslo/cvp/core/base/0/CVPBase.swf? - ORIGINAL_DST/
80.239.152.153 application/x-shockwave-flash
1426267538.024    130 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 329 GET
http://js.moatads.com/turner763610601596/moatad.js - ORIGINAL_DST/
80.239.148.9 application/x-javascript

On Fri, Mar 13, 2015 at 12:18 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
> 13.03.15 21:58, Monah Baki ?????:
> > Hi All,
> >
> > Installed squid on CentOS 6.6 and it's working, but mY access.log
> > shows all TCP_MISS and no TCP_HIT. The following config:
> >
> > squid.conf # Squid normally listens to port 3128 http_port 3128
> > http_port 3129 intercept
>
> And that's all????
>
> >
> >
> >
> > iptables
> >
> > # Generated by iptables-save v1.4.7 on Fri Mar 13 16:04:02 2015
> > *nat :PREROUTING ACCEPT [10:2031] :POSTROUTING ACCEPT [0:0] :OUTPUT
> > ACCEPT [0:0] -A PREROUTING -s 147.245.252.13/32 -p tcp -m tcp
> > --dport 80 -j ACCEPT -A PREROUTING -s 10.0.0.24/32 -p tcp -m tcp
> > --dport 80 -j ACCEPT -A PREROUTING -s 147.245.252.13/32 -p tcp -m
> > tcp --dport 80 -j ACCEPT -A PREROUTING -p tcp -m tcp --dport 80 -j
> > REDIRECT --to-ports 3129 -A POSTROUTING -j MASQUERADE COMMIT #
> > Completed on Fri Mar 13 16:04:02 2015 # Generated by iptables-save
> > v1.4.7 on Fri Mar 13 16:04:02 2015 *filter :INPUT ACCEPT [0:0]
> > :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [1818:649971] -A INPUT -m
> > state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j
> > REJECT --reject-with icmp-port-unreachable -A INPUT -i lo -j
> > ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j
> > ACCEPT -A INPUT -i eth0 -p tcp -m tcp --dport 3129 -m state
> > --state NEW,ESTABLISHED -j ACCEPT -A INPUT -i eth0 -p tcp -m tcp
> > --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT -A INPUT -j
> > REJECT --reject-with icmp-host-prohibited -A FORWARD -j REJECT
> > --reject-with icmp-host-prohibited COMMIT # Completed on Fri Mar 13
> > 16:04:02 2015 # Generated by iptables-save v1.4.7 on Fri Mar 13
> > 16:04:02 2015 *mangle :PREROUTING ACCEPT [68:6199] :INPUT ACCEPT
> > [68:6199] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [26:3064]
> > :POSTROUTING ACCEPT [26:3064] -A PREROUTING -p tcp -m tcp --dport
> > 3129 -j DROP COMMIT # Completed on Fri Mar 13 16:04:02 2015
> >
> >
> > Accessing sites, shows the IP address of the proxy 147.245.252.13.
> >
> > Am I missing something in IPTables that it is not caching?
> >
> >
> > Thanks Monah
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/3edb21ad/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Mar 13 17:15:25 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 13 Mar 2015 18:15:25 +0100
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
Message-ID: <201503131815.25685.Antony.Stone@squid.open.source.it>

On Friday 13 March 2015 at 17:47:44 (EU time), Monah Baki wrote:

> acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> #http_access deny to_localhost
> 
> http_access allow localnet
> http_access allow localhost

You've got the standard references here (and above, for cache manager access) 
for localhost, and yet I don't see it defined anywhere - have you deliberately 
removed it?

> http_access deny all
> 
> http_port 3128
> http_port 3129 intercept
> 
> cache_dir ufs /usr/local/squid/var/cache/squid 350000 16 256
> 
> 
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern .        0    20%    4320
> 
> half_closed_clients off
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> vary_ignore_expire on
> reload_into_ims on
> memory_pools off
> cache_mem 4096 MB
> visible_hostname isn-phc-cache
> minimum_object_size 0 bytes
> maximum_object_size 512 MB
> maximum_object_size 512 KB
> ipcache_size 1024
> ipcache_low 90
> ipcache_high 95
> cache_swap_low 98
> cache_swap_high 100
> fqdncache_size 16384
> retry_on_error on
> offline_mode off
> logfile_rotate 10
> dns_nameservers 8.8.8.8 41.78.211.30

> access.log:
> 
> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
> http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
> image/gif

I see quote a lot of entries in your access log for things to do with advert 
servers.  Are you certain that these objects haven't been marked by the server 
as "nocache" or similar?

Try accessing something simple and plain, such as the squid project home page 
http://www.squid-cache.org/ and see what shows up in your access log.

Also, try configuring a browser to use the proxy listening on port 3128 and see 
if that starts showing you cache hits.


Regards,


Antony.

-- 
Late in 1972 President Richard Nixon announced that the rate of increase of 
inflation was decreasing.   This was the first time a sitting president used a 
third derivative to advance his case for re-election.

 - Hugo Rossi, Notices of the American Mathematical Society

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Fri Mar 13 17:23:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 06:23:15 +1300
Subject: [squid-users] squid intercept config
In-Reply-To: <201503131815.25685.Antony.Stone@squid.open.source.it>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
 <201503131815.25685.Antony.Stone@squid.open.source.it>
Message-ID: <55031D03.4000605@treenet.co.nz>

On 14/03/2015 6:15 a.m., Antony Stone wrote:
> On Friday 13 March 2015 at 17:47:44 (EU time), Monah Baki wrote:
>>
>> http_access allow localhost manager
>> http_access deny manager
>>
>> #http_access deny to_localhost
>>
>> http_access allow localnet
>> http_access allow localhost
> 
> You've got the standard references here (and above, for cache manager access) 
> for localhost, and yet I don't see it defined anywhere - have you deliberately 
> removed it?

Current Squid versions define those ACLs automatically.

Amos



From monahbaki at gmail.com  Fri Mar 13 17:27:55 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 13 Mar 2015 13:27:55 -0400
Subject: [squid-users] squid intercept config
In-Reply-To: <55031D03.4000605@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
 <201503131815.25685.Antony.Stone@squid.open.source.it>
 <55031D03.4000605@treenet.co.nz>
Message-ID: <CALP3=x8_Xnhj_AdcJFODOG=A46VQjZav9q+tewCPVZG6cU2L2w@mail.gmail.com>

It's working now, all I did is rem'd the following:

# half_closed_clients off
# quick_abort_min 0 KB
# quick_abort_max 0 KB
# vary_ignore_expire on
# reload_into_ims on
# memory_pools off
# cache_mem 4096 MB
# # memory_cache_shared on
visible_hostname isn-phc-cache
minimum_object_size 0 bytes
maximum_object_size 512 MB
maximum_object_size 512 KB
ipcache_size 1024
# ipcache_low 90
# ipcache_high 95
cache_swap_low 98
cache_swap_high 100
# fqdncache_size 16384
# retry_on_error on
# offline_mode off
logfile_rotate 10
dns_nameservers 8.8.8.8 41.78.211.30

I can see tcp_hits.

Note to self, something I do not know, don't add it.


On Fri, Mar 13, 2015 at 1:23 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/03/2015 6:15 a.m., Antony Stone wrote:
> > On Friday 13 March 2015 at 17:47:44 (EU time), Monah Baki wrote:
> >>
> >> http_access allow localhost manager
> >> http_access deny manager
> >>
> >> #http_access deny to_localhost
> >>
> >> http_access allow localnet
> >> http_access allow localhost
> >
> > You've got the standard references here (and above, for cache manager
> access)
> > for localhost, and yet I don't see it defined anywhere - have you
> deliberately
> > removed it?
>
> Current Squid versions define those ACLs automatically.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/db5ca771/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 13 17:33:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 06:33:17 +1300
Subject: [squid-users] squid intercept config
In-Reply-To: <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
Message-ID: <55031F5D.7030107@treenet.co.nz>

On 14/03/2015 5:47 a.m., Monah Baki wrote:

<snip>

> half_closed_clients off
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> vary_ignore_expire on
> reload_into_ims on
> memory_pools off
> cache_mem 4096 MB
> visible_hostname isn-phc-cache
> minimum_object_size 0 bytes

> maximum_object_size 512 MB
> maximum_object_size 512 KB

KB value overwriting MB value.


> ipcache_size 1024
> ipcache_low 90
> ipcache_high 95
> cache_swap_low 98
> cache_swap_high 100
> fqdncache_size 16384
> retry_on_error on
> offline_mode off
> logfile_rotate 10
> dns_nameservers 8.8.8.8 41.78.211.30
> 
> 
> 
> 
> access.log:
> 
> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
> http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
> image/gif
> 1426267535.211    198 10.0.0.23 TCP_MISS/200 412 GET
> http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
> image/gif
> 1426267535.211    198 10.0.0.23 TCP_MISS/200 412 GET
> http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
> image/gif
> 1426267535.223    301 10.0.0.23 TCP_MISS/200 222 GET
> http://rma-api.gravity.com/v1/beacons/log? - ORIGINAL_DST/80.239.148.18
> text/html
> 1426267535.244    195 10.0.0.23 TCP_MISS/200 412 GET
> http://jadserve.postrelease.com/trk.gif? - ORIGINAL_DST/54.225.133.227
> image/gif


Lots of Akamai hosted requests. Akamai play tricks with DNS responses.

Check your cache.log for security warnings;
 <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

Note that objects failing the Host validation are not cacheable.


> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET
> http://hpr.outbrain.com/utils/get? - ORIGINAL_DST/50.31.185.42 text/x-json
> 1426267535.345    412 10.0.0.23 TCP_MISS/200 11179 GET
> http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40 text/javascript
> 1426267535.346    411 10.0.0.23 TCP_MISS/200 423 GET
> http://t1.visualrevenue.com/? - ORIGINAL_DST/64.74.232.44 image/gif

Not sure about them. Maybe genuine MISS, maybe not.

It could also be the issues Antony pointed out, with the objects just
naturally not being cacheable.


> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327 GET
> http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
> - ORIGINAL_DST/80.239.152.153 application/x-javascript

There is a hit.

I guess you are new to Squid-3 ?
 Squid is HTTP/1.1 compliant now and the caching rules are slightly
different from requirements on HTTP/1.0 software. A lot of content that
previously could not be stored now can (authenticated, private,
no-cache, etc.). But being sensitive info also requires revalidation in
order to be used, so they show up like the above.

Amos



From yvoinov at gmail.com  Fri Mar 13 18:43:54 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Mar 2015 00:43:54 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <55031F5D.7030107@treenet.co.nz>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
 <55031F5D.7030107@treenet.co.nz>
Message-ID: <55032FEA.5060205@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



13.03.15 23:33, Amos Jeffries ?????:
> On 14/03/2015 5:47 a.m., Monah Baki wrote:
> 
> <snip>
> 
>> half_closed_clients off quick_abort_min 0 KB quick_abort_max 0
>> KB vary_ignore_expire on reload_into_ims on memory_pools off 
>> cache_mem 4096 MB visible_hostname isn-phc-cache 
>> minimum_object_size 0 bytes
> 
>> maximum_object_size 512 MB maximum_object_size 512 KB
> 
> KB value overwriting MB value.
> 
> 
>> ipcache_size 1024 ipcache_low 90 ipcache_high 95 cache_swap_low
>> 98 cache_swap_high 100 fqdncache_size 16384 retry_on_error on 
>> offline_mode off logfile_rotate 10 dns_nameservers 8.8.8.8
>> 41.78.211.30
>> 
>> 
>> 
>> 
>> access.log:
>> 
>> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET 
>> http://jadserve.postrelease.com/trk.gif? -
>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211    198
>> 10.0.0.23 TCP_MISS/200 412 GET 
>> http://jadserve.postrelease.com/trk.gif? -
>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211    198
>> 10.0.0.23 TCP_MISS/200 412 GET 
>> http://jadserve.postrelease.com/trk.gif? -
>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.223    301
>> 10.0.0.23 TCP_MISS/200 222 GET 
>> http://rma-api.gravity.com/v1/beacons/log? -
>> ORIGINAL_DST/80.239.148.18 text/html 1426267535.244    195
>> 10.0.0.23 TCP_MISS/200 412 GET 
>> http://jadserve.postrelease.com/trk.gif? -
>> ORIGINAL_DST/54.225.133.227 image/gif
> 
> 
> Lots of Akamai hosted requests. Akamai play tricks with DNS
> responses.
In my installation I've used local Unbound DNS cache and, before it,
forced DNS interception to him with Cisco. :)

So, I don't care about any hosts DNS quirks. ;)

> 
> Check your cache.log for security warnings; 
> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
> 
> Note that objects failing the Host validation are not cacheable.
> 
> 
>> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET 
>> http://hpr.outbrain.com/utils/get? - ORIGINAL_DST/50.31.185.42
>> text/x-json 1426267535.345    412 10.0.0.23 TCP_MISS/200 11179
>> GET http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40
>> text/javascript 1426267535.346    411 10.0.0.23 TCP_MISS/200 423
>> GET http://t1.visualrevenue.com/? - ORIGINAL_DST/64.74.232.44
>> image/gif
> 
> Not sure about them. Maybe genuine MISS, maybe not.

Agressive dynamic content caching requires some special tweaks. ;)

> 
> It could also be the issues Antony pointed out, with the objects
> just naturally not being cacheable.
> 
> 
>> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327
>> GET 
>> http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
>>
>> 
- - ORIGINAL_DST/80.239.152.153 application/x-javascript
> 
> There is a hit.
> 
> I guess you are new to Squid-3 ? Squid is HTTP/1.1 compliant now
> and the caching rules are slightly different from requirements on
> HTTP/1.0 software. A lot of content that previously could not be
> stored now can (authenticated, private, no-cache, etc.). But being
> sensitive info also requires revalidation in order to be used, so
> they show up like the above.
> 
> Amos
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVAy/qAAoJENNXIZxhPexGOUEH/2yt1ql+ndo1We1E06LvIZl7
4PXY1kzuHT6EpOYO9LpLKtE+dPNYJuHKiUEF2hAGz5DP/heKq8PFRBTkMD18sueN
jm+UfP8BdxgRYuiQWtWNteV0gbH4nOBeJ6QwqlEHMwcsdPtkwWCGA0MS6co+IXKb
poouP6xQoNddx/UKicu6PQZDj5HRmynTP2c0mJuFEdlQxONgFiP4mqSFBwWhH/B/
hhdSfxg53xfQ+2B5TsVrKyxmJoIYpHgFZid/pk+Q2bb0WIy8bhHA72EHPjIu5K5Z
wobLGng+oE0i2erqtZiFR8daGdKcRW7FDYzHi+LJEHJj3i+z0mRIQkGTn3Nxfhg=
=Cnai
-----END PGP SIGNATURE-----


From alberto2perez at gmail.com  Fri Mar 13 19:52:19 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Fri, 13 Mar 2015 15:52:19 -0400
Subject: [squid-users] squid intercept config
In-Reply-To: <55032FEA.5060205@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
 <55031F5D.7030107@treenet.co.nz> <55032FEA.5060205@gmail.com>
Message-ID: <CAMZauGp8w-Gu_oS6rL6G94-QKd_114iaTh8=zX2f9DLna5=coQ@mail.gmail.com>

Can you share more details about "Agressive dynamic content caching
requires some special tweaks" I am very interested.

Thanks



On 3/13/15, Yuri Voinov <yvoinov at gmail.com> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
> 13.03.15 23:33, Amos Jeffries ?????:
>> On 14/03/2015 5:47 a.m., Monah Baki wrote:
>>
>> <snip>
>>
>>> half_closed_clients off quick_abort_min 0 KB quick_abort_max 0
>>> KB vary_ignore_expire on reload_into_ims on memory_pools off
>>> cache_mem 4096 MB visible_hostname isn-phc-cache
>>> minimum_object_size 0 bytes
>>
>>> maximum_object_size 512 MB maximum_object_size 512 KB
>>
>> KB value overwriting MB value.
>>
>>
>>> ipcache_size 1024 ipcache_low 90 ipcache_high 95 cache_swap_low
>>> 98 cache_swap_high 100 fqdncache_size 16384 retry_on_error on
>>> offline_mode off logfile_rotate 10 dns_nameservers 8.8.8.8
>>> 41.78.211.30
>>>
>>>
>>>
>>>
>>> access.log:
>>>
>>> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
>>> http://jadserve.postrelease.com/trk.gif? -
>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211    198
>>> 10.0.0.23 TCP_MISS/200 412 GET
>>> http://jadserve.postrelease.com/trk.gif? -
>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211    198
>>> 10.0.0.23 TCP_MISS/200 412 GET
>>> http://jadserve.postrelease.com/trk.gif? -
>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.223    301
>>> 10.0.0.23 TCP_MISS/200 222 GET
>>> http://rma-api.gravity.com/v1/beacons/log? -
>>> ORIGINAL_DST/80.239.148.18 text/html 1426267535.244    195
>>> 10.0.0.23 TCP_MISS/200 412 GET
>>> http://jadserve.postrelease.com/trk.gif? -
>>> ORIGINAL_DST/54.225.133.227 image/gif
>>
>>
>> Lots of Akamai hosted requests. Akamai play tricks with DNS
>> responses.
> In my installation I've used local Unbound DNS cache and, before it,
> forced DNS interception to him with Cisco. :)
>
> So, I don't care about any hosts DNS quirks. ;)
>
>>
>> Check your cache.log for security warnings;
>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>>
>> Note that objects failing the Host validation are not cacheable.
>>
>>
>>> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET
>>> http://hpr.outbrain.com/utils/get? - ORIGINAL_DST/50.31.185.42
>>> text/x-json 1426267535.345    412 10.0.0.23 TCP_MISS/200 11179
>>> GET http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40
>>> text/javascript 1426267535.346    411 10.0.0.23 TCP_MISS/200 423
>>> GET http://t1.visualrevenue.com/? - ORIGINAL_DST/64.74.232.44
>>> image/gif
>>
>> Not sure about them. Maybe genuine MISS, maybe not.
>
> Agressive dynamic content caching requires some special tweaks. ;)
>
>>
>> It could also be the issues Antony pointed out, with the objects
>> just naturally not being cacheable.
>>
>>
>>> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304 327
>>> GET
>>> http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
>>>
>>>
> - - ORIGINAL_DST/80.239.152.153 application/x-javascript
>>
>> There is a hit.
>>
>> I guess you are new to Squid-3 ? Squid is HTTP/1.1 compliant now
>> and the caching rules are slightly different from requirements on
>> HTTP/1.0 software. A lot of content that previously could not be
>> stored now can (authenticated, private, no-cache, etc.). But being
>> sensitive info also requires revalidation in order to be used, so
>> they show up like the above.
>>
>> Amos
>>
>> _______________________________________________ squid-users mailing
>> list squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJVAy/qAAoJENNXIZxhPexGOUEH/2yt1ql+ndo1We1E06LvIZl7
> 4PXY1kzuHT6EpOYO9LpLKtE+dPNYJuHKiUEF2hAGz5DP/heKq8PFRBTkMD18sueN
> jm+UfP8BdxgRYuiQWtWNteV0gbH4nOBeJ6QwqlEHMwcsdPtkwWCGA0MS6co+IXKb
> poouP6xQoNddx/UKicu6PQZDj5HRmynTP2c0mJuFEdlQxONgFiP4mqSFBwWhH/B/
> hhdSfxg53xfQ+2B5TsVrKyxmJoIYpHgFZid/pk+Q2bb0WIy8bhHA72EHPjIu5K5Z
> wobLGng+oE0i2erqtZiFR8daGdKcRW7FDYzHi+LJEHJj3i+z0mRIQkGTn3Nxfhg=
> =Cnai
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From yvoinov at gmail.com  Fri Mar 13 20:01:44 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Mar 2015 02:01:44 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CAMZauGp8w-Gu_oS6rL6G94-QKd_114iaTh8=zX2f9DLna5=coQ@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>	<54FA7DED.9090902@treenet.co.nz>	<CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>	<55030DE0.60606@gmail.com>	<CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>	<55031F5D.7030107@treenet.co.nz>	<55032FEA.5060205@gmail.com>
 <CAMZauGp8w-Gu_oS6rL6G94-QKd_114iaTh8=zX2f9DLna5=coQ@mail.gmail.com>
Message-ID: <55034228.8030800@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

This is know-how to himself. ;)

To be serious,

you must carefully play with refresh_pattern(s), and some squid.conf
parameters (and also with store ID feature) to get higher HIT ratio.

Just for example (this is NOT complete config! No responsibility or
any guarantees in case of simple copy-n-pasted into your configs! This
is AS IS example!):

# Keep swf in cache even if asked not to
refresh_pattern -i \.(swf)(\?|$)	10080	90%	43200	override-expire
ignore-reload reload-into-ims ignore-private
# .NET cache
refresh_pattern -i \.(as(h|p)x?)(\?|$)	10080	90%	43200	reload-into-ims
# Updates: Windows, Adobe, Java
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)			4320
80% 43200	reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)	
4320 80% 43200	reload-into-ims
refresh_pattern -i
my.windowsupdate.website.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
4320 80% 43200	reload-into-ims
refresh_pattern -i adobe.com/.*\.(zip|exe)	4320	80%	43200	reload-into-ims
refresh_pattern -i java.com/.*\.(zip|exe)	4320	80%	43200	reload-into-ims
refresh_pattern -i sun.com/.*\.(zip|exe)	4320	80%	43200	reload-into-ims
refresh_pattern -i google\.com.*\.(zip|exe)	4320	80%	43200	reload-into-ims
refresh_pattern -i macromedia\.com.*\.(zip|exe)	4320	80%	43200
reload-into-ims
# Other long-lived items
refresh_pattern -i
\.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4)(\?|$)			14400
99%	518400	ignore-no-store override-expire ignore-reload
reload-into-ims ignore-private ignore-must-revalidate
refresh_pattern -i
\.((m?|x?|s?)htm(l?)|css|js|xml|php|json)(\?|$)			10080	90%	86400
ignore-no-store override-expire override-lastmod reload-into-ims
ignore-private ignore-must-revalidate
# Default patterns
refresh_pattern -i (/cgi-bin/|\?)	0	0%	0
refresh_pattern	.	0	20%	10080	override-lastmod reload-into-ims

The example above also requires some additional cached-related
parameters to be changed.

Also, you strictly recommended to research average users activity AND
play around VARY http headers.

And others.

Each squid setup is place-specific. And depending your access/deny
lists, security policy, users/network activity etc.etc.etc.

WBR, Yuri

PS. Your question has NO simple answer. Beware - copy-n-paste any
foreign config can not guarantee the same results for YOU.

14.03.15 1:52, Alberto Perez ?????:
> Can you share more details about "Agressive dynamic content
> caching requires some special tweaks" I am very interested.
> 
> Thanks
> 
> 
> 
> On 3/13/15, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> 
> 13.03.15 23:33, Amos Jeffries ?????:
>>>> On 14/03/2015 5:47 a.m., Monah Baki wrote:
>>>> 
>>>> <snip>
>>>> 
>>>>> half_closed_clients off quick_abort_min 0 KB
>>>>> quick_abort_max 0 KB vary_ignore_expire on reload_into_ims
>>>>> on memory_pools off cache_mem 4096 MB visible_hostname
>>>>> isn-phc-cache minimum_object_size 0 bytes
>>>> 
>>>>> maximum_object_size 512 MB maximum_object_size 512 KB
>>>> 
>>>> KB value overwriting MB value.
>>>> 
>>>> 
>>>>> ipcache_size 1024 ipcache_low 90 ipcache_high 95
>>>>> cache_swap_low 98 cache_swap_high 100 fqdncache_size 16384
>>>>> retry_on_error on offline_mode off logfile_rotate 10
>>>>> dns_nameservers 8.8.8.8 41.78.211.30
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> access.log:
>>>>> 
>>>>> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET 
>>>>> http://jadserve.postrelease.com/trk.gif? - 
>>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211
>>>>> 198 10.0.0.23 TCP_MISS/200 412 GET 
>>>>> http://jadserve.postrelease.com/trk.gif? - 
>>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211
>>>>> 198 10.0.0.23 TCP_MISS/200 412 GET 
>>>>> http://jadserve.postrelease.com/trk.gif? - 
>>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.223
>>>>> 301 10.0.0.23 TCP_MISS/200 222 GET 
>>>>> http://rma-api.gravity.com/v1/beacons/log? - 
>>>>> ORIGINAL_DST/80.239.148.18 text/html 1426267535.244    195 
>>>>> 10.0.0.23 TCP_MISS/200 412 GET 
>>>>> http://jadserve.postrelease.com/trk.gif? - 
>>>>> ORIGINAL_DST/54.225.133.227 image/gif
>>>> 
>>>> 
>>>> Lots of Akamai hosted requests. Akamai play tricks with DNS 
>>>> responses.
> In my installation I've used local Unbound DNS cache and, before
> it, forced DNS interception to him with Cisco. :)
> 
> So, I don't care about any hosts DNS quirks. ;)
> 
>>>> 
>>>> Check your cache.log for security warnings; 
>>>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>>>>
>>>>
>>>> 
Note that objects failing the Host validation are not cacheable.
>>>> 
>>>> 
>>>>> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET 
>>>>> http://hpr.outbrain.com/utils/get? -
>>>>> ORIGINAL_DST/50.31.185.42 text/x-json 1426267535.345    412
>>>>> 10.0.0.23 TCP_MISS/200 11179 GET
>>>>> http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40 
>>>>> text/javascript 1426267535.346    411 10.0.0.23
>>>>> TCP_MISS/200 423 GET http://t1.visualrevenue.com/? -
>>>>> ORIGINAL_DST/64.74.232.44 image/gif
>>>> 
>>>> Not sure about them. Maybe genuine MISS, maybe not.
> 
> Agressive dynamic content caching requires some special tweaks. ;)
> 
>>>> 
>>>> It could also be the issues Antony pointed out, with the
>>>> objects just naturally not being cacheable.
>>>> 
>>>> 
>>>>> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304
>>>>> 327 GET 
>>>>> http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
>>>>>
>>>>>
>
>>>>> 
- - ORIGINAL_DST/80.239.152.153 application/x-javascript
>>>> 
>>>> There is a hit.
>>>> 
>>>> I guess you are new to Squid-3 ? Squid is HTTP/1.1 compliant
>>>> now and the caching rules are slightly different from
>>>> requirements on HTTP/1.0 software. A lot of content that
>>>> previously could not be stored now can (authenticated,
>>>> private, no-cache, etc.). But being sensitive info also
>>>> requires revalidation in order to be used, so they show up
>>>> like the above.
>>>> 
>>>> Amos
>>>> 
>>>> _______________________________________________ squid-users
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVA0InAAoJENNXIZxhPexG6JAIALq2tAxa9Vawr1/Rkojl0UFj
HQF9p/4mk0ZHPnL4zkV6h/Ctg/s+AgK+O/H38ncn+2JS4eyiZfSHLOxmxkmrKi11
av/yjG++JGnhQkic/3y7ETOSkvaDuAbDP+Iwrtuc+kBpJz54No9Pu37oVlIOdMLZ
uv/8Bpk9uQEc3kE5FCgCmM2nIr2tuxr6opK6T5DZ2TvcqnQin752P60R91iS7unF
XHX3tsGsFvrKflEEC7w1xDRn3u3kSGrx+gPpktA0dv6vT8ATXqPEV5+anIEZVfLZ
NKDIwoeSNHYMMknlK7QTUlcNjuq+UXmfcO3mp+eraUQbGRkxwqTPxRwvIqp/43U=
=VW9B
-----END PGP SIGNATURE-----


From alberto2perez at gmail.com  Sat Mar 14 00:45:48 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Fri, 13 Mar 2015 20:45:48 -0400
Subject: [squid-users] squid intercept config
In-Reply-To: <55034228.8030800@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54FA7DED.9090902@treenet.co.nz>
 <CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>
 <55030DE0.60606@gmail.com>
 <CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>
 <55031F5D.7030107@treenet.co.nz> <55032FEA.5060205@gmail.com>
 <CAMZauGp8w-Gu_oS6rL6G94-QKd_114iaTh8=zX2f9DLna5=coQ@mail.gmail.com>
 <55034228.8030800@gmail.com>
Message-ID: <CAMZauGpA5spVJ0JtMXFkeBECbR5jES=rPZi3oS42_f-=53G8ew@mail.gmail.com>

Thanks a lot Yuri,
I made some merge with my config and some of this options, I will see now
how HIT rate it goes, my squid run so limited of bandwidth that I need to
be as much aggressive as I can caching the content.

Thanks again for sharing, very appreciated

Alberto

On Fri, Mar 13, 2015 at 4:01 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> This is know-how to himself. ;)
>
> To be serious,
>
> you must carefully play with refresh_pattern(s), and some squid.conf
> parameters (and also with store ID feature) to get higher HIT ratio.
>
> Just for example (this is NOT complete config! No responsibility or
> any guarantees in case of simple copy-n-pasted into your configs! This
> is AS IS example!):
>
> # Keep swf in cache even if asked not to
> refresh_pattern -i \.(swf)(\?|$)        10080   90%     43200
>  override-expire
> ignore-reload reload-into-ims ignore-private
> # .NET cache
> refresh_pattern -i \.(as(h|p)x?)(\?|$)  10080   90%     43200
>  reload-into-ims
> # Updates: Windows, Adobe, Java
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
>        4320
> 80% 43200       reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
> 4320 80% 43200  reload-into-ims
> refresh_pattern -i
> my.windowsupdate.website.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
> 4320 80% 43200  reload-into-ims
> refresh_pattern -i adobe.com/.*\.(zip|exe)      4320    80%     43200
>  reload-into-ims
> refresh_pattern -i java.com/.*\.(zip|exe)       4320    80%     43200
>  reload-into-ims
> refresh_pattern -i sun.com/.*\.(zip|exe)        4320    80%     43200
>  reload-into-ims
> refresh_pattern -i google\.com.*\.(zip|exe)     4320    80%     43200
>  reload-into-ims
> refresh_pattern -i macromedia\.com.*\.(zip|exe) 4320    80%     43200
> reload-into-ims
> # Other long-lived items
> refresh_pattern -i
> \.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4)(\?|$)
>       14400
> 99%     518400  ignore-no-store override-expire ignore-reload
> reload-into-ims ignore-private ignore-must-revalidate
> refresh_pattern -i
> \.((m?|x?|s?)htm(l?)|css|js|xml|php|json)(\?|$)                 10080
>  90%     86400
> ignore-no-store override-expire override-lastmod reload-into-ims
> ignore-private ignore-must-revalidate
> # Default patterns
> refresh_pattern -i (/cgi-bin/|\?)       0       0%      0
> refresh_pattern .       0       20%     10080   override-lastmod
> reload-into-ims
>
> The example above also requires some additional cached-related
> parameters to be changed.
>
> Also, you strictly recommended to research average users activity AND
> play around VARY http headers.
>
> And others.
>
> Each squid setup is place-specific. And depending your access/deny
> lists, security policy, users/network activity etc.etc.etc.
>
> WBR, Yuri
>
> PS. Your question has NO simple answer. Beware - copy-n-paste any
> foreign config can not guarantee the same results for YOU.
>
> 14.03.15 1:52, Alberto Perez ?????:
> > Can you share more details about "Agressive dynamic content
> > caching requires some special tweaks" I am very interested.
> >
> > Thanks
> >
> >
> >
> > On 3/13/15, Yuri Voinov <yvoinov at gmail.com> wrote:
> >
> >
> > 13.03.15 23:33, Amos Jeffries ?????:
> >>>> On 14/03/2015 5:47 a.m., Monah Baki wrote:
> >>>>
> >>>> <snip>
> >>>>
> >>>>> half_closed_clients off quick_abort_min 0 KB
> >>>>> quick_abort_max 0 KB vary_ignore_expire on reload_into_ims
> >>>>> on memory_pools off cache_mem 4096 MB visible_hostname
> >>>>> isn-phc-cache minimum_object_size 0 bytes
> >>>>
> >>>>> maximum_object_size 512 MB maximum_object_size 512 KB
> >>>>
> >>>> KB value overwriting MB value.
> >>>>
> >>>>
> >>>>> ipcache_size 1024 ipcache_low 90 ipcache_high 95
> >>>>> cache_swap_low 98 cache_swap_high 100 fqdncache_size 16384
> >>>>> retry_on_error on offline_mode off logfile_rotate 10
> >>>>> dns_nameservers 8.8.8.8 41.78.211.30
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> access.log:
> >>>>>
> >>>>> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
> >>>>> http://jadserve.postrelease.com/trk.gif? -
> >>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211
> >>>>> 198 10.0.0.23 TCP_MISS/200 412 GET
> >>>>> http://jadserve.postrelease.com/trk.gif? -
> >>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.211
> >>>>> 198 10.0.0.23 TCP_MISS/200 412 GET
> >>>>> http://jadserve.postrelease.com/trk.gif? -
> >>>>> ORIGINAL_DST/54.225.133.227 image/gif 1426267535.223
> >>>>> 301 10.0.0.23 TCP_MISS/200 222 GET
> >>>>> http://rma-api.gravity.com/v1/beacons/log? -
> >>>>> ORIGINAL_DST/80.239.148.18 text/html 1426267535.244    195
> >>>>> 10.0.0.23 TCP_MISS/200 412 GET
> >>>>> http://jadserve.postrelease.com/trk.gif? -
> >>>>> ORIGINAL_DST/54.225.133.227 image/gif
> >>>>
> >>>>
> >>>> Lots of Akamai hosted requests. Akamai play tricks with DNS
> >>>> responses.
> > In my installation I've used local Unbound DNS cache and, before
> > it, forced DNS interception to him with Cisco. :)
> >
> > So, I don't care about any hosts DNS quirks. ;)
> >
> >>>>
> >>>> Check your cache.log for security warnings;
> >>>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
> >>>>
> >>>>
> >>>>
> Note that objects failing the Host validation are not cacheable.
> >>>>
> >>>>
> >>>>> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET
> >>>>> http://hpr.outbrain.com/utils/get? -
> >>>>> ORIGINAL_DST/50.31.185.42 text/x-json 1426267535.345    412
> >>>>> 10.0.0.23 TCP_MISS/200 11179 GET
> >>>>> http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40
> >>>>> text/javascript 1426267535.346    411 10.0.0.23
> >>>>> TCP_MISS/200 423 GET http://t1.visualrevenue.com/? -
> >>>>> ORIGINAL_DST/64.74.232.44 image/gif
> >>>>
> >>>> Not sure about them. Maybe genuine MISS, maybe not.
> >
> > Agressive dynamic content caching requires some special tweaks. ;)
> >
> >>>>
> >>>> It could also be the issues Antony pointed out, with the
> >>>> objects just naturally not being cacheable.
> >>>>
> >>>>
> >>>>> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304
> >>>>> 327 GET
> >>>>>
> http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
> >>>>>
> >>>>>
> >
> >>>>>
> - - ORIGINAL_DST/80.239.152.153 application/x-javascript
> >>>>
> >>>> There is a hit.
> >>>>
> >>>> I guess you are new to Squid-3 ? Squid is HTTP/1.1 compliant
> >>>> now and the caching rules are slightly different from
> >>>> requirements on HTTP/1.0 software. A lot of content that
> >>>> previously could not be stored now can (authenticated,
> >>>> private, no-cache, etc.). But being sensitive info also
> >>>> requires revalidation in order to be used, so they show up
> >>>> like the above.
> >>>>
> >>>> Amos
> >>>>
> >>>> _______________________________________________ squid-users
> >>>> mailing list squid-users at lists.squid-cache.org
> >>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>
> >> _______________________________________________ squid-users
> >> mailing list squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJVA0InAAoJENNXIZxhPexG6JAIALq2tAxa9Vawr1/Rkojl0UFj
> HQF9p/4mk0ZHPnL4zkV6h/Ctg/s+AgK+O/H38ncn+2JS4eyiZfSHLOxmxkmrKi11
> av/yjG++JGnhQkic/3y7ETOSkvaDuAbDP+Iwrtuc+kBpJz54No9Pu37oVlIOdMLZ
> uv/8Bpk9uQEc3kE5FCgCmM2nIr2tuxr6opK6T5DZ2TvcqnQin752P60R91iS7unF
> XHX3tsGsFvrKflEEC7w1xDRn3u3kSGrx+gPpktA0dv6vT8ATXqPEV5+anIEZVfLZ
> NKDIwoeSNHYMMknlK7QTUlcNjuq+UXmfcO3mp+eraUQbGRkxwqTPxRwvIqp/43U=
> =VW9B
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150313/59497ba5/attachment.htm>

From alex at samad.com.au  Sat Mar 14 01:39:56 2015
From: alex at samad.com.au (Alex Samad)
Date: Sat, 14 Mar 2015 12:39:56 +1100
Subject: [squid-users] Cento 6 repo
Message-ID: <CAJ+Q1PXn_ymFBdP3c9EAKKx0ksM2gbHsvxeLRiOvUmrnkkk8gA@mail.gmail.com>

Hi

Quick on

squid.x86_64                        7:3.4.10-1.el6              @squid
squid-debuginfo.x86_64              7:3.4.10-1.el6              squid
squid-helpers.x86_64                7:3.4.10-1.el6              squid
squid-sysvinit.x86_64               7:3.4.3-1.el6               squid

the sysvinit is not up to date !


and
yum install squid-helpers.x86_64
Error: Package: 7:squid-helpers-3.4.10-1.el6.x86_64 (squid)
           Requires: perl(Crypt::OpenSSL::X509)

any chance to host the perl package at the same site ?


From l4ng1t at gmail.com  Sat Mar 14 03:19:28 2015
From: l4ng1t at gmail.com (Donny Vibianto)
Date: Sat, 14 Mar 2015 10:19:28 +0700
Subject: [squid-users] negotiate_wrapper: fgets() failed! dying..
In-Reply-To: <mdstpc$h8v$1@ger.gmane.org>
References: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>
 <CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q@mail.gmail.com>
 <mdstpc$h8v$1@ger.gmane.org>
Message-ID: <CAC49LV7BK8DR5vw5bxwuJFbZvjuYheqUtvKh3+rjrW-tzg+bGA@mail.gmail.com>

hi markus,

unfortunately i move to centos but i still got same error after 9 hours
running.

2015/03/14 10:05:43| negotiate_wrapper: received Kerberos token
2015/03/14 10:05:43| negotiate_wrapper: Starting version 1.0.1
2015/03/14 10:05:43| negotiate_wrapper: NTLM command: /usr/bin/ntlm_auth
--diagnostics --helper-protocol=squid-2.5-ntlmssp
2015/03/14 10:05:43| negotiate_wrapper: Kerberos command:
/usr/lib64/squid/negotiate_kerberos_auth -k /etc/squid/PROXY.keytab -s
GSS_C_NO_NAME
FATAL: Received Bus Error...dying.
2015/03/14 10:05:43 kid2| ctx: enter level  0: '
http://citrine.gundam-dc.com/lng/common/cocos/GameView_v2/res/Battle/radar/radar_b_ring.png?ver=1
'
2015/03/14 10:05:43 kid2| Closing HTTP port [::]:8000
2015/03/14 10:05:43 kid2| storeDirWriteCleanLogs: Starting...
2015/03/14 10:05:43 kid2|   Finished.  Wrote 0 entries.
2015/03/14 10:05:43 kid2|   Took 0.00 seconds (  0.00 entries/sec).
CPU Usage: 0.133 seconds = 0.066 user + 0.067 sys
Maximum Resident Size: 70224 KB
Page faults with physical i/o: 0
2015/03/14 10:05:43| negotiate_wrapper: fgets() failed! dying..... errno=1
(Operation not permitted)
2015/03/14 10:05:43| negotiate_wrapper: fgets() failed! dying..... errno=1
(Operation not permitted)

thanks for reply


donny


On Fri, Mar 13, 2015 at 3:43 AM, Markus Moeller <huaraz at moeller.plus.com>
wrote:

>   Do you get any more details when you start the wrapper with ?d ?
>
> Markus
>
>  "Donny Vibianto" <donny.vibianto at gmail.com> wrote in message
> news:CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q at mail.gmail.com...
>   anyone please...?
>
> On Sat, Mar 7, 2015 at 10:02 PM, Donny Vibianto <donny.vibianto at gmail.com>
> wrote:
>
>>  Hi Guys,
>>
>> After two weeks successful running several authentication in my
>> development environment with average 10-20 users, i encourage myself to put
>> in my production. it was up and ran with +-1000 users but only took 3-5
>> hours then squid suddenly stopped with error:
>>
>>  2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying.....
>> errno=1 (Operation not permitted)
>> 2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying.....
>> errno=1 (Operation not permitted)
>> 2015/03/06 15:07:59| negotiate_wrapper: fgets() failed! dying.....
>> errno=1 (Operation not permitted)
>> 2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
>> oYG2MIGzoAMKAQChCwYJKoZIhvcSAQICooGeBIGbYIGYBgkqhkiG9xIBAgICAG+BiDCBhaADAgEFoQMCAQ+ieTB3oAMCARKicARupdwIysaz6zjRSqsI8V4K0X67z4t5a9aOT7WPlyWRrp+1ol2zL6CYTcfZIyAq8q3D00mf+vpIeoiDDmkUkr+vXN+xkpXkWdX5pMD1hBrF4EDOL1RIp9XjpkdfIcEgg8Oia0Ay153sPK3+Tif4bGE=
>> RickyC at company.local
>> '
>> 2015/03/06 15:07:59| negotiate_wrapper: Return 'AF
>> oYG1MIGyoAMKAQChCwYJKoZIhvcSAQICooGdBIGaYIGXBgkqhkiG9xIBAgICAG+BhzCBhKADAgEFoQMCAQ+ieDB2oAMCARKibwRtX5xuxTxrgsKQpg3Y+kUXLOng15XJ7eDByao5YtNPZByv/zRtrz13QgKkCuk+VkXnCAzaii0ri4Mxvd+4BoskIrjf5FuPP3W59wMTCtkPJD85igR/OmQ4Ch09DJ51WGwnOizMuCW+9jg6EsFa1Q==
>> JanTS at company.local
>>
>> i use ubuntu server 14.04 with newest squid 3.5.2
>>
>>  Squid Cache: Version 3.5.2
>> Service Name: squid
>> configure options:  '--enable-build-info'
>> '--enable-removal-policies=lru,heap' '--enable-ltdl-install'
>> '--enable-storeio=ufs,aufs,rock' '--enable-auth-basic=LDAP'
>> '--enable-auth-negotiate=wrapper,kerberos'
>> '--enable-external-acl-helpers=LDAP_group' '--enable-translation'
>> '--enable-ssl-crtd' '--enable-gnuregex' '--enable-xmalloc-debug'
>> '--enable-xmalloc-debug-trace' '--enable-xmalloc-statistics'
>> '--enable-async-io' '--enable-icmp' '--enable-delay-pools'
>> '--enable-useragent-log' '--enable-kill-parent-hack' '--enable-htpc'
>> '--enable-forw-via-db' '--enable-cache-digests' '--enable-underscores'
>> '--enable-x-accelerator-vary' '--enable-esi' '--enable-inline'
>> '--enable-linux-netfilter' '--with-openssl' '--with-large-files'
>>
>> here is my squid.conf:
>>
>>  # ===================== ACL Cachemgr
>> ============================================
>> acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
>> acl managerAdmin src "/usr/local/squid/etc/mgradmin.txt"
>> acl stream url_regex -i "/usr/local/squid/etc/stream"
>>
>> acl download url_regex -i "/usr/local/squid/etc/download"
>> acl whitelist url_regex -i "/usr/local/squid/etc/whitelist"
>> acl blacklist url_regex -i "/usr/local/squid/etc/blacklist"
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl http proto http
>> acl CONNECT method CONNECT
>>
>> # ==================== Authenticate using negotiate_wrapper
>> =====================
>> auth_param negotiate program
>> /usr/local/squid/libexec/negotiate_wrapper_auth -d --ntlm
>> /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
>> --kerberos /usr/local/squid/libexec/negotiate_kerberos_auth -s GSS_C_NO_NAME
>> auth_param negotiate children 50 startup=0 idle=1
>> auth_param negotiate keep_alive off
>> # ==================== Authenticate using NTLM
>> ==================================
>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
>> --helper-protocol=squid-2.5-ntlmssp
>> auth_param ntlm children 20 startup=0 idle=1
>> auth_param ntlm keep_alive off
>> # ==================== Authenticate using Basic LDAP
>> ============================
>> auth_param basic program /usr/local/squid/libexec/basic_ldap_auth -R -b
>> "dc=company,dc=local" -D squid at company.local -w "password" -f
>> sAMAccountName=%s -h idhqvdc01.company.local,idhqvdc02.company.local
>> auth_param basic children 5 startup=0 idle=1
>> auth_param basic realm AGDS Proxy: Please enter your username and
>> password domain
>> auth_param basic credentialsttl 1 minute
>> # ==================== Authenticate to Group Security Actice Directory
>> ==========
>> external_acl_type memberof ipv4 children-max=10 children-startup=1 %LOGIN
>> /usr/local/squid/libexec/ext_ldap_group_acl -R -K -S -b
>> "dc=company,dc=local" -D squid at company.local -w "password" -f
>> "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%g,ou=groups,ou=resources,dc=company,dc=local))"
>> -h idhqvdc01.company.local,idhqvdc02.company.local
>>
>> acl auth proxy_auth REQUIRED
>> # ==================== ACL Access hour user
>> =====================================
>> acl ach1 external memberof "/usr/local/squid/etc/ach1.txt" # access hour
>> 1
>> acl ach2 external memberof "/usr/local/squid/etc/ach2.txt" # access hour
>> 2
>> acl ach3 external memberof "/usr/local/squid/etc/ach3.txt" # access hour
>> 3
>> acl ach4 external memberof "/usr/local/squid/etc/ach4.txt" # access hour
>> 4
>> acl ach2time time D 10:00-11:59
>> acl ach2time time D 13:00-14:59
>> acl ach3time time D 08:00-09:59
>> acl ach3time time D 15:00-16:59
>> acl ach4time time D 08:00-16:59
>> acl bebastime time D 00:01-07:59 12:00-13:59 17:00-23:59
>>
>> #
>> ==============================================================================
>> http_access deny !Safe_ports # Deny requests to certain unsafe ports
>> http_access deny CONNECT !SSL_ports # Deny CONNECT to other than secure
>> SSL ports
>> http_access allow manager localhost # Only allow cachemgr access from
>> localhost
>> http_access allow manager managerAdmin
>> http_access deny manager
>>
>> #
>> ==============================================================================
>> #http_access allow localnet
>> http_access allow localhost
>> http_access deny blacklist !bebastime
>>
>> http_access allow http Safe_ports whitelist
>> http_access allow CONNECT SSL_ports whitelist
>> #http_access deny all !auth
>>
>> #http_access allow http Safe_ports ach1
>> #http_access allow CONNECT SSL_ports ach1
>> #http_access allow http Safe_ports ach2 !ach2time
>> #http_access allow CONNECT SSL_ports ach2 !ach2time
>> #http_access allow http Safe_ports ach3 !ach3time
>> #http_access allow CONNECT SSL_ports ach3 !ach3time
>> #http_access allow http Safe_ports ach4 !ach4time
>> #http_access allow CONNECT SSL_ports ach4 !ach4time
>>
>> #http_access allow accesshours1
>> #http_access allow accesshours2 !ach2time
>> #http_access allow accesshours3 !ach3time
>> #http_access allow accesshours4 !ach3time
>>
>> http_access allow ach1
>> http_access allow ach2 !ach2time
>> http_access allow ach3 !ach3time
>> http_access allow ach4 !ach4time
>>
>> http_access deny all # Deny all other access to this proxy
>> #
>> ==============================================================================
>>
>> cache_dir rock /cache1/squid 97485 max-swap-rate=200 swap-timeout=300
>> cache_dir rock /cache2/squid 97485 max-swap-rate=200 swap-timeout=300
>> coredump_dir /usr/local/squid/var/cache/squid
>> # =============================== Refresh Pattern
>> ==============================
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> refresh_pattern . 0 20% 4320
>>
>> cache_effective_user proxy
>> visible_hostname proxy.company.local
>> cache_mgr proxyastragraphia
>> cachemgr_passwd secret all
>> #err_page_stylesheet /usr/local/squid/etc/default.css
>> pinger_enable off
>> #workers 2
>>
>> i was tried to put different acl and put my auth_param at the top of my
>> conf but still dying error. what should i do?
>> any assistant or hint would be very appreciate. thanks
>>
>>
>> Donny Vibianto
>>
>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150314/a30e7421/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar 14 04:45:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 17:45:19 +1300
Subject: [squid-users] negotiate_wrapper: fgets() failed! dying..
In-Reply-To: <CAC49LV7BK8DR5vw5bxwuJFbZvjuYheqUtvKh3+rjrW-tzg+bGA@mail.gmail.com>
References: <CAC49LV6Y=mYtNpv-fWFJviAqQYPKUGKK2W3c83pevFvcq_U9Cw@mail.gmail.com>
 <CAC49LV6SRXbiFcGxqZgAoaHPj1qeifERtSN63ZrDsa_b=iW+1Q@mail.gmail.com>
 <mdstpc$h8v$1@ger.gmane.org>
 <CAC49LV7BK8DR5vw5bxwuJFbZvjuYheqUtvKh3+rjrW-tzg+bGA@mail.gmail.com>
Message-ID: <5503BCDF.7080705@treenet.co.nz>

On 14/03/2015 4:19 p.m., Donny Vibianto wrote:
> hi markus,
> 
> unfortunately i move to centos but i still got same error after 9 hours
> running.
> 
> 2015/03/14 10:05:43| negotiate_wrapper: received Kerberos token
> 2015/03/14 10:05:43| negotiate_wrapper: Starting version 1.0.1
> 2015/03/14 10:05:43| negotiate_wrapper: NTLM command: /usr/bin/ntlm_auth
> --diagnostics --helper-protocol=squid-2.5-ntlmssp
> 2015/03/14 10:05:43| negotiate_wrapper: Kerberos command:
> /usr/lib64/squid/negotiate_kerberos_auth -k /etc/squid/PROXY.keytab -s
> GSS_C_NO_NAME
> FATAL: Received Bus Error...dying.

Aha! "Bus Error" is a memory failure in the machine.

The helper problems are just side effects of Squid having died and
abandoned it.


It could be the physical memory, or it could be Squid built with CPU
architecture optimizations for a different machine architecture than you
are running it on.

* Try using a Squid built with --disable-arch-native.

* If the probem remains, check your Squid 32-bit/64-bit type matches the
CPU architecture type.

* If its not those, its probably hardware issues.


Amos



From squid3 at treenet.co.nz  Sat Mar 14 05:00:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 18:00:26 +1300
Subject: [squid-users] Cento 6 repo
In-Reply-To: <CAJ+Q1PXn_ymFBdP3c9EAKKx0ksM2gbHsvxeLRiOvUmrnkkk8gA@mail.gmail.com>
References: <CAJ+Q1PXn_ymFBdP3c9EAKKx0ksM2gbHsvxeLRiOvUmrnkkk8gA@mail.gmail.com>
Message-ID: <5503C06A.9020901@treenet.co.nz>

On 14/03/2015 2:39 p.m., Alex Samad wrote:
> Hi
> 
> Quick on
> 
> squid.x86_64                        7:3.4.10-1.el6              @squid
> squid-debuginfo.x86_64              7:3.4.10-1.el6              squid
> squid-helpers.x86_64                7:3.4.10-1.el6              squid
> squid-sysvinit.x86_64               7:3.4.3-1.el6               squid
> 
> the sysvinit is not up to date !
> 

The init script is not exactly changing much.

> 
> and
> yum install squid-helpers.x86_64
> Error: Package: 7:squid-helpers-3.4.10-1.el6.x86_64 (squid)
>            Requires: perl(Crypt::OpenSSL::X509)
> 
> any chance to host the perl package at the same site ?

The build dependencies should all be available in mainstream
repositories. That reads to me like you are missing a particular perl
module (possible from the perl CPAN repository).

Amos



From squid3 at treenet.co.nz  Sat Mar 14 05:07:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 18:07:40 +1300
Subject: [squid-users] i hope to use external ACL + ldap at squid 3.5.2,
 but i don't find ext_ldap_group_acl and basic_ldap_auth from
 /squid/libexec/
In-Reply-To: <55031248.8080700@yahoo.com>
References: <55031248.8080700@yahoo.com>
Message-ID: <5503C21C.60906@treenet.co.nz>

On 14/03/2015 5:37 a.m., johnzeng wrote:
> 
> Hello All:
> 
> i hope to use external ACL + ldap at squid 3.5.2, but i don't find
> ext_ldap_group_acl and basic_ldap_auth from /squid/libexec/
> 
> if possible , please give me some advisement . Thanks
> 

You are missing the LDAP libraries needed to build them.

> This is my config
> 
> 
> configure options: '--prefix=/accerater/webcache3'
> '--enable-follow-x-forwarded-for' '--enable-snmp'
> '--enable-linux-netfilter' '--enable-storeio=aufs,rock'
> '--enable-wccpv2' '--with-large-files'
> '--enable-removal-policies=lru,heap' '--enable-async-io=128'
> '--enable-http-violations'

All of these ...

> '--enable-default-err-language=English'
> '--enable-err-languages=English' '--enable-referer-log'
> '--enable-useragent-log'

... to here are no longer existing otpions.

> '--with-maxfd=65536'
> '--enable-large-cache-files' '--enable-delay-pools'
> '--enable-forward-log' '--with-pthreads' 'LIBS=-ltcmalloc'
> '--disable-internal-dns'

Disable of interal DNS is no longer an existing option.

> '--enable-url-rewrite-helpers'
> '--enable-log-daemon-helpers' '--enable-epoll'
> '--enable-ltdl-convenience' '--with-included-ltdl'
> '--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped'
> 
> This is full file at /squid/libexec
> 
> 
> basic_db_auth basic_ncsa_auth basic_smb_auth digest_file_auth
> ext_unix_group_acl log_file_daemon storeid_file_rewrite
> basic_fake_auth basic_nis_auth basic_smb_auth.sh ext_delayer_acl
> ext_wbinfo_group_acl negotiate_wrapper_auth unlinkd
> basic_getpwnam_auth basic_pop3_auth basic_smb_lm_auth
> ext_file_userip_acl helper-mux.pl ntlm_fake_auth url_fake_rewrite
> basic_msnt_multi_domain_auth basic_radius_auth cachemgr.cgi
> ext_sql_session_acl log_db_daemon ntlm_smb_lm_auth url_fake_rewrite.sh

Amos



From squid3 at treenet.co.nz  Sat Mar 14 05:14:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 18:14:06 +1300
Subject: [squid-users] Reverse Proxy Funny Logging Issue
In-Reply-To: <2be09011d9c5f16220f0c781c793e242@dweimer.net>
References: <0113b4b74c8c9e0a05cd692641aecf25@dweimer.net>
 <50FF7561.6050301@treenet.co.nz>
 <88bdb4a5d0b47bb9e459b91b079b7425@dweimer.net>
 <8a05b3c589b46021b42d5a59a7b11e8a@dweimer.net>
 <5100BB15.3030502@treenet.co.nz>
 <9f1caca0e85b1fd2d2ed88366f7516a4@dweimer.net>
 <2be09011d9c5f16220f0c781c793e242@dweimer.net>
Message-ID: <5503C39E.6010604@treenet.co.nz>

On 14/03/2015 5:19 a.m., dweimer wrote:
>>
>> Last night I applied the FreeBSD 10.1-RELEASE-p6 Update and Upgraded
>> the ports which included Squid 3.4.12, I enabled the LAX HTTP option
>> in the ports configuration with adds the --enable-http-violations
>> compile option. With the intention to enable broken_posts option in
>> the configuration. I will hopefully be able to apply any necessary
>> changes to the production system after I test them now.
>> When doing this update I did have a thought the system is running in a
>> FreeBSD jail and not on the base system is there a chance this issue
>> is caused by running within a jail? curious if anyone has ran into
>> specific issues running Squid in a FreeBSD jail before?
> 
> Well I am at a loss, debugging hasn't led to anything more than a
> timeout occurs. I was able to create a test PHP form to upload files on
> an Apache server and upload up to a 264MB file. I didn't try any larger
> files though I suspect it would work up to the configured 1024MB that I
> had Apache configured for. So its not all HTTPS only those files going
> to our OWA and Sharepoint servers. The only settings I can find that
> changes the behavior at all is to change the "write_timeout" to
> something smaller, like 45 seconds and then it errors sooner instead of
> taking forever to give up.
> 
> I tried uninstalling the Squid 3.4 FreeBSD port and using the 3.3 port
> instead on the test system, no change. I also tried installing 3.5 from
> source using the same configure options that my 3.4 port returned with
> squid -v, again no change.
> 
> I have verified that the IIS logs show a client request timeout has
> occurred, the broken_posts allow didn't create any change in behavior. I
> do know that if I point the browser directly to the Exchange server it
> works, so its only broken going through the reverse the proxy. If I
> point the browser through a forwarding Squid proxy that knows how to
> talk directly to the exchange server instead of the reverse proxy it
> works with no special settings. If I post a large debugging file to a
> website do I have any volunteers to look at it and see if they can see
> what's going on?
> 


It sounds like its not actually a Squid problem. There is enough code
churn between the major versions you tried that things should have
changed between them.


Could it be the end-of-line issue in Python?
<http://wiki.squid-cache.org/Features/AddonHelpers#What_language_are_helper_meant_to_be_written_in.3F>

Amos


From squid3 at treenet.co.nz  Sat Mar 14 05:41:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 18:41:44 +1300
Subject: [squid-users] Read Timeout
In-Reply-To: <ea40eb71a0a98c6f4ace2ec4f5d7740d@posteo.de>
References: <ea40eb71a0a98c6f4ace2ec4f5d7740d@posteo.de>
Message-ID: <5503CA18.30100@treenet.co.nz>

On 14/03/2015 2:34 a.m., sci wrote:
> Hi,
> 
> I search a solution for the follow "problem":
> 
> A website with a simple button (POST) needs more than 15min to send the
> information what the clients needs (a simple excel sheet).
> We don't want to change the global settings at the "Read Timeout", so I
> try to find something like a ACL for the squid.
> 
> Is it possible to at a ACL to the squid.conf like:
> 
> # rule only for the slow website
> acl slow-server dstdomain .someserver-slow.com
> read_timeout 30 minutes slow-server
> # end rule
> 

15min is not exactly a Squid limit. Its just set with a default to match
other limits in the Internet.

A read happens (and the read_timeout restarts) whenever any TCP packets
arrive. At some point between 5 and 60min (usually lower values though)
with no packets at all occuring the TCP stacks and NAT systems all along
the network path of the connections start discarding their records about
the connection.

If the server is not responding with at least one packet within a
5-60min period the connection has a growing risk of not existing
anymore. Squid default lets it hang for 15min before considering it too
risky to use anymore and releases all the resources (and there are a LOT
of resources used down the whole chain of machinery between client and
server).

Don't worry about extending the timeout globally to 30min. It will have
no effect at all on servers or traffic that responds promptly to requests.


The best thing to do though is get the server fixed (ie complain to the
people responsible for the code generating those documents). Sometimes
they can fix teh total speed, sometimes not. But either way HTTP
contains several mechanisms allowing object to be streamed to the client
as they are generated, with error recovery and data validation even.

Amos



From johnzeng2013 at yahoo.com  Sat Mar 14 07:12:36 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 15:12:36 +0800
Subject: [squid-users] i hope to use external ACL + ldap at squid 3.5.2,
 but i don't find ext_ldap_group_acl and basic_ldap_auth from
 /squid/libexec/
In-Reply-To: <5503C21C.60906@treenet.co.nz>
References: <55031248.8080700@yahoo.com> <5503C21C.60906@treenet.co.nz>
Message-ID: <5503DF64.9040201@yahoo.com>


Hi Amos:

               How i will  build LDAP libraries for squid ?

               We built openldap-2.4.39 environment .

                We operated  command " source /etc/ld.so.conf " , 
include  /accerater/env/ldap/lib  /accerater/env/ldap/include

/accerater/env/ldap/lib

liblber-2.4.so.2       liblber.a   liblber.so libldap-2.4.so.2.10.2  
libldap.la libldap_r-2.4.so.2.10.2  libldap_r.la  libldap.so
liblber-2.4.so.2.10.2  liblber.la  libldap-2.4.so.2 
libldap.a              libldap_r-2.4.so.2 libldap_r.a              
libldap_r.so

/accerater/env/ldap/include

lber.h  lber_types.h  ldap_cdefs.h  ldap_features.h  ldap.h 
ldap_schema.h  ldap_utf8.h  ldif.h  slapi-plugin.h


This is new configure in according to your advisement


./configure --prefix=/accerater/webcache3 
--enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter 
--enable-storeio=aufs,rock --enable-wccpv2 --with-large-files 
--enable-removal-policies=lru,heap --enable-async-io=128 
--enable-http-violations  --with-maxfd=65536 --enable-large-cache-files 
--enable-delay-pools --enable-forward-log --with-pthreads 
LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers 
--enable-epoll --enable-ltdl-convenience --with-included-ltdl 
--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped 
--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group 
--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB 
--enable-auth-digest=file,LDAP --enable-cache-digests 
--enable-auth-negotiate=kerberos,wrapper --enable-auth-ntlm=fake,smb_lm




if possible , please advise me how to do .


? 2015?03?14? 13:07, Amos Jeffries ??:
> On 14/03/2015 5:37 a.m., johnzeng wrote:
>> Hello All:
>>
>> i hope to use external ACL + ldap at squid 3.5.2, but i don't find
>> ext_ldap_group_acl and basic_ldap_auth from /squid/libexec/
>>
>> if possible , please give me some advisement . Thanks
>>
> You are missing the LDAP libraries needed to build them.
>
>> This is my config
>>
>>
>> configure options: '--prefix=/accerater/webcache3'
>> '--enable-follow-x-forwarded-for' '--enable-snmp'
>> '--enable-linux-netfilter' '--enable-storeio=aufs,rock'
>> '--enable-wccpv2' '--with-large-files'
>> '--enable-removal-policies=lru,heap' '--enable-async-io=128'
>> '--enable-http-violations'
> All of these ...
>
>> '--enable-default-err-language=English'
>> '--enable-err-languages=English' '--enable-referer-log'
>> '--enable-useragent-log'
> ... to here are no longer existing otpions.
>
>> '--with-maxfd=65536'
>> '--enable-large-cache-files' '--enable-delay-pools'
>> '--enable-forward-log' '--with-pthreads' 'LIBS=-ltcmalloc'
>> '--disable-internal-dns'
> Disable of interal DNS is no longer an existing option.
>
>> '--enable-url-rewrite-helpers'
>> '--enable-log-daemon-helpers' '--enable-epoll'
>> '--enable-ltdl-convenience' '--with-included-ltdl'
>> '--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped'
>>
>> This is full file at /squid/libexec
>>
>>
>> basic_db_auth basic_ncsa_auth basic_smb_auth digest_file_auth
>> ext_unix_group_acl log_file_daemon storeid_file_rewrite
>> basic_fake_auth basic_nis_auth basic_smb_auth.sh ext_delayer_acl
>> ext_wbinfo_group_acl negotiate_wrapper_auth unlinkd
>> basic_getpwnam_auth basic_pop3_auth basic_smb_lm_auth
>> ext_file_userip_acl helper-mux.pl ntlm_fake_auth url_fake_rewrite
>> basic_msnt_multi_domain_auth basic_radius_auth cachemgr.cgi
>> ext_sql_session_acl log_db_daemon ntlm_smb_lm_auth url_fake_rewrite.sh
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Sat Mar 14 07:29:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 14 Mar 2015 13:29:42 +0600
Subject: [squid-users] squid intercept config
In-Reply-To: <CAMZauGpA5spVJ0JtMXFkeBECbR5jES=rPZi3oS42_f-=53G8ew@mail.gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>	<54FA7DED.9090902@treenet.co.nz>	<CALP3=x_ch1PBYbxU3uLOz6-4B8nzPBzqomP1QWZuLTGO112uVg@mail.gmail.com>	<55030DE0.60606@gmail.com>	<CALP3=x8TPqhnpNh4YhNuVjBHOiZdvQyZCqs9BbGKvLbEe4mq6g@mail.gmail.com>	<55031F5D.7030107@treenet.co.nz>	<55032FEA.5060205@gmail.com>	<CAMZauGp8w-Gu_oS6rL6G94-QKd_114iaTh8=zX2f9DLna5=coQ@mail.gmail.com>	<55034228.8030800@gmail.com>
 <CAMZauGpA5spVJ0JtMXFkeBECbR5jES=rPZi3oS42_f-=53G8ew@mail.gmail.com>
Message-ID: <5503E366.1000608@gmail.com>

And dont forget that cache must be warmed up first, before it can cause 
increase HIT-ratio.

14.03.15 6:45, Alberto Perez ?????:
> Thanks a lot Yuri,
> I made some merge with my config and some of this options, I will see 
> now how HIT rate it goes, my squid run so limited of bandwidth that I 
> need to be as much aggressive as I can caching the content.
>
> Thanks again for sharing, very appreciated
>
> Alberto
>
> On Fri, Mar 13, 2015 at 4:01 PM, Yuri Voinov <yvoinov at gmail.com 
> <mailto:yvoinov at gmail.com>> wrote:
>
>     -----BEGIN PGP SIGNED MESSAGE-----
>     Hash: SHA1
>
>     This is know-how to himself. ;)
>
>     To be serious,
>
>     you must carefully play with refresh_pattern(s), and some squid.conf
>     parameters (and also with store ID feature) to get higher HIT ratio.
>
>     Just for example (this is NOT complete config! No responsibility or
>     any guarantees in case of simple copy-n-pasted into your configs! This
>     is AS IS example!):
>
>     # Keep swf in cache even if asked not to
>     refresh_pattern -i \.(swf)(\?|$)        10080   90%  43200 
>      override-expire
>     ignore-reload reload-into-ims ignore-private
>     # .NET cache
>     refresh_pattern -i \.(as(h|p)x?)(\?|$)  10080   90%  43200 
>      reload-into-ims
>     # Updates: Windows, Adobe, Java
>     refresh_pattern -i
>     microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
>     <http://microsoft.com/.*%5C.%28cab%7Cexe%7Cms[i%7Cu%7Cf%7Cp]%7Casf%7Cwm[v%7Ca]%7Cdat%7Czip%29>
>                        4320
>     80% 43200       reload-into-ims
>     refresh_pattern -i
>     windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
>     <http://windowsupdate.com/.*%5C.%28cab%7Cexe%7Cms[i%7Cu%7Cf%7Cp]%7Casf%7Cwm[v%7Ca]%7Cdat%7Czip%29>
>     4320 80% 43200  reload-into-ims
>     refresh_pattern -i
>     my.windowsupdate.website.com/.*\.(cab|exe|ms[i|u|f|p]|asf|wm[v|a]|dat|zip)
>     <http://my.windowsupdate.website.com/.*%5C.%28cab%7Cexe%7Cms[i%7Cu%7Cf%7Cp]%7Casf%7Cwm[v%7Ca]%7Cdat%7Czip%29>
>     4320 80% 43200  reload-into-ims
>     refresh_pattern -i adobe.com/.*\.(zip|exe)
>     <http://adobe.com/.*%5C.%28zip%7Cexe%29>      4320 80%     43200 
>      reload-into-ims
>     refresh_pattern -i java.com/.*\.(zip|exe)
>     <http://java.com/.*%5C.%28zip%7Cexe%29>       4320 80%     43200 
>      reload-into-ims
>     refresh_pattern -i sun.com/.*\.(zip|exe)
>     <http://sun.com/.*%5C.%28zip%7Cexe%29>        4320 80%     43200 
>      reload-into-ims
>     refresh_pattern -i google\.com.*\.(zip|exe)     4320    80%  
>      43200   reload-into-ims
>     refresh_pattern -i macromedia\.com.*\.(zip|exe) 4320    80%    43200
>     reload-into-ims
>     # Other long-lived items
>     refresh_pattern -i
>     \.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4)(\?|$)          
>               14400
>     99%     518400  ignore-no-store override-expire ignore-reload
>     reload-into-ims ignore-private ignore-must-revalidate
>     refresh_pattern -i
>     \.((m?|x?|s?)htm(l?)|css|js|xml|php|json)(\?|$)    10080   90%   
>      86400
>     ignore-no-store override-expire override-lastmod reload-into-ims
>     ignore-private ignore-must-revalidate
>     # Default patterns
>     refresh_pattern -i (/cgi-bin/|\?)       0    0%      0
>     refresh_pattern .       0       20%     10080  override-lastmod
>     reload-into-ims
>
>     The example above also requires some additional cached-related
>     parameters to be changed.
>
>     Also, you strictly recommended to research average users activity AND
>     play around VARY http headers.
>
>     And others.
>
>     Each squid setup is place-specific. And depending your access/deny
>     lists, security policy, users/network activity etc.etc.etc.
>
>     WBR, Yuri
>
>     PS. Your question has NO simple answer. Beware - copy-n-paste any
>     foreign config can not guarantee the same results for YOU.
>
>     14.03.15 1:52, Alberto Perez ?????:
>     > Can you share more details about "Agressive dynamic content
>     > caching requires some special tweaks" I am very interested.
>     >
>     > Thanks
>     >
>     >
>     >
>     > On 3/13/15, Yuri Voinov <yvoinov at gmail.com
>     <mailto:yvoinov at gmail.com>> wrote:
>     >
>     >
>     > 13.03.15 23:33, Amos Jeffries ?????:
>     >>>> On 14/03/2015 5:47 a.m., Monah Baki wrote:
>     >>>>
>     >>>> <snip>
>     >>>>
>     >>>>> half_closed_clients off quick_abort_min 0 KB
>     >>>>> quick_abort_max 0 KB vary_ignore_expire on reload_into_ims
>     >>>>> on memory_pools off cache_mem 4096 MB visible_hostname
>     >>>>> isn-phc-cache minimum_object_size 0 bytes
>     >>>>
>     >>>>> maximum_object_size 512 MB maximum_object_size 512 KB
>     >>>>
>     >>>> KB value overwriting MB value.
>     >>>>
>     >>>>
>     >>>>> ipcache_size 1024 ipcache_low 90 ipcache_high 95
>     >>>>> cache_swap_low 98 cache_swap_high 100 fqdncache_size 16384
>     >>>>> retry_on_error on offline_mode off logfile_rotate 10
>     >>>>> dns_nameservers 8.8.8.8 41.78.211.30
>     >>>>>
>     >>>>>
>     >>>>>
>     >>>>>
>     >>>>> access.log:
>     >>>>>
>     >>>>> 1426267535.210    198 10.0.0.23 TCP_MISS/200 412 GET
>     >>>>> http://jadserve.postrelease.com/trk.gif? -
>     >>>>> ORIGINAL_DST/54.225.133.227 <http://54.225.133.227>
>     image/gif 1426267535.211
>     >>>>> 198 10.0.0.23 TCP_MISS/200 412 GET
>     >>>>> http://jadserve.postrelease.com/trk.gif? -
>     >>>>> ORIGINAL_DST/54.225.133.227 <http://54.225.133.227>
>     image/gif 1426267535.211
>     >>>>> 198 10.0.0.23 TCP_MISS/200 412 GET
>     >>>>> http://jadserve.postrelease.com/trk.gif? -
>     >>>>> ORIGINAL_DST/54.225.133.227 <http://54.225.133.227>
>     image/gif 1426267535.223
>     >>>>> 301 10.0.0.23 TCP_MISS/200 222 GET
>     >>>>> http://rma-api.gravity.com/v1/beacons/log? -
>     >>>>> ORIGINAL_DST/80.239.148.18 <http://80.239.148.18> text/html
>     1426267535.244    195
>     >>>>> 10.0.0.23 TCP_MISS/200 412 GET
>     >>>>> http://jadserve.postrelease.com/trk.gif? -
>     >>>>> ORIGINAL_DST/54.225.133.227 <http://54.225.133.227> image/gif
>     >>>>
>     >>>>
>     >>>> Lots of Akamai hosted requests. Akamai play tricks with DNS
>     >>>> responses.
>     > In my installation I've used local Unbound DNS cache and, before
>     > it, forced DNS interception to him with Cisco. :)
>     >
>     > So, I don't care about any hosts DNS quirks. ;)
>     >
>     >>>>
>     >>>> Check your cache.log for security warnings;
>     >>>> <http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>     >>>>
>     >>>>
>     >>>>
>     Note that objects failing the Host validation are not cacheable.
>     >>>>
>     >>>>
>     >>>>> 1426267535.333    423 10.0.0.23 TCP_MISS/200 1420 GET
>     >>>>> http://hpr.outbrain.com/utils/get? -
>     >>>>> ORIGINAL_DST/50.31.185.42 <http://50.31.185.42> text/x-json
>     1426267535.345    412
>     >>>>> 10.0.0.23 TCP_MISS/200 11179 GET
>     >>>>> http://p.visualrevenue.com/? - ORIGINAL_DST/50.31.185.40
>     <http://50.31.185.40>
>     >>>>> text/javascript 1426267535.346 411 10.0.0.23
>     >>>>> TCP_MISS/200 423 GET http://t1.visualrevenue.com/? -
>     >>>>> ORIGINAL_DST/64.74.232.44 <http://64.74.232.44> image/gif
>     >>>>
>     >>>> Not sure about them. Maybe genuine MISS, maybe not.
>     >
>     > Agressive dynamic content caching requires some special tweaks. ;)
>     >
>     >>>>
>     >>>> It could also be the issues Antony pointed out, with the
>     >>>> objects just naturally not being cacheable.
>     >>>>
>     >>>>
>     >>>>> 1426267535.363    128 10.0.0.23 TCP_REFRESH_UNMODIFIED/304
>     >>>>> 327 GET
>     >>>>>
>     http://z.cdn.turner.com/cnn/.element/widget/video/videoapi/api/js/vendor/jquery.ba-bbq.js
>     >>>>>
>     >>>>>
>     >
>     >>>>>
>     - - ORIGINAL_DST/80.239.152.153 <http://80.239.152.153>
>     application/x-javascript
>     >>>>
>     >>>> There is a hit.
>     >>>>
>     >>>> I guess you are new to Squid-3 ? Squid is HTTP/1.1 compliant
>     >>>> now and the caching rules are slightly different from
>     >>>> requirements on HTTP/1.0 software. A lot of content that
>     >>>> previously could not be stored now can (authenticated,
>     >>>> private, no-cache, etc.). But being sensitive info also
>     >>>> requires revalidation in order to be used, so they show up
>     >>>> like the above.
>     >>>>
>     >>>> Amos
>     >>>>
>     >>>> _______________________________________________ squid-users
>     >>>> mailing list squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >>>> http://lists.squid-cache.org/listinfo/squid-users
>     >>>>
>     >> _______________________________________________ squid-users
>     >> mailing list squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >> http://lists.squid-cache.org/listinfo/squid-users
>     >>
>     -----BEGIN PGP SIGNATURE-----
>     Version: GnuPG v2
>
>     iQEcBAEBAgAGBQJVA0InAAoJENNXIZxhPexG6JAIALq2tAxa9Vawr1/Rkojl0UFj
>     HQF9p/4mk0ZHPnL4zkV6h/Ctg/s+AgK+O/H38ncn+2JS4eyiZfSHLOxmxkmrKi11
>     av/yjG++JGnhQkic/3y7ETOSkvaDuAbDP+Iwrtuc+kBpJz54No9Pu37oVlIOdMLZ
>     uv/8Bpk9uQEc3kE5FCgCmM2nIr2tuxr6opK6T5DZ2TvcqnQin752P60R91iS7unF
>     XHX3tsGsFvrKflEEC7w1xDRn3u3kSGrx+gPpktA0dv6vT8ATXqPEV5+anIEZVfLZ
>     NKDIwoeSNHYMMknlK7QTUlcNjuq+UXmfcO3mp+eraUQbGRkxwqTPxRwvIqp/43U=
>     =VW9B
>     -----END PGP SIGNATURE-----
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150314/b7d80ad4/attachment.htm>

From squid3 at treenet.co.nz  Sat Mar 14 10:40:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Mar 2015 23:40:33 +1300
Subject: [squid-users] i hope to use external ACL + ldap at squid 3.5.2,
 but i don't find ext_ldap_group_acl and basic_ldap_auth from
 /squid/libexec/
In-Reply-To: <5503DF64.9040201@yahoo.com>
References: <55031248.8080700@yahoo.com> <5503C21C.60906@treenet.co.nz>
 <5503DF64.9040201@yahoo.com>
Message-ID: <55041021.3080902@treenet.co.nz>

On 14/03/2015 8:12 p.m., johnzeng wrote:
> 
> Hi Amos:
> 
>               How i will  build LDAP libraries for squid ?
> 
>               We built openldap-2.4.39 environment .
> 
>                We operated  command " source /etc/ld.so.conf " ,
> include  /accerater/env/ldap/lib  /accerater/env/ldap/include
> 
> /accerater/env/ldap/lib
> 
> liblber-2.4.so.2       liblber.a   liblber.so libldap-2.4.so.2.10.2 
> libldap.la libldap_r-2.4.so.2.10.2  libldap_r.la  libldap.so
> liblber-2.4.so.2.10.2  liblber.la  libldap-2.4.so.2
> libldap.a              libldap_r-2.4.so.2 libldap_r.a             
> libldap_r.so
> 
> /accerater/env/ldap/include
> 
> lber.h  lber_types.h  ldap_cdefs.h  ldap_features.h  ldap.h
> ldap_schema.h  ldap_utf8.h  ldif.h  slapi-plugin.h
> 
> 
> This is new configure in according to your advisement
> 
> 
> ./configure --prefix=/accerater/webcache3

Add these:

 CXXFLAGS="-I/accerater/env/ldap/include"
 LDFLAGS="-L/accerater/env/ldap/lib"


Amos


From jlay at slave-tothe-box.net  Sat Mar 14 11:57:46 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 14 Mar 2015 05:57:46 -0600
Subject: [squid-users] Quick peek/splice logging question
Message-ID: <1426334266.3953.4.camel@JamesiMac>

Hey all,

Today I switched my setup from:

ssl_bump splice broken_sites
ssl_bump bump all

to

ssl_bump splice all
ssl_bump bump all

and this appears to be working (broken sites were ones that just would
not bump.  Now in my squid logs I see:

Mar 14 05:45:50 gateway (squid-1): 192.168.1.110 - -
[14/Mar/2015:05:45:50 -0600] "#026#003#001  HTTP/1.1" 400 3572
TAG_NONE:HIER_NONE

I tried changing my log format to:

logformat common %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%
Sh %ssl::>cert_subject

But still no go.  Any help would rock...thank you.

James




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150314/5f923b7f/attachment.htm>

From johnzeng2013 at yahoo.com  Sat Mar 14 12:07:54 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 20:07:54 +0800
Subject: [squid-users] i hope to use external ACL + ldap at squid 3.5.2,
 but i don't find ext_ldap_group_acl and basic_ldap_auth from
 /squid/libexec/
In-Reply-To: <55041021.3080902@treenet.co.nz>
References: <55031248.8080700@yahoo.com> <5503C21C.60906@treenet.co.nz>
 <5503DF64.9040201@yahoo.com> <55041021.3080902@treenet.co.nz>
Message-ID: <5504249A.6040706@yahoo.com>



Hello Amos:

                      Thanks again , i sloved the problem in according 
to your advisement .




On 14/03/2015 8:12 p.m., johnzeng wrote:
>> Hi Amos:
>>
>>                How i will  build LDAP libraries for squid ?
>>
>>                We built openldap-2.4.39 environment .
>>
>>                 We operated  command " source /etc/ld.so.conf " ,
>> include  /accerater/env/ldap/lib  /accerater/env/ldap/include
>>
>> /accerater/env/ldap/lib
>>
>> liblber-2.4.so.2       liblber.a   liblber.so libldap-2.4.so.2.10.2
>> libldap.la libldap_r-2.4.so.2.10.2  libldap_r.la  libldap.so
>> liblber-2.4.so.2.10.2  liblber.la  libldap-2.4.so.2
>> libldap.a              libldap_r-2.4.so.2 libldap_r.a
>> libldap_r.so
>>
>> /accerater/env/ldap/include
>>
>> lber.h  lber_types.h  ldap_cdefs.h  ldap_features.h  ldap.h
>> ldap_schema.h  ldap_utf8.h  ldif.h  slapi-plugin.h
>>
>>
>> This is new configure in according to your advisement
>>
>>
>> ./configure --prefix=/accerater/webcache3
> Add these:
>
>   CXXFLAGS="-I/accerater/env/ldap/include"
>   LDFLAGS="-L/accerater/env/ldap/lib"
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From johnzeng2013 at yahoo.com  Sat Mar 14 12:34:27 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 20:34:27 +0800
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
	problem
Message-ID: <55042AD3.1010806@yahoo.com>


Hello All:

I try to compile helpers/external_acl/session too , and Db evirontment
is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .

But i can't confirm .

./configure --prefix=/accerater/webcache3
--enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter
--enable-storeio=aufs,rock --enable-wccpv2 --with-large-files
--enable-removal-policies=lru,heap --enable-async-io=128
--enable-http-violations --with-maxfd=65536 --enable-large-cache-files
--enable-delay-pools --enable-forward-log --with-pthreads
LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers
--enable-epoll --enable-ltdl-convenience --with-included-ltdl
--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped
CXXFLAGS="-I/accerater/env/ldap/include -I/accerater/env/bdb/include"
LDFLAGS="-L/accerater/env/ldap/lib -L/accerater/env/bdb/lib"


This is error compile info

libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
-D_FILE_OFFSET_BITS=64 -I/accerater/env/bdb/include -march=native -m32
-o ext_session_acl ext_session_acl.o -L/accerater/env/bdb/lib
../../../compat/.libs/libcompat-squid.a
ext_session_acl.o: In function `init_db()':
ext_session_acl.cc:(.text+0x5a): undefined reference to `db_env_create'
ext_session_acl.cc:(.text+0x101): undefined reference to `db_create'
ext_session_acl.cc:(.text+0x1d6): undefined reference to `db_create'



From johnzeng2013 at yahoo.com  Sat Mar 14 12:39:54 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 20:39:54 +0800
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
	problem
In-Reply-To: <55042AD3.1010806@yahoo.com>
References: <55042AD3.1010806@yahoo.com>
Message-ID: <55042C1A.3020809@yahoo.com>


Hello All

i read the link

http://www.squid-cache.org/mail-archive/squid-users/201112/0339.html

but i cat config.log | grep HAVE_DB_H in according to the advisement
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1
> | #define HAVE_DB_H 1




> Hello All:
>
> I try to compile helpers/external_acl/session too , and Db evirontment
> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .
>
> But i can't confirm .
>
> ./configure --prefix=/accerater/webcache3
> --enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter
> --enable-storeio=aufs,rock --enable-wccpv2 --with-large-files
> --enable-removal-policies=lru,heap --enable-async-io=128
> --enable-http-violations --with-maxfd=65536 --enable-large-cache-files
> --enable-delay-pools --enable-forward-log --with-pthreads
> LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers
> --enable-epoll --enable-ltdl-convenience --with-included-ltdl
> --enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped
> CXXFLAGS="-I/accerater/env/ldap/include -I/accerater/env/bdb/include"
> LDFLAGS="-L/accerater/env/ldap/lib -L/accerater/env/bdb/lib"
>
>
> This is error compile info
>
> libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments
> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
> -D_FILE_OFFSET_BITS=64 -I/accerater/env/bdb/include -march=native -m32
> -o ext_session_acl ext_session_acl.o -L/accerater/env/bdb/lib
> ../../../compat/.libs/libcompat-squid.a
> ext_session_acl.o: In function `init_db()':
> ext_session_acl.cc:(.text+0x5a): undefined reference to `db_env_create'
> ext_session_acl.cc:(.text+0x101): undefined reference to `db_create'
> ext_session_acl.cc:(.text+0x1d6): undefined reference to `db_create'
>



From rwmaillists at googlemail.com  Sat Mar 14 13:06:46 2015
From: rwmaillists at googlemail.com (RW)
Date: Sat, 14 Mar 2015 13:06:46 +0000
Subject: [squid-users] keep data after delete swap.state
References: <1426137330809-4670344.post@n4.nabble.com>
 <55012627.1030107@treenet.co.nz>
Message-ID: <20150314130646.4c412a99@gumby.homeunix.com>

On Thu, 12 Mar 2015 18:37:43 +1300
Amos Jeffries wrote:

> On 12/03/2015 6:15 p.m., HackXBack wrote:
> > if there is error in swap.state file and want to recreate it what
> > is the option to do that without losing data on hdd ?
> 
> 1) stop Squid
> 2) manually erase the broken swap.state file(s)
> 3) run squid -z
> 4) start Squid

If you're using LRU expiry, the "LRU reference age" may well
plummet and be erratic for a while. This is because the objects get
queued in the order they are found on the disk. It will eventually sort
itself out as new and accessed object go to the back of the queue. 

I mention this because the first time I saw it it worried me enough
that I ended-up deleting the cache to be on the safe side.



From squid3 at treenet.co.nz  Sat Mar 14 14:11:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Mar 2015 03:11:00 +1300
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
 problem
In-Reply-To: <55042C1A.3020809@yahoo.com>
References: <55042AD3.1010806@yahoo.com> <55042C1A.3020809@yahoo.com>
Message-ID: <55044174.9090505@treenet.co.nz>

On 15/03/2015 1:39 a.m., johnzeng wrote:
> 
> Hello All
> 
> i read the link
> 
> http://www.squid-cache.org/mail-archive/squid-users/201112/0339.html
> 
> but i cat config.log | grep HAVE_DB_H in according to the advisement
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
>> | #define HAVE_DB_H 1
> 
> 
> 
> 
>> Hello All:
>>
>> I try to compile helpers/external_acl/session too , and Db evirontment
>> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .
>>
>> But i can't confirm .
>>
>> ./configure --prefix=/accerater/webcache3
>> --enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter
>> --enable-storeio=aufs,rock --enable-wccpv2 --with-large-files
>> --enable-removal-policies=lru,heap --enable-async-io=128
>> --enable-http-violations --with-maxfd=65536 --enable-large-cache-files
>> --enable-delay-pools --enable-forward-log --with-pthreads
>> LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers
>> --enable-epoll --enable-ltdl-convenience --with-included-ltdl
>> --enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped
>> CXXFLAGS="-I/accerater/env/ldap/include -I/accerater/env/bdb/include"
>> LDFLAGS="-L/accerater/env/ldap/lib -L/accerater/env/bdb/lib"
>>
>>
>> This is error compile info
>>
>> libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments
>> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
>> -D_FILE_OFFSET_BITS=64 -I/accerater/env/bdb/include -march=native -m32
>> -o ext_session_acl ext_session_acl.o -L/accerater/env/bdb/lib
>> ../../../compat/.libs/libcompat-squid.a
>> ext_session_acl.o: In function `init_db()':
>> ext_session_acl.cc:(.text+0x5a): undefined reference to `db_env_create'
>> ext_session_acl.cc:(.text+0x101): undefined reference to `db_create'
>> ext_session_acl.cc:(.text+0x1d6): undefined reference to `db_create'


The BDB library seems not to be built or found at the -L location you
provided.

When its found by ./configure an -lbdb or similar entry would be added
for it after the libcompat-squid.a name.


Amos


From johnzeng2013 at yahoo.com  Sat Mar 14 14:25:27 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 22:25:27 +0800
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
 problem
In-Reply-To: <55044174.9090505@treenet.co.nz>
References: <55042AD3.1010806@yahoo.com> <55042C1A.3020809@yahoo.com>
 <55044174.9090505@treenet.co.nz>
Message-ID: <550444D7.5040707@yahoo.com>

Hello Amos:


                      This is bdb lib info .

/accerater/env/bdb/lib# ls
libdb-4.8.a  libdb-4.8.la  libdb-4.8.so  libdb-4.so  libdb.a 
libdb_cxx-4.8.a  libdb_cxx-4.8.la  libdb_cxx-4.8.so  libdb_cxx-4.so 
libdb_cxx.a  libdb_cxx.so  libdb.so


/accerater/env/bdb/include# ls
db_cxx.h  db.h


On 15/03/2015 1:39 a.m., johnzeng wrote:
>> Hello All
>>
>> i read the link
>>
>> http://www.squid-cache.org/mail-archive/squid-users/201112/0339.html
>>
>> but i cat config.log | grep HAVE_DB_H in according to the advisement
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>
>>
>>
>>> Hello All:
>>>
>>> I try to compile helpers/external_acl/session too , and Db evirontment
>>> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .
>>>
>>> But i can't confirm .
>>>
>>> ./configure --prefix=/accerater/webcache3
>>> --enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter
>>> --enable-storeio=aufs,rock --enable-wccpv2 --with-large-files
>>> --enable-removal-policies=lru,heap --enable-async-io=128
>>> --enable-http-violations --with-maxfd=65536 --enable-large-cache-files
>>> --enable-delay-pools --enable-forward-log --with-pthreads
>>> LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers
>>> --enable-epoll --enable-ltdl-convenience --with-included-ltdl
>>> --enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped
>>> CXXFLAGS="-I/accerater/env/ldap/include -I/accerater/env/bdb/include"
>>> LDFLAGS="-L/accerater/env/ldap/lib -L/accerater/env/bdb/lib"
>>>
>>>
>>> This is error compile info
>>>
>>> libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments
>>> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
>>> -D_FILE_OFFSET_BITS=64 -I/accerater/env/bdb/include -march=native -m32
>>> -o ext_session_acl ext_session_acl.o -L/accerater/env/bdb/lib
>>> ../../../compat/.libs/libcompat-squid.a
>>> ext_session_acl.o: In function `init_db()':
>>> ext_session_acl.cc:(.text+0x5a): undefined reference to `db_env_create'
>>> ext_session_acl.cc:(.text+0x101): undefined reference to `db_create'
>>> ext_session_acl.cc:(.text+0x1d6): undefined reference to `db_create'
>
> The BDB library seems not to be built or found at the -L location you
> provided.
>
> When its found by ./configure an -lbdb or similar entry would be added
> for it after the libcompat-squid.a name.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From johnzeng2013 at yahoo.com  Sat Mar 14 14:41:04 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Sat, 14 Mar 2015 22:41:04 +0800
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
 problem
In-Reply-To: <55044174.9090505@treenet.co.nz>
References: <55042AD3.1010806@yahoo.com> <55042C1A.3020809@yahoo.com>
 <55044174.9090505@treenet.co.nz>
Message-ID: <55044880.5040101@yahoo.com>



  Hello Amos :

                     When i run ./configure ..... , this is screen info .


checking whether dbopen is declared... no
checking if dbopen needs -ldb... no



configure: WARNING: Could not find krb5-config in path
configure: WARNING: ldap.h: accepted by the compiler, rejected by the preprocessor!
configure: WARNING: ldap.h: proceeding with the compiler's result
configure: WARNING: lber.h: accepted by the compiler, rejected by the preprocessor!
configure: WARNING: lber.h: proceeding with the compiler's result
configure: WARNING: Neither SASL nor SASL2 found
configure: WARNING: Samba wbinfo not found in default location. ext_wbinfo_group_acl may not work on this machine
configure: WARNING: cppunit does not appear to be installed. squid does not require this, but code testing with 'make check' will fail.
configure: WARNING: db.h: accepted by the compiler, rejected by the preprocessor!
configure: WARNING: db.h: proceeding with the compiler's result


checking db.h usability... yes
checking db.h presence... no
checking for db.h... yes
checking for db_185.h... (cached) no

checking db_185.h usability... no
checking db_185.h presence... no
checking for db_185.h... no

checking whether dbopen is declared... no
checking if dbopen needs -ldb... no



configure: BUILD C++ FLAGS: -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -I/accerater/env/ldap/include -I/accerater/env/bdb/include -march=native
configure: BUILD EXTRA C++ FLAGS: -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Werror -pipe -D_REENTRANT
configure: BUILD Tools C++ FLAGS: -march=native -m32 -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -I/accerater/env/ldap/include -I/accerater/env/bdb/include -march=native


there aren't the part -lbdb

>> Hello All
>>
>> i read the link
>>
>> http://www.squid-cache.org/mail-archive/squid-users/201112/0339.html
>>
>> but i cat config.log | grep HAVE_DB_H in according to the advisement
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>> | #define HAVE_DB_H 1
>>
>>
>>
>>> Hello All:
>>>
>>> I try to compile helpers/external_acl/session too , and Db evirontment
>>> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .
>>>
>>> But i can't confirm .
>>>
>>> ./configure --prefix=/accerater/webcache3
>>> --enable-follow-x-forwarded-for --enable-snmp --enable-linux-netfilter
>>> --enable-storeio=aufs,rock --enable-wccpv2 --with-large-files
>>> --enable-removal-policies=lru,heap --enable-async-io=128
>>> --enable-http-violations --with-maxfd=65536 --enable-large-cache-files
>>> --enable-delay-pools --enable-forward-log --with-pthreads
>>> LIBS=-ltcmalloc --enable-url-rewrite-helpers --enable-log-daemon-helpers
>>> --enable-epoll --enable-ltdl-convenience --with-included-ltdl
>>> --enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped
>>> CXXFLAGS="-I/accerater/env/ldap/include -I/accerater/env/bdb/include"
>>> LDFLAGS="-L/accerater/env/ldap/lib -L/accerater/env/bdb/lib"
>>>
>>>
>>> This is error compile info
>>>
>>> libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments
>>> -Wshadow -Werror -pipe -D_REENTRANT -m32 -D_LARGEFILE_SOURCE
>>> -D_FILE_OFFSET_BITS=64 -I/accerater/env/bdb/include -march=native -m32
>>> -o ext_session_acl ext_session_acl.o -L/accerater/env/bdb/lib
>>> ../../../compat/.libs/libcompat-squid.a
>>> ext_session_acl.o: In function `init_db()':
>>> ext_session_acl.cc:(.text+0x5a): undefined reference to `db_env_create'
>>> ext_session_acl.cc:(.text+0x101): undefined reference to `db_create'
>>> ext_session_acl.cc:(.text+0x1d6): undefined reference to `db_create'
>
> The BDB library seems not to be built or found at the -L location you
> provided.
>
> When its found by ./configure an -lbdb or similar entry would be added
> for it after the libcompat-squid.a name.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From stan.prescott at gmail.com  Sun Mar 15 13:51:07 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 15 Mar 2015 09:51:07 -0400
Subject: [squid-users] Squid 3.5.2 will only start with empty cache
Message-ID: <CANLNtGRp8wLXnwYq42PhSAEQOWHk-a7-N0GjvQcwF2Br8PmCRA@mail.gmail.com>

I have been trying to get Squid 3.5.2 to work with the Smoothwall Express
3.1 Linux firewall distribution. Specifically, I have modified the Squid
version included with Smoothwall Express 3.1 to enable HTTPS caching. I
have had this working successfully up to Squid version 3.4.10. Now with
trying to upgrade to Squid 3.5.2 I am having problems that I didn't
encounter with prior versions of Squid.

The first issue I had, which is now resolved, was improper permissions of
the shm folder (in SWE found in /dev/shm). Changing the folder permissions
to Squid user and group allowed Squid 3.5.2 to start. However, now it will
only start with an empty cache directory. Once it starts with an empty
cache directory, it seems to function correctly as far as caching SSL
encrypted web pages. However, if Squid needs to be restarted for any
reason, it will not restart until the cache directory
(/var/spool/squid/cache) is emptied.

The error I am getting when trying to start Squid 3.5.2 without an empty
cache is

*2015/03/14 00:29:47 kid1| Current Directory is /*

*2015/03/14 00:29:47 kid1| Starting Squid Cache version 3.5.2 for
i586-pc-linux-gnu...*
*2015/03/14 00:29:47 kid1| Service Name: squid*
*2015/03/14 00:29:47 kid1| Process ID 7261*
*2015/03/14 00:29:47 kid1| Process Roles: worker*
*2015/03/14 00:29:47 kid1| With 1024 file descriptors available*
*2015/03/14 00:29:47 kid1| Initializing IP Cache...*
*2015/03/14 00:29:47 kid1| DNS Socket created at 0.0.0.0, FD 8*
*2015/03/14 00:29:47 kid1| Adding nameserver 127.0.0.1 from
/etc/resolv.conf*
*2015/03/14 00:29:47 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
processes*
*FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*

*Squid Cache (Version 3.5.2): Terminated abnormally.*
*CPU Usage: 0.027 seconds = 0.020 user + 0.007 sys*
*Maximum Resident Size: 26752 KB*
*Page faults with physical i/o: 0*
*2015/03/14 00:29:47.830 kid1| Acl.cc(380) ~ACL: freeing ACL *

This is my squid.conf file with SSL caching using ssl-bump enabled.

*visible_hostname smoothwall*

*# Uncomment the following to send debug info to /var/log/squid/cache.log*
*debug_options ALL,1 33,2 28,9*

*# ACCESS CONTROLS*
*# ----------------------------------------------------------------*
*acl localhostgreen src 192.168.100.1*
*acl localnetgreen src 192.168.100.0/24 <http://192.168.100.0/24>*

*acl SSL_ports port 445 443 441 563*
*acl Safe_ports port 80      # http*
*acl Safe_ports port 81      # smoothwall http*
*acl Safe_ports port 21      # ftp *
*acl Safe_ports port 445 443 441 563 # https, snews*
*acl Safe_ports port 70      # gopher*
*acl Safe_ports port 210         # wais  *
*acl Safe_ports port 1025-65535 # unregistered ports*
*acl Safe_ports port 280        # http-mgmt*
*acl Safe_ports port 488        # gss-http *
*acl Safe_ports port 591        # filemaker*
*acl Safe_ports port 777        # multiling http*

*acl CONNECT method CONNECT*

*# TAG: http_access*
*# ----------------------------------------------------------------*


*http_access deny !Safe_ports*
*http_access deny CONNECT !SSL_ports*

*http_access allow localnetgreen*
*http_access allow CONNECT localnetgreen*

*http_access allow localhostgreen*
*http_access allow CONNECT localhostgreen*

*# http_port and https_port*
*#----------------------------------------------------------------------------*

*# A random port for forward-proxy port needed for SSL*
*http_port 8081*

*http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*ssl_bump server-first all*

*ssl_bump none localhostgreen*
*sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
*sslcrtd_children 5*

*sslproxy_session_cache_size 4 MB*

*http_access deny all*

*cache_replacement_policy heap GDSF*
*memory_replacement_policy heap GDSF*

*# CACHE OPTIONS*
*#
----------------------------------------------------------------------------*
*cache_effective_user squid*
*cache_effective_group squid*

*cache_swap_high 100*
*cache_swap_low 80*

*cache_mem 8 MB*
*maximum_object_size_in_memory 512 KB*

*cache_access_log /var/log/squid/access.log*
*cache_log /var/log/squid/cache.log*
*cache_store_log none*
*error_directory /usr/share/errors/en-us*
*log_mime_hdrs off*

*cache_dir diskd /var/spool/squid/cache 1024 16 256 Q1=64 Q2=72*

*request_header_access Content-Type allow all*
*request_header_access Date allow all*
*request_header_access Host allow all*
*request_header_access If-Modified-Since allow all*
*request_header_access Pragma allow all*
*request_header_access Accept allow all*
*request_header_access Accept-Charset allow all*
*request_header_access Accept-Encoding allow all*
*request_header_access Accept-Language allow all*
*request_header_access Connection allow all*
*request_header_access All allow all*

*maximum_object_size 33 MB*

*minimum_object_size 0 KB*


*request_body_max_size 0 KB*

*# OTHER OPTIONS*
*#
----------------------------------------------------------------------------*
*forwarded_for off*

*pid_filename /var/run/squid.pid*

*shutdown_lifetime 3 seconds*
*icp_port 3130*

*half_closed_clients off*

*umask 022*

*logfile_rotate 0*

*strip_query_terms off*


Any help would be greatly appresciated.

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150315/e1e91efe/attachment.htm>

From eliezer at ngtech.co.il  Sun Mar 15 14:21:11 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 15 Mar 2015 16:21:11 +0200
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraris/CDNs Booster
In-Reply-To: <54AA918C.10300@gmail.com>
References: <1418374260739-4668683.post@n4.nabble.com>
 <004501d01673$0f445150$2dccf3f0$@netstream.ps>
 <1418415837677-4668693.post@n4.nabble.com>
 <1418628416193-4668707.post@n4.nabble.com>
 <1418825003303-4668738.post@n4.nabble.com>
 <1419155198462-4668803.post@n4.nabble.com>
 <1420464046420-4668929.post@n4.nabble.com> <54AA918C.10300@gmail.com>
Message-ID: <55059557.6010702@ngtech.co.il>

Hey Yuti,

If indeed regex were the solution to all patterns the humanity state was 
very poor knowledge and many other related areas.
Also to provide a bulletproof solution you would need more then couple 
hours of sitting in-front of the PC.

Some argue about the benefit of hardcoded commercial solutions but still 
they give grounds for many users and even if some do not like their 
stability and would like to have a "dreamy" solution I would not just 
run after a dream just because someone was inventing or thinking about 
it in a split of a sec.

All The Bests,
Eliezer Croitoru

On 05/01/2015 15:28, Yuri Voinov wrote:
> This is not right.
>
> We HAVE good cache solution in our current Squid proxy.
>
> It named storeid_file_rewrite. And it built in Squid by default.
>
> All we need - right config for it.
>
> We have it. It's quick and easy solution with half-dozen regular
> expressions. And we achieve 70% hit-ratio for dynamic content.
>
> So, we don't need any hardcoded commercial analogue.
>
> Bye.



From yvoinov at gmail.com  Sun Mar 15 14:29:15 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 15 Mar 2015 20:29:15 +0600
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraris/CDNs Booster
In-Reply-To: <55059557.6010702@ngtech.co.il>
References: <1418374260739-4668683.post@n4.nabble.com>
 <004501d01673$0f445150$2dccf3f0$@netstream.ps>
 <1418415837677-4668693.post@n4.nabble.com>
 <1418628416193-4668707.post@n4.nabble.com>
 <1418825003303-4668738.post@n4.nabble.com>
 <1419155198462-4668803.post@n4.nabble.com>
 <1420464046420-4668929.post@n4.nabble.com> <54AA918C.10300@gmail.com>
 <55059557.6010702@ngtech.co.il>
Message-ID: <5505973B.5000609@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I'm talking not about universal silver-bullit solution.

Just that everyone is looking for and find their own solution. It was
only about the same techniques that are used to achieve the goal.

What suits me - not necessarily suit everyone. Fred disagree with me,
but it's true.

Consequently, everyone will find their own solution.

15.03.15 20:21, Eliezer Croitoru ?????:
> Hey Yuti,
> 
> If indeed regex were the solution to all patterns the humanity 
> state was very poor knowledge and many other related areas. Also
> to provide a bulletproof solution you would need more then couple 
> hours of sitting in-front of the PC.
> 
> Some argue about the benefit of hardcoded commercial solutions but 
> still they give grounds for many users and even if some do not
> like their stability and would like to have a "dreamy" solution I
> would not just run after a dream just because someone was inventing
> or thinking about it in a split of a sec.
> 
> All The Bests, Eliezer Croitoru
> 
> On 05/01/2015 15:28, Yuri Voinov wrote:
>> This is not right.
>> 
>> We HAVE good cache solution in our current Squid proxy.
>> 
>> It named storeid_file_rewrite. And it built in Squid by default.
>> 
>> All we need - right config for it.
>> 
>> We have it. It's quick and easy solution with half-dozen regular
>>  expressions. And we achieve 70% hit-ratio for dynamic content.
>> 
>> So, we don't need any hardcoded commercial analogue.
>> 
>> Bye.
> 
> _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVBZc7AAoJENNXIZxhPexGZ+0IAKKwTpX5ZSdsV5s+YMTXe3Fc
rEKkqZhI3oiLdGQDfmOpeHEMYF8CJbrO7fR1WAw4SRpbejgePIjXwiDNfo4IpDxi
P88/9MYjZiCG63vevjUrUhBntBADKHK26EplJS5jAHvP4cmnbvYwm8IGLoUjx6gL
IiXi7VCPYdloeBf4EJrORblAnSpADBiFiD0kNPxj+A1/eNUOyKJtkRLkF+fQ7gCi
s2Xu7Y+SAstKL5rQwDpe+CYUuzPinZfziiVMlE7Dh1sOWvfarSamOLl26VsqJzdn
luOE1PAeq6RJxLCLt/PkB7k8G0U4BoeWirKySEPRBiVWfUHOq66Ye5M4KDIIGic=
=hUU4
-----END PGP SIGNATURE-----


From eliezer at ngtech.co.il  Sun Mar 15 14:35:48 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 15 Mar 2015 16:35:48 +0200
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
 problem
In-Reply-To: <55042AD3.1010806@yahoo.com>
References: <55042AD3.1010806@yahoo.com>
Message-ID: <550598C4.9030607@ngtech.co.il>

Hey,

What OS are you building squid on?

Eliezer

On 14/03/2015 14:34, johnzeng wrote:
> Hello All:
>
> I try to compile helpers/external_acl/session too , and Db evirontment
> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb version .
>
> But i can't confirm .



From yvoinov at gmail.com  Sun Mar 15 15:18:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 15 Mar 2015 21:18:09 +0600
Subject: [squid-users] Squid 2.7,
 3.4 and 3.5 Videos/Music/Images/Libraris/CDNs Booster
In-Reply-To: <55059557.6010702@ngtech.co.il>
References: <1418374260739-4668683.post@n4.nabble.com>
 <004501d01673$0f445150$2dccf3f0$@netstream.ps>
 <1418415837677-4668693.post@n4.nabble.com>
 <1418628416193-4668707.post@n4.nabble.com>
 <1418825003303-4668738.post@n4.nabble.com>
 <1419155198462-4668803.post@n4.nabble.com>
 <1420464046420-4668929.post@n4.nabble.com> <54AA918C.10300@gmail.com>
 <55059557.6010702@ngtech.co.il>
Message-ID: <5505A2B1.6050409@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

And yes, this is VERY old thread. Somebody's is brake.... ;)


15.03.15 20:21, Eliezer Croitoru ?????:
> Hey Yuti,
> 
> If indeed regex were the solution to all patterns the humanity
> state was very poor knowledge and many other related areas. Also to
> provide a bulletproof solution you would need more then couple 
> hours of sitting in-front of the PC.
> 
> Some argue about the benefit of hardcoded commercial solutions but
> still they give grounds for many users and even if some do not like
> their stability and would like to have a "dreamy" solution I would
> not just run after a dream just because someone was inventing or
> thinking about it in a split of a sec.
> 
> All The Bests, Eliezer Croitoru
> 
> On 05/01/2015 15:28, Yuri Voinov wrote:
>> This is not right.
>> 
>> We HAVE good cache solution in our current Squid proxy.
>> 
>> It named storeid_file_rewrite. And it built in Squid by default.
>> 
>> All we need - right config for it.
>> 
>> We have it. It's quick and easy solution with half-dozen regular 
>> expressions. And we achieve 70% hit-ratio for dynamic content.
>> 
>> So, we don't need any hardcoded commercial analogue.
>> 
>> Bye.
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVBaKxAAoJENNXIZxhPexGk7cIAMSz8yfBAEeVldx3DSUE1drJ
4m4U6k2IBm+co8rz9VcH/0EFWEdSVZBazpP0K3V6ZcLHGR+i6FLr/n4Yk1Hy6h6j
tBQKx/H9im4YelG4bV0UNpHnFH7eA8jOY7ExkFmDB2aIUwjRSSlmJUgHcJghBnFt
KpMYQqm/Xt6UsdUNX8J9BVWX90E5HVA8YtzCeu0A3two5z472SR9D6IPRXiqnZxA
7Bkt2VnKy/st+FIxDf5ulxZSINfHK3WLtY4BdihQBHVfdWB708y+Xi02w+8lBEwf
ubWYCYnxlCnhH9PeMy3T5LqNJsNdAoRsStU5OPIpkMf/7XX3n89+9z8CQ328gew=
=7KVL
-----END PGP SIGNATURE-----


From hack.back at hotmail.com  Sun Mar 15 15:09:03 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 15 Mar 2015 08:09:03 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <55018D37.3050502@treenet.co.nz>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com> <54FEABB1.20402@treenet.co.nz>
 <1425980371350-4670291.post@n4.nabble.com> <55018D37.3050502@treenet.co.nz>
Message-ID: <1426432143831-4670434.post@n4.nabble.com>

Okay Solved Thanks Amos,
where i can find all configure options that can be used for squid ?
can you give me link in wiki.
Thanks Mate ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073487757627-blocks-of-1-bytes-tp4670271p4670434.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun Mar 15 15:22:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Mar 2015 04:22:26 +1300
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073487757627 blocks of 1 bytes!
In-Reply-To: <1426432143831-4670434.post@n4.nabble.com>
References: <1425856760429-4670271.post@n4.nabble.com>
 <54FD1A0C.8060209@treenet.co.nz> <1425930052114-4670285.post@n4.nabble.com>
 <1425930240507-4670286.post@n4.nabble.com> <54FEABB1.20402@treenet.co.nz>
 <1425980371350-4670291.post@n4.nabble.com> <55018D37.3050502@treenet.co.nz>
 <1426432143831-4670434.post@n4.nabble.com>
Message-ID: <5505A3B2.4040809@treenet.co.nz>

On 16/03/2015 4:09 a.m., HackXBack wrote:
> Okay Solved Thanks Amos,
> where i can find all configure options that can be used for squid ?

./configure --help lists them with a ver brief explanation.

Amos


From alex at samad.com.au  Sun Mar 15 22:24:34 2015
From: alex at samad.com.au (Alex Samad)
Date: Mon, 16 Mar 2015 09:24:34 +1100
Subject: [squid-users] help setting up hierarchy
Message-ID: <CAJ+Q1PVNS1Xbt4Mrp+c+Coa8dKAzXUxAL=f0teGrYJWTBMVMRg@mail.gmail.com>

Hi

I have 2 squid boxes that exist in my 2 DC.

They are on the same vlan/ ip network and i use dns round robin

cache_peer <other> sibling 3128 3130 proxy-only

in  addition to this I added in


# ICP ALLOW
acl icp_allowed src 10.3.2.1/32 << the ip of the other squid box to allow icp


http_access allow icp_allowed << need to allow this so that squid -a
can request from squid-b with out authenticating (do I need todo this)

icp_port 3130
icp_access allow icp_allowed
icp_access deny all

these are running squid-3.1.10-29.el6.x86_64

my new box (in the office) is running
squid-3.4.10-1.el6.x86_64

cache_peer squid-b parent 3128 0 weighted-round-robin weight=5
cache_peer squid-a parent 3128 0 weighted-round-robin weight=2

I had to turn on ICP I kept seeing error of not allowed !

We have authenticated access to the proxy, usually via ntlm so all
requests are logged against a user.

I do have some boxes that need unauthenticated access

Config questions
1) how to I get user authentication to flow through
  if a user requests from squid-a and it takes it from squid-b. I
would like the user id's logged on both
  if a user requests from new squid to either squid-a or squid-b. I
would like the auth (which would be done on new-squid) to flow through
to either squid-a or squid-b.
2)


From alex at samad.com.au  Sun Mar 15 22:26:10 2015
From: alex at samad.com.au (Alex Samad)
Date: Mon, 16 Mar 2015 09:26:10 +1100
Subject: [squid-users] help setting up hierarchy
In-Reply-To: <CAJ+Q1PVNS1Xbt4Mrp+c+Coa8dKAzXUxAL=f0teGrYJWTBMVMRg@mail.gmail.com>
References: <CAJ+Q1PVNS1Xbt4Mrp+c+Coa8dKAzXUxAL=f0teGrYJWTBMVMRg@mail.gmail.com>
Message-ID: <CAJ+Q1PWAUF=yYKGvXESZMbc7e_T1nP+QY_eA-JhnTxaWY_AV2A@mail.gmail.com>

Hi

Sorry gmail sent before I could finish

On 16 March 2015 at 09:24, Alex Samad <alex at samad.com.au> wrote:
> Hi
>
> I have 2 squid boxes that exist in my 2 DC.
>
> They are on the same vlan/ ip network and i use dns round robin
>
> cache_peer <other> sibling 3128 3130 proxy-only
>
> in  addition to this I added in
>
>
> # ICP ALLOW
> acl icp_allowed src 10.3.2.1/32 << the ip of the other squid box to allow icp
>
>
> http_access allow icp_allowed << need to allow this so that squid -a
> can request from squid-b with out authenticating (do I need todo this)
>
> icp_port 3130
> icp_access allow icp_allowed
> icp_access deny all
>
> these are running squid-3.1.10-29.el6.x86_64
>
> my new box (in the office) is running
> squid-3.4.10-1.el6.x86_64
>
> cache_peer squid-b parent 3128 0 weighted-round-robin weight=5
> cache_peer squid-a parent 3128 0 weighted-round-robin weight=2
>
> I had to turn on ICP I kept seeing error of not allowed !
>
> We have authenticated access to the proxy, usually via ntlm so all
> requests are logged against a user.
>
> I do have some boxes that need unauthenticated access
>
> Config questions
> 1) how to I get user authentication to flow through
>   if a user requests from squid-a and it takes it from squid-b. I
> would like the user id's logged on both
>   if a user requests from new squid to either squid-a or squid-b. I
> would like the auth (which would be done on new-squid) to flow through
> to either squid-a or squid-b.
2) how do I setup ICP to work properly
3) is the cache_peer to squid-a squid-b from new-squid type parent ?
4) do I need to allow ICP clients full access, this is the squid-a to
squid-b link ?


From johnzeng2013 at yahoo.com  Sun Mar 15 23:28:17 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Mon, 16 Mar 2015 07:28:17 +0800
Subject: [squid-users] squid 3.2.5 helpers/external_acl/session compile
 problem
In-Reply-To: <550598C4.9030607@ngtech.co.il>
References: <55042AD3.1010806@yahoo.com> <550598C4.9030607@ngtech.co.il>
Message-ID: <55061591.8080207@yahoo.com>


Hello Dear Eliezer:

                              Ubuntu 10.04 and  linux kernel : 
2.6.32-33-generic


? 2015?03?15? 22:35, Eliezer Croitoru ??:
> Hey,
>
> What OS are you building squid on?
>
> Eliezer
>
> On 14/03/2015 14:34, johnzeng wrote:
>> Hello All:
>>
>> I try to compile helpers/external_acl/session too , and Db evirontment
>> is db-4.8.30 ( Berkeley DB) , Maybe it will be caused by error Bdb 
>> version .
>>
>> But i can't confirm .
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Mar 16 02:18:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Mar 2015 15:18:38 +1300
Subject: [squid-users] help setting up hierarchy
In-Reply-To: <CAJ+Q1PWAUF=yYKGvXESZMbc7e_T1nP+QY_eA-JhnTxaWY_AV2A@mail.gmail.com>
References: <CAJ+Q1PVNS1Xbt4Mrp+c+Coa8dKAzXUxAL=f0teGrYJWTBMVMRg@mail.gmail.com>
 <CAJ+Q1PWAUF=yYKGvXESZMbc7e_T1nP+QY_eA-JhnTxaWY_AV2A@mail.gmail.com>
Message-ID: <55063D7E.6060508@treenet.co.nz>

On 16/03/2015 11:26 a.m., Alex Samad wrote:
> Hi
> 
> Sorry gmail sent before I could finish
> 
> On 16 March 2015 at 09:24, Alex Samad <alex at samad.com.au> wrote:
>> Hi
>>
>> I have 2 squid boxes that exist in my 2 DC.
>>
>> They are on the same vlan/ ip network and i use dns round robin
>>
>> cache_peer <other> sibling 3128 3130 proxy-only
>>
>> in  addition to this I added in
>>
>>
>> # ICP ALLOW
>> acl icp_allowed src 10.3.2.1/32 << the ip of the other squid box to allow icp
>>
>>
>> http_access allow icp_allowed << need to allow this so that squid -a
>> can request from squid-b with out authenticating (do I need todo this)
>>
>> icp_port 3130
>> icp_access allow icp_allowed
>> icp_access deny all
>>
>> these are running squid-3.1.10-29.el6.x86_64
>>
>> my new box (in the office) is running
>> squid-3.4.10-1.el6.x86_64
>>
>> cache_peer squid-b parent 3128 0 weighted-round-robin weight=5
>> cache_peer squid-a parent 3128 0 weighted-round-robin weight=2
>>
>> I had to turn on ICP I kept seeing error of not allowed !
>>
>> We have authenticated access to the proxy, usually via ntlm so all
>> requests are logged against a user.
>>
>> I do have some boxes that need unauthenticated access
>>
>> Config questions
>> 1) how to I get user authentication to flow through
>>   if a user requests from squid-a and it takes it from squid-b. I
>> would like the user id's logged on both
>>   if a user requests from new squid to either squid-a or squid-b. I
>> would like the auth (which would be done on new-squid) to flow through
>> to either squid-a or squid-b.

This is not possible with NTLM authentication.

NTLM is authenticating the TCP connection between client and proxy
underneath the HTTP layer and has a complex handshake setting up
security token per-connection with the DC server. The TCP connection
outbound from the proxy is a different connection, and also is not from
the client.

Its possible with Negotiate/Kerberos or Basic auth. Even though
Negotiate is also authenticating the TCP connection the handshake is
simpler and the token can be relayed to the peer proxy.

NP: Though be careful in an environment using NTLM. You may get
Negotiate/NTLM tokens flowing around, which wont work any more than NTLM
does.


> 2) how do I setup ICP to work properly

Use HTCP for better HIT ratio with less false positives in HTTP/1.1.

> 3) is the cache_peer to squid-a squid-b from new-squid type parent ?

No. But to get the authentication to work you will need login=PASSTHRU
parameter (and be using Basic or Negotiate/Kerberos).

> 4) do I need to allow ICP clients full access, this is the squid-a to
> squid-b link ?

You should not have to. However, it also should not matter - when the
first proxy is doing auth you kow the traffic coming out of it is
authenticated. Not doing auth twice is faster.

Amos



From priya_agarwal at students.iitmandi.ac.in  Mon Mar 16 05:42:31 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Mon, 16 Mar 2015 11:12:31 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
 <5502B3EE.9040908@treenet.co.nz>
 <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
 <5502D129.7030809@treenet.co.nz>
 <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
Message-ID: <CALTPfpGZJ3WjPh1NWW8wYmig9sd4vEKGxfT74+L-3ir2434DCQ@mail.gmail.com>

Hi,

I am now linking the libraries during ./configure with LDFLAGS, LIBS and
CXXFLAGS options (Makefile.am is same as it was)  Compile is failing
presently.

main.o: In function `of_init':
/media/NewVolume/yocto/build_t4240qds_release/tmp/sysroots/t4240qds/usr/include/usdpaa/of.h:52:
undefined reference to `of_init_path(char const*)'
collect2: error: ld returned 1 exit status
powerpc-fsl_networking-linux-libtool: link: rm -f ".libs/squidS.o"

Does it mean I haven't linked the the library that has of_init() [But I
think I have done that as ./configure passed and libraries are even visible
in log]
Or I need to link something else?

Here are the flags passed to ./configure:
EXTRA_OECONF_append = "    LDFLAGS="-L=/usr/lib/" \
            LIBS="-lusdpaa_dma -lusdpaa_dma_mem -lusdpaa_of -lusdpaa_fman
-lusdpaa_qbman -lusdpaa_syscfg" \
            CXXFLAGS="-I=/usr/include/""

Here is the logifle:
http://pastebin.com/u6SGDvGW

On Mon, Mar 16, 2015 at 11:06 AM, Priya Agarwal <
priya_agarwal at students.iitmandi.ac.in> wrote:

> Hi,
>
> I am now linking the libraries during ./configure with LDFLAGS, LIBS and
> CXXFLAGS options (Makefile.am is same as it was)  Compile is failing
> presently.
>
> main.o: In function `of_init':
> /media/NewVolume/yocto/build_t4240qds_release/tmp/sysroots/t4240qds/usr/include/usdpaa/of.h:52:
> undefined reference to `of_init_path(char const*)'
> collect2: error: ld returned 1 exit status
> powerpc-fsl_networking-linux-libtool: link: rm -f ".libs/squidS.o"
>
> Does it mean I haven't linked the the library that has of_init() [But I
> think I have done that as ./configure passed and libraries are even visible
> in log]
> Or I need to link something else?
>
> Here are the flags passed to ./configure:
> EXTRA_OECONF_append = "    LDFLAGS="-L=/usr/lib/" \
>             LIBS="-lusdpaa_dma -lusdpaa_dma_mem -lusdpaa_of -lusdpaa_fman
> -lusdpaa_qbman -lusdpaa_syscfg" \
>             CXXFLAGS="-I=/usr/include/""
>
> Attached the logfile too.
>
>
>
> On Fri, Mar 13, 2015 at 5:29 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 14/03/2015 12:28 a.m., Priya Agarwal wrote:
>> > I tried what you advised. Getting the same error for both methods
>> > (./configure LDFLAGS=-L<../tmp/../lib CXXFLAGS=-I<.../tmp../include or
>> > editing Makefile.am appropriately). autoreconf is failing.
>>
>> I see "<" characters in your paths. That is invalid. As is the -I paths
>> segments "..." and "tmp.." looks like you are missing '/' somewhere.
>>
>>
>> > And also I am getting many such warnings:
>> >
>> > | src/Common.am:16: warning: 'INCLUDES' is the old name for
>> 'AM_CPPFLAGS'
>> > (or '*_CPPFLAGS')
>> > | compat/Makefile.am:5:   'src/Common.am' included from here
>> > | src/Common.am:16: warning: 'INCLUDES' is the old name for
>> 'AM_CPPFLAGS'
>> > (or '*_CPPFLAGS')
>> > | helpers/basic_auth/DB/Makefile.am:1:   'src/Common.am' included from
>> here
>> > | src/Common.am:16: warning: 'INCLUDES' is the old name for
>> 'AM_CPPFLAGS'
>> > (or '*_CPPFLAGS')
>> > | helpers/basic_auth/LDAP/Makefile.am:1:   'src/Common.am' included from
>> > here
>> > | src/Common.am:16: warning: 'INCLUDES' is the old name for
>> 'AM_CPPFLAGS'
>> > (or '*_CPPFLAGS')
>> > | helpers/basic_auth/MSNT-multi-domain/Makefile.am:1:   'src/Common.am'
>> > included from here
>> > | src/Common.am:16: warning: 'INCLUDES' is the old name for
>> 'AM_CPPFLAGS'
>> > (or '*_CPPFLAGS')
>> >
>>
>> Those are just warnings because you are working with an old Squid
>> version and autotools have changed their requirements since. The current
>> release dont have quite so many warnings (some remain). Those can be
>> ignore.
>>
>> I does mean that what I wrote as AM_CPPFLAGS needs to instead be written
>> as INCLUDES in your Squid versions Makefile.am.
>>
>>
>> > Final error:
>> > | autoreconf: automake failed with exit status: 1
>> > | ERROR: autoreconf execution failed.
>> >
>> > So is something wrong with the path?
>>
>> I see "<" characters in what you
>>
>> >
>> > I have attached the logfile as well which shows the detailed output.
>> >
>>
>> Buried in the warnings I see this:
>>
>> src/Makefile.am:661: error: '#' comment at start of rule is unportable
>>
>>
>> automake syntax has two forms of comment.
>>  ## comments are autoreconf comments and ignored
>>  # comments are copied through as-is to the final Makefile
>>
>> If you are using multi-line wrapped lists of things, that can cause
>> issues. Its easier to just never use comments inside the wrapped lines.
>>
>>
>> Other things to watch out for auth makefiles:
>>
>> * indentation for rules needs to be one tab, not spaces. This needs
>> checking after each copy-paste you do.
>>
>> * multi-line rules and lists use '\' character ending to explicitly
>> define the wrapping.
>>   Be careful that lists of libraries etc use them on each line up to,
>> but not on, the final line of the list.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150316/7cc19e1b/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 16 06:07:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Mar 2015 19:07:46 +1300
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>	<5502B3EE.9040908@treenet.co.nz>	<CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>	<5502D129.7030809@treenet.co.nz>
 <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
Message-ID: <55067332.1020500@treenet.co.nz>

On 16/03/2015 6:36 p.m., Priya Agarwal wrote:
> Hi,
> 
> I am now linking the libraries during ./configure with LDFLAGS, LIBS and
> CXXFLAGS options (Makefile.am is same as it was)  Compile is failing
> presently.
> 
> main.o: In function `of_init':
> /media/NewVolume/yocto/build_t4240qds_release/tmp/sysroots/t4240qds/usr/include/usdpaa/of.h:52:
> undefined reference to `of_init_path(char const*)'
> collect2: error: ld returned 1 exit status
> powerpc-fsl_networking-linux-libtool: link: rm -f ".libs/squidS.o"
> 
> Does it mean I haven't linked the the library that has of_init() [But I
> think I have done that as ./configure passed and libraries are even visible
> in log]
> Or I need to link something else?

Close. It means that the libraries listed on the command line after the
main.o object do not define that symbol.

Its possible the library was linked first or via the wrong variable. Or
that the C++ symbol mangling is different between what is wanted and
what the library defines.


> 
> Here are the flags passed to ./configure:
> EXTRA_OECONF_append = "    LDFLAGS="-L=/usr/lib/" \
>             LIBS="-lusdpaa_dma -lusdpaa_dma_mem -lusdpaa_of -lusdpaa_fman
> -lusdpaa_qbman -lusdpaa_syscfg" \
>             CXXFLAGS="-I=/usr/include/""
> 

I see 12 separate command line arguments there. Not the 3 that should exist.

Hint: shell expansion is applied to " symbols on the ./configure command
line.

Amos



From eliezer at ngtech.co.il  Mon Mar 16 10:52:49 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 16 Mar 2015 12:52:49 +0200
Subject: [squid-users] Access Problem to VMWARE,
 IBM and Truste.com through Squid
In-Reply-To: <DUB122-W24E9F6A993E331FF5CDCA2A6170@phx.gbl>
References: <DUB122-W2800D8E6214F8CD8C9250AA6160@phx.gbl>,
 <524C4B6B.4060908@ngtech.co.il>, <DUB122-W3F6112421264711845A33A6160@phx.gbl>,
 <524D2E00.4020901@treenet.co.nz>,
 <DUB122-W264B5876044A40EF292C17A6170@phx.gbl>
 <DUB122-W24E9F6A993E331FF5CDCA2A6170@phx.gbl>
Message-ID: <5506B601.8020003@ngtech.co.il>

Hey,

This is a very old post but I was unable to understand the solution to 
the issue.
I have looked again on the logs and since they show a 500 internal 
server error response I understood that the issue is not on the 
server(site) side only and might be on the client(eg other side of the 
internet and can be either a proxy or a web-browser).

Sometimes websites do not like to handle "special" request which was not 
meant to be there in the first place.

As an example:
I have seen couple times in the past(before 2013) that a public service 
was honoring *any* "X-Forwarded-For" headers which resulted in all sort 
of weird results while displaying the page.
One example was that instead of a link to the external web page address 
I got lots of internal urls which was not suppose to be there at all.
On this specific incident I had the chance to see how the "internal" 
network infrastructure is designed and implemented and which I know 
nobody planned to show me.

All The Bests,
Eliezer Croitoru

On 03/10/2013 13:26, Babelo Gmvsdm wrote:
> Hi,
>
> First of all Thx Amos for your enlightenment, even if  I had to admit that it's not yet
> all clear for me, My knowledge of proxy is very light.
>
> So a little update about my last post.
>
> I told that blocking the access to Truste.com is not a solution, so it's not really true.
>
> Usually I block sites using DBs used by SquidGuard, and here this method does not work (I don't know why).
> But, blocking via Squid ACL works, I mean there is no popup anymore and websites are accessible, so it's not a perfect solution but a least is helps.
>
> If you have an Idea of what I could do to solve this better that this patch it would be great.
>
> At least thx for all you have already done.
>
> HErC. 		 	   		
>



From alex at samad.com.au  Tue Mar 17 05:19:44 2015
From: alex at samad.com.au (Alex Samad)
Date: Tue, 17 Mar 2015 16:19:44 +1100
Subject: [squid-users] help setting up hierarchy
In-Reply-To: <55063D7E.6060508@treenet.co.nz>
References: <CAJ+Q1PVNS1Xbt4Mrp+c+Coa8dKAzXUxAL=f0teGrYJWTBMVMRg@mail.gmail.com>
 <CAJ+Q1PWAUF=yYKGvXESZMbc7e_T1nP+QY_eA-JhnTxaWY_AV2A@mail.gmail.com>
 <55063D7E.6060508@treenet.co.nz>
Message-ID: <CAJ+Q1PW=xB7OTLQauJ4tHkse-_=-Mz5creRnARy0UargiJfC-A@mail.gmail.com>

[snip]

>>>
>>> Config questions
>>> 1) how to I get user authentication to flow through
>>>   if a user requests from squid-a and it takes it from squid-b. I
>>> would like the user id's logged on both
>>>   if a user requests from new squid to either squid-a or squid-b. I
>>> would like the auth (which would be done on new-squid) to flow through
>>> to either squid-a or squid-b.
>
> This is not possible with NTLM authentication.
>
> NTLM is authenticating the TCP connection between client and proxy
> underneath the HTTP layer and has a complex handshake setting up
> security token per-connection with the DC server. The TCP connection
> outbound from the proxy is a different connection, and also is not from
> the client.
>
> Its possible with Negotiate/Kerberos or Basic auth. Even though
> Negotiate is also authenticating the TCP connection the handshake is
> simpler and the token can be relayed to the peer proxy.
>
> NP: Though be careful in an environment using NTLM. You may get
> Negotiate/NTLM tokens flowing around, which wont work any more than NTLM
> does.

Sounds like the simplest thing to do is turn on authentication on all
the boxes and allow then non auth access to each other


>
>
>> 2) how do I setup ICP to work properly
>
> Use HTCP for better HIT ratio with less false positives in HTTP/1.1.

Ta, i will have to have a read, does it work. any examples on how to setup?

>
>> 3) is the cache_peer to squid-a squid-b from new-squid type parent ?
>
> No. But to get the authentication to work you will need login=PASSTHRU
> parameter (and be using Basic or Negotiate/Kerberos).
what if I just want the authenticated user id to flow through. So the
authentication happen on the office squid and then it forward to the
DC squid, the dc squid can log the user name in the user field is that
possible ?

>
>> 4) do I need to allow ICP clients full access, this is the squid-a to
>> squid-b link ?
>
> You should not have to. However, it also should not matter - when the
> first proxy is doing auth you kow the traffic coming out of it is
> authenticated. Not doing auth twice is faster.

Is there a way to say any one attaching on port X doesn't need to be
authenticated but on port Y does.
My issues is that in the office I have a few eclipse users who had a
lot of problems with out previous proxy solution. they are setup to
use the office proxy in a nonauth way. but now I want to setup auth on
this squid box. I was thinking there could be a non auth port and a
auth port.

A


>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From brijeshbmehta at gmail.com  Tue Mar 17 09:50:44 2015
From: brijeshbmehta at gmail.com (Brijesh B. Mehta)
Date: Tue, 17 Mar 2015 15:20:44 +0530
Subject: [squid-users] How can i keep log files for longer periods?
Message-ID: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>

Hello,

I am new to squid and using it for few days. i found that in my squid
configuration (which default i guess) access log of only three days is
being maintained (/var/log/squid3/). Now i want to store access log of
more than 3 days say for month so what changes i need to make in my
configuration file? I already read about rotate a log file but it
confused me so i haven't tried it yet.
Kindly provide me some solution

Thanks

Regards,

Mr. Brijesh B. Mehta
Research Scholar,
Computer Engineering Department,
S. V. National Institute of Technology,
Surat - 395007
Gujarat, India
http://brijeshbmehta.wordpress.com/

Tips:
If you forward this email, please delete the forward history,
including my email address.
Remember, erasing the history helps to prevent SPAMMERS from mining
addresses and viruses from being propagated.
Also enter the addresses on the "Bcc" line to hide them from others.


From yvoinov at gmail.com  Tue Mar 17 10:18:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 17 Mar 2015 16:18:53 +0600
Subject: [squid-users] How can i keep log files for longer periods?
In-Reply-To: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
References: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
Message-ID: <5507FF8D.2020603@gmail.com>

With OS facility, no?

17.03.15 15:50, Brijesh B. Mehta ?????:
> Hello,
>
> I am new to squid and using it for few days. i found that in my squid
> configuration (which default i guess) access log of only three days is
> being maintained (/var/log/squid3/). Now i want to store access log of
> more than 3 days say for month so what changes i need to make in my
> configuration file? I already read about rotate a log file but it
> confused me so i haven't tried it yet.
> Kindly provide me some solution
>
> Thanks
>
> Regards,
>
> Mr. Brijesh B. Mehta
> Research Scholar,
> Computer Engineering Department,
> S. V. National Institute of Technology,
> Surat - 395007
> Gujarat, India
> http://brijeshbmehta.wordpress.com/
>
> Tips:
> If you forward this email, please delete the forward history,
> including my email address.
> Remember, erasing the history helps to prevent SPAMMERS from mining
> addresses and viruses from being propagated.
> Also enter the addresses on the "Bcc" line to hide them from others.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid at visolve.com  Tue Mar 17 10:24:14 2015
From: squid at visolve.com (squid-list)
Date: Tue, 17 Mar 2015 15:54:14 +0530
Subject: [squid-users] How can i keep log files for longer periods?
In-Reply-To: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
References: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
Message-ID: <550800CE.7070303@visolve.com>

Hi,

You can use the following in squid configuration to have access log for 
log time.

*logfile_rotate 10*

It will keep last 10 access log of squid. If you wish to have log for 
month, use it as 30. You may rotate squid log using crontab. Following 
will rotate log at every morning 6.

00      6       *       *       *      squid -k rotate

- Siva Prakash

On 03/17/2015 03:20 PM, Brijesh B. Mehta wrote:
> Hello,
>
> I am new to squid and using it for few days. i found that in my squid
> configuration (which default i guess) access log of only three days is
> being maintained (/var/log/squid3/). Now i want to store access log of
> more than 3 days say for month so what changes i need to make in my
> configuration file? I already read about rotate a log file but it
> confused me so i haven't tried it yet.
> Kindly provide me some solution
>
> Thanks
>
> Regards,
>
> Mr. Brijesh B. Mehta
> Research Scholar,
> Computer Engineering Department,
> S. V. National Institute of Technology,
> Surat - 395007
> Gujarat, India
> http://brijeshbmehta.wordpress.com/
>
> Tips:
> If you forward this email, please delete the forward history,
> including my email address.
> Remember, erasing the history helps to prevent SPAMMERS from mining
> addresses and viruses from being propagated.
> Also enter the addresses on the "Bcc" line to hide them from others.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/41bce270/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 17 11:27:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Mar 2015 00:27:57 +1300
Subject: [squid-users] How can i keep log files for longer periods?
In-Reply-To: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
References: <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg@mail.gmail.com>
Message-ID: <55080FBD.3010807@treenet.co.nz>

On 17/03/2015 10:50 p.m., Brijesh B. Mehta wrote:
> Hello,
> 
> I am new to squid and using it for few days. i found that in my squid
> configuration (which default i guess) access log of only three days is
> being maintained (/var/log/squid3/). Now i want to store access log of
> more than 3 days say for month so what changes i need to make in my
> configuration file? I already read about rotate a log file but it
> confused me so i haven't tried it yet.
> Kindly provide me some solution
> 

Since you are using the Debian package it will be in /etc/logrotate.d/squid3

Amos



From brijeshbmehta at gmail.com  Tue Mar 17 12:42:15 2015
From: brijeshbmehta at gmail.com (Brijesh B. Mehta)
Date: Tue, 17 Mar 2015 18:12:15 +0530
Subject: [squid-users] squid-users Digest, Vol 7, Issue 70
In-Reply-To: <mailman.3.1426593601.20323.squid-users@lists.squid-cache.org>
References: <mailman.3.1426593601.20323.squid-users@lists.squid-cache.org>
Message-ID: <CA+2wEE30MjqbKioMmVthnfvcXaNpKyK2ZT0x-iCSD=WO4GXyrA@mail.gmail.com>

Thank you all,
I have set 'logfile_rotate 30'. Lets see how it works
Regards,

Mr. Brijesh B. Mehta
Research Scholar,
Computer Engineering Department,
S. V. National Institute of Technology,
Surat - 395007
Gujarat, India
http://brijeshbmehta.wordpress.com/

Tips:
If you forward this email, please delete the forward history,
including my email address.
Remember, erasing the history helps to prevent SPAMMERS from mining
addresses and viruses from being propagated.
Also enter the addresses on the "Bcc" line to hide them from others.


On 17 March 2015 at 17:30,  <squid-users-request at lists.squid-cache.org> wrote:
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: help setting up hierarchy (Alex Samad)
>    2. How can i keep log files for longer periods? (Brijesh B. Mehta)
>    3. Re: How can i keep log files for longer periods? (Yuri Voinov)
>    4. Re: How can i keep log files for longer periods? (squid-list)
>    5. Re: How can i keep log files for longer periods? (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 17 Mar 2015 16:19:44 +1100
> From: Alex Samad <alex at samad.com.au>
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] help setting up hierarchy
> Message-ID:
>         <CAJ+Q1PW=xB7OTLQauJ4tHkse-_=-Mz5creRnARy0UargiJfC-A at mail.gmail.com>
> Content-Type: text/plain; charset=UTF-8
>
> [snip]
>
>>>>
>>>> Config questions
>>>> 1) how to I get user authentication to flow through
>>>>   if a user requests from squid-a and it takes it from squid-b. I
>>>> would like the user id's logged on both
>>>>   if a user requests from new squid to either squid-a or squid-b. I
>>>> would like the auth (which would be done on new-squid) to flow through
>>>> to either squid-a or squid-b.
>>
>> This is not possible with NTLM authentication.
>>
>> NTLM is authenticating the TCP connection between client and proxy
>> underneath the HTTP layer and has a complex handshake setting up
>> security token per-connection with the DC server. The TCP connection
>> outbound from the proxy is a different connection, and also is not from
>> the client.
>>
>> Its possible with Negotiate/Kerberos or Basic auth. Even though
>> Negotiate is also authenticating the TCP connection the handshake is
>> simpler and the token can be relayed to the peer proxy.
>>
>> NP: Though be careful in an environment using NTLM. You may get
>> Negotiate/NTLM tokens flowing around, which wont work any more than NTLM
>> does.
>
> Sounds like the simplest thing to do is turn on authentication on all
> the boxes and allow then non auth access to each other
>
>
>>
>>
>>> 2) how do I setup ICP to work properly
>>
>> Use HTCP for better HIT ratio with less false positives in HTTP/1.1.
>
> Ta, i will have to have a read, does it work. any examples on how to setup?
>
>>
>>> 3) is the cache_peer to squid-a squid-b from new-squid type parent ?
>>
>> No. But to get the authentication to work you will need login=PASSTHRU
>> parameter (and be using Basic or Negotiate/Kerberos).
> what if I just want the authenticated user id to flow through. So the
> authentication happen on the office squid and then it forward to the
> DC squid, the dc squid can log the user name in the user field is that
> possible ?
>
>>
>>> 4) do I need to allow ICP clients full access, this is the squid-a to
>>> squid-b link ?
>>
>> You should not have to. However, it also should not matter - when the
>> first proxy is doing auth you kow the traffic coming out of it is
>> authenticated. Not doing auth twice is faster.
>
> Is there a way to say any one attaching on port X doesn't need to be
> authenticated but on port Y does.
> My issues is that in the office I have a few eclipse users who had a
> lot of problems with out previous proxy solution. they are setup to
> use the office proxy in a nonauth way. but now I want to setup auth on
> this squid box. I was thinking there could be a non auth port and a
> auth port.
>
> A
>
>
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 17 Mar 2015 15:20:44 +0530
> From: "Brijesh B. Mehta" <brijeshbmehta at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] How can i keep log files for longer periods?
> Message-ID:
>         <CA+2wEE1YYna3QK-t7=x6-RJpraiF-4eectCrDKJuae7xdZ_HAg at mail.gmail.com>
> Content-Type: text/plain; charset=UTF-8
>
> Hello,
>
> I am new to squid and using it for few days. i found that in my squid
> configuration (which default i guess) access log of only three days is
> being maintained (/var/log/squid3/). Now i want to store access log of
> more than 3 days say for month so what changes i need to make in my
> configuration file? I already read about rotate a log file but it
> confused me so i haven't tried it yet.
> Kindly provide me some solution
>
> Thanks
>
> Regards,
>
> Mr. Brijesh B. Mehta
> Research Scholar,
> Computer Engineering Department,
> S. V. National Institute of Technology,
> Surat - 395007
> Gujarat, India
> http://brijeshbmehta.wordpress.com/
>
> Tips:
> If you forward this email, please delete the forward history,
> including my email address.
> Remember, erasing the history helps to prevent SPAMMERS from mining
> addresses and viruses from being propagated.
> Also enter the addresses on the "Bcc" line to hide them from others.
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 17 Mar 2015 16:18:53 +0600
> From: Yuri Voinov <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] How can i keep log files for longer
>         periods?
> Message-ID: <5507FF8D.2020603 at gmail.com>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
> With OS facility, no?
>
> 17.03.15 15:50, Brijesh B. Mehta ?????:
>> Hello,
>>
>> I am new to squid and using it for few days. i found that in my squid
>> configuration (which default i guess) access log of only three days is
>> being maintained (/var/log/squid3/). Now i want to store access log of
>> more than 3 days say for month so what changes i need to make in my
>> configuration file? I already read about rotate a log file but it
>> confused me so i haven't tried it yet.
>> Kindly provide me some solution
>>
>> Thanks
>>
>> Regards,
>>
>> Mr. Brijesh B. Mehta
>> Research Scholar,
>> Computer Engineering Department,
>> S. V. National Institute of Technology,
>> Surat - 395007
>> Gujarat, India
>> http://brijeshbmehta.wordpress.com/
>>
>> Tips:
>> If you forward this email, please delete the forward history,
>> including my email address.
>> Remember, erasing the history helps to prevent SPAMMERS from mining
>> addresses and viruses from being propagated.
>> Also enter the addresses on the "Bcc" line to hide them from others.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> ------------------------------
>
> Message: 4
> Date: Tue, 17 Mar 2015 15:54:14 +0530
> From: squid-list <squid at visolve.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] How can i keep log files for longer
>         periods?
> Message-ID: <550800CE.7070303 at visolve.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> Hi,
>
> You can use the following in squid configuration to have access log for
> log time.
>
> *logfile_rotate 10*
>
> It will keep last 10 access log of squid. If you wish to have log for
> month, use it as 30. You may rotate squid log using crontab. Following
> will rotate log at every morning 6.
>
> 00      6       *       *       *      squid -k rotate
>
> - Siva Prakash
>
> On 03/17/2015 03:20 PM, Brijesh B. Mehta wrote:
>> Hello,
>>
>> I am new to squid and using it for few days. i found that in my squid
>> configuration (which default i guess) access log of only three days is
>> being maintained (/var/log/squid3/). Now i want to store access log of
>> more than 3 days say for month so what changes i need to make in my
>> configuration file? I already read about rotate a log file but it
>> confused me so i haven't tried it yet.
>> Kindly provide me some solution
>>
>> Thanks
>>
>> Regards,
>>
>> Mr. Brijesh B. Mehta
>> Research Scholar,
>> Computer Engineering Department,
>> S. V. National Institute of Technology,
>> Surat - 395007
>> Gujarat, India
>> http://brijeshbmehta.wordpress.com/
>>
>> Tips:
>> If you forward this email, please delete the forward history,
>> including my email address.
>> Remember, erasing the history helps to prevent SPAMMERS from mining
>> addresses and viruses from being propagated.
>> Also enter the addresses on the "Bcc" line to hide them from others.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/41bce270/attachment-0001.html>
>
> ------------------------------
>
> Message: 5
> Date: Wed, 18 Mar 2015 00:27:57 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] How can i keep log files for longer
>         periods?
> Message-ID: <55080FBD.3010807 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 17/03/2015 10:50 p.m., Brijesh B. Mehta wrote:
>> Hello,
>>
>> I am new to squid and using it for few days. i found that in my squid
>> configuration (which default i guess) access log of only three days is
>> being maintained (/var/log/squid3/). Now i want to store access log of
>> more than 3 days say for month so what changes i need to make in my
>> configuration file? I already read about rotate a log file but it
>> confused me so i haven't tried it yet.
>> Kindly provide me some solution
>>
>
> Since you are using the Debian package it will be in /etc/logrotate.d/squid3
>
> Amos
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 7, Issue 70
> ******************************************


From sam at idsdoc.com  Tue Mar 17 17:35:10 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Tue, 17 Mar 2015 11:35:10 -0600
Subject: [squid-users] Refresh ACL list only
Message-ID: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>

Hello all,

Does anyone know of a way to reload a single ACL list? I have a very
complicated and large config file that takes around 30 seconds to reload
when I run the (squid3 -k reconfigure) command. I have several ACL lists
that need to be updated throughout the day and it would be nice if I could
only reload those ACL lists and not the entire config. Its problematic
because while its reloading, the server is effectively down and disrupts
Internet access for the rest of the users. Below is a small sample of the
lists that will be updated. If I could add a TTL to the lists so squid
would reload them periodically without a full reconfigure would be ideal.



acl GLOBAL-WHITELIST dstdomain "/etc/squid3/whitelists/GLOBAL-WHITELIST"
acl UNRESTRICTED-WHITELIST dstdomain
"/etc/squid3/whitelists/UNRESTRICTED-WHITELIST"
acl DEV-WHITELIST dstdomain "/etc/squid3/whitelists/DEV-WHITELIST"
acl SALES-WHITELIST dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"


Thanks


-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/dac0c903/attachment.htm>

From yvoinov at gmail.com  Tue Mar 17 17:39:49 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 17 Mar 2015 23:39:49 +0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
Message-ID: <550866E5.5090309@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Did you hear about rewriters and filters? I.e., squidGuard, or
Dansguardian? Or, of course
https://www.urlfilterdb.com/products/ufdbguard.html
?
It has separate server process which can be restart VERY quickly
independently of squid.

17.03.15 23:35, Samuel Anderson ?????:
> Hello all,
> 
> Does anyone know of a way to reload a single ACL list? I have a
> very complicated and large config file that takes around 30 seconds
> to reload when I run the (squid3 -k reconfigure) command. I have
> several ACL lists that need to be updated throughout the day and it
> would be nice if I could only reload those ACL lists and not the
> entire config. Its problematic because while its reloading, the
> server is effectively down and disrupts Internet access for the
> rest of the users. Below is a small sample of the lists that will
> be updated. If I could add a TTL to the lists so squid would reload
> them periodically without a full reconfigure would be ideal.
> 
> 
> 
> acl GLOBAL-WHITELIST dstdomain
> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl
> UNRESTRICTED-WHITELIST dstdomain 
> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl DEV-WHITELIST
> dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl
> SALES-WHITELIST dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"
> 
> 
> Thanks
> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVCGblAAoJENNXIZxhPexGRqwIAIS3iw5wIt9FPi85aH+vWmA8
QJYyo8ChpnTGsKnAgpAMoSRFobo6AZjL9ABrRx7kGz2NC/VAla93NNR7SKr+mDdr
Z9jz9DRVRSAm4D1rC3+xvdQowoN2UraxYDj9fCQKczfU0whc4Qwool+n36gocPZH
I0nSbv40MhSTCO/Zybo1eonW/VQ4i9LopGFVI5q+dYwRRleu8Rh4Pg1qRBRzmKa4
5O+yCglKumIzMe4Pqa2JFQ6oq9VAimEslin7hoXS1VXRH8lE9Hbg0kKpuaWEiyFG
ySmdKoFu1O70Ffug48vXi1EQXAkE5C6xmtBHlCBxtiOf8kFoUHkyslJtEniA8Yw=
=+8IA
-----END PGP SIGNATURE-----


From sam at idsdoc.com  Tue Mar 17 17:59:12 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Tue, 17 Mar 2015 11:59:12 -0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <550866E5.5090309@gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
Message-ID: <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>

Unfortunately thats not really an option for me. I've already built
everything just using squid. It works great and does everything I need it
to do with the exception of refreshing the ACL lists. I just need to find a
way to refresh those single lists without disrupting Internet traffic to
the users. If anyone knows how to do this I would greatly appreciate it.

On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Did you hear about rewriters and filters? I.e., squidGuard, or
> Dansguardian? Or, of course
> https://www.urlfilterdb.com/products/ufdbguard.html
> ?
> It has separate server process which can be restart VERY quickly
> independently of squid.
>
> 17.03.15 23:35, Samuel Anderson ?????:
> > Hello all,
> >
> > Does anyone know of a way to reload a single ACL list? I have a
> > very complicated and large config file that takes around 30 seconds
> > to reload when I run the (squid3 -k reconfigure) command. I have
> > several ACL lists that need to be updated throughout the day and it
> > would be nice if I could only reload those ACL lists and not the
> > entire config. Its problematic because while its reloading, the
> > server is effectively down and disrupts Internet access for the
> > rest of the users. Below is a small sample of the lists that will
> > be updated. If I could add a TTL to the lists so squid would reload
> > them periodically without a full reconfigure would be ideal.
> >
> >
> >
> > acl GLOBAL-WHITELIST dstdomain
> > "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl
> > UNRESTRICTED-WHITELIST dstdomain
> > "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl DEV-WHITELIST
> > dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl
> > SALES-WHITELIST dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"
> >
> >
> > Thanks
> >
> >
> >
> >
> > _______________________________________________ squid-users mailing
> > list squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJVCGblAAoJENNXIZxhPexGRqwIAIS3iw5wIt9FPi85aH+vWmA8
> QJYyo8ChpnTGsKnAgpAMoSRFobo6AZjL9ABrRx7kGz2NC/VAla93NNR7SKr+mDdr
> Z9jz9DRVRSAm4D1rC3+xvdQowoN2UraxYDj9fCQKczfU0whc4Qwool+n36gocPZH
> I0nSbv40MhSTCO/Zybo1eonW/VQ4i9LopGFVI5q+dYwRRleu8Rh4Pg1qRBRzmKa4
> 5O+yCglKumIzMe4Pqa2JFQ6oq9VAimEslin7hoXS1VXRH8lE9Hbg0kKpuaWEiyFG
> ySmdKoFu1O70Ffug48vXi1EQXAkE5C6xmtBHlCBxtiOf8kFoUHkyslJtEniA8Yw=
> =+8IA
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/9f555c6e/attachment.htm>

From bpk678 at gmail.com  Tue Mar 17 18:00:51 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 17 Mar 2015 14:00:51 -0400
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
Message-ID: <1426615251.9328.17.camel@desktop.bpk2.com>

On Tue, 2015-03-17 at 11:59 -0600, Samuel Anderson wrote:
> Unfortunately thats not really an option for me. I've already built
> everything just using squid. It works great and does everything I need
> it to do with the exception of refreshing the ACL lists. I just need
> to find a way to refresh those single lists without disrupting
> Internet traffic to the users. If anyone knows how to do this I would
> greatly appreciate it.
> 
> On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
>         -----BEGIN PGP SIGNED MESSAGE-----
>         Hash: SHA1
>         
>         Did you hear about rewriters and filters? I.e., squidGuard, or
>         Dansguardian? Or, of course
>         https://www.urlfilterdb.com/products/ufdbguard.html
>         ?
>         It has separate server process which can be restart VERY
>         quickly
>         independently of squid.
>         
>         17.03.15 23:35, Samuel Anderson ?????:
>         > Hello all,
>         >
>         > Does anyone know of a way to reload a single ACL list? I
>         have a
>         > very complicated and large config file that takes around 30
>         seconds
>         > to reload when I run the (squid3 -k reconfigure) command. I
>         have
>         > several ACL lists that need to be updated throughout the day
>         and it
>         > would be nice if I could only reload those ACL lists and not
>         the
>         > entire config. Its problematic because while its reloading,
>         the
>         > server is effectively down and disrupts Internet access for
>         the
>         > rest of the users. Below is a small sample of the lists that
>         will
>         > be updated. If I could add a TTL to the lists so squid would
>         reload
>         > them periodically without a full reconfigure would be ideal.
>         >
>         >
>         >
>         > acl GLOBAL-WHITELIST dstdomain
>         > "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl
>         > UNRESTRICTED-WHITELIST dstdomain
>         > "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl
>         DEV-WHITELIST
>         > dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl
>         > SALES-WHITELIST dstdomain
>         "/etc/squid3/whitelists/SALES-WHITELIST"
>         >
>         >
>         > Thanks
>         >
>         >
>         >
>         >
>         > _______________________________________________ squid-users
>         mailing
>         > list squid-users at lists.squid-cache.org
>         > http://lists.squid-cache.org/listinfo/squid-users
>         >
>         -----BEGIN PGP SIGNATURE-----
>         Version: GnuPG v2
>         
>         iQEcBAEBAgAGBQJVCGblAAoJENNXIZxhPexGRqwIAIS3iw5wIt9FPi85aH
>         +vWmA8
>         QJYyo8ChpnTGsKnAgpAMoSRFobo6AZjL9ABrRx7kGz2NC/VAla93NNR7SKr
>         +mDdr
>         Z9jz9DRVRSAm4D1rC3+xvdQowoN2UraxYDj9fCQKczfU0whc4Qwool
>         +n36gocPZH
>         I0nSbv40MhSTCO/Zybo1eonW/VQ4i9LopGFVI5q
>         +dYwRRleu8Rh4Pg1qRBRzmKa4
>         5O
>         +yCglKumIzMe4Pqa2JFQ6oq9VAimEslin7hoXS1VXRH8lE9Hbg0kKpuaWEiyFG
>         ySmdKoFu1O70Ffug48vXi1EQXAkE5C6xmtBHlCBxtiOf8kFoUHkyslJtEniA8Yw=
>         =+8IA
>         -----END PGP SIGNATURE-----
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> -- 
> Samuel Anderson  |  Information Technology Administrator  |
>  International Document Services
> 
> 
> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
> 
> 
> 
> CONFIDENTIALITY NOTICE:
> This e-mail and any attachments are confidential. If you are not an
> intended recipient, please contact the sender to report the error and
> delete all copies of this message from your system.  Any unauthorized
> review, use, disclosure or distribution is prohibited.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

do you have the luxury of multiple squid instances behind a load
balancer?  mark one offline at the LB, reconfigure, mark online at the
LB.  Lather, rinse, repeat.



From yvoinov at gmail.com  Tue Mar 17 18:02:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Mar 2015 00:02:27 +0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
Message-ID: <55086C33.6030501@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Unfortunately,

there is completely impossible in current Squid realisation.

Only with ufdbGuard.

Or, of course,

you can build hierarchical cache cluster.... :) with load balanser
before it. And reload ACL's lists consequentially.


17.03.15 23:59, Samuel Anderson ?????:
> Unfortunately thats not really an option for me. I've already
> built everything just using squid. It works great and does
> everything I need it to do with the exception of refreshing the ACL
> lists. I just need to find a way to refresh those single lists
> without disrupting Internet traffic to the users. If anyone knows
> how to do this I would greatly appreciate it.
> 
> On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov <yvoinov at gmail.com>
> wrote:
> 
> Did you hear about rewriters and filters? I.e., squidGuard, or 
> Dansguardian? Or, of course 
> https://www.urlfilterdb.com/products/ufdbguard.html ? It has
> separate server process which can be restart VERY quickly 
> independently of squid.
> 
> 17.03.15 23:35, Samuel Anderson ?????:
>>>> Hello all,
>>>> 
>>>> Does anyone know of a way to reload a single ACL list? I have
>>>> a very complicated and large config file that takes around 30
>>>> seconds to reload when I run the (squid3 -k reconfigure)
>>>> command. I have several ACL lists that need to be updated
>>>> throughout the day and it would be nice if I could only
>>>> reload those ACL lists and not the entire config. Its
>>>> problematic because while its reloading, the server is
>>>> effectively down and disrupts Internet access for the rest of
>>>> the users. Below is a small sample of the lists that will be
>>>> updated. If I could add a TTL to the lists so squid would
>>>> reload them periodically without a full reconfigure would be
>>>> ideal.
>>>> 
>>>> 
>>>> 
>>>> acl GLOBAL-WHITELIST dstdomain 
>>>> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl 
>>>> UNRESTRICTED-WHITELIST dstdomain 
>>>> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl
>>>> DEV-WHITELIST dstdomain
>>>> "/etc/squid3/whitelists/DEV-WHITELIST" acl SALES-WHITELIST
>>>> dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"
>>>> 
>>>> 
>>>> Thanks
>>>> 
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________ squid-users
>>>> mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVCGwzAAoJENNXIZxhPexGb6kH/RMLM/8K3IISji7UCkClnEU8
3sHRe/rTPGkqd9ihlbB5pFy7+d5+mVWJg3k4PH8tVeAQz7U775NUpphr4UivjVe/
wBWKsxQedZ8kCOMV2lELoi1+Jof8psCczs8WcZbPnnHgafSLRi1ok7uGQa1na2r0
HtIjvltMs3tsw6XTukwqNoLuoATOifMq/Kd7IdZYseBgYgQFIGw4KOXIQSJHytm/
+48xZIJngHGZ9l+FkCgGY38ONIViJpzLNcYuj9VZVPhTju2zGco4kGdet1hDTdt7
fxS95Cmu0JB6bv/9dx9qBZjPghY/uNbOWOvRhTy0SRh8ZgOeP6KJY02gY/a0Lvk=
=+dNq
-----END PGP SIGNATURE-----


From alberto2perez at gmail.com  Tue Mar 17 18:08:38 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Tue, 17 Mar 2015 14:08:38 -0400
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
Message-ID: <CAMZauGoTHqrYsS=otfcUiVFkdpwYqcDLsTTiuXLizFE0U5AVdg@mail.gmail.com>

I did this once, with external acl, you can specify a ttl of each
external acl and also make your acl reload the list any time you want.

My external acl loaded users (in your case domains) from a list in
hard disk to memory, and then each time squid asked acl verify for a
preset  elapsed time to reuse loaded in memory list or load it again
before answer.

Be aware that dst-domain is a fast acl and you will turn this check
into a slow acl so you should consider if you really need this.

I can provide more details if you decide to use it like this.

Hope it helps



On 3/17/15, Samuel Anderson <sam at idsdoc.com> wrote:
> Hello all,
>
> Does anyone know of a way to reload a single ACL list? I have a very
> complicated and large config file that takes around 30 seconds to reload
> when I run the (squid3 -k reconfigure) command. I have several ACL lists
> that need to be updated throughout the day and it would be nice if I could
> only reload those ACL lists and not the entire config. Its problematic
> because while its reloading, the server is effectively down and disrupts
> Internet access for the rest of the users. Below is a small sample of the
> lists that will be updated. If I could add a TTL to the lists so squid
> would reload them periodically without a full reconfigure would be ideal.
>
>
>
> acl GLOBAL-WHITELIST dstdomain "/etc/squid3/whitelists/GLOBAL-WHITELIST"
> acl UNRESTRICTED-WHITELIST dstdomain
> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST"
> acl DEV-WHITELIST dstdomain "/etc/squid3/whitelists/DEV-WHITELIST"
> acl SALES-WHITELIST dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"
>
>
> Thanks
>
>
> --
> Samuel Anderson  |  Information Technology Administrator  |  International
> Document Services
>
> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
>
> --
> CONFIDENTIALITY NOTICE:
> This e-mail and any attachments are confidential. If you are not an
> intended recipient, please contact the sender to report the error and
> delete all copies of this message from your system.  Any unauthorized
> review, use, disclosure or distribution is prohibited.
>


From yvoinov at gmail.com  Tue Mar 17 18:08:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Mar 2015 00:08:35 +0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <1426615251.9328.17.camel@desktop.bpk2.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <1426615251.9328.17.camel@desktop.bpk2.com>
Message-ID: <55086DA3.902@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Brendan reads my thoughts. :)

You can, of course, use two or more squid instances and Cisco with
configured WCCP protocol before it. WCCP can plays with several cache
instances in load balancing role. Running squid at this moment sends
"here I am" messages to WCCP-enabled router, which will redirect
traffic on alive cache. The same time you can reconfigure second squid
instance a visa versa.

18.03.15 0:00, Brendan Kearney ?????:
> On Tue, 2015-03-17 at 11:59 -0600, Samuel Anderson wrote:
>> Unfortunately thats not really an option for me. I've already
>> built everything just using squid. It works great and does
>> everything I need it to do with the exception of refreshing the
>> ACL lists. I just need to find a way to refresh those single
>> lists without disrupting Internet traffic to the users. If anyone
>> knows how to do this I would greatly appreciate it.
>> 
>> On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov
>> <yvoinov at gmail.com> wrote:
> Did you hear about rewriters and filters? I.e., squidGuard, or 
> Dansguardian? Or, of course 
> https://www.urlfilterdb.com/products/ufdbguard.html ? It has
> separate server process which can be restart VERY quickly 
> independently of squid.
> 
> 17.03.15 23:35, Samuel Anderson ?????:
>> Hello all,
> 
>> Does anyone know of a way to reload a single ACL list? I
> have a
>> very complicated and large config file that takes around 30
> seconds
>> to reload when I run the (squid3 -k reconfigure) command. I
> have
>> several ACL lists that need to be updated throughout the day
> and it
>> would be nice if I could only reload those ACL lists and not
> the
>> entire config. Its problematic because while its reloading,
> the
>> server is effectively down and disrupts Internet access for
> the
>> rest of the users. Below is a small sample of the lists that
> will
>> be updated. If I could add a TTL to the lists so squid would
> reload
>> them periodically without a full reconfigure would be ideal.
> 
> 
> 
>> acl GLOBAL-WHITELIST dstdomain 
>> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl 
>> UNRESTRICTED-WHITELIST dstdomain 
>> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl
> DEV-WHITELIST
>> dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl 
>> SALES-WHITELIST dstdomain
> "/etc/squid3/whitelists/SALES-WHITELIST"
> 
> 
>> Thanks
> 
> 
> 
> 
>> _______________________________________________ squid-users
> mailing
>> list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
>> 
>> 
>> 
>> -- Samuel Anderson  |  Information Technology Administrator  | 
>> International Document Services
>> 
>> 
>> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT
>> 84020-4607
>> 
>> 
>> 
>> CONFIDENTIALITY NOTICE: This e-mail and any attachments are
>> confidential. If you are not an intended recipient, please
>> contact the sender to report the error and delete all copies of
>> this message from your system.  Any unauthorized review, use,
>> disclosure or distribution is prohibited. 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> do you have the luxury of multiple squid instances behind a load 
> balancer?  mark one offline at the LB, reconfigure, mark online at
> the LB.  Lather, rinse, repeat.
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVCG2jAAoJENNXIZxhPexGR4oH/jiPuWnYLhvtU8HnW6KkvjbG
74s8OipJ/j8zzdVZg7Uep+BdcZ1nbnzKSkG/50ByEZ6KTnlF67TAB5QhX6YRlrSU
NNnRpLKIxPuXRkl6kIkEpulGA8D+0ErlhWrM5NwgQpxzT+mqfBtS/u1lfq51CZFw
HoC6OGrTWtBNDwOOnd8V1CySQKr5xw4M6bNBcyr2E33+Dcww5T/XwY0s4gImFeUW
yA9Je0aihFPyVFcBsa7UcEMkoXPShMqu40zrRZ2scvyp9UYjYrD88XRzhLxcxGae
mlt6ia0IeVVfmMfvOgA9C5MyEYe6/XkxmLSZLAU839FgKXIj60TYfy2wB2Sa5c0=
=YqPV
-----END PGP SIGNATURE-----


From yvoinov at gmail.com  Tue Mar 17 18:09:53 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Mar 2015 00:09:53 +0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAMZauGoTHqrYsS=otfcUiVFkdpwYqcDLsTTiuXLizFE0U5AVdg@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <CAMZauGoTHqrYsS=otfcUiVFkdpwYqcDLsTTiuXLizFE0U5AVdg@mail.gmail.com>
Message-ID: <55086DF1.80100@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Alberto,

quetsion is not about this completely. ;)

18.03.15 0:08, Alberto Perez ?????:
> I did this once, with external acl, you can specify a ttl of each 
> external acl and also make your acl reload the list any time you
> want.
> 
> My external acl loaded users (in your case domains) from a list in 
> hard disk to memory, and then each time squid asked acl verify for
> a preset  elapsed time to reuse loaded in memory list or load it
> again before answer.
> 
> Be aware that dst-domain is a fast acl and you will turn this
> check into a slow acl so you should consider if you really need
> this.
> 
> I can provide more details if you decide to use it like this.
> 
> Hope it helps
> 
> 
> 
> On 3/17/15, Samuel Anderson <sam at idsdoc.com> wrote:
>> Hello all,
>> 
>> Does anyone know of a way to reload a single ACL list? I have a
>> very complicated and large config file that takes around 30
>> seconds to reload when I run the (squid3 -k reconfigure) command.
>> I have several ACL lists that need to be updated throughout the
>> day and it would be nice if I could only reload those ACL lists
>> and not the entire config. Its problematic because while its
>> reloading, the server is effectively down and disrupts Internet
>> access for the rest of the users. Below is a small sample of the 
>> lists that will be updated. If I could add a TTL to the lists so
>> squid would reload them periodically without a full reconfigure
>> would be ideal.
>> 
>> 
>> 
>> acl GLOBAL-WHITELIST dstdomain
>> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl
>> UNRESTRICTED-WHITELIST dstdomain 
>> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl DEV-WHITELIST
>> dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl
>> SALES-WHITELIST dstdomain
>> "/etc/squid3/whitelists/SALES-WHITELIST"
>> 
>> 
>> Thanks
>> 
>> 
>> -- Samuel Anderson  |  Information Technology Administrator  |
>> International Document Services
>> 
>> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT
>> 84020-4607
>> 
>> -- CONFIDENTIALITY NOTICE: This e-mail and any attachments are
>> confidential. If you are not an intended recipient, please
>> contact the sender to report the error and delete all copies of
>> this message from your system.  Any unauthorized review, use,
>> disclosure or distribution is prohibited.
>> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVCG3xAAoJENNXIZxhPexGR/cH/2dwgAMBlwCm2CQVFuV5Xq6H
MM8hjT4Eqwe4K4TIRWLwgjuNTVjBGCEvN8mVSYe6WPHxv+HPzoByhv31I7IDscaw
caaJRnFg7/wMyxNnc/EmaVkUaYjHYaGbPit0iKkdK2oq/TxwZAIgppAJ5wZNTliw
SMWDgOhvqR24z1I8mnJ5WInG/tRc+duyyqS3HTWc1antJ0+ahR9Z4XRGQmPbiF+9
ayoP4tRKm+kwTjQpLjizl/Ywzw8wVajneJ+dxBc/rT2vgQjIII5LJPZfBGXM3E17
TJOAaI9GEuzgOmnNfcCMLGA6NztuA+cxGir9V67nSi901X3GvMbjKiPNwa9skaE=
=5NMs
-----END PGP SIGNATURE-----


From bpk678 at gmail.com  Tue Mar 17 18:31:41 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 17 Mar 2015 14:31:41 -0400
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <55086DA3.902@gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <1426615251.9328.17.camel@desktop.bpk2.com> <55086DA3.902@gmail.com>
Message-ID: <1426617101.9328.19.camel@desktop.bpk2.com>

On Wed, 2015-03-18 at 00:08 +0600, Yuri Voinov wrote:
> Brendan reads my thoughts. :)
> 
> You can, of course, use two or more squid instances and Cisco with
> configured WCCP protocol before it. WCCP can plays with several cache
> instances in load balancing role. Running squid at this moment sends
> "here I am" messages to WCCP-enabled router, which will redirect
> traffic on alive cache. The same time you can reconfigure second squid
> instance a visa versa.
> 
> 18.03.15 0:00, Brendan Kearney ?????:
> > On Tue, 2015-03-17 at 11:59 -0600, Samuel Anderson wrote:
> >> Unfortunately thats not really an option for me. I've already
> >> built everything just using squid. It works great and does
> >> everything I need it to do with the exception of refreshing the
> >> ACL lists. I just need to find a way to refresh those single
> >> lists without disrupting Internet traffic to the users. If anyone
> >> knows how to do this I would greatly appreciate it.
> >> 
> >> On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov
> >> <yvoinov at gmail.com> wrote:
> > Did you hear about rewriters and filters? I.e., squidGuard, or 
> > Dansguardian? Or, of course 
> > https://www.urlfilterdb.com/products/ufdbguard.html ? It has
> > separate server process which can be restart VERY quickly 
> > independently of squid.
> > 
> > 17.03.15 23:35, Samuel Anderson ?????:
> >> Hello all,
> > 
> >> Does anyone know of a way to reload a single ACL list? I
> > have a
> >> very complicated and large config file that takes around 30
> > seconds
> >> to reload when I run the (squid3 -k reconfigure) command. I
> > have
> >> several ACL lists that need to be updated throughout the day
> > and it
> >> would be nice if I could only reload those ACL lists and not
> > the
> >> entire config. Its problematic because while its reloading,
> > the
> >> server is effectively down and disrupts Internet access for
> > the
> >> rest of the users. Below is a small sample of the lists that
> > will
> >> be updated. If I could add a TTL to the lists so squid would
> > reload
> >> them periodically without a full reconfigure would be ideal.
> > 
> > 
> > 
> >> acl GLOBAL-WHITELIST dstdomain 
> >> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl 
> >> UNRESTRICTED-WHITELIST dstdomain 
> >> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl
> > DEV-WHITELIST
> >> dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl 
> >> SALES-WHITELIST dstdomain
> > "/etc/squid3/whitelists/SALES-WHITELIST"
> > 
> > 
> >> Thanks
> > 
> > 
> > 
> > 
> >> _______________________________________________ squid-users
> > mailing
> >> list squid-users at lists.squid-cache.org 
> >> http://lists.squid-cache.org/listinfo/squid-users
> > 
> >> _______________________________________________ squid-users
> >> mailing list squid-users at lists.squid-cache.org 
> >> http://lists.squid-cache.org/listinfo/squid-users
> >> 
> >> 
> >> 
> >> 
> >> -- Samuel Anderson  |  Information Technology Administrator  | 
> >> International Document Services
> >> 
> >> 
> >> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT
> >> 84020-4607
> >> 
> >> 
> >> 
> >> CONFIDENTIALITY NOTICE: This e-mail and any attachments are
> >> confidential. If you are not an intended recipient, please
> >> contact the sender to report the error and delete all copies of
> >> this message from your system.  Any unauthorized review, use,
> >> disclosure or distribution is prohibited. 
> >> _______________________________________________ squid-users
> >> mailing list squid-users at lists.squid-cache.org 
> >> http://lists.squid-cache.org/listinfo/squid-users
> > 
> > do you have the luxury of multiple squid instances behind a load 
> > balancer?  mark one offline at the LB, reconfigure, mark online at
> > the LB.  Lather, rinse, repeat.
> > 
> > _______________________________________________ squid-users mailing
> > list squid-users at lists.squid-cache.org 
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

i use haproxy to load balance 2 squid instances.  using this:

http://serverfault.com/questions/249316/how-can-i-remove-balanced-node-from-haproxy-via-command-line

you should be able to setup a process to mark you boxes offline, at
will, thereby allowing you to reconfigure your instances.



From sam at idsdoc.com  Tue Mar 17 18:34:16 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Tue, 17 Mar 2015 12:34:16 -0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <1426617101.9328.19.camel@desktop.bpk2.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <1426615251.9328.17.camel@desktop.bpk2.com>
 <55086DA3.902@gmail.com>
 <1426617101.9328.19.camel@desktop.bpk2.com>
Message-ID: <CAP6yRXg6eJvR7FrCxgAAfX5xZT5F1CPX7+Pw9eQ9Vb=6pqF29A@mail.gmail.com>

Thanks, I'll look into using haproxy.

On Tue, Mar 17, 2015 at 12:31 PM, Brendan Kearney <bpk678 at gmail.com> wrote:

> On Wed, 2015-03-18 at 00:08 +0600, Yuri Voinov wrote:
> > Brendan reads my thoughts. :)
> >
> > You can, of course, use two or more squid instances and Cisco with
> > configured WCCP protocol before it. WCCP can plays with several cache
> > instances in load balancing role. Running squid at this moment sends
> > "here I am" messages to WCCP-enabled router, which will redirect
> > traffic on alive cache. The same time you can reconfigure second squid
> > instance a visa versa.
> >
> > 18.03.15 0:00, Brendan Kearney ?????:
> > > On Tue, 2015-03-17 at 11:59 -0600, Samuel Anderson wrote:
> > >> Unfortunately thats not really an option for me. I've already
> > >> built everything just using squid. It works great and does
> > >> everything I need it to do with the exception of refreshing the
> > >> ACL lists. I just need to find a way to refresh those single
> > >> lists without disrupting Internet traffic to the users. If anyone
> > >> knows how to do this I would greatly appreciate it.
> > >>
> > >> On Tue, Mar 17, 2015 at 11:39 AM, Yuri Voinov
> > >> <yvoinov at gmail.com> wrote:
> > > Did you hear about rewriters and filters? I.e., squidGuard, or
> > > Dansguardian? Or, of course
> > > https://www.urlfilterdb.com/products/ufdbguard.html ? It has
> > > separate server process which can be restart VERY quickly
> > > independently of squid.
> > >
> > > 17.03.15 23:35, Samuel Anderson ?????:
> > >> Hello all,
> > >
> > >> Does anyone know of a way to reload a single ACL list? I
> > > have a
> > >> very complicated and large config file that takes around 30
> > > seconds
> > >> to reload when I run the (squid3 -k reconfigure) command. I
> > > have
> > >> several ACL lists that need to be updated throughout the day
> > > and it
> > >> would be nice if I could only reload those ACL lists and not
> > > the
> > >> entire config. Its problematic because while its reloading,
> > > the
> > >> server is effectively down and disrupts Internet access for
> > > the
> > >> rest of the users. Below is a small sample of the lists that
> > > will
> > >> be updated. If I could add a TTL to the lists so squid would
> > > reload
> > >> them periodically without a full reconfigure would be ideal.
> > >
> > >
> > >
> > >> acl GLOBAL-WHITELIST dstdomain
> > >> "/etc/squid3/whitelists/GLOBAL-WHITELIST" acl
> > >> UNRESTRICTED-WHITELIST dstdomain
> > >> "/etc/squid3/whitelists/UNRESTRICTED-WHITELIST" acl
> > > DEV-WHITELIST
> > >> dstdomain "/etc/squid3/whitelists/DEV-WHITELIST" acl
> > >> SALES-WHITELIST dstdomain
> > > "/etc/squid3/whitelists/SALES-WHITELIST"
> > >
> > >
> > >> Thanks
> > >
> > >
> > >
> > >
> > >> _______________________________________________ squid-users
> > > mailing
> > >> list squid-users at lists.squid-cache.org
> > >> http://lists.squid-cache.org/listinfo/squid-users
> > >
> > >> _______________________________________________ squid-users
> > >> mailing list squid-users at lists.squid-cache.org
> > >> http://lists.squid-cache.org/listinfo/squid-users
> > >>
> > >>
> > >>
> > >>
> > >> -- Samuel Anderson  |  Information Technology Administrator  |
> > >> International Document Services
> > >>
> > >>
> > >> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT
> > >> 84020-4607
> > >>
> > >>
> > >>
> > >> CONFIDENTIALITY NOTICE: This e-mail and any attachments are
> > >> confidential. If you are not an intended recipient, please
> > >> contact the sender to report the error and delete all copies of
> > >> this message from your system.  Any unauthorized review, use,
> > >> disclosure or distribution is prohibited.
> > >> _______________________________________________ squid-users
> > >> mailing list squid-users at lists.squid-cache.org
> > >> http://lists.squid-cache.org/listinfo/squid-users
> > >
> > > do you have the luxury of multiple squid instances behind a load
> > > balancer?  mark one offline at the LB, reconfigure, mark online at
> > > the LB.  Lather, rinse, repeat.
> > >
> > > _______________________________________________ squid-users mailing
> > > list squid-users at lists.squid-cache.org
> > > http://lists.squid-cache.org/listinfo/squid-users
> > >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> i use haproxy to load balance 2 squid instances.  using this:
>
>
> http://serverfault.com/questions/249316/how-can-i-remove-balanced-node-from-haproxy-via-command-line
>
> you should be able to setup a process to mark you boxes offline, at
> will, thereby allowing you to reconfigure your instances.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/3b10e245/attachment.htm>

From marcus.kool at urlfilterdb.com  Tue Mar 17 19:13:30 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 17 Mar 2015 16:13:30 -0300
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
Message-ID: <55087CDA.3010209@urlfilterdb.com>



On 03/17/2015 02:59 PM, Samuel Anderson wrote:
> Unfortunately thats not really an option for me. I've already built everything just using squid. It works great and does everything I need it to do with the exception of refreshing the ACL lists. I
> just need to find a way to refresh those single lists without disrupting Internet traffic to the users. If anyone knows how to do this I would greatly appreciate it.

I understand that you are looking into the haproxy reconfig where
the load balancing is temporarily switched off and the following states exist:

1  load balance using squid1 and squid2
2  load balancer: use squid1 only for new connections and wait up to a few hours until all existing connections through squid2 terminate
3  squid2: squid -k reconfigure and wait 35 seconds
4  load balancer: use squid2 only for new connections and wait up to a few hours until all existing connections through squid1 terminate
5  squid1: squid -k reconfigure and wait 35 seconds
6  load balance using squid1 and squid2

If the waiting in state 2 and state 4 takes too long, you might want to consider ufdbGuard.
It not only does a reconfigure in a few seconds, it also does it independently of Squid and
does not interrupt service at all since it has a configuration option to respond with
'allow all' during a reconfiguration.

Marcus


From bpk678 at gmail.com  Tue Mar 17 19:32:20 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 17 Mar 2015 15:32:20 -0400
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <55087CDA.3010209@urlfilterdb.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <55087CDA.3010209@urlfilterdb.com>
Message-ID: <1426620740.9328.23.camel@desktop.bpk2.com>

On Tue, 2015-03-17 at 16:13 -0300, Marcus Kool wrote:
> it has a configuration option to respond with
> 'allow all' during a reconfiguration.

a Fail-Open policy can be a security gap, and should be considered
carefully before implementing.  the intention of the whitelisted URLs is
to prevent access to content that is otherwise forbidden.  failing open,
even briefly, undermines that control.  what is the default setting
there?



From yvoinov at gmail.com  Tue Mar 17 19:48:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Mar 2015 01:48:25 +0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <55087CDA.3010209@urlfilterdb.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <55087CDA.3010209@urlfilterdb.com>
Message-ID: <55088509.3040005@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I'm already scored ufdbguard.

It reconfigures momentary, without squid interruption.

With arbitrarily complex ACL's.

18.03.15 1:13, Marcus Kool ?????:
> 
> 
> On 03/17/2015 02:59 PM, Samuel Anderson wrote:
>> Unfortunately thats not really an option for me. I've already
>> built everything just using squid. It works great and does
>> everything I need it to do with the exception of refreshing the
>> ACL lists. I just need to find a way to refresh those single
>> lists without disrupting Internet traffic to the users. If anyone
>> knows how to do this I would greatly appreciate it.
> 
> I understand that you are looking into the haproxy reconfig where 
> the load balancing is temporarily switched off and the following
> states exist:
> 
> 1  load balance using squid1 and squid2 2  load balancer: use
> squid1 only for new connections and wait up to a few hours until
> all existing connections through squid2 terminate 3  squid2: squid
> -k reconfigure and wait 35 seconds 4  load balancer: use squid2
> only for new connections and wait up to a few hours until all
> existing connections through squid1 terminate 5  squid1: squid -k
> reconfigure and wait 35 seconds 6  load balance using squid1 and
> squid2
> 
> If the waiting in state 2 and state 4 takes too long, you might
> want to consider ufdbGuard. It not only does a reconfigure in a few
> seconds, it also does it independently of Squid and does not
> interrupt service at all since it has a configuration option to 
> respond with 'allow all' during a reconfiguration.
> 
> Marcus _______________________________________________ squid-users
> mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVCIUJAAoJENNXIZxhPexGWi4H/2FZd+IP0rJ0dlTq2s+bvpE1
osd+NsFmL2tphwPGEXPMpMWtiL+Odwa5XFZ7Ww5s63/Tj8a6zXD+HopjaXev2TEx
umpx8R7yIKFv8s7JM0bs+9etF5XEafeL7scrFI2jzalX5HRDIOl3u4t9PV3cqSam
DncuoNZByKUjyUGKIkXCw3+OvydUGJ/4OnIWwXlY9LJ1slj6gY0r5dM2YfgLLaOW
1qB5QSSqGL50rru7SBfLs7oxInxzzddvKkqRecF5TuFmiHl2mTlb1kMy1yOzoM6x
ENU37Aqz6bae3Xb69jFKSAxt5VHyjsKQ/DK/pbCtel7M4Yz1Od1skJKT17LlWbU=
=v3Uf
-----END PGP SIGNATURE-----


From marcus.kool at urlfilterdb.com  Tue Mar 17 19:53:28 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 17 Mar 2015 16:53:28 -0300
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <1426620740.9328.23.camel@desktop.bpk2.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>	
 <550866E5.5090309@gmail.com>	
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>	
 <55087CDA.3010209@urlfilterdb.com>
 <1426620740.9328.23.camel@desktop.bpk2.com>
Message-ID: <55088638.8040606@urlfilterdb.com>



On 03/17/2015 04:32 PM, Brendan Kearney wrote:
> On Tue, 2015-03-17 at 16:13 -0300, Marcus Kool wrote:
>> it has a configuration option to respond with
>> 'allow all' during a reconfiguration.
>
> a Fail-Open policy can be a security gap, and should be considered
> carefully before implementing.  the intention of the whitelisted URLs is
> to prevent access to content that is otherwise forbidden.  failing open,
> even briefly, undermines that control.  what is the default setting
> there?

The default is 'allow all' and can be changed into 'deny all'.
Neither is perfect.

Another related parameter is url-lookup-delay-during-database-reload
which, if set, artificially gives a slow response which significantly
reduces the number of URL queries in the reconfiguration interval.

One can also do the haproxy failover scenario with ufdbguard.

1  load balance using squid1 and squid2
2  load balancer: use squid1 only for new connections and wait 2 seconds
3  ufdbguard2/squid2: ufdbguardd reload and wait 10 seconds
4  load balancer: use squid2 only for new connections and wait 2 seconds
5  ufdbguard1/squid1: ufdbguardd reload and wait 10 seconds
6  load balance using squid1 and squid2

in state 2 existing connections on squid2 are left alone and no new requests come in so it is safe to reconfigure ufdbguard
same for state 4

Marcus





From johnzeng2013 at yahoo.com  Tue Mar 17 22:26:21 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 18 Mar 2015 06:26:21 +0800
Subject: [squid-users] (about external_acl_type problem ) two people can't
 login and access internet together
Message-ID: <5508AA0D.9060008@yahoo.com>


Hello Dear All:

i test splash portal via external_acl_type ...

Although the first people succeed to login and can access internet , but
when second people succeed to login and can access internet ,

and the firest people have to login again . when the firest people
succeed to login and can access internet ,

second people have to login again .


my meaning is : There's only one person who can access internet at same
time



I guess [channel-ID] is error at my config , but i can't confirm.


if concurrency=10

how to identify or find correct [channel-ID] ,

and

Whether return value format is correct for squid ?

for example

fwrite(STDOUT, $stream_id." ERR\n");



If possible , please give me some advisement .



http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

--------Squid.conf ---------------

external_acl_type session ipv4 concurrency=10 ttl=50 %SRC
/accerater/webgui/public/wifiportal/logincheck.php
acl session_login external session
acl splash_page url_regex -i ^http://192.168.0.198/wifiportal/index.html

deny_info http://192.168.0.198/wifiportal/index.html session_login

http_access allow splash_page
http_access deny !session_login

--------------Helper program config ( php )-----------------

while (!feof(STDIN))
{
$stream_line = trim(fgets(STDIN));
$stream_array = split("[ ]+", $stream_line);
$stream_ip = trim($stream_array[1]);
$stream_id = trim($stream_array[0]);

.........

fwrite(STDOUT, $stream_id." ERR\n");

................

fwrite(STDOUT, $stream_id." OK\n");





From dan at getbusi.com  Wed Mar 18 03:16:55 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 18 Mar 2015 14:16:55 +1100
Subject: [squid-users] v3.5.x RPM for CentOS 6
Message-ID: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>

Hey Eliezer

Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6? 

I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:

1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
--> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
--> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
--> Running transaction check
---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
--> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
--> Finished Dependency Resolution
Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
           Requires: perl(Crypt::OpenSSL::X509)
 You could try using --skip-broken to work around the problem

 You could try running: rpm -Va --nofiles --nodigest

2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.

Do you have any advice on how to overcome these issues?

Thanks!
Dan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/e4699b17/attachment.htm>

From tmblue at gmail.com  Wed Mar 18 04:15:44 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 17 Mar 2015 21:15:44 -0700
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
Message-ID: <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>

I've built a 3.5 and have it running. I can look and see if I can share it with you. Don't believe there is anything special .

Tory 

Sent via the wild blue yonder


> On Mar 17, 2015, at 20:16, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Hey Eliezer
> 
> Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6? 
> 
> I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:
> 
> 1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Running transaction check
> ---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Finished Dependency Resolution
> Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
>            Requires: perl(Crypt::OpenSSL::X509)
>  You could try using --skip-broken to work around the problem
> 
>  You could try running: rpm -Va --nofiles --nodigest
> 
> 2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
> 2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
> 2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
> 2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
> 2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.
> 
> Do you have any advice on how to overcome these issues?
> 
> Thanks!
> Dan
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/d40617ea/attachment.htm>

From johnzeng2013 at yahoo.com  Wed Mar 18 04:24:54 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 18 Mar 2015 12:24:54 +0800
Subject: [squid-users] (about external_acl_type problem ) two people
 can't login and access internet together
In-Reply-To: <5508AA0D.9060008@yahoo.com>
References: <5508AA0D.9060008@yahoo.com>
Message-ID: <5508FE16.5020605@yahoo.com>

> Hello All:
>
> i test splash portal via external_acl_type ...
>
> Although the first people succeed to login and can access internet , but
> when second people succeed to login and can access internet ,
>
> and the firest people have to login again . when the firest people
> succeed to login and can access internet ,
>
> second people have to login again .
>
>
> my meaning is : There's only one person who can access internet at same
> time
>
>
>
> I guess [channel-ID] is error at my config , but i can't confirm.
>
>
> if concurrency=10
>
> how to identify or find correct [channel-ID] ,
>
> and
>
> Whether return value format is correct for squid ?
>
> for example
>
> fwrite(STDOUT, $stream_id." ERR\n");
>
>
>
> If possible , please give me some advisement .
>
>
>
> http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
> http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
>
> --------Squid.conf ---------------
>
> external_acl_type session ipv4 concurrency=10 ttl=50 %SRC
> /accerater/webgui/public/wifiportal/logincheck.php
> acl session_login external session
> acl splash_page url_regex -i ^http://192.168.0.198/wifiportal/index.html
>
> deny_info http://192.168.0.198/wifiportal/index.html session_login
>
> http_access allow splash_page
> http_access deny !session_login
>
> --------------Helper program config ( php )-----------------
>
> while (!feof(STDIN))
> {
> $stream_line = trim(fgets(STDIN));
> $stream_array = split("[ ]+", $stream_line);
> $stream_ip = trim($stream_array[1]);
> $stream_id = trim($stream_array[0]);
>
> .........
>
> fwrite(STDOUT, $stream_id." ERR\n");
>
> ................
>
> fwrite(STDOUT, $stream_id." OK\n");
>
>
>



From dan at getbusi.com  Wed Mar 18 04:49:06 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 18 Mar 2015 15:49:06 +1100
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
 <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
Message-ID: <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>

Hi Tony

Yeah, I wouldn?t mind taking a peek at your SRPM or spec file if you can share?thanks!

> On 18 Mar 2015, at 3:15 pm, Tory M Blue <tmblue at gmail.com> wrote:
> 
> I've built a 3.5 and have it running. I can look and see if I can share it with you. Don't believe there is anything special .
> 
> Tory 
> 
> Sent via the wild blue yonder
> 
> 
> On Mar 17, 2015, at 20:16, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
> 
>> Hey Eliezer
>> 
>> Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6? 
>> 
>> I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:
>> 
>> 1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>> --> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>> --> Running transaction check
>> ---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>> --> Finished Dependency Resolution
>> Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
>>            Requires: perl(Crypt::OpenSSL::X509)
>>  You could try using --skip-broken to work around the problem
>> 
>>  You could try running: rpm -Va --nofiles --nodigest
>> 
>> 2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
>> 2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>> 2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>> 2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
>> 2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.
>> 
>> Do you have any advice on how to overcome these issues?
>> 
>> Thanks!
>> Dan
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/b0f92af0/attachment.htm>

From johnzeng2013 at yahoo.com  Wed Mar 18 04:54:43 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 18 Mar 2015 12:54:43 +0800
Subject: [squid-users] (about external_acl_type problem ) two people
 can't login and access internet together
In-Reply-To: <5508FE16.5020605@yahoo.com>
References: <5508AA0D.9060008@yahoo.com> <5508FE16.5020605@yahoo.com>
Message-ID: <55090513.2060803@yahoo.com>

Hello All

Whether ttl=50 ?value) is too low , Maybe i will update ttl value to
ttl=3600 cache=1048576 .

i have a question still , Whether cached results for external_acl is
reponse from helper program ?

for example :

if FORMAT is %SRC , and helper progrm return "OK\n" ,

and external_acl_type tell squid to cache suitable %SRC ( for example :
client is 192.168.0.21 ,and will cache 192.168.0.21 into Cache valued )

if helper progrm return "ERR\n" ,

won't cache any value or cache src ip into cached negative valued ...


Whether My understanding is correct ?

	  external_acl_type name [options] FORMAT.. /path/to/helper [helper arguments..]

	Options:

	  ttl=n		TTL in seconds for cached results (defaults to 3600
	  		for 1 hour)

	  negative_ttl=n
	  		TTL for cached negative lookups (default same
	  		as ttl)



	  cache=n	Limit the result cache size, default is 262144.
			The expanded FORMAT value is used as the cache key, so
			if the details in FORMAT are highly variable a larger
			cache may be needed to produce reduction in helper load.




http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html


Hello All:

i test splash portal via external_acl_type ...

Although the first people succeed to login and can access internet , but
when second people succeed to login and can access internet ,

and the firest people have to login again . when the firest people
succeed to login and can access internet ,

second people have to login again .


my meaning is : There's only one person who can access internet at same
time



I guess [channel-ID] is error at my config , but i can't confirm.


if concurrency=10

how to identify or find correct [channel-ID] ,

and

Whether return value format is correct for squid ?

for example

fwrite(STDOUT, $stream_id." ERR\n");



If possible , please give me some advisement .



http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

--------Squid.conf ---------------

external_acl_type session ipv4 concurrency=10 ttl=50 %SRC
/accerater/webgui/public/wifiportal/logincheck.php
acl session_login external session
acl splash_page url_regex -i ^http://192.168.0.198/wifiportal/index.html

deny_info http://192.168.0.198/wifiportal/index.html session_login

http_access allow splash_page
http_access deny !session_login

--------------Helper program config ( php )-----------------

while (!feof(STDIN))
{
$stream_line = trim(fgets(STDIN));
$stream_array = split("[ ]+", $stream_line);
$stream_ip = trim($stream_array[1]);
$stream_id = trim($stream_array[0]);

.........

fwrite(STDOUT, $stream_id." ERR\n");

................

fwrite(STDOUT, $stream_id." OK\n");






From dan at getbusi.com  Wed Mar 18 04:54:15 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 18 Mar 2015 15:54:15 +1100
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
 <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
 <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>
Message-ID: <638F2526-2997-498F-8B3F-CDF6A229DC42@getbusi.com>

*Tory ? sorry.

> On 18 Mar 2015, at 3:49 pm, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Hi Tony
> 
> Yeah, I wouldn?t mind taking a peek at your SRPM or spec file if you can share?thanks!
> 
>> On 18 Mar 2015, at 3:15 pm, Tory M Blue <tmblue at gmail.com <mailto:tmblue at gmail.com>> wrote:
>> 
>> I've built a 3.5 and have it running. I can look and see if I can share it with you. Don't believe there is anything special .
>> 
>> Tory 
>> 
>> Sent via the wild blue yonder
>> 
>> 
>> On Mar 17, 2015, at 20:16, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>> 
>>> Hey Eliezer
>>> 
>>> Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6? 
>>> 
>>> I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:
>>> 
>>> 1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
>>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>> --> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>> --> Running transaction check
>>> ---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
>>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>> --> Finished Dependency Resolution
>>> Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
>>>            Requires: perl(Crypt::OpenSSL::X509)
>>>  You could try using --skip-broken to work around the problem
>>> 
>>>  You could try running: rpm -Va --nofiles --nodigest
>>> 
>>> 2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
>>> 2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
>>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>>> 2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
>>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>>> 2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
>>> 2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.
>>> 
>>> Do you have any advice on how to overcome these issues?
>>> 
>>> Thanks!
>>> Dan
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/bcc9c539/attachment.htm>

From dan at getbusi.com  Wed Mar 18 05:21:47 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 18 Mar 2015 16:21:47 +1100
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
Message-ID: <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>

Bumpity bump

Had this go down exactly the same way this past Monday at Deployment #1.

> On 10 Mar 2015, at 4:51 pm, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Hey folks
> 
> After having many of our systems running Squid 3.4.12 for a couple of weeks now we had two different deployments fail today due to SSL DB corruption.
> 
> Never seen this in almost 9 months of SSL bump being in production and there were no problems in either cache log until the ?wrong number of fields? lines, apparently.
> 
> Anyone else?
> 
> Deployment #1 log excerpt:
> wrong number of fields on line 505 (looking for field 6, got 1, '' left)
> (squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
> 2015/03/10 09:04:24 kid1| WARNING: ssl_crtd #Hlpr0 exited
> 2015/03/10 09:04:24 kid1| Too few ssl_crtd processes are running (need 1/32)
> 2015/03/10 09:04:24 kid1| Starting new helpers
> 2015/03/10 09:04:24 kid1| helperOpenServers: Starting 1/32 'squid_ssl_crtd' processes
> 2015/03/10 09:04:24 kid1| "ssl_crtd" helper returned <NULL> reply.
> wrong number of fields on line 505 (looking for field 6, got 1, '' left)
> (squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
> 
> Deployment #2 log excerpt:
> wrong number of fields on line 2 (looking for field 6, got 1, '' left)
> (squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
> 2015/03/10 15:29:16 kid1| WARNING: ssl_crtd #Hlpr0 exited
> 2015/03/10 15:29:16 kid1| Too few ssl_crtd processes are running (need 1/32)
> 2015/03/10 15:29:16 kid1| Starting new helpers
> 2015/03/10 15:29:16 kid1| helperOpenServers: Starting 1/32 'squid_ssl_crtd' processes
> 2015/03/10 15:29:17 kid1| "ssl_crtd" helper returned <NULL> reply.
> wrong number of fields on line 2 (looking for field 6, got 1, '' left)
> (squid_ssl_crtd): The SSL certificate database /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/564bf4fb/attachment.htm>

From dan at getbusi.com  Wed Mar 18 06:03:16 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 18 Mar 2015 17:03:16 +1100
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <CAC49LV4gFgO6VX4=ZeCPqOOdptPj=OVTnaBMoh1_cO+pYQ6Org@mail.gmail.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
 <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
 <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>
 <638F2526-2997-498F-8B3F-CDF6A229DC42@getbusi.com>
 <CAC49LV4gFgO6VX4=ZeCPqOOdptPj=OVTnaBMoh1_cO+pYQ6Org@mail.gmail.com>
Message-ID: <6EE379FC-7378-4998-8483-26F29752657F@getbusi.com>

Hi Donny

I gathered that much. I guess what I specifically am asking for is:

- Which CentOS 6 package includes the missing perl modules?
- How do I grant the ?pinger? the correct permissions in CentOS 6?

Cheers
Dan

> On 18 Mar 2015, at 4:58 pm, Donny Vibianto <l4ng1t at gmail.com> wrote:
> 
> hi Dan,
> 
> eliezer already made binary for centos 6.x, you just missed perl modules and pinger need to have correct permission.
> 
> 
> 
> On Wed, Mar 18, 2015 at 11:54 AM, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
> *Tory ? sorry.
> 
>> On 18 Mar 2015, at 3:49 pm, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>> 
>> Hi Tony
>> 
>> Yeah, I wouldn?t mind taking a peek at your SRPM or spec file if you can share?thanks!
>> 
>>> On 18 Mar 2015, at 3:15 pm, Tory M Blue <tmblue at gmail.com <mailto:tmblue at gmail.com>> wrote:
>>> 
>>> I've built a 3.5 and have it running. I can look and see if I can share it with you. Don't believe there is anything special .
>>> 
>>> Tory 
>>> 
>>> Sent via the wild blue yonder
>>> 
>>> 
>>> On Mar 17, 2015, at 20:16, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>>> 
>>>> Hey Eliezer
>>>> 
>>>> Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6? 
>>>> 
>>>> I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:
>>>> 
>>>> 1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
>>>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>>>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>>> --> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>>> --> Running transaction check
>>>> ---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
>>>> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
>>>> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
>>>> --> Finished Dependency Resolution
>>>> Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
>>>>            Requires: perl(Crypt::OpenSSL::X509)
>>>>  You could try using --skip-broken to work around the problem
>>>> 
>>>>  You could try running: rpm -Va --nofiles --nodigest
>>>> 
>>>> 2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
>>>> 2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
>>>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>>>> 2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
>>>> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
>>>> 2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
>>>> 2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.
>>>> 
>>>> Do you have any advice on how to overcome these issues?
>>>> 
>>>> Thanks!
>>>> Dan
>>>> 
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
>>>> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
>> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/0639b157/attachment.htm>

From ahmed.zaeem at netstream.ps  Wed Mar 18 16:57:37 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Wed, 18 Mar 2015 09:57:37 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
Message-ID: <000201d0619c$a394e120$eabea360$@netstream.ps>

Hi Guys

I need help in blocking images that has size less than 40 KB

 

 

Any guidance or help will be appreciated

 

regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/5e0b2993/attachment.htm>

From emz at norma.perm.ru  Wed Mar 18 08:50:15 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Wed, 18 Mar 2015 13:50:15 +0500
Subject: [squid-users] squid SMP and SNMP
Message-ID: <55093C47.8090108@norma.perm.ru>

Hi.

I'm gathering statistics from squid using SNMP. When I use single
process everything is fine, but when it comes to multiple workers - SNMP
doesn't work - I got timeout when trying to read data with snmpwalk.

I'm using the following tweak:

snmp_port 340${process_number}

both workers bind on ports 3401 and 3402 indeed, but then I got this
timeout.
Does anyone have a success story about squid SMP and SNMP ?

I wrote a message about this problem about a year or so, it was 3.3.x,
but situation didn't change.
Should I report this as a bug ?

Thanks.
Eugene.





From yvoinov at gmail.com  Wed Mar 18 08:53:29 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Mar 2015 14:53:29 +0600
Subject: [squid-users] Random SSL bump DB corruption
In-Reply-To: <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
References: <1221040D-631B-4285-8F55-1170C29F4809@getbusi.com>
 <7E8CA94D-5661-47CA-8167-0DE8CDEA158E@getbusi.com>
Message-ID: <55093D09.2070909@gmail.com>

As I can research,

this problem produces one of Apple service under HTTPS.

When client query something like iTunes, squid gets strange certificate 
which is corrupts DB.

I found no solution at this time. Just stop squid and cleanup SSL db.

WBR, Yuri

18.03.15 11:21, Dan Charlesworth ?????:
> Bumpity bump
>
> Had this go down exactly the same way this past Monday at Deployment #1.
>
>> On 10 Mar 2015, at 4:51 pm, Dan Charlesworth <dan at getbusi.com 
>> <mailto:dan at getbusi.com>> wrote:
>>
>> Hey folks
>>
>> After having many of our systems running Squid 3.4.12 for a couple of 
>> weeks now we had two different deployments fail today due to SSL DB 
>> corruption.
>>
>> Never seen this in almost 9 months of SSL bump being in production 
>> and there were no problems in either cache log until the ?wrong 
>> number of fields? lines, apparently.
>>
>> Anyone else?
>>
>> Deployment #1 log excerpt:
>> wrong number of fields on line 505 (looking for field 6, got 1, '' left)
>> (squid_ssl_crtd): The SSL certificate database 
>> /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
>> 2015/03/10 09:04:24 kid1| WARNING: ssl_crtd #Hlpr0 exited
>> 2015/03/10 09:04:24 kid1| Too few ssl_crtd processes are running 
>> (need 1/32)
>> 2015/03/10 09:04:24 kid1| Starting new helpers
>> 2015/03/10 09:04:24 kid1| helperOpenServers: Starting 1/32 
>> 'squid_ssl_crtd' processes
>> 2015/03/10 09:04:24 kid1| "ssl_crtd" helper returned <NULL> reply.
>> wrong number of fields on line 505 (looking for field 6, got 1, '' left)
>> (squid_ssl_crtd): The SSL certificate database 
>> /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
>>
>> Deployment #2 log excerpt:
>> wrong number of fields on line 2 (looking for field 6, got 1, '' left)
>> (squid_ssl_crtd): The SSL certificate database 
>> /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
>> 2015/03/10 15:29:16 kid1| WARNING: ssl_crtd #Hlpr0 exited
>> 2015/03/10 15:29:16 kid1| Too few ssl_crtd processes are running 
>> (need 1/32)
>> 2015/03/10 15:29:16 kid1| Starting new helpers
>> 2015/03/10 15:29:16 kid1| helperOpenServers: Starting 1/32 
>> 'squid_ssl_crtd' processes
>> 2015/03/10 15:29:17 kid1| "ssl_crtd" helper returned <NULL> reply.
>> wrong number of fields on line 2 (looking for field 6, got 1, '' left)
>> (squid_ssl_crtd): The SSL certificate database 
>> /usr/local/mwf/mwf13/squid/ssl_db is corrupted. Please rebuild
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/67d321f3/attachment.htm>

From johnzeng2013 at yahoo.com  Wed Mar 18 09:04:18 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 18 Mar 2015 17:04:18 +0800
Subject: [squid-users] (about external_acl_type problem ) two people can't
 login and access internet together
In-Reply-To: <55090513.2060803@yahoo.com>
References: <55090513.2060803@yahoo.com>
Message-ID: <55093F92.9090804@yahoo.com>


Hello All


         if possible ,please give me some advisement , thanks 



 Whether ttl=50 ?value) is too low , Maybe i will update ttl value to
ttl=3600 cache=1048576 .

i have a question still , Whether cached results for external_acl is
reponse from helper program ?

for example :

if FORMAT is %SRC , and helper progrm return "OK\n" ,

and external_acl_type tell squid to cache suitable %SRC ( for example :
client is 192.168.0.21 ,and will cache 192.168.0.21 into Cache valued )

if helper progrm return "ERR\n" ,

won't cache any value or cache src ip into cached negative valued ...


Whether My understanding is correct ?

	  external_acl_type name [options] FORMAT.. /path/to/helper [helper arguments..]

	Options:

	  ttl=n		TTL in seconds for cached results (defaults to 3600
	  		for 1 hour)

	  negative_ttl=n
	  		TTL for cached negative lookups (default same
	  		as ttl)



	  cache=n	Limit the result cache size, default is 262144.
			The expanded FORMAT value is used as the cache key, so
			if the details in FORMAT are highly variable a larger
			cache may be needed to produce reduction in helper load.




http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html


Hello All:

i test splash portal via external_acl_type ...

Although the first people succeed to login and can access internet , but
when second people succeed to login and can access internet ,

and the firest people have to login again . when the firest people
succeed to login and can access internet ,

second people have to login again .


my meaning is : There's only one person who can access internet at same
time



I guess [channel-ID] is error at my config , but i can't confirm.


if concurrency=10

how to identify or find correct [channel-ID] ,

and

Whether return value format is correct for squid ?

for example

fwrite(STDOUT, $stream_id." ERR\n");



If possible , please give me some advisement .



http://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

--------Squid.conf ---------------

external_acl_type session ipv4 concurrency=10 ttl=50 %SRC
/accerater/webgui/public/wifiportal/logincheck.php
acl session_login external session
acl splash_page url_regex -i ^http://192.168.0.198/wifiportal/index.html

deny_info http://192.168.0.198/wifiportal/index.html session_login

http_access allow splash_page
http_access deny !session_login

--------------Helper program config ( php )-----------------

while (!feof(STDIN))
{
$stream_line = trim(fgets(STDIN));
$stream_array = split("[ ]+", $stream_line);
$stream_ip = trim($stream_array[1]);
$stream_id = trim($stream_array[0]);

.........

fwrite(STDOUT, $stream_id." ERR\n");

................

fwrite(STDOUT, $stream_id." OK\n");








From squid3 at treenet.co.nz  Wed Mar 18 10:58:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Mar 2015 23:58:28 +1300
Subject: [squid-users] (about external_acl_type problem ) two people
 can't login and access internet together
In-Reply-To: <55093F92.9090804@yahoo.com>
References: <55090513.2060803@yahoo.com> <55093F92.9090804@yahoo.com>
Message-ID: <55095A54.8030404@treenet.co.nz>

On 18/03/2015 10:04 p.m., johnzeng wrote:
> 
> Hello All
> 
> 
>          if possible ,please give me some advisement , thanks 
> 
> 
> 
>  Whether ttl=50 ?value) is too low , Maybe i will update ttl value to
> ttl=3600 cache=1048576 .

Whatever. You know better than anyone else about that decision.

> 
> i have a question still , Whether cached results for external_acl is
> reponse from helper program ?
> 

Yes.

> for example :
> 
> if FORMAT is %SRC , and helper progrm return "OK\n" ,
> 
> and external_acl_type tell squid to cache suitable %SRC ( for example :
> client is 192.168.0.21 ,and will cache 192.168.0.21 into Cache valued )


The %SRC format is the cache key. The "OK" is the value cached for the
key "192.168.0.21".

Whenever "192.168.0.21" is looked up the stored value for it may be used
instead of calling the helper again.


> 
> if helper progrm return "ERR\n" ,
> 
> won't cache any value or cache src ip into cached negative valued ...

"ERR" is a successful "negative lookup". The negative_ttl=N value
applies to how long those get stored.

Squid default is to store both positive (OK) and negative (ERR) results
for the same TTL.

Amos


From squid3 at treenet.co.nz  Wed Mar 18 11:04:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 00:04:49 +1300
Subject: [squid-users] squid SMP and SNMP
In-Reply-To: <55093C47.8090108@norma.perm.ru>
References: <55093C47.8090108@norma.perm.ru>
Message-ID: <55095BD1.2090501@treenet.co.nz>

On 18/03/2015 9:50 p.m., Eugene M. Zheganin wrote:
> Hi.
> 
> I'm gathering statistics from squid using SNMP. When I use single
> process everything is fine, but when it comes to multiple workers - SNMP
> doesn't work - I got timeout when trying to read data with snmpwalk.
> 
> I'm using the following tweak:
> 
> snmp_port 340${process_number}
> 
> both workers bind on ports 3401 and 3402 indeed, but then I got this
> timeout.
> Does anyone have a success story about squid SMP and SNMP ?


SNMP is on the list of SMP-aware features.

The worker receiving the SNMP request will contact other workers to
fetch the data for producing the SNMP response. This may take some time.


> 
> I wrote a message about this problem about a year or so, it was 3.3.x,
> but situation didn't change.

Nothing has changed in SNMP or other mgr report generation code since then.

Amos



From squid3 at treenet.co.nz  Wed Mar 18 11:06:11 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 00:06:11 +1300
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000201d0619c$a394e120$eabea360$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
Message-ID: <55095C23.6040407@treenet.co.nz>

On 19/03/2015 5:57 a.m., snakeeyes wrote:
> Hi Guys
> 
> I need help in blocking images that has size less than 40 KB
> 
> Any guidance or help will be appreciated
> 
>  

Use the Squid provided access controls to manage access to things.
<http://wiki.squid-cache.org/SquidFaq/SquidAcl>

Amos


From squid3 at treenet.co.nz  Wed Mar 18 11:11:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 00:11:04 +1300
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <6EE379FC-7378-4998-8483-26F29752657F@getbusi.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
 <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
 <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>
 <638F2526-2997-498F-8B3F-CDF6A229DC42@getbusi.com>
 <CAC49LV4gFgO6VX4=ZeCPqOOdptPj=OVTnaBMoh1_cO+pYQ6Org@mail.gmail.com>
 <6EE379FC-7378-4998-8483-26F29752657F@getbusi.com>
Message-ID: <55095D48.4030406@treenet.co.nz>

On 18/03/2015 7:03 p.m., Dan Charlesworth wrote:
> Hi Donny
> 
> I gathered that much. I guess what I specifically am asking for is:
> 
> - Which CentOS 6 package includes the missing perl modules?

perl ?

The helpers that require it are scripts, not compiled binaries. So they
should run with any perl 4/5 version normally installed.

Even just installing the modules with cpan should work.


> - How do I grant the ?pinger? the correct permissions in CentOS 6?

Should be possible just to install Squid with:
  make install install-pinger

Amos



From johnzeng2013 at yahoo.com  Wed Mar 18 12:53:22 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 18 Mar 2015 20:53:22 +0800
Subject: [squid-users] (about external_acl_type problem ) two people
 can't login and access internet together
In-Reply-To: <55095A54.8030404@treenet.co.nz>
References: <55090513.2060803@yahoo.com> <55093F92.9090804@yahoo.com>
 <55095A54.8030404@treenet.co.nz>
Message-ID: <55097542.4000302@yahoo.com>


Hello Amos:

                    Thanks again , and i tested for the part and sloved 
the problem just now .


                     Have a good day with you .


                     Best Regards



From emz at norma.perm.ru  Wed Mar 18 13:50:55 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Wed, 18 Mar 2015 18:50:55 +0500
Subject: [squid-users] squid SMP and SNMP
In-Reply-To: <55095BD1.2090501@treenet.co.nz>
References: <55093C47.8090108@norma.perm.ru> <55095BD1.2090501@treenet.co.nz>
Message-ID: <550982BF.1040903@norma.perm.ru>

Hi.

On 18.03.2015 16:04, Amos Jeffries wrote:
>
> SNMP is on the list of SMP-aware features.
>
> The worker receiving the SNMP request will contact other workers to
> fetch the data for producing the SNMP response. This may take some time.
>
Yeah, but it seems like it doesn't happen. Plus, I'm getting the errors
in the cache.log on each attempt:

[root at taiga:etc/squid]# snmpwalk localhost:3402
1.3.6.1.4.1.3495.1.2.1.0     
Timeout: No Response from localhost:3402

and in the log:

2015/03/18 18:48:26 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:46682: (22) Invalid argument
2015/03/18 18:48:49 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument
2015/03/18 18:48:50 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument
2015/03/18 18:48:51 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument
2015/03/18 18:48:52 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument
2015/03/18 18:48:53 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument
2015/03/18 18:48:54 kid3| comm_udp_sendto: FD 34, (family=2)
127.0.0.1:36623: (22) Invalid argument

Thanks.
Eugene.



From squid3 at treenet.co.nz  Wed Mar 18 14:02:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 03:02:58 +1300
Subject: [squid-users] squid SMP and SNMP
In-Reply-To: <550982BF.1040903@norma.perm.ru>
References: <55093C47.8090108@norma.perm.ru> <55095BD1.2090501@treenet.co.nz>
 <550982BF.1040903@norma.perm.ru>
Message-ID: <55098592.30207@treenet.co.nz>

On 19/03/2015 2:50 a.m., Eugene M. Zheganin wrote:
> Hi.
> 
> On 18.03.2015 16:04, Amos Jeffries wrote:
>>
>> SNMP is on the list of SMP-aware features.
>>
>> The worker receiving the SNMP request will contact other workers to
>> fetch the data for producing the SNMP response. This may take some time.
>>
> Yeah, but it seems like it doesn't happen. Plus, I'm getting the errors
> in the cache.log on each attempt:
> 
> [root at taiga:etc/squid]# snmpwalk localhost:3402
> 1.3.6.1.4.1.3495.1.2.1.0     
> Timeout: No Response from localhost:3402
> 
> and in the log:
> 
> 2015/03/18 18:48:26 kid3| comm_udp_sendto: FD 34, (family=2)
> 127.0.0.1:46682: (22) Invalid argument

Process kid3 (SMP coordinator) is attempting to respond.

Since you configured:
  snmp_port 340${process_number}

and the coordinator is process number 3 I think it will be using port
3403 for that response.

Amos



From leolistas at solutti.com.br  Wed Mar 18 14:32:26 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Wed, 18 Mar 2015 11:32:26 -0300
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <55095C23.6040407@treenet.co.nz>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz>
Message-ID: <55098C7A.3070502@solutti.com.br>

On 18/03/15 08:06, Amos Jeffries wrote:
> On 19/03/2015 5:57 a.m., snakeeyes wrote:
>> I need help in blocking images that has size less than 40 KB
>>
>>   
> Use the Squid provided access controls to manage access to things.
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl>
>

     You should know that you cannot evaluate the response size using 
only the request data. So to acchieve what you want, data from the reply 
must be considered as well, the response size for example.

     Images can be identified by the presence of '.jpg' or '.png' on the 
request URL, but images can be generated on-the-fly by scripts as well, 
so you wont see those extensions all the time. In that case, analyzing 
replies mime headers can be usefull as well, the reply mime type having 
'image' is a great indication that we're receiving an image.

     Put all that together and you'll acchieve the rules you want to. 
But keep in mind that you'll probably break A LOT of sites who 'slices' 
images, background images, menus and all sort of things. I would call 
that a VERY bad idea, but can be acchieved with a few rules.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From Richard.Aspley at hammonds-uk.com  Wed Mar 18 14:36:25 2015
From: Richard.Aspley at hammonds-uk.com (Rich549)
Date: Wed, 18 Mar 2015 07:36:25 -0700 (PDT)
Subject: [squid-users] Increase number of ext_ldap_group_acl processes
Message-ID: <1426689385646-4670484.post@n4.nabble.com>

Just a quick question, if I want to increase the number of ext_ldap_group_acl
processes that start with Squid then what would I need to add into my
config?

My reason for asking is because I keep seeing this in the cache.log:

2015/03/18 12:25:58| WARNING: external ACL 'internet_domain_group' queue
overload. Using stale result.

Restarting the Squid service clears the error and I don't see it for weeks
but I'd rather just put a fix in place and have read that increasing the
number of processes can do that.

Thanks,

Rich



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Increase-number-of-ext-ldap-group-acl-processes-tp4670484.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar 18 15:03:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 04:03:19 +1300
Subject: [squid-users] Increase number of ext_ldap_group_acl processes
In-Reply-To: <1426689385646-4670484.post@n4.nabble.com>
References: <1426689385646-4670484.post@n4.nabble.com>
Message-ID: <550993B7.5030801@treenet.co.nz>

On 19/03/2015 3:36 a.m., Rich549 wrote:
> Just a quick question, if I want to increase the number of ext_ldap_group_acl
> processes that start with Squid then what would I need to add into my
> config?
> 
> My reason for asking is because I keep seeing this in the cache.log:
> 
> 2015/03/18 12:25:58| WARNING: external ACL 'internet_domain_group' queue
> overload. Using stale result.
> 
> Restarting the Squid service clears the error and I don't see it for weeks
> but I'd rather just put a fix in place and have read that increasing the
> number of processes can do that.


Please see the squid.conf documentation:
 <http://www.squid-cache.org/Doc/config/external_acl_type/>

Amos


From ahmed.zaeem at netstream.ps  Thu Mar 19 00:35:28 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Wed, 18 Mar 2015 17:35:28 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <55098C7A.3070502@solutti.com.br>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
Message-ID: <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>

Thank you so much  Amos and leonardo
Can you provide me any sample config to start with ?
I feel it so difficult to me .
I had a look @ ""ACL elements"" section in thw wiki about matching size of image but didn?t find clear thing.
So again I feel that I will create access list that match size > than 50 Byte and with mime type like jpg or bmp and then deny it.

Could you help me with startup config plz ?

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Leonardo Rodrigues
Sent: Wednesday, March 18, 2015 7:32 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 18/03/15 08:06, Amos Jeffries wrote:
> On 19/03/2015 5:57 a.m., snakeeyes wrote:
>> I need help in blocking images that has size less than 40 KB
>>
>>   
> Use the Squid provided access controls to manage access to things.
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl>
>

     You should know that you cannot evaluate the response size using only the request data. So to acchieve what you want, data from the reply must be considered as well, the response size for example.

     Images can be identified by the presence of '.jpg' or '.png' on the request URL, but images can be generated on-the-fly by scripts as well, so you wont see those extensions all the time. In that case, analyzing replies mime headers can be usefull as well, the reply mime type having 'image' is a great indication that we're receiving an image.

     Put all that together and you'll acchieve the rules you want to. 
But keep in mind that you'll probably break A LOT of sites who 'slices' 
images, background images, menus and all sort of things. I would call that a VERY bad idea, but can be acchieved with a few rules.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From jaumshock at gmail.com  Wed Mar 18 15:46:22 2015
From: jaumshock at gmail.com (Joao Paulo Monticelli Gaspar)
Date: Wed, 18 Mar 2015 12:46:22 -0300
Subject: [squid-users] Squid + AD + Kerb auth question
Message-ID: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>

Hey people

I have a doubt and couldn't find the answer anywhere yet, I'm using SQUID
integrate to a W2K8 AD server with kerb auth, and everything works fine,
the main reason of chosing this setup is for the SingleSignOn capabilities
of the configuration, but on my ACCESS.LOG I cant see the users that are
visitating the sites...

is possible to show that info with this setup, or by any other setup use
maintain the SOO?

Thx in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/b1802604/attachment.htm>

From Richard.Aspley at hammonds-uk.com  Wed Mar 18 16:05:04 2015
From: Richard.Aspley at hammonds-uk.com (Rich549)
Date: Wed, 18 Mar 2015 09:05:04 -0700 (PDT)
Subject: [squid-users] Increase number of ext_ldap_group_acl processes
In-Reply-To: <550993B7.5030801@treenet.co.nz>
References: <1426689385646-4670484.post@n4.nabble.com>
 <550993B7.5030801@treenet.co.nz>
Message-ID: <1426694704086-4670488.post@n4.nabble.com>

Thanks! I've added the children-startup=15 to my config but it seems to be
ignoring it. An excerpt of my config is:

external_acl_type internet_domain_group children-startup=15 %LOGIN
/usr/lib/squid3/ext_ldap_group_acl -R -P -b (this then goes on to provide
details of AD structure etc).

Have I missed something?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Increase-number-of-ext-ldap-group-acl-processes-tp4670484p4670488.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Mar 18 20:11:46 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Mar 2015 22:11:46 +0200
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <55095D48.4030406@treenet.co.nz>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
 <52A4253A-9AC3-42AA-AD70-B3EE2162B533@gmail.com>
 <18014212-3A89-43A8-BAE9-D7C5DB8C60A7@getbusi.com>
 <638F2526-2997-498F-8B3F-CDF6A229DC42@getbusi.com>
 <CAC49LV4gFgO6VX4=ZeCPqOOdptPj=OVTnaBMoh1_cO+pYQ6Org@mail.gmail.com>
 <6EE379FC-7378-4998-8483-26F29752657F@getbusi.com>
 <55095D48.4030406@treenet.co.nz>
Message-ID: <5509DC02.8090703@ngtech.co.il>

Hey List,

Sorry but it takes time(for me) to test squid 3.5.
I have built a testing beta of 3.5 for CentOS 7 but yet to publish it 
officially.

Since you have asked, then the main issue is that RH RPM auto building 
tools helps to find dependencies and there for most of the helpers 
need\requires EPEL repositories.
Due to this fact I have separated squid "core" and "helpers" into 
different packages.

The pinger is part of the "core" squid package and one of the tests I 
ran on the 6 branch that the pinger would get suid flaged since it 
"needs" it or rather the OS "requires" it since it handles ICMP.

For most regular setups pinger is not needed to make the service work 
and serve the clients.
If your setup requires it then you will need to enable suid or else you 
can disable pinger using "pinger_enable off" also look at:
http://www.squid-cache.org/Doc/config/pinger_enable/

"chmod u+s /path/pinger" should be the correct suid set command (and I 
think that if you ask about it then you probably don't need it).

Eliezer

On 18/03/2015 13:11, Amos Jeffries wrote:
> perl ?
>
> The helpers that require it are scripts, not compiled binaries. So they
> should run with any perl 4/5 version normally installed.
>
> Even just installing the modules with cpan should work.
>
>
>> >- How do I grant the ?pinger? the correct permissions in CentOS 6?
> Should be possible just to install Squid with:
>    make install install-pinger
>
> Amos



From eliezer at ngtech.co.il  Wed Mar 18 20:44:50 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Mar 2015 22:44:50 +0200
Subject: [squid-users] v3.5.x RPM for CentOS 6
In-Reply-To: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
References: <0D451BB9-006D-4B71-A561-A0BCEED56DEB@getbusi.com>
Message-ID: <5509E3C2.3090505@ngtech.co.il>

Hey Dan,

I will put more efforts into it and will try to publish 3.4.12 and 3.5.2 
for CentOS 6 this week.

About the pinger issue, Indeed there is one.. the suid is not set inside 
the spec files and I will add it later while I am considering patching 
the squid.conf defaults from pinger on to off.
 From my experience the sysadmins of a CentOS based systems have 
experience in life to understand that suid should not be taken lightly.

As a sysadmin: Not just anyone that will ask me to set suid for a file 
will get it.

Eliezer

References:
http://www.rpm.org/max-rpm/s1-rpm-anywhere-specifying-file-attributes.html
http://www.linuxnix.com/2011/12/suid-set-suid-linuxunix.html
http://www.cyberciti.biz/faq/unix-bsd-linux-setuid-file/
http://www.squid-cache.org/Doc/config/pinger_enable/
http://stackoverflow.com/questions/10312344/why-traceroute-sends-udp-packets-and-not-icmp-ones


On 18/03/2015 05:16, Dan Charlesworth wrote:
> Hey Eliezer
>
> Do you have any plans to maintain a Squid 3.5.x rpm for CentOS 6?
>
> I can see you?ve published one for CentOS 7. In fact I tried to use your spec file from the EL7 version to build an EL6 rpm, but ran into errors when updating from 3.4.12:
>
> 1. Installing the separate squid-helpers package had a dependency error I?m not sure how to resolve:
> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Processing Dependency: perl(DBI) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Running transaction check
> ---> Package perl-DBI.x86_64 0:1.609-4.el6 will be installed
> ---> Package squid-helpers.x86_64 7:3.5.2-1.el6 will be installed
> --> Processing Dependency: perl(Crypt::OpenSSL::X509) for package: 7:squid-helpers-3.5.2-1.el6.x86_64
> --> Finished Dependency Resolution
> Error: Package: 7:squid-helpers-3.5.2-1.el6.x86_64 (getbusi-dev)
>             Requires: perl(Crypt::OpenSSL::X509)
>   You could try using --skip-broken to work around the problem
>
>   You could try running: rpm -Va --nofiles --nodigest
>
> 2. Having disabled all the helpers which are missing because of that package everything was okay except for an error regarding the ?ICMP Pinger?:
> 2015/03/18 14:13:25| pinger: Initialising ICMP pinger ...
> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
> 2015/03/18 14:13:25| pinger: Unable to start ICMP pinger.
> 2015/03/18 14:13:25|  icmp_sock: (1) Operation not permitted
> 2015/03/18 14:13:25| pinger: Unable to start ICMPv6 pinger.
> 2015/03/18 14:13:25| FATAL: pinger: Unable to open any ICMP sockets.
>
> Do you have any advice on how to overcome these issues?
>
> Thanks!
> Dan
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From huaraz at moeller.plus.com  Wed Mar 18 20:54:31 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Wed, 18 Mar 2015 20:54:31 -0000
Subject: [squid-users] Squid + AD + Kerb auth question
In-Reply-To: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>
References: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>
Message-ID: <mecoma$vkk$1@ger.gmane.org>

How does the config file look like ?  

Markus

"Joao Paulo Monticelli Gaspar" <jaumshock at gmail.com> wrote in message news:CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg at mail.gmail.com...
Hey people 

I have a doubt and couldn't find the answer anywhere yet, I'm using SQUID integrate to a W2K8 AD server with kerb auth, and everything works fine, the main reason of chosing this setup is for the SingleSignOn capabilities of the configuration, but on my ACCESS.LOG I cant see the users that are visitating the sites...

is possible to show that info with this setup, or by any other setup use maintain the SOO?

Thx in advance.


--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/685fc200/attachment.htm>

From stan.prescott at gmail.com  Wed Mar 18 22:29:22 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 18 Mar 2015 18:29:22 -0400
Subject: [squid-users] Squid 3.5.2 will only start if cache directory is
	empty
Message-ID: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>

I posted this message to the list a few days ago but haven't received any
responses yet. I am hoping someone might be able to provide some insight on
what is going on.

I have been trying to get Squid 3.5.2 to work with the Smoothwall Express
3.1 Linux firewall distribution. Specifically, I have modified the Squid
version included with Smoothwall Express 3.1 to enable HTTPS caching. I
have had this working successfully up to Squid version 3.4.10. Now with
trying to upgrade to Squid 3.5.2 I am having problems that I didn't
encounter with prior versions of Squid.

The first issue I had, which is now resolved, was improper permissions of
the shm folder (in SWE found in /dev/shm). Changing the folder permissions
to Squid user and group allowed Squid 3.5.2 to start. However, now it will
only start with an empty cache directory. Once it starts with an empty
cache directory, it seems to function correctly as far as caching SSL
encrypted web pages. However, if Squid needs to be restarted for any
reason, it will not restart until the cache directory
(/var/spool/squid/cache) is emptied.

The error I am getting when trying to start Squid 3.5.2 without an empty
cache is

*2015/03/14 00:29:47 kid1| Current Directory is /*

*2015/03/14 00:29:47 kid1| Starting Squid Cache version 3.5.2 for
i586-pc-linux-gnu...*
*2015/03/14 00:29:47 kid1| Service Name: squid*
*2015/03/14 00:29:47 kid1| Process ID 7261*
*2015/03/14 00:29:47 kid1| Process Roles: worker*
*2015/03/14 00:29:47 kid1| With 1024 file descriptors available*
*2015/03/14 00:29:47 kid1| Initializing IP Cache...*
*2015/03/14 00:29:47 kid1| DNS Socket created at 0.0.0.0, FD 8*
*2015/03/14 00:29:47 kid1| Adding nameserver 127.0.0.1 from
/etc/resolv.conf*
*2015/03/14 00:29:47 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
processes*
*FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*

*Squid Cache (Version 3.5.2): Terminated abnormally.*
*CPU Usage: 0.027 seconds = 0.020 user + 0.007 sys*
*Maximum Resident Size: 26752 KB*
*Page faults with physical i/o: 0*
*2015/03/14 00:29:47.830 kid1| Acl.cc(380) ~ACL: freeing ACL *


What is the "squid-ssl_session_cache". Am I supposed to define that
somewhere in the
Squid configuration? Is that why I am getting that error message because an
ssl_session_cache is not defined somewhere?

This is my squid.conf file with SSL caching using ssl-bump enabled.

*visible_hostname smoothwall*

*# Uncomment the following to send debug info to /var/log/squid/cache.log*
*debug_options ALL,1 33,2 28,9*

*# ACCESS CONTROLS*
*# ----------------------------------------------------------------*
*acl localhostgreen src 192.168.100.1*
*acl localnetgreen src 192.168.100.0/24 <http://192.168.100.0/24>*

*acl SSL_ports port 445 443 441 563*
*acl Safe_ports port 80      # http*
*acl Safe_ports port 81      # smoothwall http*
*acl Safe_ports port 21      # ftp *
*acl Safe_ports port 445 443 441 563 # https, snews*
*acl Safe_ports port 70      # gopher*
*acl Safe_ports port 210         # wais  *
*acl Safe_ports port 1025-65535 # unregistered ports*
*acl Safe_ports port 280        # http-mgmt*
*acl Safe_ports port 488        # gss-http *
*acl Safe_ports port 591        # filemaker*
*acl Safe_ports port 777        # multiling http*

*acl CONNECT method CONNECT*

*# TAG: http_access*
*# ----------------------------------------------------------------*


*http_access deny !Safe_ports*
*http_access deny CONNECT !SSL_ports*

*http_access allow localnetgreen*
*http_access allow CONNECT localnetgreen*

*http_access allow localhostgreen*
*http_access allow CONNECT localhostgreen*

*# http_port and https_port*
*#----------------------------------------------------------------------------*

*# A random port for forward-proxy port needed for SSL*
*http_port 8081*

*http_port 192.168.100.1:800 <http://192.168.100.1:800/> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*https_port 192.168.100.1:808 <http://192.168.100.1:808/> intercept
ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*ssl_bump server-first all*

*ssl_bump none localhostgreen*
*sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
*sslcrtd_children 5*

*sslproxy_session_cache_size 4 MB*

*http_access deny all*

*cache_replacement_policy heap GDSF*
*memory_replacement_policy heap GDSF*

*# CACHE OPTIONS*
*#
----------------------------------------------------------------------------*
*cache_effective_user squid*
*cache_effective_group squid*

*cache_swap_high 100*
*cache_swap_low 80*

*cache_mem 8 MB*
*maximum_object_size_in_memory 512 KB*

*cache_access_log /var/log/squid/access.log*
*cache_log /var/log/squid/cache.log*
*cache_store_log none*
*error_directory /usr/share/errors/en-us*
*log_mime_hdrs off*

*cache_dir diskd /var/spool/squid/cache 1024 16 256 Q1=64 Q2=72*

*request_header_access Content-Type allow all*
*request_header_access Date allow all*
*request_header_access Host allow all*
*request_header_access If-Modified-Since allow all*
*request_header_access Pragma allow all*
*request_header_access Accept allow all*
*request_header_access Accept-Charset allow all*
*request_header_access Accept-Encoding allow all*
*request_header_access Accept-Language allow all*
*request_header_access Connection allow all*
*request_header_access All allow all*

*maximum_object_size 33 MB*

*minimum_object_size 0 KB*


*request_body_max_size 0 KB*

*# OTHER OPTIONS*
*#
----------------------------------------------------------------------------*
*forwarded_for off*

*pid_filename /var/run/squid.pid*

*shutdown_lifetime 3 seconds*
*icp_port 3130*

*half_closed_clients off*

*umask 022*

*logfile_rotate 0*

*strip_query_terms off*


Any help would be greatly appresciated.

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/d3aa8da1/attachment.htm>

From huaraz at moeller.plus.com  Wed Mar 18 23:20:55 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Wed, 18 Mar 2015 23:20:55 -0000
Subject: [squid-users] Squid + AD + Kerb auth question
In-Reply-To: <mecoma$vkk$1@ger.gmane.org>
References: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>
 <mecoma$vkk$1@ger.gmane.org>
Message-ID: <med18q$gal$1@ger.gmane.org>

Hi,

  From which network do you surf ?  From localnet ? 

  Can you send sample log entries ?

Markus

From: Joao Paulo Monticelli Gaspar 
Sent: Wednesday, March 18, 2015 9:18 PM
To: Markus Moeller 
Subject: Re: [squid-users] Squid + AD + Kerb auth question

squid.conf 

visible_hostname proxy.joznet.local

auth_param negotiate program /usr/lib64/squid/squid_kerb_auth
auth_param negotiate children 10
auth_param negotiate keep_alive on
auth_param basic credentialsttl 2 hours

acl ad_auth proxy_auth REQUIRED

acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

acl localnet src 192.168.1.0/24 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

http_access allow manager localhost
http_access deny manager

http_access deny !Safe_ports


http_access deny CONNECT !SSL_ports


http_access allow localnet

http_access allow localhost
http_access allow ad_auth
http_access deny all


http_port 3128

hierarchy_stoplist cgi-bin ?


coredump_dir /var/spool/squid


refresh_pattern ^ftp: 1440 20% 10080

refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

****************************************************************************************
krb5.conf

[logging]
default = FILE:/var/log/krb5libs.log
kdc = FILE:/var/log/krb5kdc.log
admin_server = FILE:/var/log/kadmind.log

[libdefaults]
default_realm = JOZNET.LOCAL
dns_lookup_realm = false
dns_lookup_kdc = false
ticket_lifetime = 24h
renew_lifetime = 7d
forwardable = true

; for Windows 2008 with AES

;        default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
;        default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
;        permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

; for MIT/Heimdal kdc no need to restrict encryption type

[realms]
JOZNET.LOCAL = {
  kdc = srvjoznt.joznet.local:88
  admin_server = srvjoznt.joznet.local:749
  default_domain = joznet.local 
}

[domain_realm]
.joznet.local= JOZNET.LOCAL
joznet.local= JOZNET.LOCAL

[pam]
debuf = false
ticket_lifetime = 36000
renew_lifetime = 36000
forwardable = true
krb4_convert = false


2015-03-18 17:54 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>:

  How does the config file look like ?  

  Markus

  "Joao Paulo Monticelli Gaspar" <jaumshock at gmail.com> wrote in message news:CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg at mail.gmail.com...
  Hey people 

  I have a doubt and couldn't find the answer anywhere yet, I'm using SQUID integrate to a W2K8 AD server with kerb auth, and everything works fine, the main reason of chosing this setup is for the SingleSignOn capabilities of the configuration, but on my ACCESS.LOG I cant see the users that are visitating the sites...

  is possible to show that info with this setup, or by any other setup use maintain the SOO?

  Thx in advance.

------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/d1e5b00b/attachment.htm>

From huaraz at moeller.plus.com  Wed Mar 18 23:46:42 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Wed, 18 Mar 2015 23:46:42 -0000
Subject: [squid-users] Squid + AD + Kerb auth question
In-Reply-To: <med18q$gal$1@ger.gmane.org>
References: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>
 <mecoma$vkk$1@ger.gmane.org> <med18q$gal$1@ger.gmane.org>
Message-ID: <med2p5$8as$1@ger.gmane.org>

Hi Joao

Then you hit

http_access allow localnet


and not

http_access allow ad_auth

Comment out the following line in squid.conf 

http_access allow localnet


and try again.

Markus

From: Joao Paulo Monticelli Gaspar 
Sent: Wednesday, March 18, 2015 11:38 PM
To: Markus Moeller 
Subject: Re: [squid-users] Squid + AD + Kerb auth question

yes, I'm using localnet, this is a virtual test lab enviorment, here are some log entries 

1426694349.225  59653 192.168.1.251 TCP_MISS/200 4775 CONNECT p5-ib4juqow2smme-qg5sbffb457kogr5-505177-i2-v6exp3-ds.metric.gstatic.com:443 - DIRECT/216.58.222.35 -
1426694352.258  62686 192.168.1.251 TCP_MISS/200 4774 CONNECT p5-ib4juqow2smme-qg5sbffb457kogr5-505177-i1-v6exp3-v4.metric.gstatic.com:443 - DIRECT/216.58.222.46 -
1426694613.543  58996 192.168.1.251 TCP_MISS/200 1112 CONNECT safebrowsing.google.com:443 - DIRECT/173.194.42.133 -

when I looked at the access.log manual pages I saw that if squid cant get user info, he uses the - sign on the access, and we can see it there, but why he cant get the user info?


2015-03-18 20:20 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>:

  Hi,

    From which network do you surf ?  From localnet ? 

    Can you send sample log entries ?

  Markus

  From: Joao Paulo Monticelli Gaspar 
  Sent: Wednesday, March 18, 2015 9:18 PM
  To: Markus Moeller 
  Subject: Re: [squid-users] Squid + AD + Kerb auth question

  squid.conf 

  visible_hostname proxy.joznet.local

  auth_param negotiate program /usr/lib64/squid/squid_kerb_auth
  auth_param negotiate children 10
  auth_param negotiate keep_alive on
  auth_param basic credentialsttl 2 hours

  acl ad_auth proxy_auth REQUIRED

  acl manager proto cache_object
  acl localhost src 127.0.0.1/32 ::1
  acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

  acl localnet src 192.168.1.0/24 # RFC1918 possible internal network
  acl localnet src fc00::/7       # RFC 4193 local private network range
  acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

  acl SSL_ports port 443
  acl Safe_ports port 80 # http
  acl Safe_ports port 21 # ftp
  acl Safe_ports port 443 # https
  acl Safe_ports port 70 # gopher
  acl Safe_ports port 210 # wais
  acl Safe_ports port 1025-65535 # unregistered ports
  acl Safe_ports port 280 # http-mgmt
  acl Safe_ports port 488 # gss-http
  acl Safe_ports port 591 # filemaker
  acl Safe_ports port 777 # multiling http
  acl CONNECT method CONNECT

  http_access allow manager localhost
  http_access deny manager

  http_access deny !Safe_ports


  http_access deny CONNECT !SSL_ports


  http_access allow localnet

  http_access allow localhost
  http_access allow ad_auth
  http_access deny all


  http_port 3128

  hierarchy_stoplist cgi-bin ?


  coredump_dir /var/spool/squid


  refresh_pattern ^ftp: 1440 20% 10080

  refresh_pattern ^gopher: 1440 0% 1440
  refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
  refresh_pattern . 0 20% 4320

  ****************************************************************************************
  krb5.conf

  [logging]
  default = FILE:/var/log/krb5libs.log
  kdc = FILE:/var/log/krb5kdc.log
  admin_server = FILE:/var/log/kadmind.log

  [libdefaults]
  default_realm = JOZNET.LOCAL
  dns_lookup_realm = false
  dns_lookup_kdc = false
  ticket_lifetime = 24h
  renew_lifetime = 7d
  forwardable = true

  ; for Windows 2008 with AES

  ;        default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
  ;        default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
  ;        permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

  ; for MIT/Heimdal kdc no need to restrict encryption type

  [realms]
  JOZNET.LOCAL = {
    kdc = srvjoznt.joznet.local:88
    admin_server = srvjoznt.joznet.local:749
    default_domain = joznet.local 
  }

  [domain_realm]
  .joznet.local= JOZNET.LOCAL
  joznet.local= JOZNET.LOCAL

  [pam]
  debuf = false
  ticket_lifetime = 36000
  renew_lifetime = 36000
  forwardable = true
  krb4_convert = false


  2015-03-18 17:54 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>:

    How does the config file look like ?  

    Markus

    "Joao Paulo Monticelli Gaspar" <jaumshock at gmail.com> wrote in message news:CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg at mail.gmail.com...
    Hey people 

    I have a doubt and couldn't find the answer anywhere yet, I'm using SQUID integrate to a W2K8 AD server with kerb auth, and everything works fine, the main reason of chosing this setup is for the SingleSignOn capabilities of the configuration, but on my ACCESS.LOG I cant see the users that are visitating the sites...

    is possible to show that info with this setup, or by any other setup use maintain the SOO?

    Thx in advance.

----------------------------------------------------------------------------
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users




  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150318/4dc37201/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 19 04:11:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 17:11:40 +1300
Subject: [squid-users] Squid 3.5.2 will only start if cache directory is
 empty
In-Reply-To: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>
References: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>
Message-ID: <550A4C7C.9020807@treenet.co.nz>

On 19/03/2015 11:29 a.m., Stanford Prescott wrote:
> I posted this message to the list a few days ago but haven't received any
> responses yet. I am hoping someone might be able to provide some insight on
> what is going on.
> 
> I have been trying to get Squid 3.5.2 to work with the Smoothwall Express
> 3.1 Linux firewall distribution. Specifically, I have modified the Squid
> version included with Smoothwall Express 3.1 to enable HTTPS caching. I
> have had this working successfully up to Squid version 3.4.10. Now with
> trying to upgrade to Squid 3.5.2 I am having problems that I didn't
> encounter with prior versions of Squid.
> 
> The first issue I had, which is now resolved, was improper permissions of
> the shm folder (in SWE found in /dev/shm). Changing the folder permissions
> to Squid user and group allowed Squid 3.5.2 to start. However, now it will
> only start with an empty cache directory.

Ouch. /dev/shm is a folder for system shared-memory sockets to be
created by applications. It should be owned by root user and group, with
777 permissions. Squid (or the OS kernel) should be able to create
"files" inside it, but it should not be owned by Squid.


> Once it starts with an empty
> cache directory, it seems to function correctly as far as caching SSL
> encrypted web pages. However, if Squid needs to be restarted for any
> reason, it will not restart until the cache directory
> (/var/spool/squid/cache) is emptied.

That HTTP data cache is unrelated to the SSL session cache. Its contents
should not matter.


> *2015/03/14 00:29:47 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
> processes*
> *FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*
> 

> 
> What is the "squid-ssl_session_cache". Am I supposed to define that
> somewhere in the
> Squid configuration? Is that why I am getting that error message because an
> ssl_session_cache is not defined somewhere?

The .shm is the name of a shared memory socket "file" name. You have
sslproxy_session_cache_size defined with a size so the SSL session
ticket cache is used.

Please try patching your Squid with
<http://www.squid-cache.org/Versions/v4/changesets/squid-4-13984.patch>.
It should resolve many permissions issues Squid 3.5 workers are having
on startup.


> 
> This is my squid.conf file with SSL caching using ssl-bump enabled.
> 

> 
> *# A random port for forward-proxy port needed for SSL*
> *http_port 8081*
> 
> *http_port 192.168.100.1:800 <http://192.168.100.1:800/> intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*
> 
> *https_port 192.168.100.1:808 <http://192.168.100.1:808/> intercept
> ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

Why two ports? one is usually sufficient.

> 
> *sslproxy_cert_error allow all*
> *sslproxy_flags DONT_VERIFY_PEER*

Please remove the DONT_VERIFY_PEER flag setting. It allows external
servers to corrupt your TLS certificates with garbage and hijack
connections.


> *ssl_bump server-first all*
> 
> *ssl_bump none localhostgreen*
> *sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
> *sslcrtd_children 5*
> 
> *sslproxy_session_cache_size 4 MB*



> 
> *cache_access_log /var/log/squid/access.log*
> *cache_log /var/log/squid/cache.log*

You dont need these three:
> *cache_store_log none*
> *error_directory /usr/share/errors/en-us*
> *log_mime_hdrs off*

.. all they do is set the defaults to be used.


> 
> *request_header_access Content-Type allow all*
> *request_header_access Date allow all*
> *request_header_access Host allow all*
> *request_header_access If-Modified-Since allow all*
> *request_header_access Pragma allow all*
> *request_header_access Accept allow all*
> *request_header_access Accept-Charset allow all*
> *request_header_access Accept-Encoding allow all*
> *request_header_access Accept-Language allow all*
> *request_header_access Connection allow all*
> *request_header_access All allow all*

The above settings do nothing but waste CPU time. You can remove them.

What you are instructing Squid to do is effectively "allow certain
headers X, Y, Z, oh and every other header too".

> 
> *shutdown_lifetime 3 seconds*

NOTE: very short shutdown time can corrupt the HTTP data cache as the
memory index does not have enough time to complete saving to disk.

Amos


From squid3 at treenet.co.nz  Thu Mar 19 04:40:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 17:40:35 +1300
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
Message-ID: <550A5343.4020206@treenet.co.nz>

On 19/03/2015 1:35 p.m., snakeeyes wrote:
> Thank you so much  Amos and leonardo
> Can you provide me any sample config to start with ?
> I feel it so difficult to me .
> I had a look @ ""ACL elements"" section in thw wiki about matching size of image but didn?t find clear thing.
> So again I feel that I will create access list that match size > than 50 Byte and with mime type like jpg or bmp and then deny it.
> 
> Could you help me with startup config plz ?

You mean hand over a cut-n-paste example that you can use and when
things go wrong not understand how to fix?

Sure:
 acl images rep_header Content-Type ^image/ ^x-image/
 acl small rep_header Content-Length ^[1234]?[0-9]$
 http_reply_access deny small images


BUT like Leonardo said, censoring the Internet not as easy as all that.

* Images come in *many* data formats (Content-Type values), some of
which are shared with other non-image things - like octet-stream which
literally means "unknown binary data". They can come embedded inside
other objects, JSON, CSS, archive files (like zip / gzip / xz / ar /
cab) ... even plain old HTML can have base64 blobs of image data in them
which gets decoded by a script... and so on.

For every point of censorship there is a bypass.

* The Content-Length is also not guaranteed to be existing. The object
may be of undefined length streamed in small chunks or as a blob with no
size known until the end of the transaction.


What it comes down to is that you need to know exactly what you are
looking for in the protocol, and use the appropriate ACL types to match
with. Which in turn requires knowing what ACLs you have available and
how to use them to construct *_access rules matching your needs.


When you do have to make abnormal things happen be as precise and
specific as you can. Every bit of fuzz/approximation *will* cause
trouble at some point during production traffic.


So, why are you doing this?

Amos



From dan at getbusi.com  Thu Mar 19 05:36:52 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 19 Mar 2015 16:36:52 +1100
Subject: [squid-users] Server-first SSL bump in Squid 3.5.x
Message-ID: <1BFBB56B-8D79-417D-8B76-04F19C29EEC6@getbusi.com>

Hey y?all

Finally got 3.5.2 running. I was under the impression that using server-first SSL bump would still be compatible, despite all the Peek & Splice changes, but apparently not. Hopefully someone can explain what might be going wrong here ...

Using the same SSL Bump config that we used for 3.4, we now seeing this happen:
19/Mar/2015-16:21:32     22 d4:f4:6f:71:90:e6 10.0.1.71 TCP_DENIED 200 0 CONNECT 94.31.29.230:443 - server-first - HIER_NONE/- - -

Instead of this:
19/Mar/2015-14:42:04    736 d4:f4:6f:71:90:e6 10.0.1.71 TCP_MISS 200 96913 GET https://code.jquery.com/jquery-1.11.0.min.js - server-first Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%208_2%20like%20Mac%20OS%20X)%20AppleWebKit/600.1.4%20(KHTML,%20like%20Gecko)%20Mobile/12D508 ORIGINAL_DST/94.31.29.53 application/x-javascript -

This request happens in a little splash page which is designed to test if squid?s CA cert is installed on the client and redirect them to some instructions if it?s not. This definitely isn?t happening for all intercepted HTTPS requests, just this (particularly important) one and some others.

SSL Bump config:
ssl_bump none localhost
ssl_bump server-first all
sslproxy_cert_error deny all

sslcrtd_program /usr/bin/squid_ssl_crtd -s /path/to/squid/ssl_db -M 4MB
sslcrtd_children 32 startup=5 idle=1

DNAT intercepting port config:
https_port 3130 intercept name=3130 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/path/to/squid/proxy-cert.cer key=/path/to/squid/proxy-key.key

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/dd97a11f/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar 19 06:18:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 19:18:55 +1300
Subject: [squid-users] Server-first SSL bump in Squid 3.5.x
In-Reply-To: <1BFBB56B-8D79-417D-8B76-04F19C29EEC6@getbusi.com>
References: <1BFBB56B-8D79-417D-8B76-04F19C29EEC6@getbusi.com>
Message-ID: <550A6A4F.9050409@treenet.co.nz>

On 19/03/2015 6:36 p.m., Dan Charlesworth wrote:
> Hey y?all
> 
> Finally got 3.5.2 running. I was under the impression that using server-first SSL bump would still be compatible, despite all the Peek & Splice changes, but apparently not. Hopefully someone can explain what might be going wrong here ...
> 

Sadly "being compatible" with an broken design does not mean "working".
server-first only works nicely if the client, Squid, and server are
operating with the same TLS features - which is uncommon.


> Using the same SSL Bump config that we used for 3.4, we now seeing this happen:
> 19/Mar/2015-16:21:32     22 d4:f4:6f:71:90:e6 10.0.1.71 TCP_DENIED 200 0 CONNECT 94.31.29.230:443 - server-first - HIER_NONE/- - -
> 

The CONNECT request in the clear-text HTTP layer is now subject to
access controls before any bumping takes place. Earlier Squid would let
the CONNECT through if you were bumping, even if it would have been
blocked by your access controls normally.

This is unrelated to server-first or any other ssl_bump action.

> Instead of this:
> 19/Mar/2015-14:42:04    736 d4:f4:6f:71:90:e6 10.0.1.71 TCP_MISS 200 96913 GET https://code.jquery.com/jquery-1.11.0.min.js - server-first Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%208_2%20like%20Mac%20OS%20X)%20AppleWebKit/600.1.4%20(KHTML,%20like%20Gecko)%20Mobile/12D508 ORIGINAL_DST/94.31.29.53 application/x-javascript -
> 

That is a different HTTP message from inside the encryption.


Amos



From dan at getbusi.com  Thu Mar 19 06:41:56 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 19 Mar 2015 17:41:56 +1100
Subject: [squid-users] Server-first SSL bump in Squid 3.5.x
In-Reply-To: <550A6A4F.9050409@treenet.co.nz>
References: <1BFBB56B-8D79-417D-8B76-04F19C29EEC6@getbusi.com>
 <550A6A4F.9050409@treenet.co.nz>
Message-ID: <DB21193F-F2F4-4A44-8C5D-A32F584D2048@getbusi.com>

Right, I see.

So I?ve got a special ACL to always allow that Test URL for the sake of our certcheck ? but it?s doing it by dstdomain. So if there are rules to say ?always redirect to the certificate splash page if you can?t connect to the URL?, then it will never pass it because the initial CONNECT step can never match a dstdomain and will always be DENIED.

So what I really need to do is change that test URL?s ACL to be a dst instead (and find a URL that isn?t going to resolve to different IPs over time). Okay.

While we?re at it, is there a Peek & Splice "equivalent" of the config I posted before?

Kind regards
Dan

> On 19 Mar 2015, at 5:18 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 19/03/2015 6:36 p.m., Dan Charlesworth wrote:
>> Hey y?all
>> 
>> Finally got 3.5.2 running. I was under the impression that using server-first SSL bump would still be compatible, despite all the Peek & Splice changes, but apparently not. Hopefully someone can explain what might be going wrong here ...
>> 
> 
> Sadly "being compatible" with an broken design does not mean "working".
> server-first only works nicely if the client, Squid, and server are
> operating with the same TLS features - which is uncommon.
> 
> 
>> Using the same SSL Bump config that we used for 3.4, we now seeing this happen:
>> 19/Mar/2015-16:21:32     22 d4:f4:6f:71:90:e6 10.0.1.71 TCP_DENIED 200 0 CONNECT 94.31.29.230:443 - server-first - HIER_NONE/- - -
>> 
> 
> The CONNECT request in the clear-text HTTP layer is now subject to
> access controls before any bumping takes place. Earlier Squid would let
> the CONNECT through if you were bumping, even if it would have been
> blocked by your access controls normally.
> 
> This is unrelated to server-first or any other ssl_bump action.
> 
>> Instead of this:
>> 19/Mar/2015-14:42:04    736 d4:f4:6f:71:90:e6 10.0.1.71 TCP_MISS 200 96913 GET https://code.jquery.com/jquery-1.11.0.min.js - server-first Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%208_2%20like%20Mac%20OS%20X)%20AppleWebKit/600.1.4%20(KHTML,%20like%20Gecko)%20Mobile/12D508 ORIGINAL_DST/94.31.29.53 application/x-javascript -
>> 
> 
> That is a different HTTP message from inside the encryption.
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From scoimbatore at vormetric.com  Thu Mar 19 07:11:55 2015
From: scoimbatore at vormetric.com (Saravanan Coimbatore)
Date: Thu, 19 Mar 2015 07:11:55 +0000
Subject: [squid-users] Squid not responding during file upload.
Message-ID: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>

Hello all, 

I am using Squid 3.4 to inspect content that heads out to the cloud from enterprise. I have two c-icap filter that does the content inspection. 

Observation: 
- Upload 3000 1M files to cloud passes through successfully. 
- Upload 300 40M files to cloud results in multiple failures. Some of errors: 400 Bad Request, Request Timed out.. 

Tcpdump of the 40MB file upload tests indicate the following:
- Boto client used to upload sends packet to squid proxy. 
- Proxy does not acknowledge. 
- Client sends the data again at least 6 times, Squid does not respond. 
- After 20-25 seconds of this (where Squid did not send any data to cloud), Cloud storage vendor returns a "BAD Request" response. 

Uploading 300 files seems to be a load that should be manageable by Squid. Can anyone guide me on how to optimize Squid for the above scenario? Are there any performance parameters that I can tweak so Squid handles this correctly?

Thanks, 
Saravanan
 

From emz at norma.perm.ru  Thu Mar 19 07:45:23 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Thu, 19 Mar 2015 12:45:23 +0500
Subject: [squid-users] squid SMP and SNMP
In-Reply-To: <55098592.30207@treenet.co.nz>
References: <55093C47.8090108@norma.perm.ru> <55095BD1.2090501@treenet.co.nz>
 <550982BF.1040903@norma.perm.ru> <55098592.30207@treenet.co.nz>
Message-ID: <550A7E93.4000906@norma.perm.ru>

Hi.

On 18.03.2015 19:02, Amos Jeffries wrote:
> Process kid3 (SMP coordinator) is attempting to respond.
>
> Since you configured:
>   snmp_port 340${process_number}
>
> and the coordinator is process number 3 I think it will be using port
> 3403 for that response.
>
>
Nobody is listening on these ports:

[root at taiga:local/squidquotas]# netstat -an | grep udp | grep
340              
udp46      0      0 *.3401                 *.*                   
udp46      0      0 *.3402                 *.*                   
[root at taiga:local/squidquotas]#

Eugene.


From vdoctor at neuf.fr  Thu Mar 19 08:31:10 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 19 Mar 2015 01:31:10 -0700 (PDT)
Subject: [squid-users] Open Squid Box - FREE
Message-ID: <1426753870618-4670502.post@n4.nabble.com>

*WAN Optimization and Internet Acceleration in Open Source*.
OpenSquidBox is an Open Source of an already pre-configured Squid Proxy
Cache Server under Linux that can be installed within few minute.
It?s an ISO Software Appliance that can be loaded on any hardware and
virtual appliance.
It contains an already pre-installed & configured 64 bits Linux OS and Squid
Proxy Cache software and includes a web graphical console for easy
configuration & management of your cache server.
The Installation of the ISO file on your own hardware/software appliance
takes only few minutes.
No extra manual installation or configuration is required.
Your cache server is then immediately ready to work.
Easy customizable solution for those you need to install rapidly a Cache
Server or want to learn & practice Squid Cache with a nice open source
graphical web console.

Dedicated website about  * <http://osb.unveiltech.com> OpenSquidBox* 

*Startup Users*:
You are not yet an expert in Linux nor in Squid Cache but you need something
ready to go to work/play with it.
You can not invest time to investigate how to install/setup and configure.

*Advanced Admins*:
You need to setup a new Proxy Cache server but you do not have time to
install and configure it.
You need something ready-to-use and to install on your hardware appliance.
Within few minutes you have something installed and working.
Worry-free solution.

*Professionals*:
You are looking for a software appliance solution to deploy at your
customers site. 	
You need something ready-to-use and to install on your hardware appliance.
Get an immediate solution within few minutes.
Easy configurable solution. 

*Main Features*
ISO Software Appliance solution ready to download
ISO file already containing Linux OS pre-configured
Contains most popular Squid Proxy Cache software pre-configured
Easy to Install on your own hardware appliance
64 bits OS and Proxy Cache Server
Installation in few minutes
No extra manual installation or configuration required
Works on Hardware or Virtual Appliance
Already preconfigured with default settings
Includes a web graphical console for easy configuration & management:
	Modern graphical console
	Realtime and Mbps graphs
	No need to manually configure setting files
	Rapid access to configuration with web console
Easy Customizable Solution
Ready to use solution
Good solution to learn and practice Squid Proxy Cache
Open Source solution ("Root" account is provided for free)

Version 1.03 - March 19th 2015
ISO is now available to all in Open Source including the SquidVideoBooster
plugin trial 7 days

*Installation*:
- Download the ISO
- Burn a CD or USB stick
- Boot on the CD/USB and install
- Once installed, go to the web console: http://opensquidbox-ip-address:81

Feel free for comment, suggest or improve it...
Enjoy,
Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Open-Squid-Box-FREE-tp4670502.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar 19 09:06:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 22:06:57 +1300
Subject: [squid-users] Open Squid Box - FREE
In-Reply-To: <1426753870618-4670502.post@n4.nabble.com>
References: <1426753870618-4670502.post@n4.nabble.com>
Message-ID: <550A91B1.80804@treenet.co.nz>

Unveiltech already have a listing for Squid based product since years
back. <http://www.squid-cache.org/Support/products.html>

And most of what the feature description can also be used as-is to
describe the default Squid packages from squid-cache.org provide on
installation without any configuration or tuning.

So whats new about this?
 You put it on a LiveCD and ... ?

Amos



From vdoctor at neuf.fr  Thu Mar 19 09:06:02 2015
From: vdoctor at neuf.fr (Stakres)
Date: Thu, 19 Mar 2015 02:06:02 -0700 (PDT)
Subject: [squid-users] Open Squid Box - FREE
In-Reply-To: <550A91B1.80804@treenet.co.nz>
References: <1426753870618-4670502.post@n4.nabble.com>
 <550A91B1.80804@treenet.co.nz>
Message-ID: <1426755962593-4670504.post@n4.nabble.com>

Hi Amos,

This is not a LiveCD, this is a *complete solution* including Squid, web
console, statistics, graphs, StoreID plugin, etc...
An Open solution for people who needs an all-in-one system ready and running
in 10 min maxi...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Open-Squid-Box-FREE-tp4670502p4670504.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar 19 09:26:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Mar 2015 22:26:02 +1300
Subject: [squid-users] Squid not responding during file upload.
In-Reply-To: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
References: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
Message-ID: <550A962A.6070707@treenet.co.nz>

On 19/03/2015 8:11 p.m., Saravanan Coimbatore wrote:
> Hello all, 
> 
> I am using Squid 3.4 to inspect content that heads out to the cloud from enterprise. I have two c-icap filter that does the content inspection. 
> 
> Observation: 
> - Upload 3000 1M files to cloud passes through successfully. 
> - Upload 300 40M files to cloud results in multiple failures. Some of errors: 400 Bad Request, Request Timed out.. 
> 
> Tcpdump of the 40MB file upload tests indicate the following:
> - Boto client used to upload sends packet to squid proxy. 

 Squid on receiving requests sends them to the ICAP REQMOD service,
   and waits for its response,
 then sends the ICAP REQMOD result to the origin server,
   and waits for its response,
 then sends that to the ICAP RESPMOD service,
   and waits for its response,
 then sends that to the client.

So...
 What is the ICAP service and the origin server doing?


> - Proxy does not acknowledge. 

What type of "acknowledge" are you expecting here? HTTP or TCP level?

> - Client sends the data again at least 6 times, Squid does not respond. 

At TCP or HTTP layer?

> - After 20-25 seconds of this (where Squid did not send any data to cloud), Cloud storage vendor returns a "BAD Request" response. 
> 
> Uploading 300 files seems to be a load that should be manageable by Squid. Can anyone guide me on how to optimize Squid for the above scenario? Are there any performance parameters that I can tweak so Squid handles this correctly?
> 


Amos



From nobs at nobswolf.info  Thu Mar 19 10:28:59 2015
From: nobs at nobswolf.info (nobs at nobswolf.info)
Date: Thu, 19 Mar 2015 11:28:59 +0100
Subject: [squid-users] Redirect on Debian 7
Message-ID: <20150319102859.GA13665@nobswolf.info>

Hello,

I use the standard-Squid on Debian 7 and I'd like to create a 
redirect script.

The documentation looks quite simple, but its not very logical to me.
Some say just repeating the URL is ok, others say there is an ID
that needs to get repeated in the answer. Some say you need to
send an OK with the answer.

Then I tried to use a simple script with tee to debug. It works
quite well on the console, but does nothing in Squid. Squid 
behaves strange when I use the script.

So: Is there a tutorial that fits the Debian 7 version (3.1.20-2.2+deb7u2)
for a dummy like me that explains how to create a redirect script 
including logging?

Thanks in advance

nobs


From johnzeng2013 at yahoo.com  Thu Mar 19 11:37:02 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Thu, 19 Mar 2015 19:37:02 +0800
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279: HTTP
 reply without Date:
Message-ID: <550AB4DE.7040304@yahoo.com>


Hello All

i check squid log, and i found some Warning info and bug info , Whether
it will affect normal access ?

if possible, please give me some direction for sloving the problem


2015/03/19 19:29:02 kid1| WARNING: 1 swapin MD5 mismatches
2015/03/19 19:29:02 kid1| Could not parse headers from on disk object
2015/03/19 19:29:02 kid1| BUG 3279: HTTP reply without Date:
2015/03/19 19:29:02 kid1| StoreEntry->key: 04F6FAEC243D0C8E4A3DAB9C14276F04
2015/03/19 19:29:02 kid1| StoreEntry->next: 0
2015/03/19 19:29:02 kid1| StoreEntry->mem_obj: 0xb096600
2015/03/19 19:29:02 kid1| StoreEntry->timestamp: -1
2015/03/19 19:29:02 kid1| StoreEntry->lastref: 1426764542
2015/03/19 19:29:02 kid1| StoreEntry->expires: -1
2015/03/19 19:29:02 kid1| StoreEntry->lastmod: -1
2015/03/19 19:29:02 kid1| StoreEntry->swap_file_sz: 0
2015/03/19 19:29:02 kid1| StoreEntry->refcount: 1
2015/03/19 19:29:02 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/03/19 19:29:02 kid1| StoreEntry->swap_dirn: -1
2015/03/19 19:29:02 kid1| StoreEntry->swap_filen: -1
2015/03/19 19:29:02 kid1| StoreEntry->lock_count: 3
2015/03/19 19:29:02 kid1| StoreEntry->mem_status: 0
2015/03/19 19:29:02 kid1| StoreEntry->ping_status: 2
2015/03/19 19:29:02 kid1| StoreEntry->store_status: 1
2015/03/19 19:29:02 kid1| StoreEntry->swap_status: 0
2015/03/19 19:29:02 kid1| assertion failed: store.cc:1885: "isEmpty()"



From scoimbatore at vormetric.com  Thu Mar 19 17:26:05 2015
From: scoimbatore at vormetric.com (Saravanan Coimbatore)
Date: Thu, 19 Mar 2015 17:26:05 +0000
Subject: [squid-users] Squid not responding during file upload.
In-Reply-To: <550A962A.6070707@treenet.co.nz>
References: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
 <550A962A.6070707@treenet.co.nz>
Message-ID: <DM2PR0601MB1312D7D6713302F09AE087F8AE010@DM2PR0601MB1312.namprd06.prod.outlook.com>



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, March 19, 2015 2:26 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid not responding during file upload.

On 19/03/2015 8:11 p.m., Saravanan Coimbatore wrote:
> Hello all, 
> 
> I am using Squid 3.4 to inspect content that heads out to the cloud from enterprise. I have two c-icap filter that does the content inspection. 
> 
> Observation: 
> - Upload 3000 1M files to cloud passes through successfully. 
> - Upload 300 40M files to cloud results in multiple failures. Some of errors: 400 Bad Request, Request Timed out.. 
> 
> Tcpdump of the 40MB file upload tests indicate the following:
> - Boto client used to upload sends packet to squid proxy. 

 Squid on receiving requests sends them to the ICAP REQMOD service,
   and waits for its response,
 then sends the ICAP REQMOD result to the origin server,
   and waits for its response,
 then sends that to the ICAP RESPMOD service,
   and waits for its response,
 then sends that to the client.

So...
 What is the ICAP service and the origin server doing?

<SC> ICAP Service inspects the data that passes through it, and does selective filtering based on user policies. There are two icap services that handles two different service providers. Would having two icap services cause any delay. Does Squid send data to a icap service if the icap service has returned 204 in the preview handler?  

The Origin server is S3 or Box. We did tests without Squid in between, and the success ratio is high. We are trying to isolate this on a component basis, but the tcp dump shows that the squid did not respond at the TCP level. We were wondering if this is because Squid is busy. 


> - Proxy does not acknowledge. 

What type of "acknowledge" are you expecting here? HTTP or TCP level?
<SC> TCP level. 

> - Client sends the data again at least 6 times, Squid does not respond. 

At TCP or HTTP layer?
<SC> At TCP Layer. 

> - After 20-25 seconds of this (where Squid did not send any data to cloud), Cloud storage vendor returns a "BAD Request" response. 
> 
> Uploading 300 files seems to be a load that should be manageable by Squid. Can anyone guide me on how to optimize Squid for the above scenario? Are there any performance parameters that I can tweak so Squid handles this correctly?
> 


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From hack.back at hotmail.com  Thu Mar 19 17:18:35 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 19 Mar 2015 10:18:35 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:769:
	"Comm::IsConnOpen(conn)"
In-Reply-To: <1426248859590-4670392.post@n4.nabble.com>
References: <1424025171679-4669842.post@n4.nabble.com>
 <1424209440077-4669927.post@n4.nabble.com> <54E3F058.3040200@treenet.co.nz>
 <1424253241949-4669944.post@n4.nabble.com>
 <1424377008695-4669964.post@n4.nabble.com>
 <1424449064181-4670001.post@n4.nabble.com>
 <1426128477672-4670340.post@n4.nabble.com>
 <1426248859590-4670392.post@n4.nabble.com>
Message-ID: <1426785515202-4670510.post@n4.nabble.com>

this bug still exist on high traffic ....



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-769-Comm-IsConnOpen-conn-tp4669842p4670510.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From huaraz at moeller.plus.com  Thu Mar 19 22:19:15 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Thu, 19 Mar 2015 22:19:15 -0000
Subject: [squid-users] Squid + AD + Kerb auth question
In-Reply-To: <med2p5$8as$1@ger.gmane.org>
References: <CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg@mail.gmail.com>
 <mecoma$vkk$1@ger.gmane.org> <med18q$gal$1@ger.gmane.org>
 <med2p5$8as$1@ger.gmane.org>
Message-ID: <mefi17$3h9$1@ger.gmane.org>

Hi Joao,

   OK now you use the authentication rule. 

   How did you create the keytab ?   Does the hostname match the keytab entry ?

  Can you run the helper with ?d to get more debug ? 

Markus


From: Joao Paulo Monticelli Gaspar 
Sent: Thursday, March 19, 2015 12:41 AM
To: Markus Moeller 
Subject: Re: [squid-users] Squid + AD + Kerb auth question

gettin access denied now 

watch the logs


==> /var/log/squid/squid.out <==

==> /var/log/squid/access.log <==
1426725527.219      1 192.168.1.251 TCP_DENIED/407 4509 GET http://www.eset.com.br/download/business - NONE/- text/html

==> /var/log/squid/cache.log <==
2015/03/18 21:38:47| authenticateNegotiateHandleReply: Error validating user via Negotiate. Error returned 'BH gss_acquire_cred() failed: Unspecified GSS failure.  Minor code may provide more information. '

guess my SOO isnt working right?

2015-03-18 20:46 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>:

  Hi Joao

  Then you hit

  http_access allow localnet


  and not

  http_access allow ad_auth

  Comment out the following line in squid.conf 

  http_access allow localnet


  and try again.

  Markus

  From: Joao Paulo Monticelli Gaspar 
  Sent: Wednesday, March 18, 2015 11:38 PM
  To: Markus Moeller 
  Subject: Re: [squid-users] Squid + AD + Kerb auth question

  yes, I'm using localnet, this is a virtual test lab enviorment, here are some log entries 

  1426694349.225  59653 192.168.1.251 TCP_MISS/200 4775 CONNECT p5-ib4juqow2smme-qg5sbffb457kogr5-505177-i2-v6exp3-ds.metric.gstatic.com:443 - DIRECT/216.58.222.35 -
  1426694352.258  62686 192.168.1.251 TCP_MISS/200 4774 CONNECT p5-ib4juqow2smme-qg5sbffb457kogr5-505177-i1-v6exp3-v4.metric.gstatic.com:443 - DIRECT/216.58.222.46 -
  1426694613.543  58996 192.168.1.251 TCP_MISS/200 1112 CONNECT safebrowsing.google.com:443 - DIRECT/173.194.42.133 -

  when I looked at the access.log manual pages I saw that if squid cant get user info, he uses the - sign on the access, and we can see it there, but why he cant get the user info?


  2015-03-18 20:20 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>: 


    Hi,

      From which network do you surf ?  From localnet ? 

      Can you send sample log entries ?

    Markus

    From: Joao Paulo Monticelli Gaspar 
    Sent: Wednesday, March 18, 2015 9:18 PM
    To: Markus Moeller 
    Subject: Re: [squid-users] Squid + AD + Kerb auth question

    squid.conf 

    visible_hostname proxy.joznet.local

    auth_param negotiate program /usr/lib64/squid/squid_kerb_auth
    auth_param negotiate children 10
    auth_param negotiate keep_alive on
    auth_param basic credentialsttl 2 hours

    acl ad_auth proxy_auth REQUIRED

    acl manager proto cache_object
    acl localhost src 127.0.0.1/32 ::1
    acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

    acl localnet src 192.168.1.0/24 # RFC1918 possible internal network
    acl localnet src fc00::/7       # RFC 4193 local private network range
    acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

    acl SSL_ports port 443
    acl Safe_ports port 80 # http
    acl Safe_ports port 21 # ftp
    acl Safe_ports port 443 # https
    acl Safe_ports port 70 # gopher
    acl Safe_ports port 210 # wais
    acl Safe_ports port 1025-65535 # unregistered ports
    acl Safe_ports port 280 # http-mgmt
    acl Safe_ports port 488 # gss-http
    acl Safe_ports port 591 # filemaker
    acl Safe_ports port 777 # multiling http
    acl CONNECT method CONNECT

    http_access allow manager localhost
    http_access deny manager

    http_access deny !Safe_ports


    http_access deny CONNECT !SSL_ports


    http_access allow localnet

    http_access allow localhost
    http_access allow ad_auth
    http_access deny all


    http_port 3128

    hierarchy_stoplist cgi-bin ?


    coredump_dir /var/spool/squid


    refresh_pattern ^ftp: 1440 20% 10080

    refresh_pattern ^gopher: 1440 0% 1440
    refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
    refresh_pattern . 0 20% 4320

    ****************************************************************************************
    krb5.conf

    [logging]
    default = FILE:/var/log/krb5libs.log
    kdc = FILE:/var/log/krb5kdc.log
    admin_server = FILE:/var/log/kadmind.log

    [libdefaults]
    default_realm = JOZNET.LOCAL
    dns_lookup_realm = false
    dns_lookup_kdc = false
    ticket_lifetime = 24h
    renew_lifetime = 7d
    forwardable = true

    ; for Windows 2008 with AES

    ;        default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
    ;        default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
    ;        permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

    ; for MIT/Heimdal kdc no need to restrict encryption type

    [realms]
    JOZNET.LOCAL = {
      kdc = srvjoznt.joznet.local:88
      admin_server = srvjoznt.joznet.local:749
      default_domain = joznet.local 
    }

    [domain_realm]
    .joznet.local= JOZNET.LOCAL
    joznet.local= JOZNET.LOCAL

    [pam]
    debuf = false
    ticket_lifetime = 36000
    renew_lifetime = 36000
    forwardable = true
    krb4_convert = false


    2015-03-18 17:54 GMT-03:00 Markus Moeller <huaraz at moeller.plus.com>:

      How does the config file look like ?  

      Markus

      "Joao Paulo Monticelli Gaspar" <jaumshock at gmail.com> wrote in message news:CAFjXhx=idbdXEQxbZy56tr5m3FZTasu2tqGwLcLYdi_S-s3eQg at mail.gmail.com...
      Hey people 

      I have a doubt and couldn't find the answer anywhere yet, I'm using SQUID integrate to a W2K8 AD server with kerb auth, and everything works fine, the main reason of chosing this setup is for the SingleSignOn capabilities of the configuration, but on my ACCESS.LOG I cant see the users that are visitating the sites...

      is possible to show that info with this setup, or by any other setup use maintain the SOO?

      Thx in advance.

--------------------------------------------------------------------------
      _______________________________________________
      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users


      _______________________________________________
      squid-users mailing list
      squid-users at lists.squid-cache.org
      http://lists.squid-cache.org/listinfo/squid-users




    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users




  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/9440ac21/attachment.htm>

From dan at getbusi.com  Thu Mar 19 22:32:02 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 20 Mar 2015 09:32:02 +1100
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <550AB4DE.7040304@yahoo.com>
References: <550AB4DE.7040304@yahoo.com>
Message-ID: <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>

Hi John

This bug has been affecting me on an off for a while as well. I believe it
only affects aufs and, unfortunately, has been around for years.

See: http://bugs.squid-cache.org/show_bug.cgi?id=3279
And see: http://bugs.squid-cache.org/show_bug.cgi?id=3483

On 19 March 2015 at 22:37, johnzeng <johnzeng2013 at yahoo.com> wrote:

>
> Hello All
>
> i check squid log, and i found some Warning info and bug info , Whether
> it will affect normal access ?
>
> if possible, please give me some direction for sloving the problem
>
>
> 2015/03/19 19:29:02 kid1| WARNING: 1 swapin MD5 mismatches
> 2015/03/19 19:29:02 kid1| Could not parse headers from on disk object
> 2015/03/19 19:29:02 kid1| BUG 3279: HTTP reply without Date:
> 2015/03/19 19:29:02 kid1| StoreEntry->key: 04F6FAEC243D0C8E4A3DAB9C14276F04
> 2015/03/19 19:29:02 kid1| StoreEntry->next: 0
> 2015/03/19 19:29:02 kid1| StoreEntry->mem_obj: 0xb096600
> 2015/03/19 19:29:02 kid1| StoreEntry->timestamp: -1
> 2015/03/19 19:29:02 kid1| StoreEntry->lastref: 1426764542
> 2015/03/19 19:29:02 kid1| StoreEntry->expires: -1
> 2015/03/19 19:29:02 kid1| StoreEntry->lastmod: -1
> 2015/03/19 19:29:02 kid1| StoreEntry->swap_file_sz: 0
> 2015/03/19 19:29:02 kid1| StoreEntry->refcount: 1
> 2015/03/19 19:29:02 kid1| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
> 2015/03/19 19:29:02 kid1| StoreEntry->swap_dirn: -1
> 2015/03/19 19:29:02 kid1| StoreEntry->swap_filen: -1
> 2015/03/19 19:29:02 kid1| StoreEntry->lock_count: 3
> 2015/03/19 19:29:02 kid1| StoreEntry->mem_status: 0
> 2015/03/19 19:29:02 kid1| StoreEntry->ping_status: 2
> 2015/03/19 19:29:02 kid1| StoreEntry->store_status: 1
> 2015/03/19 19:29:02 kid1| StoreEntry->swap_status: 0
> 2015/03/19 19:29:02 kid1| assertion failed: store.cc:1885: "isEmpty()"
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/da40e6b6/attachment.htm>

From dan at getbusi.com  Fri Mar 20 00:07:02 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 20 Mar 2015 11:07:02 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <550683CE.5090802@treenet.co.nz>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
 <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>
 <CAN8nrKCDrPibjwsyKyvfg1Qa4-ZSZdqeDgj-MNKJ8Ufz2qzV1Q@mail.gmail.com>
 <55064BF8.4010004@treenet.co.nz>
 <CAN8nrKDKAy+xq9MwCWO-aLiBmVaVF3jjJKUcvxjoK6w1xFcCOA@mail.gmail.com>
 <55065F5C.7090707@treenet.co.nz>
 <E26A14FE-041C-44E5-94A3-B63843EE1F1F@getbusi.com>
 <550683CE.5090802@treenet.co.nz>
Message-ID: <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>

Well I got 3.5.2 into production for a few hours and Bad Things happened:

1) A hefty performance hit
Load average was maybe a tad higher but CPU. memory and I/O were about the same. However the system seemed to top out at around 40 requests per second (on a client that usually hits 100?150 rps) and squid became very slow to respond to squidclient requests:
[root at proxy-LS5 ~]# time squidclient -p 8080 mgr:utilization | grep client_http.requests
client_http.requests = 40.965955/sec
client_http.requests = 41.168528/sec
client_http.requests = 42.111847/sec
client_http.requests = 166646

real	0m7.163s
user	0m0.002s
sys	0m0.006s

2) Lots of Segment Violations
These obviously suck. Backtrace attached.

Just cannot win. Is it possible these two issues are due to the patch for #4206?


> On 16 Mar 2015, at 6:18 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 16/03/2015 7:16 p.m., Dan Charlesworth wrote:
>> Hey again Amos -
>> 
>> Unfortunately the patch for #4206 won?t apply to squid-3.4.12. I was going to try creating a new one but couldn?t find an equivalent line in client_side.cc for that version.
>> 
>> I guess the #4206 issue doesn?t apply to v3.4.x after all?
> 
> Correct. Oh well.
> 
> 
>> 
>> [Not a C programmer]
>> 
>> Thanks for your time today.
>> 
>> P.S. I'd love to upgrade to v3.5 but I'm waiting for somebody smarter than me to take the lead on a CentOS 6 RPM SPEC file.
> 
> Eliezer to the rescue ;-)
> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5
> 
> 
> Amos
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/23f8693a/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: 352-segment-violation.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/23f8693a/attachment.txt>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/23f8693a/attachment-0001.htm>

From johnzeng2013 at yahoo.com  Fri Mar 20 00:50:24 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 20 Mar 2015 08:50:24 +0800
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
Message-ID: <550B6ED0.9030107@yahoo.com>



Hello Dan :

                  Thanks for your advisement .
> Hi John
>
> This bug has been affecting me on an off for a while as well. I 
> believe it only affects aufs and, unfortunately, has been around for 
> years.
>
> See: http://bugs.squid-cache.org/show_bug.cgi?id=3279
> And see: http://bugs.squid-cache.org/show_bug.cgi?id=3483
>
> On 19 March 2015 at 22:37, johnzeng <johnzeng2013 at yahoo.com 
> <mailto:johnzeng2013 at yahoo.com>> wrote:
>
>
>     Hello All
>
>     i check squid log, and i found some Warning info and bug info ,
>     Whether
>     it will affect normal access ?
>
>     if possible, please give me some direction for sloving the problem
>
>
>     2015/03/19 19:29:02 kid1| WARNING: 1 swapin MD5 mismatches
>     2015/03/19 19:29:02 kid1| Could not parse headers from on disk object
>     2015/03/19 19:29:02 kid1| BUG 3279: HTTP reply without Date:
>     2015/03/19 19:29:02 kid1| StoreEntry->key:
>     04F6FAEC243D0C8E4A3DAB9C14276F04
>     2015/03/19 19:29:02 kid1| StoreEntry->next: 0
>     2015/03/19 19:29:02 kid1| StoreEntry->mem_obj: 0xb096600
>     2015/03/19 19:29:02 kid1| StoreEntry->timestamp: -1
>     2015/03/19 19:29:02 kid1| StoreEntry->lastref: 1426764542
>     2015/03/19 19:29:02 kid1| StoreEntry->expires: -1
>     2015/03/19 19:29:02 kid1| StoreEntry->lastmod: -1
>     2015/03/19 19:29:02 kid1| StoreEntry->swap_file_sz: 0
>     2015/03/19 19:29:02 kid1| StoreEntry->refcount: 1
>     2015/03/19 19:29:02 kid1| StoreEntry->flags:
>     PRIVATE,FWD_HDR_WAIT,VALIDATED
>     2015/03/19 19:29:02 kid1| StoreEntry->swap_dirn: -1
>     2015/03/19 19:29:02 kid1| StoreEntry->swap_filen: -1
>     2015/03/19 19:29:02 kid1| StoreEntry->lock_count: 3
>     2015/03/19 19:29:02 kid1| StoreEntry->mem_status: 0
>     2015/03/19 19:29:02 kid1| StoreEntry->ping_status: 2
>     2015/03/19 19:29:02 kid1| StoreEntry->store_status: 1
>     2015/03/19 19:29:02 kid1| StoreEntry->swap_status: 0
>     2015/03/19 19:29:02 kid1| assertion failed: store.cc:1885: "isEmpty()"
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>



From alberto2perez at gmail.com  Fri Mar 20 01:01:34 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Thu, 19 Mar 2015 21:01:34 -0400
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
Message-ID: <CAMZauGo+K2XGQdRyhep7rOm0wyEJPTJKwuV=FUBvq0nP1D8DEA@mail.gmail.com>

I read once in this list a response to this same question, each time I
see this in my cache logs I stop squid, remove swap.state file and run
squid3 -z, after that start squid again and the issue its gone.

Regards

On 3/19/15, Dan Charlesworth <dan at getbusi.com> wrote:
> Hi John
>
> This bug has been affecting me on an off for a while as well. I believe it
> only affects aufs and, unfortunately, has been around for years.
>
> See: http://bugs.squid-cache.org/show_bug.cgi?id=3279
> And see: http://bugs.squid-cache.org/show_bug.cgi?id=3483
>
> On 19 March 2015 at 22:37, johnzeng <johnzeng2013 at yahoo.com> wrote:
>
>>
>> Hello All
>>
>> i check squid log, and i found some Warning info and bug info , Whether
>> it will affect normal access ?
>>
>> if possible, please give me some direction for sloving the problem
>>
>>
>> 2015/03/19 19:29:02 kid1| WARNING: 1 swapin MD5 mismatches
>> 2015/03/19 19:29:02 kid1| Could not parse headers from on disk object
>> 2015/03/19 19:29:02 kid1| BUG 3279: HTTP reply without Date:
>> 2015/03/19 19:29:02 kid1| StoreEntry->key:
>> 04F6FAEC243D0C8E4A3DAB9C14276F04
>> 2015/03/19 19:29:02 kid1| StoreEntry->next: 0
>> 2015/03/19 19:29:02 kid1| StoreEntry->mem_obj: 0xb096600
>> 2015/03/19 19:29:02 kid1| StoreEntry->timestamp: -1
>> 2015/03/19 19:29:02 kid1| StoreEntry->lastref: 1426764542
>> 2015/03/19 19:29:02 kid1| StoreEntry->expires: -1
>> 2015/03/19 19:29:02 kid1| StoreEntry->lastmod: -1
>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_file_sz: 0
>> 2015/03/19 19:29:02 kid1| StoreEntry->refcount: 1
>> 2015/03/19 19:29:02 kid1| StoreEntry->flags:
>> PRIVATE,FWD_HDR_WAIT,VALIDATED
>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_dirn: -1
>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_filen: -1
>> 2015/03/19 19:29:02 kid1| StoreEntry->lock_count: 3
>> 2015/03/19 19:29:02 kid1| StoreEntry->mem_status: 0
>> 2015/03/19 19:29:02 kid1| StoreEntry->ping_status: 2
>> 2015/03/19 19:29:02 kid1| StoreEntry->store_status: 1
>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_status: 0
>> 2015/03/19 19:29:02 kid1| assertion failed: store.cc:1885: "isEmpty()"
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>


From sam at idsdoc.com  Fri Mar 20 01:01:46 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Thu, 19 Mar 2015 19:01:46 -0600
Subject: [squid-users] Squid will not authenticate NTLM/Kerberos when behind
	a haproxy load balancer
Message-ID: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>

Hello All,

I have 2 squid servers that authenticate correctly when you point your
browser to either of them. I'm using a negotiate_wrapper. I set it up
following this (
http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory
)

I would like to set both servers behind a haproxy load balancer, however
when you try to utilize the haproxy load balancer, it will not authenticate
anymore. It just gives an error asking to authenticate.

Any ideas?

Thanks in advance.



##HAPROXY.CFG##

global
log /dev/log local0
log /dev/log local1 notice
chroot /var/lib/haproxy
user haproxy
group haproxy
daemon

defaults
log global
mode http
option httplog
option dontlognull
        contimeout 5000
        clitimeout 50000
        srvtimeout 50000

# reverse proxy-squid
listen  proxy 10.10.0.254:3128
mode http
        cookie  SERVERID insert indirect nocache
        balance roundrobin
        option httpclose
        option forwardfor header X-Client
        server  squid1 10.10.0.253:3128 check inter 2000 rise 2 fall 5
        server  squid2 10.10.0.252:3128 check inter 2000 rise 2 fall 5




##SQUID.CONF##


#Kerberos and NTLM authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=****.LOCAL --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d
-s GSS_C_NO_NAME
auth_param negotiate children 30
auth_param negotiate keep_alive off

# LDAP authentication
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b
"DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local" -w
"****" -f sAMAccountName=%s -h 10.0.0.200,10.0.0.199,10.0.0.194,10.0.0.193
auth_param basic children 150
auth_param basic realm Please enter your Domain credentials to continue
auth_param basic credentialsttl 1 hour

# AD group membership commands
external_acl_type ldap_group ttl=60 children-startup=10 children-max=50
children-idle=2 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
"DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local" -w
"****" -f "(&(objectclass=person)
(sAMAccountname=%v)(memberof=CN=%a,OU=PROXY,ou=ALL
 Groups,DC=****,DC=local))" -h
dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local

acl auth proxy_auth REQUIRED

acl REQGROUPS external ldap_group PROXY-HIGHLY-RESTRICTIVE
PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE PROXY-UNRESTRICTED
PROXY-DEV PROXY-SALES

http_access deny !auth all
http_access deny !REQGROUPS all





-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/6cfe6b22/attachment.htm>

From dan at getbusi.com  Fri Mar 20 01:03:36 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Thu, 19 Mar 2015 18:03:36 -0700 (PDT)
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAMZauGo+K2XGQdRyhep7rOm0wyEJPTJKwuV=FUBvq0nP1D8DEA@mail.gmail.com>
References: <CAMZauGo+K2XGQdRyhep7rOm0wyEJPTJKwuV=FUBvq0nP1D8DEA@mail.gmail.com>
Message-ID: <1426813416140.a6216ea7@Nodemailer>

Alberto -




I created a script to do that overnight, every night, and it did not stop the error occurring during the following day.

On Fri, Mar 20, 2015 at 12:01 PM, Alberto Perez <alberto2perez at gmail.com>
wrote:

> I read once in this list a response to this same question, each time I
> see this in my cache logs I stop squid, remove swap.state file and run
> squid3 -z, after that start squid again and the issue its gone.
> Regards
> On 3/19/15, Dan Charlesworth <dan at getbusi.com> wrote:
>> Hi John
>>
>> This bug has been affecting me on an off for a while as well. I believe it
>> only affects aufs and, unfortunately, has been around for years.
>>
>> See: http://bugs.squid-cache.org/show_bug.cgi?id=3279
>> And see: http://bugs.squid-cache.org/show_bug.cgi?id=3483
>>
>> On 19 March 2015 at 22:37, johnzeng <johnzeng2013 at yahoo.com> wrote:
>>
>>>
>>> Hello All
>>>
>>> i check squid log, and i found some Warning info and bug info , Whether
>>> it will affect normal access ?
>>>
>>> if possible, please give me some direction for sloving the problem
>>>
>>>
>>> 2015/03/19 19:29:02 kid1| WARNING: 1 swapin MD5 mismatches
>>> 2015/03/19 19:29:02 kid1| Could not parse headers from on disk object
>>> 2015/03/19 19:29:02 kid1| BUG 3279: HTTP reply without Date:
>>> 2015/03/19 19:29:02 kid1| StoreEntry->key:
>>> 04F6FAEC243D0C8E4A3DAB9C14276F04
>>> 2015/03/19 19:29:02 kid1| StoreEntry->next: 0
>>> 2015/03/19 19:29:02 kid1| StoreEntry->mem_obj: 0xb096600
>>> 2015/03/19 19:29:02 kid1| StoreEntry->timestamp: -1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->lastref: 1426764542
>>> 2015/03/19 19:29:02 kid1| StoreEntry->expires: -1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->lastmod: -1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_file_sz: 0
>>> 2015/03/19 19:29:02 kid1| StoreEntry->refcount: 1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->flags:
>>> PRIVATE,FWD_HDR_WAIT,VALIDATED
>>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_dirn: -1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_filen: -1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->lock_count: 3
>>> 2015/03/19 19:29:02 kid1| StoreEntry->mem_status: 0
>>> 2015/03/19 19:29:02 kid1| StoreEntry->ping_status: 2
>>> 2015/03/19 19:29:02 kid1| StoreEntry->store_status: 1
>>> 2015/03/19 19:29:02 kid1| StoreEntry->swap_status: 0
>>> 2015/03/19 19:29:02 kid1| assertion failed: store.cc:1885: "isEmpty()"
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/fa75fc31/attachment.htm>

From bpk678 at gmail.com  Fri Mar 20 01:27:37 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 19 Mar 2015 21:27:37 -0400
Subject: [squid-users] Squid will not authenticate NTLM/Kerberos when
 behind a haproxy load balancer
In-Reply-To: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
References: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
Message-ID: <1426814857.2281.23.camel@desktop.bpk2.com>

On Thu, 2015-03-19 at 19:01 -0600, Samuel Anderson wrote:
> Hello All,
> 
> 
> I have 2 squid servers that authenticate correctly when you point your
> browser to either of them. I'm using a negotiate_wrapper. I set it up
> following this
> (http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory) 
> 
> 
> I would like to set both servers behind a haproxy load balancer,
> however when you try to utilize the haproxy load balancer, it will not
> authenticate anymore. It just gives an error asking to authenticate.
> 
> 
> Any ideas?
> 
> 
> Thanks in advance.
> 
> 
> 
> 
> 
> 
> ##HAPROXY.CFG##
> 
> 
> global
> log /dev/log local0
> log /dev/log local1 notice
> chroot /var/lib/haproxy
> user haproxy
> group haproxy
> daemon
> 
> 
> defaults
> log global
> mode http
> option httplog
> option dontlognull
>         contimeout 5000
>         clitimeout 50000
>         srvtimeout 50000
> 
> 
> # reverse proxy-squid
> listen  proxy 10.10.0.254:3128
> mode http
>         cookie  SERVERID insert indirect nocache
>         balance roundrobin
>         option httpclose
>         option forwardfor header X-Client
>         server  squid1 10.10.0.253:3128 check inter 2000 rise 2 fall 5
>         server  squid2 10.10.0.252:3128 check inter 2000 rise 2 fall 5
> 
> 
> 
> 
> 
> 
> 
> 
> ##SQUID.CONF##
> 
> 
> 
> 
> #Kerberos and NTLM authentication
> auth_param negotiate program /usr/local/bin/negotiate_wrapper
> --ntlm /usr/bin/ntlm_auth --diagnostics
> --helper-protocol=squid-2.5-ntlmssp --domain=****.LOCAL
> --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
> auth_param negotiate children 30
> auth_param negotiate keep_alive off
> 
> 
> # LDAP authentication
> auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b
> "DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local"
> -w "****" -f sAMAccountName=%s -h
> 10.0.0.200,10.0.0.199,10.0.0.194,10.0.0.193
> auth_param basic children 150
> auth_param basic realm Please enter your Domain credentials to
> continue
> auth_param basic credentialsttl 1 hour
> 
> 
> # AD group membership commands
> external_acl_type ldap_group ttl=60 children-startup=10
> children-max=50 children-idle=2 %
> LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> "DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local"
> -w "****" -f "(&(objectclass=person) (sAMAccountname=%v)(memberof=CN=%
> a,OU=PROXY,ou=ALL  Groups,DC=****,DC=local))" -h
> dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local
> 
> 
> acl auth proxy_auth REQUIRED
> 
> 
> 
> acl REQGROUPS external ldap_group PROXY-HIGHLY-RESTRICTIVE
> PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE PROXY-UNRESTRICTED
> PROXY-DEV PROXY-SALES
> 
> 
> http_access deny !auth all
> http_access deny !REQGROUPS all
> 
> 
> 
> 
> 
> 
> 
> 
> 
> -- 
> Samuel Anderson  |  Information Technology Administrator  |
>  International Document Services
> 
> 
> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
> 
> 
> 
> CONFIDENTIALITY NOTICE:
> This e-mail and any attachments are confidential. If you are not an
> intended recipient, please contact the sender to report the error and
> delete all copies of this message from your system.  Any unauthorized
> review, use, disclosure or distribution is prohibited.

how did you create and distribute the keytab for the proxies?  you must
create one keytab and put the same exact one on each of the proxies.
the KVNO numbers must match on every proxy.  run "klist
-Kket /path/to/the.keytab" on the proxies to check.

kerberos is heavily dependent on DNS.  the keytab should contain
PRIMARY/instance.domain.tld at REALM where PRIMARY is HTTP,
instance.domain.tld is the FQDN of the 10.10.0.254 IP, not either or
both of the individual proxies, and REALM should be the Kerberos REALM.

did you export the environment variable for the keytab?  on fedora, i
put the following in /etc/sysconfig/squid:

KRB5_KTNAME=/etc/squid/squid.keytab
export KRB5_KTNAME

do you get a HTTP ticket from the directory?  from a command prompt,
what does "klist tickets" show?  you can also install the XP resource
kit and run kerbtray.exe to get that info.  win7 and newer may have it
built in.



From sam at idsdoc.com  Fri Mar 20 01:32:16 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Thu, 19 Mar 2015 19:32:16 -0600
Subject: [squid-users] Squid will not authenticate NTLM/Kerberos when
 behind a haproxy load balancer
In-Reply-To: <1426814857.2281.23.camel@desktop.bpk2.com>
References: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
 <1426814857.2281.23.camel@desktop.bpk2.com>
Message-ID: <CAP6yRXjaSgUcnXQ7PLrGv971ZzWFeQe1kAUT_wuGPUwi2-Ot0g@mail.gmail.com>

Hey, I actually just figured it out. literally about 2 minutes ago.

I changed the mode from (http) to (tcp) in the HAPROXY.CFG

It looks like its able to authenticate again. Thanks for the response.

On Thu, Mar 19, 2015 at 7:27 PM, Brendan Kearney <bpk678 at gmail.com> wrote:

> On Thu, 2015-03-19 at 19:01 -0600, Samuel Anderson wrote:
> > Hello All,
> >
> >
> > I have 2 squid servers that authenticate correctly when you point your
> > browser to either of them. I'm using a negotiate_wrapper. I set it up
> > following this
> > (
> http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory
> )
> >
> >
> > I would like to set both servers behind a haproxy load balancer,
> > however when you try to utilize the haproxy load balancer, it will not
> > authenticate anymore. It just gives an error asking to authenticate.
> >
> >
> > Any ideas?
> >
> >
> > Thanks in advance.
> >
> >
> >
> >
> >
> >
> > ##HAPROXY.CFG##
> >
> >
> > global
> > log /dev/log local0
> > log /dev/log local1 notice
> > chroot /var/lib/haproxy
> > user haproxy
> > group haproxy
> > daemon
> >
> >
> > defaults
> > log global
> > mode http
> > option httplog
> > option dontlognull
> >         contimeout 5000
> >         clitimeout 50000
> >         srvtimeout 50000
> >
> >
> > # reverse proxy-squid
> > listen  proxy 10.10.0.254:3128
> > mode http
> >         cookie  SERVERID insert indirect nocache
> >         balance roundrobin
> >         option httpclose
> >         option forwardfor header X-Client
> >         server  squid1 10.10.0.253:3128 check inter 2000 rise 2 fall 5
> >         server  squid2 10.10.0.252:3128 check inter 2000 rise 2 fall 5
> >
> >
> >
> >
> >
> >
> >
> >
> > ##SQUID.CONF##
> >
> >
> >
> >
> > #Kerberos and NTLM authentication
> > auth_param negotiate program /usr/local/bin/negotiate_wrapper
> > --ntlm /usr/bin/ntlm_auth --diagnostics
> > --helper-protocol=squid-2.5-ntlmssp --domain=****.LOCAL
> > --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s GSS_C_NO_NAME
> > auth_param negotiate children 30
> > auth_param negotiate keep_alive off
> >
> >
> > # LDAP authentication
> > auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b
> > "DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local"
> > -w "****" -f sAMAccountName=%s -h
> > 10.0.0.200,10.0.0.199,10.0.0.194,10.0.0.193
> > auth_param basic children 150
> > auth_param basic realm Please enter your Domain credentials to
> > continue
> > auth_param basic credentialsttl 1 hour
> >
> >
> > # AD group membership commands
> > external_acl_type ldap_group ttl=60 children-startup=10
> > children-max=50 children-idle=2 %
> > LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
> > "DC=****,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=****,DC=local"
> > -w "****" -f "(&(objectclass=person) (sAMAccountname=%v)(memberof=CN=%
> > a,OU=PROXY,ou=ALL  Groups,DC=****,DC=local))" -h
> > dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local
> >
> >
> > acl auth proxy_auth REQUIRED
> >
> >
> >
> > acl REQGROUPS external ldap_group PROXY-HIGHLY-RESTRICTIVE
> > PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE PROXY-UNRESTRICTED
> > PROXY-DEV PROXY-SALES
> >
> >
> > http_access deny !auth all
> > http_access deny !REQGROUPS all
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > --
> > Samuel Anderson  |  Information Technology Administrator  |
> >  International Document Services
> >
> >
> > IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
> >
> >
> >
> > CONFIDENTIALITY NOTICE:
> > This e-mail and any attachments are confidential. If you are not an
> > intended recipient, please contact the sender to report the error and
> > delete all copies of this message from your system.  Any unauthorized
> > review, use, disclosure or distribution is prohibited.
>
> how did you create and distribute the keytab for the proxies?  you must
> create one keytab and put the same exact one on each of the proxies.
> the KVNO numbers must match on every proxy.  run "klist
> -Kket /path/to/the.keytab" on the proxies to check.
>
> kerberos is heavily dependent on DNS.  the keytab should contain
> PRIMARY/instance.domain.tld at REALM where PRIMARY is HTTP,
> instance.domain.tld is the FQDN of the 10.10.0.254 IP, not either or
> both of the individual proxies, and REALM should be the Kerberos REALM.
>
> did you export the environment variable for the keytab?  on fedora, i
> put the following in /etc/sysconfig/squid:
>
> KRB5_KTNAME=/etc/squid/squid.keytab
> export KRB5_KTNAME
>
> do you get a HTTP ticket from the directory?  from a command prompt,
> what does "klist tickets" show?  you can also install the XP resource
> kit and run kerbtray.exe to get that info.  win7 and newer may have it
> built in.
>
>


-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/361ede3b/attachment.htm>

From bpk678 at gmail.com  Fri Mar 20 01:43:18 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 19 Mar 2015 21:43:18 -0400
Subject: [squid-users] Squid will not authenticate NTLM/Kerberos when
 behind a haproxy load balancer
In-Reply-To: <CAP6yRXjaSgUcnXQ7PLrGv971ZzWFeQe1kAUT_wuGPUwi2-Ot0g@mail.gmail.com>
References: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
 <1426814857.2281.23.camel@desktop.bpk2.com>
 <CAP6yRXjaSgUcnXQ7PLrGv971ZzWFeQe1kAUT_wuGPUwi2-Ot0g@mail.gmail.com>
Message-ID: <1426815798.2281.29.camel@desktop.bpk2.com>

On Thu, 2015-03-19 at 19:32 -0600, Samuel Anderson wrote:
> Hey, I actually just figured it out. literally about 2 minutes ago.
> 
> 
> I changed the mode from (http) to (tcp) in the HAPROXY.CFG
> 
> 
> It looks like its able to authenticate again. Thanks for the
> response.  
> 
> On Thu, Mar 19, 2015 at 7:27 PM, Brendan Kearney <bpk678 at gmail.com>
> wrote:
>         On Thu, 2015-03-19 at 19:01 -0600, Samuel Anderson wrote:
>         > Hello All,
>         >
>         >
>         > I have 2 squid servers that authenticate correctly when you
>         point your
>         > browser to either of them. I'm using a negotiate_wrapper. I
>         set it up
>         > following this
>         >
>         (http://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory)
>         >
>         >
>         > I would like to set both servers behind a haproxy load
>         balancer,
>         > however when you try to utilize the haproxy load balancer,
>         it will not
>         > authenticate anymore. It just gives an error asking to
>         authenticate.
>         >
>         >
>         > Any ideas?
>         >
>         >
>         > Thanks in advance.
>         >
>         >
>         >
>         >
>         >
>         >
>         > ##HAPROXY.CFG##
>         >
>         >
>         > global
>         > log /dev/log local0
>         > log /dev/log local1 notice
>         > chroot /var/lib/haproxy
>         > user haproxy
>         > group haproxy
>         > daemon
>         >
>         >
>         > defaults
>         > log global
>         > mode http
>         > option httplog
>         > option dontlognull
>         >         contimeout 5000
>         >         clitimeout 50000
>         >         srvtimeout 50000
>         >
>         >
>         > # reverse proxy-squid
>         > listen  proxy 10.10.0.254:3128
>         > mode http
>         >         cookie  SERVERID insert indirect nocache
>         >         balance roundrobin
>         >         option httpclose
>         >         option forwardfor header X-Client
>         >         server  squid1 10.10.0.253:3128 check inter 2000
>         rise 2 fall 5
>         >         server  squid2 10.10.0.252:3128 check inter 2000
>         rise 2 fall 5
>         >
>         >
>         >
>         >
>         >
>         >
>         >
>         >
>         > ##SQUID.CONF##
>         >
>         >
>         >
>         >
>         > #Kerberos and NTLM authentication
>         > auth_param negotiate
>         program /usr/local/bin/negotiate_wrapper
>         > --ntlm /usr/bin/ntlm_auth --diagnostics
>         > --helper-protocol=squid-2.5-ntlmssp --domain=****.LOCAL
>         > --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s
>         GSS_C_NO_NAME
>         > auth_param negotiate children 30
>         > auth_param negotiate keep_alive off
>         >
>         >
>         > # LDAP authentication
>         > auth_param basic program /usr/lib/squid3/basic_ldap_auth -R
>         -b
>         > "DC=****,DC=local" -D "CN=SQUID,OU=Service
>         Accounts,DC=****,DC=local"
>         > -w "****" -f sAMAccountName=%s -h
>         > 10.0.0.200,10.0.0.199,10.0.0.194,10.0.0.193
>         > auth_param basic children 150
>         > auth_param basic realm Please enter your Domain credentials
>         to
>         > continue
>         > auth_param basic credentialsttl 1 hour
>         >
>         >
>         > # AD group membership commands
>         > external_acl_type ldap_group ttl=60 children-startup=10
>         > children-max=50 children-idle=2 %
>         > LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
>         > "DC=****,DC=local" -D "CN=SQUID,OU=Service
>         Accounts,DC=****,DC=local"
>         > -w "****" -f "(&(objectclass=person) (sAMAccountname=%
>         v)(memberof=CN=%
>         > a,OU=PROXY,ou=ALL  Groups,DC=****,DC=local))" -h
>         > dc1.****.local,dc2.****.local,dc3.****.local,dc4.****.local
>         >
>         >
>         > acl auth proxy_auth REQUIRED
>         >
>         >
>         >
>         > acl REQGROUPS external ldap_group PROXY-HIGHLY-RESTRICTIVE
>         > PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE
>         PROXY-UNRESTRICTED
>         > PROXY-DEV PROXY-SALES
>         >
>         >
>         > http_access deny !auth all
>         > http_access deny !REQGROUPS all
>         >
>         >
>         >
>         >
>         >
>         >
>         >
>         >
>         >
>         > --
>         > Samuel Anderson  |  Information Technology Administrator  |
>         >  International Document Services
>         >
>         >
>         > IDS  |  11629 South 700 East, Suite 200  |  Draper, UT
>         84020-4607
>         >
>         >
>         >
>         
>         > CONFIDENTIALITY NOTICE:
>         > This e-mail and any attachments are confidential. If you are
>         not an
>         > intended recipient, please contact the sender to report the
>         error and
>         > delete all copies of this message from your system.  Any
>         unauthorized
>         > review, use, disclosure or distribution is prohibited.
>         
>         how did you create and distribute the keytab for the proxies?
>         you must
>         create one keytab and put the same exact one on each of the
>         proxies.
>         the KVNO numbers must match on every proxy.  run "klist
>         -Kket /path/to/the.keytab" on the proxies to check.
>         
>         kerberos is heavily dependent on DNS.  the keytab should
>         contain
>         PRIMARY/instance.domain.tld at REALM where PRIMARY is HTTP,
>         instance.domain.tld is the FQDN of the 10.10.0.254 IP, not
>         either or
>         both of the individual proxies, and REALM should be the
>         Kerberos REALM.
>         
>         did you export the environment variable for the keytab?  on
>         fedora, i
>         put the following in /etc/sysconfig/squid:
>         
>         KRB5_KTNAME=/etc/squid/squid.keytab
>         export KRB5_KTNAME
>         
>         do you get a HTTP ticket from the directory?  from a command
>         prompt,
>         what does "klist tickets" show?  you can also install the XP
>         resource
>         kit and run kerbtray.exe to get that info.  win7 and newer may
>         have it
>         built in.
>         
> 
> 
> 
> 
> -- 
> Samuel Anderson  |  Information Technology Administrator  |
>  International Document Services
> 
> 
> IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607
> 
> 
> 
> CONFIDENTIALITY NOTICE:
> This e-mail and any attachments are confidential. If you are not an
> intended recipient, please contact the sender to report the error and
> delete all copies of this message from your system.  Any unauthorized
> review, use, disclosure or distribution is prohibited.

you will benefit from using the HTTP profile in HAProxy.  if HTTP vs TCP
is causing you grief, then you may want to review the config.

i am using (mind the wrap):

global
        #debug
        daemon
        log localhost local1 notice
        log-send-hostname router
        #uid 996
        #gid 995
        pidfile /var/run/haproxy.pid
        stats socket /var/run/haproxy.sock level admin
        stats maxconn 2

defaults
        balance leastconn

        log global

        mode http

        option httplog
        option http-server-close
        option forwardfor except 127.0.0.0/8

        stats enable
        stats hide-version
        stats refresh 5s
        stats scope   .
        stats show-legends
        stats uri     /admin?stats

        timeout http-request    10s
        timeout queue           1m
        timeout connect         10s
        timeout client          1m
        timeout server          1m
        timeout http-keep-alive 10s
        timeout check           10s

listen proxy 192.168.37.1:8080
	# option to HTTP/1.1 should be on one line
        option httpchk GET /squid-internal-periodic/store_digest
HTTP/1.1
        server proxy1 192.168.88.1:3128 check inter 10000
        server proxy2 192.168.88.2:3128 check inter 10000

listen proxy1 192.168.37.1:8081
	# option to HTTP/1.1 should be on one line
        option httpchk GET /squid-internal-periodic/store_digest
HTTP/1.1
        server proxy1 192.168.88.1:3128 check inter 10000

listen proxy2 192.168.37.1:8082
	# option to HTTP/1.1 should be on one line
        option httpchk GET /squid-internal-periodic/store_digest
HTTP/1.1
        server proxy2 192.168.88.2:3128 check inter 10000

by using the HTTP mode, you can get more intelligent service checking,
and more reliably determine a devices status.



From johnzeng2013 at yahoo.com  Fri Mar 20 01:54:52 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 20 Mar 2015 09:54:52 +0800
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
 <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>
 <CAN8nrKCDrPibjwsyKyvfg1Qa4-ZSZdqeDgj-MNKJ8Ufz2qzV1Q@mail.gmail.com>
 <55064BF8.4010004@treenet.co.nz>
 <CAN8nrKDKAy+xq9MwCWO-aLiBmVaVF3jjJKUcvxjoK6w1xFcCOA@mail.gmail.com>
 <55065F5C.7090707@treenet.co.nz>
 <E26A14FE-041C-44E5-94A3-B63843EE1F1F@getbusi.com>
 <550683CE.5090802@treenet.co.nz>
 <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
Message-ID: <550B7DEC.6030105@yahoo.com>


    Hello Dan:

                         i used 3.5.2 just now , i worried 3.5.3 isn't 
very stable too ,

                         i use 2.7stable 9 ago ,  and you ?

                        if version is 3.xxx , which version is stablest 
until now .


                        Best Regard

? 2015?03?20? 08:07, Dan Charlesworth ??:
> Well I got 3.5.2 into production for a few hours and Bad Things happened:
>
> *1) A hefty performance hit*
> Load average was maybe a tad higher but CPU. memory and I/O were about 
> the same. However the system seemed to top out at around 40 requests 
> per second (on a client that usually hits 100?150 rps) and squid 
> became very slow to respond to squidclient requests:
> [root at proxy-LS5 ~]# time squidclient -p 8080 mgr:utilization | grep 
> client_http.requests
> client_http.requests = 40.965955/sec
> client_http.requests = 41.168528/sec
> client_http.requests = 42.111847/sec
> client_http.requests = 166646
>
> real0m7.163s
> user0m0.002s
> sys0m0.006s
>
> *2) Lots of Segment Violations*
> These obviously suck. Backtrace attached.
>
> Just cannot win. Is it possible these two issues are due to the patch 
> for #4206?
>
>
>
>
>> On 16 Mar 2015, at 6:18 pm, Amos Jeffries <squid3 at treenet.co.nz 
>> <mailto:squid3 at treenet.co.nz>> wrote:
>>
>> On 16/03/2015 7:16 p.m., Dan Charlesworth wrote:
>>> Hey again Amos -
>>>
>>> Unfortunately the patch for #4206 won?t apply to squid-3.4.12. I was 
>>> going to try creating a new one but couldn?t find an equivalent line 
>>> in client_side.cc for that version.
>>>
>>> I guess the #4206 issue doesn?t apply to v3.4.x after all?
>>
>> Correct. Oh well.
>>
>>
>>>
>>> [Not a C programmer]
>>>
>>> Thanks for your time today.
>>>
>>> P.S. I'd love to upgrade to v3.5 but I'm waiting for somebody 
>>> smarter than me to take the lead on a CentOS 6 RPM SPEC file.
>>
>> Eliezer to the rescue ;-)
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5
>>
>>
>> Amos
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From geekguy at geek-guy.com  Fri Mar 20 01:55:17 2015
From: geekguy at geek-guy.com (Lawrence Pingree)
Date: Thu, 19 Mar 2015 18:55:17 -0700
Subject: [squid-users] SNMP queries to squid never go beyond 1 GB
Message-ID: <007f01d062b0$eb577d50$c20677f0$@geek-guy.com>

No matter what cache_mem I set it seems that MRTG queries via SNMP never
seem to get beyond 1 GB even running the latest 3.5 code. Amos, is the code
capable of allocating more than one gig of memory?

 

Storage Mem Size @ xxxxx

The statistics were last updated Thursday, 19 March 2015 at 18:48,
at which time 'squid 3.5.1' had been up for 10:27:16.

`Daily' Graph (5 Minute Average)



	
Max

Average

Current


Mem Size 

980.8 MBytes

648.1 MBytes

139.5 MBytes

 

 

 

"Convert your dreams to achievable and realistic goals, this way the journey
is satisfying and progressive." - LP

 

Best regards,

The Geek Guy



Lawrence Pingree

 <http://www.lawrencepingree.com/resume/>
http://www.lawrencepingree.com/resume/

 

 

Author of "The Manager's Guide to Becoming Great"

 <http://www.management-book.com/> http://www.Management-Book.com

 

 
<https://webportal.isc2.org/custom/CertificationVerificationResults.aspx?FN=
Lawrence&LN=Pingree&CN=76042> 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/63551da5/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 1875 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/63551da5/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.gif
Type: image/gif
Size: 2852 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/63551da5/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 3024 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150319/63551da5/attachment-0001.png>

From eliezer at ngtech.co.il  Fri Mar 20 02:45:12 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 20 Mar 2015 04:45:12 +0200
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
Message-ID: <550B89B8.7010107@ngtech.co.il>

Hey Dan and John,

If indeed this bug is only for UFS\AUFS cache_dir then I would try to 
make sure that large-rock will not sustain the same issue.

I have not seen in any of the bug reports anything that would reproduce 
the issue.
To make sure the issue is understood and can or cannot be reproduced 
using ufs\aufs will give one direction.
I would try to test large rock in my next testing round with SMP but if 
anyone has some option to test it first I will be glad if it will be 
done to make sure ufs\aufs is the culprit.

Also if indeed it's with aufs\ufs only with SMP then it means that the 
issue is related to the way SMP can make a ufs\aufs cache_dir dirty and 
there for the answer would be pretty simple to the issue in hands.

Eliezer

On 20/03/2015 00:32, Dan Charlesworth wrote:
> Hi John
>
> This bug has been affecting me on an off for a while as well. I believe it
> only affects aufs and, unfortunately, has been around for years.
>
> See:http://bugs.squid-cache.org/show_bug.cgi?id=3279
> And see:http://bugs.squid-cache.org/show_bug.cgi?id=3483



From eliezer at ngtech.co.il  Fri Mar 20 02:52:11 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 20 Mar 2015 04:52:11 +0200
Subject: [squid-users] Squid will not authenticate NTLM/Kerberos when
 behind a haproxy load balancer
In-Reply-To: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
References: <CAP6yRXhoaUsO55OFB0eiRZe2Vj25A6xdDrev_-TpEQ2rDRDGMQ@mail.gmail.com>
Message-ID: <550B8B5B.3010601@ngtech.co.il>

Hey Samuel,

Not related to your post at squid-cache, I have tried to access your 
site from my testing grounds and I do not seem to be able to access it.
Not even an ICMP echo ping.

It is maybe something in the route between my client to your server but 
I was wondering if I should contact my ISP or you know about something?

Eliezer



From dan at getbusi.com  Fri Mar 20 03:17:53 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 20 Mar 2015 14:17:53 +1100
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <550B89B8.7010107@ngtech.co.il>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
 <550B89B8.7010107@ngtech.co.il>
Message-ID: <CAN8nrKBKNA1hSfSdNC4EaDyGT6z5V-uT1FtLq_P+NDn2pqKQOg@mail.gmail.com>

Hey Eliezer

I don't actually use SMP. I could be wrong about the aufs thing; I haven't
personally tested?and don't currently plan to test?any other cache types. I
just gleaned that from the comments in the bug reports.

Kind regards
Dan


On 20 March 2015 at 13:45, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> Hey Dan and John,
>
> If indeed this bug is only for UFS\AUFS cache_dir then I would try to make
> sure that large-rock will not sustain the same issue.
>
> I have not seen in any of the bug reports anything that would reproduce
> the issue.
> To make sure the issue is understood and can or cannot be reproduced using
> ufs\aufs will give one direction.
> I would try to test large rock in my next testing round with SMP but if
> anyone has some option to test it first I will be glad if it will be done
> to make sure ufs\aufs is the culprit.
>
> Also if indeed it's with aufs\ufs only with SMP then it means that the
> issue is related to the way SMP can make a ufs\aufs cache_dir dirty and
> there for the answer would be pretty simple to the issue in hands.
>
> Eliezer
>
> On 20/03/2015 00:32, Dan Charlesworth wrote:
>
>> Hi John
>>
>> This bug has been affecting me on an off for a while as well. I believe it
>> only affects aufs and, unfortunately, has been around for years.
>>
>> See:http://bugs.squid-cache.org/show_bug.cgi?id=3279
>> And see:http://bugs.squid-cache.org/show_bug.cgi?id=3483
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/7c6087c3/attachment.htm>

From johnzeng2013 at yahoo.com  Fri Mar 20 03:29:25 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Fri, 20 Mar 2015 11:29:25 +0800
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
 <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>
 <CAN8nrKCDrPibjwsyKyvfg1Qa4-ZSZdqeDgj-MNKJ8Ufz2qzV1Q@mail.gmail.com>
 <55064BF8.4010004@treenet.co.nz>
 <CAN8nrKDKAy+xq9MwCWO-aLiBmVaVF3jjJKUcvxjoK6w1xFcCOA@mail.gmail.com>
 <55065F5C.7090707@treenet.co.nz>
 <E26A14FE-041C-44E5-94A3-B63843EE1F1F@getbusi.com>
 <550683CE.5090802@treenet.co.nz>
 <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
Message-ID: <550B9415.8040805@yahoo.com>


Hello Dan:

                  i used squid 2.7stable9 ago ,and i worried whether 
squid 3.5.2 is stablest for us until now too .

                  and you ?

                  Do you think Whether version is stablest at squid 
3.xxx  ?







> Well I got 3.5.2 into production for a few hours and Bad Things happened:
>
> *1) A hefty performance hit*
> Load average was maybe a tad higher but CPU. memory and I/O were about 
> the same. However the system seemed to top out at around 40 requests 
> per second (on a client that usually hits 100?150 rps) and squid 
> became very slow to respond to squidclient requests:
> [root at proxy-LS5 ~]# time squidclient -p 8080 mgr:utilization | grep 
> client_http.requests
> client_http.requests = 40.965955/sec
> client_http.requests = 41.168528/sec
> client_http.requests = 42.111847/sec
> client_http.requests = 166646
>
> real0m7.163s
> user0m0.002s
> sys0m0.006s
>
> *2) Lots of Segment Violations*
> These obviously suck. Backtrace attached.
>
> Just cannot win. Is it possible these two issues are due to the patch 
> for #4206?
>
>
>
>
>> On 16 Mar 2015, at 6:18 pm, Amos Jeffries <squid3 at treenet.co.nz 
>> <mailto:squid3 at treenet.co.nz>> wrote:
>>
>> On 16/03/2015 7:16 p.m., Dan Charlesworth wrote:
>>> Hey again Amos -
>>>
>>> Unfortunately the patch for #4206 won?t apply to squid-3.4.12. I was 
>>> going to try creating a new one but couldn?t find an equivalent line 
>>> in client_side.cc for that version.
>>>
>>> I guess the #4206 issue doesn?t apply to v3.4.x after all?
>>
>> Correct. Oh well.
>>
>>
>>>
>>> [Not a C programmer]
>>>
>>> Thanks for your time today.
>>>
>>> P.S. I'd love to upgrade to v3.5 but I'm waiting for somebody 
>>> smarter than me to take the lead on a CentOS 6 RPM SPEC file.
>>
>> Eliezer to the rescue ;-)
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5
>>
>>
>> Amos
>>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dan at getbusi.com  Fri Mar 20 03:31:28 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 20 Mar 2015 14:31:28 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <550B9415.8040805@yahoo.com>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
 <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>
 <CAN8nrKCDrPibjwsyKyvfg1Qa4-ZSZdqeDgj-MNKJ8Ufz2qzV1Q@mail.gmail.com>
 <55064BF8.4010004@treenet.co.nz>
 <CAN8nrKDKAy+xq9MwCWO-aLiBmVaVF3jjJKUcvxjoK6w1xFcCOA@mail.gmail.com>
 <55065F5C.7090707@treenet.co.nz>
 <E26A14FE-041C-44E5-94A3-B63843EE1F1F@getbusi.com>
 <550683CE.5090802@treenet.co.nz>
 <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
 <550B9415.8040805@yahoo.com>
Message-ID: <2F573102-94BB-4FC6-B61C-74BECFDA7F68@getbusi.com>

John -

For us the 3.4 series is definitely the stablest.

I was hoping 3.5.2 + plus a patch would avoid the error in this thread?s subject?and it might have done?but it introduced two other major problems (for us).

> On 20 Mar 2015, at 2:29 pm, johnzeng <johnzeng2013 at yahoo.com> wrote:
> 
> 
> Hello Dan:
> 
>                 i used squid 2.7stable9 ago ,and i worried whether squid 3.5.2 is stablest for us until now too .
> 
>                 and you ?
> 
>                 Do you think Whether version is stablest at squid 3.xxx  ?
> 
> 
> 
> 
> 
> 
> 
>> Well I got 3.5.2 into production for a few hours and Bad Things happened:
>> 
>> *1) A hefty performance hit*
>> Load average was maybe a tad higher but CPU. memory and I/O were about the same. However the system seemed to top out at around 40 requests per second (on a client that usually hits 100?150 rps) and squid became very slow to respond to squidclient requests:
>> [root at proxy-LS5 ~]# time squidclient -p 8080 mgr:utilization | grep client_http.requests
>> client_http.requests = 40.965955/sec
>> client_http.requests = 41.168528/sec
>> client_http.requests = 42.111847/sec
>> client_http.requests = 166646
>> 
>> real0m7.163s
>> user0m0.002s
>> sys0m0.006s
>> 
>> *2) Lots of Segment Violations*
>> These obviously suck. Backtrace attached.
>> 
>> Just cannot win. Is it possible these two issues are due to the patch for #4206?
>> 
>> 
>> 
>> 
>>> On 16 Mar 2015, at 6:18 pm, Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> wrote:
>>> 
>>> On 16/03/2015 7:16 p.m., Dan Charlesworth wrote:
>>>> Hey again Amos -
>>>> 
>>>> Unfortunately the patch for #4206 won?t apply to squid-3.4.12. I was going to try creating a new one but couldn?t find an equivalent line in client_side.cc for that version.
>>>> 
>>>> I guess the #4206 issue doesn?t apply to v3.4.x after all?
>>> 
>>> Correct. Oh well.
>>> 
>>> 
>>>> 
>>>> [Not a C programmer]
>>>> 
>>>> Thanks for your time today.
>>>> 
>>>> P.S. I'd love to upgrade to v3.5 but I'm waiting for somebody smarter than me to take the lead on a CentOS 6 RPM SPEC file.
>>> 
>>> Eliezer to the rescue ;-)
>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5
>>> 
>>> 
>>> Amos
>>> 
>> 
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From alberto2perez at gmail.com  Fri Mar 20 03:37:35 2015
From: alberto2perez at gmail.com (Alberto Perez)
Date: Thu, 19 Mar 2015 23:37:35 -0400
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAN8nrKBKNA1hSfSdNC4EaDyGT6z5V-uT1FtLq_P+NDn2pqKQOg@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
 <550B89B8.7010107@ngtech.co.il>
 <CAN8nrKBKNA1hSfSdNC4EaDyGT6z5V-uT1FtLq_P+NDn2pqKQOg@mail.gmail.com>
Message-ID: <CAMZauGrPSHHn8AmpyaxNFUsHeSCzsohjAfM3Eo1eQxd8cesqTQ@mail.gmail.com>

Another one here not using SMP, and using aufs.

I stopped seen this issue frequently when I reduced my cache size,
from 70 GB to 30 GB now.

Regards

On 3/19/15, Dan Charlesworth <dan at getbusi.com> wrote:
> Hey Eliezer
>
> I don't actually use SMP. I could be wrong about the aufs thing; I haven't
> personally tested?and don't currently plan to test?any other cache types. I
> just gleaned that from the comments in the bug reports.
>
> Kind regards
> Dan
>
>
> On 20 March 2015 at 13:45, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>
>> Hey Dan and John,
>>
>> If indeed this bug is only for UFS\AUFS cache_dir then I would try to
>> make
>> sure that large-rock will not sustain the same issue.
>>
>> I have not seen in any of the bug reports anything that would reproduce
>> the issue.
>> To make sure the issue is understood and can or cannot be reproduced
>> using
>> ufs\aufs will give one direction.
>> I would try to test large rock in my next testing round with SMP but if
>> anyone has some option to test it first I will be glad if it will be done
>> to make sure ufs\aufs is the culprit.
>>
>> Also if indeed it's with aufs\ufs only with SMP then it means that the
>> issue is related to the way SMP can make a ufs\aufs cache_dir dirty and
>> there for the answer would be pretty simple to the issue in hands.
>>
>> Eliezer
>>
>> On 20/03/2015 00:32, Dan Charlesworth wrote:
>>
>>> Hi John
>>>
>>> This bug has been affecting me on an off for a while as well. I believe
>>> it
>>> only affects aufs and, unfortunately, has been around for years.
>>>
>>> See:http://bugs.squid-cache.org/show_bug.cgi?id=3279
>>> And see:http://bugs.squid-cache.org/show_bug.cgi?id=3483
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>


From dan at getbusi.com  Fri Mar 20 03:40:18 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 20 Mar 2015 14:40:18 +1100
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
	HTTP reply without Date:
In-Reply-To: <CAMZauGrPSHHn8AmpyaxNFUsHeSCzsohjAfM3Eo1eQxd8cesqTQ@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
 <550B89B8.7010107@ngtech.co.il>
 <CAN8nrKBKNA1hSfSdNC4EaDyGT6z5V-uT1FtLq_P+NDn2pqKQOg@mail.gmail.com>
 <CAMZauGrPSHHn8AmpyaxNFUsHeSCzsohjAfM3Eo1eQxd8cesqTQ@mail.gmail.com>
Message-ID: <B60C34D4-6D40-436D-BC64-00D82EEB4BCB@getbusi.com>

Ours usually run 50?100 GB.

We don?t see it super frequently. But when it happens it tends to keep happening over and over until the swap.sate?s rebuilt.

> On 20 Mar 2015, at 2:37 pm, Alberto Perez <alberto2perez at gmail.com> wrote:
> 
> Another one here not using SMP, and using aufs.
> 
> I stopped seen this issue frequently when I reduced my cache size,
> from 70 GB to 30 GB now.
> 
> Regards
> 
> On 3/19/15, Dan Charlesworth <dan at getbusi.com> wrote:
>> Hey Eliezer
>> 
>> I don't actually use SMP. I could be wrong about the aufs thing; I haven't
>> personally tested?and don't currently plan to test?any other cache types. I
>> just gleaned that from the comments in the bug reports.
>> 
>> Kind regards
>> Dan
>> 
>> 
>> On 20 March 2015 at 13:45, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> 
>>> Hey Dan and John,
>>> 
>>> If indeed this bug is only for UFS\AUFS cache_dir then I would try to
>>> make
>>> sure that large-rock will not sustain the same issue.
>>> 
>>> I have not seen in any of the bug reports anything that would reproduce
>>> the issue.
>>> To make sure the issue is understood and can or cannot be reproduced
>>> using
>>> ufs\aufs will give one direction.
>>> I would try to test large rock in my next testing round with SMP but if
>>> anyone has some option to test it first I will be glad if it will be done
>>> to make sure ufs\aufs is the culprit.
>>> 
>>> Also if indeed it's with aufs\ufs only with SMP then it means that the
>>> issue is related to the way SMP can make a ufs\aufs cache_dir dirty and
>>> there for the answer would be pretty simple to the issue in hands.
>>> 
>>> Eliezer
>>> 
>>> On 20/03/2015 00:32, Dan Charlesworth wrote:
>>> 
>>>> Hi John
>>>> 
>>>> This bug has been affecting me on an off for a while as well. I believe
>>>> it
>>>> only affects aufs and, unfortunately, has been around for years.
>>>> 
>>>> See:http://bugs.squid-cache.org/show_bug.cgi?id=3279
>>>> And see:http://bugs.squid-cache.org/show_bug.cgi?id=3483
>>>> 
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>> 
>> 



From squid3 at treenet.co.nz  Fri Mar 20 05:27:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 18:27:13 +1300
Subject: [squid-users] Redirect on Debian 7
In-Reply-To: <20150319102859.GA13665@nobswolf.info>
References: <20150319102859.GA13665@nobswolf.info>
Message-ID: <550BAFB1.5030901@treenet.co.nz>

On 19/03/2015 11:28 p.m., nobs at nobswolf.info wrote:
> Hello,
> 
> I use the standard-Squid on Debian 7 and I'd like to create a 
> redirect script.
> 
> The documentation looks quite simple, but its not very logical to me.
> Some say just repeating the URL is ok, others say there is an ID
> that needs to get repeated in the answer. Some say you need to
> send an OK with the answer.

That depends on what version of the helper protocol the tutorial was
written about. It also varies by when helper type you are writing.

The definitive reference is the Squid wiki feature page about helpers
<http://wiki.squid-cache.org/Features/AddonHelpers>


> 
> Then I tried to use a simple script with tee to debug. It works
> quite well on the console, but does nothing in Squid. Squid 
> behaves strange when I use the script.

The brief FAQ section at the top of the wiki page has outline of the
common problems encountered.
You can find example scripts in the Squid sources as helpers called "fake".

<http://bazaar.launchpad.net/~squid/squid/3.5/files/head:/helpers/url_rewrite/fake/>

Amos


From squid3 at treenet.co.nz  Fri Mar 20 05:59:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 18:59:59 +1300
Subject: [squid-users] Squid 3.5.2 will only start if cache directory is
 empty
In-Reply-To: <CANLNtGSnc4RrvJyVspsSesYF4Mtefh1ovNoo1GkJxahQQMk-YA@mail.gmail.com>
References: <CANLNtGSYSs6oWKXa5hkYVnu44J7Rp832M7yQo3v-6=mJDfnw-w@mail.gmail.com>	<550A4C7C.9020807@treenet.co.nz>
 <CANLNtGSnc4RrvJyVspsSesYF4Mtefh1ovNoo1GkJxahQQMk-YA@mail.gmail.com>
Message-ID: <550BB75F.5070104@treenet.co.nz>

On 20/03/2015 3:26 a.m., Stanford Prescott wrote:
> On Thu, Mar 19, 2015 at 12:11 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
> 
>> On 19/03/2015 11:29 a.m., Stanford Prescott wrote:
>>> I posted this message to the list a few days ago but haven't received any
>>> responses yet. I am hoping someone might be able to provide some insight
>> on
>>> what is going on.
>>>
>>> I have been trying to get Squid 3.5.2 to work with the Smoothwall Express
>>> 3.1 Linux firewall distribution. Specifically, I have modified the Squid
>>> version included with Smoothwall Express 3.1 to enable HTTPS caching. I
>>> have had this working successfully up to Squid version 3.4.10. Now with
>>> trying to upgrade to Squid 3.5.2 I am having problems that I didn't
>>> encounter with prior versions of Squid.
>>>
>>> The first issue I had, which is now resolved, was improper permissions of
>>> the shm folder (in SWE found in /dev/shm). Changing the folder
>> permissions
>>> to Squid user and group allowed Squid 3.5.2 to start. However, now it
>> will
>>> only start with an empty cache directory.
>>
>> Ouch. /dev/shm is a folder for system shared-memory sockets to be
>> created by applications. It should be owned by root user and group, with
>> 777 permissions. Squid (or the OS kernel) should be able to create
>> "files" inside it, but it should not be owned by Squid.
>>
>>
>>
> When I encountered the "Permission denied" errors for shm_open it was
> suggested by another user that I try changing /dev/shm to squid:root and
> 775. When I made that change the permission denied error went away. I will
> try your suggestion to go back to root:root and permissions of 777.
> 
>>> Once it starts with an empty
>>> cache directory, it seems to function correctly as far as caching SSL
>>> encrypted web pages. However, if Squid needs to be restarted for any
>>> reason, it will not restart until the cache directory
>>> (/var/spool/squid/cache) is emptied.
>>
>> That HTTP data cache is unrelated to the SSL session cache. Its contents
>> should not matter.
>>
>>
>>
> That's what I thought as well but still, squid will only start if I first
> empty that cache directory. Perhaps the patch you suggest below will fix
> this.

Maybe that is a separate side effect of the fast shutdown and crashing
issues. Please try just erasing the swap.state for the cache if it
happens again.


> 
>>> *2015/03/14 00:29:47 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
>>> processes*
>>> *FATAL: Ipc::Mem::Segment::open failed to
>>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*
>>>
>>
>>>
>>> What is the "squid-ssl_session_cache". Am I supposed to define that
>>> somewhere in the
>>> Squid configuration? Is that why I am getting that error message because
>> an
>>> ssl_session_cache is not defined somewhere?
>>
>> The .shm is the name of a shared memory socket "file" name. You have
>> sslproxy_session_cache_size defined with a size so the SSL session
>> ticket cache is used.
>>
>> Please try patching your Squid with
>> <http://www.squid-cache.org/Versions/v4/changesets/squid-4-13984.patch>.
>> It should resolve many permissions issues Squid 3.5 workers are having
>> on startup.
>>
>> I will try that.
>>>
>>> This is my squid.conf file with SSL caching using ssl-bump enabled.
>>>
>>
>>>
>>> *# A random port for forward-proxy port needed for SSL*
>>> *http_port 8081*
>>>
>>> *http_port 192.168.100.1:800 <http://192.168.100.1:800/> intercept
>> ssl-bump
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*
>>>
>>> *https_port 192.168.100.1:808 <http://192.168.100.1:808/> intercept
>>> ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*
>>
>> Why two ports? one is usually sufficient.
>>
>>
> 
> 
> Without the "http_port 8081" (or any other random port, it seemed) I would
> get "no forward-proxy port" errors when trying to start squid with SSL
> proxying enabled.
> 

Ah sorry. I see my mailers fancy markup bold+italics blinded me to the
missing 's' on the first intercept port. That config was fine (and correct).

> 
>> These were "carry-overs" from the original squid.conf used by earlier
>> versions of Smoothwall. I will remove them.
>>>
>>> *request_header_access Content-Type allow all*
>>> *request_header_access Date allow all*
>>> *request_header_access Host allow all*
>>> *request_header_access If-Modified-Since allow all*
>>> *request_header_access Pragma allow all*
>>> *request_header_access Accept allow all*
>>> *request_header_access Accept-Charset allow all*
>>> *request_header_access Accept-Encoding allow all*
>>> *request_header_access Accept-Language allow all*
>>> *request_header_access Connection allow all*
>>> *request_header_access All allow all*
>>
>> The above settings do nothing but waste CPU time. You can remove them.
>>
>> What you are instructing Squid to do is effectively "allow certain
>> headers X, Y, Z, oh and every other header too".
>>
>>
> These options were suggested to me by a user who wished to be able to put
> Squid in "stealth mode" meaning, for sites that won't accept connections
> from a proxy to mask squid's headers with proxy settings to appear to the
> sites as a non-proxied connection. When the user has a site that requires
> this, the website url is placed as an acl before that block. Should I not
> have those lines unless there is a website that requires them?
> 

Thought so, its not doing what you were told it does. Replace that "All"
entry - which allows every header through explicitly, with:
   request_header_access Other deny all


But there are some more critical HTTP/1.1 headers you will also need to
add allows for to get traffic working properly:
  TE, Transfer-Encoding, Expect, Authorization, Proxy-Authorization,
If-Not-Modified, If-Unmodified-Since, If-Match, If-None-Match, ETag,
Via, Range, If-Range, Content-Encoding, Content-Disposition,

You can remove the Accept, Accept-Language, Accept-Charset headers. They
are optional and often used in browser footprinting.

The Pragma header can also probably go, its only used in requests by
some very old non-HTTP/1.1 software in place of the
Cache-Control:no-cache feature.


>>>
>>> *shutdown_lifetime 3 seconds*
>>
>> NOTE: very short shutdown time can corrupt the HTTP data cache as the
>> memory index does not have enough time to complete saving to disk.
>>
>>
> Another carry-over from earlier versions of Squid. What would be a more
> reasonable time frame for shutdown_lifetime?

We publish Squid with a default of 30sec. Altering from that depends on
how long the client connections tend to be through the proxy. I believe
these days you need it to be long enough to fully download a 1MB object.

Squid starts rejecting new requests as soon as shutdown is required,
then for the shutdown lifetime TTL its just waiting for the existing
transactions to complete. Most clients tend to finish any given download
within 20-30 seconds.

Amos


From squid3 at treenet.co.nz  Fri Mar 20 06:12:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 19:12:58 +1300
Subject: [squid-users] Squid not responding during file upload.
In-Reply-To: <DM2PR0601MB1312D7D6713302F09AE087F8AE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
References: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
 <550A962A.6070707@treenet.co.nz>
 <DM2PR0601MB1312D7D6713302F09AE087F8AE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
Message-ID: <550BBA6A.4080809@treenet.co.nz>

On 20/03/2015 6:26 a.m., Saravanan Coimbatore wrote:
> 
> From: Amos Jeffries
> 
> On 19/03/2015 8:11 p.m., Saravanan Coimbatore wrote:
>> Hello all, 
>>
>> I am using Squid 3.4 to inspect content that heads out to the cloud from enterprise. I have two c-icap filter that does the content inspection. 
>>
>> Observation: 
>> - Upload 3000 1M files to cloud passes through successfully. 
>> - Upload 300 40M files to cloud results in multiple failures. Some of errors: 400 Bad Request, Request Timed out.. 
>>
>> Tcpdump of the 40MB file upload tests indicate the following:
>> - Boto client used to upload sends packet to squid proxy. 
> 
>  Squid on receiving requests sends them to the ICAP REQMOD service,
>    and waits for its response,
>  then sends the ICAP REQMOD result to the origin server,
>    and waits for its response,
>  then sends that to the ICAP RESPMOD service,
>    and waits for its response,
>  then sends that to the client.
> 
> So...
>  What is the ICAP service and the origin server doing?
> 
> <SC> ICAP Service inspects the data that passes through it, and does selective filtering based on user policies. There are two icap services that handles two different service providers. Would having two icap services cause any delay. Does Squid send data to a icap service if the icap service has returned 204 in the preview handler?  
> 

Good to know. Though I meant more along the lines of what data its been
sent and what Squid got back from it etc, in each step of the above
theoretical sequence of operations. If they all worked properly there
would be no hang problem.


> The Origin server is S3 or Box. We did tests without Squid in between, and the success ratio is high. We are trying to isolate this on a component basis, but the tcp dump shows that the squid did not respond at the TCP level. We were wondering if this is because Squid is busy. 
> 
> 
>> - Proxy does not acknowledge. 
> 
> What type of "acknowledge" are you expecting here? HTTP or TCP level?
> <SC> TCP level. 
> 

Aha. That only happens if the receive buffer in Squid is full, or Squid
believes the request is fully received and is now waiting for the
response data for it to deliver.

Amos


From squid3 at treenet.co.nz  Fri Mar 20 06:17:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 19:17:01 +1300
Subject: [squid-users] SNMP queries to squid never go beyond 1 GB
In-Reply-To: <007f01d062b0$eb577d50$c20677f0$@geek-guy.com>
References: <007f01d062b0$eb577d50$c20677f0$@geek-guy.com>
Message-ID: <550BBB5D.8010806@treenet.co.nz>

On 20/03/2015 2:55 p.m., Lawrence Pingree wrote:
> No matter what cache_mem I set it seems that MRTG queries via SNMP never
> seem to get beyond 1 GB even running the latest 3.5 code. Amos, is the code
> capable of allocating more than one gig of memory?

Yes but the SNMPv2 used by Squid only has 32-bit counters.

We have quite a few 32-bit wrap related bugs waiting somebody who knows
SNMP to do a library upgrade to SNMPv3.

Amos



From squid3 at treenet.co.nz  Fri Mar 20 06:22:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 19:22:48 +1300
Subject: [squid-users] SNMP queries to squid never go beyond 1 GB
In-Reply-To: <550BBB5D.8010806@treenet.co.nz>
References: <007f01d062b0$eb577d50$c20677f0$@geek-guy.com>
 <550BBB5D.8010806@treenet.co.nz>
Message-ID: <550BBCB8.6080504@treenet.co.nz>

On 20/03/2015 7:17 p.m., Amos Jeffries wrote:
> On 20/03/2015 2:55 p.m., Lawrence Pingree wrote:
>> No matter what cache_mem I set it seems that MRTG queries via SNMP never
>> seem to get beyond 1 GB even running the latest 3.5 code. Amos, is the code
>> capable of allocating more than one gig of memory?
> 
> Yes but the SNMPv2 used by Squid only has 32-bit counters.
> 
> We have quite a few 32-bit wrap related bugs waiting somebody who knows
> SNMP to do a library upgrade to SNMPv3.

Forgot to mention the other possibility...

cache_mem is an HTTP data cache and has the same object limits as any
disk cache. None of them may store more than 2^27-1 objects due to
design limits in the indexing. cache_mem also has a much higher rate of
0-byte and small objects than disks usually see.

So its very possible that simply the size of 2^27-1 worth of the small
and most popular objects in your traffic == 1GB.

Amos



From scoimbatore at vormetric.com  Fri Mar 20 06:31:53 2015
From: scoimbatore at vormetric.com (Saravanan Coimbatore)
Date: Fri, 20 Mar 2015 06:31:53 +0000
Subject: [squid-users] Squid not responding during file upload.
In-Reply-To: <550BBA6A.4080809@treenet.co.nz>
References: <DM2PR0601MB1312843AFD8F4D2361EEA69FAE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
 <550A962A.6070707@treenet.co.nz>
 <DM2PR0601MB1312D7D6713302F09AE087F8AE010@DM2PR0601MB1312.namprd06.prod.outlook.com>
 <550BBA6A.4080809@treenet.co.nz>
Message-ID: <DM2PR0601MB13126DD75A09F2E75AC3D1F9AE0E0@DM2PR0601MB1312.namprd06.prod.outlook.com>



-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Thursday, March 19, 2015 11:13 PM
To: Saravanan Coimbatore; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid not responding during file upload.

On 20/03/2015 6:26 a.m., Saravanan Coimbatore wrote:
> 
> From: Amos Jeffries
> 
> On 19/03/2015 8:11 p.m., Saravanan Coimbatore wrote:
>> Hello all,
>>
>> I am using Squid 3.4 to inspect content that heads out to the cloud from enterprise. I have two c-icap filter that does the content inspection. 
>>
>> Observation: 
>> - Upload 3000 1M files to cloud passes through successfully. 
>> - Upload 300 40M files to cloud results in multiple failures. Some of errors: 400 Bad Request, Request Timed out.. 
>>
>> Tcpdump of the 40MB file upload tests indicate the following:
>> - Boto client used to upload sends packet to squid proxy. 
> 
>  Squid on receiving requests sends them to the ICAP REQMOD service,
>    and waits for its response,
>  then sends the ICAP REQMOD result to the origin server,
>    and waits for its response,
>  then sends that to the ICAP RESPMOD service,
>    and waits for its response,
>  then sends that to the client.
> 
> So...
>  What is the ICAP service and the origin server doing?
> 
> <SC> ICAP Service inspects the data that passes through it, and does selective filtering based on user policies. There are two icap services that handles two different service providers. Would having two icap services cause any delay. Does Squid send data to a icap service if the icap service has returned 204 in the preview handler?  
> 

Good to know. Though I meant more along the lines of what data its been sent and what Squid got back from it etc, in each step of the above theoretical sequence of operations. If they all worked properly there would be no hang problem.


> The Origin server is S3 or Box. We did tests without Squid in between, and the success ratio is high. We are trying to isolate this on a component basis, but the tcp dump shows that the squid did not respond at the TCP level. We were wondering if this is because Squid is busy. 
> 
> 
>> - Proxy does not acknowledge. 
> 
> What type of "acknowledge" are you expecting here? HTTP or TCP level?
> <SC> TCP level. 
> 

Aha. That only happens if the receive buffer in Squid is full, or Squid believes the request is fully received and is now waiting for the response data for it to deliver.

<SC> Is there a debug message that will be logged when Squid receive buffer is full?
<SC> What config parameter that we can use to increase Squid receive buffer?
Amos

From squid3 at treenet.co.nz  Fri Mar 20 07:28:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 20:28:22 +1300
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
References: <79efaedce25a8b9d22b59625e158fe38@treenet.co.nz>
 <1424841145197.56934ef5@Nodemailer>
 <E42A1770-6F1C-4209-B0B8-1438477542E5@getbusi.com>
 <54F06091.5010806@treenet.co.nz>
 <0A4C4218-E6B2-47BF-A6BA-C1FFCFAE5AB4@getbusi.com>
 <CAN8nrKCDrPibjwsyKyvfg1Qa4-ZSZdqeDgj-MNKJ8Ufz2qzV1Q@mail.gmail.com>
 <55064BF8.4010004@treenet.co.nz>
 <CAN8nrKDKAy+xq9MwCWO-aLiBmVaVF3jjJKUcvxjoK6w1xFcCOA@mail.gmail.com>
 <55065F5C.7090707@treenet.co.nz>
 <E26A14FE-041C-44E5-94A3-B63843EE1F1F@getbusi.com>
 <550683CE.5090802@treenet.co.nz>
 <C344A272-1AE8-4EE9-B474-54FB73A7998B@getbusi.com>
Message-ID: <550BCC16.3040103@treenet.co.nz>

On 20/03/2015 1:07 p.m., Dan Charlesworth wrote:
> Well I got 3.5.2 into production for a few hours and Bad Things happened:
> 
> *1) A hefty performance hit*
> Load average was maybe a tad higher but CPU. memory and I/O were about the same. 
> However the system seemed to top out at around 40 requests per second (on a 
> client that usually hits 100?150 rps) and squid became very slow to respond to 
> squidclient requests:
> [root at proxy-LS5 ~]# time squidclient -p 8080 mgr:utilization | grep 
> client_http.requests
> client_http.requests = 40.965955/sec
> client_http.requests = 41.168528/sec
> client_http.requests = 42.111847/sec
> client_http.requests = 166646
> 
> real0m7.163s
> user0m0.002s
> sys0m0.006s
> 
> *2) Lots of Segment Violations*
> These obviously suck. Backtrace attached.

Still with symbols erased so I cant see what is going on :-(

("HackXBack" is having a very similar set of issues, but on different
traffic)

> 
> Just cannot win. Is it possible these two issues are due to the patch for #4206?
> 

Doubtful, 4206 patch was only affecting the tunnel.cc code handling for
CONNECT payloads. Your assertion is in client_side.cc

FWIW, the sponsor of that patch is happily running a 3.5.2 snapshot in a
very busy production system (~975 RPS at peak) with no problems reported
to me since it went live.
 We did find and fix a few of the other bugs that are patched in the
snapshot to get it to that state though. So if you are having problems
with the basic 3.5.2 bundle the latest snapshot may be worth a try as well.
There are also some patches in squid-4 that I'm due to be backporting in
the next few hrs related to worker security permissions (being denied
when they should be allowed), and PID fiel issues

Amos



From monahbaki at gmail.com  Fri Mar 20 10:04:09 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 20 Mar 2015 06:04:09 -0400
Subject: [squid-users] How to run squidclient
Message-ID: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>

Hi all,

I am running CentOS 6.6 64 bit, and need to get some information from the
command line.

Compiled squid as:
./configure --prefix=/home/cache --enable-follow-x-forwarded-for
--with-large-files --enable-ssl --disable-ipv6 --enable-esi
--enable-kill-parent-hack --enable-snmp --with-pthreads
--with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
--enable-storeio=ufs,aufs,diskd,rock


[root at ISN-PHC-Cache bin]# ./squidclient mgr:info
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 02:29:53 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3552
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close



#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_reply_access allow all
http_access allow localnet
http_access allow localhost


# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
cache_dir ufs /home/cache/var/cache/squid 350000 16 256


#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320




Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/a8b03aad/attachment.htm>

From john.kj1984 at gmail.com  Fri Mar 20 11:04:30 2015
From: john.kj1984 at gmail.com (john jacob)
Date: Fri, 20 Mar 2015 16:34:30 +0530
Subject: [squid-users] Error when using peek/splice/terminate with Squid
	3.5.1
Message-ID: <CAOoFbVcQZ69wz8R+r=y2vnoeFctaDRJ56qexhoDH1SV5JKpkAQ@mail.gmail.com>

Also this issue is no more appearing if I peek step 1 alone and splice
the remaining ones.

acl step1 at_step  SslBump1
acl step2 at_step  SslBump2
acl step3 at_step  SslBump3


ssl_bump peek step1 all
ssl_bump splice all

So I guess the issue is with the PeerConnector module where
SSL_connect method is being used to connect and parse the server
certificate.


I had added this as a bug as well.

http://bugs.squid-cache.org/show_bug.cgi?id=4202

Regards,
John





*From:* John Killimangalam Jacob
*Sent:* Monday, February 16, 2015 11:25 AM
*To:* 'squid-users at lists.squid-cache.org'
*Subject:* Error when using peek/splice/terminate with Squid 3.5.1



Hi All,



I am trying to configure an intercept proxy with peek/splice/terminate
features in Squid 3.5.1 on CentOS 7 - 64 bit. I wanted to peak at steps 1
and step 2 and to decide on terminate on step 3 based on the SNI and server
certificate values. It is working only for https://www.google.com, but lot
of other ssl sites (likes of https://www.yahoo.com etc) are not getting
loaded logging an ? *Error negotiating SSL on FD 36: error:140920E3:SSL
routines:SSL3_GET_SERVER_HELLO:parse tlsext  *?  in the cache.log (trying
the same sites using openssl s_client command works). I was wondering if it
has to do anything with my config or open ssl (version 1.0.1e) or anything
else. The web sites are being accessed from a windows 7 workstation with IE
8 and Firefox 35.0.1 . Below is the squid.config section for peek and
splice I am using.



*acl step1 at_step  SslBump1*

*acl step2 at_step  SslBump2*

*acl step3 at_step  SslBump3*



*external_acl_type SSL_URL_Filter %SRC %ssl::>sni %ssl::<cert_subject
</path/to/urlfilterscript>*

*acl URL_Allowed external SSL_URL_Filter*



*ssl_bump peek step1 all*

*ssl_bump peek step2 all *

*ssl_bump terminate step3 !URL_Allowed*

*ssl_bump splice step3 all*



*# Squid normally listens to port 3128*

*http_port 3128*

*http_port 3129 intercept*

*https_port 3130 intercept ssl-bump
cert=/tmp/sslcertificates/server.cert.pem
key=/tmp/sslcertificates/server.key.pem*



Thanks in Advance,

John
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/6f4a8594/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Mar 20 23:05:43 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Fri, 20 Mar 2015 16:05:43 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <550A5343.4020206@treenet.co.nz>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
Message-ID: <000001d06362$64bff170$2e3fd450$@netstream.ps>

Hi amos , thanks for reply
I have tried @ top of squidf.conf

acl images rep_header Content-Type ^image/ ^x-image/  
acl small rep_header Content-Length ^[1234]?[0-9]$  
http_reply_access deny small images

are you sure that its blocking images with size >40KB ????
also I didn?t see extensions like jpg or bmp or similar like that ??!!

I have used many several sites , its being all allowed
Can you  advise ?




regards

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, March 18, 2015 9:41 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 19/03/2015 1:35 p.m., snakeeyes wrote:
> Thank you so much  Amos and leonardo
> Can you provide me any sample config to start with ?
> I feel it so difficult to me .
> I had a look @ ""ACL elements"" section in thw wiki about matching size of image but didn?t find clear thing.
> So again I feel that I will create access list that match size > than 50 Byte and with mime type like jpg or bmp and then deny it.
> 
> Could you help me with startup config plz ?

You mean hand over a cut-n-paste example that you can use and when things go wrong not understand how to fix?

Sure:
 acl images rep_header Content-Type ^image/ ^x-image/  acl small rep_header Content-Length ^[1234]?[0-9]$  http_reply_access deny small images


BUT like Leonardo said, censoring the Internet not as easy as all that.

* Images come in *many* data formats (Content-Type values), some of which are shared with other non-image things - like octet-stream which literally means "unknown binary data". They can come embedded inside other objects, JSON, CSS, archive files (like zip / gzip / xz / ar /
cab) ... even plain old HTML can have base64 blobs of image data in them which gets decoded by a script... and so on.

For every point of censorship there is a bypass.

* The Content-Length is also not guaranteed to be existing. The object may be of undefined length streamed in small chunks or as a blob with no size known until the end of the transaction.


What it comes down to is that you need to know exactly what you are looking for in the protocol, and use the appropriate ACL types to match with. Which in turn requires knowing what ACLs you have available and how to use them to construct *_access rules matching your needs.


When you do have to make abnormal things happen be as precise and specific as you can. Every bit of fuzz/approximation *will* cause trouble at some point during production traffic.


So, why are you doing this?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Fri Mar 20 23:12:44 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Fri, 20 Mar 2015 16:12:44 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz> 
Message-ID: <000101d06363$5f9fba80$1edf2f80$@netstream.ps>

I did try with google.com and yahoo.com

It seems not blocking images there 

Can u help with that plz ?

-----Original Message-----
From: snakeeyes [mailto:ahmed.zaeem at netstream.ps] 
Sent: Friday, March 20, 2015 4:06 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] i want to block images with size more than 40 KB

Hi amos , thanks for reply
I have tried @ top of squidf.conf

acl images rep_header Content-Type ^image/ ^x-image/ acl small rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny small images

are you sure that its blocking images with size >40KB ????
also I didn?t see extensions like jpg or bmp or similar like that ??!!

I have used many several sites , its being all allowed Can you  advise ?




regards

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, March 18, 2015 9:41 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 19/03/2015 1:35 p.m., snakeeyes wrote:
> Thank you so much  Amos and leonardo
> Can you provide me any sample config to start with ?
> I feel it so difficult to me .
> I had a look @ ""ACL elements"" section in thw wiki about matching size of image but didn?t find clear thing.
> So again I feel that I will create access list that match size > than 50 Byte and with mime type like jpg or bmp and then deny it.
> 
> Could you help me with startup config plz ?

You mean hand over a cut-n-paste example that you can use and when things go wrong not understand how to fix?

Sure:
 acl images rep_header Content-Type ^image/ ^x-image/  acl small rep_header Content-Length ^[1234]?[0-9]$  http_reply_access deny small images


BUT like Leonardo said, censoring the Internet not as easy as all that.

* Images come in *many* data formats (Content-Type values), some of which are shared with other non-image things - like octet-stream which literally means "unknown binary data". They can come embedded inside other objects, JSON, CSS, archive files (like zip / gzip / xz / ar /
cab) ... even plain old HTML can have base64 blobs of image data in them which gets decoded by a script... and so on.

For every point of censorship there is a bypass.

* The Content-Length is also not guaranteed to be existing. The object may be of undefined length streamed in small chunks or as a blob with no size known until the end of the transaction.


What it comes down to is that you need to know exactly what you are looking for in the protocol, and use the appropriate ACL types to match with. Which in turn requires knowing what ACLs you have available and how to use them to construct *_access rules matching your needs.


When you do have to make abnormal things happen be as precise and specific as you can. Every bit of fuzz/approximation *will* cause trouble at some point during production traffic.


So, why are you doing this?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Fri Mar 20 14:36:08 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 20 Mar 2015 20:36:08 +0600
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000101d06363$5f9fba80$1edf2f80$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000101d06363$5f9fba80$1edf2f80$@netstream.ps>
Message-ID: <550C3058.3070607@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

He answered you. Just re-read answer.

This is not possible absolutely in all cases.

World is changed. Too sad.

21.03.15 5:12, snakeeyes ?????:
> I did try with google.com and yahoo.com
> 
> It seems not blocking images there
> 
> Can u help with that plz ?
> 
> -----Original Message----- From: snakeeyes
> [mailto:ahmed.zaeem at netstream.ps] Sent: Friday, March 20, 2015 4:06
> PM To: 'Amos Jeffries' Cc: squid-users at lists.squid-cache.org 
> Subject: RE: [squid-users] i want to block images with size more
> than 40 KB
> 
> Hi amos , thanks for reply I have tried @ top of squidf.conf
> 
> acl images rep_header Content-Type ^image/ ^x-image/ acl small
> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny
> small images
> 
> are you sure that its blocking images with size >40KB ???? also I
> didn?t see extensions like jpg or bmp or similar like that ??!!
> 
> I have used many several sites , its being all allowed Can you
> advise ?
> 
> 
> 
> 
> regards
> 
> -----Original Message----- From: squid-users
> [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
> Amos Jeffries Sent: Wednesday, March 18, 2015 9:41 PM To:
> squid-users at lists.squid-cache.org Subject: Re: [squid-users] i want
> to block images with size more than 40 KB
> 
> On 19/03/2015 1:35 p.m., snakeeyes wrote:
>> Thank you so much  Amos and leonardo Can you provide me any
>> sample config to start with ? I feel it so difficult to me . I
>> had a look @ ""ACL elements"" section in thw wiki about matching
>> size of image but didn?t find clear thing. So again I feel that I
>> will create access list that match size > than 50 Byte and with
>> mime type like jpg or bmp and then deny it.
>> 
>> Could you help me with startup config plz ?
> 
> You mean hand over a cut-n-paste example that you can use and when
> things go wrong not understand how to fix?
> 
> Sure: acl images rep_header Content-Type ^image/ ^x-image/  acl
> small rep_header Content-Length ^[1234]?[0-9]$  http_reply_access
> deny small images
> 
> 
> BUT like Leonardo said, censoring the Internet not as easy as all
> that.
> 
> * Images come in *many* data formats (Content-Type values), some of
> which are shared with other non-image things - like octet-stream
> which literally means "unknown binary data". They can come embedded
> inside other objects, JSON, CSS, archive files (like zip / gzip /
> xz / ar / cab) ... even plain old HTML can have base64 blobs of
> image data in them which gets decoded by a script... and so on.
> 
> For every point of censorship there is a bypass.
> 
> * The Content-Length is also not guaranteed to be existing. The
> object may be of undefined length streamed in small chunks or as a
> blob with no size known until the end of the transaction.
> 
> 
> What it comes down to is that you need to know exactly what you are
> looking for in the protocol, and use the appropriate ACL types to
> match with. Which in turn requires knowing what ACLs you have
> available and how to use them to construct *_access rules matching
> your needs.
> 
> 
> When you do have to make abnormal things happen be as precise and
> specific as you can. Every bit of fuzz/approximation *will* cause
> trouble at some point during production traffic.
> 
> 
> So, why are you doing this?
> 
> Amos
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJVDDBXAAoJENNXIZxhPexGeJYIAJdUgDYAW8YS7AQSCcJ9XT/S
Ln65rhi5xYgxlJFUdRhlySiqfqueJpbfwm++QkVUXx/tsa7tCMTDpB882nPydtl+
0BkDseYHjwu+kVbBweXAJXvRoo74zNjXmAVv4Ib4o92Pnz8WAElxB8zJpB/zw4LH
ZQaw3e0e82OGCDcsrd/yYsWR5nDYP00KQQcKglf4gaajJP2Gy6GaNOORcbdsBiOR
Ypzcw/0mEpVIVYtvz3F5y46gsa5I0ocacwo+6S8UYiMEejOqMH/t/yFCsG5t1G/1
8Q3Vvr/77KR2K7MJkliFnjbEguItFPu2m8bnNIr0am08g5SnBmjqvgrGiHhHDdA=
=1Ocj
-----END PGP SIGNATURE-----


From idharper at gmail.com  Fri Mar 20 15:54:14 2015
From: idharper at gmail.com (Ian Harper)
Date: Fri, 20 Mar 2015 15:54:14 +0000
Subject: [squid-users] squid just stopping
Message-ID: <CAAq-SCBy3uJQ85YJDwJZC4L+W_Ft9UuNaoNm7rqtsLAA2iyM6Q@mail.gmail.com>

I have squid running ok on one machine doing basic forwarding but when I
try to add a setting for url redirection it just stops with no errors.

I cloned the machine and owrked on the url redirection and eventually
settled on squirm and got it working, when I went back to the original
machine to try it again it did the same.

When I try to run it maunally on different port etc using a different
config file on either machine it does the same.

Below is the log output

squid -f squid.conf-2 -X -N -a 3129
2015/03/20 15:45:22.131| command-line -X overrides: ALL,7
2015/03/20 15:45:22.132| http(s)_port: found Listen on Port: 3129
2015/03/20 15:45:22.132| http(s)_port: found Listen on wildcard address:
*:3129
2015/03/20 15:45:22.132| CacheManager::registerAction: registering legacy
mem
2015/03/20 15:45:22.132| CacheManager::findAction: looking for action mem
2015/03/20 15:45:22.132| Action not found.
2015/03/20 15:45:22.132| CacheManager::registerAction: registered mem
2015/03/20 15:45:22.132| CacheManager::registerAction: registering legacy
squidaio_counts
2015/03/20 15:45:22.132| CacheManager::findAction: looking for action
squidaio_counts
2015/03/20 15:45:22.132| Action not found.
2015/03/20 15:45:22.132| CacheManager::registerAction: registered
squidaio_counts
2015/03/20 15:45:22.132| CacheManager::registerAction: registering legacy
diskd
2015/03/20 15:45:22.132| CacheManager::findAction: looking for action diskd
2015/03/20 15:45:22.132| Action not found.
2015/03/20 15:45:22.132| CacheManager::registerAction: registered diskd
2015/03/20 15:45:22.132| Detected IPv6 hybrid or v4-mapping stack...
2015/03/20 15:45:22.132| IPv6 transport Enabled
2015/03/20 15:45:22.132| aclDestroyACLs: invoked
2015/03/20 15:45:22.132| cbdataFree: 0x7ff55b585af8
2015/03/20 15:45:22.132| cbdataFree: Freeing 0x7ff55b585af8
2015/03/20 15:45:22.132| ACL::Prototype::Registered: invoked for type src
2015/03/20 15:45:22.132| ACL::Prototype::Registered:    yes
2015/03/20 15:45:22.132| ACL::FindByName 'all'
2015/03/20 15:45:22.132| ACL::FindByName found no match
2015/03/20 15:45:22.132| aclParseAclLine: Creating ACL 'all'
2015/03/20 15:45:22.132| ACL::Prototype::Factory: cloning an object for
type 'src'
2015/03/20 15:45:22.132| aclIpParseIpData: all
2015/03/20 15:45:22.132| aclIpParseIpData: magic 'all' found.
2015/03/20 15:45:22.132| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.133| ACL::FindByName 'all'
2015/03/20 15:45:22.133| Processing Configuration File: squid.conf-2 (depth
0)
2015/03/20 15:45:22.134| Processing: 'visible_hostname mailsrv1'
2015/03/20 15:45:22.134| Processing: 'debug_options rotate=3 ALL,2'
2015/03/20 15:45:22.134| Processing: 'cache_effective_user squid'
2015/03/20 15:45:22.134| Processing: 'cache_effective_group squid'
2015/03/20 15:45:22.134| Processing: 'url_rewrite_program
/usr/local/squirm/bin/squirm'
2015/03/20 15:45:22.134| Processing: 'redirect_children 5'
2015/03/20 15:45:22.134| Processing: 'dns_nameservers 208.67.222.222
208.67.220.220 192.168.32.1 192.168.32.2 192.168.40.254 192.168.40.5'
2015/03/20 15:45:22.134| Processing: 'http_port 3329'
2015/03/20 15:45:22.134| http(s)_port: found Listen on Port: 3329
2015/03/20 15:45:22.134| http(s)_port: found Listen on wildcard address:
*:3329
2015/03/20 15:45:22.134| Processing: 'hierarchy_stoplist cgi-bin ?'
2015/03/20 15:45:22.134| Processing: 'coredump_dir /var/spool/squid'
2015/03/20 15:45:22.134| Processing: 'refresh_pattern ^ftp:
1440    20%     10080'
2015/03/20 15:45:22.134| Processing: 'refresh_pattern ^gopher:  1440
0%      1440'
2015/03/20 15:45:22.134| Processing: 'refresh_pattern -i (/cgi-bin/|\?)
0       0%      0'
2015/03/20 15:45:22.134| Processing: 'refresh_pattern .         0
20%     4320'
2015/03/20 15:45:22.134| Processing: 'delay_initial_bucket_level 0'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| Log definition name 'squid' file
'/var/log/squid/access.log'
2015/03/20 15:45:22.134| wccp2_add_service_list: added service id 0
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| aclParseAclList: looking for ACL name 'all'
2015/03/20 15:45:22.134| ACL::FindByName 'all'
2015/03/20 15:45:22.134| tools.cc(671) uniqueHostname:  Config: '
2015/03/20 15:45:22.134| tools.cc(671) uniqueHostname:  Config: '
2015/03/20 15:45:22.134| Initializing https proxy context
2015/03/20 15:45:22.139| Using SSLv2/SSLv3.
2015/03/20 15:45:22.141| Setting RSA key generation callback.
2015/03/20 15:45:22.141| Setting certificate verification callback.
2015/03/20 15:45:22.141| Setting CA certificate locations.
2015/03/20 15:45:22.153| leave_suid: PID 16685 called
2015/03/20 15:45:22.153| leave_suid: PID 16685 giving up root, becoming
'squid'
2015/03/20 15:45:22.153| command-line -X overrides: rotate=3 ALL,2
[root at host2 squid]#


and this is the contents on the squid.conf-2 file


*more squid.conf-2*
#
visible_hostname mailsrv1
# Recommended minimum configuration:
debug_options rotate=3 ALL,2

cache_effective_user squid
cache_effective_group squid
url_rewrite_program /usr/local/squirm/bin/squirm
redirect_children 5

# add dns server for squid to use specifically
dns_nameservers 208.67.222.222 208.67.220.220 192.168.32.1 192.168.32.2
192.168.40.254 192.168.40.5

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost

# Deny requests to certain unsafe ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

# And finally deny all other access to this proxy

# Squid normally listens to port 3128
http_port 3329

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?


# Uncomment and adjust the following to add a disk cache directory.

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

delay_initial_bucket_level 0

Can anyone tell me what I am doing wrong ? all I am after is a non-caching
squid proxy server which will do some url redirection.

Thanks

Ian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/580c68ed/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 20 16:13:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Mar 2015 05:13:59 +1300
Subject: [squid-users] How to run squidclient
In-Reply-To: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
Message-ID: <550C4747.3060508@treenet.co.nz>

On 20/03/2015 11:04 p.m., Monah Baki wrote:
> Hi all,
> 
> I am running CentOS 6.6 64 bit, and need to get some information from the
> command line.
> 
> Compiled squid as:
> ./configure --prefix=/home/cache --enable-follow-x-forwarded-for
> --with-large-files --enable-ssl --disable-ipv6 --enable-esi
> --enable-kill-parent-hack --enable-snmp --with-pthreads
> --with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
> --enable-storeio=ufs,aufs,diskd,rock
> 
> 
> [root at ISN-PHC-Cache bin]# ./squidclient mgr:info
> HTTP/1.1 403 Forbidden
> Server: squid/3.5.2
> Mime-Version: 1.0
> Date: Fri, 20 Mar 2015 02:29:53 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3552
> X-Squid-Error: ERR_ACCESS_DENIED 0
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from isn-phc-cache
> Via: 1.1 isn-phc-cache (squid/3.5.2)
> Connection: close
> 

Well the request is getting through to the proxy "isn-phc-cache". But
for some reason its being rejected.

You have this:
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 

So it should be allowed. You are running squidclient connecting to the
default (localhost:3128) proxy server.

Use -v option to get more verbose output from squidclient about what its
doing. You can repeat the option several times to get more detailed
debug info.

The other thing to watch out for is whether ./squidclient run from that
/bin directory is the 3.5.2 version or not. If its from an older version
of Squid it wont have all the options the new ones do.

Amos


From squid3 at treenet.co.nz  Fri Mar 20 16:26:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Mar 2015 05:26:32 +1300
Subject: [squid-users] squid just stopping
In-Reply-To: <CAAq-SCBy3uJQ85YJDwJZC4L+W_Ft9UuNaoNm7rqtsLAA2iyM6Q@mail.gmail.com>
References: <CAAq-SCBy3uJQ85YJDwJZC4L+W_Ft9UuNaoNm7rqtsLAA2iyM6Q@mail.gmail.com>
Message-ID: <550C4A38.3020203@treenet.co.nz>

On 21/03/2015 4:54 a.m., Ian Harper wrote:
> I have squid running ok on one machine doing basic forwarding but when I
> try to add a setting for url redirection it just stops with no errors.
> 
> I cloned the machine and owrked on the url redirection and eventually
> settled on squirm and got it working, when I went back to the original
> machine to try it again it did the same.
> 
> When I try to run it maunally on different port etc using a different
> config file on either machine it does the same.
> 
> Below is the log output
> 

No that was *not* the log output produced by -X up until the point where
cache.log was created. There is a cache.log file also produced with the
rest of the activity and probably the fault details.



> 
> and this is the contents on the squid.conf-2 file
> 
> 

Apart from the fact that "redirect_children" no longer exists. (Use
url_rewrite_children instead.). Your config looks okay.

Amos


From monahbaki at gmail.com  Fri Mar 20 16:33:47 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 20 Mar 2015 12:33:47 -0400
Subject: [squid-users] How to run squidclient
In-Reply-To: <550C4747.3060508@treenet.co.nz>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
 <550C4747.3060508@treenet.co.nz>
Message-ID: <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>

Hi Amos,

[root at ISN-PHC-Cache bin]# ./squidclient -V
Version: 3.5.2




[root at ISN-PHC-Cache bin]# ./squidclient -vv mgr:info
verbosity level set to 2
Request:
GET cache_object://localhost/info HTTP/1.0
Host: localhost
User-Agent: squidclient/3.5.2
Accept: */*
Connection: close


.
Transport detected: IPv4-only
Resolving localhost ...
Connecting... localhost (127.0.0.1:3128)
Connected to: localhost (127.0.0.1:3128)
Sending HTTP request ...
done.
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 17:29:54 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3549
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2015 The Squid Software
Foundation and contributors">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--
 /*
 * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
 *
 * Squid software is distributed under GPLv2+ license and includes
 * contributions from numerous individuals and organizations.
 * Please see the COPYING and CONTRIBUTORS files for details.
 */

/*
 Stylesheet for Squid Error pages
 Adapted from design by Free CSS Templates
 http://www.freecsstemplates.org
 Released for free under a Creative Commons Attribution 2.5 License
*/

/* Page basics */
* {
    font-family: verdana, sans-serif;
}

html body {
    margin: 0;
    padding: 0;
    background: #efefef;
    font-size: 12px;
    color: #1e1e1e;
}

/* Page displayed title area */
#titles {
    margin-left: 15px;
    padding: 10px;
    padding-left: 100px;
    background: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat
left;
}

/* initial title */
#titles h1 {
    color: #000000;
}
#titles h2 {
    color: #000000;
}

/* special event: FTP success page titles */
#titles ftpsuccess {
    background-color:#00ff00;
    width:100%;
}

/* Page displayed body content area */
#content {
    padding: 10px;
    background: #ffffff;
}

/* General text */
p {
}

/* error brief description */
#error p {
}

/* some data which may have caused the problem */
#data {
}

/* the error message received from the system or other software */
#sysmsg {
}

pre {
    font-family:sans-serif;
}

/* special event: FTP / Gopher directory listing */
#dirmsg {
    font-family: courier;
    color: black;
    font-size: 10pt;
}
#dirlisting {
    margin-left: 2%;
    margin-right: 2%;
}
#dirlisting tr.entry td.icon,td.filename,td.size,td.date {
    border-bottom: groove;
}
#dirlisting td.size {
    width: 50px;
    text-align: right;
    padding-right: 5px;
}

/* horizontal lines */
hr {
    margin: 0;
}

/* page displayed footer area */
#footer {
    font-size: 9px;
    padding-left: 10px;
}


body
:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya,
sans-serif; float: right; }
:lang(he) { direction: rtl; }
 --></style>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="cache_object://localhost/info">cache_object://localhost/info</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is <a href="mailto:webmaster
?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20isn-phc-cache%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2020%20Mar%202015%2017%3A29%3A54%20GMT%0D%0A%0D%0AClientIP%3A%2010.0.0.24%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Finfo%20HTTP%2F1.0%0AHost%3A%20localhost%0D%0AUser-Agent%3A%20squidclient%2F3.5.2%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20close%0D%0A%0D%0A%0D%0A">webmaster</a>.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Fri, 20 Mar 2015 17:29:54 GMT by isn-phc-cache
(squid/3.5.2)</p>
<!-- ERR_ACCESS_DENIED -->
</div>
</body></html>


On Fri, Mar 20, 2015 at 12:13 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 20/03/2015 11:04 p.m., Monah Baki wrote:
> > Hi all,
> >
> > I am running CentOS 6.6 64 bit, and need to get some information from the
> > command line.
> >
> > Compiled squid as:
> > ./configure --prefix=/home/cache --enable-follow-x-forwarded-for
> > --with-large-files --enable-ssl --disable-ipv6 --enable-esi
> > --enable-kill-parent-hack --enable-snmp --with-pthreads
> > --with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
> > --enable-storeio=ufs,aufs,diskd,rock
> >
> >
> > [root at ISN-PHC-Cache bin]# ./squidclient mgr:info
> > HTTP/1.1 403 Forbidden
> > Server: squid/3.5.2
> > Mime-Version: 1.0
> > Date: Fri, 20 Mar 2015 02:29:53 GMT
> > Content-Type: text/html;charset=utf-8
> > Content-Length: 3552
> > X-Squid-Error: ERR_ACCESS_DENIED 0
> > Vary: Accept-Language
> > Content-Language: en
> > X-Cache: MISS from isn-phc-cache
> > Via: 1.1 isn-phc-cache (squid/3.5.2)
> > Connection: close
> >
>
> Well the request is getting through to the proxy "isn-phc-cache". But
> for some reason its being rejected.
>
> You have this:
> >
> > # Only allow cachemgr access from localhost
> > http_access allow localhost manager
> > http_access deny manager
> >
>
> So it should be allowed. You are running squidclient connecting to the
> default (localhost:3128) proxy server.
>
> Use -v option to get more verbose output from squidclient about what its
> doing. You can repeat the option several times to get more detailed
> debug info.
>
> The other thing to watch out for is whether ./squidclient run from that
> /bin directory is the 3.5.2 version or not. If its from an older version
> of Squid it wont have all the options the new ones do.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/5cd388df/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 20 16:55:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Mar 2015 05:55:30 +1300
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000001d06362$64bff170$2e3fd450$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
Message-ID: <550C5102.7020402@treenet.co.nz>

On 21/03/2015 12:05 p.m., snakeeyes wrote:
> Hi amos , thanks for reply
> I have tried @ top of squidf.conf
> 
> acl images rep_header Content-Type ^image/ ^x-image/  
> acl small rep_header Content-Length ^[1234]?[0-9]$  
> http_reply_access deny small images
> 
> are you sure that its blocking images with size >40KB ????

Sorry I slightly mis-read your request. What I gave is blocking images
*smaller* than 40 bytes (see what I mean about cut-n-paste without
understanding?).

To block images *over* 40 bytes change that to:
 http_reply_access deny !small images


> also I didn?t see extensions like jpg or bmp or similar like that ??!!

Because HTTP does not transfer files. It transfers data. Sometimes data
can *also* be found inside "files", sometimes not.

HTTP Content-Type header describes what format the data is. In this case
you requested images in general, so thats the pattern I gave.

Amos



From squid3 at treenet.co.nz  Fri Mar 20 17:00:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Mar 2015 06:00:48 +1300
Subject: [squid-users] How to run squidclient
In-Reply-To: <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
 <550C4747.3060508@treenet.co.nz>
 <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>
Message-ID: <550C5240.30606@treenet.co.nz>

Interesting.

I wonder if your Squid is resolving "localhost" domain name as ::1 and
rejecting it because IPv6 is disabled, therefore not permitted. Or if
its the domain name not matching the proxy name.

Try adding "-j isn-phc-cache" which sets the Host: header to match what
the cache thinks its public domain name is.

Amos



From monahbaki at gmail.com  Fri Mar 20 17:15:20 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 20 Mar 2015 13:15:20 -0400
Subject: [squid-users] How to run squidclient
In-Reply-To: <550C5240.30606@treenet.co.nz>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
 <550C4747.3060508@treenet.co.nz>
 <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>
 <550C5240.30606@treenet.co.nz>
Message-ID: <CALP3=x_dCv-+T3Xkbj2APd2jeK6knRpg17SYr455vbEJKD5Drg@mail.gmail.com>

Regarding DNS lookup, if I type nslookup 10.0.0.24 or nslookup
isn-phc-cache,
Our nameservers in /etc/resolv.conf are google's name server

Do I need to resolve first to use squidclient???



[root at ISN-PHC-Cache bin]# ./squidclient -vv -j isn-phc-cache mgr:info
verbosity level set to 2
Request:
GET cache_object://localhost/info HTTP/1.0
Host: isn-phc-cache
User-Agent: squidclient/3.5.2
Accept: */*
Connection: close


.
Transport detected: IPv4-only
Resolving localhost ...
Connecting... localhost (127.0.0.1:3128)
Connected to: localhost (127.0.0.1:3128)
Sending HTTP request ...
done.
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 18:11:21 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3553
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2015 The Squid Software
Foundation and contributors">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--
 /*
 * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
 *
 * Squid software is distributed under GPLv2+ license and includes
 * contributions from numerous individuals and organizations.
 * Please see the COPYING and CONTRIBUTORS files for details.
 */

/*
 Stylesheet for Squid Error pages
 Adapted from design by Free CSS Templates
 http://www.freecsstemplates.org
 Released for free under a Creative Commons Attribution 2.5 License
*/

/* Page basics */
* {
    font-family: verdana, sans-serif;
}

html body {
    margin: 0;
    padding: 0;
    background: #efefef;
    font-size: 12px;
    color: #1e1e1e;
}

/* Page displayed title area */
#titles {
    margin-left: 15px;
    padding: 10px;
    padding-left: 100px;
    background: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat
left;
}

/* initial title */
#titles h1 {
    color: #000000;
}
#titles h2 {
    color: #000000;
}

/* special event: FTP success page titles */
#titles ftpsuccess {
    background-color:#00ff00;
    width:100%;
}

/* Page displayed body content area */
#content {
    padding: 10px;
    background: #ffffff;
}

/* General text */
p {
}

/* error brief description */
#error p {
}

/* some data which may have caused the problem */
#data {
}

/* the error message received from the system or other software */
#sysmsg {
}

pre {
    font-family:sans-serif;
}

/* special event: FTP / Gopher directory listing */
#dirmsg {
    font-family: courier;
    color: black;
    font-size: 10pt;
}
#dirlisting {
    margin-left: 2%;
    margin-right: 2%;
}
#dirlisting tr.entry td.icon,td.filename,td.size,td.date {
    border-bottom: groove;
}
#dirlisting td.size {
    width: 50px;
    text-align: right;
    padding-right: 5px;
}

/* horizontal lines */
hr {
    margin: 0;
}

/* page displayed footer area */
#footer {
    font-size: 9px;
    padding-left: 10px;
}


body
:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya,
sans-serif; float: right; }
:lang(he) { direction: rtl; }
 --></style>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="cache_object://localhost/info">cache_object://localhost/info</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is <a href="mailto:webmaster
?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20isn-phc-cache%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2020%20Mar%202015%2018%3A11%3A21%20GMT%0D%0A%0D%0AClientIP%3A%2010.0.0.24%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Finfo%20HTTP%2F1.0%0AHost%3A%20isn-phc-cache%0D%0AUser-Agent%3A%20squidclient%2F3.5.2%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20close%0D%0A%0D%0A%0D%0A">webmaster</a>.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Fri, 20 Mar 2015 18:11:21 GMT by isn-phc-cache
(squid/3.5.2)</p>
<!-- ERR_ACCESS_DENIED -->
</div>
</body></html>


On Fri, Mar 20, 2015 at 1:00 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Interesting.
>
> I wonder if your Squid is resolving "localhost" domain name as ::1 and
> rejecting it because IPv6 is disabled, therefore not permitted. Or if
> its the domain name not matching the proxy name.
>
> Try adding "-j isn-phc-cache" which sets the Host: header to match what
> the cache thinks its public domain name is.
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/4ad4911a/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 20 18:25:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Mar 2015 07:25:31 +1300
Subject: [squid-users] How to run squidclient
In-Reply-To: <CALP3=x_dCv-+T3Xkbj2APd2jeK6knRpg17SYr455vbEJKD5Drg@mail.gmail.com>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
 <550C4747.3060508@treenet.co.nz>
 <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>
 <550C5240.30606@treenet.co.nz>
 <CALP3=x_dCv-+T3Xkbj2APd2jeK6knRpg17SYr455vbEJKD5Drg@mail.gmail.com>
Message-ID: <550C661B.5040308@treenet.co.nz>

On 21/03/2015 6:15 a.m., Monah Baki wrote:
> Regarding DNS lookup, if I type nslookup 10.0.0.24 or nslookup
> isn-phc-cache,
> Our nameservers in /etc/resolv.conf are google's name server
> 
> Do I need to resolve first to use squidclient???
> 

No, the squidclient resolving is done as you saw in its output and gets
the right IPv4-only and 127.0.0.1.

The problem will appear later when you view error messages or directory
listings generated by Squid. All the icons and generated URLs will be
using that "isn-phc-cache" as their domain.


I'm not exactly sure what the problem is. Your config is pretty much
default and I dont hit this on my test proxies.

Please try these (mind the wrap):

 squidclient -j isn-phc-cache:3128 cache_object://isn-phc-cache:3128/info

 squidclient -j isn-phc-cache:3128
http://isn-phc-cache:3128/squid-internal-mgr/info

 squidclient -j isn-phc-cache:3128
http://isn-phc-cache/squid-internal-mgr/info

Amos



From monahbaki at gmail.com  Fri Mar 20 18:53:28 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 20 Mar 2015 14:53:28 -0400
Subject: [squid-users] How to run squidclient
In-Reply-To: <550C661B.5040308@treenet.co.nz>
References: <CALP3=x9J7GpP3DEqyeKhHqYjdwt96b7Pra+4418-0RyKZKNxEQ@mail.gmail.com>
 <550C4747.3060508@treenet.co.nz>
 <CALP3=x_mV1+f=5bcEmYnOeU5y3NCJwaaCcmpLnfPMWoEZ-59Vw@mail.gmail.com>
 <550C5240.30606@treenet.co.nz>
 <CALP3=x_dCv-+T3Xkbj2APd2jeK6knRpg17SYr455vbEJKD5Drg@mail.gmail.com>
 <550C661B.5040308@treenet.co.nz>
Message-ID: <CALP3=x9y-HFf7L4SqZSJX+Ja5Wff777QY89zW-=zBK+Df=XvPw@mail.gmail.com>

[support at ISN-PHC-Cache bin]$ ./squidclient -j isn-phc-cache:3128
cache_object://isn-phc-cache:3128/info
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 19:46:58 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3568
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2015 The Squid Software
Foundation and contributors">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--
 /*
 * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
 *
 * Squid software is distributed under GPLv2+ license and includes
 * contributions from numerous individuals and organizations.
 * Please see the COPYING and CONTRIBUTORS files for details.
 */

/*
 Stylesheet for Squid Error pages
 Adapted from design by Free CSS Templates
 http://www.freecsstemplates.org
 Released for free under a Creative Commons Attribution 2.5 License
*/

/* Page basics */
* {
    font-family: verdana, sans-serif;
}

html body {
    margin: 0;
    padding: 0;
    background: #efefef;
    font-size: 12px;
    color: #1e1e1e;
}

/* Page displayed title area */
#titles {
    margin-left: 15px;
    padding: 10px;
    padding-left: 100px;
    background: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat
left;
}

/* initial title */
#titles h1 {
    color: #000000;
}
#titles h2 {
    color: #000000;
}

/* special event: FTP success page titles */
#titles ftpsuccess {
    background-color:#00ff00;
    width:100%;
}

/* Page displayed body content area */
#content {
    padding: 10px;
    background: #ffffff;
}

/* General text */
p {
}

/* error brief description */
#error p {
}

/* some data which may have caused the problem */
#data {
}

/* the error message received from the system or other software */
#sysmsg {
}

pre {
    font-family:sans-serif;
}

/* special event: FTP / Gopher directory listing */
#dirmsg {
    font-family: courier;
    color: black;
    font-size: 10pt;
}
#dirlisting {
    margin-left: 2%;
    margin-right: 2%;
}
#dirlisting tr.entry td.icon,td.filename,td.size,td.date {
    border-bottom: groove;
}
#dirlisting td.size {
    width: 50px;
    text-align: right;
    padding-right: 5px;
}

/* horizontal lines */
hr {
    margin: 0;
}

/* page displayed footer area */
#footer {
    font-size: 9px;
    padding-left: 10px;
}


body
:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya,
sans-serif; float: right; }
:lang(he) { direction: rtl; }
 --></style>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="cache_object://isn-phc-cache/info">cache_object://isn-phc-cache/info</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is <a href="mailto:webmaster
?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20isn-phc-cache%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2020%20Mar%202015%2019%3A46%3A58%20GMT%0D%0A%0D%0AClientIP%3A%2010.0.0.24%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Finfo%20HTTP%2F1.0%0AHost%3A%20isn-phc-cache%3A3128%0D%0AUser-Agent%3A%20squidclient%2F3.5.2%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20close%0D%0A%0D%0A%0D%0A">webmaster</a>.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Fri, 20 Mar 2015 19:46:58 GMT by isn-phc-cache
(squid/3.5.2)</p>
<!-- ERR_ACCESS_DENIED -->
</div>
</body></html>

==================================

[support at ISN-PHC-Cache bin]$ ./squidclient -j isn-phc-cache:3128
http://isn-phc-cache:3128/squid-internal-mgr/info
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 19:47:44 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3621
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2015 The Squid Software
Foundation and contributors">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--
 /*
 * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
 *
 * Squid software is distributed under GPLv2+ license and includes
 * contributions from numerous individuals and organizations.
 * Please see the COPYING and CONTRIBUTORS files for details.
 */

/*
 Stylesheet for Squid Error pages
 Adapted from design by Free CSS Templates
 http://www.freecsstemplates.org
 Released for free under a Creative Commons Attribution 2.5 License
*/

/* Page basics */
* {
    font-family: verdana, sans-serif;
}

html body {
    margin: 0;
    padding: 0;
    background: #efefef;
    font-size: 12px;
    color: #1e1e1e;
}

/* Page displayed title area */
#titles {
    margin-left: 15px;
    padding: 10px;
    padding-left: 100px;
    background: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat
left;
}

/* initial title */
#titles h1 {
    color: #000000;
}
#titles h2 {
    color: #000000;
}

/* special event: FTP success page titles */
#titles ftpsuccess {
    background-color:#00ff00;
    width:100%;
}

/* Page displayed body content area */
#content {
    padding: 10px;
    background: #ffffff;
}

/* General text */
p {
}

/* error brief description */
#error p {
}

/* some data which may have caused the problem */
#data {
}

/* the error message received from the system or other software */
#sysmsg {
}

pre {
    font-family:sans-serif;
}

/* special event: FTP / Gopher directory listing */
#dirmsg {
    font-family: courier;
    color: black;
    font-size: 10pt;
}
#dirlisting {
    margin-left: 2%;
    margin-right: 2%;
}
#dirlisting tr.entry td.icon,td.filename,td.size,td.date {
    border-bottom: groove;
}
#dirlisting td.size {
    width: 50px;
    text-align: right;
    padding-right: 5px;
}

/* horizontal lines */
hr {
    margin: 0;
}

/* page displayed footer area */
#footer {
    font-size: 9px;
    padding-left: 10px;
}


body
:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya,
sans-serif; float: right; }
:lang(he) { direction: rtl; }
 --></style>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="http://isn-phc-cache:3128/squid-internal-mgr/info">
http://isn-phc-cache:3128/squid-internal-mgr/info</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is <a href="mailto:webmaster
?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20isn-phc-cache%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2020%20Mar%202015%2019%3A47%3A44%20GMT%0D%0A%0D%0AClientIP%3A%2010.0.0.24%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fsquid-internal-mgr%2Finfo%20HTTP%2F1.0%0AHost%3A%20isn-phc-cache%3A3128%0D%0AUser-Agent%3A%20squidclient%2F3.5.2%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20close%0D%0A%0D%0A%0D%0A">webmaster</a>.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Fri, 20 Mar 2015 19:47:44 GMT by isn-phc-cache
(squid/3.5.2)</p>
<!-- ERR_ACCESS_DENIED -->
</div>
</body></html>



=====================================

[support at ISN-PHC-Cache bin]$ ./squidclient -j isn-phc-cache:3128
http://isn-phc-cache/squid-internal-mgr/info
HTTP/1.1 403 Forbidden
Server: squid/3.5.2
Mime-Version: 1.0
Date: Fri, 20 Mar 2015 19:48:05 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3611
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from isn-phc-cache
Via: 1.1 isn-phc-cache (squid/3.5.2)
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "
http://www.w3.org/TR/html4/strict.dtd">
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2015 The Squid Software
Foundation and contributors">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--
 /*
 * Copyright (C) 1996-2015 The Squid Software Foundation and contributors
 *
 * Squid software is distributed under GPLv2+ license and includes
 * contributions from numerous individuals and organizations.
 * Please see the COPYING and CONTRIBUTORS files for details.
 */

/*
 Stylesheet for Squid Error pages
 Adapted from design by Free CSS Templates
 http://www.freecsstemplates.org
 Released for free under a Creative Commons Attribution 2.5 License
*/

/* Page basics */
* {
    font-family: verdana, sans-serif;
}

html body {
    margin: 0;
    padding: 0;
    background: #efefef;
    font-size: 12px;
    color: #1e1e1e;
}

/* Page displayed title area */
#titles {
    margin-left: 15px;
    padding: 10px;
    padding-left: 100px;
    background: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat
left;
}

/* initial title */
#titles h1 {
    color: #000000;
}
#titles h2 {
    color: #000000;
}

/* special event: FTP success page titles */
#titles ftpsuccess {
    background-color:#00ff00;
    width:100%;
}

/* Page displayed body content area */
#content {
    padding: 10px;
    background: #ffffff;
}

/* General text */
p {
}

/* error brief description */
#error p {
}

/* some data which may have caused the problem */
#data {
}

/* the error message received from the system or other software */
#sysmsg {
}

pre {
    font-family:sans-serif;
}

/* special event: FTP / Gopher directory listing */
#dirmsg {
    font-family: courier;
    color: black;
    font-size: 10pt;
}
#dirlisting {
    margin-left: 2%;
    margin-right: 2%;
}
#dirlisting tr.entry td.icon,td.filename,td.size,td.date {
    border-bottom: groove;
}
#dirlisting td.size {
    width: 50px;
    text-align: right;
    padding-right: 5px;
}

/* horizontal lines */
hr {
    margin: 0;
}

/* page displayed footer area */
#footer {
    font-size: 9px;
    padding-left: 10px;
}


body
:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya,
sans-serif; float: right; }
:lang(he) { direction: rtl; }
 --></style>
</head><body id=ERR_ACCESS_DENIED>
<div id="titles">
<h1>ERROR</h1>
<h2>The requested URL could not be retrieved</h2>
</div>
<hr>

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="http://isn-phc-cache/squid-internal-mgr/info">
http://isn-phc-cache/squid-internal-mgr/info</a></p>

<blockquote id="error">
<p><b>Access Denied.</b></p>
</blockquote>

<p>Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.</p>

<p>Your cache administrator is <a href="mailto:webmaster
?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20isn-phc-cache%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2020%20Mar%202015%2019%3A48%3A05%20GMT%0D%0A%0D%0AClientIP%3A%2010.0.0.24%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fsquid-internal-mgr%2Finfo%20HTTP%2F1.0%0AHost%3A%20isn-phc-cache%3A3128%0D%0AUser-Agent%3A%20squidclient%2F3.5.2%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20close%0D%0A%0D%0A%0D%0A">webmaster</a>.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Fri, 20 Mar 2015 19:48:05 GMT by isn-phc-cache
(squid/3.5.2)</p>
<!-- ERR_ACCESS_DENIED -->
</div>
</body></html>


On Fri, Mar 20, 2015 at 2:25 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/03/2015 6:15 a.m., Monah Baki wrote:
> > Regarding DNS lookup, if I type nslookup 10.0.0.24 or nslookup
> > isn-phc-cache,
> > Our nameservers in /etc/resolv.conf are google's name server
> >
> > Do I need to resolve first to use squidclient???
> >
>
> No, the squidclient resolving is done as you saw in its output and gets
> the right IPv4-only and 127.0.0.1.
>
> The problem will appear later when you view error messages or directory
> listings generated by Squid. All the icons and generated URLs will be
> using that "isn-phc-cache" as their domain.
>
>
> I'm not exactly sure what the problem is. Your config is pretty much
> default and I dont hit this on my test proxies.
>
> Please try these (mind the wrap):
>
>  squidclient -j isn-phc-cache:3128 cache_object://isn-phc-cache:3128/info
>
>  squidclient -j isn-phc-cache:3128
> http://isn-phc-cache:3128/squid-internal-mgr/info
>
>  squidclient -j isn-phc-cache:3128
> http://isn-phc-cache/squid-internal-mgr/info
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150320/673b6072/attachment.htm>

From ahmed.zaeem at netstream.ps  Sat Mar 21 05:04:44 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Fri, 20 Mar 2015 22:04:44 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <550C5102.7020402@treenet.co.nz>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
Message-ID: <001a01d06394$8c26ff60$a474fe20$@netstream.ps>

Hi Amos , thanks  it seems okay for normal http sites

I want to ask , is there a trick can we do it so that it be applied to google & yahoo images search ??

Here is wt I see in yahoo logs , just small logs and all images are allowed and not blocked
=====

1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT js.dmtry.com:443 - DIRECT/184.170.128.58 -
1426881749.077    103 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881749.752     29 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881750.098     21 xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? - DIRECT/23.21.149.132 image/gif
1426881750.731  62443 xx.70 TCP_MISS/200 122185 CONNECT www.gstatic.com:443 - DIRECT/206.126.112.185 -
1426881751.476   xx.70 TCP_MISS/200 4191 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881753.005   1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 - DIRECT/76.13.28.21 -
1426881762.280  12994 188.161.107.70 TCP_MISS/200 5502 CONNECT d.adgear.com:443 - DIRECT/205.204.71.140 -
1426881764.215  16497 xx70 TCP_MISS/200 9832 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881764.216  16453 x.70 TCP_MISS/200 6534 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.044  18777 x.70 TCP_MISS/200 11132 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.681  15193 xx.107.70 TCP_MISS/200 6225 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881765.691  14149 xx.107.70 TCP_MISS/200 832 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881766.046 116219 xx.70 TCP_MISS/200 529 CONNECT d31qbv1cthcecs.cloudfront.net:443 - DIRECT/54.230.16.189 -
1426881766.714    296 xx.70 TCP_MISS/200 2228 POST http://ocsp.verisign.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881770.049 117609 xx107.70 TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 - DIRECT/54.240.160.97 -
1426881780.403  67786 xx.70 TCP_MISS/200 852 CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881781.519    353 xx.70 TCP_MISS/200 571 GET http://data.cnn.com/jsonp/breaking_news/domestic.json? - DIRECT/157.166.249.67 application/javascript
1426881782.057 118788 xx.70 TCP_MISS/200 19972 CONNECT cdn2sitescout-a.akamaihd.net:443 - DIRECT/23.15.4.18 -
1426881790.558  71055 xx TCP_MISS/200 26805 CONNECT s.yimg.com:443 - DIRECT/206.190.56.191 -
1426881814.445 100461 xx TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881818.437  70709 xx70 TCP_MISS/200 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -


regards

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Friday, March 20, 2015 9:56 AM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 21/03/2015 12:05 p.m., snakeeyes wrote:
> Hi amos , thanks for reply
> I have tried @ top of squidf.conf
> 
> acl images rep_header Content-Type ^image/ ^x-image/ acl small 
> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny small 
> images
> 
> are you sure that its blocking images with size >40KB ????

Sorry I slightly mis-read your request. What I gave is blocking images
*smaller* than 40 bytes (see what I mean about cut-n-paste without understanding?).

To block images *over* 40 bytes change that to:
 http_reply_access deny !small images


> also I didn?t see extensions like jpg or bmp or similar like that ??!!

Because HTTP does not transfer files. It transfers data. Sometimes data can *also* be found inside "files", sometimes not.

HTTP Content-Type header describes what format the data is. In this case you requested images in general, so thats the pattern I gave.

Amos



From rafael.akchurin at diladele.com  Fri Mar 20 20:09:31 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 20 Mar 2015 20:09:31 +0000
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <001a01d06394$8c26ff60$a474fe20$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <001a01d06394$8c26ff60$a474fe20$@netstream.ps>
Message-ID: <DB5PR04MB1128F372872E3809973E36218F0E0@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hello Snakeeyes,

Please also take into account the fact that Google image search results are "inlined" (at least thumbnails).

Best regards,
Rafael

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of snakeeyes
Sent: Saturday, March 21, 2015 6:05 AM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

Hi Amos , thanks  it seems okay for normal http sites

I want to ask , is there a trick can we do it so that it be applied to google & yahoo images search ??

Here is wt I see in yahoo logs , just small logs and all images are allowed and not blocked =====

1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT js.dmtry.com:443 - DIRECT/184.170.128.58 -
1426881749.077    103 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881749.752     29 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881750.098     21 xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? - DIRECT/23.21.149.132 image/gif
1426881750.731  62443 xx.70 TCP_MISS/200 122185 CONNECT www.gstatic.com:443 - DIRECT/206.126.112.185 -
1426881751.476   xx.70 TCP_MISS/200 4191 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881753.005   1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 - DIRECT/76.13.28.21 -
1426881762.280  12994 188.161.107.70 TCP_MISS/200 5502 CONNECT d.adgear.com:443 - DIRECT/205.204.71.140 -
1426881764.215  16497 xx70 TCP_MISS/200 9832 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881764.216  16453 x.70 TCP_MISS/200 6534 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.044  18777 x.70 TCP_MISS/200 11132 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.681  15193 xx.107.70 TCP_MISS/200 6225 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881765.691  14149 xx.107.70 TCP_MISS/200 832 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881766.046 116219 xx.70 TCP_MISS/200 529 CONNECT d31qbv1cthcecs.cloudfront.net:443 - DIRECT/54.230.16.189 -
1426881766.714    296 xx.70 TCP_MISS/200 2228 POST http://ocsp.verisign.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881770.049 117609 xx107.70 TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 - DIRECT/54.240.160.97 -
1426881780.403  67786 xx.70 TCP_MISS/200 852 CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881781.519    353 xx.70 TCP_MISS/200 571 GET http://data.cnn.com/jsonp/breaking_news/domestic.json? - DIRECT/157.166.249.67 application/javascript
1426881782.057 118788 xx.70 TCP_MISS/200 19972 CONNECT cdn2sitescout-a.akamaihd.net:443 - DIRECT/23.15.4.18 -
1426881790.558  71055 xx TCP_MISS/200 26805 CONNECT s.yimg.com:443 - DIRECT/206.190.56.191 -
1426881814.445 100461 xx TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881818.437  70709 xx70 TCP_MISS/200 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -


regards

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Friday, March 20, 2015 9:56 AM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 21/03/2015 12:05 p.m., snakeeyes wrote:
> Hi amos , thanks for reply
> I have tried @ top of squidf.conf
> 
> acl images rep_header Content-Type ^image/ ^x-image/ acl small 
> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny small 
> images
> 
> are you sure that its blocking images with size >40KB ????

Sorry I slightly mis-read your request. What I gave is blocking images
*smaller* than 40 bytes (see what I mean about cut-n-paste without understanding?).

To block images *over* 40 bytes change that to:
 http_reply_access deny !small images


> also I didn?t see extensions like jpg or bmp or similar like that ??!!

Because HTTP does not transfer files. It transfers data. Sometimes data can *also* be found inside "files", sometimes not.

HTTP Content-Type header describes what format the data is. In this case you requested images in general, so thats the pattern I gave.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From eliezer at ngtech.co.il  Sat Mar 21 17:46:56 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 21 Mar 2015 19:46:56 +0200
Subject: [squid-users] WARNING: 1 swapin MD5 mismatches and BUG 3279:
 HTTP reply without Date:
In-Reply-To: <CAMZauGrPSHHn8AmpyaxNFUsHeSCzsohjAfM3Eo1eQxd8cesqTQ@mail.gmail.com>
References: <550AB4DE.7040304@yahoo.com>
 <CAN8nrKDVo2W_fFG-wB3uiiJLU5O1T7Kxb17_b2ZPgfkhpq7OOg@mail.gmail.com>
 <550B89B8.7010107@ngtech.co.il>
 <CAN8nrKBKNA1hSfSdNC4EaDyGT6z5V-uT1FtLq_P+NDn2pqKQOg@mail.gmail.com>
 <CAMZauGrPSHHn8AmpyaxNFUsHeSCzsohjAfM3Eo1eQxd8cesqTQ@mail.gmail.com>
Message-ID: <550DAE90.30201@ngtech.co.il>

Alberto,

What are the details of the machine?

Can you run the next script on the machine?
http://ngtech.co.il/squid/basic_data.sh

Eliezer

On 20/03/2015 05:37, Alberto Perez wrote:
> Another one here not using SMP, and using aufs.
>
> I stopped seen this issue frequently when I reduced my cache size,
> from 70 GB to 30 GB now.
>
> Regards



From hack.back at hotmail.com  Sat Mar 21 22:48:15 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 21 Mar 2015 15:48:15 -0700 (PDT)
Subject: [squid-users] cant register with http://bugs.squid-cache.org
Message-ID: <1426978095610-4670556.post@n4.nabble.com>

i try to register to report a bug but i got this error msg

You submitted changes to createaccount.cgi with an invalid token, which may
indicate that someone tried to abuse you, for instance by making you click
on a URL which redirected you here without your consent.

Are you sure you want to commit these changes? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cant-register-with-http-bugs-squid-cache-org-tp4670556.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From pieter at insync.za.net  Tue Mar 24 17:19:15 2015
From: pieter at insync.za.net (Pieter De Wit)
Date: Wed, 25 Mar 2015 06:19:15 +1300
Subject: [squid-users] Testing - please ignore
Message-ID: <81a7ebc2ae995e991b9a77545b4d27ef@insync.za.net>

 

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150325/45a45dfc/attachment.htm>

From yvoinov at gmail.com  Tue Mar 24 18:18:02 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 00:18:02 +0600
Subject: [squid-users] Testing - please ignore
In-Reply-To: <81a7ebc2ae995e991b9a77545b4d27ef@insync.za.net>
References: <81a7ebc2ae995e991b9a77545b4d27ef@insync.za.net>
Message-ID: <5511AA5A.1020701@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Pong :)

24.03.15 23:19, Pieter De Wit ?????:
> 
> 
> 
> 
> 
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEapaAAoJENNXIZxhPexGiBEH/3JZjBb6dq5NlHyb+aWlmY+3
rFHq++uqFSTz6XlUowz25332gqJCxrAILyB0Qt/aRLJzLuu3lwYxg47YUr4KICMG
hcBDRzgZYLVm36dytC91KUgiLdtkQ0jB1o0VgdqinOpGYpzJ8dQx/UjQSHrcnvCZ
8MV2NOCO2oDJF0x4Ov5KxCrENiWbQCvp2RBljjDeMHHeWL9BIOFld29o6vA/Rv0U
SGkX1jtJ29U9SdLSeWgxJgWzEkqV/kikfrOYIlpF/sOtf2aWiRF8lTA6jQ+odIE7
EseYjFeIDjubLqXKQGGSQgLd+6nYh1iHFx3zffhQ9IwimSs7BV7PaY06pMtjYb8=
=4nQw
-----END PGP SIGNATURE-----


From bpk678 at gmail.com  Tue Mar 24 18:25:30 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Tue, 24 Mar 2015 14:25:30 -0400
Subject: [squid-users] load balancing and site failover
In-Reply-To: <1427206737.17590.33.camel@desktop.bpk2.com>
References: <1427206737.17590.33.camel@desktop.bpk2.com>
Message-ID: <1427221530.17590.37.camel@desktop.bpk2.com>

On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
> while load balancing is not a requirement in a proxy environment, it
> does afford a great deal of functionality, scaling and fault tolerance
> in one.  several if not many on this list probably employ them for their
> proxies and likely other technologies, but they are not all created
> equal.
> 
> i recently looked to see if a specific feature was in HAProxy.  i was
> looking to see if HAProxy could reply to a new connection with a RST
> packet if no pool member was available.
> 
> the idea behind this is, if all of the proxies are not passing the
> service check and are marked down by the load balancer, the reply of a
> RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
> tells the browser to failover to the next proxy assigned by the PAC
> file.
> 
> where i work, we have this configuration working.  the load balancers
> are configured with the option to send a reset when no proxy is
> available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
> a specific order based on which proxy VIP is assigned as the primary.
> In every case, if the primary VIP does not have an available pool
> member, the browser fails over to the next in the list.  failover would
> happen again, if the secondary VIP replies with a RST during the
> connection establishing.  the process repeats until a TCP connection
> establishes or all proxies assigned have been exhausted.  the browser
> will use the proxy VIP that it successfully connects to, for the
> duration of the session.  once the browser is closed and reopened, the
> evaluation of the PAC file occurs again, and the process starts anew.
> plug-ins such as Proxy Selector are the exception to this, and can be
> used to reevaluate a PAC file by selecting it for use.
> 
> we have used this configuration several times, when we found an ISP link
> was flapping or some other issue more global in nature than just the
> proxies was affecting our egress and internet access.  i can attest to
> the solution as working and elegantly handling site wide failures.
> 
> being that the solutions where i work are proprietary commercial
> products, i wanted to find an open source product that does this.  i
> have been a long time user of HAProxy, and have recommended it for
> others here, but sadly they cannot perform this function.  per their
> mailing list, they use the network stack of the OS for connection
> establishment and cannot cause a RST to be sent to the client during a
> TCP handshake if no pool member is available.
> 
> they suggested an external helper that manipulates IPTables rules based
> on a pool member being available.  they do not feel that a feature like
> this belongs in a layer 4/7 reverse proxy application.
> 
> my search for a load balancer solution went through ipvsadm, balance and
> haproxy before i selected haproxy.  haproxy was more feature rich than
> balance, and easier to implement than ipvsadm.  do any other list
> members have a need for such a feature from their load balancers?  do
> any other list members have site failover solutions that have been
> tested or used and would consider sharing their design and/or pain
> points?  i am not looking for secret sauce or confidential info, but
> more high level architecture decisions and such.
> 

trying to send this again, as it was rejected previously.



From monahbaki at gmail.com  Tue Mar 24 19:18:15 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Tue, 24 Mar 2015 15:18:15 -0400
Subject: [squid-users] I am seeing the following in my cache.log
Message-ID: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>

Running squid 3.5.2 on Centos 6.6

./configure --prefix=/home/cache --enable-follow-x-forwarded-for
--with-large-files --enable-ssl --disable-ipv6 --enable-esi
--enable-kill-parent-hack --enable-snmp --with-pthreads
--with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
--enable-storeio=ufs,aufs,diskd,rock

We have around 50 users. I am seeing hundreds of thousands of the following:


2015/03/24 14:57:34.910| SECURITY ALERT: By user agent: Mozilla/5.0
(Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
Chrome/20.0.1092.0 Safari/536.6
2015/03/24 14:57:34.910| SECURITY ALERT: on URL: www.facebook.com:443
2015/03/24 14:57:34.946| SECURITY ALERT: Host header forgery detected
on local=85.115.52.158:80 remote=196.245.252.34:36732 FD 49 flags=33
(local IP does not match any domain IP)


Then after 2 hours, I get the message in my cacahe.log:

2015/03/24 16:41:42.478| SECURITY ALERT: By user agent: Mozilla/5.0
(Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
Chrome/20.0.1092.0 Safari/536.6
2015/03/24 16:41:42.478| SECURITY ALERT: on URL: www.facebook.com:443
2015/03/24 16:41:42.478| WARNING: 1 swapin MD5 mismatches
2015/03/24 16:41:42.478| Could not parse headers from on disk object
2015/03/24 16:41:42.478| BUG 3279: HTTP reply without Date:
2015/03/24 16:41:42.478| StoreEntry->key: 23F0D6046AB8FE86440CAD447524FCBC
2015/03/24 16:41:42.478| StoreEntry->next: 0
2015/03/24 16:41:42.478| StoreEntry->mem_obj: 0x1d56470
2015/03/24 16:41:42.478| StoreEntry->timestamp: -1
2015/03/24 16:41:42.478| StoreEntry->lastref: 1427211702
2015/03/24 16:41:42.478| StoreEntry->expires: -1
2015/03/24 16:41:42.478| StoreEntry->lastmod: -1
2015/03/24 16:41:42.478| StoreEntry->swap_file_sz: 0
2015/03/24 16:41:42.478| StoreEntry->refcount: 1
2015/03/24 16:41:42.478| StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED
2015/03/24 16:41:42.478| StoreEntry->swap_dirn: -1
2015/03/24 16:41:42.478| StoreEntry->swap_filen: -1
2015/03/24 16:41:42.478| StoreEntry->lock_count: 2
2015/03/24 16:41:42.478| StoreEntry->mem_status: 0
2015/03/24 16:41:42.478| StoreEntry->ping_status: 2
2015/03/24 16:41:42.478| StoreEntry->store_status: 1
2015/03/24 16:41:42.478| StoreEntry->swap_status: 0
2015/03/24 16:41:42.747| SECURITY ALERT: Host header forgery detected
on local=85.115.52.158:80 remote=197.255.252.34:44348 FD 20 flags=33
(local IP does not match any domain IP)
2015/03/24 16:41:42.747| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
2015/03/24 16:41:42.747| SECURITY ALERT: on URL: us-mg5.mail.yahoo.com:443
2015/03/24 16:41:42.772| SECURITY ALERT: Host header forgery detected
on local=85.115.52.158:80 remote=197.255.252.34:44349 FD 20 flags=33
(local IP does not match any domain IP)
2015/03/24 16:41:42.772| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
2015/03/24 16:41:42.772| SECURITY ALERT: on URL: csync.flickr.com:443
2015/03/24 16:41:42.800| SECURITY ALERT: Host header forgery detected
on local=85.115.33.158:80 remote=197.255.252.34:13505 FD 20 flags=33
(local IP does not match any domain IP)
2015/03/24 16:41:42.800| SECURITY ALERT: By user agent: Mozilla/5.0
(Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
Chrome/20.0.1092.0 Safari/536.6
2015/03/24 16:41:42.800| SECURITY ALERT: on URL: www.facebook.com:443
2015/03/24 16:41:43.115| SECURITY ALERT: Host header forgery detected
on local=85.115.33.158:80 remote=197.255.252.34:13506 FD 31 flags=33
(local IP does not match any domain IP)
2015/03/24 16:41:43.115| SECURITY ALERT: By user agent: Mozilla/5.0
(Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
Chrome/20.0.1092.0 Safari/536.6
2015/03/24 16:41:43.115| SECURITY ALERT: on URL: www.facebook.com:443
2015/03/24 16:41:43.115| assertion failed: store.cc:1885: "isEmpty()"


Then I get a message "running out of file descriptors", for that I did
the following:
echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range
echo 8192 > /proc/sys/net/ipv4/tcp_max_syn_backlog

In my /etc/security/limits.conf, added the following:
* - nofile 65535



My squid.conf

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines
acl blockeddomain dstdomain "/home/cache/etc/blocked.domain.acl"

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT
acl isnsnmp snmp_community public

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

cachemgr_passwd password all

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access deny blockeddomain
http_access allow localnet
http_access allow localhost
snmp_access allow isnsnmp localnet

# And finally deny all other access to this proxy
http_access deny all
# snmp_access deny all

# Squid normally listens to port 3128
http_port 3128
http_port 3129 intercept
snmp_port 3401

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
cache_dir aufs /home/cache/var/cache/squid 350000 16 256

# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid

access_log daemon:/home/cache/var/logs/access.log squid
cache_log /home/cache/var/logs/cache.log


#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

half_closed_clients off
# quick_abort_min 0 KB
# quick_abort_max 0 KB
# vary_ignore_expire on
# reload_into_ims on
# memory_pools off
cache_mem 9216 MB
memory_cache_mode always
client_persistent_connections off
server_persistent_connections off
visible_hostname isn-phc-cache
minimum_object_size 0 KB
maximum_object_size 96 MB
maximum_object_size_in_memory 1 MB
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
quick_abort_min 1024 KB
quick_abort_max 2048 KB
quick_abort_pct 90
ipcache_size 10240
# ipcache_low 90
# ipcache_high 95
cache_swap_low 98
cache_swap_high 100
# fqdncache_size 16384
# retry_on_error on
# offline_mode off
logfile_rotate 10
dns_nameservers 8.8.8.8 41.78.211.30




Was the thousands of thousands of SECURITY ALERT the cause of this?


Thanks
Monah


From yvoinov at gmail.com  Tue Mar 24 19:24:08 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 01:24:08 +0600
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
Message-ID: <5511B9D8.3040607@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Feel free fo look at this:

http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery


25.03.15 1:18, Monah Baki ?????:
> Running squid 3.5.2 on Centos 6.6
> 
> ./configure --prefix=/home/cache --enable-follow-x-forwarded-for 
> --with-large-files --enable-ssl --disable-ipv6 --enable-esi 
> --enable-kill-parent-hack --enable-snmp --with-pthreads 
> --with-filedescriptors=65535 --enable-cachemgr-hostname=hostname 
> --enable-storeio=ufs,aufs,diskd,rock
> 
> We have around 50 users. I am seeing hundreds of thousands of the
> following:
> 
> 
> 2015/03/24 14:57:34.910| SECURITY ALERT: By user agent:
> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko) 
> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 14:57:34.910| SECURITY
> ALERT: on URL: www.facebook.com:443 2015/03/24 14:57:34.946|
> SECURITY ALERT: Host header forgery detected on
> local=85.115.52.158:80 remote=196.245.252.34:36732 FD 49 flags=33 
> (local IP does not match any domain IP)
> 
> 
> Then after 2 hours, I get the message in my cacahe.log:
> 
> 2015/03/24 16:41:42.478| SECURITY ALERT: By user agent:
> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko) 
> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.478| SECURITY
> ALERT: on URL: www.facebook.com:443 2015/03/24 16:41:42.478|
> WARNING: 1 swapin MD5 mismatches 2015/03/24 16:41:42.478| Could not
> parse headers from on disk object 2015/03/24 16:41:42.478| BUG
> 3279: HTTP reply without Date: 2015/03/24 16:41:42.478|
> StoreEntry->key: 23F0D6046AB8FE86440CAD447524FCBC 2015/03/24
> 16:41:42.478| StoreEntry->next: 0 2015/03/24 16:41:42.478|
> StoreEntry->mem_obj: 0x1d56470 2015/03/24 16:41:42.478|
> StoreEntry->timestamp: -1 2015/03/24 16:41:42.478|
> StoreEntry->lastref: 1427211702 2015/03/24 16:41:42.478|
> StoreEntry->expires: -1 2015/03/24 16:41:42.478|
> StoreEntry->lastmod: -1 2015/03/24 16:41:42.478|
> StoreEntry->swap_file_sz: 0 2015/03/24 16:41:42.478|
> StoreEntry->refcount: 1 2015/03/24 16:41:42.478| StoreEntry->flags:
> PRIVATE,FWD_HDR_WAIT,VALIDATED 2015/03/24 16:41:42.478|
> StoreEntry->swap_dirn: -1 2015/03/24 16:41:42.478|
> StoreEntry->swap_filen: -1 2015/03/24 16:41:42.478|
> StoreEntry->lock_count: 2 2015/03/24 16:41:42.478|
> StoreEntry->mem_status: 0 2015/03/24 16:41:42.478|
> StoreEntry->ping_status: 2 2015/03/24 16:41:42.478|
> StoreEntry->store_status: 1 2015/03/24 16:41:42.478|
> StoreEntry->swap_status: 0 2015/03/24 16:41:42.747| SECURITY ALERT:
> Host header forgery detected on local=85.115.52.158:80
> remote=197.255.252.34:44348 FD 20 flags=33 (local IP does not match
> any domain IP) 2015/03/24 16:41:42.747| SECURITY ALERT: By user
> agent: WNetCore/0.1.1.1 2015/03/24 16:41:42.747| SECURITY ALERT: on
> URL: us-mg5.mail.yahoo.com:443 2015/03/24 16:41:42.772| SECURITY
> ALERT: Host header forgery detected on local=85.115.52.158:80
> remote=197.255.252.34:44349 FD 20 flags=33 (local IP does not match
> any domain IP) 2015/03/24 16:41:42.772| SECURITY ALERT: By user
> agent: WNetCore/0.1.1.1 2015/03/24 16:41:42.772| SECURITY ALERT: on
> URL: csync.flickr.com:443 2015/03/24 16:41:42.800| SECURITY ALERT:
> Host header forgery detected on local=85.115.33.158:80
> remote=197.255.252.34:13505 FD 20 flags=33 (local IP does not match
> any domain IP) 2015/03/24 16:41:42.800| SECURITY ALERT: By user
> agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.800|
> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24
> 16:41:43.115| SECURITY ALERT: Host header forgery detected on
> local=85.115.33.158:80 remote=197.255.252.34:13506 FD 31 flags=33 
> (local IP does not match any domain IP) 2015/03/24 16:41:43.115|
> SECURITY ALERT: By user agent: Mozilla/5.0 (Windows NT 6.1)
> AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0
> Safari/536.6 2015/03/24 16:41:43.115| SECURITY ALERT: on URL:
> www.facebook.com:443 2015/03/24 16:41:43.115| assertion failed:
> store.cc:1885: "isEmpty()"
> 
> 
> Then I get a message "running out of file descriptors", for that I
> did the following: echo 1024 65535 >
> /proc/sys/net/ipv4/ip_local_port_range echo 8192 >
> /proc/sys/net/ipv4/tcp_max_syn_backlog
> 
> In my /etc/security/limits.conf, added the following: * - nofile
> 65535
> 
> 
> 
> My squid.conf
> 
> # # Recommended minimum configuration: #
> 
> # Example rule allowing access from your local networks. # Adapt to
> list your (internal) IP networks from where browsing # should be
> allowed acl localnet src 10.0.0.0/8    # RFC1918 possible internal
> network acl localnet src 172.16.0.0/12    # RFC1918 possible
> internal network acl localnet src 192.168.0.0/16    # RFC1918
> possible internal network acl localnet src fc00::/7       # RFC
> 4193 local private network range acl localnet src fe80::/10      #
> RFC 4291 link-local (directly plugged) machines acl blockeddomain
> dstdomain "/home/cache/etc/blocked.domain.acl"
> 
> acl SSL_ports port 443 acl Safe_ports port 80        # http acl
> Safe_ports port 21        # ftp acl Safe_ports port 443        #
> https acl Safe_ports port 70        # gopher acl Safe_ports port
> 210        # wais acl Safe_ports port 1025-65535    # unregistered
> ports acl Safe_ports port 280        # http-mgmt acl Safe_ports
> port 488        # gss-http acl Safe_ports port 591        #
> filemaker acl Safe_ports port 777        # multiling http acl
> CONNECT method CONNECT acl isnsnmp snmp_community public
> 
> # # Recommended minimum Access Permission configuration: # # Deny
> requests to certain unsafe ports http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports http_access deny
> CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost http_access allow
> localhost manager http_access deny manager
> 
> cachemgr_passwd password all
> 
> # We strongly recommend the following be uncommented to protect
> innocent # web applications running on the proxy server who think
> the only # one who can access services on "localhost" is a local
> user #http_access deny to_localhost
> 
> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
> #
> 
> # Example rule allowing access from your local networks. # Adapt
> localnet in the ACL section to list your (internal) IP networks #
> from where browsing should be allowed http_access deny
> blockeddomain http_access allow localnet http_access allow
> localhost snmp_access allow isnsnmp localnet
> 
> # And finally deny all other access to this proxy http_access deny
> all # snmp_access deny all
> 
> # Squid normally listens to port 3128 http_port 3128 http_port 3129
> intercept snmp_port 3401
> 
> # Uncomment and adjust the following to add a disk cache
> directory. #cache_dir ufs /usr/local/squid/var/cache/squid 100 16
> 256 cache_dir aufs /home/cache/var/cache/squid 350000 16 256
> 
> # Leave coredumps in the first cache dir coredump_dir
> /usr/local/squid/var/cache/squid
> 
> access_log daemon:/home/cache/var/logs/access.log squid cache_log
> /home/cache/var/logs/cache.log
> 
> 
> # # Add any of your own refresh_pattern entries above these. # 
> refresh_pattern ^ftp:        1440    20%    10080 refresh_pattern
> ^gopher:    1440    0%    1440 refresh_pattern -i (/cgi-bin/|\?) 0
> 0%    0 refresh_pattern .        0    20%    4320
> 
> half_closed_clients off # quick_abort_min 0 KB # quick_abort_max 0
> KB # vary_ignore_expire on # reload_into_ims on # memory_pools off 
> cache_mem 9216 MB memory_cache_mode always 
> client_persistent_connections off server_persistent_connections
> off visible_hostname isn-phc-cache minimum_object_size 0 KB 
> maximum_object_size 96 MB maximum_object_size_in_memory 1 MB 
> memory_replacement_policy lru cache_replacement_policy heap LFUDA 
> quick_abort_min 1024 KB quick_abort_max 2048 KB quick_abort_pct 90 
> ipcache_size 10240 # ipcache_low 90 # ipcache_high 95 
> cache_swap_low 98 cache_swap_high 100 # fqdncache_size 16384 #
> retry_on_error on # offline_mode off logfile_rotate 10 
> dns_nameservers 8.8.8.8 41.78.211.30
> 
> 
> 
> 
> Was the thousands of thousands of SECURITY ALERT the cause of
> this?
> 
> 
> Thanks Monah _______________________________________________ 
> squid-users mailing list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEbnYAAoJENNXIZxhPexGMQgIAMICUhBRZMOIl96wFRHaUz+U
mFOoHX+dSr5C6hh7mm44ZFq9pWYb8c4EEx+kp5olttpPVFh3cfpl8h8tY73DfWUD
bWojvcQA+NqQ6rIvJcSAwOJ+bWXx3fPTUefGcAOJ+gZho90DyyLxy6vDSMt/0rnV
GkpUCTT7r3w/EKQbavjRpAcnEdm0L/tSv70Tui+SlU4ksVn77yIoLJ6xD183B4U+
6Heg/x0sd2sMG6nx3452V7v5eaaQXAO3teby/VOUeRthuaFdJDTDn60j96h5Xc8Y
MSXIbk5qBecRuhVKs6vt2gWC59LYvjNnpSyCv1NyF0PWADi5QPlUVvGxtfUHfHU=
=5wYL
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Tue Mar 24 20:05:57 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Tue, 24 Mar 2015 16:05:57 -0400
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <5511B9D8.3040607@gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
Message-ID: <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>

Thanks Yuri for the URL. The company is a small ISP using policy based
routing, so using WPAD or GPO isn't feasible.

If the cause of the server running out of file descriptions and giving
the "assertion failed: store.cc:1885: "isEmpty()" error, I prefer to
inform the enduser to fix his computer.

Thanks
Monah


On Tue, Mar 24, 2015 at 3:24 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Feel free fo look at this:
>
> http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
>
>
> 25.03.15 1:18, Monah Baki ?????:
>> Running squid 3.5.2 on Centos 6.6
>>
>> ./configure --prefix=/home/cache --enable-follow-x-forwarded-for
>> --with-large-files --enable-ssl --disable-ipv6 --enable-esi
>> --enable-kill-parent-hack --enable-snmp --with-pthreads
>> --with-filedescriptors=65535 --enable-cachemgr-hostname=hostname
>> --enable-storeio=ufs,aufs,diskd,rock
>>
>> We have around 50 users. I am seeing hundreds of thousands of the
>> following:
>>
>>
>> 2015/03/24 14:57:34.910| SECURITY ALERT: By user agent:
>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 14:57:34.910| SECURITY
>> ALERT: on URL: www.facebook.com:443 2015/03/24 14:57:34.946|
>> SECURITY ALERT: Host header forgery detected on
>> local=85.115.52.158:80 remote=196.245.252.34:36732 FD 49 flags=33
>> (local IP does not match any domain IP)
>>
>>
>> Then after 2 hours, I get the message in my cacahe.log:
>>
>> 2015/03/24 16:41:42.478| SECURITY ALERT: By user agent:
>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.478| SECURITY
>> ALERT: on URL: www.facebook.com:443 2015/03/24 16:41:42.478|
>> WARNING: 1 swapin MD5 mismatches 2015/03/24 16:41:42.478| Could not
>> parse headers from on disk object 2015/03/24 16:41:42.478| BUG
>> 3279: HTTP reply without Date: 2015/03/24 16:41:42.478|
>> StoreEntry->key: 23F0D6046AB8FE86440CAD447524FCBC 2015/03/24
>> 16:41:42.478| StoreEntry->next: 0 2015/03/24 16:41:42.478|
>> StoreEntry->mem_obj: 0x1d56470 2015/03/24 16:41:42.478|
>> StoreEntry->timestamp: -1 2015/03/24 16:41:42.478|
>> StoreEntry->lastref: 1427211702 2015/03/24 16:41:42.478|
>> StoreEntry->expires: -1 2015/03/24 16:41:42.478|
>> StoreEntry->lastmod: -1 2015/03/24 16:41:42.478|
>> StoreEntry->swap_file_sz: 0 2015/03/24 16:41:42.478|
>> StoreEntry->refcount: 1 2015/03/24 16:41:42.478| StoreEntry->flags:
>> PRIVATE,FWD_HDR_WAIT,VALIDATED 2015/03/24 16:41:42.478|
>> StoreEntry->swap_dirn: -1 2015/03/24 16:41:42.478|
>> StoreEntry->swap_filen: -1 2015/03/24 16:41:42.478|
>> StoreEntry->lock_count: 2 2015/03/24 16:41:42.478|
>> StoreEntry->mem_status: 0 2015/03/24 16:41:42.478|
>> StoreEntry->ping_status: 2 2015/03/24 16:41:42.478|
>> StoreEntry->store_status: 1 2015/03/24 16:41:42.478|
>> StoreEntry->swap_status: 0 2015/03/24 16:41:42.747| SECURITY ALERT:
>> Host header forgery detected on local=85.115.52.158:80
>> remote=197.255.252.34:44348 FD 20 flags=33 (local IP does not match
>> any domain IP) 2015/03/24 16:41:42.747| SECURITY ALERT: By user
>> agent: WNetCore/0.1.1.1 2015/03/24 16:41:42.747| SECURITY ALERT: on
>> URL: us-mg5.mail.yahoo.com:443 2015/03/24 16:41:42.772| SECURITY
>> ALERT: Host header forgery detected on local=85.115.52.158:80
>> remote=197.255.252.34:44349 FD 20 flags=33 (local IP does not match
>> any domain IP) 2015/03/24 16:41:42.772| SECURITY ALERT: By user
>> agent: WNetCore/0.1.1.1 2015/03/24 16:41:42.772| SECURITY ALERT: on
>> URL: csync.flickr.com:443 2015/03/24 16:41:42.800| SECURITY ALERT:
>> Host header forgery detected on local=85.115.33.158:80
>> remote=197.255.252.34:13505 FD 20 flags=33 (local IP does not match
>> any domain IP) 2015/03/24 16:41:42.800| SECURITY ALERT: By user
>> agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
>> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.800|
>> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24
>> 16:41:43.115| SECURITY ALERT: Host header forgery detected on
>> local=85.115.33.158:80 remote=197.255.252.34:13506 FD 31 flags=33
>> (local IP does not match any domain IP) 2015/03/24 16:41:43.115|
>> SECURITY ALERT: By user agent: Mozilla/5.0 (Windows NT 6.1)
>> AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0
>> Safari/536.6 2015/03/24 16:41:43.115| SECURITY ALERT: on URL:
>> www.facebook.com:443 2015/03/24 16:41:43.115| assertion failed:
>> store.cc:1885: "isEmpty()"
>>
>>
>> Then I get a message "running out of file descriptors", for that I
>> did the following: echo 1024 65535 >
>> /proc/sys/net/ipv4/ip_local_port_range echo 8192 >
>> /proc/sys/net/ipv4/tcp_max_syn_backlog
>>
>> In my /etc/security/limits.conf, added the following: * - nofile
>> 65535
>>
>>
>>
>> My squid.conf
>>
>> # # Recommended minimum configuration: #
>>
>> # Example rule allowing access from your local networks. # Adapt to
>> list your (internal) IP networks from where browsing # should be
>> allowed acl localnet src 10.0.0.0/8    # RFC1918 possible internal
>> network acl localnet src 172.16.0.0/12    # RFC1918 possible
>> internal network acl localnet src 192.168.0.0/16    # RFC1918
>> possible internal network acl localnet src fc00::/7       # RFC
>> 4193 local private network range acl localnet src fe80::/10      #
>> RFC 4291 link-local (directly plugged) machines acl blockeddomain
>> dstdomain "/home/cache/etc/blocked.domain.acl"
>>
>> acl SSL_ports port 443 acl Safe_ports port 80        # http acl
>> Safe_ports port 21        # ftp acl Safe_ports port 443        #
>> https acl Safe_ports port 70        # gopher acl Safe_ports port
>> 210        # wais acl Safe_ports port 1025-65535    # unregistered
>> ports acl Safe_ports port 280        # http-mgmt acl Safe_ports
>> port 488        # gss-http acl Safe_ports port 591        #
>> filemaker acl Safe_ports port 777        # multiling http acl
>> CONNECT method CONNECT acl isnsnmp snmp_community public
>>
>> # # Recommended minimum Access Permission configuration: # # Deny
>> requests to certain unsafe ports http_access deny !Safe_ports
>>
>> # Deny CONNECT to other than secure SSL ports http_access deny
>> CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost http_access allow
>> localhost manager http_access deny manager
>>
>> cachemgr_passwd password all
>>
>> # We strongly recommend the following be uncommented to protect
>> innocent # web applications running on the proxy server who think
>> the only # one who can access services on "localhost" is a local
>> user #http_access deny to_localhost
>>
>> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>>
>> # Example rule allowing access from your local networks. # Adapt
>> localnet in the ACL section to list your (internal) IP networks #
>> from where browsing should be allowed http_access deny
>> blockeddomain http_access allow localnet http_access allow
>> localhost snmp_access allow isnsnmp localnet
>>
>> # And finally deny all other access to this proxy http_access deny
>> all # snmp_access deny all
>>
>> # Squid normally listens to port 3128 http_port 3128 http_port 3129
>> intercept snmp_port 3401
>>
>> # Uncomment and adjust the following to add a disk cache
>> directory. #cache_dir ufs /usr/local/squid/var/cache/squid 100 16
>> 256 cache_dir aufs /home/cache/var/cache/squid 350000 16 256
>>
>> # Leave coredumps in the first cache dir coredump_dir
>> /usr/local/squid/var/cache/squid
>>
>> access_log daemon:/home/cache/var/logs/access.log squid cache_log
>> /home/cache/var/logs/cache.log
>>
>>
>> # # Add any of your own refresh_pattern entries above these. #
>> refresh_pattern ^ftp:        1440    20%    10080 refresh_pattern
>> ^gopher:    1440    0%    1440 refresh_pattern -i (/cgi-bin/|\?) 0
>> 0%    0 refresh_pattern .        0    20%    4320
>>
>> half_closed_clients off # quick_abort_min 0 KB # quick_abort_max 0
>> KB # vary_ignore_expire on # reload_into_ims on # memory_pools off
>> cache_mem 9216 MB memory_cache_mode always
>> client_persistent_connections off server_persistent_connections
>> off visible_hostname isn-phc-cache minimum_object_size 0 KB
>> maximum_object_size 96 MB maximum_object_size_in_memory 1 MB
>> memory_replacement_policy lru cache_replacement_policy heap LFUDA
>> quick_abort_min 1024 KB quick_abort_max 2048 KB quick_abort_pct 90
>> ipcache_size 10240 # ipcache_low 90 # ipcache_high 95
>> cache_swap_low 98 cache_swap_high 100 # fqdncache_size 16384 #
>> retry_on_error on # offline_mode off logfile_rotate 10
>> dns_nameservers 8.8.8.8 41.78.211.30
>>
>>
>>
>>
>> Was the thousands of thousands of SECURITY ALERT the cause of
>> this?
>>
>>
>> Thanks Monah _______________________________________________
>> squid-users mailing list squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVEbnYAAoJENNXIZxhPexGMQgIAMICUhBRZMOIl96wFRHaUz+U
> mFOoHX+dSr5C6hh7mm44ZFq9pWYb8c4EEx+kp5olttpPVFh3cfpl8h8tY73DfWUD
> bWojvcQA+NqQ6rIvJcSAwOJ+bWXx3fPTUefGcAOJ+gZho90DyyLxy6vDSMt/0rnV
> GkpUCTT7r3w/EKQbavjRpAcnEdm0L/tSv70Tui+SlU4ksVn77yIoLJ6xD183B4U+
> 6Heg/x0sd2sMG6nx3452V7v5eaaQXAO3teby/VOUeRthuaFdJDTDn60j96h5Xc8Y
> MSXIbk5qBecRuhVKs6vt2gWC59LYvjNnpSyCv1NyF0PWADi5QPlUVvGxtfUHfHU=
> =5wYL
> -----END PGP SIGNATURE-----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From yvoinov at gmail.com  Tue Mar 24 20:07:45 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 02:07:45 +0600
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
Message-ID: <5511C411.5030202@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Running out of filedescriptors is another problem. You probably can
re-build your squid with higher value of corresponding parameter.


25.03.15 2:05, Monah Baki ?????:
> Thanks Yuri for the URL. The company is a small ISP using policy
> based routing, so using WPAD or GPO isn't feasible.
> 
> If the cause of the server running out of file descriptions and
> giving the "assertion failed: store.cc:1885: "isEmpty()" error, I
> prefer to inform the enduser to fix his computer.
> 
> Thanks Monah
> 
> 
> On Tue, Mar 24, 2015 at 3:24 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote: Feel free fo look at this:
> 
> http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
> 
> 
> 25.03.15 1:18, Monah Baki ?????:
>>>> Running squid 3.5.2 on Centos 6.6
>>>> 
>>>> ./configure --prefix=/home/cache
>>>> --enable-follow-x-forwarded-for --with-large-files
>>>> --enable-ssl --disable-ipv6 --enable-esi 
>>>> --enable-kill-parent-hack --enable-snmp --with-pthreads 
>>>> --with-filedescriptors=65535
>>>> --enable-cachemgr-hostname=hostname 
>>>> --enable-storeio=ufs,aufs,diskd,rock
>>>> 
>>>> We have around 50 users. I am seeing hundreds of thousands of
>>>> the following:
>>>> 
>>>> 
>>>> 2015/03/24 14:57:34.910| SECURITY ALERT: By user agent: 
>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
>>>> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>> 14:57:34.910| SECURITY ALERT: on URL: www.facebook.com:443
>>>> 2015/03/24 14:57:34.946| SECURITY ALERT: Host header forgery
>>>> detected on local=85.115.52.158:80
>>>> remote=196.245.252.34:36732 FD 49 flags=33 (local IP does not
>>>> match any domain IP)
>>>> 
>>>> 
>>>> Then after 2 hours, I get the message in my cacahe.log:
>>>> 
>>>> 2015/03/24 16:41:42.478| SECURITY ALERT: By user agent: 
>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
>>>> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>> 16:41:42.478| SECURITY ALERT: on URL: www.facebook.com:443
>>>> 2015/03/24 16:41:42.478| WARNING: 1 swapin MD5 mismatches
>>>> 2015/03/24 16:41:42.478| Could not parse headers from on disk
>>>> object 2015/03/24 16:41:42.478| BUG 3279: HTTP reply without
>>>> Date: 2015/03/24 16:41:42.478| StoreEntry->key:
>>>> 23F0D6046AB8FE86440CAD447524FCBC 2015/03/24 16:41:42.478|
>>>> StoreEntry->next: 0 2015/03/24 16:41:42.478| 
>>>> StoreEntry->mem_obj: 0x1d56470 2015/03/24 16:41:42.478| 
>>>> StoreEntry->timestamp: -1 2015/03/24 16:41:42.478| 
>>>> StoreEntry->lastref: 1427211702 2015/03/24 16:41:42.478| 
>>>> StoreEntry->expires: -1 2015/03/24 16:41:42.478| 
>>>> StoreEntry->lastmod: -1 2015/03/24 16:41:42.478| 
>>>> StoreEntry->swap_file_sz: 0 2015/03/24 16:41:42.478| 
>>>> StoreEntry->refcount: 1 2015/03/24 16:41:42.478|
>>>> StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED 2015/03/24
>>>> 16:41:42.478| StoreEntry->swap_dirn: -1 2015/03/24
>>>> 16:41:42.478| StoreEntry->swap_filen: -1 2015/03/24
>>>> 16:41:42.478| StoreEntry->lock_count: 2 2015/03/24
>>>> 16:41:42.478| StoreEntry->mem_status: 0 2015/03/24
>>>> 16:41:42.478| StoreEntry->ping_status: 2 2015/03/24
>>>> 16:41:42.478| StoreEntry->store_status: 1 2015/03/24
>>>> 16:41:42.478| StoreEntry->swap_status: 0 2015/03/24
>>>> 16:41:42.747| SECURITY ALERT: Host header forgery detected on
>>>> local=85.115.52.158:80 remote=197.255.252.34:44348 FD 20
>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>> 16:41:42.747| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
>>>> 2015/03/24 16:41:42.747| SECURITY ALERT: on URL:
>>>> us-mg5.mail.yahoo.com:443 2015/03/24 16:41:42.772| SECURITY 
>>>> ALERT: Host header forgery detected on
>>>> local=85.115.52.158:80 remote=197.255.252.34:44349 FD 20
>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>> 16:41:42.772| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
>>>> 2015/03/24 16:41:42.772| SECURITY ALERT: on URL:
>>>> csync.flickr.com:443 2015/03/24 16:41:42.800| SECURITY
>>>> ALERT: Host header forgery detected on
>>>> local=85.115.33.158:80 remote=197.255.252.34:13505 FD 20
>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>> 16:41:42.800| SECURITY ALERT: By user agent: Mozilla/5.0
>>>> (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>>>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.800| 
>>>> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24 
>>>> 16:41:43.115| SECURITY ALERT: Host header forgery detected
>>>> on local=85.115.33.158:80 remote=197.255.252.34:13506 FD 31
>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>> 16:41:43.115| SECURITY ALERT: By user agent: Mozilla/5.0
>>>> (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>>>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:43.115|
>>>> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24
>>>> 16:41:43.115| assertion failed: store.cc:1885: "isEmpty()"
>>>> 
>>>> 
>>>> Then I get a message "running out of file descriptors", for
>>>> that I did the following: echo 1024 65535 > 
>>>> /proc/sys/net/ipv4/ip_local_port_range echo 8192 > 
>>>> /proc/sys/net/ipv4/tcp_max_syn_backlog
>>>> 
>>>> In my /etc/security/limits.conf, added the following: * -
>>>> nofile 65535
>>>> 
>>>> 
>>>> 
>>>> My squid.conf
>>>> 
>>>> # # Recommended minimum configuration: #
>>>> 
>>>> # Example rule allowing access from your local networks. #
>>>> Adapt to list your (internal) IP networks from where browsing
>>>> # should be allowed acl localnet src 10.0.0.0/8    # RFC1918
>>>> possible internal network acl localnet src 172.16.0.0/12    #
>>>> RFC1918 possible internal network acl localnet src
>>>> 192.168.0.0/16    # RFC1918 possible internal network acl
>>>> localnet src fc00::/7       # RFC 4193 local private network
>>>> range acl localnet src fe80::/10      # RFC 4291 link-local
>>>> (directly plugged) machines acl blockeddomain dstdomain
>>>> "/home/cache/etc/blocked.domain.acl"
>>>> 
>>>> acl SSL_ports port 443 acl Safe_ports port 80        # http
>>>> acl Safe_ports port 21        # ftp acl Safe_ports port 443
>>>> # https acl Safe_ports port 70        # gopher acl Safe_ports
>>>> port 210        # wais acl Safe_ports port 1025-65535    #
>>>> unregistered ports acl Safe_ports port 280        # http-mgmt
>>>> acl Safe_ports port 488        # gss-http acl Safe_ports port
>>>> 591        # filemaker acl Safe_ports port 777        #
>>>> multiling http acl CONNECT method CONNECT acl isnsnmp
>>>> snmp_community public
>>>> 
>>>> # # Recommended minimum Access Permission configuration: # #
>>>> Deny requests to certain unsafe ports http_access deny
>>>> !Safe_ports
>>>> 
>>>> # Deny CONNECT to other than secure SSL ports http_access
>>>> deny CONNECT !SSL_ports
>>>> 
>>>> # Only allow cachemgr access from localhost http_access
>>>> allow localhost manager http_access deny manager
>>>> 
>>>> cachemgr_passwd password all
>>>> 
>>>> # We strongly recommend the following be uncommented to
>>>> protect innocent # web applications running on the proxy
>>>> server who think the only # one who can access services on
>>>> "localhost" is a local user #http_access deny to_localhost
>>>> 
>>>> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR
>>>> CLIENTS #
>>>> 
>>>> # Example rule allowing access from your local networks. #
>>>> Adapt localnet in the ACL section to list your (internal) IP
>>>> networks # from where browsing should be allowed http_access
>>>> deny blockeddomain http_access allow localnet http_access
>>>> allow localhost snmp_access allow isnsnmp localnet
>>>> 
>>>> # And finally deny all other access to this proxy http_access
>>>> deny all # snmp_access deny all
>>>> 
>>>> # Squid normally listens to port 3128 http_port 3128
>>>> http_port 3129 intercept snmp_port 3401
>>>> 
>>>> # Uncomment and adjust the following to add a disk cache 
>>>> directory. #cache_dir ufs /usr/local/squid/var/cache/squid
>>>> 100 16 256 cache_dir aufs /home/cache/var/cache/squid 350000
>>>> 16 256
>>>> 
>>>> # Leave coredumps in the first cache dir coredump_dir 
>>>> /usr/local/squid/var/cache/squid
>>>> 
>>>> access_log daemon:/home/cache/var/logs/access.log squid
>>>> cache_log /home/cache/var/logs/cache.log
>>>> 
>>>> 
>>>> # # Add any of your own refresh_pattern entries above these.
>>>> # refresh_pattern ^ftp:        1440    20%    10080
>>>> refresh_pattern ^gopher:    1440    0%    1440
>>>> refresh_pattern -i (/cgi-bin/|\?) 0 0%    0 refresh_pattern .
>>>> 0    20%    4320
>>>> 
>>>> half_closed_clients off # quick_abort_min 0 KB #
>>>> quick_abort_max 0 KB # vary_ignore_expire on #
>>>> reload_into_ims on # memory_pools off cache_mem 9216 MB
>>>> memory_cache_mode always client_persistent_connections off
>>>> server_persistent_connections off visible_hostname
>>>> isn-phc-cache minimum_object_size 0 KB maximum_object_size 96
>>>> MB maximum_object_size_in_memory 1 MB 
>>>> memory_replacement_policy lru cache_replacement_policy heap
>>>> LFUDA quick_abort_min 1024 KB quick_abort_max 2048 KB
>>>> quick_abort_pct 90 ipcache_size 10240 # ipcache_low 90 #
>>>> ipcache_high 95 cache_swap_low 98 cache_swap_high 100 #
>>>> fqdncache_size 16384 # retry_on_error on # offline_mode off
>>>> logfile_rotate 10 dns_nameservers 8.8.8.8 41.78.211.30
>>>> 
>>>> 
>>>> 
>>>> 
>>>> Was the thousands of thousands of SECURITY ALERT the cause
>>>> of this?
>>>> 
>>>> 
>>>> Thanks Monah _______________________________________________ 
>>>> squid-users mailing list squid-users at lists.squid-cache.org 
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEcQRAAoJENNXIZxhPexGL2QIAJrNvdh/tvGcDjgUXl2nFC+B
4NfZgnx75nBf8DXOtZuRDPqZl6xdAySxMt1JVPz1GWh0j1+zK5RV40qHXcB73iVd
UIYXZV/HJxYpXIFkjjp6Cs1BcMI9hVGgDVQD/aEiy58FXGeXidI7yP65Xf4KO2XC
vNi/E5ceuJS2HxaEPn92QIvFMGHKB3b+xCACpAk9pWkUKM4UpHOaXgYrpoIWyLx+
+vimU0plLs9SBNaG6DQrq52A0sPO0LlsXHszuQ/DlT/vPJJYMks/Z7Qe2PuHgHOl
g61sspOAPpaSUZx6dhRuc9g8tclZ8TrxeFgXKl0pETKqYfVMBPlNRVTJn3Kgrrk=
=mERF
-----END PGP SIGNATURE-----


From monahbaki at gmail.com  Tue Mar 24 20:09:33 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Tue, 24 Mar 2015 16:09:33 -0400
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <5511C411.5030202@gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
 <5511C411.5030202@gmail.com>
Message-ID: <CALP3=x821sJ9dNv4ejms35dYW4Vc-BV3fZ-3ZNoyTtE2yat3Qg@mail.gmail.com>

I compiled it with --with-filedescriptors=65535, anything else that can help?

Thanks

On Tue, Mar 24, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Running out of filedescriptors is another problem. You probably can
> re-build your squid with higher value of corresponding parameter.
>
>
> 25.03.15 2:05, Monah Baki ?????:
>> Thanks Yuri for the URL. The company is a small ISP using policy
>> based routing, so using WPAD or GPO isn't feasible.
>>
>> If the cause of the server running out of file descriptions and
>> giving the "assertion failed: store.cc:1885: "isEmpty()" error, I
>> prefer to inform the enduser to fix his computer.
>>
>> Thanks Monah
>>
>>
>> On Tue, Mar 24, 2015 at 3:24 PM, Yuri Voinov <yvoinov at gmail.com>
>> wrote: Feel free fo look at this:
>>
>> http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
>>
>>
>> 25.03.15 1:18, Monah Baki ?????:
>>>>> Running squid 3.5.2 on Centos 6.6
>>>>>
>>>>> ./configure --prefix=/home/cache
>>>>> --enable-follow-x-forwarded-for --with-large-files
>>>>> --enable-ssl --disable-ipv6 --enable-esi
>>>>> --enable-kill-parent-hack --enable-snmp --with-pthreads
>>>>> --with-filedescriptors=65535
>>>>> --enable-cachemgr-hostname=hostname
>>>>> --enable-storeio=ufs,aufs,diskd,rock
>>>>>
>>>>> We have around 50 users. I am seeing hundreds of thousands of
>>>>> the following:
>>>>>
>>>>>
>>>>> 2015/03/24 14:57:34.910| SECURITY ALERT: By user agent:
>>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
>>>>> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>>> 14:57:34.910| SECURITY ALERT: on URL: www.facebook.com:443
>>>>> 2015/03/24 14:57:34.946| SECURITY ALERT: Host header forgery
>>>>> detected on local=85.115.52.158:80
>>>>> remote=196.245.252.34:36732 FD 49 flags=33 (local IP does not
>>>>> match any domain IP)
>>>>>
>>>>>
>>>>> Then after 2 hours, I get the message in my cacahe.log:
>>>>>
>>>>> 2015/03/24 16:41:42.478| SECURITY ALERT: By user agent:
>>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like
>>>>> Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>>> 16:41:42.478| SECURITY ALERT: on URL: www.facebook.com:443
>>>>> 2015/03/24 16:41:42.478| WARNING: 1 swapin MD5 mismatches
>>>>> 2015/03/24 16:41:42.478| Could not parse headers from on disk
>>>>> object 2015/03/24 16:41:42.478| BUG 3279: HTTP reply without
>>>>> Date: 2015/03/24 16:41:42.478| StoreEntry->key:
>>>>> 23F0D6046AB8FE86440CAD447524FCBC 2015/03/24 16:41:42.478|
>>>>> StoreEntry->next: 0 2015/03/24 16:41:42.478|
>>>>> StoreEntry->mem_obj: 0x1d56470 2015/03/24 16:41:42.478|
>>>>> StoreEntry->timestamp: -1 2015/03/24 16:41:42.478|
>>>>> StoreEntry->lastref: 1427211702 2015/03/24 16:41:42.478|
>>>>> StoreEntry->expires: -1 2015/03/24 16:41:42.478|
>>>>> StoreEntry->lastmod: -1 2015/03/24 16:41:42.478|
>>>>> StoreEntry->swap_file_sz: 0 2015/03/24 16:41:42.478|
>>>>> StoreEntry->refcount: 1 2015/03/24 16:41:42.478|
>>>>> StoreEntry->flags: PRIVATE,FWD_HDR_WAIT,VALIDATED 2015/03/24
>>>>> 16:41:42.478| StoreEntry->swap_dirn: -1 2015/03/24
>>>>> 16:41:42.478| StoreEntry->swap_filen: -1 2015/03/24
>>>>> 16:41:42.478| StoreEntry->lock_count: 2 2015/03/24
>>>>> 16:41:42.478| StoreEntry->mem_status: 0 2015/03/24
>>>>> 16:41:42.478| StoreEntry->ping_status: 2 2015/03/24
>>>>> 16:41:42.478| StoreEntry->store_status: 1 2015/03/24
>>>>> 16:41:42.478| StoreEntry->swap_status: 0 2015/03/24
>>>>> 16:41:42.747| SECURITY ALERT: Host header forgery detected on
>>>>> local=85.115.52.158:80 remote=197.255.252.34:44348 FD 20
>>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>>> 16:41:42.747| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
>>>>> 2015/03/24 16:41:42.747| SECURITY ALERT: on URL:
>>>>> us-mg5.mail.yahoo.com:443 2015/03/24 16:41:42.772| SECURITY
>>>>> ALERT: Host header forgery detected on
>>>>> local=85.115.52.158:80 remote=197.255.252.34:44349 FD 20
>>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>>> 16:41:42.772| SECURITY ALERT: By user agent: WNetCore/0.1.1.1
>>>>> 2015/03/24 16:41:42.772| SECURITY ALERT: on URL:
>>>>> csync.flickr.com:443 2015/03/24 16:41:42.800| SECURITY
>>>>> ALERT: Host header forgery detected on
>>>>> local=85.115.33.158:80 remote=197.255.252.34:13505 FD 20
>>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>>> 16:41:42.800| SECURITY ALERT: By user agent: Mozilla/5.0
>>>>> (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>>>>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:42.800|
>>>>> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24
>>>>> 16:41:43.115| SECURITY ALERT: Host header forgery detected
>>>>> on local=85.115.33.158:80 remote=197.255.252.34:13506 FD 31
>>>>> flags=33 (local IP does not match any domain IP) 2015/03/24
>>>>> 16:41:43.115| SECURITY ALERT: By user agent: Mozilla/5.0
>>>>> (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko)
>>>>> Chrome/20.0.1092.0 Safari/536.6 2015/03/24 16:41:43.115|
>>>>> SECURITY ALERT: on URL: www.facebook.com:443 2015/03/24
>>>>> 16:41:43.115| assertion failed: store.cc:1885: "isEmpty()"
>>>>>
>>>>>
>>>>> Then I get a message "running out of file descriptors", for
>>>>> that I did the following: echo 1024 65535 >
>>>>> /proc/sys/net/ipv4/ip_local_port_range echo 8192 >
>>>>> /proc/sys/net/ipv4/tcp_max_syn_backlog
>>>>>
>>>>> In my /etc/security/limits.conf, added the following: * -
>>>>> nofile 65535
>>>>>
>>>>>
>>>>>
>>>>> My squid.conf
>>>>>
>>>>> # # Recommended minimum configuration: #
>>>>>
>>>>> # Example rule allowing access from your local networks. #
>>>>> Adapt to list your (internal) IP networks from where browsing
>>>>> # should be allowed acl localnet src 10.0.0.0/8    # RFC1918
>>>>> possible internal network acl localnet src 172.16.0.0/12    #
>>>>> RFC1918 possible internal network acl localnet src
>>>>> 192.168.0.0/16    # RFC1918 possible internal network acl
>>>>> localnet src fc00::/7       # RFC 4193 local private network
>>>>> range acl localnet src fe80::/10      # RFC 4291 link-local
>>>>> (directly plugged) machines acl blockeddomain dstdomain
>>>>> "/home/cache/etc/blocked.domain.acl"
>>>>>
>>>>> acl SSL_ports port 443 acl Safe_ports port 80        # http
>>>>> acl Safe_ports port 21        # ftp acl Safe_ports port 443
>>>>> # https acl Safe_ports port 70        # gopher acl Safe_ports
>>>>> port 210        # wais acl Safe_ports port 1025-65535    #
>>>>> unregistered ports acl Safe_ports port 280        # http-mgmt
>>>>> acl Safe_ports port 488        # gss-http acl Safe_ports port
>>>>> 591        # filemaker acl Safe_ports port 777        #
>>>>> multiling http acl CONNECT method CONNECT acl isnsnmp
>>>>> snmp_community public
>>>>>
>>>>> # # Recommended minimum Access Permission configuration: # #
>>>>> Deny requests to certain unsafe ports http_access deny
>>>>> !Safe_ports
>>>>>
>>>>> # Deny CONNECT to other than secure SSL ports http_access
>>>>> deny CONNECT !SSL_ports
>>>>>
>>>>> # Only allow cachemgr access from localhost http_access
>>>>> allow localhost manager http_access deny manager
>>>>>
>>>>> cachemgr_passwd password all
>>>>>
>>>>> # We strongly recommend the following be uncommented to
>>>>> protect innocent # web applications running on the proxy
>>>>> server who think the only # one who can access services on
>>>>> "localhost" is a local user #http_access deny to_localhost
>>>>>
>>>>> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR
>>>>> CLIENTS #
>>>>>
>>>>> # Example rule allowing access from your local networks. #
>>>>> Adapt localnet in the ACL section to list your (internal) IP
>>>>> networks # from where browsing should be allowed http_access
>>>>> deny blockeddomain http_access allow localnet http_access
>>>>> allow localhost snmp_access allow isnsnmp localnet
>>>>>
>>>>> # And finally deny all other access to this proxy http_access
>>>>> deny all # snmp_access deny all
>>>>>
>>>>> # Squid normally listens to port 3128 http_port 3128
>>>>> http_port 3129 intercept snmp_port 3401
>>>>>
>>>>> # Uncomment and adjust the following to add a disk cache
>>>>> directory. #cache_dir ufs /usr/local/squid/var/cache/squid
>>>>> 100 16 256 cache_dir aufs /home/cache/var/cache/squid 350000
>>>>> 16 256
>>>>>
>>>>> # Leave coredumps in the first cache dir coredump_dir
>>>>> /usr/local/squid/var/cache/squid
>>>>>
>>>>> access_log daemon:/home/cache/var/logs/access.log squid
>>>>> cache_log /home/cache/var/logs/cache.log
>>>>>
>>>>>
>>>>> # # Add any of your own refresh_pattern entries above these.
>>>>> # refresh_pattern ^ftp:        1440    20%    10080
>>>>> refresh_pattern ^gopher:    1440    0%    1440
>>>>> refresh_pattern -i (/cgi-bin/|\?) 0 0%    0 refresh_pattern .
>>>>> 0    20%    4320
>>>>>
>>>>> half_closed_clients off # quick_abort_min 0 KB #
>>>>> quick_abort_max 0 KB # vary_ignore_expire on #
>>>>> reload_into_ims on # memory_pools off cache_mem 9216 MB
>>>>> memory_cache_mode always client_persistent_connections off
>>>>> server_persistent_connections off visible_hostname
>>>>> isn-phc-cache minimum_object_size 0 KB maximum_object_size 96
>>>>> MB maximum_object_size_in_memory 1 MB
>>>>> memory_replacement_policy lru cache_replacement_policy heap
>>>>> LFUDA quick_abort_min 1024 KB quick_abort_max 2048 KB
>>>>> quick_abort_pct 90 ipcache_size 10240 # ipcache_low 90 #
>>>>> ipcache_high 95 cache_swap_low 98 cache_swap_high 100 #
>>>>> fqdncache_size 16384 # retry_on_error on # offline_mode off
>>>>> logfile_rotate 10 dns_nameservers 8.8.8.8 41.78.211.30
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Was the thousands of thousands of SECURITY ALERT the cause
>>>>> of this?
>>>>>
>>>>>
>>>>> Thanks Monah _______________________________________________
>>>>> squid-users mailing list squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>> _______________________________________________ squid-users
>>> mailing list squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVEcQRAAoJENNXIZxhPexGL2QIAJrNvdh/tvGcDjgUXl2nFC+B
> 4NfZgnx75nBf8DXOtZuRDPqZl6xdAySxMt1JVPz1GWh0j1+zK5RV40qHXcB73iVd
> UIYXZV/HJxYpXIFkjjp6Cs1BcMI9hVGgDVQD/aEiy58FXGeXidI7yP65Xf4KO2XC
> vNi/E5ceuJS2HxaEPn92QIvFMGHKB3b+xCACpAk9pWkUKM4UpHOaXgYrpoIWyLx+
> +vimU0plLs9SBNaG6DQrq52A0sPO0LlsXHszuQ/DlT/vPJJYMks/Z7Qe2PuHgHOl
> g61sspOAPpaSUZx6dhRuc9g8tclZ8TrxeFgXKl0pETKqYfVMBPlNRVTJn3Kgrrk=
> =mERF
> -----END PGP SIGNATURE-----


From yvoinov at gmail.com  Tue Mar 24 20:10:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 02:10:33 +0600
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <CALP3=x821sJ9dNv4ejms35dYW4Vc-BV3fZ-3ZNoyTtE2yat3Qg@mail.gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
 <5511C411.5030202@gmail.com>
 <CALP3=x821sJ9dNv4ejms35dYW4Vc-BV3fZ-3ZNoyTtE2yat3Qg@mail.gmail.com>
Message-ID: <5511C4B9.2020701@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

This can be insufficient.

25.03.15 2:09, Monah Baki ?????:
> I compiled it with --with-filedescriptors=65535, anything else that
> can help?
> 
> Thanks
> 
> On Tue, Mar 24, 2015 at 4:07 PM, Yuri Voinov <yvoinov at gmail.com>
> wrote: Running out of filedescriptors is another problem. You
> probably can re-build your squid with higher value of corresponding
> parameter.
> 
> 
> 25.03.15 2:05, Monah Baki ?????:
>>>> Thanks Yuri for the URL. The company is a small ISP using
>>>> policy based routing, so using WPAD or GPO isn't feasible.
>>>> 
>>>> If the cause of the server running out of file descriptions
>>>> and giving the "assertion failed: store.cc:1885: "isEmpty()"
>>>> error, I prefer to inform the enduser to fix his computer.
>>>> 
>>>> Thanks Monah
>>>> 
>>>> 
>>>> On Tue, Mar 24, 2015 at 3:24 PM, Yuri Voinov
>>>> <yvoinov at gmail.com> wrote: Feel free fo look at this:
>>>> 
>>>> http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
>>>> 
>>>> 
>>>> 25.03.15 1:18, Monah Baki ?????:
>>>>>>> Running squid 3.5.2 on Centos 6.6
>>>>>>> 
>>>>>>> ./configure --prefix=/home/cache 
>>>>>>> --enable-follow-x-forwarded-for --with-large-files 
>>>>>>> --enable-ssl --disable-ipv6 --enable-esi 
>>>>>>> --enable-kill-parent-hack --enable-snmp
>>>>>>> --with-pthreads --with-filedescriptors=65535 
>>>>>>> --enable-cachemgr-hostname=hostname 
>>>>>>> --enable-storeio=ufs,aufs,diskd,rock
>>>>>>> 
>>>>>>> We have around 50 users. I am seeing hundreds of
>>>>>>> thousands of the following:
>>>>>>> 
>>>>>>> 
>>>>>>> 2015/03/24 14:57:34.910| SECURITY ALERT: By user
>>>>>>> agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6
>>>>>>> (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6
>>>>>>> 2015/03/24 14:57:34.910| SECURITY ALERT: on URL:
>>>>>>> www.facebook.com:443 2015/03/24 14:57:34.946| SECURITY
>>>>>>> ALERT: Host header forgery detected on
>>>>>>> local=85.115.52.158:80 remote=196.245.252.34:36732 FD
>>>>>>> 49 flags=33 (local IP does not match any domain IP)
>>>>>>> 
>>>>>>> 
>>>>>>> Then after 2 hours, I get the message in my
>>>>>>> cacahe.log:
>>>>>>> 
>>>>>>> 2015/03/24 16:41:42.478| SECURITY ALERT: By user
>>>>>>> agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6
>>>>>>> (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6
>>>>>>> 2015/03/24 16:41:42.478| SECURITY ALERT: on URL:
>>>>>>> www.facebook.com:443 2015/03/24 16:41:42.478| WARNING:
>>>>>>> 1 swapin MD5 mismatches 2015/03/24 16:41:42.478| Could
>>>>>>> not parse headers from on disk object 2015/03/24
>>>>>>> 16:41:42.478| BUG 3279: HTTP reply without Date:
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->key: 
>>>>>>> 23F0D6046AB8FE86440CAD447524FCBC 2015/03/24
>>>>>>> 16:41:42.478| StoreEntry->next: 0 2015/03/24
>>>>>>> 16:41:42.478| StoreEntry->mem_obj: 0x1d56470 2015/03/24
>>>>>>> 16:41:42.478| StoreEntry->timestamp: -1 2015/03/24
>>>>>>> 16:41:42.478| StoreEntry->lastref: 1427211702
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->expires: -1
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->lastmod: -1
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->swap_file_sz: 0
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->refcount: 1
>>>>>>> 2015/03/24 16:41:42.478| StoreEntry->flags:
>>>>>>> PRIVATE,FWD_HDR_WAIT,VALIDATED 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->swap_dirn: -1 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->swap_filen: -1 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->lock_count: 2 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->mem_status: 0 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->ping_status: 2 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->store_status: 1 2015/03/24 16:41:42.478|
>>>>>>> StoreEntry->swap_status: 0 2015/03/24 16:41:42.747|
>>>>>>> SECURITY ALERT: Host header forgery detected on 
>>>>>>> local=85.115.52.158:80 remote=197.255.252.34:44348 FD
>>>>>>> 20 flags=33 (local IP does not match any domain IP)
>>>>>>> 2015/03/24 16:41:42.747| SECURITY ALERT: By user agent:
>>>>>>> WNetCore/0.1.1.1 2015/03/24 16:41:42.747| SECURITY
>>>>>>> ALERT: on URL: us-mg5.mail.yahoo.com:443 2015/03/24
>>>>>>> 16:41:42.772| SECURITY ALERT: Host header forgery
>>>>>>> detected on local=85.115.52.158:80
>>>>>>> remote=197.255.252.34:44349 FD 20 flags=33 (local IP
>>>>>>> does not match any domain IP) 2015/03/24 16:41:42.772|
>>>>>>> SECURITY ALERT: By user agent: WNetCore/0.1.1.1 
>>>>>>> 2015/03/24 16:41:42.772| SECURITY ALERT: on URL: 
>>>>>>> csync.flickr.com:443 2015/03/24 16:41:42.800| SECURITY 
>>>>>>> ALERT: Host header forgery detected on 
>>>>>>> local=85.115.33.158:80 remote=197.255.252.34:13505 FD
>>>>>>> 20 flags=33 (local IP does not match any domain IP)
>>>>>>> 2015/03/24 16:41:42.800| SECURITY ALERT: By user agent:
>>>>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML,
>>>>>>> like Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>>>>> 16:41:42.800| SECURITY ALERT: on URL:
>>>>>>> www.facebook.com:443 2015/03/24 16:41:43.115| SECURITY
>>>>>>> ALERT: Host header forgery detected on
>>>>>>> local=85.115.33.158:80 remote=197.255.252.34:13506 FD
>>>>>>> 31 flags=33 (local IP does not match any domain IP)
>>>>>>> 2015/03/24 16:41:43.115| SECURITY ALERT: By user agent:
>>>>>>> Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML,
>>>>>>> like Gecko) Chrome/20.0.1092.0 Safari/536.6 2015/03/24
>>>>>>> 16:41:43.115| SECURITY ALERT: on URL:
>>>>>>> www.facebook.com:443 2015/03/24 16:41:43.115| assertion
>>>>>>> failed: store.cc:1885: "isEmpty()"
>>>>>>> 
>>>>>>> 
>>>>>>> Then I get a message "running out of file descriptors",
>>>>>>> for that I did the following: echo 1024 65535 > 
>>>>>>> /proc/sys/net/ipv4/ip_local_port_range echo 8192 > 
>>>>>>> /proc/sys/net/ipv4/tcp_max_syn_backlog
>>>>>>> 
>>>>>>> In my /etc/security/limits.conf, added the following: *
>>>>>>> - nofile 65535
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> My squid.conf
>>>>>>> 
>>>>>>> # # Recommended minimum configuration: #
>>>>>>> 
>>>>>>> # Example rule allowing access from your local
>>>>>>> networks. # Adapt to list your (internal) IP networks
>>>>>>> from where browsing # should be allowed acl localnet
>>>>>>> src 10.0.0.0/8    # RFC1918 possible internal network
>>>>>>> acl localnet src 172.16.0.0/12    # RFC1918 possible
>>>>>>> internal network acl localnet src 192.168.0.0/16    #
>>>>>>> RFC1918 possible internal network acl localnet src
>>>>>>> fc00::/7       # RFC 4193 local private network range
>>>>>>> acl localnet src fe80::/10      # RFC 4291 link-local 
>>>>>>> (directly plugged) machines acl blockeddomain
>>>>>>> dstdomain "/home/cache/etc/blocked.domain.acl"
>>>>>>> 
>>>>>>> acl SSL_ports port 443 acl Safe_ports port 80        #
>>>>>>> http acl Safe_ports port 21        # ftp acl Safe_ports
>>>>>>> port 443 # https acl Safe_ports port 70        # gopher
>>>>>>> acl Safe_ports port 210        # wais acl Safe_ports
>>>>>>> port 1025-65535    # unregistered ports acl Safe_ports
>>>>>>> port 280        # http-mgmt acl Safe_ports port 488
>>>>>>> # gss-http acl Safe_ports port 591        # filemaker
>>>>>>> acl Safe_ports port 777        # multiling http acl
>>>>>>> CONNECT method CONNECT acl isnsnmp snmp_community
>>>>>>> public
>>>>>>> 
>>>>>>> # # Recommended minimum Access Permission
>>>>>>> configuration: # # Deny requests to certain unsafe
>>>>>>> ports http_access deny !Safe_ports
>>>>>>> 
>>>>>>> # Deny CONNECT to other than secure SSL ports
>>>>>>> http_access deny CONNECT !SSL_ports
>>>>>>> 
>>>>>>> # Only allow cachemgr access from localhost
>>>>>>> http_access allow localhost manager http_access deny
>>>>>>> manager
>>>>>>> 
>>>>>>> cachemgr_passwd password all
>>>>>>> 
>>>>>>> # We strongly recommend the following be uncommented
>>>>>>> to protect innocent # web applications running on the
>>>>>>> proxy server who think the only # one who can access
>>>>>>> services on "localhost" is a local user #http_access
>>>>>>> deny to_localhost
>>>>>>> 
>>>>>>> # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM
>>>>>>> YOUR CLIENTS #
>>>>>>> 
>>>>>>> # Example rule allowing access from your local
>>>>>>> networks. # Adapt localnet in the ACL section to list
>>>>>>> your (internal) IP networks # from where browsing
>>>>>>> should be allowed http_access deny blockeddomain
>>>>>>> http_access allow localnet http_access allow localhost
>>>>>>> snmp_access allow isnsnmp localnet
>>>>>>> 
>>>>>>> # And finally deny all other access to this proxy
>>>>>>> http_access deny all # snmp_access deny all
>>>>>>> 
>>>>>>> # Squid normally listens to port 3128 http_port 3128 
>>>>>>> http_port 3129 intercept snmp_port 3401
>>>>>>> 
>>>>>>> # Uncomment and adjust the following to add a disk
>>>>>>> cache directory. #cache_dir ufs
>>>>>>> /usr/local/squid/var/cache/squid 100 16 256 cache_dir
>>>>>>> aufs /home/cache/var/cache/squid 350000 16 256
>>>>>>> 
>>>>>>> # Leave coredumps in the first cache dir coredump_dir 
>>>>>>> /usr/local/squid/var/cache/squid
>>>>>>> 
>>>>>>> access_log daemon:/home/cache/var/logs/access.log
>>>>>>> squid cache_log /home/cache/var/logs/cache.log
>>>>>>> 
>>>>>>> 
>>>>>>> # # Add any of your own refresh_pattern entries above
>>>>>>> these. # refresh_pattern ^ftp:        1440    20%
>>>>>>> 10080 refresh_pattern ^gopher:    1440    0%    1440 
>>>>>>> refresh_pattern -i (/cgi-bin/|\?) 0 0%    0
>>>>>>> refresh_pattern . 0    20%    4320
>>>>>>> 
>>>>>>> half_closed_clients off # quick_abort_min 0 KB # 
>>>>>>> quick_abort_max 0 KB # vary_ignore_expire on # 
>>>>>>> reload_into_ims on # memory_pools off cache_mem 9216
>>>>>>> MB memory_cache_mode always
>>>>>>> client_persistent_connections off 
>>>>>>> server_persistent_connections off visible_hostname 
>>>>>>> isn-phc-cache minimum_object_size 0 KB
>>>>>>> maximum_object_size 96 MB maximum_object_size_in_memory
>>>>>>> 1 MB memory_replacement_policy lru
>>>>>>> cache_replacement_policy heap LFUDA quick_abort_min
>>>>>>> 1024 KB quick_abort_max 2048 KB quick_abort_pct 90
>>>>>>> ipcache_size 10240 # ipcache_low 90 # ipcache_high 95
>>>>>>> cache_swap_low 98 cache_swap_high 100 # fqdncache_size
>>>>>>> 16384 # retry_on_error on # offline_mode off 
>>>>>>> logfile_rotate 10 dns_nameservers 8.8.8.8 41.78.211.30
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> Was the thousands of thousands of SECURITY ALERT the
>>>>>>> cause of this?
>>>>>>> 
>>>>>>> 
>>>>>>> Thanks Monah
>>>>>>> _______________________________________________ 
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org 
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>> 
>>>>> _______________________________________________
>>>>> squid-users mailing list squid-users at lists.squid-cache.org 
>>>>> http://lists.squid-cache.org/listinfo/squid-users
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEcS5AAoJENNXIZxhPexGOvsH/jIec++e3SnyfPy0nD6VOE/t
SJS7Vx2N/O+2eL3kTcU2PqDMLwWtOEmtiG+eh1O4+BD7t2PbhO+RQ+nbO2lw7DfW
JoM23L060gOGf0P1xmvGFnkTHDYhg7yre5OwX90etUC4iaRac9IHGXTWrmv1VyVd
Ce9hHXScpStML0A338HFot8+yInF1vutoqEmZpO4FYwC25ylbsSgTc4EX6hk/IEE
+mw4v+a7FqidaED8lzOYGjvR0uGmrkBAHgrNdnrBmQ/5ojolr1SqBQ7Ug/Q37Duh
MKoRhqji+Z7i3AHVtTMN/UsktOH4/0BIai+iFDuwPd3ifOybNLNk4Z4n0RZ46Ys=
=PNUx
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Wed Mar 25 05:34:45 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Tue, 24 Mar 2015 22:34:45 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz> 
Message-ID: <000601d066bd$675f1100$361d3300$@netstream.ps>

BTW   can squid block dynamically loaded images, and ajax request which return images.
I want that on yahoo and google
Is that possible ?
=========================
 , thanks  it seems okay for normal http sites

I want to ask , is there a trick can we do it so that it be applied to google & yahoo images search ??

Here is wt I see in yahoo logs , just small logs and all images are allowed and not blocked =====

1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT js.dmtry.com:443 - DIRECT/184.170.128.58 -
1426881749.077    103 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881749.752     29 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881750.098     21 xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? - DIRECT/23.21.149.132 image/gif
1426881750.731  62443 xx.70 TCP_MISS/200 122185 CONNECT www.gstatic.com:443 - DIRECT/206.126.112.185 -
1426881751.476   xx.70 TCP_MISS/200 4191 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 -
1426881753.005   1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 - DIRECT/76.13.28.21 -
1426881762.280  12994 188.161.107.70 TCP_MISS/200 5502 CONNECT d.adgear.com:443 - DIRECT/205.204.71.140 -
1426881764.215  16497 xx70 TCP_MISS/200 9832 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881764.216  16453 x.70 TCP_MISS/200 6534 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.044  18777 x.70 TCP_MISS/200 11132 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
1426881765.681  15193 xx.107.70 TCP_MISS/200 6225 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881765.691  14149 xx.107.70 TCP_MISS/200 832 CONNECT comet.yahoo.com:443 - DIRECT/72.30.196.161 -
1426881766.046 116219 xx.70 TCP_MISS/200 529 CONNECT d31qbv1cthcecs.cloudfront.net:443 - DIRECT/54.230.16.189 -
1426881766.714    296 xx.70 TCP_MISS/200 2228 POST http://ocsp.verisign.com/ - DIRECT/23.9.123.27 application/ocsp-response
1426881770.049 117609 xx107.70 TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 - DIRECT/54.240.160.97 -
1426881780.403  67786 xx.70 TCP_MISS/200 852 CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881781.519    353 xx.70 TCP_MISS/200 571 GET http://data.cnn.com/jsonp/breaking_news/domestic.json? - DIRECT/157.166.249.67 application/javascript
1426881782.057 118788 xx.70 TCP_MISS/200 19972 CONNECT cdn2sitescout-a.akamaihd.net:443 - DIRECT/23.15.4.18 -
1426881790.558  71055 xx TCP_MISS/200 26805 CONNECT s.yimg.com:443 - DIRECT/206.190.56.191 -
1426881814.445 100461 xx TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 - DIRECT/98.139.180.149 -
1426881818.437  70709 xx70 TCP_MISS/200 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -


regards

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Friday, March 20, 2015 9:56 AM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

On 21/03/2015 12:05 p.m., snakeeyes wrote:
> Hi amos , thanks for reply
> I have tried @ top of squidf.conf
> 
> acl images rep_header Content-Type ^image/ ^x-image/ acl small 
> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny small 
> images
> 
> are you sure that its blocking images with size >40KB ????

Sorry I slightly mis-read your request. What I gave is blocking images
*smaller* than 40 bytes (see what I mean about cut-n-paste without understanding?).

To block images *over* 40 bytes change that to:
 http_reply_access deny !small images


> also I didn?t see extensions like jpg or bmp or similar like that ??!!

Because HTTP does not transfer files. It transfers data. Sometimes data can *also* be found inside "files", sometimes not.

HTTP Content-Type header describes what format the data is. In this case you requested images in general, so thats the pattern I gave.

Amos



From bpk678 at gmail.com  Tue Mar 24 20:55:04 2015
From: bpk678 at gmail.com (brendan kearney)
Date: Tue, 24 Mar 2015 16:55:04 -0400
Subject: [squid-users] load balancing and site failover
In-Reply-To: <1427221530.17590.37.camel@desktop.bpk2.com>
References: <1427206737.17590.33.camel@desktop.bpk2.com>
 <1427221530.17590.37.camel@desktop.bpk2.com>
Message-ID: <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>

Was not sure if bugzilla was used for mailing list issues.  If you would
like me to open one, I will but it looks like the list is working again.
On Mar 24, 2015 2:25 PM, "Brendan Kearney" <bpk678 at gmail.com> wrote:

> On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
> > while load balancing is not a requirement in a proxy environment, it
> > does afford a great deal of functionality, scaling and fault tolerance
> > in one.  several if not many on this list probably employ them for their
> > proxies and likely other technologies, but they are not all created
> > equal.
> >
> > i recently looked to see if a specific feature was in HAProxy.  i was
> > looking to see if HAProxy could reply to a new connection with a RST
> > packet if no pool member was available.
> >
> > the idea behind this is, if all of the proxies are not passing the
> > service check and are marked down by the load balancer, the reply of a
> > RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
> > tells the browser to failover to the next proxy assigned by the PAC
> > file.
> >
> > where i work, we have this configuration working.  the load balancers
> > are configured with the option to send a reset when no proxy is
> > available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
> > a specific order based on which proxy VIP is assigned as the primary.
> > In every case, if the primary VIP does not have an available pool
> > member, the browser fails over to the next in the list.  failover would
> > happen again, if the secondary VIP replies with a RST during the
> > connection establishing.  the process repeats until a TCP connection
> > establishes or all proxies assigned have been exhausted.  the browser
> > will use the proxy VIP that it successfully connects to, for the
> > duration of the session.  once the browser is closed and reopened, the
> > evaluation of the PAC file occurs again, and the process starts anew.
> > plug-ins such as Proxy Selector are the exception to this, and can be
> > used to reevaluate a PAC file by selecting it for use.
> >
> > we have used this configuration several times, when we found an ISP link
> > was flapping or some other issue more global in nature than just the
> > proxies was affecting our egress and internet access.  i can attest to
> > the solution as working and elegantly handling site wide failures.
> >
> > being that the solutions where i work are proprietary commercial
> > products, i wanted to find an open source product that does this.  i
> > have been a long time user of HAProxy, and have recommended it for
> > others here, but sadly they cannot perform this function.  per their
> > mailing list, they use the network stack of the OS for connection
> > establishment and cannot cause a RST to be sent to the client during a
> > TCP handshake if no pool member is available.
> >
> > they suggested an external helper that manipulates IPTables rules based
> > on a pool member being available.  they do not feel that a feature like
> > this belongs in a layer 4/7 reverse proxy application.
> >
> > my search for a load balancer solution went through ipvsadm, balance and
> > haproxy before i selected haproxy.  haproxy was more feature rich than
> > balance, and easier to implement than ipvsadm.  do any other list
> > members have a need for such a feature from their load balancers?  do
> > any other list members have site failover solutions that have been
> > tested or used and would consider sharing their design and/or pain
> > points?  i am not looking for secret sauce or confidential info, but
> > more high level architecture decisions and such.
> >
>
> trying to send this again, as it was rejected previously.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150324/38a51280/attachment.htm>

From yvoinov at gmail.com  Tue Mar 24 20:58:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 02:58:04 +0600
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000601d066bd$675f1100$361d3300$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps>
Message-ID: <5511CFDC.5040006@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Don't think so.

Probably you'll have to write your own helper to handle dynamic
content. Or use the content adaptaion feature.

25.03.15 11:34, snakeeyes ?????:
> BTW   can squid block dynamically loaded images, and ajax request
> which return images. I want that on yahoo and google Is that
> possible ? ========================= , thanks  it seems okay for
> normal http sites
> 
> I want to ask , is there a trick can we do it so that it be applied
> to google & yahoo images search ??
> 
> Here is wt I see in yahoo logs , just small logs and all images are
> allowed and not blocked =====
> 
> 1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT
> js.dmtry.com:443 - DIRECT/184.170.128.58 - 1426881749.077    103
> xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ -
> DIRECT/23.9.123.27 application/ocsp-response 1426881749.752     29
> xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ -
> DIRECT/23.9.123.27 application/ocsp-response 1426881750.098     21
> xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? -
> DIRECT/23.21.149.132 image/gif 1426881750.731  62443 xx.70
> TCP_MISS/200 122185 CONNECT www.gstatic.com:443 -
> DIRECT/206.126.112.185 - 1426881751.476   xx.70 TCP_MISS/200 4191
> CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 - 
> 1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT
> secure.footprint.net:443 - DIRECT/8.12.219.125 - 1426881753.005
> 1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 -
> DIRECT/76.13.28.21 - 1426881762.280  12994 188.161.107.70
> TCP_MISS/200 5502 CONNECT d.adgear.com:443 - DIRECT/205.204.71.140
> - 1426881764.215  16497 xx70 TCP_MISS/200 9832 CONNECT
> ads.yahoo.com:443 - DIRECT/98.139.225.43 - 1426881764.216  16453
> x.70 TCP_MISS/200 6534 CONNECT ads.yahoo.com:443 -
> DIRECT/98.139.225.43 - 1426881765.044  18777 x.70 TCP_MISS/200
> 11132 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 - 
> 1426881765.681  15193 xx.107.70 TCP_MISS/200 6225 CONNECT
> comet.yahoo.com:443 - DIRECT/72.30.196.161 - 1426881765.691  14149
> xx.107.70 TCP_MISS/200 832 CONNECT comet.yahoo.com:443 -
> DIRECT/72.30.196.161 - 1426881766.046 116219 xx.70 TCP_MISS/200 529
> CONNECT d31qbv1cthcecs.cloudfront.net:443 - DIRECT/54.230.16.189 - 
> 1426881766.714    296 xx.70 TCP_MISS/200 2228 POST
> http://ocsp.verisign.com/ - DIRECT/23.9.123.27
> application/ocsp-response 1426881770.049 117609 xx107.70
> TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 -
> DIRECT/54.240.160.97 - 1426881780.403  67786 xx.70 TCP_MISS/200 852
> CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 - 1426881781.519
> 353 xx.70 TCP_MISS/200 571 GET
> http://data.cnn.com/jsonp/breaking_news/domestic.json? -
> DIRECT/157.166.249.67 application/javascript 1426881782.057 118788
> xx.70 TCP_MISS/200 19972 CONNECT cdn2sitescout-a.akamaihd.net:443 -
> DIRECT/23.15.4.18 - 1426881790.558  71055 xx TCP_MISS/200 26805
> CONNECT s.yimg.com:443 - DIRECT/206.190.56.191 - 1426881814.445
> 100461 xx TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 -
> DIRECT/98.139.180.149 - 1426881818.437  70709 xx70 TCP_MISS/200
> 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -
> 
> 
> regards
> 
> -----Original Message----- From: Amos Jeffries
> [mailto:squid3 at treenet.co.nz] Sent: Friday, March 20, 2015 9:56 AM 
> To: snakeeyes Cc: squid-users at lists.squid-cache.org Subject: Re:
> [squid-users] i want to block images with size more than 40 KB
> 
> On 21/03/2015 12:05 p.m., snakeeyes wrote:
>> Hi amos , thanks for reply I have tried @ top of squidf.conf
>> 
>> acl images rep_header Content-Type ^image/ ^x-image/ acl small 
>> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny
>> small images
>> 
>> are you sure that its blocking images with size >40KB ????
> 
> Sorry I slightly mis-read your request. What I gave is blocking
> images *smaller* than 40 bytes (see what I mean about cut-n-paste
> without understanding?).
> 
> To block images *over* 40 bytes change that to: http_reply_access
> deny !small images
> 
> 
>> also I didn?t see extensions like jpg or bmp or similar like that
>> ??!!
> 
> Because HTTP does not transfer files. It transfers data. Sometimes
> data can *also* be found inside "files", sometimes not.
> 
> HTTP Content-Type header describes what format the data is. In this
> case you requested images in general, so thats the pattern I gave.
> 
> Amos
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEc/cAAoJENNXIZxhPexGXQoH/3Av/ylPj06Pjdd75yqC/Z9n
vDhkFqIQcfxC0ErLcRk1Zp2tNJi8LFFDS4qNys9dlj2A5yloQ5ymXZA39ehJMFgo
s4G6gXWM+KvXaaQ/W5AZodEW2Q3NrOlyhKn58VyHEud4KIg7a8tt7RKywxgY7+Ff
sTEg/FqvWawhkCCmO8WhSzauD9if30vdwjXXLh3BsWD2J3JmC9pylqBn3vGkANMF
Zj+ycq6EkZykPfTSD+wIKw+XovDp3dNFtF7BEyjnCVsJJVW7aJqDMx0fl1N7JAhV
qy9zuIBvop3s4aEZhCufn6+uVIFiaJ2hQ02OyIeAOlBX+moUT5/3We4SrOPWB1M=
=cM6V
-----END PGP SIGNATURE-----


From ahmed.zaeem at netstream.ps  Wed Mar 25 06:25:32 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Tue, 24 Mar 2015 23:25:32 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <5511CFDC.5040006@gmail.com>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
Message-ID: <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps>

Thank you.
Can you help me with this feature ?


cheers

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Tuesday, March 24, 2015 1:58 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Don't think so.

Probably you'll have to write your own helper to handle dynamic content. Or use the content adaptaion feature.

25.03.15 11:34, snakeeyes ?????:
> BTW   can squid block dynamically loaded images, and ajax request
> which return images. I want that on yahoo and google Is that possible 
> ? ========================= , thanks  it seems okay for normal http 
> sites
> 
> I want to ask , is there a trick can we do it so that it be applied to 
> google & yahoo images search ??
> 
> Here is wt I see in yahoo logs , just small logs and all images are 
> allowed and not blocked =====
> 
> 1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT
> js.dmtry.com:443 - DIRECT/184.170.128.58 - 1426881749.077    103
> xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ -
> DIRECT/23.9.123.27 application/ocsp-response 1426881749.752     29
> xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ -
> DIRECT/23.9.123.27 application/ocsp-response 1426881750.098     21
> xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? -
> DIRECT/23.21.149.132 image/gif 1426881750.731  62443 xx.70
> TCP_MISS/200 122185 CONNECT www.gstatic.com:443 -
> DIRECT/206.126.112.185 - 1426881751.476   xx.70 TCP_MISS/200 4191
> CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 - 
> 1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT
> secure.footprint.net:443 - DIRECT/8.12.219.125 - 1426881753.005
> 1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 -
> DIRECT/76.13.28.21 - 1426881762.280  12994 188.161.107.70
> TCP_MISS/200 5502 CONNECT d.adgear.com:443 - DIRECT/205.204.71.140
> - 1426881764.215  16497 xx70 TCP_MISS/200 9832 CONNECT
> ads.yahoo.com:443 - DIRECT/98.139.225.43 - 1426881764.216  16453
> x.70 TCP_MISS/200 6534 CONNECT ads.yahoo.com:443 -
> DIRECT/98.139.225.43 - 1426881765.044  18777 x.70 TCP_MISS/200
> 11132 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
> 1426881765.681  15193 xx.107.70 TCP_MISS/200 6225 CONNECT
> comet.yahoo.com:443 - DIRECT/72.30.196.161 - 1426881765.691  14149
> xx.107.70 TCP_MISS/200 832 CONNECT comet.yahoo.com:443 -
> DIRECT/72.30.196.161 - 1426881766.046 116219 xx.70 TCP_MISS/200 529 
> CONNECT d31qbv1cthcecs.cloudfront.net:443 - DIRECT/54.230.16.189 -
> 1426881766.714    296 xx.70 TCP_MISS/200 2228 POST
> http://ocsp.verisign.com/ - DIRECT/23.9.123.27 
> application/ocsp-response 1426881770.049 117609 xx107.70
> TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 -
> DIRECT/54.240.160.97 - 1426881780.403  67786 xx.70 TCP_MISS/200 852 
> CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 - 1426881781.519
> 353 xx.70 TCP_MISS/200 571 GET
> http://data.cnn.com/jsonp/breaking_news/domestic.json? -
> DIRECT/157.166.249.67 application/javascript 1426881782.057 118788
> xx.70 TCP_MISS/200 19972 CONNECT cdn2sitescout-a.akamaihd.net:443 -
> DIRECT/23.15.4.18 - 1426881790.558  71055 xx TCP_MISS/200 26805 
> CONNECT s.yimg.com:443 - DIRECT/206.190.56.191 - 1426881814.445
> 100461 xx TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 -
> DIRECT/98.139.180.149 - 1426881818.437  70709 xx70 TCP_MISS/200
> 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -
> 
> 
> regards
> 
> -----Original Message----- From: Amos Jeffries 
> [mailto:squid3 at treenet.co.nz] Sent: Friday, March 20, 2015 9:56 AM
> To: snakeeyes Cc: squid-users at lists.squid-cache.org Subject: Re:
> [squid-users] i want to block images with size more than 40 KB
> 
> On 21/03/2015 12:05 p.m., snakeeyes wrote:
>> Hi amos , thanks for reply I have tried @ top of squidf.conf
>> 
>> acl images rep_header Content-Type ^image/ ^x-image/ acl small 
>> rep_header Content-Length ^[1234]?[0-9]$ http_reply_access deny small 
>> images
>> 
>> are you sure that its blocking images with size >40KB ????
> 
> Sorry I slightly mis-read your request. What I gave is blocking images 
> *smaller* than 40 bytes (see what I mean about cut-n-paste without 
> understanding?).
> 
> To block images *over* 40 bytes change that to: http_reply_access deny 
> !small images
> 
> 
>> also I didn?t see extensions like jpg or bmp or similar like that 
>> ??!!
> 
> Because HTTP does not transfer files. It transfers data. Sometimes 
> data can *also* be found inside "files", sometimes not.
> 
> HTTP Content-Type header describes what format the data is. In this 
> case you requested images in general, so thats the pattern I gave.
> 
> Amos
> 
> _______________________________________________ squid-users mailing 
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEc/cAAoJENNXIZxhPexGXQoH/3Av/ylPj06Pjdd75yqC/Z9n
vDhkFqIQcfxC0ErLcRk1Zp2tNJi8LFFDS4qNys9dlj2A5yloQ5ymXZA39ehJMFgo
s4G6gXWM+KvXaaQ/W5AZodEW2Q3NrOlyhKn58VyHEud4KIg7a8tt7RKywxgY7+Ff
sTEg/FqvWawhkCCmO8WhSzauD9if30vdwjXXLh3BsWD2J3JmC9pylqBn3vGkANMF
Zj+ycq6EkZykPfTSD+wIKw+XovDp3dNFtF7BEyjnCVsJJVW7aJqDMx0fl1N7JAhV
qy9zuIBvop3s4aEZhCufn6+uVIFiaJ2hQ02OyIeAOlBX+moUT5/3We4SrOPWB1M=
=cM6V
-----END PGP SIGNATURE-----
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Tue Mar 24 21:46:18 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 03:46:18 +0600
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
 <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps>
Message-ID: <5511DB2A.1050201@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

So far, this has not been done. You can be the first! ;)

25.03.15 12:25, snakeeyes ?????:
> Thank you. Can you help me with this feature ?
> 
> 
> cheers
> 
> -----Original Message----- From: squid-users
> [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
> Yuri Voinov Sent: Tuesday, March 24, 2015 1:58 PM To:
> squid-users at lists.squid-cache.org Subject: Re: [squid-users] i want
> to block images with size more than 40 KB
> 
> Don't think so.
> 
> Probably you'll have to write your own helper to handle dynamic
> content. Or use the content adaptaion feature.
> 
> 25.03.15 11:34, snakeeyes ?????:
>> BTW   can squid block dynamically loaded images, and ajax
>> request which return images. I want that on yahoo and google Is
>> that possible ? ========================= , thanks  it seems okay
>> for normal http sites
> 
>> I want to ask , is there a trick can we do it so that it be
>> applied to google & yahoo images search ??
> 
>> Here is wt I see in yahoo logs , just small logs and all images
>> are allowed and not blocked =====
> 
>> 1426881748.078  70740 x.70 TCP_MISS/200 11790 CONNECT 
>> js.dmtry.com:443 - DIRECT/184.170.128.58 - 1426881749.077    103 
>> xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - 
>> DIRECT/23.9.123.27 application/ocsp-response 1426881749.752
>> 29 xx.70 TCP_MISS/200 2228 POST http://sd.symcd.com/ - 
>> DIRECT/23.9.123.27 application/ocsp-response 1426881750.098
>> 21 xx.70 TCP_MISS/200 393 GET http://ping.chartbeat.net/ping? - 
>> DIRECT/23.21.149.132 image/gif 1426881750.731  62443 xx.70 
>> TCP_MISS/200 122185 CONNECT www.gstatic.com:443 - 
>> DIRECT/206.126.112.185 - 1426881751.476   xx.70 TCP_MISS/200
>> 4191 CONNECT secure.footprint.net:443 - DIRECT/8.12.219.125 - 
>> 1426881752.215    505 xxx.70 TCP_MISS/200 459 CONNECT 
>> secure.footprint.net:443 - DIRECT/8.12.219.125 - 1426881753.005 
>> 1091 xx.70 TCP_MISS/200 5303 CONNECT av.beap.bc.yahoo.com:443 - 
>> DIRECT/76.13.28.21 - 1426881762.280  12994 188.161.107.70 
>> TCP_MISS/200 5502 CONNECT d.adgear.com:443 -
>> DIRECT/205.204.71.140 - 1426881764.215  16497 xx70 TCP_MISS/200
>> 9832 CONNECT ads.yahoo.com:443 - DIRECT/98.139.225.43 -
>> 1426881764.216  16453 x.70 TCP_MISS/200 6534 CONNECT
>> ads.yahoo.com:443 - DIRECT/98.139.225.43 - 1426881765.044  18777
>> x.70 TCP_MISS/200 11132 CONNECT ads.yahoo.com:443 -
>> DIRECT/98.139.225.43 - 1426881765.681  15193 xx.107.70
>> TCP_MISS/200 6225 CONNECT comet.yahoo.com:443 -
>> DIRECT/72.30.196.161 - 1426881765.691  14149 xx.107.70
>> TCP_MISS/200 832 CONNECT comet.yahoo.com:443 - 
>> DIRECT/72.30.196.161 - 1426881766.046 116219 xx.70 TCP_MISS/200
>> 529 CONNECT d31qbv1cthcecs.cloudfront.net:443 -
>> DIRECT/54.230.16.189 - 1426881766.714    296 xx.70 TCP_MISS/200
>> 2228 POST http://ocsp.verisign.com/ - DIRECT/23.9.123.27 
>> application/ocsp-response 1426881770.049 117609 xx107.70 
>> TCP_MISS/200 711 CONNECT d5nxst8fruw4z.cloudfront.net:443 - 
>> DIRECT/54.240.160.97 - 1426881780.403  67786 xx.70 TCP_MISS/200
>> 852 CONNECT www.yahoo.com:443 - DIRECT/98.139.180.149 -
>> 1426881781.519 353 xx.70 TCP_MISS/200 571 GET 
>> http://data.cnn.com/jsonp/breaking_news/domestic.json? - 
>> DIRECT/157.166.249.67 application/javascript 1426881782.057
>> 118788 xx.70 TCP_MISS/200 19972 CONNECT
>> cdn2sitescout-a.akamaihd.net:443 - DIRECT/23.15.4.18 -
>> 1426881790.558  71055 xx TCP_MISS/200 26805 CONNECT
>> s.yimg.com:443 - DIRECT/206.190.56.191 - 1426881814.445 100461 xx
>> TCP_MISS/200 124129 CONNECT ca.yahoo.com:443 - 
>> DIRECT/98.139.180.149 - 1426881818.437  70709 xx70 TCP_MISS/200 
>> 8503 CONNECT beap-bc.yahoo.com:443 - DIRECT/206.190.57.60 -
> 
> 
>> regards
> 
>> -----Original Message----- From: Amos Jeffries 
>> [mailto:squid3 at treenet.co.nz] Sent: Friday, March 20, 2015 9:56
>> AM To: snakeeyes Cc: squid-users at lists.squid-cache.org Subject:
>> Re: [squid-users] i want to block images with size more than 40
>> KB
> 
>> On 21/03/2015 12:05 p.m., snakeeyes wrote:
>>> Hi amos , thanks for reply I have tried @ top of squidf.conf
>>> 
>>> acl images rep_header Content-Type ^image/ ^x-image/ acl small
>>>  rep_header Content-Length ^[1234]?[0-9]$ http_reply_access
>>> deny small images
>>> 
>>> are you sure that its blocking images with size >40KB ????
> 
>> Sorry I slightly mis-read your request. What I gave is blocking
>> images *smaller* than 40 bytes (see what I mean about cut-n-paste
>> without understanding?).
> 
>> To block images *over* 40 bytes change that to: http_reply_access
>> deny !small images
> 
> 
>>> also I didn?t see extensions like jpg or bmp or similar like
>>> that ??!!
> 
>> Because HTTP does not transfer files. It transfers data.
>> Sometimes data can *also* be found inside "files", sometimes
>> not.
> 
>> HTTP Content-Type header describes what format the data is. In
>> this case you requested images in general, so thats the pattern I
>> gave.
> 
>> Amos
> 
>> _______________________________________________ squid-users
>> mailing list squid-users at lists.squid-cache.org 
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEdsqAAoJENNXIZxhPexG1HUH+wd53AlTsoahDpNqarPUZaow
TIX0kX1Qc9pIUteHEIWG/Rba2af4P2d6xZC8ZFOwsTyNYw0wfd2a9cUiN4qfM1ez
Y/NJTqS0kOMh81GL+GXaDkfatSVwk0LGkejoApNsKZHxCaIQ2lkOnNcJBuhQjA0o
enMmE0YYy7qj0+Tgbez4rDD2JRw7QtLuZ1QaYk7WuzNK0IS+xMAe9ioORu9hlYS7
8uCPgg00O1ISy1fHcIPYtkdn5tAGGRxPP5Km0ueC4ZNMzOIrsHlW7NrqvSDGdQjD
oQDg40ACtvA/qjIzP0//N9orEWetyakAdPgbHs5mgIYV8UJO1ZXZVNb6C9Q6vRw=
=zU9w
-----END PGP SIGNATURE-----


From leolistas at solutti.com.br  Tue Mar 24 21:57:14 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 24 Mar 2015 18:57:14 -0300
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <000601d066bd$675f1100$361d3300$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps>
Message-ID: <5511DDBA.8040809@solutti.com.br>

On 25/03/15 02:34, snakeeyes wrote:
> BTW   can squid block dynamically loaded images, and ajax request which return images.
> I want that on yahoo and google
> Is that possible ?

     Yes snakeeyes, that's surely possible. Maybe not easy, but only a 
few things are not possible.

     However, as said in previous emails, this is the time that you 
should stop trying to ask for 'ready-to-use-rules' and start 
understanding that, as already said previously, blocking the way you 
want is not trivial, as internet browsing can look trivial to the user 
but it's definitely not on a technical view.

     Images can come in https connections and, thus, squid cannot see 
them. If you use SSL-Bump, filtering based on the content of SSL 
connections becames possible. That's not a trivial configuration, but 
that's surely possible.


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From eliezer at ngtech.co.il  Tue Mar 24 22:00:00 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Mar 2015 00:00:00 +0200
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <5511DB2A.1050201@gmail.com>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
 <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps> <5511DB2A.1050201@gmail.com>
Message-ID: <5511DE60.8000407@ngtech.co.il>

I am pretty sure that squid doesn't have the function\option but maybe 
there are others that do posses or able to fulfill this feature.

As much as I want new features and improvements in squid it is still 
possible for others to be able to write these bits of code.

Not been done.. ie in squid... squid is not the only software on the 
planet that does all sorts of ACLs.
Even if this page:
http://ngtech.co.il/squid/who_is_running_it/

Is the reality it doesn't mean that everybody should use squid and not 
seek or look at other options.
Even all these that are mentioned in the page above have their own 
developments which are not related to squid or Linux or BSD or any other OS.

Even if someone doesn't like it this is still the reality and even Linux 
with all his helpers are humans like any other human on the planet and 
it is possible in every moment that they can make a mistake and we are 
here to help them and all the other humans that are on the plant in this 
case that a mistake is happening.

Eliezer Croitoru

On 24/03/2015 23:46, Yuri Voinov wrote:
> So far, this has not been done. You can be the first!;)



From dan at getbusi.com  Tue Mar 24 22:28:22 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Wed, 25 Mar 2015 09:28:22 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <A9C5FA23-DD01-4D1F-89FA-0D05970B8BCB@getbusi.com>
References: <550BCC16.3040103@treenet.co.nz>
 <1426836862188.51009efe@Nodemailer>
 <550BCE55.7020105@treenet.co.nz>
 <A9C5FA23-DD01-4D1F-89FA-0D05970B8BCB@getbusi.com>
Message-ID: <CAN8nrKDEH9t=SkZ0DJGA44ieTb-Q9pYWRPyHbz1vQJ04BFFJ8g@mail.gmail.com>

Resending this after the last attempt went into the mail server black hole:

Hey Amos

I decided I?m not confident enough in 3.5.HEAD, after last time, to go back
into production with it. Going to to do some more local testing first.

That being said, I now have 3.4.12 in production with optimisations
disabled and it seems to be doing fine performance and stability-wise. I
only managed to capture one crash with optimisations disabled, so far, but
it seemed to have some memory-related corruption, unfortunately.

Updates to come over the next few days.


On 23 March 2015 at 16:59, Dan Charlesworth <dan at getbusi.com> wrote:

> Hey Amos
>
> I decided I?m not confident enough in 3.5.HEAD, after last time, to go
> back into production with it. Going to to do some more local testing first.
>
> That being said, I now have 3.4.12 in production with optimisations
> disabled and it seems to be doing fine performance and stability-wise. I
> only managed to capture one crash with optimisations disabled, so far, but
> it seemed to have some memory-related corruption, unfortunately.
>
> More to come tomorrow :-)
>
> > On 20 Mar 2015, at 6:37 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> >
> > On 20/03/2015 8:34 p.m., Dan Charlesworth wrote:
> >> Thanks Amos.
> >>
> >>
> >> I'll put together a build with the upcoming snapshot on Monday, might
> even try disabling optimization for it too.
> >
> > Please do. If you're only getting 40 RPS out of the proxy during the
> > test its hard to see how not optimizing the code could be any worse, and
> > it will help identifiying some traffic details.
> >
> > Amos
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150325/fec01441/attachment.htm>

From ahmed.zaeem at netstream.ps  Wed Mar 25 07:29:41 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Wed, 25 Mar 2015 00:29:41 -0700
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <5511DE60.8000407@ngtech.co.il>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
 <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps> <5511DB2A.1050201@gmail.com>
 <5511DE60.8000407@ngtech.co.il>
Message-ID: <001201d066cd$7618f3e0$624adba0$@netstream.ps>

Hey Guys
If  I use https on squid with certificate and I import it to my exploreer

Would it be possible ?

Will squid in this case will be able to detect images as normal http ??


cheers



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Tuesday, March 24, 2015 3:00 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] i want to block images with size more than 40 KB

I am pretty sure that squid doesn't have the function\option but maybe there are others that do posses or able to fulfill this feature.

As much as I want new features and improvements in squid it is still possible for others to be able to write these bits of code.

Not been done.. ie in squid... squid is not the only software on the planet that does all sorts of ACLs.
Even if this page:
http://ngtech.co.il/squid/who_is_running_it/

Is the reality it doesn't mean that everybody should use squid and not seek or look at other options.
Even all these that are mentioned in the page above have their own developments which are not related to squid or Linux or BSD or any other OS.

Even if someone doesn't like it this is still the reality and even Linux with all his helpers are humans like any other human on the planet and it is possible in every moment that they can make a mistake and we are here to help them and all the other humans that are on the plant in this case that a mistake is happening.

Eliezer Croitoru

On 24/03/2015 23:46, Yuri Voinov wrote:
> So far, this has not been done. You can be the first!;)

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Tue Mar 24 22:34:48 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Mar 2015 04:34:48 +0600
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <001201d066cd$7618f3e0$624adba0$@netstream.ps>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
 <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps> <5511DB2A.1050201@gmail.com>
 <5511DE60.8000407@ngtech.co.il>
 <001201d066cd$7618f3e0$624adba0$@netstream.ps>
Message-ID: <5511E688.6000602@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

You are talking about this:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Sure. Possible.

25.03.15 13:29, snakeeyes ?????:
> Hey Guys If  I use https on squid with certificate and I import it
> to my exploreer
> 
> Would it be possible ?
> 
> Will squid in this case will be able to detect images as normal
> http ??

Sure.

> 
> 
> cheers
> 
> 
> 
> -----Original Message----- From: squid-users
> [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of
> Eliezer Croitoru Sent: Tuesday, March 24, 2015 3:00 PM To:
> squid-users at lists.squid-cache.org Subject: Re: [squid-users] i want
> to block images with size more than 40 KB
> 
> I am pretty sure that squid doesn't have the function\option but
> maybe there are others that do posses or able to fulfill this
> feature.
> 
> As much as I want new features and improvements in squid it is
> still possible for others to be able to write these bits of code.
> 
> Not been done.. ie in squid... squid is not the only software on
> the planet that does all sorts of ACLs. Even if this page: 
> http://ngtech.co.il/squid/who_is_running_it/
> 
> Is the reality it doesn't mean that everybody should use squid and
> not seek or look at other options. Even all these that are
> mentioned in the page above have their own developments which are
> not related to squid or Linux or BSD or any other OS.
> 
> Even if someone doesn't like it this is still the reality and even
> Linux with all his helpers are humans like any other human on the
> planet and it is possible in every moment that they can make a
> mistake and we are here to help them and all the other humans that
> are on the plant in this case that a mistake is happening.
> 
> Eliezer Croitoru
> 
> On 24/03/2015 23:46, Yuri Voinov wrote:
>> So far, this has not been done. You can be the first!;)
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________ squid-users mailing
> list squid-users at lists.squid-cache.org 
> http://lists.squid-cache.org/listinfo/squid-users
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCAAGBQJVEeaIAAoJENNXIZxhPexGgmsH/2BrsXRNj8DTWjJT555j0CUj
BHUOlQQPAZaX4fI+xTPLqz59alHPFTRwbIBtX79f18t3FP5LtFayQN6+xlwoJVE2
FqfP+4xDCFq+OMmni5nlIFtKq4QLf4igBMXU7xFMe8rwEs0paAxzsZn9Yz3vHINn
e9X/a90tha/hT+hoP1zcFDEr/Ny+XUuBkTOcdHKOCb0BHpeefrY85reLxTDzZlxd
GVGE1ztRq0R/YxVeHb6zVYJMzGjTI2XP0tUHWKz1o2KziZJGDzvDWzDkmALwS4o7
YZ6zVaULsyT+mWsq8QYJ9LkGi9361yvr53aIxnIIpeoQnij2tN6FoyOE0jWRPEY=
=fSSy
-----END PGP SIGNATURE-----


From squid3 at treenet.co.nz  Wed Mar 25 00:58:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Mar 2015 13:58:04 +1300
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
Message-ID: <5512081C.7030008@treenet.co.nz>

On 25/03/2015 9:05 a.m., Monah Baki wrote:
> Thanks Yuri for the URL. The company is a small ISP using policy based
> routing, so using WPAD or GPO isn't feasible.


Did you start reading with the problem explanation?
 the bit about whats Squid's testing for and how to interpret the log lines?

Your log is saying that there is a client sending requests on port 80
which claim to be requests *on port 443*. Even if the IP matches
facebook the port dont.

Amos


From monahbaki at gmail.com  Wed Mar 25 01:05:12 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Tue, 24 Mar 2015 21:05:12 -0400
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <5512081C.7030008@treenet.co.nz>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
 <5512081C.7030008@treenet.co.nz>
Message-ID: <CALP3=x_E97aPd4M3aTP-s8m4wka3NP=rhXasDVQw5mMmXPDcxA@mail.gmail.com>

Thanks Amos,

My problem is I only have control over the squid server. I can only
tell the ISP to take the client offline and run some AntiVirus or
better reimage the device.

Within 2 hours my cache.log grew to 50MB in size and it was repeating
the error mentioned over and over again till my squid server started
complaining about running out of file descriptors, and stopped
working.


Thanks

On Tue, Mar 24, 2015 at 8:58 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 25/03/2015 9:05 a.m., Monah Baki wrote:
>> Thanks Yuri for the URL. The company is a small ISP using policy based
>> routing, so using WPAD or GPO isn't feasible.
>
>
> Did you start reading with the problem explanation?
>  the bit about whats Squid's testing for and how to interpret the log lines?
>
> Your log is saying that there is a client sending requests on port 80
> which claim to be requests *on port 443*. Even if the IP matches
> facebook the port dont.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Mar 25 02:03:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Mar 2015 15:03:01 +1300
Subject: [squid-users] load balancing and site failover
In-Reply-To: <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>
References: <1427206737.17590.33.camel@desktop.bpk2.com>
 <1427221530.17590.37.camel@desktop.bpk2.com>
 <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>
Message-ID: <55121755.8010905@treenet.co.nz>

On 25/03/2015 9:55 a.m., brendan kearney wrote:
> Was not sure if bugzilla was used for mailing list issues.  If you would
> like me to open one, I will but it looks like the list is working again.

Bugzilla is used, list bugs under the "project services" product.


As for your query...

> On Mar 24, 2015 2:25 PM, "Brendan Kearney" wrote:
> 
>> On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
>>> while load balancing is not a requirement in a proxy environment, it
>>> does afford a great deal of functionality, scaling and fault tolerance
>>> in one.  several if not many on this list probably employ them for their
>>> proxies and likely other technologies, but they are not all created
>>> equal.
>>>
>>> i recently looked to see if a specific feature was in HAProxy.  i was
>>> looking to see if HAProxy could reply to a new connection with a RST
>>> packet if no pool member was available.
>>>
>>> the idea behind this is, if all of the proxies are not passing the
>>> service check and are marked down by the load balancer, the reply of a
>>> RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
>>> tells the browser to failover to the next proxy assigned by the PAC
>>> file.
>>>
>>> where i work, we have this configuration working.  the load balancers
>>> are configured with the option to send a reset when no proxy is
>>> available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
>>> a specific order based on which proxy VIP is assigned as the primary.
>>> In every case, if the primary VIP does not have an available pool
>>> member, the browser fails over to the next in the list.  failover would
>>> happen again, if the secondary VIP replies with a RST during the
>>> connection establishing.  the process repeats until a TCP connection
>>> establishes or all proxies assigned have been exhausted.  the browser
>>> will use the proxy VIP that it successfully connects to, for the
>>> duration of the session.  once the browser is closed and reopened, the
>>> evaluation of the PAC file occurs again, and the process starts anew.
>>> plug-ins such as Proxy Selector are the exception to this, and can be
>>> used to reevaluate a PAC file by selecting it for use.
>>>
>>> we have used this configuration several times, when we found an ISP link
>>> was flapping or some other issue more global in nature than just the
>>> proxies was affecting our egress and internet access.  i can attest to
>>> the solution as working and elegantly handling site wide failures.
>>>
>>> being that the solutions where i work are proprietary commercial
>>> products, i wanted to find an open source product that does this.  i
>>> have been a long time user of HAProxy, and have recommended it for
>>> others here, but sadly they cannot perform this function.  per their
>>> mailing list, they use the network stack of the OS for connection
>>> establishment and cannot cause a RST to be sent to the client during a
>>> TCP handshake if no pool member is available.
>>>
>>> they suggested an external helper that manipulates IPTables rules based
>>> on a pool member being available.  they do not feel that a feature like
>>> this belongs in a layer 4/7 reverse proxy application.

They are right. HTTP != TCP.

In particular TCP depends on routers having a full routing map of the
entire Internet (provided by BGP) and deciding the best upstream hop
based on that global info. Clients have one (and only one) upstream
router for each server they want to connect to.

In HTTP each proxy (aka router) performs independent upstream connection
attempts, failover, and verifies it worked before responding to the
client with a final response. Each proxy only has enough detail to check
its upstream(s). Each proxy can connect to any server (subject to ACLs).


>>>
>>> my search for a load balancer solution went through ipvsadm, balance and
>>> haproxy before i selected haproxy.  haproxy was more feature rich than
>>> balance, and easier to implement than ipvsadm.  do any other list
>>> members have a need for such a feature from their load balancers?  do
>>> any other list members have site failover solutions that have been
>>> tested or used and would consider sharing their design and/or pain
>>> points?  i am not looking for secret sauce or confidential info, but
>>> more high level architecture decisions and such.


I havent tested it but this should do what you are asking:

 acl err http_status 500-505 408
 deny_info TCP_RESET err
 http_reply_access deny err

It replaces the response from Squid with a TCP RST packet.

Amos


From squid3 at treenet.co.nz  Wed Mar 25 02:18:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Mar 2015 15:18:43 +1300
Subject: [squid-users] I am seeing the following in my cache.log
In-Reply-To: <CALP3=x_E97aPd4M3aTP-s8m4wka3NP=rhXasDVQw5mMmXPDcxA@mail.gmail.com>
References: <CALP3=x8yk9_V3J_8kBHUEQgUFVPpdgMn9ZUD_xkpeGXeHedJ2A@mail.gmail.com>
 <5511B9D8.3040607@gmail.com>
 <CALP3=x9th_u5NwLQdKgYmmHocmV3g-cVCvOtL3kJSM42nQdnmA@mail.gmail.com>
 <5512081C.7030008@treenet.co.nz>
 <CALP3=x_E97aPd4M3aTP-s8m4wka3NP=rhXasDVQw5mMmXPDcxA@mail.gmail.com>
Message-ID: <55121B03.8060501@treenet.co.nz>

On 25/03/2015 2:05 p.m., Monah Baki wrote:
> Thanks Amos,
> 
> My problem is I only have control over the squid server. I can only
> tell the ISP to take the client offline and run some AntiVirus or
> better reimage the device.

The security problem is that your proxy is receiving over port 80
(*unencrypted* origin server) a request the client apparently sent on
port 443 (encrypted origin server).

This may be caused by the client browser running a script which is
hjacking it. Or somebody between your proxy and the client MITM'ing the
connection and sending decrypted content out over the network in the
clear. Neither is a desirable situation.

> 
> Within 2 hours my cache.log grew to 50MB in size and it was repeating
> the error mentioned over and over again till my squid server started
> complaining about running out of file descriptors, and stopped
> working.

Your proxy is configured such that it adds the Via header properly for
loop detection.

However, if there is another proxy stripping away that header and a loop
happens it would directly lead to both the FD exhaustion and the
extremely large amount of log entries (once per loop).

Amos



From eliezer at ngtech.co.il  Wed Mar 25 02:44:55 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 25 Mar 2015 04:44:55 +0200
Subject: [squid-users] i want to block images with size more than 40 KB
In-Reply-To: <5511E688.6000602@gmail.com>
References: <000201d0619c$a394e120$eabea360$@netstream.ps>
 <55095C23.6040407@treenet.co.nz> <55098C7A.3070502@solutti.com.br>
 <001101d061dc$99d4bda0$cd7e38e0$@netstream.ps>
 <550A5343.4020206@treenet.co.nz>
 <000001d06362$64bff170$2e3fd450$@netstream.ps>
 <550C5102.7020402@treenet.co.nz>
 <000601d066bd$675f1100$361d3300$@netstream.ps> <5511CFDC.5040006@gmail.com>
 <000c01d066c4$7f8d8a70$7ea89f50$@netstream.ps> <5511DB2A.1050201@gmail.com>
 <5511DE60.8000407@ngtech.co.il>
 <001201d066cd$7618f3e0$624adba0$@netstream.ps> <5511E688.6000602@gmail.com>
Message-ID: <55122127.6060108@ngtech.co.il>

Squid has never been able to understand the content of the http message.
If squid indeed can do that then it's not in the src tree I know.

Eliezer

On 25/03/2015 00:34, Yuri Voinov wrote:
> You are talking about this:
>
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> Sure. Possible.



From squid3 at treenet.co.nz  Wed Mar 25 09:08:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Mar 2015 22:08:43 +1300
Subject: [squid-users] squid just stopping
In-Reply-To: <CAAq-SCDNWyoRv70e30ny9FYOuzL2PFSeL9HB5BMczKSWS-HKcQ@mail.gmail.com>
References: <CAAq-SCBy3uJQ85YJDwJZC4L+W_Ft9UuNaoNm7rqtsLAA2iyM6Q@mail.gmail.com>	<550C4A38.3020203@treenet.co.nz>	<CAAq-SCBupfvtqopQ86xHsoRAr7TmyWXirqo_aE_0QjxfE0_3UA@mail.gmail.com>	<5512049F.8050507@treenet.co.nz>
 <CAAq-SCDNWyoRv70e30ny9FYOuzL2PFSeL9HB5BMczKSWS-HKcQ@mail.gmail.com>
Message-ID: <55127B1B.1040808@treenet.co.nz>

On 25/03/2015 7:58 p.m., Ian Harper wrote:
> Nah it says its already running and bombed out so only one running, after a
> lot more digging and research I found I had to mod the config file to have
> separate logs, spools, and pid files and then I could get the two up and
> working ok.

You mean you are trying to run multiple instances of Squid on one machine?

<http://wiki.squid-cache.org/MultipleInstances>

Amos



From johnzeng2013 at yahoo.com  Wed Mar 25 10:19:08 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 25 Mar 2015 18:19:08 +0800
Subject: [squid-users] rock communication channel establishment timeout
Message-ID: <55128B9C.3040702@yahoo.com>



Hello Dear All

i meet a strange problem , when i run cache_dir rock , and i found error
info

rock communication channel establishment timeout

if possible, please give me some advisement


This is my config


cache_dir rock /accerater/webcache3/storage/cossbig1/opmizer1 9485
min-size=4097 max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cosssmall1/opmizer1 499
max-size=4096 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cossbig2/opmizer1 9090
min-size=4097 max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cosssmall2/opmizer1 478
max-size=4096 max-swap-rate=250 swap-timeout=350


this is error info

2015/03/25 18:02:41 kid4| ERROR:
/accerater/webcache3/storage/cossbig1/opmizer1/rock communication
channel establishment timeout
2015/03/25 18:02:41 kid4| Closing HTTP port [::]:3233
2015/03/25 18:02:41 kid4| Closing HTTP port [::]:3133
FATAL: Rock cache_dir at
/accerater/webcache3/storage/cossbig1/opmizer1/rock failed to open db
file: (11) Resource temporarily unavailable
2015/03/25 18:02:42 kid3| ERROR:
/accerater/webcache3/storage/cossbig1/opmizer2/rock communication
channel establishment timeout
2015/03/25 18:02:42 kid3| Closing HTTP port [::]:3232
2015/03/25 18:02:42 kid3| Closing HTTP port [::]:3132
FATAL: Rock cache_dir at
/accerater/webcache3/storage/cossbig1/opmizer2/rock failed to open db
file: (11) Resource temporarily unavaila


From johnzeng2013 at yahoo.com  Wed Mar 25 10:20:19 2015
From: johnzeng2013 at yahoo.com (johnzeng)
Date: Wed, 25 Mar 2015 18:20:19 +0800
Subject: [squid-users] rock communication channel establishment timeout
In-Reply-To: <55128B9C.3040702@yahoo.com>
References: <55128B9C.3040702@yahoo.com>
Message-ID: <55128BE3.3070801@yahoo.com>




Hello Dear All

i meet a strange problem , when i run cache_dir rock , and i found error
info

rock communication channel establishment timeout

if possible, please give me some advisement


This is my config


cache_dir rock /accerater/webcache3/storage/cossbig1/opmizer1 9485
min-size=4097 max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cosssmall1/opmizer1 499
max-size=4096 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cossbig2/opmizer1 9090
min-size=4097 max-size=262144 max-swap-rate=250 swap-timeout=350
cache_dir rock /accerater/webcache3/storage/cosssmall2/opmizer1 478
max-size=4096 max-swap-rate=250 swap-timeout=350


this is error info

2015/03/25 18:02:41 kid4| ERROR:
/accerater/webcache3/storage/cossbig1/opmizer1/rock communication
channel establishment timeout
2015/03/25 18:02:41 kid4| Closing HTTP port [::]:3233
2015/03/25 18:02:41 kid4| Closing HTTP port [::]:3133
FATAL: Rock cache_dir at
/accerater/webcache3/storage/cossbig1/opmizer1/rock failed to open db
file: (11) Resource temporarily unavailable
2015/03/25 18:02:42 kid3| ERROR:
/accerater/webcache3/storage/cossbig1/opmizer2/rock communication
channel establishment timeout
2015/03/25 18:02:42 kid3| Closing HTTP port [::]:3232
2015/03/25 18:02:42 kid3| Closing HTTP port [::]:3132
FATAL: Rock cache_dir at
/accerater/webcache3/storage/cossbig1/opmizer2/rock failed to open db
file: (11) Resource temporarily unavaila





From priya_agarwal at students.iitmandi.ac.in  Wed Mar 25 10:26:05 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Wed, 25 Mar 2015 15:56:05 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <55067332.1020500@treenet.co.nz>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
 <5502B3EE.9040908@treenet.co.nz>
 <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
 <5502D129.7030809@treenet.co.nz>
 <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
 <55067332.1020500@treenet.co.nz>
Message-ID: <CALTPfpEs5iz9PDVyP0ta-qyA=RBdjUL4NcUFb8a6DyT4xmgq3A@mail.gmail.com>

I could link the libraries. The problem was that libraries were C and thus
g++ wasn't creating right symbols.

I have another question.
what part of the source code handles shutdown or exiting (ctrl+c).
Thanks.

On Mon, Mar 16, 2015 at 11:37 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 16/03/2015 6:36 p.m., Priya Agarwal wrote:
> > Hi,
> >
> > I am now linking the libraries during ./configure with LDFLAGS, LIBS and
> > CXXFLAGS options (Makefile.am is same as it was)  Compile is failing
> > presently.
> >
> > main.o: In function `of_init':
> >
> /media/NewVolume/yocto/build_t4240qds_release/tmp/sysroots/t4240qds/usr/include/usdpaa/of.h:52:
> > undefined reference to `of_init_path(char const*)'
> > collect2: error: ld returned 1 exit status
> > powerpc-fsl_networking-linux-libtool: link: rm -f ".libs/squidS.o"
> >
> > Does it mean I haven't linked the the library that has of_init() [But I
> > think I have done that as ./configure passed and libraries are even
> visible
> > in log]
> > Or I need to link something else?
>
> Close. It means that the libraries listed on the command line after the
> main.o object do not define that symbol.
>
> Its possible the library was linked first or via the wrong variable. Or
> that the C++ symbol mangling is different between what is wanted and
> what the library defines.
>
>
> >
> > Here are the flags passed to ./configure:
> > EXTRA_OECONF_append = "    LDFLAGS="-L=/usr/lib/" \
> >             LIBS="-lusdpaa_dma -lusdpaa_dma_mem -lusdpaa_of -lusdpaa_fman
> > -lusdpaa_qbman -lusdpaa_syscfg" \
> >             CXXFLAGS="-I=/usr/include/""
> >
>
> I see 12 separate command line arguments there. Not the 3 that should
> exist.
>
> Hint: shell expansion is applied to " symbols on the ./configure command
> line.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150325/6278e1d1/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 25 11:02:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Mar 2015 00:02:33 +1300
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpEs5iz9PDVyP0ta-qyA=RBdjUL4NcUFb8a6DyT4xmgq3A@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>	<5502B3EE.9040908@treenet.co.nz>	<CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>	<5502D129.7030809@treenet.co.nz>	<CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>	<55067332.1020500@treenet.co.nz>
 <CALTPfpEs5iz9PDVyP0ta-qyA=RBdjUL4NcUFb8a6DyT4xmgq3A@mail.gmail.com>
Message-ID: <551295C9.8000004@treenet.co.nz>

On 25/03/2015 11:26 p.m., Priya Agarwal wrote:
> I could link the libraries. The problem was that libraries were C and thus
> g++ wasn't creating right symbols.

.h files fro C libraries need to contain the syntax:

#if defined(__cplusplus)
extern "C" {
#endif

  ... library API ...

#if defined(__cplusplus)
}
#endif

> 
> I have another question.
> what part of the source code handles shutdown or exiting (ctrl+c).

src/main.cc

Amos


From priya_agarwal at students.iitmandi.ac.in  Wed Mar 25 15:47:21 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Wed, 25 Mar 2015 21:17:21 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <551295C9.8000004@treenet.co.nz>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
 <5502B3EE.9040908@treenet.co.nz>
 <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
 <5502D129.7030809@treenet.co.nz>
 <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>
 <55067332.1020500@treenet.co.nz>
 <CALTPfpEs5iz9PDVyP0ta-qyA=RBdjUL4NcUFb8a6DyT4xmgq3A@mail.gmail.com>
 <551295C9.8000004@treenet.co.nz>
Message-ID: <CALTPfpE8J-FFT7doe8t-L2cfiQTk-ZMh0cvSKCk_hm1CRbZVog@mail.gmail.com>

I can't change the header files of library. Therefore when I included it in
my code I did
extern "C" {
#include<c header file>
...
}

instead of just #include and it worked.

But the compiler is showing some errors in the header files I included, but
of course there isn't anything wrong in them. I could include it this way
as well right?

Also I wrote a c file and a corresponding header file in /src folder and
appended the 'squid_sources' as this:

squid_sources = .
                         ...
                         ....
                        usdpaa_if.h \
                        usdpaa_if.c

Is this okay? As again it is showing errors in the usdpaa_if.c file and
there are no syntax errors like the compiler says.
I saw other folder in /src like compat had also included c files like this
so I thought this would be right.

Regards


On Wed, Mar 25, 2015 at 4:32 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 25/03/2015 11:26 p.m., Priya Agarwal wrote:
> > I could link the libraries. The problem was that libraries were C and
> thus
> > g++ wasn't creating right symbols.
>
> .h files fro C libraries need to contain the syntax:
>
> #if defined(__cplusplus)
> extern "C" {
> #endif
>
>   ... library API ...
>
> #if defined(__cplusplus)
> }
> #endif
>
> >
> > I have another question.
> > what part of the source code handles shutdown or exiting (ctrl+c).
>
> src/main.cc
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150325/d09ecf42/attachment.htm>

From bpk678 at gmail.com  Wed Mar 25 21:26:07 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Wed, 25 Mar 2015 17:26:07 -0400
Subject: [squid-users] load balancing and site failover
In-Reply-To: <55121755.8010905@treenet.co.nz>
References: <1427206737.17590.33.camel@desktop.bpk2.com>
 <1427221530.17590.37.camel@desktop.bpk2.com>
 <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>
 <55121755.8010905@treenet.co.nz>
Message-ID: <1427318767.24562.31.camel@desktop.bpk2.com>

On Wed, 2015-03-25 at 15:03 +1300, Amos Jeffries wrote:
> On 25/03/2015 9:55 a.m., brendan kearney wrote:
> > Was not sure if bugzilla was used for mailing list issues.  If you would
> > like me to open one, I will but it looks like the list is working again.
> 
> Bugzilla is used, list bugs under the "project services" product.
> 
> 
> As for your query...
> 
> > On Mar 24, 2015 2:25 PM, "Brendan Kearney" wrote:
> > 
> >> On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
> >>> while load balancing is not a requirement in a proxy environment, it
> >>> does afford a great deal of functionality, scaling and fault tolerance
> >>> in one.  several if not many on this list probably employ them for their
> >>> proxies and likely other technologies, but they are not all created
> >>> equal.
> >>>
> >>> i recently looked to see if a specific feature was in HAProxy.  i was
> >>> looking to see if HAProxy could reply to a new connection with a RST
> >>> packet if no pool member was available.
> >>>
> >>> the idea behind this is, if all of the proxies are not passing the
> >>> service check and are marked down by the load balancer, the reply of a
> >>> RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
> >>> tells the browser to failover to the next proxy assigned by the PAC
> >>> file.
> >>>
> >>> where i work, we have this configuration working.  the load balancers
> >>> are configured with the option to send a reset when no proxy is
> >>> available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
> >>> a specific order based on which proxy VIP is assigned as the primary.
> >>> In every case, if the primary VIP does not have an available pool
> >>> member, the browser fails over to the next in the list.  failover would
> >>> happen again, if the secondary VIP replies with a RST during the
> >>> connection establishing.  the process repeats until a TCP connection
> >>> establishes or all proxies assigned have been exhausted.  the browser
> >>> will use the proxy VIP that it successfully connects to, for the
> >>> duration of the session.  once the browser is closed and reopened, the
> >>> evaluation of the PAC file occurs again, and the process starts anew.
> >>> plug-ins such as Proxy Selector are the exception to this, and can be
> >>> used to reevaluate a PAC file by selecting it for use.
> >>>
> >>> we have used this configuration several times, when we found an ISP link
> >>> was flapping or some other issue more global in nature than just the
> >>> proxies was affecting our egress and internet access.  i can attest to
> >>> the solution as working and elegantly handling site wide failures.
> >>>
> >>> being that the solutions where i work are proprietary commercial
> >>> products, i wanted to find an open source product that does this.  i
> >>> have been a long time user of HAProxy, and have recommended it for
> >>> others here, but sadly they cannot perform this function.  per their
> >>> mailing list, they use the network stack of the OS for connection
> >>> establishment and cannot cause a RST to be sent to the client during a
> >>> TCP handshake if no pool member is available.
> >>>
> >>> they suggested an external helper that manipulates IPTables rules based
> >>> on a pool member being available.  they do not feel that a feature like
> >>> this belongs in a layer 4/7 reverse proxy application.
> 
> They are right. HTTP != TCP.
i didnt confuse that detail.  it was unknown to me that HAProxy could
not tie layer 7 status to layer 3/4 actions.  the decisions they made
and how they architected the app is why they cannot do this, not that it
is technically impossible to do it.  i may be spoiled because i work
with equipment that can do this for me.
> 
> In particular TCP depends on routers having a full routing map of the
> entire Internet (provided by BGP) and deciding the best upstream hop
> based on that global info. Clients have one (and only one) upstream
> router for each server they want to connect to.
i will contest this.  my router does not need a full BGP map to route
traffic locally on my LAN or remotely out its WAN interface.  hell, it
does not even run BGP, and i can still get to the intarwebs, no problem.
it too, only has one upstream router / default route.
> 
> In HTTP each proxy (aka router) performs independent upstream connection
> attempts, failover, and verifies it worked before responding to the
> client with a final response. Each proxy only has enough detail to check
> its upstream(s). Each proxy can connect to any server (subject to ACLs).
how are you comparing a HTTP proxy (a layer 7 application) to a router
(a layer 3 device)?  routers route traffic and proxies proxy traffic.
very different functions.  routers dont look past a certain point in the
headers in order to make decisions on where to send the traffic.
proxies look all the way to the end of the headers and sometimes into
the payload, too.  proxies are more akin to a protocol specific
firewall.  proxies also dont send the incoming traffic out an interface.
they terminate the client session, and initiate a new session on behalf
of the client.  simply because the proxy can elect how to send a request
it is making on behalf of a client, does not make a proxy a router.  the
fact that one connection is terminated and a new one is initiated rules
out a proxy from being any kind of router, in my opinion.  even with SSL
or the CONNECT Method, the connection is still made by the proxy to the
remote server.  the client never makes a connection to the remote
server, therefore the traffic was not routed.  it was proxied.
> 
> >>>
> >>> my search for a load balancer solution went through ipvsadm, balance and
> >>> haproxy before i selected haproxy.  haproxy was more feature rich than
> >>> balance, and easier to implement than ipvsadm.  do any other list
> >>> members have a need for such a feature from their load balancers?  do
> >>> any other list members have site failover solutions that have been
> >>> tested or used and would consider sharing their design and/or pain
> >>> points?  i am not looking for secret sauce or confidential info, but
> >>> more high level architecture decisions and such.
> 
> 
> I havent tested it but this should do what you are asking:
> 
>  acl err http_status 500-505 408
>  deny_info TCP_RESET err
>  http_reply_access deny err
> 
> It replaces the response from Squid with a TCP RST packet.
this is useful in the case that the proxy is alive and well, but cannot
get to the internet.  in my example, the ISP issue would seem to be
covered, though i am not sure how the actual implementation would go.
the client has a TCP session established with the load balancer, which
gets the full SYN -> SYN/ACK -> ACK treatement.  the load balancer would
get the SYN -> RST from the proxy, and presumably sends the RST back to
the client.  While this does seem to hold up logically, the
implementation may have nuances that have to be dealt with.  Does the
RST in the middle of an established TCP session cause the browser to
failover to the next proxy assigned?  i would have to test that out.

now, what about the case where the proxies are not alive and well behind
the load balancer, and they are not able to reply with a RST?  This is
the scenario that i would want the load balancer to be able to manage.
this is where tying a layer 7 status to a layer 3/4 action on the load
balancer becomes relevant.  then, the ability for the load balancer to
do this negates the need to manage this in the proxy layer, and removes
any nuances that may be encountered with the implementation.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users





From dan at getbusi.com  Wed Mar 25 23:10:54 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Thu, 26 Mar 2015 10:10:54 +1100
Subject: [squid-users] assertion failed: client_side.cc:1515:
	"connIsUsable(http->getConn())
In-Reply-To: <CAN8nrKDEH9t=SkZ0DJGA44ieTb-Q9pYWRPyHbz1vQJ04BFFJ8g@mail.gmail.com>
References: <550BCC16.3040103@treenet.co.nz>
 <1426836862188.51009efe@Nodemailer>
 <550BCE55.7020105@treenet.co.nz>
 <A9C5FA23-DD01-4D1F-89FA-0D05970B8BCB@getbusi.com>
 <CAN8nrKDEH9t=SkZ0DJGA44ieTb-Q9pYWRPyHbz1vQJ04BFFJ8g@mail.gmail.com>
Message-ID: <CAN8nrKBfe-8M7KA+N9fzPfkeh2P4XT+6V96VA4Q-2igx8Szdmg@mail.gmail.com>

Okie dokie! boxes are crashing all over the place today so I finally have
some back traces without stuff optimised out.

Here are the details from two of these crashes which occurred on two
separate deployments?please let me know if they actually contain actionable
information now and I will upload them to the bug.

Thanks folks.


On 25 March 2015 at 09:28, Dan Charlesworth <dan at getbusi.com> wrote:

> Resending this after the last attempt went into the mail server black hole:
>
> Hey Amos
>
> I decided I?m not confident enough in 3.5.HEAD, after last time, to go
> back into production with it. Going to to do some more local testing first.
>
> That being said, I now have 3.4.12 in production with optimisations
> disabled and it seems to be doing fine performance and stability-wise. I
> only managed to capture one crash with optimisations disabled, so far, but
> it seemed to have some memory-related corruption, unfortunately.
>
> Updates to come over the next few days.
>
>
> On 23 March 2015 at 16:59, Dan Charlesworth <dan at getbusi.com> wrote:
>
>> Hey Amos
>>
>> I decided I?m not confident enough in 3.5.HEAD, after last time, to go
>> back into production with it. Going to to do some more local testing first.
>>
>> That being said, I now have 3.4.12 in production with optimisations
>> disabled and it seems to be doing fine performance and stability-wise. I
>> only managed to capture one crash with optimisations disabled, so far, but
>> it seemed to have some memory-related corruption, unfortunately.
>>
>> More to come tomorrow :-)
>>
>> > On 20 Mar 2015, at 6:37 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> >
>> > On 20/03/2015 8:34 p.m., Dan Charlesworth wrote:
>> >> Thanks Amos.
>> >>
>> >>
>> >> I'll put together a build with the upcoming snapshot on Monday, might
>> even try disabling optimization for it too.
>> >
>> > Please do. If you're only getting 40 RPS out of the proxy during the
>> > test its hard to see how not optimizing the code could be any worse, and
>> > it will help identifiying some traffic details.
>> >
>> > Amos
>> >
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150326/e23f020c/attachment.htm>
-------------- next part --------------
#0  0x0000003edf832625 in raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
#1  0x0000003edf833e05 in abort () at abort.c:92
#2  0x000000000062105f in xassert (msg=0x91a311 "connIsUsable(http->getConn())", file=0x9198b0 "client_side.cc", line=1515) at debug.cc:566
#3  0x00000000005db79f in clientSocketRecipient (node=0x50f0d008, http=0x51075ab8, rep=0x12f95940, receivedData=...) at client_side.cc:1515
#4  0x000000000061b574 in clientStreamCallback (thisObject=0x451353c8, http=0x51075ab8, rep=0x12f95940, replyBuffer=...) at clientStream.cc:186
#5  0x00000000006061eb in clientReplyContext::processReplyAccessResult (this=0x5118d098, accessAllowed=...) at client_side_reply.cc:2058
#6  0x00000000006057fd in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=0x5118d098) at client_side_reply.cc:1961
#7  0x00000000007cd727 in ACLChecklist::checkCallback (this=0x573cd338, answer=...) at Checklist.cc:161
#8  0x00000000007ccddb in ACLChecklist::completeNonBlocking (this=0x573cd338) at Checklist.cc:46
#9  0x00000000007cddd6 in ACLChecklist::resumeNonBlockingCheck (this=0x573cd338, state=0xc6ca20) at Checklist.cc:279
#10 0x000000000064cc01 in ExternalACLLookup::LookupDone (data=0x573cd338, result=...) at external_acl.cc:1623
#11 0x000000000064bd37 in externalAclHandleReply (data=0x573a3918, reply=...) at external_acl.cc:1427
#12 0x000000000067e01f in helperReturnBuffer (request_number=0, srv=0x17021e8, hlp=0x1701fe8, 
    msg=0x17023a0 "ERR log=%7B%22policy_group_id%22%3A%226%22%2C%22categories%22%3A%22%5B28%5D%22%2C%22user%22%3A%2215ifrain%22%2C%22set_id%22%3A%222%22%2C%22user_group%22%3A%22stu2015%22%7D", msg_end=0x170244b "") at helper.cc:858
#13 0x000000000067e9f3 in helperHandleRead (conn=..., 
    buf=0x17023a0 "ERR log=%7B%22policy_group_id%22%3A%226%22%2C%22categories%22%3A%22%5B28%5D%22%2C%22user%22%3A%2215ifrain%22%2C%22set_id%22%3A%222%22%2C%22user_group%22%3A%22stu2015%22%7D", len=172, flag=COMM_OK, xerrno=0, data=0x17021e8) at helper.cc:951
#14 0x00000000007e6f2a in CommIoCbPtrFun::dial (this=0x4d838d80) at CommCalls.cc:188
#15 0x00000000005fb498 in CommCbFunPtrCallT<CommIoCbPtrFun>::fire (this=0x4d838d50) at CommCalls.h:376
#16 0x00000000007d2b40 in AsyncCall::make (this=0x4d838d50) at AsyncCall.cc:32
#17 0x00000000007d61ff in AsyncCallQueue::fireNext (this=0x15e4c60) at AsyncCallQueue.cc:52
#18 0x00000000007d5f5f in AsyncCallQueue::fire (this=0x15e4c60) at AsyncCallQueue.cc:38
#19 0x0000000000644bef in EventLoop::dispatchCalls (this=0x7fffad41f1c0) at EventLoop.cc:158
#20 0x0000000000644a7f in EventLoop::runOnce (this=0x7fffad41f1c0) at EventLoop.cc:135
#21 0x00000000006448b8 in EventLoop::run (this=0x7fffad41f1c0) at EventLoop.cc:99
#22 0x00000000006cddbe in SquidMain (argc=3, argv=0x7fffad41f3f8) at main.cc:1528
#23 0x00000000006cd32d in SquidMainSafe (argc=3, argv=0x7fffad41f3f8) at main.cc:1260
#24 0x00000000006cd308 in main (argc=3, argv=0x7fffad41f3f8) at main.cc:1252
-------------- next part --------------
#0  0x0000003edf832625 in raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
        resultvar = 0
        pid = <value optimized out>
        selftid = 22272
#1  0x0000003edf833e05 in abort () at abort.c:92
        save_stage = 2
        act = {__sigaction_handler = {sa_handler = 0x7fffad41f3f0, sa_sigaction = 0x7fffad41f3f0}, sa_mask = {__val = {0, 0, 270038181553, 70, 
              6467189871727941185, 270037707608, 139919232107720, 270158213848, 4294967295, 8310675446604656483, 5, 12929552, 0, 0, 0, 0}}, 
          sa_flags = -549396320, sa_restorer = 0x5}
        sigs = {__val = {32, 0 <repeats 15 times>}}
#2  0x000000000062105f in xassert (msg=0x91a311 "connIsUsable(http->getConn())", file=0x9198b0 "client_side.cc", line=1515) at debug.cc:566
        __FUNCTION__ = "xassert"
#3  0x00000000005db79f in clientSocketRecipient (node=0x50f0d008, http=0x51075ab8, rep=0x12f95940, receivedData=...) at client_side.cc:1515
        context = {p_ = 0x5118bfe8}
        mustSendLastChunk = false
#4  0x000000000061b574 in clientStreamCallback (thisObject=0x451353c8, http=0x51075ab8, rep=0x12f95940, replyBuffer=...) at clientStream.cc:186
        next = 0x50f0d008
        __FUNCTION__ = "clientStreamCallback"
#5  0x00000000006061eb in clientReplyContext::processReplyAccessResult (this=0x5118d098, accessAllowed=...) at client_side_reply.cc:2058
        __FUNCTION__ = "processReplyAccessResult"
        localTempBuffer = {flags = {error = 0}, length = 0, offset = 0, data = 0x5118c297 ""}
        buf = 0x5118c008 "HTTP/1.1 302 Found\r\nLocation: http://www.google.com.au/ads/user-lists/1062572271/?label=GTyFCMn1rwQQ76HW-gM&fmt=3&num=1&cv=7&frm=0&url=http%3A//www.kia.com.au/build-and-price/select-colour%3Fsgo%3DD4A"...
        body_buf = 0x5118c297 ""
        body_size = 0
#6  0x00000000006057fd in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=0x5118d098) at client_side_reply.cc:1961
        me = 0x5118d098
#7  0x00000000007cd727 in ACLChecklist::checkCallback (this=0x573cd338, answer=...) at Checklist.cc:161
        callback_ = 0x6057d2 <clientReplyContext::ProcessReplyAccessResult(allow_t, void*)>
        cbdata_ = 0x5118d098
        __FUNCTION__ = "checkCallback"
#8  0x00000000007ccddb in ACLChecklist::completeNonBlocking (this=0x573cd338) at Checklist.cc:46
No locals.
#9  0x00000000007cddd6 in ACLChecklist::resumeNonBlockingCheck (this=0x573cd338, state=0xc6ca20) at Checklist.cc:279
No locals.
#10 0x000000000064cc01 in ExternalACLLookup::LookupDone (data=0x573cd338, result=...) at external_acl.cc:1623
        checklist = 0x573cd338
#11 0x000000000064bd37 in externalAclHandleReply (data=0x573a3918, reply=...) at external_acl.cc:1427
        cbdata = 0x573cd338
        state = 0x573a3918
        __FUNCTION__ = "externalAclHandleReply"
        next = 0x17023a0
        entryData = {result = {code = ACCESS_DENIED, kind = 0}, notes = {<Lock> = {_vptr.Lock = 0xc39980, count_ = 0}, _vptr.NotePairs = 0xc39958, entries = {
              capacity = 16, count = 1, items = 0x5797d4b0}}, user = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, password = {
            static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, message = {static npos = 18446744073709551615, size_ = 0, len_ = 0, 
            buf_ = 0x0}, tag = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, log = {static npos = 18446744073709551615, size_ = 128, 
            len_ = 97, 
---Type <return> to continue, or q <return> to quit---
            buf_ = 0x3aee59c0 "{\"policy_group_id\":\"6\",\"categories\":\"[28]\",\"user\":\"15ifrain\",\"set_id\":\"2\",\"user_group\":\"stu2015\"}"}}
        label = 0x0
        entry = {p_ = 0x5080e440}
#12 0x000000000067e01f in helperReturnBuffer (request_number=0, srv=0x17021e8, hlp=0x1701fe8, 
    msg=0x17023a0 "ERR log=%7B%22policy_group_id%22%3A%226%22%2C%22categories%22%3A%22%5B28%5D%22%2C%22user%22%3A%2215ifrain%22%2C%22set_id%22%3A%222%22%2C%22user_group%22%3A%22stu2015%22%7D", msg_end=0x170244b "") at helper.cc:858
        response = {result = HelperReply::Error, notes = {<Lock> = {_vptr.Lock = 0xc39980, count_ = 0}, _vptr.NotePairs = 0xc39958, entries = {capacity = 16, 
              count = 1, items = 0x129dfc30}}, whichServer = {cbc = 0x0, lock = 0x0}, other_ = {buf = 0x4305caf0 "", size = 0, max_capacity = 168, 
            capacity = 2048, stolen = 0, static CBDATA_MemBuf = 15}}
        callback = 0x64b80c <externalAclHandleReply(void*, HelperReply const&)>
        cbdata = 0x573a3918
        r = 0x54d86410
        __FUNCTION__ = "helperReturnBuffer"
#13 0x000000000067e9f3 in helperHandleRead (conn=..., 
    buf=0x17023a0 "ERR log=%7B%22policy_group_id%22%3A%226%22%2C%22categories%22%3A%22%5B28%5D%22%2C%22user%22%3A%2215ifrain%22%2C%22set_id%22%3A%222%22%2C%22user_group%22%3A%22stu2015%22%7D", len=172, flag=COMM_OK, xerrno=0, data=0x17021e8) at helper.cc:951
        msg = 0x17023a0 "ERR log=%7B%22policy_group_id%22%3A%226%22%2C%22categories%22%3A%22%5B28%5D%22%2C%22user%22%3A%2215ifrain%22%2C%22set_id%22%3A%222%22%2C%22user_group%22%3A%22stu2015%22%7D"
        i = 0
        skip = 1
        t = 0x170244b ""
        srv = 0x17021e8
        hlp = 0x1701fe8
        __FUNCTION__ = "helperHandleRead"
#14 0x00000000007e6f2a in CommIoCbPtrFun::dial (this=0x4d838d80) at CommCalls.cc:188
No locals.
#15 0x00000000005fb498 in CommCbFunPtrCallT<CommIoCbPtrFun>::fire (this=0x4d838d50) at CommCalls.h:376
No locals.
#16 0x00000000007d2b40 in AsyncCall::make (this=0x4d838d50) at AsyncCall.cc:32
        __FUNCTION__ = "make"
#17 0x00000000007d61ff in AsyncCallQueue::fireNext (this=0x15e4c60) at AsyncCallQueue.cc:52
        call = {p_ = 0x4d838d50}
        __FUNCTION__ = "fireNext"
#18 0x00000000007d5f5f in AsyncCallQueue::fire (this=0x15e4c60) at AsyncCallQueue.cc:38
        made = true
#19 0x0000000000644bef in EventLoop::dispatchCalls (this=0x7fffad41f1c0) at EventLoop.cc:158
        dispatchedSome = false
#20 0x0000000000644a7f in EventLoop::runOnce (this=0x7fffad41f1c0) at EventLoop.cc:135
        sawActivity = false
        waitingEngine = 0x7fffad41f240
        __FUNCTION__ = "runOnce"
#21 0x00000000006448b8 in EventLoop::run (this=0x7fffad41f1c0) at EventLoop.cc:99
No locals.
#22 0x00000000006cddbe in SquidMain (argc=3, argv=0x7fffad41f3f8) at main.cc:1528
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        signalEngine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc39690}, loop = @0x7fffad41f1c0}
---Type <return> to continue, or q <return> to quit---
        store_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc396d0}, <No data fields>}
        comm_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc412b0}, <No data fields>}
        mainLoop = {errcount = 0, last_loop = false, engines = {capacity = 16, count = 4, items = 0x1b61090}, timeService = 0x7fffad41f230, 
          primaryEngine = 0x7fffad41f240, loop_delay = 0, error = false, runOnceResult = false}
        time_engine = {_vptr.TimeEngine = 0xc3ba90}
#23 0x00000000006cd32d in SquidMainSafe (argc=3, argv=0x7fffad41f3f8) at main.cc:1260
        __FUNCTION__ = "SquidMainSafe"
#24 0x00000000006cd308 in main (argc=3, argv=0x7fffad41f3f8) at main.cc:1252
No locals.
-------------- next part --------------
#3  0x00000000005db79f in clientSocketRecipient (node=0x50f0d008, http=0x51075ab8, rep=0x12f95940, receivedData=...) at client_side.cc:1515
1515	    assert(connIsUsable(http->getConn()));
-------------- next part --------------
#3  0x00000000005db79f in clientSocketRecipient (node=0x2c820638, http=0x2c821fa8, rep=0x2c992f50, receivedData=...) at client_side.cc:1515
1515	    assert(connIsUsable(http->getConn()));
(gdb) bt full
#0  0x000000397e232625 in raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
        resultvar = 0
        pid = <value optimized out>
        selftid = 31091
#1  0x000000397e233e05 in abort () at abort.c:92
        save_stage = 2
        act = {__sigaction_handler = {sa_handler = 0x7fff1b66ad50, sa_sigaction = 0x7fff1b66ad50}, sa_mask = {__val = {0, 0, 246929663665, 70, 
              5046418814977706629, 70, 469233704, 247066473176, 247066473152, 8310675446604656483, 8391446528407659105, 469233704, 247064022992, 9571908, 
              459710800, 247066473152}}, sa_flags = 733721288, sa_restorer = 0x2bbbb2c8}
        sigs = {__val = {32, 0 <repeats 15 times>}}
#2  0x000000000062105f in xassert (msg=0x91a311 "connIsUsable(http->getConn())", file=0x9198b0 "client_side.cc", line=1515) at debug.cc:566
        __FUNCTION__ = "xassert"
#3  0x00000000005db79f in clientSocketRecipient (node=0x2c820638, http=0x2c821fa8, rep=0x2c992f50, receivedData=...) at client_side.cc:1515
        context = {p_ = 0x2c5ceb08}
        mustSendLastChunk = false
#4  0x000000000061b574 in clientStreamCallback (thisObject=0x2c5cfbb8, http=0x2c821fa8, rep=0x2c992f50, replyBuffer=...) at clientStream.cc:186
        next = 0x2c820638
        __FUNCTION__ = "clientStreamCallback"
#5  0x00000000006061eb in clientReplyContext::processReplyAccessResult (this=0x2c1e6e08, accessAllowed=...) at client_side_reply.cc:2058
        __FUNCTION__ = "processReplyAccessResult"
        localTempBuffer = {flags = {error = 0}, length = 0, offset = 0, data = 0x2c5ceca2 ""}
        buf = 0x2c5ceb28 "HTTP/1.1 302 Found\r\nDate: Wed, 25 Mar 2015 21:12:43 GMT\r\nLocation: http://api.bizographics.com/v1/profile.json?api_key=6332f8b7316a4d1284e9c1217a367347&callback=Demdex.parseBizo&3pc=true\r\nServer: ngin"...
        body_buf = 0x2c5ceca2 ""
        body_size = 0
#6  0x00000000006057fd in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=0x2c1e6e08) at client_side_reply.cc:1961
        me = 0x2c1e6e08
#7  0x00000000007cd727 in ACLChecklist::checkCallback (this=0x1fd57528, answer=...) at Checklist.cc:161
        callback_ = 0x6057d2 <clientReplyContext::ProcessReplyAccessResult(allow_t, void*)>
        cbdata_ = 0x2c1e6e08
        __FUNCTION__ = "checkCallback"
#8  0x00000000007ccddb in ACLChecklist::completeNonBlocking (this=0x1fd57528) at Checklist.cc:46
No locals.
#9  0x00000000007cddd6 in ACLChecklist::resumeNonBlockingCheck (this=0x1fd57528, state=0xc6ca20) at Checklist.cc:279
No locals.
#10 0x000000000064cc01 in ExternalACLLookup::LookupDone (data=0x1fd57528, result=...) at external_acl.cc:1623
        checklist = 0x1fd57528
#11 0x000000000064bd37 in externalAclHandleReply (data=0x10be5948, reply=...) at external_acl.cc:1427
        cbdata = 0x1fd57528
        state = 0x10be5948
        __FUNCTION__ = "externalAclHandleReply"
        next = 0x22e9ab0
        entryData = {result = {code = ACCESS_DENIED, kind = 0}, notes = {<Lock> = {_vptr.Lock = 0xc39980, count_ = 0}, _vptr.NotePairs = 0xc39958, entries = {
              capacity = 16, count = 1, items = 0x2c1f4800}}, user = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, password = {
            static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, message = {static npos = 18446744073709551615, size_ = 0, len_ = 0, 
            buf_ = 0x0}, tag = {static npos = 18446744073709551615, size_ = 0, len_ = 0, buf_ = 0x0}, log = {static npos = 18446744073709551615, size_ = 512, 
            len_ = 147, 
---Type <return> to continue, or q <return> to quit---=
            buf_ = 0x2cb18740 "{\"deny_list\":0,\"deny_set\":3,\"categories\":\"[45]\",\"policy_group_id\":\"2\",\"user\":\"ehodges22\",\"user_group\":\"stage3\",\"deny_type\":\"filesize\",\"set_id\":\"3\"}"}}
        label = 0x0
        entry = {p_ = 0x2cb1c110}
#12 0x000000000067e01f in helperReturnBuffer (request_number=0, srv=0x22e98f8, hlp=0x22e96f8, 
    msg=0x22e9ab0 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A3%2C%22categories%22%3A%22%5B45%5D%22%2C%22policy_group_id%22%3A%222%22%2C%22user%22%3A%22ehodges22%22%2C%22user_group%22%3A%22stage3%22%2C%22deny_typ"..., msg_end=0x22e9ba9 "") at helper.cc:858
        response = {result = HelperReply::Error, notes = {<Lock> = {_vptr.Lock = 0xc39980, count_ = 0}, _vptr.NotePairs = 0xc39958, entries = {capacity = 16, 
              count = 1, items = 0x2cb8e8b0}}, whichServer = {cbc = 0x0, lock = 0x0}, other_ = {buf = 0x24a9a650 "", size = 0, max_capacity = 246, 
            capacity = 2048, stolen = 0, static CBDATA_MemBuf = 15}}
        callback = 0x64b80c <externalAclHandleReply(void*, HelperReply const&)>
        cbdata = 0x10be5948
        r = 0x1d2d7f40
        __FUNCTION__ = "helperReturnBuffer"
#13 0x000000000067e9f3 in helperHandleRead (conn=..., 
    buf=0x22e9ab0 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A3%2C%22categories%22%3A%22%5B45%5D%22%2C%22policy_group_id%22%3A%222%22%2C%22user%22%3A%22ehodges22%22%2C%22user_group%22%3A%22stage3%22%2C%22deny_typ"..., len=250, flag=COMM_OK, xerrno=0, data=0x22e98f8) at helper.cc:951
        msg = 0x22e9ab0 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A3%2C%22categories%22%3A%22%5B45%5D%22%2C%22policy_group_id%22%3A%222%22%2C%22user%22%3A%22ehodges22%22%2C%22user_group%22%3A%22stage3%22%2C%22deny_typ"...
        i = 0
        skip = 1
        t = 0x22e9ba9 ""
        srv = 0x22e98f8
        hlp = 0x22e96f8
        __FUNCTION__ = "helperHandleRead"
#14 0x00000000007e6f2a in CommIoCbPtrFun::dial (this=0x2cb910b0) at CommCalls.cc:188
No locals.
#15 0x00000000005fb498 in CommCbFunPtrCallT<CommIoCbPtrFun>::fire (this=0x2cb91080) at CommCalls.h:376
No locals.
#16 0x00000000007d2b40 in AsyncCall::make (this=0x2cb91080) at AsyncCall.cc:32
        __FUNCTION__ = "make"
#17 0x00000000007d61ff in AsyncCallQueue::fireNext (this=0x21f3040) at AsyncCallQueue.cc:52
        call = {p_ = 0x2cb91080}
        __FUNCTION__ = "fireNext"
#18 0x00000000007d5f5f in AsyncCallQueue::fire (this=0x21f3040) at AsyncCallQueue.cc:38
        made = true
#19 0x0000000000644bef in EventLoop::dispatchCalls (this=0x7fff1b66ab20) at EventLoop.cc:158
        dispatchedSome = false
#20 0x0000000000644a7f in EventLoop::runOnce (this=0x7fff1b66ab20) at EventLoop.cc:135
        sawActivity = false
        waitingEngine = 0x7fff1b66aba0
        __FUNCTION__ = "runOnce"
#21 0x00000000006448b8 in EventLoop::run (this=0x7fff1b66ab20) at EventLoop.cc:99
No locals.
#22 0x00000000006cddbe in SquidMain (argc=3, argv=0x7fff1b66ad58) at main.cc:1528
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
---Type <return> to continue, or q <return> to quit---
        signalEngine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc39690}, loop = @0x7fff1b66ab20}
        store_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc396d0}, <No data fields>}
        comm_engine = {<AsyncEngine> = {_vptr.AsyncEngine = 0xc412b0}, <No data fields>}
        mainLoop = {errcount = 0, last_loop = false, engines = {capacity = 16, count = 4, items = 0x2746bb0}, timeService = 0x7fff1b66ab90, 
          primaryEngine = 0x7fff1b66aba0, loop_delay = 0, error = false, runOnceResult = false}
        time_engine = {_vptr.TimeEngine = 0xc3ba90}
#23 0x00000000006cd32d in SquidMainSafe (argc=3, argv=0x7fff1b66ad58) at main.cc:1260
        __FUNCTION__ = "SquidMainSafe"
#24 0x00000000006cd308 in main (argc=3, argv=0x7fff1b66ad58) at main.cc:1252
No locals.
-------------- next part --------------
#0  0x000000397e232625 in raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
#1  0x000000397e233e05 in abort () at abort.c:92
#2  0x000000000062105f in xassert (msg=0x91a311 "connIsUsable(http->getConn())", file=0x9198b0 "client_side.cc", line=1515) at debug.cc:566
#3  0x00000000005db79f in clientSocketRecipient (node=0x2c820638, http=0x2c821fa8, rep=0x2c992f50, receivedData=...) at client_side.cc:1515
#4  0x000000000061b574 in clientStreamCallback (thisObject=0x2c5cfbb8, http=0x2c821fa8, rep=0x2c992f50, replyBuffer=...) at clientStream.cc:186
#5  0x00000000006061eb in clientReplyContext::processReplyAccessResult (this=0x2c1e6e08, accessAllowed=...) at client_side_reply.cc:2058
#6  0x00000000006057fd in clientReplyContext::ProcessReplyAccessResult (rv=..., voidMe=0x2c1e6e08) at client_side_reply.cc:1961
#7  0x00000000007cd727 in ACLChecklist::checkCallback (this=0x1fd57528, answer=...) at Checklist.cc:161
#8  0x00000000007ccddb in ACLChecklist::completeNonBlocking (this=0x1fd57528) at Checklist.cc:46
#9  0x00000000007cddd6 in ACLChecklist::resumeNonBlockingCheck (this=0x1fd57528, state=0xc6ca20) at Checklist.cc:279
#10 0x000000000064cc01 in ExternalACLLookup::LookupDone (data=0x1fd57528, result=...) at external_acl.cc:1623
#11 0x000000000064bd37 in externalAclHandleReply (data=0x10be5948, reply=...) at external_acl.cc:1427
#12 0x000000000067e01f in helperReturnBuffer (request_number=0, srv=0x22e98f8, hlp=0x22e96f8, 
    msg=0x22e9ab0 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A3%2C%22categories%22%3A%22%5B45%5D%22%2C%22policy_group_id%22%3A%222%22%2C%22user%22%3A%22ehodges22%22%2C%22user_group%22%3A%22stage3%22%2C%22deny_typ"..., msg_end=0x22e9ba9 "") at helper.cc:858
#13 0x000000000067e9f3 in helperHandleRead (conn=..., 
    buf=0x22e9ab0 "ERR log=%7B%22deny_list%22%3A0%2C%22deny_set%22%3A3%2C%22categories%22%3A%22%5B45%5D%22%2C%22policy_group_id%22%3A%222%22%2C%22user%22%3A%22ehodges22%22%2C%22user_group%22%3A%22stage3%22%2C%22deny_typ"..., len=250, flag=COMM_OK, xerrno=0, data=0x22e98f8) at helper.cc:951
#14 0x00000000007e6f2a in CommIoCbPtrFun::dial (this=0x2cb910b0) at CommCalls.cc:188
#15 0x00000000005fb498 in CommCbFunPtrCallT<CommIoCbPtrFun>::fire (this=0x2cb91080) at CommCalls.h:376
#16 0x00000000007d2b40 in AsyncCall::make (this=0x2cb91080) at AsyncCall.cc:32
#17 0x00000000007d61ff in AsyncCallQueue::fireNext (this=0x21f3040) at AsyncCallQueue.cc:52
#18 0x00000000007d5f5f in AsyncCallQueue::fire (this=0x21f3040) at AsyncCallQueue.cc:38
#19 0x0000000000644bef in EventLoop::dispatchCalls (this=0x7fff1b66ab20) at EventLoop.cc:158
#20 0x0000000000644a7f in EventLoop::runOnce (this=0x7fff1b66ab20) at EventLoop.cc:135
#21 0x00000000006448b8 in EventLoop::run (this=0x7fff1b66ab20) at EventLoop.cc:99
#22 0x00000000006cddbe in SquidMain (argc=3, argv=0x7fff1b66ad58) at main.cc:1528
#23 0x00000000006cd32d in SquidMainSafe (argc=3, argv=0x7fff1b66ad58) at main.cc:1260
#24 0x00000000006cd308 in main (argc=3, argv=0x7fff1b66ad58) at main.cc:1252
-------------- next part --------------
#3  0x00000000005db79f in clientSocketRecipient (node=0x2c820638, http=0x2c821fa8, rep=0x2c992f50, receivedData=...) at client_side.cc:1515
1515	    assert(connIsUsable(http->getConn()));

From stan.prescott at gmail.com  Wed Mar 25 23:41:21 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 25 Mar 2015 19:41:21 -0400
Subject: [squid-users] Squid 3.5.2 will not start correctly "FATAL:
 Ipc::Mem::Segment::open failed to shm_open(/squid-ssl_session_cache.shm):
 (2) No such file or directory"
Message-ID: <CANLNtGRN=VeC7M+V_LdNyJPqDP_cj+v5+5pqJ5gWQ9XTFL=KNA@mail.gmail.com>

I have installed Squid 3.5.2 on the Smoothwall Express 3.1 firewall
distribution and it will not start correctly. I get this error

*2015/03/25 19:28:30.623 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:30.623 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33 kid1| Current Directory is /*
*2015/03/25 19:28:33 kid1| Starting Squid Cache version 3.5.2 for
i586-pc-linux-gnu...*
*2015/03/25 19:28:33 kid1| Service Name: squid*
*2015/03/25 19:28:33 kid1| Process ID 3638*
*2015/03/25 19:28:33 kid1| Process Roles: worker*
*2015/03/25 19:28:33 kid1| With 1024 file descriptors available*
*2015/03/25 19:28:33 kid1| Initializing IP Cache...*
*2015/03/25 19:28:33 kid1| DNS Socket created at 0.0.0.0, FD 8*
*2015/03/25 19:28:33 kid1| Adding nameserver 127.0.0.1 from
/etc/resolv.conf*
*2015/03/25 19:28:33 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
processes*
*FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*

*Squid Cache (Version 3.5.2): Terminated abnormally.*
*CPU Usage: 0.023 seconds = 0.017 user + 0.007 sys*
*Maximum Resident Size: 26816 KB*
*Page faults with physical i/o: 0*
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/25 19:28:33.672 kid1| Acl.cc(380) ~ACL: freeing ACL *


I have patched main.cc with the enter_suid(); statements described in the
fix for open_shm errors. But I keep getting the error of
"squid-ssl_session_cache.shm" no such file or directory.

Where is the ssl_session_cache file or directory supposed to be? Why
doesn't one exist? What might cause this file or directory to not exist?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150325/a84835bd/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 25 23:42:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Mar 2015 12:42:34 +1300
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <CALTPfpE8J-FFT7doe8t-L2cfiQTk-ZMh0cvSKCk_hm1CRbZVog@mail.gmail.com>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>	<5502B3EE.9040908@treenet.co.nz>	<CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>	<5502D129.7030809@treenet.co.nz>	<CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>	<55067332.1020500@treenet.co.nz>	<CALTPfpEs5iz9PDVyP0ta-qyA=RBdjUL4NcUFb8a6DyT4xmgq3A@mail.gmail.com>	<551295C9.8000004@treenet.co.nz>
 <CALTPfpE8J-FFT7doe8t-L2cfiQTk-ZMh0cvSKCk_hm1CRbZVog@mail.gmail.com>
Message-ID: <551347EA.5060508@treenet.co.nz>

On 26/03/2015 4:47 a.m., Priya Agarwal wrote:
> I can't change the header files of library. Therefore when I included it in
> my code I did
> extern "C" {
> #include<c header file>
> ...
> }
> 
> instead of just #include and it worked.

Yes, that is equivalent to the correct header contents.

> 
> But the compiler is showing some errors in the header files I included, but
> of course there isn't anything wrong in them. I could include it this way
> as well right?

Well, if the compiler is complaining by definition there *is* something
wrong with them.

The lack of that extern construct in the headers shows the author has
paid no attention to C++ compatibility, and very little (if any)
attention to making the library portable.

You can build Squid with --disable-strict-error-checking to hide issues
which are non-critical. But be aware that the warnings may become
critical errors at any time with other compilers, and some may result in
undesirable run-time behaviour or bugs with the current one.

> 
> Also I wrote a c file and a corresponding header file in /src folder and
> appended the 'squid_sources' as this:
> 
> squid_sources = .
>                          ...
>                          ....
>                         usdpaa_if.h \
>                         usdpaa_if.c
> 
> Is this okay? As again it is showing errors in the usdpaa_if.c file and
> there are no syntax errors like the compiler says.
> I saw other folder in /src like compat had also included c files like this
> so I thought this would be right.

Yes, provided you also placed a '\' character on the end of the line
above the new ones (making them part of the wrapped list). AND that the
.c code in that new file will compile with your C compiler.

Amos


From squid3 at treenet.co.nz  Thu Mar 26 00:53:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Mar 2015 13:53:01 +1300
Subject: [squid-users] load balancing and site failover
In-Reply-To: <1427318767.24562.31.camel@desktop.bpk2.com>
References: <1427206737.17590.33.camel@desktop.bpk2.com>	
 <1427221530.17590.37.camel@desktop.bpk2.com>	
 <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>	
 <55121755.8010905@treenet.co.nz> <1427318767.24562.31.camel@desktop.bpk2.com>
Message-ID: <5513586D.5020407@treenet.co.nz>

On 26/03/2015 10:26 a.m., Brendan Kearney wrote:
> On Wed, 2015-03-25 at 15:03 +1300, Amos Jeffries wrote:
>> On 25/03/2015 9:55 a.m., brendan kearney wrote:
>>> Was not sure if bugzilla was used for mailing list issues.  If you would
>>> like me to open one, I will but it looks like the list is working again.
>>
>> Bugzilla is used, list bugs under the "project services" product.
>>
>>
>> As for your query...
>>
>>> On Mar 24, 2015 2:25 PM, "Brendan Kearney" wrote:
>>>
>>>> On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
>>>>> while load balancing is not a requirement in a proxy environment, it
>>>>> does afford a great deal of functionality, scaling and fault tolerance
>>>>> in one.  several if not many on this list probably employ them for their
>>>>> proxies and likely other technologies, but they are not all created
>>>>> equal.
>>>>>
>>>>> i recently looked to see if a specific feature was in HAProxy.  i was
>>>>> looking to see if HAProxy could reply to a new connection with a RST
>>>>> packet if no pool member was available.
>>>>>
>>>>> the idea behind this is, if all of the proxies are not passing the
>>>>> service check and are marked down by the load balancer, the reply of a
>>>>> RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
>>>>> tells the browser to failover to the next proxy assigned by the PAC
>>>>> file.
>>>>>
>>>>> where i work, we have this configuration working.  the load balancers
>>>>> are configured with the option to send a reset when no proxy is
>>>>> available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
>>>>> a specific order based on which proxy VIP is assigned as the primary.
>>>>> In every case, if the primary VIP does not have an available pool
>>>>> member, the browser fails over to the next in the list.  failover would
>>>>> happen again, if the secondary VIP replies with a RST during the
>>>>> connection establishing.  the process repeats until a TCP connection
>>>>> establishes or all proxies assigned have been exhausted.  the browser
>>>>> will use the proxy VIP that it successfully connects to, for the
>>>>> duration of the session.  once the browser is closed and reopened, the
>>>>> evaluation of the PAC file occurs again, and the process starts anew.
>>>>> plug-ins such as Proxy Selector are the exception to this, and can be
>>>>> used to reevaluate a PAC file by selecting it for use.
>>>>>
>>>>> we have used this configuration several times, when we found an ISP link
>>>>> was flapping or some other issue more global in nature than just the
>>>>> proxies was affecting our egress and internet access.  i can attest to
>>>>> the solution as working and elegantly handling site wide failures.
>>>>>
>>>>> being that the solutions where i work are proprietary commercial
>>>>> products, i wanted to find an open source product that does this.  i
>>>>> have been a long time user of HAProxy, and have recommended it for
>>>>> others here, but sadly they cannot perform this function.  per their
>>>>> mailing list, they use the network stack of the OS for connection
>>>>> establishment and cannot cause a RST to be sent to the client during a
>>>>> TCP handshake if no pool member is available.
>>>>>
>>>>> they suggested an external helper that manipulates IPTables rules based
>>>>> on a pool member being available.  they do not feel that a feature like
>>>>> this belongs in a layer 4/7 reverse proxy application.
>>
>> They are right. HTTP != TCP.
> i didnt confuse that detail.  it was unknown to me that HAProxy could
> not tie layer 7 status to layer 3/4 actions.  the decisions they made
> and how they architected the app is why they cannot do this, not that it
> is technically impossible to do it.  i may be spoiled because i work
> with equipment that can do this for me.
>>
>> In particular TCP depends on routers having a full routing map of the
>> entire Internet (provided by BGP) and deciding the best upstream hop
>> based on that global info. Clients have one (and only one) upstream
>> router for each server they want to connect to.
> i will contest this.  my router does not need a full BGP map to route
> traffic locally on my LAN or remotely out its WAN interface.  hell, it
> does not even run BGP, and i can still get to the intarwebs, no problem.
> it too, only has one upstream router / default route.

Then your router has more in common with proxy than usual. Its operating
with a next-hop packet relay model (OSPF? MPLS?) rather than an
end-to-end model (BGP with RIB/FIB).

>>
>> In HTTP each proxy (aka router) performs independent upstream connection
>> attempts, failover, and verifies it worked before responding to the
>> client with a final response. Each proxy only has enough detail to check
>> its upstream(s). Each proxy can connect to any server (subject to ACLs).
> how are you comparing a HTTP proxy (a layer 7 application) to a router
> (a layer 3 device)?  routers route traffic and proxies proxy traffic.

while, routers proxy TCP packets and proxies route HTTP messages.

Its the behaviour abstraction I'm talking about here.
[if you dont want to dive into theory skip to the end of this mail]

The algorithms each are capable of are the same despite differences in
details of layer and what designed mechanisms are optimal for their
protocol.
DNS recursive resolvers are also a type of caching proxy, and SMTP email
servers too. Of the surviving "old" protocols only FTP seems not to have
proxies as integral part of the design.


> very different functions.  routers dont look past a certain point in the
> headers in order to make decisions on where to send the traffic.

Same for proxies. I'm sure you're aware that Squid dont look into HTTP
payloads same as routers dont look into TCP payloads.

> proxies look all the way to the end of the headers and sometimes into
> the payload, too.

No proxies dont look into payloads. Content filters do that bit instead
of the proxy.
Just like routers can be enhanced by DPI systems doing payload
inspection at their level.

In both systems the DPI / content filter is where they cross over into
firewall activity.

>  proxies are more akin to a protocol specific
> firewall.

You have no firewall ability built into your router?

>  proxies also dont send the incoming traffic out an interface.

layer-3/4 interface is equivalent to layer-5/7 socket in abstract.

> they terminate the client session, and initiate a new session on behalf
> of the client.

not necessarily, that is the architectural difference between HTTP and
TCP. SOCKS proxies and TCP have much more in common in architectural design.

What the proxies do is terminate the *TCP* session. Obviously all
non-TCP protocols will do that.
The HTTP layer is more inline with UDP here than TCP. Although at the
layer just above HTTP there is the "browsing session" concept
implemented with cookies/auth etc which is retained across both TCP
layer connections. For cases where TCP-level sessions need to be
emulated there is connection pinning in the proxy.


>  simply because the proxy can elect how to send a request
> it is making on behalf of a client, does not make a proxy a router.

The names are terminology for the activity performed. TCP routers
normally do not re-selecting the destination endpoint, just the path.
The proxy selects the actual endpoint destination.

What you can configure each to do crosses over and you can specifically
configure a proxy to become a router, and vice versa. Doing it though
screws with their normal and optimal behavour. Router becomes like a
proxy by turning on NAT - and we probably both know the hell that
causes. Making a proxy do strictly router behaviour has a similar range
of nasty side effects.

Yes each one (proxy vs router) is designed to work at different levels,
in different ways. But how you use them determines what they do.

In particular this is the difference which you request is asking us to
provide a way to disable. Making the proxy return TCP RST if a
particular endpoint is not available - despite other endpoints
potentially being usable.
 Making the proxy behave like a router.


>  the
> fact that one connection is terminated and a new one is initiated rules
> out a proxy from being any kind of router, in my opinion.

That rules out all devices containing NAT functionality or LB software
from being routers.

>  even with SSL
> or the CONNECT Method, the connection is still made by the proxy to the
> remote server.  the client never makes a connection to the remote
> server, therefore the traffic was not routed.  it was proxied.

By definition CONNECT method is not a proxy. In particular it is the
request that the proxy stop being a HTTP proxy and become a tunnel / TCP
relay.
It is roughly equivalent to a router sending packets down a tunnel / VPN
or SOCKS interface (the proxy is the outgoing end of the tunnel/VPN).



>>
>>>>>
>>>>> my search for a load balancer solution went through ipvsadm, balance and
>>>>> haproxy before i selected haproxy.  haproxy was more feature rich than
>>>>> balance, and easier to implement than ipvsadm.  do any other list
>>>>> members have a need for such a feature from their load balancers?  do
>>>>> any other list members have site failover solutions that have been
>>>>> tested or used and would consider sharing their design and/or pain
>>>>> points?  i am not looking for secret sauce or confidential info, but
>>>>> more high level architecture decisions and such.
>>
>>
>> I havent tested it but this should do what you are asking:
>>
>>  acl err http_status 500-505 408
>>  deny_info TCP_RESET err
>>  http_reply_access deny err
>>
>> It replaces the response from Squid with a TCP RST packet.
> this is useful in the case that the proxy is alive and well, but cannot
> get to the internet.  in my example, the ISP issue would seem to be
> covered, though i am not sure how the actual implementation would go.
> the client has a TCP session established with the load balancer, which
> gets the full SYN -> SYN/ACK -> ACK treatement.  the load balancer would
> get the SYN -> RST from the proxy, and presumably sends the RST back to
> the client.  While this does seem to hold up logically, the
> implementation may have nuances that have to be dealt with.  Does the
> RST in the middle of an established TCP session cause the browser to
> failover to the next proxy assigned?  i would have to test that out.

Nod.

> 
> now, what about the case where the proxies are not alive and well behind
> the load balancer, and they are not able to reply with a RST?  This is
> the scenario that i would want the load balancer to be able to manage.
> this is where tying a layer 7 status to a layer 3/4 action on the load
> balancer becomes relevant.  then, the ability for the load balancer to
> do this negates the need to manage this in the proxy layer, and removes
> any nuances that may be encountered with the implementation.


That is a normal TCP error case. Same things happen if there is any
network level outage, or packets destined to a non-existent IP range. It
depends on the LB software what will be done about it.

Amos



From eliezer at ngtech.co.il  Thu Mar 26 15:38:36 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 26 Mar 2015 17:38:36 +0200
Subject: [squid-users] Squid 3.5.2 RPMs release for CentOS 32 and 64 bit.
In-Reply-To: <54E532BA.1010702@treenet.co.nz>
References: <54E532BA.1010702@treenet.co.nz>
Message-ID: <551427FC.5000403@ngtech.co.il>

Published at: http://www1.ngtech.co.il/wpe/?p=25

I am happy to release the new RPMs of squid 3.5.2 for Centos 6.6 64bit, 
32bit and CentOS 7 64bit.

The new release includes couple bug fixes and improvements.
The links to the RPMs are at the bottom of the article.

"What if computers would start to think??

I have been asked couple times in the past about the ?livelihood? of the PC.
Sometimes there is the possibility that the computer has some 
intelligence and life in it due to it's nature to do things for humans. 
Many computers users claims that there is some ?geany? inside this 
machine which sits in some room in the house. Sometimes it is even makes 
?sense? to the naked eye and mind.
If for example a human will try to understand what is causing some 
issues with the text editor or with the cable, he will eventually find 
out that there are things which he might not be able to understand with 
the naked eye and in most of these cases the help from others is 
required and recommended.
One case I do remember was that somebody asked me about the cause of 
some issue in run-time.
While I wanted to answer him I told him that I have couple ideas about 
what causing these issues which he is having, but there is no way in the 
world for me to know what is the real cause due to that fact that I have 
missing parts in the picture. And also if I had the whole ?picture? 
there is a real possibility which I would not be able to analyze all the 
details which are needed to conclude what is the cause for the annoying 
issues.
How is it related to a Thinking Machine??
Some people out there in the world may think that the computer is a 
marvels machine and do think that if it will have the function to 
?think? by itself and do things for humans it will bring good things.
The reality proves that the computer is a creation of a human and if the 
humans are trying to do good this machine will be a tool for good while 
if the human will try to do bad this machine will do bad.
It's the same about Artificial Intelligence which might be implemented 
in computers.
If indeed the programmer or operator do things for bad, this might be 
the result.
I do not want to speculate how the world would look like in the age of 
AI but there are some AI levels in the world, even if they are so 
limited that these AI's can crash in run-time very easily.
In some relation to the last RPM release which I was writing about a 
?Match Maker?, AI's are kind of a hot topic in this world. Many in this 
world try to help others to find their ?Missing Rib? or ?The other part 
of their "soul mate?.
While computers can help to find in a way their missing piece they still 
lack one thing: soul.
In any relationship what so ever, an employer and employ or friends from 
high school, they have something between them that might cannot be even 
understood for themselves.
If a human is behind the creation of a computer then it is possible that 
some of his soul is attached into this piece of software he writes, and 
it can be seen in the way the software is written.
So indeed there are some ?small people? running inside the computer and 
we do hope that if at all AI's will be in use, they will help people and 
will help them with their daily routines while making the world a better 
place for everybody.

For this release I am attaching a video link related to AI:
"The Evolution of Thinking Machines" with Danny Hillis

http://www.youtube.com/watch?v=LxqgGMmmj8A

Or a local mirror at:
The Evolution of Thinking Machines - 
http://ngtech.co.il/squid/videos/The_Evolution_of_Thinking_Machines.mp4

This repository is at:
http://ngtech.co.il/rpm/centos/

This release files are:
http://ngtech.co.il/rpm/centos/7/x86_64/squid-3.5.2-1.el7.centos.x86_64.rpm
http://ngtech.co.il/rpm/centos/7/x86_64/squid-helpers-3.5.2-1.el7.centos.x86_64.rpm
http://ngtech.co.il/rpm/centos/7/x86_64/squid-debuginfo-3.5.2-1.el7.centos.x86_64.rpm

http://ngtech.co.il/rpm/centos/6/x86_64/squid-3.5.2-2.el6.x86_64.rpm
http://ngtech.co.il/rpm/centos/6/x86_64/squid-helpers-3.5.2-2.el6.x86_64.rpm
http://ngtech.co.il/rpm/centos/6/x86_64/squid-debuginfo-3.5.2-2.el6.x86_64.rpm

http://ngtech.co.il/rpm/centos/6/i686/squid-3.5.2-2.el6.i686.rpm
http://ngtech.co.il/rpm/centos/6/i686/squid-helpers-3.5.2-2.el6.i686.rpm
http://ngtech.co.il/rpm/centos/6/i686/squid-debuginfo-3.5.2-2.el6.i686.rpm

To Each and everyone of them there is an *asc* file which contains  MD5 
SHA1 SHA2 SHA256 SHA384 SHA512 TIGER hashes.

All The Bests,
Eliezer Croitoru


From chris at ceegeebee.com  Thu Mar 26 22:01:21 2015
From: chris at ceegeebee.com (Chris Bennett)
Date: Fri, 27 Mar 2015 08:31:21 +1030
Subject: [squid-users] Squid 3.5.2 RPMs release for CentOS 32 and 64 bit.
In-Reply-To: <551427FC.5000403@ngtech.co.il>
References: <54E532BA.1010702@treenet.co.nz>
	<551427FC.5000403@ngtech.co.il>
Message-ID: <CABPTpJDqEGCTbKrh_w8OhO-RiKyunumtSh86WAw_yEqaC+BgXQ@mail.gmail.com>

Hi Eliezer,

I'm one of the many users of your published EL/CentOS RPMs and greatly
appreciate you making it available to the public, as well as keeping up to
date from time to time.  Just thought I'd express thanks :)

Regards,

Chris

On 27 March 2015 at 02:08, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> Published at: http://www1.ngtech.co.il/wpe/?p=25
>
> I am happy to release the new RPMs of squid 3.5.2 for Centos 6.6 64bit,
> 32bit and CentOS 7 64bit.
>
> The new release includes couple bug fixes and improvements.
> The links to the RPMs are at the bottom of the article.
>
> "What if computers would start to think??
>
> I have been asked couple times in the past about the ?livelihood? of the
> PC.
> Sometimes there is the possibility that the computer has some intelligence
> and life in it due to it's nature to do things for humans. Many computers
> users claims that there is some ?geany? inside this machine which sits in
> some room in the house. Sometimes it is even makes ?sense? to the naked eye
> and mind.
> If for example a human will try to understand what is causing some issues
> with the text editor or with the cable, he will eventually find out that
> there are things which he might not be able to understand with the naked
> eye and in most of these cases the help from others is required and
> recommended.
> One case I do remember was that somebody asked me about the cause of some
> issue in run-time.
> While I wanted to answer him I told him that I have couple ideas about
> what causing these issues which he is having, but there is no way in the
> world for me to know what is the real cause due to that fact that I have
> missing parts in the picture. And also if I had the whole ?picture? there
> is a real possibility which I would not be able to analyze all the details
> which are needed to conclude what is the cause for the annoying issues.
> How is it related to a Thinking Machine??
> Some people out there in the world may think that the computer is a
> marvels machine and do think that if it will have the function to ?think?
> by itself and do things for humans it will bring good things.
> The reality proves that the computer is a creation of a human and if the
> humans are trying to do good this machine will be a tool for good while if
> the human will try to do bad this machine will do bad.
> It's the same about Artificial Intelligence which might be implemented in
> computers.
> If indeed the programmer or operator do things for bad, this might be the
> result.
> I do not want to speculate how the world would look like in the age of AI
> but there are some AI levels in the world, even if they are so limited that
> these AI's can crash in run-time very easily.
> In some relation to the last RPM release which I was writing about a
> ?Match Maker?, AI's are kind of a hot topic in this world. Many in this
> world try to help others to find their ?Missing Rib? or ?The other part of
> their "soul mate?.
> While computers can help to find in a way their missing piece they still
> lack one thing: soul.
> In any relationship what so ever, an employer and employ or friends from
> high school, they have something between them that might cannot be even
> understood for themselves.
> If a human is behind the creation of a computer then it is possible that
> some of his soul is attached into this piece of software he writes, and it
> can be seen in the way the software is written.
> So indeed there are some ?small people? running inside the computer and we
> do hope that if at all AI's will be in use, they will help people and will
> help them with their daily routines while making the world a better place
> for everybody.
>
> For this release I am attaching a video link related to AI:
> "The Evolution of Thinking Machines" with Danny Hillis
>
> http://www.youtube.com/watch?v=LxqgGMmmj8A
>
> Or a local mirror at:
> The Evolution of Thinking Machines - http://ngtech.co.il/squid/
> videos/The_Evolution_of_Thinking_Machines.mp4
>
> This repository is at:
> http://ngtech.co.il/rpm/centos/
>
> This release files are:
> http://ngtech.co.il/rpm/centos/7/x86_64/squid-3.5.2-1.
> el7.centos.x86_64.rpm
> http://ngtech.co.il/rpm/centos/7/x86_64/squid-helpers-
> 3.5.2-1.el7.centos.x86_64.rpm
> http://ngtech.co.il/rpm/centos/7/x86_64/squid-
> debuginfo-3.5.2-1.el7.centos.x86_64.rpm
>
> http://ngtech.co.il/rpm/centos/6/x86_64/squid-3.5.2-2.el6.x86_64.rpm
> http://ngtech.co.il/rpm/centos/6/x86_64/squid-helpers-
> 3.5.2-2.el6.x86_64.rpm
> http://ngtech.co.il/rpm/centos/6/x86_64/squid-
> debuginfo-3.5.2-2.el6.x86_64.rpm
>
> http://ngtech.co.il/rpm/centos/6/i686/squid-3.5.2-2.el6.i686.rpm
> http://ngtech.co.il/rpm/centos/6/i686/squid-helpers-3.5.2-2.el6.i686.rpm
> http://ngtech.co.il/rpm/centos/6/i686/squid-debuginfo-3.5.2-2.el6.i686.rpm
>
> To Each and everyone of them there is an *asc* file which contains  MD5
> SHA1 SHA2 SHA256 SHA384 SHA512 TIGER hashes.
>
> All The Bests,
> Eliezer Croitoru
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150327/891a822d/attachment.htm>

From bpk678 at gmail.com  Thu Mar 26 22:22:31 2015
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 26 Mar 2015 18:22:31 -0400
Subject: [squid-users] load balancing and site failover
In-Reply-To: <5513586D.5020407@treenet.co.nz>
References: <1427206737.17590.33.camel@desktop.bpk2.com>
 <1427221530.17590.37.camel@desktop.bpk2.com>
 <CAARxGtjnOTC_erkHv+uX_QSbv1E5nf+fOKj-z=5LrB0f8pOyhQ@mail.gmail.com>
 <55121755.8010905@treenet.co.nz>
 <1427318767.24562.31.camel@desktop.bpk2.com>
 <5513586D.5020407@treenet.co.nz>
Message-ID: <1427408551.30756.33.camel@desktop.bpk2.com>

On Thu, 2015-03-26 at 13:53 +1300, Amos Jeffries wrote:
> On 26/03/2015 10:26 a.m., Brendan Kearney wrote:
> > On Wed, 2015-03-25 at 15:03 +1300, Amos Jeffries wrote:
> >> On 25/03/2015 9:55 a.m., brendan kearney wrote:
> >>> Was not sure if bugzilla was used for mailing list issues.  If you would
> >>> like me to open one, I will but it looks like the list is working again.
> >>
> >> Bugzilla is used, list bugs under the "project services" product.
> >>
> >>
> >> As for your query...
> >>
> >>> On Mar 24, 2015 2:25 PM, "Brendan Kearney" wrote:
> >>>
> >>>> On Tue, 2015-03-24 at 10:18 -0400, Brendan Kearney wrote:
> >>>>> while load balancing is not a requirement in a proxy environment, it
> >>>>> does afford a great deal of functionality, scaling and fault tolerance
> >>>>> in one.  several if not many on this list probably employ them for their
> >>>>> proxies and likely other technologies, but they are not all created
> >>>>> equal.
> >>>>>
> >>>>> i recently looked to see if a specific feature was in HAProxy.  i was
> >>>>> looking to see if HAProxy could reply to a new connection with a RST
> >>>>> packet if no pool member was available.
> >>>>>
> >>>>> the idea behind this is, if all of the proxies are not passing the
> >>>>> service check and are marked down by the load balancer, the reply of a
> >>>>> RST in the TCP handshake (i.e. SYN -> RST, not SYN -> SYN/ACK -> ACK)
> >>>>> tells the browser to failover to the next proxy assigned by the PAC
> >>>>> file.
> >>>>>
> >>>>> where i work, we have this configuration working.  the load balancers
> >>>>> are configured with the option to send a reset when no proxy is
> >>>>> available in the pool.  the PAC file assigns all 4 of the proxy VIPs in
> >>>>> a specific order based on which proxy VIP is assigned as the primary.
> >>>>> In every case, if the primary VIP does not have an available pool
> >>>>> member, the browser fails over to the next in the list.  failover would
> >>>>> happen again, if the secondary VIP replies with a RST during the
> >>>>> connection establishing.  the process repeats until a TCP connection
> >>>>> establishes or all proxies assigned have been exhausted.  the browser
> >>>>> will use the proxy VIP that it successfully connects to, for the
> >>>>> duration of the session.  once the browser is closed and reopened, the
> >>>>> evaluation of the PAC file occurs again, and the process starts anew.
> >>>>> plug-ins such as Proxy Selector are the exception to this, and can be
> >>>>> used to reevaluate a PAC file by selecting it for use.
> >>>>>
> >>>>> we have used this configuration several times, when we found an ISP link
> >>>>> was flapping or some other issue more global in nature than just the
> >>>>> proxies was affecting our egress and internet access.  i can attest to
> >>>>> the solution as working and elegantly handling site wide failures.
> >>>>>
> >>>>> being that the solutions where i work are proprietary commercial
> >>>>> products, i wanted to find an open source product that does this.  i
> >>>>> have been a long time user of HAProxy, and have recommended it for
> >>>>> others here, but sadly they cannot perform this function.  per their
> >>>>> mailing list, they use the network stack of the OS for connection
> >>>>> establishment and cannot cause a RST to be sent to the client during a
> >>>>> TCP handshake if no pool member is available.
> >>>>>
> >>>>> they suggested an external helper that manipulates IPTables rules based
> >>>>> on a pool member being available.  they do not feel that a feature like
> >>>>> this belongs in a layer 4/7 reverse proxy application.
> >>
> >> They are right. HTTP != TCP.
> > i didnt confuse that detail.  it was unknown to me that HAProxy could
> > not tie layer 7 status to layer 3/4 actions.  the decisions they made
> > and how they architected the app is why they cannot do this, not that it
> > is technically impossible to do it.  i may be spoiled because i work
> > with equipment that can do this for me.
> >>
> >> In particular TCP depends on routers having a full routing map of the
> >> entire Internet (provided by BGP) and deciding the best upstream hop
> >> based on that global info. Clients have one (and only one) upstream
> >> router for each server they want to connect to.
> > i will contest this.  my router does not need a full BGP map to route
> > traffic locally on my LAN or remotely out its WAN interface.  hell, it
> > does not even run BGP, and i can still get to the intarwebs, no problem.
> > it too, only has one upstream router / default route.
> 
> Then your router has more in common with proxy than usual. Its operating
> with a next-hop packet relay model (OSPF? MPLS?) rather than an
> end-to-end model (BGP with RIB/FIB).
DOCSIS 2 -> ethernet on the WAN side and locally connected on the LAN
side. :D  oh, and a static route pointing a /24 for vpn traffic to a
specific device.
> 
> >>
> >> In HTTP each proxy (aka router) performs independent upstream connection
> >> attempts, failover, and verifies it worked before responding to the
> >> client with a final response. Each proxy only has enough detail to check
> >> its upstream(s). Each proxy can connect to any server (subject to ACLs).
> > how are you comparing a HTTP proxy (a layer 7 application) to a router
> > (a layer 3 device)?  routers route traffic and proxies proxy traffic.
> 
> while, routers proxy TCP packets and proxies route HTTP messages.
> 
> Its the behaviour abstraction I'm talking about here.
> [if you dont want to dive into theory skip to the end of this mail]
> 
> The algorithms each are capable of are the same despite differences in
> details of layer and what designed mechanisms are optimal for their
> protocol.
> DNS recursive resolvers are also a type of caching proxy, and SMTP email
> servers too. Of the surviving "old" protocols only FTP seems not to have
> proxies as integral part of the design.
recursive DNS queries, yes i can see that as being proxied.  SMTP gets a
bit gray.  MUA -> MRA -> MTA, store and forward, ehh.  you can make a
convincing argument for or against, but the "on behalf of" piece is
there.  FTP and the PORT command (and subsequent bounce attack) could be
considered a proxied connection, but i dont think that was an intended
design.
> 
> 
> > very different functions.  routers dont look past a certain point in the
> > headers in order to make decisions on where to send the traffic.
> 
> Same for proxies. I'm sure you're aware that Squid dont look into HTTP
> payloads same as routers dont look into TCP payloads.
> 
> > proxies look all the way to the end of the headers and sometimes into
> > the payload, too.
> 
> No proxies dont look into payloads. Content filters do that bit instead
> of the proxy.
> Just like routers can be enhanced by DPI systems doing payload
> inspection at their level.
> 
> In both systems the DPI / content filter is where they cross over into
> firewall activity.
i sometimes forget that i have two technologies bolted together on a
"proxy appliance" and that is why i misstated that.  yes, proxies dont
inspect payload.  content inspection systems do that.
> 
> >  proxies are more akin to a protocol specific
> > firewall.
> 
> You have no firewall ability built into your router?
from a puritanical perspective, no.  routing decisions are based on
layer 3 info.  ports are layer 4 info.  a true, layer 3 only router
would not make a good firewall at all.  that is not to say you cannot
enforce a security posture with a router, as part of an overall
strategy.
> 
> >  proxies also dont send the incoming traffic out an interface.
> 
> layer-3/4 interface is equivalent to layer-5/7 socket in abstract.
your missing the point.  routers dont accept an incoming connection, and
initiate a separate outgoing connection, in order to facilitate the
end-to-end conversation.  proxies do.
> 
> > they terminate the client session, and initiate a new session on behalf
> > of the client.
> 
> not necessarily, that is the architectural difference between HTTP and
> TCP. SOCKS proxies and TCP have much more in common in architectural design.
> 
> What the proxies do is terminate the *TCP* session. Obviously all
> non-TCP protocols will do that.
> The HTTP layer is more inline with UDP here than TCP. Although at the
> layer just above HTTP there is the "browsing session" concept
> implemented with cookies/auth etc which is retained across both TCP
> layer connections. For cases where TCP-level sessions need to be
> emulated there is connection pinning in the proxy.
socks proxies are a different animal outside the scope of what i
intended to discuss.

http can be considered almost stateless, like udp, but it is still a
connection oriented protocol.  the sessions are generally short and
chatty.  these qualities are why CDNs work and can use anycast for load
distribution.
> 
> 
> >  simply because the proxy can elect how to send a request
> > it is making on behalf of a client, does not make a proxy a router.
> 
> The names are terminology for the activity performed. TCP routers
> normally do not re-selecting the destination endpoint, just the path.
> The proxy selects the actual endpoint destination.
> 
> What you can configure each to do crosses over and you can specifically
> configure a proxy to become a router, and vice versa. Doing it though
> screws with their normal and optimal behavour. Router becomes like a
> proxy by turning on NAT - and we probably both know the hell that
> causes. Making a proxy do strictly router behaviour has a similar range
> of nasty side effects.
um, no.  NAT does not cause a connection to be proxied.  NAT is a header
rewrite operation, and the connection is from endpoint to endpoint.  it
is not proxied, where the router terminates the incoming connection and
initiates a new outgoing connection to facilitate the the conversation.
> 
> Yes each one (proxy vs router) is designed to work at different levels,
> in different ways. But how you use them determines what they do.
> 
> In particular this is the difference which you request is asking us to
> provide a way to disable. Making the proxy return TCP RST if a
> particular endpoint is not available - despite other endpoints
> potentially being usable.
>  Making the proxy behave like a router.
i am looking for the load balancer to return the RST.  i asked what
others here may be doing in that space, if any are doing something.  you
provided an example of how to send a RST from a proxy when a proxy
cannot fulfill the request.
> 
> 
> >  the
> > fact that one connection is terminated and a new one is initiated rules
> > out a proxy from being any kind of router, in my opinion.
> 
> That rules out all devices containing NAT functionality or LB software
> from being routers.
no, again NAT does not result in a terminated incoming connection and a
new outgoing connection.  Load balancing does do this as it is TCP
proxying of connections.  in many cases load balancers that do TCP
proxying are intelligent and aware of TCP based protocols such as SMTP,
LDAP, HTTP, etc.  it is this intelligence that i was hoping could be
tied to the TCP proxying piece, so as to reply with the RST on the
frontend when no backend server is available.
> 
> >  even with SSL
> > or the CONNECT Method, the connection is still made by the proxy to the
> > remote server.  the client never makes a connection to the remote
> > server, therefore the traffic was not routed.  it was proxied.
> 
> By definition CONNECT method is not a proxy. In particular it is the
> request that the proxy stop being a HTTP proxy and become a tunnel / TCP
> relay.
even the CONNECT Method results in one incoming connection being
terminated, and new outgoing connection being initiated.  that is
proxying.
> It is roughly equivalent to a router sending packets down a tunnel / VPN
> or SOCKS interface (the proxy is the outgoing end of the tunnel/VPN).
no it isnt, because the router is not making the connection.  its
facilitating the connection. again no in and out connections.  the
endpoint sending the SYN is what is seen on the other end of the VPN as
making the connection.  while the packet may or may not be NAT'ed, the
connection is endpoint to endpoint.
> 
> 
> 
> >>
> >>>>>
> >>>>> my search for a load balancer solution went through ipvsadm, balance and
> >>>>> haproxy before i selected haproxy.  haproxy was more feature rich than
> >>>>> balance, and easier to implement than ipvsadm.  do any other list
> >>>>> members have a need for such a feature from their load balancers?  do
> >>>>> any other list members have site failover solutions that have been
> >>>>> tested or used and would consider sharing their design and/or pain
> >>>>> points?  i am not looking for secret sauce or confidential info, but
> >>>>> more high level architecture decisions and such.
> >>
> >>
> >> I havent tested it but this should do what you are asking:
> >>
> >>  acl err http_status 500-505 408
> >>  deny_info TCP_RESET err
> >>  http_reply_access deny err
> >>
> >> It replaces the response from Squid with a TCP RST packet.
> > this is useful in the case that the proxy is alive and well, but cannot
> > get to the internet.  in my example, the ISP issue would seem to be
> > covered, though i am not sure how the actual implementation would go.
> > the client has a TCP session established with the load balancer, which
> > gets the full SYN -> SYN/ACK -> ACK treatement.  the load balancer would
> > get the SYN -> RST from the proxy, and presumably sends the RST back to
> > the client.  While this does seem to hold up logically, the
> > implementation may have nuances that have to be dealt with.  Does the
> > RST in the middle of an established TCP session cause the browser to
> > failover to the next proxy assigned?  i would have to test that out.
> 
> Nod.
> 
> > 
> > now, what about the case where the proxies are not alive and well behind
> > the load balancer, and they are not able to reply with a RST?  This is
> > the scenario that i would want the load balancer to be able to manage.
> > this is where tying a layer 7 status to a layer 3/4 action on the load
> > balancer becomes relevant.  then, the ability for the load balancer to
> > do this negates the need to manage this in the proxy layer, and removes
> > any nuances that may be encountered with the implementation.
> 
> 
> That is a normal TCP error case. Same things happen if there is any
> network level outage, or packets destined to a non-existent IP range. It
> depends on the LB software what will be done about it.
> 
> Amos
> 





From dan at getbusi.com  Thu Mar 26 22:47:19 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Fri, 27 Mar 2015 09:47:19 +1100
Subject: [squid-users] assertion failed: ../src/ipc/AtomicWord.h:88:
	"Enabled()"
In-Reply-To: <C55F6BE7-274E-488B-AAB5-28B600D655DD@getbusi.com>
References: <CD8F4E6B-5D87-4732-80EE-402D5169B606@getbusi.com>
 <C55F6BE7-274E-488B-AAB5-28B600D655DD@getbusi.com>
Message-ID: <2FC66EBD-75C8-4ADD-A8C6-38759120173F@getbusi.com>

Bumping this because I think it might have gone into the black hole the other night.

> On 23 Mar 2015, at 5:44 pm, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Turns out it?s also shitting the bed whenever I go to an SSL site now that I?ve added --enable-storeio=rock:
> 
> 2015/03/23 17:40:13 kid1| assertion failed: ../src/ipc/AtomicWord.h:71: "Enabled()"
> 2015/03/23 17:42:02 kid1| assertion failed: ../src/ipc/AtomicWord.h:74: "Enabled()"
> 
> I feel like I?m definitely missing a dependency or something :-/
> 
>> On 23 Mar 2015, at 5:28 pm, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>> 
>> Hey!
>> 
>> Sorry for all the threads lately, folks -
>> 
>> I just recompiled by 3.5 EL6 (64-bit) RPM (using squid-3.5.2-20150321-r13782).
>> 
>> I decided to add rock to my `?enable-storeio` option, so I could try SMP and stuff, which was fine. But when I went to squid -z it, I got this crash:
>> assertion failed: ../src/ipc/AtomicWord.h:88: "Enabled()"
>> 
>> Just using:
>> cache_dir rock /var/spool/squid 20000
>> workers 2
>> 
>> I?m hoping, for a change, this is some obvious thing I?ve missed and not something I need to dig out backtraces for :-)
>> 
>> Thanks, y'all
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150327/396beef8/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 27 10:50:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Mar 2015 23:50:25 +1300
Subject: [squid-users] assertion failed: ../src/ipc/AtomicWord.h:88:
	"Enabled()"
In-Reply-To: <2FC66EBD-75C8-4ADD-A8C6-38759120173F@getbusi.com>
References: <CD8F4E6B-5D87-4732-80EE-402D5169B606@getbusi.com>
 <C55F6BE7-274E-488B-AAB5-28B600D655DD@getbusi.com>
 <2FC66EBD-75C8-4ADD-A8C6-38759120173F@getbusi.com>
Message-ID: <551535F1.50500@treenet.co.nz>

Hi Dan,
 This appears by a breakage in the 3.5 snapshots' GNU atomics detection.
Though we are still not sure why the error occurs yet with atomics disabled.

Snapshots labelled r13783 or later available in a few hrs should be fixed.

Cheers
Amos


On 27/03/2015 11:47 a.m., Dan Charlesworth wrote:
> Bumping this because I think it might have gone into the black hole the other night.
> 
>> On 23 Mar 2015, at 5:44 pm, Dan Charlesworth <dan at getbusi.com> wrote:
>>
>> Turns out it?s also shitting the bed whenever I go to an SSL site now that I?ve added --enable-storeio=rock:
>>
>> 2015/03/23 17:40:13 kid1| assertion failed: ../src/ipc/AtomicWord.h:71: "Enabled()"
>> 2015/03/23 17:42:02 kid1| assertion failed: ../src/ipc/AtomicWord.h:74: "Enabled()"
>>
>> I feel like I?m definitely missing a dependency or something :-/
>>
>>> On 23 Mar 2015, at 5:28 pm, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
>>>
>>> Hey!
>>>
>>> Sorry for all the threads lately, folks -
>>>
>>> I just recompiled by 3.5 EL6 (64-bit) RPM (using squid-3.5.2-20150321-r13782).
>>>
>>> I decided to add rock to my `?enable-storeio` option, so I could try SMP and stuff, which was fine. But when I went to squid -z it, I got this crash:
>>> assertion failed: ../src/ipc/AtomicWord.h:88: "Enabled()"
>>>
>>> Just using:
>>> cache_dir rock /var/spool/squid 20000
>>> workers 2
>>>
>>> I?m hoping, for a change, this is some obvious thing I?ve missed and not something I need to dig out backtraces for :-)
>>>
>>> Thanks, y'all
>>
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From emz at norma.perm.ru  Fri Mar 27 13:57:20 2015
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Fri, 27 Mar 2015 18:57:20 +0500
Subject: [squid-users] mmap() in squid
Message-ID: <551561C0.2050108@norma.perm.ru>

Hi.

Squid uses mmap() call from 3.4.x, and mmap() on FreeBSD it has one
specific flag - MAP_NOSYNC, which prevents dirtied pages from being
flushed on disk:

MAP_NOSYNC        Causes data dirtied via this VM map to be flushed to
                       physical media only when necessary (usually by the
                       pager) rather than gratuitously.  Typically this pre-
                       vents the update daemons from flushing pages dirtied
                       through such maps and thus allows efficient
sharing of
                       memory across unassociated processes using a file-
                       backed shared memory map.  Without this option any VM
                       pages you dirty may be flushed to disk every so often
                       (every 30-60 seconds usually) which can create
perfor-
                       mance problems if you do not need that to occur (such
                       as when you are using shared file-backed mmap regions
                       for IPC purposes).  Note that VM/file system
coherency
                       is maintained whether you use MAP_NOSYNC or not. 
This
                       option is not portable across UNIX platforms (yet),
                       though some may implement the same behavior by
default.

                       WARNING!  Extending a file with ftruncate(2),
thus cre-
                       ating a big hole, and then filling the hole by
modify-
                       ing a shared mmap() can lead to severe file
fragmenta-
                       tion.  In order to avoid such fragmentation you
should
                       always pre-allocate the file's backing store by
                       write()ing zero's into the newly extended area
prior to
                       modifying the area via your mmap().  The
fragmentation
                       problem is especially sensitive to MAP_NOSYNC pages,
                       because pages may be flushed to disk in a totally
ran-
                       dom order.

                       The same applies when using MAP_NOSYNC to implement a
                       file-based shared memory store.  It is
recommended that
                       you create the backing store by write()ing zero's to
                       the backing file rather than ftruncate()ing it.  You
                       can test file fragmentation by observing the KB/t
                       (kilobytes per transfer) results from an ``iostat 1''
                       while reading a large file sequentially, e.g. using
                       ``dd if=filename of=/dev/null bs=32k''.

                       The fsync(2) system call will flush all dirty
data and
                       metadata associated with a file, including dirty
NOSYNC
                       VM data, to physical media.  The sync(8) command and
                       sync(2) system call generally do not flush dirty
NOSYNC
                       VM data.  The msync(2) system call is obsolete since
                       BSD implements a coherent file system buffer cache.
                       However, it may be used to associate dirty VM pages
                       with file system buffers and thus cause them to be
                       flushed to physical media sooner rather than later.

Last year there was an issue with PostgreSQL, which laso started to use
mmap() in it's 9.3 release, and it had a huge regression issue on
FreeBSD. One of the measures to fight this regression (but not the only)
was adding MAP_NOSYNC to the postgresql port. So I decided to do the
same for my local squid. I created a patch, where both of two
occurencies of mmap() were supplied with this flag. I'm using squid
3.4.x patched this way about a half-a-year. Couple of days ago I sent
this patch to the FreeBSD ports system, and squid port maintainer asks
me if I'm sure squid on FreeBSD does need this. Since I'm not a skilled
programmer (though I think using mmap() with MAP_NOSYNC is a good
thing), I decided to ask here - is this flag worth bothering, since
squid isn't a database engine ?

Thanks.


From dig at digcorp.net  Fri Mar 27 15:34:00 2015
From: dig at digcorp.net (Daniel Greenwald)
Date: Fri, 27 Mar 2015 11:34:00 -0400
Subject: [squid-users] ssl_bump for specific dstdomain
In-Reply-To: <5502A807.6080700@treenet.co.nz>
References: <CAJUmE_Mq-vBr+6GQgXSThDDqp5F3+L-8+jc3H2+XN-996QpS0w@mail.gmail.com>
 <5501AAF6.6040609@gmail.com>
 <CAJUmE_PVfmsVA5Ykd2Gk6PULLEsY2jzF5e0VquZWKeAfK-W3uQ@mail.gmail.com>
 <55027817.9070607@gmail.com> <5502A807.6080700@treenet.co.nz>
Message-ID: <CAOsHgtsG76+idOJWOQE+AfpStNePOgThRGyQfv-S5ptT2NbsJQ@mail.gmail.com>

here is a python helper I wrote with help of previous posts. It takes
sni info from squid and returns OK if the domain is in
/etc/squid/domains_nobump.acl (I am not a coder..) Problem is it works
good for intercepted browser traffic but doesn't work when a user
tries to use an app on an eg android device. In my cache.log I get:
 error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca

It seems Squid doesn't send SNI info before the error shows up in
cache.log and the app barfs. I'm guessing the app is detecting the
SSLBump before squid is able to grab the SNI. Does this makes sense?
Any ideas to move this forward?


#!/usr/bin/python
import sys
import string
lines = [line.strip() for line in open('/etc/squid/domains_nobump.acl')]

while True:
    req = sys.stdin.readline()
    req = req.strip()
    if not req:
        break

    try:
        id, sni = req.split()
        sys.stderr.write('request %r\n' % req)
        sys.stderr.flush()
        for line in lines:
            if line.startswith('.'):
                if string.find(sni,line,len(sni)-len(line)) != -1 or
sni == line.lstrip('.'):  # bypass
                    sys.stdout.write('{} OK\n'.format(id))
                    sys.stdout.flush()
                    break
            else:
                if sni == line:
                    sys.stdout.write('{} OK\n'.format(id))
                    sys.stdout.flush()
                    break

        else:
            sys.stdout.write('{} ERR\n'.format(id))
            sys.stdout.flush()
    except:
        sys.stderr.write('SNICHECK INPUT: %r\n' % req)


squid.conf:

external_acl_type sni ttl=30 concurrency=60 children-max=3
children-startup=1 %ssl::>sni /usr/lib64/squid/snicheck.py
acl sni_nobump external sni
ssl_bump splice sni_nobump
ssl_bump peek step1 all
ssl_bump bump step2 all
-----------
Daniel I Greenwald




On Fri, Mar 13, 2015 at 5:04 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 13/03/2015 6:39 p.m., Yuri Voinov wrote:
>>
>>
>> 13.03.15 2:37, Mukul Gandhi ?????:
>>> On Thu, Mar 12, 2015 at 11:04 AM, Yuri Voinov <yvoinov at gmail.com>
>>> wrote:
>>
>>> You only have external helper (which is must wrote yourself) in
>>> 3.4.x.
>>
>>
>>>> Are there any examples that I can look at to implemented this
>>>> external helper for doing selective ssl_bumps. And what would
>>>> this helper script do anyways? All we have is the destination IP
>>>> address which is not really going to give us the actual HTTP
>>>> hostname.
>> Yes and no. There is one third-party helper in list archives, written
>> on python. No one of this including in squid distribution.
>>
>>
>>> Works with domains in ssl bump fully available at least 3.5.x
>>
>>
>>>> Does the 3.5.x implementation decrypt the whole payload and then
>>>> do the ssl_bump? The "peek" option seems to imply that only the
>>>> HTTP headers are peeked at.
>> Of course. As by 3.4.x. The difference is only with mechanisms.
>
> And no at the same time. HTTP message headers inside the encryption are
> encrypted and unavailable until after the decryption is decided (bumped).
>
> What gets peeked at is the TLS ClientHello and TLS ServerHello details.
> SNI may become available by peeking when raw-IP was all that was in the
> HTTP CONNECT message or intercepted TCP packets.
>
> You can then use those non-private TLS details to decide between reject,
> splice (pass-thru) or bump (decrypt) for the encrypted HTTPS data.
>
>
>>
>>>> I guess what I am asking is, is there any way we can do this
>>>> without actually decrypting the payload?
>> 3.5.x peek-and-splise functionality do bump splitted by stages.
>> Against 3.4.x, which is makes bump in one stage.
>>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ashish.patil at shreshtait.com  Sat Mar 28 15:55:17 2015
From: ashish.patil at shreshtait.com (Ashish Patil)
Date: Sat, 28 Mar 2015 21:25:17 +0530
Subject: [squid-users] Squid behaviour with external_acl_type
Message-ID: <CANQnQxQKw+Upv3v1A3u_ufLbWuE+cPpxdquJrP+8DKTGZs5VMw@mail.gmail.com>

Hello,

I am faced with a weird situation with an external acl that I have built.

The external acl gets the acl name and the IP information, does a lookup in
a MySQL table if that IP belongs to a group ( read: acl name ) and returns
an output.

The situation is as follows:
 On the first client request coming to the external helper, it performs as
expected, and the correct action is taken. Whereas from the second request
onwards, Squid just waits on the external acl, even though a response was
sent by the acl, and the same was received by Squid.

My acl's are as follows:
external_acl_type grpname ttl=10 children-startup=1 concurrency=1 %SRC %ACL
/usr/local/squid/libexec/grpname_helper
acl two external grpname
acl twoext urlpath_regex
"/usr/local/squid/etc/custom/blacklisted-two-extensions"
deny_info http://192.168.3.11/error.html two
http_access deny twoext two


The debug output for the first request coming in:
2015/03/28 20:11:30.831 kid1| external_acl.cc(868) aclMatchExternal:
grpname("192.168.3.243 two") = lookup needed
2015/03/28 20:11:30.831 kid1| external_acl.cc(871) aclMatchExternal:
"192.168.3.243 two": queueing a call.
2015/03/28 20:11:30.831 kid1| external_acl.cc(1453) Start: fg lookup in
'grpname' for '192.168.3.243 two'
2015/03/28 20:11:30.832 kid1| external_acl.cc(1508) Start:
externalAclLookup: looking up for '192.168.3.243 two' in 'grpname'.
2015/03/28 20:11:30.832 kid1| helper.cc(1208) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2015/03/28 20:11:30.832 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
helperDispatchWriteDone constructed, this=0x9db91e8 [call2903]
2015/03/28 20:11:30.832 kid1| Write.cc(29) Write: local=[::] remote=[::] FD
22 flags=1: sz 20: asynCall 0x9db91e8*1
2015/03/28 20:11:30.832 kid1| ModEpoll.cc(139) SetSelect: FD 22, type=2,
handler=1, client_data=0x970522c, timeout=0
2015/03/28 20:11:30.832 kid1| helper.cc(1350) helperDispatch:
helperDispatch: Request sent to grpname #Hlpr0, 18 bytes
2015/03/28 20:11:30.833 kid1| external_acl.cc(1518) Start:
externalAclLookup: will wait for the result of '192.168.3.243 two' in
'grpname' (ch=0x985f400).
2015/03/28 20:11:30.833 kid1| external_acl.cc(874) aclMatchExternal:
"192.168.3.243 two": return -1.
2015/03/28 20:11:30.833 kid1| Acl.cc(177) matches: checked: two = -1 async
2015/03/28 20:11:30.833 kid1| Acl.cc(177) matches: checked: http_access#1 =
-1 async
2015/03/28 20:11:30.833 kid1| Acl.cc(177) matches: checked: http_access =
-1 async
2015/03/28 20:11:30.834 kid1| Write.cc(60) HandleWrite: local=[::]
remote=[::] FD 22 flags=1: off 0, sz 20.
2015/03/28 20:11:30.834 kid1| Write.cc(100) HandleWrite: write() returns 20
2015/03/28 20:11:30.834 kid1| IoCallback.cc(108) finish: called for
local=[::] remote=[::] FD 22 flags=1 (0, 0)
2015/03/28 20:11:30.834 kid1| AsyncCall.cc(85) ScheduleCall:
IoCallback.cc(127) will call helperDispatchWriteDone(local=[::] remote=[::]
FD 22 flags=1, errno=11, data=0x9b6c4f8, size=20, buf=0x9c9e148) [call2903]
2015/03/28 20:11:30.834 kid1| AsyncCallQueue.cc(51) fireNext: entering
helperDispatchWriteDone(local=[::] remote=[::] FD 22 flags=1, errno=11,
data=0x9b6c4f8, size=20, buf=0x9c9e148)
2015/03/28 20:11:30.835 kid1| AsyncCall.cc(30) make: make call
helperDispatchWriteDone [call2903]
2015/03/28 20:11:30.835 kid1| AsyncCallQueue.cc(53) fireNext: leaving
helperDispatchWriteDone(local=[::] remote=[::] FD 22 flags=1, errno=11,
data=0x9b6c4f8, size=20, buf=0x9c9e148)
2015/03/28 20:11:30.835 kid1| ModEpoll.cc(139) SetSelect: FD 22, type=2,
handler=0, client_data=0, timeout=0
2015/03/28 20:11:30.902 kid1| comm.cc(138) commHandleRead: comm_read_try:
FD 22, size 4095, retval 4, errno 0
2015/03/28 20:11:30.902 kid1| IoCallback.cc(108) finish: called for
local=[::] remote=[::] FD 22 flags=1 (0, 0)
2015/03/28 20:11:30.902 kid1| AsyncCall.cc(85) ScheduleCall:
IoCallback.cc(127) will call helperHandleRead(local=[::] remote=[::] FD 22
flags=1, data=0x9b6c4f8, size=4, buf=0x9aa1508) [call980]
2015/03/28 20:11:30.902 kid1| AsyncCallQueue.cc(51) fireNext: entering
helperHandleRead(local=[::] remote=[::] FD 22 flags=1, data=0x9b6c4f8,
size=4, buf=0x9aa1508)
2015/03/28 20:11:30.902 kid1| AsyncCall.cc(30) make: make call
helperHandleRead [call980]
2015/03/28 20:11:30.903 kid1| helper.cc(906) helperHandleRead:
helperHandleRead: 4 bytes from grpname #Hlpr0
2015/03/28 20:11:30.903 kid1| helper.cc(932) helperHandleRead:
helperHandleRead: end of reply found
2015/03/28 20:11:30.903 kid1| HelperReply.cc(23) parse: Parsing helper
buffer
2015/03/28 20:11:30.903 kid1| HelperReply.cc(42) parse: Buff length is
larger than 2
2015/03/28 20:11:30.903 kid1| HelperReply.cc(46) parse: helper Result = OK
2015/03/28 20:11:30.903 kid1| external_acl.cc(1375) externalAclHandleReply:
reply={result=OK}
2015/03/28 20:11:30.904 kid1| external_acl.cc(1290) external_acl_cache_add:
external_acl_cache_add: Adding '192.168.3.243 two' = ALLOWED


Debug Output in the case of the enxt request:
2015/03/28 20:11:54.422 kid1| Acl.cc(157) matches: checking two
2015/03/28 20:11:54.423 kid1| external_acl.cc(868) aclMatchExternal:
grpname("192.168.3.243 two") = lookup needed
2015/03/28 20:11:54.423 kid1| external_acl.cc(871) aclMatchExternal:
"192.168.3.243 two": queueing a call.
2015/03/28 20:11:54.423 kid1| external_acl.cc(1453) Start: fg lookup in
'grpname' for '192.168.3.243 two'
2015/03/28 20:11:54.423 kid1| external_acl.cc(1508) Start:
externalAclLookup: looking up for '192.168.3.243 two' in 'grpname'.
2015/03/28 20:11:54.423 kid1| helper.cc(1208) GetFirstAvailable:
GetFirstAvailable: Running servers 1
2015/03/28 20:11:54.423 kid1| AsyncCall.cc(18) AsyncCall: The AsyncCall
helperDispatchWriteDone constructed, this=0xa0505f0 [call4372]
2015/03/28 20:11:54.424 kid1| Write.cc(29) Write: local=[::] remote=[::] FD
22 flags=1: sz 20: asynCall 0xa0505f0*1
2015/03/28 20:11:54.424 kid1| ModEpoll.cc(139) SetSelect: FD 22, type=2,
handler=1, client_data=0x970522c, timeout=0
2015/03/28 20:11:54.424 kid1| helper.cc(1350) helperDispatch:
helperDispatch: Request sent to grpname #Hlpr0, 18 bytes
2015/03/28 20:11:54.424 kid1| external_acl.cc(1518) Start:
externalAclLookup: will wait for the result of '192.168.3.243 two' in
'grpname' (ch=0x985f400).
2015/03/28 20:11:54.424 kid1| external_acl.cc(874) aclMatchExternal:
"192.168.3.243 two": return -1.
2015/03/28 20:11:54.425 kid1| Acl.cc(177) matches: checked: two = -1 async
2015/03/28 20:11:54.425 kid1| Acl.cc(177) matches: checked: http_access#1 =
-1 async
2015/03/28 20:11:54.425 kid1| Acl.cc(177) matches: checked: http_access =
-1 async
2015/03/28 20:11:54.425 kid1| Write.cc(60) HandleWrite: local=[::]
remote=[::] FD 22 flags=1: off 0, sz 20.
2015/03/28 20:11:54.425 kid1| Write.cc(100) HandleWrite: write() returns 20
2015/03/28 20:11:54.426 kid1| IoCallback.cc(108) finish: called for
local=[::] remote=[::] FD 22 flags=1 (0, 0)
2015/03/28 20:11:54.426 kid1| AsyncCall.cc(85) ScheduleCall:
IoCallback.cc(127) will call helperDispatchWriteDone(local=[::] remote=[::]
FD 22 flags=1, errno=11, data=0x9b6c4f8, size=20, buf=0x9c9e148) [call4372]
2015/03/28 20:11:54.426 kid1| AsyncCallQueue.cc(51) fireNext: entering
helperDispatchWriteDone(local=[::] remote=[::] FD 22 flags=1, errno=11,
data=0x9b6c4f8, size=20, buf=0x9c9e148)
2015/03/28 20:11:54.426 kid1| AsyncCall.cc(30) make: make call
helperDispatchWriteDone [call4372]
2015/03/28 20:11:54.426 kid1| AsyncCallQueue.cc(53) fireNext: leaving
helperDispatchWriteDone(local=[::] remote=[::] FD 22 flags=1, errno=11,
data=0x9b6c4f8, size=20, buf=0x9c9e148)
2015/03/28 20:11:54.427 kid1| comm.cc(138) commHandleRead: comm_read_try:
FD 22, size 4094, retval 4, errno 0
2015/03/28 20:11:54.427 kid1| IoCallback.cc(108) finish: called for
local=[::] remote=[::] FD 22 flags=1 (0, 0)
2015/03/28 20:11:54.427 kid1| AsyncCall.cc(85) ScheduleCall:
IoCallback.cc(127) will call helperHandleRead(local=[::] remote=[::] FD 22
flags=1, data=0x9b6c4f8, size=4, buf=0x9aa1509) [call2905]
2015/03/28 20:11:54.427 kid1| ModEpoll.cc(139) SetSelect: FD 22, type=2,
handler=0, client_data=0, timeout=0
2015/03/28 20:11:54.427 kid1| AsyncCallQueue.cc(51) fireNext: entering
helperHandleRead(local=[::] remote=[::] FD 22 flags=1, data=0x9b6c4f8,
size=4, buf=0x9aa1509)
2015/03/28 20:11:54.428 kid1| AsyncCall.cc(30) make: make call
helperHandleRead [call2905]
2015/03/28 20:11:54.428 kid1| helper.cc(906) helperHandleRead:
helperHandleRead: 4 bytes from grpname #Hlpr0


To verify Squid was getting the responses from the external acl, I ran a
strace on the squid process. Below is the trimmed output:

write(19, "0 192.168.3.243 two\n", 20)  = 20
gettimeofday({1427556227, 22424}, NULL) = 0
epoll_wait(6, {{EPOLLOUT, {u32=19, u64=13821314555071954963}}}, 4096, 201)
= 1
gettimeofday({1427556227, 22698}, NULL) = 0
epoll_ctl(6, EPOLL_CTL_MOD, 19, {EPOLLIN|EPOLLERR|EPOLLHUP, {u32=19,
u64=586827630829895699}}) = 0
gettimeofday({1427556227, 22772}, NULL) = 0
epoll_wait(6, {{EPOLLIN, {u32=19, u64=586827630829895699}}}, 4096, 201) = 1
gettimeofday({1427556227, 22854}, NULL) = 0
read(19, "OK\n\0", 4095)                = 4

write(19, "0 192.168.3.243 two\n", 20)  = 20
gettimeofday({1427556242, 856906}, NULL) = 0
epoll_wait(6, {{EPOLLOUT, {u32=19, u64=13821314555071954963}}}, 4096, 388)
= 1
gettimeofday({1427556242, 857085}, NULL) = 0
epoll_ctl(6, EPOLL_CTL_MOD, 19, {EPOLLIN|EPOLLERR|EPOLLHUP, {u32=19,
u64=586827630829895699}}) = 0
gettimeofday({1427556242, 857175}, NULL) = 0
epoll_wait(6, {{EPOLLIN, {u32=19, u64=586827630829895699}}}, 4096, 388) = 1
gettimeofday({1427556242, 857250}, NULL) = 0
read(19, "OK\n\0", 4094)                = 4


Any suggestions are welcome.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150328/720c6ef3/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 30 10:10:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Mar 2015 23:10:24 +1300
Subject: [squid-users] Squid behaviour with external_acl_type
In-Reply-To: <CANQnQxQKw+Upv3v1A3u_ufLbWuE+cPpxdquJrP+8DKTGZs5VMw@mail.gmail.com>
References: <CANQnQxQKw+Upv3v1A3u_ufLbWuE+cPpxdquJrP+8DKTGZs5VMw@mail.gmail.com>
Message-ID: <55192110.1080803@treenet.co.nz>

On 29/03/2015 4:55 a.m., Ashish Patil wrote:
> Hello,
> 
> I am faced with a weird situation with an external acl that I have built.
> 
> The external acl gets the acl name and the IP information, does a lookup in
> a MySQL table if that IP belongs to a group ( read: acl name ) and returns
> an output.
> 
> The situation is as follows:
>  On the first client request coming to the external helper, it performs as
> expected, and the correct action is taken. Whereas from the second request
> onwards, Squid just waits on the external acl, even though a response was
> sent by the acl, and the same was received by Squid.
> 
> My acl's are as follows:
> external_acl_type grpname ttl=10 children-startup=1 concurrency=1 %SRC %ACL
> /usr/local/squid/libexec/grpname_helper
> acl two external grpname
> acl twoext urlpath_regex
> "/usr/local/squid/etc/custom/blacklisted-two-extensions"
> deny_info http://192.168.3.11/error.html two
> http_access deny twoext two
> 

> 
> To verify Squid was getting the responses from the external acl, I ran a
> strace on the squid process. Below is the trimmed output:
> 

> write(19, "0 192.168.3.243 two\n", 20)  = 20
> read(19, "OK\n\0", 4095)                = 4


Problem #1:
 Squid is using concurrency channels. The helper is not sending the
channel-ID field in the reply. Try setting concurrency=0 on squid.conf
to fix that - or better, update the helper to use concurrency properly.


Problem #2:
 The helper is sending a '\0' octet after the \n.
 The \n delimits the first and second response. So the \0 is left in the
buffer until the seond response is being handled.

When the second lookup happens its response is "\0OK" not the "OK" you
were expecting. Same thing also happens with all following lookups on
this helper.

Amos


From monahbaki at gmail.com  Thu Mar  5 18:00:15 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 05 Mar 2015 18:00:15 -0000
Subject: [squid-users] squid intercept config
In-Reply-To: <54F88E69.1050804@gmail.com>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F864BE.4010901@gmail.com>
 <CALP3=x-WVgwdjjNjG3ck1Wd=wBg-C7yUua7H6dHKMDAisC7UOQ@mail.gmail.com>
 <54F866FA.2030809@gmail.com>
 <CALP3=x_Gu0seOV38NBOkf56jXHiTo5PRALwd0U+HkiGhHVMyDQ@mail.gmail.com>
 <54F868B6.4000208@gmail.com>
 <CALP3=x9Eu67rroxWdc+oVpqYdTkQGw5z17uq0EKbxCD1Wij4FA@mail.gmail.com>
 <54F872CD.4030403@gmail.com>
 <CALP3=x_P8XDqTvykY2e-tVLh9zVj-oYCiNgDrUCf+2Kk2b=LwQ@mail.gmail.com>
 <54F8743D.8080105@gmail.com>
 <CALP3=x91kyV8DRetUfO+kGsQRVHCesMv9qimsvPZ-bZLKGSejQ@mail.gmail.com>
 <54F88087.5020907@gmail.com>
 <CALP3=x8ASWi=F6xem1ixmWcqxqHMG1iO8PhDsxaoh+1S8WKM2Q@mail.gmail.com>
 <54F88E69.1050804@gmail.com>
Message-ID: <CALP3=x_myJ3FUkPbcDTFbep6UjY3KGvwz+dKsaQ8ZSdPZPv0Ww@mail.gmail.com>

On 10.0.0.24

root at ISN-PHC-CACHE:/home/support # netstat -an
Active Internet connections (including servers)
Proto Recv-Q Send-Q Local Address          Foreign Address        (state)
tcp4       0     52 10.0.0.24.22           96.255.8.226.50911
ESTABLISHED
tcp4       0      0 *.3129                 *.*                    LISTEN
tcp4       0      0 *.3128                 *.*                    LISTEN
tcp4       0      0 *.81                   *.*                    LISTEN
tcp6       0      0 *.81                   *.*                    LISTEN
tcp4       0      0 *.22                   *.*                    LISTEN
tcp6       0      0 *.22                   *.*                    LISTEN
tcp6       0      0 ::1.562                ::1.40066
ESTABLISHED
tcp6       0      0 ::1.40066              ::1.562
ESTABLISHED
tcp6       0      0 *.561                  *.*                    LISTEN
tcp6       0      0 *.562                  *.*                    LISTEN
tcp4       0      0 *.199                  *.*                    LISTEN
tcp4       0      0 *.10000                *.*                    LISTEN
udp4       0      0 *.3401                 *.*
udp4       0      0 *.34985                *.*
udp4       0      0 *.*                    *.*
udp4       0      0 *.161                  *.*
udp4       0      0 *.162                  *.*
udp4       0      0 *.10000                *.*
udp4       0      0 127.0.0.1.123          *.*
udp6       0      0 fe80::1%lo0.123        *.*
udp6       0      0 ::1.123                *.*
udp4       0      0 10.0.0.24.123          *.*
udp6       0      0 *.123                  *.*
udp4       0      0 *.123                  *.*
udp4       0      0 *.514                  *.*
udp6       0      0 *.514                  *.*



On Thu, Mar 5, 2015 at 12:12 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> - From your PC run telnet 10.0.0.24 80. You've seen if TCP socket opens.
>
> 05.03.15 23:10, Monah Baki ?????:
> > How can I confirm, I have access only to the BSD box
> >
> > Thanks
> >
> > On Thu, Mar 5, 2015 at 11:12 AM, Yuri Voinov <yvoinov at gmail.com>
> > wrote:
> >
> > Does 80 port outside BSD-box listens?
> >
> > 05.03.15 21:25, Monah Baki ?????:
> >>>> root at ISN-PHC-CACHE:/cache/squid/bin # tcpdump -n -e -ttt -i
> >>>> pflog0 tcpdump: WARNING: pflog0: no IPv4 address assigned
> >>>> tcpdump: verbose output suppressed, use -v or -vv for full
> >>>> protocol decode listening on pflog0, link-type PFLOG (OpenBSD
> >>>> pflog file), capture size 65535 bytes capability mode sandbox
> >>>> enabled 00:00:00.000000 rule 0..16777216/0(match): pass in on
> >>>> bge0: 10.0.0.106.5678
> >>>>> 255.255.255.255.5678: UDP, length 88
> >>>> 00:00:08.342860 rule 0..16777216/0(match): pass in on bge0:
> >>>> 10.0.0.14.54264
> >>>>> 10.0.0.24.22: Flags [S], seq 3823043622, win 8192, options
> >>>>> [mss
> >>>> 1460,nop,wscale 2,nop,nop,sackOK], length 0
> >>>>
> >>>>
> >>>>
> >>>> On Thu, Mar 5, 2015 at 10:20 AM, Yuri Voinov
> >>>> <yvoinov at gmail.com> wrote:
> >>>>
> >>>> Hm. No.
> >>>>
> >>>> We not checked only OS.
> >>>>
> >>>> Does your BSD really loads PF module?
> >>>>
> >>>> 05.03.15 21:16, Monah Baki ?????:
> >>>>>>> Not sure why the client is running old hard/soft ware,
> >>>>>>> could it be cause of the hardware? Is FreeBSD an issue,
> >>>>>>> should I switch to linux?
> >>>>>>>
> >>>>>>> On Thu, Mar 5, 2015 at 10:14 AM, Yuri Voinov
> >>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>
> >>>>>>> Wow, 7600!
> >>>>>>>
> >>>>>>> But why is so antique iOS?! Current is 15.4
> >>>>>>>
> >>>>>>> 05.03.15 21:09, Monah Baki ?????:
> >>>>>>>>>> PORT   STATE SERVICE VERSION 23/tcp open  telnet
> >>>>>>>>>> Cisco IOS telnetd MAC Address: 88:5A:92:63:77:81
> >>>>>>>>>> (Cisco) Device type: router Running: Cisco IOS
> >>>>>>>>>> 12.X OS CPE: cpe:/h:cisco:7600_router
> >>>>>>>>>> cpe:/o:cisco:ios:12.2 OS details: Cisco 7600
> >>>>>>>>>> router (IOS 12.2) Network Distance: 1 hop TCP
> >>>>>>>>>> Sequence Prediction: Difficulty=258 (Good luck!)
> >>>>>>>>>> IP ID Sequence Generation: Randomized Service
> >>>>>>>>>> Info: OS: IOS; Device: switch; CPE:
> >>>>>>>>>> cpe:/o:cisco:ios
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On Thu, Mar 5, 2015 at 9:31 AM, Yuri Voinov
> >>>>>>>>>> <yvoinov at gmail.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> What is Cisco model and iOS version?
> >>>>>>>>>>
> >>>>>>>>>> 05.03.15 20:25, Monah Baki ?????:
> >>>>>>>>>>>>> Yes, correct
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:23 AM, Yuri
> >>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 10.0.0.23 is your host? And 10.0.0.24 is
> >>>>>>>>>>>>> proxy box?
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 05.03.15 20:15, Monah Baki ?????:
> >>>>>>>>>>>>>>>> '--prefix=/cache/squid'
> >>>>>>>>>>>>>>>> '--enable-follow-x-forwarded-for'
> >>>>>>>>>>>>>>>> '--with-large-files' '--enable-ssl'
> >>>>>>>>>>>>>>>> '--disable-ipv6' '--enable-esi'
> >>>>>>>>>>>>>>>> '--enable-kill-parent-hack'
> >>>>>>>>>>>>>>>> '--enable-snmp' '--with-pthreads'
> >>>>>>>>>>>>>>>> '--with-filedescriptors=65535'
> >>>>>>>>>>>>>>>> '--enable-cachemgr-hostname=hostname'
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> '--enable-storeio=ufs,aufs,diskd,rock'
> >>>>>>>>>>>>>>>> '--enable-ipfw-transparent'
> >>>>>>>>>>>>>>>> '--enable-pf-transparent'
> >>>>>>>>>>>>>>>> '--with-nat-devpf'
> >>>>>>>>>>>>>>>> --enable-ltdl-convenience
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> On Thu, Mar 5, 2015 at 9:14 AM, Yuri
> >>>>>>>>>>>>>>>> Voinov <yvoinov at gmail.com> wrote:
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> This looking good too.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Stupid question:
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> With witch interception option squid
> >>>>>>>>>>>>>>>> builed?
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> I.e, squid -v?
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> 05.03.15 18:19, Monah Baki ?????:
> >>>>>>>>>>>>>>>>>>> Hi all, can anyone verify if
> >>>>>>>>>>>>>>>>>>> this is correct, need to make
> >>>>>>>>>>>>>>>>>>> ure that users will be able to
> >>>>>>>>>>>>>>>>>>> access the internet via the
> >>>>>>>>>>>>>>>>>>> squid.
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> Running FreeBSD with a single
> >>>>>>>>>>>>>>>>>>> interface with Squid-3.5.2
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> Policy based routing on Cisco
> >>>>>>>>>>>>>>>>>>> with the following:
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> interface
> >>>>>>>>>>>>>>>>>>> GigabitEthernet0/0/1.1
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> encapsulation dot1Q 1 native
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> ip address 10.0.0.9
> >>>>>>>>>>>>>>>>>>> 255.255.255.0
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> no ip redirects
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> no ip unreachables
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> ip nat inside
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> standby 1 ip 10.0.0.10
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> standby 1 priority 120
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> standby 1 preempt
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> standby 1 name HSRP
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> ip policy route-map CFLOW
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> ip access-list extended
> >>>>>>>>>>>>>>>>>>> REDIRECT
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> deny   tcp host 10.0.0.24 any
> >>>>>>>>>>>>>>>>>>> eq www
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> permit tcp host 10.0.0.23 any
> >>>>>>>>>>>>>>>>>>> eq www
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> route-map CFLOW permit 10
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> match ip address REDIRECT set
> >>>>>>>>>>>>>>>>>>> ip next-hop 10.0.0.24
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> In my /etc/pf.conf rdr pass
> >>>>>>>>>>>>>>>>>>> inet proto tcp from 10.0.0.0/8
> >>>>>>>>>>>>>>>>>>> to any port 80 -> 10.0.0.24
> >>>>>>>>>>>>>>>>>>> port 3129
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> # block in pass in log quick on
> >>>>>>>>>>>>>>>>>>> bge0 pass out log quick on bge0
> >>>>>>>>>>>>>>>>>>> pass out keep state
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> and finally in my squid.conf:
> >>>>>>>>>>>>>>>>>>> http_port 3128 http_port 3129
> >>>>>>>>>>>>>>>>>>> intercept
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> And for testing purposes from
> >>>>>>>>>>>>>>>>>>> the squid server: ./squidclient
> >>>>>>>>>>>>>>>>>>> -h 10.0.0.24 -p 3128
> >>>>>>>>>>>>>>>>>>> http://www.freebsd.org/
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> If I replace -p 3128 with -p
> >>>>>>>>>>>>>>>>>>> 80, I get a access denied, and
> >>>>>>>>>>>>>>>>>>> if I omit the -p 3128
> >>>>>>>>>>>>>>>>>>> completely, I can access the
> >>>>>>>>>>>>>>>>>>> websites.
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> tcpdump with (-p 3128)
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> 13:15:02.681106 IP
> >>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017 >
> >>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http:
> >>>>>>>>>>>>>>>>>>> Flags [.], ack 17377, win 1018,
> >>>>>>>>>>>>>>>>>>> options [nop,nop,TS val
> >>>>>>>>>>>>>>>>>>> 985588797 ecr 1054387720],
> >>>>>>>>>>>>>>>>>>> length 0 13:15:02.681421 IP
> >>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.],
> >>>>>>>>>>>>>>>>>>> seq 17377:18825, ack 289, win
> >>>>>>>>>>>>>>>>>>> 1040, options [nop,nop,TS val
> >>>>>>>>>>>>>>>>>>> 1054387720 ecr 985588501],
> >>>>>>>>>>>>>>>>>>> length 1448 13:15:02.681575 IP
> >>>>>>>>>>>>>>>>>>> wfe0.ysv.freebsd.org.http >
> >>>>>>>>>>>>>>>>>>> ISN-PHC-CACHE.44017: Flags [.],
> >>>>>>>>>>>>>>>>>>> seq 18825:20273, ack 289, win
> >>>>>>>>>>>>>>>>>>> 1040, options [nop,nop,TS val
> >>>>>>>>>>>>>>>>>>> 1054387720 ecr 985588501],
> >>>>>>>>>>>>>>>>>>> length 1448
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> Did I miss anything?
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> Thanks Monah
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>> _______________________________________________
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>
> >>>>>>>>>>>>>>>>>>>
> >
> >>>>>>>>>>>>>>>>>>>
> squid-users mailing list
> >>>>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>>>>
> >>>>>>>>>>>>>>>>>>>
> >>>>
> >>>>>>>>>>>>>>>>>>>
> > _______________________________________________
> >>>>>>>>>>>>>>>>> squid-users mailing list
> >>>>>>>>>>>>>>>>> squid-users at lists.squid-cache.org
> >>>>>>>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>>
> >>
> >
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBAgAGBQJU+I5pAAoJENNXIZxhPexG68cIAIjP0CaVSgjlexyNc4xATrws
> SGS2hfPSOwPNJ32YVL43OJKtGLAMlumeo2oxJyZjdUFKftT5g7h8ldRKY3tPcV1m
> lYo+lbS4FOA0MspbK9oGP+xiZhvB8jnxmLxM13+Rnmf1309rYhhIyL7y7yih02i/
> KIulMyYwjZC8BvsCHRAjhxIiMQ0AJEO17hzs/6OCIkIjYNiswl9D8MbH0KOuKJMy
> 4BpYMpYnzDjfJAehwD7n1xnta3GvLhHbUL7fcYO9hvCX1mzZW/06nFpKQL/5pF06
> CVmtTAtg85JlFESBlA9JHbDOV3AGn0bOC5KFtIqX3osFz46Ac+HqOV8P0kXm85k=
> =eSQL
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150305/e6175a95/attachment.htm>

From monahbaki at gmail.com  Fri Mar  6 16:38:09 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Fri, 06 Mar 2015 16:38:09 -0000
Subject: [squid-users] Fwd: squid intercept config
In-Reply-To: <201503061457.37563.Antony.Stone@squid.open.source.it>
References: <CALP3=x-1-kAvFdx5a0QkRsa6jBtvMERXoDdEF-MT3ycXKa2rUw@mail.gmail.com>
 <54F9B00A.8050302@gmail.com>
 <CALP3=x94dGKUm29hzDDdHKYgUKTv6NoMZJ1BaKusrfL=FU8bjQ@mail.gmail.com>
 <201503061457.37563.Antony.Stone@squid.open.source.it>
Message-ID: <CALP3=x_kPCBy_EbRVe8xGpdjB1WtVg18XHSM_DLcdN2N+hvQ4w@mail.gmail.com>

Windows Client - 10.0.0.23 MAC (9d:3a:96)

root at ISN-PHC-CACHE:/home/support # arp -a
 (10.0.0.9) at 00:00:0c:07:ac:01 on bge0 THIS IS THE PHYSICAL INTERFACE ON
THE ROUTER
 (10.0.0.10) at 88:5a:92:63:77:81 on bge0  THIS IS THE GATEWAY IP ON THE
DESKTOP AND SQUID SERVER
 (10.0.0.24) at a0:d3:c1:06:a5:c4 on bge0 THIS IS THE SQUID SERVER


Frame 8 and 9 is where I get my access denied.

No.     Time        Source                Destination           Protocol
Length Info
      7 0.508041    68.71.212.158         10.0.0.23             TCP
3902   80?42794 [PSH, ACK] Seq=412 Ack=401 Win=65664 Len=1460

Frame 7: 3902 bytes on wire (31216 bits), 1500 bytes captured (12000 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453922000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453922000 seconds
    [Time delta from previous captured frame: 0.000118000 seconds]
    [Time delta from previous displayed frame: 0.000118000 seconds]
    [Time since reference or first frame: 0.508041000 seconds]
    Frame Number: 7
    Frame Length: 3902 bytes (31216 bits)
    Capture Length: 1500 bytes (12000 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst:
CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst:
10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 1500
    Identification: 0x2222 (8738)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794),
Seq: 412, Ack: 401, Len: 1460

No.     Time        Source                Destination           Protocol
Length Info
      8 0.508073    68.71.212.158         10.0.0.23             TCP
170    [TCP Previous segment not captured] [TCP segment of a reassembled
PDU]

Frame 8: 170 bytes on wire (1360 bits), 170 bytes captured (1360 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453954000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453954000 seconds
    [Time delta from previous captured frame: 0.000032000 seconds]
    [Time delta from previous displayed frame: 0.000032000 seconds]
    [Time since reference or first frame: 0.508073000 seconds]
    Frame Number: 8
    Frame Length: 170 bytes (1360 bits)
    Capture Length: 170 bytes (1360 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags &&
!tcp.analysis.window_update]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst:
CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst:
10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 156
    Identification: 0x2223 (8739)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794),
Seq: 4260, Ack: 401, Len: 116

No.     Time        Source                Destination           Protocol
Length Info
      9 0.508835    10.0.0.23             68.71.212.158         TCP
60     [TCP ACKed unseen segment] 42794?80 [ACK] Seq=401 Ack=3332 Win=65536
Len=0

Frame 9: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.454716000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.454716000 seconds
    [Time delta from previous captured frame: 0.000762000 seconds]
    [Time delta from previous displayed frame: 0.000762000 seconds]
    [Time since reference or first frame: 0.508835000 seconds]
    Frame Number: 9
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags &&
!tcp.analysis.window_update]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst:
HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158
(68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00:
Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x572a (22314)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a9 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80),
Seq: 401, Ack: 3332, Len: 0

On Fri, Mar 6, 2015 at 8:57 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Friday 06 March 2015 at 14:50:50 (EU time), Monah Baki wrote:
>
> > http://wiki.squid-cache.org/ConfigExamples/Intercept/FreeBsdPf
> >
> > So something else is missing?
>
> Can you run a packet sniffer on the proxy, to see what packets come in
> (noting
> the MAC address of the previous hop), what packets go out (to what
> address/es), and whether they then seem to come back in again (and if so,
> from
> which MAC address)?
>
> That might give you a clue as to where the forwarding loop is being
> created.
>
>
> Regards,
>
>
> Antony.
>
> --
> How I want a drink, alcoholic of course, after the heavy chapters involving
> quantum mechanics.
>
>  - mnemonic for 3.14159265358979
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150306/9550eaf1/attachment.htm>
-------------- next part --------------
No.     Time        Source                Destination           Protocol Length Info
      1 0.000000    10.0.0.23             68.71.212.158         TCP      66     42794?80 [SYN] Seq=0 Win=8192 Len=0 MSS=1460 WS=256 SACK_PERM=1

Frame 1: 66 bytes on wire (528 bits), 66 bytes captured (528 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:40.945881000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652900.945881000 seconds
    [Time delta from previous captured frame: 0.000000000 seconds]
    [Time delta from previous displayed frame: 0.000000000 seconds]
    [Time since reference or first frame: 0.000000000 seconds]
    Frame Number: 1
    Frame Length: 66 bytes (528 bits)
    Capture Length: 66 bytes (528 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 52
    Identification: 0x5725 (22309)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a2 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 0, Len: 0

No.     Time        Source                Destination           Protocol Length Info
      2 0.000033    68.71.212.158         10.0.0.23             TCP      66     80?42794 [SYN, ACK] Seq=0 Ack=1 Win=65535 Len=0 MSS=1460 WS=64 SACK_PERM=1

Frame 2: 66 bytes on wire (528 bits), 66 bytes captured (528 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:40.945914000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652900.945914000 seconds
    [Time delta from previous captured frame: 0.000033000 seconds]
    [Time delta from previous displayed frame: 0.000033000 seconds]
    [Time since reference or first frame: 0.000033000 seconds]
    Frame Number: 2
    Frame Length: 66 bytes (528 bits)
    Capture Length: 66 bytes (528 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 52
    Identification: 0x2214 (8724)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 0, Ack: 1, Len: 0

No.     Time        Source                Destination           Protocol Length Info
      3 0.000795    10.0.0.23             68.71.212.158         TCP      60     42794?80 [ACK] Seq=1 Ack=1 Win=65536 Len=0

Frame 3: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:40.946676000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652900.946676000 seconds
    [Time delta from previous captured frame: 0.000762000 seconds]
    [Time delta from previous displayed frame: 0.000762000 seconds]
    [Time since reference or first frame: 0.000795000 seconds]
    Frame Number: 3
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x5726 (22310)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81ad [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 1, Ack: 1, Len: 0

No.     Time        Source                Destination           Protocol Length Info
      4 0.337285    10.0.0.23             68.71.212.158         HTTP     454    GET / HTTP/1.1 

Frame 4: 454 bytes on wire (3632 bits), 454 bytes captured (3632 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.283166000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.283166000 seconds
    [Time delta from previous captured frame: 0.336490000 seconds]
    [Time delta from previous displayed frame: 0.336490000 seconds]
    [Time since reference or first frame: 0.337285000 seconds]
    Frame Number: 4
    Frame Length: 454 bytes (3632 bits)
    Capture Length: 454 bytes (3632 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Number of per-protocol-data: 1]
    [Hypertext Transfer Protocol, key 0]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 440
    Identification: 0x5729 (22313)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x801a [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 1, Ack: 1, Len: 400
Hypertext Transfer Protocol

No.     Time        Source                Destination           Protocol Length Info
      5 0.443213    68.71.212.158         10.0.0.23             TCP      54     80?42794 [ACK] Seq=1 Ack=401 Win=65664 Len=0

Frame 5: 54 bytes on wire (432 bits), 54 bytes captured (432 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.389094000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.389094000 seconds
    [Time delta from previous captured frame: 0.105928000 seconds]
    [Time delta from previous displayed frame: 0.105928000 seconds]
    [Time since reference or first frame: 0.443213000 seconds]
    Frame Number: 5
    Frame Length: 54 bytes (432 bits)
    Capture Length: 54 bytes (432 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x2219 (8729)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 1, Ack: 401, Len: 0

No.     Time        Source                Destination           Protocol Length Info
      6 0.507923    68.71.212.158         10.0.0.23             TCP      465    [TCP segment of a reassembled PDU]

Frame 6: 465 bytes on wire (3720 bits), 465 bytes captured (3720 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453804000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453804000 seconds
    [Time delta from previous captured frame: 0.064710000 seconds]
    [Time delta from previous displayed frame: 0.064710000 seconds]
    [Time since reference or first frame: 0.507923000 seconds]
    Frame Number: 6
    Frame Length: 465 bytes (3720 bits)
    Capture Length: 465 bytes (3720 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 451
    Identification: 0x2221 (8737)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 1, Ack: 401, Len: 411

No.     Time        Source                Destination           Protocol Length Info
      7 0.508041    68.71.212.158         10.0.0.23             TCP      3902   80?42794 [PSH, ACK] Seq=412 Ack=401 Win=65664 Len=1460

Frame 7: 3902 bytes on wire (31216 bits), 1500 bytes captured (12000 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453922000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453922000 seconds
    [Time delta from previous captured frame: 0.000118000 seconds]
    [Time delta from previous displayed frame: 0.000118000 seconds]
    [Time since reference or first frame: 0.508041000 seconds]
    Frame Number: 7
    Frame Length: 3902 bytes (31216 bits)
    Capture Length: 1500 bytes (12000 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 1500
    Identification: 0x2222 (8738)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 412, Ack: 401, Len: 1460

No.     Time        Source                Destination           Protocol Length Info
      8 0.508073    68.71.212.158         10.0.0.23             TCP      170    [TCP Previous segment not captured] [TCP segment of a reassembled PDU]

Frame 8: 170 bytes on wire (1360 bits), 170 bytes captured (1360 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.453954000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.453954000 seconds
    [Time delta from previous captured frame: 0.000032000 seconds]
    [Time delta from previous displayed frame: 0.000032000 seconds]
    [Time since reference or first frame: 0.508073000 seconds]
    Frame Number: 8
    Frame Length: 170 bytes (1360 bits)
    Capture Length: 170 bytes (1360 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags && !tcp.analysis.window_update]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 156
    Identification: 0x2223 (8739)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 4260, Ack: 401, Len: 116

No.     Time        Source                Destination           Protocol Length Info
      9 0.508835    10.0.0.23             68.71.212.158         TCP      60     [TCP ACKed unseen segment] 42794?80 [ACK] Seq=401 Ack=3332 Win=65536 Len=0

Frame 9: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.454716000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.454716000 seconds
    [Time delta from previous captured frame: 0.000762000 seconds]
    [Time delta from previous displayed frame: 0.000762000 seconds]
    [Time since reference or first frame: 0.508835000 seconds]
    Frame Number: 9
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags && !tcp.analysis.window_update]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x572a (22314)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a9 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 401, Ack: 3332, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     10 0.509139    10.0.0.23             68.71.212.158         TCP      60     42794?80 [ACK] Seq=401 Ack=4376 Win=64512 Len=0

Frame 10: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.455020000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.455020000 seconds
    [Time delta from previous captured frame: 0.000304000 seconds]
    [Time delta from previous displayed frame: 0.000304000 seconds]
    [Time since reference or first frame: 0.509139000 seconds]
    Frame Number: 10
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x572b (22315)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a8 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 401, Ack: 4376, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     11 1.002814    10.0.0.23             68.71.212.158         HTTP     394    GET /favicon.ico HTTP/1.1 

Frame 11: 394 bytes on wire (3152 bits), 394 bytes captured (3152 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.948695000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.948695000 seconds
    [Time delta from previous captured frame: 0.493675000 seconds]
    [Time delta from previous displayed frame: 0.493675000 seconds]
    [Time since reference or first frame: 1.002814000 seconds]
    Frame Number: 11
    Frame Length: 394 bytes (3152 bits)
    Capture Length: 394 bytes (3152 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Number of per-protocol-data: 1]
    [Hypertext Transfer Protocol, key 0]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 380
    Identification: 0x572c (22316)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x8053 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 401, Ack: 4376, Len: 340
Hypertext Transfer Protocol

No.     Time        Source                Destination           Protocol Length Info
     12 1.004457    68.71.212.158         10.0.0.23             TCP      465    [TCP segment of a reassembled PDU]

Frame 12: 465 bytes on wire (3720 bits), 465 bytes captured (3720 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.950338000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.950338000 seconds
    [Time delta from previous captured frame: 0.001643000 seconds]
    [Time delta from previous displayed frame: 0.001643000 seconds]
    [Time since reference or first frame: 1.004457000 seconds]
    Frame Number: 12
    Frame Length: 465 bytes (3720 bits)
    Capture Length: 465 bytes (3720 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 451
    Identification: 0x222f (8751)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 4376, Ack: 741, Len: 411

No.     Time        Source                Destination           Protocol Length Info
     13 1.004557    68.71.212.158         10.0.0.23             TCP      3902   80?42794 [PSH, ACK] Seq=4787 Ack=741 Win=65664 Len=1460

Frame 13: 3902 bytes on wire (31216 bits), 1500 bytes captured (12000 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.950438000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.950438000 seconds
    [Time delta from previous captured frame: 0.000100000 seconds]
    [Time delta from previous displayed frame: 0.000100000 seconds]
    [Time since reference or first frame: 1.004557000 seconds]
    Frame Number: 13
    Frame Length: 3902 bytes (31216 bits)
    Capture Length: 1500 bytes (12000 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp:http]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 1500
    Identification: 0x2230 (8752)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 4787, Ack: 741, Len: 1460

No.     Time        Source                Destination           Protocol Length Info
     14 1.004588    68.71.212.158         10.0.0.23             TCP      116    [TCP Previous segment not captured] [TCP segment of a reassembled PDU]

Frame 14: 116 bytes on wire (928 bits), 116 bytes captured (928 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.950469000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.950469000 seconds
    [Time delta from previous captured frame: 0.000031000 seconds]
    [Time delta from previous displayed frame: 0.000031000 seconds]
    [Time since reference or first frame: 1.004588000 seconds]
    Frame Number: 14
    Frame Length: 116 bytes (928 bits)
    Capture Length: 116 bytes (928 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags && !tcp.analysis.window_update]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 102
    Identification: 0x2231 (8753)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 8635, Ack: 741, Len: 62

No.     Time        Source                Destination           Protocol Length Info
     15 1.008653    10.0.0.23             68.71.212.158         TCP      60     [TCP ACKed unseen segment] 42794?80 [ACK] Seq=741 Ack=8697 Win=65536 Len=0

Frame 15: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:41:41.954534000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425652901.954534000 seconds
    [Time delta from previous captured frame: 0.004065000 seconds]
    [Time delta from previous displayed frame: 0.004065000 seconds]
    [Time since reference or first frame: 1.008653000 seconds]
    Frame Number: 15
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: Bad TCP]
    [Coloring Rule String: tcp.analysis.flags && !tcp.analysis.window_update]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x572d (22317)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a6 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 741, Ack: 8697, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     16 120.557393  68.71.212.158         10.0.0.23             TCP      54     80?42794 [FIN, ACK] Seq=8697 Ack=741 Win=65664 Len=0

Frame 16: 54 bytes on wire (432 bits), 54 bytes captured (432 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:43:41.503274000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425653021.503274000 seconds
    [Time delta from previous captured frame: 119.548740000 seconds]
    [Time delta from previous displayed frame: 119.548740000 seconds]
    [Time since reference or first frame: 120.557393000 seconds]
    Frame Number: 16
    Frame Length: 54 bytes (432 bits)
    Capture Length: 54 bytes (432 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x24c6 (9414)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 8697, Ack: 741, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     17 120.557857  10.0.0.23             68.71.212.158         TCP      60     42794?80 [ACK] Seq=741 Ack=8698 Win=65536 Len=0

Frame 17: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:43:41.503738000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425653021.503738000 seconds
    [Time delta from previous captured frame: 0.000464000 seconds]
    [Time delta from previous displayed frame: 0.000464000 seconds]
    [Time since reference or first frame: 120.557857000 seconds]
    Frame Number: 17
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x5730 (22320)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a3 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 741, Ack: 8698, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     18 125.054542  10.0.0.23             68.71.212.158         TCP      60     42794?80 [FIN, ACK] Seq=741 Ack=8698 Win=65536 Len=0

Frame 18: 60 bytes on wire (480 bits), 60 bytes captured (480 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:43:46.000423000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425653026.000423000 seconds
    [Time delta from previous captured frame: 4.496685000 seconds]
    [Time delta from previous displayed frame: 4.496685000 seconds]
    [Time since reference or first frame: 125.054542000 seconds]
    Frame Number: 18
    Frame Length: 60 bytes (480 bits)
    Capture Length: 60 bytes (480 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: Cisco_63:77:81 (88:5a:92:63:77:81), Dst: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Destination: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Source: Cisco_63:77:81 (88:5a:92:63:77:81)
    Type: IP (0x0800)
    Padding: aaaa0000aaaa
Internet Protocol Version 4, Src: 10.0.0.23 (10.0.0.23), Dst: 68.71.212.158 (68.71.212.158)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x5731 (22321)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 127
    Protocol: TCP (6)
    Header checksum: 0x81a2 [validation disabled]
    Source: 10.0.0.23 (10.0.0.23)
    Destination: 68.71.212.158 (68.71.212.158)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 42794 (42794), Dst Port: 80 (80), Seq: 741, Ack: 8698, Len: 0

No.     Time        Source                Destination           Protocol Length Info
     19 125.054560  68.71.212.158         10.0.0.23             TCP      54     80?42794 [ACK] Seq=8698 Ack=742 Win=65664 Len=0

Frame 19: 54 bytes on wire (432 bits), 54 bytes captured (432 bits)
    Encapsulation type: Ethernet (1)
    Arrival Time: Mar  6, 2015 09:43:46.000441000 Eastern Standard Time
    [Time shift for this packet: 0.000000000 seconds]
    Epoch Time: 1425653026.000441000 seconds
    [Time delta from previous captured frame: 0.000018000 seconds]
    [Time delta from previous displayed frame: 0.000018000 seconds]
    [Time since reference or first frame: 125.054560000 seconds]
    Frame Number: 19
    Frame Length: 54 bytes (432 bits)
    Capture Length: 54 bytes (432 bits)
    [Frame is marked: False]
    [Frame is ignored: False]
    [Protocols in frame: eth:ethertype:ip:tcp]
    [Coloring Rule Name: HTTP]
    [Coloring Rule String: http || tcp.port == 80 || http2]
Ethernet II, Src: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4), Dst: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Destination: CompalIn_9d:3a:96 (20:89:84:9d:3a:96)
    Source: HewlettP_06:a5:c4 (a0:d3:c1:06:a5:c4)
    Type: IP (0x0800)
Internet Protocol Version 4, Src: 68.71.212.158 (68.71.212.158), Dst: 10.0.0.23 (10.0.0.23)
    Version: 4
    Header Length: 20 bytes
    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))
    Total Length: 40
    Identification: 0x252e (9518)
    Flags: 0x02 (Don't Fragment)
    Fragment offset: 0
    Time to live: 64
    Protocol: TCP (6)
    Header checksum: 0x0000 [validation disabled]
    Source: 68.71.212.158 (68.71.212.158)
    Destination: 10.0.0.23 (10.0.0.23)
    [Source GeoIP: Unknown]
    [Destination GeoIP: Unknown]
Transmission Control Protocol, Src Port: 80 (80), Dst Port: 42794 (42794), Seq: 8698, Ack: 742, Len: 0

From priya_agarwal at students.iitmandi.ac.in  Mon Mar 16 05:36:18 2015
From: priya_agarwal at students.iitmandi.ac.in (Priya Agarwal)
Date: Mon, 16 Mar 2015 11:06:18 +0530
Subject: [squid-users] Editing Makefile.am to include static libraries
In-Reply-To: <5502D129.7030809@treenet.co.nz>
References: <CALTPfpGmakXPMwfSXb8KzqkBVo4yhg4bzgjtdiJmE9CEt81nFw@mail.gmail.com>
 <5502B3EE.9040908@treenet.co.nz>
 <CALTPfpHz7JCgRScCc+zexpY2Ws=XwxUUUKaiqCqf5Hotj6EStQ@mail.gmail.com>
 <5502D129.7030809@treenet.co.nz>
Message-ID: <CALTPfpFK2LVNa4GFY3J25bk9SJgL=gSygzzuw42UB+FpaUT_sA@mail.gmail.com>

Hi,

I am now linking the libraries during ./configure with LDFLAGS, LIBS and
CXXFLAGS options (Makefile.am is same as it was)  Compile is failing
presently.

main.o: In function `of_init':
/media/NewVolume/yocto/build_t4240qds_release/tmp/sysroots/t4240qds/usr/include/usdpaa/of.h:52:
undefined reference to `of_init_path(char const*)'
collect2: error: ld returned 1 exit status
powerpc-fsl_networking-linux-libtool: link: rm -f ".libs/squidS.o"

Does it mean I haven't linked the the library that has of_init() [But I
think I have done that as ./configure passed and libraries are even visible
in log]
Or I need to link something else?

Here are the flags passed to ./configure:
EXTRA_OECONF_append = "    LDFLAGS="-L=/usr/lib/" \
            LIBS="-lusdpaa_dma -lusdpaa_dma_mem -lusdpaa_of -lusdpaa_fman
-lusdpaa_qbman -lusdpaa_syscfg" \
            CXXFLAGS="-I=/usr/include/""

Attached the logfile too.



On Fri, Mar 13, 2015 at 5:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/03/2015 12:28 a.m., Priya Agarwal wrote:
> > I tried what you advised. Getting the same error for both methods
> > (./configure LDFLAGS=-L<../tmp/../lib CXXFLAGS=-I<.../tmp../include or
> > editing Makefile.am appropriately). autoreconf is failing.
>
> I see "<" characters in your paths. That is invalid. As is the -I paths
> segments "..." and "tmp.." looks like you are missing '/' somewhere.
>
>
> > And also I am getting many such warnings:
> >
> > | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> > (or '*_CPPFLAGS')
> > | compat/Makefile.am:5:   'src/Common.am' included from here
> > | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> > (or '*_CPPFLAGS')
> > | helpers/basic_auth/DB/Makefile.am:1:   'src/Common.am' included from
> here
> > | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> > (or '*_CPPFLAGS')
> > | helpers/basic_auth/LDAP/Makefile.am:1:   'src/Common.am' included from
> > here
> > | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> > (or '*_CPPFLAGS')
> > | helpers/basic_auth/MSNT-multi-domain/Makefile.am:1:   'src/Common.am'
> > included from here
> > | src/Common.am:16: warning: 'INCLUDES' is the old name for 'AM_CPPFLAGS'
> > (or '*_CPPFLAGS')
> >
>
> Those are just warnings because you are working with an old Squid
> version and autotools have changed their requirements since. The current
> release dont have quite so many warnings (some remain). Those can be
> ignore.
>
> I does mean that what I wrote as AM_CPPFLAGS needs to instead be written
> as INCLUDES in your Squid versions Makefile.am.
>
>
> > Final error:
> > | autoreconf: automake failed with exit status: 1
> > | ERROR: autoreconf execution failed.
> >
> > So is something wrong with the path?
>
> I see "<" characters in what you
>
> >
> > I have attached the logfile as well which shows the detailed output.
> >
>
> Buried in the warnings I see this:
>
> src/Makefile.am:661: error: '#' comment at start of rule is unportable
>
>
> automake syntax has two forms of comment.
>  ## comments are autoreconf comments and ignored
>  # comments are copied through as-is to the final Makefile
>
> If you are using multi-line wrapped lists of things, that can cause
> issues. Its easier to just never use comments inside the wrapped lines.
>
>
> Other things to watch out for auth makefiles:
>
> * indentation for rules needs to be one tab, not spaces. This needs
> checking after each copy-paste you do.
>
> * multi-line rules and lists use '\' character ending to explicitly
> define the wrapping.
>   Be careful that lists of libraries etc use them on each line up to,
> but not on, the final line of the list.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150316/398baa91/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: log.do_compile.10426
Type: application/octet-stream
Size: 58361 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150316/398baa91/attachment.obj>

From sam at idsdoc.com  Tue Mar 17 19:49:08 2015
From: sam at idsdoc.com (Samuel Anderson)
Date: Tue, 17 Mar 2015 13:49:08 -0600
Subject: [squid-users] Refresh ACL list only
In-Reply-To: <1426620740.9328.23.camel@desktop.bpk2.com>
References: <CAP6yRXgk4M++yGz5Oft2_QRaT7+ZF9LJpGwtk2c6ruK5LatFzg@mail.gmail.com>
 <550866E5.5090309@gmail.com>
 <CAP6yRXgXZCQnRRB9coXto8E-pvzOAJ8Bs0sHbh4WGpLf6KmuYQ@mail.gmail.com>
 <55087CDA.3010209@urlfilterdb.com>
 <1426620740.9328.23.camel@desktop.bpk2.com>
Message-ID: <CAP6yRXh03iMCazy6NALY3YAnXQq8Q0KZ8hvVd_Si4_wThk=z0A@mail.gmail.com>

This is my config file. It takes about 30 seconds to reload when using the
command (sudo squid3 -k reconfigure)



http_port 3128
visible_hostname squid.######.local
error_directory /etc/squid3/errors/en

# Recommended minimum configuration:
#
#acl manager proto cache_object
#acl localhost src 127.0.0.1/32 ::1
#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/22 # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS


#Kerberos and NTLM authentication
auth_param negotiate program /usr/local/bin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=####### --kerberos /usr/lib/squid3/negotiate_kerberos_auth -d -s
GSS_C_NO_NAME
auth_param negotiate children 30
auth_param negotiate keep_alive off

# LDAP authentication
auth_param basic program /usr/lib/squid3/basic_ldap_auth -R -b
"DC=#####,DC=local" -D "CN=SQUID,OU=##### Service
Accounts,DC=#####,DC=local" -w "#########" -f sAMAccountName=%s -h
###################
auth_param basic children 150
auth_param basic realm Please enter your Domain credentials to continue
auth_param basic credentialsttl 1 hour

# AD group membership commands
external_acl_type ldap_group ttl=60 children-startup=10 children-max=50
children-idle=2 %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -K -S -b
"DC=######,DC=local" -D "CN=SQUID,OU=Service Accounts,DC=#####,DC=local" -w
"#########" -f "(&(objectclass=person)
(sAMAccountname=%v)(memberof=CN=%a,OU=PROXY,ou=ALL
Groups,DC=#####,DC=local))" -h ######################


#########################################################################

acl auth proxy_auth REQUIRED

##### Individual Allow Groups LDAP #####

acl ALLOW-ABORTION external ldap_group INTERNET-ALLOW-ABORTION
acl ALLOW-ANTISPYWARE external ldap_group INTERNET-ALLOW-ANTISPYWARE
acl ALLOW-AUDIO-VIDEO external ldap_group INTERNET-ALLOW-AUDIO-VIDEO
acl ALLOW-BLOG external ldap_group INTERNET-ALLOW-BLOG
acl ALLOW-CELLPHONES external ldap_group INTERNET-ALLOW-CELLPHONES
acl ALLOW-CHAT external ldap_group INTERNET-ALLOW-CHAT
acl ALLOW-CHILDCARE external ldap_group INTERNET-ALLOW-CHILDCARE
acl ALLOW-CLEANING external ldap_group INTERNET-ALLOW-CLEANING
acl ALLOW-CLOTHING external ldap_group INTERNET-ALLOW-CLOTHING
acl ALLOW-CONTRACEPTION external ldap_group INTERNET-ALLOW-CONTRACEPTION
acl ALLOW-CULINARY external ldap_group INTERNET-ALLOW-CULINARY
acl ALLOW-DATING external ldap_group INTERNET-ALLOW-DATING
acl ALLOW-DRUGS external ldap_group INTERNET-ALLOW-DRUGS
acl ALLOW-ECOMMERCE external ldap_group INTERNET-ALLOW-ECOMMERCE
acl ALLOW-ENTERTAINMENT external ldap_group INTERNET-ALLOW-ENTERTAINMENT
acl ALLOW-FILEHOSTING external ldap_group INTERNET-ALLOW-FILEHOSTING
acl ALLOW-FRENCHEDUCATION external ldap_group INTERNET-ALLOW-FRENCHEDUCATION
acl ALLOW-GAMES external ldap_group INTERNET-ALLOW-GAMES
acl ALLOW-GARDENING external ldap_group INTERNET-ALLOW-GARDENING
acl ALLOW-GUNS external ldap_group INTERNET-ALLOW-GUNS
acl ALLOW-HACKING external ldap_group INTERNET-ALLOW-HACKING
acl ALLOW-HOMEREPAIR external ldap_group INTERNET-ALLOW-HOMEREPAIR
acl ALLOW-HYGIENE external ldap_group INTERNET-ALLOW-HYGIENE
acl ALLOW-INSTANTMESSAGING external ldap_group
INTERNET-ALLOW-INSTANTMESSAGING
acl ALLOW-JEWELRY external ldap_group INTERNET-ALLOW-JEWELRY
acl ALLOW-JOBSEARCH external ldap_group INTERNET-ALLOW-JOBSEARCH
acl ALLOW-MARKETINGWARE external ldap_group INTERNET-ALLOW-MARKETINGWARE
acl ALLOW-MEDICAL external ldap_group INTERNET-ALLOW-MEDICAL
acl ALLOW-MOBILE-PHONE external ldap_group INTERNET-ALLOW-MOBILE-PHONE
acl ALLOW-NEWS external ldap_group INTERNET-ALLOW-NEWS
acl ALLOW-ONLINEAUCTIONS external ldap_group INTERNET-ALLOW-ONLINEAUCTIONS
acl ALLOW-ONLINEGAMES external ldap_group INTERNET-ALLOW-ONLINEGAMES
acl ALLOW-ONLINEPAYMENT external ldap_group INTERNET-ALLOW-ONLINEPAYMENT
acl ALLOW-PERSONALFINANCE external ldap_group INTERNET-ALLOW-PERSONALFINANCE
acl ALLOW-PETS external ldap_group INTERNET-ALLOW-PETS
acl ALLOW-RADIO external ldap_group INTERNET-ALLOW-RADIO
acl ALLOW-RELIGION external ldap_group INTERNET-ALLOW-RELIGION
acl ALLOW-SECT external ldap_group INTERNET-ALLOW-SECT
acl ALLOW-SEXUALITYEDUCATION external ldap_group
INTERNET-ALLOW-SEXUALITYEDUCATION
acl ALLOW-SHOPPING external ldap_group INTERNET-ALLOW-SHOPPING
acl ALLOW-SOCIALNETWORKING external ldap_group
INTERNET-ALLOW-SOCIALNETWORKING
acl ALLOW-SPORTNEWS external ldap_group INTERNET-ALLOW-SPORTNEWS
acl ALLOW-SPORTS external ldap_group INTERNET-ALLOW-SPORTS
acl ALLOW-VACATION external ldap_group INTERNET-ALLOW-VACATION
acl ALLOW-VIOLENCE external ldap_group INTERNET-ALLOW-VIOLENCE

##### Block Groups LDAP #####

acl HIGHLY-RESTRICTIVE external ldap_group PROXY-HIGHLY-RESTRICTIVE
acl MEDIUM-RESTRICTIVE external ldap_group PROXY-MEDIUM-RESTRICTIVE
acl MINIMAL-RESTRICTIVE external ldap_group PROXY-MINIMAL-RESTRICTIVE
acl UNRESTRICTED external ldap_group PROXY-UNRESTRICTED
acl DEV external ldap_group PROXY-DEV
acl SALES external ldap_group PROXY-SALES
acl REQGROUPS external ldap_group PROXY-HIGHLY-RESTRICTIVE
PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE PROXY-UNRESTRICTED
PROXY-DEV PROXY-SALES

##### Blacklist Paths #####

acl ABORTION dstdomain "/etc/squid3/blacklists/abortion/domains
acl ADS dstdomain "/etc/squid3/blacklists/ads/domains"
acl ADULT dstdomain "/etc/squid3/blacklists/adult/domains"
acl AGGRESSIVE dstdomain "/etc/squid3/blacklists/aggressive/domains
acl ALCOHOL dstdomain "/etc/squid3/blacklists/alcohol/domains
acl ANTISPYWARE dstdomain "/etc/squid3/blacklists/antispyware/domains
acl ARTNUDES dstdomain "/etc/squid3/blacklists/artnudes/domains
acl ASTROLOGY dstdomain "/etc/squid3/blacklists/astrology/domains
acl AUDIO-VIDEO dstdomain "/etc/squid3/blacklists/audio-video/domains"
acl BANKING dstdomain "/etc/squid3/blacklists/banking/domains
acl BEERLIQUORINFO dstdomain "/etc/squid3/blacklists/beerliquorinfo/domains
acl BEERLIQUORSALE dstdomain "/etc/squid3/blacklists/beerliquorsale/domains
acl BITCOIN dstdomain "/etc/squid3/blacklists/bitcoin/domains
acl BLOG dstdomain "/etc/squid3/blacklists/blog/domains
acl BOOKS dstdomain "/etc/squid3/blacklists/books/domains
acl CELEBRITY dstdomain "/etc/squid3/blacklists/celebrity/domains
acl CELLPHONES dstdomain "/etc/squid3/blacklists/cellphones/domains
acl CHAT dstdomain "/etc/squid3/blacklists/chat/domains
acl CHILDCARE dstdomain "/etc/squid3/blacklists/childcare/domains
acl CLEANING dstdomain "/etc/squid3/blacklists/cleaning/domains
acl CLOTHING dstdomain "/etc/squid3/blacklists/clothing/domains
acl CONTRACEPTION dstdomain "/etc/squid3/blacklists/contraception/domains
acl CULINARY dstdomain "/etc/squid3/blacklists/culinary/domains
acl DATING dstdomain "/etc/squid3/blacklists/dating/domains
acl DESKTOPSILLIES dstdomain "/etc/squid3/blacklists/desktopsillies/domains
acl DIALERS dstdomain "/etc/squid3/blacklists/dialers/domains
acl DRUGS dstdomain "/etc/squid3/blacklists/drugs/domains
acl ECOMMERCE dstdomain "/etc/squid3/blacklists/ecommerce/domains
acl ENTERTAINMENT dstdomain "/etc/squid3/blacklists/entertainment/domains
acl FILEHOSTING dstdomain "/etc/squid3/blacklists/filehosting/domains
acl FILESHARING dstdomain "/etc/squid3/blacklists/filesharing/domains
acl FRENCHEDUCATION dstdomain
"/etc/squid3/blacklists/frencheducation/domains
acl GAMBLING dstdomain "/etc/squid3/blacklists/gambling/domains"
acl GAMES dstdomain "/etc/squid3/blacklists/games/domains"
acl GARDENING dstdomain "/etc/squid3/blacklists/gardening/domains"
acl GOVERNMENT dstdomain "/etc/squid3/blacklists/government/domains"
acl GUNS dstdomain "/etc/squid3/blacklists/guns/domains"
acl HACKING dstdomain "/etc/squid3/blacklists/hacking/domains"
acl HOMEREPAIR dstdomain "/etc/squid3/blacklists/homerepair/domains"
acl HUMOR dstdomain "/etc/squid3/blacklists/humor/domains
acl HUNTING dstdomain "/etc/squid3/blacklists/hunting/domains
acl HYGIENE dstdomain "/etc/squid3/blacklists/hygiene/domains"
acl INSTANTMESSAGING dstdomain
"/etc/squid3/blacklists/instantmessaging/domains"
acl JEWELRY dstdomain "/etc/squid3/blacklists/jewelry/domains"
acl JOBSEARCH dstdomain "/etc/squid3/blacklists/jobsearch/domains"
acl KIDSTIMEWASTING dstdomain
"/etc/squid3/blacklists/kidstimewasting/domains"
acl LINGERIE dstdomain "/etc/squid3/blacklists/lingerie/domains
acl MAGAZINES dstdomain "/etc/squid3/blacklists/magazines/domains
acl MALWARE dstdomain "/etc/squid3/blacklists/malware/domains
acl MAIL dstdomain "/etc/squid3/blacklists/mail/domains"
acl MARKETINGWARE dstdomain "/etc/squid3/blacklists/marketingware/domains"
acl MEDICAL dstdomain "/etc/squid3/blacklists/medical/domains"
acl MIXED_ADULT dstdomain "/etc/squid3/blacklists/mixed_adult/domains"
acl MOBILE-PHONE dstdomain "/etc/squid3/blacklists/mobile-phone/domains"
acl NATURISM dstdomain "/etc/squid3/blacklists/naturism/domains"
acl NEWS dstdomain "/etc/squid3/blacklists/news/domains"
acl ONLINEAUCTIONS dstdomain "/etc/squid3/blacklists/onlineauctions/domains"
acl ONLINEGAMES dstdomain "/etc/squid3/blacklists/onlinegames/domains"
acl ONLINEPAYMENT dstdomain "/etc/squid3/blacklists/onlinepayment/domains"
acl PERSONALFINANCE dstdomain
"/etc/squid3/blacklists/personalfinance/domains"
acl PETS dstdomain "/etc/squid3/blacklists/pets/domains"
acl PHISHING dstdomain "/etc/squid3/blacklists/phishing/domains"
acl PORN dstdomain "/etc/squid3/blacklists/porn/domains"
acl PRESS dstdomain "/etc/squid3/blacklists/press/domains
acl PROXY dstdomain "/etc/squid3/blacklists/proxy/domains"
acl RADIO dstdomain "/etc/squid3/blacklists/radio/domains"
acl RELIGION dstdomain "/etc/squid3/blacklists/religion/domains"
acl REMOTE-CONTROL dstdomain "/etc/squid3/blacklists/remote-control/domains
acl RINGTONES dstdomain "/etc/squid3/blacklists/ringtones/domains"
acl SEARCHENGINES dstdomain "/etc/squid3/blacklists/searchengines/domains"
acl SECT dstdomain "/etc/squid3/blacklists/sect/domains"
acl SEXUALITY dstdomain "/etc/squid3/blacklists/sexuality/domains"
acl SEXUALITYEDUCATION dstdomain
"/etc/squid3/blacklists/sexualityeducation/domains"
acl SHOPPING dstdomain "/etc/squid3/blacklists/shopping/domains"
acl SOCIAL_NETWORKS dstdomain
"/etc/squid3/blacklists/social_networks/domains
acl SOCIALNETWORKING dstdomain
"/etc/squid3/blacklists/socialnetworking/domains"
acl SPORTNEWS dstdomain "/etc/squid3/blacklists/sportnews/domains"
acl SPORTS dstdomain "/etc/squid3/blacklists/sports/domains"
acl SPYWARE dstdomain "/etc/squid3/blacklists/spyware/domains"
acl TOBACCO dstdomain "/etc/squid3/blacklists/tobacco/domains
acl UPDATESITES dstdomain "/etc/squid3/blacklists/updatesites/domains"
acl VACATION dstdomain "/etc/squid3/blacklists/vacation/domains"
acl VIOLENCE dstdomain "/etc/squid3/blacklists/violence/domains"
acl VIRUSINFECTED dstdomain "/etc/squid3/blacklists/virusinfected/domains"
acl WAREZ dstdomain "/etc/squid3/blacklists/warez/domains"
acl WEATHER dstdomain "/etc/squid3/blacklists/weather/domains"
acl WEAPONS dstdomain "/etc/squid3/blacklists/weapons/domains"
acl WEBMAIL dstdomain "/etc/squid3/blacklists/webmail/domains"

##### Whitelist Paths #####

acl GLOBAL-WHITELIST dstdomain "/etc/squid3/whitelists/GLOBAL-WHITELIST"
acl UNRESTRICTED-WHITELIST dstdomain
"/etc/squid3/whitelists/UNRESTRICTED-WHITELIST"
acl DEV-WHITELIST dstdomain "/etc/squid3/whitelists/DEV-WHITELIST"
acl SALES-WHITELIST dstdomain "/etc/squid3/whitelists/SALES-WHITELIST"

############################################################################################

##### HTTP_ACCESS Rules #####

# Block everyone who is not a member of one of (PROXY-HIGHLY-RESTRICTIVE
PROXY-MEDIUM-RESTRICTIVE PROXY-MINIMAL-RESTRICTIVE PROXY-UNRESTRICTED
PROXY-DEV PROXY-SALES)
http_access deny !auth all
http_access deny !REQGROUPS all


# Allow all traffic to everyone to anything in (GLOBAL-WHITELIST)
http_access allow GLOBAL-WHITELIST all

# Allow categories LDAP

http_access allow ALLOW-ABORTION ABORTION
http_access allow ALLOW-ANTISPYWARE ANTISPYWARE
http_access allow ALLOW-AUDIO-VIDEO AUDIO-VIDEO
http_access allow ALLOW-BLOG BLOG
http_access allow ALLOW-CELLPHONES CELLPHONES
http_access allow ALLOW-CHAT CHAT
http_access allow ALLOW-CHILDCARE CHILDCARE
http_access allow ALLOW-CLEANING CLEANING
http_access allow ALLOW-CLOTHING CLOTHING
http_access allow ALLOW-CONTRACEPTION CONTRACEPTION
http_access allow ALLOW-CULINARY CULINARY
http_access allow ALLOW-DATING DATING
http_access allow ALLOW-DRUGS DRUGS
http_access allow ALLOW-ECOMMERCE ECOMMERCE
http_access allow ALLOW-ENTERTAINMENT ENTERTAINMENT
http_access allow ALLOW-FILEHOSTING FILEHOSTING
http_access allow ALLOW-FRENCHEDUCATION FRENCHEDUCATION
http_access allow ALLOW-GAMES GAMES
http_access allow ALLOW-GARDENING GARDENING
http_access allow ALLOW-GUNS GUNS
http_access allow ALLOW-HACKING HACKING
http_access allow ALLOW-HOMEREPAIR HOMEREPAIR
http_access allow ALLOW-HYGIENE HYGIENE
http_access allow ALLOW-INSTANTMESSAGING INSTANTMESSAGING
http_access allow ALLOW-JEWELRY JEWELRY
http_access allow ALLOW-JOBSEARCH JOBSEARCH
http_access allow ALLOW-MARKETINGWARE MARKETINGWARE
http_access allow ALLOW-MEDICAL MEDICAL
http_access allow ALLOW-MOBILE-PHONE MOBILE-PHONE
http_access allow ALLOW-NEWS NEWS
http_access allow ALLOW-ONLINEAUCTIONS ONLINEAUCTIONS
http_access allow ALLOW-ONLINEGAMES ONLINEGAMES
http_access allow ALLOW-ONLINEPAYMENT ONLINEPAYMENT
http_access allow ALLOW-PERSONALFINANCE PERSONALFINANCE
http_access allow ALLOW-PETS PETS
http_access allow ALLOW-RADIO RADIO
http_access allow ALLOW-RELIGION RELIGION
http_access allow ALLOW-SECT SECT
http_access allow ALLOW-SEXUALITYEDUCATION SEXUALITYEDUCATION
http_access allow ALLOW-SHOPPING SHOPPING
http_access allow ALLOW-SOCIALNETWORKING SOCIALNETWORKING
http_access allow ALLOW-SPORTNEWS SPORTNEWS
http_access allow ALLOW-SPORTS SPORTS
http_access allow ALLOW-VACATION VACATION
http_access allow ALLOW-VIOLENCE VIOLENCE


#### DEV ####

http_access allow DEV DEV-WHITELIST

http_access deny DEV ADULT
http_access deny DEV AGGRESSIVE
http_access deny DEV ALCOHOL
http_access deny DEV ARTNUDES
http_access deny DEV ASTROLOGY
http_access deny DEV BEERLIQUORINFO
http_access deny DEV BEERLIQUORSALE
http_access deny DEV DESKTOPSILLIES
http_access deny DEV DIALERS
http_access deny DEV GAMBLING
http_access deny DEV KIDSTIMEWASTING
http_access deny DEV LINGERIE
http_access deny DEV MALWARE
http_access deny DEV MIXED_ADULT
http_access deny DEV NATURISM
http_access deny DEV PHISHING
http_access deny DEV PORN
http_access deny DEV PROXY
http_access deny DEV RINGTONES
http_access deny DEV SEXUALITY
http_access deny DEV SOCIAL_NETWORKS
http_access deny DEV SOCIALNETWORKING
http_access deny DEV SPYWARE
http_access deny DEV TOBACCO
http_access deny DEV VIRUSINFECTED
http_access deny DEV WAREZ
http_access deny DEV WEAPONS


#### SALES ####

http_access allow SALES SALES-WHITELIST

http_access deny SALES ADULT
http_access deny SALES AGGRESSIVE
http_access deny SALES ALCOHOL
http_access deny SALES ARTNUDES
http_access deny SALES ASTROLOGY
http_access deny SALES BEERLIQUORINFO
http_access deny SALES BEERLIQUORSALE
http_access deny SALES DESKTOPSILLIES
http_access deny SALES DIALERS
http_access deny SALES GAMBLING
http_access deny SALES KIDSTIMEWASTING
http_access deny SALES LINGERIE
http_access deny SALES MALWARE
http_access deny SALES MIXED_ADULT
http_access deny SALES NATURISM
http_access deny SALES PHISHING
http_access deny SALES PORN
http_access deny SALES PROXY
http_access deny SALES RINGTONES
http_access deny SALES SEXUALITY
http_access deny SALES SPYWARE
http_access deny SALES TOBACCO
http_access deny SALES VIRUSINFECTED
http_access deny SALES WAREZ
http_access deny SALES WEAPONS


#### UNRESTRICTED ####

http_access allow UNRESTRICTED UNRESTRICTED-WHITELIST

http_access deny UNRESTRICTED ADULT
http_access deny UNRESTRICTED AGGRESSIVE
http_access deny UNRESTRICTED ALCOHOL
http_access deny UNRESTRICTED ARTNUDES
http_access deny UNRESTRICTED ASTROLOGY
http_access deny UNRESTRICTED BEERLIQUORINFO
http_access deny UNRESTRICTED BEERLIQUORSALE
http_access deny UNRESTRICTED DESKTOPSILLIES
http_access deny UNRESTRICTED DIALERS
http_access deny UNRESTRICTED GAMBLING
http_access deny UNRESTRICTED KIDSTIMEWASTING
http_access deny UNRESTRICTED LINGERIE
http_access deny UNRESTRICTED MALWARE
http_access deny UNRESTRICTED MIXED_ADULT
http_access deny UNRESTRICTED NATURISM
http_access deny UNRESTRICTED PHISHING
http_access deny UNRESTRICTED PORN
http_access deny UNRESTRICTED PROXY
http_access deny UNRESTRICTED RINGTONES
http_access deny UNRESTRICTED SEXUALITY
http_access deny UNRESTRICTED SPYWARE
http_access deny UNRESTRICTED TOBACCO
http_access deny UNRESTRICTED VIRUSINFECTED
http_access deny UNRESTRICTED WAREZ
http_access deny UNRESTRICTED WEAPONS


#### MINIMAL-RESTRICTIVE ####

http_access deny MINIMAL-RESTRICTIVE ADS
http_access deny MINIMAL-RESTRICTIVE ADULT
http_access deny MINIMAL-RESTRICTIVE AGGRESSIVE
http_access deny MINIMAL-RESTRICTIVE ALCOHOL
http_access deny MINIMAL-RESTRICTIVE ARTNUDES
http_access deny MINIMAL-RESTRICTIVE ASTROLOGY
http_access deny MINIMAL-RESTRICTIVE BEERLIQUORINFO
http_access deny MINIMAL-RESTRICTIVE BEERLIQUORSALE
http_access deny MINIMAL-RESTRICTIVE CELEBRITY
http_access deny MINIMAL-RESTRICTIVE DESKTOPSILLIES
http_access deny MINIMAL-RESTRICTIVE DIALERS
http_access deny MINIMAL-RESTRICTIVE DRUGS
http_access deny MINIMAL-RESTRICTIVE ENTERTAINMENT
http_access deny MINIMAL-RESTRICTIVE FILESHARING
http_access deny MINIMAL-RESTRICTIVE GAMBLING
http_access deny MINIMAL-RESTRICTIVE GAMES
http_access deny MINIMAL-RESTRICTIVE GUNS
http_access deny MINIMAL-RESTRICTIVE HUMOR
http_access deny MINIMAL-RESTRICTIVE HUNTING
http_access deny MINIMAL-RESTRICTIVE INSTANTMESSAGING
http_access deny MINIMAL-RESTRICTIVE KIDSTIMEWASTING
http_access deny MINIMAL-RESTRICTIVE LINGERIE
http_access deny MINIMAL-RESTRICTIVE MAGAZINES
http_access deny MINIMAL-RESTRICTIVE MALWARE
http_access deny MINIMAL-RESTRICTIVE MIXED_ADULT
http_access deny MINIMAL-RESTRICTIVE NATURISM
http_access deny MINIMAL-RESTRICTIVE ONLINEAUCTIONS
http_access deny MINIMAL-RESTRICTIVE ONLINEGAMES
http_access deny MINIMAL-RESTRICTIVE PHISHING
http_access deny MINIMAL-RESTRICTIVE PORN
http_access deny MINIMAL-RESTRICTIVE PROXY
http_access deny MINIMAL-RESTRICTIVE RINGTONES
http_access deny MINIMAL-RESTRICTIVE SEXUALITY
http_access deny MINIMAL-RESTRICTIVE SEXUALITYEDUCATION
http_access deny MINIMAL-RESTRICTIVE SPYWARE
http_access deny MINIMAL-RESTRICTIVE TOBACCO
http_access deny MINIMAL-RESTRICTIVE VIOLENCE
http_access deny MINIMAL-RESTRICTIVE VIRUSINFECTED
http_access deny MINIMAL-RESTRICTIVE WAREZ
http_access deny MINIMAL-RESTRICTIVE WEAPONS


#### MEDIUM-RESTRICTIVE ####

http_access deny MEDIUM-RESTRICTIVE ABORTION
http_access deny MEDIUM-RESTRICTIVE ADS
http_access deny MEDIUM-RESTRICTIVE ADULT
http_access deny MEDIUM-RESTRICTIVE AGGRESSIVE
http_access deny MEDIUM-RESTRICTIVE ALCOHOL
http_access deny MEDIUM-RESTRICTIVE ARTNUDES
http_access deny MEDIUM-RESTRICTIVE ASTROLOGY
http_access deny MEDIUM-RESTRICTIVE AUDIO-VIDEO
http_access deny MEDIUM-RESTRICTIVE BEERLIQUORINFO
http_access deny MEDIUM-RESTRICTIVE BEERLIQUORSALE
http_access deny MEDIUM-RESTRICTIVE BITCOIN
http_access deny MEDIUM-RESTRICTIVE CELEBRITY
http_access deny MEDIUM-RESTRICTIVE CHAT
http_access deny MEDIUM-RESTRICTIVE CONTRACEPTION
http_access deny MEDIUM-RESTRICTIVE DATING
http_access deny MEDIUM-RESTRICTIVE DESKTOPSILLIES
http_access deny MEDIUM-RESTRICTIVE DIALERS
http_access deny MEDIUM-RESTRICTIVE DRUGS
http_access deny MEDIUM-RESTRICTIVE ECOMMERCE
http_access deny MEDIUM-RESTRICTIVE ENTERTAINMENT
http_access deny MEDIUM-RESTRICTIVE FILEHOSTING
http_access deny MEDIUM-RESTRICTIVE FILESHARING
http_access deny MEDIUM-RESTRICTIVE FRENCHEDUCATION
http_access deny MEDIUM-RESTRICTIVE GAMBLING
http_access deny MEDIUM-RESTRICTIVE GAMES
http_access deny MEDIUM-RESTRICTIVE GARDENING
http_access deny MEDIUM-RESTRICTIVE GUNS
http_access deny MEDIUM-RESTRICTIVE HACKING
http_access deny MEDIUM-RESTRICTIVE HOMEREPAIR
http_access deny MEDIUM-RESTRICTIVE HUMOR
http_access deny MEDIUM-RESTRICTIVE HUNTING
http_access deny MEDIUM-RESTRICTIVE HYGIENE
http_access deny MEDIUM-RESTRICTIVE INSTANTMESSAGING
http_access deny MEDIUM-RESTRICTIVE JEWELRY
http_access deny MEDIUM-RESTRICTIVE JOBSEARCH
http_access deny MEDIUM-RESTRICTIVE KIDSTIMEWASTING
http_access deny MEDIUM-RESTRICTIVE LINGERIE
http_access deny MEDIUM-RESTRICTIVE MAGAZINES
http_access deny MEDIUM-RESTRICTIVE MALWARE
http_access deny MEDIUM-RESTRICTIVE MARKETINGWARE
http_access deny MEDIUM-RESTRICTIVE MEDICAL
http_access deny MEDIUM-RESTRICTIVE MIXED_ADULT
http_access deny MEDIUM-RESTRICTIVE MOBILE-PHONE
http_access deny MEDIUM-RESTRICTIVE NATURISM
http_access deny MEDIUM-RESTRICTIVE NEWS
http_access deny MEDIUM-RESTRICTIVE ONLINEAUCTIONS
http_access deny MEDIUM-RESTRICTIVE ONLINEGAMES
http_access deny MEDIUM-RESTRICTIVE PHISHING
http_access deny MEDIUM-RESTRICTIVE PORN
http_access deny MEDIUM-RESTRICTIVE PRESS
http_access deny MEDIUM-RESTRICTIVE PROXY
http_access deny MEDIUM-RESTRICTIVE RINGTONES
http_access deny MEDIUM-RESTRICTIVE SECT
http_access deny MEDIUM-RESTRICTIVE SEXUALITY
http_access deny MEDIUM-RESTRICTIVE SEXUALITYEDUCATION
http_access deny MEDIUM-RESTRICTIVE SHOPPING
http_access deny MEDIUM-RESTRICTIVE SOCIAL_NETWORKS
http_access deny MEDIUM-RESTRICTIVE SOCIALNETWORKING
http_access deny MEDIUM-RESTRICTIVE SPORTNEWS
http_access deny MEDIUM-RESTRICTIVE SPORTS
http_access deny MEDIUM-RESTRICTIVE SPYWARE
http_access deny MEDIUM-RESTRICTIVE TOBACCO
http_access deny MEDIUM-RESTRICTIVE VACATION
http_access deny MEDIUM-RESTRICTIVE VIOLENCE
http_access deny MEDIUM-RESTRICTIVE VIRUSINFECTED
http_access deny MEDIUM-RESTRICTIVE WAREZ
http_access deny MEDIUM-RESTRICTIVE WEAPONS


#### HIGHLY-RESTRICTIVE ####

http_access deny HIGHLY-RESTRICTIVE ABORTION
http_access deny HIGHLY-RESTRICTIVE ADS
http_access deny HIGHLY-RESTRICTIVE ADULT
http_access deny HIGHLY-RESTRICTIVE AGGRESSIVE
http_access deny HIGHLY-RESTRICTIVE ALCOHOL
http_access deny HIGHLY-RESTRICTIVE ANTISPYWARE
http_access deny HIGHLY-RESTRICTIVE ARTNUDES
http_access deny HIGHLY-RESTRICTIVE ASTROLOGY
http_access deny HIGHLY-RESTRICTIVE AUDIO-VIDEO
http_access deny HIGHLY-RESTRICTIVE BEERLIQUORINFO
http_access deny HIGHLY-RESTRICTIVE BEERLIQUORSALE
http_access deny HIGHLY-RESTRICTIVE BITCOIN
http_access deny HIGHLY-RESTRICTIVE BLOG
http_access deny HIGHLY-RESTRICTIVE BOOKS
http_access deny HIGHLY-RESTRICTIVE CELEBRITY
http_access deny HIGHLY-RESTRICTIVE CELLPHONES
http_access deny HIGHLY-RESTRICTIVE CHAT
http_access deny HIGHLY-RESTRICTIVE CHILDCARE
http_access deny HIGHLY-RESTRICTIVE CLEANING
http_access deny HIGHLY-RESTRICTIVE CLOTHING
http_access deny HIGHLY-RESTRICTIVE CONTRACEPTION
http_access deny HIGHLY-RESTRICTIVE CULINARY
http_access deny HIGHLY-RESTRICTIVE DATING
http_access deny HIGHLY-RESTRICTIVE DESKTOPSILLIES
http_access deny HIGHLY-RESTRICTIVE DIALERS
http_access deny HIGHLY-RESTRICTIVE DRUGS
http_access deny HIGHLY-RESTRICTIVE ECOMMERCE
http_access deny HIGHLY-RESTRICTIVE ENTERTAINMENT
http_access deny HIGHLY-RESTRICTIVE FILEHOSTING
http_access deny HIGHLY-RESTRICTIVE FILESHARING
http_access deny HIGHLY-RESTRICTIVE FRENCHEDUCATION
http_access deny HIGHLY-RESTRICTIVE GAMBLING
http_access deny HIGHLY-RESTRICTIVE GAMES
http_access deny HIGHLY-RESTRICTIVE GARDENING
http_access deny HIGHLY-RESTRICTIVE GUNS
http_access deny HIGHLY-RESTRICTIVE HACKING
http_access deny HIGHLY-RESTRICTIVE HOMEREPAIR
http_access deny HIGHLY-RESTRICTIVE HUMOR
http_access deny HIGHLY-RESTRICTIVE HUNTING
http_access deny HIGHLY-RESTRICTIVE HYGIENE
http_access deny HIGHLY-RESTRICTIVE INSTANTMESSAGING
http_access deny HIGHLY-RESTRICTIVE JEWELRY
http_access deny HIGHLY-RESTRICTIVE JOBSEARCH
http_access deny HIGHLY-RESTRICTIVE KIDSTIMEWASTING
http_access deny HIGHLY-RESTRICTIVE LINGERIE
http_access deny HIGHLY-RESTRICTIVE MAGAZINES
http_access deny HIGHLY-RESTRICTIVE MALWARE
http_access deny HIGHLY-RESTRICTIVE MARKETINGWARE
http_access deny HIGHLY-RESTRICTIVE MEDICAL
http_access deny HIGHLY-RESTRICTIVE MIXED_ADULT
http_access deny HIGHLY-RESTRICTIVE MOBILE-PHONE
http_access deny HIGHLY-RESTRICTIVE NATURISM
http_access deny HIGHLY-RESTRICTIVE NEWS
http_access deny HIGHLY-RESTRICTIVE ONLINEAUCTIONS
http_access deny HIGHLY-RESTRICTIVE ONLINEGAMES
http_access deny HIGHLY-RESTRICTIVE ONLINEPAYMENT
http_access deny HIGHLY-RESTRICTIVE PERSONALFINANCE
http_access deny HIGHLY-RESTRICTIVE PETS
http_access deny HIGHLY-RESTRICTIVE PHISHING
http_access deny HIGHLY-RESTRICTIVE PORN
http_access deny HIGHLY-RESTRICTIVE PRESS
http_access deny HIGHLY-RESTRICTIVE PROXY
http_access deny HIGHLY-RESTRICTIVE RADIO
http_access deny HIGHLY-RESTRICTIVE RELIGION
http_access deny HIGHLY-RESTRICTIVE RINGTONES
http_access deny HIGHLY-RESTRICTIVE SECT
http_access deny HIGHLY-RESTRICTIVE SEXUALITY
http_access deny HIGHLY-RESTRICTIVE SEXUALITYEDUCATION
http_access deny HIGHLY-RESTRICTIVE SHOPPING
http_access deny HIGHLY-RESTRICTIVE SOCIAL_NETWORKS
http_access deny HIGHLY-RESTRICTIVE SOCIALNETWORKING
http_access deny HIGHLY-RESTRICTIVE SPORTNEWS
http_access deny HIGHLY-RESTRICTIVE SPORTS
http_access deny HIGHLY-RESTRICTIVE SPYWARE
http_access deny HIGHLY-RESTRICTIVE TOBACCO
http_access deny HIGHLY-RESTRICTIVE VACATION
http_access deny HIGHLY-RESTRICTIVE VIOLENCE
http_access deny HIGHLY-RESTRICTIVE VIRUSINFECTED
http_access deny HIGHLY-RESTRICTIVE WAREZ
http_access deny HIGHLY-RESTRICTIVE WEAPONS

# Allow All

http_access allow all

#########################################################################


#### DENY PAGES ####

deny_info ERR_ACCESS_DENIED_ABORTION ABORTION
deny_info ERR_ACCESS_DENIED_ADS ADS
deny_info ERR_ACCESS_DENIED_ADULT ADULT
deny_info ERR_ACCESS_DENIED_AGGRESSIVE AGGRESSIVE
deny_info ERR_ACCESS_DENIED_ALCOHOL ALCOHOL
deny_info ERR_ACCESS_DENIED_ANTISPYWARE ANTISPYWARE
deny_info ERR_ACCESS_DENIED_ARTNUDES ARTNUDES
deny_info ERR_ACCESS_DENIED_ASTROLOGY ASTROLOGY
deny_info ERR_ACCESS_DENIED_AUDIO-VIDEO AUDIO-VIDEO
deny_info ERR_ACCESS_DENIED_BANKING BANKING
deny_info ERR_ACCESS_DENIED_BEERLIQUORINFO BEERLIQUORINFO
deny_info ERR_ACCESS_DENIED_BEERLIQUORSALE BEERLIQUORSALE
deny_info ERR_ACCESS_DENIED_BITCOIN BITCOIN
deny_info ERR_ACCESS_DENIED_BLOG BLOG
deny_info ERR_ACCESS_DENIED_BOOKS BOOKS
deny_info ERR_ACCESS_DENIED_CELEBRITY CELEBRITY
deny_info ERR_ACCESS_DENIED_CELLPHONES CELLPHONES
deny_info ERR_ACCESS_DENIED_CHAT CHAT
deny_info ERR_ACCESS_DENIED_CHILDCARE CHILDCARE
deny_info ERR_ACCESS_DENIED_CLEANING CLEANING
deny_info ERR_ACCESS_DENIED_CLOTHING CLOTHING
deny_info ERR_ACCESS_DENIED_CONTRACEPTION CONTRACEPTION
deny_info ERR_ACCESS_DENIED_CULINARY CULINARY
deny_info ERR_ACCESS_DENIED_DATING DATING
deny_info ERR_ACCESS_DENIED_DESKTOPSILLIES DESKTOPSILLIES
deny_info ERR_ACCESS_DENIED_DIALERS DIALERS
deny_info ERR_ACCESS_DENIED_DRUGS DRUGS
deny_info ERR_ACCESS_DENIED_ECOMMERCE ECOMMERCE
deny_info ERR_ACCESS_DENIED_ENTERTAINMENT ENTERTAINMENT
deny_info ERR_ACCESS_DENIED_FILEHOSTING FILEHOSTING
deny_info ERR_ACCESS_DENIED_FILESHARING FILESHARING
deny_info ERR_ACCESS_DENIED_FRENCHEDUCATION FRENCHEDUCATION
deny_info ERR_ACCESS_DENIED_GAMBLING GAMBLING
deny_info ERR_ACCESS_DENIED_GAMES GAMES
deny_info ERR_ACCESS_DENIED_GARDENING GARDENING
deny_info ERR_ACCESS_DENIED_GOVERNMENT GOVERNMENT
deny_info ERR_ACCESS_DENIED_GUNS GUNS
deny_info ERR_ACCESS_DENIED_HACKING HACKING
deny_info ERR_ACCESS_DENIED_HOMEREPAIR HOMEREPAIR
deny_info ERR_ACCESS_DENIED_HUMOR HUMOR
deny_info ERR_ACCESS_DENIED_HUNTING HUNTING
deny_info ERR_ACCESS_DENIED_HYGIENE HYGIENE
deny_info ERR_ACCESS_DENIED_INSTANTMESSAGING INSTANTMESSAGING
deny_info ERR_ACCESS_DENIED_JEWELRY JEWELRY
deny_info ERR_ACCESS_DENIED_JOBSEARCH JOBSEARCH
deny_info ERR_ACCESS_DENIED_KIDSTIMEWASTING KIDSTIMEWASTING
deny_info ERR_ACCESS_DENIED_LINGERIE LINGERIE
deny_info ERR_ACCESS_DENIED_MAGAZINES MAGAZINES
deny_info ERR_ACCESS_DENIED_MALWARE MALWARE
deny_info ERR_ACCESS_DENIED_MAIL MAIL
deny_info ERR_ACCESS_DENIED_MARKETINGWARE MARKETINGWARE
deny_info ERR_ACCESS_DENIED_MEDICAL MEDICAL
deny_info ERR_ACCESS_DENIED_MIXED_ADULT MIXED_ADULT
deny_info ERR_ACCESS_DENIED_MOBILE-PHONE MOBILE-PHONE
deny_info ERR_ACCESS_DENIED_NATURISM NATURISM
deny_info ERR_ACCESS_DENIED_NEWS NEWS
deny_info ERR_ACCESS_DENIED_ONLINEAUCTIONS ONLINEAUCTIONS
deny_info ERR_ACCESS_DENIED_ONLINEGAMES ONLINEGAMES
deny_info ERR_ACCESS_DENIED_ONLINEPAYMENT ONLINEPAYMENT
deny_info ERR_ACCESS_DENIED_PERSONALFINANCE PERSONALFINANCE
deny_info ERR_ACCESS_DENIED_PETS PETS
deny_info ERR_ACCESS_DENIED_PHISHING PHISHING
deny_info ERR_ACCESS_DENIED_PORN PORN
deny_info ERR_ACCESS_DENIED_PRESS PRESS
deny_info ERR_ACCESS_DENIED_PROXY PROXY
deny_info ERR_ACCESS_DENIED_RADIO RADIO
deny_info ERR_ACCESS_DENIED_RELIGION RELIGION
deny_info ERR_ACCESS_DENIED_RINGTONES RINGTONES
deny_info ERR_ACCESS_DENIED_SEARCHENGINE SEARCHENGINE
deny_info ERR_ACCESS_DENIED_SECT SECT
deny_info ERR_ACCESS_DENIED_SEXUALITY SEXUALITY
deny_info ERR_ACCESS_DENIED_SEXUALITYEDUCATION SEXUALITYEDUCATION
deny_info ERR_ACCESS_DENIED_SHOPPING SHOPPING
deny_info ERR_ACCESS_DENIED_SOCIAL_NETWORKS SOCIAL_NETWORKS
deny_info ERR_ACCESS_DENIED_SOCIALNETWORKING SOCIALNETWORKING
deny_info ERR_ACCESS_DENIED_SPORTNEWS SPORTNEWS
deny_info ERR_ACCESS_DENIED_SPORTS SPORTS
deny_info ERR_ACCESS_DENIED_SPYWARE SPYWARE
deny_info ERR_ACCESS_DENIED_TOBACCO TOBACCO
deny_info ERR_ACCESS_DENIED_UPDATESITES UPDATESITES
deny_info ERR_ACCESS_DENIED_VACATION VACATION
deny_info ERR_ACCESS_DENIED_VIOLENCE VIOLENCE
deny_info ERR_ACCESS_DENIED_VIRUSINFECTED VIRUSINFECTED
deny_info ERR_ACCESS_DENIED_WAREZ WAREZ
deny_info ERR_ACCESS_DENIED_WEATHER WEATHER
deny_info ERR_ACCESS_DENIED_WEAPONS WEAPONS
deny_info ERR_ACCESS_DENIED_WEBMAIL WEBMAIL

#########################################################################

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256
cache deny all

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320




On Tue, Mar 17, 2015 at 1:32 PM, Brendan Kearney <bpk678 at gmail.com> wrote:

> On Tue, 2015-03-17 at 16:13 -0300, Marcus Kool wrote:
> > it has a configuration option to respond with
> > 'allow all' during a reconfiguration.
>
> a Fail-Open policy can be a security gap, and should be considered
> carefully before implementing.  the intention of the whitelisted URLs is
> to prevent access to content that is otherwise forbidden.  failing open,
> even briefly, undermines that control.  what is the default setting
> there?
>
>


-- 
Samuel Anderson  |  Information Technology Administrator  |  International
Document Services

IDS  |  11629 South 700 East, Suite 200  |  Draper, UT 84020-4607

-- 
CONFIDENTIALITY NOTICE:
This e-mail and any attachments are confidential. If you are not an 
intended recipient, please contact the sender to report the error and 
delete all copies of this message from your system.  Any unauthorized 
review, use, disclosure or distribution is prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150317/ba8b93ea/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 20 07:27:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Mar 2015 20:27:45 +1300
Subject: [squid-users] [squid-announce] OpenSSL Advisory 2015-03-19
Message-ID: <550BCBF1.10105@treenet.co.nz>

As you may be aware a number of security vulnerabilities have just been
announced regarding OpenSSL.
 <https://openssl.org/news/secadv_20150319.txt>

Several of these potentially impact Squid with Denial of Service and
connection failure side effects when using HTTPS or the SSL-Bump feature
set.

All users of Squid HTTPS and SSL features are advised to restart Squid
after upgrading their OpenSSL library to a fixed version.


There will be no direct Squid advisory regarding this since the
vulnerability is in OpenSSL itself, not Squid.


Amos Jeffries
Squid Software Foundation
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From stan.prescott at gmail.com  Sun Mar 29 15:15:18 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Sun, 29 Mar 2015 11:15:18 -0400
Subject: [squid-users] Squid 3.5.2 ssl_crtd kids causing abnormal
	termination of startup
Message-ID: <CANLNtGQWYObuYo1piJNyE92Js48aQGkdHMsQ7Ec3Wnb+XWB-9w@mail.gmail.com>

I'm still pulling my hair out trying to figure out why Squid 3.5.2 with SSL
caching enabled will only start after the /var/spool/squid/cache is
emptied. This is the debug info I am getting when starting Squid when the
cache is not emptied.

*2015/03/29 10:27:56.896| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.900| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.900| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.900| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.900| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56 kid1| Current Directory is /*
*2015/03/29 10:27:56 kid1| Creating missing swap directories*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache exists*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/00 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/00*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/01 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/01*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/02 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/02*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/03 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/03*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/04 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/04*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/05 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/05*
*2015/03/29 10:27:56.928| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.928| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.929| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/06 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/06*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/07 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/07*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/08 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/08*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/09 exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/09*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0A exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0A*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0B exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0B*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0C exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0C*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0D exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0D*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0E exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0E*
*2015/03/29 10:27:56 kid1| /var/spool/squid/cache/0F exists*
*2015/03/29 10:27:56 kid1| Making directories in /var/spool/squid/cache/0F*
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.954 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.955 kid1| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56.956| Acl.cc(380) ~ACL: freeing ACL *
*2015/03/29 10:27:56 kid1| Current Directory is /*
*2015/03/29 10:27:56 kid1| Starting Squid Cache version 3.5.2 for
i586-pc-linux-gnu...*
*2015/03/29 10:27:56 kid1| Service Name: squid*
*2015/03/29 10:27:56 kid1| Process ID 1349*
*2015/03/29 10:27:56 kid1| Process Roles: worker*
*2015/03/29 10:27:56 kid1| With 1024 file descriptors available*
*2015/03/29 10:27:56 kid1| Initializing IP Cache...*
*2015/03/29 10:27:56 kid1| DNS Socket created at 0.0.0.0, FD 8*
*2015/03/29 10:27:56 kid1| Adding nameserver 127.0.0.1 from
/etc/resolv.conf*
*2015/03/29 10:27:56 kid1| helperOpenServers: Starting 5/5 'ssl_crtd'
processes*
*FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*

*Squid Cache (Version 3.5.2): Terminated abnormally.*
*CPU Usage: 0.010 seconds = 0.003 user + 0.007 sys*
*Maximum Resident Size: 26720 KB*
*Page faults with physical i/o: 0*


This snippet of the debug messages is repeated 5 times, one for each of the
five ssl_crtd processes (kids?) that are trying to start.

I have patched the "src/main.cc" file using a modified version of the patch
file found here <
http://www.squid-cache.org/Versions/v4/changesets/squid-4-13984.patch>.
However, the patch file at this link no longer matches the main.cc file for
3.5.2 so I had to make my best guess what that patch file should be.

*--- squid-3.5.2/src/main.cc.orig 2015-03-19 20:27:40.092678987 -0500*
*+++ squid-3.5.2/src/main.cc 2015-03-19 15:42:26.000000000 -0500*
*@@ -871,6 +871,7 @@*
*     }*

*     writePidFile();     /* write PID file */*
*+    enter_suid(); // writePidFile() uses leave_suid()*

*     reconfiguring = 0;*
* }*
*@@ -1123,11 +1124,19 @@*

*     if (!configured_once)*
*         writePidFile();     /* write PID file */*
*+ enter_suid(); // writePidFile() uses leave_suid()*

* #if defined(_SQUID_LINUX_THREADS_)*

*     squid_signal(SIGQUIT, rotate_logs, SA_RESTART);*

*+    removePidFile();*
*+    enter_suid(); // removePidFile() uses leave_suid()*
*+    if (TheKids.someSignaled(SIGINT) || TheKids.someSignaled(SIGTERM)) {*
*+         syslog(LOG_ALERT, "Exiting due to unexpected forced shutdown");*
*+         exit(1);*
*+    }*
*+*
*     squid_signal(SIGTRAP, sigusr2_handle, SA_RESTART);*

* #else*


The patch did nothing to fix the issue I am having with the errors I am
getting above with the five ssl_crtd kids failing to start giving that
error message :

*FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory*

Here are my squid 3.5.2 compile options

*CONFIG_OPTS += --enable-storeio="diskd,ufs,aufs" --enable-linux-netfilter
\*
*      --enable-removal-policies="heap,lru" --enable-delay-pools
--libdir=/usr/lib/ \*
*      --localstatedir=/var --with-dl --with-openssl
--enable-http-violations \*
*      --with-large-files --with-libcap --disable-ipv6
--with-swapdir=/var/spool/squid \*
*      --enable-ssl-crtd --enable-follow-x-forwarded-for*


And here is my squid.conf file with SSL caching enabled

*visible_hostname smoothwall*

*# Uncomment the following to send debug info to /var/log/squid/cache.log*
*debug_options ALL,1 33,2 28,9*

*# ACCESS CONTROLS*
*# ----------------------------------------------------------------*
*acl localhostgreen src 192.168.100.1*
*acl localnetgreen src 192.168.100.0/24 <http://192.168.100.0/24>*

*acl SSL_ports port 445 443 441 563*
*acl Safe_ports port 80     # http*
*acl Safe_ports port 81     # smoothwall http*
*acl Safe_ports port 21     # ftp *
*acl Safe_ports port 445 443 441 563 # https, snews*
*acl Safe_ports port 70     # gopher*
*acl Safe_ports port 210       # wais  *
*acl Safe_ports port 1025-65535 # unregistered ports*
*acl Safe_ports port 280       # http-mgmt*
*acl Safe_ports port 488       # gss-http *
*acl Safe_ports port 591       # filemaker*
*acl Safe_ports port 777       # multiling http*

*acl CONNECT method CONNECT*

*# TAG: http_access*
*# ----------------------------------------------------------------*


*http_access deny !Safe_ports*
*http_access deny CONNECT !SSL_ports*

*http_access allow localnetgreen*
*http_access allow CONNECT localnetgreen*

*http_access allow localhostgreen*
*http_access allow CONNECT localhostgreen*

*# http_port and https_port*
*#----------------------------------------------------------------------------*

*# A random port for forward-proxy port needed for SSL*
*http_port 8081*

*http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*ssl_bump server-first all*

*ssl_bump none localhostgreen*
*sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
*sslcrtd_children 5*

*sslproxy_session_cache_size 4 MB*

*http_access deny all*

*cache_replacement_policy heap GDSF*
*memory_replacement_policy heap GDSF*

*# CACHE OPTIONS*
*#
----------------------------------------------------------------------------*
*cache_effective_user squid*
*cache_effective_group squid*

*cache_swap_high 100*
*cache_swap_low 80*

*cache_mem 8 MB*
*maximum_object_size_in_memory 512 KB*

*cache_access_log /var/log/squid/access.log*
*cache_log /var/log/squid/cache.log*

*cache_dir diskd /var/spool/squid/cache 1024 16 256 Q1=64 Q2=72*

*maximum_object_size 33 MB*

*minimum_object_size 0 KB*


*request_body_max_size 0 KB*

*# OTHER OPTIONS*
*#
----------------------------------------------------------------------------*
*forwarded_for off*

*pid_filename /var/run/squid.pid*

*shutdown_lifetime 30 seconds*
*icp_port 3130*

*half_closed_clients off*

*umask 022*

*logfile_rotate 0*

*strip_query_terms off*

I think I am about ready to give up on trying to get Squid 3.5.2 working
and just fall back to Squid 3.4.10 which was the last version that worked
correctly and didn't give me these startup issues.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150329/69c75e97/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 30 11:18:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Mar 2015 00:18:36 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.3 is available
Message-ID: <5519310C.7090800@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.3 release!


This release is a bug fix release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:


* Regression Bug #4206: connection close on Expect:100-continue

It was found that large POST and PUT requests using Expect:100-continue
to a Squid-3.5.1 or 3.5.2 would reset the TCP connection instead of
allowing the upload to proceed. The working Squid-3.4 behaviour has now
been restored.


* Regression Bug #4213: negotiate_kerberos_auth segmentation faults

After Squid-3.5.2 updates to the Kerberos support it was found that this
helper was frequently, but not always, encountering a segmentation
fault. That is now fully resolved.

Also fixed in this release is support for the latest Heimdal libraries
and some unused Kerberos related code is no longer built.


* Bug #2907: high CPU usage on CONNECT when using Delay Pools

When Delay Pools was enabled Squid CONNECT handling tunnel code could
quickly empty the available pool bandwidth and would then also not wait
for it to be replenished, but repeatedly attempt to keep sending. While
this is not quite an "infinite loop" problem it is very similar in
effect, with CPU consumption reaching 100% and service through the proxy
slowing down dramatically.

While this is very old bug, it is starting to make itself felt more as
the quantity of HTTPS CONNECT requests increases.


* Bug #3805: support shared memory on MacOS X

This bug completely prevented using SMP support on MacOS X. As of this
release it should now be possible to use workers, shared memory cache
and rock storage on MacOS X.


* Bug #4204: ./configure abort when required helpers cannot be built

Previously the Squid ./configure script would treat a user-supplied list
of helpers as an optional list to attempt building, ignoring helpers
that were available but not listed. Being an optional list it would also
only warn if some of the list entries could not be built.

It is now treated as a list of required helpers - with a hard failure if
any cannot be built. This prevents automated build systems going through
a long build process only to find missing binaries at the install phase.


* basic_nis_auth and basic_getpwnam_auth updated

Other software has recently been awarded CVE allocation for bad handling
of crypt() system call failures resulting in Denial of Service. These
two Squid helpers were performing very similar operations and might
encounter the same failures. Fortunately these Squid helpers are fairly
isolated and Basic auth in Squid contains mechanisms that make it very
difficult to affect more than one client.

This is a proactive security update to prevent any future issues that
could appear as a result.



 All users of Squid-3.5 with SMP features are urged to upgrade to this
release as soon as possible.

 All users of Delay Pools are urged to upgrade to this release as soon
as possible.

 All users of basic_nis_auth or basic_getpwnam_auth are urged to upgrade
to this release as soon as possible.

 All users of Squid are urged to upgrade to this release as soon as
possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From dan at getbusi.com  Mon Mar 30 12:36:59 2015
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 30 Mar 2015 23:36:59 +1100
Subject: [squid-users] assertion failed: ../src/ipc/AtomicWord.h:88:
	"Enabled()"
In-Reply-To: <1427472709371.1f57f76a@Nodemailer>
References: <551535F1.50500@treenet.co.nz> <1427472709371.1f57f76a@Nodemailer>
Message-ID: <9165545B-7A20-46B9-BB56-B8A6C61210EF@getbusi.com>

Hey Amos

This error's still happening on the 3.5.3 RPM I just built. I know nothing about ?atomics?, mind you.  I?m all ears if you have any other suggestions :-)

Squid Cache: Version 3.5.3
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-referer-log' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--with-openssl' '--enable-ssl-crtd' '--enable-storeio=aufs,ufs,rock' '--with-aio' '--enable-wccpv2' '--enable-esi' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-maxfd=65535' '--with-dl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--without-nettle' '--disable-optimizations' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience


> On 28 Mar 2015, at 3:11 am, Dan Charlesworth <dan at getbusi.com> wrote:
> 
> Roger?thanks for heads up Amos.
> 
>  
> 
> 
> On Fri, Mar 27, 2015 at 9:50 PM, Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>> wrote:
> 
> Hi Dan,
> This appears by a breakage in the 3.5 snapshots' GNU atomics detection.
> Though we are still not sure why the error occurs yet with atomics disabled.
> 
> Snapshots labelled r13783 or later available in a few hrs should be fixed.
> 
> Cheers
> Amos
> 
> 
> On 27/03/2015 11:47 a.m., Dan Charlesworth wrote:
> > Bumping this because I think it might have gone into the black hole the other night.
> > 
> >> On 23 Mar 2015, at 5:44 pm, Dan Charlesworth <dan at getbusi.com> wrote:
> >>
> >> Turns out it?s also shitting the bed whenever I go to an SSL site now that I?ve added --enable-storeio=rock:
> >>
> >> 2015/03/23 17:40:13 kid1| assertion failed: ../src/ipc/AtomicWord.h:71: "Enabled()"
> >> 2015/03/23 17:42:02 kid1| assertion failed: ../src/ipc/AtomicWord.h:74: "Enabled()"
> >>
> >> I feel like I?m definitely missing a dependency or something :-/
> >>
> >>> On 23 Mar 2015, at 5:28 pm, Dan Charlesworth <dan at getbusi.com <mailto:dan at getbusi.com>> wrote:
> >>>
> >>> Hey!
> >>>
> >>> Sorry for all the threads lately, folks -
> >>>
> >>> I just recompiled by 3.5 EL6 (64-bit) RPM (using squid-3.5.2-20150321-r13782).
> >>>
> >>> I decided to add rock to my `?enable-storeio` option, so I could try SMP and stuff, which was fine. But when I went to squid -z it, I got this crash:
> >>> assertion failed: ../src/ipc/AtomicWord.h:88: "Enabled()"
> >>>
> >>> Just using:
> >>> cache_dir rock /var/spool/squid 20000
> >>> workers 2
> >>>
> >>> I?m hoping, for a change, this is some obvious thing I?ve missed and not something I need to dig out backtraces for :-)
> >>>
> >>> Thanks, y'all
> >>
> > 
> > 
> > 
> > 
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150330/5d2c4c1d/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 30 13:40:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Mar 2015 02:40:45 +1300
Subject: [squid-users] Squid 3.5.2 ssl_crtd kids causing abnormal
 termination of startup
In-Reply-To: <CANLNtGQWYObuYo1piJNyE92Js48aQGkdHMsQ7Ec3Wnb+XWB-9w@mail.gmail.com>
References: <CANLNtGQWYObuYo1piJNyE92Js48aQGkdHMsQ7Ec3Wnb+XWB-9w@mail.gmail.com>
Message-ID: <5519525D.3030907@treenet.co.nz>

The relevant bits of all patches that might be affecting this are now
included in 3.5.3.

Also, someone pointed out recently that /dev/shm needed to be explicitly
mounted on their OS.

Amos



From eraya at a21an.org  Tue Mar 31 09:14:17 2015
From: eraya at a21an.org (Eray Aslan)
Date: Tue, 31 Mar 2015 09:14:17 +0000
Subject: [squid-users] squid-3.5.3 eDirectory build error
Message-ID: <20150331091417.GA3041@angelfall>

>From squid-3.5.3 Changelog:
- Bug 4204: ./configure does not abort when required helpers cannot be
  built

And while trying to build eDirectory::
[...]
configure: error: Digest auth helper eDirectory ... found but cannot be
built

The following patch seems to work for me:

--- a/helpers/digest_auth/eDirectory/required.m4
+++ b/helpers/digest_auth/eDirectory/required.m4
@@ -5,4 +5,4 @@
 ## Please see the COPYING and CONTRIBUTORS files for details.
 ##
 
-AC_CHECK_HEADERS([ldap.h winldap.h],[BUILD_HELPER="LDAP"])
+AC_CHECK_HEADERS([ldap.h winldap.h],[BUILD_HELPER="eDirectory"])
--- a/helpers/digest_auth/eDirectory/Makefile.am
+++ b/helpers/digest_auth/eDirectory/Makefile.am
@@ -23,6 +23,7 @@ digest_edirectory_auth_LDADD = \
 	$(COMPAT_LIB) \
 	$(LDAPLIB) \
 	$(LBERLIB) \
+	$(NETTLELIB) \
 	$(CRYPTLIB) \
 	$(SSLLIB) \
 	$(XTRA_LIBS)

-- 
Eray


From squid3 at treenet.co.nz  Tue Mar 31 10:36:54 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Mar 2015 23:36:54 +1300
Subject: [squid-users] squid-3.5.3 eDirectory build error
In-Reply-To: <20150331091417.GA3041@angelfall>
References: <20150331091417.GA3041@angelfall>
Message-ID: <551A78C6.2080705@treenet.co.nz>

On 31/03/2015 10:14 p.m., Eray Aslan wrote:
> From squid-3.5.3 Changelog:
> - Bug 4204: ./configure does not abort when required helpers cannot be
>   built
> 
> And while trying to build eDirectory::
> [...]
> configure: error: Digest auth helper eDirectory ... found but cannot be
> built
> 

Yay working as designed :-)

Thank you for the patch. It has been applied to Squid-4 and 3.5 for the
next snapshots.

Amos



From eliezer at ngtech.co.il  Tue Mar 31 18:03:01 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 31 Mar 2015 21:03:01 +0300
Subject: [squid-users] mmap() in squid
In-Reply-To: <551561C0.2050108@norma.perm.ru>
References: <551561C0.2050108@norma.perm.ru>
Message-ID: <551AE155.7090302@ngtech.co.il>

Hey Eugene,

Since I do not have the full details about the issue and related areas I 
cannot answer and I think later others will answer this better then me.
But as for the last question about squid being a DB.
Squid in a way is also a DB like any OS is a DB.
Due to the fact that squid is kind of a DB, it's preferable for example 
to use fast disks.
While squid is indeed a DB and has lots of functions, and while it is 
also helps to cache there is always the question if you do use the cache 
system properly.
Squid is there and is open to use and will probably be there for many years.
One of the big benefits that squid has compared to other products is 
that you can try to understand any issue and get response from others 
about almost any related issue.
 From my understanding SSD disks are one of the things which can help a 
server work faster.
I have been asked couple times in the past about using SSD in production 
systems for cache.
Indeed SSD helps for the system to run and once a system production test 
has "ended" then and only then the actual production use can be the next 
step.
If indeed one of your tests proved that this patch is production ready I 
think that it might be worth to actually look at it.
Squid users list is the starter point but I am pretty sure that the 
level of the subject is for squid development rather then for squid 
users list while I found it interesting.

Thanks,
Eliezer

On 27/03/2015 16:57, Eugene M. Zheganin wrote:
> Last year there was an issue with PostgreSQL, which laso started to use
> mmap() in it's 9.3 release, and it had a huge regression issue on
> FreeBSD. One of the measures to fight this regression (but not the only)
> was adding MAP_NOSYNC to the postgresql port. So I decided to do the
> same for my local squid. I created a patch, where both of two
> occurencies of mmap() were supplied with this flag. I'm using squid
> 3.4.x patched this way about a half-a-year. Couple of days ago I sent
> this patch to the FreeBSD ports system, and squid port maintainer asks
> me if I'm sure squid on FreeBSD does need this. Since I'm not a skilled
> programmer (though I think using mmap() with MAP_NOSYNC is a good
> thing), I decided to ask here - is this flag worth bothering, since
> squid isn't a database engine ?



From junior.cunha at hscbrasil.com.br  Tue Mar 31 18:25:16 2015
From: junior.cunha at hscbrasil.com.br (Junior Cunha)
Date: Tue, 31 Mar 2015 15:25:16 -0300 (BRT)
Subject: [squid-users] Free Squid helper for dynamic content caching
In-Reply-To: <276184545.2139.1427826090293.JavaMail.root@hscbrasil.com.br>
Message-ID: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>

Hi all,

   HSC Brasil, a Brazilian company, leader in the Internet security segment, delivers an easy, customizable and ready-to-use Squid 3.5 helper for dynamic content caching for free.

   The initial release provides native support for the following services:

- IMDb trailers
- Sourceforge downloads
- R7 v?deos
- Metacafe
- MSN videos
- Terra TV
- Globo v?deos
- Facebook
- YouTube
- Google Maps

   For more information:

   https://github.com/hscbrasil/hsc-dynamic-cache

   Best regards.

   []s

--
Junior Cunha
telefone  55 (51) 3216-7007 | Porto Alegre
telefone  55 (11) 3522-8191 | S?o Paulo
site:  www.hscbrasil.com.br



From hectorchan at gmail.com  Tue Mar 31 22:18:19 2015
From: hectorchan at gmail.com (Hector Chan)
Date: Tue, 31 Mar 2015 15:18:19 -0700
Subject: [squid-users] squid with slow client
Message-ID: <CAEhCwUyrQZZKzgW0HJENsppybFn4bSy28VdgnuE-YJUNrXiAbw@mail.gmail.com>

Hi all,

How does squid behave when it is downloading a 5+GB file with a slow
client?  I see my client (curl) exited with error code 18 (
CURLE_PARTIAL_FILE) when downloading a 5+GB file from squid.  It was a
cache miss, so the file was actually being fetched from the origin server.
When it is cache hit, I don't see the curl error.

What I observed was that squid was downloading from the origin server at
about 750 KB/s, and the client was downloading from squid at about 10 to 50
KB/s.  The client was geographically far from squid.

I am using the ufs disk cache:

cache_replacement_policy lru
minimum_object_size 1 bytes
cache_dir ufs /data/squid/cache 130000 16 256 max-size=26843545600

Here is the error I found in cache.log:

2015/03/18 16:53:07.263| client_side_reply.cc(1185) replyStatus:
clientReplyStatus: transfer is DONE
2015/03/18 16:53:07.263| client_side_reply.cc(1201) replyStatus:
clientReplyStatus: client didn't get all it expected
2015/03/18 16:53:07.263| cbdata.cc(510) cbdataReferenceValid:
cbdataReferenceValid: 0x1707a48
2015/03/18 16:53:07.263| client_side.cc(1917) stopSending: sending error
(local=127.0.0.1:8443 remote=127.0.0.1:54359 FD 8 flags=1):
STREAM_UNPLANNED_COMPLETE; old receiving erro
r: none

Thanks,
Hector
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150331/a077b574/attachment.htm>

