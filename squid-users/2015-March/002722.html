<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] urlpath_regex
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20urlpath_regex&In-Reply-To=%3CHKNPR04MB1934E2EDFB3930798D5FB37E8060%40HKNPR04MB193.apcprd04.prod.outlook.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002721.html">
   <LINK REL="Next"  HREF="002728.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] urlpath_regex</H1>
    <B>James Harper</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20urlpath_regex&In-Reply-To=%3CHKNPR04MB1934E2EDFB3930798D5FB37E8060%40HKNPR04MB193.apcprd04.prod.outlook.com%3E"
       TITLE="[squid-users] urlpath_regex">james at ejbdigital.com.au
       </A><BR>
    <I>Thu Mar 12 10:52:59 UTC 2015</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="002721.html">[squid-users] urlpath_regex
</A></li>
        <LI>Next message (by thread): <A HREF="002728.html">[squid-users] urlpath_regex
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2722">[ date ]</a>
              <a href="thread.html#2722">[ thread ]</a>
              <a href="subject.html#2722">[ subject ]</a>
              <a href="author.html#2722">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>&gt;<i> Three things;
</I>&gt;<i> 
</I>&gt;<i> * by re-writing you are generating an entirely new request with the
</I>&gt;<i> apt-cacher server URL as the destination. The HTTP message details about
</I>&gt;<i> what was originally requested and from where is *gone* when the traffic
</I>&gt;<i> leaves for the server. The solution for that is outlined at the end of
</I>&gt;<i> this mail.
</I>&gt;<i> 
</I>&gt;<i> * the .cab also contain &quot;static&quot; content for the updates installer files
</I>&gt;<i> and DLLs etc. particularly for the older Windows versions.
</I>
Long before I started using apt-cacher, I was having a problem with .cab?xxx files being cached when they shouldn't be, so I'm excluding them for now.

&gt;<i> * if you leave Squid as being allowed to cache the content it will do so
</I>&gt;<i> for most of the visible page contents people see. And for a moderate
</I>&gt;<i> portion of the updates as well. Its just the range request fetches
</I>&gt;<i> inside archives that Squid wont cache without tuning.
</I>
AFAICT, the &quot;tuning&quot; involves telling squid to grab the whole file, and with some of these updates being &gt;800MB, I think squid is the wrong tool for the job here.

My setup has squid running on an old laptop with a fairly small disk, and apt-cacher running on a server with a very large disk. The server isn't always on though, so I can't put squid there. I'm content with windows updates failing when the server is off, it's just for a home network.

&gt;<i> &gt;
</I>&gt;<i> &gt; It works except for the wuau_path acl. The line:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; url_rewrite_access allow wuau_repo wuau_path
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; matches all paths on the wuau_repo access list, not just those ending in
</I>&gt;<i> &gt; .psf. I get hits for paths ending in .cab, and even worse, paths with a
</I>&gt;<i> &gt; ? in them.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; What am I doing wrong?
</I>&gt;<i> 
</I>&gt;<i> Strange. It should only be matching for URLs ending in &quot;.psf&quot;.
</I>&gt;<i> 
</I>&gt;<i> Be aware that regex is quite literaly only looking at the line ending
</I>&gt;<i> and does explicitly mean URLs such as these as well:
</I>&gt;<i> 
</I>&gt;<i>  <A HREF="http://download.windowsupdate.com/blah.cab?voodoo.psf">http://download.windowsupdate.com/blah.cab?voodoo.psf</A>
</I>&gt;<i>  <A HREF="http://download.windowsupdate.com/?BLAH.PSF">http://download.windowsupdate.com/?BLAH.PSF</A>
</I>&gt;<i> 
</I>&gt;<i> If you need it to match only the things that look (to humans) like
</I>&gt;<i> on-disk file names I recommend:
</I>&gt;<i> 
</I>&gt;<i>    -i \.psf(\?.*)?$
</I>
I will rewrite as per your suggestion, but as I'm only applying the regex to windowsupdate.com urls I don't think this is the problem I'm having.

&gt;<i> Did you build your Squid normally, or with a specific regex library?
</I>
Normally, I guess.

# /usr/local/squid/sbin/squid -v
Squid Cache: Version 3.5.2
Service Name: squid
configure options:  '--enable-storeio=ufs' '--enable-linux-netfilter' '--with-openssl' --enable-ltdl-convenience

&gt;<i> 
</I>&gt;<i> Is there another config line adding to wuau_path somewhere in your
</I>&gt;<i> squid.conf?
</I>
No

&gt;<i> 
</I>&gt;<i> Is Squid actually running with the squid.conf you think it is?
</I>&gt;<i>  (sounds silly, but mistakes happen)
</I>
Yes

I also tried the same thing with http_access and that works as expected - *.psf files are allowed, non *.psf file are denied. I'm thinking bug at the point... I'll do some more testing and see if I can narrow it doen.

&gt;<i> Also, unrelated to this ... have you tried using cache_peer instead of
</I>&gt;<i> URL-rewriting ?
</I>&gt;<i> 
</I>&gt;<i>  cache_peer server parent 8080 0
</I>&gt;<i>  cache_peer_access server allow wuau_repo wuau_path
</I>&gt;<i>  cache_peer_access server deny all
</I>&gt;<i> 
</I>&gt;<i> I assuming its listening on port 8080. It may need the &quot;originserver&quot;
</I>&gt;<i> option as well.
</I>
apt-cacher expects a url such that if you wanted to get:

<A HREF="http://ftp.debian.org/debian/somefile">http://ftp.debian.org/debian/somefile</A>

you would ask apt-cacher for:

<A HREF="http://server:3142/apt-cacher/ftp.debian.org/debian/somefile">http://server:3142/apt-cacher/ftp.debian.org/debian/somefile</A>

And it knows about debian package lists and what it can throw out when etc, and if you specify ftp.debian.org and ftp.au.debian.org as apt sources, it's smart enough to know that a file cached from one can be used in the other. I'm probably pushing the friendship by using it to cache windows packages but so far it seems to do the job okay. Keeping the archive clean might be a problem though.

&gt;<i> apt-cacher should fetch from the Internet, not looping back through Squid.
</I>&gt;<i>
</I>
Given that the server might also retrieve other urls, and all the proxying is transparent, the best I could come up with was to match the browser agent string from apt-cacher when it requests a url:

acl apt_cacher browser apt-cacher

and then exclude that from caching and rewriting.

Thanks

James
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="002721.html">[squid-users] urlpath_regex
</A></li>
	<LI>Next message (by thread): <A HREF="002728.html">[squid-users] urlpath_regex
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2722">[ date ]</a>
              <a href="thread.html#2722">[ thread ]</a>
              <a href="subject.html#2722">[ subject ]</a>
              <a href="author.html#2722">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
