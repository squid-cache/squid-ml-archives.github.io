From eliezer at ngtech.co.il  Fri May  1 04:13:53 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 01 May 2015 07:13:53 +0300
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <5540FA20.9050208@gmail.com>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar> <5540FA20.9050208@gmail.com>
Message-ID: <5542FD81.5010404@ngtech.co.il>

On 29/04/2015 18:34, Yuri Voinov wrote:
> You really sure 20 children is enough for 1200 clients? Also whenever
> bypass on?

I will add "what language is the helper??".

Eliezer



From squid3 at treenet.co.nz  Fri May  1 05:04:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 17:04:48 +1200
Subject: [squid-users] squid-ldap-group not ERR
In-Reply-To: <DUB130-W389C0D9426B5EB2D1C779A80D70@phx.gbl>
References: <DUB130-W389C0D9426B5EB2D1C779A80D70@phx.gbl>
Message-ID: <55430970.6030503@treenet.co.nz>

On 30/04/2015 1:26 a.m., Alex Delgado wrote:
> Hello,
>  
> I'm trying to configure squid to validate Windows users  by group with squid-ldap-group.
>  
> Server is CENTOS 6.5 . I've installed samba, krb and squid from source.
>  
> Also, I've configured samba and krb, so centos server is a Windows member.
>  
> When I type :
>  
> /usr/lib64/squid/squid_ldap_group -R -b "dc=domain,dc=local" -f "(&(sAMAccountName=%v)(memberOf=cn=%a,dc=domain,dc=local))" -D "cn=user,cn=Users,dc=edvhold,dc=local" -W /dir/dir/ldpass.txt -h pdcserver
> user group
>  
> I got:
>  
> ERR
>  
> Does anybody what the erro is?

"ERR" is the helper protocol code for denied. The user account "user" is
not a member of the group "group" which is being checked.

Amos



From squid3 at treenet.co.nz  Fri May  1 05:10:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 17:10:43 +1200
Subject: [squid-users] How do I no-cache the following url?
In-Reply-To: <trinity-1cf746ce-df58-46b9-bff3-59b7368ac70c-1430354820884@3capp-mailcom-bs04>
References: <trinity-1cf746ce-df58-46b9-bff3-59b7368ac70c-1430354820884@3capp-mailcom-bs04>
Message-ID: <55430AD3.5020506@treenet.co.nz>

On 30/04/2015 12:47 p.m., Hussam Al-Tayeb wrote:
> What rule would I have to add to not cache the following url?
> http://images.example.com\imageview.gif?anything

That is not a URL. '\' is not a valid domain name character.


> Everything up to the "?" is an exact match.
> So I want to not cache
> http://images.example.com\imageview.gif?
> http://images.example.com\imageview.gif?anything
> http://images.example.com\imageview.gif?anything.gif
> etc...
> Thank you.
> 

The below answer assumes that you really meant the URL
http://images.example.com/imageview.gif?anything


 acl foo url_regex ^http://images\.example\.com/imageview\.gif
 cache deny foo


Amos



From squid3 at treenet.co.nz  Fri May  1 05:31:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 17:31:31 +1200
Subject: [squid-users] Cache strategy advice
In-Reply-To: <55427EEF.8030608@seiner.com>
References: <55427EEF.8030608@seiner.com>
Message-ID: <55430FB3.201@treenet.co.nz>

On 1/05/2015 7:13 a.m., Yan Seiner wrote:
> I am building a small embedded squid box.
> 
> It has 4GB of ram, dual core CPU, and a 32GB SSD.
> 
> Since I'm running a tiny embedded linux distro (openwrt) most of those
> resources are available; I'm only using about 1MB of RAM and about 300MB
> of the SSD.
> 
> My incoming internet service is 30 to 60 Mb/sec.
> 
> My goals:
> 
> Maximize throughput (I don't want squid to slow down the connection)
> Minimize wear on the SSD
> 
> I am planning to set up two workers but beyond that I'm not really how
> to effectively use what I have.

I advise using only one worker (non-SMP) on dual-core systems. Squid
workers will happily consume the entirety of any CPU cores you let it
use at peak times. So leaving one for the OS and helpers etc to use is a
good idea.

Single worker of Squid with memory-only caching can cope with upwards of
50Mbps. So your traffic expectations should not be a worry there unless
the cores are very slow (MHz range).


If you want to minimize wear on the SSD avoid cache_dir storage types
entirely they are guaranteed to wear it out faster than normal. Current
Squid will run fine with only memory-only caching. Adjust the cache_mem
directive as wanted - default is a 256MB memory cache.

Also, avoid having the device swap memory at all costs - even with the
SSD. If it reaches the point of swapping Squid performance will drop
radically and the SSD wear will increase to match.


Any other features are optional or depend on exactly what you want to be
doing policy-wise with the proxy.

Amos



From squid3 at treenet.co.nz  Fri May  1 05:42:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 17:42:13 +1200
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <5542FD81.5010404@ngtech.co.il>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar> <5540FA20.9050208@gmail.com>
 <5542FD81.5010404@ngtech.co.il>
Message-ID: <55431235.10305@treenet.co.nz>

On 1/05/2015 4:13 p.m., Eliezer Croitoru wrote:
> On 29/04/2015 18:34, Yuri Voinov wrote:
>> You really sure 20 children is enough for 1200 clients? Also whenever
>> bypass on?
> 
> I will add "what language is the helper??".

It's a patched jesred, so "C".

The problem described "jesred hanging" is clearly a problem in jesred
itself. Not Squid.

My guess is that its been patched to cope with the action code and
kv-pair syntax. But not concurrency enabled. Which is mandatory on the
Store-ID interface.

Amos



From squid3 at treenet.co.nz  Fri May  1 05:43:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 17:43:42 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430403073796-4670979.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
Message-ID: <5543128E.3080402@treenet.co.nz>

On 1/05/2015 2:11 a.m., HackXBack wrote:
> Like i mentioned before i was facing this error and squid restart every few
> seconds,
> now i found what is causing this error for me
> it is :range_offset_limit none partial
> when i make range_offset_limit 0
> then the error goes out,
> now i cant cache 206 contents , if i make 206 contents hit , then i will get
> assertion error !
> so this is strange !!

Christos has just tracked this down as a side effect of a bug combining
connection pinning with pipeline_prefetch. The fix will be in the 3.5.4
and 3.4.13 release(s) due out later today.

Do you have SSL-Bump configured, or any software using NTLM or
Negotiate/Kerberos (even end-to-end over a non-authenticating proxy)?

Amos



From eliezer at ngtech.co.il  Fri May  1 07:55:34 2015
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 01 May 2015 10:55:34 +0300
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <55431235.10305@treenet.co.nz>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar> <5540FA20.9050208@gmail.com>
 <5542FD81.5010404@ngtech.co.il> <55431235.10305@treenet.co.nz>
Message-ID: <55433176.8040500@ngtech.co.il>

So I assume it's a rewrite for the urls.
I will try to take a look at jesered and later might be able to identify 
a way to use a golang helper for a basic load test comparison.

Eliezer

On 01/05/2015 08:42, Amos Jeffries wrote:
> On 1/05/2015 4:13 p.m., Eliezer Croitoru wrote:
>> On 29/04/2015 18:34, Yuri Voinov wrote:
>>> You really sure 20 children is enough for 1200 clients? Also whenever
>>> bypass on?
>>
>> I will add "what language is the helper??".
>
> It's a patched jesred, so "C".
>
> The problem described "jesred hanging" is clearly a problem in jesred
> itself. Not Squid.
>
> My guess is that its been patched to cope with the action code and
> kv-pair syntax. But not concurrency enabled. Which is mandatory on the
> Store-ID interface.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>




From ahaitoute at rinis.nl  Fri May  1 08:22:39 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Fri, 1 May 2015 10:22:39 +0200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
	connections?
In-Reply-To: <5541F70F.4010603@treenet.co.nz>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
 <13BF7DA2-AC5C-41BB-9E94-758BF2A06D59@rinis.nl>
 <5540C6D3.8040900@treenet.co.nz>
 <012A5BDD-E225-49B5-8352-B6CA28A194FD@rinis.nl>
 <5541F70F.4010603@treenet.co.nz>
Message-ID: <8CF53257-5CF2-4003-B035-84582A824D3D@rinis.nl>

After adding the /var/run/squid directory it works!

There?s a bug reported about it at Red Hat. https://bugzilla.redhat.com/show_bug.cgi?id=1102842 <https://bugzilla.redhat.com/show_bug.cgi?id=1102842>

Abdelouahed

> Op 30 apr. 2015, om 11:34 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
> Does /var/run/squid exist?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150501/81221948/attachment.htm>

From squid3 at treenet.co.nz  Fri May  1 08:25:08 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 20:25:08 +1200
Subject: [squid-users] ACL why does this not work?
In-Reply-To: <5542C136.50904@seiner.com>
References: <5542C136.50904@seiner.com>
Message-ID: <55433864.4070608@treenet.co.nz>

On 1/05/2015 11:56 a.m., Yan Seiner wrote:
> I am trying to prevent squid from proxying to an authorized subnet.
> 
> I want to write a set of acl rules that say that if a request does not
> come from the authorized subnet then it should not be allowed to connect
> to the authorized web server.
> 
> acl auth_net src 192.168.4.0/24
> acl auth dst 192.168.4.1
> http_access deny !auth_net auth
> 
> AFAICT something like the above should work but it doesn't.  squid
> proxies requests from anywhere on the network to the authorized
> webserver, getting right around the firewall.
> 
> Any suggestions on how to make this work?

You either got the order wrong
(<http://wiki.squid-cache.org/SquidFaq/OrderIsImportant>) or the DNS
results are not what you think they are.

We cant really say without knowing what your whole config is.

Amos



From hack.back at hotmail.com  Fri May  1 10:05:30 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 1 May 2015 03:05:30 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073468065319 blocks of 1 bytes!
Message-ID: <1430474730358-4671004.post@n4.nabble.com>

Squid Cache (Version 3.4.12): Terminated abnormally.
CPU Usage: 0.036 seconds = 0.012 user + 0.024 sys
Maximum Resident Size: 101264 KB
Page faults with physical i/o: 0
2015/05/01 12:20:04 kid1| Set Current Directory to /var/spool/squid
2015/05/01 12:20:04 kid1| Starting Squid Cache version 3.4.12 for
x86_64-unknown-linux-gnu...
2015/05/01 12:20:04 kid1| Process ID 31971
2015/05/01 12:20:04 kid1| Process Roles: worker
2015/05/01 12:20:04 kid1| With 65535 file descriptors available
2015/05/01 12:20:04 kid1| Initializing IP Cache...
2015/05/01 12:20:04 kid1| DNS Socket created at 0.0.0.0, FD 7
2015/05/01 12:20:04 kid1| Adding nameserver 10.150.15.2 from
/etc/resolv.conf
2015/05/01 12:20:04 kid1| helperOpenServers: Starting 40/50 'ssl_crtd'
processes
2015/05/01 12:20:04 kid1| helperOpenServers: Starting 1/1 'rewriter.pl'
processes
2015/05/01 12:20:04 kid1| helperOpenServers: Starting 1/1 'storeid.pl'
processes
2015/05/01 12:20:04 kid1| Logfile: opening log /var/log/squid/access.log
2015/05/01 12:20:04 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/access.log'
FATAL: xcalloc: Unable to allocate 18446744073468065319 blocks of 1 bytes!

Squid Cache (Version 3.4.12): Terminated abnormally.
CPU Usage: 0.032 seconds = 0.012 user + 0.020 sys
Maximum Resident Size: 101280 KB
Page faults with physical i/o: 0








root at issa:~/squid-3.4.12# gdb  /usr/sbin/squid /var/spool/squid/core
GNU gdb (GDB) 7.4.1-debian
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later
<http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/sbin/squid...(no debugging symbols found)...done.
[New LWP 31710]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `(squid-1) -YC -f /etc/squid/squid.conf'.
Program terminated with signal 6, Aborted.
#0  0x00007f9ccb14c165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) backtrace
#0  0x00007f9ccb14c165 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f9ccb14f3e0 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x0000000000628612 in fatal_dump(char const*) ()
#3  0x000000000085dc4d in xcalloc ()
#4  0x00000000005bdb06 in cacheDigestInit(CacheDigest*, int, int) ()
#5  0x00000000005bdc5f in cacheDigestCreate(int, int) ()
#6  0x00000000006e9779 in storeDigestInit() ()
#7  0x00000000006e071d in storeInit() ()
#8  0x000000000069ab1a in mainInitialize() ()
#9  0x000000000069b572 in SquidMain(int, char**) ()
#10 0x000000000069adbf in SquidMainSafe(int, char**) ()
#11 0x000000000069ad9c in main ()






Squid Cache: Version 3.4.12
configure options:  '--prefix=/usr' '--bindir=/usr/bin'
'--sbindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--sysconfdir=/etc/squid' '--localstatedir=/var' '--libdir=/usr/lib'
'--includedir=/usr/include' '--datadir=/usr/share/squid'
'--infodir=/usr/share/info' '--mandir=/usr/share/man'
'--disable-dependency-tracking' '--disable-strict-error-checking'
'--with-pthreads' '--with-aufs-threads=512' '--enable-storeio=ufs,aufs'
'--enable-removal-policies=lru,heap' '--with-aio' '--with-dl'
'--disable-icmp' '--enable-icap-client' '--disable-wccp' '--enable-wccpv2'
'--enable-cache-digests' '--enable-http-violations'
'--enable-linux-netfilter' '--enable-follow-x-forwarded-for'
'--enable-zph-qos' '--with-default-user=proxy'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-swapdir=/var/spool/squid' '--enable-ltdl-convenience'
'--with-filedescriptors=65536' '--enable-ssl' '--enable-ssl-crtd'
'--with-openssl' '--enable-snmp' '--disable-auth' '--disable-ipv6'
'--enable-arp-acl' '--enable-epoll' '--enable-referer-log'
'--enable-truncate' '--disable-unlinkd' '--enable-useragent-log'
'--enable-eui' '--enable-large-cache-files' 'CFLAGS=-march=native
-mtune=native -pipe -DNUMTHREADS=512' 'CXXFLAGS=-march=native -mtune=native
-pipe -DNUMTHREADS=512' 'LDFLAGS=-Wl,-Bsymbolic-functions'
'CPPFLAGS=-I/usr/include/openssl'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073468065319-blocks-of-1-bytes-tp4671004.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Fri May  1 10:09:42 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 1 May 2015 03:09:42 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <5543128E.3080402@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz>
Message-ID: <1430474982238-4671005.post@n4.nabble.com>

squid.conf you can see it all , and the answer on your question is no i dont
have .


# should be allowed
acl localnet src 10.11.20.0/24
acl localnet src 10.150.15.0/24

# ACL for rewriter
acl fakespeed url_regex -i
\.*(speedtest|espeed).*\/((latency|random.*|upload)\.(jpg|txt|php)).*
acl rewriter-link url_regex -i
^http.*(google|googlesyndication)\.com\/(pagead|js)\/(bg|js)\/.*\.js

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
#
-----------------------------------------------------------------------------
cache_peer 10.11.20.100 parent 80 0
cache_peer_access 10.11.20.100 allow fakespeed
cache_peer_access 10.11.20.100 deny all

# OPTIONS INFLUENCING REQUEST FORWARDING
#
-----------------------------------------------------------------------------
never_direct allow fakespeed
never_direct deny all
always_direct deny fakespeed





# add on squid.conf to remove ads
########################
acl ads-block url_regex -i "/etc/squid/ads.block"
http_access deny ads-block
http_reply_access deny ads-block

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

##Redirect some sites to storeid
################################################################################################
################################################################################################
################################################################################################
# Windows update acls
acl windowsupdate dstdomain windowsupdate.microsoft.com
acl windowsupdate dstdomain .update.microsoft.com
acl windowsupdate dstdomain download.windowsupdate.com
acl windowsupdate dstdomain redir.metaservices.microsoft.com
acl windowsupdate dstdomain images.metaservices.microsoft.com
acl windowsupdate dstdomain c.microsoft.com
acl windowsupdate dstdomain www.download.windowsupdate.com
acl windowsupdate dstdomain wustat.windows.com
acl windowsupdate dstdomain crl.microsoft.com
acl windowsupdate dstdomain sls.microsoft.com
acl windowsupdate dstdomain productactivation.one.microsoft.com
acl windowsupdate dstdomain ntservicepack.microsoft.com

# Windows update methods
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com

# Windows updates rules
http_access allow CONNECT wuCONNECT localnet
http_access allow CONNECT wuCONNECT localhost
http_access allow windowsupdate localnet
http_access allow windowsupdate localhost

acl store_rewrite_list url_regex -i fbcdn\/.*(jpg|gif|png|swf)
acl store_rewrite_list url_regex -i (akamaihd|fbcdn|facebook)\.(net|com)\/.*
acl store_rewrite_list url_regex -i attachment\.fbsbx\.com
acl store_rewrite_list url_regex -i fbcdn-dragon-a\.akamaihd\.net
acl store_rewrite_list url_regex -i socialpointgames\.com
acl store_rewrite_list url_regex -i miniclipcdn\.com
acl store_rewrite_list url_regex -i
syntasia\.hs\.llnwd\.net\/[a-z][0-9]+\/baseballheroes\/.*
acl store_rewrite_list url_regex -i \.google\-analytics\.com
acl store_rewrite_list url_regex -i google\-analytics\.com
acl store_rewrite_list url_regex -i video\.google\.com\/ThumbnailServer
acl store_rewrite_list url_regex -i
(youtube|google).*(videoplayback|liveplay)
acl store_rewrite_list url_regex -i
youtube.*(ptracking|stream_204|player_204|gen_204).*
acl store_rewrite_list url_regex -i
(youtube|google|googlevideo).*videoplayback.*
acl store_rewrite_list url_regex -i c\.android\.clients\.google\.com
acl store_rewrite_list url_regex -i phobos\.apple\.com
acl store_rewrite_list url_regex -i \.apple\.com
acl store_rewrite_list url_regex -i \/speedtest\/.*(jpg|txt|png|swf)
acl store_rewrite_list url_regex -i speedtest.*\/.*(jpg|txt|png|swf)
acl store_rewrite_list url_regex -i \.youjizz\.com\/.*(3gp|mpg|flv|mp4)
acl store_rewrite_list url_regex -i \.phncdn\.com\/.*(mp4|flv|3gp|mpg|wmv)
acl store_rewrite_list url_regex -i \.cdn13\.com\/.*(flv|mp3|mp4|3gp|wmv)
acl store_rewrite_list url_regex -i \.filehippo\.com\/.*
acl store_rewrite_list url_regex -i filehippo\.com\/.*
acl store_rewrite_list url_regex -i dl\.sourceforge\.net\/project\/.*
acl store_rewrite_list url_regex -i googlevideo\.com
acl store_rewrite_list url_regex -i reverbnation\.com
acl store_rewrite_list url_regex -i
c2lo\.reverbnation\.com\/audio_player\/ec_stream_song\/.*
acl store_rewrite_list url_regex -i (4shared|4shared\-china)\.com
acl store_rewrite_list url_regex -i 4shared\.com
acl store_rewrite_list url_regex -i bp\.blogspot\.com\/.*
acl store_rewrite_list url_regex -i ytimg\.com
acl store_rewrite_list url_regex -i (ggpht|googleusercontent)\.com
acl store_rewrite_list url_regex -i (s|i[0-9]*)\.ytimg\.com\/.*
acl store_rewrite_list url_regex -i video\.google\.com\/ThumbnailServer
acl store_rewrite_list url_regex -i (google\.co(m|\.uk|\.id))\/.*
acl store_rewrite_list url_regex -i (\.gstatic\.com.*|\.wikimapia\.org)
acl store_rewrite_list url_regex -i gstatic.com\/images.*
acl store_rewrite_list url_regex -i gstatic.com\/.*
acl store_rewrite_list url_regex -i bing\.(com|net)\/.*
acl store_rewrite_list url_regex -i (dmcdn\.net|dailymotion\.com).*
acl store_rewrite_list url_regex -i avast\.com
acl store_rewrite_list url_regex -i geo\.kaspersky\.com
acl store_rewrite_list url_regex -i update\.avg\.com
acl store_rewrite_list url_regex -i
(cbk|mt|khm|mlt|tbn|mw)[0-9]?.google\.co(m|\.uk|\.id)
acl store_rewrite_list url_regex -i
(\.doubleclick\.net|\.quantserve\.com|\.googlesyndication\.com|yieldmanager|cpxinteractive)
acl store_rewrite_list url_regex -i sdlc\-esd\.sun\.com
acl store_rewrite_list url_regex -i cloudfront\.net
acl store_rewrite_list url_regex -i sendspace\.com
acl store_rewrite_list url_regex -i rapidshare\.com
acl store_rewrite_list url_regex -i 185\.27\.237\.[\d]*
acl store_rewrite_list url_regex -i syntasia\.hs\.llnwd\.net
acl store_rewrite_list url_regex -i playspace\.r\.worldssl\.net
acl store_rewrite_list url_regex -i playit\.pk
acl store_rewrite_list url_regex -i attachment\.fbsbx\.com
acl store_rewrite_list url_regex -i firedrive\.com
acl store_rewrite_list url_regex -i cache\.pack\.google\.com
acl store_rewrite_list url_regex -i pack\.google\.com
acl store_rewrite_list url_regex -i dropboxusercontent\.com
acl store_rewrite_list url_regex -i aclst\.com
acl store_rewrite_list url_regex -i blackberry\.com
acl store_rewrite_list url_regex -i (bitgravity|opera)\.com
acl store_rewrite_list url_regex -i ggpht\.co(m|\.(id|uk))
acl store_rewrite_list url_regex -i instagram\.com
acl store_rewrite_list url_regex -i virtualearth\.net
acl store_rewrite_list url_regex -i cnet\.com
acl store_rewrite_list url_regex -i xvideos\.com
acl store_rewrite_list url_regex -i .*xhcdn.*
acl store_rewrite_list url_regex -i steampowered\.com
acl store_rewrite_list url_regex -i starhub\.com
acl store_rewrite_list url_regex -i (wargaming|hwcdn)\.net
acl store_rewrite_list url_regex -i indowebster\.com
acl store_rewrite_list url_regex -i filetrip\.net
acl store_rewrite_list url_regex -i get4mobile\.net
acl store_rewrite_list url_regex -i tube8\.com
acl store_rewrite_list url_regex -i (redtube|redtubefiles)\.com
acl store_rewrite_list url_regex -i .*nsimg.*
acl store_rewrite_list url_regex -i .*mystreamservice.*
acl store_rewrite_list url_regex -i youjizz\.com
acl store_rewrite_list url_regex -i .*phncdn.*
acl store_rewrite_list url_regex -i .*keezmovies.*
acl store_rewrite_list url_regex -i .*youporn.*
acl store_rewrite_list url_regex -i .*rncdn.*
acl store_rewrite_list url_regex -i .*spankwire.*
acl store_rewrite_list url_regex -i .*pornhub.*
acl store_rewrite_list url_regex -i .*playvid.*
acl store_rewrite_list url_regex -i .*maxporn.*
acl store_rewrite_list url_regex -i .*fucktube.*
acl store_rewrite_list url_regex -i .*slutload-media.*
acl store_rewrite_list url_regex -i .*hardsextube.*
acl store_rewrite_list url_regex -i public\.extremetube\.phncdn\.com
acl store_rewrite_list url_regex -i video\.pornhub\.phncdn\.com
acl store_rewrite_list url_regex -i public\.keezmovies\.phncdn\.com
acl store_rewrite_list url_regex -i public\.youporn\.phncdn\.com
acl store_rewrite_list url_regex -i public\.spankwire\.phncdn\.com
acl store_rewrite_list url_regex -i public\.keezmovies\.com
acl store_rewrite_list url_regex -i public\.spankwire\.com
acl store_rewrite_list url_regex -i pornhub\.com
acl store_rewrite_list url_regex -i slutload-media\.com
acl store_rewrite_list url_regex -i hardsextube\.com

acl store_rewrite_list_domain url_regex
^http:\/\/([a-zA-Z-]+[0-9-]+)\.[A-Za-z]*\.[A-Za-z]*
acl store_rewrite_list_domain url_regex
(([a-z]{1,2}[0-9]{1,3})|([0-9]{1,3}[a-z]{1,2}))\.[a-z]*[0-9]?\.[a-z]{3}
acl store_rewrite_list_path urlpath_regex
\.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|flv|avc|zip|mp3|3gp|rar|on2|mar|exe)$
acl store_rewrite_list_domain_CDN url_regex (khm|mt)[0-9]?.google.com
streamate.doublepimp.com.*\.js\? photos-[a-z].ak.fbcdn.net
\.rapidshare\.com.*\/[0-9]*\/.*\/[^\/]*
^http:\/\/(www\.ziddu\.com.*\.[^\/]{3,4})\/(.*) \.doubleclick\.net.*
yieldmanager cpxinteractive
^http:\/\/[.a-z0-9]*\.photobucket\.com.*\.[a-z]{3}$ quantserve\.com

store_id_access allow store_rewrite_list
store_id_access allow store_rewrite_list_domain
store_id_access allow store_rewrite_list_path
store_id_access allow store_rewrite_list_domain_CDN


####for looping 302 on youtube
acl text-html rep_mime_type text/html
acl http302 http_status 302

store_miss deny text-html
store_miss deny http302
send_hit deny text-html
send_hit deny http302

acl norewrite url_regex -i redirector\.c\.android\.clients\.google\.com
store_id_access deny norewrite

##this for send to storeid
acl youtube_to_storeid url_regex -i
^https?:\/\/.*(youtube|google).*(set_awesome|stream_204|playback|ptracking|watchtime|atr|player_204|videogoodput|get_video|get_video_info|s\?|delayplay|ads|qoe|gen_204).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*
acl youtube_to_storeid url_regex -i
^https?:\/\/.*(youtube|google).*videoplayback.*

acl gvt1_to_storeid url_regex -i ^https?:\/\/.*\.gvt1\.com\/market\/.*
acl mgccw_to_storeid url_regex -i ^https?:\/\/.*\.mgccw\.com\/.*

store_id_access allow youtube_to_storeid
store_id_access allow gvt1_to_storeid
store_id_access allow mgccw_to_storeid

## this for 206
acl partial dstdomain .googlevideo.com
acl partial dstdomain .youtube.com
acl partial dstdomain .mgccw.com
range_offset_limit none partial
store_id_access allow  partial

acl partial_content  url_regex -i ^http:\/\/122\.102\.49.*
acl partial_content  url_regex -i ^http:\/\/202\.93\.20.*
acl partial_content  url_regex -i ^http:\/\/armdl\.adobe\.com/pub.*
acl partial_content  url_regex -i ^http:\/\/download\.cdn\.mozilla\.net.*
acl partial_content  url_regex -i ^http.*netmarble\.co\.id.*
acl partial_content  url_regex -i ^http.*gemscool\.com.*
acl partial_content  url_regex -i ^http.*crossfire\.web\.id.*
acl partial_content  url_regex -i ^http.*garenanow\.com.*
acl partial_content  url_regex -i ^http.*winnerinter\.co\.id.*
acl partial_content  url_regex -i ^http.*starhub\.com.*
acl partial_content  url_regex -i ^http.*lytogame\.com.*
acl partial_content  url_regex -i ^http.*megaxus\.com
acl partial_content  url_regex -i ^http.*images\.offensive-security\.com.*
acl partial_content  url_regex -i ^http.*download\.windowsupdate\.com
acl partial_content  url_regex -i ^http.*ws\.microsoft\.com
acl partial_content  url_regex -i ^http.*fs41\.idup\.in.*
acl partial_content  url_regex -i ^http.*tusfiles\.net.*
acl partial_content  url_regex -i ^http.*files\.jalantikus\.com.*
acl partial_content  url_regex -i ^http.*cdn.files.bagas31.com.*
acl partial_content  url_regex -i ^http\/\/dl\.google\.com.*
acl partial_content  url_regex -i ^http.*\.c\.pack\.google\.com.*
acl partial_content  url_regex -i
^http.*\.(exe|psf|msi|msp|msu|dmg|cab|apk)$
range_offset_limit 1 KB partial_content





acl queryreg url_regex -i gemscool\.com\/registration\/.*
acl queryreg url_regex -i gemscool\.com\/isiGcash\/.*
acl queryreg url_regex -i ^http.*live\.mytrans\.com.*
acl queryreg url_regex -i ^http.*socialpointgames\.com\/dragoncity.*USERID.*
acl queryreg url_regex -i ^http.*fb_source=bookmark_apps.*
acl queryreg url_regex -i ^http.*gvoucher.*
acl queryreg url_regex -i ^http.*\.(asp|aspx|php|xml)(\?.*|)$

cache deny queryreg

acl playstoreandroid url_regex -i
c.android.clients.google.com.market.GetBinary.GetBinary.*
store_id_access allow playstoreandroid

#acl DENYCACHE urlpath_regex
\.(ini|ui|lst|inf|pak|ver|patch|md5|cfg|lst|list|rsc|log|conf|dbd|db|aspx|js|m3u8|ts|css)$
acl DENYCACHE urlpath_regex
(notice.html|notice.swf|afs.dat|dat.asp|patchinfo.xml|version.list|iepngfix.htc|updates.txt|patchlist.txt|update.ver)
acl DENYCACHE urlpath_regex (pointblank.css|login_form.css|form.css)$
cache deny DENYCACHE

acl capcha url_regex -i (captcha|captcha-login)
cache deny capcha

acl bau dstdomain iconnect.bau.edu.lb
cache deny bau

acl bau1 dstdomain iconnect.bau.edu.lb
always_direct allow bau1

acl betty dstdomain shop.betbetty.com
cache deny betty

acl betty1 dstdomain shop.betbetty.com
always_direct allow betty1
always_direct allow all
########################################################################

acl InvalidCert ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
acl InvalidCert ssl_error X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT
acl InvalidCert ssl_error X509_V_ERR_CERT_NOT_YET_VALID
acl InvalidCert ssl_error X509_V_ERR_ERROR_IN_CERT_NOT_BEFORE_FIELD
acl InvalidCert ssl_error X509_V_ERR_CERT_HAS_EXPIRED
acl InvalidCert ssl_error X509_V_ERR_ERROR_IN_CERT_NOT_AFTER_FIELD
acl InvalidCert ssl_error X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY

###FB - Insta - Google ..
acl spesifik-bump-dom dst 73.252.120.6
acl spesifik-bump-dom dst 216.58.192.0/19
acl spesifik-bump-dom dst 173.252.64.0/18
acl spesifik-bump-dom dst 31.13.93.0/24
acl spesifik-bump-dom dst 74.125.71.0/24
##iphone candy crush
acl spesifik-bump-dom dst 217.212.243.0/24
##iphone facebook
acl spesifik-bump-dom dst 179.60.192.0/24
##ipad facebook
acl spesifik-bump-dom dst 31.13.64.0/24
##iphone 6s
acl spesifik-bump-dom dst 31.13.83.0/24
acl spesifik-bump-dom dst 31.13.84.0/24
acl spesifik-bump-dom dst 2.16.0.0/13



acl https_login1 dstdomain \.(mail.yahoo.com|gmail.com)
acl https_login1 dstdomain gmail.com
acl appspot dstdomain appspot.com
acl https_login url_regex -i
^https.*(login|Login|signin|Signin|signup|Signup|Logout|logout).*
acl https_login url_regex -i ^https.*cdn\.yimg\.com.*


acl url_nobump1 url_regex -i ^https:\/\/android\.*

ssl_bump none spesifik-bump-dom
ssl_bump none InvalidCert
ssl_bump none https_login
ssl_bump none appspot
ssl_bump none url_nobump1
ssl_bump server-first all

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

################################################################################################
################################################################################################
################################################################################################
################################################################################################
################################################################################################
################################################################################################



## DNS
#dns_nameservers 127.0.0.1

# Squid normally listens to port 3128
https_port 3127 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl_cert/myCA.pem
http_port  3129
http_port  3128 intercept

sslcrtd_program /usr/lib/squid/ssl_crtd -s /etc/squid/ssl_db/certs/ -M 16MB
sslcrtd_children 50 startup=40 idle=1

sslproxy_capath /etc/ssl/certs
sslproxy_options NO_SSLv2 NO_SSLv3 SSL_OP_NO_TICKET

ssl_unclean_shutdown on
sslproxy_cert_error allow all

# -------------------------------------
# Tuning parameters
# -------------------------------------
#################################################################
#################################################################
store_dir_select_algorithm round-robin


ipcache_size 4096
ipcache_low 95
ipcache_high 98

memory_replacement_policy heap LRU
cache_replacement_policy heap LFUDA

cache_swap_low 90
cache_swap_high 95

# -------------------------------------
# Memory parameters
# -------------------------------------
cache_mem 1 Gb
memory_pools_limit 100 MB

maximum_object_size_in_memory 1024 KB
maximum_object_size 4096 MB
minimum_object_size 0 bytes

# Default is 20
store_objects_per_bucket 128

# Shutdown delay before terminate connections
shutdown_lifetime 30 second

# Cache user
cache_effective_user proxy
cache_effective_group proxy

# Cache manager
cache_mgr email at email.com

# Turn off collect per-client statistics
client_db off

# Hide internal networks details outside
via off
forwarded_for delete

# Do not show Squid version
httpd_suppress_version_string on

##for full url in access.log
strip_query_terms off

positive_dns_ttl 15 hours

#REFRESH PATTERN
refresh_pattern -i
\.*(speedtest|espeed).*\/((latency|random.*|upload)\.(jpg|txt|php)).* 0 0% 0
refresh_pattern -i \.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4)(\?|$)		
14400	99%	518400	ignore-no-store override-expire ignore-reload
reload-into-ims ignore-private ignore-must-revalidate
refresh_pattern -i
\.((m?|x?|s?)htm(l?)|js|xml|php|json|css|jsp|asx|asp|asp\?.*|aspx|shtml|phtml)(\?|$)		
10080	90%	86400	ignore-no-store override-expire override-lastmod
reload-into-ims ignore-private ignore-must-revalidate
refresh_pattern -i \.(asp|aspx)(\?.*)?$ 0 0% 0
refresh_pattern -i ^http.*manifest.googlevideo.com\/.* 0 0% 0
refresh_pattern -i ^http.*shop.betbetty.com\/.* 0 0% 0
refresh_pattern -i ^livebet.b2bgaming.net\/.* 0 0% 0
refresh_pattern -i
(UpdaterModifier.exe|FreeStyle.exe|PBLauncher.exe|update.exe|NewLauncher.exe|NewAvalon.exe|hon.exe.zip|cabal.exe)$
0 50% 1440
refresh_pattern -i (PointBlank.exe.zip|HSUpdate.exe.zip|PBConfig.exe.zip) 0
50% 1440
refresh_pattern -i
(wks_avira-win32-en-pecl.info.gz|wks_avira10-win32-en-pecl.info.gz|servers.def.vpx)$
0 50% 1440
refresh_pattern -i
(setup.exe.gz|avscan.exe.gz|avguard.exe.gz|filelist.zip|AvaClient.exe) 0 50%
1440
refresh_pattern -i (livescore.com|goal.com|sbobet) 0 50% 60
refresh_pattern -i nonton\.com 360 50% 1440 reload-into-ims ignore-reload
override-expire
#refresh_pattern -i
\.(mnft|ver|html|htm|jsp|asx|asp|asp\?.*|aspx|shtml|phtml) 0 0% 0
#refresh_pattern -i \.index\.(html|htm)$ 0 75% 10080
refresh_pattern -i
^https?\:\/\/www\.google\.co(m|\.id).*(newtab|manifest)\?.* 0 0% 0
refresh_pattern .*(begin|start)\=[1-9][0-9].* 0 0% 0
refresh_pattern -i \.(php|lst|ini|list) 0 0% 0
refresh_pattern
(update.ini|Update.ini|version.list|Version.list|update.1st|update.exe|autoup.exe)
0 0% 0

# refresh pattern
----------------------------------------------------------------------------------#
refresh_pattern -i (xtrap|login|sources) 0 0% 0
refresh_pattern .*(begin|start)\=[1-9][0-9].* 0 0% 0
#refresh_pattern -i
\.(html|htm|ini|ver|patch|lst|inf|htc|jsp|asx|asp|aspx|cfg|md5|key|pub|list|db|log|cgi|js|txt)$
0 0% 0
refresh_pattern
(update.ini|Update.ini|version.list|Version.list|update.1st|update.exe|autoup.exe)
0 0% 0
refresh_pattern
(hackshield|HackShield|HSUpdate|HShield|hsupdate|nprotect|update3) 0 50% 420
override-expire override-lastmod reload-into-ims

#refresh_pattern for out from storeid
refresh_pattern -i ^http.*cdn\.youtube\/.* 79900 80% 799000 override-expire
override-lastmod reload-into-ims ignore-reload ignore-no-cache
ignore-no-store ignore-private ignore-auth ignore-must-revalidate
refresh_pattern -i ^http.*(storeid\.cdn|cdn\.porno).* 79900 80% 799000
override-expire override-lastmod reload-into-ims ignore-reload
ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
refresh_pattern -i (storeid\.cdn|cdn|googlevideo\.com).* 79900 80% 799000
override-expire override-lastmod reload-into-ims ignore-reload
ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate










#REFRESH PATTERN
#refresh_pattern -i \.(js|css)$ 0 20% 4320 ignore-reload ignore-no-store
ignore-must-revalidate ignore-private ignore-auth store-stale
#refresh_pattern -i \.(nup|vdf|idx|gem|mcs|avc|vpx)$ 0 20% 4320
ignore-reload ignore-must-revalidate ignore-private store-stale
ignore-no-store
#refresh_pattern -i ^http:\/\/safebrowsing-cache\.google\.com\/.* 525600
100% 525600 override-expire override-lastmod reload-into-ims ignore-reload
ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#refresh_pattern -i ^http.*\/hackshield\/.* 0 20% 1440 ignore-no-store
ignore-must-revalidate ignore-private ignore-auth store-stale
#refresh_pattern -i \.(iop)$ 0 50% 1440 ignore-reload ignore-no-store
ignore-must-revalidate ignore-private ignore-auth store-stale
refresh_pattern -i
^http.*(netmarble\.co\.id|gemscool\.com|crossfire\.web\.id|garenanow\.com|winnerinter\.co\.id|starhub\.com|lytogame\.com|megaxus\.com).*
0 20% 4320 ignore-no-store ignore-private ignore-auth store-stale
#refresh_pattern -i \.(jpg|jpeg|raw|pnm|gif|bmp|tiff|swf|png|webp)(\?.*|)$
525600 100% 525600 override-expire override-lastmod reload-into-ims
ignore-reload ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#refresh_pattern -i
\.(web|avi|f4v|m4v|mpg|3gp|wmv|mov|rmvb|mkv|swf|dat|flv|fla|mp4|webm|00[1-9])
525600 100% 525600 override-expire override-lastmod reload-into-ims
ignore-reload ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#refresh_pattern -i \.(docx?|pptx?|xlsx?|pdf|lit|rtf|pdb|epub|prc|djvu)
525600 100% 525600 override-expire override-lastmod reload-into-ims
ignore-reload ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#refresh_pattern -i
\.(exe|rar|zip|tar|mar|iso|dcr|bz2|gz|7z|uha|bin|dmg|zipx|msi|msu|msp|cab|diff)
525600 100% 525600 override-expire override-lastmod reload-into-ims
ignore-reload ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#refresh_pattern -i \.(mid|mp3|wav|mka|aac|ogg|amr|amf|au|wma|rma) 525600
100% 525600 override-expire override-lastmod reload-into-ims ignore-reload
ignore-no-cache ignore-no-store ignore-private ignore-auth
ignore-must-revalidate
#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern -i (/cgi-bin/|mrtg|graph|\?)	0	0%	0
refresh_pattern	.	0	20%	10080	override-lastmod reload-into-ims





# local
qos_flows local-hit=0x30
# sibling
qos_flows sibling-hit=0x31
# parent
qos_flows parent-hit=0x32
# preserve
# qos_flows disable-preserve-miss





cache_dir aufs /cache01/1 400000 961 256
cache_dir aufs /cache01/2 400000 961 256
cache_dir aufs /cache01/3 400000 961 256









#LOGS
coredump_dir /var/spool/squid
error_directory /usr/share/squid/errors/templates
mime_table /etc/squid/mime.conf
pid_filename    /var/run/squid.pid

cache_log /var/log/squid/cache.log
access_log /var/log/squid/access.log
cache_store_log /var/log/squid/store.log

logfile_daemon /usr/lib/squid/log_file_daemon
logfile_rotate 12



# OPTIONS FOR REWRITE URL
#
-----------------------------------------------------------------------------
url_rewrite_program /etc/squid/rewriter.pl
url_rewrite_children 1 startup=1 idle=1 concurrency=99

url_rewrite_access allow fakespeed
url_rewrite_access allow rewriter-link
url_rewrite_access deny all

redirector_bypass on


store_id_program /etc/squid/storeid.pl
store_id_children 1 startup=1 idle=1 concurrency=100





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671005.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Fri May  1 10:12:06 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 1 May 2015 03:12:06 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073468065319 blocks of 1 bytes!
In-Reply-To: <1430474730358-4671004.post@n4.nabble.com>
References: <1430474730358-4671004.post@n4.nabble.com>
Message-ID: <1430475126431-4671006.post@n4.nabble.com>

when i decrease cache_dir , error disappear , but i need to use them all
since my dir's being full ...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073468065319-blocks-of-1-bytes-tp4671004p4671006.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri May  1 10:47:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 22:47:48 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430474982238-4671005.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430474982238-4671005.post@n4.nabble.com>
Message-ID: <554359D4.1030002@treenet.co.nz>

On 1/05/2015 10:09 p.m., HackXBack wrote:
> squid.conf you can see it all , and the answer on your question is no i dont
> have .
> 

Actually the answer was "yes".

> ssl_bump server-first all
> 

Which is good news. The bug will probably disappear when you move to the
3.5.4 version. Which will be available in a few hours.

Amos



From squid3 at treenet.co.nz  Fri May  1 11:55:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 01 May 2015 23:55:46 +1200
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073468065319 blocks of 1 bytes!
In-Reply-To: <1430475126431-4671006.post@n4.nabble.com>
References: <1430474730358-4671004.post@n4.nabble.com>
 <1430475126431-4671006.post@n4.nabble.com>
Message-ID: <554369C2.3060009@treenet.co.nz>

On 1/05/2015 10:12 p.m., HackXBack wrote:
> when i decrease cache_dir , error disappear , but i need to use them all
> since my dir's being full ...
> 

Um ... "full" is the normal operating sate of any cache. Only when the
cache is full will obsolete content start to get garbage collected.

It sounds to me like you are still trying to archive the entire Internet
on your one server.

Amos



From squid3 at treenet.co.nz  Fri May  1 13:05:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 01:05:22 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430474982238-4671005.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430474982238-4671005.post@n4.nabble.com>
Message-ID: <55437A12.9080605@treenet.co.nz>

On 1/05/2015 10:09 p.m., HackXBack wrote:
> squid.conf you can see it all , and the answer on your question is no i dont
> have .
> 

Okay, a bit of an audit...

> 
> # should be allowed
> acl localnet src 10.11.20.0/24
> acl localnet src 10.150.15.0/24
> 
> # ACL for rewriter
> acl fakespeed url_regex -i
> \.*(speedtest|espeed).*\/((latency|random.*|upload)\.(jpg|txt|php)).*

The trailing ".*" at the end of this is not useful.


> acl rewriter-link url_regex -i
> ^http.*(google|googlesyndication)\.com\/(pagead|js)\/(bg|js)\/.*\.js
> 
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
> #
> -----------------------------------------------------------------------------
> cache_peer 10.11.20.100 parent 80 0
> cache_peer_access 10.11.20.100 allow fakespeed
> cache_peer_access 10.11.20.100 deny all
> 
> # OPTIONS INFLUENCING REQUEST FORWARDING
> #
> -----------------------------------------------------------------------------
> never_direct allow fakespeed
> never_direct deny all
> always_direct deny fakespeed
> 
> 
> # add on squid.conf to remove ads
> ########################
> acl ads-block url_regex -i "/etc/squid/ads.block"
> http_access deny ads-block
> http_reply_access deny ads-block

Since you are denying requests being made to these URL the only possible
reply will be the "Access Denied" error page.

You are then denying that error page being delivered, and replacing it
with ... the "Access Denied" error page.

Sigh.

Hint #1:  Your Squid would operate a bit faster by removing that
"http_reply_access deny ads-block" line.

Hint #2: it is a deny using slow regex pattern. Your Squid would work
even faster by moving the "http_access deny ads-block" down below the
"deny manager" config line.

> 
> acl SSL_ports port 443
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl CONNECT method CONNECT
> 
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> ##Redirect some sites to storeid
> ################################################################################################
> ################################################################################################
> ################################################################################################
> # Windows update acls
> acl windowsupdate dstdomain windowsupdate.microsoft.com
> acl windowsupdate dstdomain .update.microsoft.com
> acl windowsupdate dstdomain download.windowsupdate.com
> acl windowsupdate dstdomain redir.metaservices.microsoft.com
> acl windowsupdate dstdomain images.metaservices.microsoft.com
> acl windowsupdate dstdomain c.microsoft.com
> acl windowsupdate dstdomain www.download.windowsupdate.com
> acl windowsupdate dstdomain wustat.windows.com
> acl windowsupdate dstdomain crl.microsoft.com
> acl windowsupdate dstdomain sls.microsoft.com
> acl windowsupdate dstdomain productactivation.one.microsoft.com
> acl windowsupdate dstdomain ntservicepack.microsoft.com
> 
> # Windows update methods
> acl wuCONNECT dstdomain www.update.microsoft.com
> acl wuCONNECT dstdomain sls.microsoft.com
> 
> # Windows updates rules
> http_access allow CONNECT wuCONNECT localnet
> http_access allow CONNECT wuCONNECT localhost
> http_access allow windowsupdate localnet
> http_access allow windowsupdate localhost
> 

The below is one massively huge regex pattern for Squid to try and
compile then process. Lets see what can be removed...

Firstly, remove all the ".*" which are on the end of patterns.


> acl store_rewrite_list url_regex -i fbcdn\/.*(jpg|gif|png|swf)
> acl store_rewrite_list url_regex -i (akamaihd|fbcdn|facebook)\.(net|com)\/.*
> acl store_rewrite_list url_regex -i attachment\.fbsbx\.com
> acl store_rewrite_list url_regex -i fbcdn-dragon-a\.akamaihd\.net
> acl store_rewrite_list url_regex -i socialpointgames\.com
> acl store_rewrite_list url_regex -i miniclipcdn\.com
> acl store_rewrite_list url_regex -i
> syntasia\.hs\.llnwd\.net\/[a-z][0-9]+\/baseballheroes\/.*

> acl store_rewrite_list url_regex -i \.google\-analytics\.com
> acl store_rewrite_list url_regex -i google\-analytics\.com

The two above patterns overlap. The seconds one will match everything
the first one does, and more.
... Remove the "-i \.google\-analytics\.com" line.

> acl store_rewrite_list url_regex -i video\.google\.com\/ThumbnailServer
> acl store_rewrite_list url_regex -i
> (youtube|google).*(videoplayback|liveplay)
> acl store_rewrite_list url_regex -i
> youtube.*(ptracking|stream_204|player_204|gen_204).*
> acl store_rewrite_list url_regex -i
> (youtube|google|googlevideo).*videoplayback.*

The above line overlaps completely with an earlier pttern.
Remove the "(youtube|google|googlevideo).*videoplayback.*" line.


> acl store_rewrite_list url_regex -i c\.android\.clients\.google\.com
> acl store_rewrite_list url_regex -i phobos\.apple\.com
> acl store_rewrite_list url_regex -i \.apple\.com

These apple.com patterns overlap completely.
Remove the "-i phobos\.apple\.com" line.

> acl store_rewrite_list url_regex -i \/speedtest\/.*(jpg|txt|png|swf)
> acl store_rewrite_list url_regex -i speedtest.*\/.*(jpg|txt|png|swf)

These speedtest patterns also overlap completely.
 Remove the "-i \/speedtest\/.*(jpg|txt|png|swf)" line.

> acl store_rewrite_list url_regex -i \.youjizz\.com\/.*(3gp|mpg|flv|mp4)
> acl store_rewrite_list url_regex -i \.phncdn\.com\/.*(mp4|flv|3gp|mpg|wmv)
> acl store_rewrite_list url_regex -i \.cdn13\.com\/.*(flv|mp3|mp4|3gp|wmv)
> acl store_rewrite_list url_regex -i \.filehippo\.com\/.*
> acl store_rewrite_list url_regex -i filehippo\.com\/.*

The above two lines are overlap.
Remove the "-i \.filehippo\.com\/.*" line.

Remove the ".*" from the end of the "-i filehippo\.com\/.*" line.

> acl store_rewrite_list url_regex -i dl\.sourceforge\.net\/project\/.*
> acl store_rewrite_list url_regex -i googlevideo\.com
> acl store_rewrite_list url_regex -i reverbnation\.com
> acl store_rewrite_list url_regex -i
> c2lo\.reverbnation\.com\/audio_player\/ec_stream_song\/.*
> acl store_rewrite_list url_regex -i (4shared|4shared\-china)\.com
> acl store_rewrite_list url_regex -i 4shared\.com
> acl store_rewrite_list url_regex -i bp\.blogspot\.com\/.*
> acl store_rewrite_list url_regex -i ytimg\.com
> acl store_rewrite_list url_regex -i (ggpht|googleusercontent)\.com
> acl store_rewrite_list url_regex -i (s|i[0-9]*)\.ytimg\.com\/.*

The above line is a more complicated pattern matching only things that
are matched by a simpler pattern elsewhere.
Remove the "-i (s|i[0-9]*)\.ytimg\.com\/.*" line.

> acl store_rewrite_list url_regex -i video\.google\.com\/ThumbnailServer

The above line is an exact duplicate of an earlier entry. Remove it.

> acl store_rewrite_list url_regex -i (google\.co(m|\.uk|\.id))\/.*
> acl store_rewrite_list url_regex -i (\.gstatic\.com.*|\.wikimapia\.org)
> acl store_rewrite_list url_regex -i gstatic.com\/images.*
> acl store_rewrite_list url_regex -i gstatic.com\/.*

You have three patterns there doing the same match.
 Remove the "-i gstatic.com\/.*" and "-i gstatic.com\/images.*" lines

Replace the "-i (\.gstatic\.com.*|\.wikimapia\.org)" line with:
  -i (gstatic\.com|\.wikimapia\.org)

> acl store_rewrite_list url_regex -i bing\.(com|net)\/.*
> acl store_rewrite_list url_regex -i (dmcdn\.net|dailymotion\.com).*
> acl store_rewrite_list url_regex -i avast\.com
> acl store_rewrite_list url_regex -i geo\.kaspersky\.com
> acl store_rewrite_list url_regex -i update\.avg\.com
> acl store_rewrite_list url_regex -i
> (cbk|mt|khm|mlt|tbn|mw)[0-9]?.google\.co(m|\.uk|\.id)
> acl store_rewrite_list url_regex -i
> (\.doubleclick\.net|\.quantserve\.com|\.googlesyndication\.com|yieldmanager|cpxinteractive)
> acl store_rewrite_list url_regex -i sdlc\-esd\.sun\.com
> acl store_rewrite_list url_regex -i cloudfront\.net
> acl store_rewrite_list url_regex -i sendspace\.com
> acl store_rewrite_list url_regex -i rapidshare\.com
> acl store_rewrite_list url_regex -i 185\.27\.237\.[\d]*
> acl store_rewrite_list url_regex -i syntasia\.hs\.llnwd\.net
> acl store_rewrite_list url_regex -i playspace\.r\.worldssl\.net
> acl store_rewrite_list url_regex -i playit\.pk
> acl store_rewrite_list url_regex -i attachment\.fbsbx\.com
> acl store_rewrite_list url_regex -i firedrive\.com
> acl store_rewrite_list url_regex -i cache\.pack\.google\.com
> acl store_rewrite_list url_regex -i pack\.google\.com

You have other patterns that match on just "google.com".
Remove the "-i cache\.pack\.google\.com" and "-i pack\.google\.com" lines.

> acl store_rewrite_list url_regex -i dropboxusercontent\.com
> acl store_rewrite_list url_regex -i aclst\.com
> acl store_rewrite_list url_regex -i blackberry\.com
> acl store_rewrite_list url_regex -i (bitgravity|opera)\.com
> acl store_rewrite_list url_regex -i ggpht\.co(m|\.(id|uk))
> acl store_rewrite_list url_regex -i instagram\.com
> acl store_rewrite_list url_regex -i virtualearth\.net
> acl store_rewrite_list url_regex -i cnet\.com
> acl store_rewrite_list url_regex -i xvideos\.com
> acl store_rewrite_list url_regex -i .*xhcdn.*
> acl store_rewrite_list url_regex -i steampowered\.com
> acl store_rewrite_list url_regex -i starhub\.com
> acl store_rewrite_list url_regex -i (wargaming|hwcdn)\.net
> acl store_rewrite_list url_regex -i indowebster\.com
> acl store_rewrite_list url_regex -i filetrip\.net
> acl store_rewrite_list url_regex -i get4mobile\.net
> acl store_rewrite_list url_regex -i tube8\.com
> acl store_rewrite_list url_regex -i (redtube|redtubefiles)\.com

Replace the above pattern with:
  -i redtube(files)?\.com

> acl store_rewrite_list url_regex -i .*nsimg.*
> acl store_rewrite_list url_regex -i .*mystreamservice.*
> acl store_rewrite_list url_regex -i youjizz\.com
> acl store_rewrite_list url_regex -i .*phncdn.*
> acl store_rewrite_list url_regex -i .*keezmovies.*
> acl store_rewrite_list url_regex -i .*youporn.*
> acl store_rewrite_list url_regex -i .*rncdn.*
> acl store_rewrite_list url_regex -i .*spankwire.*
> acl store_rewrite_list url_regex -i .*pornhub.*
> acl store_rewrite_list url_regex -i .*playvid.*
> acl store_rewrite_list url_regex -i .*maxporn.*
> acl store_rewrite_list url_regex -i .*fucktube.*
> acl store_rewrite_list url_regex -i .*slutload-media.*
> acl store_rewrite_list url_regex -i .*hardsextube.*

All of these lines ...

> acl store_rewrite_list url_regex -i public\.extremetube\.phncdn\.com
> acl store_rewrite_list url_regex -i video\.pornhub\.phncdn\.com
> acl store_rewrite_list url_regex -i public\.keezmovies\.phncdn\.com
> acl store_rewrite_list url_regex -i public\.youporn\.phncdn\.com
> acl store_rewrite_list url_regex -i public\.spankwire\.phncdn\.com
> acl store_rewrite_list url_regex -i public\.keezmovies\.com
> acl store_rewrite_list url_regex -i public\.spankwire\.com
> acl store_rewrite_list url_regex -i pornhub\.com
> acl store_rewrite_list url_regex -i slutload-media\.com
> acl store_rewrite_list url_regex -i hardsextube\.com

.. to here are duplicates. Remove them all.

> 
> acl store_rewrite_list_domain url_regex
> ^http:\/\/([a-zA-Z-]+[0-9-]+)\.[A-Za-z]*\.[A-Za-z]*

So, this matches almost every URL that exists with a domain containing
at least two '.' characters.

Go through what remains of the store_rewrite_list ACL lines and remove
the patterns that are searching for domians with two dots in them.
For example "attachment\.fbsbx\.com".

Also simplify the patterns that are search for domains with two dots OR
some alternative. For Example; "-i ggpht\.co(m|\.(id|uk))" can be
replaced with "-i ggpht\.com" since the .co.id and .co.uk versions will
be matched here.


> acl store_rewrite_list_domain url_regex
> (([a-z]{1,2}[0-9]{1,3})|([0-9]{1,3}[a-z]{1,2}))\.[a-z]*[0-9]?\.[a-z]{3}

For the non- http:// URLs the above pattern did not match, this repeats
the search in a little more detail and matches all those 2-dot domains
and along with a number of others in the http:// URL space.


> acl store_rewrite_list_path urlpath_regex
> \.(jp(e?g|e|2)|gif|png|tiff?|bmp|ico|flv|avc|zip|mp3|3gp|rar|on2|mar|exe)$
> acl store_rewrite_list_domain_CDN url_regex (khm|mt)[0-9]?.google.com

Your "store_rewrite_list" list contains a pattern that matches the
sub-string "google.com" without any qualifications.

 You can replace the "(khm|mt)[0-9]?.google.com" with "google\.com"

 Then remove all the store_rewrite_list pattens for google.com


> streamate.doublepimp.com.*\.js\? photos-[a-z].ak.fbcdn.net
> \.rapidshare\.com.*\/[0-9]*\/.*\/[^\/]*
> ^http:\/\/(www\.ziddu\.com.*\.[^\/]{3,4})\/(.*) \.doubleclick\.net.*
> yieldmanager cpxinteractive
> ^http:\/\/[.a-z0-9]*\.photobucket\.com.*\.[a-z]{3}$ quantserve\.com
> 
> store_id_access allow store_rewrite_list
> store_id_access allow store_rewrite_list_domain
> store_id_access allow store_rewrite_list_path
> store_id_access allow store_rewrite_list_domain_CDN
> 

Since these are all doing "allow" action the order does not matter.

Your Squid will work much faster if you switch the order of these lines
so the smaller / simpler patterns are tested first:

 store_id_access allow store_rewrite_list_domain
 store_id_access allow store_rewrite_list_path
 store_id_access allow store_rewrite_list_domain_CDN
 store_id_access allow store_rewrite_list


> 
> ####for looping 302 on youtube
> acl text-html rep_mime_type text/html
> acl http302 http_status 302
> 
> store_miss deny text-html

Seriously? After all the patterns forcing caching and refresh_patterns
trying to force things to be stored longer than their designers coded
for ... you never want any HTML content to be cached?


> store_miss deny http302
> send_hit deny text-html
> send_hit deny http302
> 
> acl norewrite url_regex -i redirector\.c\.android\.clients\.google\.com
> store_id_access deny norewrite

You have a pattern in the store_rewrite_list* ACLs that already allows
anything matching "google.com" to be re-written. By the time this config
line takes effect nothing can ever match it.

> 
> ##this for send to storeid
> acl youtube_to_storeid url_regex -i
> ^https?:\/\/.*(youtube|google).*(set_awesome|stream_204|playback|ptracking|watchtime|atr|player_204|videogoodput|get_video|get_video_info|s\?|delayplay|ads|qoe|gen_204).*(video_id|docid|\&v|content_v)\=([^\&\s]*).*
> acl youtube_to_storeid url_regex -i
> ^https?:\/\/.*(youtube|google).*videoplayback.*
> 
> acl gvt1_to_storeid url_regex -i ^https?:\/\/.*\.gvt1\.com\/market\/.*
> acl mgccw_to_storeid url_regex -i ^https?:\/\/.*\.mgccw\.com\/.*
> 

Both of the above ACLs are looking for domains with 2 dots in them.
Your store_rewrite_list_domain ACL has already matched and passed the
request to the Store-ID helper. Neither of these ACLs will ever be used.


> store_id_access allow youtube_to_storeid
> store_id_access allow gvt1_to_storeid
> store_id_access allow mgccw_to_storeid
> 
> ## this for 206
> acl partial dstdomain .googlevideo.com
> acl partial dstdomain .youtube.com
> acl partial dstdomain .mgccw.com
> range_offset_limit none partial
> store_id_access allow  partial
> 


> 
> acl queryreg url_regex -i gemscool\.com\/registration\/.*
> acl queryreg url_regex -i gemscool\.com\/isiGcash\/.*
> acl queryreg url_regex -i ^http.*live\.mytrans\.com.*
> acl queryreg url_regex -i ^http.*socialpointgames\.com\/dragoncity.*USERID.*
> acl queryreg url_regex -i ^http.*fb_source=bookmark_apps.*
> acl queryreg url_regex -i ^http.*gvoucher.*
> acl queryreg url_regex -i ^http.*\.(asp|aspx|php|xml)(\?.*|)$
> 
> cache deny queryreg
> 
> acl playstoreandroid url_regex -i
> c.android.clients.google.com.market.GetBinary.GetBinary.*
> store_id_access allow playstoreandroid

Here is another pattern that has no effect behind the 2-dot domain name
ones. In particular the 2-dot pattern matching URLs without requiring
http:// initial prefix.


Thats all I've got time for today. The config has plenty of other little
gotchas though.

Amos


From hierony_milanisti at yahoo.co.id  Fri May  1 13:18:47 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Fri, 1 May 2015 13:18:47 +0000 (UTC)
Subject: [squid-users] Clear Logs file
Message-ID: <1668498798.159840.1430486327020.JavaMail.yahoo@mail.yahoo.com>

Dear Fellow,

I want to clear my logs file, so that they will be fresh.
How can I do this in safe way?

Thanks in advance.

?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150501/df110457/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri May  1 13:27:16 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 1 May 2015 15:27:16 +0200
Subject: [squid-users] Clear Logs file
In-Reply-To: <1668498798.159840.1430486327020.JavaMail.yahoo@mail.yahoo.com>
References: <1668498798.159840.1430486327020.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <201505011527.16519.Antony.Stone@squid.open.source.it>

On Friday 01 May 2015 at 15:18:47 (EU time), Hierony Manurung wrote:

> I want to clear my logs file, so that they will be fresh.
> How can I do this in safe way?

Use the rotate facility.

http://wiki.squid-cache.org/SquidFaq/SquidLogs


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yan at seiner.com  Fri May  1 13:33:12 2015
From: yan at seiner.com (Yan Seiner)
Date: Fri, 01 May 2015 06:33:12 -0700
Subject: [squid-users] ACL why does this not work?
In-Reply-To: <55433864.4070608@treenet.co.nz>
References: <5542C136.50904@seiner.com> <55433864.4070608@treenet.co.nz>
Message-ID: <55438098.4040608@seiner.com>


On 05/01/2015 01:25 AM, Amos Jeffries wrote:
> On 1/05/2015 11:56 a.m., Yan Seiner wrote:
>> I am trying to prevent squid from proxying to an authorized subnet.
>>
>> I want to write a set of acl rules that say that if a request does not
>> come from the authorized subnet then it should not be allowed to connect
>> to the authorized web server.
>>
>> acl auth_net src 192.168.4.0/24
>> acl auth dst 192.168.4.1
>> http_access deny !auth_net auth
>>
>> AFAICT something like the above should work but it doesn't.  squid
>> proxies requests from anywhere on the network to the authorized
>> webserver, getting right around the firewall.
>>
>> Any suggestions on how to make this work?
> You either got the order wrong
> (<http://wiki.squid-cache.org/SquidFaq/OrderIsImportant>) or the DNS
> results are not what you think they are.
>
> We cant really say without knowing what your whole config is.
>
> Amos

Hi Amos:

Here's my config; it's pretty basic.

I have 4 subnets: dmz, auth, guest, and tenant.  Only the auth subnet 
should be allowed access to the webserver on 192.168.4.1. The web server 
does not listen on any of the other subnets.

cache_mem 3072 MB

acl dmz_net src 192.168.3.0/24
acl auth_net src 192.168.4.0/24
acl guest_net src 192.168.5.0/24
acl tenant_net src 192.168.6.0/24

acl dmz dst 192.168.3.1
acl auth dst 192.168.4.1
acl guest dst 192.168.5.1
acl tenant dst 192.168.6.1

acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10

acl ssl_ports port 443

acl safe_ports port 80
acl safe_ports port 21
acl safe_ports port 443
acl safe_ports port 70
acl safe_ports port 210
acl safe_ports port 1025-65535
acl safe_ports port 280
acl safe_ports port 488
acl safe_ports port 591
acl safe_ports port 777
acl connect method connect

http_access deny !auth_net auth

http_access deny !safe_ports
http_access deny connect !ssl_ports

http_access allow localhost manager
http_access deny manager

http_access deny to_localhost

http_access allow localnet
http_access allow localhost

http_access deny all

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

access_log none
cache_log /dev/null
cache_store_log /dev/null
logfile_rotate 0

logfile_daemon /dev/null



From keller.eric at gmail.com  Fri May  1 15:00:13 2015
From: keller.eric at gmail.com (Eric Keller)
Date: Fri, 01 May 2015 15:00:13 +0000
Subject: [squid-users] Squid-deb-proxy legacy-tools_0.1_all.deb Size mismatch
Message-ID: <CAF8s8J_rYv5=KuyOgs-YR9-7oxtPwfBw8k8OKO6Dq+xggvip4A@mail.gmail.com>

Hi everyone,

I did encounter a strange behavior with my squid-deb-proxy server.
on the master Debian repository server I forced publish an already existing
Debian package having the same version 0.1 (my bad, I won't do it again)

my squid-deb-proxy configuration looks like:

...
# refresh pattern for debs and udebs
refresh_pattern deb$   129600 100% 129600
refresh_pattern udeb$   129600 100% 129600
refresh_pattern tar.gz$  129600 100% 129600

# always refresh Packages and Release files
refresh_pattern \/(Packages|Sources)(|\.bz2|\.gz|\.xz)$ 0 0% 0
refresh_pattern \/Release(|\.gpg)$ 0 0% 0
refresh_pattern \/InRelease$ 0 0% 0
...

as far as I understand, the Packages and Release files are always refreshed
from the master repository server, but I quite do not understand the
meaning of "129600 100% 129600" for Debian packages.

I interpret that the Debian packages stay in the cache and are not
refreshed. So my package legacy-tools_0.1_all.deb and Release file got
updated on the master repository and only the Release file got updated
through the squid-deb-proxy but the old version mismatching the Release
size of the package is still available in the cache.

does this make sense?

Best Regards
--
Eric
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150501/52acda2b/attachment.htm>

From squid3 at treenet.co.nz  Fri May  1 15:46:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:46:02 +1200
Subject: [squid-users] Squid-deb-proxy legacy-tools_0.1_all.deb Size
	mismatch
In-Reply-To: <CAF8s8J_rYv5=KuyOgs-YR9-7oxtPwfBw8k8OKO6Dq+xggvip4A@mail.gmail.com>
References: <CAF8s8J_rYv5=KuyOgs-YR9-7oxtPwfBw8k8OKO6Dq+xggvip4A@mail.gmail.com>
Message-ID: <55439FBA.8050509@treenet.co.nz>

On 2/05/2015 3:00 a.m., Eric Keller wrote:
> Hi everyone,
> 
> I did encounter a strange behavior with my squid-deb-proxy server.
> on the master Debian repository server I forced publish an already existing
> Debian package having the same version 0.1 (my bad, I won't do it again)
> 
> my squid-deb-proxy configuration looks like:
> 
> ...
> # refresh pattern for debs and udebs
> refresh_pattern deb$   129600 100% 129600
> refresh_pattern udeb$   129600 100% 129600
> refresh_pattern tar.gz$  129600 100% 129600
> 
> # always refresh Packages and Release files
> refresh_pattern \/(Packages|Sources)(|\.bz2|\.gz|\.xz)$ 0 0% 0
> refresh_pattern \/Release(|\.gpg)$ 0 0% 0
> refresh_pattern \/InRelease$ 0 0% 0
> ...
> 
> as far as I understand, the Packages and Release files are always refreshed
> from the master repository server, but I quite do not understand the
> meaning of "129600 100% 129600" for Debian packages.
> 
> I interpret that the Debian packages stay in the cache and are not
> refreshed. So my package legacy-tools_0.1_all.deb and Release file got
> updated on the master repository and only the Release file got updated
> through the squid-deb-proxy but the old version mismatching the Release
> size of the package is still available in the cache.
> 
> does this make sense?

Yes, and will remain in cache for 129600 minutes (90 days).

Good example of how forcing things to cache with refresh_pattern can
bite back badly.

If you know the exact URL of the object, you can do a force-reload like so:
 squidclient -H 'Cache-Control:no-cache\n' \
   http://.../legacy-tools_0.1_all.deb

Or, IMHO upload a new package with incremented version so any other
proxies that have picked it up by now can get fixed quietly as well.

Amos



From squid3 at treenet.co.nz  Fri May  1 15:57:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:57:36 +1200
Subject: [squid-users] ACL why does this not work?
In-Reply-To: <55438098.4040608@seiner.com>
References: <5542C136.50904@seiner.com> <55433864.4070608@treenet.co.nz>
 <55438098.4040608@seiner.com>
Message-ID: <5543A270.2010503@treenet.co.nz>

On 2/05/2015 1:33 a.m., Yan Seiner wrote:
> 
> On 05/01/2015 01:25 AM, Amos Jeffries wrote:
>> On 1/05/2015 11:56 a.m., Yan Seiner wrote:
>>> I am trying to prevent squid from proxying to an authorized subnet.
>>>
>>> I want to write a set of acl rules that say that if a request does not
>>> come from the authorized subnet then it should not be allowed to connect
>>> to the authorized web server.
>>>
>>> acl auth_net src 192.168.4.0/24
>>> acl auth dst 192.168.4.1
>>> http_access deny !auth_net auth
>>>
>>> AFAICT something like the above should work but it doesn't.  squid
>>> proxies requests from anywhere on the network to the authorized
>>> webserver, getting right around the firewall.
>>>
>>> Any suggestions on how to make this work?
>> You either got the order wrong
>> (<http://wiki.squid-cache.org/SquidFaq/OrderIsImportant>) or the DNS
>> results are not what you think they are.
>>
>> We cant really say without knowing what your whole config is.
>>
>> Amos
> 
> Hi Amos:
> 
> Here's my config; it's pretty basic.
> 
> I have 4 subnets: dmz, auth, guest, and tenant.  Only the auth subnet
> should be allowed access to the webserver on 192.168.4.1. The web server
> does not listen on any of the other subnets.

Where the server listens is only relevant if the clients are talking
directly to it. When they go through the proxy its the proxy talking to
the server.

Still, what you have configured below does define the policy correctly.


Two things to do:

1) enable access.log and see what gets logged when you test it.

2) configure "debug_options 28,3" and see what shows up in cache.log
when you test it.

Some more notes below...

> 
> cache_mem 3072 MB
> 
> acl dmz_net src 192.168.3.0/24
> acl auth_net src 192.168.4.0/24
> acl guest_net src 192.168.5.0/24
> acl tenant_net src 192.168.6.0/24
> 
> acl dmz dst 192.168.3.1
> acl auth dst 192.168.4.1
> acl guest dst 192.168.5.1
> acl tenant dst 192.168.6.1
> 
> acl localnet src 10.0.0.0/8
> acl localnet src 172.16.0.0/12
> acl localnet src 192.168.0.0/16
> acl localnet src fc00::/7
> acl localnet src fe80::/10
> 
> acl ssl_ports port 443
> 
> acl safe_ports port 80
> acl safe_ports port 21
> acl safe_ports port 443
> acl safe_ports port 70
> acl safe_ports port 210
> acl safe_ports port 1025-65535
> acl safe_ports port 280
> acl safe_ports port 488
> acl safe_ports port 591
> acl safe_ports port 777
> acl connect method connect

Ah, method names are case-sensitive.

At the very least that should be:
  acl connect method CONNECT

> 
> http_access deny !auth_net auth
> 
> http_access deny !safe_ports
> http_access deny connect !ssl_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> http_access deny to_localhost
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> 
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
> 
> access_log none
> cache_log /dev/null

Dont do that. cache.log is where Squid publishes the critical failures
that your *really* need to be aware of.

> cache_store_log /dev/null

If you have a current Squid version, just erase the line. store.log is
not enabled by default since 3.1.

> logfile_rotate 0
> 
> logfile_daemon /dev/null
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri May  1 15:35:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:35:03 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2015:1 Incorrect
	X509 server	certificate valdidation
Message-ID: <55439D27.2090500@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2015:1
__________________________________________________________________

Advisory ID:            SQUID-2015:1
Date:                   May 01, 2015
Summary:                Incorrect X509 server certificate valdidation
Affected versions:      Squid 3.2 -> 3.2.13
                        Squid 3.3 -> 3.3.13
                        Squid 3.4 -> 3.4.12
                        Squid 3.5 -> 3.5.3
Fixed in version:       Squid 3.5.4, 3.4.13, 3.3.14, 3.2.14
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2015_1.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3455
__________________________________________________________________

Problem Description:

 Squid configured with client-first SSL-bump does not correctly
 validate X509 server certificate domain / hostname fields.

__________________________________________________________________

Severity:

 The bug is important because it allows remote servers to bypass
 client certificate validation. Some attackers may also be able
 to use valid certificates for one domain signed by a global
 Certificate Authority to abuse an unrelated domain.

 However, the bug is exploitable only if you have configured
 Squid to perform SSL Bumping with the "client-first" or "bump"
 mode of operation.

 Sites that do not use SSL-Bump are not vulnerable.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 3.5.4, 3.4.13, 3.3.14, and
 3.2.14.

 In addition, patches addressing this problem for stable releases
 can be found in our patch archives:

Squid 3.2:
http://www.squid-cache.org/Versions/v3/3.2/changesets/squid-3.2-11836.patch

Squid 3.3:
http://www.squid-cache.org/Versions/v3/3.3/changesets/squid-3.3-12690.patch

Squid 3.4:
http://www.squid-cache.org/Versions/v3/3.4/changesets/squid-3.4-13222.patch

Squid 3.5:
http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13817.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x, 3.0 and 3.1 are not vulnerable to the problem.

1) Run "squid -v" to determine if SSL support is enabled.

 All Squid built without SSL support are not vulnerable to the
 problem.


2) Run "squid -k parse 2>&1 | grep ssl_bump" to determine if
SSL-Bump is being used.

 All Squid-3.2, 3.3, 3.4, and 3.5 operating with ssl_bump omitted
 from squid.conf are not vulnerable to the problem.

 All unpatched Squid-3.x operating with "ssl_bump client-first"
 in squid.conf are vulnerable to the problem.

 All unpatched Squid-3.x operating with "ssl_bump bump" in
 squid.conf are vulnerable to the problem.

__________________________________________________________________

Workaround:

 There is no workaround for Squid-3.2.

 For Squid-3.3 and 3.4, upgrade the squid.conf settings to use
 "ssl_bump server-first".

 For Squid-3.5, upgrade the squid.conf settings to use a
 "ssl_bump peek" operation before the "bump" operation.

  NOTE that these workarounds do not resolve the vulnerability,
  but allow Squid to relay (or mimic) the invalid certificate to
  clients and depends on validation in the client.


Or,

 Disable SSL-Bump. Which may be done in the following ways:

 * Build Squid-3.2, 3.3, or 3.4 with ./configure --disable-ssl

 * Build Squid-3.5 with ./configure --without-openssl

 * Remove from squid.conf (and include'd files) any ssl_bump
   directives.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the squid-users at squid-cache.org mailing list is your primary
 support point. For subscription details see
 http://www.squid-cache.org/Support/mailing-lists.html.

 For reporting of non-security bugs in the latest release
 the squid bugzilla database should be used
 http://bugs.squid-cache.org/.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at squid-cache.org mailing list. It's a closed list
 (though anyone can post) and security related bug reports are
 treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The vulnerability was discovered and reported by a contributor
 who wishes to remain anonymous.

 The vulnerability was fixed and tested by
  Amos Jeffries, Treehouse Networks Ltd. and
  Christos Tsantilas, The Measurement Factory.

__________________________________________________________________

Revision history:

 2015-04-29 01:35 GMT Initial Report
 2015-05-01 13:49 GMT CVE Assignment
 2015-05-01 12:50 GMT Patches and Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri May  1 15:35:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:35:35 +1200
Subject: [squid-users] [squid-announce] Squid 3.3.14 is available
Message-ID: <55439D47.1020001@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.3.14 release!


This release is a security fix release resolving several vulnerabilities
found in the prior 3.3 releases.

    REMINDER: This and older releases are already deprecated by
              Squid-3.4 availablility.


The major changes to be aware of:


* CVE-2015-3455 : SQUID-2015:1 Incorrect X509 server certificate valdidation

  http://www.squid-cache.org/Advisories/SQUID-2015_1.txt

The bug is important because it allows remote servers to bypass client
certificate validation. Some attackers may also be able to use valid
certificates for one domain signed by a global Certificate Authority to
abuse an unrelated domain.

However, the bug is exploitable only if you have configured Squid to
perform SSL Bumping with the "client-first" mode of operation.

Sites that do not use SSL-Bump are not vulnerable.

A squid.conf workaround is available for quick use and those unable to
upgrade. See the Advisory notice for details.


* CVE-2014-7141, CVE-2014-7142 : SQUID-2014:4 Multiple issues in pinger
ICMP processing.

Several bugs allow any remote server to perform a denial of service
attack on the Squid service by crashing the pinger.

Some of these bugs allow attackers to leak arbitrary amounts of
information from the heap into Squid log files. This is of higher
importance than usual because the pinger process operates with root
priviliges.


* CVE-2014-6270 : SQUID-2014:3 Buffer overflow in SNMP processing

The bug is important because it allows remote attackers to crash Squid,
causing a disruption in service.  However, the bug is exploitable only
if you have configured Squid to receive SNMP messages.

Sites that do not use SNMP are not vulnerable.



 All users are urged to upgrade as soon as possible.

 See the ChangeLog for the full list of changes in this and earlier
 releases.


Please remember to run "squid -k parse" when testing upgrade to a new
version of Squid. It will audit your configuration files and report
any identifiable issues the new release will have in your installation
before you "press go". We are still removing the infamous "Bungled
Config" halting points and adding checks, so if something is not
identified please report it.



Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.3/RELEASENOTES.html
when you are ready to make the switch to Squid-3.3

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.3/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.3/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri May  1 15:35:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:35:46 +1200
Subject: [squid-users] [squid-announce] Squid 3.4.13 is available
Message-ID: <55439D52.9050404@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.4.13 release!


This release is a security fix release resolving a vulnerability and som
ebugs found in the prior 3.4 releases.

    REMINDER: This and older releases are already deprecated by
              Squid-3.5 availablility.


The major changes to be aware of:


* CVE-2015-3455 : SQUID-2015:1 Incorrect X509 server certificate valdidation

  http://www.squid-cache.org/Advisories/SQUID-2015_1.txt

The bug is important because it allows remote servers to bypass client
certificate validation. Some attackers may also be able to use valid
certificates for one domain signed by a global Certificate Authority to
abuse an unrelated domain.

However, the bug is exploitable only if you have configured Squid to
perform SSL Bumping with the "client-first" mode of operation.

Sites that do not use SSL-Bump are not vulnerable.

A squid.conf workaround is available for quick use and those unable to
upgrade. See the Advisory notice for details.


* Regression Bug 4212: ssl_crtd crashes with corrupt database

The fix for Bug 3664 introduced a regression on BSD and Linux where
lockf() implementations appear not to lock the entire file correctly or
as reliably as flock(). As a result ssl_crtd records would overwrite
each other. The helper would abort Squid on detecting the corruption.



 All users are urged to upgrade as soon as possible.

 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.4/RELEASENOTES.html
when you are ready to make the switch to Squid-3.4

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri May  1 15:35:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:35:29 +1200
Subject: [squid-users] [squid-announce] Squid 3.2.14 is available
Message-ID: <55439D41.4010603@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.2.14 release!


This release is a security fix release resolving a vulnerability found
in the prior 3.2 releases.

    REMINDER: This and older releases are already deprecated by
              Squid-3.3 availablility.


The major changes to be aware of:


* CVE-2015-3455 : SQUID-2015:1 Incorrect X509 server certificate valdidation

  http://www.squid-cache.org/Advisories/SQUID-2015_1.txt

The bug is important because it allows remote servers to bypass client
certificate validation. Some attackers may also be able to use valid
certificates for one domain signed by a global Certificate Authority to
abuse an unrelated domain.

However, the bug is exploitable only if you have configured Squid to
perform SSL Bumping with the "client-first" mode of operation.

Sites that do not use SSL-Bump are not vulnerable.

A squid.conf workaround is available for quick use and those unable to
upgrade. See the Advisory notice for details.



 All users are urged to upgrade as soon as possible.

 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please remember to run "squid -k parse" when testing upgrade to a new
version of Squid. It will audit your configuration files and report
any identifiable issues the new release will have in your installation
before you "press go".


Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html
when you are ready to make the switch to Squid-3.2

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

http://www.squid-cache.org/Versions/v3/3.2/
ftp://ftp.squid-cache.org/pub/squid/
ftp://ftp.squid-cache.org/pub/archive/3.2/

or the mirrors. For a list of mirror sites see

http://www.squid-cache.org/Download/http-mirrors.html
http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please
file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri May  1 15:35:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 03:35:56 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.4 is available
Message-ID: <55439D5C.2070106@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.4 release!


This release is a security and bug fix release resolving several
critical issues found in the prior Squid releases.


The major changes to be aware of:


* CVE-2015-3455 : SQUID-2015:1 Incorrect X509 server certificate valdidation

  http://www.squid-cache.org/Advisories/SQUID-2015_1.txt

The bug is important because it allows remote servers to bypass client
certificate validation. Some attackers may also be able to use valid
certificates for one domain signed by a global Certificate Authority to
abuse an unrelated domain.

However, the bug is exploitable only if you have configured Squid to
perform SSL Bumping with the "client-first" or "bump" modes of operation.

Sites that do not use SSL-Bump are not vulnerable.

A squid.conf workaround is available for quick use and those unable to
upgrade. See the Advisory notice for details.


* Add server_name ACL matching server name(s) obtained from various sources

This ACL type allows SSL-Bumped traffic to match on the best available
server name information. Taking its value from CONNECT URI, TLS SNI, or
Server X509 cetificate depending on which the current stage of TLS
processing makes available.

It is designed for use primarily for deciding ssl_bump logic based on
server domain name. Unlike dstdomain it does not perform rDNS lookup
when presented with a raw-IP address.


* Support for resuming TLS sessions

TLS and SSL contain a session resume feature which does not supply X509
certificates for Squid to mimic during the decryption. Previously Squid
has had to abort these connections, causing various client errors.

This release brings support for automatic splicing of resumed TLS
sessions. Bumping is not possible due to lack of certificate
information, and the old behaviour of responding with an error is
causing too many complaints.


* Basic support for ALPN and NPN TLS extensions

These TLS extensions are required to correctly splice or bump port 443
traffic now the port is being heavily overloaded for use by non-HTTPS
protocols wrapped in TLS.

When bumping Squid negotiates for HTTP/1.1 over TLS (HTTPS) to be the
protocol used by both server and client so that Squid can process it.


* Multiple SSL-Bump related crashes

Several different causes of assertion failure when performing SSL-Bump
have been fixed.


* Add Kerberos support for MAC OS X 10.x

Support for Apple custom Kerberos implementation is added in this release.



 All users of Squid-3.5 with SSL-Bump features are urged to upgrade to
this release as soon as possible.

 All users of Squid are encouraged to upgrade to this release as time
permits.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From mailinglist at cd.kcfam.net  Sat May  2 00:52:02 2015
From: mailinglist at cd.kcfam.net (Casey Daniels)
Date: Fri, 01 May 2015 20:52:02 -0400
Subject: [squid-users] CHROOT Problems
Message-ID: <55441FB2.6040504@cd.kcfam.net>

Hello,
  I'm trying to get squid up and running under a CHROOT (which by the 
way the instructions on wiki appear to point to a lot of directories 
that are different if you compile and install with out changing anything)

Prior to attempting the CHROOT I had Squid running fine, however there 
appears to be some issues now that I'm running under chroot.

Here is my log file

<---- START LOG----->

2015/05/01 20:33:21| Starting Squid Cache version 3.3.9 for 
x86_64-unknown-linux-gnu...
2015/05/01 20:33:21| Process ID 3095
2015/05/01 20:33:21| Process Roles: master worker
2015/05/01 20:33:21| With 1024 file descriptors available
2015/05/01 20:33:21| Initializing IP Cache...
2015/05/01 20:33:21| DNS Socket created at [::], FD 5
2015/05/01 20:33:21| DNS Socket created at 0.0.0.0, FD 6
2015/05/01 20:33:21| Adding domain kcfam.net from /etc/resolv.conf
2015/05/01 20:33:21| Adding nameserver [::1] from /etc/resolv.conf
2015/05/01 20:33:21| WARNING: rejecting '[::1]' as a name server, 
because it is not a numeric IP address
2015/05/01 20:33:21| Adding nameserver 127.0.0.1 from /etc/resolv.conf
2015/05/01 20:33:21| Logfile: opening log 
daemon:/usr/local/squid/var/logs/access.log
2015/05/01 20:33:21| Logfile Daemon: opening log 
/usr/local/squid/var/logs/access.log
2015/05/01 20:33:21| ipcCreate: 
/usr/local/squid/libexec/log_file_daemon: (2) No such file or directory
2015/05/01 20:33:21| ipcCreate: 
/usr/local/squid/libexec/log_file_daemon: (2) No such file or directory
2015/05/01 20:33:21| Local cache digest enabled; rebuild/rewrite every 
3600/3600 sec
2015/05/01 20:33:21| Store logging disabled
2015/05/01 20:33:21| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2015/05/01 20:33:21| Target number of buckets: 1008
2015/05/01 20:33:21| Using 8192 Store buckets
2015/05/01 20:33:21| Max Mem  size: 262144 KB
2015/05/01 20:33:21| Max Swap size: 0 KB
2015/05/01 20:33:21| Using Least Load store dir selection
2015/05/01 20:33:21| Set Current Directory to 
/usr/local/squid/var/cache/squid
2015/05/01 20:33:21| Loaded Icons.
2015/05/01 20:33:21| HTCP Disabled.
2015/05/01 20:33:21| Pinger socket opened on FD 11
2015/05/01 20:33:21| Squid plugin modules loaded: 0
2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) No 
such file or directory
2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) No 
such file or directory
2015/05/01 20:33:21| Adaptation support is off.
2015/05/01 20:33:21| Accepting HTTP Socket connections at 
local=[::]:3128 remote=[::] FD 9 flags=9
2015/05/02 00:33:22| logfileHandleWrite: 
daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken 
pipe)
2015/05/02 00:33:22| Closing HTTP port [::]:3128
2015/05/02 00:33:22| storeDirWriteCleanLogs: Starting...
2015/05/02 00:33:22|   Finished.  Wrote 0 entries.
2015/05/02 00:33:22|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: I don't handle this error well!
Squid Cache (Version 3.3.9): Terminated abnormally.
CPU Usage: 0.019 seconds = 0.013 user + 0.006 sys
Maximum Resident Size: 49008 KB
Page faults with physical i/o: 0
Memory usage for squid via mallinfo():
	total space in arena:    4764 KB
	Ordinary blocks:         4694 KB      4 blks
	Small blocks:               0 KB      1 blks
	Holding blocks:          1324 KB      4 blks
	Free Small blocks:          0 KB
	Free Ordinary blocks:      69 KB
	Total in use:            6018 KB 126%
	Total free:                69 KB 1%
2015/05/02 00:33:22| Closing Pinger socket on FD 11

<--- END LOG --->

I see three errors.

Number 1
2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) No 
such file or directory

Number 2
2015/05/01 20:33:21| ipcCreate: 
/usr/local/squid/libexec/log_file_daemon: (2) No such file or directory

Number 3
2015/05/02 00:33:22| logfileHandleWrite: 
daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken 
pipe)


Not knowing much about the inter workings of squid, and what not, but 
I'm guessing that Number 3 will be resolved when Number 2 is resolved. 
And Number 1 and 2 are closely related.

The issue is I'm at a stand still have to proceed with fixing Number 1 
and 2.
my chroot directive is

chroot /srv/squid

and I've copied all of the /usr/local/squid/libexec/ to 
/srv/squid/usr/local/squid/libexec

So i believe I have it in the right place.  And the file permissions 
appear to be the same within the CHROOT as they are in the normal 
directories.

Any Ideas?

Casey




From o.calvano at gmail.com  Sat May  2 02:58:05 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sat, 2 May 2015 04:58:05 +0200
Subject: [squid-users] Squid and Kerberos problems
Message-ID: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>

Hi

I request your help because i want use NTLM/Kerberos for authenticate my
user.

For NTLM, i use Winbind, no problems,

[root at gw]# wbinfo -t
checking the trust secret for domain MYADDOMAIN via RPC calls succeeded

but for Kerberos, i can't create the .keytab


[root at gw]# kinit MYUSERNAME
Password for MYUSERNAME at MYADDOMAIN.FR:

[root at gw]# klist
Ticket cache: KEYRING:persistent:0:0
Default principal: MYUSERNAME at MYADDOMAIN.FR

Valid starting       Expires              Service principal
02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/MYADDOMAIN.FR at MYADDOMAIN.FR
        renew until 09/05/2015 04:51:07

MYUSERNAME is the same account that i join the domain (net join) with
winbind


after, i put:

msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
-k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose

and i have a error:

[root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
--computer-name OPHTCYSRV1V4-K --upn HTTP/
gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/udandom = 84
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-jnxTuG
 -- reload: Reloading Kerberos Context
 -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
 -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/
gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
password.
 -- create_default_machine_password: Default machine password for
OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets...
 -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
cache found)
 -- try_user_creds: User ticket cache was not valid.
Error: could not find any credentials to authenticate with. Neither keytab,
     default machine password, nor calling user's tickets worked. Try
     "kinit"ing yourself some tickets with permission to create computer
     objects, or pre-creating the computer object in AD and selecting
     'reset account'.
 -- ~KRB5Context: Destroying Kerberos Context



same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
ophtcysrv1v4.myaddomain.fr


anyone know the origin of this error ?

thanks
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/492358a6/attachment.htm>

From michael.pelletier at palmbeachschools.org  Sat May  2 03:12:25 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Fri, 1 May 2015 23:12:25 -0400
Subject: [squid-users] adding a header by group membership
Message-ID: <CAEnCSG5_0RBGQ4NFA-6=gAodBvdQ9MXkiFt8JF2VD_cUr0nBvQ@mail.gmail.com>

Hello,

I wish to modify a request header if the user is a member of a group. The
example below I am trying to restrict people at work to ONLY the work email
address UNLESS they are in the group "FullEmailAccess". Is this correct?

acl FullEmailAccess proxy_auth -i "[a file containing users. One per line]"

request_header_access X-GoogApps-Allowed-Domains allow all
request_header_add X-GoogApps-Allowed-Domains "[work domain]"
!FullEmailAccess


Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150501/a5ea69f7/attachment.htm>

From jagannath.naidu at fosteringlinux.com  Sat May  2 03:45:28 2015
From: jagannath.naidu at fosteringlinux.com (Jagannath Naidu)
Date: Sat, 2 May 2015 09:15:28 +0530
Subject: [squid-users] NTLM AUTH: All redirector processes are busy
In-Reply-To: <CA+8bHvzxBf2WnXNvYz=jRK5RhacPdj840n3ye_Ze6Xu1Lh23GQ@mail.gmail.com>
References: <CA+8bHvza9tLGaHk=hCcY8+NzOcKFBDrZJzzpzT__7LTiGkpPgg@mail.gmail.com>
 <CA+8bHvzxBf2WnXNvYz=jRK5RhacPdj840n3ye_Ze6Xu1Lh23GQ@mail.gmail.com>
Message-ID: <CA+8bHvxnGAUd5WS=9j5=M41BxnPgJa2me4AwDUw+4_+PyBFacg@mail.gmail.com>

...... ???

On 30/04/2015, Jagannath Naidu <jagannath.naidu at fosteringlinux.com> wrote:
> Is there even any solution for this. ?
> Do any one have this working ?
>
> On 29 April 2015 at 17:04, Jagannath Naidu <
> jagannath.naidu at fosteringlinux.com> wrote:
>
>> Hi List/Amos,
>>
>> I am facing an using squid in production.
>>
>> I get these messages in cache.log, and service stop for a period of time
>> (like 14 seconds). During this period, users get panic as they get "proxy
>> server resfusing connections". And automatically the service starts
>> functioning again. But this happens very frequently whole day.
>>
>> 2015/04/29 10:34:10| WARNING: All redirector processes are busy.
>> 2015/04/29 10:34:10| WARNING: 15 pending requests queued
>> 2015/04/29 10:34:10| storeDirWriteCleanLogs: Starting...
>> 2015/04/29 10:34:10| WARNING: Closing open FD 3327
>> 2015/04/29 10:34:10|     65536 entries written so far.
>> 2015/04/29 10:34:10|    131072 entries written so far.
>> 2015/04/29 10:34:10|    196608 entries written so far.
>> 2015/04/29 10:34:10|    262144 entries written so far.
>> 2015/04/29 10:34:10|    327680 entries written so far.
>> 2015/04/29 10:34:10|    393216 entries written so far.
>> 2015/04/29 10:34:10|    458752 entries written so far.
>> 2015/04/29 10:34:10|    524288 entries written so far.
>> 2015/04/29 10:34:10|    589824 entries written so far.
>> 2015/04/29 10:34:10|    655360 entries written so far.
>> 2015/04/29 10:34:10|   Finished.  Wrote 716101 entries.
>> 2015/04/29 10:34:10|   Took 0.22 seconds (3266168.90 entries/sec).
>> FATAL: Too many queued redirector requests
>> Squid Cache (Version 3.1.10): Terminated abnormally.
>> CPU Usage: 4206.393 seconds = 3778.049 user + 428.344 sys
>> Maximum Resident Size: 2599760 KB
>> Page faults with physical i/o: 0
>> Memory usage for squid via mallinfo():
>>         total space in arena:  750272 KB
>>         Ordinary blocks:       717419 KB   6620 blks
>>         Small blocks:               0 KB      1 blks
>>         Holding blocks:         23020 KB     11 blks
>>         Free Small blocks:          0 KB
>>         Free Ordinary blocks:   32852 KB
>>         Total in use:          740439 KB 99%
>>         Total free:             32852 KB 4%
>> fgets() failed! dying..... errno=1 (Operation not permitted)
>> 2015/04/29 10:34:19| Starting Squid Cache version 3.1.10 for
>> x86_64-redhat-linux-gnu...
>> 2015/04/29 10:34:19| Process ID 4326
>> 2015/04/29 10:34:19| With 100000 file descriptors available
>> 2015/04/29 10:34:19| Initializing IP Cache...
>> 2015/04/29 10:34:19| DNS Socket created at [::], FD 8
>> 2015/04/29 10:34:19| DNS Socket created at 0.0.0.0, FD 9
>> 2015/04/29 10:34:19| Adding nameserver 172.16.3.34 from squid.conf
>> 2015/04/29 10:34:19| Adding nameserver 10.1.2.91 from squid.conf
>> 2015/04/29 10:34:19| helperOpenServers: Starting 5/5 'squidGuard'
>> processes
>> 2015/04/29 10:34:19| helperOpenServers: Starting 1500/1500 'ntlm_auth'
>> processes
>> 2015/04/29 10:34:24| helperOpenServers: Starting 150/150
>> 'wbinfo_group.pl'
>> processes
>>
>>
>> ntlm helpers count is 1500 and external "wbinfo_group.pl" helpers are
>> 150.
>>
>> squid.conf
>> ###################################################
>>
>> max_filedesc 100000
>> acl manager proto cache_object
>> acl localhost src 172.16.50.61
>> http_access allow manager localhost
>> dns_nameservers 172.16.3.34 10.1.2.91
>> acl allowips src 172.16.58.187 172.16.16.192 172.16.58.113 172.16.58.63
>> 172.16.58.98 172.16.60.244 172.16.58.165 172.16.58.157
>> http_access allow allowips
>> #acl haproxy src 172.16.50.61
>> #follow_x_forwarded_for allow haproxy
>> #follow_x_forwarded_for deny all
>> #acl manager proto cache_object
>> acl localnet src 172.16.0.0/16
>> acl manager proto cache_object
>> acl localhost src 127.0.0.1
>> acl localnet src fc00::/7 # RFC 4193 local private network range
>> acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged)
>> machines
>> acl office dstdomain "/etc/squid/officesites"
>> http_access allow office
>> log_ip_on_direct off
>> #debug_options ALL,3
>> #logformat squid %9d.%03d %6d %s %s/%03d %d %s %s %s %s%s/%s %s
>> logformat squid %ts.%03tu %tl %3tr %3dt %3un %>a %Ss/%>Hs %<st %rm %ru
>> %Sh/%<A %mt
>> access_log /var/log/squid/access1.log squid
>> auth_param basic realm Squid proxy-caching web server
>> auth_param basic credentialsttl 2 hours external_acl_type nt_group ttl=0
>> children=60 %LOGIN /usr/lib64/squid/wbinfo_group.pl
>> #auth_param ntlm program /etc/squid/helper-mux.pl /usr/bin/ntlm_auth
>> --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>> auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
>> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>> auth_param ntlm children 1500
>> #auth_param ntlm children 500
>> auth_param ntlm keep_alive off
>> auth_param ntlm program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5-ntlmssp --domain=HTMEDIA.NET
>> external_acl_type wbinfo_group_helper ttl=600 children=150 %LOGIN
>> /usr/lib64/squid/wbinfo_group.pl -d
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> cl Safe_ports port 8080 #https
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>> acl auth proxy_auth REQUIRED
>>
>>
>> and rest of acls and http_access rules configured ...............
>>
>>
>>
>> It seems the helper programs are not closing automatically after serving
>> and causes this issue. Could anyone help resolving this issue.
>>
>> [root at GGNPROXY01 squid]# rpm -qa | grep squid
>> squid-3.1.10-19.el6_4.x86_64
>>
>> [root at GGNPROXY01 squid]# rpm -qa | grep winbind
>> samba-winbind-clients-3.6.9-164.el6.x86_64
>> samba-winbind-3.6.9-164.el6.x86_64
>>
>> [root at GGNPROXY01 squid]# lsb_release -a
>> LSB Version:
>> :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
>> Distributor ID:    RedHatEnterpriseServer
>> Description:    Red Hat Enterprise Linux Server release 6.5 (Santiago)
>> Release:    6.5
>> Codename:    Santiago
>>
>>
>> --
>> Thanks & Regards
>>
>> B Jagannath
>> Keen & Able Computers Pvt. Ltd.
>> +919871324006
>>
>
>
>
> --
> Thanks & Regards
>
> B Jagannath
> Keen & Able Computers Pvt. Ltd.
> +919871324006
>


-- 
Thanks & Regards

B Jagannath
Keen & Able Computers Pvt. Ltd.
+919871324006


From squid3 at treenet.co.nz  Sat May  2 05:37:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 17:37:47 +1200
Subject: [squid-users] adding a header by group membership
In-Reply-To: <CAEnCSG5_0RBGQ4NFA-6=gAodBvdQ9MXkiFt8JF2VD_cUr0nBvQ@mail.gmail.com>
References: <CAEnCSG5_0RBGQ4NFA-6=gAodBvdQ9MXkiFt8JF2VD_cUr0nBvQ@mail.gmail.com>
Message-ID: <554462AB.3030406@treenet.co.nz>

On 2/05/2015 3:12 p.m., Michael Pelletier wrote:
> Hello,
> 
> I wish to modify a request header if the user is a member of a group. The
> example below I am trying to restrict people at work to ONLY the work email
> address UNLESS they are in the group "FullEmailAccess". Is this correct?

Does it work ?

Amos


From squid3 at treenet.co.nz  Sat May  2 05:37:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 02 May 2015 17:37:07 +1200
Subject: [squid-users] NTLM AUTH: All redirector processes are busy
In-Reply-To: <CA+8bHvxnGAUd5WS=9j5=M41BxnPgJa2me4AwDUw+4_+PyBFacg@mail.gmail.com>
References: <CA+8bHvza9tLGaHk=hCcY8+NzOcKFBDrZJzzpzT__7LTiGkpPgg@mail.gmail.com>
 <CA+8bHvzxBf2WnXNvYz=jRK5RhacPdj840n3ye_Ze6Xu1Lh23GQ@mail.gmail.com>
 <CA+8bHvxnGAUd5WS=9j5=M41BxnPgJa2me4AwDUw+4_+PyBFacg@mail.gmail.com>
Message-ID: <55446283.8020705@treenet.co.nz>

On 2/05/2015 3:45 p.m., Jagannath Naidu wrote:
> ...... ???

It means exactly what it says.
"
2015/04/29 10:34:10| WARNING: All redirector processes are busy.
2015/04/29 10:34:10| WARNING: 15 pending requests queued
"

Notice how "NTLM" is not "redirector".


N=5 redirectors running, R=15 requests queued in a queue that is only
allowed to grow to R <= 2xN length.


If you have such a high load that 1500 NTLM authenticators are
necessary, its a bit odd that 150 winbind helpers are sufficient to cope
with the matching group lookups. 5 SG helpers are obviously not enough
for the total traffic either way.



Amos



From chris9 at cpalmer.me.uk  Sat May  2 11:07:13 2015
From: chris9 at cpalmer.me.uk (Chris Palmer)
Date: Sat, 2 May 2015 12:07:13 +0100
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
Message-ID: <4d032c7eb0e7e4d04a3583b16bca73ff.squirrel@cpalmer.me.uk>

I just built 3.5.4 and deployed (on FC21). Most pages work, but SSL to
e.g. Google and Yahoo fail. It is easily provoked by simply using the
search bar in firefox or IE.

Cache.log contains entries such as

2015/05/02 11:51:34 kid1| local=[::] remote=[2a00:1450:400c:c05::93]:443
FD 13 flags=1: read/write failure: (107) Transport endpoint is not
connected

Most SSL sites are ok, and all non-SSL sites I have tried. I am not using
SSL-Bump.

It was built using eactly the same options as 3.5.3. Anyone else
experiencing this? Otherwise I will have to dig deeper...

Many thanks
Chris


From michael.pelletier at palmbeachschools.org  Sat May  2 17:41:23 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Sat, 2 May 2015 13:41:23 -0400
Subject: [squid-users] adding a header by group membership
In-Reply-To: <554462AB.3030406@treenet.co.nz>
References: <CAEnCSG5_0RBGQ4NFA-6=gAodBvdQ9MXkiFt8JF2VD_cUr0nBvQ@mail.gmail.com>
 <554462AB.3030406@treenet.co.nz>
Message-ID: <CAEnCSG7m6kca=bfgUiqYBD2vT7JzWTHGEqdV7NWARy06U7iPHQ@mail.gmail.com>

It does not work as the group acl is of type "slow" while header
modification is of type "fast".

I am looking at ECap to do the modification if the user is in a user group.
Does this sound like I am going down the right path?

Does anyone know of a good example of doing header add\mods with ECap when
a user has matched a user group?

Michael

On Sat, May 2, 2015 at 1:37 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 2/05/2015 3:12 p.m., Michael Pelletier wrote:
> > Hello,
> >
> > I wish to modify a request header if the user is a member of a group. The
> > example below I am trying to restrict people at work to ONLY the work
> email
> > address UNLESS they are in the group "FullEmailAccess". Is this correct?
>
> Does it work ?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/858a0393/attachment.htm>

From o.calvano at gmail.com  Sat May  2 20:39:07 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sat, 2 May 2015 22:39:07 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
Message-ID: <CAJajPedKm1gPiDmR9Aa4wE=mHzkU4HJqfJyeTP0UtQ03k=jZ-w@mail.gmail.com>

Anyone ?

Le samedi 2 mai 2015, Olivier CALVANO <o.calvano at gmail.com> a ?crit :

> Hi
>
> I request your help because i want use NTLM/Kerberos for authenticate my
> user.
>
> For NTLM, i use Winbind, no problems,
>
> [root at gw]# wbinfo -t
> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>
> but for Kerberos, i can't create the .keytab
>
>
> [root at gw]# kinit MYUSERNAME
> Password for MYUSERNAME at MYADDOMAIN.FR
> <javascript:_e(%7B%7D,'cvml','MYUSERNAME at MYADDOMAIN.FR');>:
>
> [root at gw]# klist
> Ticket cache: KEYRING:persistent:0:0
> Default principal: MYUSERNAME at MYADDOMAIN.FR
> <javascript:_e(%7B%7D,'cvml','MYUSERNAME at MYADDOMAIN.FR');>
>
> Valid starting       Expires              Service principal
> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
> MYADDOMAIN.FR at MYADDOMAIN.FR
> <javascript:_e(%7B%7D,'cvml','MYADDOMAIN.FR at MYADDOMAIN.FR');>
>         renew until 09/05/2015 04:51:07
>
> MYUSERNAME is the same account that i join the domain (net join) with
> winbind
>
>
> after, i put:
>
> msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
> -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>
> and i have a error:
>
> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
> --computer-name OPHTCYSRV1V4-K --upn HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>  -- init_password: Wiping the computer password structure
>  -- generate_new_password: Generating a new, random password for the
> computer account
>  -- generate_new_password:  Characters read from /dev/udandom = 84
>  -- create_fake_krb5_conf: Created a fake krb5.conf file:
> /tmp/.msktkrb5.conf-jnxTuG
>  -- reload: Reloading Kerberos Context
>  -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
>  -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
> from local keytab...
>  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
>  -- try_machine_keytab_princ: Authentication with keytab failed
>  -- try_machine_keytab_princ: Trying to authenticate for host/
> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
>  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
>  -- try_machine_keytab_princ: Authentication with keytab failed
>  -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
> password.
>  -- create_default_machine_password: Default machine password for
> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
>  -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
> not found in Kerberos database)
>  -- try_machine_password: Authentication with password failed
>  -- try_user_creds: Checking if default ticket cache has tickets...
>  -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
> cache found)
>  -- try_user_creds: User ticket cache was not valid.
> Error: could not find any credentials to authenticate with. Neither keytab,
>      default machine password, nor calling user's tickets worked. Try
>      "kinit"ing yourself some tickets with permission to create computer
>      objects, or pre-creating the computer object in AD and selecting
>      'reset account'.
>  -- ~KRB5Context: Destroying Kerberos Context
>
>
>
> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
> ophtcysrv1v4.myaddomain.fr
>
>
> anyone know the origin of this error ?
>
> thanks
> Olivier
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/e869d0c9/attachment.htm>

From michael.pelletier at palmbeachschools.org  Sat May  2 21:38:21 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Sat, 2 May 2015 17:38:21 -0400
Subject: [squid-users] Looking for a good tutorial for writing a custom eCap
	filter
Message-ID: <CAEnCSG7Ygz6f_OXmXcg8xWo7S0+PVp6kTVq_aKhskAisx42UiA@mail.gmail.com>

Hello,

I wish to write a custom eCap filter and I am looking for some
documentation.Basically, I wish to add the X-GoogApps-Allowed-Domains ONLY
when a user matches an AD group else no header should be added. We are a
school and we restrict students' email but not employees.

I tried request_header_add but you can not mix a "fast" acl with a "slow"
acl (AD Group membership acl).

Thanks in advance,
Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/d8d4f6ef/attachment.htm>

From rafael.akchurin at diladele.com  Sat May  2 21:42:38 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 2 May 2015 21:42:38 +0000
Subject: [squid-users] Looking for a good tutorial for writing a custom
 eCap	filter
In-Reply-To: <CAEnCSG7Ygz6f_OXmXcg8xWo7S0+PVp6kTVq_aKhskAisx42UiA@mail.gmail.com>
References: <CAEnCSG7Ygz6f_OXmXcg8xWo7S0+PVp6kTVq_aKhskAisx42UiA@mail.gmail.com>
Message-ID: <DB5PR04MB11285000EE57D602F050FAF18FD40@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hello Michael,

Are you interested in writing of eCAP filter (or C++ in general) or wish to solve your problem? If first please see http://www.e-cap.org/ especially adapter or clamav samples at http://www.e-cap.org/Downloads.

If latter please take a look at qlproxy (ICAP filter for Squid) the version 4.1 can do exactly what you require (non foss!)

Best regards,
Rafael

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Michael Pelletier
Sent: Saturday, May 2, 2015 11:38 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Looking for a good tutorial for writing a custom eCap filter

Hello,
I wish to write a custom eCap filter and I am looking for some documentation.Basically, I wish to add the X-GoogApps-Allowed-Domains ONLY when a user matches an AD group else no header should be added. We are a school and we restrict students' email but not employees.
I tried request_header_add but you can not mix a "fast" acl with a "slow" acl (AD Group membership acl).

Thanks in advance,
Michael

Disclaimer: Under Florida law, e-mail addresses are public records. If you do not want your e-mail address released in response to a public records request, do not send electronic mail to this entity. Instead, contact this office by phone or in writing.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/d85ff3c0/attachment.htm>

From huaraz at moeller.plus.com  Sat May  2 22:25:27 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 2 May 2015 23:25:27 +0100
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
Message-ID: <mi3isu$fof$1@ger.gmane.org>

Which OS and Kerberos version do you have ?  There might be some issue with the cache used KEYRING:persistent:0:0

Markus

"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com...
Hi


I request your help because i want use NTLM/Kerberos for authenticate my user.


For NTLM, i use Winbind, no problems, 

[root at gw]# wbinfo -t
checking the trust secret for domain MYADDOMAIN via RPC calls succeeded


but for Kerberos, i can't create the .keytab


[root at gw]# kinit MYUSERNAME
Password for MYUSERNAME at MYADDOMAIN.FR:

[root at gw]# klist
Ticket cache: KEYRING:persistent:0:0
Default principal: MYUSERNAME at MYADDOMAIN.FR

Valid starting       Expires              Service principal
02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/MYADDOMAIN.FR at MYADDOMAIN.FR
        renew until 09/05/2015 04:51:07


MYUSERNAME is the same account that i join the domain (net join) with winbind



after, i put:

msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose


and i have a error:

[root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
-- init_password: Wiping the computer password structure
-- generate_new_password: Generating a new, random password for the computer account
-- generate_new_password:  Characters read from /dev/udandom = 84
-- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-jnxTuG
-- reload: Reloading Kerberos Context
-- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
-- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_keytab_princ: Trying to authenticate for host/gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
-- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
-- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_password: Authentication with password failed
-- try_user_creds: Checking if default ticket cache has tickets...
-- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
-- try_user_creds: User ticket cache was not valid.
Error: could not find any credentials to authenticate with. Neither keytab,
     default machine password, nor calling user's tickets worked. Try
     "kinit"ing yourself some tickets with permission to create computer
     objects, or pre-creating the computer object in AD and selecting
     'reset account'.
-- ~KRB5Context: Destroying Kerberos Context




same error if i change gw.srv1-v4.tcy.myinternetdomain.org to ophtcysrv1v4.myaddomain.fr



anyone know the origin of this error ?


thanks

Olivier





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150502/db4ae659/attachment.htm>

From hack.back at hotmail.com  Sat May  2 22:58:26 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 2 May 2015 15:58:26 -0700 (PDT)
Subject: [squid-users] FATAL: xcalloc: Unable to allocate
 18446744073468065319 blocks of 1 bytes!
In-Reply-To: <554369C2.3060009@treenet.co.nz>
References: <1430474730358-4671004.post@n4.nabble.com>
 <1430475126431-4671006.post@n4.nabble.com> <554369C2.3060009@treenet.co.nz>
Message-ID: <1430607506399-4671040.post@n4.nabble.com>

Hmmmm
no sir , i dont want archive the entire Internet on one server, but i have
hdd with 2T and i can use only 0.8T from it ..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FATAL-xcalloc-Unable-to-allocate-18446744073468065319-blocks-of-1-bytes-tp4671004p4671040.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sat May  2 22:59:53 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 2 May 2015 15:59:53 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <55437A12.9080605@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430474982238-4671005.post@n4.nabble.com>
 <55437A12.9080605@treenet.co.nz>
Message-ID: <1430607593915-4671041.post@n4.nabble.com>

Thanks you amos for giving time,
but about this part :

####for looping 302 on youtube
acl text-html rep_mime_type text/html
acl http302 http_status 302

store_miss deny text-html
store_miss deny http302
send_hit deny text-html
send_hit deny http302

i use this config with patch file to make youtube not making loop 302 and
then videos will not open and give tv old screen with error accrued ,



--- src/client_side_request.cc  2014-03-09 06:40:56.000000000 -0300
+++ src/client_side_request.cc  2014-04-21 02:53:11.277155130 -0300
@@ -545,6 +545,16 @@
             }
             debugs(85, 3, HERE << "validate IP " << clientConn->local << "
non-match from Host: IP " << ia->in_addrs[i]);
         }
+ 
+        if (true) {
+            unsigned short port = clientConn->local.port();
+            debugs(85, 3, HERE << "[anti-forgery] Host-non-matched remote
IP (" << clientConn->local << ") was replaced with the first Host resolved
IP (" << ia->in_addrs[0] << ":" << clientConn->local.port() << ")");
+            clientConn->local = ia->in_addrs[0];
+            clientConn->local.port(port);
+            http->request->flags.hostVerified = true;
+            http->doCallouts();
+            return;
+        }
     }
     debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
possible from Host:");
     hostHeaderVerifyFailed("local IP", "any domain IP");


--- src/Server.cc
+++ src/Server.cc
@@ -31,6 +31,7 @@
  */
 
 #include "squid.h"
+#include "acl/FilledChecklist.h"
 #include "acl/Gadgets.h"
 #include "base/TextException.h"
 #include "comm/Connection.h"
@@ -174,6 +175,8 @@
     // give entry the reply because haveParsedReplyHeaders() expects it
there
     entry->replaceHttpReply(theFinalReply, false); // but do not write yet
     haveParsedReplyHeaders(); // update the entry/reply (e.g., set
timestamps)
+    if (EBIT_TEST(entry->flags, ENTRY_CACHABLE) && blockCaching())
+        entry->release();
     entry->startWriting(); // write the updated entry to store
 
     return theFinalReply;
@@ -533,6 +536,24 @@
     currentOffset = partial ? theFinalReply->content_range->spec.offset :
0;
 }
 
+/// whether to prevent caching of an otherwise cachable response
+bool
+ServerStateData::blockCaching()
+{
+    if (const Acl::Tree *acl = Config.accessList.storeMiss) {
+        // This relatively expensive check is not in
StoreEntry::checkCachable:
+        // That method lacks HttpRequest and may be called too many times.
+        ACLFilledChecklist ch(acl, originalRequest(), NULL);
+        ch.reply = const_cast<HttpReply*>(entry->getReply()); //
ACLFilledChecklist API bug
+        HTTPMSGLOCK(ch.reply);
+        if (ch.fastCheck() != ACCESS_ALLOWED) { // when in doubt, block
+            debugs(20, 3, "store_miss prohibits caching");
+            return true;
+        }
+    }
+    return false;
+}
+
 HttpRequest *
 ServerStateData::originalRequest()
 {
--- src/Server.h
+++ src/Server.h
@@ -131,6 +131,8 @@
     /// Entry-dependent callbacks use this check to quit if the entry went
bad
     bool abortOnBadEntry(const char *abortReason);
 
+    bool blockCaching();
+
 #if USE_ADAPTATION
     void startAdaptation(const Adaptation::ServiceGroupPointer &group,
HttpRequest *cause);
     void adaptVirginReplyBody(const char *buf, ssize_t len);
--- src/SquidConfig.h
+++ src/SquidConfig.h
@@ -375,6 +375,8 @@
         acl_access *AlwaysDirect;
         acl_access *ASlists;
         acl_access *noCache;
+        acl_access *sendHit;
+        acl_access *storeMiss;
         acl_access *stats_collection;
 #if SQUID_SNMP
 
--- src/cf.data.pre
+++ src/cf.data.pre
@@ -4843,18 +4843,97 @@
 NAME: cache no_cache
 TYPE: acl_access
 DEFAULT: none
-DEFAULT_DOC: Allow caching, unless rules exist in squid.conf.
+DEFAULT_DOC: By default, this directive is unused and has no effect.
 LOC: Config.accessList.noCache
 DOC_START
-	A list of ACL elements which, if matched and denied, cause the request to
-	not be satisfied from the cache and the reply to not be cached.
-	In other words, use this to force certain objects to never be cached.
-
-	You must use the words 'allow' or 'deny' to indicate whether items
-	matching the ACL should be allowed or denied into the cache.
+	Requests denied by this directive will not be served from the cache
+	and their responses will not be stored in the cache. This directive
+	has no effect on other transactions and on already cached responses.
 
 	This clause supports both fast and slow acl types.
 	See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	This and the two other similar caching directives listed below are
+	checked at different transaction processing stages, have different
+	access to response information, affect different cache operations,
+	and differ in slow ACLs support:
+
+	* cache: Checked before Squid makes a hit/miss determination.
+		No access to reply information!
+		Denies both serving a hit and storing a miss.
+		Supports both fast and slow ACLs.
+	* send_hit: Checked after a hit was detected.
+		Has access to reply (hit) information.
+		Denies serving a hit only.
+		Supports fast ACLs only.
+	* store_miss: Checked before storing a cachable miss.
+		Has access to reply (miss) information.
+		Denies storing a miss only.
+		Supports fast ACLs only.
+
+	If you are not sure which of the three directives to use, apply the
+	following decision logic:
+
+	* If your ACL(s) are of slow type _and_ need response info, redesign.
+	  Squid does not support that particular combination at this time.
+        Otherwise:
+	* If your directive ACL(s) are of slow type, use "cache"; and/or
+	* if your directive ACL(s) need no response info, use "cache".
+        Otherwise:
+	* If you do not want the response cached, use store_miss; and/or
+	* if you do not want a hit on a cached response, use send_hit.
+DOC_END
+
+NAME: send_hit
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.sendHit
+DOC_START
+	Responses denied by this directive will not be served from the cache
+	(but may still be cached, see store_miss). This directive has no
+	effect on the responses it allows and on the cached objects.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives.
+
+	Unlike the "cache" directive, send_hit only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	For example:
+
+		# apply custom Store ID mapping to some URLs
+		acl MapMe dstdomain .c.example.com
+		store_id_program ...
+		store_id_access allow MapMe
+
+		# but prevent caching of special responses
+		# such as 302 redirects that cause StoreID loops
+		acl Ordinary http_status 200-299
+		store_miss deny MapMe !Ordinary
+
+		# and do not serve any previously stored special responses
+		# from the cache (in case they were already cached before
+		# the above store_miss rule was in effect).
+		send_hit deny MapMe !Ordinary
+DOC_END
+
+NAME: store_miss
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.storeMiss
+DOC_START
+	Responses denied by this directive will not be cached (but may still
+	be served from the cache, see send_hit). This directive has no
+	effect on the responses it allows and on the already cached responses.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives. See the
+	send_hit directive for a usage example.
+
+	Unlike the "cache" directive, store_miss only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
 DOC_END
 
 NAME: max_stale
--- src/client_side_reply.cc
+++ src/client_side_reply.cc
@@ -545,6 +545,11 @@
        ) {
         http->logType = LOG_TCP_NEGATIVE_HIT;
         sendMoreData(result);
+    } else if (blockedHit()) {
+        debugs(88, 5, "send_hit forces a MISS");
+        http->logType = LOG_TCP_MISS;
+        processMiss();
+        return;
     } else if (!http->flags.internal && refreshCheckHTTP(e, r)) {
         debugs(88, 5, "clientCacheHit: in refreshCheck() block");
         /*
@@ -773,6 +778,30 @@
     }
 }
 
+/// whether squid.conf send_hit prevents us from serving this hit
+bool
+clientReplyContext::blockedHit() const
+{
+    if (!Config.accessList.sendHit)
+        return false; // hits are not blocked by default
+
+    if (http->flags.internal)
+        return false; // internal content "hits" cannot be blocked
+
+    if (const HttpReply *rep = http->storeEntry()->getReply()) {
+        std::auto_ptr<ACLFilledChecklist>
chl(clientAclChecklistCreate(Config.accessList.sendHit, http));
+        chl->reply = const_cast<HttpReply*>(rep); // ACLChecklist API bug
+        HTTPMSGLOCK(chl->reply);
+        return chl->fastCheck() != ACCESS_ALLOWED; // when in doubt, block
+    }
+
+    // This does not happen, I hope, because we are called from CacheHit,
which
+    // is called via a storeClientCopy() callback, and store should
initialize
+    // the reply before calling that callback.
+    debugs(88, 3, "Missing reply!");
+    return false;
+}
+
 void
 clientReplyContext::purgeRequestFindObjectToPurge()
 {
--- src/client_side_reply.h
+++ src/client_side_reply.h
@@ -140,6 +140,7 @@
     void triggerInitialStoreRead();
     void sendClientOldEntry();
     void purgeAllCached();
+    bool blockedHit() const;
 
     void sendBodyTooLargeError();
     void sendPreconditionFailedError();





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671041.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Sat May  2 23:10:58 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sat, 2 May 2015 16:10:58 -0700 (PDT)
Subject: [squid-users] about Incorrect X509 server certificate valdidation
Message-ID: <1430608258747-4671042.post@n4.nabble.com>

You mention this part :
Severity:

 The bug is important because it allows remote servers to bypass
 client certificate validation. Some attackers may also be able
 to use valid certificates for one domain signed by a global
 Certificate Authority to abuse an unrelated domain. 


you mean that there is a way to use certificate that signed by a global
certificate authority (Trusted CA) ?
if yes then we can use it and then no need to import our self certificate in
client browser to force it as trusted ?
Thanks.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/about-Incorrect-X509-server-certificate-valdidation-tp4671042.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From spaceman4445 at gmail.com  Sun May  3 01:00:00 2015
From: spaceman4445 at gmail.com (vintech)
Date: Sat, 2 May 2015 18:00:00 -0700 (PDT)
Subject: [squid-users] Getting timeout 301 with GET command using cURL.
Message-ID: <1430614800792-4671043.post@n4.nabble.com>

Hi,

Iam running squid 3.5.2 stable , however i have problems while i connect it
through GET as it gives me an HTTP 301 error.
Also while i use the squid with browser iam able to open and connect to the
destination but with get and cURl i am having issues and it gives timed out
error.
Also using bind for DNS so no problems in resolving also. Also iam using
random ports which are NAT to default ports. 


Here's the debug info:

----------

* About to connect() to proxy x.x.186.209 port 49539 (#0)
* Trying x.x.186.209... * Connected to x.x.186.209 (x.x.186.209) port 49539
(#0)
* Proxy auth using Basic with user 'cntservers'
> GET http://www.bookbyte.com/buyback2.aspx?isbns=0470405449 HTTP/1.1
Proxy-Authorization: Basic Y250c2VydmVyczo=
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:28.0) Gecko/20100101
Firefox/28.0
Host: www.bookbyte.com
Accept: */*
Proxy-Connection: Keep-Alive

< HTTP/1.1 301 Moved Permanently
< Server: CloudFront
< Date: Sat, 02 May 2015 02:20:17 GMT
< Content-Type: text/html
< Content-Length: 183
< Location: https://www.bookbyte.com/buyback2.aspx?isbns=0470405449
< X-Cache: Redirect from cloudfront
< X-Amz-Cf-Id: C3MVSCo5QoyoZa09U48QHd54SiP7UEigcQkujqNETLT5eMwJXWnunw==
< X-Cache: MISS from JDar
< X-Cache-Lookup: MISS from JDar:11000
< Via: 1.1 2eaad1ad7617abb10fd0dd05b1db7182.cloudfront.net (CloudFront), 1.1
JDar (squid/3.5.2)
< Connection: keep-alive
< 
* Ignoring the response-body
* Closing connection #0
* Issue another request to this URL:
'https://www.bookbyte.com/buyback2.aspx?isbns=0470405449'
* About to connect() to proxy x.x.186.209 port 49539 (#0)
* Trying x.x.186.209... * Connected to x.x.186.209 (x.x.186.209) port 49539
(#0)
* Establish HTTP proxy tunnel to www.bookbyte.com:443
* Proxy auth using Basic with user 'ctserve'
> CONNECT www.bookbyte.com:443 HTTP/1.1
Host: www.bookbyte.com:443
Proxy-Authorization: Basic Y250c2VydmVyczo=
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:28.0) Gecko/20100101
Firefox/28.0
Proxy-Connection: Keep-Alive

< HTTP/1.1 200 Connection established
< 
* Proxy replied OK to CONNECT request
* successfully set certificate verify locations:
* CAfile: none
CApath: /etc/ssl/certs
* Connected to x.x.186.209 (x.x.186.209) port 49539 (#0)
* SSL connection using AES256-SHA
* Server certificate:
* subject: OU=GT37244111; OU=See www.rapidssl.com/resources/cps (c)14;
OU=Domain Control Validated - RapidSSL(R); CN=*.bookbyte.com
* start date: 2015-01-29 02:59:29 GMT
* expire date: 2016-07-12 13:16:46 GMT
* subjectAltName: www.bookbyte.com matched
* issuer: C=US; O=GeoTrust Inc.; CN=RapidSSL SHA256 CA - G3
* SSL certificate verify ok.
> GET /buyback2.aspx?isbns=0470405449 HTTP/1.1
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:28.0) Gecko/20100101
Firefox/28.0
Host: www.bookbyte.com
Accept: */*

* Operation timed out after 8000 milliseconds with 0 bytes received
* Closing connection #0


------------------


Following in the squid config:

#
# Recommended minimum configuration:
#
cache_effective_user squid
cache_effective_group squid
visible_hostname JDar
######################################
####################################
aclsquid_localmyip x.x.199.254
tcp_outgoing_address x.x.199.254 squid_local
####################
# Anynymous
include /etc/proxy/anonymous.conf
##################
include /etc/proxy/outgoing.conf
####################################
http_port 3128
http_port 3129
http_port 3130
http_port 3131
###################################
# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acllocalnetsrc 10.0.0.0/8     # RFC1918 possible internal network
acllocalnetsrc 172.16.0.0/12  # RFC1918 possible internal network
acllocalnetsrc 192.168.0.0/16 # RFC1918 possible internal network
acllocalnetsrc fc00::/7       # RFC 4193 local private network range
acllocalnetsrc fe80::/10      # RFC 4291 link-local (directly plugged)
machines

aclSSL_ports port 443
aclSafe_ports port 80          # http
aclSafe_ports port 21          # ftp
aclSafe_ports port 443         # https
aclSafe_ports port 70          # gopher
aclSafe_ports port 210         # wais
aclSafe_ports port 1025-65535  # unregistered ports
aclSafe_ports port 280         # http-mgmt
aclSafe_ports port 488         # gss-http
aclSafe_ports port 591         # filemaker
aclSafe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access allow all

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dirufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
####################################
dns_v4_first on 
cache_mem 2 GB
workers 8
#m8ximum_object_size_in_memory 10 M
strip_query_terms off
fqdncache_size 65535
memory_replacement_policy heap GDSF
cache_replacement_policy heap GDSF
dns_nameservers 127.0.0.1
client_dst_passthru off
host_verify_strict off
range_offset_limit -1 
quick_abort_min -1
##read_ahead_gap 128 KB
logfile_rotate 1
max_filedescriptors 65535
######################################
memory_pools off
pconn_timeout 2 minutes
persistent_request_timeout 1 minute

-----------------------------------------------------

Please suggest!

Thanks!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Getting-timeout-301-with-GET-command-using-cURL-tp4671043.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun May  3 04:05:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 03 May 2015 16:05:16 +1200
Subject: [squid-users] adding a header by group membership
In-Reply-To: <CAEnCSG7m6kca=bfgUiqYBD2vT7JzWTHGEqdV7NWARy06U7iPHQ@mail.gmail.com>
References: <CAEnCSG5_0RBGQ4NFA-6=gAodBvdQ9MXkiFt8JF2VD_cUr0nBvQ@mail.gmail.com>	<554462AB.3030406@treenet.co.nz>
 <CAEnCSG7m6kca=bfgUiqYBD2vT7JzWTHGEqdV7NWARy06U7iPHQ@mail.gmail.com>
Message-ID: <55459E7C.4070108@treenet.co.nz>

On 3/05/2015 5:41 a.m., Michael Pelletier wrote:
> It does not work as the group acl is of type "slow" while header
> modification is of type "fast".

Corrrect, and that is the answer to your question.

If you have a new enough Squid, use a auth/group check in http_access
and a note ACL check in the header manipulation code.

You may have to write a wrapper aronud the group helper to send back to
Squid the "group=X" kv-pair for note ACL to work with.

Amos



From squid3 at treenet.co.nz  Sun May  3 04:26:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 03 May 2015 16:26:36 +1200
Subject: [squid-users] Getting timeout 301 with GET command using cURL.
In-Reply-To: <1430614800792-4671043.post@n4.nabble.com>
References: <1430614800792-4671043.post@n4.nabble.com>
Message-ID: <5545A37C.8070007@treenet.co.nz>

On 3/05/2015 1:00 p.m., vintech wrote:
> Hi,
> 
> Iam running squid 3.5.2 stable

Please upgade to the current version, which is 3.5.4 today.


> , however i have problems while i connect it
> through GET as it gives me an HTTP 301 error.

There is no such thing as a "301 error".

*CoudFront* are telling your browser the http:// URL you are trying to
has been converted to a https:// location. That is all, no error.


Later on when trying to fetch the HTTPS the connection hangs. That is a
problem, but there is no info in your mail that would help track it
down. Squid is apparently setting up TCP tunnel correctly , and the TLS
handshake also works across it.

I suspect ICMP packets being lost or dropped breaking PMTUD, maybe ECN,
or Window Scaling. Maybe just an error in CloudFront.



> Also while i use the squid with browser iam able to open and connect to the
> destination but with get and cURl i am having issues and it gives timed out
> error.
> Also using bind for DNS so no problems in resolving also. Also iam using
> random ports which are NAT to default ports. 

Not a good idea. It maybe (or not) related to this problem.

Amos



From squid3 at treenet.co.nz  Sun May  3 04:59:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 03 May 2015 16:59:56 +1200
Subject: [squid-users] about Incorrect X509 server certificate
	valdidation
In-Reply-To: <1430608258747-4671042.post@n4.nabble.com>
References: <1430608258747-4671042.post@n4.nabble.com>
Message-ID: <5545AB4C.6020201@treenet.co.nz>

On 3/05/2015 11:10 a.m., HackXBack wrote:
> You mention this part :
> Severity:
> 
>  The bug is important because it allows remote servers to bypass
>  client certificate validation. Some attackers may also be able
>  to use valid certificates for one domain signed by a global
>  Certificate Authority to abuse an unrelated domain. 
> 
> 
> you mean that there is a way to use certificate that signed by a global
> certificate authority (Trusted CA) ?

There was a possible way for some certificates which would also be
abusing this bug to pass the global CA checks they do before signing.


> if yes then we can use it and then no need to import our self certificate in
> client browser to force it as trusted ?

No. The vulnerability was attack traffic having the attackers
certificate removed and re-encrypted using *yours*. The clients always
have to trust your certificate.

You cannot use one of the attacker-type certificates in Squid because a)
they are not CA signing certificates, and b) they are "broken" in ways
that clients should already validate against. That is why server-first
mode is not vulnerable when client-first is. In server-first mode the
breakage gets mimic'd and the client rejects the certificate (not Squid).

Amos



From squid3 at treenet.co.nz  Sun May  3 05:22:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 03 May 2015 17:22:45 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430607593915-4671041.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430474982238-4671005.post@n4.nabble.com>
 <55437A12.9080605@treenet.co.nz> <1430607593915-4671041.post@n4.nabble.com>
Message-ID: <5545B0A5.9070304@treenet.co.nz>

On 3/05/2015 10:59 a.m., HackXBack wrote:
> Thanks you amos for giving time,
> but about this part :
> 
> ####for looping 302 on youtube
> acl text-html rep_mime_type text/html
> acl http302 http_status 302
> 
> store_miss deny text-html
> store_miss deny http302
> send_hit deny text-html
> send_hit deny http302
> 
> i use this config with patch file to make youtube not making loop 302 and
> then videos will not open and give tv old screen with error accrued ,
> 

My comment was only about the single line:

 "store_miss deny text-html"


The oddness is thusly:


When forcing almost everything to cache you notice that many pages dont
show up right, dynamic ones dont always work right, or content is not
being updated properly when it should.

Your solution then is obviously to prevent forcing of just the text/html
parts. Is *seems* the right thing to do - but isn't.


In doing that you overlook the fact that the text/html part is what
makes the other non-HTML parts so "dynamic". Every single page load of
the text/html part may be pointing to different scripts, images, videos
etc causing the ones already in your cache to be ignored and the new
ones fetched. This is a drag on your cache now having to track N copies
of what the page *used* to look like, but those images/videos/etc may
never again be used.

What we call the "cache control" headers (date, cache-control, expires,
last-modified, etag etc) are not what makes the content dynamic, they
are just indications by the site author / server of how long the HTML
parts of the site are going to be making use of those objects. The
dynamic bit is usually the HTML and how it references things.


Store-ID gets around that a little bit by telling your cache that *you
think* certain objects are going to be used under multiple names with
some pattern regex can match _reliably_.

The *right* thing to be doing is to let sites determine their own
caching and only use the forced caching for specific sites (and if
possible specific URLs) which are found to be broken.

Amos


From o.calvano at gmail.com  Sun May  3 05:30:14 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 3 May 2015 07:30:14 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <mi3isu$fof$1@ger.gmane.org>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
Message-ID: <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>

Hi


Thanks for your answer

CentOS Linux release 7.1.1503 (Core)

krb5-workstation-1.12.2-14.el7.x86_64
krb5-libs-1.12.2-14.el7.x86_64

regards
olivier


2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

>   Which OS and Kerberos version do you have ?  There might be some issue
> with the cache used KEYRING:persistent:0:0
> Markus
>
>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
> news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com...
>     Hi
>
> I request your help because i want use NTLM/Kerberos for authenticate my
> user.
>
> For NTLM, i use Winbind, no problems,
>
> [root at gw]# wbinfo -t
> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>
> but for Kerberos, i can't create the .keytab
>
>
> [root at gw]# kinit MYUSERNAME
> Password for MYUSERNAME at MYADDOMAIN.FR:
>
> [root at gw]# klist
> Ticket cache: KEYRING:persistent:0:0
> Default principal: MYUSERNAME at MYADDOMAIN.FR
>
> Valid starting       Expires              Service principal
> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
> MYADDOMAIN.FR at MYADDOMAIN.FR
>         renew until 09/05/2015 04:51:07
>
> MYUSERNAME is the same account that i join the domain (net join) with
> winbind
>
>
> after, i put:
>
> msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
> -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>
> and i have a error:
>
> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
> --computer-name OPHTCYSRV1V4-K --upn HTTP/
> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
> -- init_password: Wiping the computer password structure
> -- generate_new_password: Generating a new, random password for the
> computer account
> -- generate_new_password:  Characters read from /dev/udandom = 84
> -- create_fake_krb5_conf: Created a fake krb5.conf file:
> /tmp/.msktkrb5.conf-jnxTuG
> -- reload: Reloading Kerberos Context
> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
> from local keytab...
> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
> -- try_machine_keytab_princ: Authentication with keytab failed
> -- try_machine_keytab_princ: Trying to authenticate for host/
> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
> -- try_machine_keytab_princ: Authentication with keytab failed
> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
> password.
> -- create_default_machine_password: Default machine password for
> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
> -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
> not found in Kerberos database)
> -- try_machine_password: Authentication with password failed
> -- try_user_creds: Checking if default ticket cache has tickets...
> -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
> cache found)
> -- try_user_creds: User ticket cache was not valid.
> Error: could not find any credentials to authenticate with. Neither keytab,
>      default machine password, nor calling user's tickets worked. Try
>      "kinit"ing yourself some tickets with permission to create computer
>      objects, or pre-creating the computer object in AD and selecting
>      'reset account'.
> -- ~KRB5Context: Destroying Kerberos Context
>
>
>
> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
> ophtcysrv1v4.myaddomain.fr
>
>
> anyone know the origin of this error ?
>
> thanks
> Olivier
>
>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/a2315434/attachment.htm>

From hack.back at hotmail.com  Sun May  3 13:29:16 2015
From: hack.back at hotmail.com (HackXBack)
Date: Sun, 3 May 2015 06:29:16 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <5543128E.3080402@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz>
Message-ID: <1430659756828-4671050.post@n4.nabble.com>

i upgrade to 3.4.13 and still using range_offset_limit none
making 
assertion failed: comm.cc:178: "fd_table[conn->fd].......
i think you forget to upload the patch ?
Thanks..



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671050.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From huaraz at moeller.plus.com  Sun May  3 11:25:46 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 3 May 2015 12:25:46 +0100
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
Message-ID: <mi50jt$oe8$1@ger.gmane.org>

Did you compile msktutil or is it a package in centos ? 

Markus

"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
Hi



Thanks for your answer

CentOS Linux release 7.1.1503 (Core)

krb5-workstation-1.12.2-14.el7.x86_64
krb5-libs-1.12.2-14.el7.x86_64


regards

olivier



2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

  Which OS and Kerberos version do you have ?  There might be some issue with the cache used KEYRING:persistent:0:0

  Markus

  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com...
  Hi


  I request your help because i want use NTLM/Kerberos for authenticate my user.


  For NTLM, i use Winbind, no problems, 

  [root at gw]# wbinfo -t
  checking the trust secret for domain MYADDOMAIN via RPC calls succeeded


  but for Kerberos, i can't create the .keytab


  [root at gw]# kinit MYUSERNAME
  Password for MYUSERNAME at MYADDOMAIN.FR:

  [root at gw]# klist
  Ticket cache: KEYRING:persistent:0:0
  Default principal: MYUSERNAME at MYADDOMAIN.FR

  Valid starting       Expires              Service principal
  02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/MYADDOMAIN.FR at MYADDOMAIN.FR
          renew until 09/05/2015 04:51:07


  MYUSERNAME is the same account that i join the domain (net join) with winbind



  after, i put:

  msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose


  and i have a error:

  [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
  -- init_password: Wiping the computer password structure
  -- generate_new_password: Generating a new, random password for the computer account
  -- generate_new_password:  Characters read from /dev/udandom = 84
  -- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-jnxTuG
  -- reload: Reloading Kerberos Context
  -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
  -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
  -- try_machine_keytab_princ: Authentication with keytab failed
  -- try_machine_keytab_princ: Trying to authenticate for host/gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
  -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
  -- try_machine_keytab_princ: Authentication with keytab failed
  -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
  -- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
  -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
  -- try_machine_password: Authentication with password failed
  -- try_user_creds: Checking if default ticket cache has tickets...
  -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
  -- try_user_creds: User ticket cache was not valid.
  Error: could not find any credentials to authenticate with. Neither keytab,
       default machine password, nor calling user's tickets worked. Try
       "kinit"ing yourself some tickets with permission to create computer
       objects, or pre-creating the computer object in AD and selecting
       'reset account'.
  -- ~KRB5Context: Destroying Kerberos Context




  same error if i change gw.srv1-v4.tcy.myinternetdomain.org to ophtcysrv1v4.myaddomain.fr



  anyone know the origin of this error ?


  thanks

  Olivier




------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/cf148dde/attachment.htm>

From hussam.tayeb at gmx.com  Sun May  3 12:47:17 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sun, 3 May 2015 14:47:17 +0200
Subject: [squid-users] vary headers
Message-ID: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>

Hello, how would I deny caching to replies containing Vary: User-Agent http header?

I already use:

acl hasVary rep_header Vary .
store_miss deny hasVary

but it won't block caching of "Vary: User-Agent" replies.
thank you


From hierony_milanisti at yahoo.co.id  Sun May  3 15:13:12 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Sun, 3 May 2015 15:13:12 +0000 (UTC)
Subject: [squid-users] Distributed Cache Problems
In-Reply-To: <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>
References: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
 <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>

?Hierony Manurung
Del Institute of Technology
Network Management

      Pada Minggu, 3 Mei 2015 22:10, Hierony Manurung <hierony_milanisti at yahoo.co.id> menulis:
   

 
Dear Fellow,I am implement a Distributed cache system (1 child Proxy, 2 parent proxy as sibling).
in the cache peer algorithm, I use Weighted Round-Robin.
But when I analyze the log using Squeezer, I am found that there is no hit requestin the parent proxy.

I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), and the log analyze
result.
I am very pleased that you want to lend your time to help me (Because the deadline is very close).

Thanks in advance.

?


Hierony Manurung
Del Institute of Technology
Network Management

    

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/b029a34c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: DistributedCache-Problem.zip
Type: application/octet-stream
Size: 7150 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/b029a34c/attachment.obj>

From szabados0701 at gmail.com  Sun May  3 16:49:32 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Sun, 3 May 2015 18:49:32 +0200
Subject: [squid-users] Regex difficulties
Message-ID: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>

Hi,

I'm trying to hook up these regex files for url filtering:
http://www.squidguard.org/Doc/Examples/08.expressionlist
http://www.squidguard.org/Doc/Examples/09.whiteexpression

I've created the ACL, but when I start squid, I get all these error
messages:

aclParseRegexList: Invalid regular expression [...]
Unmatched ( or \(

I've checked the regex with a regex validator, all I got is some unescaped
"/" chars, corrected the escaping, but the issue still persists.

What am I doing wrong?

Thanks,
Balazs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/cd89f62c/attachment.htm>

From chris9 at cpalmer.me.uk  Sun May  3 17:01:10 2015
From: chris9 at cpalmer.me.uk (Chris Palmer)
Date: Sun, 03 May 2015 18:01:10 +0100
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
References: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
Message-ID: <55465456.2090108@cpalmer.me.uk>

Two other reports of the same problem (accessing some SSL sites) after 
upgrading to Squid 3.5.4...

https://bugs.archlinux.org/task/44811

I'm at a bit of a loss to know where to start looking.
Just in case, I tried disabling ICAP (was using it for clamav) but no 
difference.

Chris

> Send squid-users mailing list submissions to
> Date: Sat, 2 May 2015 12:07:13 +0100
> From: "Chris Palmer" <chris9 at cpalmer.me.uk>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
> Message-ID: <4d032c7eb0e7e4d04a3583b16bca73ff.squirrel at cpalmer.me.uk>
> Content-Type: text/plain;charset=iso-8859-1
>
> I just built 3.5.4 and deployed (on FC21). Most pages work, but SSL to
> e.g. Google and Yahoo fail. It is easily provoked by simply using the
> search bar in firefox or IE.
>
> Cache.log contains entries such as
>
> 2015/05/02 11:51:34 kid1| local=[::] remote=[2a00:1450:400c:c05::93]:443
> FD 13 flags=1: read/write failure: (107) Transport endpoint is not
> connected
>
> Most SSL sites are ok, and all non-SSL sites I have tried. I am not using
> SSL-Bump.
>
> It was built using eactly the same options as 3.5.3. Anyone else
> experiencing this? Otherwise I will have to dig deeper...
>
> Many thanks
> Chris
>



From yvoinov at gmail.com  Sun May  3 17:04:18 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 03 May 2015 23:04:18 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>
Message-ID: <55465512.3090709@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
03.05.15 18:47, Hussam Al-Tayeb ?????:
> Hello, how would I deny caching to replies containing Vary: User-Agent http header?
For what?
>
>
> I already use:
>
> acl hasVary rep_header Vary .
> store_miss deny hasVary
>
> but it won't block caching of "Vary: User-Agent" replies.
> thank you
Headers has own acl.
Example:

# Strip User-Agent from Vary
request_header_access Vary deny all
request_header_replace Vary Accept-Encoding

>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRlUSAAoJENNXIZxhPexGfE8H/0UHm/CgeOFI8RCE+rJGhCf8
GuCH5y5JlzWiTI94Hr24GL7JcuY7jsv0g3L1dzu6N9VYqiS4xc+ZO7MKFzdUgs5J
UChdvRCzL9GplX4s3EHbQyYNrqGknQApEPia4bUw4wkVHIi5oKpquIJMD2WRd9Sp
WCw062opW/R9ef0hd6Gj/BmEASj3xGYMz6T5KgaYclhDmOUUvnru/A3OUZZZ+SGj
hYsZO24Jsya/SHrMvN7l1exfpeHj1/U0n+mHYmHrIyZYCcQV4XZAnMU8n/HauVhx
/tVQH380sMLNLEp+ShqjtwELKvRc3xF4YXsjcMRwXtqLN0C6R/HxDs8iThc/O1M=
=gLKe
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sun May  3 17:12:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 03 May 2015 23:12:55 +0600
Subject: [squid-users] Distributed Cache Problems
In-Reply-To: <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
References: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
 <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>
 <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55465717.5020304@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Did you observe your caches access logs in runtime? For example, with
squidview?

This report contains no meaningful info.

03.05.15 21:13, Hierony Manurung ?????:
>  Hierony Manurung
> Del Institute of Technology
> Network Management
>
>       Pada Minggu, 3 Mei 2015 22:10, Hierony Manurung
<hierony_milanisti at yahoo.co.id> menulis:
>   
>
> 
> Dear Fellow,I am implement a Distributed cache system (1 child Proxy,
2 parent proxy as sibling).
> in the cache peer algorithm, I use Weighted Round-Robin.
> But when I analyze the log using Squeezer, I am found that there is no
hit requestin the parent proxy.
>
> I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), and
the log analyze
> result.
> I am very pleased that you want to lend your time to help me (Because
the deadline is very close).
>
> Thanks in advance.
>
> 
>
>
> Hierony Manurung
> Del Institute of Technology
> Network Management
>
>    
>
>   
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRlcXAAoJENNXIZxhPexGQB0IAKdeCgDyz51VKHb8sSZHFkcr
R2/1wg46uwSHd1dAGtfrjf+5U2mxykTWnG/qsuQsYHlmrkRgc61USuCjk56sUyOW
A8OnmeGs57D4eGwlFpBaY5qnz47QqBJpeDFfqQyqAFSKgmgvbYsI4OO/HVG2xMTi
dEMdtHGNYjpU/9kRH6OClkny/nfiYSj76N4z1HvjJD4MG+uQVjwPFc0cIkec3Mz0
FyD5kKgCYqCYMqSu9dfBBih1r6DNoY1TPwdk2B1tJFQgB+z94E+nG3x8v045jEm5
jlamgXjrd1f3oBUyoxH5t9EEenmCgwZaB4TmsLQ848qSM/H6LjZGVKGDC5tiD7k=
=GDpd
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6ac91ee4/attachment.htm>

From yvoinov at gmail.com  Sun May  3 17:16:22 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 03 May 2015 23:16:22 +0600
Subject: [squid-users] Distributed Cache Problems
In-Reply-To: <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
References: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
 <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>
 <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <554657E6.1030900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
About your configs:

# Uncomment and adjust the following to add a disk cache directory.
cache_dir aufs /var/spool/squid 100 16 256

This is toy cache. Are you are serious?

cache_mem 0 KB

You completely disabled memory cache. You sure this is good idea?

maximum_object_size 16384 KB

Really? Did you see modern web objects? Simple site home page starting
from 1 Mb - without images.

Finally: You serious expects that Squid with this config will be cache any?

03.05.15 21:13, Hierony Manurung ?????:
>  Hierony Manurung
> Del Institute of Technology
> Network Management
>
>       Pada Minggu, 3 Mei 2015 22:10, Hierony Manurung
<hierony_milanisti at yahoo.co.id> menulis:
>   
>
> 
> Dear Fellow,I am implement a Distributed cache system (1 child Proxy,
2 parent proxy as sibling).
> in the cache peer algorithm, I use Weighted Round-Robin.
> But when I analyze the log using Squeezer, I am found that there is no
hit requestin the parent proxy.
>
> I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), and
the log analyze
> result.
> I am very pleased that you want to lend your time to help me (Because
the deadline is very close).
>
> Thanks in advance.
>
> 
>
>
> Hierony Manurung
> Del Institute of Technology
> Network Management
>
>    
>
>   
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRlfmAAoJENNXIZxhPexGS7AH+gNzIQj2zp6kEB+E5xb8UgfB
D8FOje6baqmbu+tBsNmUuE17/4d08GxG3N8oULPA82cZLySgI7u00uONkrmwS4rP
8xTpB+kQlQ3kxIMSxBsqAy1NiMWNEnsNM4gDzmbWQQl7uQ5Ms0KPD4CeBnZ1EJTv
kACHpSCZSJpUjIMk8vc/T8QTL+GGwUGyQRv+f8ZnnITiB5qsif+74eAx0Jtm/rBd
rRx7GcSPZ6INL6lJxOJdflVVIhj2AL2gv+CMjSjMwS4+dBpvc6480OA7iB4+l6uc
DnuR/xIvN8/IqSPWvo8RxSy3OGJcWwVbODxW0flDRPj82KWOsq9c3Y3D0dU0YSk=
=g8XE
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/2f9817a4/attachment.htm>

From o.calvano at gmail.com  Sun May  3 17:24:21 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 3 May 2015 19:24:21 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <mi50jt$oe8$1@ger.gmane.org>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
Message-ID: <CAJajPecBcrbW+jtiwF2J=ujZ4KwDTWF6oPzJF56pVz+-gfNfLw@mail.gmail.com>

Hi

i have compiled the 1.0rc version :



[root at gw msktutil-1.0rc1]# ./msktutil -c -b "CN=COMPUTERS" -s HTTP/
ophtcysrv1v4.myaddomain.fr -k /etc/squid/PROXY.keytab --computer-name
OPHTCYSRV1V4-K --upn HTTP/ophtcysrv1v4.myasdomain.fr --server
myad.myaddomain.fr --verbose --enctypes 28
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/urandom = 93
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-jPXQHu
 -- reload: Reloading Kerberos Context
 -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
 -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/
gw.srv1-v4.tcy.sodiaal.ophelys.org from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
password.
 -- create_default_machine_password: Default machine password for
OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets...
 -- finalize_exec: Authenticated using method 5
 -- LDAPConnection: Connecting to LDAP server: myad.myaddomain.fr
SASL/GSSAPI authentication started
SASL username: Myusername at MYADDOMAIN.FR
SASL SSF: 56
SASL data security layer installed.
 -- ldap_get_base_dn: Determining default LDAP base: dc=MYDOMAIN,dc=FR
 -- ldap_check_account: Checking that a computer account for
OPHTCYSRV1V4-K$ exists
 -- ldap_check_account: Computer account not found, create the account
No computer account for OPHTCYSRV1V4-K found, creating a new one.
 -- ldap_check_account_strings: Inspecting (and updating) computer account
attributes
 -- ldap_check_account_strings: Found userPrincipalName =
 -- ldap_check_account_strings: userPrincipalName should be HTTP/
ophtcysrv1v4.myaddomain.fr at MYADDOMAIN.FR
 -- ldap_set_userAccountControl_flag: Setting userAccountControl bit at
0x200000 to 0x0
 -- ldap_set_userAccountControl_flag: userAccountControl not changed 0x1000
 -- ldap_get_kvno: KVNO is 1
 -- set_password: Attempting to reset computer's password
 -- set_password: Try change password using user's ticket cache
 -- ldap_get_pwdLastSet: pwdLastSet is 130751472429170776
Error: Unable to set machine password for OPHTCYSRV1V4-K$: (3)
Authentication error
Error: set_password failed
 -- ~KRB5Context: Destroying Kerberos Context






2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

>   Did you compile msktutil or is it a package in centos ?
>
> Markus
>
>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
> news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
>    Hi
>
>
> Thanks for your answer
>
> CentOS Linux release 7.1.1503 (Core)
>
> krb5-workstation-1.12.2-14.el7.x86_64
> krb5-libs-1.12.2-14.el7.x86_64
>
> regards
> olivier
>
>
> 2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:
>
>>   Which OS and Kerberos version do you have ?  There might be some issue
>> with the cache used KEYRING:persistent:0:0
>> Markus
>>
>>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
>> news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com.
>> ..
>>      Hi
>>
>> I request your help because i want use NTLM/Kerberos for authenticate my
>> user.
>>
>> For NTLM, i use Winbind, no problems,
>>
>> [root at gw]# wbinfo -t
>> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>>
>> but for Kerberos, i can't create the .keytab
>>
>>
>> [root at gw]# kinit MYUSERNAME
>> Password for MYUSERNAME at MYADDOMAIN.FR:
>>
>> [root at gw]# klist
>> Ticket cache: KEYRING:persistent:0:0
>> Default principal: MYUSERNAME at MYADDOMAIN.FR
>>
>> Valid starting       Expires              Service principal
>> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
>> MYADDOMAIN.FR at MYADDOMAIN.FR
>>         renew until 09/05/2015 04:51:07
>>
>> MYUSERNAME is the same account that i join the domain (net join) with
>> winbind
>>
>>
>> after, i put:
>>
>> msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
>> -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>>
>> and i have a error:
>>
>> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
>> --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>> -- init_password: Wiping the computer password structure
>> -- generate_new_password: Generating a new, random password for the
>> computer account
>> -- generate_new_password:  Characters read from /dev/udandom = 84
>> -- create_fake_krb5_conf: Created a fake krb5.conf file:
>> /tmp/.msktkrb5.conf-jnxTuG
>> -- reload: Reloading Kerberos Context
>> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
>> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
>> from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_keytab_princ: Trying to authenticate for host/
>> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
>> password.
>> -- create_default_machine_password: Default machine password for
>> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
>> -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
>> not found in Kerberos database)
>> -- try_machine_password: Authentication with password failed
>> -- try_user_creds: Checking if default ticket cache has tickets...
>> -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
>> cache found)
>> -- try_user_creds: User ticket cache was not valid.
>> Error: could not find any credentials to authenticate with. Neither
>> keytab,
>>      default machine password, nor calling user's tickets worked. Try
>>      "kinit"ing yourself some tickets with permission to create computer
>>      objects, or pre-creating the computer object in AD and selecting
>>      'reset account'.
>> -- ~KRB5Context: Destroying Kerberos Context
>>
>>
>>
>> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
>> ophtcysrv1v4.myaddomain.fr
>>
>>
>> anyone know the origin of this error ?
>>
>> thanks
>> Olivier
>>
>>
>> ------------------------------
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/3e31a876/attachment.htm>

From o.calvano at gmail.com  Sun May  3 17:28:22 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 3 May 2015 19:28:22 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <mi50jt$oe8$1@ger.gmane.org>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
Message-ID: <CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ@mail.gmail.com>

hoo i have deleted "--enctypes 28"

and now:

[root at gw msktutil-1.0rc1]# ./msktutil -c -b "CN=COMPUTERS" -s HTTP/
ophtcysrv1v4.myaddomain.fr -k /etc/squid/PROXY.keytab --computer-name
OPHTCYSRV1V4-K --upn HTTP/ophtcysrv1v4.myaddomain.fr --server
myad.myaddomain.fr --verbose
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/urandom = 94
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-RyUQcT
 -- reload: Reloading Kerberos Context
 -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
 -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No
such file or directory)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No
such file or directory)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/
mydnshostname.fr from local keytab...
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
(Client not found in Kerberos database)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
password.
 -- create_default_machine_password: Default machine password for
OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed
(Preauthentication failed)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets...
 -- finalize_exec: Authenticated using method 5
 -- LDAPConnection: Connecting to LDAP server: myad.myaddomain.fr
SASL/GSSAPI authentication started
SASL username: Myusername at myaddomain.fr
SASL SSF: 56
SASL data security layer installed.
 -- ldap_get_base_dn: Determining default LDAP base: dc=SODIAAL,dc=FR
 -- ldap_check_account: Checking that a computer account for
OPHTCYSRV1V4-K$ exists
 -- ldap_check_account: Checking computer account - found
 -- ldap_check_account: Found userAccountControl = 0x1000
 -- ldap_check_account: Found supportedEncryptionTypes = 28
 -- ldap_check_account: Found dNSHostName = mydnshostname.fr
 -- ldap_check_account: userPrincipal specified on command line
 -- ldap_check_account_strings: Inspecting (and updating) computer account
attributes
 -- ldap_check_account_strings: Found userPrincipalName = HTTP/
ophtcysrv1v4.myaddomain.fr at myaddomain.fr
 -- ldap_check_account_strings: userPrincipalName should be HTTP/
ophtcysrv1v4.myaddomain.fr at myaddomain.fr
 -- ldap_check_account_strings: Nothing to do
 -- ldap_set_supportedEncryptionTypes: No need to change
msDs-supportedEncryptionTypes they are 28
 -- ldap_set_userAccountControl_flag: Setting userAccountControl bit at
0x200000 to 0x0
 -- ldap_set_userAccountControl_flag: userAccountControl not changed 0x1000
 -- ldap_get_kvno: KVNO is 1
 -- set_password: Attempting to reset computer's password
 -- set_password: Try change password using user's ticket cache
 -- ldap_get_pwdLastSet: pwdLastSet is 130751472429170776
 -- set_password: Successfully set password.
 -- ldap_add_principal: Checking that adding principal HTTP/
ophtcysrv1v4.myaddomain.fr to OPHTCYSRV1V4-K$ won't cause a conflict
 -- ldap_add_principal: Adding principal HTTP/ophtcysrv1v4.myaddomain.fr to
LDAP entry
 -- ldap_add_principal: Checking that adding principal host/mydnshostname.fr
to OPHTCYSRV1V4-K$ won't cause a conflict
 -- ldap_add_principal: Adding principal host/mydnshostname.fr to LDAP entry
 -- execute: Updating all entries for mydnshostname.fr in the keytab
WRFILE:/etc/squid/PROXY.keytab
 -- update_keytab: Updating all entries for OPHTCYSRV1V4-K$
 -- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x17
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x11
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x12
 -- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
 -- add_principal_keytab: Removing entries with kvno < 0
 -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
enctype=23
 -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
enctype=17
 -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
enctype=18
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x17
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x11
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x12
 -- add_principal_keytab: Adding principal to keytab: HTTP/
ophtcysrv1v4.myaddomain.fr
 -- add_principal_keytab: Removing entries with kvno < 0
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x17
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x11
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x12
 -- add_principal_keytab: Adding principal to keytab: host/OPHTCYSRV1V4-K
 -- add_principal_keytab: Removing entries with kvno < 0
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x17
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x11
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x12
 -- update_keytab: Entries for SPN HTTP/ophtcysrv1v4.myaddomain.fr have
already been added. Skipping ...
 -- add_principal_keytab: Adding principal to keytab: host/mydnshostname.fr
 -- add_principal_keytab: Removing entries with kvno < 0
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x17
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x11
 -- add_principal_keytab:     Using salt of
myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
 -- add_principal_keytab:   Adding entry of enctype 0x12
 -- wait_for_new_kvno: Checking new kvno via ldap
 -- ldap_get_kvno: KVNO is 1
Waiting for account replication (0 seconds past)
 -- ldap_get_kvno: KVNO is 2
 -- ~KRB5Context: Destroying Kerberos Context



it's good for you ?

regards
olivier


2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

>   Did you compile msktutil or is it a package in centos ?
>
> Markus
>
>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
> news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
>    Hi
>
>
> Thanks for your answer
>
> CentOS Linux release 7.1.1503 (Core)
>
> krb5-workstation-1.12.2-14.el7.x86_64
> krb5-libs-1.12.2-14.el7.x86_64
>
> regards
> olivier
>
>
> 2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:
>
>>   Which OS and Kerberos version do you have ?  There might be some issue
>> with the cache used KEYRING:persistent:0:0
>> Markus
>>
>>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
>> news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com.
>> ..
>>      Hi
>>
>> I request your help because i want use NTLM/Kerberos for authenticate my
>> user.
>>
>> For NTLM, i use Winbind, no problems,
>>
>> [root at gw]# wbinfo -t
>> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>>
>> but for Kerberos, i can't create the .keytab
>>
>>
>> [root at gw]# kinit MYUSERNAME
>> Password for MYUSERNAME at MYADDOMAIN.FR:
>>
>> [root at gw]# klist
>> Ticket cache: KEYRING:persistent:0:0
>> Default principal: MYUSERNAME at MYADDOMAIN.FR
>>
>> Valid starting       Expires              Service principal
>> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
>> MYADDOMAIN.FR at MYADDOMAIN.FR
>>         renew until 09/05/2015 04:51:07
>>
>> MYUSERNAME is the same account that i join the domain (net join) with
>> winbind
>>
>>
>> after, i put:
>>
>> msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
>> -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>>
>> and i have a error:
>>
>> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
>> --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>> -- init_password: Wiping the computer password structure
>> -- generate_new_password: Generating a new, random password for the
>> computer account
>> -- generate_new_password:  Characters read from /dev/udandom = 84
>> -- create_fake_krb5_conf: Created a fake krb5.conf file:
>> /tmp/.msktkrb5.conf-jnxTuG
>> -- reload: Reloading Kerberos Context
>> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
>> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
>> from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_keytab_princ: Trying to authenticate for host/
>> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
>> password.
>> -- create_default_machine_password: Default machine password for
>> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
>> -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
>> not found in Kerberos database)
>> -- try_machine_password: Authentication with password failed
>> -- try_user_creds: Checking if default ticket cache has tickets...
>> -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
>> cache found)
>> -- try_user_creds: User ticket cache was not valid.
>> Error: could not find any credentials to authenticate with. Neither
>> keytab,
>>      default machine password, nor calling user's tickets worked. Try
>>      "kinit"ing yourself some tickets with permission to create computer
>>      objects, or pre-creating the computer object in AD and selecting
>>      'reset account'.
>> -- ~KRB5Context: Destroying Kerberos Context
>>
>>
>>
>> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
>> ophtcysrv1v4.myaddomain.fr
>>
>>
>> anyone know the origin of this error ?
>>
>> thanks
>> Olivier
>>
>>
>> ------------------------------
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/0a4b6abd/attachment.htm>

From yvoinov at gmail.com  Sun May  3 17:29:01 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 03 May 2015 23:29:01 +0600
Subject: [squid-users] Distributed Cache Problems
In-Reply-To: <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
References: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
 <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>
 <1825603368.364864.1430665992332.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55465ADD.7010905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also refresh_pattern directives order is important.

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ This one overrides all others
directives below. Look at this     comment: # Add any of your own
refresh_pattern entries ABOVE these.

#
-----------------------------------------------------------------------------
#
#                               Optimizing cache
hits                           #
#
------------------------------------------------------------------------------#
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90%
432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$
10080 90% 43200 override-expire ignore-no-cache ignore-no-store
ignore-private
refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
refresh_pattern .               0       20%     4320

There is nothing surprising in the fact that a cache hit zeroing.

03.05.15 21:13, Hierony Manurung ?????:
>  Hierony Manurung
> Del Institute of Technology
> Network Management
>
>       Pada Minggu, 3 Mei 2015 22:10, Hierony Manurung
<hierony_milanisti at yahoo.co.id> menulis:
>   
>
> 
> Dear Fellow,I am implement a Distributed cache system (1 child Proxy,
2 parent proxy as sibling).
> in the cache peer algorithm, I use Weighted Round-Robin.
> But when I analyze the log using Squeezer, I am found that there is no
hit requestin the parent proxy.
>
> I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), and
the log analyze
> result.
> I am very pleased that you want to lend your time to help me (Because
the deadline is very close).
>
> Thanks in advance.
>
> 
>
>
> Hierony Manurung
> Del Institute of Technology
> Network Management
>
>    
>
>   
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRlrdAAoJENNXIZxhPexGhNsIALk4flrH6MaET8XJ2ESNhKES
jEcjcULGOCbsM39iHuBO1lfszCSOJ7A0c6grLVkKBZMnRuHjPoCAI0NYco0Rlsp7
le9b+bHQeBWT2QmmSPhyYeTEWO0tvrxu7ErE4vrDRI+73j0dZaVSKkp8hCu4K3GS
wTsH71MP7rv1QPI3Axydhs3EUT3nDqUYGbbIXF059raerq/1Qy+ywLSrrBBHQbBC
7skvZK8X9MS3hOzwUpBHQf9FqKlCY4mH9a1gW5HM2e93M6gGzF/pY2mR3yLH5CAN
KtWfg6gfm9W1Evb1gUGZ4PphifgaujGlQUVwtHQWpmUVatwDQ7rBR89tAnqlf1o=
=idXT
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/f7cbc353/attachment.htm>

From o.calvano at gmail.com  Sun May  3 17:34:37 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Sun, 3 May 2015 19:34:37 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <mi50jt$oe8$1@ger.gmane.org>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
Message-ID: <CAJajPeeh0=PZ97SPsUbw1H9zB+PW45ZvbA2Bu2fvegULcmx8vA@mail.gmail.com>

hum a new problems ??

[root at gw]# msktutil --auto-update --verbose --computer-name ophtcysrv1v4-k
-k /etc/squid/PROXY.keytab
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/urandom = 84
 -- get_dc_host: Attempting to find Domain Controller to use via DNS SRV
record in domain MYDOMAIN.FR for procotol tcp
 -- get_dc_host: Found DC: dc122001.mydomain.fr
 -- get_dc_host: Canonicalizing DC through forward/reverse lookup...
Error: gethostbyaddr failed
 -- get_dc_host: Found Domain Controller:
Error: get_dc_host failed
 -- ~KRB5Context: Destroying Kerberos Context






2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

>   Did you compile msktutil or is it a package in centos ?
>
> Markus
>
>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
> news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
>    Hi
>
>
> Thanks for your answer
>
> CentOS Linux release 7.1.1503 (Core)
>
> krb5-workstation-1.12.2-14.el7.x86_64
> krb5-libs-1.12.2-14.el7.x86_64
>
> regards
> olivier
>
>
> 2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:
>
>>   Which OS and Kerberos version do you have ?  There might be some issue
>> with the cache used KEYRING:persistent:0:0
>> Markus
>>
>>  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message
>> news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com.
>> ..
>>      Hi
>>
>> I request your help because i want use NTLM/Kerberos for authenticate my
>> user.
>>
>> For NTLM, i use Winbind, no problems,
>>
>> [root at gw]# wbinfo -t
>> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>>
>> but for Kerberos, i can't create the .keytab
>>
>>
>> [root at gw]# kinit MYUSERNAME
>> Password for MYUSERNAME at MYADDOMAIN.FR:
>>
>> [root at gw]# klist
>> Ticket cache: KEYRING:persistent:0:0
>> Default principal: MYUSERNAME at MYADDOMAIN.FR
>>
>> Valid starting       Expires              Service principal
>> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
>> MYADDOMAIN.FR at MYADDOMAIN.FR
>>         renew until 09/05/2015 04:51:07
>>
>> MYUSERNAME is the same account that i join the domain (net join) with
>> winbind
>>
>>
>> after, i put:
>>
>> msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org
>> -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>>
>> and i have a error:
>>
>> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
>> --computer-name OPHTCYSRV1V4-K --upn HTTP/
>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>> -- init_password: Wiping the computer password structure
>> -- generate_new_password: Generating a new, random password for the
>> computer account
>> -- generate_new_password:  Characters read from /dev/udandom = 84
>> -- create_fake_krb5_conf: Created a fake krb5.conf file:
>> /tmp/.msktkrb5.conf-jnxTuG
>> -- reload: Reloading Kerberos Context
>> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
>> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
>> from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_keytab_princ: Trying to authenticate for host/
>> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>> (Client not found in Kerberos database)
>> -- try_machine_keytab_princ: Authentication with keytab failed
>> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
>> password.
>> -- create_default_machine_password: Default machine password for
>> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
>> -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
>> not found in Kerberos database)
>> -- try_machine_password: Authentication with password failed
>> -- try_user_creds: Checking if default ticket cache has tickets...
>> -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
>> cache found)
>> -- try_user_creds: User ticket cache was not valid.
>> Error: could not find any credentials to authenticate with. Neither
>> keytab,
>>      default machine password, nor calling user's tickets worked. Try
>>      "kinit"ing yourself some tickets with permission to create computer
>>      objects, or pre-creating the computer object in AD and selecting
>>      'reset account'.
>> -- ~KRB5Context: Destroying Kerberos Context
>>
>>
>>
>> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
>> ophtcysrv1v4.myaddomain.fr
>>
>>
>> anyone know the origin of this error ?
>>
>> thanks
>> Olivier
>>
>>
>> ------------------------------
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/fda2ba90/attachment.htm>

From huaraz at moeller.plus.com  Sun May  3 17:36:41 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 3 May 2015 18:36:41 +0100
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <CAJajPecBcrbW+jtiwF2J=ujZ4KwDTWF6oPzJF56pVz+-gfNfLw@mail.gmail.com>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
 <CAJajPecBcrbW+jtiwF2J=ujZ4KwDTWF6oPzJF56pVz+-gfNfLw@mail.gmail.com>
Message-ID: <mi5mbc$pps$1@ger.gmane.org>

Hi Olivier,

   You may need to check with the msktutil authors as this is not directly related to squid. 

Regards
Markus

"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPecBcrbW+jtiwF2J=ujZ4KwDTWF6oPzJF56pVz+-gfNfLw at mail.gmail.com...
Hi


i have compiled the 1.0rc version :



[root at gw msktutil-1.0rc1]# ./msktutil -c -b "CN=COMPUTERS" -s HTTP/ophtcysrv1v4.myaddomain.fr -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/ophtcysrv1v4.myasdomain.fr --server myad.myaddomain.fr --verbose --enctypes 28
-- init_password: Wiping the computer password structure
-- generate_new_password: Generating a new, random password for the computer account
-- generate_new_password:  Characters read from /dev/urandom = 93
-- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-jPXQHu
-- reload: Reloading Kerberos Context
-- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
-- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_keytab_princ: Trying to authenticate for host/gw.srv1-v4.tcy.sodiaal.ophelys.org from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
-- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
-- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_password: Authentication with password failed
-- try_user_creds: Checking if default ticket cache has tickets...
-- finalize_exec: Authenticated using method 5
-- LDAPConnection: Connecting to LDAP server: myad.myaddomain.fr
SASL/GSSAPI authentication started
SASL username: Myusername at MYADDOMAIN.FR
SASL SSF: 56
SASL data security layer installed.
-- ldap_get_base_dn: Determining default LDAP base: dc=MYDOMAIN,dc=FR
-- ldap_check_account: Checking that a computer account for OPHTCYSRV1V4-K$ exists
-- ldap_check_account: Computer account not found, create the account
No computer account for OPHTCYSRV1V4-K found, creating a new one.
-- ldap_check_account_strings: Inspecting (and updating) computer account attributes
-- ldap_check_account_strings: Found userPrincipalName =
-- ldap_check_account_strings: userPrincipalName should be HTTP/ophtcysrv1v4.myaddomain.fr at MYADDOMAIN.FR
-- ldap_set_userAccountControl_flag: Setting userAccountControl bit at 0x200000 to 0x0
-- ldap_set_userAccountControl_flag: userAccountControl not changed 0x1000
-- ldap_get_kvno: KVNO is 1
-- set_password: Attempting to reset computer's password
-- set_password: Try change password using user's ticket cache
-- ldap_get_pwdLastSet: pwdLastSet is 130751472429170776
Error: Unable to set machine password for OPHTCYSRV1V4-K$: (3) Authentication error
Error: set_password failed
-- ~KRB5Context: Destroying Kerberos Context







2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

  Did you compile msktutil or is it a package in centos ? 

  Markus

  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
  Hi



  Thanks for your answer

  CentOS Linux release 7.1.1503 (Core)

  krb5-workstation-1.12.2-14.el7.x86_64
  krb5-libs-1.12.2-14.el7.x86_64


  regards

  olivier



  2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

    Which OS and Kerberos version do you have ?  There might be some issue with the cache used KEYRING:persistent:0:0

    Markus

    "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com...
    Hi


    I request your help because i want use NTLM/Kerberos for authenticate my user.


    For NTLM, i use Winbind, no problems, 

    [root at gw]# wbinfo -t
    checking the trust secret for domain MYADDOMAIN via RPC calls succeeded


    but for Kerberos, i can't create the .keytab


    [root at gw]# kinit MYUSERNAME
    Password for MYUSERNAME at MYADDOMAIN.FR:

    [root at gw]# klist
    Ticket cache: KEYRING:persistent:0:0
    Default principal: MYUSERNAME at MYADDOMAIN.FR

    Valid starting       Expires              Service principal
    02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/MYADDOMAIN.FR at MYADDOMAIN.FR
            renew until 09/05/2015 04:51:07


    MYUSERNAME is the same account that i join the domain (net join) with winbind



    after, i put:

    msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose


    and i have a error:

    [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
    -- init_password: Wiping the computer password structure
    -- generate_new_password: Generating a new, random password for the computer account
    -- generate_new_password:  Characters read from /dev/udandom = 84
    -- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-jnxTuG
    -- reload: Reloading Kerberos Context
    -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
    -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
    -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_keytab_princ: Authentication with keytab failed
    -- try_machine_keytab_princ: Trying to authenticate for host/gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
    -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_keytab_princ: Authentication with keytab failed
    -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
    -- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
    -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_password: Authentication with password failed
    -- try_user_creds: Checking if default ticket cache has tickets...
    -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
    -- try_user_creds: User ticket cache was not valid.
    Error: could not find any credentials to authenticate with. Neither keytab,
         default machine password, nor calling user's tickets worked. Try
         "kinit"ing yourself some tickets with permission to create computer
         objects, or pre-creating the computer object in AD and selecting
         'reset account'.
    -- ~KRB5Context: Destroying Kerberos Context




    same error if i change gw.srv1-v4.tcy.myinternetdomain.org to ophtcysrv1v4.myaddomain.fr



    anyone know the origin of this error ?


    thanks

    Olivier




----------------------------------------------------------------------------
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users




------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/ef88b3b2/attachment.htm>

From hussam.tayeb at gmx.com  Sun May  3 18:07:48 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sun, 3 May 2015 20:07:48 +0200
Subject: [squid-users] vary headers
In-Reply-To: <55465512.3090709@gmail.com>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
Message-ID: <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>



> Sent: Sunday, May 03, 2015 at 8:04 PM
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> Headers has own acl.
> Example:
> 
> # Strip User-Agent from Vary
> request_header_access Vary deny all
> request_header_replace Vary Accept-Encoding
> 
vary headers are reply headers, not request headers.
Anyway, that one requires enabling http-violations option which I don't want to do.
I simply don't want to cache them (i.e I don't want them stored in cache). How can I do that?
Thank you.


From yvoinov at gmail.com  Sun May  3 18:45:35 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 00:45:35 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>
Message-ID: <55466CCF.6000005@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 

04.05.15 0:07, Hussam Al-Tayeb ?????:
>
>
>> Sent: Sunday, May 03, 2015 at 8:04 PM
>> From: "Yuri Voinov" <yvoinov at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>>
>> Headers has own acl.
>> Example:
>>
>> # Strip User-Agent from Vary
>> request_header_access Vary deny all
>> request_header_replace Vary Accept-Encoding
>>
> vary headers are reply headers, not request headers.
I know.
>
> Anyway, that one requires enabling http-violations option which I
don't want to do.
Manipulating headers really violates HTTP, so your simple want to break
HTTP 1.1? :)))))))))))))))))


I simply don't want to cache them (i.e I don't want them stored in
cache). How can I do that?

I understand what do your want. But for what?
>
> Thank you.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRmzOAAoJENNXIZxhPexG0fEIAKk/u26PZfD+XzCeXmg0JWUv
lbDdZ4AWSdlA3g9tbMnT7nnob2qEemUXT9SVC3mNyHW/FXv8wEytSF56bT7vcEkB
rw58T1akUXHmisgAQrU5LHZpTK3FELz6DGyVRsgaINB34EcQI7ECupZVNb1XXvTK
Va374XgpsdZYcintNa89EJvs9qEGuK/4Wpvh43WsA2H2bUfjZffNwi6AEkK2ExjP
ZC72nI45Udr+OKu8Un6+tWh9/M036YLm8ey82xA5s4FmBcoC/vsDGAS4MF8F3r+l
H0L7bo/mgdOeYPaoRf896aWIoY4HxTYU3K5lsZIm8jJvkq3Qq+Fgnu1/CIqYHKs=
=FkN0
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sun May  3 18:50:04 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 00:50:04 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>
Message-ID: <55466DDC.1050609@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
#  TAG: cache_vary
#    This option is not yet supported by Squid-3.

This is option your want. But - ooooooops! - not yet supported.

04.05.15 0:07, Hussam Al-Tayeb ?????:
>
>
>> Sent: Sunday, May 03, 2015 at 8:04 PM
>> From: "Yuri Voinov" <yvoinov at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>>
>> Headers has own acl.
>> Example:
>>
>> # Strip User-Agent from Vary
>> request_header_access Vary deny all
>> request_header_replace Vary Accept-Encoding
>>
> vary headers are reply headers, not request headers.
> Anyway, that one requires enabling http-violations option which I
don't want to do.
> I simply don't want to cache them (i.e I don't want them stored in
cache). How can I do that?
> Thank you.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRm3bAAoJENNXIZxhPexG/o4H/iDda1g5czSQSkWnoMhmMmmV
Hdk5wvhI1/2m3VnxwLuBC45nd01pESBZnKfi0ROa+BU5NH0Ig5CbNQJPP4RBEr6X
sZf/TnLHp3nUryAXKIoyphlp1NXl5NCAJon45X3HtXoijGiJwghzkPuVi+c6w0Ak
65e5mCjPrlDmUENFz5BPXJcq+ctkhbGKkqrHSeajP653kuS7VJE+W2Ia+y5q1dXp
DJT2XEkgW59FdFCz0yjPGOUvkwhp6XJi4JmulgMRlqPZhZcZNjyTp5JPZ8MZJUzu
2rXHsenWkbv6mDjVsLto9cJn12v/vCQxwvO7Y3xsbuyioa4pIsvbdT+2o+G9N7c=
=Sl3l
-----END PGP SIGNATURE-----



From hussam.tayeb at gmx.com  Sun May  3 18:54:00 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sun, 3 May 2015 20:54:00 +0200
Subject: [squid-users] vary headers
In-Reply-To: <55466CCF.6000005@gmail.com>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
Message-ID: <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>

> Sent: Sunday, May 03, 2015 at 9:45 PM
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers

> 
> I understand what do your want. But for what?
> >

because a "wget --server-response http://someurl" operation that replies with a "Vary: user-agent" header always results in a MISS even if the same wget version (same user-agent) and computer. Instead, multiple copies of the file are stored. That means those stored entries are redundant and hence I would rather not store them.


From yvoinov at gmail.com  Sun May  3 18:55:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 00:55:40 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
Message-ID: <55466F2C.6050501@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Will be good enough simple strip user-agent from vary header.

http://www.fastly.com/blog/best-practices-for-using-the-vary-header/

04.05.15 0:54, Hussam Al-Tayeb ?????:
>> Sent: Sunday, May 03, 2015 at 9:45 PM
>> From: "Yuri Voinov" <yvoinov at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>
>>
>> I understand what do your want. But for what?
>>>
>
> because a "wget --server-response http://someurl" operation that
replies with a "Vary: user-agent" header always results in a MISS even
if the same wget version (same user-agent) and computer. Instead,
multiple copies of the file are stored. That means those stored entries
are redundant and hence I would rather not store them.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRm8sAAoJENNXIZxhPexG4WsIALmu5tXvdwxbaFD3gghJAEi5
rF0kukSlScPgkX2PXLszI4X3H355F/fMGsMfRPLe4YjkBSB5b/S0aIdP4437Tnp6
xHvW6o3uh18C03f3tkQ/E44ZTJlIZXSL407tVhcyP5j0VzU24njLsA4X65JOavuW
T5ne5t7SKDRKkwCM6mZlzdB5muKdML21LuojYxmTwcTzu7602VC4a40R5GxvhH03
6Et0tFHg8ZQzzg5MKAKEL1XRTEZhqmH8WziW8jZAiPjwRsgl8CvZdUY3CCh8fw91
nxzJuTHiLoWZJ3Wy59nIJ8HGofZYssv/nXTaS1DeNA4/IARc2Fl3/ox2DFng5P0=
=mMsR
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sun May  3 19:03:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 01:03:31 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
Message-ID: <55467103.10107@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Something like this:

reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding

04.05.15 0:54, Hussam Al-Tayeb ?????:
> wget --server-response 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRnEDAAoJENNXIZxhPexGum4H/0kt4YS0yQzKAJrwAih/bb/v
N5RHgPAApHmVo3EcRfQvFpRDh0C6C3eAX5cD8FV7v+v0zPsc9c1DQr9PsM65gJZs
hYelmH0lUFJDud/TKk1MrmbybZRAEY+FyG1/0FHqbWtFLmAMIHRmVzIjmkqgGu1N
S7MhjHyWXSvVGplaVlH4UdUMeLlXxNjTSJtr7keTWW6CF8GVU/r/O08fa9IRwZeJ
QabJ21yptVvu66ruCtjVJrUSGu1wir+iEgDE9RQIv6KSPTIxeVxL7z5/Bv4/vKtx
jN+SszSL9Fkg+Ik01dEwRQiAXtyPKo0jVl5jsI07K5HJtRUzrLjVlYd5W947EOs=
=un97
-----END PGP SIGNATURE-----



From hussam.tayeb at gmx.com  Sun May  3 19:07:30 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sun, 3 May 2015 21:07:30 +0200
Subject: [squid-users] vary headers
In-Reply-To: <55466F2C.6050501@gmail.com>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <55466F2C.6050501@gmail.com>
Message-ID: <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>



> Sent: Sunday, May 03, 2015 at 9:55 PM
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>  
> Will be good enough simple strip user-agent from vary header.
> 
> http://www.fastly.com/blog/best-practices-for-using-the-vary-header/
> 

That works on destination server level and no proxy or client.

Plus if someone was to try:
reply_header_access Vary deny all
reply_header_replace Vary Accept-Encoding
What are the implications? Anything affects gzipped/non-gzipped content?

Nevertheless, I would rather simply not cache instead manipulate http headers.
Do you know how to do that?


From yvoinov at gmail.com  Sun May  3 19:19:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 01:19:42 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <55466F2C.6050501@gmail.com>
 <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>
Message-ID: <554674CE.1050607@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 


04.05.15 1:07, Hussam Al-Tayeb ?????:
> What are the implications? Anything affects gzipped/non-gzipped content?
Sure. Did you cache support gzip?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRnTOAAoJENNXIZxhPexGdHoIAKeMi7E2Ctu6E7jbQX1sU3DH
GkFIsKo4R7jiHKAK+2CU/uc8FkshMZPqzpM0/u7NEDHaD/eUzMOsonD4sLF6q9Ep
SHYO21WjA5dRSddKQHWhJ8fHef7ASOXsFED6NM8nwu2VdX9WP5BxkptKrUg96BGI
YVV1U7j+Bjk+qCkIcv0/9MH5RqiKJzy/lgyJQBJPTdxIKVL/P+G72wT0IaPtUqxC
K86E2i1ff40z6qXFSPn0d7ZZi5qJOCg3pC02Hf4E/2iT0x+YUbLBgCCQ01itjzxh
l1WR8Xi5k2KZX6HyF/fVKDBHcrjGPVDK/A75S+wO2yPuR/jPnyahmRR0WBOZTbo=
=QJk3
-----END PGP SIGNATURE-----



From yvoinov at gmail.com  Sun May  3 19:26:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 01:26:55 +0600
Subject: [squid-users] vary headers
In-Reply-To: <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <55466F2C.6050501@gmail.com>
 <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>
Message-ID: <5546767F.1020106@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Understand. You problem is duplicates. So, you need to de-duplicate
identical content.

May be, store ID saves you?

04.05.15 1:07, Hussam Al-Tayeb ?????:
>
>
>> Sent: Sunday, May 03, 2015 at 9:55 PM
>> From: "Yuri Voinov" <yvoinov at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>>
>>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA256
>> 
>> Will be good enough simple strip user-agent from vary header.
>>
>> http://www.fastly.com/blog/best-practices-for-using-the-vary-header/
>>
>
> That works on destination server level and no proxy or client.
>
> Plus if someone was to try:
> reply_header_access Vary deny all
> reply_header_replace Vary Accept-Encoding
> What are the implications? Anything affects gzipped/non-gzipped content?
>
> Nevertheless, I would rather simply not cache instead manipulate http
headers.
> Do you know how to do that?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVRnZ/AAoJENNXIZxhPexGxzkIALLwTmU4pcDUy/RQ49UpC3na
h5EC2+8ZQNGWI14dk87VvIblgKfdazxpJHe7Kq3l4cwdxzol72AO7FHqD1goq/Vl
BqkuYRtkOZ/P28ADLGEpGhogeFyg18Qr6VfZqvT9lJrvOJ9f7cueksSc5b2dUWEI
TptNLE79rgFFDIefUiMOHXrqzTPCrjdRJabenMEr6klJjjnEyQw7mhvui//cXXEB
c3xe5jKY+OK/RL0OkI9Ydd00pRTdLANBZ/eHaK6vIpoh86nmQPl05HqS0LfK+MkU
8b/lH/sQt9rHCrNveYyDZwaVMtyIL+btcBSwzWdVYGGw1Qr6JutRuB3kFmHkO5A=
=3EXv
-----END PGP SIGNATURE-----



From hussam.tayeb at gmx.com  Sun May  3 19:28:25 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Sun, 3 May 2015 21:28:25 +0200
Subject: [squid-users] vary headers
In-Reply-To: <554674CE.1050607@gmail.com>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <55466F2C.6050501@gmail.com>
 <trinity-81aa39e2-fa3d-4b8a-a949-021cfd4a5b89-1430680050008@3capp-mailcom-bs08>,
 <554674CE.1050607@gmail.com>
Message-ID: <trinity-570f3034-ec77-426c-aa60-8f847ae9c0b9-1430681305071@3capp-mailcom-bs14>



> Sent: Sunday, May 03, 2015 at 10:19 PM
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> 
> Sure. Did you cache support gzip?

I don't understand that question, sorry.

Anyway, I will wait till someone knows how to not cache them. Thank you.


From huaraz at moeller.plus.com  Sun May  3 22:32:56 2015
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 3 May 2015 23:32:56 +0100
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ@mail.gmail.com>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
 <CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ@mail.gmail.com>
Message-ID: <mi67mr$qjp$1@ger.gmane.org>

So this worked ?

Markus

"Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ at mail.gmail.com...
hoo i have deleted "--enctypes 28"


and now:

[root at gw msktutil-1.0rc1]# ./msktutil -c -b "CN=COMPUTERS" -s HTTP/ophtcysrv1v4.myaddomain.fr -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/ophtcysrv1v4.myaddomain.fr --server myad.myaddomain.fr --verbose
-- init_password: Wiping the computer password structure
-- generate_new_password: Generating a new, random password for the computer account
-- generate_new_password:  Characters read from /dev/urandom = 94
-- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-RyUQcT
-- reload: Reloading Kerberos Context
-- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
-- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No such file or directory)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No such file or directory)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_keytab_princ: Trying to authenticate for host/mydnshostname.fr from local keytab...
-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
-- try_machine_keytab_princ: Authentication with keytab failed
-- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
-- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
-- try_machine_password: Error: krb5_get_init_creds_keytab failed (Preauthentication failed)
-- try_machine_password: Authentication with password failed
-- try_user_creds: Checking if default ticket cache has tickets...
-- finalize_exec: Authenticated using method 5
-- LDAPConnection: Connecting to LDAP server: myad.myaddomain.fr
SASL/GSSAPI authentication started
SASL username: Myusername at myaddomain.fr
SASL SSF: 56
SASL data security layer installed.
-- ldap_get_base_dn: Determining default LDAP base: dc=SODIAAL,dc=FR
-- ldap_check_account: Checking that a computer account for OPHTCYSRV1V4-K$ exists
-- ldap_check_account: Checking computer account - found
-- ldap_check_account: Found userAccountControl = 0x1000
-- ldap_check_account: Found supportedEncryptionTypes = 28
-- ldap_check_account: Found dNSHostName = mydnshostname.fr
-- ldap_check_account: userPrincipal specified on command line
-- ldap_check_account_strings: Inspecting (and updating) computer account attributes
-- ldap_check_account_strings: Found userPrincipalName = HTTP/ophtcysrv1v4.myaddomain.fr at myaddomain.fr
-- ldap_check_account_strings: userPrincipalName should be HTTP/ophtcysrv1v4.myaddomain.fr at myaddomain.fr
-- ldap_check_account_strings: Nothing to do
-- ldap_set_supportedEncryptionTypes: No need to change msDs-supportedEncryptionTypes they are 28
-- ldap_set_userAccountControl_flag: Setting userAccountControl bit at 0x200000 to 0x0
-- ldap_set_userAccountControl_flag: userAccountControl not changed 0x1000
-- ldap_get_kvno: KVNO is 1
-- set_password: Attempting to reset computer's password
-- set_password: Try change password using user's ticket cache
-- ldap_get_pwdLastSet: pwdLastSet is 130751472429170776
-- set_password: Successfully set password.
-- ldap_add_principal: Checking that adding principal HTTP/ophtcysrv1v4.myaddomain.fr to OPHTCYSRV1V4-K$ won't cause a conflict
-- ldap_add_principal: Adding principal HTTP/ophtcysrv1v4.myaddomain.fr to LDAP entry
-- ldap_add_principal: Checking that adding principal host/mydnshostname.fr to OPHTCYSRV1V4-K$ won't cause a conflict
-- ldap_add_principal: Adding principal host/mydnshostname.fr to LDAP entry
-- execute: Updating all entries for mydnshostname.fr in the keytab WRFILE:/etc/squid/PROXY.keytab
-- update_keytab: Updating all entries for OPHTCYSRV1V4-K$
-- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x17
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x11
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x12
-- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
-- add_principal_keytab: Removing entries with kvno < 0
-- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2, enctype=23
-- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2, enctype=17
-- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2, enctype=18
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x17
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x11
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x12
-- add_principal_keytab: Adding principal to keytab: HTTP/ophtcysrv1v4.myaddomain.fr
-- add_principal_keytab: Removing entries with kvno < 0
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x17
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x11
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x12
-- add_principal_keytab: Adding principal to keytab: host/OPHTCYSRV1V4-K
-- add_principal_keytab: Removing entries with kvno < 0
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x17
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x11
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x12
-- update_keytab: Entries for SPN HTTP/ophtcysrv1v4.myaddomain.fr have already been added. Skipping ...
-- add_principal_keytab: Adding principal to keytab: host/mydnshostname.fr
-- add_principal_keytab: Removing entries with kvno < 0
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x17
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x11
-- add_principal_keytab:     Using salt of myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
-- add_principal_keytab:   Adding entry of enctype 0x12
-- wait_for_new_kvno: Checking new kvno via ldap
-- ldap_get_kvno: KVNO is 1
Waiting for account replication (0 seconds past)
-- ldap_get_kvno: KVNO is 2
-- ~KRB5Context: Destroying Kerberos Context




it's good for you ?


regards

olivier



2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

  Did you compile msktutil or is it a package in centos ? 

  Markus

  "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com...
  Hi



  Thanks for your answer

  CentOS Linux release 7.1.1503 (Core)

  krb5-workstation-1.12.2-14.el7.x86_64
  krb5-libs-1.12.2-14.el7.x86_64


  regards

  olivier



  2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com>:

    Which OS and Kerberos version do you have ?  There might be some issue with the cache used KEYRING:persistent:0:0

    Markus

    "Olivier CALVANO" <o.calvano at gmail.com> wrote in message news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com...
    Hi


    I request your help because i want use NTLM/Kerberos for authenticate my user.


    For NTLM, i use Winbind, no problems, 

    [root at gw]# wbinfo -t
    checking the trust secret for domain MYADDOMAIN via RPC calls succeeded


    but for Kerberos, i can't create the .keytab


    [root at gw]# kinit MYUSERNAME
    Password for MYUSERNAME at MYADDOMAIN.FR:

    [root at gw]# klist
    Ticket cache: KEYRING:persistent:0:0
    Default principal: MYUSERNAME at MYADDOMAIN.FR

    Valid starting       Expires              Service principal
    02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/MYADDOMAIN.FR at MYADDOMAIN.FR
            renew until 09/05/2015 04:51:07


    MYUSERNAME is the same account that i join the domain (net join) with winbind



    after, i put:

    msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose


    and i have a error:

    [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab --computer-name OPHTCYSRV1V4-K --upn HTTP/gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
    -- init_password: Wiping the computer password structure
    -- generate_new_password: Generating a new, random password for the computer account
    -- generate_new_password:  Characters read from /dev/udandom = 84
    -- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-jnxTuG
    -- reload: Reloading Kerberos Context
    -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
    -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$ from local keytab...
    -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_keytab_princ: Authentication with keytab failed
    -- try_machine_keytab_princ: Trying to authenticate for host/gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
    -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_keytab_princ: Authentication with keytab failed
    -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with password.
    -- create_default_machine_password: Default machine password for OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
    -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
    -- try_machine_password: Authentication with password failed
    -- try_user_creds: Checking if default ticket cache has tickets...
    -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
    -- try_user_creds: User ticket cache was not valid.
    Error: could not find any credentials to authenticate with. Neither keytab,
         default machine password, nor calling user's tickets worked. Try
         "kinit"ing yourself some tickets with permission to create computer
         objects, or pre-creating the computer object in AD and selecting
         'reset account'.
    -- ~KRB5Context: Destroying Kerberos Context




    same error if i change gw.srv1-v4.tcy.myinternetdomain.org to ophtcysrv1v4.myaddomain.fr



    anyone know the origin of this error ?


    thanks

    Olivier




----------------------------------------------------------------------------
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users




------------------------------------------------------------------------------
  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users


  _______________________________________________
  squid-users mailing list
  squid-users at lists.squid-cache.org
  http://lists.squid-cache.org/listinfo/squid-users





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/59db3a2f/attachment.htm>

From o.calvano at gmail.com  Sun May  3 23:06:05 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Mon, 4 May 2015 01:06:05 +0200
Subject: [squid-users] Squid and Kerberos problems
In-Reply-To: <mi67mr$qjp$1@ger.gmane.org>
References: <CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg@mail.gmail.com>
 <mi3isu$fof$1@ger.gmane.org>
 <CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ@mail.gmail.com>
 <mi50jt$oe8$1@ger.gmane.org>
 <CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ@mail.gmail.com>
 <mi67mr$qjp$1@ger.gmane.org>
Message-ID: <CAJajPefRrCgXtsxK5Yhj+AmgENBfbg2DFkQL=7wKj54sZpj2_g@mail.gmail.com>

No i de with the msktutil dev :)

Thanks for your help

Le lundi 4 mai 2015, Markus Moeller <huaraz at moeller.plus.com> a ?crit :

>   So this worked ?
>
> Markus
>
>  "Olivier CALVANO" <o.calvano at gmail.com
> <javascript:_e(%7B%7D,'cvml','o.calvano at gmail.com');>> wrote in message
> news:CAJajPeddju9t4QAiPSmT-5JUsn4Gf6Nj0Pff3JBJ+BzXzTXOUQ at mail.gmail.com...
>    hoo i have deleted "--enctypes 28"
>
> and now:
>
> [root at gw msktutil-1.0rc1]# ./msktutil -c -b "CN=COMPUTERS" -s HTTP/
> ophtcysrv1v4.myaddomain.fr -k /etc/squid/PROXY.keytab --computer-name
> OPHTCYSRV1V4-K --upn HTTP/ophtcysrv1v4.myaddomain.fr --server
> myad.myaddomain.fr --verbose
> -- init_password: Wiping the computer password structure
> -- generate_new_password: Generating a new, random password for the
> computer account
> -- generate_new_password:  Characters read from /dev/urandom = 94
> -- create_fake_krb5_conf: Created a fake krb5.conf file:
> /tmp/.msktkrb5.conf-RyUQcT
> -- reload: Reloading Kerberos Context
> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
> from local keytab...
> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No
> such file or directory)
> -- try_machine_keytab_princ: Authentication with keytab failed
> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
> from local keytab...
> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (No
> such file or directory)
> -- try_machine_keytab_princ: Authentication with keytab failed
> -- try_machine_keytab_princ: Trying to authenticate for host/
> mydnshostname.fr from local keytab...
> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
> (Client not found in Kerberos database)
> -- try_machine_keytab_princ: Authentication with keytab failed
> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
> password.
> -- create_default_machine_password: Default machine password for
> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
> -- try_machine_password: Error: krb5_get_init_creds_keytab failed
> (Preauthentication failed)
> -- try_machine_password: Authentication with password failed
> -- try_user_creds: Checking if default ticket cache has tickets...
> -- finalize_exec: Authenticated using method 5
> -- LDAPConnection: Connecting to LDAP server: myad.myaddomain.fr
> SASL/GSSAPI authentication started
> SASL username: Myusername at myaddomain.fr
> <javascript:_e(%7B%7D,'cvml','Myusername at myaddomain.fr');>
> SASL SSF: 56
> SASL data security layer installed.
> -- ldap_get_base_dn: Determining default LDAP base: dc=SODIAAL,dc=FR
> -- ldap_check_account: Checking that a computer account for
> OPHTCYSRV1V4-K$ exists
> -- ldap_check_account: Checking computer account - found
> -- ldap_check_account: Found userAccountControl = 0x1000
> -- ldap_check_account: Found supportedEncryptionTypes = 28
> -- ldap_check_account: Found dNSHostName = mydnshostname.fr
> -- ldap_check_account: userPrincipal specified on command line
> -- ldap_check_account_strings: Inspecting (and updating) computer account
> attributes
> -- ldap_check_account_strings: Found userPrincipalName = HTTP/
> ophtcysrv1v4.myaddomain.fr at myaddomain.fr
> <javascript:_e(%7B%7D,'cvml','ophtcysrv1v4.myaddomain.fr at myaddomain.fr');>
> -- ldap_check_account_strings: userPrincipalName should be HTTP/
> ophtcysrv1v4.myaddomain.fr at myaddomain.fr
> <javascript:_e(%7B%7D,'cvml','ophtcysrv1v4.myaddomain.fr at myaddomain.fr');>
> -- ldap_check_account_strings: Nothing to do
> -- ldap_set_supportedEncryptionTypes: No need to change
> msDs-supportedEncryptionTypes they are 28
> -- ldap_set_userAccountControl_flag: Setting userAccountControl bit at
> 0x200000 to 0x0
> -- ldap_set_userAccountControl_flag: userAccountControl not changed 0x1000
> -- ldap_get_kvno: KVNO is 1
> -- set_password: Attempting to reset computer's password
> -- set_password: Try change password using user's ticket cache
> -- ldap_get_pwdLastSet: pwdLastSet is 130751472429170776
> -- set_password: Successfully set password.
> -- ldap_add_principal: Checking that adding principal HTTP/
> ophtcysrv1v4.myaddomain.fr to OPHTCYSRV1V4-K$ won't cause a conflict
> -- ldap_add_principal: Adding principal HTTP/ophtcysrv1v4.myaddomain.fr
> to LDAP entry
> -- ldap_add_principal: Checking that adding principal host/
> mydnshostname.fr to OPHTCYSRV1V4-K$ won't cause a conflict
> -- ldap_add_principal: Adding principal host/mydnshostname.fr to LDAP
> entry
> -- execute: Updating all entries for mydnshostname.fr in the keytab
> WRFILE:/etc/squid/PROXY.keytab
> -- update_keytab: Updating all entries for OPHTCYSRV1V4-K$
> -- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x17
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x11
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x12
> -- add_principal_keytab: Adding principal to keytab: OPHTCYSRV1V4-K$
> -- add_principal_keytab: Removing entries with kvno < 0
> -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
> enctype=23
> -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
> enctype=17
> -- add_principal_keytab: Deleting OPHTCYSRV1V4-K$@myaddomain.fr kvno=2,
> enctype=18
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x17
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x11
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x12
> -- add_principal_keytab: Adding principal to keytab: HTTP/
> ophtcysrv1v4.myaddomain.fr
> -- add_principal_keytab: Removing entries with kvno < 0
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x17
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x11
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x12
> -- add_principal_keytab: Adding principal to keytab: host/OPHTCYSRV1V4-K
> -- add_principal_keytab: Removing entries with kvno < 0
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x17
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x11
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x12
> -- update_keytab: Entries for SPN HTTP/ophtcysrv1v4.myaddomain.fr have
> already been added. Skipping ...
> -- add_principal_keytab: Adding principal to keytab: host/mydnshostname.fr
> -- add_principal_keytab: Removing entries with kvno < 0
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x17
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x11
> -- add_principal_keytab:     Using salt of
> myaddomain.frhostophtcysrv1v4-k.myaddomain.fr
> -- add_principal_keytab:   Adding entry of enctype 0x12
> -- wait_for_new_kvno: Checking new kvno via ldap
> -- ldap_get_kvno: KVNO is 1
> Waiting for account replication (0 seconds past)
> -- ldap_get_kvno: KVNO is 2
> -- ~KRB5Context: Destroying Kerberos Context
>
>
>
> it's good for you ?
>
> regards
> olivier
>
>
> 2015-05-03 13:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com
> <javascript:_e(%7B%7D,'cvml','huaraz at moeller.plus.com');>>:
>
>>   Did you compile msktutil or is it a package in centos ?
>>
>> Markus
>>
>>  "Olivier CALVANO" <o.calvano at gmail.com
>> <javascript:_e(%7B%7D,'cvml','o.calvano at gmail.com');>> wrote in message
>> news:CAJajPecQD+_1KRUfwa9eAC4iYAKapZBLyg-9vuueKLGWUecopQ at mail.gmail.com.
>> ..
>>     Hi
>>
>>
>> Thanks for your answer
>>
>> CentOS Linux release 7.1.1503 (Core)
>>
>> krb5-workstation-1.12.2-14.el7.x86_64
>> krb5-libs-1.12.2-14.el7.x86_64
>>
>> regards
>> olivier
>>
>>
>> 2015-05-03 0:25 GMT+02:00 Markus Moeller <huaraz at moeller.plus.com
>> <javascript:_e(%7B%7D,'cvml','huaraz at moeller.plus.com');>>:
>>
>>>   Which OS and Kerberos version do you have ?  There might be some
>>> issue with the cache used KEYRING:persistent:0:0
>>> Markus
>>>
>>>  "Olivier CALVANO" <o.calvano at gmail.com
>>> <javascript:_e(%7B%7D,'cvml','o.calvano at gmail.com');>> wrote in message
>>> news:CAJajPefo3t8b1=_v5PFj3H0gq4Jk3OosuTW8gNHY7Z-Gs21qLg at mail.gmail.com.
>>> ..
>>>      Hi
>>>
>>> I request your help because i want use NTLM/Kerberos for authenticate my
>>> user.
>>>
>>> For NTLM, i use Winbind, no problems,
>>>
>>> [root at gw]# wbinfo -t
>>> checking the trust secret for domain MYADDOMAIN via RPC calls succeeded
>>>
>>> but for Kerberos, i can't create the .keytab
>>>
>>>
>>> [root at gw]# kinit MYUSERNAME
>>> Password for MYUSERNAME at MYADDOMAIN.FR
>>> <javascript:_e(%7B%7D,'cvml','MYUSERNAME at MYADDOMAIN.FR');>:
>>>
>>> [root at gw]# klist
>>> Ticket cache: KEYRING:persistent:0:0
>>> Default principal: MYUSERNAME at MYADDOMAIN.FR
>>> <javascript:_e(%7B%7D,'cvml','MYUSERNAME at MYADDOMAIN.FR');>
>>>
>>> Valid starting       Expires              Service principal
>>> 02/05/2015 04:51:25  02/05/2015 14:51:25  krbtgt/
>>> MYADDOMAIN.FR at MYADDOMAIN.FR
>>> <javascript:_e(%7B%7D,'cvml','MYADDOMAIN.FR at MYADDOMAIN.FR');>
>>>         renew until 09/05/2015 04:51:07
>>>
>>> MYUSERNAME is the same account that i join the domain (net join) with
>>> winbind
>>>
>>>
>>> after, i put:
>>>
>>> msktutil -c -b "CN=COMPUTERS" -s HTTP/
>>> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
>>> --computer-name OPHTCYSRV1V4-K --upn HTTP/
>>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>>>
>>> and i have a error:
>>>
>>> [root at gw etc]# msktutil -c -b "CN=COMPUTERS" -s HTTP/
>>> gw.srv1-v4.tcy.myinternetdomain.org -k /etc/squid/PROXY.keytab
>>> --computer-name OPHTCYSRV1V4-K --upn HTTP/
>>> gw.srv1-v4.tcy.myinternetdomain.org --server adserver1 --verbose
>>> -- init_password: Wiping the computer password structure
>>> -- generate_new_password: Generating a new, random password for the
>>> computer account
>>> -- generate_new_password:  Characters read from /dev/udandom = 84
>>> -- create_fake_krb5_conf: Created a fake krb5.conf file:
>>> /tmp/.msktkrb5.conf-jnxTuG
>>> -- reload: Reloading Kerberos Context
>>> -- finalize_exec: SAM Account Name is: OPHTCYSRV1V4-K$
>>> -- try_machine_keytab_princ: Trying to authenticate for OPHTCYSRV1V4-K$
>>> from local keytab...
>>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>>> (Client not found in Kerberos database)
>>> -- try_machine_keytab_princ: Authentication with keytab failed
>>> -- try_machine_keytab_princ: Trying to authenticate for host/
>>> gw.srv1-v4.tcy.myinternetdomain.org from local keytab...
>>> -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed
>>> (Client not found in Kerberos database)
>>> -- try_machine_keytab_princ: Authentication with keytab failed
>>> -- try_machine_password: Trying to authenticate for OPHTCYSRV1V4-K$ with
>>> password.
>>> -- create_default_machine_password: Default machine password for
>>> OPHTCYSRV1V4-K$ is ophtcysrv1v4-k
>>> -- try_machine_password: Error: krb5_get_init_creds_keytab failed
>>> (Client not found in Kerberos database)
>>> -- try_machine_password: Authentication with password failed
>>> -- try_user_creds: Checking if default ticket cache has tickets...
>>> -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
>>> cache found)
>>> -- try_user_creds: User ticket cache was not valid.
>>> Error: could not find any credentials to authenticate with. Neither
>>> keytab,
>>>      default machine password, nor calling user's tickets worked. Try
>>>      "kinit"ing yourself some tickets with permission to create computer
>>>      objects, or pre-creating the computer object in AD and selecting
>>>      'reset account'.
>>> -- ~KRB5Context: Destroying Kerberos Context
>>>
>>>
>>>
>>> same error if i change gw.srv1-v4.tcy.myinternetdomain.org to
>>> ophtcysrv1v4.myaddomain.fr
>>>
>>>
>>> anyone know the origin of this error ?
>>>
>>> thanks
>>> Olivier
>>>
>>>
>>> ------------------------------
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> <javascript:_e(%7B%7D,'cvml','squid-users at lists.squid-cache.org');>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> <javascript:_e(%7B%7D,'cvml','squid-users at lists.squid-cache.org');>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>> ------------------------------
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <javascript:_e(%7B%7D,'cvml','squid-users at lists.squid-cache.org');>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <javascript:_e(%7B%7D,'cvml','squid-users at lists.squid-cache.org');>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> ------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <javascript:_e(%7B%7D,'cvml','squid-users at lists.squid-cache.org');>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/adcd9a5c/attachment.htm>

From "darrenjbreeze" at netvigator.com  Sun May  3 23:35:22 2015
From: "darrenjbreeze" at netvigator.com (Darren B.)
Date: Mon, 04 May 2015 07:35:22 +0800
Subject: [squid-users] Squid ubuntu build error
Message-ID: <mailman.3.1736411253.1101126.squid-users@lists.squid-cache.org>

Hi

I have struck a small issue in building squid from source (Ubuntu 14.04 
current source packages)

cp: cannot stat 
?/home/darren/squid3-3.3.8/debian/tmp/usr/share/squid3/icons?: No such 
file or directory
dh_install: cp -a 
/home/darren/squid3-3.3.8/debian/tmp/usr/share/squid3/icons 
debian/squid3-common//usr/share/squid3/ returned exit code 1

it looks like something upstream of this error is not creating 
tmp/usr/share/squid3 so creating the icons directory fails.

If I create the missing directory and do a make it's all fine but I am 
trying to create packages from source (adding SSL) and the clena up 
prior to the make deletes the directories.

This looks like a bug in the distro as I have built previous version s 
from source without a bother.

Can anyone point me to the offending code?

thanks

Darren Breeze









From weiguang0314 at 163.com  Mon May  4 12:46:07 2015
From: weiguang0314 at 163.com (=?utf-8?B?R3VhbmcgV2Vp?=)
Date: Mon, 4 May 2015 20:46:07 +0800
Subject: [squid-users] about squid choose sibing
Message-ID: <tencent_2ABFE65138399BCA3A12A0C4@qq.com>

sibing, cache digests and NetDB
Squid's network measurement database is designed to measure the proximity of origin servers. In other words, by querying this database, Squid knows how close it is to the origin server.

Think about that
web server A& parent squid B(located in China), child squid C(in China), child squid D(in USA)
or 
web server A & parent squid B(located in China), parent squid C(in China), parent squid D(in USA)
If a American user visits squid B and didn't hit, B will query the cache digests and find what the user want is both in C and D. Then B forward it to the closest squid server. I think it will always forwad to squid C (as C is more close to the origin server), but D is better to the user.
Is it right? 
Can squid do that choose sibing according to the region or how close the network from user to the origin server?

Thanks,
Allen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/6867ef71/attachment.htm>

From squid3 at treenet.co.nz  Mon May  4 09:38:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 May 2015 21:38:12 +1200
Subject: [squid-users] Squid-deb-proxy legacy-tools_0.1_all.deb Size
	mismatch
In-Reply-To: <CAF8s8J_S151FGsfcYg2cymT=fwkMsUPzpfWFnLwbokpaRRiaWg@mail.gmail.com>
References: <CAF8s8J_rYv5=KuyOgs-YR9-7oxtPwfBw8k8OKO6Dq+xggvip4A@mail.gmail.com>
 <55439FBA.8050509@treenet.co.nz>
 <CAF8s8J_S151FGsfcYg2cymT=fwkMsUPzpfWFnLwbokpaRRiaWg@mail.gmail.com>
Message-ID: <55473E04.8020502@treenet.co.nz>

On 4/05/2015 5:35 p.m., Eric Keller wrote:
> Thanks for the confirmation.
> 
> Would it be something to change the behaviour of squid-deb-proxy, matching
> the master repository Package.gz file for checksum when the cache gets hit
> and force re cache of the package if it's different?

I dont think so, the squid-deb-proxy package is working fine for what
its stated purpose is. The problem was just that your forced upload
causing an unintended state to exist in the repository.

Amos



From squid3 at treenet.co.nz  Mon May  4 13:40:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 01:40:01 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430659756828-4671050.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
Message-ID: <554776B1.2080401@treenet.co.nz>

On 4/05/2015 1:29 a.m., HackXBack wrote:
> i upgrade to 3.4.13 and still using range_offset_limit none
> making 
> assertion failed: comm.cc:178: "fd_table[conn->fd].......
> i think you forget to upload the patch ?

The experimental bug 3775 patch?
 That is in the current production release (3.5.4).

NP: 3.4 series is deprecated. It does not get bug fixes anymore unless
they are exploitable security bugs (and maybe not even then).

Amos



From squid3 at treenet.co.nz  Mon May  4 09:49:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 May 2015 21:49:51 +1200
Subject: [squid-users] vary headers
In-Reply-To: <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>
Message-ID: <554740BF.9080505@treenet.co.nz>

On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
>> Sent: Sunday, May 03, 2015 at 9:45 PM
>> From: "Yuri Voinov"
> 
>> 
>> I understand what do your want. But for what?
>>> 
> 
> because a "wget --server-response http://someurl" operation that
> replies with a "Vary: user-agent" header always results in a MISS
> even if the same wget version (same user-agent) and computer.
> Instead, multiple copies of the file are stored.

That is not right, the wget being used twice should be MISS then HIT,
just like any oter cacheable traffic.

The nasty thing with Vary:User-Agent is that browsers UA string embeds
so much plugin info they are changing between each different client
request. Which defeats the purpose of caching one clients reply for use
by other clients.

NP: what you had with the store_miss should be working.
 Are you using Squid-3.5 ?
 How are you identifying a fail ?

Amos


From squid3 at treenet.co.nz  Mon May  4 13:59:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 01:59:50 +1200
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <55465456.2090108@cpalmer.me.uk>
References: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
 <55465456.2090108@cpalmer.me.uk>
Message-ID: <55477B56.4080304@treenet.co.nz>

On 4/05/2015 5:01 a.m., Chris Palmer wrote:
> Two other reports of the same problem (accessing some SSL sites) after
> upgrading to Squid 3.5.4...
> 
> https://bugs.archlinux.org/task/44811
> 
> I'm at a bit of a loss to know where to start looking.

Do you see a pattern like what was seen in that bug report?
 IPv6 servers all having 0-bytes transferred on the CONNECT,
 but IPv4 servers having many bytes delivered over it.

The log entries in that report look like Squid is hitting path-MTUd issues.

Amos



From yvoinov at gmail.com  Mon May  4 13:02:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 04 May 2015 19:02:40 +0600
Subject: [squid-users] Youtube redirection loop?
Message-ID: <55476DF0.6040008@gmail.com>

Hi gents.

I made a little research and found one funny problem with Youtube/HTML5 
caching.

When youtube output video, it starts with similar URL:

https://r12---sn-n8v7ln7y.googlevideo.com/videoplayback?c=web&clen=362967&cpn=6UqpMRDeKbePCzao&cver=as3&dur=30.046&expire=1430763762&fexp=900720%2C905652%2C907263%2C908213%2C916943%2C934954%2C938028%2C938813%2C9406849%2C9407992%2C9408352%2C9408708%2C9409252%2C9412764%2C9412987%2C9413139%2C945132%2C946008%2C948124%2C952612%2C952637%2C952642%2C957201&gir=yes&id=o-AKfRzAOg4Wx4u0w4O4uMwhUypQ8lfQ0riWhgnXOmtT_L&ip=178.88.163.102&ipbits=0&itag=140&keepalive=yes&key=cms1&lmt=1406330458866919&mime=audio%2Fmp4&pl=24&range=0-208895&ratebypass=yes&requiressl=yes&signature=72081B97C3180361969C07CDF014AB28B3A98C11.80C66D318F85132651B8044DA65F07A6C910A47B&source=youtube&sparams=clen,dur,expire,gir,id,initcwndbps,ip,ipbits,itag,keepalive,lmt,mime,mm,mn,ms,mv,pl,requiressl,source,upn&sver=3&upn=OgQOo91qH8U&redirect_counter=1&req_id=c3d2118808f2e5a8&cms_redirect=yes&mm=30&mn=sn-n8v7ln7y&ms=nxu&mt=1430741825&mv=u

which has mime type text/plain.

The size of this is above 512 bytes now (consists approx. 1-2 Kb). It is 
not good idea to deny caching redirector with cache object size, because 
of too much objects with size 1-2 Kb will gone cache.

So, when we cache this URL, we got redirection loop and youtube works 
poor or not works completely.

Yes, when I no cache a whole comain (googlevideo.com), it works with 
some delays finally. But I suggest that store-id is also stop working. 
In any case, YT shows serious delays full time with Chrome/FF (in IE all 
works perfectly).

The problem is - audio/video outputs from THE SAME domain.

I suggest, that to solve problem (squid with store-id, and I need cache 
YT) I need to make no cache for text/plain mime from .googlevideo.com 
domain.

I have no idea how to make it with Squid.

Some advice will be helpful.

WBR, Yuri



From keller.eric at gmail.com  Mon May  4 05:35:20 2015
From: keller.eric at gmail.com (Eric Keller)
Date: Mon, 04 May 2015 05:35:20 +0000
Subject: [squid-users] Squid-deb-proxy legacy-tools_0.1_all.deb Size
	mismatch
In-Reply-To: <55439FBA.8050509@treenet.co.nz>
References: <CAF8s8J_rYv5=KuyOgs-YR9-7oxtPwfBw8k8OKO6Dq+xggvip4A@mail.gmail.com>
 <55439FBA.8050509@treenet.co.nz>
Message-ID: <CAF8s8J_S151FGsfcYg2cymT=fwkMsUPzpfWFnLwbokpaRRiaWg@mail.gmail.com>

Thanks for the confirmation.

Would it be something to change the behaviour of squid-deb-proxy, matching
the master repository Package.gz file for checksum when the cache gets hit
and force re cache of the package if it's different?

Have a good day
--
Eric

On Fri, May 1, 2015, 17:46 Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 2/05/2015 3:00 a.m., Eric Keller wrote:
> > Hi everyone,
> >
> > I did encounter a strange behavior with my squid-deb-proxy server.
> > on the master Debian repository server I forced publish an already
> existing
> > Debian package having the same version 0.1 (my bad, I won't do it again)
> >
> > my squid-deb-proxy configuration looks like:
> >
> > ...
> > # refresh pattern for debs and udebs
> > refresh_pattern deb$   129600 100% 129600
> > refresh_pattern udeb$   129600 100% 129600
> > refresh_pattern tar.gz$  129600 100% 129600
> >
> > # always refresh Packages and Release files
> > refresh_pattern \/(Packages|Sources)(|\.bz2|\.gz|\.xz)$ 0 0% 0
> > refresh_pattern \/Release(|\.gpg)$ 0 0% 0
> > refresh_pattern \/InRelease$ 0 0% 0
> > ...
> >
> > as far as I understand, the Packages and Release files are always
> refreshed
> > from the master repository server, but I quite do not understand the
> > meaning of "129600 100% 129600" for Debian packages.
> >
> > I interpret that the Debian packages stay in the cache and are not
> > refreshed. So my package legacy-tools_0.1_all.deb and Release file got
> > updated on the master repository and only the Release file got updated
> > through the squid-deb-proxy but the old version mismatching the Release
> > size of the package is still available in the cache.
> >
> > does this make sense?
>
> Yes, and will remain in cache for 129600 minutes (90 days).
>
> Good example of how forcing things to cache with refresh_pattern can
> bite back badly.
>
> If you know the exact URL of the object, you can do a force-reload like so:
>  squidclient -H 'Cache-Control:no-cache\n' \
>    http://.../legacy-tools_0.1_all.deb
>
> Or, IMHO upload a new package with incremented version so any other
> proxies that have picked it up by now can get fixed quietly as well.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/d03c66e5/attachment.htm>

From squid3 at treenet.co.nz  Mon May  4 10:02:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 May 2015 22:02:18 +1200
Subject: [squid-users] Regex difficulties
In-Reply-To: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>
References: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>
Message-ID: <554743AA.7020307@treenet.co.nz>

On 4/05/2015 4:49 a.m., Bal?zs Szabados wrote:
> Hi,
> 
> I'm trying to hook up these regex files for url filtering:
> http://www.squidguard.org/Doc/Examples/08.expressionlist
> http://www.squidguard.org/Doc/Examples/09.whiteexpression
> 
> I've created the ACL, but when I start squid, I get all these error
> messages:
> 
> aclParseRegexList: Invalid regular expression [...]
> Unmatched ( or \(
> 
> I've checked the regex with a regex validator, all I got is some unescaped
> "/" chars, corrected the escaping, but the issue still persists.
> 
> What am I doing wrong?

The bit that you eliminated there with "[...]" was the critical piece
needed to debug.

Without it we dont know where Squid was up to in reading the regex
pattern list.

Amos


From squid3 at treenet.co.nz  Mon May  4 11:39:15 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 May 2015 23:39:15 +1200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
 connections?
In-Reply-To: <8EC7192A-44C5-476D-9E76-2D8C1DE3AAC3@rinis.nl>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
 <8EC7192A-44C5-476D-9E76-2D8C1DE3AAC3@rinis.nl>
Message-ID: <55475A63.3050605@treenet.co.nz>

On 4/05/2015 10:37 p.m., Abdelouahed Haitoute wrote:
> Hello Amos,
> 
> Could you by the way explain to me what TCP_MISS_ABORTED means in the squid log?

It means the client request needed Squid to contact a server (cache
MISS). The client then disconnected (ABORTED) before the answer could be
sent back to it.


> 
> Its not clear for me what the source of the problem is: client or (squid)server?

The important details that add to the above are the time the transaction
lasted (62854ms / 62.9 sec), bytes transferred (0 / none), and that a
server (192.168.0.20) was contacted.

So it seems that somewhere on the server connection things are being
hung. The client is just giving up on waiting.

Amos



From chris9 at cpalmer.me.uk  Mon May  4 08:53:11 2015
From: chris9 at cpalmer.me.uk (Chris Palmer)
Date: Mon, 04 May 2015 09:53:11 +0100
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <55465456.2090108@cpalmer.me.uk>
References: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
 <55465456.2090108@cpalmer.me.uk>
Message-ID: <55473377.80401@cpalmer.me.uk>

There has been a change in behaviour in 3.5.4. It now really does prefer 
to contact a site using an ipv6 address rather than a v4. The network 
stack here doesn't permit v6 so the traffic to sites such as google was 
failing. Setting the following restored the previous behaviour:

dns_v4_first on

Thanks to Dan Charlesworth for pointing me in the correct direction.

Chris

On 03/05/15 18:01, Chris Palmer wrote:
> Two other reports of the same problem (accessing some SSL sites) after 
> upgrading to Squid 3.5.4...
>
> https://bugs.archlinux.org/task/44811
>
> I'm at a bit of a loss to know where to start looking.
> Just in case, I tried disabling ICAP (was using it for clamav) but no 
> difference.
>
> Chris
>
>> Send squid-users mailing list submissions to
>> Date: Sat, 2 May 2015 12:07:13 +0100
>> From: "Chris Palmer" <chris9 at cpalmer.me.uk>
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
>> Message-ID: <4d032c7eb0e7e4d04a3583b16bca73ff.squirrel at cpalmer.me.uk>
>> Content-Type: text/plain;charset=iso-8859-1
>>
>> I just built 3.5.4 and deployed (on FC21). Most pages work, but SSL to
>> e.g. Google and Yahoo fail. It is easily provoked by simply using the
>> search bar in firefox or IE.
>>
>> Cache.log contains entries such as
>>
>> 2015/05/02 11:51:34 kid1| local=[::] remote=[2a00:1450:400c:c05::93]:443
>> FD 13 flags=1: read/write failure: (107) Transport endpoint is not
>> connected
>>
>> Most SSL sites are ok, and all non-SSL sites I have tried. I am not 
>> using
>> SSL-Bump.
>>
>> It was built using eactly the same options as 3.5.3. Anyone else
>> experiencing this? Otherwise I will have to dig deeper...
>>
>> Many thanks
>> Chris
>>
>



From squid3 at treenet.co.nz  Mon May  4 10:10:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 May 2015 22:10:50 +1200
Subject: [squid-users] Squid ubuntu build error
In-Reply-To: <20150503233529.76052E1652@lists.squid-cache.org>
References: <20150503233529.76052E1652@lists.squid-cache.org>
Message-ID: <554745AA.8090704@treenet.co.nz>

On 4/05/2015 11:35 a.m., Darren B. wrote:
> Hi
> 
> I have struck a small issue in building squid from source (Ubuntu 14.04
> current source packages)


FYI: I've tried providing back-ports for Ubuntu a while back. But the
churn in dpkg tools themselves this past 2 Debian cyces has been so much
that the .deb do not build on other OS versions from one year to the next.

Ubuntu also did a transition from squid3 to squid packaging way ahead of
out upstream sources making it a sort-of clean process or Debian doing so.
I recommend you make sure you are using the .src.deb out of your Ubuntu
version as a basis for any re-builds.


> 
> Can anyone point me to the offending code?

The offending code is buried in the autoconf toolchain. The directries
existence is automatic when the icons/ directory is "make install"ed.

Amos



From ahmed.zaeem at netstream.ps  Mon May  4 19:13:21 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Mon, 4 May 2015 12:13:21 -0700
Subject: [squid-users] Error negotiating SSL connection on FD 12: Success
Message-ID: <000001d0869e$633225d0$29967170$@netstream.ps>

Hi

I created privste & public keys for squid , but it still give me error for
negotiating 

 

 

https_port 443 accel key=/root/CA/myCA/private/squid.local.key
cert=/root/CA/myCA/certs/squid.local.crt

 

 

cache.log

 

2015/05/04 11:59:08 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:10 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:19 kid1| Error negotiating SSL connection on FD 12: Success
(0)

2015/05/04 11:59:21 kid1| Error negotiating SSL connection on FD 12: Success
(0)

 

 

Any help ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/62d40bcd/attachment.htm>

From ahaitoute at rinis.nl  Mon May  4 10:37:14 2015
From: ahaitoute at rinis.nl (Abdelouahed Haitoute)
Date: Mon, 4 May 2015 12:37:14 +0200
Subject: [squid-users] how to achieve squid to handle 2000 concurrent
	connections?
In-Reply-To: <55344F34.1070607@treenet.co.nz>
References: <B58F59BA-00C2-4ACB-A4E7-C9542F1BA6B4@rinis.nl>
 <55344F34.1070607@treenet.co.nz>
Message-ID: <8EC7192A-44C5-476D-9E76-2D8C1DE3AAC3@rinis.nl>

Hello Amos,

Could you by the way explain to me what TCP_MISS_ABORTED means in the squid log?

Its not clear for me what the source of the problem is: client or (squid)server?

Abdelouahed

> Op 20 apr. 2015, om 02:58 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
> Squid is still responding by the client has given up. As shown by the
> _ABORTED in the squid log.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/f4063023/attachment.htm>

From squid3 at treenet.co.nz  Mon May  4 15:00:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 03:00:18 +1200
Subject: [squid-users] about squid choose sibing
In-Reply-To: <tencent_2ABFE65138399BCA3A12A0C4@qq.com>
References: <tencent_2ABFE65138399BCA3A12A0C4@qq.com>
Message-ID: <55478982.2020506@treenet.co.nz>

On 5/05/2015 12:46 a.m., Guang Wei wrote:
> sibing, cache digests and NetDB
> Squid's network measurement database is designed to measure the proximity of origin servers. In other words, by querying this database, Squid knows how close it is to the origin server.
> 

Not quite. NetDB contains all destination timings including other proxies.


> Think about that
> web server A& parent squid B(located in China), child squid C(in China), child squid D(in USA)
> or 
> web server A & parent squid B(located in China), parent squid C(in China), parent squid D(in USA)

parent of what? child of what?

Please document the connectivity segments in grapviz notation
 (client -> proxyB -> server),
or describe clearly what each network connection is for all connections
separately,
or show squid.conf cache_peer lines labelled by proxy.

Amos



From hussam.tayeb at gmx.com  Mon May  4 15:15:25 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Mon, 4 May 2015 17:15:25 +0200
Subject: [squid-users] vary headers
In-Reply-To: <554740BF.9080505@treenet.co.nz>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
Message-ID: <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>



> Sent: Monday, May 04, 2015 at 12:49 PM
> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
> >> Sent: Sunday, May 03, 2015 at 9:45 PM
> >> From: "Yuri Voinov"
> > 
> >> 
> >> I understand what do your want. But for what?
> >>> 
> > 
> > because a "wget --server-response http://someurl" operation that
> > replies with a "Vary: user-agent" header always results in a MISS
> > even if the same wget version (same user-agent) and computer.
> > Instead, multiple copies of the file are stored.
> 
> That is not right, the wget being used twice should be MISS then HIT,
> just like any oter cacheable traffic.
> 
> The nasty thing with Vary:User-Agent is that browsers UA string embeds
> so much plugin info they are changing between each different client
> request. Which defeats the purpose of caching one clients reply for use
> by other clients.
> 
> NP: what you had with the store_miss should be working.
>  Are you using Squid-3.5 ?
>  How are you identifying a fail ?
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
Hello. I am using 3.5.4
There are new objects on disk that have the Vary: User-Agent Http header.
I can tell for example if type head -n13 /home/squid/04/D1/0004D122


From squid3 at treenet.co.nz  Mon May  4 15:32:12 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 03:32:12 +1200
Subject: [squid-users] vary headers
In-Reply-To: <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
 <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>
Message-ID: <554790FC.5050105@treenet.co.nz>

On 5/05/2015 3:15 a.m., Hussam Al-Tayeb wrote:
> 
> 
>> Sent: Monday, May 04, 2015 at 12:49 PM
>> From: "Amos Jeffries" <squid3 at treenet.co.nz>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>>
>> On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
>>>> Sent: Sunday, May 03, 2015 at 9:45 PM
>>>> From: "Yuri Voinov"
>>>
>>>>
>>>> I understand what do your want. But for what?
>>>>>
>>>
>>> because a "wget --server-response http://someurl" operation that
>>> replies with a "Vary: user-agent" header always results in a MISS
>>> even if the same wget version (same user-agent) and computer.
>>> Instead, multiple copies of the file are stored.
>>
>> That is not right, the wget being used twice should be MISS then HIT,
>> just like any oter cacheable traffic.
>>
>> The nasty thing with Vary:User-Agent is that browsers UA string embeds
>> so much plugin info they are changing between each different client
>> request. Which defeats the purpose of caching one clients reply for use
>> by other clients.
>>
>> NP: what you had with the store_miss should be working.
>>  Are you using Squid-3.5 ?
>>  How are you identifying a fail ?
>>

> Hello. I am using 3.5.4
> There are new objects on disk that have the Vary: User-Agent Http header.
> I can tell for example if type head -n13 /home/squid/04/D1/0004D122
> 

That is not a good way to identify. All it means is that the object used
a disk file for its transfer. Cache files are also sometimes used as
on-disk buffers.

If you check with store.log for ID (0004D122) you should expect to see
that file pushed to disk, then a cache index RELEASED action performed.
The file part may stay on disk until something else needs to use the
same filename.

The ways to identify caching activity is:
 * access.log - checking that no HIT or REFRESH occur on the relevant
URLs, or
 * store.log - checking that objects with the URLs are all getting that
RELEASED action.
 * cache.log - setting "debug_options 20,3" and watching for "store_miss
prohibits caching"

There is unfortunatly currently no easy debugs linking what URL
store_miss prohibited to correlate the logs :-(

Amos





From ulises at vianetcon.com.ar  Mon May  4 15:35:28 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Mon, 04 May 2015 12:35:28 -0300
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <55433176.8040500@ngtech.co.il>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar> <5540FA20.9050208@gmail.com>
 <5542FD81.5010404@ngtech.co.il> <55431235.10305@treenet.co.nz>
 <55433176.8040500@ngtech.co.il>
Message-ID: <554791C0.7080301@vianetcon.com.ar>

Hello Amos and Eliezer,

Regarding this:

 > My guess is that its been patched to cope with the action code and
 > kv-pair syntax. But not concurrency enabled. Which is mandatory on the
 > Store-ID interface.

You are right about the kv-pair syntax patch, that's exactly it. But 
about concurrency, we're running Squid as a unique process. Do we need 
to modify jesred in order to handle Store-ID interface? The only 
modification we did to jesred is that we changed the output string that 
it gives back to Squid.


About this:

 > You really sure 20 children is enough for 1200 clients? Also whenever
 > bypass on?

Is there any formula to calculate how many children should we use based 
on the amount of requests per second? Or is it just try and error?




We' re getting the impresion that jesred processes are waiting for some 
kind of signal to die once they're no longer necessary, ergo they just 
stay there, doing nothing.



Regards,
Ulises


El 01/05/15 04:55,   Croitoru escribi?:
> So I assume it's a rewrite for the urls.
> I will try to take a look at jesered and later might be able to 
> identify a way to use a golang helper for a basic load test comparison.
>
> Eliezer
>
> On 01/05/2015 08:42, Amos Jeffries wrote:
>> On 1/05/2015 4:13 p.m., Eliezer Croitoru wrote:
>>> On 29/04/2015 18:34, Yuri Voinov wrote:
>>>> You really sure 20 children is enough for 1200 clients? Also whenever
>>>> bypass on?
>>>
>>> I will add "what language is the helper??".
>>
>> It's a patched jesred, so "C".
>>
>> The problem described "jesred hanging" is clearly a problem in jesred
>> itself. Not Squid.
>>
>> My guess is that its been patched to cope with the action code and
>> kv-pair syntax. But not concurrency enabled. Which is mandatory on the
>> Store-ID interface.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/14f99615/attachment.htm>

From squid3 at treenet.co.nz  Mon May  4 15:57:52 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 03:57:52 +1200
Subject: [squid-users] A lot of open rewriter heplers and are hanging!
 Squid 3.5
In-Reply-To: <554791C0.7080301@vianetcon.com.ar>
References: <5540DB94.8090307@vianetcon.com.ar>
 <5540E976.3030902@vianetcon.com.ar> <5540FA20.9050208@gmail.com>
 <5542FD81.5010404@ngtech.co.il> <55431235.10305@treenet.co.nz>
 <55433176.8040500@ngtech.co.il> <554791C0.7080301@vianetcon.com.ar>
Message-ID: <55479700.10506@treenet.co.nz>

On 5/05/2015 3:35 a.m., Ulises Nicolini wrote:
> Hello Amos and Eliezer,
> 
> Regarding this:
> 
>> My guess is that its been patched to cope with the action code and
>> kv-pair syntax. But not concurrency enabled. Which is mandatory on the
>> Store-ID interface.
> 
> You are right about the kv-pair syntax patch, that's exactly it. But
> about concurrency, we're running Squid as a unique process. Do we need
> to modify jesred in order to handle Store-ID interface? The only
> modification we did to jesred is that we changed the output string that
> it gives back to Squid.
> 

The Store-ID interface requires concurreny support in the helpers it uses.

For your jesred it would mean receiving the channel-ID value at the
start of each request line from Squid and echoing it back as a prefix on
the answer lines.


> 
> About this:
> 
>> You really sure 20 children is enough for 1200 clients? Also whenever
>> bypass on?
> 
> Is there any formula to calculate how many children should we use based
> on the amount of requests per second? Or is it just try and error?
> 

There is, but you need to know the helper req/sec capacity to alculate it.

The easy way with current Squid is to set a high children number, with a
lower start=N and small idle=N parameters. Squid starts what helpers are
needed dynamically.


> 
> We' re getting the impresion that jesred processes are waiting for some
> kind of signal to die once they're no longer necessary, ergo they just
> stay there, doing nothing.
> 

If a helper dont have concurrency but Squid is expecting it to, the
first request may "work" but a second one will definitely hang.

Amos


From squid3 at treenet.co.nz  Mon May  4 16:10:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 04:10:59 +1200
Subject: [squid-users] Regex difficulties
In-Reply-To: <554743AA.7020307@treenet.co.nz>
References: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>
 <554743AA.7020307@treenet.co.nz>
Message-ID: <55479A13.20301@treenet.co.nz>

On 4/05/2015 10:02 p.m., Amos Jeffries wrote:
> On 4/05/2015 4:49 a.m., Bal?zs Szabados wrote:
>> Hi,
>>
>> I'm trying to hook up these regex files for url filtering:
>> http://www.squidguard.org/Doc/Examples/08.expressionlist
>> http://www.squidguard.org/Doc/Examples/09.whiteexpression
>>

Looking at those files, they contain single pattern lines exceeding 2KB
in length.

Squid will not read more than 2048 bytes on any single file line.
Including the line terminator. The result will be that the regex parser
is presented with a truncated line.

Amos



From hussam.tayeb at gmx.com  Mon May  4 16:38:15 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Mon, 4 May 2015 18:38:15 +0200
Subject: [squid-users] vary headers
In-Reply-To: <554790FC.5050105@treenet.co.nz>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
 <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>,
 <554790FC.5050105@treenet.co.nz>
Message-ID: <trinity-8e3105e3-f2bf-42db-af4a-058335061d49-1430757495182@3capp-mailcom-bs15>



> Sent: Monday, May 04, 2015 at 6:32 PM
> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> To: "Hussam Al-Tayeb" <hussam.tayeb at gmx.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> On 5/05/2015 3:15 a.m., Hussam Al-Tayeb wrote:
> > 
> > 
> >> Sent: Monday, May 04, 2015 at 12:49 PM
> >> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> >> To: squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] vary headers
> >>
> >> On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
> >>>> Sent: Sunday, May 03, 2015 at 9:45 PM
> >>>> From: "Yuri Voinov"
> >>>
> >>>>
> >>>> I understand what do your want. But for what?
> >>>>>
> >>>
> >>> because a "wget --server-response http://someurl" operation that
> >>> replies with a "Vary: user-agent" header always results in a MISS
> >>> even if the same wget version (same user-agent) and computer.
> >>> Instead, multiple copies of the file are stored.
> >>
> >> That is not right, the wget being used twice should be MISS then HIT,
> >> just like any oter cacheable traffic.
> >>
> >> The nasty thing with Vary:User-Agent is that browsers UA string embeds
> >> so much plugin info they are changing between each different client
> >> request. Which defeats the purpose of caching one clients reply for use
> >> by other clients.
> >>
> >> NP: what you had with the store_miss should be working.
> >>  Are you using Squid-3.5 ?
> >>  How are you identifying a fail ?
> >>
> 
> > Hello. I am using 3.5.4
> > There are new objects on disk that have the Vary: User-Agent Http header.
> > I can tell for example if type head -n13 /home/squid/04/D1/0004D122
> > 
> 
> That is not a good way to identify. All it means is that the object used
> a disk file for its transfer. Cache files are also sometimes used as
> on-disk buffers.
> 
> If you check with store.log for ID (0004D122) you should expect to see
> that file pushed to disk, then a cache index RELEASED action performed.
> The file part may stay on disk until something else needs to use the
> same filename.
> 
> The ways to identify caching activity is:
>  * access.log - checking that no HIT or REFRESH occur on the relevant
> URLs, or
>  * store.log - checking that objects with the URLs are all getting that
> RELEASED action.
>  * cache.log - setting "debug_options 20,3" and watching for "store_miss
> prohibits caching"
> 
> There is unfortunatly currently no easy debugs linking what URL
> store_miss prohibited to correlate the logs :-(
> 
> Amos
> 
> 
> 
> 
ok, I tried setting debug_options.
this is part of what I found:
http://pastebin.com/raw.php?i=4HTw9es3

So it looks like it only blocks caching of Vary header if followed by "Accept-Encoding: gzip,deflate"?


From squid3 at treenet.co.nz  Mon May  4 18:04:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 06:04:01 +1200
Subject: [squid-users] vary headers
In-Reply-To: <trinity-8e3105e3-f2bf-42db-af4a-058335061d49-1430757495182@3capp-mailcom-bs15>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
 <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>,
 <554790FC.5050105@treenet.co.nz>
 <trinity-8e3105e3-f2bf-42db-af4a-058335061d49-1430757495182@3capp-mailcom-bs15>
Message-ID: <5547B491.20302@treenet.co.nz>

On 5/05/2015 4:38 a.m., Hussam Al-Tayeb wrote:
> 
> 
>> Sent: Monday, May 04, 2015 at 6:32 PM
>> From: "Amos Jeffries" <squid3 at treenet.co.nz>
>> To: "Hussam Al-Tayeb" <hussam.tayeb at gmx.com>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] vary headers
>>
>> On 5/05/2015 3:15 a.m., Hussam Al-Tayeb wrote:
>>>
>>>
>>>> Sent: Monday, May 04, 2015 at 12:49 PM
>>>> From: "Amos Jeffries" <squid3 at treenet.co.nz>
>>>> To: squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] vary headers
>>>>
>>>> On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
>>>>>> Sent: Sunday, May 03, 2015 at 9:45 PM
>>>>>> From: "Yuri Voinov"
>>>>>
>>>>>>
>>>>>> I understand what do your want. But for what?
>>>>>>>
>>>>>
>>>>> because a "wget --server-response http://someurl" operation that
>>>>> replies with a "Vary: user-agent" header always results in a MISS
>>>>> even if the same wget version (same user-agent) and computer.
>>>>> Instead, multiple copies of the file are stored.
>>>>
>>>> That is not right, the wget being used twice should be MISS then HIT,
>>>> just like any oter cacheable traffic.
>>>>
>>>> The nasty thing with Vary:User-Agent is that browsers UA string embeds
>>>> so much plugin info they are changing between each different client
>>>> request. Which defeats the purpose of caching one clients reply for use
>>>> by other clients.
>>>>
>>>> NP: what you had with the store_miss should be working.
>>>>  Are you using Squid-3.5 ?
>>>>  How are you identifying a fail ?
>>>>
>>
>>> Hello. I am using 3.5.4
>>> There are new objects on disk that have the Vary: User-Agent Http header.
>>> I can tell for example if type head -n13 /home/squid/04/D1/0004D122
>>>
>>
>> That is not a good way to identify. All it means is that the object used
>> a disk file for its transfer. Cache files are also sometimes used as
>> on-disk buffers.
>>
>> If you check with store.log for ID (0004D122) you should expect to see
>> that file pushed to disk, then a cache index RELEASED action performed.
>> The file part may stay on disk until something else needs to use the
>> same filename.
>>
>> The ways to identify caching activity is:
>>  * access.log - checking that no HIT or REFRESH occur on the relevant
>> URLs, or
>>  * store.log - checking that objects with the URLs are all getting that
>> RELEASED action.
>>  * cache.log - setting "debug_options 20,3" and watching for "store_miss
>> prohibits caching"
>>
>> There is unfortunatly currently no easy debugs linking what URL
>> store_miss prohibited to correlate the logs :-(
>>
>> Amos
>>
>>
>>
>>
> ok, I tried setting debug_options.
> this is part of what I found:
> http://pastebin.com/raw.php?i=4HTw9es3
> 
> So it looks like it only blocks caching of Vary header if followed by "Accept-Encoding: gzip,deflate"?
> 

Ah, your regex pattern was "." so if Vary header exists at all it will
block that response caching.

Since its working now use:

 acl hasVary rep_header Vary User-Agent


Amos


From hussam.tayeb at gmx.com  Mon May  4 18:09:45 2015
From: hussam.tayeb at gmx.com (Hussam Al-Tayeb)
Date: Mon, 4 May 2015 20:09:45 +0200
Subject: [squid-users] vary headers
In-Reply-To: <5547B491.20302@treenet.co.nz>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
 <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>,
 <554790FC.5050105@treenet.co.nz>
 <trinity-8e3105e3-f2bf-42db-af4a-058335061d49-1430757495182@3capp-mailcom-bs15>,
 <5547B491.20302@treenet.co.nz>
Message-ID: <trinity-9a899f99-a1d5-4620-ba5f-9189e6d2b713-1430762984990@3capp-mailcom-bs15>



> Sent: Monday, May 04, 2015 at 9:04 PM
> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> To: "Hussam Al-Tayeb" <hussam.tayeb at gmx.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] vary headers
>
> On 5/05/2015 4:38 a.m., Hussam Al-Tayeb wrote:
> > 
> > 
> >> Sent: Monday, May 04, 2015 at 6:32 PM
> >> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> >> To: "Hussam Al-Tayeb" <hussam.tayeb at gmx.com>
> >> Cc: squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] vary headers
> >>
> >> On 5/05/2015 3:15 a.m., Hussam Al-Tayeb wrote:
> >>>
> >>>
> >>>> Sent: Monday, May 04, 2015 at 12:49 PM
> >>>> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> >>>> To: squid-users at lists.squid-cache.org
> >>>> Subject: Re: [squid-users] vary headers
> >>>>
> >>>> On 4/05/2015 6:54 a.m., Hussam Al-Tayeb wrote:
> >>>>>> Sent: Sunday, May 03, 2015 at 9:45 PM
> >>>>>> From: "Yuri Voinov"
> >>>>>
> >>>>>>
> >>>>>> I understand what do your want. But for what?
> >>>>>>>
> >>>>>
> >>>>> because a "wget --server-response http://someurl" operation that
> >>>>> replies with a "Vary: user-agent" header always results in a MISS
> >>>>> even if the same wget version (same user-agent) and computer.
> >>>>> Instead, multiple copies of the file are stored.
> >>>>
> >>>> That is not right, the wget being used twice should be MISS then HIT,
> >>>> just like any oter cacheable traffic.
> >>>>
> >>>> The nasty thing with Vary:User-Agent is that browsers UA string embeds
> >>>> so much plugin info they are changing between each different client
> >>>> request. Which defeats the purpose of caching one clients reply for use
> >>>> by other clients.
> >>>>
> >>>> NP: what you had with the store_miss should be working.
> >>>>  Are you using Squid-3.5 ?
> >>>>  How are you identifying a fail ?
> >>>>
> >>
> >>> Hello. I am using 3.5.4
> >>> There are new objects on disk that have the Vary: User-Agent Http header.
> >>> I can tell for example if type head -n13 /home/squid/04/D1/0004D122
> >>>
> >>
> >> That is not a good way to identify. All it means is that the object used
> >> a disk file for its transfer. Cache files are also sometimes used as
> >> on-disk buffers.
> >>
> >> If you check with store.log for ID (0004D122) you should expect to see
> >> that file pushed to disk, then a cache index RELEASED action performed.
> >> The file part may stay on disk until something else needs to use the
> >> same filename.
> >>
> >> The ways to identify caching activity is:
> >>  * access.log - checking that no HIT or REFRESH occur on the relevant
> >> URLs, or
> >>  * store.log - checking that objects with the URLs are all getting that
> >> RELEASED action.
> >>  * cache.log - setting "debug_options 20,3" and watching for "store_miss
> >> prohibits caching"
> >>
> >> There is unfortunatly currently no easy debugs linking what URL
> >> store_miss prohibited to correlate the logs :-(
> >>
> >> Amos
> >>
> >>
> >>
> >>
> > ok, I tried setting debug_options.
> > this is part of what I found:
> > http://pastebin.com/raw.php?i=4HTw9es3
> > 
> > So it looks like it only blocks caching of Vary header if followed by "Accept-Encoding: gzip,deflate"?
> > 
> 
> Ah, your regex pattern was "." so if Vary header exists at all it will
> block that response caching.
> 
> Since its working now use:
> 
>  acl hasVary rep_header Vary User-Agent
> 
> 
> Amos
> 
Ok, thank you. How would I modify that to include
Vary: somethingelse, User-Agent
and Vary: User-Agent, somethingelse?
Thanks again!


From szabados0701 at gmail.com  Mon May  4 21:25:31 2015
From: szabados0701 at gmail.com (=?UTF-8?Q?Bal=C3=A1zs_Szabados?=)
Date: Mon, 4 May 2015 23:25:31 +0200
Subject: [squid-users] Regex difficulties
In-Reply-To: <55479A13.20301@treenet.co.nz>
References: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>
 <554743AA.7020307@treenet.co.nz> <55479A13.20301@treenet.co.nz>
Message-ID: <CADddWkqSYKOpMBLpz=KUpEjZfEYj9EHwAAtNtTNcyFzYGWPZWA@mail.gmail.com>

Hi Amos,

I tried troubleshooting you've mentioned. I just enabled one file
containing regexes.
I've measured it:
root at OpenWrt:~# wc -c /etc/squid/blacklists/regex_allow
1636 /etc/squid/blacklists/regex_allow

Since the whole file is 1636 bytes, I assume ever line of regexes is in
must be smaller than that.
Starting up squid I'm getting these:

2015/05/04 23:17:44| aclParseRegexList: Invalid regular expression
'(.*(acoon\.de|openacoon\.de|omgili\.com|youtube\.com|slug\.ch|\/search\?|mister-wong\.de|mister-wong\.com|abacho\.|seekport\.|flickr\.com|altavista|ask\..com|answers\.com|clusty\.com|exalead\.com|metacrawler|dogpile|lycos|google|yahoo|dmoz|search\.|wikise':
Unmatched ( or \(

I've created a file with just that part in the error message, and it seems
to be 256 bytes (wc -c again).
Isn't that possible, that this is the limit, you've mentioned?

Thanks,
Balazs


2015-05-04 18:10 GMT+02:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 4/05/2015 10:02 p.m., Amos Jeffries wrote:
> > On 4/05/2015 4:49 a.m., Bal?zs Szabados wrote:
> >> Hi,
> >>
> >> I'm trying to hook up these regex files for url filtering:
> >> http://www.squidguard.org/Doc/Examples/08.expressionlist
> >> http://www.squidguard.org/Doc/Examples/09.whiteexpression
> >>
>
> Looking at those files, they contain single pattern lines exceeding 2KB
> in length.
>
> Squid will not read more than 2048 bytes on any single file line.
> Including the line terminator. The result will be that the regex parser
> is presented with a truncated line.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150504/d7d7b464/attachment.htm>

From vdoctor at neuf.fr  Mon May  4 21:29:13 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 4 May 2015 14:29:13 -0700 (PDT)
Subject: [squid-users] Number of clients accessing cache: 0
Message-ID: <1430774953477-4671102.post@n4.nabble.com>

Hi All,
Seems the number of connected clients is always 0 (zero) since the 3.5.3...

We have tested with 10+ differents and simultaneous client ips and the
number always shows 0.
Latest tested build, the 3.5.4 official, still 0 as clients accessing the
cache...

Is there something wrong here ?

Here is a sample:
/usr/local/squid3/bin/squidclient -h 127.0.0.1 -p 3128 mgr:info |grep
"Number of"
        Number of clients accessing cache:      0
        Number of HTTP requests received:       9189
        Number of ICP messages received:        8581
        Number of ICP messages sent:    8625
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Number of file desc currently in use:   30


Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Number-of-clients-accessing-cache-0-tp4671102.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon May  4 22:07:35 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 4 May 2015 15:07:35 -0700 (PDT)
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <55476DF0.6040008@gmail.com>
References: <55476DF0.6040008@gmail.com>
Message-ID: <1430777255139-4671103.post@n4.nabble.com>

Okay Sir,
this is the solution

1st: put this conf in your squid.conf

####for looping 302 on youtube
acl text-html rep_mime_type text/html
acl http302 http_status 302
store_miss deny text-html
store_miss deny http302
send_hit deny text-html
send_hit deny http302


2nd: use this patch:



--- src/client_side_request.cc  2014-03-09 06:40:56.000000000 -0300
+++ src/client_side_request.cc  2014-04-21 02:53:11.277155130 -0300
@@ -545,6 +545,16 @@
             }
             debugs(85, 3, HERE << "validate IP " << clientConn->local << "
non-match from Host: IP " << ia->in_addrs[i]);
         }
+ 
+        if (true) {
+            unsigned short port = clientConn->local.port();
+            debugs(85, 3, HERE << "[anti-forgery] Host-non-matched remote
IP (" << clientConn->local << ") was replaced with the first Host resolved
IP (" << ia->in_addrs[0] << ":" << clientConn->local.port() << ")");
+            clientConn->local = ia->in_addrs[0];
+            clientConn->local.port(port);
+            http->request->flags.hostVerified = true;
+            http->doCallouts();
+            return;
+        }
     }
     debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
possible from Host:");
     hostHeaderVerifyFailed("local IP", "any domain IP");


--- src/Server.cc
+++ src/Server.cc
@@ -31,6 +31,7 @@
  */
 
 #include "squid.h"
+#include "acl/FilledChecklist.h"
 #include "acl/Gadgets.h"
 #include "base/TextException.h"
 #include "comm/Connection.h"
@@ -174,6 +175,8 @@
     // give entry the reply because haveParsedReplyHeaders() expects it
there
     entry->replaceHttpReply(theFinalReply, false); // but do not write yet
     haveParsedReplyHeaders(); // update the entry/reply (e.g., set
timestamps)
+    if (EBIT_TEST(entry->flags, ENTRY_CACHABLE) && blockCaching())
+        entry->release();
     entry->startWriting(); // write the updated entry to store
 
     return theFinalReply;
@@ -533,6 +536,24 @@
     currentOffset = partial ? theFinalReply->content_range->spec.offset :
0;
 }
 
+/// whether to prevent caching of an otherwise cachable response
+bool
+ServerStateData::blockCaching()
+{
+    if (const Acl::Tree *acl = Config.accessList.storeMiss) {
+        // This relatively expensive check is not in
StoreEntry::checkCachable:
+        // That method lacks HttpRequest and may be called too many times.
+        ACLFilledChecklist ch(acl, originalRequest(), NULL);
+        ch.reply = const_cast<HttpReply*>(entry->getReply()); //
ACLFilledChecklist API bug
+        HTTPMSGLOCK(ch.reply);
+        if (ch.fastCheck() != ACCESS_ALLOWED) { // when in doubt, block
+            debugs(20, 3, "store_miss prohibits caching");
+            return true;
+        }
+    }
+    return false;
+}
+
 HttpRequest *
 ServerStateData::originalRequest()
 {
--- src/Server.h
+++ src/Server.h
@@ -131,6 +131,8 @@
     /// Entry-dependent callbacks use this check to quit if the entry went
bad
     bool abortOnBadEntry(const char *abortReason);
 
+    bool blockCaching();
+
 #if USE_ADAPTATION
     void startAdaptation(const Adaptation::ServiceGroupPointer &group,
HttpRequest *cause);
     void adaptVirginReplyBody(const char *buf, ssize_t len);
--- src/SquidConfig.h
+++ src/SquidConfig.h
@@ -375,6 +375,8 @@
         acl_access *AlwaysDirect;
         acl_access *ASlists;
         acl_access *noCache;
+        acl_access *sendHit;
+        acl_access *storeMiss;
         acl_access *stats_collection;
 #if SQUID_SNMP
 
--- src/cf.data.pre
+++ src/cf.data.pre
@@ -4843,18 +4843,97 @@
 NAME: cache no_cache
 TYPE: acl_access
 DEFAULT: none
-DEFAULT_DOC: Allow caching, unless rules exist in squid.conf.
+DEFAULT_DOC: By default, this directive is unused and has no effect.
 LOC: Config.accessList.noCache
 DOC_START
-	A list of ACL elements which, if matched and denied, cause the request to
-	not be satisfied from the cache and the reply to not be cached.
-	In other words, use this to force certain objects to never be cached.
-
-	You must use the words 'allow' or 'deny' to indicate whether items
-	matching the ACL should be allowed or denied into the cache.
+	Requests denied by this directive will not be served from the cache
+	and their responses will not be stored in the cache. This directive
+	has no effect on other transactions and on already cached responses.
 
 	This clause supports both fast and slow acl types.
 	See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	This and the two other similar caching directives listed below are
+	checked at different transaction processing stages, have different
+	access to response information, affect different cache operations,
+	and differ in slow ACLs support:
+
+	* cache: Checked before Squid makes a hit/miss determination.
+		No access to reply information!
+		Denies both serving a hit and storing a miss.
+		Supports both fast and slow ACLs.
+	* send_hit: Checked after a hit was detected.
+		Has access to reply (hit) information.
+		Denies serving a hit only.
+		Supports fast ACLs only.
+	* store_miss: Checked before storing a cachable miss.
+		Has access to reply (miss) information.
+		Denies storing a miss only.
+		Supports fast ACLs only.
+
+	If you are not sure which of the three directives to use, apply the
+	following decision logic:
+
+	* If your ACL(s) are of slow type _and_ need response info, redesign.
+	  Squid does not support that particular combination at this time.
+        Otherwise:
+	* If your directive ACL(s) are of slow type, use "cache"; and/or
+	* if your directive ACL(s) need no response info, use "cache".
+        Otherwise:
+	* If you do not want the response cached, use store_miss; and/or
+	* if you do not want a hit on a cached response, use send_hit.
+DOC_END
+
+NAME: send_hit
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.sendHit
+DOC_START
+	Responses denied by this directive will not be served from the cache
+	(but may still be cached, see store_miss). This directive has no
+	effect on the responses it allows and on the cached objects.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives.
+
+	Unlike the "cache" directive, send_hit only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
+
+	For example:
+
+		# apply custom Store ID mapping to some URLs
+		acl MapMe dstdomain .c.example.com
+		store_id_program ...
+		store_id_access allow MapMe
+
+		# but prevent caching of special responses
+		# such as 302 redirects that cause StoreID loops
+		acl Ordinary http_status 200-299
+		store_miss deny MapMe !Ordinary
+
+		# and do not serve any previously stored special responses
+		# from the cache (in case they were already cached before
+		# the above store_miss rule was in effect).
+		send_hit deny MapMe !Ordinary
+DOC_END
+
+NAME: store_miss
+TYPE: acl_access
+DEFAULT: none
+DEFAULT_DOC: By default, this directive is unused and has no effect.
+LOC: Config.accessList.storeMiss
+DOC_START
+	Responses denied by this directive will not be cached (but may still
+	be served from the cache, see send_hit). This directive has no
+	effect on the responses it allows and on the already cached responses.
+
+	Please see the "cache" directive for a summary of differences among
+	store_miss, send_hit, and cache directives. See the
+	send_hit directive for a usage example.
+
+	Unlike the "cache" directive, store_miss only supports fast acl
+	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
 DOC_END
 
 NAME: max_stale
--- src/client_side_reply.cc
+++ src/client_side_reply.cc
@@ -545,6 +545,11 @@
        ) {
         http->logType = LOG_TCP_NEGATIVE_HIT;
         sendMoreData(result);
+    } else if (blockedHit()) {
+        debugs(88, 5, "send_hit forces a MISS");
+        http->logType = LOG_TCP_MISS;
+        processMiss();
+        return;
     } else if (!http->flags.internal && refreshCheckHTTP(e, r)) {
         debugs(88, 5, "clientCacheHit: in refreshCheck() block");
         /*
@@ -773,6 +778,30 @@
     }
 }
 
+/// whether squid.conf send_hit prevents us from serving this hit
+bool
+clientReplyContext::blockedHit() const
+{
+    if (!Config.accessList.sendHit)
+        return false; // hits are not blocked by default
+
+    if (http->flags.internal)
+        return false; // internal content "hits" cannot be blocked
+
+    if (const HttpReply *rep = http->storeEntry()->getReply()) {
+        std::auto_ptr<ACLFilledChecklist>
chl(clientAclChecklistCreate(Config.accessList.sendHit, http));
+        chl->reply = const_cast<HttpReply*>(rep); // ACLChecklist API bug
+        HTTPMSGLOCK(chl->reply);
+        return chl->fastCheck() != ACCESS_ALLOWED; // when in doubt, block
+    }
+
+    // This does not happen, I hope, because we are called from CacheHit,
which
+    // is called via a storeClientCopy() callback, and store should
initialize
+    // the reply before calling that callback.
+    debugs(88, 3, "Missing reply!");
+    return false;
+}
+
 void
 clientReplyContext::purgeRequestFindObjectToPurge()
 {
--- src/client_side_reply.h
+++ src/client_side_reply.h
@@ -140,6 +140,7 @@
     void triggerInitialStoreRead();
     void sendClientOldEntry();
     void purgeAllCached();
+    bool blockedHit() const;
 
     void sendBodyTooLargeError();
     void sendPreconditionFailedError();




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671103.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon May  4 22:09:07 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 4 May 2015 15:09:07 -0700 (PDT)
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <554776B1.2080401@treenet.co.nz>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz>
Message-ID: <1430777347185-4671104.post@n4.nabble.com>

but am not ready now to use 3.5.4
can i use this patch on 3.4 without any problem ?
Thanks Amos.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-178-fd-table-conn-fd-halfClosedReader-NULL-tp4670979p4671104.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hack.back at hotmail.com  Mon May  4 22:09:59 2015
From: hack.back at hotmail.com (HackXBack)
Date: Mon, 4 May 2015 15:09:59 -0700 (PDT)
Subject: [squid-users] Number of clients accessing cache: 0
In-Reply-To: <1430774953477-4671102.post@n4.nabble.com>
References: <1430774953477-4671102.post@n4.nabble.com>
Message-ID: <1430777399216-4671105.post@n4.nabble.com>

root at issa:~# squidclient -h 127.0.0.1 -p 3128 mgr:info |grep "Number of"
Sending HTTP request ... done.
        Number of clients accessing cache:      0
        Number of HTTP requests received:       6498250
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Number of file desc currently in use: 1927




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Number-of-clients-accessing-cache-0-tp4671102p4671105.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Mon May  4 22:29:17 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 4 May 2015 22:29:17 +0000
Subject: [squid-users] Squid ubuntu build error
In-Reply-To: <20150503233529.2ED6FE164F@lists.squid-cache.org>
References: <20150503233529.2ED6FE164F@lists.squid-cache.org>
Message-ID: <DB5PR04MB1128FE0A75E397C98D7B88AD8FD20@DB5PR04MB1128.eurprd04.prod.outlook.com>

This is how we rebuild Squid 3 from Ubuntu for SSL bump (taken from http://docs.diladele.com/administrator_guide_4_1/system_configuration/https_filtering/recompile_squid.html)


apt-get update && apt-get -y upgrade

# install build tools
apt-get -y install devscripts build-essential fakeroot libssl-dev

# fetch the source for the package to re-build
apt-get source squid3

# fetch dependent packages for the build
apt-get -y build-dep squid3

# build the package
cd squid3-3.3.8 && dpkg-buildpackage -rfakeroot -b

# install some more required packages
apt-get -y install ssl-cert
apt-get -y install squid-langpack

# install recompiled packages
dpkg --install squid3-common_3.3.8-1ubuntu6.2_all.deb
dpkg --install squid3_3.3.8-1ubuntu6.2_amd64.deb
dpkg --install squidclient_3.3.8-1ubuntu6.2_amd64.deb

# put the squid on hold to prevent updating
apt-mark hold squid3 squid3-common

Raf



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Darren B.
Sent: Monday, May 4, 2015 1:35 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid ubuntu build error

Hi

I have struck a small issue in building squid from source (Ubuntu 14.04 current source packages)

cp: cannot stat
?/home/darren/squid3-3.3.8/debian/tmp/usr/share/squid3/icons?: No such file or directory
dh_install: cp -a
/home/darren/squid3-3.3.8/debian/tmp/usr/share/squid3/icons
debian/squid3-common//usr/share/squid3/ returned exit code 1

it looks like something upstream of this error is not creating
tmp/usr/share/squid3 so creating the icons directory fails.

If I create the missing directory and do a make it's all fine but I am trying to create packages from source (adding SSL) and the clena up prior to the make deletes the directories.

This looks like a bug in the distro as I have built previous version s from source without a bother.

Can anyone point me to the offending code?

thanks

Darren Breeze







_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From prashanth.prabhu at gmail.com  Mon May  4 22:59:07 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Mon, 4 May 2015 15:59:07 -0700
Subject: [squid-users] Squid crashes with 3.5.1
Message-ID: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>

Hi folks,

I am seeing a bunch of Squid crashes after moving recently to 3.5.1.
The stack trace for the various crashes is below, along with info I
have been able to gather for them. This is on a setup where we have
Squid running in conjunction with c-icap (both on the same box).The
steps that led to these crashes isn't known yet.

Squid runs in non-transparent mode. It proxies both HTTP and HTTPS connections.

A couple of the crashes look to have been caused in the path where
Squid interacts with c-icap. The c-icap version is somewhat old --
0.1.7 -- although it was working well with a version 3.3.13 Squid
until the recent move.

Do any of the fixes (listed below, where I have found references) help
here? Any other known issues that may be related?


Thanks.
Prashanth


pprabhu:~$ sudo egrep 'assertion failed' /var/log/elastica/gateway/cache.log
2015/03/31 19:56:09| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/01 07:04:59| assertion failed: Read.cc:205: "params.data == data"
2015/04/01 07:37:31| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/01 11:06:29| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
2015/04/08 05:01:57| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/08 14:46:01| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/08 18:04:13| assertion failed: Read.cc:205: "params.data ==
data" IMMEDIATELY.
2015/04/13 09:38:28| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/20 23:34:46| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/28 23:12:40| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
2015/04/28 23:59:07| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
2015/04/29 18:06:01| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"



2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
----
http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-Read-cc-205-quot-params-data-data-quot-td4670624.html


pprabhu:~$ gdb /usr/local/sbin/squid core-squid-508-1430264784
GNU gdb (Ubuntu/Linaro 7.4-2012.04-0ubuntu2.1) 7.4-2012.04
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
<http://bugs.launchpad.net/gdb-linaro/>...
Reading symbols from /usr/local/sbin/squid...done.
[New LWP 508]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `/usr/local/sbin/squid -N -f /etc/squid/squid.conf -D'.
Program terminated with signal 6, Aborted.
#0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) bt
#0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fd235dbd83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
    callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
    at Read.cc:205
#4  0x000000000060dc54 in IdleConnList::clearHandlers (this=0xe1b078,
conn=...) at pconn.cc:157
#5  0x00000000006111cf in IdleConnList::pop (this=0xe1b078) at pconn.cc:223
#6  0x0000000000804f13 in Adaptation::Icap::ServiceRep::getConnection
(this=0xe1ac88, retriableXact=<optimized out>,
    reused=@0x7fff42b82bef: false) at ServiceRep.cc:116
#7  0x0000000000810f7f in Adaptation::Icap::Xaction::openConnection
(this=0x1847a68) at Xaction.cc:125
#8  0x000000000081fbc0 in Adaptation::Icap::ModXact::start
(this=0x1847a68) at ModXact.cc:101
#9  0x00000000006f5a84 in JobDialer<AsyncJob>::dial (this=0x18f81e0,
call=...) at ../../src/base/AsyncJobCalls.h:174
#10 0x00000000006f2309 in AsyncCall::make (this=0x18f81b0) at AsyncCall.cc:40
#11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
out>) at AsyncCallQueue.cc:56
#12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8d80) at
AsyncCallQueue.cc:42
#13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff42b82dc0) at
EventLoop.cc:120
#14 0x0000000000593d70 in EventLoop::run (this=0x7fff42b82dc0) at
EventLoop.cc:82
#15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
argv=<optimized out>) at main.cc:1508
#16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
argc=<optimized out>) at main.cc:1240
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
(gdb) fr 3
#3  0x0000000000796908 in comm_read_cancel (fd=40,
    callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
    at Read.cc:205
205 Read.cc: No such file or directory.
----



2015/04/28 23:59:07| assertion failed: Read.cc:69:
"fd_table[conn->fd].halfClosedReader != NULL"
----
(gdb) bt
#0  0x00007f7ba4ad80d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f7ba4adb83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x000000000058174f in xassert (msg=0x899348 "g read handler on FD
", file=0x8991bf "nish", line=69) at debug.cc:544
#3  0x0000000000797410 in comm_read_base (conn=..., buf=0x124cbf0 "",
size=65535, callback=...) at Read.cc:69
#4  0x000000000080f1b9 in comm_read (callback=..., len=65535,
buf=0x124cbf0 "", conn=...) at ../../../src/comm/Read.h:58
#5  Adaptation::Icap::Xaction::scheduleRead (this=0x124bb78) at Xaction.cc:397
#6  0x000000000081ba78 in Adaptation::Icap::ModXact::readMore
(this=0x124bb78) at ModXact.cc:561
#7  0x0000000000823742 in
Adaptation::Icap::ModXact::handleCommConnected (this=0x124bb78) at
ModXact.cc:191
#8  0x000000000080ff2b in Adaptation::Icap::Xaction::noteCommConnected
(this=0x124bb78, io=...) at Xaction.cc:266
#9  0x0000000000813a4b in JobDialer<Adaptation::Icap::Xaction>::dial
(this=0x1d150a0, call=...) at ../../../src/base/AsyncJobCalls.h:174
#10 0x00000000006f2309 in AsyncCall::make (this=0x1d15070) at AsyncCall.cc:40
#11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
out>) at AsyncCallQueue.cc:56
#12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8a50) at
AsyncCallQueue.cc:42
#13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff37077350) at
EventLoop.cc:120
#14 0x0000000000593d70 in EventLoop::run (this=0x7fff37077350) at
EventLoop.cc:82
#15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
argv=<optimized out>) at main.cc:1508
#16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
argc=<optimized out>) at main.cc:1240
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233


http://bugs.squid-cache.org/show_bug.cgi?id=3775
-- Stacktrace is different. c-icap connection appears to be where the assert
   was tripped.

http://bugs.squid-cache.org/show_bug.cgi?id=4173
-- Dup of 3775
----


2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
----
http://bugs.squid-cache.org/show_bug.cgi?id=3329
-- Directed here, via
   http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-quot-F-gt-flags-open-quot-td4670788.html
----


From nathan at getoffmalawn.com  Tue May  5 00:40:41 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Tue, 5 May 2015 10:40:41 +1000
Subject: [squid-users] Number of clients accessing cache: 0
In-Reply-To: <1430777399216-4671105.post@n4.nabble.com>
References: <1430774953477-4671102.post@n4.nabble.com>
 <1430777399216-4671105.post@n4.nabble.com>
Message-ID: <CAGUJm7aR6vzV-Rtg77ckcj+Toa1DRqGSei42kE_5oSz91gEM5g@mail.gmail.com>

Working just fine for me on 3.5.3 and 3.5.4:

[root at box ~]# squidmgr info
Squid Object Cache: Version 3.5.3
[snip]
    Number of clients accessing cache:    4187
    Number of HTTP requests received:    247419


[root at box2 ~]# squidmgr info
Squid Object Cache: Version 3.5.4
[snip]
    Number of clients accessing cache:    31
    Number of HTTP requests received:    22140

On 5 May 2015 at 08:09, HackXBack <hack.back at hotmail.com> wrote:
> root at issa:~# squidclient -h 127.0.0.1 -p 3128 mgr:info |grep "Number of"
> Sending HTTP request ... done.
>         Number of clients accessing cache:      0
>         Number of HTTP requests received:       6498250
>         Number of ICP messages received:        0
>         Number of ICP messages sent:    0
>         Number of queued ICP replies:   0
>         Number of HTCP messages received:       0
>         Number of HTCP messages sent:   0
>         Number of file desc currently in use: 1927
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Number-of-clients-accessing-cache-0-tp4671102p4671105.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From nathan at getoffmalawn.com  Tue May  5 00:56:48 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Tue, 5 May 2015 10:56:48 +1000
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
Message-ID: <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>

These are fixed in 3.5.4.

Nathan.

On 5 May 2015 at 08:59, Prashanth Prabhu <prashanth.prabhu at gmail.com> wrote:
> Hi folks,
>
> I am seeing a bunch of Squid crashes after moving recently to 3.5.1.
> The stack trace for the various crashes is below, along with info I
> have been able to gather for them. This is on a setup where we have
> Squid running in conjunction with c-icap (both on the same box).The
> steps that led to these crashes isn't known yet.
>
> Squid runs in non-transparent mode. It proxies both HTTP and HTTPS connections.
>
> A couple of the crashes look to have been caused in the path where
> Squid interacts with c-icap. The c-icap version is somewhat old --
> 0.1.7 -- although it was working well with a version 3.3.13 Squid
> until the recent move.
>
> Do any of the fixes (listed below, where I have found references) help
> here? Any other known issues that may be related?
>
>
> Thanks.
> Prashanth
>
>
> pprabhu:~$ sudo egrep 'assertion failed' /var/log/elastica/gateway/cache.log
> 2015/03/31 19:56:09| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/01 07:04:59| assertion failed: Read.cc:205: "params.data == data"
> 2015/04/01 07:37:31| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/01 11:06:29| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
> 2015/04/08 05:01:57| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/08 14:46:01| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/08 18:04:13| assertion failed: Read.cc:205: "params.data ==
> data" IMMEDIATELY.
> 2015/04/13 09:38:28| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/20 23:34:46| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/28 23:12:40| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
> 2015/04/28 23:59:07| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> 2015/04/29 18:06:01| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
>
>
>
> 2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
> ----
> http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-Read-cc-205-quot-params-data-data-quot-td4670624.html
>
>
> pprabhu:~$ gdb /usr/local/sbin/squid core-squid-508-1430264784
> GNU gdb (Ubuntu/Linaro 7.4-2012.04-0ubuntu2.1) 7.4-2012.04
> Copyright (C) 2012 Free Software Foundation, Inc.
> License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
> This is free software: you are free to change and redistribute it.
> There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
> and "show warranty" for details.
> This GDB was configured as "x86_64-linux-gnu".
> For bug reporting instructions, please see:
> <http://bugs.launchpad.net/gdb-linaro/>...
> Reading symbols from /usr/local/sbin/squid...done.
> [New LWP 508]
>
> warning: Can't read pathname for load map: Input/output error.
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> Core was generated by `/usr/local/sbin/squid -N -f /etc/squid/squid.conf -D'.
> Program terminated with signal 6, Aborted.
> #0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) bt
> #0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007fd235dbd83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
>     callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
> const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
>     at Read.cc:205
> #4  0x000000000060dc54 in IdleConnList::clearHandlers (this=0xe1b078,
> conn=...) at pconn.cc:157
> #5  0x00000000006111cf in IdleConnList::pop (this=0xe1b078) at pconn.cc:223
> #6  0x0000000000804f13 in Adaptation::Icap::ServiceRep::getConnection
> (this=0xe1ac88, retriableXact=<optimized out>,
>     reused=@0x7fff42b82bef: false) at ServiceRep.cc:116
> #7  0x0000000000810f7f in Adaptation::Icap::Xaction::openConnection
> (this=0x1847a68) at Xaction.cc:125
> #8  0x000000000081fbc0 in Adaptation::Icap::ModXact::start
> (this=0x1847a68) at ModXact.cc:101
> #9  0x00000000006f5a84 in JobDialer<AsyncJob>::dial (this=0x18f81e0,
> call=...) at ../../src/base/AsyncJobCalls.h:174
> #10 0x00000000006f2309 in AsyncCall::make (this=0x18f81b0) at AsyncCall.cc:40
> #11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
> out>) at AsyncCallQueue.cc:56
> #12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8d80) at
> AsyncCallQueue.cc:42
> #13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff42b82dc0) at
> EventLoop.cc:120
> #14 0x0000000000593d70 in EventLoop::run (this=0x7fff42b82dc0) at
> EventLoop.cc:82
> #15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
> argv=<optimized out>) at main.cc:1508
> #16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
> argc=<optimized out>) at main.cc:1240
> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
> (gdb) fr 3
> #3  0x0000000000796908 in comm_read_cancel (fd=40,
>     callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
> const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
>     at Read.cc:205
> 205 Read.cc: No such file or directory.
> ----
>
>
>
> 2015/04/28 23:59:07| assertion failed: Read.cc:69:
> "fd_table[conn->fd].halfClosedReader != NULL"
> ----
> (gdb) bt
> #0  0x00007f7ba4ad80d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007f7ba4adb83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x000000000058174f in xassert (msg=0x899348 "g read handler on FD
> ", file=0x8991bf "nish", line=69) at debug.cc:544
> #3  0x0000000000797410 in comm_read_base (conn=..., buf=0x124cbf0 "",
> size=65535, callback=...) at Read.cc:69
> #4  0x000000000080f1b9 in comm_read (callback=..., len=65535,
> buf=0x124cbf0 "", conn=...) at ../../../src/comm/Read.h:58
> #5  Adaptation::Icap::Xaction::scheduleRead (this=0x124bb78) at Xaction.cc:397
> #6  0x000000000081ba78 in Adaptation::Icap::ModXact::readMore
> (this=0x124bb78) at ModXact.cc:561
> #7  0x0000000000823742 in
> Adaptation::Icap::ModXact::handleCommConnected (this=0x124bb78) at
> ModXact.cc:191
> #8  0x000000000080ff2b in Adaptation::Icap::Xaction::noteCommConnected
> (this=0x124bb78, io=...) at Xaction.cc:266
> #9  0x0000000000813a4b in JobDialer<Adaptation::Icap::Xaction>::dial
> (this=0x1d150a0, call=...) at ../../../src/base/AsyncJobCalls.h:174
> #10 0x00000000006f2309 in AsyncCall::make (this=0x1d15070) at AsyncCall.cc:40
> #11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
> out>) at AsyncCallQueue.cc:56
> #12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8a50) at
> AsyncCallQueue.cc:42
> #13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff37077350) at
> EventLoop.cc:120
> #14 0x0000000000593d70 in EventLoop::run (this=0x7fff37077350) at
> EventLoop.cc:82
> #15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
> argv=<optimized out>) at main.cc:1508
> #16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
> argc=<optimized out>) at main.cc:1240
> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
>
>
> http://bugs.squid-cache.org/show_bug.cgi?id=3775
> -- Stacktrace is different. c-icap connection appears to be where the assert
>    was tripped.
>
> http://bugs.squid-cache.org/show_bug.cgi?id=4173
> -- Dup of 3775
> ----
>
>
> 2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
> ----
> http://bugs.squid-cache.org/show_bug.cgi?id=3329
> -- Directed here, via
>    http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-quot-F-gt-flags-open-quot-td4670788.html
> ----
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue May  5 01:01:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 13:01:34 +1200
Subject: [squid-users] vary headers
In-Reply-To: <trinity-9a899f99-a1d5-4620-ba5f-9189e6d2b713-1430762984990@3capp-mailcom-bs15>
References: <trinity-661bbaac-9cc5-46e7-90ef-2e822679f433-1430657237668@3capp-mailcom-bs10>,
 <55465512.3090709@gmail.com>
 <trinity-bb4dc971-ee6f-4c0f-89de-06319f44ab6c-1430676468750@3capp-mailcom-bs02>,
 <55466CCF.6000005@gmail.com>
 <trinity-4d0bc4d7-b8e5-4105-abf1-b6aab138db25-1430679240647@3capp-mailcom-bs08>,
 <554740BF.9080505@treenet.co.nz>
 <trinity-33a8e2c0-2df6-4c59-a970-df639a943227-1430752525160@3capp-mailcom-bs12>,
 <554790FC.5050105@treenet.co.nz>
 <trinity-8e3105e3-f2bf-42db-af4a-058335061d49-1430757495182@3capp-mailcom-bs15>,
 <5547B491.20302@treenet.co.nz>
 <trinity-9a899f99-a1d5-4620-ba5f-9189e6d2b713-1430762984990@3capp-mailcom-bs15>
Message-ID: <5548166E.9010406@treenet.co.nz>

On 5/05/2015 6:09 a.m., Hussam Al-Tayeb wrote:
> 
> Ok, thank you. How would I modify that to include
> Vary: somethingelse, User-Agent
> and Vary: User-Agent, somethingelse?
> Thanks again!
> 

The part after the "Vary" is a list of regular expressions. Whitesapace
separated, so use '.' where the match would be a space character.

Amos



From nathan at getoffmalawn.com  Tue May  5 01:02:12 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Tue, 5 May 2015 11:02:12 +1000
Subject: [squid-users] Error negotiating SSL connection on FD 12: Success
In-Reply-To: <000001d0869e$633225d0$29967170$@netstream.ps>
References: <000001d0869e$633225d0$29967170$@netstream.ps>
Message-ID: <CAGUJm7atntTyDptogkz08w4H573zvynkwE5FzGgpwCy=iGvDuw@mail.gmail.com>

You're experiencing http://bugs.squid-cache.org/show_bug.cgi?id=4236 -
give the patch on there a try and see if it helps. It should tell you
what's really failing.

You'll start getting messages like this:

Error negotiating SSL connection on FD 439:
error:00000005:lib(0):func(0):DH lib (5/-1/0)

Which, in my experience, indicates a client is attempting to put
non-SSL traffic through that https_port, e.g. HTTP.

Nathan.

On 5 May 2015 at 05:13, snakeeyes <ahmed.zaeem at netstream.ps> wrote:
> Hi
>
> I created privste & public keys for squid , but it still give me error for
> negotiating
>
>
>
>
>
> https_port 443 accel key=/root/CA/myCA/private/squid.local.key
> cert=/root/CA/myCA/certs/squid.local.crt
>
>
>
>
>
> cache.log
>
>
>
> 2015/05/04 11:59:08 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:09 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:10 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:19 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
> 2015/05/04 11:59:21 kid1| Error negotiating SSL connection on FD 12: Success
> (0)
>
>
>
>
>
> Any help ?
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Tue May  5 01:02:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 13:02:38 +1200
Subject: [squid-users] Number of clients accessing cache: 0
In-Reply-To: <1430774953477-4671102.post@n4.nabble.com>
References: <1430774953477-4671102.post@n4.nabble.com>
Message-ID: <554816AE.1070705@treenet.co.nz>

On 5/05/2015 9:29 a.m., Stakres wrote:
> Hi All,
> Seems the number of connected clients is always 0 (zero) since the 3.5.3...

Do you have clientdb disabled?

Amos



From squid3 at treenet.co.nz  Tue May  5 01:03:46 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 13:03:46 +1200
Subject: [squid-users] assertion failed: comm.cc:178:
 "fd_table[conn->fd].halfClosedReader != NULL"
In-Reply-To: <1430777347185-4671104.post@n4.nabble.com>
References: <1430403073796-4670979.post@n4.nabble.com>
 <5543128E.3080402@treenet.co.nz> <1430659756828-4671050.post@n4.nabble.com>
 <554776B1.2080401@treenet.co.nz> <1430777347185-4671104.post@n4.nabble.com>
Message-ID: <554816F2.8050303@treenet.co.nz>

On 5/05/2015 10:09 a.m., HackXBack wrote:
> but am not ready now to use 3.5.4
> can i use this patch on 3.4 without any problem ?

You seem to be manually backporting other patches. This one is no different.

Amos



From squid3 at treenet.co.nz  Tue May  5 01:05:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 13:05:07 +1200
Subject: [squid-users] Regex difficulties
In-Reply-To: <CADddWkqSYKOpMBLpz=KUpEjZfEYj9EHwAAtNtTNcyFzYGWPZWA@mail.gmail.com>
References: <CADddWkoC=BfQt59aqMAvVqSDhMiHbRoV+n2s6qUC1Fn4KmsiOg@mail.gmail.com>	<554743AA.7020307@treenet.co.nz>	<55479A13.20301@treenet.co.nz>
 <CADddWkqSYKOpMBLpz=KUpEjZfEYj9EHwAAtNtTNcyFzYGWPZWA@mail.gmail.com>
Message-ID: <55481743.8010904@treenet.co.nz>

On 5/05/2015 9:25 a.m., Bal?zs Szabados wrote:
> Hi Amos,
> 
> I tried troubleshooting you've mentioned. I just enabled one file
> containing regexes.
> I've measured it:
> root at OpenWrt:~# wc -c /etc/squid/blacklists/regex_allow
> 1636 /etc/squid/blacklists/regex_allow
> 
> Since the whole file is 1636 bytes, I assume ever line of regexes is in
> must be smaller than that.
> Starting up squid I'm getting these:
> 
> 2015/05/04 23:17:44| aclParseRegexList: Invalid regular expression
> '(.*(acoon\.de|openacoon\.de|omgili\.com|youtube\.com|slug\.ch|\/search\?|mister-wong\.de|mister-wong\.com|abacho\.|seekport\.|flickr\.com|altavista|ask\..com|answers\.com|clusty\.com|exalead\.com|metacrawler|dogpile|lycos|google|yahoo|dmoz|search\.|wikise':
> Unmatched ( or \(
> 
> I've created a file with just that part in the error message, and it seems
> to be 256 bytes (wc -c again).
> Isn't that possible, that this is the limit, you've mentioned?

It maybe... in very old Squid. The line limit was raised to KBs some
years ago.

Amos



From chris at ceegeebee.com  Tue May  5 04:08:58 2015
From: chris at ceegeebee.com (Chris Bennett)
Date: Tue, 5 May 2015 13:38:58 +0930
Subject: [squid-users] SSL MITM with unencrypted parent proxy
Message-ID: <20150505040858.GA11641@cgb-linux.rcmb.lan>

Hi there,

I'm experimenting with WAN acceleration & block caching (wanproxy.org
for those interested).  This works great for HTTP:

client <-> squid1 <-> wanproxy <-> VPN <-> wanproxy <-> squid2 <-> inet

With SSL, I suspect the data between squid and squid2 (in a
child/parent configuration) will be encrypted with a new tunnel (I
haven't tested it yet).  If that is the case, is there anyway to
configure squid1 and squid2 to communicate in cleartext for the
child/parent communication?

Any thoughts or ideas would be much appreciated.

Regards,

Chris


From Jason_Haar at trimble.com  Tue May  5 04:35:30 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Tue, 5 May 2015 16:35:30 +1200
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <55473377.80401@cpalmer.me.uk>
References: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
 <55465456.2090108@cpalmer.me.uk> <55473377.80401@cpalmer.me.uk>
Message-ID: <55484892.1090309@trimble.com>

On 04/05/15 20:53, Chris Palmer wrote:
> There has been a change in behaviour in 3.5.4. It now really does
> prefer to contact a site using an ipv6 address rather than a v4. The
> network stack here doesn't permit v6 so the traffic to sites such as
> google was failing. Setting the following restored the previous
> behaviour:
>
> dns_v4_first on

As far as I'm aware squid won't try to use ipv6 unless your server has a
Global address, so that shouldn't be needed? Also, wouldn't squid simply
treat that as a DNS name that resolves to a bunch of addresses, so as
long as the IPv6 addresses fail to connect at all, it should have still
ended up succeeding with ipv4 addresses?

Finally, I'm running squid-3.5.4, don't have ipv6 (just like everyone
else, I still do have the standard fe80:xxx ipv6 link local address) and
google.com works just fine without "dns_v4_first" - which implies my
statements above are correct

ie this smells like you actually do have ipv6 enabled, but it's broken
in some subtle way (like the pmtu issue Amos mentioned)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From vdoctor at neuf.fr  Tue May  5 05:03:49 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 4 May 2015 22:03:49 -0700 (PDT)
Subject: [squid-users] Number of clients accessing cache: 0
In-Reply-To: <554816AE.1070705@treenet.co.nz>
References: <1430774953477-4671102.post@n4.nabble.com>
 <554816AE.1070705@treenet.co.nz>
Message-ID: <1430802229320-4671117.post@n4.nabble.com>

Hi Amos,
Well, as usual, you found the reason 
"client_db" was off, now it shows the numbers...

Thanks Amos.
Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Number-of-clients-accessing-cache-0-tp4671102p4671117.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue May  5 05:33:53 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 17:33:53 +1200
Subject: [squid-users] SSL MITM with unencrypted parent proxy
In-Reply-To: <20150505040858.GA11641@cgb-linux.rcmb.lan>
References: <20150505040858.GA11641@cgb-linux.rcmb.lan>
Message-ID: <55485641.5050708@treenet.co.nz>

On 5/05/2015 4:08 p.m., Chris Bennett wrote:
> Hi there,
> 
> I'm experimenting with WAN acceleration & block caching (wanproxy.org
> for those interested).  This works great for HTTP:
> 
> client <-> squid1 <-> wanproxy <-> VPN <-> wanproxy <-> squid2 <-> inet
> 
> With SSL, I suspect the data between squid and squid2 (in a
> child/parent configuration) will be encrypted with a new tunnel (I
> haven't tested it yet).  If that is the case, is there anyway to
> configure squid1 and squid2 to communicate in cleartext for the
> child/parent communication?

Squid will not permit HTTPS decrypted requests over un-encrypted
channels. If it does thats a bug we need to fix ASAP.

However, explicit proxies can receive TLS connections. The two proxies
will happily use those connections for any type of traffic, including
ones like https:// with special security requirements.

* Configure the squid2 with an https_port for receiving regular proxy
traffic (but over TLS/SSL).

* Configure the squid1 cache_peer parent line with "ssl" option (and any
supporting options that may be required or desired).


Note that for proper security these cache_peer links can be setup with
self-signed certificates, doing both server and client certificate
authentication. Which is the proper usage TLS was designed for and
cannot be MITM'd.

Amos



From squid3 at treenet.co.nz  Tue May  5 06:20:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 18:20:51 +1200
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <55484892.1090309@trimble.com>
References: <mailman.5.1430568002.22662.squid-users@lists.squid-cache.org>
 <55465456.2090108@cpalmer.me.uk> <55473377.80401@cpalmer.me.uk>
 <55484892.1090309@trimble.com>
Message-ID: <55486143.7010008@treenet.co.nz>

On 5/05/2015 4:35 p.m., Jason Haar wrote:
> On 04/05/15 20:53, Chris Palmer wrote:
>> There has been a change in behaviour in 3.5.4. It now really does
>> prefer to contact a site using an ipv6 address rather than a v4. The
>> network stack here doesn't permit v6 so the traffic to sites such as
>> google was failing. Setting the following restored the previous
>> behaviour:
>>
>> dns_v4_first on
> 
> As far as I'm aware squid won't try to use ipv6 unless your server has a
> Global address, so that shouldn't be needed? Also, wouldn't squid simply
> treat that as a DNS name that resolves to a bunch of addresses, so as
> long as the IPv6 addresses fail to connect at all, it should have still
> ended up succeeding with ipv4 addresses?
> 
> Finally, I'm running squid-3.5.4, don't have ipv6 (just like everyone
> else, I still do have the standard fe80:xxx ipv6 link local address) and
> google.com works just fine without "dns_v4_first" - which implies my
> statements above are correct
> 
> ie this smells like you actually do have ipv6 enabled, but it's broken
> in some subtle way (like the pmtu issue Amos mentioned)
> 


The tunnel.cc code producing that read/write error is one of the bits
still broken in regards to errno usage. So I dont entirely trust that
"endpoint not connected" detail, it seems right but could be something
subtly different.

Ayways, to get the output at all the TCP SYN/ACK handshake has to have
already setup the IPv6 connection with no errors. Then a (first? TLS?)
read/write operation attempted on the IPv6 socket fails and does that
log message.

There is supposed to be callback event protections preventing closed
sockets being used for read/write (adding to suspicion about the log
message), which is where I'm currently trying to figure out what state
things could be in.

Amos



From prashanth.prabhu at gmail.com  Tue May  5 07:32:17 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Tue, 5 May 2015 00:32:17 -0700
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
 <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
Message-ID: <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>

Hi Nathan,

> These are fixed in 3.5.4.


Thanks. Do you have the bug ID's that fixed them?


Regards.
Prashanth


On 4 May 2015 at 17:56, Nathan Hoad <nathan at getoffmalawn.com> wrote:
> These are fixed in 3.5.4.
>
> Nathan.
>
> On 5 May 2015 at 08:59, Prashanth Prabhu <prashanth.prabhu at gmail.com> wrote:
>> Hi folks,
>>
>> I am seeing a bunch of Squid crashes after moving recently to 3.5.1.
>> The stack trace for the various crashes is below, along with info I
>> have been able to gather for them. This is on a setup where we have
>> Squid running in conjunction with c-icap (both on the same box).The
>> steps that led to these crashes isn't known yet.
>>
>> Squid runs in non-transparent mode. It proxies both HTTP and HTTPS connections.
>>
>> A couple of the crashes look to have been caused in the path where
>> Squid interacts with c-icap. The c-icap version is somewhat old --
>> 0.1.7 -- although it was working well with a version 3.3.13 Squid
>> until the recent move.
>>
>> Do any of the fixes (listed below, where I have found references) help
>> here? Any other known issues that may be related?
>>
>>
>> Thanks.
>> Prashanth
>>
>>
>> pprabhu:~$ sudo egrep 'assertion failed' /var/log/elastica/gateway/cache.log
>> 2015/03/31 19:56:09| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/01 07:04:59| assertion failed: Read.cc:205: "params.data == data"
>> 2015/04/01 07:37:31| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/01 11:06:29| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
>> 2015/04/08 05:01:57| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/08 14:46:01| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/08 18:04:13| assertion failed: Read.cc:205: "params.data ==
>> data" IMMEDIATELY.
>> 2015/04/13 09:38:28| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/20 23:34:46| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/28 23:12:40| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
>> 2015/04/28 23:59:07| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> 2015/04/29 18:06:01| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>>
>>
>>
>> 2015/04/28 23:46:24| assertion failed: Read.cc:205: "params.data == data"
>> ----
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-Read-cc-205-quot-params-data-data-quot-td4670624.html
>>
>>
>> pprabhu:~$ gdb /usr/local/sbin/squid core-squid-508-1430264784
>> GNU gdb (Ubuntu/Linaro 7.4-2012.04-0ubuntu2.1) 7.4-2012.04
>> Copyright (C) 2012 Free Software Foundation, Inc.
>> License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
>> This is free software: you are free to change and redistribute it.
>> There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
>> and "show warranty" for details.
>> This GDB was configured as "x86_64-linux-gnu".
>> For bug reporting instructions, please see:
>> <http://bugs.launchpad.net/gdb-linaro/>...
>> Reading symbols from /usr/local/sbin/squid...done.
>> [New LWP 508]
>>
>> warning: Can't read pathname for load map: Input/output error.
>> [Thread debugging using libthread_db enabled]
>> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
>> Core was generated by `/usr/local/sbin/squid -N -f /etc/squid/squid.conf -D'.
>> Program terminated with signal 6, Aborted.
>> #0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
>> (gdb) bt
>> #0  0x00007fd235dba0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
>> #1  0x00007fd235dbd83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
>>     callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
>> const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
>>     at Read.cc:205
>> #4  0x000000000060dc54 in IdleConnList::clearHandlers (this=0xe1b078,
>> conn=...) at pconn.cc:157
>> #5  0x00000000006111cf in IdleConnList::pop (this=0xe1b078) at pconn.cc:223
>> #6  0x0000000000804f13 in Adaptation::Icap::ServiceRep::getConnection
>> (this=0xe1ac88, retriableXact=<optimized out>,
>>     reused=@0x7fff42b82bef: false) at ServiceRep.cc:116
>> #7  0x0000000000810f7f in Adaptation::Icap::Xaction::openConnection
>> (this=0x1847a68) at Xaction.cc:125
>> #8  0x000000000081fbc0 in Adaptation::Icap::ModXact::start
>> (this=0x1847a68) at ModXact.cc:101
>> #9  0x00000000006f5a84 in JobDialer<AsyncJob>::dial (this=0x18f81e0,
>> call=...) at ../../src/base/AsyncJobCalls.h:174
>> #10 0x00000000006f2309 in AsyncCall::make (this=0x18f81b0) at AsyncCall.cc:40
>> #11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
>> out>) at AsyncCallQueue.cc:56
>> #12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8d80) at
>> AsyncCallQueue.cc:42
>> #13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff42b82dc0) at
>> EventLoop.cc:120
>> #14 0x0000000000593d70 in EventLoop::run (this=0x7fff42b82dc0) at
>> EventLoop.cc:82
>> #15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
>> argv=<optimized out>) at main.cc:1508
>> #16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
>> argc=<optimized out>) at main.cc:1240
>> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
>> (gdb) fr 3
>> #3  0x0000000000796908 in comm_read_cancel (fd=40,
>>     callback=0x6109a0 <IdleConnList::Read(RefCount<Comm::Connection>
>> const&, char*, unsigned long, Comm::Flag, int, void*)>, data=0xe1b078)
>>     at Read.cc:205
>> 205 Read.cc: No such file or directory.
>> ----
>>
>>
>>
>> 2015/04/28 23:59:07| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"
>> ----
>> (gdb) bt
>> #0  0x00007f7ba4ad80d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
>> #1  0x00007f7ba4adb83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
>> #2  0x000000000058174f in xassert (msg=0x899348 "g read handler on FD
>> ", file=0x8991bf "nish", line=69) at debug.cc:544
>> #3  0x0000000000797410 in comm_read_base (conn=..., buf=0x124cbf0 "",
>> size=65535, callback=...) at Read.cc:69
>> #4  0x000000000080f1b9 in comm_read (callback=..., len=65535,
>> buf=0x124cbf0 "", conn=...) at ../../../src/comm/Read.h:58
>> #5  Adaptation::Icap::Xaction::scheduleRead (this=0x124bb78) at Xaction.cc:397
>> #6  0x000000000081ba78 in Adaptation::Icap::ModXact::readMore
>> (this=0x124bb78) at ModXact.cc:561
>> #7  0x0000000000823742 in
>> Adaptation::Icap::ModXact::handleCommConnected (this=0x124bb78) at
>> ModXact.cc:191
>> #8  0x000000000080ff2b in Adaptation::Icap::Xaction::noteCommConnected
>> (this=0x124bb78, io=...) at Xaction.cc:266
>> #9  0x0000000000813a4b in JobDialer<Adaptation::Icap::Xaction>::dial
>> (this=0x1d150a0, call=...) at ../../../src/base/AsyncJobCalls.h:174
>> #10 0x00000000006f2309 in AsyncCall::make (this=0x1d15070) at AsyncCall.cc:40
>> #11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized
>> out>) at AsyncCallQueue.cc:56
>> #12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd8a50) at
>> AsyncCallQueue.cc:42
>> #13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fff37077350) at
>> EventLoop.cc:120
>> #14 0x0000000000593d70 in EventLoop::run (this=0x7fff37077350) at
>> EventLoop.cc:82
>> #15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
>> argv=<optimized out>) at main.cc:1508
>> #16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
>> argc=<optimized out>) at main.cc:1240
>> #17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
>>
>>
>> http://bugs.squid-cache.org/show_bug.cgi?id=3775
>> -- Stacktrace is different. c-icap connection appears to be where the assert
>>    was tripped.
>>
>> http://bugs.squid-cache.org/show_bug.cgi?id=4173
>> -- Dup of 3775
>> ----
>>
>>
>> 2015/04/07 18:23:54| assertion failed: comm.cc:557: "F->flags.open"
>> ----
>> http://bugs.squid-cache.org/show_bug.cgi?id=3329
>> -- Directed here, via
>>    http://squid-web-proxy-cache.1019090.n4.nabble.com/assertion-failed-comm-cc-557-quot-F-gt-flags-open-quot-td4670788.html
>> ----
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue May  5 09:07:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 21:07:43 +1200
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
 <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
 <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>
Message-ID: <5548885F.6080602@treenet.co.nz>

On 5/05/2015 7:32 p.m., Prashanth Prabhu wrote:
> Hi Nathan,
> 
>> These are fixed in 3.5.4.
> 
> 
> Thanks. Do you have the bug ID's that fixed them?
> 

<http://bugs.squid-cache.org/show_bug.cgi?id=3775> IIRC.

Amos



From christos at chtsanti.net  Tue May  5 09:45:10 2015
From: christos at chtsanti.net (Christos Tsantilas)
Date: Tue, 05 May 2015 12:45:10 +0300
Subject: [squid-users] Assert(call->dialer.handler == callback)
In-Reply-To: <554254FF.2080603@opendium.com>
References: <554254FF.2080603@opendium.com>
Message-ID: <55489126.6000701@chtsanti.net>

Hi Steve,
  We have similar crashes.

I created a new bug report in squid bugzilla (I did not found any other 
similar report), using your stack trace:
   http://bugs.squid-cache.org/show_bug.cgi?id=4238

Also I attached a patch here, which probably fixes this problem. Can you 
please test it?

Regards,
    Christos


On 04/30/2015 07:14 PM, Steve Hill wrote:
>
> I've just migrated a system from Squid 3.4.10 to 3.5.3 and I'm getting
> frequent crashes with an assertion of "call->dialer.handler == callback"
> in Read.cc:comm_read_cancel().
>
> call->dialer.handler == (IOCB *) 0x7ffe1493b2d0
> <TunnelStateData::ReadClient(Comm::ConnectionPointer const&, char*,
> size_t, Comm::Flag, int, void*)>
>
> callback == <IdleConnList::Read(Comm::ConnectionPointer const&, char*,
> size_t, Comm::Flag, int, void*)>
>
>
> This is quite a busy system doing server-first ssl_bump and I get a lot
> of SSL negotiation errors in cache.log (these were present under 3.4.10
> too).  I think a good chunk of these are Team Viewer, which abuses
> CONNECTs to port 443 of remote servers to do non-SSL traffic, so
> obviously isn't going to work with ssl_bump.  I _suspect_ that the
> assertion may be being triggered by these SSL errors (e.g. connection
> being unexpectedly torn down because SSL negotiation failed?), but I
> can't easily prove that.
>
> I don't quite understand the comm_read_cancel() function though - as far
> as I can see, the callback parameter is only used in the assert() - is
> that correct?
>
>
> Stack trace:
> #0  0x00007ffe1155d625 in raise (sig=6) at
> ../nptl/sysdeps/unix/sysv/linux/raise.c:64
> #1  0x00007ffe1155ee05 in abort () at abort.c:92
> #2  0x00007ffe148210df in xassert (msg=Unhandled dwarf expression opcode
> 0xf3
> ) at debug.cc:544
> #3  0x00007ffe14a62787 in comm_read_cancel (fd=600,
> callback=0x7ffe148c8dd0 <IdleConnList::Read(Comm::ConnectionPointer
> const&, char*, size_t, Comm::Flag, int, void*)>,
>      data=0x7ffe176c8298) at Read.cc:204
> #4  0x00007ffe148c5e62 in IdleConnList::clearHandlers
> (this=0x7ffe176c8298, conn=...) at pconn.cc:157
> #5  0x00007ffe148c94ab in IdleConnList::findUseable
> (this=0x7ffe176c8298, key=...) at pconn.cc:269
> #6  0x00007ffe148c979d in PconnPool::pop (this=0x7ffe145db010, dest=...,
> domain=Unhandled dwarf expression opcode 0xf3
> ) at pconn.cc:449
> #7  0x00007ffe14852142 in FwdState::pconnPop (this=Unhandled dwarf
> expression opcode 0xf3
> ) at FwdState.cc:1153
> #8  0x00007ffe14855605 in FwdState::connectStart (this=0x7ffe2034c4e8)
> at FwdState.cc:850
> #9  0x00007ffe14856a31 in FwdState::startConnectionOrFail
> (this=0x7ffe2034c4e8) at FwdState.cc:398
> #10 0x00007ffe148d2fa5 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at
> peer_select.cc:302
> #11 0x00007ffe148d6a1d in peerSelectDnsResults (ia=0x7ffe14f0ac20,
> details=Unhandled dwarf expression opcode 0xf3
> ) at peer_select.cc:383
> #12 0x00007ffe148a8e71 in ipcache_nbgethostbyname (name=Unhandled dwarf
> expression opcode 0xf3
> ) at ipcache.cc:518
> #13 0x00007ffe148d23c1 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at
> peer_select.cc:259
> #14 0x00007ffe148d6a1d in peerSelectDnsResults (ia=0x7ffe14f0ac20,
> details=Unhandled dwarf expression opcode 0xf3
> ) at peer_select.cc:383
> #15 0x00007ffe148a8e71 in ipcache_nbgethostbyname (name=Unhandled dwarf
> expression opcode 0xf3
> ) at ipcache.cc:518
> #16 0x00007ffe148d23c1 in peerSelectDnsPaths (psstate=0x7ffe1fd0c028) at
> peer_select.cc:259
> #17 0x00007ffe148d382b in peerSelectFoo (ps=0x7ffe1fd0c028) at
> peer_select.cc:522
> #18 0x00007ffe149bba6a in ACLChecklist::checkCallback
> (this=0x7ffe2065b9e8, answer=...) at Checklist.cc:167
> #19 0x00007ffe148d3f5a in peerSelectFoo (ps=0x7ffe1fd0c028) at
> peer_select.cc:459
> #20 0x00007ffe148d5176 in peerSelect (paths=0x7ffe2034c540,
> request=0x7ffe1b660b70, al=Unhandled dwarf expression opcode 0xf3
> ) at peer_select.cc:163
> #21 0x00007ffe14852ae3 in FwdState::Start (clientConn=...,
> entry=0x7ffe1b0da790, request=0x7ffe1b660b70, al=...) at FwdState.cc:366
> #22 0x00007ffe14801401 in clientReplyContext::processMiss
> (this=0x7ffe1fcf5838) at client_side_reply.cc:691
> #23 0x00007ffe14801eb0 in clientReplyContext::doGetMoreData
> (this=0x7ffe1fcf5838) at client_side_reply.cc:1797
> #24 0x00007ffe14805a89 in ClientHttpRequest::httpStart
> (this=0x7ffe1dcda618) at client_side_request.cc:1518
> #25 0x00007ffe14808cac in ClientHttpRequest::processRequest
> (this=0x7ffe1dcda618) at client_side_request.cc:1504
> #26 0x00007ffe14809013 in ClientHttpRequest::doCallouts
> (this=0x7ffe1dcda618) at client_side_request.cc:1830
> #27 0x00007ffe1480b453 in checkNoCacheDoneWrapper (answer=...,
> data=0x7ffe1e5db378) at client_side_request.cc:1400
> #28 0x00007ffe149bba6a in ACLChecklist::checkCallback
> (this=0x7ffe1c88b4a8, answer=...) at Checklist.cc:167
> #29 0x00007ffe1480b40a in ClientRequestContext::checkNoCache
> (this=0x7ffe1e5db378) at client_side_request.cc:1385
> #30 0x00007ffe14809c04 in ClientHttpRequest::doCallouts
> (this=0x7ffe1dcda618) at client_side_request.cc:1748
> #31 0x00007ffe1480d109 in ClientRequestContext::clientAccessCheckDone
> (this=0x7ffe1e5db378, answer=Unhandled dwarf expression opcode 0xf3
> ) at client_side_request.cc:821
> #32 0x00007ffe1480d898 in ClientRequestContext::clientAccessCheck2
> (this=0x7ffe1e5db378) at client_side_request.cc:718
> #33 0x00007ffe14809767 in ClientHttpRequest::doCallouts
> (this=0x7ffe1dcda618) at client_side_request.cc:1721
> #34 0x00007ffe1480afca in ClientHttpRequest::handleAdaptedHeader
> (this=0x7ffe1dcda618, msg=Unhandled dwarf expression opcode 0xf3
> ) at client_side_request.cc:1935
> #35 0x00007ffe14abbcaa in JobDialer<Adaptation::Initiator>::dial
> (this=0x7ffe1ce04990, call=...) at ../../src/base/AsyncJobCalls.h:174
> #36 0x00007ffe149bea69 in AsyncCall::make (this=0x7ffe1ce04960) at
> AsyncCall.cc:40
> #37 0x00007ffe149c272f in AsyncCallQueue::fireNext (this=Unhandled dwarf
> expression opcode 0xf3
> ) at AsyncCallQueue.cc:56
> #38 0x00007ffe149c2a60 in AsyncCallQueue::fire (this=0x7ffe16f70bf0) at
> AsyncCallQueue.cc:42
> #39 0x00007ffe1484110c in EventLoop::runOnce (this=0x7fffcb8c4be0) at
> EventLoop.cc:120
> #40 0x00007ffe148412c8 in EventLoop::run (this=0x7fffcb8c4be0) at
> EventLoop.cc:82
> #41 0x00007ffe148ae191 in SquidMain (argc=Unhandled dwarf expression
> opcode 0xf3
> ) at main.cc:1511
> #42 0x00007ffe148af2e9 in SquidMainSafe (argc=Unhandled dwarf expression
> opcode 0xf3
> ) at main.cc:1243
> #43 main (argc=Unhandled dwarf expression opcode 0xf3
> ) at main.cc:1236
>
> (sorry about the DWARF errors - it looks like I've got a version
> mismatch between gcc and gdb)


From yvoinov at gmail.com  Tue May  5 09:59:09 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 05 May 2015 15:59:09 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1430777255139-4671103.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com>
Message-ID: <5548946D.4080107@gmail.com>



05.05.15 4:07, HackXBack ?????:
> Okay Sir,
> this is the solution
>
> 1st: put this conf in your squid.conf
>
> ####for looping 302 on youtube
> acl text-html rep_mime_type text/html
> acl http302 http_status 302
> store_miss deny text-html
> store_miss deny http302
> send_hit deny text-html
> send_hit deny http302
This works on 3.5.x and above only. store_* directives absent in 3.4.x 
series.
>
>
> 2nd: use this patch:
>
>
>
> --- src/client_side_request.cc  2014-03-09 06:40:56.000000000 -0300
> +++ src/client_side_request.cc  2014-04-21 02:53:11.277155130 -0300
> @@ -545,6 +545,16 @@
>               }
>               debugs(85, 3, HERE << "validate IP " << clientConn->local << "
> non-match from Host: IP " << ia->in_addrs[i]);
>           }
> +
> +        if (true) {
> +            unsigned short port = clientConn->local.port();
> +            debugs(85, 3, HERE << "[anti-forgery] Host-non-matched remote
> IP (" << clientConn->local << ") was replaced with the first Host resolved
> IP (" << ia->in_addrs[0] << ":" << clientConn->local.port() << ")");
> +            clientConn->local = ia->in_addrs[0];
> +            clientConn->local.port(port);
> +            http->request->flags.hostVerified = true;
> +            http->doCallouts();
> +            return;
> +        }
>       }
>       debugs(85, 3, HERE << "FAIL: validate IP " << clientConn->local << "
> possible from Host:");
>       hostHeaderVerifyFailed("local IP", "any domain IP");
>
>
> --- src/Server.cc
> +++ src/Server.cc
> @@ -31,6 +31,7 @@
>    */
>   
>   #include "squid.h"
> +#include "acl/FilledChecklist.h"
>   #include "acl/Gadgets.h"
>   #include "base/TextException.h"
>   #include "comm/Connection.h"
> @@ -174,6 +175,8 @@
>       // give entry the reply because haveParsedReplyHeaders() expects it
> there
>       entry->replaceHttpReply(theFinalReply, false); // but do not write yet
>       haveParsedReplyHeaders(); // update the entry/reply (e.g., set
> timestamps)
> +    if (EBIT_TEST(entry->flags, ENTRY_CACHABLE) && blockCaching())
> +        entry->release();
>       entry->startWriting(); // write the updated entry to store
>   
>       return theFinalReply;
> @@ -533,6 +536,24 @@
>       currentOffset = partial ? theFinalReply->content_range->spec.offset :
> 0;
>   }
>   
> +/// whether to prevent caching of an otherwise cachable response
> +bool
> +ServerStateData::blockCaching()
> +{
> +    if (const Acl::Tree *acl = Config.accessList.storeMiss) {
> +        // This relatively expensive check is not in
> StoreEntry::checkCachable:
> +        // That method lacks HttpRequest and may be called too many times.
> +        ACLFilledChecklist ch(acl, originalRequest(), NULL);
> +        ch.reply = const_cast<HttpReply*>(entry->getReply()); //
> ACLFilledChecklist API bug
> +        HTTPMSGLOCK(ch.reply);
> +        if (ch.fastCheck() != ACCESS_ALLOWED) { // when in doubt, block
> +            debugs(20, 3, "store_miss prohibits caching");
> +            return true;
> +        }
> +    }
> +    return false;
> +}
> +
>   HttpRequest *
>   ServerStateData::originalRequest()
>   {
> --- src/Server.h
> +++ src/Server.h
> @@ -131,6 +131,8 @@
>       /// Entry-dependent callbacks use this check to quit if the entry went
> bad
>       bool abortOnBadEntry(const char *abortReason);
>   
> +    bool blockCaching();
> +
>   #if USE_ADAPTATION
>       void startAdaptation(const Adaptation::ServiceGroupPointer &group,
> HttpRequest *cause);
>       void adaptVirginReplyBody(const char *buf, ssize_t len);
> --- src/SquidConfig.h
> +++ src/SquidConfig.h
> @@ -375,6 +375,8 @@
>           acl_access *AlwaysDirect;
>           acl_access *ASlists;
>           acl_access *noCache;
> +        acl_access *sendHit;
> +        acl_access *storeMiss;
>           acl_access *stats_collection;
>   #if SQUID_SNMP
>   
> --- src/cf.data.pre
> +++ src/cf.data.pre
> @@ -4843,18 +4843,97 @@
>   NAME: cache no_cache
>   TYPE: acl_access
>   DEFAULT: none
> -DEFAULT_DOC: Allow caching, unless rules exist in squid.conf.
> +DEFAULT_DOC: By default, this directive is unused and has no effect.
>   LOC: Config.accessList.noCache
>   DOC_START
> -	A list of ACL elements which, if matched and denied, cause the request to
> -	not be satisfied from the cache and the reply to not be cached.
> -	In other words, use this to force certain objects to never be cached.
> -
> -	You must use the words 'allow' or 'deny' to indicate whether items
> -	matching the ACL should be allowed or denied into the cache.
> +	Requests denied by this directive will not be served from the cache
> +	and their responses will not be stored in the cache. This directive
> +	has no effect on other transactions and on already cached responses.
>   
>   	This clause supports both fast and slow acl types.
>   	See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
> +
> +	This and the two other similar caching directives listed below are
> +	checked at different transaction processing stages, have different
> +	access to response information, affect different cache operations,
> +	and differ in slow ACLs support:
> +
> +	* cache: Checked before Squid makes a hit/miss determination.
> +		No access to reply information!
> +		Denies both serving a hit and storing a miss.
> +		Supports both fast and slow ACLs.
> +	* send_hit: Checked after a hit was detected.
> +		Has access to reply (hit) information.
> +		Denies serving a hit only.
> +		Supports fast ACLs only.
> +	* store_miss: Checked before storing a cachable miss.
> +		Has access to reply (miss) information.
> +		Denies storing a miss only.
> +		Supports fast ACLs only.
> +
> +	If you are not sure which of the three directives to use, apply the
> +	following decision logic:
> +
> +	* If your ACL(s) are of slow type _and_ need response info, redesign.
> +	  Squid does not support that particular combination at this time.
> +        Otherwise:
> +	* If your directive ACL(s) are of slow type, use "cache"; and/or
> +	* if your directive ACL(s) need no response info, use "cache".
> +        Otherwise:
> +	* If you do not want the response cached, use store_miss; and/or
> +	* if you do not want a hit on a cached response, use send_hit.
> +DOC_END
> +
> +NAME: send_hit
> +TYPE: acl_access
> +DEFAULT: none
> +DEFAULT_DOC: By default, this directive is unused and has no effect.
> +LOC: Config.accessList.sendHit
> +DOC_START
> +	Responses denied by this directive will not be served from the cache
> +	(but may still be cached, see store_miss). This directive has no
> +	effect on the responses it allows and on the cached objects.
> +
> +	Please see the "cache" directive for a summary of differences among
> +	store_miss, send_hit, and cache directives.
> +
> +	Unlike the "cache" directive, send_hit only supports fast acl
> +	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
> +
> +	For example:
> +
> +		# apply custom Store ID mapping to some URLs
> +		acl MapMe dstdomain .c.example.com
> +		store_id_program ...
> +		store_id_access allow MapMe
> +
> +		# but prevent caching of special responses
> +		# such as 302 redirects that cause StoreID loops
> +		acl Ordinary http_status 200-299
> +		store_miss deny MapMe !Ordinary
> +
> +		# and do not serve any previously stored special responses
> +		# from the cache (in case they were already cached before
> +		# the above store_miss rule was in effect).
> +		send_hit deny MapMe !Ordinary
> +DOC_END
> +
> +NAME: store_miss
> +TYPE: acl_access
> +DEFAULT: none
> +DEFAULT_DOC: By default, this directive is unused and has no effect.
> +LOC: Config.accessList.storeMiss
> +DOC_START
> +	Responses denied by this directive will not be cached (but may still
> +	be served from the cache, see send_hit). This directive has no
> +	effect on the responses it allows and on the already cached responses.
> +
> +	Please see the "cache" directive for a summary of differences among
> +	store_miss, send_hit, and cache directives. See the
> +	send_hit directive for a usage example.
> +
> +	Unlike the "cache" directive, store_miss only supports fast acl
> +	types.  See http://wiki.squid-cache.org/SquidFaq/SquidAcl for details.
>   DOC_END
>   
>   NAME: max_stale
> --- src/client_side_reply.cc
> +++ src/client_side_reply.cc
> @@ -545,6 +545,11 @@
>          ) {
>           http->logType = LOG_TCP_NEGATIVE_HIT;
>           sendMoreData(result);
> +    } else if (blockedHit()) {
> +        debugs(88, 5, "send_hit forces a MISS");
> +        http->logType = LOG_TCP_MISS;
> +        processMiss();
> +        return;
>       } else if (!http->flags.internal && refreshCheckHTTP(e, r)) {
>           debugs(88, 5, "clientCacheHit: in refreshCheck() block");
>           /*
> @@ -773,6 +778,30 @@
>       }
>   }
>   
> +/// whether squid.conf send_hit prevents us from serving this hit
> +bool
> +clientReplyContext::blockedHit() const
> +{
> +    if (!Config.accessList.sendHit)
> +        return false; // hits are not blocked by default
> +
> +    if (http->flags.internal)
> +        return false; // internal content "hits" cannot be blocked
> +
> +    if (const HttpReply *rep = http->storeEntry()->getReply()) {
> +        std::auto_ptr<ACLFilledChecklist>
> chl(clientAclChecklistCreate(Config.accessList.sendHit, http));
> +        chl->reply = const_cast<HttpReply*>(rep); // ACLChecklist API bug
> +        HTTPMSGLOCK(chl->reply);
> +        return chl->fastCheck() != ACCESS_ALLOWED; // when in doubt, block
> +    }
> +
> +    // This does not happen, I hope, because we are called from CacheHit,
> which
> +    // is called via a storeClientCopy() callback, and store should
> initialize
> +    // the reply before calling that callback.
> +    debugs(88, 3, "Missing reply!");
> +    return false;
> +}
> +
>   void
>   clientReplyContext::purgeRequestFindObjectToPurge()
>   {
> --- src/client_side_reply.h
> +++ src/client_side_reply.h
> @@ -140,6 +140,7 @@
>       void triggerInitialStoreRead();
>       void sendClientOldEntry();
>       void purgeAllCached();
> +    bool blockedHit() const;
>   
>       void sendBodyTooLargeError();
>       void sendPreconditionFailedError();
This is also not solution. One of most biggest traffic source must have 
native solution in proxy. Not a crutch.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671103.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Again. I want simple thing: to limit selected mime-types caching from 
selected domains. Point.
On my opinion, this will completely solve YT caching problem without any 
crutches/patches.



From squid-users at sernet.de  Tue May  5 10:43:57 2015
From: squid-users at sernet.de (=?UTF-8?B?U3RlZmFuIEvDvGdsZXI=?=)
Date: Tue, 05 May 2015 12:43:57 +0200
Subject: [squid-users] squid does not send cached object to an icap-server
Message-ID: <E1YpaKT-004oBN-9u@intern.SerNet.DE>

Hello.


I have a short question using squid as an ICAP-client.


It seems that squid doesn't send an already downloaded (and cached) 
object to an ICAP-server.

Here is a short description what I have done:

1. downloading a word-document with a macro-virus. The Virus-scanner 
(ICAP-server) uses an old pattern-file and does not detect the virus.

The object is now in cache.

2. updating the virus-scanner to the newest pattern-file. The 
virus-scanner will now detect the macro virus.

3. downloading the same word-document. The object has been delivered to 
the client without a new virus scan.



And now some log-entries:

1. First download of the word document:

access.log:
2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET 
http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229 application/msword

icap.log:
2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD 
icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD 
icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -

AV-Scanner:
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting ICAP 
request decoding
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request 
message decoded in 1 chunks
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished ICAP 
request decoding
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting ICAP 
request processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting 
service processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at 
<GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished 
service processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request 
for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
Details: '')
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create 
response headers type: CLEAN 204
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send headers
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished ICAP 
request processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core library 
session cleared
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection 
closed by foreign host while waiting for requests
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core library 
session cleared
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting ICAP 
request decoding
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request 
message decoded in 259 chunks
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished ICAP 
request decoding
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting ICAP 
request processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
service processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting virus 
scanning for resource at: <GET http://www.intern/virus.doc HTTP/1.1>
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting virus 
scanning for resource at: <GET http://www.intern/virus.doc HTTP/1.1>
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: 
[service_scanner]File 'virus.doc' content is stored in 
'/var/spool/avira-icap/icap-tmp.6baFv3'
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished 
service processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request 
for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
Details: '')
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create 
response headers type: CLEAN
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP 
headers for response type: CLEAN
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send headers
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the 
original body (552960 bytes)
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished ICAP 
request processing
May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core library 
session cleared





2. Second download of the word document (after the pattern-update):

access.log:
2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET 
http://www.intern/virus.doc - HIER_NONE/- application/msword

icap.log:
2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD 
icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -

AV-Scanner:
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting ICAP 
request decoding
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request 
message decoded in 1 chunks
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished ICAP 
request decoding
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting ICAP 
request processing
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting 
service processing
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD processing
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at 
<GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished 
service processing
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request 
for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
Details: '')
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create 
response headers type: CLEAN 204
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send headers
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished ICAP 
request processing
May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core library 
session cleared


And now my question: Is this a bug in squid - or is it possible to tell 
squid to send already cached object to the icap-server?

Kind regards,

Stefan Kuegler


From yvoinov at gmail.com  Tue May  5 10:51:39 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 05 May 2015 16:51:39 +0600
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1YpaKT-004oBN-9u@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE>
Message-ID: <5548A0BB.1060404@gmail.com>

This is not squid issue but your AV engine library or ICAP intermediate 
AV library configuration.

05.05.15 16:43, Stefan K?gler ?????:
> Hello.
>
>
> I have a short question using squid as an ICAP-client.
>
>
> It seems that squid doesn't send an already downloaded (and cached) 
> object to an ICAP-server.
>
> Here is a short description what I have done:
>
> 1. downloading a word-document with a macro-virus. The Virus-scanner 
> (ICAP-server) uses an old pattern-file and does not detect the virus.
>
> The object is now in cache.
>
> 2. updating the virus-scanner to the newest pattern-file. The 
> virus-scanner will now detect the macro virus.
>
> 3. downloading the same word-document. The object has been delivered 
> to the client without a new virus scan.
>
>
>
> And now some log-entries:
>
> 1. First download of the word document:
>
> access.log:
> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET 
> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229 
> application/msword
>
> icap.log:
> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD 
> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD 
> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>
> AV-Scanner:
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting 
> ICAP request decoding
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request 
> message decoded in 1 chunks
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished 
> ICAP request decoding
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting 
> ICAP request processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting 
> service processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD 
> processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at 
> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished 
> service processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request 
> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
> Details: '')
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create 
> response headers type: CLEAN 204
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send headers
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished 
> ICAP request processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core library 
> session cleared
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection 
> closed by foreign host while waiting for requests
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core library 
> session cleared
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
> ICAP request decoding
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request 
> message decoded in 259 chunks
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished 
> ICAP request decoding
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
> ICAP request processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
> service processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD 
> processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
> virus scanning for resource at: <GET http://www.intern/virus.doc 
> HTTP/1.1>
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting 
> virus scanning for resource at: <GET http://www.intern/virus.doc 
> HTTP/1.1>
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: 
> [service_scanner]File 'virus.doc' content is stored in 
> '/var/spool/avira-icap/icap-tmp.6baFv3'
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished 
> service processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request 
> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
> Details: '')
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create 
> response headers type: CLEAN
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP 
> headers for response type: CLEAN
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send headers
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the 
> original body (552960 bytes)
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished 
> ICAP request processing
> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core library 
> session cleared
>
>
>
>
>
> 2. Second download of the word document (after the pattern-update):
>
> access.log:
> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET 
> http://www.intern/virus.doc - HIER_NONE/- application/msword
>
> icap.log:
> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD 
> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>
> AV-Scanner:
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting 
> ICAP request decoding
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request 
> message decoded in 1 chunks
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished 
> ICAP request decoding
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting 
> ICAP request processing
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting 
> service processing
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD 
> processing
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at 
> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished 
> service processing
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request 
> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'. 
> Details: '')
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create 
> response headers type: CLEAN 204
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send headers
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished 
> ICAP request processing
> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core library 
> session cleared
>
>
> And now my question: Is this a bug in squid - or is it possible to 
> tell squid to send already cached object to the icap-server?
>
> Kind regards,
>
> Stefan Kuegler
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From chris at ceegeebee.com  Tue May  5 11:19:59 2015
From: chris at ceegeebee.com (Chris Bennett)
Date: Tue, 5 May 2015 20:49:59 +0930
Subject: [squid-users] SSL MITM with unencrypted parent proxy
In-Reply-To: <55485641.5050708@treenet.co.nz>
References: <20150505040858.GA11641@cgb-linux.rcmb.lan>
 <55485641.5050708@treenet.co.nz>
Message-ID: <20150505111959.GB11641@cgb-linux.rcmb.lan>

Hi Amos,

Thanks for the quick reply.

> However, explicit proxies can receive TLS connections. The two proxies
> will happily use those connections for any type of traffic, including
> ones like https:// with special security requirements.
> 
> * Configure the squid2 with an https_port for receiving regular proxy
> traffic (but over TLS/SSL).
> 
> * Configure the squid1 cache_peer parent line with "ssl" option (and any
> supporting options that may be required or desired).

I don't think this would allow me to use wanproxy at any point on both
sides of the configuration though, or am I misunderstood?

Regards,

Chris


From squid-users at sernet.de  Tue May  5 11:45:07 2015
From: squid-users at sernet.de (=?UTF-8?B?U3RlZmFuIEvDvGdsZXI=?=)
Date: Tue, 05 May 2015 13:45:07 +0200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <5548A0BB.1060404@gmail.com>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
Message-ID: <E1YpbHf-004qCG-AC@intern.SerNet.DE>

Hi Yuri.

Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
> This is not squid issue but your AV engine library or ICAP intermediate
> AV library configuration.

Thank you for your answer.

Can you explain me a litte bit more detailed why this is not a squid issue?

In the icap-logfile, I can see a REQMOD-request _AND_ a RESPMOD-request 
to the icap-server if the object is not in cache.

But - if the object is in cache - I can only see a REQMOD-request to the 
icap-server. I am missing RESPMOD.

It seems to me, that it is a decision of the client (squid) which 
request (REQMOD or RESPMOD) will be send to the icap-server (AV-scanner) 
- and not a decision of the av-library.

Regards, Stefan

>
> 05.05.15 16:43, Stefan K?gler ?????:
>> Hello.
>>
>>
>> I have a short question using squid as an ICAP-client.
>>
>>
>> It seems that squid doesn't send an already downloaded (and cached)
>> object to an ICAP-server.
>>
>> Here is a short description what I have done:
>>
>> 1. downloading a word-document with a macro-virus. The Virus-scanner
>> (ICAP-server) uses an old pattern-file and does not detect the virus.
>>
>> The object is now in cache.
>>
>> 2. updating the virus-scanner to the newest pattern-file. The
>> virus-scanner will now detect the macro virus.
>>
>> 3. downloading the same word-document. The object has been delivered
>> to the client without a new virus scan.
>>
>>
>>
>> And now some log-entries:
>>
>> 1. First download of the word document:
>>
>> access.log:
>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>> application/msword
>>
>> icap.log:
>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>
>> AV-Scanner:
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>> ICAP request decoding
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>> message decoded in 1 chunks
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>> ICAP request decoding
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>> ICAP request processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>> service processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>> processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at
>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>> service processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request
>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>> Details: '')
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>> response headers type: CLEAN 204
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send headers
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>> ICAP request processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core library
>> session cleared
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection
>> closed by foreign host while waiting for requests
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core library
>> session cleared
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>> ICAP request decoding
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>> message decoded in 259 chunks
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>> ICAP request decoding
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>> ICAP request processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>> service processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>> processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>> virus scanning for resource at: <GET http://www.intern/virus.doc
>> HTTP/1.1>
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>> virus scanning for resource at: <GET http://www.intern/virus.doc
>> HTTP/1.1>
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>> [service_scanner]File 'virus.doc' content is stored in
>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>> service processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request
>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>> Details: '')
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>> response headers type: CLEAN
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP
>> headers for response type: CLEAN
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send headers
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>> original body (552960 bytes)
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>> ICAP request processing
>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core library
>> session cleared
>>
>>
>>
>>
>>
>> 2. Second download of the word document (after the pattern-update):
>>
>> access.log:
>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>
>> icap.log:
>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>
>> AV-Scanner:
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>> ICAP request decoding
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>> message decoded in 1 chunks
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>> ICAP request decoding
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>> ICAP request processing
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>> service processing
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>> processing
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at
>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>> service processing
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request
>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>> Details: '')
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>> response headers type: CLEAN 204
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send headers
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>> ICAP request processing
>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core library
>> session cleared
>>
>>
>> And now my question: Is this a bug in squid - or is it possible to
>> tell squid to send already cached object to the icap-server?
>>
>> Kind regards,
>>
>> Stefan Kuegler
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue May  5 11:53:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 05 May 2015 23:53:47 +1200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <5548A0BB.1060404@gmail.com>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
Message-ID: <5548AF4B.5060907@treenet.co.nz>

On 5/05/2015 10:51 p.m., Yuri Voinov wrote:
> This is not squid issue but your AV engine library or ICAP intermediate
> AV library configuration.
> 

No, it is Squid not supporting the RESPMOD POSTCACHE vectoring point in
ICAP.
<http://wiki.squid-cache.org/Features/ICAP#Squid_Details>

One very good reason for a) caching things only so long as they should
be cached, and b) not relying solely on proxy AV scanning.

Amos




From squid3 at treenet.co.nz  Tue May  5 12:09:07 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 06 May 2015 00:09:07 +1200
Subject: [squid-users] SSL MITM with unencrypted parent proxy
In-Reply-To: <20150505111959.GB11641@cgb-linux.rcmb.lan>
References: <20150505040858.GA11641@cgb-linux.rcmb.lan>
 <55485641.5050708@treenet.co.nz> <20150505111959.GB11641@cgb-linux.rcmb.lan>
Message-ID: <5548B2E3.5070703@treenet.co.nz>

On 5/05/2015 11:19 p.m., Chris Bennett wrote:
> Hi Amos,
> 
> Thanks for the quick reply.
> 
>> However, explicit proxies can receive TLS connections. The two proxies
>> will happily use those connections for any type of traffic, including
>> ones like https:// with special security requirements.
>>
>> * Configure the squid2 with an https_port for receiving regular proxy
>> traffic (but over TLS/SSL).
>>
>> * Configure the squid1 cache_peer parent line with "ssl" option (and any
>> supporting options that may be required or desired).
> 
> I don't think this would allow me to use wanproxy at any point on both
> sides of the configuration though, or am I misunderstood?

If you want wanproxy to be a party to the transactions you need it
configured for TLS in its equivalent of what I said for squid2.

The TLS explicit proxy connection then goes squid1->wanproxy and
wanproxy becomes responsible for ensuring TLS end-2-end security.


PS. we just got one big step closer to supporting CONNECT over next-hop
proxies with some redesign in squid-4 today. But its still a ways off.

Amos



From sergio.stateri at globo.com  Tue May  5 13:50:30 2015
From: sergio.stateri at globo.com (stateri)
Date: Tue, 5 May 2015 06:50:30 -0700 (PDT)
Subject: [squid-users] How can I just foward all connections from my squid
 to another proxy that requires autentication?
Message-ID: <1430833830246-4671131.post@n4.nabble.com>

Hi

I like to foward all connection to my squid (that doesn requires
authentication) to another web proxy that requires authentication.

How can I do that?

I'm trying to do something like that:

cache_peer 192.168.4.63 parent 3128 0 no-query default login=myuser:mypass
http_access allow all

My target proxy is 192.168.4.63 port 3128, and my user in this proxy is
myuser and the password id mypass.
When I use this configuration, the squid log is:

1430834680.164      0 10.10.10.36 TCP_DENIED/403 3843 GET
http://www.squid-cache.org/Artwork/SN.png - HIER_NONE/- text/html

Does anyone can help me?

Thanks in advance,

Sergio Stateri Junior
sergio.stateri at globo.com



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-can-I-just-foward-all-connections-from-my-squid-to-another-proxy-that-requires-autentication-tp4671131.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ambadasvh at teledna.com  Tue May  5 14:25:35 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Tue, 5 May 2015 14:25:35 +0000
Subject: [squid-users] Client IP spoofing via squid proxy
Message-ID: <HKXPR03MB200B730A796D31DE53351C8A1D10@HKXPR03MB200.apcprd03.prod.outlook.com>

Hi,

I trying to spoof client IP via squid proxy by following  squid's TPROXY4 wiki page:
http://wiki.squid-cache.org/Features/Tproxy4

But I want to know whether squid can spoof client IP when we send proxy format HTTP request from Mozilla (ie configuring proxy & port in mozilla). Can squid proxy behave transparently towards only the web server & not the client?

I've tried sending proxy format HTTP request from client to squid box (on 3129  tproxy port), but I am getting Header forgery error Also its trying to connect to itself instead of web server. I am trying to understand why squid is trying to match host header's DNS with the destination IP instead of connecting to host header's DNS (like normal proxy behaviour on port 3128).

Regards,
Ambadas

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150505/54b3241c/attachment.htm>

From squid3 at treenet.co.nz  Tue May  5 14:48:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 06 May 2015 02:48:34 +1200
Subject: [squid-users] How can I just foward all connections from my
 squid to another proxy that requires autentication?
In-Reply-To: <1430833830246-4671131.post@n4.nabble.com>
References: <1430833830246-4671131.post@n4.nabble.com>
Message-ID: <5548D842.4050108@treenet.co.nz>

On 6/05/2015 1:50 a.m., stateri wrote:
> Hi
> 
> I like to foward all connection to my squid (that doesn requires
> authentication) to another web proxy that requires authentication.
> 
> How can I do that?
> 
> I'm trying to do something like that:
> 
> cache_peer 192.168.4.63 parent 3128 0 no-query default login=myuser:mypass
> http_access allow all
> 
> My target proxy is 192.168.4.63 port 3128, and my user in this proxy is
> myuser and the password id mypass.

Assuming the other proxy accepts Basic auth that is the correct
cache_peer config for what you want to do.

> When I use this configuration, the squid log is:
> 
> 1430834680.164      0 10.10.10.36 TCP_DENIED/403 3843 GET
> http://www.squid-cache.org/Artwork/SN.png - HIER_NONE/- text/html

No server (or cache_peer) was used for that request. It was just denied.

Which shows your stated rules above are not the whole config story.

Amos



From yvoinov at gmail.com  Tue May  5 14:52:31 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 05 May 2015 20:52:31 +0600
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1YpbHf-004qCG-AC@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE>
Message-ID: <5548D92F.9070308@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 

http://i.imgur.com/mW7gNwD.png

http://squidclamav.darold.net/config.html

This is for squidclamav (I use it and have no problems with malware).

05.05.15 17:45, Stefan K?gler ?????:
> Hi Yuri.
>
> Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
>> This is not squid issue but your AV engine library or ICAP intermediate
>> AV library configuration.
>
> Thank you for your answer.
>
> Can you explain me a litte bit more detailed why this is not a squid
issue?
>
> In the icap-logfile, I can see a REQMOD-request _AND_ a
RESPMOD-request to the icap-server if the object is not in cache.
>
> But - if the object is in cache - I can only see a REQMOD-request to
the icap-server. I am missing RESPMOD.
>
> It seems to me, that it is a decision of the client (squid) which
request (REQMOD or RESPMOD) will be send to the icap-server (AV-scanner)
- and not a decision of the av-library.
>
> Regards, Stefan
>
>>
>> 05.05.15 16:43, Stefan K?gler ?????:
>>> Hello.
>>>
>>>
>>> I have a short question using squid as an ICAP-client.
>>>
>>>
>>> It seems that squid doesn't send an already downloaded (and cached)
>>> object to an ICAP-server.
>>>
>>> Here is a short description what I have done:
>>>
>>> 1. downloading a word-document with a macro-virus. The Virus-scanner
>>> (ICAP-server) uses an old pattern-file and does not detect the virus.
>>>
>>> The object is now in cache.
>>>
>>> 2. updating the virus-scanner to the newest pattern-file. The
>>> virus-scanner will now detect the macro virus.
>>>
>>> 3. downloading the same word-document. The object has been delivered
>>> to the client without a new virus scan.
>>>
>>>
>>>
>>> And now some log-entries:
>>>
>>> 1. First download of the word document:
>>>
>>> access.log:
>>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>>> application/msword
>>>
>>> icap.log:
>>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>
>>> AV-Scanner:
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>> ICAP request decoding
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>>> message decoded in 1 chunks
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>> ICAP request decoding
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>> ICAP request processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>> service processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>>> processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at
>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>> service processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request
>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>> Details: '')
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>>> response headers type: CLEAN 204
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send headers
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>> ICAP request processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core library
>>> session cleared
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection
>>> closed by foreign host while waiting for requests
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core library
>>> session cleared
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>> ICAP request decoding
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>>> message decoded in 259 chunks
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>> ICAP request decoding
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>> ICAP request processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>> service processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>>> processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>> HTTP/1.1>
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>> HTTP/1.1>
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>>> [service_scanner]File 'virus.doc' content is stored in
>>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>> service processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request
>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>> Details: '')
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>>> response headers type: CLEAN
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP
>>> headers for response type: CLEAN
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send headers
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>>> original body (552960 bytes)
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>> ICAP request processing
>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core library
>>> session cleared
>>>
>>>
>>>
>>>
>>>
>>> 2. Second download of the word document (after the pattern-update):
>>>
>>> access.log:
>>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>>
>>> icap.log:
>>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>
>>> AV-Scanner:
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>> ICAP request decoding
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>>> message decoded in 1 chunks
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>> ICAP request decoding
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>> ICAP request processing
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>> service processing
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>>> processing
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at
>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>> service processing
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request
>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>> Details: '')
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>>> response headers type: CLEAN 204
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send headers
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>> ICAP request processing
>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core library
>>> session cleared
>>>
>>>
>>> And now my question: Is this a bug in squid - or is it possible to
>>> tell squid to send already cached object to the icap-server?
>>>
>>> Kind regards,
>>>
>>> Stefan Kuegler
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVSNkvAAoJENNXIZxhPexGsh8IAJGL1gSY3rzshF+BeHmsqZIJ
4L0y2fjrQ66Q8Jz8fKk5saSemIdDRigH0fPAt4Bbb8cVnMcniP09cZ/lspaz3NxA
blodVyDYSLnmWIYzFfg19nd3UWDgIq4yOz3/rXCmHEkQ5sXrJQhJeP4Azeyez4Zj
Qef9ae75cbHexa12U8KERr9SDSnN18tRt4SPz8ZRaoYsoqIC4WRfkO8a0NPfHJp0
cYVj8pwHwbz5TPzYpPrGRR/rPbeO5FOVlIDVrxdHbafLjeYofVR8UOnKn67dxIVu
MJuunsVNtbPaWcDaGkUQ5Z8vvebGDB3pRPNm8XHXp7idGoDTQFJ6JbdK7ofA6do=
=VGI/
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Tue May  5 15:00:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 06 May 2015 03:00:04 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKXPR03MB200B730A796D31DE53351C8A1D10@HKXPR03MB200.apcprd03.prod.outlook.com>
References: <HKXPR03MB200B730A796D31DE53351C8A1D10@HKXPR03MB200.apcprd03.prod.outlook.com>
Message-ID: <5548DAF4.4000408@treenet.co.nz>

On 6/05/2015 2:25 a.m., Ambadas Hibare wrote:
> Hi,
> 
> I trying to spoof client IP via squid proxy by following  squid's
> TPROXY4 wiki page: http://wiki.squid-cache.org/Features/Tproxy4
> 
> But I want to know whether squid can spoof client IP when we send
> proxy format HTTP request from Mozilla (ie configuring proxy & port
> in mozilla). Can squid proxy behave transparently towards only the
> web server & not the client?

No. It can be both ways, or just towards the client.


> 
> I've tried sending proxy format HTTP request from client to squid box
> (on 3129  tproxy port), but I am getting Header forgery error Also
> its trying to connect to itself instead of web server. I am trying to
> understand why squid is trying to match host header's DNS with the
> destination IP instead of connecting to host header's DNS (like
> normal proxy behaviour on port 3128).
> 

To prevent CVE-2009-0801 happening.

You must not send regular forward-proxy traffic to a tproxy or intercept
port. Forwarding loops are guaranteed if you do.

Amos


From marke at wadafarms.com  Tue May  5 16:41:47 2015
From: marke at wadafarms.com (markme)
Date: Tue, 5 May 2015 09:41:47 -0700 (PDT)
Subject: [squid-users] FQDN assigned to loopback
Message-ID: <1430844107262-4671136.post@n4.nabble.com>

I have a FQDN assigned to a loopback address for development purposes but
unfortunately every time I enter in that URL into my browser it goes through
Squid and is trying to access the loopback address on the squid server and
not on the client machine. Is there a way around this?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FQDN-assigned-to-loopback-tp4671136.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue May  5 17:26:06 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 5 May 2015 18:26:06 +0100
Subject: [squid-users] FQDN assigned to loopback
In-Reply-To: <1430844107262-4671136.post@n4.nabble.com>
References: <1430844107262-4671136.post@n4.nabble.com>
Message-ID: <201505051826.06959.Antony.Stone@squid.open.source.it>

On Tuesday 05 May 2015 at 17:41, markme wrote:

> I have a FQDN assigned to a loopback address for development purposes but
> unfortunately every time I enter in that URL into my browser it goes
> through Squid and is trying to access the loopback address on the squid
> server and not on the client machine. Is there a way around this?

Yes, mark it as an exception in your browser proxy settings.  All that I've 
seen have a list of "URLs not to send to the proxy" and it processes those 
direct.

Regards,


Antony.

-- 
In the Beginning there was nothing, which exploded.

 - Terry Pratchett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chris9 at cpalmer.me.uk  Wed May  6 07:20:06 2015
From: chris9 at cpalmer.me.uk (Chris Palmer)
Date: Wed, 06 May 2015 08:20:06 +0100
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <mailman.240.1430806886.2807.squid-users@lists.squid-cache.org>
References: <mailman.240.1430806886.2807.squid-users@lists.squid-cache.org>
Message-ID: <5549C0A6.50806@cpalmer.me.uk>

 >>> There has been a change in behaviour in 3.5.4. It now really does
 >>> prefer to contact a site using an ipv6 address rather than a v4. The
 >>> network stack here doesn't permit v6 so the traffic to sites such as
 >>> google was failing. Setting the following restored the previous
 >>> behaviour:
 >>>
 >>> dns_v4_first on
 >>
 >> As far as I'm aware squid won't try to use ipv6 unless your server has a
 >> Global address, so that shouldn't be needed? Also, wouldn't squid simply
 >> treat that as a DNS name that resolves to a bunch of addresses, so as
 >> long as the IPv6 addresses fail to connect at all, it should have still
 >> ended up succeeding with ipv4 addresses?
 >>

The description of dns_v4_first (squid.conf.documented) says
  With the IPv6 Internet being as fast or faster than IPv4 Internet
  for most networks Squid prefers to contact websites over IPv6.

  This option reverses the order of preference to make Squid contact
  dual-stack websites over IPv4 first. Squid will still perform both
  IPv6 and IPv4 DNS lookups before connecting.

This does indicate specific treatment of IPv6 addresses.

 >> Finally, I'm running squid-3.5.4, don't have ipv6 (just like everyone
 >> else, I still do have the standard fe80:xxx ipv6 link local address) and
 >> google.com works just fine without "dns_v4_first" - which implies my
 >> statements above are correct
 >>
 >> ie this smells like you actually do have ipv6 enabled, but it's broken
 >> in some subtle way (like the pmtu issue Amos mentioned)
 >>
 >
 >
 > The tunnel.cc code producing that read/write error is one of the bits
 > still broken in regards to errno usage. So I dont entirely trust that
 > "endpoint not connected" detail, it seems right but could be something
 > subtly different.
 >
 > Ayways, to get the output at all the TCP SYN/ACK handshake has to have
 > already setup the IPv6 connection with no errors. Then a (first? TLS?)
 > read/write operation attempted on the IPv6 socket fails and does that
 > log message
 >
 > There is supposed to be callback event protections preventing closed
 > sockets being used for read/write (adding to suspicion about the log
 > message), which is where I'm currently trying to figure out what state
 > things could be in.
 >
 > Amos
 >

I've done some more investigation. The problem is not SSL-related, but 
merely IPv6-related.
A network trace confirms that, in the failing IPv6 case, no traffic 
leaves the squid host. This is
expected, as there is no v6 routing. I had the default IPv6 link-local 
address on each interface,
but as an experiment removed all IPv6 addresses from all interfaces, and 
the problem is still the
same. Similarly a traceroute6 correctly reports no route.

In the "configure" help, there is an option to not build v6 support. The 
comment indicates that
squid probes the kernel to determine if it is v6-capable. The problem 
arises when the kernel is compiled
with v6 support, but v6 is not operational on any interface - no 
addresses, no routes etc.

With 3.5.3 I didn't "see" the default "dns_first_v4 off" actually doing 
anything. So for a destination host
that had a v4 address it would try to use those, and it worked. I don't 
know whether it never tried the
v6 address, or maybe it did, realised it had failed, and then 
successfully tried one of the other v4
addresses. With 3.5.4 it definitely goes for the v6 address and then 
fails hard (transport not connected)
without attempting to use any of the v4 addresses.

My hunch is that 3.5.4 has introduced a problem with error handling in 
the v6 code, causing it to fail and
never try any other addresses.

For the majority of installations that do not have v6 capability, I 
suspect that the default setting
of "dns_first_v4 off" is inefficient for sites with both v4 and v6 
addresses as it always tries the v6 addresses
and fails, then goes for the v4 ones that work?? If that is the case, a 
better default for v4
installations might be "dns_first_v4 on". It would obviously fail on 
v6-only destinations but that is to
be expected.

There is a warning in the documentation about using dns_first_v4 though 
which I don't really understand.
I'd like to know what the implications are - and whether I would be 
better simply building squid
without v6 support at all.

Chris


From ashish_behl at yahoo.com  Wed May  6 10:31:50 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Wed, 6 May 2015 03:31:50 -0700 (PDT)
Subject: [squid-users] Error negotiating SSL connection on FD 12: Success
In-Reply-To: <CAGUJm7atntTyDptogkz08w4H573zvynkwE5FzGgpwCy=iGvDuw@mail.gmail.com>
References: <000001d0869e$633225d0$29967170$@netstream.ps>
 <CAGUJm7atntTyDptogkz08w4H573zvynkwE5FzGgpwCy=iGvDuw@mail.gmail.com>
Message-ID: <1430908310298-4671139.post@n4.nabble.com>

I an encountering the same issue.

Using squid 3.5.4 inside docker container, I have set up proxy in my browser
to point to the squid proxy port.
I have also seperated the HTTP and HTTPS ports in squid as well as in
browser.
Full details of the error are on stackoverflow:
http://stackoverflow.com/questions/30057104/squid-ssl-bump-3-5-4-error-error-negotiating-ssl-connection-on-fd-10-success

Please let me know what is wrong here.


Nathan Hoad wrote
> You're experiencing http://bugs.squid-cache.org/show_bug.cgi?id=4236 -
> give the patch on there a try and see if it helps. It should tell you
> what's really failing.
> 
> You'll start getting messages like this:
> 
> Error negotiating SSL connection on FD 439:
> error:00000005:lib(0):func(0):DH lib (5/-1/0)
> 
> Which, in my experience, indicates a client is attempting to put
> non-SSL traffic through that https_port, e.g. HTTP.
> 
> Nathan.
> 
> On 5 May 2015 at 05:13, snakeeyes &lt;

> ahmed.zaeem@

> &gt; wrote:
>> Hi
>>
>> I created privste & public keys for squid , but it still give me error
>> for
>> negotiating
>>
>>
>> https_port 443 accel key=/root/CA/myCA/private/squid.local.key
>> cert=/root/CA/myCA/certs/squid.local.crt
>> cache.log
>>
>>
>>
>> 2015/05/04 11:59:19 kid1| Error negotiating SSL connection on FD 12:
>> Success
>> (0)
>>
>> 2015/05/04 11:59:21 kid1| Error negotiating SSL connection on FD 12:
>> Success
>> (0)
>>
>>
>>
>> Any help ?
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> 

> squid-users at .squid-cache

>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-connection-on-FD-12-Success-tp4671090p4671139.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rodrigo.mauricio at rac.com.br  Wed May  6 12:41:05 2015
From: rodrigo.mauricio at rac.com.br (Rodrigo Lopes Mauricio)
Date: Wed, 06 May 2015 09:41:05 -0300
Subject: [squid-users] Geolocation Vs Squid
Message-ID: <554A0BE1.9070204@rac.com.br>

Hi everybody.

Is the any know issue with squid and geolocation cookies?
Every website that have some component that identify your region don't 
work here.
If I get off the proxy, loads ok.

Is there any config that I should do?

I have squid3/wheezy uptodate 3.1.20-2.2+deb7u2



Thanks in advance
Rodrigo



From squid3 at treenet.co.nz  Wed May  6 12:49:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 00:49:41 +1200
Subject: [squid-users] Error negotiating SSL connection on FD 12: Success
In-Reply-To: <1430908310298-4671139.post@n4.nabble.com>
References: <000001d0869e$633225d0$29967170$@netstream.ps>
 <CAGUJm7atntTyDptogkz08w4H573zvynkwE5FzGgpwCy=iGvDuw@mail.gmail.com>
 <1430908310298-4671139.post@n4.nabble.com>
Message-ID: <554A0DE5.9070301@treenet.co.nz>

On 6/05/2015 10:31 p.m., Ashish Behl wrote:
> I an encountering the same issue.
> 
> Using squid 3.5.4 inside docker container, I have set up proxy in my browser
> to point to the squid proxy port.
> I have also seperated the HTTP and HTTPS ports in squid as well as in
> browser.
> Full details of the error are on stackoverflow:
> http://stackoverflow.com/questions/30057104/squid-ssl-bump-3-5-4-error-error-negotiating-ssl-connection-on-fd-10-success
> 
> Please let me know what is wrong here.

You are connecting the curl and browser to port 8080. Which is
configured to *only* receive traffic from the OS NAT system (intercept
flag).

Remove the "intercept" flag from Squid if you are going to connect to
that port with clients, or duplicate the ssl-bump configuration on the
port 8080 line.

If you are only doing this for "testing". Then please stop. Test what is
actually going to be used - in the *way* that it is actually going to be
used. As if your tester was one of the real clients.

HTTP (and HTTPS) are remarkably complicated these days. Testing with a
completely different type of traffic than you expect to occur normally,
is not going to get you anywhere near a working system.

Amos



From Antony.Stone at squid.open.source.it  Wed May  6 12:51:35 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 6 May 2015 13:51:35 +0100
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A0BE1.9070204@rac.com.br>
References: <554A0BE1.9070204@rac.com.br>
Message-ID: <201505061351.36101.Antony.Stone@squid.open.source.it>

On Wednesday 06 May 2015 at 13:41, Rodrigo Lopes Mauricio wrote:

> Hi everybody.
> 
> Is the any know issue with squid and geolocation cookies?

I'm not familiar with geolocation services using cookies - mainly because a 
cookie is something sent to your computer by the server, so the server would 
have to know where you are before it could send you the cookie containing any 
location information.

Geolocation normally works based on your public IP address, and linking that 
to various databases of ISPs and their coverage areas.

> Every website that have some component that identify your region don't
> work here.

Can you give us an example of one you have tried and found this problem with?

> If I get off the proxy, loads ok.

Is there any difference in your outbound public IP address when you go direct, 
or via the proxy?

Regards,


Antony.

-- 
This space intentionally has nothing but text explaining why this space has 
nothing but text explaining that this space would otherwise have been left 
blank, and would otherwise have been left blank.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rodrigo.mauricio at rac.com.br  Wed May  6 12:57:00 2015
From: rodrigo.mauricio at rac.com.br (Rodrigo Lopes Mauricio)
Date: Wed, 06 May 2015 09:57:00 -0300
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <201505061351.36101.Antony.Stone@squid.open.source.it>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
Message-ID: <554A0F9C.9070900@rac.com.br>

Hi Antony. Thanks for your answer.

The public IP is the same with or without squid.

This is the site in question:
http://minhaclaro.claro.com.br

I can access this one without proxy only.
When I go via proxy, nothing is loaded, just blank page.


Thanks


On 06/05/2015 09:51, Antony Stone wrote:
> On Wednesday 06 May 2015 at 13:41, Rodrigo Lopes Mauricio wrote:
>
>> Hi everybody.
>>
>> Is the any know issue with squid and geolocation cookies?
> I'm not familiar with geolocation services using cookies - mainly because a
> cookie is something sent to your computer by the server, so the server would
> have to know where you are before it could send you the cookie containing any
> location information.
>
> Geolocation normally works based on your public IP address, and linking that
> to various databases of ISPs and their coverage areas.
>
>> Every website that have some component that identify your region don't
>> work here.
> Can you give us an example of one you have tried and found this problem with?
>
>> If I get off the proxy, loads ok.
> Is there any difference in your outbound public IP address when you go direct,
> or via the proxy?
>
> Regards,
>
>
> Antony.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/af14c798/attachment.htm>

From squid3 at treenet.co.nz  Wed May  6 13:09:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 01:09:00 +1200
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A0F9C.9070900@rac.com.br>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br>
Message-ID: <554A126C.2030207@treenet.co.nz>

On 7/05/2015 12:57 a.m., Rodrigo Lopes Mauricio wrote:
> Hi Antony. Thanks for your answer.
> 
> The public IP is the same with or without squid.
> 
> This is the site in question:
> http://minhaclaro.claro.com.br
> 
> I can access this one without proxy only.
> When I go via proxy, nothing is loaded, just blank page.


It sounds like they are maybe using extra HTTP level checks to identify
a proxy and assuming the IP is faked. A lot of people do that to get
around the breakage caused by sites assuming they can know the
geo-location of people.


If you have a recent Squid try these two options. Try them individually
to see if you can get away with only using one:

  forwarded_for transparent
  via off


PS. if your Squid does not accept that "transparent" parameter, please
upgrade. It may be that an upgrade would solve the problems anyway.

Amos




From rodrigo.mauricio at rac.com.br  Wed May  6 13:48:50 2015
From: rodrigo.mauricio at rac.com.br (Rodrigo Lopes Mauricio)
Date: Wed, 06 May 2015 10:48:50 -0300
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A126C.2030207@treenet.co.nz>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br> <554A126C.2030207@treenet.co.nz>
Message-ID: <554A1BC2.90201@rac.com.br>

I made a clone of my virtual machine and made some tests...
The two options didn't work.

I'll purge my squid and will compile it again.

Is there any specfic parameter that I have to use?


Thanks


On 06/05/2015 10:09, Amos Jeffries wrote:
> On 7/05/2015 12:57 a.m., Rodrigo Lopes Mauricio wrote:
>> Hi Antony. Thanks for your answer.
>>
>> The public IP is the same with or without squid.
>>
>> This is the site in question:
>> http://minhaclaro.claro.com.br
>>
>> I can access this one without proxy only.
>> When I go via proxy, nothing is loaded, just blank page.
>
> It sounds like they are maybe using extra HTTP level checks to identify
> a proxy and assuming the IP is faked. A lot of people do that to get
> around the breakage caused by sites assuming they can know the
> geo-location of people.
>
>
> If you have a recent Squid try these two options. Try them individually
> to see if you can get away with only using one:
>
>    forwarded_for transparent
>    via off
>
>
> PS. if your Squid does not accept that "transparent" parameter, please
> upgrade. It may be that an upgrade would solve the problems anyway.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/fd0f8d29/attachment.htm>

From squid3 at treenet.co.nz  Wed May  6 13:49:43 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 01:49:43 +1200
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A0F9C.9070900@rac.com.br>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br>
Message-ID: <554A1BF7.6080100@treenet.co.nz>

On 7/05/2015 12:57 a.m., Rodrigo Lopes Mauricio wrote:
> Hi Antony. Thanks for your answer.
> 
> The public IP is the same with or without squid.
> 
> This is the site in question:
> http://minhaclaro.claro.com.br
> 

Just for fun I went and looked at how this was working.

The default public facing page presented by that IIS/6.0 server is a
page saying "Under Constrution" and reports its reply content as being
located at http://172.30.0.13/iisstart.htm

172.30.0.13 is a private RFC1918 space IP address, not for use on the
global Internet.

Ouch.

That goes on and does an HTML level redirect (30x redirect works
better). Presenting the client with a web form (huh?) in the clear (ouch
#2) containing a fixed set of logins fields (ouch #3) to send over HTTPS
to a third-party domain using a very weak cipher protected by MD5 hash
(alarm bells). Resulting in yet anothet HTML level redirect.

If you access that server without the right details from the form it
diverts you to http://go.microsoft.com/ and sends your details there
along with search query terms for "HTTP 404" (grr).

I would look deeper, but this is already making me want to strangle someone.

Amos




From rodrigo.mauricio at rac.com.br  Wed May  6 14:09:31 2015
From: rodrigo.mauricio at rac.com.br (Rodrigo Lopes Mauricio)
Date: Wed, 06 May 2015 11:09:31 -0300
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A1BF7.6080100@treenet.co.nz>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br> <554A1BF7.6080100@treenet.co.nz>
Message-ID: <554A209B.3060007@rac.com.br>

ouch#4 here...

I purged my squid and installed a new one with apt-get (don't know if I 
have the skills to compile one).
Now I received a message (one is better than nothing) on mozilla:

/Error 404--Not Found//
//From RFC 2068 Hypertext Transfer Protocol -- HTTP/1.1://
//10.4.5 404 Not Found//
//
//The server has not found anything matching the Request-URI. No 
indication is given of whether the condition is temporary or permanent./





On 06/05/2015 10:49, Amos Jeffries wrote:
> On 7/05/2015 12:57 a.m., Rodrigo Lopes Mauricio wrote:
>> Hi Antony. Thanks for your answer.
>>
>> The public IP is the same with or without squid.
>>
>> This is the site in question:
>> http://minhaclaro.claro.com.br
>>
> Just for fun I went and looked at how this was working.
>
> The default public facing page presented by that IIS/6.0 server is a
> page saying "Under Constrution" and reports its reply content as being
> located at http://172.30.0.13/iisstart.htm
>
> 172.30.0.13 is a private RFC1918 space IP address, not for use on the
> global Internet.
>
> Ouch.
>
> That goes on and does an HTML level redirect (30x redirect works
> better). Presenting the client with a web form (huh?) in the clear (ouch
> #2) containing a fixed set of logins fields (ouch #3) to send over HTTPS
> to a third-party domain using a very weak cipher protected by MD5 hash
> (alarm bells). Resulting in yet anothet HTML level redirect.
>
> If you access that server without the right details from the form it
> diverts you to http://go.microsoft.com/ and sends your details there
> along with search query terms for "HTTP 404" (grr).
>
> I would look deeper, but this is already making me want to strangle someone.
>
> Amos
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/a951df34/attachment.htm>

From squid3 at treenet.co.nz  Wed May  6 14:10:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 02:10:48 +1200
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A1BC2.90201@rac.com.br>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br> <554A126C.2030207@treenet.co.nz>
 <554A1BC2.90201@rac.com.br>
Message-ID: <554A20E8.8010401@treenet.co.nz>

On 7/05/2015 1:48 a.m., Rodrigo Lopes Mauricio wrote:
> I made a clone of my virtual machine and made some tests...
> The two options didn't work.
> 
> I'll purge my squid and will compile it again.
> 
> Is there any specfic parameter that I have to use?

Those two options are the ones Squid emits that identify a proxy in use.
Since they dont work theres little else you can do.

 It could be your browser showing a blank page in objection to the
ancient cipher the https:// URLs involved require.
 It could be your browser or any network security device objecting to
automated javascripts triggering HTTP->HTTPS transition.
 It could be IIS/6 hanging when receiving an HTTP/1.1 POST message.
 It could be the ASP engine inside it crashing when it receives an
HTTP/1.1 compliant POST message (nice one!).
 It could be anything the server objects to in the difference between
HTTP (via proxy) and HTTPS (non-proxied) messages since both are involved.

Amos



From ashish_behl at yahoo.com  Wed May  6 13:59:30 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Wed, 6 May 2015 06:59:30 -0700 (PDT)
Subject: [squid-users] Error negotiating SSL connection on FD 12: Success
In-Reply-To: <554A0DE5.9070301@treenet.co.nz>
References: <000001d0869e$633225d0$29967170$@netstream.ps>
 <CAGUJm7atntTyDptogkz08w4H573zvynkwE5FzGgpwCy=iGvDuw@mail.gmail.com>
 <1430908310298-4671139.post@n4.nabble.com> <554A0DE5.9070301@treenet.co.nz>
Message-ID: <1430920770111-4671149.post@n4.nabble.com>

Thanks a lot for your Answer Amos,
My mistake, 

I have to use intercept and use squid as transparent proxy (I was lazy to
setup a router, setup transparent proxy m/c. I should do it now.). 
I have changed the configuration to use http_port instead of https_port and
then removed "intercept". this works outside docker.

2nd step is to try this inside docker.

Thanks again for your help.


Amos Jeffries wrote
> You are connecting the curl and browser to port 8080. Which is
> configured to *only* receive traffic from the OS NAT system (intercept
> flag).
> 
> Remove the "intercept" flag from Squid if you are going to connect to
> that port with clients, or duplicate the ssl-bump configuration on the
> port 8080 line.
> 
> If you are only doing this for "testing". Then please stop. Test what is
> actually going to be used - in the *way* that it is actually going to be
> used. As if your tester was one of the real clients.
> 
> HTTP (and HTTPS) are remarkably complicated these days. Testing with a
> completely different type of traffic than you expect to occur normally,
> is not going to get you anywhere near a working system.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-connection-on-FD-12-Success-tp4671090p4671149.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed May  6 14:38:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 02:38:23 +1200
Subject: [squid-users] Geolocation Vs Squid
In-Reply-To: <554A209B.3060007@rac.com.br>
References: <554A0BE1.9070204@rac.com.br>
 <201505061351.36101.Antony.Stone@squid.open.source.it>
 <554A0F9C.9070900@rac.com.br> <554A1BF7.6080100@treenet.co.nz>
 <554A209B.3060007@rac.com.br>
Message-ID: <554A275F.3020500@treenet.co.nz>

On 7/05/2015 2:09 a.m., Rodrigo Lopes Mauricio wrote:
> ouch#4 here...
> 
> I purged my squid and installed a new one with apt-get (don't know if I
> have the skills to compile one).
> Now I received a message (one is better than nothing) on mozilla:
> 
> /Error 404--Not Found//
> //From RFC 2068 Hypertext Transfer Protocol -- HTTP/1.1://
> //10.4.5 404 Not Found//
> //
> //The server has not found anything matching the Request-URI. No
> indication is given of whether the condition is temporary or permanent./
> 

Thats the go.microsoft.com search page results embeded in the IIS error
page. I got it without any proxy at all if the Cookie and form details
were not exactly what was presented in that second redirect.

Amos




From marke at wadafarms.com  Wed May  6 15:11:52 2015
From: marke at wadafarms.com (markme)
Date: Wed, 6 May 2015 08:11:52 -0700 (PDT)
Subject: [squid-users] FQDN assigned to loopback
In-Reply-To: <201505051826.06959.Antony.Stone@squid.open.source.it>
References: <1430844107262-4671136.post@n4.nabble.com>
 <201505051826.06959.Antony.Stone@squid.open.source.it>
Message-ID: <1430925112590-4671151.post@n4.nabble.com>

It worked. Thanks!



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/FQDN-assigned-to-loopback-tp4671136p4671151.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed May  6 15:48:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 03:48:05 +1200
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL pages
In-Reply-To: <5549C0A6.50806@cpalmer.me.uk>
References: <mailman.240.1430806886.2807.squid-users@lists.squid-cache.org>
 <5549C0A6.50806@cpalmer.me.uk>
Message-ID: <554A37B5.8020402@treenet.co.nz>

On 6/05/2015 7:20 p.m., Chris Palmer wrote:
>>>> There has been a change in behaviour in 3.5.4. It now really does
>>>> prefer to contact a site using an ipv6 address rather than a v4. The
>>>> network stack here doesn't permit v6 so the traffic to sites such as
>>>> google was failing. Setting the following restored the previous
>>>> behaviour:
>>>>
>>>> dns_v4_first on
>>>
>>> As far as I'm aware squid won't try to use ipv6 unless your server has a
>>> Global address, so that shouldn't be needed? Also, wouldn't squid simply
>>> treat that as a DNS name that resolves to a bunch of addresses, so as
>>> long as the IPv6 addresses fail to connect at all, it should have still
>>> ended up succeeding with ipv4 addresses?
>>>
> 
> The description of dns_v4_first (squid.conf.documented) says
>  With the IPv6 Internet being as fast or faster than IPv4 Internet
>  for most networks Squid prefers to contact websites over IPv6.
> 
>  This option reverses the order of preference to make Squid contact
>  dual-stack websites over IPv4 first. Squid will still perform both
>  IPv6 and IPv4 DNS lookups before connecting.
> 
> This does indicate specific treatment of IPv6 addresses.

Yes. RFC 6540:

"
   o  IPv6 support must be equivalent or better in quality and
      functionality when compared to IPv4 support in a new or updated IP
      implementation.
"

We do this in Squid by the default connection being to attempt to use
IPv6 addresses first, then IPv4.

On a machine properly supporting IPv6 - including the case where IPv6
addresses have not been assigned. There is no latency.

On a machine where IPv6 has been mangled or broken by attempts to
disable it (which is actually not possible in modern systems). Then by
default IPv4 suffers as much as IPv6.


> 
>>> Finally, I'm running squid-3.5.4, don't have ipv6 (just like everyone
>>> else, I still do have the standard fe80:xxx ipv6 link local address) and
>>> google.com works just fine without "dns_v4_first" - which implies my
>>> statements above are correct
>>>
>>> ie this smells like you actually do have ipv6 enabled, but it's broken
>>> in some subtle way (like the pmtu issue Amos mentioned)
>>>
>>
>>
>> The tunnel.cc code producing that read/write error is one of the bits
>> still broken in regards to errno usage. So I dont entirely trust that
>> "endpoint not connected" detail, it seems right but could be something
>> subtly different.
>>
>> Ayways, to get the output at all the TCP SYN/ACK handshake has to have
>> already setup the IPv6 connection with no errors. Then a (first? TLS?)
>> read/write operation attempted on the IPv6 socket fails and does that
>> log message
>>
>> There is supposed to be callback event protections preventing closed
>> sockets being used for read/write (adding to suspicion about the log
>> message), which is where I'm currently trying to figure out what state
>> things could be in.
>>
>> Amos
>>
> 
> I've done some more investigation. The problem is not SSL-related, but
> merely IPv6-related.
> A network trace confirms that, in the failing IPv6 case, no traffic
> leaves the squid host. This is
> expected, as there is no v6 routing. I had the default IPv6 link-local
> address on each interface,
> but as an experiment removed all IPv6 addresses from all interfaces, and
> the problem is still the
> same. Similarly a traceroute6 correctly reports no route.
> 
> In the "configure" help, there is an option to not build v6 support. The
> comment indicates that
> squid probes the kernel to determine if it is v6-capable. The problem
> arises when the kernel is compiled
> with v6 support, but v6 is not operational on any interface - no
> addresses, no routes etc.
> 
> With 3.5.3 I didn't "see" the default "dns_first_v4 off" actually doing
> anything. So for a destination host
> that had a v4 address it would try to use those, and it worked. I don't
> know whether it never tried the
> v6 address, or maybe it did, realised it had failed, and then
> successfully tried one of the other v4
> addresses. With 3.5.4 it definitely goes for the v6 address and then
> fails hard (transport not connected)
> without attempting to use any of the v4 addresses.
> 
> My hunch is that 3.5.4 has introduced a problem with error handling in
> the v6 code, causing it to fail and
> never try any other addresses.
> 
> For the majority of installations that do not have v6 capability, I
> suspect that the default setting
> of "dns_first_v4 off" is inefficient for sites with both v4 and v6
> addresses as it always tries the v6 addresses
> and fails, then goes for the v4 ones that work?? If that is the case, a
> better default for v4
> installations might be "dns_first_v4 on". It would obviously fail on
> v6-only destinations but that is to
> be expected.

On a properly configured machine the inefficiency is measured in
nanoseconds or less. Its one syscall per IP that fails.
This also helps meet the RFC 6540 criteria, since the inefficiency is
added as prefix to IPv4 connections usage (enhancing IPv6 relative
performance).

> 
> There is a warning in the documentation about using dns_first_v4 though
> which I don't really understand.

This bug itself is a good example about what the warning is about.

If you run Squid with "dns_v4_first on", or if it was the default this
fairy major bug would not have been found for a long time. The same
thing applies for regular routing problems, and IPv6 connectivity
issues, etc, etc. all of them are hidden away out of sight but still
very much existing when dns_v4_first is used.

They *will* come up to bite you later on. Its best to lets you know
where the problems are so they can be solved quickly.



> I'd like to know what the implications are - and whether I would be
> better simply building squid
> without v6 support at all.

I think the bug was introduced by the fix to bug 4234. If you can assist
by building and running Squid with that bug patch reversed it would help.

Amos


From michael.pelletier at palmbeachschools.org  Wed May  6 17:53:00 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 6 May 2015 13:53:00 -0400
Subject: [squid-users] Any Greasyspoon iCAP users out there?
Message-ID: <CAEnCSG7Ud3BQoVnsFuQFOBO5W7qEk-Dr3gAqc81Wb2kw=bKZ0g@mail.gmail.com>

Hello,
I am having some difficulties with the greasyspoon icap server. The demo
scripts don't work and I can not find any api documentation. The plan is to
use greasyspoon icap server now then migrate to ecap later.

Who is using greasyspoon icap server? I could sure use some help.


Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/87aaa7cf/attachment.htm>

From stan.prescott at gmail.com  Wed May  6 22:58:49 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 6 May 2015 17:58:49 -0500
Subject: [squid-users] 3.5.4 need more help with peek and splice and
	external helper
Message-ID: <CANLNtGRgGiBSWbuYAA14t6=SV+Kxh-pvghdq5fgTCyb30DJaPg@mail.gmail.com>

I have still been trying to get peek and splice to work. Specifically I
want to allow the admins of our firewall distro to enter websites that they
do not want to bump on the squid UI page. I have been fiddling with info
that Amos and Nathan have provided me but with no success so far. Here is a
snippet of squid.conf with most of the pertinent SSL configuration.

*http_access allow localhostgreen*
*http_access allow CONNECT localhostgreen*

*# http_port and https_port*
*#----------------------------------------------------------------------------*

*http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept*
*https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*http_port 127.0.0.1:800 <http://127.0.0.1:800> intercept*

*sslproxy_cert_error allow all*
*sslproxy_flags DONT_VERIFY_PEER*
*sslproxy_session_cache_size 4 MB*

*ssl_bump none localhostgreen*

*external_acl_type sni ttl=30 concurrency=60 children-max=3
children-startup=1 %ssl::>sni /var/smoothwall/mods/proxy/libexec/bumphelper*

*acl sni_exclusions external sni*
*acl tcp_level at_step SslBump1*
*acl client_hello_peeked at_step SslBump2*

*ssl_bump none localhostgreen*

*ssl_bump peek tcp_level all*
*ssl_bump splice client_hello_peeked sni_exclusions*
*ssl_bump bump all*

*sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MB*
*sslcrtd_children 5*


These were provided by Nathan to try. He also provided an example helper
script in python to try, but our distro doesn't grok python so I tried to
get it translated to perl and this what I came up with.

*#!/usr/bin/perl*

*# run loop until an empty read, which indicates the process should shut
down.*

*while (<STDIN>)*
*{*
*  my ($concurrency_id, $sni) = split;*

*  if ($sni eq 'wellsfargo.com <http://wellsfargo.com>')*
*  {*
*     print "$concurreny_id OK\n";*
*  }*
*  else*
*  {*
*     print "$concurreny_id ERR\n";*
*  }*
*}*


When I start Squid with this configuration, the helper script "bumphelper"
gets loaded as a process along with squid and ssl_crtd. When I try to
browse any SSL websites there is no connection and it times out. HTTP
browsing is fine. When I remove those peek and splice related lines and add
"ssl_bump server-first all" back to squid conf then bumping of SSL sites is
successful.

I suspect my "bumphelper" script is not doing what I intend it to do.

Suggestions very welcome.

Stan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/b7738ccb/attachment.htm>

From Jason_Haar at trimble.com  Wed May  6 23:21:32 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Thu, 7 May 2015 11:21:32 +1200
Subject: [squid-users] 3.5.4 need more help with peek and splice and
 external helper
In-Reply-To: <CANLNtGRgGiBSWbuYAA14t6=SV+Kxh-pvghdq5fgTCyb30DJaPg@mail.gmail.com>
References: <CANLNtGRgGiBSWbuYAA14t6=SV+Kxh-pvghdq5fgTCyb30DJaPg@mail.gmail.com>
Message-ID: <554AA1FC.40302@trimble.com>

On 07/05/15 10:58, Stanford Prescott wrote:
> When I start Squid with this configuration, the helper script
> "bumphelper" gets loaded as a process along with squid and ssl_crtd.
> When I try to browse any SSL websites there is no connection and it
> times out.

The problem is that you're calling perl with the default I/O buffering
left *enabled*. You need to add "$|=1;" near the top so that perl will
flush I/O immediately - that should stop the hanging

Good use of words in your acl names - I think that really helps in
understanding just what is going on with the smoke-n-mirrors that is SSL
intercept :-)

-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From nathan at getoffmalawn.com  Thu May  7 01:15:18 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Thu, 7 May 2015 11:15:18 +1000
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CANLNtGRGpzc6d495x-H=v=m_zDOqHyHLg2Kd=VoUd4S77SKxZg@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
 <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>
 <CANLNtGRGpzc6d495x-H=v=m_zDOqHyHLg2Kd=VoUd4S77SKxZg@mail.gmail.com>
Message-ID: <CAGUJm7YdfW3NepcZ2NDakYBML7onZ3dYMoFeUv0mkWf_7NPaTA@mail.gmail.com>

Hi Stan,

Yep, I think the server_name acl in 3.5.4 should provide what you want
without the need for an external acl now. I haven't used it as the
external acl fits my usecase. I imagine doing something like this
should work for server_name though...

acl sni_exclusions ssl::server_name wellsfargo.com
acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2

ssl_bump peek tcp_level all
ssl_bump splice client_hello_peeked sni_exclusions
ssl_bump bump all

Hopefully your other issue with your perl helper hanging has been
resolved by Jason's recommendation! Though if this does what you want,
you may not need the helper.

Also, please try to keep messages to the mailing list - this is all
information that will help others :)

Thanks,

Nathan.

On 5 May 2015 at 13:20, Stanford Prescott <stan.prescott at gmail.com> wrote:
> Hi Nathan. I have decided to try to go ahead and try to get peek and splice
> working for Squid on the Smoothwall Express firewall distro since we will
> not be able to migrate to Squid 4.x when it debuts. You previously kindly
> offered an example of a squid.conf setup for me to try to get it working.
>
> external_acl_type sni ttl=30 concurrency=X children-max=Y
> children-startup=Z %ssl::>sni /path/to/your/helper
>
> acl sni_exclusions external sni
> acl tcp_level at_step SslBump1
> acl client_hello_peeked at_step SslBump2
>
> ssl_bump peek tcp_level all
> ssl_bump splice client_hello_peeked sni_exclusions
> ssl_bump bump all
>
> Amos says he has back ported a server_name acl with the 3.5.4 release. Does
> this now mean that the "external_acl_type" is no longer needed for this sort
> of function? Specifically, I want to be able to allow my users to enter
> websites that they do not want bumped, like banking websites. I wasn't able
> to get the squid.conf and helper script example you provided to work for me.
>
> Does the new server_name acl change how this can be done? Would you be able
> to provide a new example for me to try based on this new acl in squid 3.5.4?
>
> Any help is greatly appreciated.
>
> Stan
>
>
> On Sun, Apr 12, 2015 at 7:25 PM, Nathan Hoad <nathan at getoffmalawn.com>
> wrote:
>>
>> Hi Stan,
>>
>> For peek and splice, you need to decide based on the SNI name, not the
>> domain name, which for 3.5 means you need to use an external ACL
>> helper that processes %ssl::>sni. For 4.0 there will be a server_name
>> ACL you can use instead.
>>
>> On top of that, you also need to make sure this external ACL helper
>> runs at the correct "bump step", with the at_step ACL, e.g...
>>
>> external_acl_type sni ttl=30 concurrency=X children-max=Y
>> children-startup=Z %ssl::>sni /path/to/your/helper
>>
>> acl sni_exclusions external sni
>> acl tcp_level at_step SslBump1
>> acl client_hello_peeked at_step SslBump2
>>
>> ssl_bump peek tcp_level all
>> ssl_bump splice client_hello_peeked sni_exclusions
>> ssl_bump bump all
>>
>> Hope that helps,
>>
>> Nathan.
>>
>> On 13 April 2015 at 04:12, Stanford Prescott <stan.prescott at gmail.com>
>> wrote:
>> > I would like to give my users the ability to "not bump" certain sites. I
>> > tried to use the examples given on the SSLPeekandSplice wiki page but
>> > can't
>> > get it to work.
>> >
>> > This is a snippet of my squid.conf file.
>> >
>> > https_port 192.168.10.1:808 intercept ssl-bump
>> > generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB
>> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >
>> > http_port 192.168.20.1:800 intercept
>> > https_port 192.168.20.1:808 intercept ssl-bump
>> > generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=4MB
>> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >
>> > http_port 127.0.0.1:800 intercept
>> >
>> > sslproxy_cert_error allow all
>> > sslproxy_flags DONT_VERIFY_PEER
>> > sslproxy_session_cache_size 4 MB
>> >
>> > acl serverIsBank dstdomain wellsfargo.com
>> >
>> > ssl_bump server-first all
>> >
>> > ssl_bump none localhostgreen
>> > ssl_bump none localhostpurple
>> >
>> > ssl_bump splice serverIsBank
>> > ssl_bump peek all
>> > ssl_bump bump all
>> > sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
>> > /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
>> > sslcrtd_children 5
>> >
>> >
>> > When I start squid I don't get any error messages and all pages, http
>> > and
>> > https, load properly. The problem is, using the example above, the
>> > https://www.wellsfargo.com website is still getting bumped, evidenced by
>> > the
>> > appearance of the ssl website in the web proxy access logs. When I don't
>> > have ssl_bump enabled then no https websites appear in the access logs,
>> > as
>> > it should be. But, enabling ssl_bump and peek and splice, web sites that
>> > I
>> > am trying not to bump still seem to be getting bumped.
>> >
>> > Any suggestions on how to properly "not bump" certain websites.
>> >
>> > Thanks,
>> >
>> > Stan
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>
>


From stan.prescott at gmail.com  Thu May  7 02:24:20 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 6 May 2015 21:24:20 -0500
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CAGUJm7YdfW3NepcZ2NDakYBML7onZ3dYMoFeUv0mkWf_7NPaTA@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
 <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>
 <CANLNtGRGpzc6d495x-H=v=m_zDOqHyHLg2Kd=VoUd4S77SKxZg@mail.gmail.com>
 <CAGUJm7YdfW3NepcZ2NDakYBML7onZ3dYMoFeUv0mkWf_7NPaTA@mail.gmail.com>
Message-ID: <CANLNtGQ=Z11aVw1TrjLqZ-LPj6O-L1tPNxNeba-EfWDmAphdQw@mail.gmail.com>

Jason helped me a lot although I am still having trouble getting that
helper working. It got to the point that only the website I didn't want
bumped was getting bumped because I had my logic switched in the helper
script to nothing getting bumped at all. Jason pointed out that I appear to
be using transparent intercept proxy and that I shouldn't do that until I
have everything worked out with the regular proxy since bumping is
difficult to do with transparent proxy.

I have been using transparent proxy (intercept) with the https_port
declaration with great success so far as seen here

*http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept*
*https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*


For peek and splice should I not be using intercept?

I will give your suggestions a try and get back to you.

Also, I apologize for inadvertently not including our correspondence in the
squid-users list. I was just clicking the reply button not realizing it
wasn't a "reply all". I think I've got it figured out how to include the
list in replies.

Thanks Nathan.

Stan

On Wed, May 6, 2015 at 8:15 PM, Nathan Hoad <nathan at getoffmalawn.com> wrote:

> Hi Stan,
>
> Yep, I think the server_name acl in 3.5.4 should provide what you want
> without the need for an external acl now. I haven't used it as the
> external acl fits my usecase. I imagine doing something like this
> should work for server_name though...
>
> acl sni_exclusions ssl::server_name wellsfargo.com
> acl tcp_level at_step SslBump1
> acl client_hello_peeked at_step SslBump2
>
> ssl_bump peek tcp_level all
> ssl_bump splice client_hello_peeked sni_exclusions
> ssl_bump bump all
>
> Hopefully your other issue with your perl helper hanging has been
> resolved by Jason's recommendation! Though if this does what you want,
> you may not need the helper.
>
> Also, please try to keep messages to the mailing list - this is all
> information that will help others :)
>
> Thanks,
>
> Nathan.
>
> On 5 May 2015 at 13:20, Stanford Prescott <stan.prescott at gmail.com> wrote:
> > Hi Nathan. I have decided to try to go ahead and try to get peek and
> splice
> > working for Squid on the Smoothwall Express firewall distro since we will
> > not be able to migrate to Squid 4.x when it debuts. You previously kindly
> > offered an example of a squid.conf setup for me to try to get it working.
> >
> > external_acl_type sni ttl=30 concurrency=X children-max=Y
> > children-startup=Z %ssl::>sni /path/to/your/helper
> >
> > acl sni_exclusions external sni
> > acl tcp_level at_step SslBump1
> > acl client_hello_peeked at_step SslBump2
> >
> > ssl_bump peek tcp_level all
> > ssl_bump splice client_hello_peeked sni_exclusions
> > ssl_bump bump all
> >
> > Amos says he has back ported a server_name acl with the 3.5.4 release.
> Does
> > this now mean that the "external_acl_type" is no longer needed for this
> sort
> > of function? Specifically, I want to be able to allow my users to enter
> > websites that they do not want bumped, like banking websites. I wasn't
> able
> > to get the squid.conf and helper script example you provided to work for
> me.
> >
> > Does the new server_name acl change how this can be done? Would you be
> able
> > to provide a new example for me to try based on this new acl in squid
> 3.5.4?
> >
> > Any help is greatly appreciated.
> >
> > Stan
> >
> >
> > On Sun, Apr 12, 2015 at 7:25 PM, Nathan Hoad <nathan at getoffmalawn.com>
> > wrote:
> >>
> >> Hi Stan,
> >>
> >> For peek and splice, you need to decide based on the SNI name, not the
> >> domain name, which for 3.5 means you need to use an external ACL
> >> helper that processes %ssl::>sni. For 4.0 there will be a server_name
> >> ACL you can use instead.
> >>
> >> On top of that, you also need to make sure this external ACL helper
> >> runs at the correct "bump step", with the at_step ACL, e.g...
> >>
> >> external_acl_type sni ttl=30 concurrency=X children-max=Y
> >> children-startup=Z %ssl::>sni /path/to/your/helper
> >>
> >> acl sni_exclusions external sni
> >> acl tcp_level at_step SslBump1
> >> acl client_hello_peeked at_step SslBump2
> >>
> >> ssl_bump peek tcp_level all
> >> ssl_bump splice client_hello_peeked sni_exclusions
> >> ssl_bump bump all
> >>
> >> Hope that helps,
> >>
> >> Nathan.
> >>
> >> On 13 April 2015 at 04:12, Stanford Prescott <stan.prescott at gmail.com>
> >> wrote:
> >> > I would like to give my users the ability to "not bump" certain
> sites. I
> >> > tried to use the examples given on the SSLPeekandSplice wiki page but
> >> > can't
> >> > get it to work.
> >> >
> >> > This is a snippet of my squid.conf file.
> >> >
> >> > https_port 192.168.10.1:808 intercept ssl-bump
> >> > generate-host-certificates=on
> >> > dynamic_cert_mem_cache_size=4MB
> >> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
> >> >
> >> > http_port 192.168.20.1:800 intercept
> >> > https_port 192.168.20.1:808 intercept ssl-bump
> >> > generate-host-certificates=on
> >> > dynamic_cert_mem_cache_size=4MB
> >> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
> >> >
> >> > http_port 127.0.0.1:800 intercept
> >> >
> >> > sslproxy_cert_error allow all
> >> > sslproxy_flags DONT_VERIFY_PEER
> >> > sslproxy_session_cache_size 4 MB
> >> >
> >> > acl serverIsBank dstdomain wellsfargo.com
> >> >
> >> > ssl_bump server-first all
> >> >
> >> > ssl_bump none localhostgreen
> >> > ssl_bump none localhostpurple
> >> >
> >> > ssl_bump splice serverIsBank
> >> > ssl_bump peek all
> >> > ssl_bump bump all
> >> > sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> >> > /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
> >> > sslcrtd_children 5
> >> >
> >> >
> >> > When I start squid I don't get any error messages and all pages, http
> >> > and
> >> > https, load properly. The problem is, using the example above, the
> >> > https://www.wellsfargo.com website is still getting bumped,
> evidenced by
> >> > the
> >> > appearance of the ssl website in the web proxy access logs. When I
> don't
> >> > have ssl_bump enabled then no https websites appear in the access
> logs,
> >> > as
> >> > it should be. But, enabling ssl_bump and peek and splice, web sites
> that
> >> > I
> >> > am trying not to bump still seem to be getting bumped.
> >> >
> >> > Any suggestions on how to properly "not bump" certain websites.
> >> >
> >> > Thanks,
> >> >
> >> > Stan
> >> >
> >> > _______________________________________________
> >> > squid-users mailing list
> >> > squid-users at lists.squid-cache.org
> >> > http://lists.squid-cache.org/listinfo/squid-users
> >> >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/c5a72b63/attachment.htm>

From stan.prescott at gmail.com  Thu May  7 02:28:26 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 6 May 2015 21:28:26 -0500
Subject: [squid-users] 3.5.4 need more help with peek and splice and
 external helper
In-Reply-To: <554AB906.3080809@trimble.com>
References: <CANLNtGRgGiBSWbuYAA14t6=SV+Kxh-pvghdq5fgTCyb30DJaPg@mail.gmail.com>
 <554AA1FC.40302@trimble.com>
 <CANLNtGRWeo+bqe8ePWtz+Vpa_XYi+OOExrm7QR6=qYVKfR_Mdg@mail.gmail.com>
 <554AAD5D.4020908@trimble.com>
 <CANLNtGQ+T0Y3VkpAoDhg5Hphon2zqxu9tCafPX7yspYpfvfC4A@mail.gmail.com>
 <554AB906.3080809@trimble.com>
Message-ID: <CANLNtGSCh8vKFUKOLEDtg5j1+mjB+6oLGwUN4_MQ-xduPGes-g@mail.gmail.com>

I am using intercept. It has worked well for me for the ssl-bump so far.

*http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept*
*https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*


I haven't ever tried it without intercept. I will try it and see what
happens.

On Wed, May 6, 2015 at 7:59 PM, Jason Haar <Jason_Haar at trimble.com> wrote:

>  On 07/05/15 12:45, Stanford Prescott wrote:
>
> *1430958788.054   5572 192.168.100.104 TCP_TUNNEL/200 2964 CONNECT
> 172.225.222.201:443 <http://172.225.222.201:443> -
> ORIGINAL_DST/172.225.222.201 <http://172.225.222.201> -*
>
> That smells like transparent/intercept? Is that correct? You have to NOT
> do that until you've got it working via the standard proxy option. It's
> very hard to do SSL intercept transparently
>
> --
> Cheers
>
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150506/2df63320/attachment.htm>

From ambadasvh at teledna.com  Thu May  7 04:59:16 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Thu, 7 May 2015 04:59:16 +0000
Subject: [squid-users]  Client IP spoofing via squid proxy
Message-ID: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>

Hi,

Thanks for replying.

I did a full transparent tproxy setup for squid proxy on linux(RHEL 6) machine as below:

Version: squid-3.5.1
configure options:  '--enable-follow-x-forwarded-for' '--enable-linux-netfilter' --enable-ltdl-convenience

squid.conf:
http_port 3128
http_port 3129 tproxy

Linux Kernel Configuration:
NF_CONNTRACK=m
NETFILTER_TPROXY=m
NETFILTER_XT_MATCH_SOCKET=m
NETFILTER_XT_TARGET_TPROXY=m

Routing configuration:
ip -f inet rule add fwmark 1 lookup 100
ip -f inet route add local default dev eth1 table 100 ip -f inet6 rule add fwmark 1 lookup 100 ip -f inet6 route add local default dev eth1 table 100

echo 1 > /proc/sys/net/ipv4/ip_forward
echo 0 > /proc/sys/net/ipv4/conf/default/rp_filter
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter

iptables Configuration:
iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 1 iptables -t mangle -A DIVERT -j ACCEPT iptables  -t mangle -A PREROUTING -p tcp -m socket -j DIVERT iptables  -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129

The below machines are on local LAN setup Client IP: 172.16.5.110 Client's gateway: 10.0.0.102 DNS Server IP: 172.16.1.7 (same for both client & squid machine) Web server IP: 216.58.196.110 (google.com)

Squid Machine has 2 eth interfaces,
eth1 (facing client): 10.0.0.102
eth0 (connecting to web): 172.16.5.102 

While browsing, the client is getting connection timeout. After analyzing the squid side traces, i found that client is doing DNS (for google.com) & connecting to that DNS IP on 80 port. Squid is able to intercept the request on 3129 port, doing DNS and trying to connect to google.com (using spoofed client IP) but is getting RST packet.
Can you you please tell me what is missing here?

Please find the attached trace.


Regards,
Ambadas


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: 05 May 2015 20:30
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Client IP spoofing via squid proxy

On 6/05/2015 2:25 a.m., Ambadas Hibare wrote:
> Hi,
> 
> I trying to spoof client IP via squid proxy by following  squid's
> TPROXY4 wiki page: http://wiki.squid-cache.org/Features/Tproxy4
> 
> But I want to know whether squid can spoof client IP when we send 
> proxy format HTTP request from Mozilla (ie configuring proxy & port in 
> mozilla). Can squid proxy behave transparently towards only the web 
> server & not the client?

No. It can be both ways, or just towards the client.


> 
> I've tried sending proxy format HTTP request from client to squid box 
> (on 3129  tproxy port), but I am getting Header forgery error Also its 
> trying to connect to itself instead of web server. I am trying to 
> understand why squid is trying to match host header's DNS with the 
> destination IP instead of connecting to host header's DNS (like normal 
> proxy behaviour on port 3128).
> 

To prevent CVE-2009-0801 happening.

You must not send regular forward-proxy traffic to a tproxy or intercept port. Forwarding loops are guaranteed if you do.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Squid102_20150505.pcap
Type: application/octet-stream
Size: 19394 bytes
Desc: Squid102_20150505.pcap
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150507/187a6057/attachment.obj>

From o.calvano at gmail.com  Thu May  7 04:59:57 2015
From: o.calvano at gmail.com (Olivier CALVANO)
Date: Thu, 7 May 2015 06:59:57 +0200
Subject: [squid-users] Best malware/phishing/virus list ?
Message-ID: <CAJajPec7jBgr8XBgYrNpqLQDvth22D4ZRpF41aPCeRkPmFAxHA@mail.gmail.com>

hi

Anyone Know a good list for spam assassin of malware/phishing/virus URL ?

Thanks
Olivier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150507/d47477e8/attachment.htm>

From squid3 at treenet.co.nz  Thu May  7 05:11:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 17:11:57 +1200
Subject: [squid-users] Best malware/phishing/virus list ?
In-Reply-To: <CAJajPec7jBgr8XBgYrNpqLQDvth22D4ZRpF41aPCeRkPmFAxHA@mail.gmail.com>
References: <CAJajPec7jBgr8XBgYrNpqLQDvth22D4ZRpF41aPCeRkPmFAxHA@mail.gmail.com>
Message-ID: <554AF41D.7020006@treenet.co.nz>

On 7/05/2015 4:59 p.m., Olivier CALVANO wrote:
> hi
> 
> Anyone Know a good list for spam assassin of malware/phishing/virus URL ?

Squid is an HTTP software. Not SMTP. If you are trying to use Squid to
control spam problems you got serious trouble ahead.


PS. IMHO, the best anti-spam list is xbl.spamhaus.org. Configure that in
your MTA to cut the bulk of garbage. Complicated (and slow) systems like
Spamassin make a good second line of defence behind that, but not frontline.

Amos



From squid3 at treenet.co.nz  Thu May  7 05:34:31 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 07 May 2015 17:34:31 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
Message-ID: <554AF967.5090406@treenet.co.nz>

On 7/05/2015 4:59 p.m., Ambadas Hibare wrote:
> Hi,
> 
> Thanks for replying.
> 
> I did a full transparent tproxy setup for squid proxy on linux(RHEL 6) machine as below:
> 
> Version: squid-3.5.1
> configure options:  '--enable-follow-x-forwarded-for' '--enable-linux-netfilter' --enable-ltdl-convenience
> 
> squid.conf:
> http_port 3128
> http_port 3129 tproxy
> 
> Linux Kernel Configuration:
> NF_CONNTRACK=m
> NETFILTER_TPROXY=m
> NETFILTER_XT_MATCH_SOCKET=m
> NETFILTER_XT_TARGET_TPROXY=m
> 
> Routing configuration:
> ip -f inet rule add fwmark 1 lookup 100
> ip -f inet route add local default dev eth1 table 100 ip -f inet6 rule add fwmark 1 lookup 100 ip -f inet6 route add local default dev eth1 table 100
> 
> echo 1 > /proc/sys/net/ipv4/ip_forward
> echo 0 > /proc/sys/net/ipv4/conf/default/rp_filter
> echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
> echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter
> 
> iptables Configuration:
> iptables -t mangle -N DIVERT
> iptables -t mangle -A DIVERT -j MARK --set-mark 1 iptables -t mangle -A DIVERT -j ACCEPT iptables  -t mangle -A PREROUTING -p tcp -m socket -j DIVERT iptables  -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129
> 
> The below machines are on local LAN setup Client IP: 172.16.5.110 Client's gateway: 10.0.0.102 DNS Server IP: 172.16.1.7 (same for both client & squid machine) Web server IP: 216.58.196.110 (google.com)
> 
> Squid Machine has 2 eth interfaces,
> eth1 (facing client): 10.0.0.102
> eth0 (connecting to web): 172.16.5.102 
> 

And MAC addresses please? TPROXY mixes up all the IPs.


> While browsing, the client is getting connection timeout. After analyzing the squid side traces, i found that client is doing DNS (for google.com) & connecting to that DNS IP on 80 port. Squid is able to intercept the request on 3129 port, doing DNS and trying to connect to google.com (using spoofed client IP) but is getting RST packet.
> Can you you please tell me what is missing here?

Your "ip route" rules use eth1, but your rp_filter settings only change
eth0. Also your iptables rules do not distinguish by ethN.

So its possible that rp_filter is still affecting traffic on eth1 trying
to be TPROXY'd.

Also its possible the eth0 traffic being TPROXY'd is not finding a
usable route table entry.

Your trace shows the MAC address *:c4 contacting Squid (MAC address
*:e4) and delivering an HTTP request. Squid (*:e4) then contacts the
remote server be sending a TCP SYN packet ... which the MAC address *:c4
rejects.


Some possibilities about what is actually going on:

1) Squid SYN packet gets to server. The server SYN ACK gets routed to
client, which rejects with RST. The RST gets routed to Squid.

2) Squid SYN packet hits rp_filter protection which RST. (If *:c4 is a
NIC on the Squid box).

3) Squid SYN packet sent out wrong ethN interface (towards client) and
the router that way rejects the SYN with RST, since it knows routing bak
to Squid is invalid.

Amos


From ambadasvh at teledna.com  Thu May  7 06:09:34 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Thu, 7 May 2015 06:09:34 +0000
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <554AF967.5090406@treenet.co.nz>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
Message-ID: <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>

HI,

Client IP: 172.16.5.110
Client Mac: 00:23:7D:E8:AC:C4

Squid Box:

eth0 IP: 172.16.5.102
eth0 Mac: 18:A9:05:3C:12:E4

eth1 IP: 10.0.0.102
eth1 Mac: 18:A9:05:3C:12:E6

> "Your "ip route" rules use eth1, but your rp_filter settings only change eth0. Also your iptables rules do not distinguish by ethN."

Yes. Should that setting be applied on both eths' or only the one facing the client?
Also want to know if it's possible to do tproxy setup with just one eth at squid box?

> "Your trace shows the MAC address *:c4 contacting Squid (MAC address *:e4) and delivering an HTTP request. Squid (*:e4) then contacts the remote server be sending > a TCP SYN packet ... which the MAC address *:c4 rejects."

In trace it shows squid (*:e4) (packet# 83) is contacting the web server (google.com) via client IP (172.16.5.110). So it's getting spoofed!? But not able to understand why client is sending RST to google (packet# 84) just after that & response

PS. The default gateway for client is squid box IP (eth1). 

Regards,
Ambadas


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: 07 May 2015 11:05
To: Ambadas Hibare; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Client IP spoofing via squid proxy

On 7/05/2015 4:59 p.m., Ambadas Hibare wrote:
> Hi,
> 
> Thanks for replying.
> 
> I did a full transparent tproxy setup for squid proxy on linux(RHEL 6) machine as below:
> 
> Version: squid-3.5.1
> configure options:  '--enable-follow-x-forwarded-for' 
> '--enable-linux-netfilter' --enable-ltdl-convenience
> 
> squid.conf:
> http_port 3128
> http_port 3129 tproxy
> 
> Linux Kernel Configuration:
> NF_CONNTRACK=m
> NETFILTER_TPROXY=m
> NETFILTER_XT_MATCH_SOCKET=m
> NETFILTER_XT_TARGET_TPROXY=m
> 
> Routing configuration:
> ip -f inet rule add fwmark 1 lookup 100 ip -f inet route add local 
> default dev eth1 table 100 ip -f inet6 rule add fwmark 1 lookup 100 ip 
> -f inet6 route add local default dev eth1 table 100
> 
> echo 1 > /proc/sys/net/ipv4/ip_forward echo 0 > 
> /proc/sys/net/ipv4/conf/default/rp_filter
> echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
> echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter
> 
> iptables Configuration:
> iptables -t mangle -N DIVERT
> iptables -t mangle -A DIVERT -j MARK --set-mark 1 iptables -t mangle 
> -A DIVERT -j ACCEPT iptables  -t mangle -A PREROUTING -p tcp -m socket 
> -j DIVERT iptables  -t mangle -A PREROUTING -p tcp --dport 80 -j 
> TPROXY --tproxy-mark 0x1/0x1 --on-port 3129
> 
> The below machines are on local LAN setup Client IP: 172.16.5.110 
> Client's gateway: 10.0.0.102 DNS Server IP: 172.16.1.7 (same for both 
> client & squid machine) Web server IP: 216.58.196.110 (google.com)
> 
> Squid Machine has 2 eth interfaces,
> eth1 (facing client): 10.0.0.102
> eth0 (connecting to web): 172.16.5.102
> 

And MAC addresses please? TPROXY mixes up all the IPs.


> While browsing, the client is getting connection timeout. After analyzing the squid side traces, i found that client is doing DNS (for google.com) & connecting to that DNS IP on 80 port. Squid is able to intercept the request on 3129 port, doing DNS and trying to connect to google.com (using spoofed client IP) but is getting RST packet.
> Can you you please tell me what is missing here?

Your "ip route" rules use eth1, but your rp_filter settings only change
eth0. Also your iptables rules do not distinguish by ethN.

So its possible that rp_filter is still affecting traffic on eth1 trying
to be TPROXY'd.

Also its possible the eth0 traffic being TPROXY'd is not finding a
usable route table entry.

Your trace shows the MAC address *:c4 contacting Squid (MAC address
*:e4) and delivering an HTTP request. Squid (*:e4) then contacts the
remote server be sending a TCP SYN packet ... which the MAC address *:c4
rejects.


Some possibilities about what is actually going on:

1) Squid SYN packet gets to server. The server SYN ACK gets routed to
client, which rejects with RST. The RST gets routed to Squid.

2) Squid SYN packet hits rp_filter protection which RST. (If *:c4 is a
NIC on the Squid box).

3) Squid SYN packet sent out wrong ethN interface (towards client) and
the router that way rejects the SYN with RST, since it knows routing bak
to Squid is invalid.

Amos

From vkukk at xvidservices.com  Thu May  7 11:16:20 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Thu, 07 May 2015 14:16:20 +0300
Subject: [squid-users] Different replacement policy for different filenames
Message-ID: <554B4984.7000703@xvidservices.com>

Hi

I'd like to apply different replacement policy for different filenames. 
I have searched through the documentation and found that replacement 
policy can be defined only per cache_dir[s] and squid can sort into 
different cache_dir's based on file size, but not based on file name. 
Maybe I have missed some of the documentation, so that is why I'm asking 
here.

Is it possible to sort into cache_dir's based on filename (some regex 
will do)? The result I'm trying to achieve is that certain files which 
names are known, but size varies (and overlaps with other files) are 
replaced with heap GDSF while all the other files are replaced with heal 
LFUDA policy.

Can this be done with squid?

Best regards,
Veiko


From squid3 at treenet.co.nz  Thu May  7 12:16:01 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 08 May 2015 00:16:01 +1200
Subject: [squid-users] Different replacement policy for different
	filenames
In-Reply-To: <554B4984.7000703@xvidservices.com>
References: <554B4984.7000703@xvidservices.com>
Message-ID: <554B5781.8010705@treenet.co.nz>

On 7/05/2015 11:16 p.m., Veiko Kukk wrote:
> Hi
> 
> I'd like to apply different replacement policy for different filenames.
> I have searched through the documentation and found that replacement
> policy can be defined only per cache_dir[s] and squid can sort into
> different cache_dir's based on file size, but not based on file name.
> Maybe I have missed some of the documentation, so that is why I'm asking
> here.
> 
> Is it possible to sort into cache_dir's based on filename (some regex
> will do)? The result I'm trying to achieve is that certain files which
> names are known, but size varies (and overlaps with other files) are
> replaced with heap GDSF while all the other files are replaced with heal
> LFUDA policy.
> 
> Can this be done with squid?

No. Replacement policy is a type of hash table for a cache_dir such that
the single entry that is most obsolete can efficiently be found and
discarded at the time point where cache runs out of space. The whole
data structure for the cache_dir index is different for each mechanism.

Why are you wanting this? What are you actually trying to do?

Amos



From squid3 at treenet.co.nz  Thu May  7 12:37:39 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 08 May 2015 00:37:39 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
Message-ID: <554B5C93.3070304@treenet.co.nz>

On 7/05/2015 6:09 p.m., Ambadas Hibare wrote:
> HI,
> 
> Client IP: 172.16.5.110
> Client Mac: 00:23:7D:E8:AC:C4
> 
> Squid Box:
> 
> eth0 IP: 172.16.5.102
> eth0 Mac: 18:A9:05:3C:12:E4
> 
> eth1 IP: 10.0.0.102
> eth1 Mac: 18:A9:05:3C:12:E6
> 
>> "Your "ip route" rules use eth1, but your rp_filter settings only change eth0. Also your iptables rules do not distinguish by ethN."
> 
> Yes. Should that setting be applied on both eths' or only the one facing the client?

The one facing the *server* at minimum. Doing it on both wont hurt for
experimenting. But when this is working try setting the client-facing
NIC off again.


> Also want to know if it's possible to do tproxy setup with just one eth at squid box?

Of course. You just have to configure the packet routing explicitly on
the router the Squid box is connected to as well as the Squid box
itself. To prevent server responses (SYN ACK etc) being sent to the
client when they should go to Squid.

> 

>> "Your trace shows the MAC address *:c4 contacting Squid (MAC
>> address
*:e4) and delivering an HTTP request. Squid (*:e4) then contacts the
remote server be sending > a TCP SYN packet ... which the MAC address
*:c4 rejects."
> 
> In trace it shows squid (*:e4) (packet# 83) is contacting the web
server (google.com) via client IP (172.16.5.110). So it's getting
spoofed!? But not able to understand why client is sending RST to google
(packet# 84) just after that & response


Because one of the SYN (from Squid) or SYN-ACK packet (reply from
server) is arriving at the client when it should have been delivered
elsewhere.


the packets doing this:
 client -----> Squid -SYN-> server
 client <-------------ACK-- server
 client -RST-> Squid

or this:
 client -----> Squid -SYN-\
 client <-----------------/
 client -RST-> Squid


> PS. The default gateway for client is squid box IP (eth1). 

The part routing traffic from client<->Squid is working. The part
Squid<->server is going wrong.

Amos


From vkukk at xvidservices.com  Thu May  7 13:54:56 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Thu, 07 May 2015 16:54:56 +0300
Subject: [squid-users] Different replacement policy for different
	filenames
In-Reply-To: <554B5781.8010705@treenet.co.nz>
References: <554B4984.7000703@xvidservices.com>
 <554B5781.8010705@treenet.co.nz>
Message-ID: <554B6EB0.4060802@xvidservices.com>


On 07/05/15 15:16, Amos Jeffries wrote:
> On 7/05/2015 11:16 p.m., Veiko Kukk wrote:
>> Is it possible to sort into cache_dir's based on filename (some regex
>> will do)? The result I'm trying to achieve is that certain files which
>> names are known, but size varies (and overlaps with other files) are
>> replaced with heap GDSF while all the other files are replaced with heal
>> LFUDA policy.
>>
>> Can this be done with squid?
>
> No. Replacement policy is a type of hash table for a cache_dir such that
> the single entry that is most obsolete can efficiently be found and
> discarded at the time point where cache runs out of space. The whole
> data structure for the cache_dir index is different for each mechanism.
>
> Why are you wanting this? What are you actually trying to do?
>
> Amos

Hi,

I'm trying to have different cache replacement policy for different 
files, based on filename. To avoid bigger files pushing some smaller 
files (more important files) out from cache in case of LFUDA and at the 
same time have high byte hit rate for bigger files. I could achieve this 
if squid could be configured to sort files based on filename/url into 
different cache folders.

Best regards,
Veiko


From yvoinov at gmail.com  Thu May  7 14:33:50 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 07 May 2015 20:33:50 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1430777255139-4671103.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com>
Message-ID: <554B77CE.8060402@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
For 3.4.x series need patch. Correct patch. This copy-n-pasted is broken.

Also, you have forgotten one thing: YT redirector has text/plain mime
type, not text/html. Just trace your YT session and check every exchange
between client and server.

In general: We can get very simple and reliable solution, if we can
focuse rep_mime_type acl by single domain. That's all we need.

05.05.15 4:07, HackXBack ?????:
> store_miss deny http302

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVS3fOAAoJENNXIZxhPexGOQsH/j+CZzrSF1kHeJ0gFzrhH3D+
YsfBIHVdH+89GQcgqhHulnLMf5RxscHaCt318BNvZpk1eOLlimxGKw/AYXRekZ5c
uDM05HNXcykj1sHYlepIcl+5aDrNIHs4aW8lhf/uo+wc8YSxxTo9JhSMde7q0H81
xz7p9dGm+mQ/JzzkCqDevynMojb3Qe1+DNWeqtbti2JaxBhzxN/vYfsFk+Gm8ImT
bto/Z2C9moas7cFrIaGnmD0sbOJsu4OOXW1bru4Ne7ux2nbH2OtiYl6cN1GnUxq7
0g/0gtTfwWKy0YBzMbrt0Bu4pZ1rXKYV1rbbiyR5hdmojHL32Mmez/CqRoG9lPs=
=EPjy
-----END PGP SIGNATURE-----



From stan.prescott at gmail.com  Thu May  7 17:40:20 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Thu, 7 May 2015 12:40:20 -0500
Subject: [squid-users] squid 3.5.3 can't get peek and splice to not bump
 certain sites
In-Reply-To: <CANLNtGQ=Z11aVw1TrjLqZ-LPj6O-L1tPNxNeba-EfWDmAphdQw@mail.gmail.com>
References: <CANLNtGSRTu3zHwYkX30ROb3=_FuZ_35awk44m95EQuS30rz9sQ@mail.gmail.com>
 <CAGUJm7bWQq0ttRbk4jesL-PLnGuDxfVeeoC2dv6ZjzbD+Cx9JA@mail.gmail.com>
 <CANLNtGRGpzc6d495x-H=v=m_zDOqHyHLg2Kd=VoUd4S77SKxZg@mail.gmail.com>
 <CAGUJm7YdfW3NepcZ2NDakYBML7onZ3dYMoFeUv0mkWf_7NPaTA@mail.gmail.com>
 <CANLNtGQ=Z11aVw1TrjLqZ-LPj6O-L1tPNxNeba-EfWDmAphdQw@mail.gmail.com>
Message-ID: <CANLNtGRkYdZETi8zdKnJBDtowNn_v2pJ8fF4up+c69sHX8RFuA@mail.gmail.com>

I've tried using the new server_name acl you provided an example of and
Jason's suggestions for getting the external acl and bumphelper script
working but It only results in everything still being bumped. The website
I'm trying to use as a test for non-bumping, wellsfargo.com, still gets
bumped along with everything else.

Could it be an issue with using the website domain name and the scripts are
not recognizing the website's SNI info as a match to not be bumped?

On Wed, May 6, 2015 at 9:24 PM, Stanford Prescott <stan.prescott at gmail.com>
wrote:

> Jason helped me a lot although I am still having trouble getting that
> helper working. It got to the point that only the website I didn't want
> bumped was getting bumped because I had my logic switched in the helper
> script to nothing getting bumped at all. Jason pointed out that I appear to
> be using transparent intercept proxy and that I shouldn't do that until I
> have everything worked out with the regular proxy since bumping is
> difficult to do with transparent proxy.
>
> I have been using transparent proxy (intercept) with the https_port
> declaration with great success so far as seen here
>
> *http_port 192.168.100.1:800 <http://192.168.100.1:800> intercept*
> *https_port 192.168.100.1:808 <http://192.168.100.1:808> intercept
> ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*
>
>
> For peek and splice should I not be using intercept?
>
> I will give your suggestions a try and get back to you.
>
> Also, I apologize for inadvertently not including our correspondence in
> the squid-users list. I was just clicking the reply button not realizing it
> wasn't a "reply all". I think I've got it figured out how to include the
> list in replies.
>
> Thanks Nathan.
>
> Stan
>
> On Wed, May 6, 2015 at 8:15 PM, Nathan Hoad <nathan at getoffmalawn.com>
> wrote:
>
>> Hi Stan,
>>
>> Yep, I think the server_name acl in 3.5.4 should provide what you want
>> without the need for an external acl now. I haven't used it as the
>> external acl fits my usecase. I imagine doing something like this
>> should work for server_name though...
>>
>> acl sni_exclusions ssl::server_name wellsfargo.com
>> acl tcp_level at_step SslBump1
>> acl client_hello_peeked at_step SslBump2
>>
>> ssl_bump peek tcp_level all
>> ssl_bump splice client_hello_peeked sni_exclusions
>> ssl_bump bump all
>>
>> Hopefully your other issue with your perl helper hanging has been
>> resolved by Jason's recommendation! Though if this does what you want,
>> you may not need the helper.
>>
>> Also, please try to keep messages to the mailing list - this is all
>> information that will help others :)
>>
>> Thanks,
>>
>> Nathan.
>>
>> On 5 May 2015 at 13:20, Stanford Prescott <stan.prescott at gmail.com>
>> wrote:
>> > Hi Nathan. I have decided to try to go ahead and try to get peek and
>> splice
>> > working for Squid on the Smoothwall Express firewall distro since we
>> will
>> > not be able to migrate to Squid 4.x when it debuts. You previously
>> kindly
>> > offered an example of a squid.conf setup for me to try to get it
>> working.
>> >
>> > external_acl_type sni ttl=30 concurrency=X children-max=Y
>> > children-startup=Z %ssl::>sni /path/to/your/helper
>> >
>> > acl sni_exclusions external sni
>> > acl tcp_level at_step SslBump1
>> > acl client_hello_peeked at_step SslBump2
>> >
>> > ssl_bump peek tcp_level all
>> > ssl_bump splice client_hello_peeked sni_exclusions
>> > ssl_bump bump all
>> >
>> > Amos says he has back ported a server_name acl with the 3.5.4 release.
>> Does
>> > this now mean that the "external_acl_type" is no longer needed for this
>> sort
>> > of function? Specifically, I want to be able to allow my users to enter
>> > websites that they do not want bumped, like banking websites. I wasn't
>> able
>> > to get the squid.conf and helper script example you provided to work
>> for me.
>> >
>> > Does the new server_name acl change how this can be done? Would you be
>> able
>> > to provide a new example for me to try based on this new acl in squid
>> 3.5.4?
>> >
>> > Any help is greatly appreciated.
>> >
>> > Stan
>> >
>> >
>> > On Sun, Apr 12, 2015 at 7:25 PM, Nathan Hoad <nathan at getoffmalawn.com>
>> > wrote:
>> >>
>> >> Hi Stan,
>> >>
>> >> For peek and splice, you need to decide based on the SNI name, not the
>> >> domain name, which for 3.5 means you need to use an external ACL
>> >> helper that processes %ssl::>sni. For 4.0 there will be a server_name
>> >> ACL you can use instead.
>> >>
>> >> On top of that, you also need to make sure this external ACL helper
>> >> runs at the correct "bump step", with the at_step ACL, e.g...
>> >>
>> >> external_acl_type sni ttl=30 concurrency=X children-max=Y
>> >> children-startup=Z %ssl::>sni /path/to/your/helper
>> >>
>> >> acl sni_exclusions external sni
>> >> acl tcp_level at_step SslBump1
>> >> acl client_hello_peeked at_step SslBump2
>> >>
>> >> ssl_bump peek tcp_level all
>> >> ssl_bump splice client_hello_peeked sni_exclusions
>> >> ssl_bump bump all
>> >>
>> >> Hope that helps,
>> >>
>> >> Nathan.
>> >>
>> >> On 13 April 2015 at 04:12, Stanford Prescott <stan.prescott at gmail.com>
>> >> wrote:
>> >> > I would like to give my users the ability to "not bump" certain
>> sites. I
>> >> > tried to use the examples given on the SSLPeekandSplice wiki page but
>> >> > can't
>> >> > get it to work.
>> >> >
>> >> > This is a snippet of my squid.conf file.
>> >> >
>> >> > https_port 192.168.10.1:808 intercept ssl-bump
>> >> > generate-host-certificates=on
>> >> > dynamic_cert_mem_cache_size=4MB
>> >> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >> >
>> >> > http_port 192.168.20.1:800 intercept
>> >> > https_port 192.168.20.1:808 intercept ssl-bump
>> >> > generate-host-certificates=on
>> >> > dynamic_cert_mem_cache_size=4MB
>> >> > cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem
>> >> >
>> >> > http_port 127.0.0.1:800 intercept
>> >> >
>> >> > sslproxy_cert_error allow all
>> >> > sslproxy_flags DONT_VERIFY_PEER
>> >> > sslproxy_session_cache_size 4 MB
>> >> >
>> >> > acl serverIsBank dstdomain wellsfargo.com
>> >> >
>> >> > ssl_bump server-first all
>> >> >
>> >> > ssl_bump none localhostgreen
>> >> > ssl_bump none localhostpurple
>> >> >
>> >> > ssl_bump splice serverIsBank
>> >> > ssl_bump peek all
>> >> > ssl_bump bump all
>> >> > sslcrtd_program /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
>> >> > /var/smoothwall/mods/proxy/lib/ssl_db -M 4MB
>> >> > sslcrtd_children 5
>> >> >
>> >> >
>> >> > When I start squid I don't get any error messages and all pages, http
>> >> > and
>> >> > https, load properly. The problem is, using the example above, the
>> >> > https://www.wellsfargo.com website is still getting bumped,
>> evidenced by
>> >> > the
>> >> > appearance of the ssl website in the web proxy access logs. When I
>> don't
>> >> > have ssl_bump enabled then no https websites appear in the access
>> logs,
>> >> > as
>> >> > it should be. But, enabling ssl_bump and peek and splice, web sites
>> that
>> >> > I
>> >> > am trying not to bump still seem to be getting bumped.
>> >> >
>> >> > Any suggestions on how to properly "not bump" certain websites.
>> >> >
>> >> > Thanks,
>> >> >
>> >> > Stan
>> >> >
>> >> > _______________________________________________
>> >> > squid-users mailing list
>> >> > squid-users at lists.squid-cache.org
>> >> > http://lists.squid-cache.org/listinfo/squid-users
>> >> >
>> >
>> >
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150507/92906310/attachment.htm>

From squidcache at mindchasers.com  Thu May  7 18:52:36 2015
From: squidcache at mindchasers.com (Bob Cochran)
Date: Thu, 07 May 2015 14:52:36 -0400
Subject: [squid-users] Best solution for content filtering using squid?
Message-ID: <554BB474.7020405@mindchasers.com>

Hi,

What is the best solution with squid for content filtering using lists 
of domains that should be blocked?

We have been using squidGuard, and it works.  However, we would like to 
know if there is a better solution out there.

We also tried using squid as the content filter (acl ban lists) but 
found the performance was poor when we allowed the list to grow to 
thousands of blocked domains.

Thank you,

Bob



From yvoinov at gmail.com  Thu May  7 18:54:56 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 08 May 2015 00:54:56 +0600
Subject: [squid-users] Best solution for content filtering using squid?
In-Reply-To: <554BB474.7020405@mindchasers.com>
References: <554BB474.7020405@mindchasers.com>
Message-ID: <554BB500.9050900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
ufdbguard

08.05.15 0:52, Bob Cochran ?????:
> Hi,
>
> What is the best solution with squid for content filtering using lists
of domains that should be blocked?
>
> We have been using squidGuard, and it works.  However, we would like
to know if there is a better solution out there.
>
> We also tried using squid as the content filter (acl ban lists) but
found the performance was poor when we allowed the list to grow to
thousands of blocked domains.
>
> Thank you,
>
> Bob
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVS7UAAAoJENNXIZxhPexGiukIAMMgbhu2Ejw0bXqv6GEKUg05
wl8GXnsbSCWs2OnG9vlrJzevyehzcSKmFG9duRotH/0i1RUYLmvzZGtdHiggqfaO
/Z55s0K/1iAkUzj50ZKmQpbf3gOzZze1MYiKStWE0sFL9GgAbkeKRqH+aMqikQGn
+6VXZZv1oPdpoR9816C6AJxohV+xGBtB9WG3egds/qJx+UoAbRYJklGNN4ECuW5L
EBJUHV8bubW6T/ERMPKToe+3UrW4u11oKwu1MM5Iayk5P0HxwdzkooZnpSFvaSHI
o+8ncrMF7tG0ZcNQn17Mr8ke2J10WvBfWswhC4bQ3uA3pPe8ohFu2PZ5ooaybQM=
=rPyB
-----END PGP SIGNATURE-----



From marcus.kool at urlfilterdb.com  Thu May  7 18:57:03 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 07 May 2015 15:57:03 -0300
Subject: [squid-users] Best solution for content filtering using squid?
In-Reply-To: <554BB474.7020405@mindchasers.com>
References: <554BB474.7020405@mindchasers.com>
Message-ID: <554BB57F.9060607@urlfilterdb.com>

Hi,

What is it that makes you want to go to a better solution ?

did you look at ufdbGuard?

Marcus

PS: Beware! I am biased since I wrote ufdbGuard.


On 05/07/2015 03:52 PM, Bob Cochran wrote:
> Hi,
>
> What is the best solution with squid for content filtering using lists of domains that should be blocked?
>
> We have been using squidGuard, and it works.  However, we would like to know if there is a better solution out there.
>
> We also tried using squid as the content filter (acl ban lists) but found the performance was poor when we allowed the list to grow to thousands of blocked domains.
>
> Thank you,
>
> Bob
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squidcache at mindchasers.com  Thu May  7 19:49:13 2015
From: squidcache at mindchasers.com (Bob Cochran)
Date: Thu, 07 May 2015 15:49:13 -0400
Subject: [squid-users] Best solution for content filtering using squid?
In-Reply-To: <554BB57F.9060607@urlfilterdb.com>
References: <554BB474.7020405@mindchasers.com>
 <554BB57F.9060607@urlfilterdb.com>
Message-ID: <554BC1B9.8090409@mindchasers.com>

On 05/07/2015 02:57 PM, Marcus Kool wrote:
> did you look at ufdbGuard? 

Thank you.  I did look at it briefly and moved on when I saw that a paid 
license was required if a commercial product made use of it. Perhaps I'm 
wrong about this?

We're looking for something that can be extended beyond what squidGuard 
offers and has an active developer community.  We want to do things like 
modify the database on the fly, switch on & off various blocks at will, 
and much more without having to reconfigure / restart.

Bob



From marcus.kool at urlfilterdb.com  Thu May  7 20:10:45 2015
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 07 May 2015 17:10:45 -0300
Subject: [squid-users] Best solution for content filtering using squid?
In-Reply-To: <554BC1B9.8090409@mindchasers.com>
References: <554BB474.7020405@mindchasers.com>
 <554BB57F.9060607@urlfilterdb.com> <554BC1B9.8090409@mindchasers.com>
Message-ID: <554BC6C5.6060503@urlfilterdb.com>



On 05/07/2015 04:49 PM, Bob Cochran wrote:
> On 05/07/2015 02:57 PM, Marcus Kool wrote:
>> did you look at ufdbGuard?
>
> Thank you.  I did look at it briefly and moved on when I saw that a paid license was required if a commercial product made use of it. Perhaps I'm wrong about this?

I would like to know where you read that...

The FAQ says that it is free.  ufdbGuard is Open Source Software and free to use.
You can use any text-based database _or_ opt for a license to the URL database of www.URLfilterDB.com

> We're looking for something that can be extended beyond what squidGuard offers and has an active developer community.  We want to do things like modify the database on the fly, switch on & off various
> blocks at will, and much more without having to reconfigure / restart.

seems you are looking for ufdbguard.
About "no reconfigure": ufdbGuard also needs to reconfigure when the config file or the URL database changes.
However, ufdbGuard reconfigures faster and without interrupting the proxy, so during a short interval
there is a functional proxy without filter.

Marcus

> Bob



From rafael.akchurin at diladele.com  Thu May  7 20:56:37 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 7 May 2015 20:56:37 +0000
Subject: [squid-users] Best solution for content filtering using squid?
In-Reply-To: <554BB474.7020405@mindchasers.com>
References: <554BB474.7020405@mindchasers.com>
Message-ID: <DB5PR04MB1128CA26091278078A7C00FA8FDF0@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hello Bob,

If you do not mind taking a look at third party web filtering products integrating with Squid using for example ICAP them I would humbly recommend taking a look at qlproxy (quintolabs.com). 

It is a commercial product though not sure if this fits your expectations.

Best regards,
Rafael

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Bob Cochran
Sent: Thursday, May 7, 2015 8:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Best solution for content filtering using squid?

Hi,

What is the best solution with squid for content filtering using lists of domains that should be blocked?

We have been using squidGuard, and it works.  However, we would like to know if there is a better solution out there.

We also tried using squid as the content filter (acl ban lists) but found the performance was poor when we allowed the list to grow to thousands of blocked domains.

Thank you,

Bob

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From hack.back at hotmail.com  Thu May  7 20:59:09 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 7 May 2015 13:59:09 -0700 (PDT)
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <554B77CE.8060402@gmail.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
Message-ID: <1431032349609-4671177.post@n4.nabble.com>

for me this patch work, 
but did you find this simple solution ?
btw this loop is not new i use this patch more than 1 year



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671177.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu May  7 21:17:55 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 08 May 2015 03:17:55 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1431032349609-4671177.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
 <1431032349609-4671177.post@n4.nabble.com>
Message-ID: <554BD683.7050708@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
I think, this loop is changed in YT during last year.

HTML5 was winished since 2015. YT URL scheme was chagnged this year. So,
text/html is not valid for prevention looping. I see text/plain
redirector in YT exchange.

08.05.15 2:59, HackXBack ?????:
> for me this patch work, 
> but did you find this simple solution ?
> btw this loop is not new i use this patch more than 1 year
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671177.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVS9aDAAoJENNXIZxhPexG3qgH/1MTrlqDfj7/RKklcpewbDQ4
voOOVqsqfyv/8muLDRcV5l0mtXCWSb4pCoZADRCnHfhTXIkvQAmblTLUuS9xMrWb
JYgqiPAKV7xJ86rEL5PyldGGphxMjCnkuTzqYGREEUyDfn3tCh7qGbtWvVdwjBBP
kYprY2v7ehaa0tZ39UEwvYX3Vc+meyMO4hBMlRuoIasQDPXk5+sIkvtAbSi4rBYP
/m1/lwbJU7ADYOAuPfMTCZJCiEXQFriDITBAcrn8J/gm/ARkRU9VXJf1HPZFAvkC
CfQHY/M2OuI2+5TWCf2nu556+Ct8/oMQ0BlFrhcEnGKC/PCpQEkpW7BxljibAIs=
=gceU
-----END PGP SIGNATURE-----



From hack.back at hotmail.com  Thu May  7 21:25:05 2015
From: hack.back at hotmail.com (HackXBack)
Date: Thu, 7 May 2015 14:25:05 -0700 (PDT)
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <554BD683.7050708@gmail.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
 <1431032349609-4671177.post@n4.nabble.com> <554BD683.7050708@gmail.com>
Message-ID: <1431033905569-4671179.post@n4.nabble.com>

you are right, but this patch still work with me.
i dont know if we can find better solution for this like you said by acl



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671179.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu May  7 21:54:33 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 08 May 2015 03:54:33 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1431033905569-4671179.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
 <1431032349609-4671177.post@n4.nabble.com> <554BD683.7050708@gmail.com>
 <1431033905569-4671179.post@n4.nabble.com>
Message-ID: <554BDF19.9000703@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Feature with acl will be useful. Not only YT uses this redirection scheme.

08.05.15 3:25, HackXBack ?????:
> you are right, but this patch still work with me.
> i dont know if we can find better solution for this like you said by acl
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671179.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVS98ZAAoJENNXIZxhPexG1skIAJBrsJKJryX7HJMqpMjD6zDk
sGuEyVZc2jbfu86wssdbe8WeEydKMGVmYuJ3QeQO7D/xHoIL647z4AriaEeDNKgK
o4gPq6VqA6PwU9jLxCT0/HoJqXjnjYNy7aAazqF1at8pp/RtDhePe+8u59qO4U54
U1lWwL/OvpKDm63LsKU2EjHOdZOvm3QH+d8vz6rWVto+ZO5/omI0vKR+DT9iqEwI
cQmtbuQ4KMEa2rmTq9So5Ih0ZI+n+DZ9sWSw7XOWguw/AFS1Bp24RXALXGiQ/oGJ
9iLXYO1fcDdnxM8XaJNvrqrCWxXWp0HkbiPbQ24d9Y729wXmApZuM9B/qD5IbDo=
=/tGd
-----END PGP SIGNATURE-----



From dm at belkam.com  Fri May  8 05:23:04 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 08 May 2015 09:23:04 +0400
Subject: [squid-users] squid 3.5.4 dies
Message-ID: <554C4838.9040808@belkam.com>

Hello!

Several days ago I updated my self-compiled squid 3.4 to 3.5.4,
server is ubuntu 12.04 x86-64.
All I see in logs is
2015/05/08 09:13:47 kid1| local=192.168.42.130:32785 
remote=74.125.205.132:443 FD 808 flags=1: read/write failure: (32) 
Broken pipe
2015/05/08 09:14:55 kid1| Set Current Directory to /var/spool/squid3
2015/05/08 09:14:55 kid1| Starting Squid Cache version 3.5.4 for 
x86_64-unknown-linux-gnu...
2015/05/08 09:14:55 kid1| Service Name: squid
2015/05/08 09:14:55 kid1| Process ID 16211
2015/05/08 09:14:55 kid1| Process Roles: worker
2015/05/08 09:14:55 kid1| With 32768 file descriptors available
2015/05/08 09:14:55 kid1| Initializing IP Cache...

I had 3.5.4 on test server , which is hardware and OS identical, but , 
definitely, has lower load,
it worked with 3.5 since it was released and no such problem.
Could you tell me how can I find where problem  is?

Thank you!



From squid3 at treenet.co.nz  Fri May  8 05:45:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 08 May 2015 17:45:34 +1200
Subject: [squid-users] Different replacement policy for different
	filenames
In-Reply-To: <554B6EB0.4060802@xvidservices.com>
References: <554B4984.7000703@xvidservices.com>
 <554B5781.8010705@treenet.co.nz> <554B6EB0.4060802@xvidservices.com>
Message-ID: <554C4D7E.7080001@treenet.co.nz>

On 8/05/2015 1:54 a.m., Veiko Kukk wrote:
> 
> On 07/05/15 15:16, Amos Jeffries wrote:
>> On 7/05/2015 11:16 p.m., Veiko Kukk wrote:
>>> Is it possible to sort into cache_dir's based on filename (some regex
>>> will do)? The result I'm trying to achieve is that certain files which
>>> names are known, but size varies (and overlaps with other files) are
>>> replaced with heap GDSF while all the other files are replaced with heal
>>> LFUDA policy.
>>>
>>> Can this be done with squid?
>>
>> No. Replacement policy is a type of hash table for a cache_dir such that
>> the single entry that is most obsolete can efficiently be found and
>> discarded at the time point where cache runs out of space. The whole
>> data structure for the cache_dir index is different for each mechanism.
>>
>> Why are you wanting this? What are you actually trying to do?
>>
>> Amos
> 
> Hi,
> 
> I'm trying to have different cache replacement policy for different
> files, based on filename. To avoid bigger files pushing some smaller
> files (more important files) out from cache in case of LFUDA and at the
> same time have high byte hit rate for bigger files. I could achieve this
> if squid could be configured to sort files based on filename/url into
> different cache folders.

There you go. Your problem statement is phrased in terms of sizes - "big
files", "little files".

cache_dir sectioned with min-size / max-size options using different
replacement policy in each will do as close to what you need as is possible.


PS. One can;t use "filename" because there is no such thing as "file" in
HTTP. Only messages and objects with URLs. Any given URL can present a
wide range of different sized objects. Any semblance to "files" is an
illusion.


Amos



From squid3 at treenet.co.nz  Fri May  8 06:14:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 08 May 2015 18:14:17 +1200
Subject: [squid-users] squid 3.5.4 dies
In-Reply-To: <554C4838.9040808@belkam.com>
References: <554C4838.9040808@belkam.com>
Message-ID: <554C5439.6090702@treenet.co.nz>

On 8/05/2015 5:23 p.m., Dmitry Melekhov wrote:
> Hello!
> 
> Several days ago I updated my self-compiled squid 3.4 to 3.5.4,
> server is ubuntu 12.04 x86-64.
> All I see in logs is
> 2015/05/08 09:13:47 kid1| local=192.168.42.130:32785
> remote=74.125.205.132:443 FD 808 flags=1: read/write failure: (32)
> Broken pipe
> 2015/05/08 09:14:55 kid1| Set Current Directory to /var/spool/squid3
> 2015/05/08 09:14:55 kid1| Starting Squid Cache version 3.5.4 for
> x86_64-unknown-linux-gnu...
> 2015/05/08 09:14:55 kid1| Service Name: squid
> 2015/05/08 09:14:55 kid1| Process ID 16211
> 2015/05/08 09:14:55 kid1| Process Roles: worker
> 2015/05/08 09:14:55 kid1| With 32768 file descriptors available
> 2015/05/08 09:14:55 kid1| Initializing IP Cache...
> 
> I had 3.5.4 on test server , which is hardware and OS identical, but ,
> definitely, has lower load,
> it worked with 3.5 since it was released and no such problem.
> Could you tell me how can I find where problem  is?

Various methods of how to get detailed info about this type of problem
are outlined at <http://wiki.squid-cache.org/SquidFaq/BugReporting>

Probably the section about running under gdb in live traffic will be
most useful.

Amos



From squid3 at treenet.co.nz  Fri May  8 06:40:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 08 May 2015 18:40:50 +1200
Subject: [squid-users] Best malware/phishing/virus list ?
In-Reply-To: <CAJajPefYLD8s2BbVU-F+G6ce2GuEhSJ7-56wiTQSd2O68rVs3Q@mail.gmail.com>
References: <CAJajPec7jBgr8XBgYrNpqLQDvth22D4ZRpF41aPCeRkPmFAxHA@mail.gmail.com>	<554AF41D.7020006@treenet.co.nz>
 <CAJajPefYLD8s2BbVU-F+G6ce2GuEhSJ7-56wiTQSd2O68rVs3Q@mail.gmail.com>
Message-ID: <554C5A72.5040909@treenet.co.nz>

On 8/05/2015 6:13 p.m., Olivier CALVANO wrote:
> Are you sure ? ;)
> 
> 
> malware/phishing/virus are sent by mail sur, but it's include a URL
> and we want block on squid this type of URL
> 

Yes. I'm sure.

* Not letting the MTA get the spam in the first place improves filter
performance immensely.


* MTA is just another client from HTTP perspective. If they are setup to
automatically fetch URLs from spam and infected emails I can guarantee
you that they are going to pull in web bugs, malware etc and increase
the spam problem. Blocklists simply cannot keep up fast enough with the
rapidly changing domain names used by malware.


* Even if you want spamassasin lists for converting into Squid lists,
the spamassasin help is the better place to be asking for them.


Amos

> 
> 2015-05-07 7:11 GMT+02:00 Amos Jeffries:
> 
>> On 7/05/2015 4:59 p.m., Olivier CALVANO wrote:
>>> hi
>>>
>>> Anyone Know a good list for spam assassin of malware/phishing/virus URL ?
>>
>> Squid is an HTTP software. Not SMTP. If you are trying to use Squid to
>> control spam problems you got serious trouble ahead.
>>
>>
>> PS. IMHO, the best anti-spam list is xbl.spamhaus.org. Configure that in
>> your MTA to cut the bulk of garbage. Complicated (and slow) systems like
>> Spamassin make a good second line of defence behind that, but not
>> frontline.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From prashanth.prabhu at gmail.com  Fri May  8 07:29:34 2015
From: prashanth.prabhu at gmail.com (Prashanth Prabhu)
Date: Fri, 8 May 2015 00:29:34 -0700
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <5548885F.6080602@treenet.co.nz>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
 <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
 <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>
 <5548885F.6080602@treenet.co.nz>
Message-ID: <CAMFQPn9OMUeZpftnXnhdHkA7D2Bw3jCTavkc_Xv=SAWk9h1UTQ@mail.gmail.com>

Hi Amos,

> <http://bugs.squid-cache.org/show_bug.cgi?id=3775> IIRC.

I patched in the bug fix -- I assume the patch file listed in the bug
report is the most current? -- but it didn't help. I saw the same
crash in a couple of patched systems. The stack trace is pasted below.

As I pointed out in my earlier email, the crash occurs with c-icap
connections, whereas the fix appears to have been targeted at SSL
connections. Sounds to me like this must be a different issue
altogether. Are there any other fixes that may resolve this issue?

Thanks.
Prashanth

Stack trace.
----
pprabhu:~$ gdb /usr/local/sbin/squid core-squid-30399-1431062067
GNU gdb (Ubuntu/Linaro 7.4-2012.04-0ubuntu2.1) 7.4-2012.04
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
<http://bugs.launchpad.net/gdb-linaro/>...
Reading symbols from /usr/local/sbin/squid...done.
[New LWP 30399]

warning: Can't read pathname for load map: Input/output error.
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `/usr/local/sbin/squid -N -f /etc/squid/squid.conf -D'.
Program terminated with signal 6, Aborted.
#0  0x00007f0f381e50d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) bt
#0  0x00007f0f381e50d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f0f381e883b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x000000000058174f in xassert (
    msg=0x899548 "fd_table[conn->fd].halfClosedReader != NULL",
    file=0x8993bf "Read.cc", line=69) at debug.cc:544
#3  0x0000000000797410 in comm_read_base (conn=..., buf=0x1578380 "",
    size=65535, callback=...) at Read.cc:69
#4  0x000000000080f1b9 in comm_read (callback=..., len=65535,
    buf=0x1578380 "", conn=...) at ../../../src/comm/Read.h:58
#5  Adaptation::Icap::Xaction::scheduleRead (this=0x14c81a8)
    at Xaction.cc:397
#6  0x000000000081ba78 in Adaptation::Icap::ModXact::readMore (
    this=0x14c81a8) at ModXact.cc:561
#7  0x0000000000823742 in Adaptation::Icap::ModXact::handleCommConnected (
    this=0x14c81a8) at ModXact.cc:191
#8  0x000000000080ff2b in Adaptation::Icap::Xaction::noteCommConnected (
    this=0x14c81a8, io=...) at Xaction.cc:266
#9  0x0000000000813a4b in JobDialer<Adaptation::Icap::Xaction>::dial (
    this=0x128f8e0, call=...) at ../../../src/base/AsyncJobCalls.h:174
#10 0x00000000006f2309 in AsyncCall::make (this=0x128f8b0) at AsyncCall.cc:40
#11 0x00000000006f61ec in AsyncCallQueue::fireNext (this=<optimized out>)
    at AsyncCallQueue.cc:56
#12 0x00000000006f6540 in AsyncCallQueue::fire (this=0xdd9bf0)
    at AsyncCallQueue.cc:42
#13 0x0000000000593bbc in EventLoop::runOnce (this=0x7fffe6f214d0)
    at EventLoop.cc:120
#14 0x0000000000593d70 in EventLoop::run (this=0x7fffe6f214d0)
    at EventLoop.cc:82
#15 0x00000000005f7d83 in SquidMain (argc=<optimized out>,
    argv=<optimized out>) at main.cc:1508
#16 0x000000000050a0db in SquidMainSafe (argv=<optimized out>,
    argc=<optimized out>) at main.cc:1240
#17 main (argc=<optimized out>, argv=<optimized out>) at main.cc:1233
----


On 5 May 2015 at 02:07, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 5/05/2015 7:32 p.m., Prashanth Prabhu wrote:
>> Hi Nathan,
>>
>>> These are fixed in 3.5.4.
>>
>>
>> Thanks. Do you have the bug ID's that fixed them?
>>
>
> <http://bugs.squid-cache.org/show_bug.cgi?id=3775> IIRC.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From dm at belkam.com  Fri May  8 09:35:22 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 08 May 2015 13:35:22 +0400
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <CAMFQPn9OMUeZpftnXnhdHkA7D2Bw3jCTavkc_Xv=SAWk9h1UTQ@mail.gmail.com>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
 <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
 <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>
 <5548885F.6080602@treenet.co.nz>
 <CAMFQPn9OMUeZpftnXnhdHkA7D2Bw3jCTavkc_Xv=SAWk9h1UTQ@mail.gmail.com>
Message-ID: <554C835A.5080600@belkam.com>

08.05.2015 11:29, Prashanth Prabhu ?????:
>
> As I pointed out in my earlier email, the crash occurs with c-icap
> connections
I guess this is why my 3.5.4 crashes too, although had no chance to 
debug yet,
I found info in log that there are no enough redirectors, which never 
happend with 3.4.



From dm at belkam.com  Fri May  8 09:37:50 2015
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 08 May 2015 13:37:50 +0400
Subject: [squid-users] Squid crashes with 3.5.1
In-Reply-To: <554C835A.5080600@belkam.com>
References: <CAMFQPn-PQ_bVukF6U1+94tHmhN+xzcnGM8jBa1uycViJB-wG3g@mail.gmail.com>
 <CAGUJm7boRCq+8w5FnxqK9CqWPcGL6njM7yHD5wzhuhfUAQoQ-A@mail.gmail.com>
 <CAMFQPn9JWrDH+pHoxuyW4mmRGN93mRUL8jszTCqBG95OY3opLw@mail.gmail.com>
 <5548885F.6080602@treenet.co.nz>
 <CAMFQPn9OMUeZpftnXnhdHkA7D2Bw3jCTavkc_Xv=SAWk9h1UTQ@mail.gmail.com>
 <554C835A.5080600@belkam.com>
Message-ID: <554C83EE.6020300@belkam.com>

08.05.2015 13:35, Dmitry Melekhov ?????:
> 08.05.2015 11:29, Prashanth Prabhu ?????:
>>
>> As I pointed out in my earlier email, the crash occurs with c-icap
>> connections
> I guess this is why my 3.5.4 crashes too, although had no chance to 
> debug yet,
> I found info in log that there are no enough redirectors, which never 
> happend with 3.4.
>
Oops, sorry, I meant and I have ...
:-)



From jc at info-systems.de  Fri May  8 10:46:10 2015
From: jc at info-systems.de (Jakob Curdes)
Date: Fri, 08 May 2015 12:46:10 +0200
Subject: [squid-users] Reverse Proxy and SSL client side renegotiation
Message-ID: <554C93F2.2010704@info-systems.de>

Hello all, I have configured squid 3.3.8 (CentOS 7 rpm) as an SSL 
reverse proxy which works fine. However, I would like to make it as 
secure as possible. The SSLLabs test showed
"Secure Client-Initiated Renegotiation *Supported* *DoS DANGER* (more 
info 
<https://community.qualys.com/blogs/securitylabs/2011/10/31/tls-renegotiation-and-denial-of-service-attacks?_ga=1.161215733.973769323.1423134297>)"

I found an old thread here where it was suggested it depends on the 
default of the OpenSSL library installed and that on compiling squid, 
you can disable this option by specifying SSL_OP_ALL=0. However I would 
like to stick to the RPM if possible.
Is there a way to disable this via a configuration option? I tried to 
pass options=!ALL in the config but then no SSL conection is possible as 
the peers do not find any common cipher....

I have put together everything else to get a secure SSL connection which 
also gets an A grade in the qualys SSL test. I will post it here when it 
is done and I can also will put it on the squid wiki.

Best regards,
Jakob Curdes

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150508/e19cc6e3/attachment.htm>

From squid3 at treenet.co.nz  Fri May  8 12:22:17 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 09 May 2015 00:22:17 +1200
Subject: [squid-users] Reverse Proxy and SSL client side renegotiation
In-Reply-To: <554C93F2.2010704@info-systems.de>
References: <554C93F2.2010704@info-systems.de>
Message-ID: <554CAA79.6050202@treenet.co.nz>

On 8/05/2015 10:46 p.m., Jakob Curdes wrote:
> Hello all, I have configured squid 3.3.8 (CentOS 7 rpm) as an SSL
> reverse proxy which works fine. However, I would like to make it as
> secure as possible. The SSLLabs test showed
> "Secure Client-Initiated Renegotiation *Supported* *DoS DANGER* (more
> info
> <https://community.qualys.com/blogs/securitylabs/2011/10/31/tls-renegotiation-and-denial-of-service-attacks?_ga=1.161215733.973769323.1423134297>)"
> 
> 
> I found an old thread here where it was suggested it depends on the
> default of the OpenSSL library installed and that on compiling squid,
> you can disable this option by specifying SSL_OP_ALL=0. However I would
> like to stick to the RPM if possible.

Very old thread. Your version of Squid should already contain the
relevant change that would have caused.


> Is there a way to disable this via a configuration option? I tried to
> pass options=!ALL in the config but then no SSL conection is possible as
> the peers do not find any common cipher....

Er, yes. You have to follow !ALL with the explicit ':' or ',' separated
list of things which you do want to work.

The real answer though is to use an up to date OpenSSL version.

Amos



From ambadasvh at teledna.com  Fri May  8 13:56:35 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Fri, 8 May 2015 13:56:35 +0000
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <554B5C93.3070304@treenet.co.nz>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
Message-ID: <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>

Hi Amos,

It's happening as you said:

the packets doing this:
 client -----> Squid -SYN-> server
 client <-------------ACK-- server
 client -RST-> Squid 

There's a firewall in between squid & web server which is directly sending SYN-ACK to client instead of squid.

But in my requirement, the clients are configured with IP & Port. Is there any possible way/approach by which I can make client IP hide towards web server?

Any help appreciated


Regards,
Ambadas


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: 07 May 2015 18:08
To: Ambadas Hibare; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Client IP spoofing via squid proxy

On 7/05/2015 6:09 p.m., Ambadas Hibare wrote:
> HI,
> 
> Client IP: 172.16.5.110
> Client Mac: 00:23:7D:E8:AC:C4
> 
> Squid Box:
> 
> eth0 IP: 172.16.5.102
> eth0 Mac: 18:A9:05:3C:12:E4
> 
> eth1 IP: 10.0.0.102
> eth1 Mac: 18:A9:05:3C:12:E6
> 
>> "Your "ip route" rules use eth1, but your rp_filter settings only change eth0. Also your iptables rules do not distinguish by ethN."
> 
> Yes. Should that setting be applied on both eths' or only the one facing the client?

The one facing the *server* at minimum. Doing it on both wont hurt for experimenting. But when this is working try setting the client-facing NIC off again.


> Also want to know if it's possible to do tproxy setup with just one eth at squid box?

Of course. You just have to configure the packet routing explicitly on the router the Squid box is connected to as well as the Squid box itself. To prevent server responses (SYN ACK etc) being sent to the client when they should go to Squid.

> 

>> "Your trace shows the MAC address *:c4 contacting Squid (MAC address
*:e4) and delivering an HTTP request. Squid (*:e4) then contacts the remote server be sending > a TCP SYN packet ... which the MAC address
*:c4 rejects."
> 
> In trace it shows squid (*:e4) (packet# 83) is contacting the web
server (google.com) via client IP (172.16.5.110). So it's getting spoofed!? But not able to understand why client is sending RST to google (packet# 84) just after that & response


Because one of the SYN (from Squid) or SYN-ACK packet (reply from
server) is arriving at the client when it should have been delivered elsewhere.


the packets doing this:
 client -----> Squid -SYN-> server
 client <-------------ACK-- server
 client -RST-> Squid

or this:
 client -----> Squid -SYN-\
 client <-----------------/
 client -RST-> Squid


> PS. The default gateway for client is squid box IP (eth1). 

The part routing traffic from client<->Squid is working. The part Squid<->server is going wrong.

Amos

From squid3 at treenet.co.nz  Fri May  8 16:01:35 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 09 May 2015 04:01:35 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
Message-ID: <554CDDDF.1060605@treenet.co.nz>

On 9/05/2015 1:56 a.m., Ambadas Hibare wrote:
> Hi Amos,
> 
> It's happening as you said:
> 
> the packets doing this:
>  client -----> Squid -SYN-> server
>  client <-------------ACK-- server
>  client -RST-> Squid 
> 
> There's a firewall in between squid & web server which is directly sending SYN-ACK to client instead of squid.
> 
> But in my requirement, the clients are configured with IP & Port. Is there any possible way/approach by which I can make client IP hide towards web server?
> 
> Any help appreciated


With Squid-3.4 or later:
 <http://www.squid-cache.org/Doc/config/spoof_client_ip/>

set it to "deny all"

Amos



From mailinglist at cd.kcfam.net  Fri May  8 19:35:56 2015
From: mailinglist at cd.kcfam.net (Casey Daniels)
Date: Fri, 08 May 2015 15:35:56 -0400
Subject: [squid-users] CHROOT Problems Part II
In-Reply-To: <55441FB2.6040504@cd.kcfam.net>
References: <55441FB2.6040504@cd.kcfam.net>
Message-ID: <554D101C.9000902@cd.kcfam.net>

I managed to get most of the errors cleaned up, in a LONG and TEDIOUS 
process, however when I assumed fixing one error would fix another error 
was wrong.  I'm still getting the following error.

logfileHandleWrite: daemon:/usr/local/squid/var/logs/access.log: error 
writing ((32) Broken pipe)

It only happens in the CHROOT environment.  If i take that directive 
out, squid works, if I put it in, it breaks and gives me this error.

Any help would be much appreciative.

Casey

On 05/01/2015 08:52 PM, Casey Daniels wrote:
> Hello,
>  I'm trying to get squid up and running under a CHROOT (which by the 
> way the instructions on wiki appear to point to a lot of directories 
> that are different if you compile and install with out changing anything)
>
> Prior to attempting the CHROOT I had Squid running fine, however there 
> appears to be some issues now that I'm running under chroot.
>
> Here is my log file
>
> <---- START LOG----->
>
> 2015/05/01 20:33:21| Starting Squid Cache version 3.3.9 for 
> x86_64-unknown-linux-gnu...
> 2015/05/01 20:33:21| Process ID 3095
> 2015/05/01 20:33:21| Process Roles: master worker
> 2015/05/01 20:33:21| With 1024 file descriptors available
> 2015/05/01 20:33:21| Initializing IP Cache...
> 2015/05/01 20:33:21| DNS Socket created at [::], FD 5
> 2015/05/01 20:33:21| DNS Socket created at 0.0.0.0, FD 6
> 2015/05/01 20:33:21| Adding domain kcfam.net from /etc/resolv.conf
> 2015/05/01 20:33:21| Adding nameserver [::1] from /etc/resolv.conf
> 2015/05/01 20:33:21| WARNING: rejecting '[::1]' as a name server, 
> because it is not a numeric IP address
> 2015/05/01 20:33:21| Adding nameserver 127.0.0.1 from /etc/resolv.conf
> 2015/05/01 20:33:21| Logfile: opening log 
> daemon:/usr/local/squid/var/logs/access.log
> 2015/05/01 20:33:21| Logfile Daemon: opening log 
> /usr/local/squid/var/logs/access.log
> 2015/05/01 20:33:21| ipcCreate: 
> /usr/local/squid/libexec/log_file_daemon: (2) No such file or directory
> 2015/05/01 20:33:21| ipcCreate: 
> /usr/local/squid/libexec/log_file_daemon: (2) No such file or directory
> 2015/05/01 20:33:21| Local cache digest enabled; rebuild/rewrite every 
> 3600/3600 sec
> 2015/05/01 20:33:21| Store logging disabled
> 2015/05/01 20:33:21| Swap maxSize 0 + 262144 KB, estimated 20164 objects
> 2015/05/01 20:33:21| Target number of buckets: 1008
> 2015/05/01 20:33:21| Using 8192 Store buckets
> 2015/05/01 20:33:21| Max Mem  size: 262144 KB
> 2015/05/01 20:33:21| Max Swap size: 0 KB
> 2015/05/01 20:33:21| Using Least Load store dir selection
> 2015/05/01 20:33:21| Set Current Directory to 
> /usr/local/squid/var/cache/squid
> 2015/05/01 20:33:21| Loaded Icons.
> 2015/05/01 20:33:21| HTCP Disabled.
> 2015/05/01 20:33:21| Pinger socket opened on FD 11
> 2015/05/01 20:33:21| Squid plugin modules loaded: 0
> 2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) 
> No such file or directory
> 2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) 
> No such file or directory
> 2015/05/01 20:33:21| Adaptation support is off.
> 2015/05/01 20:33:21| Accepting HTTP Socket connections at 
> local=[::]:3128 remote=[::] FD 9 flags=9
> 2015/05/02 00:33:22| logfileHandleWrite: 
> daemon:/usr/local/squid/var/logs/access.log: error writing ((32) 
> Broken pipe)
> 2015/05/02 00:33:22| Closing HTTP port [::]:3128
> 2015/05/02 00:33:22| storeDirWriteCleanLogs: Starting...
> 2015/05/02 00:33:22|   Finished.  Wrote 0 entries.
> 2015/05/02 00:33:22|   Took 0.00 seconds (  0.00 entries/sec).
> FATAL: I don't handle this error well!
> Squid Cache (Version 3.3.9): Terminated abnormally.
> CPU Usage: 0.019 seconds = 0.013 user + 0.006 sys
> Maximum Resident Size: 49008 KB
> Page faults with physical i/o: 0
> Memory usage for squid via mallinfo():
>     total space in arena:    4764 KB
>     Ordinary blocks:         4694 KB      4 blks
>     Small blocks:               0 KB      1 blks
>     Holding blocks:          1324 KB      4 blks
>     Free Small blocks:          0 KB
>     Free Ordinary blocks:      69 KB
>     Total in use:            6018 KB 126%
>     Total free:                69 KB 1%
> 2015/05/02 00:33:22| Closing Pinger socket on FD 11
>
> <--- END LOG --->
>
> I see three errors.
>
> Number 1
> 2015/05/01 20:33:21| ipcCreate: /usr/local/squid/libexec/pinger: (2) 
> No such file or directory
>
> Number 2
> 2015/05/01 20:33:21| ipcCreate: 
> /usr/local/squid/libexec/log_file_daemon: (2) No such file or directory
>
> Number 3
> 2015/05/02 00:33:22| logfileHandleWrite: 
> daemon:/usr/local/squid/var/logs/access.log: error writing ((32) 
> Broken pipe)
>
>
> Not knowing much about the inter workings of squid, and what not, but 
> I'm guessing that Number 3 will be resolved when Number 2 is resolved. 
> And Number 1 and 2 are closely related.
>
> The issue is I'm at a stand still have to proceed with fixing Number 1 
> and 2.
> my chroot directive is
>
> chroot /srv/squid
>
> and I've copied all of the /usr/local/squid/libexec/ to 
> /srv/squid/usr/local/squid/libexec
>
> So i believe I have it in the right place.  And the file permissions 
> appear to be the same within the CHROOT as they are in the normal 
> directories.
>
> Any Ideas?
>
> Casey
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat May  9 09:25:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 09 May 2015 21:25:49 +1200
Subject: [squid-users] CHROOT Problems Part II
In-Reply-To: <554D101C.9000902@cd.kcfam.net>
References: <55441FB2.6040504@cd.kcfam.net> <554D101C.9000902@cd.kcfam.net>
Message-ID: <554DD29D.9000501@treenet.co.nz>

On 9/05/2015 7:35 a.m., Casey Daniels wrote:
> I managed to get most of the errors cleaned up, in a LONG and TEDIOUS
> process, however when I assumed fixing one error would fix another error
> was wrong.  I'm still getting the following error.
> 
> logfileHandleWrite: daemon:/usr/local/squid/var/logs/access.log: error
> writing ((32) Broken pipe)
> 
> It only happens in the CHROOT environment.  If i take that directive
> out, squid works, if I put it in, it breaks and gives me this error.
> 
> Any help would be much appreciative.

Either the log daemon helper is not installed where it should be, or the
access.log file is in a different location in your chroot.

Amos



From mailinglist at cd.kcfam.net  Sat May  9 13:45:39 2015
From: mailinglist at cd.kcfam.net (Casey Daniels)
Date: Sat, 09 May 2015 09:45:39 -0400
Subject: [squid-users] CHROOT Problems Part II
In-Reply-To: <554DD29D.9000501@treenet.co.nz>
References: <55441FB2.6040504@cd.kcfam.net> <554D101C.9000902@cd.kcfam.net>
 <554DD29D.9000501@treenet.co.nz>
Message-ID: <554E0F83.8080005@cd.kcfam.net>



On 05/09/2015 05:25 AM, Amos Jeffries wrote:
> On 9/05/2015 7:35 a.m., Casey Daniels wrote:
>> I managed to get most of the errors cleaned up, in a LONG and TEDIOUS
>> process, however when I assumed fixing one error would fix another error
>> was wrong.  I'm still getting the following error.
>>
>> logfileHandleWrite: daemon:/usr/local/squid/var/logs/access.log: error
>> writing ((32) Broken pipe)
>>
>> It only happens in the CHROOT environment.  If i take that directive
>> out, squid works, if I put it in, it breaks and gives me this error.
>>
>> Any help would be much appreciative.
> Either the log daemon helper is not installed where it should be, or the
> access.log file is in a different location in your chroot.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
Actually figured it out this morning.  It was neither, it ended up being 
/dev/null missing.
Thank you for the response.

Casey


From baim.lubis at gmail.com  Sun May 10 06:31:10 2015
From: baim.lubis at gmail.com (Ibrahim Lubis)
Date: Sun, 10 May 2015 13:31:10 +0700
Subject: [squid-users] Squid as transparent in 'caching layer'
Message-ID: <CAAVkS7KoS-nsxNMJgaGAH6zMw9hq97HLH_d=MG4yLmWZfc55_w@mail.gmail.com>

Hi,

Most of all know about tiered network
topology(access,aggregation/dist,core) from core than to firewall and then
to router. For redundancy usually there 2 core and 2 firewall. I was
thinking adding a transparent caching layer between core and firewall,just
adding squid box. It is okay just adding 2 independent squid box or I need
some sync between squid box ? What if I add not 2 but 6 and doing
active-active on both core n firewall? Can anybody give me insight ? Btw My
objective is to save some bandwidths from user for internet access.

Thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150510/88408d0a/attachment.htm>

From squid3 at treenet.co.nz  Sun May 10 06:57:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 May 2015 18:57:24 +1200
Subject: [squid-users] Squid as transparent in 'caching layer'
In-Reply-To: <CAAVkS7KoS-nsxNMJgaGAH6zMw9hq97HLH_d=MG4yLmWZfc55_w@mail.gmail.com>
References: <CAAVkS7KoS-nsxNMJgaGAH6zMw9hq97HLH_d=MG4yLmWZfc55_w@mail.gmail.com>
Message-ID: <554F0154.6090404@treenet.co.nz>

On 10/05/2015 6:31 p.m., Ibrahim Lubis wrote:
> Hi,
> 
> Most of all know about tiered network
> topology(access,aggregation/dist,core) from core than to firewall and then
> to router. For redundancy usually there 2 core and 2 firewall. I was
> thinking adding a transparent caching layer between core and firewall,just
> adding squid box. It is okay just adding 2 independent squid box or I need
> some sync between squid box ? What if I add not 2 but 6 and doing
> active-active on both core n firewall? Can anybody give me insight ? Btw My
> objective is to save some bandwidths from user for internet access.

Go with independent Squid boxes until you are happy that they are
operating properly and you know whats going on. Number of Squid does not
matter much, so long as they each can handle the traffic load you put
through. If you are new to this start with just one and put only a small
amount of the traffic through, then increase gradually until you need 2,
and so on.

Sync'ing between the Squid caches, and interception proxying can each
have unwanted side effects. Its best to deal with those in separately to
avoid confusion and troubles.


"active-active on both core n firewall" does not matter. You MUST NOT
perform destination-NAT (or TPROXY) on any machine other than the Squid
box receiving the TCP connection from client(s). The firewalls and core
only perform *routing* (perhapse over a tunnel) to get the TCP packets
to the right Squid box. This has the nice side effect of greatly
reducing the amount of data the firewalls need to sync.


Hints for beginners:

 Caching can make some traffic appear slower - all MISS and some REFRESH
transactions. There is extra packet processing done by the proxy and
latency getting the packets around. This is the tradeoff for bandwidth
saving. Super-fast HITs and traffic optimization can make up for that,
but not always.

Amos



From squid3 at treenet.co.nz  Sun May 10 07:00:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 May 2015 19:00:29 +1200
Subject: [squid-users] 3.5.4 Can't access Google or Yahoo SSL,	pages
In-Reply-To: <554E4398.8050900@cpalmer.me.uk>
References: <554BBA06.2030203@cpalmer.me.uk> <554C4E5D.5000201@treenet.co.nz>
 <554E4398.8050900@cpalmer.me.uk>
Message-ID: <554F020D.2000304@treenet.co.nz>

Some good news in this front. We've managed to find the bit missing from
the r13811 patch.

3.5.4 should work with
<http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13824.patch>.

There is already another important SSL related fix, so using the r13825
or later snapshot (out in a few hrs) may be better than just patching.

Amos



From yvoinov at gmail.com  Sun May 10 10:35:03 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 10 May 2015 16:35:03 +0600
Subject: [squid-users] Squid as transparent in 'caching layer'
In-Reply-To: <554F0154.6090404@treenet.co.nz>
References: <CAAVkS7KoS-nsxNMJgaGAH6zMw9hq97HLH_d=MG4yLmWZfc55_w@mail.gmail.com>
 <554F0154.6090404@treenet.co.nz>
Message-ID: <554F3457.2090900@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Amos,

independent proxies also supported by Cisco WCCP. For redundancy it can
group any numbers of transparent proxies.

WBR, Yuri

10.05.15 12:57, Amos Jeffries ?????:
> On 10/05/2015 6:31 p.m., Ibrahim Lubis wrote:
>> Hi,
>>
>> Most of all know about tiered network
>> topology(access,aggregation/dist,core) from core than to firewall and
then
>> to router. For redundancy usually there 2 core and 2 firewall. I was
>> thinking adding a transparent caching layer between core and
firewall,just
>> adding squid box. It is okay just adding 2 independent squid box or I
need
>> some sync between squid box ? What if I add not 2 but 6 and doing
>> active-active on both core n firewall? Can anybody give me insight ?
Btw My
>> objective is to save some bandwidths from user for internet access.
>
> Go with independent Squid boxes until you are happy that they are
> operating properly and you know whats going on. Number of Squid does not
> matter much, so long as they each can handle the traffic load you put
> through. If you are new to this start with just one and put only a small
> amount of the traffic through, then increase gradually until you need 2,
> and so on.
>
> Sync'ing between the Squid caches, and interception proxying can each
> have unwanted side effects. Its best to deal with those in separately to
> avoid confusion and troubles.
>
>
> "active-active on both core n firewall" does not matter. You MUST NOT
> perform destination-NAT (or TPROXY) on any machine other than the Squid
> box receiving the TCP connection from client(s). The firewalls and core
> only perform *routing* (perhapse over a tunnel) to get the TCP packets
> to the right Squid box. This has the nice side effect of greatly
> reducing the amount of data the firewalls need to sync.
>
>
> Hints for beginners:
>
>  Caching can make some traffic appear slower - all MISS and some REFRESH
> transactions. There is extra packet processing done by the proxy and
> latency getting the packets around. This is the tradeoff for bandwidth
> saving. Super-fast HITs and traffic optimization can make up for that,
> but not always.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVTzRXAAoJENNXIZxhPexGXJYIAMtb90ri0hymGN7ZGTVH98cy
uZbNjQ2kYQqxXGCkkSFECpjM0wqkONF6pPGrL1YqcecZCkmGNS6ExE6r4FMuX8y1
oBE2z9OfaN/4CfMq4+WvE0jwtyOSVyKIUSUKr+I2qTNCubg0kFgr9yWONOdLbUDJ
FJ06c1qqb1U8u8ZsYFTL7/hfTgVRr6QjnGQlnNcCwzU+/QIAtAP7GyRxJB0b0yxJ
i2M/LQ+d1LJMhCgX6ICgBas5x+GXXB3KHtH0jAn/xF854qciQhbOrMf0O/j/ac19
4XB8qfqsGkIvPe3TcPSYypyOJn1dXILpb7mmNogGzh+rE4nmdRG7cam6MX3En8c=
=SXkU
-----END PGP SIGNATURE-----



From baim.lubis at gmail.com  Sun May 10 16:09:59 2015
From: baim.lubis at gmail.com (Ibrahim Lubis)
Date: Sun, 10 May 2015 23:09:59 +0700
Subject: [squid-users] Squid as transparent in 'caching layer'
In-Reply-To: <554F3457.2090900@gmail.com>
References: <CAAVkS7KoS-nsxNMJgaGAH6zMw9hq97HLH_d=MG4yLmWZfc55_w@mail.gmail.com>
 <554F0154.6090404@treenet.co.nz> <554F3457.2090900@gmail.com>
Message-ID: <CAAVkS7+-ukQaqHqnynppxAY5rgeh5eRVmHVDsTLe-taS+P_L+A@mail.gmail.com>

Thx all for the info
On May 10, 2015 5:35 PM, "Yuri Voinov" <yvoinov at gmail.com> wrote:

>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Amos,
>
> independent proxies also supported by Cisco WCCP. For redundancy it can
> group any numbers of transparent proxies.
>
> WBR, Yuri
>
> 10.05.15 12:57, Amos Jeffries ?????:
> > On 10/05/2015 6:31 p.m., Ibrahim Lubis wrote:
> >> Hi,
> >>
> >> Most of all know about tiered network
> >> topology(access,aggregation/dist,core) from core than to firewall and
> then
> >> to router. For redundancy usually there 2 core and 2 firewall. I was
> >> thinking adding a transparent caching layer between core and
> firewall,just
> >> adding squid box. It is okay just adding 2 independent squid box or I
> need
> >> some sync between squid box ? What if I add not 2 but 6 and doing
> >> active-active on both core n firewall? Can anybody give me insight ?
> Btw My
> >> objective is to save some bandwidths from user for internet access.
> >
> > Go with independent Squid boxes until you are happy that they are
> > operating properly and you know whats going on. Number of Squid does not
> > matter much, so long as they each can handle the traffic load you put
> > through. If you are new to this start with just one and put only a small
> > amount of the traffic through, then increase gradually until you need 2,
> > and so on.
> >
> > Sync'ing between the Squid caches, and interception proxying can each
> > have unwanted side effects. Its best to deal with those in separately to
> > avoid confusion and troubles.
> >
> >
> > "active-active on both core n firewall" does not matter. You MUST NOT
> > perform destination-NAT (or TPROXY) on any machine other than the Squid
> > box receiving the TCP connection from client(s). The firewalls and core
> > only perform *routing* (perhapse over a tunnel) to get the TCP packets
> > to the right Squid box. This has the nice side effect of greatly
> > reducing the amount of data the firewalls need to sync.
> >
> >
> > Hints for beginners:
> >
> >  Caching can make some traffic appear slower - all MISS and some REFRESH
> > transactions. There is extra packet processing done by the proxy and
> > latency getting the packets around. This is the tradeoff for bandwidth
> > saving. Super-fast HITs and traffic optimization can make up for that,
> > but not always.
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVTzRXAAoJENNXIZxhPexGXJYIAMtb90ri0hymGN7ZGTVH98cy
> uZbNjQ2kYQqxXGCkkSFECpjM0wqkONF6pPGrL1YqcecZCkmGNS6ExE6r4FMuX8y1
> oBE2z9OfaN/4CfMq4+WvE0jwtyOSVyKIUSUKr+I2qTNCubg0kFgr9yWONOdLbUDJ
> FJ06c1qqb1U8u8ZsYFTL7/hfTgVRr6QjnGQlnNcCwzU+/QIAtAP7GyRxJB0b0yxJ
> i2M/LQ+d1LJMhCgX6ICgBas5x+GXXB3KHtH0jAn/xF854qciQhbOrMf0O/j/ac19
> 4XB8qfqsGkIvPe3TcPSYypyOJn1dXILpb7mmNogGzh+rE4nmdRG7cam6MX3En8c=
> =SXkU
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150510/6799ab4c/attachment.htm>

From ambadasvh at teledna.com  Mon May 11 05:56:36 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Mon, 11 May 2015 05:56:36 +0000
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <554CDDDF.1060605@treenet.co.nz>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <554CDDDF.1060605@treenet.co.nz>
Message-ID: <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>

Hi Amos,

But in my requirement, the clients are configured with Squid IP & Port. Is there any possible way/approach by which I can make "Squid IP" hide towards web server?

sorry for typo, I meant squid IP

Regards,
Ambadas


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: 08 May 2015 21:32
To: Ambadas Hibare; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Client IP spoofing via squid proxy

On 9/05/2015 1:56 a.m., Ambadas Hibare wrote:
> Hi Amos,
> 
> It's happening as you said:
> 
> the packets doing this:
>  client -----> Squid -SYN-> server
>  client <-------------ACK-- server
>  client -RST-> Squid 
> 
> There's a firewall in between squid & web server which is directly sending SYN-ACK to client instead of squid.
> 
> But in my requirement, the clients are configured with IP & Port. Is there any possible way/approach by which I can make client IP hide towards web server?
> 
> Any help appreciated


With Squid-3.4 or later:
 <http://www.squid-cache.org/Doc/config/spoof_client_ip/>

set it to "deny all"

Amos


From squid3 at treenet.co.nz  Mon May 11 10:30:48 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 May 2015 22:30:48 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <554CDDDF.1060605@treenet.co.nz>
 <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
Message-ID: <555084D8.4080804@treenet.co.nz>

On 11/05/2015 5:56 p.m., Ambadas Hibare wrote:
> Hi Amos,
> 
> But in my requirement, the clients are configured with Squid IP & Port. Is there any possible way/approach by which I can make "Squid IP" hide towards web server?
> 

No. Hiding the Squid IP in the TCP/IP layers without full TPROXY in both
directions is not possible.

Why do you have that requirement?
 what is the problem it is actually trying to solve?

Amos



From yvoinov at gmail.com  Mon May 11 11:18:52 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 11 May 2015 17:18:52 +0600
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <555084D8.4080804@treenet.co.nz>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <554CDDDF.1060605@treenet.co.nz>
 <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <555084D8.4080804@treenet.co.nz>
Message-ID: <5550901C.70502@gmail.com>

I think, this is requirement for invisible proxy, Amos

11.05.15 16:30, Amos Jeffries ?????:
> On 11/05/2015 5:56 p.m., Ambadas Hibare wrote:
>> Hi Amos,
>>
>> But in my requirement, the clients are configured with Squid IP & Port. Is there any possible way/approach by which I can make "Squid IP" hide towards web server?
>>
> No. Hiding the Squid IP in the TCP/IP layers without full TPROXY in both
> directions is not possible.
>
> Why do you have that requirement?
>   what is the problem it is actually trying to solve?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ambadasvh at teledna.com  Mon May 11 11:27:31 2015
From: ambadasvh at teledna.com (Ambadas Hibare)
Date: Mon, 11 May 2015 11:27:31 +0000
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <555084D8.4080804@treenet.co.nz>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <554CDDDF.1060605@treenet.co.nz>
 <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <555084D8.4080804@treenet.co.nz>
Message-ID: <HKNPR03MB19362D62B142F2443B0CA55A1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>

Hi,

The problem is many clients are already preconfigured with proxy ip/port settings due to previous setup.

If you don?t mind, may I know like its squid's feature, or the Linux OS feature, which doesn?t do transparency only towards web server?


Regards,
Ambadas


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: 11 May 2015 16:01
To: Ambadas Hibare; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Client IP spoofing via squid proxy

On 11/05/2015 5:56 p.m., Ambadas Hibare wrote:
> Hi Amos,
> 
> But in my requirement, the clients are configured with Squid IP & Port. Is there any possible way/approach by which I can make "Squid IP" hide towards web server?
> 

No. Hiding the Squid IP in the TCP/IP layers without full TPROXY in both directions is not possible.

Why do you have that requirement?
 what is the problem it is actually trying to solve?

Amos


From vdoctor at neuf.fr  Mon May 11 12:01:35 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 11 May 2015 05:01:35 -0700 (PDT)
Subject: [squid-users] Cache peers with different load
Message-ID: <1431345695914-4671204.post@n4.nabble.com>

Hi All,

A crazy thing I cannot understand:
- 3 squid 3.5.4

the child (172.10.1.1) is like that:
cache_peer 172.10.1.2 parent 8182 8183 proxy-only weighted-round-robin
background-ping no-tproxy
cache_peer 172.10.1.3 parent 8182 8183 proxy-only weighted-round-robin
background-ping no-tproxy

ICP is allowed on the 3 squids.

Traffic is not equal, not balanced as I expect.
cache .2 is 200MB after 10 min
cache .3 is 4GB in the same time

Sure I'm missing something here, but what ? 

Thanks in advance for your input...
Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-peers-with-different-load-tp4671204.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon May 11 13:06:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 May 2015 01:06:26 +1200
Subject: [squid-users] Client IP spoofing via squid proxy
In-Reply-To: <HKNPR03MB19362D62B142F2443B0CA55A1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
References: <HKXPR03MB200C8616441D69FD31CE6C6A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554AF967.5090406@treenet.co.nz>
 <HKXPR03MB20056C850394D616A780C54A1DF0@HKXPR03MB200.apcprd03.prod.outlook.com>
 <554B5C93.3070304@treenet.co.nz>
 <HKNPR03MB19328B5DD73DB75C44EA197A1DE0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <554CDDDF.1060605@treenet.co.nz>
 <HKNPR03MB1931E92D06EFFDC2882A9EAA1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
 <555084D8.4080804@treenet.co.nz>
 <HKNPR03MB19362D62B142F2443B0CA55A1DB0@HKNPR03MB193.apcprd03.prod.outlook.com>
Message-ID: <5550A952.5090109@treenet.co.nz>

On 11/05/2015 11:27 p.m., Ambadas Hibare wrote:
> Hi,
> 
> The problem is many clients are already preconfigured with proxy ip/port settings due to previous setup.
> 

huh? your "problem" is that clients are setup correctly?

TPROXY and NAT interception (aka. hjacking attack on users) are the
*hacked workaround* way to do proxying when one has no better choices
due to broken UA implementation or configuration.

It also has nothing to do with the Squid->server connections.

So I ask again, why do you say you are required to perform IP spoofing
(aka. forgery of users identification details) on outbound server
connections?

I think that you or someone setting the requirements is mistaken about
what is needed. Possibly even mistaken about what some problem is.

...

Clients which *are* configured to use a proxy explicitly, can continue
to use Squid as that proxy. If the proxy receiving IP or domain has
changed the traffic can be NAT'ed to the new proxy with no need to
change anything - not even configure Squid with "intercept" flags.


Clients which are *not* configured to use the proxy, but needing to be
gatewayed through it need to be TPROXY or NAT intercepted. But only as
per common "normal" client connection interception.

In both cases the server connections will "just work" when proxied
without TPROXY spoofing. If TPROXY spoofing is performed the routing
needs special configuration we have been over already.


> If you don?t mind, may I know like its squid's feature, or the Linux OS feature, which doesn?t do transparency only towards web server?
> 

Both. The OS restrictions more than Squid. And legal restrictions in
many places.

Consider that your workplace might need a guard to see your passport or
similar form of private ID to purchase some service for you. Is it legal
(and right) for that guard to photocopy it, then use it as their own
name/ID to do some things of their choosing?

Compare that to a Guard who does the same copying, but uses it saying to
the service provider "I come representing the person identified by this
token".

Amos



From squid3 at treenet.co.nz  Mon May 11 13:33:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 May 2015 01:33:38 +1200
Subject: [squid-users] Cache peers with different load
In-Reply-To: <1431345695914-4671204.post@n4.nabble.com>
References: <1431345695914-4671204.post@n4.nabble.com>
Message-ID: <5550AFB2.4070401@treenet.co.nz>

On 12/05/2015 12:01 a.m., Stakres wrote:
> Hi All,
> 
> A crazy thing I cannot understand:
> - 3 squid 3.5.4
> 
> the child (172.10.1.1) is like that:
> cache_peer 172.10.1.2 parent 8182 8183 proxy-only weighted-round-robin
> background-ping no-tproxy
> cache_peer 172.10.1.3 parent 8182 8183 proxy-only weighted-round-robin
> background-ping no-tproxy
> 
> ICP is allowed on the 3 squids.
> 
> Traffic is not equal, not balanced as I expect.
> cache .2 is 200MB after 10 min
> cache .3 is 4GB in the same time
> 
> Sure I'm missing something here, but what ? 

The difference between bandwidth load, transaction load, and connection
load.

Squid load balancing algorithms are all message load balancing
algorithms. They do not balance bandwidth load. They all contain
connection (and/or message) bias.

This is explained in the
<http://wiki.squid-cache.org/Features/LoadBalance#Weighted_Round-Robin>
last few paragraphs about response time bias. The connection bias and
effects (other than latency) described in the round-robin section of
that page are also having an effect since this is a variation on
round-robin.

Amos



From vdoctor at neuf.fr  Mon May 11 14:28:44 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 11 May 2015 07:28:44 -0700 (PDT)
Subject: [squid-users] Cache peers with different load
In-Reply-To: <5550AFB2.4070401@treenet.co.nz>
References: <1431345695914-4671204.post@n4.nabble.com>
 <5550AFB2.4070401@treenet.co.nz>
Message-ID: <1431354524713-4671207.post@n4.nabble.com>

Hi Amos,

OK, got it.
But why a so big gap on the 2 parents ?
The 3 squids are on the same range, connected to the same switch, all in 1Gb
NIC.
No problem if there are some MB difference, but here it's 10+ times more
between 2 parents 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-peers-with-different-load-tp4671204p4671207.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon May 11 15:27:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 May 2015 03:27:30 +1200
Subject: [squid-users] Cache peers with different load
In-Reply-To: <1431354524713-4671207.post@n4.nabble.com>
References: <1431345695914-4671204.post@n4.nabble.com>
 <5550AFB2.4070401@treenet.co.nz> <1431354524713-4671207.post@n4.nabble.com>
Message-ID: <5550CA62.3070502@treenet.co.nz>

On 12/05/2015 2:28 a.m., Stakres wrote:
> Hi Amos,
> 
> OK, got it.
> But why a so big gap on the 2 parents ?
> The 3 squids are on the same range, connected to the same switch, all in 1Gb
> NIC.
> No problem if there are some MB difference, but here it's 10+ times more
> between 2 parents 


Only vague guesses I'm afraid.

Could be anything from a fluke 2-peer artifact of "Happy Eyeballs"
always makign one peer get the 'unused' connection, through to the way
placing any network I/O load on a Squid (ie peer 2) decreases the CPU
cycles needed for ICP packets to be read/written from the network, to
fast-path optimization in the shared switch routing.

Amos



From sebag at vianetcon.com.ar  Mon May 11 18:41:27 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Mon, 11 May 2015 15:41:27 -0300
Subject: [squid-users] Jesred 3.0 available under GPL on GitHub
Message-ID: <5550F7D7.10904@vianetcon.com.ar>

This time I'm writing not to ask for help, but to give something back.
As we received much help from this mail list during our migration from 
Squid 2.7 to 3.x and in the proccess we had to make some modifications 
to jesred (helper) to make it compatible with Squid, we have decided to 
start a new GitHub project with the source code for anyone to use or modify.

If you want to clone it: https://github.com/sawcache/jesred.wiki.git


It works with Squid >= 3.4


Comments on how to improve it are welcomed



Regards,
Ulises, Ariel & Sebastian


From hierony_milanisti at yahoo.co.id  Sun May  3 13:15:02 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Sun, 3 May 2015 13:15:02 +0000 (UTC)
Subject: [squid-users] Distributed Cache Problems
Message-ID: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>


Dear Fellow,
I am implement a Distributed cache system (1 child Proxy, 2 parent proxy as sibling).
in the cache peer algorithm, I use Weighted Round-Robin.
But when I analyze the log using Squeezer, I am found that there is no hit requestin the parent proxy.

I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), the Hierarchy that I used, and the log analyze
result.
For Simplicity, Just download the Zip file.
I am very pleased that you want to lend your time to help me (Because the deadline is very close).

Thanks in advance.

?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Child-Conf.conf
Type: application/octet-stream
Size: 4367 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Parent1-Conf.conf
Type: application/octet-stream
Size: 3952 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Parent2-Conf.conf
Type: application/octet-stream
Size: 3907 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: CacheHierarchy.png
Type: image/png
Size: 35336 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment.png>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: DistributedCache-Problem.zip
Type: application/octet-stream
Size: 42193 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/6a19da67/attachment-0003.obj>

From hierony_milanisti at yahoo.co.id  Sun May  3 15:10:35 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Sun, 3 May 2015 15:10:35 +0000 (UTC)
Subject: [squid-users] Distributed Cache Problems
In-Reply-To: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
References: <167236348.344246.1430658902438.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1633257250.360699.1430665836282.JavaMail.yahoo@mail.yahoo.com>


 


Dear Fellow,
I am implement a Distributed cache system (1 child Proxy, 2 parent proxy as sibling).
in the cache peer algorithm, I use Weighted Round-Robin.
But when I analyze the log using Squeezer, I am found that there is no hit requestin the parent proxy.

I have attached the conf (Child.conf, Parent1.conf,Parent2.conf), and the log analyze
result.
For Simplicity, Just download the Zip file.
I am very pleased that you want to lend your time to help me (Because the deadline is very close).

Thanks in advance.

?Hierony Manurung
Del Institute of Technology
Network Management

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/595e7522/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Child-Conf.conf
Type: application/octet-stream
Size: 4367 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/595e7522/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Parent1-Conf.conf
Type: application/octet-stream
Size: 3952 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/595e7522/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Parent2-Conf.conf
Type: application/octet-stream
Size: 3907 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/595e7522/attachment-0002.obj>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150503/595e7522/attachment.html>

From yvoinov at gmail.com  Tue May 12 12:24:11 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 12 May 2015 18:24:11 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1431033905569-4671179.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
 <1431032349609-4671177.post@n4.nabble.com> <554BD683.7050708@gmail.com>
 <1431033905569-4671179.post@n4.nabble.com>
Message-ID: <5551F0EB.9090604@gmail.com>

Solved.

I've add 3975 backport patch,

then this one:

acl text-html rep_mime_type text/html
acl http302 http_status 302
store_miss deny text-html
store_miss deny http302
send_hit deny text-html
send_hit deny http302

and this one:

# For YT block useragent header
acl googledomain_ua_deny dstdomain .youtube.com .googlevideo.com
request_header_access User-Agent deny googledomain_ua_deny

Now loop is gone.

Note: strip User-Agent may lead some side effect!

08.05.15 3:25, HackXBack ?????:
> you are right, but this patch still work with me.
> i dont know if we can find better solution for this like you said by acl
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671179.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ulises at vianetcon.com.ar  Tue May 12 13:34:57 2015
From: ulises at vianetcon.com.ar (Ulises Nicolini)
Date: Tue, 12 May 2015 10:34:57 -0300
Subject: [squid-users] Jesred 3.0 available under GPL on GitHub
In-Reply-To: <5550F7D7.10904@vianetcon.com.ar>
References: <5550F7D7.10904@vianetcon.com.ar>
Message-ID: <55520181.3000300@vianetcon.com.ar>

Hi! We detect the first bug. The correct Github link for Jesred 3 
rewriter helper proyect is

https://github.com/sawcache/jesred

Comments on how to improve it are welcomed



Regards,
Ulises, Ariel & Sebastian

El 11/05/15 15:41, Sebastian Goicochea escribi?:
> This time I'm writing not to ask for help, but to give something back.
> As we received much help from this mail list during our migration from 
> Squid 2.7 to 3.x and in the proccess we had to make some modifications 
> to jesred (helper) to make it compatible with Squid, we have decided 
> to start a new GitHub project with the source code for anyone to use 
> or modify.
>
> If you want to clone it: https://github.com/sawcache/jesred.wiki.git
>
>
> It works with Squid >= 3.4
>
>
> Comments on how to improve it are welcomed
>
>
>
> Regards,
> Ulises, Ariel & Sebastian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150512/606903a3/attachment.htm>

From jetsystemservices at gmail.com  Tue May 12 15:25:50 2015
From: jetsystemservices at gmail.com (Jose Torres-Berrocal)
Date: Tue, 12 May 2015 11:25:50 -0400
Subject: [squid-users] Need help debugging my squid configuration
Message-ID: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>

Hello All.

I am new to Squid.  I have compiled squid 3.3.8 on Lubuntu 14.04.1
system following the thread
http://ubuntuforums.org/showthread.php?t=68246

I have also turn on --enabled-ssl-crtd on the compilation rules.

When starting squid terminates but does not provide much error
information that I could go on an try to fix.

I tried starting the squid with -X option in the startup script and
adding debug_options to the configuration file, but still does not get
any valuable information.  Not at least for me as a newbe.

The syslog errors are:

May 12 10:46:03 Lubuntu kernel: [12418.628583] init: squid3 main
process ended, respawning
May 12 10:46:03 Lubuntu kernel: [12418.823803] init: squid3 main
process (15760) terminated with status 1
May 12 10:46:03 Lubuntu kernel: [12418.823817] init: squid3 main
process ended, respawning
May 12 10:46:04 Lubuntu kernel: [12419.019733] init: squid3 main
process (15772) terminated with status 1
May 12 10:46:04 Lubuntu kernel: [12419.019746] init: squid3 respawning
too fast, stopped

The cache.log is:

2015/05/12 10:46:03.953| tools.cc(664) enter_suid: enter_suid: PID
15760 taking root privileges
2015/05/12 10:46:03.953| cache_manager.cc(102) registerProfile:
registering legacy config
2015/05/12 10:46:03.953| cache_manager.cc(87) registerProfile:
registered profile: config
2015/05/12 10:46:03.953| mem.cc(498) Report: Memory pools are 'on';
limit: 5.000 MB
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: '
2015/05/12 10:46:03.953| Acl.cc(364) ~ACL: ACL::~ACL: ' (this repeats
for many lines)

My configuration file content is:

debug_options=ALL,3
auth_param basic program /usr/lib/squid3/basic_radius_auth -f /etc/radius_config
auth_param basic children 5
auth_param basic realm Squid Proxy Server Web Access
auth_param basic credentialsttl 30 second
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl localnet src 192.168.56.0/24
acl radius proxy_auth REQUIRED
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny !radius
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128 ssl-bump cert=/etc/squid3/serverkey.pem
generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
always_direct allow all
sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 6 startup=2 idle=2
cache_dir ufs /squidcache/cache_dir 256 16 256
coredump_dir /var/spool/squid3
url_rewrite_children 6 startup=2 idle=2
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320
cache_effective_user proxy
url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
cache_effective_group proxy


From mailinglist at cd.kcfam.net  Tue May 12 18:17:23 2015
From: mailinglist at cd.kcfam.net (Casey Daniels)
Date: Tue, 12 May 2015 14:17:23 -0400
Subject: [squid-users] SSL Peak and Splice
Message-ID: <555243B3.7000007@cd.kcfam.net>

Hi,
   I've been trying to figure out how to do some web filtering on HTTPs, 
with no really good options given the layout I have.  But then I just 
happened to see this feature for Squid 3.5, and was wondering if I'm 
understanding it correctly.

With the Peak and Splice feature, is it possible to run squid in a 
transparent mode for SSL, and check for certain host and either deny the 
connection all together or allow the connection without further 
interference from Squid?  Would this be completely transparent without 
adding a trusted certificate from the proxy server to all user devices?

Thank You


From yvoinov at gmail.com  Tue May 12 19:12:26 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 May 2015 01:12:26 +0600
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <555243B3.7000007@cd.kcfam.net>
References: <555243B3.7000007@cd.kcfam.net>
Message-ID: <5552509A.4050108@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

13.05.15 0:17, Casey Daniels ?????:
> Hi,
>   I've been trying to figure out how to do some web filtering on
HTTPs, with no really good options given the layout I have.  But then I
just happened to see this feature for Squid 3.5, and was wondering if
I'm understanding it correctly.
>
> With the Peak and Splice feature, is it possible to run squid in a
transparent mode for SSL, and check for certain host and either deny the
connection all together or allow the connection without further
interference from Squid?  Would this be completely transparent without
adding a trusted certificate from the proxy server to all user devices?
>
> Thank You
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVUlCaAAoJENNXIZxhPexG7o4H/0a9DUPGUmhSGQgjTydct3en
7/xM5V8GuNlMTftEZTuLODAxF7YODCSvejunaoU66o/9MzCFLy7vgvM/iWNSe9b5
K03PJfWd/ZgH1NKymgGKdZpySN+8yRIrCJBCWSV9RnJGHievM8lE4ANSuy25eJlZ
PJDaHy8kNZnaIx/8wB/4NM/bu0JF01KKWxSgoPHjOnkbrLGsShbHX6VBVUBVuX5o
cBbKJAn18c6c7R9KQsv6yPBQ/nXa7Ql6dQ9yq4eAXETjJmnlS19USaUkgEYSHS1y
xZfGKHC1Ye0qi5UaD2YNDGRuWFQJdwdGEi8dXUpbSO/Dr2d4uKQopgMB+1zvirc=
=bsBF
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Wed May 13 07:34:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 May 2015 19:34:09 +1200
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
Message-ID: <5552FE71.2010608@treenet.co.nz>

On 13/05/2015 3:25 a.m., Jose Torres-Berrocal wrote:
> Hello All.
> 
> I am new to Squid.  I have compiled squid 3.3.8 on Lubuntu 14.04.1
> system following the thread
> http://ubuntuforums.org/showthread.php?t=68246
> 
> I have also turn on --enabled-ssl-crtd on the compilation rules.

The option alone is not enough to enable SSL support. Your version also
requires --enable-ssl

However, what you are trying to do has not worked in that version for
some years. When you want to participate in the arms race that is
SSL-Bump you must use the latest of the latest versions of Squid. Today
that is squid-3.5.4 snapshot r18325 or later.


> 
> When starting squid terminates but does not provide much error
> information that I could go on an try to fix.
> 
> I tried starting the squid with -X option in the startup script and
> adding debug_options to the configuration file, but still does not get
> any valuable information.  Not at least for me as a newbe.

-X will spew out so much information the critical piece gets lost.

* Ensure your Squid is built with both the above mentioned ./configure
options.

* Run squid -k parse

* Run normally. cache.log will contain any critical messages.
 - in this case I suspect a mesage from the ssl_crtd heleper about
initializing its certificate database.

* If necessary add "debugs_options ALL,1" to your squid.conf to get
non-critical but important messages as well.


Amos



From squid3 at treenet.co.nz  Wed May 13 07:25:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 May 2015 19:25:24 +1200
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <555243B3.7000007@cd.kcfam.net>
References: <555243B3.7000007@cd.kcfam.net>
Message-ID: <5552FC64.9070108@treenet.co.nz>

On 13/05/2015 6:17 a.m., Casey Daniels wrote:
> Hi,
>   I've been trying to figure out how to do some web filtering on HTTPs,
> with no really good options given the layout I have.  But then I just
> happened to see this feature for Squid 3.5, and was wondering if I'm
> understanding it correctly.
> 
> With the Peak and Splice feature, is it possible to run squid in a
> transparent mode for SSL, and check for certain host and either deny the
> connection all together or allow the connection without further
> interference from Squid?  Would this be completely transparent without
> adding a trusted certificate from the proxy server to all user devices?

Depends on how you define "host" and what the TLS ClientHello
information contains.

If you define "host" in the official standard Internet terminology (a
single machine). Then no its not possible. NAT and "load balancing"
utterly destroyed the ability to determine if the host being spoken to
is the host indicated in the packets.
 Case in point is your interceptor - a completely different host to the
one the client sees in its packets. Nothing stops other interceptors
existing upstream from you.

If by "host" you actally meant FQDN or host *name*. It can be done when
and only when the TLS SNI information is made available by the client.

Amos



From simon at baladia.gov.kw  Wed May 13 08:45:21 2015
From: simon at baladia.gov.kw (Simon Dcunha)
Date: Wed, 13 May 2015 11:45:21 +0300 (AST)
Subject: [squid-users] transparent proxy
Message-ID: <1193011433.16500.1431506721234.JavaMail.zimbra@baladia.gov.kw>

Dear All,

I want to implement transparent proxy with wccp2. kindly appreciate if someone can advise me a link explaining the steps to follow 


regards

simon

-- 
---------
Network Administrator
Kuwait Municipality!!!

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From squid3 at treenet.co.nz  Wed May 13 09:48:45 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 May 2015 21:48:45 +1200
Subject: [squid-users] transparent proxy
In-Reply-To: <1193011433.16500.1431506721234.JavaMail.zimbra@baladia.gov.kw>
References: <1193011433.16500.1431506721234.JavaMail.zimbra@baladia.gov.kw>
Message-ID: <55531DFD.8000903@treenet.co.nz>

On 13/05/2015 8:45 p.m., Simon Dcunha wrote:
> Dear All,
> 
> I want to implement transparent proxy with wccp2. kindly appreciate if someone can advise me a link explaining the steps to follow 
> 

That would be the Squid wiki.

<http://wiki.squid-cache.org/ConfigExamples#Interception>

Amos


From jetsystemservices at gmail.com  Wed May 13 13:53:06 2015
From: jetsystemservices at gmail.com (Jose Torres-Berrocal)
Date: Wed, 13 May 2015 09:53:06 -0400
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
Message-ID: <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>

As said I followed the thread I included in the initial email. I have
added the --enable-ssl and --with-open-ssl directives to the
compilation. The debug_option setting I have used you can see in the
squid.conf that I included. I tried with ALL, 9 also.  The logs do not
show any informative error.

I have run sudo squid3 -k parse but it does not return any error. I do
not know how to run it as proxy user (password is unknown).

How can I download the squid source you mention from Ubuntu repository?


From yvoinov at gmail.com  Wed May 13 13:55:51 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 13 May 2015 19:55:51 +0600
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
 <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
Message-ID: <555357E7.2000004@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Latest Squid source's not in repositories. They are here:

http://www.squid-cache.org/Download/

13.05.15 19:53, Jose Torres-Berrocal ?????:
> As said I followed the thread I included in the initial email. I have
> added the --enable-ssl and --with-open-ssl directives to the
> compilation. The debug_option setting I have used you can see in the
> squid.conf that I included. I tried with ALL, 9 also.  The logs do not
> show any informative error.
>
> I have run sudo squid3 -k parse but it does not return any error. I do
> not know how to run it as proxy user (password is unknown).
>
> How can I download the squid source you mention from Ubuntu repository?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVU1fnAAoJENNXIZxhPexGQ/kH+wS4Hr3JfDw3+r56RRay1dsi
musKEpxCFPx7ajml+XFkhdOla99q0syxKdnIatWeIkgzeHIDvb9lFXg9S/6mwK64
e2kWrIO4B2BCJkoC7lE7q7QZ5d4nG5CqG1tC6kh9iHneqLgTVGVZ71kaC3xiZyNb
GIiI/9H26XrnWFISTJ+/bmk4afKhx9gx5OPJMgGsGHu8LDfANdbqTnYOiQCS7180
7SkNkZBFsSPSv1qKWNdEwXXecpS6JTMFvkNA633YdF5xAWtTdkZMRl/8VzS9iJI/
pS7sty9bDhyQDUgcK2O+3oaXFzu89JSad2xmeDUsXjLBftAxLya53zhoXNRxvUg=
=t5Ej
-----END PGP SIGNATURE-----



From michael.pelletier at palmbeachschools.org  Wed May 13 17:42:59 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 13 May 2015 13:42:59 -0400
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
Message-ID: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>

Hello,

What does this warning mean?
assertion failed: DestinationIp.cc:64: checklist->conn() &&
checklist->conn()->clientConnection != NULL


Michael

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150513/c6ae401e/attachment.htm>

From squid3 at treenet.co.nz  Wed May 13 19:23:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 May 2015 07:23:05 +1200
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
In-Reply-To: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>
References: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>
Message-ID: <5553A499.1060608@treenet.co.nz>

On 14/05/2015 5:42 a.m., Michael Pelletier wrote:
> Hello,
> 
> What does this warning mean?
> assertion failed: DestinationIp.cc:64: checklist->conn() &&
> checklist->conn()->clientConnection != NULL

That is no warning. It is an assertion failure. Your Squid is crashing.

It appears to be bug 3616 which was fixed about 3 years ago. What
version of Squid are you using?

Amos



From michael.pelletier at palmbeachschools.org  Wed May 13 21:25:55 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 13 May 2015 17:25:55 -0400
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
In-Reply-To: <5553A499.1060608@treenet.co.nz>
References: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>
 <5553A499.1060608@treenet.co.nz>
Message-ID: <CAEnCSG5qmay388JSis8gxLwUF_WVFQXD-wCNCaxfeeuE1MrT7w@mail.gmail.com>

I am running 3.4.12


On Wed, May 13, 2015 at 3:23 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/05/2015 5:42 a.m., Michael Pelletier wrote:
> > Hello,
> >
> > What does this warning mean?
> > assertion failed: DestinationIp.cc:64: checklist->conn() &&
> > checklist->conn()->clientConnection != NULL
>
> That is no warning. It is an assertion failure. Your Squid is crashing.
>
> It appears to be bug 3616 which was fixed about 3 years ago. What
> version of Squid are you using?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150513/27975b36/attachment.htm>

From michael.pelletier at palmbeachschools.org  Wed May 13 21:34:51 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Wed, 13 May 2015 17:34:51 -0400
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
In-Reply-To: <CAEnCSG5qmay388JSis8gxLwUF_WVFQXD-wCNCaxfeeuE1MrT7w@mail.gmail.com>
References: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>
 <5553A499.1060608@treenet.co.nz>
 <CAEnCSG5qmay388JSis8gxLwUF_WVFQXD-wCNCaxfeeuE1MrT7w@mail.gmail.com>
Message-ID: <CAEnCSG5S0OddjnbDpSinseHf6sw5scNZFd35tfAfVtzD=xVZ9Q@mail.gmail.com>

Squid does recover. What do you think?

On Wed, May 13, 2015 at 5:25 PM, Michael Pelletier <
michael.pelletier at palmbeachschools.org> wrote:

> I am running 3.4.12
>
>
> On Wed, May 13, 2015 at 3:23 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 14/05/2015 5:42 a.m., Michael Pelletier wrote:
>> > Hello,
>> >
>> > What does this warning mean?
>> > assertion failed: DestinationIp.cc:64: checklist->conn() &&
>> > checklist->conn()->clientConnection != NULL
>>
>> That is no warning. It is an assertion failure. Your Squid is crashing.
>>
>> It appears to be bug 3616 which was fixed about 3 years ago. What
>> version of Squid are you using?
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150513/2a382849/attachment.htm>

From mailinglist at cd.kcfam.net  Wed May 13 22:47:31 2015
From: mailinglist at cd.kcfam.net (Casey Daniels - mailinglist)
Date: Wed, 13 May 2015 18:47:31 -0400 (EDT)
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <5552FC64.9070108@treenet.co.nz>
References: <555243B3.7000007@cd.kcfam.net> <5552FC64.9070108@treenet.co.nz>
Message-ID: <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>

> On May 13, 2015 at 3:25 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>
> On 13/05/2015 6:17 a.m., Casey Daniels wrote:
> > Hi,
> > I've been trying to figure out how to do some web filtering on HTTPs,
> > with no really good options given the layout I have. But then I just
> > happened to see this feature for Squid 3.5, and was wondering if I'm
> > understanding it correctly.
> >
> > With the Peak and Splice feature, is it possible to run squid in a
> > transparent mode for SSL, and check for certain host and either deny the
> > connection all together or allow the connection without further
> > interference from Squid? Would this be completely transparent without
> > adding a trusted certificate from the proxy server to all user devices?
>
> Depends on how you define "host" and what the TLS ClientHello
> information contains.
>
> If you define "host" in the official standard Internet terminology (a
> single machine). Then no its not possible. NAT and "load balancing"
> utterly destroyed the ability to determine if the host being spoken to
> is the host indicated in the packets.
> Case in point is your interceptor - a completely different host to the
> one the client sees in its packets. Nothing stops other interceptors
> existing upstream from you.
>
> If by "host" you actally meant FQDN or host *name*. It can be done when
> and only when the TLS SNI information is made available by the client.
>
> Amos
>

Yes the second option, not the particular machine, but the FQDN
(i.e.<http://www.cooking.com> )
When is the TLS SNI information made available by the client? 
 
Casey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150513/dd1ba705/attachment.htm>

From jetsystemservices at gmail.com  Thu May 14 01:16:01 2015
From: jetsystemservices at gmail.com (Jose Torres-Berrocal)
Date: Wed, 13 May 2015 21:16:01 -0400
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <555357E7.2000004@gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
 <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
 <555357E7.2000004@gmail.com>
Message-ID: <CABU5kW50Cw6B7ONq0Q88X+NbS3u76=8i+GAAGsu-HbFw2PotbA@mail.gmail.com>

Source from squid repository does not come directly compatible with
the OS.  Source from Ubuntu repository is made compatible to the OS.

It has a folder call debian which has a file called rules which in
term is used with the configure script for the compile options.

What file or script do I have to change in the squid repository source
to be able to use the debian/rules file?

On Wed, May 13, 2015 at 9:55 AM, Yuri Voinov <yvoinov at gmail.com> wrote:
>
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> Latest Squid source's not in repositories. They are here:
>
> http://www.squid-cache.org/Download/
>
> 13.05.15 19:53, Jose Torres-Berrocal ?????:
>> As said I followed the thread I included in the initial email. I have
>> added the --enable-ssl and --with-open-ssl directives to the
>> compilation. The debug_option setting I have used you can see in the
>> squid.conf that I included. I tried with ALL, 9 also.  The logs do not
>> show any informative error.
>>
>> I have run sudo squid3 -k parse but it does not return any error. I do
>> not know how to run it as proxy user (password is unknown).
>>
>> How can I download the squid source you mention from Ubuntu repository?
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVU1fnAAoJENNXIZxhPexGQ/kH+wS4Hr3JfDw3+r56RRay1dsi
> musKEpxCFPx7ajml+XFkhdOla99q0syxKdnIatWeIkgzeHIDvb9lFXg9S/6mwK64
> e2kWrIO4B2BCJkoC7lE7q7QZ5d4nG5CqG1tC6kh9iHneqLgTVGVZ71kaC3xiZyNb
> GIiI/9H26XrnWFISTJ+/bmk4afKhx9gx5OPJMgGsGHu8LDfANdbqTnYOiQCS7180
> 7SkNkZBFsSPSv1qKWNdEwXXecpS6JTMFvkNA633YdF5xAWtTdkZMRl/8VzS9iJI/
> pS7sty9bDhyQDUgcK2O+3oaXFzu89JSad2xmeDUsXjLBftAxLya53zhoXNRxvUg=
> =t5Ej
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu May 14 04:34:24 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 May 2015 16:34:24 +1200
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <CABU5kW50Cw6B7ONq0Q88X+NbS3u76=8i+GAAGsu-HbFw2PotbA@mail.gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
 <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
 <555357E7.2000004@gmail.com>
 <CABU5kW50Cw6B7ONq0Q88X+NbS3u76=8i+GAAGsu-HbFw2PotbA@mail.gmail.com>
Message-ID: <555425D0.6010707@treenet.co.nz>

On 14/05/2015 1:16 p.m., Jose Torres-Berrocal wrote:
> Source from squid repository does not come directly compatible with
> the OS.  Source from Ubuntu repository is made compatible to the OS.
> 
> It has a folder call debian which has a file called rules which in
> term is used with the configure script for the compile options.
> 
> What file or script do I have to change in the squid repository source
> to be able to use the debian/rules file?

The debian/ folder contents for 3.3 are much different from the ones
needed for other Squid versions.

You are better off using the instructions at:
 <http://wiki.squid-cache.org/KnowledgeBase/Ubuntu>
to compile and install a new Squid binary. Like this command line:
  ./configure (the options needed) && make install


Amos


From squid3 at treenet.co.nz  Thu May 14 04:37:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 May 2015 16:37:19 +1200
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>
References: <555243B3.7000007@cd.kcfam.net> <5552FC64.9070108@treenet.co.nz>
 <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>
Message-ID: <5554267F.4070605@treenet.co.nz>

On 14/05/2015 10:47 a.m., Casey Daniels - mailinglist wrote:
>> On May 13, 2015 at 3:25 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>>
>> On 13/05/2015 6:17 a.m., Casey Daniels wrote:
>>> Hi,
>>> I've been trying to figure out how to do some web filtering on HTTPs,
>>> with no really good options given the layout I have. But then I just
>>> happened to see this feature for Squid 3.5, and was wondering if I'm
>>> understanding it correctly.
>>>
>>> With the Peak and Splice feature, is it possible to run squid in a
>>> transparent mode for SSL, and check for certain host and either deny the
>>> connection all together or allow the connection without further
>>> interference from Squid? Would this be completely transparent without
>>> adding a trusted certificate from the proxy server to all user devices?
>>
>> Depends on how you define "host" and what the TLS ClientHello
>> information contains.
>>
>> If you define "host" in the official standard Internet terminology (a
>> single machine). Then no its not possible. NAT and "load balancing"
>> utterly destroyed the ability to determine if the host being spoken to
>> is the host indicated in the packets.
>> Case in point is your interceptor - a completely different host to the
>> one the client sees in its packets. Nothing stops other interceptors
>> existing upstream from you.
>>
>> If by "host" you actally meant FQDN or host *name*. It can be done when
>> and only when the TLS SNI information is made available by the client.
>>
>> Amos
>>
> 
> Yes the second option, not the particular machine, but the FQDN
> (i.e.<http://www.cooking.com> )


 # get TLS SNI details etc
 ssl_bump peek all

 # some get rejected
 acl blocked ssl:server_name .example.com
 ssl_bump reject blocked

 # the rest allowed without decrypting
 ssl_bump splice all


> When is the TLS SNI information made available by the client? 

They send it or they dont. Nothign you or we can do about it.

Amos


From squid3 at treenet.co.nz  Thu May 14 04:40:42 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 May 2015 16:40:42 +1200
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
In-Reply-To: <CAEnCSG5S0OddjnbDpSinseHf6sw5scNZFd35tfAfVtzD=xVZ9Q@mail.gmail.com>
References: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>	<5553A499.1060608@treenet.co.nz>	<CAEnCSG5qmay388JSis8gxLwUF_WVFQXD-wCNCaxfeeuE1MrT7w@mail.gmail.com>
 <CAEnCSG5S0OddjnbDpSinseHf6sw5scNZFd35tfAfVtzD=xVZ9Q@mail.gmail.com>
Message-ID: <5554274A.9030202@treenet.co.nz>

On 14/05/2015 9:34 a.m., Michael Pelletier wrote:
> Squid does recover. What do you think?
> 

I think its a bug that needs fixing.

Can you get a trace of what ACLs is being tested using "debug_options
28,5" ?

Amos



From michael.pelletier at palmbeachschools.org  Thu May 14 05:07:10 2015
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Thu, 14 May 2015 01:07:10 -0400
Subject: [squid-users] assertion failed: DestinationIp.cc:64:
 checklist->conn() && checklist->conn()->clientConnection != NULL
In-Reply-To: <5554274A.9030202@treenet.co.nz>
References: <CAEnCSG738vwze9zw5bicUSFPeRxhGf-oUvbsQjf-S4u4=ZEVvg@mail.gmail.com>
 <5553A499.1060608@treenet.co.nz>
 <CAEnCSG5qmay388JSis8gxLwUF_WVFQXD-wCNCaxfeeuE1MrT7w@mail.gmail.com>
 <CAEnCSG5S0OddjnbDpSinseHf6sw5scNZFd35tfAfVtzD=xVZ9Q@mail.gmail.com>
 <5554274A.9030202@treenet.co.nz>
Message-ID: <CAEnCSG4+TmJa9usEuGU=6BT8PJniV1nGo9aJaqRPHgi+Jf9kBw@mail.gmail.com>

I can try. I only saw it once under heavy load. I will see what I can do...

Michael

On Thu, May 14, 2015 at 12:40 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 14/05/2015 9:34 a.m., Michael Pelletier wrote:
> > Squid does recover. What do you think?
> >
>
> I think its a bug that needs fixing.
>
> Can you get a trace of what ACLs is being tested using "debug_options
> 28,5" ?
>
> Amos
>
>

-- 


*Disclaimer: *Under Florida law, e-mail addresses are public records. If 
you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150514/fb1d97be/attachment.htm>

From simon at baladia.gov.kw  Thu May 14 06:46:23 2015
From: simon at baladia.gov.kw (Simon Dcunha)
Date: Thu, 14 May 2015 09:46:23 +0300 (AST)
Subject: [squid-users] transparent proxy
In-Reply-To: <1454138819.17704.1431585978556.JavaMail.zimbra@baladia.gov.kw>
References: <1193011433.16500.1431506721234.JavaMail.zimbra@baladia.gov.kw>
 <55531DFD.8000903@treenet.co.nz>
Message-ID: <336561011.17705.1431585983430.JavaMail.zimbra@baladia.gov.kw>

Dear Amos,

Thanks for the immediate reply
i had checked the wiki but was just looking around if there was some stuff which is more easier to implement 

thanks and regards

simon

----- Original Message -----
From: "Amos Jeffries" <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Sent: Wednesday, May 13, 2015 12:48:45 PM
Subject: Re: [squid-users] transparent proxy

On 13/05/2015 8:45 p.m., Simon Dcunha wrote:
> Dear All,
> 
> I want to implement transparent proxy with wccp2. kindly appreciate if someone can advise me a link explaining the steps to follow 
> 

That would be the Squid wiki.

<http://wiki.squid-cache.org/ConfigExamples#Interception>

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From mailinglist at cd.kcfam.net  Thu May 14 07:43:59 2015
From: mailinglist at cd.kcfam.net (Casey Daniels - mailinglist)
Date: Thu, 14 May 2015 03:43:59 -0400 (EDT)
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <5554267F.4070605@treenet.co.nz>
References: <555243B3.7000007@cd.kcfam.net> <5552FC64.9070108@treenet.co.nz>
 <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>
 <5554267F.4070605@treenet.co.nz>
Message-ID: <1318724671.125452.1431589440051.JavaMail.open-xchange@oxuslxltgw00.lxa.perfora.net>


> On May 14, 2015 at 12:37 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>
> On 14/05/2015 10:47 a.m., Casey Daniels - mailinglist wrote:
> >> On May 13, 2015 at 3:25 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
> >>
> >>
> >> On 13/05/2015 6:17 a.m., Casey Daniels wrote:
> >>> Hi,
> >>> I've been trying to figure out how to do some web filtering on HTTPs,
> >>> with no really good options given the layout I have. But then I just
> >>> happened to see this feature for Squid 3.5, and was wondering if I'm
> >>> understanding it correctly.
> >>>
> >>> With the Peak and Splice feature, is it possible to run squid in a
> >>> transparent mode for SSL, and check for certain host and either deny the
> >>> connection all together or allow the connection without further
> >>> interference from Squid? Would this be completely transparent without
> >>> adding a trusted certificate from the proxy server to all user devices?
> >>
> >> Depends on how you define "host" and what the TLS ClientHello
> >> information contains.
> >>
> >> If you define "host" in the official standard Internet terminology (a
> >> single machine). Then no its not possible. NAT and "load balancing"
> >> utterly destroyed the ability to determine if the host being spoken to
> >> is the host indicated in the packets.
> >> Case in point is your interceptor - a completely different host to the
> >> one the client sees in its packets. Nothing stops other interceptors
> >> existing upstream from you.
> >>
> >> If by "host" you actally meant FQDN or host *name*. It can be done when
> >> and only when the TLS SNI information is made available by the client.
> >>
> >> Amos
> >>
> >
> > Yes the second option, not the particular machine, but the FQDN
> > (i.e.<http://www.cooking.com> )
>
>
> # get TLS SNI details etc
> ssl_bump peek all
>
> # some get rejected
> acl blocked ssl:server_name .example.com
> ssl_bump reject blocked
>
> # the rest allowed without decrypting
> ssl_bump splice all
>
>
> > When is the TLS SNI information made available by the client?
>
> They send it or they dont. Nothign you or we can do about it.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 
Thank You,
Casey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150514/ee8bec88/attachment.htm>

From Walter.H at mathemainzel.info  Thu May 14 09:47:07 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 14 May 2015 11:47:07 +0200
Subject: [squid-users] assertion Failed, when using dst_as
Message-ID: <55546F1B.10305@mathemainzel.info>

Hello,

following 3 lines

acl block_isps_list dst_as ###
deny_info ERR_ISP_BLOCKED block_isps_list
http_access deny block_isps_list

bring when starting squid like this
squid -N -d 1

the following messages
assertion failed: mem.cc:220: "MemPools[type]"
Aborted

my system: a VM with CentOS 6.5 (64-bit), using binary 3.4.10 from 
Eliezer Croitoru

when using without these 3 lines squid (with ssl-bump) runs stable 
without and problems;

Thanks in advance

Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150514/6bce5aae/attachment.bin>

From yvoinov at gmail.com  Thu May 14 21:45:07 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 15 May 2015 03:45:07 +0600
Subject: [squid-users] Youtube redirection loop?
In-Reply-To: <1431033905569-4671179.post@n4.nabble.com>
References: <55476DF0.6040008@gmail.com>
 <1430777255139-4671103.post@n4.nabble.com> <554B77CE.8060402@gmail.com>
 <1431032349609-4671177.post@n4.nabble.com> <554BD683.7050708@gmail.com>
 <1431033905569-4671179.post@n4.nabble.com>
Message-ID: <55551763.6020807@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
UPDATE:

It is enoudg to strip User-Agent only for one domain:

# For YT block useragent header
acl youtube_dom dstdomain .youtube.com
request_header_access User-Agent deny youtube_dom

Note: Some clips can't play for unknown reason. Will research.

08.05.15 3:25, HackXBack ?????:
> you are right, but this patch still work with me.
> i dont know if we can find better solution for this like you said by acl
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Youtube-redirection-loop-tp4671084p4671179.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVVRdjAAoJENNXIZxhPexGeQwIAIF2DN5+HxMWEgvZAQR3SGKi
Ex8FsDKGgZv3uE8NG+GOYqgM57GUVpSejaRh7teExEz6/jTXYMWWrdfruw5N4fq8
mga2R2233st/1JvckvE7HHGlSOR5LzVp412S1qLr2hRJNEqCSdy/7KT2V1fDiWDm
TThWEF5R5hP4GIW/rKCNANGZNqG5r2C165eV/Tr8+1QqOTO6pcd9lm34EoMf4Xk3
6f8RJRfaRvqZPlD98jpelGLIJQrlXC6EQze62K9LYIJUlU2Nt4E+gpog5y9Qcxyk
adFiphhMjy1s30489c3Jy9VLxEZnJzRw2nLhsGTGgl5e5WSWcB8rhsCbRlHllV4=
=mm7k
-----END PGP SIGNATURE-----



From mailinglist at cd.kcfam.net  Thu May 14 23:15:48 2015
From: mailinglist at cd.kcfam.net (Casey Daniels)
Date: Thu, 14 May 2015 19:15:48 -0400
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <5554267F.4070605@treenet.co.nz>
References: <555243B3.7000007@cd.kcfam.net> <5552FC64.9070108@treenet.co.nz>
 <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>
 <5554267F.4070605@treenet.co.nz>
Message-ID: <55552CA4.6080206@cd.kcfam.net>



On 05/14/2015 12:37 AM, Amos Jeffries wrote:
>> Yes the second option, not the particular machine, but the FQDN
>> (i.e.<http://www.cooking.com> )
>
>   # get TLS SNI details etc
>   ssl_bump peek all
>
>   # some get rejected
>   acl blocked ssl:server_name .example.com
>   ssl_bump reject blocked
>
>   # the rest allowed without decrypting
>   ssl_bump splice all
>
>
>> When is the TLS SNI information made available by the client?
> They send it or they dont. Nothign you or we can do about it.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

One Follow up question.

You said "They send it or they don't. Nothing you or we can do about 
it." Are you referring to that we don't have control if they send it or 
not, or there is nothing we can do if they don't?

My question is, is there some way to either reject the conection, or do 
a full SSL bump the connection for further examnation if the TLS SNI 
information isn't present?  From my understanding all modern browsers 
should be sending the TLS SNI information, and the SSL fallback has been 
disabled by default on them except for Windows IE.  So blocking 
connections that fail to give TLS SNI information doesn't appear to be a 
problem except for people using outdated devices.

Casey




From squid3 at treenet.co.nz  Thu May 14 23:47:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 May 2015 11:47:19 +1200
Subject: [squid-users] SSL Peak and Splice
In-Reply-To: <55552CA4.6080206@cd.kcfam.net>
References: <555243B3.7000007@cd.kcfam.net> <5552FC64.9070108@treenet.co.nz>
 <758160208.107150.1431557251442.JavaMail.open-xchange@oxuslxltgw04.lxa.perfora.net>
 <5554267F.4070605@treenet.co.nz> <55552CA4.6080206@cd.kcfam.net>
Message-ID: <55553407.2020000@treenet.co.nz>

On 15/05/2015 11:15 a.m., Casey Daniels wrote:
> 
> 
> On 05/14/2015 12:37 AM, Amos Jeffries wrote:
>>> Yes the second option, not the particular machine, but the FQDN
>>> (i.e.<http://www.cooking.com> )
>>
>>   # get TLS SNI details etc
>>   ssl_bump peek all
>>
>>   # some get rejected
>>   acl blocked ssl:server_name .example.com
>>   ssl_bump reject blocked
>>

Sorry, I see now that should have been:
 ssl_bump terminate blocked


>>   # the rest allowed without decrypting
>>   ssl_bump splice all
>>
>>
>>> When is the TLS SNI information made available by the client?
>> They send it or they dont. Nothign you or we can do about it.
>>
> 
> One Follow up question.
> 
> You said "They send it or they don't. Nothing you or we can do about
> it." Are you referring to that we don't have control if they send it or
> not, or there is nothing we can do if they don't?
> 

Both.


> My question is, is there some way to either reject the conection, or do
> a full SSL bump the connection for further examnation if the TLS SNI
> information isn't present?

Have a read through the "actions" list in
<http://wiki.squid-cache.org/Features/SslPeekAndSplice>.

In the above config snippet the "peek" action will get the server FQDN
from client SNI in intercepted traffic, or if it gets to step 2 the
server name from the certificate.


Write down in words the exact sequence of things you want Squid to do
and usually that will be what the config options look like.


>  From my understanding all modern browsers
> should be sending the TLS SNI information, and the SSL fallback has been
> disabled by default on them except for Windows IE.  So blocking
> connections that fail to give TLS SNI information doesn't appear to be a
> problem except for people using outdated devices.

Or the over-50% of web traffic that is not sent by browsers. SNI is a
relatively new feature and usage is growing, so YMWV.

Amos


From rafael.akchurin at diladele.com  Fri May 15 12:47:36 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 15 May 2015 12:47:36 +0000
Subject: [squid-users] Squid 3.5.4 for Microsoft Windows 64-bit is available
Message-ID: <1431694059813.73113@diladele.com>

Greetings everyone,

The CygWin based build of Squid proxy for Microsoft Windows version 3.5.4 is now available (amd64 only!).

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.4-RELEASENOTES.html.<http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.4-RELEASENOTES.html>
* Ready to use MSI package can be downloaded from http://squid.diladele.com.

Thanks a lot for Squid developers for making this great software.

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub - https://github.com/diladele/squid3-windows.<https://github.com/diladele/squid3-windows> Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com.

Best regards,
Rafael

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150515/763cd8d3/attachment.htm>

From jetsystemservices at gmail.com  Fri May 15 14:33:29 2015
From: jetsystemservices at gmail.com (Jose Torres-Berrocal)
Date: Fri, 15 May 2015 10:33:29 -0400
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <555425D0.6010707@treenet.co.nz>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
 <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
 <555357E7.2000004@gmail.com>
 <CABU5kW50Cw6B7ONq0Q88X+NbS3u76=8i+GAAGsu-HbFw2PotbA@mail.gmail.com>
 <555425D0.6010707@treenet.co.nz>
Message-ID: <CABU5kW4-Hp8VuY29nwhoduWTgaQyzLg5HSDt98eFJ+1xMoneHg@mail.gmail.com>

I willl try to find help on Ubuntu Forums how to compile it.

But I really would like to solve my squid 3.3.8 problem.  For which I
started this thread.
It may be bugy on SSL_BUMP but should work must of the time. It
compiles in Ubuntu as they do have an Ubuntu source for 3.3.8 version.

I need to find why is starting and terminated to fix the problem.  If
I succesfully compile 3.5.4 but the problem affects 3.5.4 also, then I
have accomplish nothing.

On Thu, May 14, 2015 at 12:34 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 14/05/2015 1:16 p.m., Jose Torres-Berrocal wrote:
>> Source from squid repository does not come directly compatible with
>> the OS.  Source from Ubuntu repository is made compatible to the OS.
>>
>> It has a folder call debian which has a file called rules which in
>> term is used with the configure script for the compile options.
>>
>> What file or script do I have to change in the squid repository source
>> to be able to use the debian/rules file?
>
> The debian/ folder contents for 3.3 are much different from the ones
> needed for other Squid versions.
>
> You are better off using the instructions at:
>  <http://wiki.squid-cache.org/KnowledgeBase/Ubuntu>
> to compile and install a new Squid binary. Like this command line:
>   ./configure (the options needed) && make install
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From Walter.H at mathemainzel.info  Fri May 15 18:14:19 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 15 May 2015 20:14:19 +0200
Subject: [squid-users] IPv6 and syntax?
Message-ID: <5556377B.7040109@mathemainzel.info>

Hello,

is IPv6 somewhat similar to IPv4?

e.g.

I would write

acl block_ipv4_range dst  84.84.84.0/24
deny_info errorpage block_ipv4_range
http_access deny block_ipv4_range

to block any hosts within this IPv4 range

how would be the syntax for blocking any hosts within a specific IPv6 subnet
e.g. [2408:8000::]/24

should it be this?

acl block_ipv6_subnet dst 2408:8000::/24
deny_info errorpage block_ipv6_subnet
http_access deny block_ipv6_subnet


Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150515/ae04de94/attachment.bin>

From hack.back at hotmail.com  Fri May 15 18:56:06 2015
From: hack.back at hotmail.com (HackXBack)
Date: Fri, 15 May 2015 11:56:06 -0700 (PDT)
Subject: [squid-users] squid stop working without any error
Message-ID: <1431716166638-4671242.post@n4.nabble.com>

in cache.log i found this,

2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 11185: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 14703: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 14416: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 12458: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 10336: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 597: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 6053: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 13730: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 11108: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 8037: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
connection on FD 14745: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
alert unknown ca (1/0)
2015/05/15 21:06:41 kid1| ctx: enter level  0:
'http://storeid.cdn.fbcdn/p100x100/10348376_806835629388827_6352898774493962027_n.png'
2015/05/15 21:06:41 kid1| Closing HTTP port 0.0.0.0:3129
2015/05/15 21:06:41 kid1| Closing HTTP port 0.0.0.0:3128
2015/05/15 21:06:41 kid1| Closing HTTPS port 0.0.0.0:3127
2015/05/15 21:06:41 kid1| storeDirWriteCleanLogs: Starting...
2015/05/15 21:06:41 kid1|     65536 entries written so far.
2015/05/15 21:06:41 kid1|    131072 entries written so far.
2015/05/15 21:06:41 kid1|    196608 entries written so far.
2015/05/15 21:06:41 kid1|    262144 entries written so far.
2015/05/15 21:06:41 kid1|    327680 entries written so far.
2015/05/15 21:06:41 kid1|    393216 entries written so far.
2015/05/15 21:06:41 kid1|    458752 entries written so far.
2015/05/15 21:06:42 kid1|    524288 entries written so far.
2015/05/15 21:06:42 kid1|    589824 entries written so far.
2015/05/15 21:06:42 kid1|    655360 entries written so far.
2015/05/15 21:06:42 kid1|    720896 entries written so far.
2015/05/15 21:06:42 kid1|    786432 entries written so far.







after rebuilding squid stop working , when i start it again it work for
couple of hours then the same ...

and in the end of cache.log i found this

2015/05/15 22:09:39 kid1| Rebuilding storage in /cache05/4 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/1 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/2 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/3 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/4 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/1 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/2 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/3 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/4 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/1 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/2 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/3 (dirty log)
2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/4 (dirty log)
2015/05/15 22:09:39 kid1| Using Least Load store dir selection
2015/05/15 22:09:39 kid1| Set Current Directory to /var/spool/squid
2015/05/15 22:09:39 kid1| Finished loading MIME types and icons.



and no backtrace report found ...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-stop-working-without-any-error-tp4671242.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Fri May 15 19:17:16 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 16 May 2015 01:17:16 +0600
Subject: [squid-users] squid stop working without any error
In-Reply-To: <1431716166638-4671242.post@n4.nabble.com>
References: <1431716166638-4671242.post@n4.nabble.com>
Message-ID: <5556463C.5010905@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Because this is not assert.

alert unknown ca




16.05.15 0:56, HackXBack ?????:
> in cache.log i found this,
>
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 11185: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 14703: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 14416: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 12458: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 10336: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 597: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 6053: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 13730: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 11108: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 8037: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| clientNegotiateSSL: Error negotiating SSL
> connection on FD 14745: error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1
> alert unknown ca (1/0)
> 2015/05/15 21:06:41 kid1| ctx: enter level  0:
>
'http://storeid.cdn.fbcdn/p100x100/10348376_806835629388827_6352898774493962027_n.png'
> 2015/05/15 21:06:41 kid1| Closing HTTP port 0.0.0.0:3129
> 2015/05/15 21:06:41 kid1| Closing HTTP port 0.0.0.0:3128
> 2015/05/15 21:06:41 kid1| Closing HTTPS port 0.0.0.0:3127
> 2015/05/15 21:06:41 kid1| storeDirWriteCleanLogs: Starting...
> 2015/05/15 21:06:41 kid1|     65536 entries written so far.
> 2015/05/15 21:06:41 kid1|    131072 entries written so far.
> 2015/05/15 21:06:41 kid1|    196608 entries written so far.
> 2015/05/15 21:06:41 kid1|    262144 entries written so far.
> 2015/05/15 21:06:41 kid1|    327680 entries written so far.
> 2015/05/15 21:06:41 kid1|    393216 entries written so far.
> 2015/05/15 21:06:41 kid1|    458752 entries written so far.
> 2015/05/15 21:06:42 kid1|    524288 entries written so far.
> 2015/05/15 21:06:42 kid1|    589824 entries written so far.
> 2015/05/15 21:06:42 kid1|    655360 entries written so far.
> 2015/05/15 21:06:42 kid1|    720896 entries written so far.
> 2015/05/15 21:06:42 kid1|    786432 entries written so far.
>
>
>
>
>
>
>
> after rebuilding squid stop working , when i start it again it work for
> couple of hours then the same ...
>
> and in the end of cache.log i found this
>
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache05/4 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/1 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/2 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/3 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache06/4 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/1 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/2 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/3 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache07/4 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/1 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/2 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/3 (dirty log)
> 2015/05/15 22:09:39 kid1| Rebuilding storage in /cache08/4 (dirty log)
> 2015/05/15 22:09:39 kid1| Using Least Load store dir selection
> 2015/05/15 22:09:39 kid1| Set Current Directory to /var/spool/squid
> 2015/05/15 22:09:39 kid1| Finished loading MIME types and icons.
>
>
>
> and no backtrace report found ...
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-stop-working-without-any-error-tp4671242.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVVkY8AAoJENNXIZxhPexGbzIIAL589XWIR4Ze9UHceRuYDJb1
CHodgTQRjoMnFFjQ9zhchPVpmGqvIwcu/fxkGOLB5FXGNwVVo4ALD7dRgq0grHVc
Fpw8q9Ukloa25RoeKyz79sWcQcFx/FwO0/KPbuMD8CmlBLl7Tt0A3OqC7mpPbFsj
s883EAb7eD+PDargnNDCqCnlzPvRgiGWgNZsxCjV8FoZTRs4uZM+efI4yHMD0Zhc
L4jFoIe4n7XXR7LWpvWoI5oSMAhKp0cCDGvgGMKOjj9Wq1iBjD8KSs++BFjGVsFF
xWLe7sN+YldAP0XT9TMRh5/YkyYXx/Su57GhEaElTkzizmvi0I7nfKafbqdO5WI=
=gVdl
-----END PGP SIGNATURE-----



From squid3 at treenet.co.nz  Fri May 15 23:41:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 May 2015 11:41:58 +1200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <5556377B.7040109@mathemainzel.info>
References: <5556377B.7040109@mathemainzel.info>
Message-ID: <55568446.7030909@treenet.co.nz>

On 16/05/2015 6:14 a.m., Walter H. wrote:
> Hello,
> 
> is IPv6 somewhat similar to IPv4?

Somewhat, yes.

> 
> e.g.
> 
> I would write
> 
> acl block_ipv4_range dst  84.84.84.0/24
> deny_info errorpage block_ipv4_range
> http_access deny block_ipv4_range
> 
> to block any hosts within this IPv4 range

Taking a step asside, that is not quite what those rules do. They block
access from anywhere *to* the IP address range (TCP/IP packet
destination on the request messages).

If you were trying to prevent those hosts themselves from accessing
anything through the proxy you need the "src" ACL type.


> 
> how would be the syntax for blocking any hosts within a specific IPv6
> subnet
> e.g. [2408:8000::]/24

FYI the [] syntax is URL format - for uses when a port may exist. So the
':' between IP:port dont get confused.

> 
> should it be this?
> 
> acl block_ipv6_subnet dst 2408:8000::/24
> deny_info errorpage block_ipv6_subnet
> http_access deny block_ipv6_subnet

Yes. Though the /N CIDR range is probably different. An IPv4 /24 is
equivalent to an IPv6 /52  (255 separate pieces of hardware with a
mandatory /64 each).

Amos


From squid3 at treenet.co.nz  Sat May 16 00:28:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 May 2015 12:28:14 +1200
Subject: [squid-users] Need help debugging my squid configuration
In-Reply-To: <CABU5kW4-Hp8VuY29nwhoduWTgaQyzLg5HSDt98eFJ+1xMoneHg@mail.gmail.com>
References: <CABU5kW7juAy61VgNcujLszPw0jhLvVT5DnCbmMi+wktFYZx3Cg@mail.gmail.com>
 <5552FE71.2010608@treenet.co.nz>
 <CABU5kW57aDh4XZrLy6xCthgm1ZDo8M_-owrOb+8D15FCMGx8Bg@mail.gmail.com>
 <CABU5kW7c84bQttf9rTiS77ba0bw7C+NVcNRphOKcNNNB=Ln+DA@mail.gmail.com>
 <555357E7.2000004@gmail.com>
 <CABU5kW50Cw6B7ONq0Q88X+NbS3u76=8i+GAAGsu-HbFw2PotbA@mail.gmail.com>
 <555425D0.6010707@treenet.co.nz>
 <CABU5kW4-Hp8VuY29nwhoduWTgaQyzLg5HSDt98eFJ+1xMoneHg@mail.gmail.com>
Message-ID: <55568F1E.1070009@treenet.co.nz>

On 16/05/2015 2:33 a.m., Jose Torres-Berrocal wrote:
> I willl try to find help on Ubuntu Forums how to compile it.
> 

I linked to the Ubuntu instructions page in my last email.

"
  <http://wiki.squid-cache.org/KnowledgeBase/Ubuntu>
 to compile and install a new Squid binary. Like this command line:
   ./configure (the options needed) && make install
"

The section "compiling" on that page lists the mandatory ./configure
options required for system integration on Debian/Ubuntu systems.

The output of "squid -v" (or "squid3 -v" if its that old) command will
tell you what else the distributor built with. It is up to you which of
those others you choose to use for your own custom build.


> But I really would like to solve my squid 3.3.8 problem.  For which I
> started this thread.
> It may be bugy on SSL_BUMP but should work must of the time. It
> compiles in Ubuntu as they do have an Ubuntu source for 3.3.8 version.

I can tell you right now that even if you get all this going 3.3 will
still not be able to bump the traffic from many major websites when the
browser is Chrome or Firefox.

TLS was designed to be an unbreakable protection - when properly used it
is. They only reason it can be broken at all today is bad
implementations by browsers/clients and servers. As the flaws get fixed
(and they are slowly) bumping ceases to work. The attacking technique(s)
coded into 3.3 have already been defended against by those browsers and
the major popular websites.


> 
> I need to find why is starting and terminated to fix the problem.  If
> I succesfully compile 3.5.4 but the problem affects 3.5.4 also, then I
> have accomplish nothing.

You will have accomplished proof that the bug still exists and we have a
reason to help you fix it. We will then fix it *in the current code*.

 - the SSL related code has had 3 major re-writes and several major
feature alterations since 3.3, and

 - many people actively use the code between 3.3.8 and 3.5.4 without
seeing this same problem.

So chances of the same bug existing in the SSL logics is quite small.
Though you may hit other bugs.


If you fix it yourself, the version number must change and suddenly you
have a different packege - the distro one is still broken. You might as
well just use the already fixed latest sources, and have the support
that comes with doing so.

Amos



From Walter.H at mathemainzel.info  Sat May 16 06:22:04 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 16 May 2015 08:22:04 +0200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <55568446.7030909@treenet.co.nz>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz>
Message-ID: <5556E20C.4090409@mathemainzel.info>

On 16.05.2015 01:41, Amos Jeffries wrote:
> On 16/05/2015 6:14 a.m., Walter H. wrote:
>> Hello,
>>
>> is IPv6 somewhat similar to IPv4?
> Somewhat, yes.
I just wondered because of the "different" behaviour;
>> e.g.
>>
>> I would write
>>
>> acl block_ipv4_range dst  84.84.84.0/24
>> deny_info errorpage block_ipv4_range
>> http_access deny block_ipv4_range
>>
>> to block any hosts within this IPv4 range
> Taking a step asside, that is not quite what those rules do. They block
> access from anywhere *to* the IP address range (TCP/IP packet
> destination on the request messages).
>
yes this should be the intention, that you get an error (in this case 
the errorpage) when
you have e.g.  http://84.84.84.2/ or https://84.84.84.2/ as URL in your 
browser ...
> If you were trying to prevent those hosts themselves from accessing
> anything through the proxy you need the "src" ACL type.
I know;
>> how would be the syntax for blocking any hosts within a specific IPv6
>> subnet
>> e.g. [2408:8000::]/24
> FYI the [] syntax is URL format - for uses when a port may exist. So the
> ':' between IP:port dont get confused.
>
I noticed the difference, but wondered why e.g. /etc/hosts.deny contains 
this:
sshd: [2408:8000::]/24

>> should it be this?
>>
>> acl block_ipv6_subnet dst 2408:8000::/24
>> deny_info errorpage block_ipv6_subnet
>> http_access deny block_ipv6_subnet
> Yes. Though the /N CIDR range is probably different. An IPv4 /24 is
> equivalent to an IPv6 /52  (255 separate pieces of hardware with a
> mandatory /64 each).
>
why I'm asking, because; when having both sections in squid.conf and 
doing SSL-bump
you get a different reaction in the browser:

https://84.84.84.22/
brings the 'errorpage' as expected
the generated certificate has the IP-address (84.84.84.22) as its common 
name;

but
https://[2408:8000::3]/
behaves different in various browsers:

- IE 7: brings a certificate error, when accepting you get the errorpage
            the generated certificate has the IP-address 2408:8000::3 as 
its common name

- later FF (17+) do nothing, older FF (3.6) bring
        "The proxy server is refusing connections
        Firefox is configured to use a proxy server that is refusing 
connections."

- Chrome 42 brings ' Your connection is not private' and 
NET::ERR_CERT_COMMON_NAME_INVALID
      when clicking advanced and proceed with warning you get the errorpage
            the generated certificate has the IP-address 2408:8000::3 as 
its common name

trying https://[2408:8000:0:0:0:0:0:3]/  does an automatic reduction to 
https://[2408:8000::3]/ by the browser

does it seem to be problematic, when having an TLS-server with an IPv6 
address only without DNS, because of the comm name?

Thanks,
Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150516/155e4ca4/attachment.bin>

From squid3 at treenet.co.nz  Sat May 16 08:13:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 May 2015 20:13:59 +1200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <5556E20C.4090409@mathemainzel.info>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz> <5556E20C.4090409@mathemainzel.info>
Message-ID: <5556FC47.4040409@treenet.co.nz>

On 16/05/2015 6:22 p.m., Walter H. wrote:
> On 16.05.2015 01:41, Amos Jeffries wrote:
>> On 16/05/2015 6:14 a.m., Walter H. wrote:
>>> Hello,
>>>
>>> is IPv6 somewhat similar to IPv4?
>> Somewhat, yes.
> I just wondered because of the "different" behaviour;
>>> e.g.
>>>
>>> I would write
>>>
>>> acl block_ipv4_range dst  84.84.84.0/24
>>> deny_info errorpage block_ipv4_range
>>> http_access deny block_ipv4_range
>>>
>>> to block any hosts within this IPv4 range
>> Taking a step asside, that is not quite what those rules do. They block
>> access from anywhere *to* the IP address range (TCP/IP packet
>> destination on the request messages).
>>
> yes this should be the intention, that you get an error (in this case
> the errorpage) when
> you have e.g.  http://84.84.84.2/ or https://84.84.84.2/ as URL in your
> browser ...

It will block that, and any domain name which resolves to those IPs.


>> If you were trying to prevent those hosts themselves from accessing
>> anything through the proxy you need the "src" ACL type.
> I know;
>>> how would be the syntax for blocking any hosts within a specific IPv6
>>> subnet
>>> e.g. [2408:8000::]/24
>> FYI the [] syntax is URL format - for uses when a port may exist. So the
>> ':' between IP:port dont get confused.
>>
> I noticed the difference, but wondered why e.g. /etc/hosts.deny contains
> this:
> sshd: [2408:8000::]/24

It differs by program. Depends on how v6-experienced the developer who
wrote the code was. There are some new to IPv6 seems to think they
always have to use []. Same as there are people who believe every domain
name begins with www, or email addresses have to contain alphanumerics.

> 
>>> should it be this?
>>>
>>> acl block_ipv6_subnet dst 2408:8000::/24
>>> deny_info errorpage block_ipv6_subnet
>>> http_access deny block_ipv6_subnet
>> Yes. Though the /N CIDR range is probably different. An IPv4 /24 is
>> equivalent to an IPv6 /52  (255 separate pieces of hardware with a
>> mandatory /64 each).
>>
> why I'm asking, because; when having both sections in squid.conf and
> doing SSL-bump
> you get a different reaction in the browser:
> 
> https://84.84.84.22/
> brings the 'errorpage' as expected
> the generated certificate has the IP-address (84.84.84.22) as its common
> name;
> 
> but
> https://[2408:8000::3]/
> behaves different in various browsers:
> 
> - IE 7: brings a certificate error, when accepting you get the errorpage
>            the generated certificate has the IP-address 2408:8000::3 as
> its common name
> 
> - later FF (17+) do nothing, older FF (3.6) bring
>        "The proxy server is refusing connections
>        Firefox is configured to use a proxy server that is refusing
> connections."
> 
> - Chrome 42 brings ' Your connection is not private' and
> NET::ERR_CERT_COMMON_NAME_INVALID
>      when clicking advanced and proceed with warning you get the errorpage
>            the generated certificate has the IP-address 2408:8000::3 as
> its common name
> 
> trying https://[2408:8000:0:0:0:0:0:3]/  does an automatic reduction to
> https://[2408:8000::3]/ by the browser
> 
> does it seem to be problematic, when having an TLS-server with an IPv6
> address only without DNS, because of the comm name?

That is a different issue entirely.

X.509 certificate format explicitly states that the field contains the
domain name registered to the entity the certificate was issued to.
There are certain things that are done with it that can only be done
with a domain name (wildcard validation, domain name comparisons, etc).

Going by that description it seems Firefox and Chrome are a bit broken.

Amos



From Walter.H at mathemainzel.info  Sat May 16 11:09:28 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 16 May 2015 13:09:28 +0200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <5556FC47.4040409@treenet.co.nz>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz> <5556E20C.4090409@mathemainzel.info>
 <5556FC47.4040409@treenet.co.nz>
Message-ID: <55572568.7050905@mathemainzel.info>

On 16.05.2015 10:13, Amos Jeffries wrote:
> On 16/05/2015 6:22 p.m., Walter H. wrote:
>> On 16.05.2015 01:41, Amos Jeffries wrote:
>>> On 16/05/2015 6:14 a.m., Walter H. wrote:
>>>> Hello,
>>>>
>>>> is IPv6 somewhat similar to IPv4?
>>> Somewhat, yes.
>> I just wondered because of the "different" behaviour;
>>>> e.g.
>>>>
>>>> I would write
>>>>
>>>> acl block_ipv4_range dst  84.84.84.0/24
>>>> deny_info errorpage block_ipv4_range
>>>> http_access deny block_ipv4_range
>>>>
>>>> to block any hosts within this IPv4 range
>>> Taking a step asside, that is not quite what those rules do. They block
>>> access from anywhere *to* the IP address range (TCP/IP packet
>>> destination on the request messages).
>>>
>> yes this should be the intention, that you get an error (in this case
>> the errorpage) when
>> you have e.g.  http://84.84.84.2/ or https://84.84.84.2/ as URL in your
>> browser ...
> It will block that, and any domain name which resolves to those IPs.
>
yes, that is the intention;

I would have done it this way:

acl block_whole_network dst_as 4837
deny_info errorpage block_whole_network
http_access deny block_whole_network

but this crashes squid ...

as workaround I've got a file listing any range for one AS number
and doing this:

acl block_as4837 dst "block-as4837-acl.squid"

and one of these files has more than 600(!) entries ...

>> does it seem to be problematic, when having an TLS-server with an IPv6
>> address only without DNS, because of the comm name?
> That is a different issue entirely.
yes and hoping no browser ever will accept a common name of just '*'
> Going by that description it seems Firefox and Chrome are a bit broken.
IE, too;

Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150516/7ee554f7/attachment.bin>

From vdoctor at neuf.fr  Sun May 17 08:39:35 2015
From: vdoctor at neuf.fr (Stakres)
Date: Sun, 17 May 2015 01:39:35 -0700 (PDT)
Subject: [squid-users] Cache peers with different load
In-Reply-To: <5550CA62.3070502@treenet.co.nz>
References: <1431345695914-4671204.post@n4.nabble.com>
 <5550AFB2.4070401@treenet.co.nz> <1431354524713-4671207.post@n4.nabble.com>
 <5550CA62.3070502@treenet.co.nz>
Message-ID: <1431851975568-4671249.post@n4.nabble.com>

Hi Amos,
Thanks for the explainations 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-peers-with-different-load-tp4671204p4671249.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid-users at sernet.de  Mon May 18 11:17:40 2015
From: squid-users at sernet.de (Stefan Kuegler)
Date: Mon, 18 May 2015 13:17:40 +0200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <5548D92F.9070308@gmail.com>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
Message-ID: <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE>

Hi Yuri.
>
> http://i.imgur.com/mW7gNwD.png
>
> http://squidclamav.darold.net/config.html
>
> This is for squidclamav (I use it and have no problems with malware).

I just installed squidclamav - but the behaviour is always the same. An 
object which has been stored in squid-cache will not be detected by an 
icap server because squid does not scan the body again:

squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing 
request data handler.
pool hits:5 allocations: 1
Allocating from objects pool object 0
Requested service: squidclamav
squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing 
preview header.
squidclamav.c(358) squidclamav_check_preview_handler: DEBUG X-Client-IP: 
192.168.216.54
squidclamav.c(1319) extract_http_info: DEBUG method GET
squidclamav.c(1330) extract_http_info: DEBUG url 
http://www.intern/eicar_com.zip
squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL 
requested: http://www.intern/eicar_com.zip
squidclamav.c(430) squidclamav_check_preview_handler: DEBUG 
Content-Length: 0
squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body 
data, allow 204
squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing 
request data.
Storing to objects pool object 0
Log request to access log file /var/log/c-icap/access.log
Width: 0, Parameter:

Any idea, how I can solve that problem. It seems that the only way to be 
secure is to disable caching in squid. But I hope, this can't be the 
solution.

Regards,
Stefan
>
> 05.05.15 17:45, Stefan K?gler ?????:
>> Hi Yuri.
>>
>> Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
>>> This is not squid issue but your AV engine library or ICAP intermediate
>>> AV library configuration.
>>
>> Thank you for your answer.
>>
>> Can you explain me a litte bit more detailed why this is not a squid
> issue?
>>
>> In the icap-logfile, I can see a REQMOD-request _AND_ a
> RESPMOD-request to the icap-server if the object is not in cache.
>>
>> But - if the object is in cache - I can only see a REQMOD-request to
> the icap-server. I am missing RESPMOD.
>>
>> It seems to me, that it is a decision of the client (squid) which
> request (REQMOD or RESPMOD) will be send to the icap-server (AV-scanner)
> - and not a decision of the av-library.
>>
>> Regards, Stefan
>>
>>>
>>> 05.05.15 16:43, Stefan K?gler ?????:
>>>> Hello.
>>>>
>>>>
>>>> I have a short question using squid as an ICAP-client.
>>>>
>>>>
>>>> It seems that squid doesn't send an already downloaded (and cached)
>>>> object to an ICAP-server.
>>>>
>>>> Here is a short description what I have done:
>>>>
>>>> 1. downloading a word-document with a macro-virus. The Virus-scanner
>>>> (ICAP-server) uses an old pattern-file and does not detect the virus.
>>>>
>>>> The object is now in cache.
>>>>
>>>> 2. updating the virus-scanner to the newest pattern-file. The
>>>> virus-scanner will now detect the macro virus.
>>>>
>>>> 3. downloading the same word-document. The object has been delivered
>>>> to the client without a new virus scan.
>>>>
>>>>
>>>>
>>>> And now some log-entries:
>>>>
>>>> 1. First download of the word document:
>>>>
>>>> access.log:
>>>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>>>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>>>> application/msword
>>>>
>>>> icap.log:
>>>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>
>>>> AV-Scanner:
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>> ICAP request decoding
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>>>> message decoded in 1 chunks
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>> ICAP request decoding
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>> ICAP request processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>> service processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>>>> processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at
>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>> service processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request
>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>> Details: '')
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>>>> response headers type: CLEAN 204
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send headers
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>> ICAP request processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core library
>>>> session cleared
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection
>>>> closed by foreign host while waiting for requests
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core library
>>>> session cleared
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>> ICAP request decoding
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>>>> message decoded in 259 chunks
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>> ICAP request decoding
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>> ICAP request processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>> service processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>>>> processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>> HTTP/1.1>
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>> HTTP/1.1>
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>>>> [service_scanner]File 'virus.doc' content is stored in
>>>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>> service processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request
>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>> Details: '')
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>>>> response headers type: CLEAN
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP
>>>> headers for response type: CLEAN
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send headers
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>>>> original body (552960 bytes)
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>> ICAP request processing
>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core library
>>>> session cleared
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> 2. Second download of the word document (after the pattern-update):
>>>>
>>>> access.log:
>>>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>>>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>>>
>>>> icap.log:
>>>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>
>>>> AV-Scanner:
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>> ICAP request decoding
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>>>> message decoded in 1 chunks
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>> ICAP request decoding
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>> ICAP request processing
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>> service processing
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>>>> processing
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at
>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>> service processing
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request
>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>> Details: '')
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>>>> response headers type: CLEAN 204
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send headers
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>> ICAP request processing
>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core library
>>>> session cleared
>>>>
>>>>
>>>> And now my question: Is this a bug in squid - or is it possible to
>>>> tell squid to send already cached object to the icap-server?
>>>>
>>>> Kind regards,
>>>>
>>>> Stefan Kuegler
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQEcBAEBCAAGBQJVSNkvAAoJENNXIZxhPexGsh8IAJGL1gSY3rzshF+BeHmsqZIJ
> 4L0y2fjrQ66Q8Jz8fKk5saSemIdDRigH0fPAt4Bbb8cVnMcniP09cZ/lspaz3NxA
> blodVyDYSLnmWIYzFfg19nd3UWDgIq4yOz3/rXCmHEkQ5sXrJQhJeP4Azeyez4Zj
> Qef9ae75cbHexa12U8KERr9SDSnN18tRt4SPz8ZRaoYsoqIC4WRfkO8a0NPfHJp0
> cYVj8pwHwbz5TPzYpPrGRR/rPbeO5FOVlIDVrxdHbafLjeYofVR8UOnKn67dxIVu
> MJuunsVNtbPaWcDaGkUQ5Z8vvebGDB3pRPNm8XHXp7idGoDTQFJ6JbdK7ofA6do=
> =VGI/
> -----END PGP SIGNATURE-----
>

Viele Gr??e - Stefan K?gler
SerNet GmbH
-- 


From vkukk at xvidservices.com  Mon May 18 11:23:16 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Mon, 18 May 2015 14:23:16 +0300
Subject: [squid-users] Squid 3.4.10 and sslcrtd
Message-ID: <5559CBA4.5080206@xvidservices.com>

Hi

I'd like to know if I understand Squid documentation properly.
I have following http_port and sslbump configuration:

http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=off 
cert=/var/spool/squid/ssl_cert/squid_ca.pem
ssl_bump server-first all

 From documentation:
generate-host-certificates[=<on|off>]
Dynamically create SSL server certificates for the destination hosts of 
bumped CONNECT requests. When enabled, the cert and key options are used 
to sign generated certificates. Otherwise generated certificate will be 
selfsigned.

I guess, that means, if generate-host-certificates=off, there is no need 
for sslcrtd_program. Do I understand this correctly?

Unfortunately, Squid exits with fatal error when trying to start without 
sslcrtd_program configuration option.

2015/05/18 11:10:40 kid1| Accepting SSL bumped HTTP Socket connections 
at local=127.0.0.1:3128 remote=[::] FD 27 flags=9
2015/05/18 11:10:40 kid1| Done reading /var/spool/squid swaplog (0 entries)
2015/05/18 11:10:40 kid1| Store rebuilding is 0.00% complete
2015/05/18 11:10:40 kid1| Finished rebuilding storage from disk.
2015/05/18 11:10:40 kid1|         0 Entries scanned
2015/05/18 11:10:40 kid1|         0 Invalid entries.
2015/05/18 11:10:40 kid1|         0 With invalid flags.
2015/05/18 11:10:40 kid1|         0 Objects loaded.
2015/05/18 11:10:40 kid1|         0 Objects expired.
2015/05/18 11:10:40 kid1|         0 Objects cancelled.
2015/05/18 11:10:40 kid1|         0 Duplicate URLs purged.
2015/05/18 11:10:40 kid1|         0 Swapfile clashes avoided.
2015/05/18 11:10:40 kid1|   Took 0.01 seconds (  0.00 objects/sec).
2015/05/18 11:10:40 kid1| Beginning Validation Procedure
2015/05/18 11:10:40 kid1|   Completed Validation Procedure
2015/05/18 11:10:40 kid1|   Validated 0 Entries
2015/05/18 11:10:40 kid1|   store_swap_size = 0.00 KB
2015/05/18 11:10:40 kid1| WARNING: ssl_crtd #Hlpr0 exited
2015/05/18 11:10:40 kid1| Too few ssl_crtd processes are running (need 1/32)
2015/05/18 11:10:40 kid1| Closing HTTP port 127.0.0.1:3128
2015/05/18 11:10:40 kid1| storeDirWriteCleanLogs: Starting...
2015/05/18 11:10:40 kid1|   Finished.  Wrote 0 entries.
2015/05/18 11:10:40 kid1|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

Why does it still need sslcrtd_program? Note that error message WARNING: 
ssl_crtd #Hlpr0 exited is misleading, because currently, all sslcrtd 
related configuration options are commented out and none of the ssl_crtd 
processes are started.

Best regards,
Veiko


From squid3 at treenet.co.nz  Mon May 18 11:25:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 May 2015 23:25:38 +1200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <55572568.7050905@mathemainzel.info>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz> <5556E20C.4090409@mathemainzel.info>
 <5556FC47.4040409@treenet.co.nz> <55572568.7050905@mathemainzel.info>
Message-ID: <5559CC32.5040105@treenet.co.nz>

On 16/05/2015 11:09 p.m., Walter H. wrote:
> On 16.05.2015 10:13, Amos Jeffries wrote:
>> On 16/05/2015 6:22 p.m., Walter H. wrote:
>>> On 16.05.2015 01:41, Amos Jeffries wrote:
>>>> On 16/05/2015 6:14 a.m., Walter H. wrote:
>>>>> Hello,
>>>>>
>>>>> is IPv6 somewhat similar to IPv4?
>>>> Somewhat, yes.
>>> I just wondered because of the "different" behaviour;
>>>>> e.g.
>>>>>
>>>>> I would write
>>>>>
>>>>> acl block_ipv4_range dst  84.84.84.0/24
>>>>> deny_info errorpage block_ipv4_range
>>>>> http_access deny block_ipv4_range
>>>>>
>>>>> to block any hosts within this IPv4 range
>>>> Taking a step asside, that is not quite what those rules do. They block
>>>> access from anywhere *to* the IP address range (TCP/IP packet
>>>> destination on the request messages).
>>>>
>>> yes this should be the intention, that you get an error (in this case
>>> the errorpage) when
>>> you have e.g.  http://84.84.84.2/ or https://84.84.84.2/ as URL in your
>>> browser ...
>> It will block that, and any domain name which resolves to those IPs.
>>
> yes, that is the intention;
> 
> I would have done it this way:
> 
> acl block_whole_network dst_as 4837
> deny_info errorpage block_whole_network
> http_access deny block_whole_network
> 
> but this crashes squid ...

Ouch. Is that the <http://bugs.squid-cache.org/show_bug.cgi?id=3579> crash?

I would like to fix that, but need the backtrace.


> 
> as workaround I've got a file listing any range for one AS number
> and doing this:
> 
> acl block_as4837 dst "block-as4837-acl.squid"
> 
> and one of these files has more than 600(!) entries ...
> 
>>> does it seem to be problematic, when having an TLS-server with an IPv6
>>> address only without DNS, because of the comm name?
>> That is a different issue entirely.
> yes and hoping no browser ever will accept a common name of just '*'
>> Going by that description it seems Firefox and Chrome are a bit broken.
> IE, too;

IE is doing the right thing in your description. That cert-with-IP
warning is the correct / working behaviour. The Firefox hang and Chrome
"insecure" warning are the broken bits.

Amos



From squid3 at treenet.co.nz  Mon May 18 11:39:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 May 2015 23:39:56 +1200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE>
Message-ID: <5559CF8C.7080106@treenet.co.nz>

On 18/05/2015 11:17 p.m., Stefan Kuegler wrote:
> Hi Yuri.
>>
>> http://i.imgur.com/mW7gNwD.png
>>
>> http://squidclamav.darold.net/config.html
>>
>> This is for squidclamav (I use it and have no problems with malware).
> 
> I just installed squidclamav - but the behaviour is always the same. An
> object which has been stored in squid-cache will not be detected by an
> icap server because squid does not scan the body again:
> 
> squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing
> request data handler.
> pool hits:5 allocations: 1
> Allocating from objects pool object 0
> Requested service: squidclamav
> squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing
> preview header.
> squidclamav.c(358) squidclamav_check_preview_handler: DEBUG X-Client-IP:
> 192.168.216.54
> squidclamav.c(1319) extract_http_info: DEBUG method GET
> squidclamav.c(1330) extract_http_info: DEBUG url
> http://www.intern/eicar_com.zip
> squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL
> requested: http://www.intern/eicar_com.zip
> squidclamav.c(430) squidclamav_check_preview_handler: DEBUG
> Content-Length: 0
> squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body
> data, allow 204
> squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing
> request data.
> Storing to objects pool object 0
> Log request to access log file /var/log/c-icap/access.log
> Width: 0, Parameter:
> 
> Any idea, how I can solve that problem. It seems that the only way to be
> secure is to disable caching in squid. But I hope, this can't be the
> solution.


see my earlier reply.

Amos



From yan at seiner.com  Mon May 18 11:55:17 2015
From: yan at seiner.com (Yan Seiner)
Date: Mon, 18 May 2015 04:55:17 -0700
Subject: [squid-users] Saving memory cache to disk on reboot?
Message-ID: <5559D325.10804@seiner.com>

The title says it all - is it possible to save the memory cache to disk 
on reboot?

I reboot my systems weekly and I wonder if this would be any advantage.


From yvoinov at gmail.com  Mon May 18 12:01:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 May 2015 18:01:28 +0600
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE>
Message-ID: <5559D498.1080507@gmail.com>

http://squidclamav.darold.net/config.html


        Trust your cache (obsolete/unused in v6.x)

One of the main configuration directive for performance improvement is 
'trust_cache'. SquidClamav detect if the file to download is already 
stored in Squid cache. If you activate 'trust_cache', SquidClamav will 
not scan a file comming from Squid cache as it may have already been 
scanned during the first download. If trust_cache is disabled, no matter 
if the file is stored in the cache, SquidClamav will rescan the same 
file at each client request. I really recommand you to activate this 
directive.

	trust_cache 0

Trusted cache is disable by default as you may want to start with a 
fresh cache.


Why you need rescan cached object again? You don't trust your cache? Or 
what?

18.05.15 17:17, Stefan Kuegler ?????:
> Hi Yuri.
>>
>> http://i.imgur.com/mW7gNwD.png
>>
>> http://squidclamav.darold.net/config.html
>>
>> This is for squidclamav (I use it and have no problems with malware).
>
> I just installed squidclamav - but the behaviour is always the same. 
> An object which has been stored in squid-cache will not be detected by 
> an icap server because squid does not scan the body again:
>
> squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing 
> request data handler.
> pool hits:5 allocations: 1
> Allocating from objects pool object 0
> Requested service: squidclamav
> squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing 
> preview header.
> squidclamav.c(358) squidclamav_check_preview_handler: DEBUG 
> X-Client-IP: 192.168.216.54
> squidclamav.c(1319) extract_http_info: DEBUG method GET
> squidclamav.c(1330) extract_http_info: DEBUG url 
> http://www.intern/eicar_com.zip
> squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL 
> requested: http://www.intern/eicar_com.zip
> squidclamav.c(430) squidclamav_check_preview_handler: DEBUG 
> Content-Length: 0
> squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body 
> data, allow 204
> squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing 
> request data.
> Storing to objects pool object 0
> Log request to access log file /var/log/c-icap/access.log
> Width: 0, Parameter:
>
> Any idea, how I can solve that problem. It seems that the only way to 
> be secure is to disable caching in squid. But I hope, this can't be 
> the solution.
>
> Regards,
> Stefan
>>
>> 05.05.15 17:45, Stefan K?gler ?????:
>>> Hi Yuri.
>>>
>>> Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
>>>> This is not squid issue but your AV engine library or ICAP 
>>>> intermediate
>>>> AV library configuration.
>>>
>>> Thank you for your answer.
>>>
>>> Can you explain me a litte bit more detailed why this is not a squid
>> issue?
>>>
>>> In the icap-logfile, I can see a REQMOD-request _AND_ a
>> RESPMOD-request to the icap-server if the object is not in cache.
>>>
>>> But - if the object is in cache - I can only see a REQMOD-request to
>> the icap-server. I am missing RESPMOD.
>>>
>>> It seems to me, that it is a decision of the client (squid) which
>> request (REQMOD or RESPMOD) will be send to the icap-server (AV-scanner)
>> - and not a decision of the av-library.
>>>
>>> Regards, Stefan
>>>
>>>>
>>>> 05.05.15 16:43, Stefan K?gler ?????:
>>>>> Hello.
>>>>>
>>>>>
>>>>> I have a short question using squid as an ICAP-client.
>>>>>
>>>>>
>>>>> It seems that squid doesn't send an already downloaded (and cached)
>>>>> object to an ICAP-server.
>>>>>
>>>>> Here is a short description what I have done:
>>>>>
>>>>> 1. downloading a word-document with a macro-virus. The Virus-scanner
>>>>> (ICAP-server) uses an old pattern-file and does not detect the virus.
>>>>>
>>>>> The object is now in cache.
>>>>>
>>>>> 2. updating the virus-scanner to the newest pattern-file. The
>>>>> virus-scanner will now detect the macro virus.
>>>>>
>>>>> 3. downloading the same word-document. The object has been delivered
>>>>> to the client without a new virus scan.
>>>>>
>>>>>
>>>>>
>>>>> And now some log-entries:
>>>>>
>>>>> 1. First download of the word document:
>>>>>
>>>>> access.log:
>>>>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>>>>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>>>>> application/msword
>>>>>
>>>>> icap.log:
>>>>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>
>>>>> AV-Scanner:
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>> ICAP request decoding
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>>>>> message decoded in 1 chunks
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>> ICAP request decoding
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>> ICAP request processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>> service processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>>>>> processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at
>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>> service processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request
>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>> Details: '')
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>>>>> response headers type: CLEAN 204
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send 
>>>>> headers
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>> ICAP request processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core 
>>>>> library
>>>>> session cleared
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection
>>>>> closed by foreign host while waiting for requests
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core 
>>>>> library
>>>>> session cleared
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>> ICAP request decoding
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>>>>> message decoded in 259 chunks
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>> ICAP request decoding
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>> ICAP request processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>> service processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>>>>> processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>> HTTP/1.1>
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>> HTTP/1.1>
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>>>>> [service_scanner]File 'virus.doc' content is stored in
>>>>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>> service processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request
>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>> Details: '')
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>>>>> response headers type: CLEAN
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP
>>>>> headers for response type: CLEAN
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send 
>>>>> headers
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>>>>> original body (552960 bytes)
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>> ICAP request processing
>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core 
>>>>> library
>>>>> session cleared
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> 2. Second download of the word document (after the pattern-update):
>>>>>
>>>>> access.log:
>>>>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>>>>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>>>>
>>>>> icap.log:
>>>>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>
>>>>> AV-Scanner:
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>> ICAP request decoding
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>>>>> message decoded in 1 chunks
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>> ICAP request decoding
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>> ICAP request processing
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>> service processing
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>>>>> processing
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at
>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>> service processing
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request
>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>> Details: '')
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>>>>> response headers type: CLEAN 204
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send 
>>>>> headers
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>> ICAP request processing
>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core 
>>>>> library
>>>>> session cleared
>>>>>
>>>>>
>>>>> And now my question: Is this a bug in squid - or is it possible to
>>>>> tell squid to send already cached object to the icap-server?
>>>>>
>>>>> Kind regards,
>>>>>
>>>>> Stefan Kuegler
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v2
>>
>> iQEcBAEBCAAGBQJVSNkvAAoJENNXIZxhPexGsh8IAJGL1gSY3rzshF+BeHmsqZIJ
>> 4L0y2fjrQ66Q8Jz8fKk5saSemIdDRigH0fPAt4Bbb8cVnMcniP09cZ/lspaz3NxA
>> blodVyDYSLnmWIYzFfg19nd3UWDgIq4yOz3/rXCmHEkQ5sXrJQhJeP4Azeyez4Zj
>> Qef9ae75cbHexa12U8KERr9SDSnN18tRt4SPz8ZRaoYsoqIC4WRfkO8a0NPfHJp0
>> cYVj8pwHwbz5TPzYpPrGRR/rPbeO5FOVlIDVrxdHbafLjeYofVR8UOnKn67dxIVu
>> MJuunsVNtbPaWcDaGkUQ5Z8vvebGDB3pRPNm8XHXp7idGoDTQFJ6JbdK7ofA6do=
>> =VGI/
>> -----END PGP SIGNATURE-----
>>
>
> Viele Gr??e - Stefan K?gler
> SerNet GmbH

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/5c526e91/attachment.htm>

From leolistas at solutti.com.br  Mon May 18 12:21:17 2015
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 18 May 2015 09:21:17 -0300
Subject: [squid-users] Saving memory cache to disk on reboot?
In-Reply-To: <5559D325.10804@seiner.com>
References: <5559D325.10804@seiner.com>
Message-ID: <5559D93D.6050507@solutti.com.br>

On 18/05/15 08:55, Yan Seiner wrote:
> The title says it all - is it possible to save the memory cache to 
> disk on reboot?
>
> I reboot my systems weekly and I wonder if this would be any advantage.

     Initially, let's say that a cache can ALWAYS be lost. Sometimes it 
may not be desirable, but losing a cache must not create problems, the 
cache will simply be repopulated again and no problems should occur.

     Losing terabytes of cache is not be a good idea, as that amount of 
data would take some days to be repopulated and thus, during that time, 
you'll have bad hit ratios on your cache.

     As you say you're using memory cache, i'll assume that you're 
dealing with 16Gb or 32Gb of cache. We're not talking on terabytes, 
we're talking on few gigabytes.

     On that scenario, i would not worry about loosing it. Unless you're 
serving just a few specific pages on that cache, which is not usually 
the case, your hit ratio already shouldn't be too high, so loosing the 
cache shouldn't be a problem, it will be populated again in few hours, 
depending the number of clients and traffic generated by them.

     And my only question here is: why rebooting weekly ? Assuming 
you're running Linux or some Unix variant, that's absolutely unnecessary.


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From squid3 at treenet.co.nz  Mon May 18 12:28:23 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 00:28:23 +1200
Subject: [squid-users] Squid 3.4.10 and sslcrtd
In-Reply-To: <5559CBA4.5080206@xvidservices.com>
References: <5559CBA4.5080206@xvidservices.com>
Message-ID: <5559DAE7.5050104@treenet.co.nz>

On 18/05/2015 11:23 p.m., Veiko Kukk wrote:
> Hi
> 
> I'd like to know if I understand Squid documentation properly.
> I have following http_port and sslbump configuration:
> 
> http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=off
> cert=/var/spool/squid/ssl_cert/squid_ca.pem
> ssl_bump server-first all
> 
> From documentation:
> generate-host-certificates[=<on|off>]
> Dynamically create SSL server certificates for the destination hosts of
> bumped CONNECT requests. When enabled, the cert and key options are used
> to sign generated certificates. Otherwise generated certificate will be
> selfsigned.
> 
> I guess, that means, if generate-host-certificates=off, there is no need
> for sslcrtd_program. Do I understand this correctly?

Good question. The answer is yes.

> 
> Why does it still need sslcrtd_program? Note that error message WARNING:
> ssl_crtd #Hlpr0 exited is misleading, because currently, all sslcrtd
> related configuration options are commented out and none of the ssl_crtd
> processes are started.

Having a directive commented out means the default value for it is used.
There is a default helper built by --enable-ssl-crtd that gets used
unless you specify otherwise.

Currently Squid is not detecting that the helper is unused, so checks
for its existence and attempts to run some. Some other helpers also have
this problem.

The workaround is to also explicitly configure:
 sslcrtd_children 0

Amos



From squid-users at sernet.de  Mon May 18 13:15:15 2015
From: squid-users at sernet.de (Stefan Kuegler)
Date: Mon, 18 May 2015 15:15:15 +0200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <5559D498.1080507@gmail.com>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE> <5559D498.1080507@gmail.com>
Message-ID: <E1YuKt0-00HJCO-S8@intern.SerNet.DE>



Am 18.05.2015 um 14:01 schrieb Yuri Voinov:
> http://squidclamav.darold.net/config.html
>
>
>         Trust your cache (obsolete/unused in v6.x)
>
> One of the main configuration directive for performance improvement is
> 'trust_cache'. SquidClamav detect if the file to download is already
> stored in Squid cache. If you activate 'trust_cache', SquidClamav will
> not scan a file comming from Squid cache as it may have already been
> scanned during the first download. If trust_cache is disabled, no matter
> if the file is stored in the cache, SquidClamav will rescan the same
> file at each client request. I really recommand you to activate this
> directive.
>
> 	trust_cache 0
Yes, this option is set
>
> Trusted cache is disable by default as you may want to start with a
> fresh cache.
>
>
> Why you need rescan cached object again? You don't trust your cache? Or
> what?
>

I never can't trust the cache.

For example, a zip-file has been downloaded and it has been scanned by 
the virus-scanner. The virus scanner has classified the file as clean - 
because the virus in this file is too new for the scanner.

But - after a pattern-update one or two hours later - the virus-scanner 
will detect the same download as a virus (because it is a virus) - but 
squid does not scan the body of the cached object again - and still 
deliveres the virus to the client.

Regards,
Stefan
> 18.05.15 17:17, Stefan Kuegler ?????:
>> Hi Yuri.
>>>
>>> http://i.imgur.com/mW7gNwD.png
>>>
>>> http://squidclamav.darold.net/config.html
>>>
>>> This is for squidclamav (I use it and have no problems with malware).
>>
>> I just installed squidclamav - but the behaviour is always the same.
>> An object which has been stored in squid-cache will not be detected by
>> an icap server because squid does not scan the body again:
>>
>> squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing
>> request data handler.
>> pool hits:5 allocations: 1
>> Allocating from objects pool object 0
>> Requested service: squidclamav
>> squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing
>> preview header.
>> squidclamav.c(358) squidclamav_check_preview_handler: DEBUG
>> X-Client-IP: 192.168.216.54
>> squidclamav.c(1319) extract_http_info: DEBUG method GET
>> squidclamav.c(1330) extract_http_info: DEBUG url
>> http://www.intern/eicar_com.zip
>> squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL
>> requested: http://www.intern/eicar_com.zip
>> squidclamav.c(430) squidclamav_check_preview_handler: DEBUG
>> Content-Length: 0
>> squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body
>> data, allow 204
>> squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing
>> request data.
>> Storing to objects pool object 0
>> Log request to access log file /var/log/c-icap/access.log
>> Width: 0, Parameter:
>>
>> Any idea, how I can solve that problem. It seems that the only way to
>> be secure is to disable caching in squid. But I hope, this can't be
>> the solution.
>>
>> Regards,
>> Stefan
>>>
>>> 05.05.15 17:45, Stefan K?gler ?????:
>>>> Hi Yuri.
>>>>
>>>> Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
>>>>> This is not squid issue but your AV engine library or ICAP
>>>>> intermediate
>>>>> AV library configuration.
>>>>
>>>> Thank you for your answer.
>>>>
>>>> Can you explain me a litte bit more detailed why this is not a squid
>>> issue?
>>>>
>>>> In the icap-logfile, I can see a REQMOD-request _AND_ a
>>> RESPMOD-request to the icap-server if the object is not in cache.
>>>>
>>>> But - if the object is in cache - I can only see a REQMOD-request to
>>> the icap-server. I am missing RESPMOD.
>>>>
>>>> It seems to me, that it is a decision of the client (squid) which
>>> request (REQMOD or RESPMOD) will be send to the icap-server (AV-scanner)
>>> - and not a decision of the av-library.
>>>>
>>>> Regards, Stefan
>>>>
>>>>>
>>>>> 05.05.15 16:43, Stefan K?gler ?????:
>>>>>> Hello.
>>>>>>
>>>>>>
>>>>>> I have a short question using squid as an ICAP-client.
>>>>>>
>>>>>>
>>>>>> It seems that squid doesn't send an already downloaded (and cached)
>>>>>> object to an ICAP-server.
>>>>>>
>>>>>> Here is a short description what I have done:
>>>>>>
>>>>>> 1. downloading a word-document with a macro-virus. The Virus-scanner
>>>>>> (ICAP-server) uses an old pattern-file and does not detect the virus.
>>>>>>
>>>>>> The object is now in cache.
>>>>>>
>>>>>> 2. updating the virus-scanner to the newest pattern-file. The
>>>>>> virus-scanner will now detect the macro virus.
>>>>>>
>>>>>> 3. downloading the same word-document. The object has been delivered
>>>>>> to the client without a new virus scan.
>>>>>>
>>>>>>
>>>>>>
>>>>>> And now some log-entries:
>>>>>>
>>>>>> 1. First download of the word document:
>>>>>>
>>>>>> access.log:
>>>>>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>>>>>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>>>>>> application/msword
>>>>>>
>>>>>> icap.log:
>>>>>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>>
>>>>>> AV-Scanner:
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>> ICAP request decoding
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>>>>>> message decoded in 1 chunks
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>> ICAP request decoding
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>> ICAP request processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>> service processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>>>>>> processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Resource at
>>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>> service processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The request
>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>> Details: '')
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>>>>>> response headers type: CLEAN 204
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send
>>>>>> headers
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>> ICAP request processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core
>>>>>> library
>>>>>> session cleared
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Connection
>>>>>> closed by foreign host while waiting for requests
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core
>>>>>> library
>>>>>> session cleared
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>> ICAP request decoding
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>>>>>> message decoded in 259 chunks
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>> ICAP request decoding
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>> ICAP request processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>> service processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>>>>>> processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>>> HTTP/1.1>
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>>> HTTP/1.1>
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>>>>>> [service_scanner]File 'virus.doc' content is stored in
>>>>>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>> service processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The request
>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>> Details: '')
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>>>>>> response headers type: CLEAN
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding HTTP
>>>>>> headers for response type: CLEAN
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send
>>>>>> headers
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>>>>>> original body (552960 bytes)
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>> ICAP request processing
>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core
>>>>>> library
>>>>>> session cleared
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> 2. Second download of the word document (after the pattern-update):
>>>>>>
>>>>>> access.log:
>>>>>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>>>>>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>>>>>
>>>>>> icap.log:
>>>>>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>>
>>>>>> AV-Scanner:
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>> ICAP request decoding
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>>>>>> message decoded in 1 chunks
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>> ICAP request decoding
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>> ICAP request processing
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>> service processing
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>>>>>> processing
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Resource at
>>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be scanned
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>> service processing
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The request
>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>> Details: '')
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>>>>>> response headers type: CLEAN 204
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send
>>>>>> headers
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>> ICAP request processing
>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core
>>>>>> library
>>>>>> session cleared
>>>>>>
>>>>>>
>>>>>> And now my question: Is this a bug in squid - or is it possible to
>>>>>> tell squid to send already cached object to the icap-server?
>>>>>>
>>>>>> Kind regards,
>>>>>>
>>>>>> Stefan Kuegler
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v2
>>>
>>> iQEcBAEBCAAGBQJVSNkvAAoJENNXIZxhPexGsh8IAJGL1gSY3rzshF+BeHmsqZIJ
>>> 4L0y2fjrQ66Q8Jz8fKk5saSemIdDRigH0fPAt4Bbb8cVnMcniP09cZ/lspaz3NxA
>>> blodVyDYSLnmWIYzFfg19nd3UWDgIq4yOz3/rXCmHEkQ5sXrJQhJeP4Azeyez4Zj
>>> Qef9ae75cbHexa12U8KERr9SDSnN18tRt4SPz8ZRaoYsoqIC4WRfkO8a0NPfHJp0
>>> cYVj8pwHwbz5TPzYpPrGRR/rPbeO5FOVlIDVrxdHbafLjeYofVR8UOnKn67dxIVu
>>> MJuunsVNtbPaWcDaGkUQ5Z8vvebGDB3pRPNm8XHXp7idGoDTQFJ6JbdK7ofA6do=
>>> =VGI/
>>> -----END PGP SIGNATURE-----
>>>
>>
>> Viele Gr??e - Stefan K?gler
>> SerNet GmbH
>


From yvoinov at gmail.com  Mon May 18 13:22:27 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 May 2015 19:22:27 +0600
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1YuKt0-00HJCO-S8@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE> <5559D498.1080507@gmail.com>
 <E1YuKt0-00HJCO-S8@intern.SerNet.DE>
Message-ID: <5559E793.1060809@gmail.com>

My setup never send infected file against clean cached version.

If you mean really dynamic URL - this is another problem, which can't 
related with I-CAP and AV scanning.

In general, in the past I've checked my cache with AV offline every 
week. But never seen infected files. Also with old version of squidclamav.

Now my cache is trusted and never serve infected files. Only one check 
is executing - during populating on-disk cache.

Just FYI - proxy scanning never completely replace clients end point 
protection. This is not silver bullit. Accordingly, the client antivirus 
software is still necessary.

Keep in mind that when not carefully adjust the dynamic content on the 
proxy, it can pass the infected and clean versions of the same file to 
clients. Because they look different for the proxy. Proxy operates with 
the URL, not to the actual files.

18.05.15 19:15, Stefan Kuegler ?????:
>
>
> Am 18.05.2015 um 14:01 schrieb Yuri Voinov:
>> http://squidclamav.darold.net/config.html
>>
>>
>>         Trust your cache (obsolete/unused in v6.x)
>>
>> One of the main configuration directive for performance improvement is
>> 'trust_cache'. SquidClamav detect if the file to download is already
>> stored in Squid cache. If you activate 'trust_cache', SquidClamav will
>> not scan a file comming from Squid cache as it may have already been
>> scanned during the first download. If trust_cache is disabled, no matter
>> if the file is stored in the cache, SquidClamav will rescan the same
>> file at each client request. I really recommand you to activate this
>> directive.
>>
>>     trust_cache 0
> Yes, this option is set
>>
>> Trusted cache is disable by default as you may want to start with a
>> fresh cache.
>>
>>
>> Why you need rescan cached object again? You don't trust your cache? Or
>> what?
>>
>
> I never can't trust the cache.
>
> For example, a zip-file has been downloaded and it has been scanned by 
> the virus-scanner. The virus scanner has classified the file as clean 
> - because the virus in this file is too new for the scanner.
>
> But - after a pattern-update one or two hours later - the 
> virus-scanner will detect the same download as a virus (because it is 
> a virus) - but squid does not scan the body of the cached object again 
> - and still deliveres the virus to the client.
>
> Regards,
> Stefan
>> 18.05.15 17:17, Stefan Kuegler ?????:
>>> Hi Yuri.
>>>>
>>>> http://i.imgur.com/mW7gNwD.png
>>>>
>>>> http://squidclamav.darold.net/config.html
>>>>
>>>> This is for squidclamav (I use it and have no problems with malware).
>>>
>>> I just installed squidclamav - but the behaviour is always the same.
>>> An object which has been stored in squid-cache will not be detected by
>>> an icap server because squid does not scan the body again:
>>>
>>> squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing
>>> request data handler.
>>> pool hits:5 allocations: 1
>>> Allocating from objects pool object 0
>>> Requested service: squidclamav
>>> squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing
>>> preview header.
>>> squidclamav.c(358) squidclamav_check_preview_handler: DEBUG
>>> X-Client-IP: 192.168.216.54
>>> squidclamav.c(1319) extract_http_info: DEBUG method GET
>>> squidclamav.c(1330) extract_http_info: DEBUG url
>>> http://www.intern/eicar_com.zip
>>> squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL
>>> requested: http://www.intern/eicar_com.zip
>>> squidclamav.c(430) squidclamav_check_preview_handler: DEBUG
>>> Content-Length: 0
>>> squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body
>>> data, allow 204
>>> squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing
>>> request data.
>>> Storing to objects pool object 0
>>> Log request to access log file /var/log/c-icap/access.log
>>> Width: 0, Parameter:
>>>
>>> Any idea, how I can solve that problem. It seems that the only way to
>>> be secure is to disable caching in squid. But I hope, this can't be
>>> the solution.
>>>
>>> Regards,
>>> Stefan
>>>>
>>>> 05.05.15 17:45, Stefan K?gler ?????:
>>>>> Hi Yuri.
>>>>>
>>>>> Am 05.05.2015 um 12:51 schrieb Yuri Voinov:
>>>>>> This is not squid issue but your AV engine library or ICAP
>>>>>> intermediate
>>>>>> AV library configuration.
>>>>>
>>>>> Thank you for your answer.
>>>>>
>>>>> Can you explain me a litte bit more detailed why this is not a squid
>>>> issue?
>>>>>
>>>>> In the icap-logfile, I can see a REQMOD-request _AND_ a
>>>> RESPMOD-request to the icap-server if the object is not in cache.
>>>>>
>>>>> But - if the object is in cache - I can only see a REQMOD-request to
>>>> the icap-server. I am missing RESPMOD.
>>>>>
>>>>> It seems to me, that it is a decision of the client (squid) which
>>>> request (REQMOD or RESPMOD) will be send to the icap-server 
>>>> (AV-scanner)
>>>> - and not a decision of the av-library.
>>>>>
>>>>> Regards, Stefan
>>>>>
>>>>>>
>>>>>> 05.05.15 16:43, Stefan K?gler ?????:
>>>>>>> Hello.
>>>>>>>
>>>>>>>
>>>>>>> I have a short question using squid as an ICAP-client.
>>>>>>>
>>>>>>>
>>>>>>> It seems that squid doesn't send an already downloaded (and cached)
>>>>>>> object to an ICAP-server.
>>>>>>>
>>>>>>> Here is a short description what I have done:
>>>>>>>
>>>>>>> 1. downloading a word-document with a macro-virus. The 
>>>>>>> Virus-scanner
>>>>>>> (ICAP-server) uses an old pattern-file and does not detect the 
>>>>>>> virus.
>>>>>>>
>>>>>>> The object is now in cache.
>>>>>>>
>>>>>>> 2. updating the virus-scanner to the newest pattern-file. The
>>>>>>> virus-scanner will now detect the macro virus.
>>>>>>>
>>>>>>> 3. downloading the same word-document. The object has been 
>>>>>>> delivered
>>>>>>> to the client without a new virus scan.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> And now some log-entries:
>>>>>>>
>>>>>>> 1. First download of the word document:
>>>>>>>
>>>>>>> access.log:
>>>>>>> 2015-05-05 12:23:52    144 192.168.2.54 TCP_MISS/200 553301 GET
>>>>>>> http://www.intern/virus.doc - HIER_DIRECT/193.175.80.229
>>>>>>> application/msword
>>>>>>>
>>>>>>> icap.log:
>>>>>>> 2015-05-05 12:23:52      5 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>>> 2015-05-05 12:23:52    130 192.168.2.54 ICAP_MOD/200 553897 RESPMOD
>>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>>>
>>>>>>> AV-Scanner:
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Request
>>>>>>> message decoded in 1 chunks
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>>> ICAP request processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Starting
>>>>>>> service processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: REQMOD
>>>>>>> processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: 
>>>>>>> Resource at
>>>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be 
>>>>>>> scanned
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>>> service processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: The 
>>>>>>> request
>>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>>> Details: '')
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Create
>>>>>>> response headers type: CLEAN 204
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Send
>>>>>>> headers
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Finished
>>>>>>> ICAP request processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D2B0700] INFO: Core
>>>>>>> library
>>>>>>> session cleared
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: 
>>>>>>> Connection
>>>>>>> closed by foreign host while waiting for requests
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24D1AF700] INFO: Core
>>>>>>> library
>>>>>>> session cleared
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Request
>>>>>>> message decoded in 259 chunks
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>>> ICAP request processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>>> service processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: RESPMOD
>>>>>>> processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>>>> HTTP/1.1>
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Starting
>>>>>>> virus scanning for resource at: <GET http://www.intern/virus.doc
>>>>>>> HTTP/1.1>
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO:
>>>>>>> [service_scanner]File 'virus.doc' content is stored in
>>>>>>> '/var/spool/avira-icap/icap-tmp.6baFv3'
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>>> service processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: The 
>>>>>>> request
>>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>>> Details: '')
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Create
>>>>>>> response headers type: CLEAN
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Adding 
>>>>>>> HTTP
>>>>>>> headers for response type: CLEAN
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send
>>>>>>> headers
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Send the
>>>>>>> original body (552960 bytes)
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Finished
>>>>>>> ICAP request processing
>>>>>>> May  5 12:23:52 sk1 av-icapd[12412]: [7FD24CFAD700] INFO: Core
>>>>>>> library
>>>>>>> session cleared
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> 2. Second download of the word document (after the pattern-update):
>>>>>>>
>>>>>>> access.log:
>>>>>>> 2015-05-05 12:27:43     35 192.168.2.54 TCP_MEM_HIT/200 553309 GET
>>>>>>> http://www.intern/virus.doc - HIER_NONE/- application/msword
>>>>>>>
>>>>>>> icap.log:
>>>>>>> 2015-05-05 12:27:43      2 192.168.2.54 ICAP_ECHO/204 135 REQMOD
>>>>>>> icap://127.0.0.1:1344/service_scanner - -/127.0.0.1 -
>>>>>>>
>>>>>>> AV-Scanner:
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Request
>>>>>>> message decoded in 1 chunks
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>>> ICAP request decoding
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>>> ICAP request processing
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Starting
>>>>>>> service processing
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: REQMOD
>>>>>>> processing
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: 
>>>>>>> Resource at
>>>>>>> <GET http://www.intern/virus.doc HTTP/1.1> has no body to be 
>>>>>>> scanned
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>>> service processing
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: The 
>>>>>>> request
>>>>>>> for URI 'http://www.intern/virus.doc' was allowed (Reason: 'Clean'.
>>>>>>> Details: '')
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Create
>>>>>>> response headers type: CLEAN 204
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Send
>>>>>>> headers
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Finished
>>>>>>> ICAP request processing
>>>>>>> May  5 12:27:43 sk1 av-icapd[12412]: [7FD24C4A2700] INFO: Core
>>>>>>> library
>>>>>>> session cleared
>>>>>>>
>>>>>>>
>>>>>>> And now my question: Is this a bug in squid - or is it possible to
>>>>>>> tell squid to send already cached object to the icap-server?
>>>>>>>
>>>>>>> Kind regards,
>>>>>>>
>>>>>>> Stefan Kuegler
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>> Version: GnuPG v2
>>>>
>>>> iQEcBAEBCAAGBQJVSNkvAAoJENNXIZxhPexGsh8IAJGL1gSY3rzshF+BeHmsqZIJ
>>>> 4L0y2fjrQ66Q8Jz8fKk5saSemIdDRigH0fPAt4Bbb8cVnMcniP09cZ/lspaz3NxA
>>>> blodVyDYSLnmWIYzFfg19nd3UWDgIq4yOz3/rXCmHEkQ5sXrJQhJeP4Azeyez4Zj
>>>> Qef9ae75cbHexa12U8KERr9SDSnN18tRt4SPz8ZRaoYsoqIC4WRfkO8a0NPfHJp0
>>>> cYVj8pwHwbz5TPzYpPrGRR/rPbeO5FOVlIDVrxdHbafLjeYofVR8UOnKn67dxIVu
>>>> MJuunsVNtbPaWcDaGkUQ5Z8vvebGDB3pRPNm8XHXp7idGoDTQFJ6JbdK7ofA6do=
>>>> =VGI/
>>>> -----END PGP SIGNATURE-----
>>>>
>>>
>>> Viele Gr??e - Stefan K?gler
>>> SerNet GmbH
>>



From Walter.H at mathemainzel.info  Mon May 18 15:20:20 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 18 May 2015 17:20:20 +0200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <5559CC32.5040105@treenet.co.nz>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz> <5556E20C.4090409@mathemainzel.info>
 <5556FC47.4040409@treenet.co.nz> <55572568.7050905@mathemainzel.info>
 <5559CC32.5040105@treenet.co.nz>
Message-ID: <555A0334.5040304@mathemainzel.info>

On 18.05.2015 13:25, Amos Jeffries wrote:
>> I would have done it this way:
>>
>> acl block_whole_network dst_as 4837
>> deny_info errorpage block_whole_network
>> http_access deny block_whole_network
>>
>> but this crashes squid ...
>>
> Ouch. Is that the<http://bugs.squid-cache.org/show_bug.cgi?id=3579>  crash?
>
yes, this crash;
> I would like to fix that, but need the backtrace.
how would a generate the backtrace?
>>
>>>> does it seem to be problematic, when having an TLS-server with an IPv6
>>>> address only without DNS, because of the comm name?
>>> That is a different issue entirely.
>> yes and hoping no browser ever will accept a common name of just '*'
>>> Going by that description it seems Firefox and Chrome are a bit broken.
>> IE, too;
> IE is doing the right thing in your description. That cert-with-IP
> warning is the correct / working behaviour.
not really, with IPv4 it doesn't bring this warning - the CA cert using 
in squid is installed in both the FF certstore
and the windows system certstore (IE and Chrome use this)
>   The Firefox hang and Chrome
> "insecure" warning are the broken bits.

Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/2a117271/attachment.bin>

From squid3 at treenet.co.nz  Mon May 18 15:46:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 03:46:03 +1200
Subject: [squid-users] IPv6 and syntax?
In-Reply-To: <555A0334.5040304@mathemainzel.info>
References: <5556377B.7040109@mathemainzel.info>
 <55568446.7030909@treenet.co.nz> <5556E20C.4090409@mathemainzel.info>
 <5556FC47.4040409@treenet.co.nz> <55572568.7050905@mathemainzel.info>
 <5559CC32.5040105@treenet.co.nz> <555A0334.5040304@mathemainzel.info>
Message-ID: <555A093B.9090101@treenet.co.nz>

On 19/05/2015 3:20 a.m., Walter H. wrote:
> On 18.05.2015 13:25, Amos Jeffries wrote:
>>> I would have done it this way:
>>>
>>> acl block_whole_network dst_as 4837
>>> deny_info errorpage block_whole_network
>>> http_access deny block_whole_network
>>>
>>> but this crashes squid ...
>>>
>> Ouch. Is that the<http://bugs.squid-cache.org/show_bug.cgi?id=3579> 
>> crash?
>>
> yes, this crash;
>> I would like to fix that, but need the backtrace.
> how would a generate the backtrace?

Several ways possible are listed in
<http://wiki.squid-cache.org/SquidFaq/BugReporting>.

>>>
>>>>> does it seem to be problematic, when having an TLS-server with an IPv6
>>>>> address only without DNS, because of the comm name?
>>>> That is a different issue entirely.
>>> yes and hoping no browser ever will accept a common name of just '*'
>>>> Going by that description it seems Firefox and Chrome are a bit broken.
>>> IE, too;
>> IE is doing the right thing in your description. That cert-with-IP
>> warning is the correct / working behaviour.
> not really, with IPv4 it doesn't bring this warning - the CA cert using

Ah, right.


Amos


From canon0905 at hotmail.com  Mon May 18 17:49:33 2015
From: canon0905 at hotmail.com (Andres Granados)
Date: Mon, 18 May 2015 11:49:33 -0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
Message-ID: <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>

hello!I need help on how to block pornographic images of google, I was trying different options and still do not succeed, try: http_reply_access with request_header_add, and even with a configuration dns, I think is to request_header_add the best, though not it has worked for me, I hope your help, is to implement a school, thanks! 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/54dcbbcd/attachment.htm>

From canon0905 at hotmail.com  Mon May 18 17:52:58 2015
From: canon0905 at hotmail.com (Andres Granados)
Date: Mon, 18 May 2015 11:52:58 -0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <mailman.1018.1431959260.3069.squid-users@lists.squid-cache.org>
References: <mailman.1018.1431959260.3069.squid-users@lists.squid-cache.org>
Message-ID: <SNT151-W402202647959DF48188D94AEC40@phx.gbl>

hello!I need help on how to block pornographic images of google, I was trying different options and still do not succeed, try: http_reply_access with request_header_add, and even with a configuration dns, I think is to request_header_add the best, though not it has worked for me, I hope your help, is to implement a school, thanks! 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/97d7e6f1/attachment.htm>

From yvoinov at gmail.com  Mon May 18 17:53:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 18 May 2015 23:53:28 +0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
Message-ID: <555A2718.7080904@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
You need to use solution which is exists over 10 years. url_rewrite_program.

With good blacklist good redirector can be block almost all unwanted
content.

Three most known programs:

squidgiard
DansGuardian
ufdbguard

and more.

18.05.15 23:49, Andres Granados ?????:
> hello!I need help on how to block pornographic images of google, I was trying different options and
still do not succeed, try: http_reply_access with request_header_add,
and even with a configuration dns, I think is to request_header_add the
best, though not it has worked for me, I hope your help, is to implement
a school, thanks!                                                     
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVWicYAAoJENNXIZxhPexGTnkIAINJpl2qLr8y8ZjqpxByLyS/
RNT7ke52Bc9g/TIwn9vkdnr3JUAvks8c25ODMaj9Nol/wE+wjfIyg3d+ZupukhPu
rZ2h3rfSWW/bTln3srDJ+4y+gUS5zd5mLxIT/wynrkDiMsVHcuYyWA99a62d3FuY
Ej2jggCjaNWeQKA003lwMQNmZs1KW35pNV8kW4c0WRQWI3dT6++b/70LXDygJaRX
OXEvdIaLgnJLAtYxGICVN7rPAmv7fzbFA9yIVbU9d+TdYVtjhXyU6wPOyPIQWBBr
54Lgvp5Ueoc+sEl6niDBWz/ZZrYt+6fkWZAYdrswLkMCr34zZ3SatE7bV3G68Lo=
=n6on
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/1f3dea41/attachment.htm>

From rafael.akchurin at diladele.com  Mon May 18 18:21:00 2015
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 18 May 2015 18:21:00 +0000
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
Message-ID: <DB5PR04MB11280751D6CB4436981343118FC40@DB5PR04MB1128.eurprd04.prod.outlook.com>

Hello Andres,

N.B. Please take my answer with a huge grain of salt.

The google images search returns image results  (thumbnails) as inline base 64 encoded images in the CSS. Selectively blocking them with any URL redirector will not work. It is possible to completely block google images but I assume this is not the way to follow for a school where a lot of pupil projects need access to the image search results.

Our current recommend the strategy for blocking explicit search results for schools are (please note you must be doing SSL decryption for this to work!):

- enforce Safe Search for Google. In this way proxy adds some special parameter to URL and some HTTP header to all outgoing requests to google servers asking it to show only safe images. Unfortunately this sometime misses quite offensive images as no safe search is safe enough.

- add keyword filtering to filter results of image search as it also contains some textual description of images and their original URLs that greatly improves detection rate. Some possible overblocking definitely will occur.

- add image tone detection filtering to the mix (works only for JPEGs for now and higly experimental) :(

In order to do that you need to integrate a content scanning server with your Squid - as possible variant consider taking a look at what we develop exactly for this purpose (search google for qlproxy ICAP - please note it is a commercial product).

Other choices are DansGuardian as parent proxy and SquidGuard/ufdbGuard/other redirectors.
I hope some other members of the list can explain how to do what your require with these.

Best regards,
Rafael


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Andres Granados
Sent: Monday, May 18, 2015 7:50 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] block inappropriate images of google

hello!
I need help on how to block pornographic images of google, I was trying different options and still do not succeed, try: http_reply_access with request_header_add, and even with a configuration dns, I think is to request_header_add the best, though not it has worked for me, I hope your help, is to implement a school, thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/c6cf7d29/attachment.htm>

From dhottinger at harrisonburg.k12.va.us  Mon May 18 18:25:07 2015
From: dhottinger at harrisonburg.k12.va.us (Dwayne Hottinger)
Date: Mon, 18 May 2015 14:25:07 -0400
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <DB5PR04MB11280751D6CB4436981343118FC40@DB5PR04MB1128.eurprd04.prod.outlook.com>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <DB5PR04MB11280751D6CB4436981343118FC40@DB5PR04MB1128.eurprd04.prod.outlook.com>
Message-ID: <CAGzaBQG-2yvOhXwWP772wBZHUFBiO06OW2bn0pC+kJNz3t4NMg@mail.gmail.com>

There is a way to use an internal dns server to redirect all google
searches to their safe search google.  This does help with inappropriate
images and searches that maybe quesionable.  Just 'google' dns safesearch.
You should get a few hits.  Im currently doing this and it works very well.

On Mon, May 18, 2015 at 2:21 PM, Rafael Akchurin <
rafael.akchurin at diladele.com> wrote:

>  Hello Andres,
>
>
>
> N.B. Please take my answer with a huge grain of salt.
>
>
>
> The google images search returns image results  (thumbnails) as inline
> base 64 encoded images in the CSS. Selectively blocking them with any URL
> redirector will not work. It is possible to completely block google images
> but I assume this is not the way to follow for a school where a lot of
> pupil projects need access to the image search results.
>
>
>
> Our current recommend the strategy for blocking explicit search results
> for schools are (please note you must be doing SSL decryption for this to
> work!):
>
>
>
> - enforce Safe Search for Google. In this way proxy adds some special
> parameter to URL and some HTTP header to all outgoing requests to google
> servers asking it to show only safe images. Unfortunately this sometime
> misses quite offensive images as no safe search is safe enough.
>
>
>
> - add keyword filtering to filter results of image search as it also
> contains some textual description of images and their original URLs that
> greatly improves detection rate. Some possible overblocking definitely will
> occur.
>
>
>
> - add image tone detection filtering to the mix (works only for JPEGs for
> now and higly experimental) :(
>
>
>
> In order to do that you need to integrate a content scanning server with
> your Squid ? as possible variant consider taking a look at what we develop
> exactly for this purpose (search google for qlproxy ICAP ? please note it
> is a commercial product).
>
>
>
> Other choices are DansGuardian as parent proxy and
> SquidGuard/ufdbGuard/other redirectors.
>
> I hope some other members of the list can explain how to do what your
> require with these.
>
>
>
> Best regards,
>
> Rafael
>
>
>
>
>
> *From:* squid-users [mailto:squid-users-bounces at lists.squid-cache.org] *On
> Behalf Of *Andres Granados
> *Sent:* Monday, May 18, 2015 7:50 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] block inappropriate images of google
>
>
>
> hello!
>
> I need help on how to block pornographic images of google, I was trying
> different options and still do not succeed, try: http_reply_access with
> request_header_add, and even with a configuration dns, I think is to
> request_header_add the best, though not it has worked for me, I hope your
> help, is to implement a school, thanks!
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
Dwayne Hottinger
Supervisor of Network Operations
Harrisonburg City Public Schools
http://staff.harrisonburg.k12.va.us/~dhottinger
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150518/f5420dbc/attachment.htm>

From yvoinov at gmail.com  Mon May 18 18:30:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 19 May 2015 00:30:25 +0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <CAGzaBQG-2yvOhXwWP772wBZHUFBiO06OW2bn0pC+kJNz3t4NMg@mail.gmail.com>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <DB5PR04MB11280751D6CB4436981343118FC40@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <CAGzaBQG-2yvOhXwWP772wBZHUFBiO06OW2bn0pC+kJNz3t4NMg@mail.gmail.com>
Message-ID: <555A2FC1.5070605@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Transparent DNS interception is more reliable technics, but requires
some advanced things. Including continious support.

And also this will not proof against browser anti-proxy plugins.

BTW, gents, we are talking about advanced internet users - Hola,
Browsec, FriGate, ZenMate etc.- already invented. And they CAN'T be
blocked with HTTP/HTTPS proxy.

To block this bypass solution you need advanced
infrastructure/experienced network admin.

19.05.15 0:25, Dwayne Hottinger ?????:
> There is a way to use an internal dns server to redirect all google
> searches to their safe search google.  This does help with inappropriate
> images and searches that maybe quesionable.  Just 'google' dns safesearch.
> You should get a few hits.  Im currently doing this and it works very
well.
>
> On Mon, May 18, 2015 at 2:21 PM, Rafael Akchurin <
> rafael.akchurin at diladele.com> wrote:
>
>>  Hello Andres,
>>
>>
>>
>> N.B. Please take my answer with a huge grain of salt.
>>
>>
>>
>> The google images search returns image results  (thumbnails) as inline
>> base 64 encoded images in the CSS. Selectively blocking them with any URL
>> redirector will not work. It is possible to completely block google
images
>> but I assume this is not the way to follow for a school where a lot of
>> pupil projects need access to the image search results.
>>
>>
>>
>> Our current recommend the strategy for blocking explicit search results
>> for schools are (please note you must be doing SSL decryption for this to
>> work!):
>>
>>
>>
>> - enforce Safe Search for Google. In this way proxy adds some special
>> parameter to URL and some HTTP header to all outgoing requests to google
>> servers asking it to show only safe images. Unfortunately this sometime
>> misses quite offensive images as no safe search is safe enough.
>>
>>
>>
>> - add keyword filtering to filter results of image search as it also
>> contains some textual description of images and their original URLs that
>> greatly improves detection rate. Some possible overblocking
definitely will
>> occur.
>>
>>
>>
>> - add image tone detection filtering to the mix (works only for JPEGs for
>> now and higly experimental) :(
>>
>>
>>
>> In order to do that you need to integrate a content scanning server with
>> your Squid ? as possible variant consider taking a look at what we
develop
>> exactly for this purpose (search google for qlproxy ICAP ? please note it
>> is a commercial product).
>>
>>
>>
>> Other choices are DansGuardian as parent proxy and
>> SquidGuard/ufdbGuard/other redirectors.
>>
>> I hope some other members of the list can explain how to do what your
>> require with these.
>>
>>
>>
>> Best regards,
>>
>> Rafael
>>
>>
>>
>>
>>
>> *From:* squid-users
[mailto:squid-users-bounces at lists.squid-cache.org] *On
>> Behalf Of *Andres Granados
>> *Sent:* Monday, May 18, 2015 7:50 PM
>> *To:* squid-users at lists.squid-cache.org
>> *Subject:* [squid-users] block inappropriate images of google
>>
>>
>>
>> hello!
>>
>> I need help on how to block pornographic images of google, I was trying
>> different options and still do not succeed, try: http_reply_access with
>> request_header_add, and even with a configuration dns, I think is to
>> request_header_add the best, though not it has worked for me, I hope your
>> help, is to implement a school, thanks!
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVWi/AAAoJENNXIZxhPexG73UIAI2HY2WgJDoz9/TaJmrVv0HR
hjFqjv2lvy1wpIs5MBTeyOkwIMjbDsnuU24cKpuOIXxyTJFZD6MRhR5MKT3PFhMX
SvPOvkxhTHvMKKs0G2vYuFD+vvLYdxY5dIg2HZ0Jui7cb7dvlwf8l67prPZvBIZL
Mh9UpLmqa7OrRd6T17EzaR0sv7tC2h1zA5AlbNDlIn/4ee2MHyn4aiL13hFHgoYe
N326TEx7PSb2IHwDkZqIbhcRHShUQC8RxsLr0vTLpiiNTFtAoaECLOQt9435+XZt
vDjHnIt3A7kkME5+f2FpMw5GNhPwFDr7Nb0MlxxRxMJQUQMhZ6xab3mdUenIsf4=
=taCf
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150519/7a0720e1/attachment.htm>

From yvoinov at gmail.com  Mon May 18 18:35:28 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 19 May 2015 00:35:28 +0600
Subject: [squid-users] block inappropriate images of google
In-Reply-To: <CAGzaBQG-2yvOhXwWP772wBZHUFBiO06OW2bn0pC+kJNz3t4NMg@mail.gmail.com>
References: <SNT151-W241B10C4BE4023C6DBBB16AEC40@phx.gbl>
 <SNT151-W15E8218F1270D9CCAA54FFAEC40@phx.gbl>
 <DB5PR04MB11280751D6CB4436981343118FC40@DB5PR04MB1128.eurprd04.prod.outlook.com>
 <CAGzaBQG-2yvOhXwWP772wBZHUFBiO06OW2bn0pC+kJNz3t4NMg@mail.gmail.com>
Message-ID: <555A30F0.3090808@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Also note:

Most browser's anti-proxy plugins exists for Android/Apple.

19.05.15 0:25, Dwayne Hottinger ?????:
> There is a way to use an internal dns server to redirect all google
> searches to their safe search google.  This does help with inappropriate
> images and searches that maybe quesionable.  Just 'google' dns safesearch.
> You should get a few hits.  Im currently doing this and it works very
well.
>
> On Mon, May 18, 2015 at 2:21 PM, Rafael Akchurin <
> rafael.akchurin at diladele.com> wrote:
>
>>  Hello Andres,
>>
>>
>>
>> N.B. Please take my answer with a huge grain of salt.
>>
>>
>>
>> The google images search returns image results  (thumbnails) as inline
>> base 64 encoded images in the CSS. Selectively blocking them with any URL
>> redirector will not work. It is possible to completely block google
images
>> but I assume this is not the way to follow for a school where a lot of
>> pupil projects need access to the image search results.
>>
>>
>>
>> Our current recommend the strategy for blocking explicit search results
>> for schools are (please note you must be doing SSL decryption for this to
>> work!):
>>
>>
>>
>> - enforce Safe Search for Google. In this way proxy adds some special
>> parameter to URL and some HTTP header to all outgoing requests to google
>> servers asking it to show only safe images. Unfortunately this sometime
>> misses quite offensive images as no safe search is safe enough.
>>
>>
>>
>> - add keyword filtering to filter results of image search as it also
>> contains some textual description of images and their original URLs that
>> greatly improves detection rate. Some possible overblocking
definitely will
>> occur.
>>
>>
>>
>> - add image tone detection filtering to the mix (works only for JPEGs for
>> now and higly experimental) :(
>>
>>
>>
>> In order to do that you need to integrate a content scanning server with
>> your Squid ? as possible variant consider taking a look at what we
develop
>> exactly for this purpose (search google for qlproxy ICAP ? please note it
>> is a commercial product).
>>
>>
>>
>> Other choices are DansGuardian as parent proxy and
>> SquidGuard/ufdbGuard/other redirectors.
>>
>> I hope some other members of the list can explain how to do what your
>> require with these.
>>
>>
>>
>> Best regards,
>>
>> Rafael
>>
>>
>>
>>
>>
>> *From:* squid-users
[mailto:squid-users-bounces at lists.squid-cache.org] *On
>> Behalf Of *Andres Granados
>> *Sent:* Monday, May 18, 2015 7:50 PM
>> *To:* squid-users at lists.squid-cache.org
>> *Subject:* [squid-users] block inappropriate images of google
>>
>>
>>
>> hello!
>>
>> I need help on how to block pornographic images of google, I was trying
>> different options and still do not succeed, try: http_reply_access with
>> request_header_add, and even with a configuration dns, I think is to
>> request_header_add the best, though not it has worked for me, I hope your
>> help, is to implement a school, thanks!
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVWjDvAAoJENNXIZxhPexGanoIAMwbZb8Lg2Nb2xMnh7ywTbPY
HRMLi7jd6E47PyRNKeWYOj+0DpRlb158CS3ZfFICqDLiQpYQnYZ33Mjz545GkW/C
61fAEcjerQfSOSGvD8+ZEXxVW4sJbuO0PIl/JlP9soN6VHpzhDVGyLC1KX4TA6F2
8A4B4FAMrFYvVqyExfYDJso/uMKTzPd+ObeodZVI50ziezYFqKwCnepPsVMTQ5Zg
SnWJY82M53IXLS8saLkfWd7vSrzFLLWkzxWkycYGn24YCgbD5q+MTR+n1hSgb3qn
6eddjXrri9xSdve9rjzdNkij5anuNkwf5JxdMMUnImOopxxxOq7WdwvuSNcMAKI=
=+hhF
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150519/dd33f74d/attachment.htm>

From avhernandez at uci.cu  Mon May 18 19:05:35 2015
From: avhernandez at uci.cu (=?UTF-8?B?QW1hdXJ5IFZpZXJhIEhlcm7DoW5kZXo=?=)
Date: Mon, 18 May 2015 15:05:35 -0400
Subject: [squid-users] about squid trouble
Message-ID: <555A37FF.20808@uci.cu>

Hi everyone:

I recently subscribe to this list and I'm going to introduce myself briefly.

I work at the University of Computer Sciences at Havana, Cuba (6000 
users aproximately). We surf using squid, but in many ocassions when we 
are surfing I get an error page of exceed quota telling me that the 
cuota of other user is finished. I'm worried, because I think that is 
possible that others users will be surfing with my account and i will be 
surfing with the account of others users. Could you help me please. 
Thanks in advance.
Best regards, Amaury.


From Antony.Stone at squid.open.source.it  Mon May 18 19:25:38 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 May 2015 21:25:38 +0200
Subject: [squid-users] about squid trouble
In-Reply-To: <555A37FF.20808@uci.cu>
References: <555A37FF.20808@uci.cu>
Message-ID: <201505182125.38782.Antony.Stone@squid.open.source.it>

On Monday 18 May 2015 at 21:05:35 (EU time), Amaury Viera Hern?ndez wrote:

> I work at the University of Computer Sciences at Havana, Cuba (6000
> users aproximately). We surf using squid, but in many ocassions when we
> are surfing I get an error page of exceed quota telling me that the
> cuota of other user is finished. I'm worried, because I think that is
> possible that others users will be surfing with my account and i will be
> surfing with the account of others users. Could you help me please.

We might well be able to help you, but only if you give us some information to 
go on.

For a good start, send us your squid.conf (you can omit comments and blank 
lines), and also describe your network setup - is there just a single Squid 
server, or several; how do clients authenticate to identify the user; and how 
are you implementing quota limits?


Regards,


Antony.

-- 
"The future is already here.   It's just not evenly distributed yet."

 - William Gibson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vdoctor at neuf.fr  Mon May 18 20:58:59 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 18 May 2015 13:58:59 -0700 (PDT)
Subject: [squid-users] How to cache Chrome Installer ?
Message-ID: <1431982739402-4671271.post@n4.nabble.com>

Hi All,

Has someone of you already cached this object ?
*http://r8---sn-n4g-jqbe.gvt1.com/edgedl/chrome/win/776B03BEAFB2810D/42.0.2311.152_chrome_installer.exe*

I know this is a dynamic object provided by Google, we tried with the
StoreID by not yet able to get a TCP_HIT from Squid.
If any idea, let me/us know, thanks in advance.

Bye Fred




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon May 18 21:20:25 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 19 May 2015 03:20:25 +0600
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <1431982739402-4671271.post@n4.nabble.com>
References: <1431982739402-4671271.post@n4.nabble.com>
Message-ID: <555A5799.9040701@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
# Adobe/Java and other updates
acl adobe_java_updates urlpath_regex "/usr/local/squid/etc/urlregex.updates"
range_offset_limit none adobe_java_updates

store_id_access allow adobe_java_updates

store_id_program /usr/local/squid/libexec/storeid_file_rewrite
/usr/local/squid/etc/storeid.conf

refresh_pattern -i adobe.com/.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i java.com/.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i sun.com/.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i google\.com.*\.(zip|exe)    4320    80%    43200   
reload-into-ims
refresh_pattern -i macromedia\.com.*\.(zip|exe)    4320    80%   
43200    reload-into-ims

storeid.conf fragment:

^http:\/\/(.*)\/(.*)\/([0-9].*)\/(.*)\.*(zip|exe)$               
http://$1/$3/$4.$5
^https:\/\/(.*)\/(.*)\/([0-9].*)\/(.*)\.*(zip|exe)$               
https://$1/$3/$4.$5
^http:\/\/(.*)\/([0-9].*)\/(.*)\.*(zip|exe)$                   
http://$1/$3.$4
^https:\/\/(.*)\/([0-9].*)\/(.*)\.*(zip|exe)$                   
https://$1/$3.$4
^http:\/\/(.*?)\/(.*?)\.(jp(?:e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4|zip|exe)\?(?:.*?)$       
http://$1/$2.$3
^https:\/\/(.*?)\/(.*?)\.(jp(?:e?g|e|2)|gif|png|tiff?|bmp|ico|webp|flv|mp4|zip|exe)\?(?:.*?)$       
https://$1/$2.$3

urlregex.updates:

adobe\.com.*\.(zip|exe)
java\.com.*\.(zip|exe)
sun\.com.*\.(zip|exe)
google\.com.*\.(zip|exe)
macromedia\.com.*\.(zip|exe)


19.05.15 2:58, Stakres ?????:
> Hi All,
>
> Has someone of you already cached this object ?
>
*http://r8---sn-n4g-jqbe.gvt1.com/edgedl/chrome/win/776B03BEAFB2810D/42.0.2311.152_chrome_installer.exe*
>
> I know this is a dynamic object provided by Google, we tried with the
> StoreID by not yet able to get a TCP_HIT from Squid.
> If any idea, let me/us know, thanks in advance.
>
> Bye Fred
>
>
>
>
> --
> View this message in context:
http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVWleZAAoJENNXIZxhPexGgmQH/jM/G6RdydgauRKfHh2NwBzr
sJ0wvfYtzbtJj73YbqixUvStSC4hMc0kvUnfpIjJ2miFXGvSTD3d4NWruwggL33F
qeELX6vkxpEtVfzesgwyxO1kg9dPTeo114ytZ73/WziyLUKz9s62loRiIdUZVfc+
WPlkJqXQTmXoRzZRerYi5692GJeLB2WjM0+dXLgDg7XiIzfu+xmOuk58aNev/cdH
xG9ObtZ/nGXD9BFHvVLBdC/i8ONgH66ogUOeXHVLBD+24PRzBqIt/ad8rjnD9sqq
zlTXr3+OZepzYW2DKkjnBYZYrukT16yis7w5e2gBQYaPTVzThqnTcZvC+icCiio=
=Z0ok
-----END PGP SIGNATURE-----



From vdoctor at neuf.fr  Mon May 18 21:18:44 2015
From: vdoctor at neuf.fr (Stakres)
Date: Mon, 18 May 2015 14:18:44 -0700 (PDT)
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <555A5799.9040701@gmail.com>
References: <1431982739402-4671271.post@n4.nabble.com>
 <555A5799.9040701@gmail.com>
Message-ID: <1431983924158-4671273.post@n4.nabble.com>

Hi Yuri,

Do you get a TCP_HIT with your rules ?
>From my side, i get this: *X-Cache: MISS* from blablabla...

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271p4671273.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sima_yi at operamail.com  Tue May 19 00:33:03 2015
From: sima_yi at operamail.com (PSA4444)
Date: Mon, 18 May 2015 17:33:03 -0700 (PDT)
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
Message-ID: <1431995583201-4671274.post@n4.nabble.com>

I tried to upgrade from squid 3.2.x to 3.5.x and found my url rewrite script
no longer works.
After trial and error, I found the latest version it works in is the latest
3.3.x so I've upgraded to this for now.

Here is my url_rewrite_program: http://pastebin.com/uaYUCkyY

I haven't been able to figure out how to port this over to the new
url_rewrite_program format.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-3-to-3-5-url-rewrite-program-changes-tp4671274.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue May 19 06:05:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 18:05:51 +1200
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
In-Reply-To: <1431995583201-4671274.post@n4.nabble.com>
References: <1431995583201-4671274.post@n4.nabble.com>
Message-ID: <555AD2BF.9060901@treenet.co.nz>

On 19/05/2015 12:33 p.m., PSA4444 wrote:
> I tried to upgrade from squid 3.2.x to 3.5.x and found my url rewrite script
> no longer works.
> After trial and error, I found the latest version it works in is the latest
> 3.3.x so I've upgraded to this for now.
> 
> Here is my url_rewrite_program: http://pastebin.com/uaYUCkyY
> 
> I haven't been able to figure out how to port this over to the new
> url_rewrite_program format.
> 

Take a read through <http://wiki.squid-cache.org/Features/AddonHelpers>.

Notice how the helper input and output lines have different syntax. Your
helper is just taking the input line, adjusting its URL and dumping it
straight back at Squid with all the extra input parameters. That has not
been correct operation since Squid-2.4 (when there were no extra
parameters).


Anyhow your helper is doing the equivalent of:

 acl HTTPS proto HTTPS
 acl api urlpath_regex ^/api

 acl site dstdomain site.domain.com
 deny_info 301:https://api.domain.com%R site
 http_access deny HTTPS api site

 acl site2 dstdomain site2.domain.com
 http_access deny HTTPS api site2
 deny_info 301:https://api2.domain.com%R site2


Amos



From Andre.Albsmeier at siemens.com  Tue May 19 06:29:17 2015
From: Andre.Albsmeier at siemens.com (Andre Albsmeier)
Date: Tue, 19 May 2015 08:29:17 +0200
Subject: [squid-users] Squid 3.5: internal-static icons on ftp:// requests
Message-ID: <20150519062917.GA51255@bali>

When browsing e.g.

ftp://ftp.mozilla.org/pub/thunderbird/releases/31.5.0/win32/en-GB/

I miss the internally generated icons and receive an error message
in the logs:

2015/05/17 20:03:44 kid1| internalStart: unknown request:
GET /squid-internal-static/icons/silk/arrow_up.png HTTP/1.1
Host: ftp.mozilla.org
User-Agent: Mozilla/5.0 (X11; FreeBSD i386; rv:31.0) Gecko/20100101 Firefox/31.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: de-de,de;q=0.8,en-gb;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive


The corresponding request for getting the icon is

 ftp://ftp.mozilla.org/squid-internal-static/icons/silk/application.png

and this fails. If I (manually) convert the request into a http request

 http://ftp.mozilla.org/squid-internal-static/icons/silk/application.png

the icon gets loaded (this works as global_internal_static is set to on).
Shouldn't all internal-static request be http instead of ftp? I have now
patched my squid so it enforces http for internal stuff with this patch:

--- src/client_side.cc.ORI	2015-03-28 11:58:05.000000000 +0100
+++ src/client_side.cc	2015-05-18 19:38:20.982160000 +0200
@@ -2683,16 +2683,18 @@
                    ':' << request->port);
             http->flags.internal = true;
         } else if (Config.onoff.global_internal_static && internalStaticCheck(request->urlpath.termedBuf())) {
             debugs(33, 2, "internal URL found: " << request->url.getScheme() << "://" << request->GetHost() <<
                    ':' << request->port << " (global_internal_static on)");
             request->SetHost(internalHostname());
             request->port = getMyPort();
             http->flags.internal = true;
+request->url.setScheme( AnyP::PROTO_HTTP );
+debugs(33, 2, "NEW internal URL: " << request->url.getScheme() << "://" << request->GetHost() << ':' << request->port << " (global_internal_static on)");
         } else
             debugs(33, 2, "internal URL found: " << request->url.getScheme() << "://" << request->GetHost() <<
                    ':' << request->port << " (not this proxy)");
     }
 
     if (http->flags.internal)
         request->login[0] = '\0';
 

and now the icons on ftp://ftp.mozilla.org/ appear but I wonder if it
is really needed to patch squid for that... ;-).

Thanks,

	-Andre


From squid3 at treenet.co.nz  Tue May 19 06:58:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 18:58:33 +1200
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <1431982739402-4671271.post@n4.nabble.com>
References: <1431982739402-4671271.post@n4.nabble.com>
Message-ID: <555ADF19.7010106@treenet.co.nz>

On 19/05/2015 8:58 a.m., Stakres wrote:
> Hi All,
> 
> Has someone of you already cached this object ?
> *http://r8---sn-n4g-jqbe.gvt1.com/edgedl/chrome/win/776B03BEAFB2810D/42.0.2311.152_chrome_installer.exe*
> 

I get a 15min cacheable 302 response from that.


> I know this is a dynamic object provided by Google, we tried with the
> StoreID by not yet able to get a TCP_HIT from Squid.
> If any idea, let me/us know, thanks in advance.


Are you perhapse looking at your logs with out query strings?
 That cacheable 302 I mentioned redirects to itself with lots of extra
CMS query string parameters.


Following that redirect dumps out a program binary with "Vary: *" which
changes per request. If you look at the chrome installer itself the
application binary contains per-user embeded code and "stuff". So no its
not cacheable.


The only cacheable part of this is the 302 response.

Amos



From squid3 at treenet.co.nz  Tue May 19 07:52:14 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 19:52:14 +1200
Subject: [squid-users] Squid 3.5: internal-static icons on ftp://
	requests
In-Reply-To: <20150519062917.GA51255@bali>
References: <20150519062917.GA51255@bali>
Message-ID: <555AEBAE.30008@treenet.co.nz>

On 19/05/2015 6:29 p.m., Andre Albsmeier wrote:
> When browsing e.g.
> 
> ftp://ftp.mozilla.org/pub/thunderbird/releases/31.5.0/win32/en-GB/
> 
<snip>
> and now the icons on ftp://ftp.mozilla.org/ appear but I wonder if it
> is really needed to patch squid for that... ;-).

This is http://bugs.squid-cache.org/show_bug.cgi?id=4132. Thank you for
the patch a slightly tweaked version (no extra debugs message) is now
applied to Squid-4 and should be in the next releases.

Amos



From vdoctor at neuf.fr  Tue May 19 07:48:32 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 19 May 2015 00:48:32 -0700 (PDT)
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <555ADF19.7010106@treenet.co.nz>
References: <1431982739402-4671271.post@n4.nabble.com>
 <555ADF19.7010106@treenet.co.nz>
Message-ID: <1432021712457-4671279.post@n4.nabble.com>

Hi Amos,

By deleting the "Vary:*" from the headers, we should be able to cache the
object, correct ?
And by de-duplicating using the url until the "?", we get a single object.
We do all the same YouTube also containing "user" data, so I don't see why
it cannot work with this Chrome Installer url... 

Bye Fred



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271p4671279.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue May 19 08:24:40 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 19 May 2015 14:24:40 +0600
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <1431983924158-4671273.post@n4.nabble.com>
References: <1431982739402-4671271.post@n4.nabble.com>
 <555A5799.9040701@gmail.com> <1431983924158-4671273.post@n4.nabble.com>
Message-ID: <555AF348.7080801@gmail.com>

Store ID can't get TCP_HIT. URL is changed.

19.05.15 3:18, Stakres ?????:
> Hi Yuri,
>
> Do you get a TCP_HIT with your rules ?
>  From my side, i get this: *X-Cache: MISS* from blablabla...
>
> Bye Fred
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271p4671273.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid-users at sernet.de  Tue May 19 08:36:39 2015
From: squid-users at sernet.de (Stefan Kuegler)
Date: Tue, 19 May 2015 10:36:39 +0200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <5559CF8C.7080106@treenet.co.nz>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE> <5559CF8C.7080106@treenet.co.nz>
Message-ID: <E1Yud0x-000Vvj-AD@intern.SerNet.DE>

Hi Amos.

Am 18.05.2015 um 13:39 schrieb Amos Jeffries:
> On 18/05/2015 11:17 p.m., Stefan Kuegler wrote:
>> Hi Yuri.
>>>
>>> http://i.imgur.com/mW7gNwD.png
>>>
>>> http://squidclamav.darold.net/config.html
>>>
>>> This is for squidclamav (I use it and have no problems with malware).
>>
>> I just installed squidclamav - but the behaviour is always the same. An
>> object which has been stored in squid-cache will not be detected by an
>> icap server because squid does not scan the body again:
>>
>> squidclamav.c(283) squidclamav_init_request_data: DEBUG initializing
>> request data handler.
>> pool hits:5 allocations: 1
>> Allocating from objects pool object 0
>> Requested service: squidclamav
>> squidclamav.c(337) squidclamav_check_preview_handler: DEBUG processing
>> preview header.
>> squidclamav.c(358) squidclamav_check_preview_handler: DEBUG X-Client-IP:
>> 192.168.216.54
>> squidclamav.c(1319) extract_http_info: DEBUG method GET
>> squidclamav.c(1330) extract_http_info: DEBUG url
>> http://www.intern/eicar_com.zip
>> squidclamav.c(389) squidclamav_check_preview_handler: DEBUG URL
>> requested: http://www.intern/eicar_com.zip
>> squidclamav.c(430) squidclamav_check_preview_handler: DEBUG
>> Content-Length: 0
>> squidclamav.c(449) squidclamav_check_preview_handler: DEBUG No body
>> data, allow 204
>> squidclamav.c(304) squidclamav_release_request_data: DEBUG Releasing
>> request data.
>> Storing to objects pool object 0
>> Log request to access log file /var/log/c-icap/access.log
>> Width: 0, Parameter:
>>
>> Any idea, how I can solve that problem. It seems that the only way to be
>> secure is to disable caching in squid. But I hope, this can't be the
>> solution.
>
>
> see my earlier reply.

Thank you for your answer.

Will the RESPMOD POSTCACHE vectoring point be available in future 
versions of squid?


Regards,
Stefan


From squid3 at treenet.co.nz  Tue May 19 08:53:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 20:53:50 +1200
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <555AF348.7080801@gmail.com>
References: <1431982739402-4671271.post@n4.nabble.com>
 <555A5799.9040701@gmail.com> <1431983924158-4671273.post@n4.nabble.com>
 <555AF348.7080801@gmail.com>
Message-ID: <555AFA1E.6090605@treenet.co.nz>

On 19/05/2015 8:24 p.m., Yuri Voinov wrote:
> Store ID can't get TCP_HIT. URL is changed.
> 
> 19.05.15 3:18, Stakres ?????:
>> Hi Yuri,
>>
>> Do you get a TCP_HIT with your rules ?
>>  From my side, i get this: *X-Cache: MISS* from blablabla...
>>


Also, X-Cache header can lie in HTTP/1.1 traffic. The logics producing
it is old HTTP/1.0-only code with no concept of revalidation. The cached
content which gets revalidated usually gets labelled "MISS" when objects
was cached but instead of "REFRESH" even if the cached content was sent
to the client.

Amos



From squid3 at treenet.co.nz  Tue May 19 09:04:22 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 21:04:22 +1200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <E1Yud0x-000Vvj-AD@intern.SerNet.DE>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE> <5559CF8C.7080106@treenet.co.nz>
 <E1Yud0x-000Vvj-AD@intern.SerNet.DE>
Message-ID: <555AFC96.6000404@treenet.co.nz>

On 19/05/2015 8:36 p.m., Stefan Kuegler wrote:
> 
> Will the RESPMOD POSTCACHE vectoring point be available in future
> versions of squid?
> 

Unkown at this point. Its a lot of work and extra complexity we are
generally trying to avoid adding.

However, Alex Rousskov from Measurement Factory put a project proposal
to squid-dev mid last year and I've not hear anything since. If its
seriously important for you I suggest getting in touch with him about
sponsoring it.

Amos



From squid3 at treenet.co.nz  Tue May 19 09:22:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 May 2015 21:22:50 +1200
Subject: [squid-users] squid does not send cached object to an
	icap-server
In-Reply-To: <555AFC96.6000404@treenet.co.nz>
References: <E1YpaKT-004oBN-9u@intern.SerNet.DE> <5548A0BB.1060404@gmail.com>
 <E1YpbHf-004qCG-AC@intern.SerNet.DE> <5548D92F.9070308@gmail.com>
 <E1YuJ3E-00HD0Y-7j@intern.SerNet.DE> <5559CF8C.7080106@treenet.co.nz>
 <E1Yud0x-000Vvj-AD@intern.SerNet.DE> <555AFC96.6000404@treenet.co.nz>
Message-ID: <555B00EA.8000207@treenet.co.nz>

On 19/05/2015 9:04 p.m., Amos Jeffries wrote:
> On 19/05/2015 8:36 p.m., Stefan Kuegler wrote:
>>
>> Will the RESPMOD POSTCACHE vectoring point be available in future
>> versions of squid?
>>
> 
> Unkown at this point. Its a lot of work and extra complexity we are
> generally trying to avoid adding.
> 
> However, Alex Rousskov from Measurement Factory put a project proposal
> to squid-dev mid last year and I've not hear anything since. If its
> seriously important for you I suggest getting in touch with him about
> sponsoring it.
> 

PS. You could achieve the same thing using two proxies. One to cache and
one to AV scan.

Something much like this:

workers 2

# frontend worker (proxy)
if ${process_number} = 1
 http_port 3128
 cache_peer localhost parent 8080 0 no-query
endif

# backend worker (cache)
if ${process_number} = 2
 http_port localhost:8080
 visible_hostname backend.local
 forwarded_for transparent
 # traffic is expected only to come from the other worker
 http_access deny !localhost

 cache_dir ...
 # config specific to the caching

endif
...
# shared config, including the RESPMOD pre-cache AV scanning callouts.


Amos



From vdoctor at neuf.fr  Tue May 19 09:40:54 2015
From: vdoctor at neuf.fr (Stakres)
Date: Tue, 19 May 2015 02:40:54 -0700 (PDT)
Subject: [squid-users] How to cache Chrome Installer ?
In-Reply-To: <555AF348.7080801@gmail.com>
References: <1431982739402-4671271.post@n4.nabble.com>
 <555A5799.9040701@gmail.com> <1431983924158-4671273.post@n4.nabble.com>
 <555AF348.7080801@gmail.com>
Message-ID: <1432028454397-4671286.post@n4.nabble.com>

Hi Yuri,
the url does not change, i use the same url for the tests:
http://r8---sn-n4g-jqbe.gvt1.com/edgedl/chrome/win/776B03BEAFB2810D/42.0.2311.152_chrome_installer.exe

it should work with the storeid.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/How-to-cache-Chrome-Installer-tp4671271p4671286.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From monahbaki at gmail.com  Tue May 19 10:41:00 2015
From: monahbaki at gmail.com (Monah Baki)
Date: Tue, 19 May 2015 06:41:00 -0400
Subject: [squid-users] PAC file on a squid proxy
Message-ID: <CALP3=x80Wrs3_Czt+nEhkv8tf6bxg9COXVPH+Nbix9G2chLCPw@mail.gmail.com>

Hi all,

Our upstream proxy (cloud based) requires a PAC file to be deployed on each
workstation. Is there a way to have a PAC file on a squid servers and then
have users use the local squid servers instead.


Thanks
Monah
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150519/69cdbb09/attachment.htm>

From vkukk at xvidservices.com  Tue May 19 10:43:13 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Tue, 19 May 2015 13:43:13 +0300
Subject: [squid-users] Squid 3.4.10 and sslcrtd
In-Reply-To: <5559DAE7.5050104@treenet.co.nz>
References: <5559CBA4.5080206@xvidservices.com>
 <5559DAE7.5050104@treenet.co.nz>
Message-ID: <555B13C1.1090807@xvidservices.com>

On 18/05/15 15:28, Amos Jeffries wrote:
> Having a directive commented out means the default value for it is used.
> There is a default helper built by --enable-ssl-crtd that gets used
> unless you specify otherwise.
>
> Currently Squid is not detecting that the helper is unused, so checks
> for its existence and attempts to run some. Some other helpers also have
> this problem.
>
> The workaround is to also explicitly configure:
>   sslcrtd_children 0

Unfortunately, this results in error:

2015/05/19 10:41:08| Processing: sslcrtd_children 0
2015/05/19 10:41:08| ERROR: The maximum number of processes cannot be 
less than 1.
FATAL: Bungled /etc/squid/conf.d/squid.conf line 12: sslcrtd_children 0
Squid Cache (Version 3.4.10): Terminated abnormally.
CPU Usage: 0.004 seconds = 0.003 user + 0.001 sys
Maximum Resident Size: 26320 KB
Page faults with physical i/o: 0

Veiko


From squid3 at treenet.co.nz  Tue May 19 13:06:58 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 01:06:58 +1200
Subject: [squid-users] PAC file on a squid proxy
In-Reply-To: <CALP3=x80Wrs3_Czt+nEhkv8tf6bxg9COXVPH+Nbix9G2chLCPw@mail.gmail.com>
References: <CALP3=x80Wrs3_Czt+nEhkv8tf6bxg9COXVPH+Nbix9G2chLCPw@mail.gmail.com>
Message-ID: <555B3572.7040500@treenet.co.nz>

On 19/05/2015 10:41 p.m., Monah Baki wrote:
> Hi all,
> 
> Our upstream proxy (cloud based) requires a PAC file to be deployed on each
> workstation. Is there a way to have a PAC file on a squid servers and then
> have users use the local squid servers instead.

Squid does not support PAC files for upstream peers.

Depending on whats inside it though you may be able to re-create the
logics inside it using squid.conf settings though.

Amos



From apani at yandex.ru  Tue May 19 13:12:50 2015
From: apani at yandex.ru (sp_)
Date: Tue, 19 May 2015 06:12:50 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <54F857C3.6050200@gmail.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
Message-ID: <1432041170469-4671291.post@n4.nabble.com>

Hi,

were there any improvements in squid 3.5 recently?
I've tried peek-n-spice again in 3.5.4, but again transparent proxy for
hosts using SNI is not working properly. 

My config for ssl-bump is the following:






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671291.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue May 19 14:08:06 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 02:08:06 +1200
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1432041170469-4671291.post@n4.nabble.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com>
Message-ID: <555B43C6.70101@treenet.co.nz>

On 20/05/2015 1:12 a.m., sp_ wrote:
> Hi,
> 
> were there any improvements in squid 3.5 recently?
> I've tried peek-n-spice again in 3.5.4, but again transparent proxy for
> hosts using SNI is not working properly. 
> 
> My config for ssl-bump is the following:
> 
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump splice step2 all
> ssl_bump bump all


Whats "not working" about it?

Amos



From sebag at vianetcon.com.ar  Tue May 19 15:41:40 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Tue, 19 May 2015 12:41:40 -0300
Subject: [squid-users] Storage mem in 3.5.4, not sure what is happening
Message-ID: <555B59B4.8070304@vianetcon.com.ar>

Hello everyone, we're having a problem, we updated from squid 3.5.3 to 
3.5.4 (bellow are the configuration options) and enabled rock fs.

The problem that arose is that I don't know if objects are being stored 
in RAM, but what I do know is that there are no MEM_HITs in the 
access.log file


This is how I enabled rock:
cache_dir rock  /cache/rock  512  max-size=32768


This is what I get from squidclient:

Cache information for squid:
         Hits as % of all requests:      5min: 3.5%, 60min: 3.6%
         Hits as % of bytes sent:        5min: 4.0%, 60min: 4.0%
         Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
         Disk hits as % of hit requests: 5min: 37.6%, 60min: 33.7%
         Storage Swap size:      63691356 KB
         Storage Swap capacity:   7.2% used, 92.8% free
         Storage Mem size:       1048576 KB
         Storage Mem capacity:   100.0% used,  0.0% free
         Mean Object Size:       382.79 KB
         Requests given to unlinkd:      0


And this is what I get from an SNMP query:

snmpwalk -c public  -v1 localhost:1611 -m /etc/squid/mib.txt cacheSysVMsize
SQUID-MIB::cacheSysVMsize.0 = INTEGER: 4304


Configuration options:

squid-3.5.3
./configure  --prefix=/usr/local --datadir=/usr/local/share 
--bindir=/usr/local/sbin --libexecdir=/usr/local/lib/squid 
--localstatedir=/var --sysconfdir=/etc/squid3 --enable-delay-pools 
--enable-ssl --enable-ssl-crtd --enable-linux-netfilter --enable-eui 
--enable-snmp --enable-gnuregex --enable-ltdl-convenience 
--enable-removal-policies="lru heap" --enable-http-violations 
--with-openssl --with-filedescriptors=24321 --enable-poll --enable-epoll

squid-3.5.4
./configure  --prefix=/usr/local --datadir=/usr/local/share 
--bindir=/usr/local/sbin --libexecdir=/usr/local/lib/squid 
--localstatedir=/var --sysconfdir=/etc/squid3 --enable-delay-pools 
--enable-ssl --enable-ssl-crtd --enable-linux-netfilter --enable-eui 
--enable-snmp --enable-gnuregex --enable-ltdl-convenience 
--enable-removal-policies="lru heap" --enable-http-violations 
--with-openssl --with-filedescriptors=24321 --enable-poll --enable-epoll 
--enable-storeio=aufs,ufs,rock'



Thanks,
Sebastian


From alex at samad.com.au  Wed May 20 04:36:31 2015
From: alex at samad.com.au (Alex Samad)
Date: Wed, 20 May 2015 14:36:31 +1000
Subject: [squid-users] https_port question
Message-ID: <CAJ+Q1PWp=EFS-EjQtW_EuoZms1ctOzHwyZSTx3NvjxQZ0Q=Hbw@mail.gmail.com>

Hi

Looking at http://www.squid-cache.org/Doc/config/https_port/

I am trying to work out where I place intermediary CA certs.

I am setting up a reverse proxy setup, trying to terminate the SSL here.

cert=  points to SSL certificate PEM file, this seems to be a public
and private combo file. can I also place intermediary here ?

Alex


From squid3 at treenet.co.nz  Wed May 20 04:53:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 16:53:33 +1200
Subject: [squid-users] Storage mem in 3.5.4, not sure what is happening
In-Reply-To: <555B59B4.8070304@vianetcon.com.ar>
References: <555B59B4.8070304@vianetcon.com.ar>
Message-ID: <555C134D.4050901@treenet.co.nz>

On 20/05/2015 3:41 a.m., Sebastian Goicochea wrote:
> Hello everyone, we're having a problem, we updated from squid 3.5.3 to
> 3.5.4 (bellow are the configuration options) and enabled rock fs.
> 
> The problem that arose is that I don't know if objects are being stored
> in RAM, but what I do know is that there are no MEM_HITs in the
> access.log file
> 
> 
> This is how I enabled rock:
> cache_dir rock  /cache/rock  512  max-size=32768
> 
> 
> This is what I get from squidclient:

Whats the "Start Time" information displayed at the top of that report?
 and does it remain unchanged across many minutes?

I suspect that with no memory hits and sine 5min == 60min hits your
Squid may be in a cycle of crashes or assertions.

NOTE: to enable rock and other SMP features you need /var/run/squid and
/dev/shm to exist. Some OS seem not to create or mount them by default.

Amos



From squid3 at treenet.co.nz  Wed May 20 04:58:56 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 16:58:56 +1200
Subject: [squid-users] https_port question
In-Reply-To: <CAJ+Q1PWp=EFS-EjQtW_EuoZms1ctOzHwyZSTx3NvjxQZ0Q=Hbw@mail.gmail.com>
References: <CAJ+Q1PWp=EFS-EjQtW_EuoZms1ctOzHwyZSTx3NvjxQZ0Q=Hbw@mail.gmail.com>
Message-ID: <555C1490.3070208@treenet.co.nz>

On 20/05/2015 4:36 p.m., Alex Samad wrote:
> Hi
> 
> Looking at http://www.squid-cache.org/Doc/config/https_port/
> 
> I am trying to work out where I place intermediary CA certs.

In the file pointed to by cacert= parameter seems to work for some.


> 
> I am setting up a reverse proxy setup, trying to terminate the SSL here.
> 
> cert=  points to SSL certificate PEM file, this seems to be a public
> and private combo file. can I also place intermediary here ?

You can place any or all of the server keys in there. But due to
oddities in the OpenSSL API you need both the cert= and cacert=
https_port parameters pointing at it to use the intermediary ones inside.

Amos



From namidya at gmail.com  Wed May 20 05:03:46 2015
From: namidya at gmail.com (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0JvQvtC30LjRhtC60LjQuQ==?=)
Date: Wed, 20 May 2015 17:03:46 +1200
Subject: [squid-users] pass ssl through Squid reverse proxy
Message-ID: <CAJRnUm6qor8xSvvV6Wjk25O2=op=A6-mnAAgDdJp0cM7Nj-ypw@mail.gmail.com>

Hello,

I have a requirement to setup a reverse proxy for a secure connection to a
server where tomcat is working.
Services on tomcat already have ssl setup, but as tomcat doesn't have
explicit certificate and a key file for ssl connection, I'd like to let the
https traffic from a client to pass through the reverse proxy directly to a
server and use encryption on a server instead of on reverse proxy.

Can anyone please give an idea if it possible to do?

Thank you for comments.

Dmitry.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/485d91c2/attachment.htm>

From h.wahl at ifw-dresden.de  Wed May 20 05:44:49 2015
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Wed, 20 May 2015 07:44:49 +0200
Subject: [squid-users] Squid 3.5.4 + OpenBSD 5.7 + ROCK store = kid1
 registration timed out
Message-ID: <555C1F51.3030204@ifw-dresden.de>

Good morning,

I try to use rock store for cache in OpenBSD 5.7 with Squid 3.5.4. This
should require SMP for the disker processes. Apparently the kids start,
but what I get further is this:

May 20 07:40:26 squid02 squid[29404]: Starting Squid Cache version 3.5.4
for x86_64-unknown-openbsd5.7...
May 20 07:40:26 squid02 squid[29404]: Service Name: squid
May 20 07:40:26 squid02 squid[29404]: Process ID 29404
May 20 07:40:26 squid02 squid[29404]: Process Roles: coordinator
May 20 07:40:26 squid02 squid[29404]: With 32768 file descriptors available
May 20 07:40:26 squid02 squid[15546]: Starting Squid Cache version 3.5.4
for x86_64-unknown-openbsd5.7...
May 20 07:40:26 squid02 squid[5339]: Starting Squid Cache version 3.5.4
for x86_64-unknown-openbsd5.7...
May 20 07:40:26 squid02 squid[15546]: Service Name: squid
May 20 07:40:26 squid02 squid[5339]: Service Name: squid
May 20 07:40:26 squid02 squid[15546]: Process ID 15546
May 20 07:40:26 squid02 squid[5339]: Process ID 5339
May 20 07:40:26 squid02 squid[15546]: Process Roles: disker
May 20 07:40:26 squid02 squid[5339]: Process Roles: worker
May 20 07:40:26 squid02 squid[15546]: With 32768 file descriptors available
May 20 07:40:26 squid02 squid[5339]: With 32768 file descriptors available
May 20 07:40:26 squid02 squid[29404]: WARNING: Can't find current
directory, getcwd: (13) Permission denied
May 20 07:40:26 squid02 squid[15546]: WARNING: Can't find current
directory, getcwd: (13) Permission denied
May 20 07:40:26 squid02 squid[5339]: WARNING: Can't find current
directory, getcwd: (13) Permission denied
May 20 07:40:26 squid02 squid[29404]: Squid plugin modules loaded: 0
May 20 07:40:26 squid02 squid[5339]: Sending SNMP messages from 0.0.0.0:3401
May 20 07:40:26 squid02 squid[5339]: Squid plugin modules loaded: 0
May 20 07:40:26 squid02 squid[15546]: Squid plugin modules loaded: 0
May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.2:3128
May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 127.0.0.1:3128
May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.3:3128
May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.11:3128
May 20 07:40:32 squid02 squid[5339]: kid1 registration timed out
May 20 07:40:32 squid02 squid[15546]: kid2 registration timed out

The kids registration times out. Any idea where to adjust what?

Thanks and regards
Henri

-- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Dr. Doreen Kirmse

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/ee001c27/attachment.sig>

From apani at yandex.ru  Wed May 20 08:22:52 2015
From: apani at yandex.ru (sp_)
Date: Wed, 20 May 2015 01:22:52 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <555B43C6.70101@treenet.co.nz>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
Message-ID: <1432110172715-4671299.post@n4.nabble.com>

Hello Amos,

I still get IP-addresses instead of domain names:





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671299.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From emperor.cu at gmail.com  Wed May 20 10:18:02 2015
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Wed, 20 May 2015 12:18:02 +0200
Subject: [squid-users] squid with sslump compile
Message-ID: <CALBaCdtoJB0mdzqZUx3KbXg9meDi-JBAegqhxeaYOYaxsmF2AQ@mail.gmail.com>

Hi i'm compiling squid 3.4.8 with ssl but not create the ssl_crtd to use it
after .
the compile process runn fine.. and works ok but

i use --enable-ssl and checking on lists of squid i found was old way to
compile
then i try it again with --with-openssl and nothing on the

/usr/lib/squid3/ not appears that ssl_crtd .... any idea? thanxs
i'm using Debian 8

 ~$ sudo squid3 -v
Squid Cache: Version 3.4.8
Debian linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstat
edir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/et
c/squid3' '--mandir=/usr/share/man' '--with-openssl' '--enable-ssl-certd'
'--enable-inline' '--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-remov
al-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests'
'--enable-icap-client' '--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain
,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerb
eros_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--
disable-translation' '--with-swapdir=/var/spool/squid3'
'--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
'--with-filedescriptors=65536' '--with-large-files' '--with-default
-user=proxy' '--with-open-ssl=/etc/ssl/' '--enable-build-info=Debian linux'
'--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2
-fPIE -fstack-protector-strong -Wformat -We
rror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'


-- 
Antonio Pe?a
Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/6b79a6e1/attachment.htm>

From squid3 at treenet.co.nz  Wed May 20 10:41:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 22:41:02 +1200
Subject: [squid-users] pass ssl through Squid reverse proxy
In-Reply-To: <CAJRnUm6qor8xSvvV6Wjk25O2=op=A6-mnAAgDdJp0cM7Nj-ypw@mail.gmail.com>
References: <CAJRnUm6qor8xSvvV6Wjk25O2=op=A6-mnAAgDdJp0cM7Nj-ypw@mail.gmail.com>
Message-ID: <555C64BE.2080302@treenet.co.nz>

On 20/05/2015 5:03 p.m., ??????? ???????? wrote:
> Hello,
> 
> I have a requirement to setup a reverse proxy for a secure connection to a
> server where tomcat is working.
> Services on tomcat already have ssl setup, but as tomcat doesn't have
> explicit certificate and a key file for ssl connection, I'd like to let the
> https traffic from a client to pass through the reverse proxy directly to a
> server and use encryption on a server instead of on reverse proxy.
> 
> Can anyone please give an idea if it possible to do?

What you are asking for is not possible with Squid. But its also
unnecessary and very much *not* reverse-proxy.


What reverse-proxy Squid do is terminate the client TLS connection then
proxy the embeded HTTP request(s) to the backend server(s). Optionally
using TLS on the connections between Squid and the server (Tomcat).

Like so:
 https_port 443 accel defaultdomain=example.com \
    ssl cert=/path/to/example.com.pem

 cache_peer tomcat.local 443 0 originserver ssl

... and the relevant TLS parameters. With DNS of course pointing at the
Squid instead of tomcat.

THe iwki config example
<http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate>
should have what you need.

Amos



From squid3 at treenet.co.nz  Wed May 20 10:51:55 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 22:51:55 +1200
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1432110172715-4671299.post@n4.nabble.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com>
Message-ID: <555C674B.8010005@treenet.co.nz>

On 20/05/2015 8:22 p.m., sp_ wrote:
> Hello Amos,
> 
> I still get IP-addresses instead of domain names:
> 

That appears to be because the request are just denied. Not peeked or
spliced.

When a new TCP connection is intercepted Squid starts with only the IP
address. Generates a fake CONNECT request from that detail, and checks
http_access for whether to allow/deny that connection. Only if that is
allowed will bumping checks begin to take place - during which SNI
becomes available.

 It seems to me that your http_access logic is actively denying the
initial CONNECT request when only IP is known.

Amos



From squid3 at treenet.co.nz  Wed May 20 10:59:49 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 22:59:49 +1200
Subject: [squid-users] Squid 3.5.4 + OpenBSD 5.7 + ROCK store = kid1
 registration timed out
In-Reply-To: <555C1F51.3030204@ifw-dresden.de>
References: <555C1F51.3030204@ifw-dresden.de>
Message-ID: <555C6925.4080008@treenet.co.nz>

On 20/05/2015 5:44 p.m., Henri Wahl wrote:
> Good morning,
> 
> I try to use rock store for cache in OpenBSD 5.7 with Squid 3.5.4. This
> should require SMP for the disker processes. Apparently the kids start,
> but what I get further is this:
> 
> May 20 07:40:26 squid02 squid[29404]: Starting Squid Cache version 3.5.4
> for x86_64-unknown-openbsd5.7...
> May 20 07:40:26 squid02 squid[29404]: Service Name: squid
> May 20 07:40:26 squid02 squid[29404]: Process ID 29404
> May 20 07:40:26 squid02 squid[29404]: Process Roles: coordinator
> May 20 07:40:26 squid02 squid[29404]: With 32768 file descriptors available
> May 20 07:40:26 squid02 squid[15546]: Starting Squid Cache version 3.5.4
> for x86_64-unknown-openbsd5.7...
> May 20 07:40:26 squid02 squid[5339]: Starting Squid Cache version 3.5.4
> for x86_64-unknown-openbsd5.7...
> May 20 07:40:26 squid02 squid[15546]: Service Name: squid
> May 20 07:40:26 squid02 squid[5339]: Service Name: squid
> May 20 07:40:26 squid02 squid[15546]: Process ID 15546
> May 20 07:40:26 squid02 squid[5339]: Process ID 5339
> May 20 07:40:26 squid02 squid[15546]: Process Roles: disker
> May 20 07:40:26 squid02 squid[5339]: Process Roles: worker
> May 20 07:40:26 squid02 squid[15546]: With 32768 file descriptors available
> May 20 07:40:26 squid02 squid[5339]: With 32768 file descriptors available
> May 20 07:40:26 squid02 squid[29404]: WARNING: Can't find current
> directory, getcwd: (13) Permission denied

Are you running Squid as root?
The master process needs to be started as root to assign the
low-privilege workers the correct privilege for things they do.

Not doing so would lead to effects like the workers not being able to
communicate with the coordinator...


> May 20 07:40:26 squid02 squid[15546]: WARNING: Can't find current
> directory, getcwd: (13) Permission denied
> May 20 07:40:26 squid02 squid[5339]: WARNING: Can't find current
> directory, getcwd: (13) Permission denied
> May 20 07:40:26 squid02 squid[29404]: Squid plugin modules loaded: 0
> May 20 07:40:26 squid02 squid[5339]: Sending SNMP messages from 0.0.0.0:3401
> May 20 07:40:26 squid02 squid[5339]: Squid plugin modules loaded: 0
> May 20 07:40:26 squid02 squid[15546]: Squid plugin modules loaded: 0
> May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.2:3128
> May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 127.0.0.1:3128
> May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.3:3128
> May 20 07:40:32 squid02 squid[5339]: Closing HTTP port 172.19.13.11:3128
> May 20 07:40:32 squid02 squid[5339]: kid1 registration timed out
> May 20 07:40:32 squid02 squid[15546]: kid2 registration timed out
> 
> The kids registration times out. Any idea where to adjust what?

The worker startup is taking too long. We tend to see this when Squid
with very large caches are started and need to do a full "DIRTY" scan of
the cache files.

Its hard to say for sure what the exact problem is though without lots
of details about the config and cache.log outut from the workers.

Amos


From squid3 at treenet.co.nz  Wed May 20 11:06:32 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 23:06:32 +1200
Subject: [squid-users] Squid 3.4.10 and sslcrtd
In-Reply-To: <555B13C1.1090807@xvidservices.com>
References: <5559CBA4.5080206@xvidservices.com>
 <5559DAE7.5050104@treenet.co.nz> <555B13C1.1090807@xvidservices.com>
Message-ID: <555C6AB8.2030102@treenet.co.nz>

On 19/05/2015 10:43 p.m., Veiko Kukk wrote:
> On 18/05/15 15:28, Amos Jeffries wrote:
>> Having a directive commented out means the default value for it is used.
>> There is a default helper built by --enable-ssl-crtd that gets used
>> unless you specify otherwise.
>>
>> Currently Squid is not detecting that the helper is unused, so checks
>> for its existence and attempts to run some. Some other helpers also have
>> this problem.
>>
>> The workaround is to also explicitly configure:
>>   sslcrtd_children 0
> 
> Unfortunately, this results in error:
> 
> 2015/05/19 10:41:08| Processing: sslcrtd_children 0
> 2015/05/19 10:41:08| ERROR: The maximum number of processes cannot be
> less than 1.
> FATAL: Bungled /etc/squid/conf.d/squid.conf line 12: sslcrtd_children 0
> Squid Cache (Version 3.4.10): Terminated abnormally.
> CPU Usage: 0.004 seconds = 0.003 user + 0.001 sys
> Maximum Resident Size: 26320 KB
> Page faults with physical i/o: 0
> 

Ouch, sorry. Maybe this will work:
 sslcrtd_children 1 startup=0

Otherwise you are left with re-building Squid. --disable-ssl-crtd would
do if you never want to use the helper. Or the patch now applied on
Squid-4
(<http://www.squid-cache.org/Versions/v4/changesets/squid-4-14080.patch>) should
also apply fairly easily to your version.

Amos


From squid3 at treenet.co.nz  Wed May 20 11:09:50 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 May 2015 23:09:50 +1200
Subject: [squid-users] squid with sslump compile
In-Reply-To: <CALBaCdtoJB0mdzqZUx3KbXg9meDi-JBAegqhxeaYOYaxsmF2AQ@mail.gmail.com>
References: <CALBaCdtoJB0mdzqZUx3KbXg9meDi-JBAegqhxeaYOYaxsmF2AQ@mail.gmail.com>
Message-ID: <555C6B7E.7040506@treenet.co.nz>

On 20/05/2015 10:18 p.m., Tony Pe?a wrote:
> Hi i'm compiling squid 3.4.8 with ssl but not create the ssl_crtd to use it
> after .
> the compile process runn fine.. and works ok but
> 
> i use --enable-ssl and checking on lists of squid i found was old way to
> compile
> then i try it again with --with-openssl and nothing on the
> 
> /usr/lib/squid3/ not appears that ssl_crtd .... any idea? thanxs
> i'm using Debian 8
> 
>  ~$ sudo squid3 -v
> Squid Cache: Version 3.4.8
> Debian linux
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstat
> edir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
> '--disable-maintainer-mode' '--disable-dependency-tracking'
> '--disable-silent-rules' '--datadir=/usr/share/squid3' '--sysconfdir=/et
> c/squid3' '--mandir=/usr/share/man' '--with-openssl' '--enable-ssl-certd'

That should be --enable-ssl-crtd  not "certd". This is very probably
your problem.

NP: You also need the libssl-dev package installed to build any of the
OpenSSL functionality.

Amos



From apani at yandex.ru  Wed May 20 11:33:39 2015
From: apani at yandex.ru (sp_)
Date: Wed, 20 May 2015 04:33:39 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <555C674B.8010005@treenet.co.nz>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
Message-ID: <1432121619772-4671306.post@n4.nabble.com>

I have tried to remove all the restrictions, but still:



-SP



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671306.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vrogoziansky.squid at gmail.com  Wed May 20 13:45:15 2015
From: vrogoziansky.squid at gmail.com (Vadim Rogoziansky)
Date: Wed, 20 May 2015 16:45:15 +0300
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1432121619772-4671306.post@n4.nabble.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com>
Message-ID: <555C8FEB.1090805@gmail.com>

Hi,

check something like this

acl step1 at_step SslBump1
ssl_bump stare step1 all

acl sslBumpDeniedDstDomain ssl::server_name google.com
ssl_bump splice sslBumpDeniedDstDomain

ssl_bump bump all



On 5/20/2015 2:33 PM, sp_ wrote:
> I have tried to remove all the restrictions, but still:
>
>
>
> -SP
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671306.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


---
This email is free from viruses and malware because avast! Antivirus protection is active.
http://www.avast.com



From vkukk at xvidservices.com  Wed May 20 14:01:24 2015
From: vkukk at xvidservices.com (Veiko Kukk)
Date: Wed, 20 May 2015 17:01:24 +0300
Subject: [squid-users] Squid 3.4.10 and sslcrtd
In-Reply-To: <555C6AB8.2030102@treenet.co.nz>
References: <5559CBA4.5080206@xvidservices.com>
 <5559DAE7.5050104@treenet.co.nz> <555B13C1.1090807@xvidservices.com>
 <555C6AB8.2030102@treenet.co.nz>
Message-ID: <555C93B4.8030802@xvidservices.com>

On 20/05/15 14:06, Amos Jeffries wrote:
> Ouch, sorry. Maybe this will work:
>   sslcrtd_children 1 startup=0
>
> Otherwise you are left with re-building Squid. --disable-ssl-crtd would
> do if you never want to use the helper. Or the patch now applied on
> Squid-4
> (<http://www.squid-cache.org/Versions/v4/changesets/squid-4-14080.patch>) should
> also apply fairly easily to your version.

Thank you, 'sslcrtd_children 1 startup=0' works.

Veiko


From apani at yandex.ru  Wed May 20 14:16:28 2015
From: apani at yandex.ru (sp_)
Date: Wed, 20 May 2015 07:16:28 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <555C8FEB.1090805@gmail.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
Message-ID: <1432131388558-4671309.post@n4.nabble.com>

Hi Vadim,

I've tried using these options - did not help.

I've even tried to add %rd to logs, but still, IPs are show:




Vadim Rogoziansky wrote
> Hi,
> 
> check something like this
> 
> acl step1 at_step SslBump1
> ssl_bump stare step1 all
> 
> acl sslBumpDeniedDstDomain ssl::server_name google.com
> ssl_bump splice sslBumpDeniedDstDomain
> 
> ssl_bump bump all





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671309.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From nickb at itsou.com  Wed May 20 14:42:26 2015
From: nickb at itsou.com (Nick Belnap)
Date: Wed, 20 May 2015 14:42:26 +0000
Subject: [squid-users] Novice question on TPROXY and SSL-BUMP behavior
Message-ID: <BY2PR05MB142B37A683F61D25D34B2B2ACC20@BY2PR05MB142.namprd05.prod.outlook.com>

I've been tasked with preventing a client's users from accessing consumer Gmail accounts while only accessing their corporate Google Apps accounts.  Google gives an overview here:  https://support.google.com/a/answer/1668854?hl=en.

So, I've setup Squid 3.54 on CentOS 7 with ssl-bump and dynamic certificates and "request_header_add".  When manually configuring proxy settings on my browser I get the desired result.  I also see TCP_MISS in the Squid access.log file -- here's a sample:

1432131282.366     85 192.168.6.134 TCP_MISS/200 521 GET https://mail.google.com/mail/images/cleardot.gif? - HIER_DIRECT/216.58.217.37 image/gif
1432131282.373     40 192.168.6.134 TCP_MISS/200 496 POST https://clients1.google.com/tbproxy/af/query? - HIER_DIRECT/216.58.217.46 text/xml
1432131286.863    109 192.168.6.134 TCP_MISS/200 1191 POST https://accounts.google.com/accountLoginInfoXhr - HIER_DIRECT/216.58.217.45 application/javascript
1432131286.930     41 192.168.6.134 TCP_MISS/200 501 POST https://clients1.google.com/tbproxy/af/query? - HIER_DIRECT/216.58.217.46 text/xml
1432131287.673  10190 192.168.6.134 TCP_TUNNEL/200 3888 CONNECT gmail.com:443 - HIER_DIRECT/216.58.217.37 -
1432131289.543     74 192.168.6.134 TCP_MISS/302 2930 POST https://accounts.google.com/ServiceLoginAuth - HIER_DIRECT/216.58.217.45 text/html
1432131289.690    140 192.168.6.134 TCP_MISS/302 2900 GET https://accounts.google.com/CheckCookie? - HIER_DIRECT/216.58.217.45 text/html
1432131289.828     82 192.168.6.134 TCP_MISS/302 891 GET https://mail.google.com/mail/? - HIER_DIRECT/216.58.217.37 text/html
1432131289.911     79 192.168.6.134 TCP_MISS/200 1884 GET https://accounts.google.com/b/0/DomainRestrictedNetwork? - HIER_DIRECT/216.58.217.45 text/html
1432131289.969     33 192.168.6.134 TCP_MISS/200 4353 GET https://www.google.com/intl/en/images/logos/accounts_logo.gif - HIER_DIRECT/216.58.217.36 image/gif

I'm trying to set up this single machine as a transparent bridge with Squid so that I can go throw it in between the LAN and the router/firewall and just have it intercept web traffic without having to reconfigure clients (Windows desktops wouldn't be a big deal but mobile devices would be a problem).  So, with that in mind I've got 2 NICs on the box and have configured it as a bridge.  I'm using ebtables to redirect traffic for port 80 and 443 up to iptables which then in turn redirects to Squid using TPROXY.  All this seems to work.  Here's my ebtables entries:

# inbound traffic
ebtables -t broute -A BROUTING -p IPv4 --ip-proto tcp --ip-dport 80 -j redirect --redirect-target DROP
ebtables -t broute -A BROUTING -p IPv4 --ip-proto tcp --ip-dport 443 -j redirect --redirect-target DROP

# returning outbound traffic
ebtables -t broute -A BROUTING -p IPv4 --ip-proto tcp --ip-sport 80 -j redirect --redirect-target DROP
ebtables -t broute -A BROUTING -p IPv4 --ip-proto tcp --ip-sport 443 -j redirect --redirect-target DROP

Here's my iptables entries:

## interface facing clients
CLIENT_IFACE=ens4
## interface facing Internet
INET_IFACE=enp1s0

#Setup DIVERT chain to mark packets:
iptables -t mangle -N DIVERT
#Use DIVERT to prevent existing connections going through TPROXY twice:
iptables  -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
#DIVERT chain: mark packets and accept
iptables -t mangle -A DIVERT -j MARK --set-mark 1
iptables -t mangle -A DIVERT -j ACCEPT
#Mark all other (new) packets and use TPROXY to pass into Squid:
iptables  -t mangle -A PREROUTING -i $CLIENT_IFACE -p tcp --dport 80 -j TPROXY --on-ip 0.0.0.0 --on-port 3126 --tproxy-mark 0x1/0x1
iptables  -t mangle -A PREROUTING -i $INET_IFACE -p tcp --sport 80 -j MARK --set-mark 0x1/0x1
iptables  -t mangle -A PREROUTING -i $CLIENT_IFACE -p tcp --dport 443 -j TPROXY --on-ip 0.0.0.0 --on-port 3127 --tproxy-mark 0x1/0x1
iptables  -t mangle -A PREROUTING -i $INET_IFACE -p tcp --sport 443 -j MARK --set-mark 0x1/0x1

( I have Squid listening on 3126 for HTTP and TPROXY, 3127 for HTTPS, TPROXY and SSLBUMP, and 3128 for manual connections and SSLBUMP (see Squid config below).

I also have these entries among other things for TPROXY to work:

ip rule add fwmark 1/1 table 100
ip route add local 0.0.0.0/0 dev lo table 100

So, with all this in place, when I access the web without manually configuring my browsers proxy settings I am able to browse using this transparent redirect mode.  However, the sslbump does not seem to be working right in this mode and thus my Google "request_header_add" does not seem to be working.   Here's a sample of what I see in Squid's access.log when going to Gmail through the transparent redirection:

1432132109.580  16670 192.168.6.133 TCP_TUNNEL/200 4791 CONNECT 98.139.225.168:443 - ORIGINAL_DST/98.139.225.168 -
1432132109.581  19654 192.168.6.133 TCP_TUNNEL/200 6480 CONNECT 98.137.201.111:443 - ORIGINAL_DST/98.137.201.111 -
1432132109.582  19655 192.168.6.133 TCP_TUNNEL/200 6480 CONNECT 98.137.201.111:443 - ORIGINAL_DST/98.137.201.111 -
1432132109.582  16655 192.168.6.133 TCP_TUNNEL/200 2765 CONNECT 152.163.66.141:443 - ORIGINAL_DST/152.163.66.141 -
1432132109.582  19605 192.168.6.133 TCP_TUNNEL/200 6496 CONNECT 98.138.74.35:443 - ORIGINAL_DST/98.138.74.35 -
1432132109.582  19535 192.168.6.133 TCP_TUNNEL/200 3636 CONNECT 68.142.123.254:443 - ORIGINAL_DST/68.142.123.254 -
1432132109.583  19535 192.168.6.133 TCP_TUNNEL/200 3636 CONNECT 68.142.123.254:443 - ORIGINAL_DST/68.142.123.254 -
1432132109.583  19544 192.168.6.133 TCP_TUNNEL/200 4835 CONNECT 98.138.250.100:443 - ORIGINAL_DST/98.138.250.100 -
1432132109.583  19545 192.168.6.133 TCP_TUNNEL/200 4835 CONNECT 98.138.250.100:443 - ORIGINAL_DST/98.138.250.100 -
1432132119.593  12771 192.168.6.133 TCP_TUNNEL/200 3861 CONNECT 216.58.217.36:443 - ORIGINAL_DST/216.58.217.36 -

Here's my squid.conf file:

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl Google dstdomain .google.com

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

http_port 3126 tproxy
https_port 3127 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem
http_port 3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem

#always_direct allow all
ssl_bump server-first Google
#sslproxy_cert_error deny all
#sslproxy_flags DONT_VERIFY_PEER

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
shutdown_lifetime 1 second

request_header_add X-GoogApps-Allowed-Domains "mydomain.com" Google

Being a novice at Squid and iptables I've done a lot of Googling to get this far but have hit the wall I think with this problem.  Can anyone see why the ssl-bump might not be working for tproxy connections?

What am I missing?


From sebag at vianetcon.com.ar  Wed May 20 14:50:42 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Wed, 20 May 2015 11:50:42 -0300
Subject: [squid-users] Storage mem in 3.5.4, not sure what is happening
In-Reply-To: <555C134D.4050901@treenet.co.nz>
References: <555B59B4.8070304@vianetcon.com.ar>
 <555C134D.4050901@treenet.co.nz>
Message-ID: <555C9F42.5030603@vianetcon.com.ar>

Hello Amos,

Here's up time
Start Time:    Wed, 20 May 2015 14:33:02 GMT
Current Time:    Wed, 20 May 2015 14:41:46 GMT

It's a short period of time because we've been restariting it.

/var/run/squid and /dev/shm exist

# ls /var/run/squid -1
squid-coordinator.ipc
squid-kid-1.ipc
squid-kid-2.ipc

# ls /dev/shm -1
squid-cache1.p1.rock_map_anchors.shm
squid-cache1.p1.rock_map_slices.shm
squid-cache1.p1.rock_spaces.shm
squid-cache_mem_ex.shm
squid-cache_mem_map_anchors.shm
squid-cache_mem_map_slices.shm
squid-cache_mem_space.shm
squid-cf__metadata.shm
squid-cf__queues.shm
squid-cf__readers.shm
squid-io_file__metadata.shm
squid-io_file__queues.shm
squid-io_file__readers.shm
squid-squid-page-pool.shm

We've found something interesting: disabling rock gets hot objets out of 
0 and we get MEM_HITs right away.

Cache information for squid:
     Hits as % of all requests:    5min: 5.0%, 60min: 4.6%
     Hits as % of bytes sent:    5min: 10.7%, 60min: 8.8%
     Memory hits as % of hit requests:    5min: 36.6%, 60min: 36.9%
     Disk hits as % of hit requests:    5min: 38.3%, 60min: 40.7%
     Storage Swap size:    173674816 KB
     Storage Swap capacity:    19.7% used, 80.3% free
     Storage Mem size:    115528 KB
     Storage Mem capacity:    11.0% used, 89.0% free
     Mean Object Size:    526.13 KB
     Requests given to unlinkd:    0

Internal Data Structures:
     337773 StoreEntries
       8536 StoreEntries with MemObjects
       8382 Hot Object Cache Items
     330097 on-disk objects



Is there any relation between rock beeing active and storage mem?


Thanks a lot,
Sebastian


El 20/05/15 a las 01:53, Amos Jeffries escribi?:
> On 20/05/2015 3:41 a.m., Sebastian Goicochea wrote:
>> Hello everyone, we're having a problem, we updated from squid 3.5.3 to
>> 3.5.4 (bellow are the configuration options) and enabled rock fs.
>>
>> The problem that arose is that I don't know if objects are being stored
>> in RAM, but what I do know is that there are no MEM_HITs in the
>> access.log file
>>
>>
>> This is how I enabled rock:
>> cache_dir rock  /cache/rock  512  max-size=32768
>>
>>
>> This is what I get from squidclient:
> Whats the "Start Time" information displayed at the top of that report?
>   and does it remain unchanged across many minutes?
>
> I suspect that with no memory hits and sine 5min == 60min hits your
> Squid may be in a cycle of crashes or assertions.
>
> NOTE: to enable rock and other SMP features you need /var/run/squid and
> /dev/shm to exist. Some OS seem not to create or mount them by default.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From israel870730 at gmail.com  Wed May 20 17:17:17 2015
From: israel870730 at gmail.com (Israel Romero Garcia)
Date: Wed, 20 May 2015 13:17:17 -0400
Subject: [squid-users] need help plissss
Message-ID: <CADTgVuHC0YRHh=SNEhduZq=GsbMdQGTRZM--NxLQwhAbgmezPQ@mail.gmail.com>

Hello people, sorry but my english that no is very good,although squid is a
lenguaje international.

I need help because my squid does not allow a group of users on the LAN to
connect to an FTPS (FTP + SSL) with passive port range 10000 to 12000, I
would appreciate any help, Greetings
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/fb46ad0d/attachment.htm>

From stan.prescott at gmail.com  Wed May 20 17:36:31 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 20 May 2015 12:36:31 -0500
Subject: [squid-users] New server_name acl causes fatal error starting Squid
	3.5.4
Message-ID: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>

After a diversion getting SquidClamAV working, i am back to trying to get
peek and splice working. I am trying to put together information from
previous recommendations I have received. Right now, I can't get the
server_name acl working. When I put this in my squid.conf

*acl nobumpSites ssl:server_name .example.com <http://example.com>*

I get a fatal error starting squid  using that acl saying the acl is
"Bungled".

Is the form of the acl incorrect?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/1da68483/attachment.htm>

From stan.prescott at gmail.com  Wed May 20 17:45:28 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 20 May 2015 12:45:28 -0500
Subject: [squid-users] New server_name acl causes fatal error starting
	Squid 3.5.4
In-Reply-To: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
Message-ID: <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>

Never mind. I figured the acl out. I was using someone else's instructions
who accidentally left out the double :: *ssl::server_name* using just a
single :.

On Wed, May 20, 2015 at 12:36 PM, Stanford Prescott <stan.prescott at gmail.com
> wrote:

> After a diversion getting SquidClamAV working, i am back to trying to get
> peek and splice working. I am trying to put together information from
> previous recommendations I have received. Right now, I can't get the
> server_name acl working. When I put this in my squid.conf
>
> *acl nobumpSites ssl:server_name .example.com <http://example.com>*
>
> I get a fatal error starting squid  using that acl saying the acl is
> "Bungled".
>
> Is the form of the acl incorrect?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/e2eb9d53/attachment.htm>

From stan.prescott at gmail.com  Wed May 20 18:03:49 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Wed, 20 May 2015 13:03:49 -0500
Subject: [squid-users] New server_name acl causes fatal error starting
	Squid 3.5.4
In-Reply-To: <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
 <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
Message-ID: <CANLNtGTYFZn1yT11FuiBUMsH_GH1RWDyapRZ=T2PnSjjPZk7xQ@mail.gmail.com>

I think I finally figured out how to not bump certain sites and to bump all
others. I put this in squid.conf







*acl step1 at_step SslBump1acl step2 at_step SslBump2acl nobumpSites
ssl::server_name .wellsfargo.com <http://wellsfargo.com>ssl_bump peek
step1ssl_bump splice step2 nobumpSitesssl_bump bump all*

When I check the access log I see that the wellsfargo.com com only appears
as http://wellsfargo.com without any of the full URL but any other https
site I see as, for example, https://yahoo.com with the full URL.

Are the lines in the squid.conf correct and is it doing what I want it do,
which is to not bump the nobumpSites and bump all other sites that are not
in nobumpSites?

On Wed, May 20, 2015 at 12:45 PM, Stanford Prescott <stan.prescott at gmail.com
> wrote:

> Never mind. I figured the acl out. I was using someone else's instructions
> who accidentally left out the double :: *ssl::server_name* using just a
> single :.
>
> On Wed, May 20, 2015 at 12:36 PM, Stanford Prescott <
> stan.prescott at gmail.com> wrote:
>
>> After a diversion getting SquidClamAV working, i am back to trying to get
>> peek and splice working. I am trying to put together information from
>> previous recommendations I have received. Right now, I can't get the
>> server_name acl working. When I put this in my squid.conf
>>
>> *acl nobumpSites ssl:server_name .example.com <http://example.com>*
>>
>> I get a fatal error starting squid  using that acl saying the acl is
>> "Bungled".
>>
>> Is the form of the acl incorrect?
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/a168c062/attachment.htm>

From shenan.hawkins at gmail.com  Wed May 20 18:14:01 2015
From: shenan.hawkins at gmail.com (Shenan Hawkins)
Date: Wed, 20 May 2015 14:14:01 -0400
Subject: [squid-users] Custom User-Agent header based on domain?
Message-ID: <CAFzetitLBMgJqDEQRy5BV1rLsYgSRfGgqeuBxaWvkB4yehH5Ow@mail.gmail.com>

Is it possible to construct a stanza for squid such that a custom
User-Agent request header is sent based upon the requested domain?

It seems easy enough to add a custom header for that situation, but not
replace one.

For instance, the idea would be "all requests to google.ca get User-Agent:
blahblahblah1, and all requests to google.com get User-Agent:
blahblahblah2".



Thanks,
Shenan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150520/ed26cc16/attachment.htm>

From yvoinov at gmail.com  Wed May 20 18:28:42 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 21 May 2015 00:28:42 +0600
Subject: [squid-users] Custom User-Agent header based on domain?
In-Reply-To: <CAFzetitLBMgJqDEQRy5BV1rLsYgSRfGgqeuBxaWvkB4yehH5Ow@mail.gmail.com>
References: <CAFzetitLBMgJqDEQRy5BV1rLsYgSRfGgqeuBxaWvkB4yehH5Ow@mail.gmail.com>
Message-ID: <555CD25A.10303@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
acl you_dom dstdomain .youdomain.com
request_header_access User-Agent deny you_dom
request_header_replace User-Agent Nutscrape/1.0 (CP/M; 8-bit)

21.05.15 0:14, Shenan Hawkins ?????:
> Is it possible to construct a stanza for squid such that a custom
> User-Agent request header is sent based upon the requested domain?
>
> It seems easy enough to add a custom header for that situation, but not
> replace one.
>
> For instance, the idea would be "all requests to google.ca get User-Agent:
> blahblahblah1, and all requests to google.com get User-Agent:
> blahblahblah2".
>
>
>
> Thanks,
> Shenan
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVXNJaAAoJENNXIZxhPexGpV4H/1y3E4h/pVXpHzziz5TdOZjU
mPQTWo4Nlfvo2D1nw8nHKEz6bUXuTlKfFygWh1hC8vcinYHYMms7O02CnMYecAAf
voZuIHbXjkKpHhbVVUKKEiXJVVFARzmoLR+YCKu7X/CNf+Zr+xPEx9s6W8a0iEvU
hxjlcfjldD2xRPg1s7CmPu3cfuy31FbR1C8Qnz5NxB6LHSoLMJJiummjCcsuUGsW
gp0SvnNcUp8D8iX/BFL1I/rZSxFrllNFVjZXfpgJ+XC5FYdX1i+61LwKxeE5X7FB
kaqWRJGfuxAZbCpKEknDRFljUFxD9Tv2T3acLpMIDDh1D197H3m16LHXTIH5/v4=
=MIwV
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/fc5866bf/attachment.htm>

From squid3 at treenet.co.nz  Thu May 21 03:41:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 15:41:26 +1200
Subject: [squid-users] need help plissss
In-Reply-To: <CADTgVuHC0YRHh=SNEhduZq=GsbMdQGTRZM--NxLQwhAbgmezPQ@mail.gmail.com>
References: <CADTgVuHC0YRHh=SNEhduZq=GsbMdQGTRZM--NxLQwhAbgmezPQ@mail.gmail.com>
Message-ID: <555D53E6.9030601@treenet.co.nz>

On 21/05/2015 5:17 a.m., Israel Romero Garcia wrote:
> Hello people, sorry but my english that no is very good,although squid is a
> lenguaje international.
> 
> I need help because my squid does not allow a group of users on the LAN to
> connect to an FTPS (FTP + SSL) with passive port range 10000 to 12000, I
> would appreciate any help, Greetings

Unfortunately Squid only supports plain-text FTP at this time.

If the FTP software is able to use HTTP CONNECT methods *and* if it is
configured to be aware of Squid being an HTTP proxy it might be usable.
Otherwise it is not possible to do.

Amos



From squid3 at treenet.co.nz  Thu May 21 04:02:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 16:02:34 +1200
Subject: [squid-users] New server_name acl causes fatal error starting
 Squid 3.5.4
In-Reply-To: <CANLNtGTYFZn1yT11FuiBUMsH_GH1RWDyapRZ=T2PnSjjPZk7xQ@mail.gmail.com>
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
 <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
 <CANLNtGTYFZn1yT11FuiBUMsH_GH1RWDyapRZ=T2PnSjjPZk7xQ@mail.gmail.com>
Message-ID: <555D58DA.802@treenet.co.nz>

On 21/05/2015 6:03 a.m., Stanford Prescott wrote:
> I think I finally figured out how to not bump certain sites and to bump all
> others. I put this in squid.conf
> 
> 
> 
> 
> 
> 
> 
> *acl step1 at_step SslBump1acl step2 at_step SslBump2acl nobumpSites
> ssl::server_name .wellsfargo.com <http://wellsfargo.com>ssl_bump peek
> step1ssl_bump splice step2 nobumpSitesssl_bump bump all*
> 
> When I check the access log I see that the wellsfargo.com com only appears
> as http://wellsfargo.com without any of the full URL but any other https
> site I see as, for example, https://yahoo.com with the full URL.
> 
> Are the lines in the squid.conf correct and is it doing what I want it do,
> which is to not bump the nobumpSites and bump all other sites that are not
> in nobumpSites?

I believe thats correct. yes.

Amos


From sima_yi at operamail.com  Thu May 21 04:29:51 2015
From: sima_yi at operamail.com (PSA4444)
Date: Wed, 20 May 2015 21:29:51 -0700 (PDT)
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
In-Reply-To: <555AD2BF.9060901@treenet.co.nz>
References: <1431995583201-4671274.post@n4.nabble.com>
 <555AD2BF.9060901@treenet.co.nz>
Message-ID: <1432182591880-4671322.post@n4.nabble.com>

Hi Amos,

Thanks for the reply.  It's weird that the <=2.4 operation still worked all
the time.
Anyway, I don't think what my helper is doing is exactly equivalent of that
because:

deny_info 301: https://api.domain.com%R site 

is a redirect, which the client will be aware of.  We want to continue to
rewrite the URL and handle it internally, without the client's browser being
aware of the change. <https://api.domain.com%R> 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-3-to-3-5-url-rewrite-program-changes-tp4671274p4671322.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Andre.Albsmeier at siemens.com  Thu May 21 05:35:36 2015
From: Andre.Albsmeier at siemens.com (Andre Albsmeier)
Date: Thu, 21 May 2015 07:35:36 +0200
Subject: [squid-users] Squid 3.5: internal-static icons on ftp://
	requests
In-Reply-To: <555AEBAE.30008@treenet.co.nz>
References: <20150519062917.GA51255@bali>
 <555AEBAE.30008@treenet.co.nz>
Message-ID: <20150521053536.GA85468@bali>

On Tue, 19-May-2015 at 19:52:14 +1200, Amos Jeffries wrote:
> On 19/05/2015 6:29 p.m., Andre Albsmeier wrote:
> > When browsing e.g.
> > 
> > ftp://ftp.mozilla.org/pub/thunderbird/releases/31.5.0/win32/en-GB/
> > 
> <snip>
> > and now the icons on ftp://ftp.mozilla.org/ appear but I wonder if it
> > is really needed to patch squid for that... ;-).
> 
> This is http://bugs.squid-cache.org/show_bug.cgi?id=4132. Thank you for
> the patch a slightly tweaked version (no extra debugs message) is now
> applied to Squid-4 and should be in the next releases.

Great, thanks. Any chance for this to hit the 3.5 branch as well?

	-Andre


From 715620615 at qq.com  Thu May 21 05:37:56 2015
From: 715620615 at qq.com (=?gb18030?B?wfXhsA==?=)
Date: Thu, 21 May 2015 13:37:56 +0800
Subject: [squid-users] receive 504 from googlecode.com
Message-ID: <tencent_24CCC9A6472C893A6A411EAA@qq.com>

Hello, I met gateway timeout when I request pages which need resources from googlecode. I use squid-3.0.STABLE18.tar.gz and here is the squid.conf file:
http_port 3128
icp_port 3130
http_access allow all 
icp_access allow all
visible_hostname node70


#cache_mem 200 MB


#cache_peer 10.1.1.95 sibling 3128 3130


cache_access_log /var/log/squid/access.log
cache_log /var/log/squid/cache.log
cache_store_log /var/log/squid/store.log


request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
#request_header_access Retry-After allow all
#request_header_access Title allow all
#request_header_access Connection allow all
#request_header_access Proxy-Connection allow all
#request_header_access All deny all




I really can not figure out what the problem is, could you please help me?


Thanks,
liulan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/9ffb77d9/attachment.htm>

From ashish_behl at yahoo.com  Thu May 21 07:58:50 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Thu, 21 May 2015 00:58:50 -0700 (PDT)
Subject: [squid-users] Compiling squid 3.5.4 with ecap enabled.
Message-ID: <1432195130200-4671325.post@n4.nabble.com>

Hello All,
I am trying to compile squid with ecap enabled (--enable-ecap)
I have compiled ecap already and placed the install files in /opt/ecap-1.0

=========================================================
curl -LOR http://www.measurement-factory.com/tmp/ecap/libecap-1.0.0.tar.gz
tar -xzvf libecap-1.0.0.tar.gz && cd libecap-1.0.0.tar.gz
./configure --prefix=/opt/ecap-1.0 'CXXFLAGS=-O2 -pipe' 'CFLAGS=-O2 -pipe'
make
sudo make install
=========================================================

I then try to compile squid in following ways (ommitted other options,
thought they are non-relevant):

a)
./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
-Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
-fPIC " --enable-ecap PKG_CONFIG_PATH=/opt/ecap-1.0/lib/pkgconfig

b)
./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
-Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
-fPIC " --enable-ecap PKG_CONFIG=/opt/ecap-1.0/lib/pkgconfig/libecap.pc

c)
./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
-Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
-fPIC " --enable-ecap PKG_CONFIG=/opt/ecap-1.0/lib/pkgconfig/libecap.pc
EXT_LIBECAP_CFLAGS='-I/opt/ecap-1.0/include -L/opt/ecap-1.0/lib'
EXT_LIBECAP_LIBS='/opt/ecap-1.0/lib/libecap.a'


I am not able to succeed and always get undefined references for libecap
methods.
Please let me know if anyone has tried it already, or there is a bug..


compiling output:
=========================================================
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Host()':
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::headerTransferEncoding'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::headerReferer'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::headerContentLength'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::headerVia'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::protocolHttp'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::protocolHttps'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::protocolFtp'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::protocolGopher'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::protocolWais'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::protocolUrn'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::protocolWhois'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:55: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:56: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:58: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:60: undefined
reference to `libecap::Name::assignHostId(int) const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o):/root/installer/squid-ssl-3.5.4/src/adaptation/ecap/Host.cc:61:
more undefined references to `libecap::Name::assignHostId(int) const' follow


.....
and more....
======================================================================



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Compiling-squid-3-5-4-with-ecap-enabled-tp4671325.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu May 21 09:49:05 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 21:49:05 +1200
Subject: [squid-users] receive 504 from googlecode.com
In-Reply-To: <tencent_24CCC9A6472C893A6A411EAA@qq.com>
References: <tencent_24CCC9A6472C893A6A411EAA@qq.com>
Message-ID: <555DAA11.50603@treenet.co.nz>

On 21/05/2015 5:37 p.m., ?? wrote:
> Hello, I met gateway timeout when I request pages which need resources from googlecode. I use squid-3.0.STABLE18.tar.gz and here is the squid.conf file:

That would be https:// URLs right?
If so you are probably hitting one of the bugs in CONNECT handling that
very old Squid like yours have. You really need to upgrade.

Amos


From emperor.cu at gmail.com  Thu May 21 09:50:28 2015
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Thu, 21 May 2015 11:50:28 +0200
Subject: [squid-users] Squid 3.4.8 with ssl-bump config.
Message-ID: <CALBaCdvZQO1Aaa_La_TPjU=ngpuBms6gUiK0QzWquhPQfrfJ7A@mail.gmail.com>

Hi again..

now work ok the compilation.. but have issues with the https sites.

squid start ok... but can't see the sites with https on the browser... i
make the certificate ... and put myCA.der on windows client

i test it with:
1- ssl-bump server-first all
2- ssl-bump client-first all

testing acl with and without...
acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
sslproxy_cert_error allow TrustedName
sslproxy_cert_error allow BadSite
sslproxy_cert_error deny all

and nothing  can't see https sites like mail.yahoo.com or facebook.com

the browser continue put out
ERROR SSL CONNECTION
ERR_SSL_PROTOCOL

i rebuild again many times /var/spool/squid_ssldb

and the logs continue saying...

 1432201755.569      0 172.16.1.20 TAG_NONE/400 3640
 Z%19%98%A50%D7%AD%19%AB%1E - HIER_NONE/- text/html
1432201756.077      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.078      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.085      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.090      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.094      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.381      1 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.383      1 172.16.1.20 TAG_NONE/400 3616
 v%C9%F0O%C9%E6%BB%A1%D2 - HIER_NONE/- text/html
1432201756.391      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.395      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.399      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.662      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.663      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.670      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html
1432201756.675      0 172.16.1.20 TAG_NONE/400 3672
%05%D5%846S/%60%E5&e@%60%D5=%CA%27%E5%E7
- HIER_NONE/- text/html
1432201756.680      0 172.16.1.20 TAG_NONE/400 4056 NONE
error:invalid-request - HIER_NONE/- text/html

here is my config
----------------------------------
 # squid3 -k parse
2015/05/21 05:42:10| Startup: Initializing Authentication Schemes ...
2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'basic'
2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'digest'
2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'negotiate'
2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'ntlm'
2015/05/21 05:42:10| Startup: Initialized Authentication.
2015/05/21 05:42:10| Processing Configuration File: /etc/squid3/squid.conf
(depth 0)
2015/05/21 05:42:10| Processing: http_port 172.16.1.10:3128 intercept
ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/etc/squid3/ssl/myCA.pem
2015/05/21 05:42:10| Starting Authentication on port 172.16.1.10:3128
2015/05/21 05:42:10| Disabling Authentication on port 172.16.1.10:3128
(interception enabled)
2015/05/21 05:42:10| Processing: hostname_aliases
debian-template.ctimegroup.local
2015/05/21 05:42:10| Processing: visible_hostname debian-template
2015/05/21 05:42:10| Processing: hierarchy_stoplist cgi-bin ?
2015/05/21 05:42:10| Processing: acl QUERY urlpath_regex cgi-bin \?
2015/05/21 05:42:10| Processing: no_cache deny QUERY
2015/05/21 05:42:10| Processing: cache_mem 1024 MB
2015/05/21 05:42:10| Processing: cache_replacement_policy heap LFUDA
2015/05/21 05:42:10| Processing: cache_dir aufs /var/spool/squid3 4096 16
256
2015/05/21 05:42:10| Processing: cache_log /var/log/squid3/cache.log
2015/05/21 05:42:10| Processing: cache_store_log none
2015/05/21 05:42:10| Processing: cache_effective_user proxy
2015/05/21 05:42:10| Processing: cache_effective_group proxy
2015/05/21 05:42:10| Processing: maximum_object_size 1024 KB
2015/05/21 05:42:10| Processing: prefer_direct on
2015/05/21 05:42:10| Processing: ftp_user anonymous at proxy.sld.cu
2015/05/21 05:42:10| Processing: negative_ttl 5 minutes
2015/05/21 05:42:10| Processing: positive_dns_ttl 6 hours
2015/05/21 05:42:10| Processing: negative_dns_ttl 5 minutes
2015/05/21 05:42:10| Processing: coredump_dir /var/spool/squid3
2015/05/21 05:42:10| Processing: shutdown_lifetime 3 seconds
2015/05/21 05:42:10| Processing: logfile_rotate 10
2015/05/21 05:42:10| Processing: access_log /var/log/squid3/access.log
squid
2015/05/21 05:42:10| Processing: half_closed_clients off
2015/05/21 05:42:10| Processing: strip_query_terms on
2015/05/21 05:42:10| Processing: refresh_pattern ^ftp:       1440    20%
10080
2015/05/21 05:42:10| Processing: refresh_pattern ^gopher:    1440    0%
 1440
2015/05/21 05:42:10| Processing: refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
2015/05/21 05:42:10| Processing: refresh_pattern .       0   20% 4320
2015/05/21 05:42:10| Processing: refresh_pattern -i
\.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
2015/05/21 05:42:10| Processing: acl SSL_ports port 443 8443 12048 2083
2015/05/21 05:42:10| Processing: acl Safe_ports port 440-442     # http
2015/05/21 05:42:10| Processing: acl Safe_ports port 443
2015/05/21 05:42:10| Processing: acl Safe_ports port 80          # http
2015/05/21 05:42:10| Processing: acl Safe_ports port 21          # ftp
2015/05/21 05:42:10| Processing: acl Safe_ports port 443         # https,
snews
2015/05/21 05:42:10| Processing: acl Safe_ports port 1025-8081   #
unregistered ports
2015/05/21 05:42:10| Processing: acl Safe_ports port 8082-9999   #
unregistered ports
2015/05/21 05:42:10| Processing: acl Safe_ports port 10001-65535 #
unregistered ports
2015/05/21 05:42:10| Processing: acl Safe_ports port 280         #
http-mgmt
2015/05/21 05:42:10| Processing: acl CONNECT method CONNECT
2015/05/21 05:42:10| Processing: acl localhost src 192.168.207.51
172.16.1.10
2015/05/21 05:42:10| Processing: http_access allow localhost
 2015/05/21 05:45:51| Processing: ssl_bump server-first all
2015/05/21 05:42:10| Processing: sslcrtd_program /usr/lib/squid3/ssl_crtd
-s /var/spool/squid3_ssldb -M 4MB
2015/05/21 05:42:10| Processing: sslcrtd_children 50 startup=1 idle=1
2015/05/21 05:42:10| Processing: acl TrustedName url_regex ^
https://www.facebook.com
2015/05/21 05:42:10| Processing: acl BadSite ssl_error
SQUID_X509_V_ERR_DOMAIN_MISMATCH
2015/05/21 05:42:10| Processing: sslproxy_cert_error allow TrustedName
2015/05/21 05:42:10| Processing: sslproxy_cert_error allow BadSite
2015/05/21 05:42:10| Processing: sslproxy_cert_error deny all
2015/05/21 05:42:10| Processing: acl network src 172.16.1.0/24
192.168.207.0/24
2015/05/21 05:42:10| Processing: http_access allow network
2015/05/21 05:42:10| Processing: acl purge method PURGE
2015/05/21 05:42:10| Processing: http_access deny !Safe_ports
2015/05/21 05:42:10| Processing: http_access deny CONNECT !SSL_ports
2015/05/21 05:42:10| Processing: http_access deny all
2015/05/21 05:42:10| Processing: always_direct allow all
2015/05/21 05:42:10| Processing: forward_max_tries 25
2015/05/21 05:42:10| Processing: never_direct allow all
2015/05/21 05:42:10| Processing: max_filedesc 16384
2015/05/21 05:42:10| Processing: dns_nameservers 8.8.8.8
2015/05/21 05:42:10| Processing: dns_nameservers 8.8.4.4
2015/05/21 05:42:10| Processing: positive_dns_ttl 8 hours
2015/05/21 05:42:10| Processing: negative_dns_ttl 30 seconds
2015/05/21 05:42:10| Initializing https proxy context
2015/05/21 05:42:10| Initializing http_port 172.16.1.10:3128 SSL context
2015/05/21 05:42:10| Using certificate in /etc/squid3/ssl/myCA.pem

any idea?

thanxs
-- 
Antonio Pe?a
Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/34e88c73/attachment.htm>

From squid3 at treenet.co.nz  Thu May 21 09:51:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 21:51:47 +1200
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
In-Reply-To: <1432182591880-4671322.post@n4.nabble.com>
References: <1431995583201-4671274.post@n4.nabble.com>
 <555AD2BF.9060901@treenet.co.nz> <1432182591880-4671322.post@n4.nabble.com>
Message-ID: <555DAAB3.4030603@treenet.co.nz>

On 21/05/2015 4:29 p.m., PSA4444 wrote:
> Hi Amos,
> 
> Thanks for the reply.  It's weird that the <=2.4 operation still worked all
> the time.
> Anyway, I don't think what my helper is doing is exactly equivalent of that
> because:
> 
> deny_info 301: https://api.domain.com%R site 
> 
> is a redirect, which the client will be aware of.  We want to continue to
> rewrite the URL and handle it internally, without the client's browser being
> aware of the change. <https://api.domain.com%R> 

If the client is doing any kind of security checking (like its supposed
to) your users will be faced with certificate errors when you re-write
the domain.

Amos


From squid3 at treenet.co.nz  Thu May 21 09:54:21 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 21:54:21 +1200
Subject: [squid-users] Squid 3.5: internal-static icons on ftp://
	requests
In-Reply-To: <20150521053536.GA85468@bali>
References: <20150519062917.GA51255@bali> <555AEBAE.30008@treenet.co.nz>
 <20150521053536.GA85468@bali>
Message-ID: <555DAB4D.3000601@treenet.co.nz>

On 21/05/2015 5:35 p.m., Andre Albsmeier wrote:
> On Tue, 19-May-2015 at 19:52:14 +1200, Amos Jeffries wrote:
>> On 19/05/2015 6:29 p.m., Andre Albsmeier wrote:
>>> When browsing e.g.
>>>
>>> ftp://ftp.mozilla.org/pub/thunderbird/releases/31.5.0/win32/en-GB/
>>>
>> <snip>
>>> and now the icons on ftp://ftp.mozilla.org/ appear but I wonder if it
>>> is really needed to patch squid for that... ;-).
>>
>> This is http://bugs.squid-cache.org/show_bug.cgi?id=4132. Thank you for
>> the patch a slightly tweaked version (no extra debugs message) is now
>> applied to Squid-4 and should be in the next releases.
> 
> Great, thanks. Any chance for this to hit the 3.5 branch as well?
> 
> 	-Andre
> 

Yes if it works for you it should be in the next release.

Amos


From Andre.Albsmeier at siemens.com  Thu May 21 09:59:17 2015
From: Andre.Albsmeier at siemens.com (Andre Albsmeier)
Date: Thu, 21 May 2015 11:59:17 +0200
Subject: [squid-users] Squid 3.5: internal-static icons on ftp://
	requests
In-Reply-To: <555DAB4D.3000601@treenet.co.nz>
References: <20150519062917.GA51255@bali> <555AEBAE.30008@treenet.co.nz>
 <20150521053536.GA85468@bali> <555DAB4D.3000601@treenet.co.nz>
Message-ID: <20150521095917.GA87557@bali>

On Thu, 21-May-2015 at 21:54:21 +1200, Amos Jeffries wrote:
> On 21/05/2015 5:35 p.m., Andre Albsmeier wrote:
> > On Tue, 19-May-2015 at 19:52:14 +1200, Amos Jeffries wrote:
> >> On 19/05/2015 6:29 p.m., Andre Albsmeier wrote:
> >>> When browsing e.g.
> >>>
> >>> ftp://ftp.mozilla.org/pub/thunderbird/releases/31.5.0/win32/en-GB/
> >>>
> >> <snip>
> >>> and now the icons on ftp://ftp.mozilla.org/ appear but I wonder if it
> >>> is really needed to patch squid for that... ;-).
> >>
> >> This is http://bugs.squid-cache.org/show_bug.cgi?id=4132. Thank you for
> >> the patch a slightly tweaked version (no extra debugs message) is now
> >> applied to Squid-4 and should be in the next releases.
> > 
> > Great, thanks. Any chance for this to hit the 3.5 branch as well?
> > 
> > 	-Andre
> > 
> 
> Yes if it works for you it should be in the next release.

It works! ;-)

	-Andre


From squid3 at treenet.co.nz  Thu May 21 10:01:20 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 22:01:20 +1200
Subject: [squid-users] Compiling squid 3.5.4 with ecap enabled.
In-Reply-To: <1432195130200-4671325.post@n4.nabble.com>
References: <1432195130200-4671325.post@n4.nabble.com>
Message-ID: <555DACF0.2050904@treenet.co.nz>

On 21/05/2015 7:58 p.m., Ashish Behl wrote:
> Hello All,
> I am trying to compile squid with ecap enabled (--enable-ecap)
> I have compiled ecap already and placed the install files in /opt/ecap-1.0
> 
> =========================================================
> curl -LOR http://www.measurement-factory.com/tmp/ecap/libecap-1.0.0.tar.gz
> tar -xzvf libecap-1.0.0.tar.gz && cd libecap-1.0.0.tar.gz
> ./configure --prefix=/opt/ecap-1.0 'CXXFLAGS=-O2 -pipe' 'CFLAGS=-O2 -pipe'
> make
> sudo make install
> =========================================================
> 
> I then try to compile squid in following ways (ommitted other options,
> thought they are non-relevant):
> 
> a)
> ./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
> -Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
> -fPIC " --enable-ecap PKG_CONFIG_PATH=/opt/ecap-1.0/lib/pkgconfig
> 
> b)
> ./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
> -Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
> -fPIC " --enable-ecap PKG_CONFIG=/opt/ecap-1.0/lib/pkgconfig/libecap.pc
> 
> c)
> ./configure --prefix=/usr CFLAGS="-g -O2 -fPIC -Wall" LDFLAGS="-fPIC -pie
> -Wl,-z,relro -Wl,-z,now" CPPFLAGS="-D_FORTIFY_SOURCE=2" CXXFLAGS="-g -O2
> -fPIC " --enable-ecap PKG_CONFIG=/opt/ecap-1.0/lib/pkgconfig/libecap.pc
> EXT_LIBECAP_CFLAGS='-I/opt/ecap-1.0/include -L/opt/ecap-1.0/lib'
> EXT_LIBECAP_LIBS='/opt/ecap-1.0/lib/libecap.a'
> 

FYI:  PKG_CONFIG_PATH is the path to the pkg-config binary.

All you should need after the libecap "make install" is:
 ./configure --prefix=/usr --enable-ecap


> 
> I am not able to succeed and always get undefined references for libecap
> methods.
> Please let me know if anyone has tried it already, or there is a bug..
> 
> 
> compiling output:
> =========================================================
> adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
> `Adaptation::Ecap::Host::Host()':

There is a bug. The libecap library headers do not support C++11
compilers properly.

The libecap authors referred me to the patch found at
<https://github.com/freebsd/freebsd-ports/blob/master/www/libecap/files/patch-src__libecap__common__area.h>
when I encountered this earlier. They are already working on a better
fix that will be in a future release of libecap.

Amos


From squid3 at treenet.co.nz  Thu May 21 10:27:26 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 22:27:26 +1200
Subject: [squid-users] Novice question on TPROXY and SSL-BUMP behavior
In-Reply-To: <BY2PR05MB142B37A683F61D25D34B2B2ACC20@BY2PR05MB142.namprd05.prod.outlook.com>
References: <BY2PR05MB142B37A683F61D25D34B2B2ACC20@BY2PR05MB142.namprd05.prod.outlook.com>
Message-ID: <555DB30E.10001@treenet.co.nz>

On 21/05/2015 2:42 a.m., Nick Belnap wrote:
> I've been tasked with preventing a client's users from accessing
consumer Gmail accounts while only accessing their corporate Google Apps
accounts. Google gives an overview here:
https://support.google.com/a/answer/1668854?hl=en.


<snip>

> 
> Being a novice at Squid and iptables I've done a lot of Googling to get this far but have hit the wall I think with this problem.  Can anyone see why the ssl-bump might not be working for tproxy connections?
> 
> What am I missing?

Same thing everyone seems to be missing with SSL-Bump. The fact that TCP
packet headers dont contain a domain name. Only the IP:port the TCP
connection is going to.

So the "Google" dstdomain ACL does not work on the fake CONNECT request
Squid generates from the IP:port details.

Replace these:
  acl Google dstdomain .google.com
  ssl_bump server-first Google

With these (in this specific order):
 acl GoogleBump ssl::server_name .google.com .gmail.com
 ssl_bump peek all
 ssl_bump bump GoogleBump
 ssl_bump splice all


Amos


From squid3 at treenet.co.nz  Thu May 21 10:42:27 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 May 2015 22:42:27 +1200
Subject: [squid-users] Squid 3.4.8 with ssl-bump config.
In-Reply-To: <CALBaCdvZQO1Aaa_La_TPjU=ngpuBms6gUiK0QzWquhPQfrfJ7A@mail.gmail.com>
References: <CALBaCdvZQO1Aaa_La_TPjU=ngpuBms6gUiK0QzWquhPQfrfJ7A@mail.gmail.com>
Message-ID: <555DB693.4080409@treenet.co.nz>

On 21/05/2015 9:50 p.m., Tony Pe?a wrote:
> Hi again..
> 
> now work ok the compilation.. but have issues with the https sites.
> 
> squid start ok... but can't see the sites with https on the browser... i
> make the certificate ... and put myCA.der on windows client
> 
> i test it with:
> 1- ssl-bump server-first all
> 2- ssl-bump client-first all
> 
> testing acl with and without...
> acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
> sslproxy_cert_error allow TrustedName
> sslproxy_cert_error allow BadSite
> sslproxy_cert_error deny all
> 
> and nothing  can't see https sites like mail.yahoo.com or facebook.com
> 
> the browser continue put out
> ERROR SSL CONNECTION
> ERR_SSL_PROTOCOL
> 
> i rebuild again many times /var/spool/squid_ssldb
> 
> and the logs continue saying...
> 
>  1432201755.569      0 172.16.1.20 TAG_NONE/400 3640
>  Z%19%98%A50%D7%AD%19%AB%1E - HIER_NONE/- text/html
> 1432201756.077      0 172.16.1.20 TAG_NONE/400 4056 NONE
> error:invalid-request - HIER_NONE/- text/html

<snip>
> 
> here is my config
> ----------------------------------
>  # squid3 -k parse
> 2015/05/21 05:42:10| Startup: Initializing Authentication Schemes ...
> 2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'basic'
> 2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'digest'
> 2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'negotiate'
> 2015/05/21 05:42:10| Startup: Initialized Authentication Scheme 'ntlm'
> 2015/05/21 05:42:10| Startup: Initialized Authentication.
> 2015/05/21 05:42:10| Processing Configuration File: /etc/squid3/squid.conf
> (depth 0)
> 2015/05/21 05:42:10| Processing: http_port 172.16.1.10:3128 intercept
> ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/etc/squid3/ssl/myCA.pem

<snip>
> any idea?

I dont see any sign of an https_port for receiving HTTPS from port 443.

Only a http_port for receiving HTTP from port 80 NAT'ed connections.

Port 443 has an entirely different (TLS protocol) binary syntax. Like
Squid logs say the traffic arriving in binary TLS format are invalid
when trying to interpret them as plain-text HTTP format.


Amos


From ahmed.zaeem at netstream.ps  Thu May 21 21:09:21 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 21 May 2015 14:09:21 -0700
Subject: [squid-users] https quick question
Message-ID: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>

Hi , 

I WANT TO ESTABLISH squid https reverse proxy on squid

 

Assume I configured and the keys  xxxxx.crt & xxxxx.key needed for the
directive

https_port 443  accl cert=/etc/squid/ssl/xxxx.crt
key=/etc/squid/ssl/xxxx.key vhost

 

the question is being asked now

 

do I need to add a certificate in my browser to get it work ?

if so , what key shoud I add ? the .cert file or the .key file ?

 

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/5cbc4b74/attachment.htm>

From sebag at vianetcon.com.ar  Thu May 21 15:07:55 2015
From: sebag at vianetcon.com.ar (Sebastian Goicochea)
Date: Thu, 21 May 2015 12:07:55 -0300
Subject: [squid-users] Storage mem in 3.5.4, not sure what is happening
In-Reply-To: <555C9F42.5030603@vianetcon.com.ar>
References: <555B59B4.8070304@vianetcon.com.ar>
 <555C134D.4050901@treenet.co.nz> <555C9F42.5030603@vianetcon.com.ar>
Message-ID: <555DF4CB.4060502@vianetcon.com.ar>

We've found the problem:

As we are only using just one worker, memory_cache_shared has to be off 
when rock is enabled.
It wasn't clear at first because I interpreted that memory_cache_shared 
could be on no matter how many workers you had or what fs were you using.



Thanks for your time,
Sebastian



El 20/05/15 a las 11:50, Sebastian Goicochea escribi?:
> Hello Amos,
>
> Here's up time
> Start Time:    Wed, 20 May 2015 14:33:02 GMT
> Current Time:    Wed, 20 May 2015 14:41:46 GMT
>
> It's a short period of time because we've been restariting it.
>
> /var/run/squid and /dev/shm exist
>
> # ls /var/run/squid -1
> squid-coordinator.ipc
> squid-kid-1.ipc
> squid-kid-2.ipc
>
> # ls /dev/shm -1
> squid-cache1.p1.rock_map_anchors.shm
> squid-cache1.p1.rock_map_slices.shm
> squid-cache1.p1.rock_spaces.shm
> squid-cache_mem_ex.shm
> squid-cache_mem_map_anchors.shm
> squid-cache_mem_map_slices.shm
> squid-cache_mem_space.shm
> squid-cf__metadata.shm
> squid-cf__queues.shm
> squid-cf__readers.shm
> squid-io_file__metadata.shm
> squid-io_file__queues.shm
> squid-io_file__readers.shm
> squid-squid-page-pool.shm
>
> We've found something interesting: disabling rock gets hot objets out 
> of 0 and we get MEM_HITs right away.
>
> Cache information for squid:
>     Hits as % of all requests:    5min: 5.0%, 60min: 4.6%
>     Hits as % of bytes sent:    5min: 10.7%, 60min: 8.8%
>     Memory hits as % of hit requests:    5min: 36.6%, 60min: 36.9%
>     Disk hits as % of hit requests:    5min: 38.3%, 60min: 40.7%
>     Storage Swap size:    173674816 KB
>     Storage Swap capacity:    19.7% used, 80.3% free
>     Storage Mem size:    115528 KB
>     Storage Mem capacity:    11.0% used, 89.0% free
>     Mean Object Size:    526.13 KB
>     Requests given to unlinkd:    0
>
> Internal Data Structures:
>     337773 StoreEntries
>       8536 StoreEntries with MemObjects
>       8382 Hot Object Cache Items
>     330097 on-disk objects
>
>
>
> Is there any relation between rock beeing active and storage mem?
>
>
> Thanks a lot,
> Sebastian
>
>
> El 20/05/15 a las 01:53, Amos Jeffries escribi?:
>> On 20/05/2015 3:41 a.m., Sebastian Goicochea wrote:
>>> Hello everyone, we're having a problem, we updated from squid 3.5.3 to
>>> 3.5.4 (bellow are the configuration options) and enabled rock fs.
>>>
>>> The problem that arose is that I don't know if objects are being stored
>>> in RAM, but what I do know is that there are no MEM_HITs in the
>>> access.log file
>>>
>>>
>>> This is how I enabled rock:
>>> cache_dir rock  /cache/rock  512  max-size=32768
>>>
>>>
>>> This is what I get from squidclient:
>> Whats the "Start Time" information displayed at the top of that report?
>>   and does it remain unchanged across many minutes?
>>
>> I suspect that with no memory hits and sine 5min == 60min hits your
>> Squid may be in a cycle of crashes or assertions.
>>
>> NOTE: to enable rock and other SMP features you need /var/run/squid and
>> /dev/shm to exist. Some OS seem not to create or mount them by default.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu May 21 13:00:36 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 01:00:36 +1200
Subject: [squid-users] https quick question
In-Reply-To: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
Message-ID: <555DD6F4.3060909@treenet.co.nz>

On 22/05/2015 9:09 a.m., snakeeyes wrote:
> Hi , 
> 
> I WANT TO ESTABLISH squid https reverse proxy on squid
> 
>  
> 
> Assume I configured and the keys  xxxxx.crt & xxxxx.key needed for the
> directive
> 
> https_port 443  accl cert=/etc/squid/ssl/xxxx.crt
> key=/etc/squid/ssl/xxxx.key vhost
> 
>  
> 
> the question is being asked now
> 
>  
> 
> do I need to add a certificate in my browser to get it work ?

No.

> 
> if so , what key shoud I add ? the .cert file or the .key file ?


If it was signed by a global truted CA then you dont have to do anything
more. Making it work for clients is what you are paying the CA for.

If those keys were signed by a custom CA you can optionally add *that
CA* to the browser trusted set. Or the user could click to add exception
when they get their popup. Some of the browsers now are ignoring
self-signed certs (provided they are valid to the server being
contacted). Or you could add TLSA records to your DNS for the domain.

Amos



From pkryon at gmail.com  Thu May 21 16:10:30 2015
From: pkryon at gmail.com (Patrick)
Date: Thu, 21 May 2015 12:10:30 -0400
Subject: [squid-users] url_rewrite_extras - not getting data excepted
Message-ID: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>

Hello,

I think I'm having a problem with the url_rewrite_extras function.  I'm
using version 3.5.3. I've been trying to use the %ue macro to send the
user= returned by my external_acl_type program.  But when I look at
the url_rewrite_program output I just get a "-" value for this macro.

Example:

Summarized config:

external_acl_type idbf_user ttl=30 %SRC /opt/idbf-dev/idbf_squid_ext_acl.py

acl idbf_allowed external idbf_user # authenticated users from external acl

http_access allow idbf_allowed
http_access deny all

url_rewrite_access  idbf_allowed
url_rewrite_extras "un: %un ue: %ue ul: %ul ui: %ui"
url_rewrite_program /usr/bin/tee -a /var/log/squid3/redir_tmp.log

redir_tmp.log:

http:// <http://www.ipchicken.com/>www.ipchicken.com/
<http://www.ipchicken.com/> un: - ue: - ul: - ui: -

But I do see my external acl user as excepted in access.log:

1432219189.592     40 172.20.15.235 TCP_MISS/404 509 GET http://
<http://www.ipchicken.com/>www.ipchicken.com/ <http://www.ipchicken.com/>
user@ <user at example.com>example.com <user at example.com> HIER_DIRECT/
209.68.27.16 text/html

Does anyone see anything I might have mis-configured or am I just not
understanding how this should work?

Thanks!

Patrick
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/5a21bed7/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri May 22 02:36:26 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 21 May 2015 19:36:26 -0700
Subject: [squid-users] https quick question
In-Reply-To: <555DD6F4.3060909@treenet.co.nz>
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz>
Message-ID: <000501d09438$1a349f00$4e9ddd00$@netstream.ps>

Thank you amos so much

So far I didn?t add CA to my browser
And I followed many docs about how to create the .key file and .crt file but always I get( ssl negotiation error)

What could be the problem

Where should I check and troubleshoot ?

BTW I have the directive 
https_port 443 accel key=/root/CA/myCA/private/squid.local.key cert=/root/CA/myCA/certs/squid.local.crt

where shoud I troubleshoot ?

appreciate  your help a lot

for start I want to start with self signed certificate but later I will buy a valid certificate

hope to help me

cheers



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, May 21, 2015 6:01 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https quick question

On 22/05/2015 9:09 a.m., snakeeyes wrote:
> Hi ,
> 
> I WANT TO ESTABLISH squid https reverse proxy on squid
> 
>  
> 
> Assume I configured and the keys  xxxxx.crt & xxxxx.key needed for the 
> directive
> 
> https_port 443  accl cert=/etc/squid/ssl/xxxx.crt 
> key=/etc/squid/ssl/xxxx.key vhost
> 
>  
> 
> the question is being asked now
> 
>  
> 
> do I need to add a certificate in my browser to get it work ?

No.

> 
> if so , what key shoud I add ? the .cert file or the .key file ?


If it was signed by a global truted CA then you dont have to do anything more. Making it work for clients is what you are paying the CA for.

If those keys were signed by a custom CA you can optionally add *that
CA* to the browser trusted set. Or the user could click to add exception when they get their popup. Some of the browsers now are ignoring self-signed certs (provided they are valid to the server being contacted). Or you could add TLSA records to your DNS for the domain.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From lucas2 at dds.nl  Thu May 21 16:58:46 2015
From: lucas2 at dds.nl (Lucas van Braam van Vloten)
Date: Thu, 21 May 2015 18:58:46 +0200
Subject: [squid-users] Proxy chain question
Message-ID: <1432227526.9790.6.camel@dds.nl>

Hello list,

In my network I have a Microsoft TMG proxy server for http(s) access to
internet.
This TMG server also serves as a reverse proxy to channel incoming
traffic to a Squid reverse proxy in the internal network (yes, two
reverse proxies in a line)

This Squid server is currently configured as a reverse proxy to allow
traffic from internet to a number of webservices that run on an internal
server.

Now I want to add a function to the squid server, in addition to the
existing function. It should serve as a proxy to allow a client on the
internal network to access a web servoce on internet.
So, put simply, the traffic goes like this:
Internal client -> Squid Proxy -> TMG proxy -> internet webservice

The reason to use this configuration is because the internet webservice
requires a client certificate for authentication, and TMG is not able to
handle this.
So now I am trying to configure this on my Squid server. I wish to make
my configuration as restrictive as possible. But I am new to the Squid
configuration, and I could use some help.

So basically, I want the following:
1. The client makes a http connection to my Squid proxy
2. The Squid proxy initiates the client certificate authenticated
connection to the internet webservice
3. The connection from the Squid proxy to Internet uses the TMG proxy.

I do not wish to use any form of caching on my Squid server.

I considered using a configuration similar to my reverse proxy
configuration, using the following structure:
(this configuration works)

=====================

# Designate a port and SSL config for this specific webservice
# Local server IP is 192.168.0.1, traffic comes in through the TMG
https_port 192.168.0.1:1443 accel
defaultsite=webservice.exposed.address.com vhost <SSL stuff>

# enforce use of https
acl webapp_SITES dstdomain webservice.exposed.address.com
http_access deny HTTP webapp_SITES
http_access allow webapp_SITES

# Configure the reverse proxy for clients that connect to the external
(exposed) address
acl webapp_URL url_regex ^https://webservice.exposed.address.com
cache_peer internal.server.lan parent 8080 0 no-query no-digest
originserver login=PASS name=webservice_APP
cache_peer_access webservice_APP allow webapp_URL
cache_peer_access webservice_APP deny all

=====================

So if I use this for my new purpose, I assume that the cache_peer would
be the internet webservice address, and I could use the sslcert option
to make it use the client certificate. Something like this:

=====================

http_port 192.168.0.1:8080 accel defaultsite=squid.server.lan vhost
acl webapp_URL url_regex ^http://squid.server.lan
cache_peer webservice.somewhere.on.internet.com parent 8443 0 no-query
no-digest originserver sslkey=/path/to/ssl/key name=webservice_APP
cache_peer_access webservice_APP allow webapp_URL
cache_peer_access webservice_APP deny all

=====================

My client makes a direct connection to the squid proxy (http) and the
squid proxy connects directly to the internet web service (https) and
handles all the SSL stuff.
However, this does not seem to work. I don't know how I can configure
squid to still use the TMG proxy to access internet.
In addition, I wonder if it is possible to limit access to this, and
only this, specific proxy function to only 1 host. All other reverse
proxy configurations on the server should be accessible to other
clients.

I hope someone could give me some advice...

Thanks!
Lucas



From ahmed.zaeem at netstream.ps  Fri May 22 03:22:31 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Thu, 21 May 2015 20:22:31 -0700
Subject: [squid-users] https quick question
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz> 
Message-ID: <000a01d0943e$8aa70830$9ff51890$@netstream.ps>

clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 45: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 45: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 54: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 29: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)




Amos can you assit with that ???

-----Original Message-----
From: snakeeyes [mailto:ahmed.zaeem at netstream.ps] 
Sent: Thursday, May 21, 2015 7:36 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] https quick question

Thank you amos so much

So far I didn?t add CA to my browser
And I followed many docs about how to create the .key file and .crt file but always I get( ssl negotiation error)

What could be the problem

Where should I check and troubleshoot ?

BTW I have the directive
https_port 443 accel key=/root/CA/myCA/private/squid.local.key cert=/root/CA/myCA/certs/squid.local.crt

where shoud I troubleshoot ?

appreciate  your help a lot

for start I want to start with self signed certificate but later I will buy a valid certificate

hope to help me

cheers



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, May 21, 2015 6:01 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https quick question

On 22/05/2015 9:09 a.m., snakeeyes wrote:
> Hi ,
> 
> I WANT TO ESTABLISH squid https reverse proxy on squid
> 
>  
> 
> Assume I configured and the keys  xxxxx.crt & xxxxx.key needed for the 
> directive
> 
> https_port 443  accl cert=/etc/squid/ssl/xxxx.crt 
> key=/etc/squid/ssl/xxxx.key vhost
> 
>  
> 
> the question is being asked now
> 
>  
> 
> do I need to add a certificate in my browser to get it work ?

No.

> 
> if so , what key shoud I add ? the .cert file or the .key file ?


If it was signed by a global truted CA then you dont have to do anything more. Making it work for clients is what you are paying the CA for.

If those keys were signed by a custom CA you can optionally add *that
CA* to the browser trusted set. Or the user could click to add exception when they get their popup. Some of the browsers now are ignoring self-signed certs (provided they are valid to the server being contacted). Or you could add TLSA records to your DNS for the domain.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From stan.prescott at gmail.com  Thu May 21 17:43:54 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Thu, 21 May 2015 12:43:54 -0500
Subject: [squid-users] New server_name acl causes fatal error starting
 Squid 3.5.4
In-Reply-To: <555D58DA.802@treenet.co.nz>
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
 <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
 <CANLNtGTYFZn1yT11FuiBUMsH_GH1RWDyapRZ=T2PnSjjPZk7xQ@mail.gmail.com>
 <555D58DA.802@treenet.co.nz>
Message-ID: <CANLNtGQyfgy7B3Ek1e+=0biB1Xshvi6B0aHqwh7K9c7NXtSeig@mail.gmail.com>

Thanks, Amos. I really appreciate that.

On Wed, May 20, 2015 at 11:02 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 21/05/2015 6:03 a.m., Stanford Prescott wrote:
> > I think I finally figured out how to not bump certain sites and to bump
> all
> > others. I put this in squid.conf
> >
> >
> >
> >
> >
> >
> >
> > *acl step1 at_step SslBump1acl step2 at_step SslBump2acl nobumpSites
> > ssl::server_name .wellsfargo.com <http://wellsfargo.com>ssl_bump peek
> > step1ssl_bump splice step2 nobumpSitesssl_bump bump all*
> >
> > When I check the access log I see that the wellsfargo.com com only
> appears
> > as http://wellsfargo.com without any of the full URL but any other https
> > site I see as, for example, https://yahoo.com with the full URL.
> >
> > Are the lines in the squid.conf correct and is it doing what I want it
> do,
> > which is to not bump the nobumpSites and bump all other sites that are
> not
> > in nobumpSites?
>
> I believe thats correct. yes.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/b8cd0b50/attachment.htm>

From siefke_listen at web.de  Thu May 21 20:32:27 2015
From: siefke_listen at web.de (Silvio Siefke)
Date: Thu, 21 May 2015 22:32:27 +0200
Subject: [squid-users] Squid with proxy
Message-ID: <20150521223227.7c3b186e6133053c87c3cb19@web.de>

Hello, 

i use squid with ziproxy for mobile traffic compression. Squid work, 
Ziproxy work, okay not so what i think but its work. My problem is,
so i understand i connect to squid and squid give traffic to ziproxy
and  then come back to squid. Right? But must i set NextProxy in 
ziproxy.conf or not? I use auth_pam for system user, is for me
easier then passwd files. 

# ziproxy
cache_peer 127.0.0.1 parent 8080 0 no-query no-digest
never_direct allow all

Squid use other port for extern traffic, localhost i use the normal
port. But when in ziproxy activate NextProxy i become ever the login
window and nothing work. When deactivate NextProxy in ziproxy all 
work normal. But normal the acl localhost and http_access allow localhost
should work without login or? Or has i understand all wrong? 

My Config: 
Squid > http://silviosiefke.fr/files/squid.conf
Ziproxy > http://silviosiefke.fr/files/ziproxy.conf


Has someone idea? 

Thank You & Nice Day
Silvio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 473 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150521/59dbebc7/attachment.sig>

From sima_yi at operamail.com  Fri May 22 00:34:59 2015
From: sima_yi at operamail.com (PSA4444)
Date: Thu, 21 May 2015 17:34:59 -0700 (PDT)
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
In-Reply-To: <555DAAB3.4030603@treenet.co.nz>
References: <1431995583201-4671274.post@n4.nabble.com>
 <555AD2BF.9060901@treenet.co.nz> <1432182591880-4671322.post@n4.nabble.com>
 <555DAAB3.4030603@treenet.co.nz>
Message-ID: <1432254899286-4671345.post@n4.nabble.com>

That would be true, but we are using a wildcard certificate, and only
rewriting the subdomain, so the certificate is still valid.

Would squid 3.5 allow us to rewrite the subdomain in a similar way to your
redirect example, without relying on a helper script?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-3-to-3-5-url-rewrite-program-changes-tp4671274p4671345.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri May 22 03:18:30 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 15:18:30 +1200
Subject: [squid-users] Proxy chain question
In-Reply-To: <1432227526.9790.6.camel@dds.nl>
References: <1432227526.9790.6.camel@dds.nl>
Message-ID: <555EA006.7080105@treenet.co.nz>

On 22/05/2015 4:58 a.m., Lucas van Braam van Vloten wrote:
> Hello list,
> 
> In my network I have a Microsoft TMG proxy server for http(s) access to
> internet.
> This TMG server also serves as a reverse proxy to channel incoming
> traffic to a Squid reverse proxy in the internal network (yes, two
> reverse proxies in a line)

Any particular reason? It may prevent Squid being able to do what you want.

> 
> This Squid server is currently configured as a reverse proxy to allow
> traffic from internet to a number of webservices that run on an internal
> server.
> 
> Now I want to add a function to the squid server, in addition to the
> existing function. It should serve as a proxy to allow a client on the
> internal network to access a web servoce on internet.
> So, put simply, the traffic goes like this:
> Internal client -> Squid Proxy -> TMG proxy -> internet webservice

>From that diagram I'm a little doubtful that reverse-proxy is the right
way to do it here. Proxy at the client end of the connectivity are
usually forward- or interceptor- proxy.


> 
> The reason to use this configuration is because the internet webservice
> requires a client certificate for authentication, and TMG is not able to
> handle this.

Squid will not be able to handle this either unless it is directly
connecting to that service without the TMG in the way.

Because TLS is point-to-point security protocol, any proxy agent in the
middle must terminate the clients TLS and start its own server
connection for the next hop. Squid does not (yet) support sending
CONNECT over a peer proxy to bypass the TMG.

So in your current setup Squid sending the client cert will be sending
it to to authenticate with the *TMG* - not the web service.


> So now I am trying to configure this on my Squid server. I wish to make
> my configuration as restrictive as possible. But I am new to the Squid
> configuration, and I could use some help.
> 
> So basically, I want the following:
> 1. The client makes a http connection to my Squid proxy
> 2. The Squid proxy initiates the client certificate authenticated
> connection to the internet webservice
> 3. The connection from the Squid proxy to Internet uses the TMG proxy.
> 
> I do not wish to use any form of caching on my Squid server.
> 
> I considered using a configuration similar to my reverse proxy
> configuration, using the following structure:
> (this configuration works)
> 
> =====================
> 
> # Designate a port and SSL config for this specific webservice
> # Local server IP is 192.168.0.1, traffic comes in through the TMG
> https_port 192.168.0.1:1443 accel
> defaultsite=webservice.exposed.address.com vhost <SSL stuff>
> 
> # enforce use of https
> acl webapp_SITES dstdomain webservice.exposed.address.com
> http_access deny HTTP webapp_SITES
> http_access allow webapp_SITES
> 
> # Configure the reverse proxy for clients that connect to the external
> (exposed) address
> acl webapp_URL url_regex ^https://webservice.exposed.address.com
> cache_peer internal.server.lan parent 8080 0 no-query no-digest
> originserver login=PASS name=webservice_APP
> cache_peer_access webservice_APP allow webapp_URL
> cache_peer_access webservice_APP deny all
> 
> =====================
> 
> So if I use this for my new purpose, I assume that the cache_peer would
> be the internet webservice address, and I could use the sslcert option
> to make it use the client certificate. Something like this:
> 
> =====================
> 
> http_port 192.168.0.1:8080 accel defaultsite=squid.server.lan vhost
> acl webapp_URL url_regex ^http://squid.server.lan
> cache_peer webservice.somewhere.on.internet.com parent 8443 0 no-query
> no-digest originserver sslkey=/path/to/ssl/key name=webservice_APP
> cache_peer_access webservice_APP allow webapp_URL
> cache_peer_access webservice_APP deny all
> 
> =====================

That is the correct way to do it outbound from Squid. But the catch as
mentioned above is where the TLS link gets terminated (the TMG or the
web service).

Since the TMG is a reverse-proxy the DNS records for the cache_peer
domain name points at the TMG instead of the Internet service. You have
to have the cache_peer directly going to the server which uses/requires
the client certificate. Which means using IP or if available the
services own host name to avoid the TMG.

Which cycles back to that first question I had at the top about why the
TMG exists at all. You may or may not be able to do this without a full
topology redesign.

Amos


From squid3 at treenet.co.nz  Fri May 22 03:22:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 15:22:38 +1200
Subject: [squid-users] https quick question
In-Reply-To: <000a01d0943e$8aa70830$9ff51890$@netstream.ps>
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz>
 <000a01d0943e$8aa70830$9ff51890$@netstream.ps>
Message-ID: <555EA0FE.1090709@treenet.co.nz>

On 22/05/2015 3:22 p.m., snakeeyes wrote:
> clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 45: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 36: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 45: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 54: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL connection on FD 29: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 

IIRC, that is OpenSSL library complaining that you passed it
un-encrypted HTTP message syntax (port 80 or 3128).

HTTP (port 80) to an http_port

HTTPS (port 443) to an https_port

FTP (port 21) to an ftp_port

... the hint is in the *_port naming.

Amos



From squid3 at treenet.co.nz  Fri May 22 03:31:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 15:31:18 +1200
Subject: [squid-users] Squid 3.3 to 3.5 url_rewrite_program changes
In-Reply-To: <1432254899286-4671345.post@n4.nabble.com>
References: <1431995583201-4671274.post@n4.nabble.com>
 <555AD2BF.9060901@treenet.co.nz> <1432182591880-4671322.post@n4.nabble.com>
 <555DAAB3.4030603@treenet.co.nz> <1432254899286-4671345.post@n4.nabble.com>
Message-ID: <555EA306.5070105@treenet.co.nz>

On 22/05/2015 12:34 p.m., PSA4444 wrote:
> That would be true, but we are using a wildcard certificate, and only
> rewriting the subdomain, so the certificate is still valid.
> 
> Would squid 3.5 allow us to rewrite the subdomain in a similar way to your
> redirect example, without relying on a helper script?

The forcedomain= option on the cache_peer directive makes Squid re-write
the domain it sends to the peer.

If you are truely using two domain names publicly for the same API
service you are only causing trouble for yourself and the rest of the
Internet. Please don't.

If you are working a transition from an old name to a new name then the
301 redirect is the correct way to do it. That ensures the clients are
all informed about the new name in a way that does not break anything,
AND clients which understand the 301 status will automatically change
all their references to the old name to the new one.

Amos



From squid3 at treenet.co.nz  Fri May 22 03:39:19 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 15:39:19 +1200
Subject: [squid-users] Squid with proxy
In-Reply-To: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
Message-ID: <555EA4E7.8060205@treenet.co.nz>

On 22/05/2015 8:32 a.m., Silvio Siefke wrote:
> Hello, 
> 
> i use squid with ziproxy for mobile traffic compression. Squid work, 
> Ziproxy work, okay not so what i think but its work. My problem is,
> so i understand i connect to squid and squid give traffic to ziproxy
> and  then come back to squid. Right? But must i set NextProxy in 
> ziproxy.conf or not? I use auth_pam for system user, is for me
> easier then passwd files. 

I dont know why you should have to. ziproxy should be perfectly capable
of contacting Internet services to respond to the requests sent from Squid.


> 
> # ziproxy
> cache_peer 127.0.0.1 parent 8080 0 no-query no-digest
> never_direct allow all
> 
> Squid use other port for extern traffic, localhost i use the normal
> port. But when in ziproxy activate NextProxy i become ever the login
> window and nothing work. When deactivate NextProxy in ziproxy all 
> work normal. But normal the acl localhost and http_access allow localhost
> should work without login or? Or has i understand all wrong? 

I am not quite understanding what you are talking about auth for. So
can't answer that question. Hopefully the above answer is enough to
solve your problem though.

> 
> My Config: 
> Squid > http://silviosiefke.fr/files/squid.conf
> Ziproxy > http://silviosiefke.fr/files/ziproxy.conf

NP: Those links go to half-blank page.

Amos


From squid3 at treenet.co.nz  Fri May 22 03:51:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 15:51:41 +1200
Subject: [squid-users] Storage mem in 3.5.4, not sure what is happening
In-Reply-To: <555DF4CB.4060502@vianetcon.com.ar>
References: <555B59B4.8070304@vianetcon.com.ar>
 <555C134D.4050901@treenet.co.nz> <555C9F42.5030603@vianetcon.com.ar>
 <555DF4CB.4060502@vianetcon.com.ar>
Message-ID: <555EA7CD.8040108@treenet.co.nz>

On 22/05/2015 3:07 a.m., Sebastian Goicochea wrote:
> We've found the problem:
> 
> As we are only using just one worker, memory_cache_shared has to be off
> when rock is enabled.
> It wasn't clear at first because I interpreted that memory_cache_shared
> could be on no matter how many workers you had or what fs were you using.
> 

Aha. IIRC its there so one can forcibly turn it OFF if needed. The
default is to enable or disable as appropriate for the number of
workers. Squid should work fine for almost every use case if you just
leave it out of the config file entirely.

Amos



From squid3 at treenet.co.nz  Fri May 22 04:27:51 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 16:27:51 +1200
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
Message-ID: <555EB047.5010102@treenet.co.nz>

On 22/05/2015 4:10 a.m., Patrick wrote:
> Hello,
> 
> I think I'm having a problem with the url_rewrite_extras function.  I'm
> using version 3.5.3. I've been trying to use the %ue macro to send the
> user= returned by my external_acl_type program.  But when I look at
> the url_rewrite_program output I just get a "-" value for this macro.
> 

I think you are probably encountering a bug found recently in
exetrnal_acl_type handling of kv-pair notes.

The patch for that is
<http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13827.patch>.
Just ported it back to 3.5 so will be in the next snapshot.

Amos



From squid3 at treenet.co.nz  Fri May 22 06:52:02 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 18:52:02 +1200
Subject: [squid-users] Saving memory cache to disk on reboot?
In-Reply-To: <5559D93D.6050507@solutti.com.br>
References: <5559D325.10804@seiner.com> <5559D93D.6050507@solutti.com.br>
Message-ID: <555ED212.1080602@treenet.co.nz>

On 19/05/2015 12:21 a.m., Leonardo Rodrigues wrote:
> On 18/05/15 08:55, Yan Seiner wrote:
>> The title says it all - is it possible to save the memory cache to
>> disk on reboot?

Squid does what it can between shutdown signal arriving and the
shutdown_timeout completing. Theres not time to do much though.

Then as Leonardo said, cache is a temporary storage. Most traffic with
objects small enough to be worth memory-only caching these days is
dynamic objects with very short lifetimes. So even if it was saved and
re-loaded from disk its likely to need a network fetch to revalidate
and/or replace soon anyways.

Amos



From emperor.cu at gmail.com  Fri May 22 07:12:53 2015
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Fri, 22 May 2015 09:12:53 +0200
Subject: [squid-users] squid 3.5.4 and ssl-bump
Message-ID: <CALBaCduvQdReN7O=Sq9PnEb4KAqPOdGuC+yXGcrQz54bfp8xhQ@mail.gmail.com>

Hi Amos...

ok now I upgrade recompile again everything from 3.4.8 to 3.5.4

this is the conf

 root at debian-template:/usr/local/squid/sbin# ./squid -k parse
2015/05/22 03:08:17| Startup: Initializing Authentication Schemes ...
2015/05/22 03:08:17| Startup: Initialized Authentication Scheme 'basic'
2015/05/22 03:08:17| Startup: Initialized Authentication Scheme 'digest'
2015/05/22 03:08:17| Startup: Initialized Authentication Scheme 'negotiate'
2015/05/22 03:08:17| Startup: Initialized Authentication Scheme 'ntlm'
2015/05/22 03:08:17| Startup: Initialized Authentication.
2015/05/22 03:08:17| Processing Configuration File: /etc/squid3/squid.conf
(depth 0)
2015/05/22 03:08:17| Processing: http_port 172.16.1.10:3128
2015/05/22 03:08:17| Processing: https_port 172.16.1.10:3129 intercept
ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/etc/squid3/ssl/myCA.pem cipher=ECDHE-RSA-RC4
-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

2015/05/22 03:08:17| Starting Authentication on port 172.16.1.10:3129
2015/05/22 03:08:17| Disabling Authentication on port 172.16.1.10:3129
(interception enabled)
2015/05/22 03:08:17| Processing: acl QUERY urlpath_regex cgi-bin \?
2015/05/22 03:08:17| Processing: no_cache deny QUERY
2015/05/22 03:08:17| Processing: access_log /var/log/squid3/access.log
squid
2015/05/22 03:08:17| Processing: coredump_dir /var/spool/squid3
2015/05/22 03:08:17| Processing: refresh_pattern ^ftp:       1440    20%
10080
2015/05/22 03:08:17| Processing: refresh_pattern ^gopher:    1440    0%
 1440
2015/05/22 03:08:17| Processing: refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
2015/05/22 03:08:17| Processing: refresh_pattern .       0   20% 4320
2015/05/22 03:08:17| Processing: cache_dir aufs /var/spool/squid3 4096 16
256
2015/05/22 03:08:17| Processing: refresh_pattern -i
\.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
2015/05/22 03:08:17| Processing: acl SSL_ports port 25      # Protocols
2015/05/22 03:08:17| Processing: acl SSL_ports port 110      # to can
2015/05/22 03:08:17| Processing: acl SSL_ports port 143     # allow hit
2015/05/22 03:08:17| Processing: acl SSL_ports port 465     # gmail account
2015/05/22 03:08:17| Processing: acl SSL_ports port 587     # on the
2015/05/22 03:08:17| Processing: acl SSL_ports port 993     # internet
2015/05/22 03:08:17| Processing: acl SSL_ports port 995     # behind a
firewall
2015/05/22 03:08:17| Processing: acl SSL_ports port 443
2015/05/22 03:08:17| Processing: acl SSL_ports port 563
2015/05/22 03:08:17| Processing: acl Safe_ports port 80      # http
2015/05/22 03:08:17| Processing: acl Safe_ports port 21      # ftp
2015/05/22 03:08:17| Processing: acl Safe_ports port 443     # https
2015/05/22 03:08:17| Processing: acl Safe_ports port 70      # gopher
2015/05/22 03:08:17| Processing: acl Safe_ports port 210     # wais
2015/05/22 03:08:17| Processing: acl Safe_ports port 1025-65535  #
unregistered ports
2015/05/22 03:08:17| Processing: acl Safe_ports port 280     # http-mgmt
2015/05/22 03:08:17| Processing: acl Safe_ports port 488     # gss-http
2015/05/22 03:08:17| Processing: acl Safe_ports port 591     # filemaker
2015/05/22 03:08:17| Processing: acl Safe_ports port 777     # multiling
http
2015/05/22 03:08:17| Processing: acl CONNECT method CONNECT
2015/05/22 03:08:17| Processing: acl purge method PURGE
2015/05/22 03:08:17| Processing: acl network src 172.16.1.0/24
2015/05/22 03:08:17| Processing: cache_mem 64 MB
2015/05/22 03:08:17| Processing: http_access allow manager localhost
2015/05/22 03:08:17| Processing: http_access deny manager
2015/05/22 03:08:17| Processing: http_access deny !Safe_ports
2015/05/22 03:08:17| Processing: http_access deny CONNECT !SSL_ports
2015/05/22 03:08:17| Processing: http_access allow localhost
2015/05/22 03:08:17| Processing: http_access allow network CONNECT
2015/05/22 03:08:17| Processing: http_access deny all
2015/05/22 03:08:17| Processing: ssl_bump server-first all
2015/05/22 03:08:17| Processing: sslcrtd_program
/usr/local/squid/libexec/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
sslcrtd_children 8 startup=1 idle=1
2015/05/22 03:08:17| Processing: sslproxy_version 3
2015/05/22 03:08:17| Processing: sslproxy_options ALL
2015/05/22 03:08:17| Processing: always_direct allow all
2015/05/22 03:08:17| Processing: never_direct allow all
2015/05/22 03:08:17| Processing: max_filedesc 16384
2015/05/22 03:08:17| Processing: dns_nameservers 8.8.8.8
2015/05/22 03:08:17| Processing: dns_nameservers 8.8.4.4
2015/05/22 03:08:17| Processing: positive_dns_ttl 8 hours
2015/05/22 03:08:17| Processing: negative_dns_ttl 30 seconds
2015/05/22 03:08:17| Initializing https proxy context
2015/05/22 03:08:17| Initializing https_port 172.16.1.10:3129 SSL context
2015/05/22 03:08:17| Using certificate in /etc/squid3/ssl/myCA.pem

and now the error is different.

can't see any site... http or https

and the logs said...

 1432278470.317      0 172.16.1.20 TAG_NONE/400 388 HEAD
/v11/2/windowsupdate/redir/v6-win7sp1-wuredir.cab?1505220707 - HIER_NONE/-
text/html
1432278470.320      0 172.16.1.20 TAG_NONE/400 2223 GET
/v11/2/windowsupdate/redir/v6-win7sp1-wuredir.cab?1505220707 - HIER_NONE/-
text/html
1432278470.323      0 172.16.1.20 TAG_NONE/400 388 HEAD
/v11/2/windowsupdate/redir/v6-win7sp1-wuredir.cab?1505220707 - HIER_NONE/-
text/html
1432278470.327      0 172.16.1.20 TAG_NONE/400 2223 GET
/v11/2/windowsupdate/redir/v6-win7sp1-wuredir.cab?1505220707 - HIER_NONE/-
text/html
1432278472.729      0 172.16.1.20 TAG_NONE/400 2193 GET
/pki/crl/products/MicRooCerAut_2010-06-23.crl - HIER_NONE/- text/html
1432278477.871      0 172.16.1.20 TAG_NONE/400 2159 GET
/pki/crl/products/WinPCA.crl - HIER_NONE/- text/html
1432278482.222      0 172.16.1.20 TAG_NONE/400 2333 POST
/service/update2?cup2key=5:1028882439&cup2hreq=1beabeae3a9008aa500f171f3efd92cac82574e42989d76d9104766a07e2e021
- HIER_NONE/- text/html
1432278482.244      0 172.16.1.20 TAG_NONE/400 2333 POST
/service/update2?cup2key=5:3993259034&cup2hreq=1beabeae3a9008aa500f171f3efd92cac82574e42989d76d9104766a07e2e021
- HIER_NONE/- text/html
1432278483.049      0 172.16.1.20 TAG_NONE/400 2201 GET
/pki/crl/products/MicRooCerAut2011_2011_03_22.crl - HIER_NONE/- text/html

remember we need to check http normal use with acl syntaxs (that part is
ok, just need the config ok to can see the same using this ssl-bump for
example domains as facebook or similar)

thanxs
-- 
Antonio Pe?a
Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/6418d1fd/attachment.htm>

From siefke_listen at web.de  Fri May 22 10:35:41 2015
From: siefke_listen at web.de (Silvio Siefke)
Date: Fri, 22 May 2015 12:35:41 +0200
Subject: [squid-users] Squid with proxy
In-Reply-To: <555EA4E7.8060205@treenet.co.nz>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
Message-ID: <20150522123541.8d3a96213812020edef927db@web.de>

On Fri, 22 May 2015 15:39:19 +1200 Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> I dont know why you should have to. ziproxy should be perfectly
> capable of contacting Internet services to respond to the requests
> sent from Squid.

Yes it works but im not sure is right, when NextProxy in ziproxy.conf
is not set. When saw the log all work, but in all tutorials which read
they say ziproxy.conf need set NextProxy="127.0.0.1" for "routing" back
to squid. But so it work all without NextProxy. What is now correct?


> I am not quite understanding what you are talking about auth for. So
> can't answer that question. Hopefully the above answer is enough to
> solve your problem though.

Squid use auth for connecting with it, when i has activated NextProxy in
ziproxy.conf then Browser ask and ask for login stuff. When not activated
NextProxy in ziproxy.conf then one time come login window and after login
all work. But what is now right, set NextProxy or not. But self when set 
NextProxy in ziproxy.conf then squid can not ask for login to ziproxy,  cause
localhost has free traffic or not?

> > Squid > http://silviosiefke.fr/files/squid.conf
> > Ziproxy > http://silviosiefke.fr/files/ziproxy.conf
> NP: Those links go to half-blank page.

Yeah thats when not know the own website hhhhh

Squid > http://silviosiefke.fr/static/files/squid.conf
Ziproxy > http://silviosiefke.fr/static/files/ziproxy.conf

Thanks & Nice Day
Silvio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 473 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/3b0b3ec8/attachment.sig>

From squid at wizonet.ch  Fri May 22 11:18:37 2015
From: squid at wizonet.ch (wn48z)
Date: Fri, 22 May 2015 13:18:37 +0200
Subject: [squid-users] Zyxel USG20 and Squid 3.3
In-Reply-To: <20150522123541.8d3a96213812020edef927db@web.de>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
Message-ID: <555F108D.9070309@wizonet.ch>

Since long time I'm trying to upgrade from Squid 2.7 to 3.x but every 
try has failed up to now.

I use a ZyXel USG 20 firewall with LAN and DMZ Zone. The Squid proxy 
server (MySquid) is running inside the DMZ with a single IP. The Zyxel 
USG 20 has a option for a "HTTP Redirect". This is defined like:

Forward all HTTP Request from LAN to WAN to MySquid Port 3128.

On MySquid, a Squid 2.7 stable version is running with this setting:

http_port 3128 transparent

It works fine - any HTTP requests from LAN goes through the MySquid Proxy.

When I upgrade from 2.7 to 3.3 with some small modifications to the 
default configuration from Squid 3.3:

ACL settings like before
http_Port 3128 intercept

I always get a "null size" / Bad Gateway 502 message in any browser 
inside the LAN for all HTTP internet pages. If I change the setting to:

http_port 3128

(without intercept) and use a manual browser proxy settings 
(MySquid:3128) - the access works through the new proxy server like before.

But that is no option - I can't and will not define manual proxy 
settings for any client in the LAN :(

I can post more detailed informations - debug logs, settings files - 
just ask what you need

Thanks, Martin


From squid at wizonet.ch  Fri May 22 11:25:32 2015
From: squid at wizonet.ch (wn48z)
Date: Fri, 22 May 2015 13:25:32 +0200
Subject: [squid-users] Zyxel USG20 and Squid 3.3
Message-ID: <555F122C.3070206@wizonet.ch>

Since long time I'm trying to upgrade from Squid 2.7 to 3.x but every
try has failed up to now.

I use a ZyXel USG 20 firewall with LAN and DMZ Zone. The Squid proxy
server (MySquid) is running inside the DMZ with a single IP. The Zyxel
USG 20 has a option for a "HTTP Redirect". This is defined like:

Forward all HTTP Request from LAN to WAN to MySquid Port 3128.

On MySquid, a Squid 2.7 stable version is running with this setting:

http_port 3128 transparent

It works fine - any HTTP requests from LAN goes through the MySquid Proxy.

When I upgrade from 2.7 to 3.3 with some small modifications to the
default configuration from Squid 3.3:

ACL settings like before
http_Port 3128 intercept

I always get a "null size" / Bad Gateway 502 message in any browser
inside the LAN for all HTTP internet pages. If I change the setting to:

http_port 3128

(without intercept) and use a manual browser proxy settings
(MySquid:3128) - the access works through the new proxy server like before.

But that is no option - I can't and will not define manual proxy
settings for any client in the LAN :(

I can post more detailed informations - debug logs, settings files -
just ask what you need

Thanks, Martin


From squid3 at treenet.co.nz  Fri May 22 11:26:13 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 May 2015 23:26:13 +1200
Subject: [squid-users] Squid with proxy
In-Reply-To: <20150522123541.8d3a96213812020edef927db@web.de>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
Message-ID: <555F1255.5000702@treenet.co.nz>

On 22/05/2015 10:35 p.m., Silvio Siefke wrote:
> On Fri, 22 May 2015 15:39:19 +1200 Amos Jeffries wrote:
> 
>> I dont know why you should have to. ziproxy should be perfectly
>> capable of contacting Internet services to respond to the requests
>> sent from Squid.
> 
> Yes it works but im not sure is right, when NextProxy in ziproxy.conf
> is not set. When saw the log all work, but in all tutorials which read
> they say ziproxy.conf need set NextProxy="127.0.0.1" for "routing" back
> to squid. But so it work all without NextProxy. What is now correct?
> 

Without NextProxy is correct if ziproxy is on the "outside" of Squid.
Like so:

 client -> Squid -> ziproxy -> Internet


If you set ziproxy to pass *requests* to Squid, the traffic will enter a
loop:
  client -> Squid -> ziproxy -> Squid -> ziproxy -> ...

Via header would have protected against that loop by aborting the
traffic. But you disabled via. So the only thing preventing your setup
DoS'ing itself by consuming all available TCP ports on the mahine is
that login popup. Ouch.

> 
>> I am not quite understanding what you are talking about auth for. So
>> can't answer that question. Hopefully the above answer is enough to
>> solve your problem though.
> 
> Squid use auth for connecting with it, when i has activated NextProxy in
> ziproxy.conf then Browser ask and ask for login stuff. When not activated
> NextProxy in ziproxy.conf then one time come login window and after login
> all work. But what is now right, set NextProxy or not. But self when set 
> NextProxy in ziproxy.conf then squid can not ask for login to ziproxy,  cause
> localhost has free traffic or not?

In your squid.conf all traffic requires authenticating. Nothing is
allowed through without it. Although anything from localhost is allowed
to send wrong credentials and get through :-( .


Your rules:
> 
> # http access
> http_access allow checkpw all
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access deny ads
> http_access deny all

- "deny ads" is not useful like this, anything getting to that check
will also be blocked by the "deny all" which follows it and is a faster
check.

- also missing the basic HTTP abuse and DoS security protections.

To let localhost I would write them like this:

 # basic security potections.
 # To let special ports through; check carefully its not abuse
 # then adjust Safe_ports and SSL_ports appropriately
 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_Ports

 # To use the deny ads ACL it would go here in the ordering,
 # before the allow rules.
 http_access deny ads

 # localhost does not require authentication
 http_access allow localhost

 # manager access only permitted from localhost
 http_access deny !localhost manager

 # anyone with a valid auth credentials is allowed
 http_access allow checkpw

 http_access deny all


You will need to re-add the CONNECT, Safe_ports and SSL_Ports ACL
definitions from the default config.


You dont really need to exempt localhost from authentication. But that
is your choice.

Amos


From ashish_behl at yahoo.com  Fri May 22 11:14:07 2015
From: ashish_behl at yahoo.com (Ashish Behl)
Date: Fri, 22 May 2015 04:14:07 -0700 (PDT)
Subject: [squid-users] Compiling squid 3.5.4 with ecap enabled.
In-Reply-To: <555DACF0.2050904@treenet.co.nz>
References: <1432195130200-4671325.post@n4.nabble.com>
 <555DACF0.2050904@treenet.co.nz>
Message-ID: <1432293247418-4671358.post@n4.nabble.com>

Thanks a lot Amos,
You are the savior.

Another small query, if answered, could save me a lot of trouble.
I am trying to compile squid 3.5.3 on the older versions of centos (5.11),
suse(11.4) amongst others.

Please let me know if there is a minimum requirement for various packages,
also g++, glibc etc for squid.
I am trying and get various errors in different phases e.g. while running
configure on centos 5, suse 10, i get 
"
configure: error: Negotiate auth helper kerberos ... found but cannot be
built
"
even though krb5-devel is installed.




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Compiling-squid-3-5-4-with-ecap-enabled-tp4671325p4671358.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri May 22 12:11:47 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 00:11:47 +1200
Subject: [squid-users] Zyxel USG20 and Squid 3.3
In-Reply-To: <555F108D.9070309@wizonet.ch>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
 <555F108D.9070309@wizonet.ch>
Message-ID: <555F1D03.3080502@treenet.co.nz>

NP: Its too late now, but please in future start new threads for new
topics. It seriously screws up reading for those of us with threaded
mailers or forum-style mirrors of the mailing list like Nabble.


On 22/05/2015 11:18 p.m., wn48z wrote:
> Since long time I'm trying to upgrade from Squid 2.7 to 3.x but every
> try has failed up to now.
> 
> I use a ZyXel USG 20 firewall with LAN and DMZ Zone. The Squid proxy
> server (MySquid) is running inside the DMZ with a single IP. The Zyxel
> USG 20 has a option for a "HTTP Redirect". This is defined like:
> 
> Forward all HTTP Request from LAN to WAN to MySquid Port 3128.

Sounds like a nasty recipe for trouble forwarding all your LAN traffic
via somewhere on the Internet to your internal proxy. I hope that is
just terrible documentation on the part of the firewall authors.


The answer to your problem sits in how this firewall feature actually
works...

* If thats a fancy name for NAT or NAPT / port-forwarding then its not
usable to get traffic to Squid.

* If its a mini proxy relaying the traffic then Squid should be setup
with a regular forward-proxy port to receive it.

* If its something else, it may or may not be workable.

Squid requires firewalls and routers on other machines to be doing
Layer-2 (routing) or Layer-3 (tunneling) packet forwarding without the
IP address destroying operations that NAT does.


> On MySquid, a Squid 2.7 stable version is running with this setting:
> 
> http_port 3128 transparent
> 
> It works fine - any HTTP requests from LAN goes through the MySquid Proxy.
> 

Well it *seems* to work. But only because Squid-2.7 was lying to you in
its logs.

Old Squid like 2.7 would take the most outrageous lies and forgery in
the TCP/IP packets and believe them. But log the HTTP level details and
tell you it was going to the place the client wanted even if the client
would actually have gone to some other server entirely had Squid not
been there in the path.

3.2 and later contain a bit more security to ensure the traffic actually
goes to the server the client was connecting to (ORIGINAL_DST or a
properly DNS listed equivalent with the same domain name).


Your firewall though is telling your Squid that the web server the
client was visiting is hosted at SquidIP:3128. NAT lies!

> But that is no option - I can't and will not define manual proxy settings for any client in the LAN :-(

No need to fear manual configuration. At the very least WPAD
auto-configuration is your friend.


You also have the easier option of placing the Squid machine physically
in the network path before or after the ZyXel. Configuring the Squid box
as a bridge + router with NAT sending port 80 traffic through Squid
directly on the same box as required to make interception work.

Amos



From squid3 at treenet.co.nz  Fri May 22 12:47:29 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 00:47:29 +1200
Subject: [squid-users] Zyxel USG20 and Squid 3.3
In-Reply-To: <555F1D03.3080502@treenet.co.nz>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
 <555F108D.9070309@wizonet.ch> <555F1D03.3080502@treenet.co.nz>
Message-ID: <555F2561.6070708@treenet.co.nz>

Flipping through yoru devices User Guide I see it has Policy Routing
features you can configure. But is very light on details so I cant help
with exact config.

However, if you can figure out how to implement these policies
<http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#When_Squid_is_in_a_DMZ_between_the_router_and_Internet>
using their controls you should be able to setup NAT on the Squid
machine to do the rest that makes it work.

Amos


From squid at wizonet.ch  Fri May 22 12:51:41 2015
From: squid at wizonet.ch (wn48z)
Date: Fri, 22 May 2015 14:51:41 +0200
Subject: [squid-users] Zyxel USG20 and Squid 3.3
In-Reply-To: <555F1D03.3080502@treenet.co.nz>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
 <555F108D.9070309@wizonet.ch> <555F1D03.3080502@treenet.co.nz>
Message-ID: <555F265D.3060902@wizonet.ch>

> NP: Its too late now, but please in future start new threads for new
> topics. It seriously screws up reading for those of us with threaded
> mailers or forum-style mirrors of the mailing list like Nabble.
Yes, this was not planned - I first used the wrong button in my email 
program (list-answer and not create). But later (before your reply) I 
also create this new thread - shall we use the other thread for further 
posts?

> Sounds like a nasty recipe for trouble forwarding all your LAN traffic
> via somewhere on the Internet to your internal proxy. I hope that is
> just terrible documentation on the part of the firewall authors.
>
>
> The answer to your problem sits in how this firewall feature actually
> works...
>
> * If thats a fancy name for NAT or NAPT / port-forwarding then its not
> usable to get traffic to Squid.
>
> * If its a mini proxy relaying the traffic then Squid should be setup
> with a regular forward-proxy port to receive it.
>
> * If its something else, it may or may not be workable.
>
> Squid requires firewalls and routers on other machines to be doing
> Layer-2 (routing) or Layer-3 (tunneling) packet forwarding without the
> IP address destroying operations that NAT does.
The ZyXel USG 20 is a linux based hardware router/firewall solution for 
small business use. I have found a small online overview to the HTTP 
Redirect functionality. In my opinion - this should work well with Squid?

http://www.manualslib.com/manual/363461/Zyxel-Communications-Zywall-Usg-20.html?page=347#manual 


The picture on this site shows exactly my configuration.
>> On MySquid, a Squid 2.7 stable version is running with this setting:
>>
>> http_port 3128 transparent
>>
>> It works fine - any HTTP requests from LAN goes through the MySquid Proxy.
>>
> Well it *seems* to work. But only because Squid-2.7 was lying to you in
> its logs.
>
> Old Squid like 2.7 would take the most outrageous lies and forgery in
> the TCP/IP packets and believe them. But log the HTTP level details and
> tell you it was going to the place the client wanted even if the client
> would actually have gone to some other server entirely had Squid not
> been there in the path.
>
> 3.2 and later contain a bit more security to ensure the traffic actually
> goes to the server the client was connecting to (ORIGINAL_DST or a
> properly DNS listed equivalent with the same domain name).
Could be that it lies - but I also use squidGuard and blocked content is 
really blocked - so I think that Squid 2.7 should work correctly.

> Your firewall though is telling your Squid that the web server the
> client was visiting is hosted at SquidIP:3128. NAT lies!
>
>> But that is no option - I can't and will not define manual proxy settings for any client in the LAN :-(
> No need to fear manual configuration. At the very least WPAD
> auto-configuration is your friend.
>
>
> You also have the easier option of placing the Squid machine physically
> in the network path before or after the ZyXel. Configuring the Squid box
> as a bridge + router with NAT sending port 80 traffic through Squid
> directly on the same box as required to make interception work.
The Squid box should not work as a network device. This is not an 
option. I think it should be possible to make Squid 3.x work if it was 
possible with Squid 2.7?

Tkanks, Martin



From pkryon at gmail.com  Fri May 22 12:59:31 2015
From: pkryon at gmail.com (Patrick)
Date: Fri, 22 May 2015 08:59:31 -0400
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <555EB047.5010102@treenet.co.nz>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
 <555EB047.5010102@treenet.co.nz>
Message-ID: <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>

Thanks for looking at this, Amos.  Unfortunately, I am still seeing the
same result with the squid-3.5.4-20150522-r13836 daily.

I was thinking I might just being seeing what's described at
http://www.squid-cache.org/Doc/config/url_rewrite_extras/ as "In practice,
a %macro expands as a dash (-) if the helper request is sent before the
required macro information is available to Squid."  But I guess if that was
the case I would have expected to see the dash only on the first request
with the subsequent requests returning the %ue value at least within the
ttl.  But I get just the dash on every request.

On Fri, May 22, 2015 at 12:27 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 22/05/2015 4:10 a.m., Patrick wrote:
> > Hello,
> >
> > I think I'm having a problem with the url_rewrite_extras function.  I'm
> > using version 3.5.3. I've been trying to use the %ue macro to send the
> > user= returned by my external_acl_type program.  But when I look at
> > the url_rewrite_program output I just get a "-" value for this macro.
> >
>
> I think you are probably encountering a bug found recently in
> exetrnal_acl_type handling of kv-pair notes.
>
> The patch for that is
> <
> http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13827.patch
> >.
> Just ported it back to 3.5 so will be in the next snapshot.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/0d2a3abd/attachment.htm>

From squid3 at treenet.co.nz  Fri May 22 13:14:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 01:14:04 +1200
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
 <555EB047.5010102@treenet.co.nz>
 <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>
Message-ID: <555F2B9C.90109@treenet.co.nz>

On 23/05/2015 12:59 a.m., Patrick wrote:
> Thanks for looking at this, Amos.  Unfortunately, I am still seeing the
> same result with the squid-3.5.4-20150522-r13836 daily.
> 
> I was thinking I might just being seeing what's described at
> http://www.squid-cache.org/Doc/config/url_rewrite_extras/ as "In practice,
> a %macro expands as a dash (-) if the helper request is sent before the
> required macro information is available to Squid."  But I guess if that was
> the case I would have expected to see the dash only on the first request
> with the subsequent requests returning the %ue value at least within the
> ttl.  But I get just the dash on every request.
> 

No, you should be getting the value from the user=X kv-pair produced by
the helper on every request. The bug fixed by Nathan was that only the
first was and the rest wrongly got "-".

If you are getting it on absolutely all requests it would seem the
helper is not producing user=X.

Or possibly ICAP is stripping it away again by creating a new "adapted"
request. If that is the case use the external_acl_type helper in the
adapted_http_access checks instead of http_access.

Amos



From lucas2 at dds.nl  Fri May 22 13:49:38 2015
From: lucas2 at dds.nl (Lucas van Braam van Vloten)
Date: Fri, 22 May 2015 15:49:38 +0200
Subject: [squid-users] Proxy chain question
In-Reply-To: <555EA006.7080105@treenet.co.nz>
References: <1432227526.9790.6.camel@dds.nl> <555EA006.7080105@treenet.co.nz>
Message-ID: <1432302578.20568.29.camel@dds.nl>

Hello,

Thanks for your reply.

> Any particular reason?
Unfortunately this double setup is not my choice, our architects
prescribe use of the TMG proxy as mandatory for all internet access from
the internal network. No exceptions.
> Since the TMG is a reverse-proxy (...)
This is true only for inbound traffic coming from internet; TMG acts as
a forward proxy for outbound traffic.

So the updated diagram for what I am trying to accomplish would be
something like this:

               http       https       https
Internal client ->  Squid  ->   TMG    -> internet webservice
                  (reverse    (forward    
                   proxy)      proxy)

> Squid will not be able to handle this either unless it is directly
connecting to that service without the TMG in the way.

I noticed that if I configure the Squid proxy as a forward proxy and the
TMG as its peer, I can initiate and authenticate a secure connection to
the internet web service from a browser in the local network (using the
squid proxy). Apparently the TMG is passed transparently and TLS is
terminated on the webservice. Intuitively I would assume that,
therefore, there should also be some way to initiate a https connection,
and handle the certificate authentication, from the squid server itself.

Considering the updated diagram, do you think this can be done in Squid?

Thanks,
Lucas




-----Original Message-----From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Proxy chain question
Date: Fri, 22 May 2015 15:18:30 +1200

On 22/05/2015 4:58 a.m., Lucas van Braam van Vloten wrote:
> Hello list,
> 
> In my network I have a Microsoft TMG proxy server for http(s) access to
> internet.
> This TMG server also serves as a reverse proxy to channel incoming
> traffic to a Squid reverse proxy in the internal network (yes, two
> reverse proxies in a line)

Any particular reason? It may prevent Squid being able to do what you want.

> 
> This Squid server is currently configured as a reverse proxy to allow
> traffic from internet to a number of webservices that run on an internal
> server.
> 
> Now I want to add a function to the squid server, in addition to the
> existing function. It should serve as a proxy to allow a client on the
> internal network to access a web servoce on internet.
> So, put simply, the traffic goes like this:
> Internal client -> Squid Proxy -> TMG proxy -> internet webservice

>From that diagram I'm a little doubtful that reverse-proxy is the right
way to do it here. Proxy at the client end of the connectivity are
usually forward- or interceptor- proxy.


> 
> The reason to use this configuration is because the internet webservice
> requires a client certificate for authentication, and TMG is not able to
> handle this.

Squid will not be able to handle this either unless it is directly
connecting to that service without the TMG in the way.

Because TLS is point-to-point security protocol, any proxy agent in the
middle must terminate the clients TLS and start its own server
connection for the next hop. Squid does not (yet) support sending
CONNECT over a peer proxy to bypass the TMG.

So in your current setup Squid sending the client cert will be sending
it to to authenticate with the *TMG* - not the web service.


> So now I am trying to configure this on my Squid server. I wish to make
> my configuration as restrictive as possible. But I am new to the Squid
> configuration, and I could use some help.
> 
> So basically, I want the following:
> 1. The client makes a http connection to my Squid proxy
> 2. The Squid proxy initiates the client certificate authenticated
> connection to the internet webservice
> 3. The connection from the Squid proxy to Internet uses the TMG proxy.
> 
> I do not wish to use any form of caching on my Squid server.
> 
> I considered using a configuration similar to my reverse proxy
> configuration, using the following structure:
> (this configuration works)
> 
> =====================
> 
> # Designate a port and SSL config for this specific webservice
> # Local server IP is 192.168.0.1, traffic comes in through the TMG
> https_port 192.168.0.1:1443 accel
> defaultsite=webservice.exposed.address.com vhost <SSL stuff>
> 
> # enforce use of https
> acl webapp_SITES dstdomain webservice.exposed.address.com
> http_access deny HTTP webapp_SITES
> http_access allow webapp_SITES
> 
> # Configure the reverse proxy for clients that connect to the external
> (exposed) address
> acl webapp_URL url_regex ^https://webservice.exposed.address.com
> cache_peer internal.server.lan parent 8080 0 no-query no-digest
> originserver login=PASS name=webservice_APP
> cache_peer_access webservice_APP allow webapp_URL
> cache_peer_access webservice_APP deny all
> 
> =====================
> 
> So if I use this for my new purpose, I assume that the cache_peer would
> be the internet webservice address, and I could use the sslcert option
> to make it use the client certificate. Something like this:
> 
> =====================
> 
> http_port 192.168.0.1:8080 accel defaultsite=squid.server.lan vhost
> acl webapp_URL url_regex ^http://squid.server.lan
> cache_peer webservice.somewhere.on.internet.com parent 8443 0 no-query
> no-digest originserver sslkey=/path/to/ssl/key name=webservice_APP
> cache_peer_access webservice_APP allow webapp_URL
> cache_peer_access webservice_APP deny all
> 
> =====================

That is the correct way to do it outbound from Squid. But the catch as
mentioned above is where the TLS link gets terminated (the TMG or the
web service).

Since the TMG is a reverse-proxy the DNS records for the cache_peer
domain name points at the TMG instead of the Internet service. You have
to have the cache_peer directly going to the server which uses/requires
the client certificate. Which means using IP or if available the
services own host name to avoid the TMG.

Which cycles back to that first question I had at the top about why the
TMG exists at all. You may or may not be able to do this without a full
topology redesign.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From siefke_listen at web.de  Fri May 22 14:05:24 2015
From: siefke_listen at web.de (Silvio Siefke)
Date: Fri, 22 May 2015 16:05:24 +0200
Subject: [squid-users] Squid with proxy
In-Reply-To: <555F1255.5000702@treenet.co.nz>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
 <555F1255.5000702@treenet.co.nz>
Message-ID: <20150522160524.aae9ac5a3187c7dfc83909cf@web.de>

On Fri, 22 May 2015 23:26:13 +1200 Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> Without NextProxy is correct if ziproxy is on the "outside" of Squid.
> Like so:
> 
>  client -> Squid -> ziproxy -> Internet

In my browser i speak only with squid, other make squid i hope. I use
ziproxy for compress the traffic, but when i saw the rate is not really
much.

http://silviosiefke.de/squid/zip.html

> If you set ziproxy to pass *requests* to Squid, the traffic will
> enter a loop:
>   client -> Squid -> ziproxy -> Squid -> ziproxy -> ...

client > squid > ziproxy > squid > client so is my plan
 
> In your squid.conf all traffic requires authenticating. Nothing is
> allowed through without it. Although anything from localhost is
> allowed to send wrong credentials and get through :-( .

localhost should work without authenticating. I think this is problem
why NextProxy in ziproxy.conf not work correct. 
 
> - "deny ads" is not useful like this, anything getting to that check
> will also be blocked by the "deny all" which follows it and is a
> faster check.
> 
> - also missing the basic HTTP abuse and DoS security protections.
> 
> To let localhost I would write them like this:
> 
>  # basic security potections.
>  # To let special ports through; check carefully its not abuse
>  # then adjust Safe_ports and SSL_ports appropriately
>  http_access deny !Safe_ports
>  http_access deny CONNECT !SSL_Ports
> 
>  # To use the deny ads ACL it would go here in the ordering,
>  # before the allow rules.
>  http_access deny ads
> 
>  # localhost does not require authentication
>  http_access allow localhost
> 
>  # manager access only permitted from localhost
>  http_access deny !localhost manager
> 
>  # anyone with a valid auth credentials is allowed
>  http_access allow checkpw
> 
>  http_access deny all
> 
> 
> You will need to re-add the CONNECT, Safe_ports and SSL_Ports ACL
> definitions from the default config.

Okay thank you, im shamed but i really has not understand what mean 
SSL Ports and so now i understand more. 

> You dont really need to exempt localhost from authentication. But that
> is your choice.

Only connection over port 15000 need authentication because is extern and
best were only my login goes. Localhost should work without any limitiation.

Thank you very much & Nice Day
Silvio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 473 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/34cc5372/attachment.sig>

From pkryon at gmail.com  Fri May 22 14:44:53 2015
From: pkryon at gmail.com (Patrick)
Date: Fri, 22 May 2015 10:44:53 -0400
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <555F2B9C.90109@treenet.co.nz>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
 <555EB047.5010102@treenet.co.nz>
 <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>
 <555F2B9C.90109@treenet.co.nz>
Message-ID: <CAJgbPpfxTfghie8+eQmn9xs3RNT_8vOzEfkxDRoKw6JWh5Fa+Q@mail.gmail.com>

Hmm, the helper does seem to be working and returning user=X as expected.
In fact the user does show up in Squid's access.log and as configured I
have no other sources for usernames.

I'm not using ICAP (at least intentionally), but switching
to adapted_http_access doesn't not seem to help me.  I switch
my url_rewrite_access from the tee I was running before to a quick script
that logs stdin and returns OK no matter what it gets.

So using...

url_rewrite_extras "%>a/%>A %ue %>rm myip=%la myport=%lp"
url_rewrite_program /opt/idbf-dev/idbf_squid_url_rewrite_test.py

My logs show:

2015-05-22 10:34:59,175 - root - DEBUG    -
http://www.ipchicken.com/images/9.gif 172.20.15.235/pcname.example.com -
GET myip=- myport=3129
2015-05-22 10:34:59,213 - root - DEBUG    -
http://www.ipchicken.com/images/pixel.gif 172.20.15.235/pcname.example.com
- GET myip=- myport=3129
2015-05-22 10:34:59,225 - root - DEBUG    -
http://www.ipchicken.com/images/green.gif 172.20.15.235/pcname.example.com
- GET myip=- myport=3129
2015-05-22 10:34:59,241 - root - DEBUG    -
http://www.ipchicken.com/images/ipc.gif 172.20.15.235/pcname.example.com -
GET myip=- myport=3129
2015-05-22 10:35:01,052 - root - DEBUG    - http://www.ipchicken.com/
172.20.15.235/pcname.example.com - GET myip=- myport=3129

And access.log shows:

1432305299.216     41 172.20.15.235 TCP_MISS/304 270 GET
http://www.ipchicken.com/images/9.gif ryonpk ORIGINAL_DST/209.68.27.16 -
1432305299.256     43 172.20.15.235 TCP_MISS/304 270 GET
http://www.ipchicken.com/images/pixel.gif ryonpk ORIGINAL_DST/209.68.27.16 -
1432305299.264     51 172.20.15.235 TCP_MISS/304 271 GET
http://www.ipchicken.com/images/green.gif ryonpk ORIGINAL_DST/209.68.27.16 -
1432305299.281     68 172.20.15.235 TCP_MISS/304 271 GET
http://www.ipchicken.com/images/ipc.gif ryonpk ORIGINAL_DST/209.68.27.16 -
1432305301.137     85 172.20.15.235 TCP_MISS/200 6488 GET
http://www.ipchicken.com/ ryonpk ORIGINAL_DST/209.68.27.16 text/html

ryonpk is the correct user passed from the helper.

On Fri, May 22, 2015 at 9:14 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 23/05/2015 12:59 a.m., Patrick wrote:
> > Thanks for looking at this, Amos.  Unfortunately, I am still seeing the
> > same result with the squid-3.5.4-20150522-r13836 daily.
> >
> > I was thinking I might just being seeing what's described at
> > http://www.squid-cache.org/Doc/config/url_rewrite_extras/ as "In
> practice,
> > a %macro expands as a dash (-) if the helper request is sent before the
> > required macro information is available to Squid."  But I guess if that
> was
> > the case I would have expected to see the dash only on the first request
> > with the subsequent requests returning the %ue value at least within the
> > ttl.  But I get just the dash on every request.
> >
>
> No, you should be getting the value from the user=X kv-pair produced by
> the helper on every request. The bug fixed by Nathan was that only the
> first was and the rest wrongly got "-".
>
> If you are getting it on absolutely all requests it would seem the
> helper is not producing user=X.
>
> Or possibly ICAP is stripping it away again by creating a new "adapted"
> request. If that is the case use the external_acl_type helper in the
> adapted_http_access checks instead of http_access.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/e6220552/attachment.htm>

From emperor.cu at gmail.com  Fri May 22 16:26:40 2015
From: emperor.cu at gmail.com (=?UTF-8?Q?Tony_Pe=C3=B1a?=)
Date: Fri, 22 May 2015 18:26:40 +0200
Subject: [squid-users] Squid + Ssl filter conf
Message-ID: <CALBaCdusb5RE-2TC3kzQQfCcL7ek1Y9n902w==tLtHi8RWZ=ZQ@mail.gmail.com>

Hi... i tired to research about squid with ssl_bump with many guides.
compiling from 3.48 to 3.5.4 and with squid.conf ok but not work for me.

someone can share please a squid.conf with your ssl_bump snipped working
actually...
every manual/guide i found are with very older version, ad the suggest way
is upgrade to last version.. but still stuck.
i'm continue searching on the google with many variables to try got
solution and now starting found my own emails on the list about this topic.

i really appretiated the help.

thanks in advance.

-- 
Antonio Pe?a
Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
<https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/faa88dab/attachment.htm>

From ahmed.zaeem at netstream.ps  Sat May 23 03:33:20 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Fri, 22 May 2015 20:33:20 -0700
Subject: [squid-users] https quick question
In-Reply-To: <555EA0FE.1090709@treenet.co.nz>
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz>
 <000a01d0943e$8aa70830$9ff51890$@netstream.ps>
 <555EA0FE.1090709@treenet.co.nz>
Message-ID: <005f01d09509$3841b920$a8c52b60$@netstream.ps>

Sorry amos ,  what shoud I modify squid.conf ?

As I told u all I added is I  installed the tool
yum -y install crypto-utils 


And generated private and public keys
genkey -days 365  xxx

then 

And added to squid.conf
https_port xxx:443 accel cert=/etc/pki/tls/certs/xxx.crt key=/etc/pki/tls/private/xxx vhost




still has same error !!

I tried from different browser and different pc and same thing !
? any help 

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Thursday, May 21, 2015 8:23 PM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https quick question

On 22/05/2015 3:22 p.m., snakeeyes wrote:
> clientNegotiateSSL: Error negotiating SSL connection on FD 36: 
> error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request 
> (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 45: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 45: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 54: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 29: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 

IIRC, that is OpenSSL library complaining that you passed it un-encrypted HTTP message syntax (port 80 or 3128).

HTTP (port 80) to an http_port

HTTPS (port 443) to an https_port

FTP (port 21) to an ftp_port

... the hint is in the *_port naming.

Amos



From stan.prescott at gmail.com  Fri May 22 18:26:19 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 22 May 2015 13:26:19 -0500
Subject: [squid-users] Squid + Ssl filter conf
In-Reply-To: <CALBaCdusb5RE-2TC3kzQQfCcL7ek1Y9n902w==tLtHi8RWZ=ZQ@mail.gmail.com>
References: <CALBaCdusb5RE-2TC3kzQQfCcL7ek1Y9n902w==tLtHi8RWZ=ZQ@mail.gmail.com>
Message-ID: <CANLNtGS4ous+g4fr0iVNevG4=oz9yPK1racGO--ePzFWeMYPWg@mail.gmail.com>

This works for me with Squid 3.5.4. Hope it helps.



*acl localhostgreen src 192.168.192.1acl localnetgreen src 192.168.192.0/24
<http://192.168.192.0/24>*
















*http_access allow localhosthttp_access deny !Safe_portshttp_access deny
CONNECT !SSL_portshttp_access allow localnetgreenhttp_access allow CONNECT
localnetgreenhttp_access allow localhostgreenhttp_access allow CONNECT
localhostgreen# http_port and
https_port#----------------------------------------------------------------------------http_port
192.168.192.1:800 <http://192.168.192.1:800> intercepthttps_port
192.168.192.1:808 <http://192.168.192.1:808> intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*

*# localhost forward-proxy port needed for ssl_bump*






*http_port 127.0.0.1:800 <http://127.0.0.1:800>
interceptsslproxy_cert_error allow allsslproxy_flags
DONT_VERIFY_PEERsslproxy_session_cache_size 4 MB*

*# Do not bump local networks*







*ssl_bump none localhostgreenssl_bump bump allsslcrtd_program
/var/smoothwall/mods/proxy/libexec/ssl_crtd -s
/var/smoothwall/mods/proxy/lib/ssl_db -M 4MBsslcrtd_children 5http_access
deny all*

Stan

On Fri, May 22, 2015 at 11:26 AM, Tony Pe?a <emperor.cu at gmail.com> wrote:

> Hi... i tired to research about squid with ssl_bump with many guides.
> compiling from 3.48 to 3.5.4 and with squid.conf ok but not work for me.
>
> someone can share please a squid.conf with your ssl_bump snipped working
> actually...
> every manual/guide i found are with very older version, ad the suggest way
> is upgrade to last version.. but still stuck.
> i'm continue searching on the google with many variables to try got
> solution and now starting found my own emails on the list about this topic.
>
> i really appretiated the help.
>
> thanks in advance.
>
> --
> Antonio Pe?a
> Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
> <https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
> Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/3013db73/attachment.htm>

From stan.prescott at gmail.com  Fri May 22 23:30:16 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Fri, 22 May 2015 18:30:16 -0500
Subject: [squid-users] Squid + Ssl filter conf
In-Reply-To: <CANLNtGS4ous+g4fr0iVNevG4=oz9yPK1racGO--ePzFWeMYPWg@mail.gmail.com>
References: <CALBaCdusb5RE-2TC3kzQQfCcL7ek1Y9n902w==tLtHi8RWZ=ZQ@mail.gmail.com>
 <CANLNtGS4ous+g4fr0iVNevG4=oz9yPK1racGO--ePzFWeMYPWg@mail.gmail.com>
Message-ID: <CANLNtGSVYZS7UO067TF+9dWBmyPBnV9-1hG4RWSEeq0wf64XGQ@mail.gmail.com>

I also forgot to mention that for Squid 3.5.x /dev/shm needs to be
root:root and privileges of 0777.

On Fri, May 22, 2015 at 1:26 PM, Stanford Prescott <stan.prescott at gmail.com>
wrote:

> This works for me with Squid 3.5.4. Hope it helps.
>
>
>
> *acl localhostgreen src 192.168.192.1acl localnetgreen src
> 192.168.192.0/24 <http://192.168.192.0/24>*
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> *http_access allow localhosthttp_access deny !Safe_portshttp_access deny
> CONNECT !SSL_portshttp_access allow localnetgreenhttp_access allow CONNECT
> localnetgreenhttp_access allow localhostgreenhttp_access allow CONNECT
> localhostgreen# http_port and
> https_port#----------------------------------------------------------------------------http_port
> 192.168.192.1:800 <http://192.168.192.1:800> intercepthttps_port
> 192.168.192.1:808 <http://192.168.192.1:808> intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/var/smoothwall/mods/proxy/ssl_cert/squidCA.pem*
>
> *# localhost forward-proxy port needed for ssl_bump*
>
>
>
>
>
>
> *http_port 127.0.0.1:800 <http://127.0.0.1:800>
> interceptsslproxy_cert_error allow allsslproxy_flags
> DONT_VERIFY_PEERsslproxy_session_cache_size 4 MB*
>
> *# Do not bump local networks*
>
>
>
>
>
>
>
> *ssl_bump none localhostgreenssl_bump bump allsslcrtd_program
> /var/smoothwall/mods/proxy/libexec/ssl_crtd -s
> /var/smoothwall/mods/proxy/lib/ssl_db -M 4MBsslcrtd_children 5http_access
> deny all*
>
> Stan
>
> On Fri, May 22, 2015 at 11:26 AM, Tony Pe?a <emperor.cu at gmail.com> wrote:
>
>> Hi... i tired to research about squid with ssl_bump with many guides.
>> compiling from 3.48 to 3.5.4 and with squid.conf ok but not work for me.
>>
>> someone can share please a squid.conf with your ssl_bump snipped working
>> actually...
>> every manual/guide i found are with very older version, ad the suggest
>> way is upgrade to last version.. but still stuck.
>> i'm continue searching on the google with many variables to try got
>> solution and now starting found my own emails on the list about this topic.
>>
>> i really appretiated the help.
>>
>> thanks in advance.
>>
>> --
>> Antonio Pe?a
>> Secure email with PGP 0x8B021001 available at https://pgp.mit.edu
>> <https://pgp.mit.edu/pks/lookup?search=0x8B021001&op=index&fingerprint=on&exact=on>
>> Fingerprint: 74E6 2974 B090 366D CE71  7BB2 6476 FA09 8B02 1001
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150522/a393b395/attachment.htm>

From squid3 at treenet.co.nz  Sat May 23 03:53:25 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 15:53:25 +1200
Subject: [squid-users] Proxy chain question
In-Reply-To: <1432302578.20568.29.camel@dds.nl>
References: <1432227526.9790.6.camel@dds.nl>	 <555EA006.7080105@treenet.co.nz>
 <1432302578.20568.29.camel@dds.nl>
Message-ID: <555FF9B5.2080801@treenet.co.nz>

On 23/05/2015 1:49 a.m., Lucas van Braam van Vloten wrote:
> Hello,
> 
> Thanks for your reply.
> 
>> Any particular reason?
> Unfortunately this double setup is not my choice, our architects
> prescribe use of the TMG proxy as mandatory for all internet access from
> the internal network. No exceptions.
>> Since the TMG is a reverse-proxy (...)
> This is true only for inbound traffic coming from internet; TMG acts as
> a forward proxy for outbound traffic.
> 

Ah. Okay.

> So the updated diagram for what I am trying to accomplish would be
> something like this:
> 
>                http       https       https
> Internal client ->  Squid  ->   TMG    -> internet webservice
>                   (reverse    (forward    
>                    proxy)      proxy)
> 
>> Squid will not be able to handle this either unless it is directly
> connecting to that service without the TMG in the way.
> 
> I noticed that if I configure the Squid proxy as a forward proxy and the
> TMG as its peer, I can initiate and authenticate a secure connection to
> the internet web service from a browser in the local network (using the
> squid proxy). Apparently the TMG is passed transparently and TLS is
> terminated on the webservice. Intuitively I would assume that,
> therefore, there should also be some way to initiate a https connection,
> and handle the certificate authentication, from the squid server itself.
> 
> Considering the updated diagram, do you think this can be done in Squid?

No, because Squid is only aware of two ways to send request - either a
connection going to the TMG, or a connection going out directly on port
443 to the server (bypassing the TMG). That latter is forbidden by
firewall rules I presume, and the connection to the TMG is not secured
for use with https:// URLs.

In the case where the browser client is sending HTTPS to Squid as a
forward proxy it does so using CONNECT requests. Squid is able to relay
those CONNECT messages to the TMG and you see it working as the tunnel
spans both hops and they are both blind to the HTTPS messages themselves.


If you could TLS encrypt the connection to the TMG Squid could send the
HTTPS messages inside that, but then the TMG would still be the agent
doing the final client-cert bits with the origin server.

Amos


From squid3 at treenet.co.nz  Sat May 23 04:20:16 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 16:20:16 +1200
Subject: [squid-users] Squid with proxy
In-Reply-To: <20150522160524.aae9ac5a3187c7dfc83909cf@web.de>
References: <20150521223227.7c3b186e6133053c87c3cb19@web.de>
 <555EA4E7.8060205@treenet.co.nz>
 <20150522123541.8d3a96213812020edef927db@web.de>
 <555F1255.5000702@treenet.co.nz>
 <20150522160524.aae9ac5a3187c7dfc83909cf@web.de>
Message-ID: <55600000.1090508@treenet.co.nz>

On 23/05/2015 2:05 a.m., Silvio Siefke wrote:
> On Fri, 22 May 2015 23:26:13 +1200 Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> 
>> Without NextProxy is correct if ziproxy is on the "outside" of Squid.
>> Like so:
>>
>>  client -> Squid -> ziproxy -> Internet
> 
> In my browser i speak only with squid, other make squid i hope. I use
> ziproxy for compress the traffic, but when i saw the rate is not really
> much.
> 
> http://silviosiefke.de/squid/zip.html
> 

Small, but reasonable.

Note that Squid is an HTTP/1.1 proxy designed to optimize traffic.
HTTP/1.1 contains features like revalidation and conditional requests
which are more efficient with bandwidth than even 100% compression would
be. So there is not much uncompressed but compressible content for
ziproxy to be working with.


>> If you set ziproxy to pass *requests* to Squid, the traffic will
>> enter a loop:
>>   client -> Squid -> ziproxy -> Squid -> ziproxy -> ...
> 
> client > squid > ziproxy > squid > client so is my plan

Squid is fetchng content from the client?

I think you are misunderstanding the "->" diagram syntax and how
connectivity works.

Request messages go:
 client -> Squid -> ziproxy -> Internet

Response messages go:
 Internet -> ziproxy -> Squid -> client

You only need to configure the request hop order - the cache_peer in
Squid, maybe proxy settings in client. The response chain is handled
automatically by TCP based on where the request went.

Squi dneeds teh cache_peer so that it goes to ziproxy instead of
straight to Internet.
 ziproxy needs *nothing* - so that is does go to Internet. If you see it
with Squid you get the infinite loop, and the client gets nothing or
errors back when the loop causes enough trouble to be noticed.

>  
>> In your squid.conf all traffic requires authenticating. Nothing is
>> allowed through without it. Although anything from localhost is
>> allowed to send wrong credentials and get through :-( .
> 
> localhost should work without authenticating. I think this is problem
> why NextProxy in ziproxy.conf not work correct. 
>  

You had two problems. The auth was one. The forwarding loop (NextProxy
being used) was another.

>> - "deny ads" is not useful like this, anything getting to that check
>> will also be blocked by the "deny all" which follows it and is a
>> faster check.
>>
>> - also missing the basic HTTP abuse and DoS security protections.
>>
>> To let localhost I would write them like this:
>>
>>  # basic security potections.
>>  # To let special ports through; check carefully its not abuse
>>  # then adjust Safe_ports and SSL_ports appropriately
>>  http_access deny !Safe_ports
>>  http_access deny CONNECT !SSL_Ports
>>
>>  # To use the deny ads ACL it would go here in the ordering,
>>  # before the allow rules.
>>  http_access deny ads
>>
>>  # localhost does not require authentication
>>  http_access allow localhost
>>
>>  # manager access only permitted from localhost
>>  http_access deny !localhost manager
>>
>>  # anyone with a valid auth credentials is allowed
>>  http_access allow checkpw
>>
>>  http_access deny all
>>
>>
>> You will need to re-add the CONNECT, Safe_ports and SSL_Ports ACL
>> definitions from the default config.
> 
> Okay thank you, im shamed but i really has not understand what mean 
> SSL Ports and so now i understand more. 

No shame. Thats what we do here in this mailing list, help people with
mistakes and problems. :-)

> 
>> You dont really need to exempt localhost from authentication. But that
>> is your choice.
> 
> Only connection over port 15000 need authentication because is extern and
> best were only my login goes. Localhost should work without any limitiation.

Then you may be interested in using a myportname ACL to enforce that.
Like this:
 http_port 15000 ... name=blah
 ...
 acl port15000 myportname blah
 http_access allow port15000 checkpw


Amos


From ahmed.zaeem at netstream.ps  Sat May 23 17:32:42 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Sat, 23 May 2015 10:32:42 -0700
Subject: [squid-users] https quick question
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz>
 <000a01d0943e$8aa70830$9ff51890$@netstream.ps>
 <555EA0FE.1090709@treenet.co.nz> 
Message-ID: <007201d0957e$7a93d130$6fbb7390$@netstream.ps>

Amos any help ?

I spent many days without any luck !

clientNegotiateSSL: Error negotiating SSL connection on FD 25: error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request (1/-1)


????????

-----Original Message-----
From: snakeeyes [mailto:ahmed.zaeem at netstream.ps] 
Sent: Friday, May 22, 2015 8:33 PM
To: 'Amos Jeffries'
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] https quick question

Sorry amos ,  what shoud I modify squid.conf ?

As I told u all I added is I  installed the tool yum -y install crypto-utils 


And generated private and public keys
genkey -days 365  xxx

then 

And added to squid.conf
https_port xxx:443 accel cert=/etc/pki/tls/certs/xxx.crt key=/etc/pki/tls/private/xxx vhost




still has same error !!

I tried from different browser and different pc and same thing !
? any help 

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz]
Sent: Thursday, May 21, 2015 8:23 PM
To: snakeeyes
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https quick question

On 22/05/2015 3:22 p.m., snakeeyes wrote:
> clientNegotiateSSL: Error negotiating SSL connection on FD 36: 
> error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request
> (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 45: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 36: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 45: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 54: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 2015/05/21 20:20:17| clientNegotiateSSL: Error negotiating SSL 
> connection on FD 29: error:1407609C:SSL 
> routines:SSL23_GET_CLIENT_HELLO:http request (1/-1)
> 

IIRC, that is OpenSSL library complaining that you passed it un-encrypted HTTP message syntax (port 80 or 3128).

HTTP (port 80) to an http_port

HTTPS (port 443) to an https_port

FTP (port 21) to an ftp_port

... the hint is in the *_port naming.

Amos



From squid3 at treenet.co.nz  Sat May 23 11:51:28 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 May 2015 23:51:28 +1200
Subject: [squid-users] https quick question
In-Reply-To: <007201d0957e$7a93d130$6fbb7390$@netstream.ps>
References: <009b01d0940a$6aee99b0$40cbcd10$@netstream.ps>
 <555DD6F4.3060909@treenet.co.nz>
 <000a01d0943e$8aa70830$9ff51890$@netstream.ps>
 <555EA0FE.1090709@treenet.co.nz>
 <007201d0957e$7a93d130$6fbb7390$@netstream.ps>
Message-ID: <556069C0.5050609@treenet.co.nz>

On 24/05/2015 5:32 a.m., snakeeyes wrote:
> Amos any help ?
> 
> I spent many days without any luck !
> 
> clientNegotiateSSL: Error negotiating SSL connection on FD 25: error:1407609B:SSL routines:SSL23_GET_CLIENT_HELLO:https proxy request (1/-1)
> 
> 
> ????????

>From perlmonks:
"
OpenSSL complains when it sees the clear text CONNECT request, spitting
out this proxy error with DEBUG on:

error message: 'SSL_accept: 'error:1407609B:SSL
routines:SSL23_GET_CLIENT_HELLO:https proxy request

"

I'm not sure how more clear that can be.

A CONNECT message - valid only for plain-text HTTP messages to a
forward-proxy. Is being delivered to your reverse-proxy https_port which
should only be receiving port 443 native traffic.

Amos



From soporte at nodoalem.com.ar  Sat May 23 12:32:55 2015
From: soporte at nodoalem.com.ar (=?iso-8859-1?Q?Soporte_T=E9cnico?=)
Date: Sat, 23 May 2015 09:32:55 -0300
Subject: [squid-users] unsubscribe
Message-ID: <003d01d09554$98137d70$c83a7850$@nodoalem.com.ar>

 



---
El software de antivirus Avast ha analizado este correo electr?nico en busca de virus.
http://www.avast.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150523/26d1d75f/attachment.htm>

From ow97 at outlook.com  Sat May 23 23:03:48 2015
From: ow97 at outlook.com (Oliver Webb)
Date: Sun, 24 May 2015 00:03:48 +0100
Subject: [squid-users] Smart Delay Pools (I think)
Message-ID: <SNT147-W91353E2C21B6A36A0DD08ACACF0@phx.gbl>

I have a squid proxy acting as a parental filter on our LAN, however the traffic balancing is a problem. My question is: Is it possible to set up something that achieves this:

NB:?
? ?Where I have referred to "users" I am refering to devices as I do not have any login system.
? ?My internet speed here is assumed to be 10Mbps

1) When several users are browsing Wikipedia the burst downloads required, assuming the bursts occur 1 at a time, each burst gets the full internet bandwidth of ~10Mbps

2) When 2 or more users are downloading files the internet bandwidth gets divided equally i.e. each user gets 5Mbps when there are 2 concurrent large downloads and each user gets 3.33Mbps when there are 3 concurrent large downloads

3) When there are 2 concurrent large downloads occurring and third user wants to browse Wikipedia the burst downloads for Wikipedia momentarily change the bandwidth allocations so that while the Wikipedia page is download each user gets 3.33Mbps and once the page is downloaded the two large dowloads get back their 5Mbps each



Many thanks for your help it is greatly appreciated

Oliver 		 	   		  

From ahmed.zaeem at netstream.ps  Sun May 24 10:19:43 2015
From: ahmed.zaeem at netstream.ps (snakeeyes)
Date: Sun, 24 May 2015 03:19:43 -0700
Subject: [squid-users] squid Ldap problem
Message-ID: <009501d0960b$2766f590$7634e0b0$@netstream.ps>

Hi  I have squid 3.5 with  LDAP on liux server openldap

 

I have been trying but no luck 

I have ldapadmin as user and pwd as 123456

I have the domain abc.com

And have a testing user as user1/123456

 

Here is what im trying and error :

 

 

 

echo "user1" "123456" | /lib/squid/basic_ldap_auth -P -R -b "dc=abc,dc=com"
-D "cn=ldapadmin,dc=abc,dc=com" -w "123456" -f sAMAccountName=%s -h
192.168.100.1

 

 

basic_ldap_auth: WARNING, could not bind to binddn 'Invalid credentials'

ERR Invalid credentials

 

 

 

 

Any help ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150524/b676d4fe/attachment.htm>

From yan at seiner.com  Sun May 24 00:23:28 2015
From: yan at seiner.com (Yan Seiner)
Date: Sat, 23 May 2015 17:23:28 -0700
Subject: [squid-users] Smart Delay Pools (I think)
In-Reply-To: <SNT147-W91353E2C21B6A36A0DD08ACACF0@phx.gbl>
References: <SNT147-W91353E2C21B6A36A0DD08ACACF0@phx.gbl>
Message-ID: <55611A00.3090103@seiner.com>


On 05/23/2015 04:03 PM, Oliver Webb wrote:
> I have a squid proxy acting as a parental filter on our LAN, however the traffic balancing is a problem. My question is: Is it possible to set up something that achieves this:
>
> NB:
>     Where I have referred to "users" I am refering to devices as I do not have any login system.
>     My internet speed here is assumed to be 10Mbps
>
> 1) When several users are browsing Wikipedia the burst downloads required, assuming the bursts occur 1 at a time, each burst gets the full internet bandwidth of ~10Mbps
>
> 2) When 2 or more users are downloading files the internet bandwidth gets divided equally i.e. each user gets 5Mbps when there are 2 concurrent large downloads and each user gets 3.33Mbps when there are 3 concurrent large downloads
>
> 3) When there are 2 concurrent large downloads occurring and third user wants to browse Wikipedia the burst downloads for Wikipedia momentarily change the bandwidth allocations so that while the Wikipedia page is download each user gets 3.33Mbps and once the page is downloaded the two large dowloads get back their 5Mbps each
>
>
>
> Many thanks for your help it is greatly appreciated

I suspect you're looking for iptables+tc; I do something similar for my 
3 networks - auth, guest, and tenant.  But really discussion of iptables 
and tc is way off topic for this list.  You probably want to spend some 
time with the man pages and figure out what I'm doing.

bandwidth_down=10000
bandwidth_up=10000
auth_down=$(( $bandwidth_down / 2 ))
auth_up=$(( $bandwidth_up / 2 ))
tenant_down=$(( $bandwidth_down / 4 ))
tenant_up=$(( $bandwidth_up / 4 ))
guest_down=$(( $bandwidth_down / 8 ))
guest_up=$(( $bandwidth_up / 8 ))

# mark our packets
# we use the FORWARD chain so we have access to both inbound and 
outbound info for the packet
# we must restore the connection mark before NAT
# and set it when the packet is all the way through

iptables -t mangle -A PREROUTING -j CONNMARK --restore-mark
iptables -t mangle -A FORWARD -s $auth -o ${outside_if} -j MARK 
--set-mark 0x04
iptables -t mangle -A FORWARD -s $guest -o ${outside_if} -j MARK 
--set-mark 0x05
iptables -t mangle -A FORWARD -s $tenant -o ${outside_if} -j MARK 
--set-mark 0x06
iptables -t mangle -A POSTROUTING -j CONNMARK --save-mark

# HTB classes on interfaces with rate limiting
# we limit uploads on the common outside interface

tc qdisc add dev ${outside_if} root handle 1: htb default 30
tc class add dev ${outside_if} parent 1: classid 1:1 htb rate 
${bandwidth_up}kbit
tc class add dev ${outside_if} parent 1:1 classid 1:14 htb rate 
${auth_up}kbit ceil ${bandwidth_up}kbit
tc class add dev ${outside_if} parent 1:1 classid 1:15 htb rate 
${guest_up}kbit ceil ${bandwidth_up}kbit
tc class add dev ${outside_if} parent 1:1 classid 1:16 htb rate 
${tenant_up}kbit ceil ${bandwidth_up}kbit

tc filter add dev ${outside_if} parent 1:0 protocol ip handle 0x04 fw 
flowid 1:14
tc filter add dev ${outside_if} parent 1:0 protocol ip handle 0x05 fw 
flowid 1:15
tc filter add dev ${outside_if} parent 1:0 protocol ip handle 0x06 fw 
flowid 1:16

# for downloads we limit on common inside interface, the one with the vlans

tc qdisc add dev ${inside_if} root handle 1: htb default 30
tc class add dev ${inside_if} parent 1: classid 1:1 htb rate 
${bandwidth_down}kbit
tc class add dev ${inside_if} parent 1:1 classid 1:14 htb rate 
${auth_down}kbit ceil ${bandwidth_down}kbit
tc class add dev ${inside_if} parent 1:1 classid 1:15 htb rate 
${guest_down}kbit ceil ${bandwidth_down}kbit
tc class add dev ${inside_if} parent 1:1 classid 1:16 htb rate 
${tenant_down}kbit ceil ${bandwidth_down}kbit

tc filter add dev ${inside_if} parent 1:0 protocol ip handle 0x04 fw 
flowid 1:14
tc filter add dev ${inside_if} parent 1:0 protocol ip handle 0x05 fw 
flowid 1:15
tc filter add dev ${inside_if} parent 1:0 protocol ip handle 0x06 fw 
flowid 1:16

>
> Oliver 		 	   		
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From vinhthuy.999 at gmail.com  Sun May 24 11:50:46 2015
From: vinhthuy.999 at gmail.com (Vinh Thuy Le Hoang)
Date: Sun, 24 May 2015 18:50:46 +0700
Subject: [squid-users] sx
Message-ID: <CABGet1tzfegoB1sU8HAboaRdehYyO1i2fXHndg9gLLB9OKVBJA@mail.gmail.com>

scc
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150524/85b479a4/attachment.htm>

From squid3 at treenet.co.nz  Sun May 24 12:58:18 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 May 2015 00:58:18 +1200
Subject: [squid-users] Smart Delay Pools (I think)
In-Reply-To: <SNT147-W91353E2C21B6A36A0DD08ACACF0@phx.gbl>
References: <SNT147-W91353E2C21B6A36A0DD08ACACF0@phx.gbl>
Message-ID: <5561CAEA.909@treenet.co.nz>

On 24/05/2015 11:03 a.m., Oliver Webb wrote:
> I have a squid proxy acting as a parental filter on our LAN, however the traffic balancing is a problem. My question is: Is it possible to set up something that achieves this:
> 
> NB: 
>    Where I have referred to "users" I am refering to devices as I do not have any login system.
>    My internet speed here is assumed to be 10Mbps
> 
> 1) When several users are browsing Wikipedia the burst downloads required, assuming the bursts occur 1 at a time, each burst gets the full internet bandwidth of ~10Mbps
> 
> 2) When 2 or more users are downloading files the internet bandwidth gets divided equally i.e. each user gets 5Mbps when there are 2 concurrent large downloads and each user gets 3.33Mbps when there are 3 concurrent large downloads
> 
> 3) When there are 2 concurrent large downloads occurring and third user wants to browse Wikipedia the burst downloads for Wikipedia momentarily change the bandwidth allocations so that while the Wikipedia page is download each user gets 3.33Mbps and once the page is downloaded the two large dowloads get back their 5Mbps each
> 

The delay pools in Squid is not quite that smart / dynamic. Delay Pools
is a bandwidth cap with selected requests scheduled within each defined cap.

You can do both (1) and (2) with a single class 2 pool shared by the
entire LAN (1310720 Bps aggregate, 655360 Bbps individual), and a
delay_pools_access denying wikipedia domains being pooled by it.

But having (3) where transaction X changes pooling mid-way depending on
some unrelated transaction is simply not possible.

Amos



From jlay at slave-tothe-box.net  Sun May 24 16:25:42 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 24 May 2015 10:25:42 -0600
Subject: [squid-users] Ssl-bump deep dive (properly creating certs)
Message-ID: <1432484742.3702.17.camel@JamesiMac>

Hey all,

So....I'm sure those on the list have seen my posts a number of times,
usually all questions (sorry I'm not very helpful).  That being said,
whenever there is something I can't get to work right, or don't
understand as well as I think I should, I do kind of a deep dive into it
for about a month.  I'm going to do that now with Squid.  I have NEVER
gotten ssl-bump to work right.  I have it "sort of" working, but there
are some issues I want to address.

So I'm going to start from scratch in a lab environment using a VM as a
client, a physical machine with two nics that are bridged and run squid
as a transparent proxy, and a physical laptop as the server.

My first question is about properly creating the certs.  Looking at:

http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

this mentions using crtd, but as I understand it, crtd isn't supported
when using transparent proxies.  So, with no crtd, as I understand it
this is what I'll need:

Server:
Self-signed CA cert (pem) <- used as cafile= in https_port
Intermediate cert signed by the above self signed CA cert (pem) <- used
as cert= in https_port
Key file for the self-signed CA cert above (pem) <- used as key= in
https_port

Client:
Self-signed CA cert from above (pem) <- in /etc/ssl/certs for linux

Any help, advice, links that would assist in better understanding this
first step in ssl-bumping transparently would be wonderful.  Thank you.

James 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150524/77bdec77/attachment.htm>

From Jason_Haar at trimble.com  Sun May 24 20:48:22 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Mon, 25 May 2015 08:48:22 +1200
Subject: [squid-users] Ssl-bump deep dive (properly creating certs)
In-Reply-To: <1432484742.3702.17.camel@JamesiMac>
References: <1432484742.3702.17.camel@JamesiMac>
Message-ID: <55623916.4010105@trimble.com>

On 25/05/15 04:25, James Lay wrote:
> My first question is about properly creating the certs.  Looking at:
>
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> this mentions using crtd, but as I understand it, crtd isn't supported
> when using transparent proxies.  So, with no crtd, as I understand it
> this is what I'll need:
>

I don't know where you got that from, but that's not true. I think you
are confusing the issue that when squid is used as a transparent HTTPS
proxy, it lacks the "easy" hostname details that a formal (ie
non-transparent) proxy has. ie when a browser asks for a secure website
via a formal proxy, it sends

CONNECT github.com:443 HTTP/1.1

So squid knows *in advance* the server is called "github.com". So it
connects to github.com, downloads the public key and then uses crtd to
create a clone of it - identical except that it's signed by your
self-created Squid CA instead of Verisign/whatever

Compare that with transparent proxy mode, where all that squid knows is
that a browser has had it's outbound tcp port 443 traffic to
192.30.252.128 redirected onto it, so it doesn't know that is
github.com. If you are using squid-3.4 or less, that's all there is to
it - there's no way to figure out the cert name in a guaranteed fashion
(there are hacks, but my own experience is that they can only work up to
95% of the time - and break for some of the largest sites). With
squid-3.5 there is "peek" - which means squid can let the initial few
packets through (ie act like "splice") - which is enough to see the
client send the SNI request to the https server and get the reply. So
"peek" allows squid to learn about the true server name of the https
server. At that point *I think* squid creates a forged cert, then
creates a new connection to the server, then links together the existing
client tcp channel with the new proxy->server tcp channel and carries on
intercepting (I think that's the outcome - there would have to be some
extra smoke-n-mirrors in there to make that happen)

In pseudo-code, it looks like this

if http_port and "CONNECT (.*) HTTP" then sni_name=$1
else if https_port and "peek" then sni_name=find_sni($ipaddress)
else if https_port then sni_name=$ipaddress


When all is said and done, transparent HTTPS intercept is the very last
thing you should be working on. You need to gets squid working 100% as a
formal proxy - and only then start looking at making that work in
transparent mode. And you *definitely* want ssl_crtd.


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/f53dcbcb/attachment.htm>

From jlay at slave-tothe-box.net  Sun May 24 21:06:29 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 24 May 2015 15:06:29 -0600
Subject: [squid-users] Ssl-bump deep dive (properly creating certs)
In-Reply-To: <55623916.4010105@trimble.com>
References: <1432484742.3702.17.camel@JamesiMac> <55623916.4010105@trimble.com>
Message-ID: <1432501589.3706.1.camel@JamesiMac>

On Mon, 2015-05-25 at 08:48 +1200, Jason Haar wrote:
> On 25/05/15 04:25, James Lay wrote:
> 
> > 
> > My first question is about properly creating the certs.  Looking at:
> > 
> > http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> > 
> > this mentions using crtd, but as I understand it, crtd isn't
> > supported when using transparent proxies.  So, with no crtd, as I
> > understand it this is what I'll need:
> > 
> 
> 
> I don't know where you got that from, but that's not true. I think you
> are confusing the issue that when squid is used as a transparent HTTPS
> proxy, it lacks the "easy" hostname details that a formal (ie
> non-transparent) proxy has. ie when a browser asks for a secure
> website via a formal proxy, it sends
> 
> CONNECT github.com:443 HTTP/1.1
> 
> So squid knows *in advance* the server is called "github.com". So it
> connects to github.com, downloads the public key and then uses crtd to
> create a clone of it - identical except that it's signed by your
> self-created Squid CA instead of Verisign/whatever
> 
> Compare that with transparent proxy mode, where all that squid knows
> is that a browser has had it's outbound tcp port 443 traffic to
> 192.30.252.128 redirected onto it, so it doesn't know that is
> github.com. If you are using squid-3.4 or less, that's all there is to
> it - there's no way to figure out the cert name in a guaranteed
> fashion (there are hacks, but my own experience is that they can only
> work up to 95% of the time - and break for some of the largest sites).
> With squid-3.5 there is "peek" - which means squid can let the initial
> few packets through (ie act like "splice") - which is enough to see
> the client send the SNI request to the https server and get the reply.
> So "peek" allows squid to learn about the true server name of the
> https server. At that point *I think* squid creates a forged cert,
> then creates a new connection to the server, then links together the
> existing client tcp channel with the new proxy->server tcp channel and
> carries on intercepting (I think that's the outcome - there would have
> to be some extra smoke-n-mirrors in there to make that happen)
> 
> In pseudo-code, it looks like this
> 
> if http_port and "CONNECT (.*) HTTP" then sni_name=$1
> else if https_port and "peek" then sni_name=find_sni($ipaddress)
> else if https_port then sni_name=$ipaddress
> 
> 
> When all is said and done, transparent HTTPS intercept is the very
> last thing you should be working on. You need to gets squid working
> 100% as a formal proxy - and only then start looking at making that
> work in transparent mode. And you *definitely* want ssl_crtd. 
> 
> 
> 
> -- 
> Cheers
> 
> Jason Haar
> Corporate Information Security Manager, Trimble Navigation Ltd.
> Phone: +1 408 481 8171
> PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Thanks for the great response Jason...I appreciate it.  I think maybe I
misread this:

http://wiki.squid-cache.org/Features/DynamicSslCert

"While SslBump itself works fine in transparent redirection environments
(e.g. those using WCCP or iptables), dynamic certificate generation does
not: To generate the certificate dynamically, Squid must know the server
domain name. That information is not available at the time the HTTPS
client TCP connection is intercepted and bumped. Currently, you cannot
use dynamic certificate generation for transparent connections until
bump-server-first is supported."

Is this no longer accurate with now that peek/splice has been
implemented?

Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150524/30f22ebb/attachment.htm>

From reet.vyas28 at gmail.com  Mon May 25 06:47:57 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Mon, 25 May 2015 12:17:57 +0530
Subject: [squid-users] Squid cache youtube and other websites
Message-ID: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>

Hi

I want to use squid to cache youtube videos, ours is media agency and lots
of bandwidth issue we are facing , so I came with solution to cache youtube.

I want to know the few things as I am new to squid and networking .

I have tplink router and 8 broadband connc and two leased line connection
so I cant make squid as router so i want to setup squid in such a way i
want to use gateway my router IP only and want all request coming on port
80 to go through squid.

Is this possible?? I am just assuming it can be done done using iptables
but if squid server is router and I dont to use squid as router cause of so
many ISP lines.

Can you please suggest how to achieve this?

Please give some ideas to implement this
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/a0073d57/attachment.htm>

From dan at getbusi.com  Mon May 25 06:51:57 2015
From: dan at getbusi.com (dan at getbusi.com)
Date: Sun, 24 May 2015 23:51:57 -0700 (PDT)
Subject: [squid-users] Squid cache youtube and other websites
In-Reply-To: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>
References: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>
Message-ID: <1432536717288.79643e43@Nodemailer>

Firstly, I think the biggest roadblocks you?re going to hit with caching YouTube are:




1) It?s all encrypted now (thanks Google). Squid can?t cache what it can?t see inside an SSL tunnel.




2) They have a pretty intense CDN which you?ll need a StoreID helper to deal with.




There are people on this list that know way more about it than me though, so I?ll let them explain how they do it.

On Mon, May 25, 2015 at 4:48 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:

> Hi
> I want to use squid to cache youtube videos, ours is media agency and lots
> of bandwidth issue we are facing , so I came with solution to cache youtube.
> I want to know the few things as I am new to squid and networking .
> I have tplink router and 8 broadband connc and two leased line connection
> so I cant make squid as router so i want to setup squid in such a way i
> want to use gateway my router IP only and want all request coming on port
> 80 to go through squid.
> Is this possible?? I am just assuming it can be done done using iptables
> but if squid server is router and I dont to use squid as router cause of so
> many ISP lines.
> Can you please suggest how to achieve this?
> Please give some ideas to implement this
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150524/90b1f3fe/attachment.htm>

From reet.vyas28 at gmail.com  Mon May 25 07:13:43 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Mon, 25 May 2015 12:43:43 +0530
Subject: [squid-users] Squid cache youtube and other websites
In-Reply-To: <1432536717288.79643e43@Nodemailer>
References: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>
 <1432536717288.79643e43@Nodemailer>
Message-ID: <CAA8ViV_-2tQxKy=UQ1BWTwF9hk9TpF1mpO=aXWpm2OzrgL-K6g@mail.gmail.com>

Hi

Thanks Dan for info. I searched google about LUSCA and scripts available
but I don't think it is working now.



On Mon, May 25, 2015 at 12:21 PM, <dan at getbusi.com> wrote:

> Firstly, I think the biggest roadblocks you?re going to hit with caching
> YouTube are:
>
> 1) It?s all encrypted now (thanks Google). Squid can?t cache what it can?t
> see inside an SSL tunnel.
>
> 2) They have a pretty intense CDN which you?ll need a StoreID helper to
> deal with.
>
> There are people on this list that know way more about it than me though,
> so I?ll let them explain how they do it.
>
>
>
>
> On Mon, May 25, 2015 at 4:48 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:
>
>>    Hi
>>
>> I want to use squid to cache youtube videos, ours is media agency and
>> lots of bandwidth issue we are facing , so I came with solution to cache
>> youtube.
>>
>> I want to know the few things as I am new to squid and networking .
>>
>> I have tplink router and 8 broadband connc and two leased line connection
>> so I cant make squid as router so i want to setup squid in such a way i
>> want to use gateway my router IP only and want all request coming on port
>> 80 to go through squid.
>>
>> Is this possible?? I am just assuming it can be done done using iptables
>> but if squid server is router and I dont to use squid as router cause of so
>> many ISP lines.
>>
>> Can you please suggest how to achieve this?
>>
>> Please give some ideas to implement this
>>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/9c542b19/attachment.htm>

From jpotter833 at because.org.uk  Mon May 25 08:38:38 2015
From: jpotter833 at because.org.uk (Mr J Potter)
Date: Mon, 25 May 2015 09:38:38 +0100
Subject: [squid-users] Alternative ways of tracking users on unauthenticated
	proxy
Message-ID: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>

Hi all,

I'm setting up a system for using iPads in our school, and I'm stuck a bit
on tracking what the students are doing on them.

First up, I reaaly don't want a Pop-up login box from a 407 response from a
proxy server, so I'm looking for some other way to track who is doing what.

What i have set up so far is PacketFence with an SSL-bump transparent proxy
(I've put the CAs o all the ipads) which works well in that users have to
log in before they get internet access. This works (they get a web page,
login and get 50 minutes of internet before it disconnects them), but the
only way I have of tracking users is by working out who was on each ipad
(from packetfence) then matching it against squid logs, which is messy.

One plan I had would be to add/remove entries in dns or hosts for users,
eg  IP address 10.2.3.4   -> hostname  fbloggs  (the user's login code) so
usernames would show up in the client hostname field, but squid caches
these I think. Another option would be via iptables somehow.

Can anyone suggest any other possible workarounds for this?

thanks,

Jim Potter
Network Manager
Oasis Brislington (formerly Brislington Enterprise College)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/eadc26e1/attachment.htm>

From squid3 at treenet.co.nz  Mon May 25 10:15:34 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 May 2015 22:15:34 +1200
Subject: [squid-users] squid Ldap problem
In-Reply-To: <009501d0960b$2766f590$7634e0b0$@netstream.ps>
References: <009501d0960b$2766f590$7634e0b0$@netstream.ps>
Message-ID: <5562F646.8070300@treenet.co.nz>

On 24/05/2015 10:19 p.m., snakeeyes wrote:
> Hi  I have squid 3.5 with  LDAP on liux server openldap
>  
> 
> echo "user1" "123456" | /lib/squid/basic_ldap_auth -P -R -b "dc=abc,dc=com"
> -D "cn=ldapadmin,dc=abc,dc=com" -w "123456" -f sAMAccountName=%s -h
> 192.168.100.1
> 
> basic_ldap_auth: WARNING, could not bind to binddn 'Invalid credentials'

What that says is that the -D ldapadmin account could not login to LDAP
to do anything.

Amos



From james at ejbdigital.com.au  Mon May 25 10:33:21 2015
From: james at ejbdigital.com.au (James Harper)
Date: Mon, 25 May 2015 10:33:21 +0000
Subject: [squid-users] Alternative ways of tracking users on
 unauthenticated	proxy
In-Reply-To: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
References: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
Message-ID: <SG2PR04MB083903BFD767EF113175EABBE8CD0@SG2PR04MB0839.apcprd04.prod.outlook.com>

> 
> Hi all,
> 
> 
> I'm setting up a system for using iPads in our school, and I'm stuck a bit on
> tracking what the students are doing on them.
> 
> 
> First up, I reaaly don't want a Pop-up login box from a 407 response from a
> proxy server, so I'm looking for some other way to track who is doing what.
> 
> What i have set up so far is PacketFence with an SSL-bump transparent proxy
> (I've put the CAs o all the ipads) which works well in that users have to log in
> before they get internet access. This works (they get a web page, login and
> get 50 minutes of internet before it disconnects them), but the only way I
> have of tracking users is by working out who was on each ipad (from
> packetfence) then matching it against squid logs, which is messy.
> 

Does packetfence provide a way to query the user that belongs to an IP address?

If so, it might be possible to write a helper that squid can call to obtain the username. And if PacketFence is popular (I'd never heard of it, but that doesn't mean anything), then someone else might have already written one.

James

From squid3 at treenet.co.nz  Mon May 25 10:37:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 May 2015 22:37:00 +1200
Subject: [squid-users] Ssl-bump deep dive (properly creating certs)
In-Reply-To: <55623916.4010105@trimble.com>
References: <1432484742.3702.17.camel@JamesiMac> <55623916.4010105@trimble.com>
Message-ID: <5562FB4C.4040906@treenet.co.nz>

On 25/05/2015 8:48 a.m., Jason Haar wrote:
> On 25/05/15 04:25, James Lay wrote:
>> My first question is about properly creating the certs.  Looking at:
>>
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>
>> this mentions using crtd, but as I understand it, crtd isn't supported
>> when using transparent proxies.  So, with no crtd, as I understand it
>> this is what I'll need:
>>
> 
> I don't know where you got that from, but that's not true. I think you
> are confusing the issue that when squid is used as a transparent HTTPS
> proxy, it lacks the "easy" hostname details that a formal (ie
> non-transparent) proxy has. ie when a browser asks for a secure website
> via a formal proxy, it sends
> 
> CONNECT github.com:443 HTTP/1.1
> 
> So squid knows *in advance* the server is called "github.com". So it
> connects to github.com, downloads the public key and then uses crtd to
> create a clone of it - identical except that it's signed by your
> self-created Squid CA instead of Verisign/whatever
> 
> Compare that with transparent proxy mode, where all that squid knows is
> that a browser has had it's outbound tcp port 443 traffic to
> 192.30.252.128 redirected onto it, so it doesn't know that is
> github.com. If you are using squid-3.4 or less, that's all there is to
> it - there's no way to figure out the cert name in a guaranteed fashion
> (there are hacks, but my own experience is that they can only work up to
> 95% of the time - and break for some of the largest sites). With
> squid-3.5 there is "peek" - which means squid can let the initial few
> packets through (ie act like "splice")

I dont think that is right.

AFAIK, peek at step-1 lets the first few client packets buffer up inside
Squid instead of leaving them in the TCP buffers. That way it can
literally *peek* at the buffer contents to find the SNI without having
consumed them.


> - which is enough to see the
> client send the SNI request to the https server and get the reply.

That would be is step-2.

Squid can selectively drain the buffered clientHello details toward the
server (peek @ step-2). Or Squid can send its own ClientHello faked
details (stare @ step 1 or 2) and repeat the peek process for the
serverHello packets.


> So
> "peek" allows squid to learn about the true server name of the https
> server.

This word "true" keeps getting thrown about in regards to the server
name. Its just *a* name the client is aware of - one of many the server
has usually.


> At that point *I think* squid creates a forged cert, then
> creates a new connection to the server, then links together the existing
> client tcp channel with the new proxy->server tcp channel and carries on
> intercepting (I think that's the outcome - there would have to be some
> extra smoke-n-mirrors in there to make that happen)

AFAIK, the original server connection is still being used. This is
stage-3. Squid should only have sent enough details to either splice
(original clients details) or bump (Squid faked details).

> 
> In pseudo-code, it looks like this
> 
> if http_port and "CONNECT (.*) HTTP" then sni_name=$1
> else if https_port and "peek" then sni_name=find_sni($ipaddress)
> else if https_port then sni_name=$ipaddress
> 
> 
> When all is said and done, transparent HTTPS intercept is the very last
> thing you should be working on. You need to gets squid working 100% as a
> formal proxy - and only then start looking at making that work in
> transparent mode. And you *definitely* want ssl_crtd.
> 


Amos



From squid3 at treenet.co.nz  Mon May 25 11:07:44 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 May 2015 23:07:44 +1200
Subject: [squid-users] Alternative ways of tracking users on
 unauthenticated proxy
In-Reply-To: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
References: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
Message-ID: <55630280.6050907@treenet.co.nz>

On 25/05/2015 8:38 p.m., Mr J Potter wrote:
> Hi all,
> 
> I'm setting up a system for using iPads in our school, and I'm stuck a bit
> on tracking what the students are doing on them.
> 
> First up, I reaaly don't want a Pop-up login box from a 407 response from a
> proxy server, so I'm looking for some other way to track who is doing what.
> 
> What i have set up so far is PacketFence with an SSL-bump transparent proxy
> (I've put the CAs o all the ipads) which works well in that users have to
> log in before they get internet access. This works (they get a web page,
> login and get 50 minutes of internet before it disconnects them), but the
> only way I have of tracking users is by working out who was on each ipad
> (from packetfence) then matching it against squid logs, which is messy.

Squid comes bundled with a ext_sql_session_acl helper that looks up a
database and produces OK/ERR (and username for logging) depending on
whether the key given to it exists in the DB already.
<http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html>

You just need to get an UID metric. IP address, MAC address, and/or
EUI-64 (IPv6 link-local) are suitable there. It sounds like your
packetfence would be a good way to populate that DB too.

> 
> One plan I had would be to add/remove entries in dns or hosts for users,
> eg  IP address 10.2.3.4   -> hostname  fbloggs  (the user's login code) so
> usernames would show up in the client hostname field, but squid caches
> these I think.

Yes. Dont do that with DNS.

Amos



From yvoinov at gmail.com  Mon May 25 11:27:20 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 25 May 2015 17:27:20 +0600
Subject: [squid-users] Squid cache youtube and other websites
In-Reply-To: <1432536717288.79643e43@Nodemailer>
References: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>
 <1432536717288.79643e43@Nodemailer>
Message-ID: <55630718.1060309@gmail.com>

Look, Ma. ;) I'm a LumberJack :))))))

http://i.imgur.com/NGn6Ao4.png
http://i.imgur.com/Uz0zXut.png

Note, that Youtube now uses QUIC protocol (especially in Chrome), which 
cannot be processed by Squid ever.

To cache Youtube, you must solve two tasks:
1. Completely force clients use HTTP/HTTPS for YT.

http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol

2. Configure and tune _correct_ SSL Bump.
3. Configure and refine Store ID feature.

All of this above is know-how partially or completely. ;)

WBR, Yuri

25.05.15 12:51, dan at getbusi.com ?????:
> Firstly, I think the biggest roadblocks you?re going to hit with 
> caching YouTube are:
>
> 1) It?s all encrypted now (thanks Google). Squid can?t cache what it 
> can?t see inside an SSL tunnel.
>
> 2) They have a pretty intense CDN which you?ll need a StoreID helper 
> to deal with.
>
> There are people on this list that know way more about it than me 
> though, so I?ll let them explain how they do it.
>
>
>
>
> On Mon, May 25, 2015 at 4:48 PM, Reet Vyas <reet.vyas28 at gmail.com 
> <mailto:reet.vyas28 at gmail.com>> wrote:
>
>     Hi
>
>     I want to use squid to cache youtube videos, ours is media agency
>     and lots of bandwidth issue we are facing , so I came with
>     solution to cache youtube.
>
>     I want to know the few things as I am new to squid and networking .
>
>     I have tplink router and 8 broadband connc and two leased line
>     connection so I cant make squid as router so i want to setup squid
>     in such a way i want to use gateway my router IP only and want all
>     request coming on port 80 to go through squid.
>
>     Is this possible?? I am just assuming it can be done done using
>     iptables but if squid server is router and I dont to use squid as
>     router cause of so many ISP lines.
>
>     Can you please suggest how to achieve this?
>
>     Please give some ideas to implement this
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/98e5022a/attachment.htm>

From matias at ufscar.br  Mon May 25 11:30:14 2015
From: matias at ufscar.br (Paulo Matias)
Date: Mon, 25 May 2015 08:30:14 -0300
Subject: [squid-users] [PATCH] SSL: Add suport for EECDH and disable
	client-initiated renegotiation
Message-ID: <556307C6.3070300@ufscar.br>

Hi,

Sorry for getting this sent to squid-users instead of the adequate
mailing list for patches (squid-dev). We have tried to send the
patch to squid-dev without a subscription (as recommended in
http://www.squid-cache.org/Support/mailing-lists.html#squid-dev),
but perhaps the message did not get to the list administrator.

This patch implements two different changes in SSL support:

* Adds support for Ephemeral Elliptic Curve Diffie-Hellman (EECDH)
  key exchange, which allows for forward secrecy with better
  performance than traditional ephemeral DH.

* Disables client-initiated renegotiation, mitigating a DoS attack
  which might be possible with some versions of the OpenSSL library.
  We have been warned about this when testing our service with
  the Qualys SSL Test (https://www.ssllabs.com/ssltest) back when
  it was running in a Debian wheezy system.
  Further information is available at:
  https://community.qualys.com/blogs/securitylabs/2011/10/31/tls-renegotiation-and-denial-of-service-attacks
  Our solution is similar to the one adopted in pureftpd:
  https://github.com/jedisct1/pure-ftpd/blob/549e94aaa093a48622efd6d91fdfb3a4236c13f4/src/tls.c#L106

We have been running Squid 3.4.8 in production with a backported
patch for two months. So far, we have no complaints from the users
(60 to 100 users per day) and the server is running stably.

We would be very happy if we could get this integrated into
Squid's source code, so please provide us feedback with any
suggestions.

Our https_port directive also follows, to serve as an example
configuration:

https_port 3443
  cert=/etc/ssl/private/proxy.crt
  key=/etc/ssl/private/proxy.key
  options=NO_SSLv3,
          No_Compression,
          SINGLE_DH_USE,
          SINGLE_ECDH_USE,
          CIPHER_SERVER_PREFERENCE
  dhparams=/etc/ssl/dhparam/dh2048.pem
  eecdhcurve=prime256v1
  cipher=ECDHE-ECDSA-CHACHA20-POLY1305:
         ECDHE-RSA-CHACHA20-POLY1305:
         DHE-RSA-CHACHA20-POLY1305:
         EECDH+ECDSA+AESGCM:
         EECDH+aRSA+AESGCM:
         EDH+aRSA+AESGCM:
         EECDH+ECDSA+SHA384:
         EECDH+aRSA+SHA384:
         EDH+aRSA+SHA384:
         EECDH+ECDSA+SHA256:
         EECDH+aRSA+SHA256:
         EDH+aRSA+SHA256:
         EECDH:EDH+aRSA:
         !RC4:!aNULL:!eNULL:!LOW:!3DES:
         !MD5:!EXP:!PSK:!SRP:!DSS

Best regards,
Paulo Matias


-------------- next part --------------
=== modified file 'doc/release-notes/release-4.sgml'
--- doc/release-notes/release-4.sgml	2015-03-28 11:12:46 +0000
+++ doc/release-notes/release-4.sgml	2015-05-20 12:28:18 +0000
@@ -133,6 +133,8 @@
 	<tag>https_port</tag>
 	<p>All <em>version=</em> <em>option=</em> values for SSLv2
 	   configuration or disabling have been removed.
+	<p>New parameter <em>eecdhcurve</em> for specifying an
+	   elliptic curve for ephemeral ECDH key exchange.
 	<p>Manual squid.conf update may be required on upgrade.
 
 	<tag>sslcrtd_children</tag>

=== modified file 'src/anyp/PortCfg.cc'
--- src/anyp/PortCfg.cc	2015-01-14 17:10:20 +0000
+++ src/anyp/PortCfg.cc	2015-05-19 14:46:04 +0000
@@ -53,6 +53,7 @@
     capath(NULL),
     crlfile(NULL),
     dhfile(NULL),
+    eecdhcurve(NULL),
     sslflags(NULL),
     sslContextSessionId(NULL),
     generateHostCertificates(false),
@@ -94,6 +95,7 @@
     safe_free(capath);
     safe_free(crlfile);
     safe_free(dhfile);
+    safe_free(eecdhcurve);
     safe_free(sslflags);
     safe_free(sslContextSessionId);
 #endif
@@ -139,6 +141,8 @@
         b->crlfile = xstrdup(crlfile);
     if (dhfile)
         b->dhfile = xstrdup(dhfile);
+    if (eecdhcurve)
+        b->eecdhcurve = xstrdup(eecdhcurve);
     if (sslflags)
         b->sslflags = xstrdup(sslflags);
     if (sslContextSessionId)

=== modified file 'src/anyp/PortCfg.h'
--- src/anyp/PortCfg.h	2015-01-13 07:25:36 +0000
+++ src/anyp/PortCfg.h	2015-05-19 14:46:04 +0000
@@ -78,6 +78,7 @@
     char *capath;
     char *crlfile;
     char *dhfile;
+    char *eecdhcurve;
     char *sslflags;
     char *sslContextSessionId; ///< "session id context" for staticSslContext
     bool generateHostCertificates; ///< dynamically make host cert for sslBump

=== modified file 'src/cache_cf.cc'
--- src/cache_cf.cc	2015-05-15 12:50:09 +0000
+++ src/cache_cf.cc	2015-05-19 14:46:04 +0000
@@ -3606,6 +3606,9 @@
     } else if (strncmp(token, "dhparams=", 9) == 0) {
         safe_free(s->dhfile);
         s->dhfile = xstrdup(token + 9);
+    } else if (strncmp(token, "eecdhcurve=", 11) == 0) {
+        safe_free(s->eecdhcurve);
+        s->eecdhcurve = xstrdup(token + 11);
     } else if (strncmp(token, "sslflags=", 9) == 0) {
         safe_free(s->sslflags);
         s->sslflags = xstrdup(token + 9);
@@ -3829,6 +3832,9 @@
     if (s->dhfile)
         storeAppendPrintf(e, " dhparams=%s", s->dhfile);
 
+    if (s->eecdhcurve)
+        storeAppendPrintf(e, " eecdhcurve=%s", s->eecdhcurve);
+
     if (s->sslflags)
         storeAppendPrintf(e, " sslflags=%s", s->sslflags);
 

=== modified file 'src/cf.data.pre'
--- src/cf.data.pre	2015-05-15 12:50:09 +0000
+++ src/cf.data.pre	2015-05-20 12:23:53 +0000
@@ -2122,6 +2122,11 @@
 				      Always create a new key when using
 				      temporary/ephemeral DH key exchanges
 
+			    SINGLE_ECDH_USE
+				      Enable ephemeral ECDH key exchange.
+				      The adopted curve should be specified
+				      using the eecdhcurve option.
+
 			    SSL_OP_NO_TICKET
 				      Disable use of RFC5077 session tickets.
 				      Some servers may have problems
@@ -2153,6 +2158,9 @@
 	   dhparams=	File containing DH parameters for temporary/ephemeral
 			DH key exchanges.
 
+	   eecdhcurve=	Elliptic Curve used for ephemeral ECDH. Supported curves
+			can be listed using "openssl ecparam -list_curves".
+
 	   sslflags=	Various flags modifying the use of SSL:
 			    DELAYED_AUTH
 				Don't request client certificates

=== modified file 'src/ssl/support.cc'
--- src/ssl/support.cc	2015-05-08 11:18:30 +0000
+++ src/ssl/support.cc	2015-05-19 14:50:44 +0000
@@ -472,6 +472,11 @@
         "NO_TICKET", SSL_OP_NO_TICKET
     },
 #endif
+#if SSL_OP_SINGLE_ECDH_USE
+    {
+        "SINGLE_ECDH_USE", SSL_OP_SINGLE_ECDH_USE
+    },
+#endif
     {
         "", 0
     },
@@ -824,11 +829,46 @@
 }
 
 static bool
+configureSslEECDH(SSL_CTX *sslContext, const char *curve)
+{
+    bool ok = true;
+#if OPENSSL_VERSION_NUMBER >= 0x0090800fL
+#ifndef OPENSSL_NO_ECDH
+    int nid = OBJ_sn2nid(curve);
+    if (!nid)
+        return false;
+
+    EC_KEY *ecdh = EC_KEY_new_by_curve_name(nid);
+    if (ecdh == NULL)
+        return false;
+
+    ok = SSL_CTX_set_tmp_ecdh(sslContext, ecdh) != 0;
+    EC_KEY_free(ecdh);
+#endif
+#endif
+    return ok;
+}
+
+static void
+ssl_info_cb(const SSL *ssl, int where, int ret)
+{
+    (void)ret;
+#ifdef SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS
+    if ((where & SSL_CB_HANDSHAKE_DONE) != 0) {
+        // disable renegotiation (CVE-2009-3555)
+        ssl->s3->flags |= SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS;
+    }
+#endif
+}
+
+static bool
 configureSslContext(SSL_CTX *sslContext, AnyP::PortCfg &port)
 {
     int ssl_error;
     SSL_CTX_set_options(sslContext, port.sslOptions);
 
+    SSL_CTX_set_info_callback(sslContext, ssl_info_cb);
+
     if (port.sslContextSessionId)
         SSL_CTX_set_session_id_context(sslContext, (const unsigned char *)port.sslContextSessionId, strlen(port.sslContextSessionId));
 
@@ -855,6 +895,15 @@
     debugs(83, 9, "Setting RSA key generation callback.");
     SSL_CTX_set_tmp_rsa_callback(sslContext, ssl_temp_rsa_cb);
 
+    if (port.eecdhcurve) {
+        debugs(83, 9, "Setting Ephemeral ECDH curve to " << port.eecdhcurve << ".");
+
+        if (!configureSslEECDH(sslContext, port.eecdhcurve)) {
+            ssl_error = ERR_get_error();
+            debugs(83, DBG_IMPORTANT, "WARNING: Unable to configure Ephemeral ECDH: " << ERR_error_string(ssl_error, NULL));
+        }
+    }
+
     debugs(83, 9, "Setting CA certificate locations.");
 
     const char *cafile = port.cafile ? port.cafile : port.clientca;
@@ -1155,6 +1204,8 @@
 
     SSL_CTX_set_options(sslContext, Ssl::parse_options(options));
 
+    SSL_CTX_set_info_callback(sslContext, ssl_info_cb);
+
     if (*cipher) {
         debugs(83, 5, "Using chiper suite " << cipher << ".");
 





From squid3 at treenet.co.nz  Mon May 25 13:46:00 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 May 2015 01:46:00 +1200
Subject: [squid-users] [PATCH] SSL: Add suport for EECDH and disable
 client-initiated renegotiation
In-Reply-To: <556307C6.3070300@ufscar.br>
References: <556307C6.3070300@ufscar.br>
Message-ID: <55632798.1090906@treenet.co.nz>

On 25/05/2015 11:30 p.m., Paulo Matias wrote:
> Hi,
> 
> Sorry for getting this sent to squid-users instead of the adequate
> mailing list for patches (squid-dev). We have tried to send the
> patch to squid-dev without a subscription (as recommended in
> http://www.squid-cache.org/Support/mailing-lists.html#squid-dev),
> but perhaps the message did not get to the list administrator.
> 

Could you subscribe then please and post it (or the updated version
after below). This has effects that I'd like our SSL devs to double check.


For my part on the audit:


* please separate into two patches - one for the renegotiation changes,
one for the EECDH.


* please avoid #ifdef and #ifndef in new code.
 - use #if defined() style instead.


Renegotiation:

* please wrap the entire ssl_info_cb() definition in the #if
conditionals and the appropriate calling lines.
 I know its a bit messy, but increasingly the library builds are lacking
renegtiation support entirely so this means smaller/faster builds.


EECDH:

FYI: with the deprecation of SSLv3 I'm working now towards a cleanup of
the SSL options with removals where possible.


* the DH parameters I think would be better added as a new option
"tls-dh=curve:/path/to/params" where the 'curve' part is optional and
implies EC when present - non-EC when absent.


* SINGLE_ECDH_USE needs to be documented in release-4.sgml
 "New <em>options=SINGLE_ECDH_USE</em> parameter to ..."


* The ECDH changes affect both https_port and http_port. They need
separate listings for each under changed directives, duplicate text on
the line items is fine.


* please implement (duplicate) all this UI parse change using the
Security::PeerOptions object (src/security/PeerOptions.*)
 - the src/ssl/* code for UI parsing and config storage is 'legacy' only
use by http(s)_port directives.
 - this may require some small changes suitable for use on client contexts
 - UI options added to Security::PeerOptions get documented in
release-4.sgml as changes for both cache_peer and tls_outgoing_options.
 - also in cf.data.pre for those directives


* configureSslEECDH() return true in the event that the chosen
configuration options are not even available.
 - please make an #else condition that displays an ERROR message at
level DBG_CRITICAL about the option(s) not being available, then return
false.
 - variable 'ok' can then become const (define on assignment) and move
fully inside the #if case.


Amos



From jlay at slave-tothe-box.net  Mon May 25 16:26:28 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 25 May 2015 10:26:28 -0600
Subject: [squid-users] Ssl-bump deep dive (self-signed certs in chain)
Message-ID: <1432571188.3754.6.camel@JamesiMac>

So following advice and instructions on this page:

http://wiki.squid-cache.org/Features/DynamicSslCert

I have set up my lab with explicit proxy by exporting http_proxy and
https_proxy.  After creating the self-signed root CA certificate above
and creating the .der file for the client, here are my results:

>From the squid side:
2015/05/25 10:02:20.161| Using certificate
in /opt/etc/squid/certs/SquidCA.pem
2015/05/25 10:02:20.170| support.cc(1743) readSslX509CertificatesChain:
Certificate is self-signed, will not be chained
I get the below when I don't specify a CA with curl, otherwise when I do
I get no error:
2015/05/25 09:21:02.229| Error negotiating SSL connection on FD 12:
error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)

And from the client side:
root at kali:~/test# curl -v https://mail.slave-tothe-box.net
* About to connect() to proxy 192.168.1.9 port 3129 (#0)
*   Trying 192.168.1.9...
* connected
* Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
* Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
> Host: mail.slave-tothe-box.net:443
> User-Agent: curl/7.26.0
> Proxy-Connection: Keep-Alive
> 
* Easy mode waiting response from proxy CONNECT
< HTTP/1.1 200 Connection established
< 
* Proxy replied OK to CONNECT request
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
* SSLv3, TLS handshake, Server hello (2):
* SSLv3, TLS handshake, CERT (11):
* SSLv3, TLS alert, Server hello (2):
* SSL certificate problem: self signed certificate in certificate chain
* Closing connection #0

And testing with specifying the .der file:
root at kali:~/test# curl --cacert /etc/ssl/certs/SquidCA.der -v
https://mail.slave-tothe-box.net
* About to connect() to proxy 192.168.1.9 port 3129 (#0)
*   Trying 192.168.1.9...
* connected
* Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
* Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
> Host: mail.slave-tothe-box.net:443
> User-Agent: curl/7.26.0
> Proxy-Connection: Keep-Alive
> 
* Easy mode waiting response from proxy CONNECT
< HTTP/1.1 200 Connection established
< 
* Proxy replied OK to CONNECT request
* error setting certificate verify locations:
  CAfile: /etc/ssl/certs/SquidCA.der
  CApath: /etc/ssl/certs

* Closing connection #0
curl: (77) error setting certificate verify locations:
  CAfile: /etc/ssl/certs/SquidCA.der
  CApath: /etc/ssl/certs


I can confirm that the server is using a bona-fide certificate issued
from StartSSL and works, so at this point I'm open to suggestions.
Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/926713f8/attachment.htm>

From matias at ufscar.br  Mon May 25 16:45:18 2015
From: matias at ufscar.br (Paulo Matias)
Date: Mon, 25 May 2015 13:45:18 -0300
Subject: [squid-users] [PATCH] SSL: Add suport for EECDH and disable
 client-initiated renegotiation
In-Reply-To: <55632798.1090906@treenet.co.nz>
References: <556307C6.3070300@ufscar.br> <55632798.1090906@treenet.co.nz>
Message-ID: <5563519E.8030000@ufscar.br>

Hi Amos,

On 25-05-2015 10:46, Amos Jeffries wrote:
> Could you subscribe then please and post it (or the updated version
> after below). This has effects that I'd like our SSL devs to double check.

Thank you for your thorough review. I will prepare the updated version
and post to the squid-dev mailing list as soon as it is ready.


Best regards,
Paulo Matias


From yvoinov at gmail.com  Mon May 25 16:51:14 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 25 May 2015 22:51:14 +0600
Subject: [squid-users] Ssl-bump deep dive (self-signed certs in chain)
In-Reply-To: <1432571188.3754.6.camel@JamesiMac>
References: <1432571188.3754.6.camel@JamesiMac>
Message-ID: <55635302.1070800@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Hm. Interesting.

You want to say, you uses ordinal server certificate, signed with
external trusted CA?

And users can't see MiTM?

25.05.15 22:26, James Lay ?????:
> So following advice and instructions on this page:
>
> http://wiki.squid-cache.org/Features/DynamicSslCert
>
> I have set up my lab with explicit proxy by exporting http_proxy and
> https_proxy.  After creating the self-signed root CA certificate above
> and creating the .der file for the client, here are my results:
>
> From the squid side:
> 2015/05/25 10:02:20.161| Using certificate
> in /opt/etc/squid/certs/SquidCA.pem
> 2015/05/25 10:02:20.170| support.cc(1743) readSslX509CertificatesChain:
> Certificate is self-signed, will not be chained
> I get the below when I don't specify a CA with curl, otherwise when I do
> I get no error:
> 2015/05/25 09:21:02.229| Error negotiating SSL connection on FD 12:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
>
> And from the client side:
> root at kali:~/test# curl -v https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * successfully set certificate verify locations:
> *   CAfile: none
>   CApath: /etc/ssl/certs
> * SSLv3, TLS handshake, Client hello (1):
> * SSLv3, TLS handshake, Server hello (2):
> * SSLv3, TLS handshake, CERT (11):
> * SSLv3, TLS alert, Server hello (2):
> * SSL certificate problem: self signed certificate in certificate chain
> * Closing connection #0
>
> And testing with specifying the .der file:
> root at kali:~/test# curl --cacert /etc/ssl/certs/SquidCA.der -v
> https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
>
> * Closing connection #0
> curl: (77) error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
>
>
> I can confirm that the server is using a bona-fide certificate issued
> from StartSSL and works, so at this point I'm open to suggestions.
> Thank you.
>
> James
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVY1MBAAoJENNXIZxhPexGlcYH/2T/L153ynVqn3s9epC7Pwvv
FxjHoamGMum6XJFooUZvQA0kaRzqhQSHduU0i6n4zWEowA4HgLkWrVeRrV/jXhxT
CbcZ+KYrO+UAMxrB04r+b4WQl6OZFcoj0ne+WecsJqgH108GGyrA+at6ibvFVNLl
ruiDntnH7fGuFV/o0J/hQfcxuHNDS7uND4iji7rSih2hIIET1ohG7EkppIaKwUAq
DHA9PtNTmF27eCZuNFXVXxbAjXsRy9NYGC+rwzmFT0Sw2A8KCKl/XBBylu+IRJqv
0TscKQeb/LH9/Jkuh5v2KMLjGaoo7hyqY8q/sjnZVySYy2wKXuXolMbYb+vyla4=
=XVIS
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/094de7b4/attachment.htm>

From yvoinov at gmail.com  Mon May 25 16:55:10 2015
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 25 May 2015 22:55:10 +0600
Subject: [squid-users] Ssl-bump deep dive (self-signed certs in chain)
In-Reply-To: <1432571188.3754.6.camel@JamesiMac>
References: <1432571188.3754.6.camel@JamesiMac>
Message-ID: <556353EE.10101@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
 
Ah, misunderstand.

Error you got means that target server certificate's CA is not visible
by Squid. Or for client.

Huh. :) I had thought that Squid suddenly turned into a hackware
:)))))))))))

25.05.15 22:26, James Lay ?????:
> So following advice and instructions on this page:
>
> http://wiki.squid-cache.org/Features/DynamicSslCert
>
> I have set up my lab with explicit proxy by exporting http_proxy and
> https_proxy.  After creating the self-signed root CA certificate above
> and creating the .der file for the client, here are my results:
>
> From the squid side:
> 2015/05/25 10:02:20.161| Using certificate
> in /opt/etc/squid/certs/SquidCA.pem
> 2015/05/25 10:02:20.170| support.cc(1743) readSslX509CertificatesChain:
> Certificate is self-signed, will not be chained
> I get the below when I don't specify a CA with curl, otherwise when I do
> I get no error:
> 2015/05/25 09:21:02.229| Error negotiating SSL connection on FD 12:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
>
> And from the client side:
> root at kali:~/test# curl -v https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * successfully set certificate verify locations:
> *   CAfile: none
>   CApath: /etc/ssl/certs
> * SSLv3, TLS handshake, Client hello (1):
> * SSLv3, TLS handshake, Server hello (2):
> * SSLv3, TLS handshake, CERT (11):
> * SSLv3, TLS alert, Server hello (2):
> * SSL certificate problem: self signed certificate in certificate chain
> * Closing connection #0
>
> And testing with specifying the .der file:
> root at kali:~/test# curl --cacert /etc/ssl/certs/SquidCA.der -v
> https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> <
> * Proxy replied OK to CONNECT request
> * error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
>
> * Closing connection #0
> curl: (77) error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
>
>
> I can confirm that the server is using a bona-fide certificate issued
> from StartSSL and works, so at this point I'm open to suggestions.
> Thank you.
>
> James
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQEcBAEBCAAGBQJVY1PuAAoJENNXIZxhPexG9WoH/09V9BB8VzXFGiJK/Sa3q29x
NdsaVmgS0SvytG+0aiVowJ4c6qf+IiEuqJiS6ymcBphPdVuvnY4pNcjpNA1Ke0AR
Kvm1KWswvSXyZvrVC4zo4Vsqd1pKFY9XBcy8N/S7l61DSsrPQfChXL0w5E2DPJ7I
fM9PvzDglshT7o1fNnfKObVsvo/CtNXJ8tc/pS78uZTeECW55QjhY55IAaQAUI2V
/uAyxxE7H73+qAlxlGHDVRzIcEN8wx/bqhVcMPNOoDy47PvN0W7XtW8EgPcOO6ej
lwDsmPrW8GhLhSWHe003aqQV0BJ8cSSjrL0HooQEyD5iTUfZUQLBKkE+0+XPZRE=
=Zb+F
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150525/478bdbf9/attachment.htm>

From yessarath at gmail.com  Mon May 25 19:20:16 2015
From: yessarath at gmail.com (S Sarath kumar)
Date: Tue, 26 May 2015 00:50:16 +0530
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CAP=gsu0qUFem7xt-nmfrMeKA--nX=iAyabJoZ7p2PXDmykp4Uw@mail.gmail.com>

hi i have been using squid3 on ubuntu 14.04.
i want to block the stream content in my lan.
hence i written a acl like below.
this acl at the top.
but still it's not blocking. anybody help me ?


acl Streaming rep_mime_type video/x-flv

http_reply_access deny mynetwork  Streaming

Regards,
Sarath kumar S


From Antony.Stone at squid.open.source.it  Mon May 25 19:33:06 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 25 May 2015 21:33:06 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <CAP=gsu0qUFem7xt-nmfrMeKA--nX=iAyabJoZ7p2PXDmykp4Uw@mail.gmail.com>
References: <CAP=gsu0qUFem7xt-nmfrMeKA--nX=iAyabJoZ7p2PXDmykp4Uw@mail.gmail.com>
Message-ID: <201505252133.07077.Antony.Stone@squid.open.source.it>

On Monday 25 May 2015 at 21:20:16 (EU time), S Sarath kumar wrote:

> hi i have been using squid3 on ubuntu 14.04.
> i want to block the stream content in my lan.
> hence i written a acl like below.
> this acl at the top.

Please post the entire squid.conf (excluding blank lines / comments).

That gives us a much better chance of answering your question.


Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Mon May 25 20:08:24 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 25 May 2015 22:08:24 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <CAP=gsu3qG77Bpy68W9C4TuwAkTxLSV83Ze_Q3Nry75PGT+Ozow@mail.gmail.com>
References: <CAP=gsu0qUFem7xt-nmfrMeKA--nX=iAyabJoZ7p2PXDmykp4Uw@mail.gmail.com>
 <201505252133.07077.Antony.Stone@squid.open.source.it>
 <CAP=gsu3qG77Bpy68W9C4TuwAkTxLSV83Ze_Q3Nry75PGT+Ozow@mail.gmail.com>
Message-ID: <201505252208.24513.Antony.Stone@squid.open.source.it>

On Monday 25 May 2015 at 21:50:12 (EU time), S Sarath kumar wrote:

> Hi,
> 
> below mentioned rules only applied
> 
> acl Streaming rep_mime_type video/flv video/x-flv
> acl mynetwork src 10.108.20.0/24
> 
> http_reply_access deny mynetwork  Streaming
> http_access allow mynetwork

1. Please reply to the list, not privately.

2. Are you saying that the above is your *entire* squid.conf?

If yes, you have more problems with it than you might realise.

If no, please do post the entire squid.conf, excluding blank lines and 
comments, and obscuring private information if appropriate (but be clear if 
you do this).


Regards,


Antony.

-- 
The lottery is a tax for people who can't do maths.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From reet.vyas28 at gmail.com  Tue May 26 06:12:48 2015
From: reet.vyas28 at gmail.com (Reet Vyas)
Date: Tue, 26 May 2015 11:42:48 +0530
Subject: [squid-users] Squid cache youtube and other websites
In-Reply-To: <55630718.1060309@gmail.com>
References: <CAA8ViV9PiPbeAMn-eE0em+SoKmkbx1DsJu0fByEZE08ur7NYXA@mail.gmail.com>
 <1432536717288.79643e43@Nodemailer> <55630718.1060309@gmail.com>
Message-ID: <CAA8ViV--MmkmOQSHYpNYdLvPutYyCG45JA3gcLfjg3zUoP6E8A@mail.gmail.com>

Hi Yuri,

Thanks for nice info. As I mentioned I have only tplink TL-R470T router and
machine with configuration of

Cent OS 6
HDD 1 TB
RAM 32 GB

So Is this possible with above router or do I have to change my router for
same. I can do this using IPtables only


On Mon, May 25, 2015 at 4:57 PM, Yuri Voinov <yvoinov at gmail.com> wrote:

>  Look, Ma. ;) I'm a LumberJack :))))))
>
> http://i.imgur.com/NGn6Ao4.png
> http://i.imgur.com/Uz0zXut.png
>
> Note, that Youtube now uses QUIC protocol (especially in Chrome), which
> cannot be processed by Squid ever.
>
> To cache Youtube, you must solve two tasks:
> 1. Completely force clients use HTTP/HTTPS for YT.
>
> http://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol
>
> 2. Configure and tune _correct_ SSL Bump.
> 3. Configure and refine Store ID feature.
>
> All of this above is know-how partially or completely. ;)
>
> WBR, Yuri
>
> 25.05.15 12:51, dan at getbusi.com ?????:
>
>  Firstly, I think the biggest roadblocks you?re going to hit with caching
> YouTube are:
>
>  1) It?s all encrypted now (thanks Google). Squid can?t cache what it
> can?t see inside an SSL tunnel.
>
>  2) They have a pretty intense CDN which you?ll need a StoreID helper to
> deal with.
>
>  There are people on this list that know way more about it than me
> though, so I?ll let them explain how they do it.
>
>
>
>
>  On Mon, May 25, 2015 at 4:48 PM, Reet Vyas <reet.vyas28 at gmail.com> wrote:
>
>>     Hi
>>
>>  I want to use squid to cache youtube videos, ours is media agency and
>> lots of bandwidth issue we are facing , so I came with solution to cache
>> youtube.
>>
>>  I want to know the few things as I am new to squid and networking .
>>
>>  I have tplink router and 8 broadband connc and two leased line
>> connection so I cant make squid as router so i want to setup squid in such
>> a way i want to use gateway my router IP only and want all request coming
>> on port 80 to go through squid.
>>
>>  Is this possible?? I am just assuming it can be done done using iptables
>> but if squid server is router and I dont to use squid as router cause of so
>> many ISP lines.
>>
>>  Can you please suggest how to achieve this?
>>
>>  Please give some ideas to implement this
>>
>
>
>
> _______________________________________________
> squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150526/5392bf8a/attachment.htm>

From squid3 at treenet.co.nz  Tue May 26 07:46:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 May 2015 19:46:09 +1200
Subject: [squid-users] Ssl-bump deep dive (self-signed certs in chain)
In-Reply-To: <1432571188.3754.6.camel@JamesiMac>
References: <1432571188.3754.6.camel@JamesiMac>
Message-ID: <556424C1.5000200@treenet.co.nz>

On 26/05/2015 4:26 a.m., James Lay wrote:
> So following advice and instructions on this page:
> 
> http://wiki.squid-cache.org/Features/DynamicSslCert
> 
> I have set up my lab with explicit proxy by exporting http_proxy and
> https_proxy.  After creating the self-signed root CA certificate above
> and creating the .der file for the client, here are my results:
> 
> From the squid side:
> 2015/05/25 10:02:20.161| Using certificate
> in /opt/etc/squid/certs/SquidCA.pem
> 2015/05/25 10:02:20.170| support.cc(1743) readSslX509CertificatesChain:
> Certificate is self-signed, will not be chained
> I get the below when I don't specify a CA with curl, otherwise when I do
> I get no error:
> 2015/05/25 09:21:02.229| Error negotiating SSL connection on FD 12:
> error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)

If that error is displayed by Squid about the clients connection. Then I
believe it means the client is attempting to perform TLS authentication
to Squid using the CA you installed there. Which is not possible as the
CA is supposed to make the client trust Squid generated certs, not the
other way around.


> 
> And from the client side:
> root at kali:~/test# curl -v https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> < 
> * Proxy replied OK to CONNECT request
> * successfully set certificate verify locations:
> *   CAfile: none
>   CApath: /etc/ssl/certs
> * SSLv3, TLS handshake, Client hello (1):
> * SSLv3, TLS handshake, Server hello (2):
> * SSLv3, TLS handshake, CERT (11):
> * SSLv3, TLS alert, Server hello (2):
> * SSL certificate problem: self signed certificate in certificate chain
> * Closing connection #0
> 
> And testing with specifying the .der file:
> root at kali:~/test# curl --cacert /etc/ssl/certs/SquidCA.der -v
> https://mail.slave-tothe-box.net
> * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> *   Trying 192.168.1.9...
> * connected
> * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
>> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
>> Host: mail.slave-tothe-box.net:443
>> User-Agent: curl/7.26.0
>> Proxy-Connection: Keep-Alive
>>
> * Easy mode waiting response from proxy CONNECT
> < HTTP/1.1 200 Connection established
> < 
> * Proxy replied OK to CONNECT request
> * error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
> 
> * Closing connection #0
> curl: (77) error setting certificate verify locations:
>   CAfile: /etc/ssl/certs/SquidCA.der
>   CApath: /etc/ssl/certs
> 
> 
> I can confirm that the server is using a bona-fide certificate issued
> from StartSSL and works, so at this point I'm open to suggestions.
> Thank you.

curl is complaining that the CA chain for the Squid-generted cert has a
self-signed CA. This is expected and desired behaviour if the
self-signed CA was sent by Squid.

The errors only occur when the self-signed CA is not sent by Squid, but
using the one installed on the client.


For that I believe you need to configure Squid to sign/generate using
the intermediate certificate. The self-signed root CA not configured in
Squid at all.

Like so:

A)
 client Trust DB installed with self-signed root CA

 squid.conf cert= configured with intermediary CA certificate

 squid.conf cafile= configured with any other intermediary CA
certificates (in order back to root CA, but excluding it).

 Squid generates per-connection certificate

OR:

B)
 client Trust DB installed with self-signed root CA

 squid.conf cert= configured with self-signed root CA

 Squid generates per-connection certificate


Note that in (B) there is no intermediary certificate at all, and Squid
does not emit any CA chain to the client.

It works exactly the same way as the globally trusted CA do. But they
are contractually obliged to refuse giving out intermediary CA for
anyones use, or loose their status as trusted CA.

Amos



From lucas2 at dds.nl  Tue May 26 09:30:57 2015
From: lucas2 at dds.nl (Lucas van Braam van Vloten)
Date: Tue, 26 May 2015 11:30:57 +0200
Subject: [squid-users] Proxy chain question
In-Reply-To: <555FF9B5.2080801@treenet.co.nz>
References: <1432227526.9790.6.camel@dds.nl>
 <555EA006.7080105@treenet.co.nz> <1432302578.20568.29.camel@dds.nl>
 <555FF9B5.2080801@treenet.co.nz>
Message-ID: <1432632657.2815.52.camel@dds.nl>

Hi,

Thanks for your extensive replies, it really boosts my understanding of
Squid :-)

> No, because Squid is only aware of two ways to send request - either a
> connection going to the TMG, or a connection going out directly on
port
> 443 to the server (bypassing the TMG). That latter is forbidden by
> firewall rules I presume, and the connection to the TMG is not secured
> for use with https:// URLs.

Hmm, bummer. I understand your point.
I wonder if it is possible to work around this limitation. It seems like
it is going to look ugly - but for now I am just exploring
possibilities.

For example, would it be possible to use two Squid instances, one to set
up the https connection ("directly" to the internet webservice) and the
second acting as forward proxy to relay all requests from the local
server through the TMG proxy? If some sort of "catch all" configuration
is possible on the second instance, the first instance does not need to
know it as a peer - if you know what I mean.

Alternatively, is it conceivable to use the iptables firewall on the
Squid box (running on RHEL 6.6) to relay traffic through the TMG? So
that Squid would not need any knowledge about this peer, and effectively
thinks it talks directly to the webservice (as required for TLS).

Thanks,
Lucas


> In the case where the browser client is sending HTTPS to Squid as a
> forward proxy it does so using CONNECT requests. Squid is able to
relay
> those CONNECT messages to the TMG and you see it working as the tunnel
> spans both hops and they are both blind to the HTTPS messages
themselves.

-----Original Message-----From: Amos Jeffries <squid3 at treenet.co.nz>
To: Lucas van Braam van Vloten <lucas2 at dds.nl>,
squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Proxy chain question
Date: Sat, 23 May 2015 15:53:25 +1200

On 23/05/2015 1:49 a.m., Lucas van Braam van Vloten wrote:
> Hello,
> 
> Thanks for your reply.
> 
>> Any particular reason?
> Unfortunately this double setup is not my choice, our architects
> prescribe use of the TMG proxy as mandatory for all internet access from
> the internal network. No exceptions.
>> Since the TMG is a reverse-proxy (...)
> This is true only for inbound traffic coming from internet; TMG acts as
> a forward proxy for outbound traffic.
> 

Ah. Okay.

> So the updated diagram for what I am trying to accomplish would be
> something like this:
> 
>                http       https       https
> Internal client ->  Squid  ->   TMG    -> internet webservice
>                   (reverse    (forward    
>                    proxy)      proxy)
> 
>> Squid will not be able to handle this either unless it is directly
> connecting to that service without the TMG in the way.
> 
> I noticed that if I configure the Squid proxy as a forward proxy and the
> TMG as its peer, I can initiate and authenticate a secure connection to
> the internet web service from a browser in the local network (using the
> squid proxy). Apparently the TMG is passed transparently and TLS is
> terminated on the webservice. Intuitively I would assume that,
> therefore, there should also be some way to initiate a https connection,
> and handle the certificate authentication, from the squid server itself.
> 
> Considering the updated diagram, do you think this can be done in Squid?

No, because Squid is only aware of two ways to send request - either a
connection going to the TMG, or a connection going out directly on port
443 to the server (bypassing the TMG). That latter is forbidden by
firewall rules I presume, and the connection to the TMG is not secured
for use with https:// URLs.

In the case where the browser client is sending HTTPS to Squid as a
forward proxy it does so using CONNECT requests. Squid is able to relay
those CONNECT messages to the TMG and you see it working as the tunnel
spans both hops and they are both blind to the HTTPS messages themselves.


If you could TLS encrypt the connection to the TMG Squid could send the
HTTPS messages inside that, but then the TMG would still be the agent
doing the final client-cert bits with the origin server.

Amos



From squid3 at treenet.co.nz  Tue May 26 10:27:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 May 2015 22:27:41 +1200
Subject: [squid-users] Proxy chain question
In-Reply-To: <1432632657.2815.52.camel@dds.nl>
References: <1432227526.9790.6.camel@dds.nl>		
 <555EA006.7080105@treenet.co.nz> <1432302578.20568.29.camel@dds.nl>	
 <555FF9B5.2080801@treenet.co.nz> <1432632657.2815.52.camel@dds.nl>
Message-ID: <55644A9D.9070608@treenet.co.nz>

On 26/05/2015 9:30 p.m., Lucas van Braam van Vloten wrote:
> Hi,
> 
> Thanks for your extensive replies, it really boosts my understanding of
> Squid :-)
> 
>> No, because Squid is only aware of two ways to send request - either a
>> connection going to the TMG, or a connection going out directly on
> port
>> 443 to the server (bypassing the TMG). That latter is forbidden by
>> firewall rules I presume, and the connection to the TMG is not secured
>> for use with https:// URLs.
> 
> Hmm, bummer. I understand your point.
> I wonder if it is possible to work around this limitation. It seems like
> it is going to look ugly - but for now I am just exploring
> possibilities.

We are slowly working towards having Squid able to generate CONNECT
messages for use over insecure peers. But that is still some ways off
and will only be in Squid-4 or later.

> 
> For example, would it be possible to use two Squid instances, one to set
> up the https connection ("directly" to the internet webservice) and the
> second acting as forward proxy to relay all requests from the local
> server through the TMG proxy? If some sort of "catch all" configuration
> is possible on the second instance, the first instance does not need to
> know it as a peer - if you know what I mean.
> 
> Alternatively, is it conceivable to use the iptables firewall on the
> Squid box (running on RHEL 6.6) to relay traffic through the TMG? So
> that Squid would not need any knowledge about this peer, and effectively
> thinks it talks directly to the webservice (as required for TLS).

There is <http://nocrew.org/software/httptunnel.html> which may help a
little if you can use it for port 443 outgoing from Squid.

Amos



From jpotter833 at because.org.uk  Tue May 26 10:39:56 2015
From: jpotter833 at because.org.uk (Mr J Potter)
Date: Tue, 26 May 2015 11:39:56 +0100
Subject: [squid-users] Alternative ways of tracking users on
 unauthenticated proxy
In-Reply-To: <55630280.6050907@treenet.co.nz>
References: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
 <55630280.6050907@treenet.co.nz>
Message-ID: <CAMyAEHt06SVmgRDJtsab3keVmhQpcTmRxcen-SWrVGLBTCjKfA@mail.gmail.com>

Hi Amos,

OK this looks promising (if not actually working...)

So I have a config line:
external_acl_type userlookup ttl=60 %SRC
/opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf --user
root --password xxxx --table currentUsers --uidcol ip  --usercol uid
--tagcol ip --persist --debug

Where currentUsers looks like:
mysql> select * from currentUsers;
+------+--------------+---------+
| uid  | ip           | enabled |
+------+--------------+---------+
| 0003 | 10.15.228.12 | 1       |
+------+--------------+---------+

so running this externally I use:

/opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf --user
root --password fv89j8j6eg2 --table currentUsers --uidcol ip --usercol uid
--tagcol ip --debug

this replies with a username if I put in:
<anything> 10.15.228.12

So what is the <anything> about? And I'm still not getting any username in
my logfiles. Do I need to use the acl name somewhere else in the config
file too?

thanks,

Jim Potter
Network Manager
Oasis Brislington (formerly Brislington Enterprise College)

On 25 May 2015 at 12:07, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 25/05/2015 8:38 p.m., Mr J Potter wrote:
> > Hi all,
> >
> > I'm setting up a system for using iPads in our school, and I'm stuck a
> bit
> > on tracking what the students are doing on them.
> >
> > First up, I reaaly don't want a Pop-up login box from a 407 response
> from a
> > proxy server, so I'm looking for some other way to track who is doing
> what.
> >
> > What i have set up so far is PacketFence with an SSL-bump transparent
> proxy
> > (I've put the CAs o all the ipads) which works well in that users have to
> > log in before they get internet access. This works (they get a web page,
> > login and get 50 minutes of internet before it disconnects them), but the
> > only way I have of tracking users is by working out who was on each ipad
> > (from packetfence) then matching it against squid logs, which is messy.
>
> Squid comes bundled with a ext_sql_session_acl helper that looks up a
> database and produces OK/ERR (and username for logging) depending on
> whether the key given to it exists in the DB already.
> <http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html>
>
> You just need to get an UID metric. IP address, MAC address, and/or
> EUI-64 (IPv6 link-local) are suitable there. It sounds like your
> packetfence would be a good way to populate that DB too.
>
> >
> > One plan I had would be to add/remove entries in dns or hosts for users,
> > eg  IP address 10.2.3.4   -> hostname  fbloggs  (the user's login code)
> so
> > usernames would show up in the client hostname field, but squid caches
> > these I think.
>
> Yes. Dont do that with DNS.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150526/86a9ffe2/attachment.htm>

From jpotter833 at because.org.uk  Tue May 26 12:07:27 2015
From: jpotter833 at because.org.uk (Mr J Potter)
Date: Tue, 26 May 2015 13:07:27 +0100
Subject: [squid-users] Alternative ways of tracking users on
 unauthenticated proxy
In-Reply-To: <CAMyAEHt06SVmgRDJtsab3keVmhQpcTmRxcen-SWrVGLBTCjKfA@mail.gmail.com>
References: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
 <55630280.6050907@treenet.co.nz>
 <CAMyAEHt06SVmgRDJtsab3keVmhQpcTmRxcen-SWrVGLBTCjKfA@mail.gmail.com>
Message-ID: <CAMyAEHtswerUQ-oWwXteoNEvNK+QSXm-BFuqFgaRpcSpc+qCBw@mail.gmail.com>

OK - got it working...

added the lines:

external_acl_type userlookup ttl=60 concurrency=1 %SRC
/opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf --user
root --password xxx --table  currentUsers --uidcol ip  --usercol uid
--tagcol ip --persist
acl userlookup external userlookup
http_access allow localnet userlookup
http_access allow localnet

Now I get this in my logfiles:
10.15.228.12 - 0001 [26/May/2015:12:56:23 +0100] "POST
http://www.bing.com/fd/ls/lsp.aspx HTTP/1.1" 204 391 TCP_MISS:ORIGINAL_DST

I'll write all this up somewhere, as variations on what I have here is what
people are always asking for:
- Users log in via a web page, not a 407 popup box
- Authenticates to AD
- Users are filtered depending on who they are (via squidGuard)
- Logs activity against users
- logs them all off at a particular time
- No proxy settings (intercept HTTP+HTTPS)


thanks,

Jim Potter
Network Manager
Oasis Brislington (formerly Brislington Enterprise College)

On 26 May 2015 at 11:39, Mr J Potter <jpotter833 at because.org.uk> wrote:

> Hi Amos,
>
> OK this looks promising (if not actually working...)
>
> So I have a config line:
> external_acl_type userlookup ttl=60 %SRC
> /opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf --user
> root --password xxxx --table currentUsers --uidcol ip  --usercol uid
> --tagcol ip --persist --debug
>
> Where currentUsers looks like:
> mysql> select * from currentUsers;
> +------+--------------+---------+
> | uid  | ip           | enabled |
> +------+--------------+---------+
> | 0003 | 10.15.228.12 | 1       |
> +------+--------------+---------+
>
> so running this externally I use:
>
> /opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf
> --user root --password fv89j8j6eg2 --table currentUsers --uidcol ip
> --usercol uid --tagcol ip --debug
>
> this replies with a username if I put in:
> <anything> 10.15.228.12
>
> So what is the <anything> about? And I'm still not getting any username in
> my logfiles. Do I need to use the acl name somewhere else in the config
> file too?
>
> thanks,
>
> Jim Potter
> Network Manager
> Oasis Brislington (formerly Brislington Enterprise College)
>
> On 25 May 2015 at 12:07, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>> On 25/05/2015 8:38 p.m., Mr J Potter wrote:
>> > Hi all,
>> >
>> > I'm setting up a system for using iPads in our school, and I'm stuck a
>> bit
>> > on tracking what the students are doing on them.
>> >
>> > First up, I reaaly don't want a Pop-up login box from a 407 response
>> from a
>> > proxy server, so I'm looking for some other way to track who is doing
>> what.
>> >
>> > What i have set up so far is PacketFence with an SSL-bump transparent
>> proxy
>> > (I've put the CAs o all the ipads) which works well in that users have
>> to
>> > log in before they get internet access. This works (they get a web page,
>> > login and get 50 minutes of internet before it disconnects them), but
>> the
>> > only way I have of tracking users is by working out who was on each ipad
>> > (from packetfence) then matching it against squid logs, which is messy.
>>
>> Squid comes bundled with a ext_sql_session_acl helper that looks up a
>> database and produces OK/ERR (and username for logging) depending on
>> whether the key given to it exists in the DB already.
>> <http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html>
>>
>> You just need to get an UID metric. IP address, MAC address, and/or
>> EUI-64 (IPv6 link-local) are suitable there. It sounds like your
>> packetfence would be a good way to populate that DB too.
>>
>> >
>> > One plan I had would be to add/remove entries in dns or hosts for users,
>> > eg  IP address 10.2.3.4   -> hostname  fbloggs  (the user's login code)
>> so
>> > usernames would show up in the client hostname field, but squid caches
>> > these I think.
>>
>> Yes. Dont do that with DNS.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150526/2574863a/attachment.htm>

From squid3 at treenet.co.nz  Tue May 26 12:24:09 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 May 2015 00:24:09 +1200
Subject: [squid-users] Alternative ways of tracking users on
 unauthenticated proxy
In-Reply-To: <CAMyAEHtswerUQ-oWwXteoNEvNK+QSXm-BFuqFgaRpcSpc+qCBw@mail.gmail.com>
References: <CAMyAEHsPyRjyDC6-voAx1YsQWk2gtQTARo8=PYHeVEqA50DgvQ@mail.gmail.com>
 <55630280.6050907@treenet.co.nz>
 <CAMyAEHt06SVmgRDJtsab3keVmhQpcTmRxcen-SWrVGLBTCjKfA@mail.gmail.com>
 <CAMyAEHtswerUQ-oWwXteoNEvNK+QSXm-BFuqFgaRpcSpc+qCBw@mail.gmail.com>
Message-ID: <556465E9.3010807@treenet.co.nz>

On 27/05/2015 12:07 a.m., Mr J Potter wrote:
> OK - got it working...
> 
> added the lines:
> 
> external_acl_type userlookup ttl=60 concurrency=1 %SRC

Yes that "<anything>" field you asked about is the concurrency
channel-ID field. You can set concurrency as high as you want. The
lookup queue length for these helpers is 2*concurrency*children instead
of 2*children for older helpers.

> /opt/squid354/libexec/ext_sql_session_acl -dsn DBI:mysql:database=pf --user
> root --password xxx --table  currentUsers --uidcol ip  --usercol uid
> --tagcol ip --persist

 "--tagcol" is optional. Its probably not that useful just to dump the
IP address back out at Squid, so you should be able to just omit it.

Amos



From pkryon at gmail.com  Wed May 27 01:38:53 2015
From: pkryon at gmail.com (Patrick)
Date: Tue, 26 May 2015 21:38:53 -0400
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <CAJgbPpfxTfghie8+eQmn9xs3RNT_8vOzEfkxDRoKw6JWh5Fa+Q@mail.gmail.com>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
 <555EB047.5010102@treenet.co.nz>
 <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>
 <555F2B9C.90109@treenet.co.nz>
 <CAJgbPpfxTfghie8+eQmn9xs3RNT_8vOzEfkxDRoKw6JWh5Fa+Q@mail.gmail.com>
Message-ID: <CAJgbPpdSRo-d1MhFTbWZR6rMNE_YpGPtdgAuANuZ4ebvqWuHXA@mail.gmail.com>

Thanks Amos, for the help so far.  Unfortunately, I'm still stuck even with
the most recent daily.. Using http_access I don't seem to get the kv-pairs
passed to my url_rewrite_program but I do see the user= value in my
access.log.  The adapted_http_access concept seemed promising but my
helpers don't seem like they're being called when I use it.  The user=
value no longer shows up in access.log and my helper logs are empty.

Is this a known issue?  Is there anything I can provide that would be
helpful?

On Fri, May 22, 2015 at 10:44 AM, Patrick <pkryon at gmail.com> wrote:

> Hmm, the helper does seem to be working and returning user=X as expected.
> In fact the user does show up in Squid's access.log and as configured I
> have no other sources for usernames.
>
> I'm not using ICAP (at least intentionally), but switching
> to adapted_http_access doesn't not seem to help me.  I switch
> my url_rewrite_access from the tee I was running before to a quick script
> that logs stdin and returns OK no matter what it gets.
>
> So using...
>
> url_rewrite_extras "%>a/%>A %ue %>rm myip=%la myport=%lp"
> url_rewrite_program /opt/idbf-dev/idbf_squid_url_rewrite_test.py
>
> My logs show:
>
> 2015-05-22 10:34:59,175 - root - DEBUG    -
> http://www.ipchicken.com/images/9.gif 172.20.15.235/pcname.example.com -
> GET myip=- myport=3129
> 2015-05-22 10:34:59,213 - root - DEBUG    -
> http://www.ipchicken.com/images/pixel.gif 172.20.15.235/pcname.example.com
> - GET myip=- myport=3129
> 2015-05-22 10:34:59,225 - root - DEBUG    -
> http://www.ipchicken.com/images/green.gif 172.20.15.235/pcname.example.com
> - GET myip=- myport=3129
> 2015-05-22 10:34:59,241 - root - DEBUG    -
> http://www.ipchicken.com/images/ipc.gif 172.20.15.235/pcname.example.com
> - GET myip=- myport=3129
> 2015-05-22 10:35:01,052 - root - DEBUG    - http://www.ipchicken.com/
> 172.20.15.235/pcname.example.com - GET myip=- myport=3129
>
> And access.log shows:
>
> 1432305299.216     41 172.20.15.235 TCP_MISS/304 270 GET
> http://www.ipchicken.com/images/9.gif ryonpk ORIGINAL_DST/209.68.27.16 -
> 1432305299.256     43 172.20.15.235 TCP_MISS/304 270 GET
> http://www.ipchicken.com/images/pixel.gif ryonpk ORIGINAL_DST/209.68.27.16
> -
> 1432305299.264     51 172.20.15.235 TCP_MISS/304 271 GET
> http://www.ipchicken.com/images/green.gif ryonpk ORIGINAL_DST/209.68.27.16
> -
> 1432305299.281     68 172.20.15.235 TCP_MISS/304 271 GET
> http://www.ipchicken.com/images/ipc.gif ryonpk ORIGINAL_DST/209.68.27.16 -
> 1432305301.137     85 172.20.15.235 TCP_MISS/200 6488 GET
> http://www.ipchicken.com/ ryonpk ORIGINAL_DST/209.68.27.16 text/html
>
> ryonpk is the correct user passed from the helper.
>
> On Fri, May 22, 2015 at 9:14 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 23/05/2015 12:59 a.m., Patrick wrote:
>> > Thanks for looking at this, Amos.  Unfortunately, I am still seeing the
>> > same result with the squid-3.5.4-20150522-r13836 daily.
>> >
>> > I was thinking I might just being seeing what's described at
>> > http://www.squid-cache.org/Doc/config/url_rewrite_extras/ as "In
>> practice,
>> > a %macro expands as a dash (-) if the helper request is sent before the
>> > required macro information is available to Squid."  But I guess if that
>> was
>> > the case I would have expected to see the dash only on the first request
>> > with the subsequent requests returning the %ue value at least within the
>> > ttl.  But I get just the dash on every request.
>> >
>>
>> No, you should be getting the value from the user=X kv-pair produced by
>> the helper on every request. The bug fixed by Nathan was that only the
>> first was and the rest wrongly got "-".
>>
>> If you are getting it on absolutely all requests it would seem the
>> helper is not producing user=X.
>>
>> Or possibly ICAP is stripping it away again by creating a new "adapted"
>> request. If that is the case use the external_acl_type helper in the
>> adapted_http_access checks instead of http_access.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150526/fa6e7223/attachment.htm>

From fredbmail at free.fr  Wed May 27 10:04:12 2015
From: fredbmail at free.fr (FredB)
Date: Wed, 27 May 2015 12:04:12 +0200 (CEST)
Subject: [squid-users] Logformat tag for a specific ACL
In-Reply-To: <1337081356.591325035.1432720633214.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1414602846.591341807.1432721052210.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello,

There is a way to "tag" an ACL in access.log ?

acl test url_regex "/tmp/myfile"

logformat fred %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh "%{User-Agent}>h"
access_log daemon:/var/log/squid/access.log fred

If I put something at the end 

logformat fred %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh "%{User-Agent}>h" test

The end of line contain test, can I do this for an Acl only ? And let the usual logformat without "test" at the end ? 

Regards 

Fred

Something like:

10.1.1.1 - fred [27/May/2015:12:00:19 +0200] "CONNECT zimbra.free.fr:443 HTTP/1.0" 200 4724 TCP_TUNNEL:HIER_DIRECT "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0"
10.1.1.1 - fred [27/May/2015:12:00:21 +0200] "CONNECT testsite.fr:443 HTTP/1.0" 200 4724 TCP_TUNNEL:HIER_DIRECT "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0" test 





From squid3 at treenet.co.nz  Wed May 27 11:38:40 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 May 2015 23:38:40 +1200
Subject: [squid-users] Logformat tag for a specific ACL
In-Reply-To: <1414602846.591341807.1432721052210.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1414602846.591341807.1432721052210.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <5565ACC0.3070102@treenet.co.nz>

On 27/05/2015 10:04 p.m., FredB wrote:
> Hello,
> 
> There is a way to "tag" an ACL in access.log ?
> 
> acl test url_regex "/tmp/myfile"
> 
> logformat fred %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh "%{User-Agent}>h"
> access_log daemon:/var/log/squid/access.log fred
> 
> If I put something at the end 
> 
> logformat fred %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh "%{User-Agent}>h" test
> 
> The end of line contain test, can I do this for an Acl only ? And let the usual logformat without "test" at the end ? 

Yes, but not to the same log file. Like this:

 access_log daemon:/var/log/squid/access.log squid !test

 access_log daemon:/var/log/squid/access_test.log fred test


Amos



From squid at borrill.org.uk  Wed May 27 15:45:56 2015
From: squid at borrill.org.uk (Stephen Borrill)
Date: Wed, 27 May 2015 16:45:56 +0100
Subject: [squid-users] ipf transparent enabled, but squid says not supported
Message-ID: <5565E6B4.2040201@borrill.org.uk>

I have:
Squid Cache: Version 3.5.4
Service Name: squid
configure options:  '--sysconfdir=/usr/pkg/etc/squid'
'--localstatedir=/var/squid' '--datarootdir=/usr/pkg/share/squid'
'--disable-strict-error-checking' '--enable-auth'
'--enable-cachemgr-hostname=localhost' '--enable-delay-pools'
'--enable-icap-client' '--enable-icmp' '--enable-poll'
'--enable-removal-policies=lru,heap'
'--enable-storeio=ufs diskd' '--with-aio' '--with-default-user=squid' 
'--with-pidfile=/var/run/squid.pid' '--disable-arch-native'
'--enable-ipf-transparent' '--enable-arp-acl' '--enable-carp' 
'--disable-ipv6' '--without-mit-krb5' '--without-heimdal-krb5' 
'--disable-snmp' '--enable-ssl' '--with-openssl=/usr/pkg'
'--enable-auth-basic=NCSA getpwnam PAM' '--enable-auth-digest=file'
'--disable-auth-negotiate' '--enable-auth-ntlm=fake smb_lm'
'--enable-external-acl-helpers=file_userip unix_group' 
'--prefix=/usr/pkg' '--build=i486--netbsdelf'
'--host=i486--netbsdelf' '--mandir=/usr/pkg/man' 
'build_alias=i486--netbsdelf' 'host_alias=i486--netbsdelf'
'CC=cc' 'CFLAGS=-O2 -I/usr/include -I/usr/pkg/include'
'LDFLAGS=-L/usr/lib -Wl,-R/usr/lib -L/usr/pkg/lib -Wl,-R/usr/pkg/lib' 
'LIBS=' 'CPPFLAGS=-I/usr/include -I/usr/pkg/include'
'CXX=c++' 'CXXFLAGS=-O2 -I/usr/include -I/usr/pkg/include'

squid.conf contains:
http_port 127.0.0.1:8006 intercept name=port_8006

Yet I see the following ev:
2015/05/27 16:02:46 kid1| WARNING: transparent proxying not supported

Same config works with earlier version of squid (3.4 and earlier). 
What's changed?

-- 
Stephen




From jlay at slave-tothe-box.net  Wed May 27 15:52:27 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 27 May 2015 09:52:27 -0600
Subject: [squid-users] ipf transparent enabled,
 but squid says not supported
In-Reply-To: <5565E6B4.2040201@borrill.org.uk>
References: <5565E6B4.2040201@borrill.org.uk>
Message-ID: <0e20c8c87be4f23f54735747be0736ef@localhost>

On 2015-05-27 09:45 AM, Stephen Borrill wrote:
> I have:
> Squid Cache: Version 3.5.4
> Service Name: squid
> configure options:  '--sysconfdir=/usr/pkg/etc/squid'
> '--localstatedir=/var/squid' '--datarootdir=/usr/pkg/share/squid'
> '--disable-strict-error-checking' '--enable-auth'
> '--enable-cachemgr-hostname=localhost' '--enable-delay-pools'
> '--enable-icap-client' '--enable-icmp' '--enable-poll'
> '--enable-removal-policies=lru,heap'
> '--enable-storeio=ufs diskd' '--with-aio' '--with-default-user=squid'
> '--with-pidfile=/var/run/squid.pid' '--disable-arch-native'
> '--enable-ipf-transparent' '--enable-arp-acl' '--enable-carp'
> '--disable-ipv6' '--without-mit-krb5' '--without-heimdal-krb5'
> '--disable-snmp' '--enable-ssl' '--with-openssl=/usr/pkg'
> '--enable-auth-basic=NCSA getpwnam PAM' '--enable-auth-digest=file'
> '--disable-auth-negotiate' '--enable-auth-ntlm=fake smb_lm'
> '--enable-external-acl-helpers=file_userip unix_group'
> '--prefix=/usr/pkg' '--build=i486--netbsdelf'
> '--host=i486--netbsdelf' '--mandir=/usr/pkg/man'
> 'build_alias=i486--netbsdelf' 'host_alias=i486--netbsdelf'
> 'CC=cc' 'CFLAGS=-O2 -I/usr/include -I/usr/pkg/include'
> 'LDFLAGS=-L/usr/lib -Wl,-R/usr/lib -L/usr/pkg/lib -Wl,-R/usr/pkg/lib'
> 'LIBS=' 'CPPFLAGS=-I/usr/include -I/usr/pkg/include'
> 'CXX=c++' 'CXXFLAGS=-O2 -I/usr/include -I/usr/pkg/include'
> 
> squid.conf contains:
> http_port 127.0.0.1:8006 intercept name=port_8006
> 
> Yet I see the following ev:
> 2015/05/27 16:02:46 kid1| WARNING: transparent proxying not supported
> 
> Same config works with earlier version of squid (3.4 and earlier).
> What's changed?

Look through your config.log...I experienced a similar thing and, upon 
running my ./configure line and watching it I saw I was missing a 
library.

James


From mmonette at 2keys.ca  Wed May 27 16:15:37 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 27 May 2015 12:15:37 -0400 (EDT)
Subject: [squid-users] Squid, Gmail.com and HSTS.
Message-ID: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>

Has anyone been able to configure Squid in a way so that if you type https://gmail.com in your browser, you are NOT presented with the "OMG HSTS I refuse to load anything" page? When I go to https://gmail.com, I get an invalid certificate because the cert is for mail.google.com, issued by my CA. If I go to https://mail.google.com, the cert is beautifully green. Why can't squid detect that gmail.com is redirecting my browser to mail.google.com and generate the cert accordingly?

Even configuring an acl for gmail.com doesn't work. It seems like even though I am punching https://gmail.com in my browser, Squid detects it as though I am typing "https://mail.google.com" in my browser and is ignoring any ACLs I have setup specifically for "gmail.com".

I can't be the only one with this issue?



I've also attempted to do:

acl bl1 gmail.com moz.com
always_direct allow bl1 <- from what I understand this bypasses squid and tells my browser to get the cert right from the site. Maybe I am wrong.

But certificates still come from Squid, so I don't see any effect from that line.

Here's my config, lots of garbage in there since I have been trying everything i can think of to get this working. I want to add that for my acl called BL1, the only one that works is moz.com . They are part of the same ACL line, so if one works, they should all work. Except they do not.

Thanks in advance.

cat /etc/squid/squid.conf

~~

debug_options ALL,9

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT


http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump bump step2 all
ssl_bump bump step3 all

acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
#acl bl1 url_regex -i ^http(s)?://gmail.com
#acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
#acl bl3 url_regex -i ^http(s)?://moz.com.*
#acl bl4 url_regex -i moz.com
deny_info http://ask.com bl1 # I was testing redirecting stuff, but since the acl is not even picked up, this stuff is useless.
http_reply_access deny bl1 # useless
#http_access deny bl1 
#http_access deny bl1 CONNECT

http_access allow localnet
http_access allow localhost

http_access allow all

http_port 3128 accel vhost allow-direct

#https_port 3129 transparent ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

sslproxy_options NO_SSLv2
sslproxy_options NO_SSLv3

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

#cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320


Mike


From mmonette at 2keys.ca  Wed May 27 16:23:33 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 27 May 2015 12:23:33 -0400
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
Message-ID: <A9B12740-03C3-4477-B978-820452FF528B@2keys.ca>

I just thought of something else. First of all I'm new to squid and I am not aware of 10% of the things its capable of yet so I will ask. 

Is squid capable of adding custom SNIs? Like could I have it so gmail.com is added to the certificate as a subject alternate name EVEN though the original certificate doesn't contain it? If such a thing is possible I would love to know the term for it so I can do some searches. 

Appreciate it!

On May 27, 2015 12:15:37 PM EDT, Michael Monette <mmonette at 2keys.ca> wrote:
>Has anyone been able to configure Squid in a way so that if you type
>https://gmail.com in your browser, you are NOT presented with the "OMG
>HSTS I refuse to load anything" page? When I go to https://gmail.com, I
>get an invalid certificate because the cert is for mail.google.com,
>issued by my CA. If I go to https://mail.google.com, the cert is
>beautifully green. Why can't squid detect that gmail.com is redirecting
>my browser to mail.google.com and generate the cert accordingly?
>
>Even configuring an acl for gmail.com doesn't work. It seems like even
>though I am punching https://gmail.com in my browser, Squid detects it
>as though I am typing "https://mail.google.com" in my browser and is
>ignoring any ACLs I have setup specifically for "gmail.com".
>
>I can't be the only one with this issue?
>
>
>
>I've also attempted to do:
>
>acl bl1 gmail.com moz.com
>always_direct allow bl1 <- from what I understand this bypasses squid
>and tells my browser to get the cert right from the site. Maybe I am
>wrong.
>
>But certificates still come from Squid, so I don't see any effect from
>that line.
>
>Here's my config, lots of garbage in there since I have been trying
>everything i can think of to get this working. I want to add that for
>my acl called BL1, the only one that works is moz.com . They are part
>of the same ACL line, so if one works, they should all work. Except
>they do not.
>
>Thanks in advance.
>
>cat /etc/squid/squid.conf
>
>~~
>
>debug_options ALL,9
>
>acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
>acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
>acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
>acl localnet src fc00::/7       # RFC 4193 local private network range
>acl localnet src fe80::/10      # RFC 4291 link-local (directly
>plugged) machines
>
>acl SSL_ports port 443
>acl Safe_ports port 80		# http
>acl Safe_ports port 21		# ftp
>acl Safe_ports port 443		# https
>acl Safe_ports port 70		# gopher
>acl Safe_ports port 210		# wais
>acl Safe_ports port 1025-65535	# unregistered ports
>acl Safe_ports port 280		# http-mgmt
>acl Safe_ports port 488		# gss-http
>acl Safe_ports port 591		# filemaker
>acl Safe_ports port 777		# multiling http
>acl CONNECT method CONNECT
>
>
>http_access deny !Safe_ports
>
>http_access deny CONNECT !SSL_ports
>
>http_access allow localhost manager
>http_access deny manager
>
>acl step1 at_step SslBump1
>acl step2 at_step SslBump2
>acl step3 at_step SslBump3
>
>ssl_bump peek step1 all
>ssl_bump bump step2 all
>ssl_bump bump step3 all
>
>acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
>#acl bl1 url_regex -i ^http(s)?://gmail.com
>#acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
>#acl bl3 url_regex -i ^http(s)?://moz.com.*
>#acl bl4 url_regex -i moz.com
>deny_info http://ask.com bl1 # I was testing redirecting stuff, but
>since the acl is not even picked up, this stuff is useless.
>http_reply_access deny bl1 # useless
>#http_access deny bl1 
>#http_access deny bl1 CONNECT
>
>http_access allow localnet
>http_access allow localhost
>
>http_access allow all
>
>http_port 3128 accel vhost allow-direct
>
>#https_port 3129 transparent ssl-bump generate-host-certificates=on
>dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem
>key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
>https_port 3129 intercept ssl-bump generate-host-certificates=on
>dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem
>key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
>
>sslproxy_cert_error allow all
>sslproxy_flags DONT_VERIFY_PEER
>
>sslproxy_options NO_SSLv2
>sslproxy_options NO_SSLv3
>
>sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>sslcrtd_children 8 startup=1 idle=1
>
>#cache_dir ufs /var/spool/squid 100 16 256
>coredump_dir /var/spool/squid
>
>refresh_pattern ^ftp:		1440	20%	10080
>refresh_pattern ^gopher:	1440	0%	1440
>refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
>refresh_pattern .		0	20%	4320
>
>
>Mike
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150527/92b9fa63/attachment.htm>

From squid3 at treenet.co.nz  Wed May 27 17:20:33 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 May 2015 05:20:33 +1200
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
Message-ID: <5565FCE1.2010909@treenet.co.nz>

On 28/05/2015 4:15 a.m., Michael Monette wrote:
> Has anyone been able to configure Squid in a way so that if you type
https://gmail.com in your browser, you are NOT presented with the "OMG
HSTS I refuse to load anything" page? When I go to https://gmail.com, I
get an invalid certificate because the cert is for mail.google.com,
issued by my CA. If I go to https://mail.google.com, the cert is
beautifully green. Why can't squid detect that gmail.com is redirecting
my browser to mail.google.com and generate the cert accordingly?

That is *actually* what their server certificate contains. Ironic isn't
it that their own certs do not comply with the restrictions they require
of all others.

Squid actually does obey HSTS requirements for secure handling of the
reqeust. Its just the browser is incapable of detecting that, notices
the custom CA and assumes the worst.

> 
> Even configuring an acl for gmail.com doesn't work. It seems like
> even
though I am punching https://gmail.com in my browser, Squid detects it
as though I am typing "https://mail.google.com" in my browser and is
ignoring any ACLs I have setup specifically for "gmail.com".
> 
> I can't be the only one with this issue?
> 
> 
> I've also attempted to do:
> 
> acl bl1 gmail.com moz.com
> always_direct allow bl1 <- from what I understand this bypasses squid and tells my browser to get the cert right from the site. Maybe I am wrong.
> 

You are. squid.conf has nothing to do with your browser.

That line tells Squid not to use any cache_peer connections when serving
a request that matches ACL "bl1".

In the very first implementation way, way back in 3.1 decrypted requests
could leak out over insecure cache_peer. So people were advised to use
"always_direct allow all" to force it to work correctly. That bug was
fixed long ago but the config still persists in the web.


> But certificates still come from Squid, so I don't see any effect from that line.
> 
> Here's my config, lots of garbage in there since I have been trying everything i can think of to get this working. I want to add that for my acl called BL1, the only one that works is moz.com . They are part of the same ACL line, so if one works, they should all work. Except they do not.
> 
> Thanks in advance.
> 
> cat /etc/squid/squid.conf
> 
> ~~
> 
> debug_options ALL,9
> 
> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> acl CONNECT method CONNECT
> 
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump bump step2 all
> ssl_bump bump step3 all

"all" at the end of ACL lines has no meaning unless there is an
authentication ACL that would otherwise be on the end and you dont want
to trigger auth popups.


> 
> acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
> #acl bl1 url_regex -i ^http(s)?://gmail.com
> #acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
> #acl bl3 url_regex -i ^http(s)?://moz.com.*
> #acl bl4 url_regex -i moz.com
> deny_info http://ask.com bl1 # I was testing redirecting stuff, but since the acl is not even picked up, this stuff is useless.
> http_reply_access deny bl1 # useless

Yes, why bother testing for request *URL* domain and blocking on the
*reply*.


> #http_access deny bl1 
> #http_access deny bl1 CONNECT
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access allow all
> 
> http_port 3128 accel vhost allow-direct
> 
> #https_port 3129 transparent ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
> https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 
> sslproxy_options NO_SSLv2
> sslproxy_options NO_SSLv3

NOTE: Only the latest sslproxy_options line has any effect. So the
NO_SSLv2 line is not obeyed.

Use this instead:
  sslproxy_options NO_SSLv2,NO_SSLv3

Although that said. SSLv2 support was dropped earlier so the latest
releases will



Anyhow, back to your ACL problem. Be aware that the SNI and cert fields
are not the HTTP request URL.

* Ensure you are using the latest Squid, today that is a 3.5.4 snapshot
(and in ~48hrs will be 3.5.5).

* Use the "ssl::server_name" ACL type if you need to block using SNI or
server certificate SubjectName.

What Squid does for intercepted port 443 traffic is generate a fake
CONNECT request using the *IP:port* of the server from TCP. http_access
is checked for that. You can block if ou wish or pass it through,
ssl_bump may decide not to bump and it can safely be passed through
peers, or acted on directly to tunnel the intercepted bytes to the server.

If you do SSL-bump with peeking at client and/or server cert the
ssl_server_name ACL matches the data found there about the server
name/alias.

After the decryption SSL-bump process has been completed will you start
to get the HTTP requests that were encrypted as HTTPS. The https_access
etc are all run normally on those since they are just regular HTTP
traffic but for https:// URLs.

Amos



From mmonette at 2keys.ca  Wed May 27 18:33:30 2015
From: mmonette at 2keys.ca (Michael Monette)
Date: Wed, 27 May 2015 14:33:30 -0400 (EDT)
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <5565FCE1.2010909@treenet.co.nz>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
 <5565FCE1.2010909@treenet.co.nz>
Message-ID: <1178914605.1395397.1432751610933.JavaMail.zimbra@2keys.ca>

Yeah I don't know what I am doing wrong but I don't have these ACL types..Or I am somehow not copy & pasting properly:

FATAL: Invalid ACL type 'ssl::server_name'
FATAL: Bungled /etc/squid/squid.conf line 54: acl nobumpsites ssl::server_name .google.com
Squid Cache (Version 3.5.4): Terminated abnormally.
CPU Usage: 0.005 seconds = 0.003 user + 0.002 sys
Maximum Resident Size: 24096 KB
Page faults with physical i/o: 0
Squid restarted
[root at ottt-corp-paz-squid-1 squid-3.5.4]# squid -v
Squid Cache: Version 3.5.4
Service Name: squid
configure options:  '--prefix=/usr' '--includedir=/usr/include' '--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-included-ltdl' --enable-ltdl-convenience


There are also issues with "at_step" now:

2015/05/27 14:32:17| FATAL: Invalid ACL type 'at_step'
FATAL: Bungled /etc/squid/squid.conf line 52: acl step1 at_step SslBump1
Squid Cache (Version 3.5.4): Terminated abnormally.
CPU Usage: 0.005 seconds = 0.003 user + 0.002 sys
Maximum Resident Size: 24080 KB
Page faults with physical i/o: 0

Did I miss something when compiling? I just followed what was on the Squid wiki.

I am all out of ideas..

Thanks, 

Mike


----- Original Message -----
From: "Amos Jeffries" <squid3 at treenet.co.nz>
To: "squid-users" <squid-users at lists.squid-cache.org>
Sent: Wednesday, May 27, 2015 1:20:33 PM
Subject: Re: [squid-users] Squid, Gmail.com and HSTS.

On 28/05/2015 4:15 a.m., Michael Monette wrote:
> Has anyone been able to configure Squid in a way so that if you type
https://gmail.com in your browser, you are NOT presented with the "OMG
HSTS I refuse to load anything" page? When I go to https://gmail.com, I
get an invalid certificate because the cert is for mail.google.com,
issued by my CA. If I go to https://mail.google.com, the cert is
beautifully green. Why can't squid detect that gmail.com is redirecting
my browser to mail.google.com and generate the cert accordingly?

That is *actually* what their server certificate contains. Ironic isn't
it that their own certs do not comply with the restrictions they require
of all others.

Squid actually does obey HSTS requirements for secure handling of the
reqeust. Its just the browser is incapable of detecting that, notices
the custom CA and assumes the worst.

> 
> Even configuring an acl for gmail.com doesn't work. It seems like
> even
though I am punching https://gmail.com in my browser, Squid detects it
as though I am typing "https://mail.google.com" in my browser and is
ignoring any ACLs I have setup specifically for "gmail.com".
> 
> I can't be the only one with this issue?
> 
> 
> I've also attempted to do:
> 
> acl bl1 gmail.com moz.com
> always_direct allow bl1 <- from what I understand this bypasses squid and tells my browser to get the cert right from the site. Maybe I am wrong.
> 

You are. squid.conf has nothing to do with your browser.

That line tells Squid not to use any cache_peer connections when serving
a request that matches ACL "bl1".

In the very first implementation way, way back in 3.1 decrypted requests
could leak out over insecure cache_peer. So people were advised to use
"always_direct allow all" to force it to work correctly. That bug was
fixed long ago but the config still persists in the web.


> But certificates still come from Squid, so I don't see any effect from that line.
> 
> Here's my config, lots of garbage in there since I have been trying everything i can think of to get this working. I want to add that for my acl called BL1, the only one that works is moz.com . They are part of the same ACL line, so if one works, they should all work. Except they do not.
> 
> Thanks in advance.
> 
> cat /etc/squid/squid.conf
> 
> ~~
> 
> debug_options ALL,9
> 
> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> acl CONNECT method CONNECT
> 
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump bump step2 all
> ssl_bump bump step3 all

"all" at the end of ACL lines has no meaning unless there is an
authentication ACL that would otherwise be on the end and you dont want
to trigger auth popups.


> 
> acl bl1 dstdomain gmail.com mail.google.com accounts.google.com moz.com
> #acl bl1 url_regex -i ^http(s)?://gmail.com
> #acl bl2 url_regex -i ^http(s)?://([a-zA-Z]+).gmail.com.*
> #acl bl3 url_regex -i ^http(s)?://moz.com.*
> #acl bl4 url_regex -i moz.com
> deny_info http://ask.com bl1 # I was testing redirecting stuff, but since the acl is not even picked up, this stuff is useless.
> http_reply_access deny bl1 # useless

Yes, why bother testing for request *URL* domain and blocking on the
*reply*.


> #http_access deny bl1 
> #http_access deny bl1 CONNECT
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access allow all
> 
> http_port 3128 accel vhost allow-direct
> 
> #https_port 3129 transparent ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
> https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myca.pem key=/etc/squid/ssl_cert/myca.pem options=NO_SSLv3
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 
> sslproxy_options NO_SSLv2
> sslproxy_options NO_SSLv3

NOTE: Only the latest sslproxy_options line has any effect. So the
NO_SSLv2 line is not obeyed.

Use this instead:
  sslproxy_options NO_SSLv2,NO_SSLv3

Although that said. SSLv2 support was dropped earlier so the latest
releases will



Anyhow, back to your ACL problem. Be aware that the SNI and cert fields
are not the HTTP request URL.

* Ensure you are using the latest Squid, today that is a 3.5.4 snapshot
(and in ~48hrs will be 3.5.5).

* Use the "ssl::server_name" ACL type if you need to block using SNI or
server certificate SubjectName.

What Squid does for intercepted port 443 traffic is generate a fake
CONNECT request using the *IP:port* of the server from TCP. http_access
is checked for that. You can block if ou wish or pass it through,
ssl_bump may decide not to bump and it can safely be passed through
peers, or acted on directly to tunnel the intercepted bytes to the server.

If you do SSL-bump with peeking at client and/or server cert the
ssl_server_name ACL matches the data found there about the server
name/alias.

After the decryption SSL-bump process has been completed will you start
to get the HTTP requests that were encrypted as HTTPS. The https_access
etc are all run normally on those since they are just regular HTTP
traffic but for https:// URLs.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From mmonette at 2keys.ca  Wed May 27 18:41:32 2015
From: mmonette at 2keys.ca (Mike)
Date: Wed, 27 May 2015 18:41:32 +0000 (UTC)
Subject: [squid-users]
	=?utf-8?q?New_server=5Fname_acl_causes_fatal_error_?=
	=?utf-8?q?starting=09Squid_3=2E5=2E4?=
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
 <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
Message-ID: <loom.20150527T204113-559@post.gmane.org>

Stanford Prescott <stan.prescott <at> gmail.com> writes:

> 
> 
> Never mind. I figured the acl out. I was using someone else's 
instructions who accidentally left out the double :: ssl::server_name 
using just a single :.


I am getting the same thing as you except I don't have the mistake you 
did. I literally copied your line into my config and it's still bombing 
out.

2015/05/27 14:38:25| FATAL: Invalid ACL type 'ssl::server_name'
FATAL: Bungled /etc/squid/squid.conf line 52: acl nobumpSites 
ssl::server_name .wellsfargo.com
Squid Cache (Version 3.5.4): Terminated abnormally.
CPU Usage: 0.006 seconds = 0.002 user + 0.004 sys
Maximum Resident Size: 24112 KB
Page faults with physical i/o: 0

I'm about to just give up on squid..losing my mind. Any ideas?


> 
> 
> On Wed, May 20, 2015 at 12:36 PM, Stanford Prescott <stan.prescott 
<at> gmail.com> wrote:
> 
> 
> After a diversion getting SquidClamAV working, i am back to trying to 
get peek and splice working. I am trying to put together information 
from previous recommendations I have received. Right now, I can't get 
the server_name acl working. When I put this in my squid.confacl 
nobumpSites ssl:server_name .example.com
> I get a fatal error starting squid? using that acl saying the acl is 
"Bungled".
> Is the form of the acl incorrect?
> 
> 
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users <at> lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From Walter.H at mathemainzel.info  Wed May 27 20:03:14 2015
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 27 May 2015 22:03:14 +0200
Subject: [squid-users] Correct Syntax for ACL?
Message-ID: <55662302.7090904@mathemainzel.info>

Hello,

would this be the correct syntax:

acl crl-file url_regex -i \.crl$

or need it to be

acl crl-file url_regex -i "\.crl$"

how does squid distinquish between a file containing rules
e.g. acl acl-file url_regex -i "/etc/url-acl.squid"
  or the rule itself
e.g.  acl acl-rule url_regex -i "\.exe$"
with the same acl-type?

Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5971 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150527/77d77780/attachment.bin>

From hussam at visp.net.lb  Wed May 27 20:11:22 2015
From: hussam at visp.net.lb (Hussam Al-Tayeb)
Date: Wed, 27 May 2015 23:11:22 +0300
Subject: [squid-users] Correct Syntax for ACL?
In-Reply-To: <55662302.7090904@mathemainzel.info>
References: <55662302.7090904@mathemainzel.info>
Message-ID: <6168778.AMGov9TQku@hades>

On Wednesday 27 May 2015 22:03:14 Walter H. wrote:
> Hello,
> 
> would this be the correct syntax:
> 
> acl crl-file url_regex -i \.crl$
> 
> or need it to be
> 
> acl crl-file url_regex -i "\.crl$"
> 
> how does squid distinquish between a file containing rules
> e.g. acl acl-file url_regex -i "/etc/url-acl.squid"
>   or the rule itself
> e.g.  acl acl-rule url_regex -i "\.exe$"
> with the same acl-type?
> 
> Thanks,
> Walter

This is what I am using right now.

acl crlfiletype rep_mime_type application/x-pkcs7-crl
store_miss deny crlfiletype
acl crl url_regex .crl$
cache deny crl



From serge.fonville at gmail.com  Wed May 27 20:18:56 2015
From: serge.fonville at gmail.com (Serge Fonville)
Date: Wed, 27 May 2015 22:18:56 +0200
Subject: [squid-users] Correct Syntax for ACL?
In-Reply-To: <6168778.AMGov9TQku@hades>
References: <55662302.7090904@mathemainzel.info> <6168778.AMGov9TQku@hades>
Message-ID: <CAOAS_+KCpWKR=eecEqx_S2z77Gzohvv31tbOErHPaPTE+PZwCg@mail.gmail.com>

Hi,

how does squid distinquish between a file containing rules
> e.g. acl acl-file url_regex -i "/etc/url-acl.squid"
>  or the rule itself
> e.g.  acl acl-rule url_regex -i "\.exe$"
> with the same acl-type?
>
>From http://www.squid-cache.org/Doc/config/acl/

Every access list definition must begin with an aclname and acltype,
	followed by either type-specific arguments or a quoted filename that
	they are read from.
Seems to suggest that using "s means the argument is a file.

HTH


Kind regards/met vriendelijke groet,

Serge Fonville

http://www.sergefonville.nl

2015-05-27 22:11 GMT+02:00 Hussam Al-Tayeb <hussam at visp.net.lb>:

> On Wednesday 27 May 2015 22:03:14 Walter H. wrote:
> > Hello,
> >
> > would this be the correct syntax:
> >
> > acl crl-file url_regex -i \.crl$
> >
> > or need it to be
> >
> > acl crl-file url_regex -i "\.crl$"
> >
> > how does squid distinquish between a file containing rules
> > e.g. acl acl-file url_regex -i "/etc/url-acl.squid"
> >   or the rule itself
> > e.g.  acl acl-rule url_regex -i "\.exe$"
> > with the same acl-type?
> >
> > Thanks,
> > Walter
>
> This is what I am using right now.
>
> acl crlfiletype rep_mime_type application/x-pkcs7-crl
> store_miss deny crlfiletype
> acl crl url_regex .crl$
> cache deny crl
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150527/1644ff30/attachment.htm>

From squid3 at treenet.co.nz  Wed May 27 23:12:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 May 2015 11:12:57 +1200
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <512309046.1394495.1432749096194.JavaMail.zimbra@2keys.ca>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
 <5565FCE1.2010909@treenet.co.nz>
 <512309046.1394495.1432749096194.JavaMail.zimbra@2keys.ca>
Message-ID: <55664F79.4070700@treenet.co.nz>

On 28/05/2015 5:51 a.m., Michael Monette wrote:
> Thanks for the reply. I am compiling the latest squid now and going to give that a shot using ssl::server_name. The precompiled binary for CentOS was only at 3.5.04 and didn't have ssl::server_name support I guess.
> 
> While I have you here, I was thinking of something else I could do. Tell me if you think it would work. You seem like an expert.
> 

I'm the Squid maintainer. On this feature I'm just an interested party,
the real expert is Christos Tsantilas who authored it.


> Can I not make it so www.gmail.com resolves to (for example) 1.2.3.4 using DNS or a hosts file, then setup an acl like this:
> 
> acl gml dst 1.2.3.4 
> # Redirect request for 1.2.3.4 to https://mail.google.com
> deny_info https://mail.google.com gml        
> # Not even sure what this does, found it on a website, didn't really question it since things were working as they were supposed to.
> http_reply_access deny bl1 

This acts on the HTTP messages inside the TLS wrapper after SSL-Bump has
decrypted the connection and the request has been sent through to the
server. By replacing whatever reply the server actually sent with a 302
status diverting the client to that URL.

It has no effect on the bumping process itself or any of the TLS layer
details. It may have an effect on HSTS since any headers the server
might send are dropped by the 302 replacement.


> 
> This way it doesn't have to monitor for a dstdomain, it watches for an IP. 

It wastes bandwidth sending a request to the server and fetching the
reply. Given that the dst-IP address is what you have from the very
first TCP packet onwards.

About the only utility it has over http_access is that its positioning
naturally restricts it to the decrypted traffic where the 302 will
always work. But by that time you have the domain name from the
encrypted HTTP message anyway.

Amos



From squid3 at treenet.co.nz  Wed May 27 23:14:57 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 May 2015 11:14:57 +1200
Subject: [squid-users] Squid, Gmail.com and HSTS.
In-Reply-To: <1178914605.1395397.1432751610933.JavaMail.zimbra@2keys.ca>
References: <412364730.1389182.1432743337627.JavaMail.zimbra@2keys.ca>
 <5565FCE1.2010909@treenet.co.nz>
 <1178914605.1395397.1432751610933.JavaMail.zimbra@2keys.ca>
Message-ID: <55664FF1.8080000@treenet.co.nz>

On 28/05/2015 6:33 a.m., Michael Monette wrote:
> Yeah I don't know what I am doing wrong but I don't have these ACL types..Or I am somehow not copy & pasting properly:
> 
> FATAL: Invalid ACL type 'ssl::server_name'
> FATAL: Bungled /etc/squid/squid.conf line 54: acl nobumpsites ssl::server_name .google.com
> Squid Cache (Version 3.5.4): Terminated abnormally.
> CPU Usage: 0.005 seconds = 0.003 user + 0.002 sys
> Maximum Resident Size: 24096 KB
> Page faults with physical i/o: 0
> Squid restarted
> [root at ottt-corp-paz-squid-1 squid-3.5.4]# squid -v
> Squid Cache: Version 3.5.4
> Service Name: squid
> configure options:  '--prefix=/usr' '--includedir=/usr/include' '--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-included-ltdl' --enable-ltdl-convenience
> 

You are missing the --with-openssl --enable-ssl-crtd options on this build.

Amos



From squid3 at treenet.co.nz  Wed May 27 23:22:04 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 May 2015 11:22:04 +1200
Subject: [squid-users] Correct Syntax for ACL?
In-Reply-To: <CAOAS_+KCpWKR=eecEqx_S2z77Gzohvv31tbOErHPaPTE+PZwCg@mail.gmail.com>
References: <55662302.7090904@mathemainzel.info> <6168778.AMGov9TQku@hades>
 <CAOAS_+KCpWKR=eecEqx_S2z77Gzohvv31tbOErHPaPTE+PZwCg@mail.gmail.com>
Message-ID: <5566519C.10608@treenet.co.nz>

On 28/05/2015 8:18 a.m., Serge Fonville wrote:
> Hi,
> 
> how does squid distinquish between a file containing rules
>> e.g. acl acl-file url_regex -i "/etc/url-acl.squid"
>>  or the rule itself
>> e.g.  acl acl-rule url_regex -i "\.exe$"
>> with the same acl-type?
>>
> From http://www.squid-cache.org/Doc/config/acl/
> 
> Every access list definition must begin with an aclname and acltype,
> 	followed by either type-specific arguments or a quoted filename that
> 	they are read from.
> Seems to suggest that using "s means the argument is a file.

Correct.

Amos



From fredbmail at free.fr  Thu May 28 08:32:51 2015
From: fredbmail at free.fr (FredB)
Date: Thu, 28 May 2015 10:32:51 +0200 (CEST)
Subject: [squid-users] Logformat tag for a specific ACL
In-Reply-To: <5565ACC0.3070102@treenet.co.nz>
Message-ID: <1008297007.593507839.1432801971561.JavaMail.root@zimbra4-e1.priv.proxad.net>


> Yes, but not to the same log file. Like this:
> 
>  access_log daemon:/var/log/squid/access.log squid !test
> 
>  access_log daemon:/var/log/squid/access_test.log fred test
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

Thank Amos,

This should be a great feature, for debugging or tag the web filtering (ports, website, mp3, etc)
Fo example, actually with many complex rules the return code 403 is not enough explicit, which rules or acl ?, and create a log file for each ACL is not a great solution (but in my case yes :) thank again )  

Fred




From h.wahl at ifw-dresden.de  Thu May 28 09:03:51 2015
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Thu, 28 May 2015 11:03:51 +0200
Subject: [squid-users] Squid 3.5.4 OpenBSD workers registration timed out
Message-ID: <5566D9F7.70304@ifw-dresden.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,
after having trouble with rock storage I try to run squid without
cache_dir but with workers enabled. I run into the same "kidx
registration timed out" trouble as before [1].

I just enabled "workers 2" which works perfectly on a Linux box but
fails on OpenBSD. Is there anything OS-specific to consider? Which
Information do you need for debugging?

In the log I find this:

May 28 10:59:22 squid02 squid[12204]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:22 squid02 squid[32101]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:22 squid02 squid[12204]: Service Name: squid
May 28 10:59:22 squid02 squid[32101]: Service Name: squid
May 28 10:59:22 squid02 squid[12204]: Process ID 12204
May 28 10:59:22 squid02 squid[32101]: Process ID 32101
May 28 10:59:22 squid02 squid[32101]: Process Roles: worker
May 28 10:59:22 squid02 squid[12204]: Process Roles: coordinator
May 28 10:59:22 squid02 squid[32101]: With 8192 file descriptors available
May 28 10:59:22 squid02 squid[12204]: With 8192 file descriptors available
May 28 10:59:22 squid02 squid[16324]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:22 squid02 squid[16324]: Service Name: squid
May 28 10:59:22 squid02 squid[16324]: Process ID 16324
May 28 10:59:22 squid02 squid[16324]: Process Roles: worker
May 28 10:59:22 squid02 squid[16324]: With 32768 file descriptors
available
May 28 10:59:22 squid02 squid[12204]: Squid plugin modules loaded: 0
May 28 10:59:22 squid02 squid[32101]: Squid plugin modules loaded: 0
May 28 10:59:22 squid02 squid[16324]: Squid plugin modules loaded: 0
May 28 10:59:28 squid02 squid[32101]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:28 squid02 squid[32101]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:28 squid02 squid[32101]: kid2 registration timed out
May 28 10:59:28 squid02 squid[16324]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:28 squid02 squid[16324]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:28 squid02 squid[16324]: kid1 registration timed out
May 28 10:59:31 squid02 squid[10011]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:31 squid02 squid[10011]: Service Name: squid
May 28 10:59:31 squid02 squid[10011]: Process ID 10011
May 28 10:59:31 squid02 squid[10011]: Process Roles: worker
May 28 10:59:31 squid02 squid[10011]: With 8192 file descriptors available
May 28 10:59:31 squid02 squid[10011]: Squid plugin modules loaded: 0
May 28 10:59:34 squid02 squid[3647]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:34 squid02 squid[3647]: Service Name: squid
May 28 10:59:34 squid02 squid[3647]: Process ID 3647
May 28 10:59:34 squid02 squid[3647]: Process Roles: worker
May 28 10:59:34 squid02 squid[3647]: With 32768 file descriptors available
May 28 10:59:34 squid02 squid[3647]: Squid plugin modules loaded: 0
May 28 10:59:37 squid02 squid[10011]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:37 squid02 squid[10011]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:37 squid02 squid[10011]: kid2 registration timed out
May 28 10:59:40 squid02 squid[3647]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:40 squid02 squid[3647]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:40 squid02 squid[3647]: kid1 registration timed out
May 28 10:59:40 squid02 squid[14886]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:40 squid02 squid[14886]: Service Name: squid
May 28 10:59:40 squid02 squid[14886]: Process ID 14886
May 28 10:59:40 squid02 squid[14886]: Process Roles: worker
May 28 10:59:40 squid02 squid[14886]: With 8192 file descriptors available
May 28 10:59:40 squid02 squid[14886]: Squid plugin modules loaded: 0
May 28 10:59:43 squid02 squid[11132]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:43 squid02 squid[11132]: Service Name: squid
May 28 10:59:43 squid02 squid[11132]: Process ID 11132
May 28 10:59:43 squid02 squid[11132]: Process Roles: worker
May 28 10:59:43 squid02 squid[11132]: With 32768 file descriptors
available
May 28 10:59:43 squid02 squid[11132]: Squid plugin modules loaded: 0
May 28 10:59:46 squid02 squid[14886]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:46 squid02 squid[14886]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:46 squid02 squid[14886]: kid2 registration timed out
May 28 10:59:49 squid02 squid[11132]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:49 squid02 squid[11132]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:49 squid02 squid[11132]: kid1 registration timed out
May 28 10:59:49 squid02 squid[2527]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:49 squid02 squid[2527]: Service Name: squid
May 28 10:59:49 squid02 squid[2527]: Process ID 2527
May 28 10:59:49 squid02 squid[2527]: Process Roles: worker
May 28 10:59:49 squid02 squid[2527]: With 8192 file descriptors available
May 28 10:59:49 squid02 squid[2527]: Squid plugin modules loaded: 0
May 28 10:59:52 squid02 squid[2943]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:52 squid02 squid[2943]: Service Name: squid
May 28 10:59:52 squid02 squid[2943]: Process ID 2943
May 28 10:59:52 squid02 squid[2943]: Process Roles: worker
May 28 10:59:52 squid02 squid[2943]: With 32768 file descriptors available
May 28 10:59:52 squid02 squid[2943]: Squid plugin modules loaded: 0
May 28 10:59:55 squid02 squid[2527]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:55 squid02 squid[2527]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:55 squid02 squid[2527]: kid2 registration timed out
May 28 10:59:58 squid02 squid[2943]: Closing HTTP port 127.0.0.1:3128
May 28 10:59:58 squid02 squid[2943]: Closing HTTP port 172.19.13.3:3128
May 28 10:59:58 squid02 squid[2943]: kid1 registration timed out
May 28 10:59:58 squid02 squid[2732]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 10:59:58 squid02 squid[2732]: Service Name: squid
May 28 10:59:58 squid02 squid[2732]: Process ID 2732
May 28 10:59:58 squid02 squid[2732]: Process Roles: worker
May 28 10:59:58 squid02 squid[2732]: With 8192 file descriptors available
May 28 10:59:58 squid02 squid[2732]: Squid plugin modules loaded: 0
May 28 11:00:01 squid02 squid[2751]: Starting Squid Cache version
3.5.4 for x86_64-unknown-openbsd5.7...
May 28 11:00:01 squid02 squid[2751]: Service Name: squid
May 28 11:00:01 squid02 squid[2751]: Process ID 2751
May 28 11:00:01 squid02 squid[2751]: Process Roles: worker
May 28 11:00:01 squid02 squid[2751]: With 32768 file descriptors available
May 28 11:00:01 squid02 squid[2751]: Squid plugin modules loaded: 0
May 28 11:00:04 squid02 squid[2732]: Closing HTTP port 127.0.0.1:3128
May 28 11:00:04 squid02 squid[2732]: Closing HTTP port 172.19.13.3:3128
May 28 11:00:04 squid02 squid[2732]: kid2 registration timed out
May 28 11:00:07 squid02 squid[2751]: Closing HTTP port 127.0.0.1:3128
May 28 11:00:07 squid02 squid[2751]: Closing HTTP port 172.19.13.3:3128
May 28 11:00:07 squid02 squid[2751]: kid1 registration timed out


Thanks for any hint and best regards

Henri


[1] http://www.spinics.net/lists/squid/msg75283.html
- -- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Dr. Doreen Kirmse
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iEYEARECAAYFAlVm2e8ACgkQnmb3Nh+6CUKcKwCffDP3xgOQUPAqTGyS8SbYOkZp
2PgAn14+8P/pC/GsPVNB9yhd0ek99Jmk
=NDCL
-----END PGP SIGNATURE-----


From jlay at slave-tothe-box.net  Thu May 28 10:22:41 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 28 May 2015 04:22:41 -0600
Subject: [squid-users] Ssl-bump deep dive (self-signed certs in chain)
In-Reply-To: <556424C1.5000200@treenet.co.nz>
References: <1432571188.3754.6.camel@JamesiMac>
 <556424C1.5000200@treenet.co.nz>
Message-ID: <1432808561.3640.9.camel@JamesiMac>

Thanks for this Amos....I will try and do more experimenting this week
with more results.

James

On Tue, 2015-05-26 at 19:46 +1200, Amos Jeffries wrote:

> On 26/05/2015 4:26 a.m., James Lay wrote:
> > So following advice and instructions on this page:
> > 
> > http://wiki.squid-cache.org/Features/DynamicSslCert
> > 
> > I have set up my lab with explicit proxy by exporting http_proxy and
> > https_proxy.  After creating the self-signed root CA certificate above
> > and creating the .der file for the client, here are my results:
> > 
> > From the squid side:
> > 2015/05/25 10:02:20.161| Using certificate
> > in /opt/etc/squid/certs/SquidCA.pem
> > 2015/05/25 10:02:20.170| support.cc(1743) readSslX509CertificatesChain:
> > Certificate is self-signed, will not be chained
> > I get the below when I don't specify a CA with curl, otherwise when I do
> > I get no error:
> > 2015/05/25 09:21:02.229| Error negotiating SSL connection on FD 12:
> > error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)
> 
> If that error is displayed by Squid about the clients connection. Then I
> believe it means the client is attempting to perform TLS authentication
> to Squid using the CA you installed there. Which is not possible as the
> CA is supposed to make the client trust Squid generated certs, not the
> other way around.
> 
> 
> > 
> > And from the client side:
> > root at kali:~/test# curl -v https://mail.slave-tothe-box.net
> > * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> > *   Trying 192.168.1.9...
> > * connected
> > * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> > * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
> >> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
> >> Host: mail.slave-tothe-box.net:443
> >> User-Agent: curl/7.26.0
> >> Proxy-Connection: Keep-Alive
> >>
> > * Easy mode waiting response from proxy CONNECT
> > < HTTP/1.1 200 Connection established
> > < 
> > * Proxy replied OK to CONNECT request
> > * successfully set certificate verify locations:
> > *   CAfile: none
> >   CApath: /etc/ssl/certs
> > * SSLv3, TLS handshake, Client hello (1):
> > * SSLv3, TLS handshake, Server hello (2):
> > * SSLv3, TLS handshake, CERT (11):
> > * SSLv3, TLS alert, Server hello (2):
> > * SSL certificate problem: self signed certificate in certificate chain
> > * Closing connection #0
> > 
> > And testing with specifying the .der file:
> > root at kali:~/test# curl --cacert /etc/ssl/certs/SquidCA.der -v
> > https://mail.slave-tothe-box.net
> > * About to connect() to proxy 192.168.1.9 port 3129 (#0)
> > *   Trying 192.168.1.9...
> > * connected
> > * Connected to 192.168.1.9 (192.168.1.9) port 3129 (#0)
> > * Establish HTTP proxy tunnel to mail.slave-tothe-box.net:443
> >> CONNECT mail.slave-tothe-box.net:443 HTTP/1.1
> >> Host: mail.slave-tothe-box.net:443
> >> User-Agent: curl/7.26.0
> >> Proxy-Connection: Keep-Alive
> >>
> > * Easy mode waiting response from proxy CONNECT
> > < HTTP/1.1 200 Connection established
> > < 
> > * Proxy replied OK to CONNECT request
> > * error setting certificate verify locations:
> >   CAfile: /etc/ssl/certs/SquidCA.der
> >   CApath: /etc/ssl/certs
> > 
> > * Closing connection #0
> > curl: (77) error setting certificate verify locations:
> >   CAfile: /etc/ssl/certs/SquidCA.der
> >   CApath: /etc/ssl/certs
> > 
> > 
> > I can confirm that the server is using a bona-fide certificate issued
> > from StartSSL and works, so at this point I'm open to suggestions.
> > Thank you.
> 
> curl is complaining that the CA chain for the Squid-generted cert has a
> self-signed CA. This is expected and desired behaviour if the
> self-signed CA was sent by Squid.
> 
> The errors only occur when the self-signed CA is not sent by Squid, but
> using the one installed on the client.
> 
> 
> For that I believe you need to configure Squid to sign/generate using
> the intermediate certificate. The self-signed root CA not configured in
> Squid at all.
> 
> Like so:
> 
> A)
>  client Trust DB installed with self-signed root CA
> 
>  squid.conf cert= configured with intermediary CA certificate
> 
>  squid.conf cafile= configured with any other intermediary CA
> certificates (in order back to root CA, but excluding it).
> 
>  Squid generates per-connection certificate
> 
> OR:
> 
> B)
>  client Trust DB installed with self-signed root CA
> 
>  squid.conf cert= configured with self-signed root CA
> 
>  Squid generates per-connection certificate
> 
> 
> Note that in (B) there is no intermediary certificate at all, and Squid
> does not emit any CA chain to the client.
> 
> It works exactly the same way as the globally trusted CA do. But they
> are contractually obliged to refuse giving out intermediary CA for
> anyones use, or loose their status as trusted CA.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150528/15a23b34/attachment.htm>

From stan.prescott at gmail.com  Thu May 28 19:47:05 2015
From: stan.prescott at gmail.com (Stanford Prescott)
Date: Thu, 28 May 2015 14:47:05 -0500
Subject: [squid-users] New server_name acl causes fatal error starting
 Squid 3.5.4
In-Reply-To: <loom.20150527T204113-559@post.gmane.org>
References: <CANLNtGSEvT_JB5Cw4A5zEx1D3N-dfoqDjfRbz+=m4EVLFXytMA@mail.gmail.com>
 <CANLNtGSSigUaKKitQv5kuPkJ+u7XFaAyheZiOLUTD5QDEY2baQ@mail.gmail.com>
 <loom.20150527T204113-559@post.gmane.org>
Message-ID: <CANLNtGTA_F1-+LHAHErLyx8V5aNjwr2HEfM55-xHtCcXscR83Q@mail.gmail.com>

I have to ask...what version of Squid are you using?

On Wed, May 27, 2015 at 1:41 PM, Mike <mmonette at 2keys.ca> wrote:

> Stanford Prescott <stan.prescott <at> gmail.com> writes:
>
> >
> >
> > Never mind. I figured the acl out. I was using someone else's
> instructions who accidentally left out the double :: ssl::server_name
> using just a single :.
>
>
> I am getting the same thing as you except I don't have the mistake you
> did. I literally copied your line into my config and it's still bombing
> out.
>
> 2015/05/27 14:38:25| FATAL: Invalid ACL type 'ssl::server_name'
> FATAL: Bungled /etc/squid/squid.conf line 52: acl nobumpSites
> ssl::server_name .wellsfargo.com
> Squid Cache (Version 3.5.4): Terminated abnormally.
> CPU Usage: 0.006 seconds = 0.002 user + 0.004 sys
> Maximum Resident Size: 24112 KB
> Page faults with physical i/o: 0
>
> I'm about to just give up on squid..losing my mind. Any ideas?
>
>
> >
> >
> > On Wed, May 20, 2015 at 12:36 PM, Stanford Prescott <stan.prescott
> <at> gmail.com> wrote:
> >
> >
> > After a diversion getting SquidClamAV working, i am back to trying to
> get peek and splice working. I am trying to put together information
> from previous recommendations I have received. Right now, I can't get
> the server_name acl working. When I put this in my squid.confacl
> nobumpSites ssl:server_name .example.com
> > I get a fatal error starting squid  using that acl saying the acl is
> "Bungled".
> > Is the form of the acl incorrect?
> >
> >
> >
> >
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users <at> lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150528/623b8991/attachment.htm>

From jlay at slave-tothe-box.net  Fri May 29 00:54:08 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 28 May 2015 18:54:08 -0600
Subject: [squid-users] Ssl-bump deep dive (testing)
Message-ID: <1432860848.3640.30.camel@JamesiMac>

So I took the advice of those here to get explicit working first, so
here's my first attempt.  My test environment is Ubuntu 15.04 Server as
the squid server with virtualbox running on it with Kali linux as the
client.  Here's my Squid 3.5.4 configure line:

/configure --prefix=/opt --enable-icap-client --with-openssl
--enable-ssl --enable-ssl-crtd --enable-linux-netfilter
--enable-follow-x-forwarded-for --with-large-files
--sysconfdir=/opt/etc/squid --enable-external-acl-helpers=none



Full squid.conf:
#####################################
acl localnet src 192.168.1.0/24

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

acl CONNECT method CONNECT

http_access allow all

sslproxy_cert_error allow all
sslproxy_cert_error deny all
sslproxy_capath /etc/ssl/certs
sslproxy_flags DONT_VERIFY_PEER 
sslproxy_options ALL

sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

http_port 3129 ssl-bump cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE

external_acl_type sni ttl=30 concurrency=10 children-max=20
children-startup=5 %ssl::>sni /opt/etc/squid/bumphelper.py

acl sni_exclusions external sni
acl tcp_level at_step SslBump1
acl client_hello_peeked at_step SslBump2

ssl_bump peek tcp_level all
ssl_bump splice client_hello_peeked sni_exclusions
ssl_bump bump all

logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh
%ssl::bump_mode %ssl::>sni %ssl::>cert_subject

access_log syslog:daemon.info mine

refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

coredump_dir /opt/var
#####################################


bumphelper.py:
#####################################
#!/usr/bin/python

import sys

while True:
    req = sys.stdin.readline()

    if not req:
        break

    id, sni = req.split()

    sys.stderr.write('request %r\n' % req)
    sys.stderr.flush()

    if sni == 'google.com':  # bypass
        sys.stdout.write('{} OK\n'.format(id))
        sys.stdout.flush()
    else:
        sys.stdout.write('{} ERR\n'.format(id))
        sys.stdout.flush()
#####################################

The tests:
root at kali:~/test# wget -d https://www.google.com
######################################
DEBUG output created by Wget 1.13.4 on linux-gnu.

URI encoding = `UTF-8'
URI encoding = `UTF-8'
--2015-05-28 17:44:31--  https://www.google.com/
Connecting to 192.168.1.6:3129... connected.
Created socket 4.
Releasing 0x092c6730 (new refcount 0).
Deleting unused 0x092c6730.

---request begin---
CONNECT www.google.com:443 HTTP/1.1
User-Agent: Wget/1.13.4 (linux-gnu)

---request end---
proxy responded with: [HTTP/1.1 200 Connection established

]

---request begin---
GET / HTTP/1.1
User-Agent: Wget/1.13.4 (linux-gnu)
Accept: */*
Host: www.google.com
Connection: Close
Proxy-Connection: Keep-Alive

---request end---
Proxy request sent, awaiting response... 
---response begin---
HTTP/1.1 503 Service Unavailable
Server: squid/3.5.4
Mime-Version: 1.0
Date: Thu, 28 May 2015 23:44:33 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3899
X-Squid-Error: ERR_SECURE_CONNECT_FAIL 32
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from analysis
Via: 1.1 analysis (squid/3.5.4)
Connection: close

---response end---
503 Service Unavailable
URI content encoding = `utf-8'
2015-05-28 17:44:32 ERROR 503: Service Unavailable.
########################################

access.log entry for the above wget:
#####################################
May 28 17:44:33 analysis squid: 192.168.1.91 - - [28/May/2015:17:44:33
-0600] "CONNECT www.google.com:443 HTTP/1.1" 200 0 TAG_NONE:HIER_DIRECT
peek www.google.com -
May 28 17:44:33 analysis squid: 192.168.1.91 - - [28/May/2015:17:44:33
-0600] "GET https://www.google.com/ HTTP/1.1" 503 4242
TAG_NONE:HIER_NONE - www.google.com -
#####################################



sudo /opt/sbin/squid -d 1 -N -f /opt/etc/squid/squid.conf
######################################
2015/05/28 17:44:33| Error negotiating SSL on FD 14:
error:00000000:lib(0):func(0):reason(0) (5/-1/32)
######################################


I see the same type of thing for apple.com and yahoo.com.  I'm assuming
this is HSTS, but I could be wrong.  MSN however works fine with the
above:
root at kali:~/test# wget -d https://www.msn.com
######################################
DEBUG output created by Wget 1.13.4 on linux-gnu.

URI encoding = `UTF-8'
URI encoding = `UTF-8'
--2015-05-28 18:24:50--  https://www.msn.com/
Connecting to 192.168.1.6:3129... connected.
Created socket 4.
Releasing 0x0a6493c0 (new refcount 0).
Deleting unused 0x0a6493c0.

---request begin---
CONNECT www.msn.com:443 HTTP/1.1
User-Agent: Wget/1.13.4 (linux-gnu)

---request end---
proxy responded with: [HTTP/1.1 200 Connection established

]

---request begin---
GET / HTTP/1.1
User-Agent: Wget/1.13.4 (linux-gnu)
Accept: */*
Host: www.msn.com
Connection: Close
Proxy-Connection: Keep-Alive

---request end---
Proxy request sent, awaiting response... 
---response begin---
HTTP/1.1 200 OK
######################################
May 28 18:24:51 analysis squid: 192.168.1.91 - - [28/May/2015:18:24:51
-0600] "CONNECT www.msn.com:443 HTTP/1.1" 200 0 TAG_NONE:HIER_DIRECT
peek www.msn.com -
May 28 18:24:52 analysis squid: 192.168.1.91 - - [28/May/2015:18:24:52
-0600] "GET https://www.msn.com/ HTTP/1.1" 200 38613
TCP_MISS:HIER_DIRECT bump www.msn.com -
######################################

I found that adding %ssl::bump_mode to logging sure helped out with
where I was at in the steps.  I also tried the new acl ssl::server_name
instead of using the external helper, but I got the same results with
google, yahoo, and apple.  Even setting ssl_bump splice all didn't work
well...it appears that yahoo, google, and apple are peek resistant.
I'll keep digging.  Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150528/e0bb9b50/attachment.htm>

From baim.lubis at gmail.com  Fri May 29 07:44:51 2015
From: baim.lubis at gmail.com (Ibrahim Lubis)
Date: Fri, 29 May 2015 14:44:51 +0700
Subject: [squid-users] It is possible using squid to cache YouTube ?
Message-ID: <CAAVkS7L=efPM4-_Y3yhgSptiYh7PLz3m8WKvaBrZrua6GNZpaA@mail.gmail.com>

Hi,

My freind asked why dont we cache youtube? In 2014 i read a website page
about how to caching youtube using squid lusca, and later in comment
section the author said the method not work anymore so I wonder it is
possible to cache YouTube using squid ? For what I know it is impossible
cause https and new google video mechanism is 2014.

Thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/e834382b/attachment.htm>

From apani at yandex.ru  Fri May 29 07:31:56 2015
From: apani at yandex.ru (sp_)
Date: Fri, 29 May 2015 00:31:56 -0700 (PDT)
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1432131388558-4671309.post@n4.nabble.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
Message-ID: <1432884716583-4671432.post@n4.nabble.com>

Hello,

does anyone have the working squid 3.5 with intercept + https? 
I've googled a lot, but seems there is no any positive experience with it.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hierony_milanisti at yahoo.co.id  Fri May 29 06:19:49 2015
From: hierony_milanisti at yahoo.co.id (Hierony Manurung)
Date: Fri, 29 May 2015 06:19:49 +0000 (UTC)
Subject: [squid-users] Problem in Stress Testing Squid Proxy
Message-ID: <1321241832.602210.1432880389223.JavaMail.yahoo@mail.yahoo.com>


Dear Fellows,
I am hitting the wall when I try Stress Testing using Apache JMeter v 2.1.3 in my Distributed Proxy Server.

For my system, i am using 1 child proxy as the load balancer and 2 parent proxy to receive
request from child proxy.

The Problem are :When I try to test with Web Browser (Request a Page), the Squid works normally (Hit, because the object has been store). But when I try stress testing by using Apache JMeter v 2.1.3 (HTTP Request testing) and test the same page the respon is not Hit.

The Access.log in Child Proxy show :"1432894029.101???? 25 192.168.88.247 TCP_MISS/404 708 GET http://tatk.del.ac.id/txt:8080/ land TIMEOUT_DEFAULT_PARENT/parent201.tatk.del.ac.id text/html"

The Access.log in Parent Proxy show :
"1432893661.084???? 68 172.30.20.200 TCP_REFRESH_UNMODIFIED/200 11934 GET http://tatk.del.ac.id/txt/ - DIRECT/172.31.20.203 text/html
1432893661.084???? 68 172.30.20.200 TCP_REFRESH_UNMODIFIED/200 11934 GET http://tatk.del.ac.id/txt/ - DIRECT/172.31.20.203 text/html"

Please help me realize what's wrong here.
Here, I attach my configuration. Just in case You need to look at it.

Thanks in advance.


?Hierony Manurung
Del Institute of Technology
Network Management
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/7c9fe59d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: childsquid.conf
Type: application/octet-stream
Size: 3961 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/7c9fe59d/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: parent201squid.conf
Type: application/octet-stream
Size: 3905 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/7c9fe59d/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: parent202squid.conf
Type: application/octet-stream
Size: 4032 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/7c9fe59d/attachment-0002.obj>

From Antony.Stone at squid.open.source.it  Fri May 29 07:56:15 2015
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 29 May 2015 09:56:15 +0200
Subject: [squid-users] It is possible using squid to cache YouTube ?
In-Reply-To: <CAAVkS7L=efPM4-_Y3yhgSptiYh7PLz3m8WKvaBrZrua6GNZpaA@mail.gmail.com>
References: <CAAVkS7L=efPM4-_Y3yhgSptiYh7PLz3m8WKvaBrZrua6GNZpaA@mail.gmail.com>
Message-ID: <201505290956.15994.Antony.Stone@squid.open.source.it>

On Friday 29 May 2015 at 09:44:51 (EU time), Ibrahim Lubis wrote:

> Hi,
> 
> My freind asked why dont we cache youtube? In 2014 i read a website page
> about how to caching youtube using squid lusca, and later in comment
> section the author said the method not work anymore so I wonder it is
> possible to cache YouTube using squid ? For what I know it is impossible
> cause https and new google video mechanism is 2014.

Please see the recent thread on this list, starting with
http://lists.squid-cache.org/pipermail/squid-users/2015-May/003717.html

http://lists.squid-cache.org/pipermail/squid-users/2015-May/003725.html in 
particular may help you in the direction of a working solution.


Regards,


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From philippe_schellemans at hotmail.com  Fri May 29 09:39:58 2015
From: philippe_schellemans at hotmail.com (Flupke)
Date: Fri, 29 May 2015 02:39:58 -0700 (PDT)
Subject: [squid-users] Squid proxy to block sites
Message-ID: <1432892398200-4671436.post@n4.nabble.com>

My squid is up and running fine with adjusted error pages. 

Now I have one problem, I download certain blacklist and create from that
block.acl files on a category. 

this part in my squid.conf 
acl badsite url_regex "/etc/squid3/blocked.acl" 
acl advertising url_regex "/var/lib/blocked/categoryAdvertising/urls" 
acl dangerous url_regex "/var/lib/blocked/categoryDangerous/urls" 
acl fishy url_regex "/var/lib/blocked/categoryFishy/urls" 
#acl porn url_regex "/var/lib/blocked/categoryPorn/urls" 
acl proxy url_regex "/var/lib/blocked/categoryProxy/urls" 
http_access deny badsite 
http_access deny advertising 
http_access deny dangerous 
http_access deny fishy 
#http_access deny porn 
http_access deny proxy 

All those files are under 1mb, one file is bigger the file of Porn is around
16mb, when loading this file, the squid service crashed. 

When I loaded this config it worked just perfect.

What can I do to walk around this issue? 

Regards, 
Flupke 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-proxy-to-block-sites-tp4671436.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From codemarauder at gmail.com  Fri May 29 10:19:18 2015
From: codemarauder at gmail.com (Nishant Sharma)
Date: Fri, 29 May 2015 15:49:18 +0530
Subject: [squid-users] Squid proxy to block sites
In-Reply-To: <1432892398200-4671436.post@n4.nabble.com>
References: <1432892398200-4671436.post@n4.nabble.com>
Message-ID: <55683D26.9070201@gmail.com>

On Friday 29 May 2015 03:09 PM, Flupke wrote:
>
> All those files are under 1mb, one file is bigger the file of Porn is around
> 16mb, when loading this file, the squid service crashed.
>
> When I loaded this config it worked just perfect.
>
> What can I do to walk around this issue?
>
Can you try to run squid in foreground with following command:

squid -NX -f /path/to/squid.conf

and see what does it say before crashing?

Regards,
Nishant


From webmaster at squidblacklist.org  Fri May 29 10:26:34 2015
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Fri, 29 May 2015 05:26:34 -0500
Subject: [squid-users] Squid proxy to block sites
In-Reply-To: <55683D26.9070201@gmail.com>
References: <1432892398200-4671436.post@n4.nabble.com>
 <55683D26.9070201@gmail.com>
Message-ID: <55683EDA.8030009@squidblacklist.org>

Here is a working conf.

---> http://www.squidblacklist.org/downloads/squid.conf.txt

And here is the worlds largest porn blacklist.  ( 23mb - 1,27x,xxx 
domains )

--> 
http://www.squidblacklist.org/downloads/squidblacklists/squid-porn.tar.gz

On 5/29/2015 5:19 AM, Nishant Sharma wrote:
> On Friday 29 May 2015 03:09 PM, Flupke wrote:
>>
>> All those files are under 1mb, one file is bigger the file of Porn is 
>> around
>> 16mb, when loading this file, the squid service crashed.
>>
>> When I loaded this config it worked just perfect.
>>
>> What can I do to walk around this issue?
>>
> Can you try to run squid in foreground with following command:
>
> squid -NX -f /path/to/squid.conf
>
> and see what does it say before crashing?
>
> Regards,
> Nishant
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Signed,

Benjamin E. Nichols
http://www.squidblacklist.org



From baim.lubis at gmail.com  Fri May 29 12:04:25 2015
From: baim.lubis at gmail.com (Ibrahim Lubis)
Date: Fri, 29 May 2015 19:04:25 +0700
Subject: [squid-users] It is possible using squid to cache YouTube ?
In-Reply-To: <201505290956.15994.Antony.Stone@squid.open.source.it>
References: <CAAVkS7L=efPM4-_Y3yhgSptiYh7PLz3m8WKvaBrZrua6GNZpaA@mail.gmail.com>
 <201505290956.15994.Antony.Stone@squid.open.source.it>
Message-ID: <CAAVkS7+Lis_Uo0g9cVDwXZL+3t7u1LzWyyr_M_Ahr1ztwVECFw@mail.gmail.com>

Hi Antony,

I consider the answer is 'it cant', even is yes it seems the road is dark
and scary.

Thx
On May 29, 2015 2:57 PM, "Antony Stone" <Antony.Stone at squid.open.source.it>
wrote:

> On Friday 29 May 2015 at 09:44:51 (EU time), Ibrahim Lubis wrote:
>
> > Hi,
> >
> > My freind asked why dont we cache youtube? In 2014 i read a website page
> > about how to caching youtube using squid lusca, and later in comment
> > section the author said the method not work anymore so I wonder it is
> > possible to cache YouTube using squid ? For what I know it is impossible
> > cause https and new google video mechanism is 2014.
>
> Please see the recent thread on this list, starting with
> http://lists.squid-cache.org/pipermail/squid-users/2015-May/003717.html
>
> http://lists.squid-cache.org/pipermail/squid-users/2015-May/003725.html in
> particular may help you in the direction of a working solution.
>
>
> Regards,
>
>
> Antony.
>
> --
> Wanted: telepath.   You know where to apply.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/b0abd5e1/attachment.htm>

From romulo at hscbrasil.com.br  Fri May 29 12:18:23 2015
From: romulo at hscbrasil.com.br (Romulo Boschetti)
Date: Fri, 29 May 2015 09:18:23 -0300 (BRT)
Subject: [squid-users] It is possible using squid to cache YouTube ?
In-Reply-To: <CAAVkS7L=efPM4-_Y3yhgSptiYh7PLz3m8WKvaBrZrua6GNZpaA@mail.gmail.com>
Message-ID: <682859035.8407.1432901903570.JavaMail.root@hscbrasil.com.br>

Helo Ibrahim, 


Yes It's possible !!! 


Please look this project https://github.com/hscbrasil/hsc-dynamic-cache 




Atenciosamente, __________________________________________________________________ 
R?mulo Giordani Boschetti 
IT Analyst - HSC Brasil 

telefone 55 (51) 3 216-7007 ? Porto Alegre 
telefone 55 (11) 3522-8191 ? S?o Paulo 

fax : 55 (51) 3 216-7001 
site: www.hscbrasil.com.br 
email: romulo at hscbrasil.com.br 
__________________________________________________________________ 

_______________________________________________ ___________________ 
----- Mensagem original -----

De: "Ibrahim Lubis" <baim.lubis at gmail.com> 
Para: squid-users at lists.squid-cache.org 
Enviadas: Sexta-feira, 29 de Maio de 2015 4:44:51 
Assunto: [squid-users] It is possible using squid to cache YouTube ? 


Hi, 
My freind asked why dont we cache youtube? In 2014 i read a website page about how to caching youtube using squid lusca, and later in comment section the author said the method not work anymore so I wonder it is possible to cache YouTube using squid ? For what I know it is impossible cause https and new google video mechanism is 2014. 
Thx 
_______________________________________________ 
squid-users mailing list 
squid-users at lists.squid-cache.org 
http://lists.squid-cache.org/listinfo/squid-users 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150529/c88d26b1/attachment.htm>

From squid at borrill.org.uk  Fri May 29 14:52:12 2015
From: squid at borrill.org.uk (Stephen Borrill)
Date: Fri, 29 May 2015 15:52:12 +0100
Subject: [squid-users] ipf transparent enabled,
	but squid says not supported
In-Reply-To: <0e20c8c87be4f23f54735747be0736ef@localhost>
References: <5565E6B4.2040201@borrill.org.uk>
 <0e20c8c87be4f23f54735747be0736ef@localhost>
Message-ID: <55687D1C.8070409@borrill.org.uk>

On 27/05/2015 16:52, James Lay wrote:
> On 2015-05-27 09:45 AM, Stephen Borrill wrote:
>> I have:
>> Squid Cache: Version 3.5.4
>> Service Name: squid
>> configure options:  '--sysconfdir=/usr/pkg/etc/squid'
>> '--localstatedir=/var/squid' '--datarootdir=/usr/pkg/share/squid'
>> '--disable-strict-error-checking' '--enable-auth'
>> '--enable-cachemgr-hostname=localhost' '--enable-delay-pools'
>> '--enable-icap-client' '--enable-icmp' '--enable-poll'
>> '--enable-removal-policies=lru,heap'
>> '--enable-storeio=ufs diskd' '--with-aio' '--with-default-user=squid'
>> '--with-pidfile=/var/run/squid.pid' '--disable-arch-native'
>> '--enable-ipf-transparent' '--enable-arp-acl' '--enable-carp'
>> '--disable-ipv6' '--without-mit-krb5' '--without-heimdal-krb5'
>> '--disable-snmp' '--enable-ssl' '--with-openssl=/usr/pkg'
>> '--enable-auth-basic=NCSA getpwnam PAM' '--enable-auth-digest=file'
>> '--disable-auth-negotiate' '--enable-auth-ntlm=fake smb_lm'
>> '--enable-external-acl-helpers=file_userip unix_group'
>> '--prefix=/usr/pkg' '--build=i486--netbsdelf'
>> '--host=i486--netbsdelf' '--mandir=/usr/pkg/man'
>> 'build_alias=i486--netbsdelf' 'host_alias=i486--netbsdelf'
>> 'CC=cc' 'CFLAGS=-O2 -I/usr/include -I/usr/pkg/include'
>> 'LDFLAGS=-L/usr/lib -Wl,-R/usr/lib -L/usr/pkg/lib -Wl,-R/usr/pkg/lib'
>> 'LIBS=' 'CPPFLAGS=-I/usr/include -I/usr/pkg/include'
>> 'CXX=c++' 'CXXFLAGS=-O2 -I/usr/include -I/usr/pkg/include'
>>
>> squid.conf contains:
>> http_port 127.0.0.1:8006 intercept name=port_8006
>>
>> Yet I see the following ev:
>> 2015/05/27 16:02:46 kid1| WARNING: transparent proxying not supported
>>
>> Same config works with earlier version of squid (3.4 and earlier).
>> What's changed?
>
> Look through your config.log...I experienced a similar thing and, upon
> running my ./configure line and watching it I saw I was missing a library.

This is down to two faults in configure:

1) If USE_SOLARIS_IPFILTER_MINOR_T_HACK is not needed, then configure 
still defines it, but with no value (i.e. confdefs.h has #define 
USE_SOLARIS_IPFILTER_MINOR_T_HACK ). All tests that use #if 
USE_SOLARIS_IPFILTER_MINOR_T_HACK will then fail.

My fix was to add squid_cv_broken_ipfilter_minor_t=0 as follows (note 
the message about netinet/ headers is also highly misleading):

         { $as_echo "$as_me:${as_lineno-$LINENO}: result: unable to make 
IPFilter work with netinet/ headers" >&5
$as_echo "unable to make IPFilter work with netinet/ headers" >&6; }
---->>        squid_cv_broken_ipfilter_minor_t=0

2) The tests for IPF headers no longer include net/if.h. Fix is to add:
#if HAVE_NET_IF_H
#include <net/if.h>
#endif

Patch to configure is:
@@ -38708,7 +38708,7 @@

          { $as_echo "$as_me:${as_lineno-$LINENO}: result: unable to 
make IPFilter work with netinet/ headers" >&5
  $as_echo "unable to make IPFilter work with netinet/ headers" >&6; }
-
+       squid_cv_broken_ipfilter_minor_t=0
  fi
  rm -f core conftest.err conftest.$ac_objext conftest.$ac_ext

@@ -38751,6 +38751,9 @@
  #if HAVE_SYS_IOCCOM_H
  #include <sys/ioccom.h>
  #endif
+#if HAVE_NET_IF_H
+#include <net/if.h>
+#endif
  #if USE_SOLARIS_IPFILTER_MINOR_T_HACK
  #undef minor_t
  #endif

-- 
Stephen



From nathan at getoffmalawn.com  Fri May 29 14:57:00 2015
From: nathan at getoffmalawn.com (Nathan Hoad)
Date: Sat, 30 May 2015 00:57:00 +1000
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <1432884716583-4671432.post@n4.nabble.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com>
 <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com>
 <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com>
 <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
Message-ID: <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>

Yes, I have it working on about a dozen deployments so far, using an
external ACL to make bumping decisions based on the SNI server name and a
few other things. No complaints from me, it Just Works.
On 29/05/2015 5:50 pm, "sp_" <apani at yandex.ru> wrote:

> Hello,
>
> does anyone have the working squid 3.5 with intercept + https?
> I've googled a lot, but seems there is no any positive experience with it.
>
>
>
> --
> View this message in context:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/d34c71ba/attachment.htm>

From jlay at slave-tothe-box.net  Fri May 29 15:14:05 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 29 May 2015 09:14:05 -0600
Subject: [squid-users] ssl_bump and SNI
In-Reply-To: <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
References: <318411425556393@web10j.yandex.ru> <54F857C3.6050200@gmail.com>
 <1432041170469-4671291.post@n4.nabble.com> <555B43C6.70101@treenet.co.nz>
 <1432110172715-4671299.post@n4.nabble.com> <555C674B.8010005@treenet.co.nz>
 <1432121619772-4671306.post@n4.nabble.com> <555C8FEB.1090805@gmail.com>
 <1432131388558-4671309.post@n4.nabble.com>
 <1432884716583-4671432.post@n4.nabble.com>
 <CAGUJm7ak7xwQpAmKXLoTK-VZUQxieECY3TDJE5z6gcuu6tgc+g@mail.gmail.com>
Message-ID: <9c4651e1184467ff80e9187bcc27981f@localhost>

On 2015-05-29 08:57 AM, Nathan Hoad wrote:
> Yes, I have it working on about a dozen deployments so far, using an
> external ACL to make bumping decisions based on the SNI server name
> and a few other things. No complaints from me, it Just Works.
> On 29/05/2015 5:50 pm, "sp_" <apani at yandex.ru> wrote:
> 
>> Hello,
>> 
>> does anyone have the working squid 3.5 with intercept + https?
>> I've googled a lot, but seems there is no any positive experience
>> with it.
>> 
>> --
>> View this message in context:
>> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
>> [1]
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users [2]
> 
> 
> Links:
> ------
> [1]
> http://squid-web-proxy-cache.1019090.n4.nabble.com/ssl-bump-and-SNI-tp4670207p4671432.html
> [2] http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Nathan,

Care to post your config and external helper?  I know I'd love to see 
concrete examples.  Thank you.

James


From h.wahl at ifw-dresden.de  Sat May 30 08:34:43 2015
From: h.wahl at ifw-dresden.de (Henri Wahl)
Date: Sat, 30 May 2015 10:34:43 +0200
Subject: [squid-users] Squid 3.5.4 OpenBSD workers registration timed out
In-Reply-To: <5566D9F7.70304@ifw-dresden.de>
References: <5566D9F7.70304@ifw-dresden.de>
Message-ID: <55697623.4050301@ifw-dresden.de>


> Thanks for any hint and best regards
> 

Is there really nobody else using this combo of OpenBSD + Squid workers?

Regards

-- 
Henri Wahl

IT Department
Leibniz-Institut fuer Festkoerper- u.
Werkstoffforschung Dresden

tel: +49 (3 51) 46 59 - 797
email: h.wahl at ifw-dresden.de
https://www.ifw-dresden.de

Nagios status monitor Nagstamon: https://nagstamon.ifw-dresden.de

DHCPv6 server dhcpy6d: https://dhcpy6d.ifw-dresden.de

S/MIME: https://nagstamon.ifw-dresden.de/pubkeys/smime.pem
PGP: https://nagstamon.ifw-dresden.de/pubkeys/pgp.asc

IFW Dresden e.V., Helmholtzstrasse 20, D-01069 Dresden
VR Dresden Nr. 1369
Vorstand: Prof. Dr. Manfred Hennecke, Dr. Doreen Kirmse

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/0023d6cd/attachment.sig>

From openarit at gmail.com  Sat May 30 13:56:16 2015
From: openarit at gmail.com (OpenAr-IT Soluciones)
Date: Sat, 30 May 2015 10:56:16 -0300
Subject: [squid-users] Squid authetication with Google Apps
In-Reply-To: <CAFK5QUExAch_-TgeJo_huKdX+oPdmYpiCFZadu6v8cig5B29jg@mail.gmail.com>
References: <CAFK5QUExAch_-TgeJo_huKdX+oPdmYpiCFZadu6v8cig5B29jg@mail.gmail.com>
Message-ID: <CAFK5QUFBuGMjvz4x+pLM=oFZT=TA73oOUhgXWdkfwAs-3UzUHg@mail.gmail.com>

Hi all, do you know a solution for this?. Please see below email.

Thanks
El 21/05/2015 19:58, "OpenAr-IT Soluciones" <openarit at gmail.com> escribi?:

> Hi All, we were asked to configure the Squid authentication to work with
> Google Apps, like Single Sign On. Do you know if there is a way to do that?
>
>
>
> Thanks in advance.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/7c9f50eb/attachment.htm>

From squid3 at treenet.co.nz  Sat May 30 06:19:03 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 May 2015 18:19:03 +1200
Subject: [squid-users] Problem in Stress Testing Squid Proxy
In-Reply-To: <1321241832.602210.1432880389223.JavaMail.yahoo@mail.yahoo.com>
References: <1321241832.602210.1432880389223.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55695657.4050605@treenet.co.nz>

On 29/05/2015 6:19 p.m., Hierony Manurung wrote:
> 
> Dear Fellows,
> I am hitting the wall when I try Stress Testing using Apache JMeter v 2.1.3 in my Distributed Proxy Server.
> 
> For my system, i am using 1 child proxy as the load balancer and 2 parent proxy to receive
> request from child proxy.
> 
> The Problem are :When I try to test with Web Browser (Request a Page), the Squid works normally (Hit, because the object has been store). But when I try stress testing by using Apache JMeter v 2.1.3 (HTTP Request testing) and test the same page the respon is not Hit.
> 
> The Access.log in Child Proxy show :"1432894029.101     25 192.168.88.247 TCP_MISS/404 708 GET http://tatk.del.ac.id/txt:8080/ land TIMEOUT_DEFAULT_PARENT/parent201.tatk.del.ac.id text/html"
> 
> The Access.log in Parent Proxy show :
> "1432893661.084     68 172.30.20.200 TCP_REFRESH_UNMODIFIED/200 11934 GET http://tatk.del.ac.id/txt/ - DIRECT/172.31.20.203 text/html
> 1432893661.084     68 172.30.20.200 TCP_REFRESH_UNMODIFIED/200 11934 GET http://tatk.del.ac.id/txt/ - DIRECT/172.31.20.203 text/html"
> 

Three things:

1) TCP_REFRESH_* is a HIT which requires revalidation.


2) these two parent proxy log records do not match the child proxy log
record.

 a) the parent log shows responses completion at a different time from
the child log.

 b) the child log shows the parent proxy not responding to a mangled URL
(see below)


3) you appear to have a URL re-writer embedding a port number into the
URL path... "http://tatk.del.ac.id/txt:8080/" or just a weird URL
requested by the client.


Amos



From Jason_Haar at trimble.com  Fri May 29 23:36:35 2015
From: Jason_Haar at trimble.com (Jason Haar)
Date: Sat, 30 May 2015 11:36:35 +1200
Subject: [squid-users] can anyone see why this ssl-bump config causes squid
	to crash?
Message-ID: <5568F803.4090801@trimble.com>

Hi there

I've got a working ssl-bump config that nevertheless causes squid-3.5.XX
(tried them all) to crash (FATAL: Received Segment Violation...dying)
every few minutes (on both Ubuntu and CentOS) - so something must be
wrong with it. Can someone see what I've done wrong? I have reported
this as a bug (http://bugs.squid-cache.org/show_bug.cgi?id=3556) but as
others appear to be working fine, I'm guessing I'm doing something
slightly differently

I have stripped my config back to only peeking and splicing - no bumping
- and yet it still crashes. If I disable the peeking, the problem goes away


http_port  3128 ssl-bump cert=/usr/local/squid/etc/squidCA.cert 
capath=/etc/ssl/certs/ generate-host-certificates=on
dynamic_cert_mem_cache_size=256MB options=ALL
https_port 3129 intercept ssl-bump
cert=/usr/local/squid/etc/squidCA.cert  capath=/etc/ssl/certs/
generate-host-certificates=on dynamic_cert_mem_cache_size=256MB options=ALL
acl SSL_https port 443
ssl_bump splice !SSL_https
acl HTTPSportButNotHTTPSsites dstdom_regex -i
"/etc/squid/acl-HTTPSportButNotHTTPSsites.txt"
acl NoSSLIntercept ssl::server_name_regex -i
"/etc/squid/acl-NoSSLIntercept.txt"
acl DiscoverCONNECTHost at_step SslBump1
acl DiscoverSNIHost at_step SslBump2
ssl_bump peek DiscoverCONNECTHost SSL_https
ssl_bump splice HTTPSportButNotHTTPSsites
ssl_bump splice NoSSLIntercept
ssl_bump splice all
sslproxy_cert_error allow HTTPSportButNotHTTPSsites
sslproxy_cert_error allow all


-- 
Cheers

Jason Haar
Corporate Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1



From codemarauder at gmail.com  Sat May 30 14:14:44 2015
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sat, 30 May 2015 19:44:44 +0530
Subject: [squid-users] Squid authetication with Google Apps
In-Reply-To: <CAFK5QUFBuGMjvz4x+pLM=oFZT=TA73oOUhgXWdkfwAs-3UzUHg@mail.gmail.com>
References: <CAFK5QUExAch_-TgeJo_huKdX+oPdmYpiCFZadu6v8cig5B29jg@mail.gmail.com>
 <CAFK5QUFBuGMjvz4x+pLM=oFZT=TA73oOUhgXWdkfwAs-3UzUHg@mail.gmail.com>
Message-ID: <EE25CE8A-E26E-4127-90C7-DAAA22AF5484@gmail.com>



Hi,

Not exactly single sign on, but I had written an IMAP auth helper which can  authenticate against google apps.

See this:
https://github.com/codemarauder/Squid-IMAPS-Auth-Helper/

It will be a Proxy-Auth with Google Apps credentials.

Regards,
Nishant

On 30 May 2015 19:26:16 GMT+05:30, OpenAr-IT Soluciones <openarit at gmail.com> wrote:
>Hi all, do you know a solution for this?. Please see below email.
>
>Thanks
>El 21/05/2015 19:58, "OpenAr-IT Soluciones" <openarit at gmail.com>
>escribi?:
>
>> Hi All, we were asked to configure the Squid authentication to work
>with
>> Google Apps, like Single Sign On. Do you know if there is a way to do
>that?
>>
>>
>>
>> Thanks in advance.
>>
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From openarit at gmail.com  Sat May 30 14:22:08 2015
From: openarit at gmail.com (OpenAr-IT Soluciones)
Date: Sat, 30 May 2015 11:22:08 -0300
Subject: [squid-users] Squid authetication with Google Apps
In-Reply-To: <EE25CE8A-E26E-4127-90C7-DAAA22AF5484@gmail.com>
References: <CAFK5QUExAch_-TgeJo_huKdX+oPdmYpiCFZadu6v8cig5B29jg@mail.gmail.com>
 <CAFK5QUFBuGMjvz4x+pLM=oFZT=TA73oOUhgXWdkfwAs-3UzUHg@mail.gmail.com>
 <EE25CE8A-E26E-4127-90C7-DAAA22AF5484@gmail.com>
Message-ID: <CAFK5QUG9eSq-omeTXip8et6OapC+9_28yFWChFT1VbdLBX1rhg@mail.gmail.com>

Thank you Nishant. As I understand in that case the user has to use the
credentials twice right?, one to authenticate in proxy and one with google
apps. It is like use openldap authentication with GADS. What our customer
needs is that the users can use the credentials only one time.

Thanks
El 30/05/2015 11:15, "Nishant Sharma" <codemarauder at gmail.com> escribi?:

>
>
> Hi,
>
> Not exactly single sign on, but I had written an IMAP auth helper which
> can  authenticate against google apps.
>
> See this:
> https://github.com/codemarauder/Squid-IMAPS-Auth-Helper/
>
> It will be a Proxy-Auth with Google Apps credentials.
>
> Regards,
> Nishant
>
> On 30 May 2015 19:26:16 GMT+05:30, OpenAr-IT Soluciones <
> openarit at gmail.com> wrote:
> >Hi all, do you know a solution for this?. Please see below email.
> >
> >Thanks
> >El 21/05/2015 19:58, "OpenAr-IT Soluciones" <openarit at gmail.com>
> >escribi?:
> >
> >> Hi All, we were asked to configure the Squid authentication to work
> >with
> >> Google Apps, like Single Sign On. Do you know if there is a way to do
> >that?
> >>
> >>
> >>
> >> Thanks in advance.
> >>
> >>
> >
> >
> >------------------------------------------------------------------------
> >
> >_______________________________________________
> >squid-users mailing list
> >squid-users at lists.squid-cache.org
> >http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Sent from my Android device with K-9 Mail. Please excuse my brevity.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/ed15c240/attachment.htm>

From codemarauder at gmail.com  Sat May 30 14:32:24 2015
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sat, 30 May 2015 20:02:24 +0530
Subject: [squid-users] Squid authetication with Google Apps
In-Reply-To: <CAFK5QUG9eSq-omeTXip8et6OapC+9_28yFWChFT1VbdLBX1rhg@mail.gmail.com>
References: <CAFK5QUExAch_-TgeJo_huKdX+oPdmYpiCFZadu6v8cig5B29jg@mail.gmail.com>
 <CAFK5QUFBuGMjvz4x+pLM=oFZT=TA73oOUhgXWdkfwAs-3UzUHg@mail.gmail.com>
 <EE25CE8A-E26E-4127-90C7-DAAA22AF5484@gmail.com>
 <CAFK5QUG9eSq-omeTXip8et6OapC+9_28yFWChFT1VbdLBX1rhg@mail.gmail.com>
Message-ID: <47BC778C-39E0-4829-8A7A-91ADF10178A3@gmail.com>



In that case you may have to look towards kerberos authentication with active directory or samba using GADS.

But even in that case, domain authentication will be picked by Internet Explorer only and not by Firefox. I am not sure about chrome though.

Proxy auth can't use cookies from google.com for any other domain. There was a long discussion with Amos on the list a couple of years back on the same topic.

Regards,
Nishant

On 30 May 2015 19:52:08 GMT+05:30, OpenAr-IT Soluciones <openarit at gmail.com> wrote:
>Thank you Nishant. As I understand in that case the user has to use the
>credentials twice right?, one to authenticate in proxy and one with
>google
>apps. It is like use openldap authentication with GADS. What our
>customer
>needs is that the users can use the credentials only one time.
>
>Thanks
>El 30/05/2015 11:15, "Nishant Sharma" <codemarauder at gmail.com>
>escribi?:
>
>>
>>
>> Hi,
>>
>> Not exactly single sign on, but I had written an IMAP auth helper
>which
>> can  authenticate against google apps.
>>
>> See this:
>> https://github.com/codemarauder/Squid-IMAPS-Auth-Helper/
>>
>> It will be a Proxy-Auth with Google Apps credentials.
>>
>> Regards,
>> Nishant
>>
>> On 30 May 2015 19:26:16 GMT+05:30, OpenAr-IT Soluciones <
>> openarit at gmail.com> wrote:
>> >Hi all, do you know a solution for this?. Please see below email.
>> >
>> >Thanks
>> >El 21/05/2015 19:58, "OpenAr-IT Soluciones" <openarit at gmail.com>
>> >escribi?:
>> >
>> >> Hi All, we were asked to configure the Squid authentication to
>work
>> >with
>> >> Google Apps, like Single Sign On. Do you know if there is a way to
>do
>> >that?
>> >>
>> >>
>> >>
>> >> Thanks in advance.
>> >>
>> >>
>> >
>> >
>>
>>------------------------------------------------------------------------
>> >
>> >_______________________________________________
>> >squid-users mailing list
>> >squid-users at lists.squid-cache.org
>> >http://lists.squid-cache.org/listinfo/squid-users
>>
>> --
>> Sent from my Android device with K-9 Mail. Please excuse my brevity.
>>

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From jlay at slave-tothe-box.net  Sat May 30 15:16:19 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 30 May 2015 09:16:19 -0600
Subject: [squid-users] Ssl-bump deep dive (sni and access control) some
	success
Message-ID: <1432998979.3683.28.camel@JamesiMac>

Config first:

####################################################
acl localnet src 192.168.1.0/24

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

acl CONNECT method CONNECT

acl step1 at_step SslBump1
acl step2 at_step SslBump2

ssl_bump peek step1 all
#https_server_names.txt has \.google\.com, \.yahoo\.com, \.msn\.com
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/https_server_names.txt"

http_access allow all

ssl_bump bump allowed_https_sites
ssl_bump terminate !allowed_https_sites

sslproxy_cert_error allow all
sslproxy_capath /etc/ssl/certs
sslproxy_flags DONT_VERIFY_PEER 
sslproxy_options ALL

sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

http_port 3128 intercept
https_port 3129 intercept ssl-bump
cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE

logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %>Hs
%<st %Ss:%Sh %ssl::bump_mode 

access_log syslog:daemon.info mine

refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

coredump_dir /opt/var
####################################################


so the above works to bump and filter out (the order of these lines
absolutely seemed to matter) if the site you go to isn't in the
allowed_https_sites acl.  The sticking point is the http_access....the
above will filter https based on the allowed_https_server_names.txt, but
completely allows ALL http, so this isn't complete yet.  Below is going
to a site in the allowed_https_sites acl:

[08:59:44 jlay at powerbook:~/test$ wget
--ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
https://www.msn.com
DEBUG output created by Wget 1.16 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-30 08:59:57--  https://www.msn.com/
Certificates loaded: 173
Resolving www.msn.com (www.msn.com)... 204.79.197.203
Caching www.msn.com => 204.79.197.203
Connecting to www.msn.com (www.msn.com)|204.79.197.203|:443...
connected.
Created socket 4.
Releasing 0x10503f98 (new refcount 1).

---request begin---
GET / HTTP/1.1
User-Agent: Wget/1.16 (linux-gnu)
Accept: */*
Host: www.msn.com
Connection: Keep-Alive

---request end---
HTTP request sent, awaiting response... 
---response begin---
HTTP/1.1 200 OK
<snip>

May 30 08:59:57 analysis squid: 192.168.1.73 - - [30/May/2015:08:59:57
-0600] "CONNECT 204.79.197.203:443 HTTP/1.1" www.msn.com 200 0
TAG_NONE:ORIGINAL_DST peek
May 30 08:59:58 analysis squid: 192.168.1.73 - - [30/May/2015:08:59:58
-0600] "GET https://www.msn.com/ HTTP/1.1" www.msn.com 200 38288
TCP_MISS:ORIGINAL_DST bump

Going to a site not in the allowed_https_sites acl:

[09:02:12 jlay at powerbook:~/test$ wget
--ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
https://www.weather.com
DEBUG output created by Wget 1.16 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-30 09:04:57--  https://www.weather.com/
Certificates loaded: 173
Resolving www.weather.com (www.weather.com)... 96.17.8.161, 96.17.8.138,
96.17.8.178, ...
Caching www.weather.com => 96.17.8.161 96.17.8.138 96.17.8.178
96.17.8.171
Connecting to www.weather.com (www.weather.com)|96.17.8.161|:443...
connected.
Created socket 4.
Releasing 0x1098c108 (new refcount 1).
GnuTLS: The TLS connection was non-properly terminated.
Closed fd 4
Unable to establish SSL connection.

May 30 09:04:57 analysis squid: 192.168.1.73 - - [30/May/2015:09:04:57
-0600] "CONNECT 96.17.8.161:443 HTTP/1.1" www.weather.com 200 0
TAG_NONE:HIER_NONE peek

However, changing http_access to http_access allow allowed_https_sites I
get:

[08:59:58 jlay at powerbook:~/test$ wget
--ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
https://www.msn.com
DEBUG output created by Wget 1.16 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-30 09:02:12--  https://www.msn.com/
Certificates loaded: 173
Resolving www.msn.com (www.msn.com)... 204.79.197.203
Caching www.msn.com => 204.79.197.203
Connecting to www.msn.com (www.msn.com)|204.79.197.203|:443...
connected.
Created socket 4.
Releasing 0x10515f98 (new refcount 1).
The certificate's owner does not match hostname ?www.msn.com?

May 30 09:02:12 analysis squid: 192.168.1.73 - - [30/May/2015:09:02:12
-0600] "CONNECT 204.79.197.203:443 HTTP/1.1" - 200 0
TCP_DENIED:HIER_NONE peek

Notice that peek did not get the SNI name per my %ssl::>sni in my
logging statement.  So as of now I have been unable to figure out how to
use access control with both http and https.  I can do one or the other,
but not both so far.  Of interest, redirects from http to https do not
appear to work

[08:37:39 jlay at powerbook:~/test$ wget www.yahoo.com
--2015-05-30 08:37:44--  http://www.yahoo.com/
Resolving www.yahoo.com (www.yahoo.com)... 206.190.36.45,
206.190.36.105, 2001:4998:c:a06::2:4008
Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.45|:80...
connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://www.yahoo.com/ [following]
--2015-05-30 08:37:44--  https://www.yahoo.com/
Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.45|:443...
connected.
ERROR: The certificate of ?www.yahoo.com? is not trusted.
ERROR: The certificate of ?www.yahoo.com? hasn't got a known issuer.

May 30 08:37:44 analysis squid: 192.168.1.73 - - [30/May/2015:08:37:44
-0600] "GET http://www.yahoo.com/ HTTP/1.1" - 301 1812
TCP_MISS:ORIGINAL_DST -
May 30 08:37:45 analysis squid: 192.168.1.73 - - [30/May/2015:08:37:45
-0600] "CONNECT 206.190.36.45:443 HTTP/1.1" www.yahoo.com 200 0
TAG_NONE:ORIGINAL_DST peek

Whereas direct does:

[08:37:45 jlay at powerbook:~/test$ wget
--ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
https://www.yahoo.com
DEBUG output created by Wget 1.16 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-30 08:38:27--  https://www.yahoo.com/
Certificates loaded: 173
Resolving www.yahoo.com (www.yahoo.com)... 206.190.36.105,
206.190.36.45, 2001:4998:c:a06::2:4008
Caching www.yahoo.com => 206.190.36.105 206.190.36.45
2001:4998:c:a06::2:4008
Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.105|:443...
connected.
Created socket 4.
Releasing 0x107800d8 (new refcount 1).

---request begin---
GET / HTTP/1.1
User-Agent: Wget/1.16 (linux-gnu)
Accept: */*
Host: www.yahoo.com
Connection: Keep-Alive

<snip>

---response end---
200 OK
cdm: 1cdm: 1cdm: 1Registered socket 4 for persistent reuse.
URI content encoding = ?utf-8?
Length: unspecified [text/html]
Saving to: ?index.html?

May 30 08:38:27 analysis squid: 192.168.1.73 - - [30/May/2015:08:38:27
-0600] "CONNECT 206.190.36.105:443 HTTP/1.1" www.yahoo.com 200 0
TAG_NONE:ORIGINAL_DST peek
May 30 08:38:28 analysis squid: 192.168.1.73 - - [30/May/2015:08:38:28
-0600] "GET https://www.yahoo.com/ HTTP/1.1" www.yahoo.com 200 325776
TCP_MISS:ORIGINAL_DST bump

I'm getting close...Amos if you're out there maybe you can shed some
light on the above.  Been at this for 5 hours now..happy Saturday!

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/5d188703/attachment.htm>

From jlay at slave-tothe-box.net  Sat May 30 16:48:45 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 30 May 2015 10:48:45 -0600
Subject: [squid-users] Conditional question
Message-ID: <1433004525.3683.31.camel@JamesiMac>

Per the docs:

#  Conditional configuration
#
#       If-statements can be used to make configuration directives
#       depend on conditions:
#
#           if <CONDITION>
#               ... regular configuration directives ...
#           [else
#               ... regular configuration directives ...]
#           endif
#
#       The else part is optional. The keywords "if", "else", and
"endif"
#       must be typed on their own lines, as if they were regular
#       configuration directives.
#
#       NOTE: An else-if condition is not supported.
#
#       These individual conditions types are supported:
#
#           true
#               Always evaluates to true.
#           false
#               Always evaluates to false.
#           <integer> = <integer>
#               Equality comparison of two integer numbers.

Anyone have any examples, documentation, heck ANYTHING that can show how
this works?  I can't seem to find a thing besides the above.  My goal is
something like the below:

if port = 80
    http_access deny all
else
    http_access allow all
endif

But nothing I'm trying as the condition expression is working.  Thank
you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/0f036b9a/attachment.htm>

From squid3 at treenet.co.nz  Sat May 30 20:45:38 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 May 2015 08:45:38 +1200
Subject: [squid-users] Conditional question
In-Reply-To: <1433004525.3683.31.camel@JamesiMac>
References: <1433004525.3683.31.camel@JamesiMac>
Message-ID: <556A2172.6050708@treenet.co.nz>

On 31/05/2015 4:48 a.m., James Lay wrote:
> Per the docs:
> 
> #  Conditional configuration
> #
> #       If-statements can be used to make configuration directives
> #       depend on conditions:
> #
> #           if <CONDITION>
> #               ... regular configuration directives ...
> #           [else
> #               ... regular configuration directives ...]
> #           endif
> #
> #       The else part is optional. The keywords "if", "else", and
> "endif"
> #       must be typed on their own lines, as if they were regular
> #       configuration directives.
> #
> #       NOTE: An else-if condition is not supported.
> #
> #       These individual conditions types are supported:
> #
> #           true
> #               Always evaluates to true.
> #           false
> #               Always evaluates to false.
> #           <integer> = <integer>
> #               Equality comparison of two integer numbers.
> 
> Anyone have any examples, documentation, heck ANYTHING that can show how
> this works?  I can't seem to find a thing besides the above.

Those are for process controls (SMP, named services, etc).

>  My goal is
> something like the below:
> 
> if port = 80
>     http_access deny all
> else
>     http_access allow all
> endif
> 
> But nothing I'm trying as the condition expression is working.  Thank
> you.

The default Squid configuration should "just work"...

  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_Ports
  ...
  # this one permits the CONNECT *:443 requests to get bumped
  http_access allow localnet
  ..
  http_access deny all

If you are using any other access controls on your client traffic you
need to keep in mind that Squid is dealing with "CONNECT raw-IP:443 ..."
requests in http_access / adapted_http_access / url_rewrite_access /
adaptation_access / ssl_bump prior to bumping them.

Amos


From jpotter833 at because.org.uk  Sat May 30 21:26:54 2015
From: jpotter833 at because.org.uk (Mr J Potter)
Date: Sat, 30 May 2015 22:26:54 +0100
Subject: [squid-users] url_rewrite_extras - not getting data excepted
In-Reply-To: <CAJgbPpdSRo-d1MhFTbWZR6rMNE_YpGPtdgAuANuZ4ebvqWuHXA@mail.gmail.com>
References: <CAJgbPpd0ftCTkjxfGn_Ss=jhYXY-8K5ZQ9_1GALC4T+JE0VXsQ@mail.gmail.com>
 <555EB047.5010102@treenet.co.nz>
 <CAJgbPpffHK3fuMnBM+hMfP-vYoQQWKaRPQdBQBEmUBNSu8J-XQ@mail.gmail.com>
 <555F2B9C.90109@treenet.co.nz>
 <CAJgbPpfxTfghie8+eQmn9xs3RNT_8vOzEfkxDRoKw6JWh5Fa+Q@mail.gmail.com>
 <CAJgbPpdSRo-d1MhFTbWZR6rMNE_YpGPtdgAuANuZ4ebvqWuHXA@mail.gmail.com>
Message-ID: <CAMyAEHtEoTdV=goYWDz8tnVQB0huKdq8sqsruyNRhmL-_f7xJw@mail.gmail.com>

Hi Patrick,

I'm seeing the same problem as you have here - I'm injecting usernames via
ext_sql_session_acl and squidguard (via url rewrite) is returning urls as
if no user is passed to it.

thanks,

Jim Potter
network manager,
BEC
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/d3d28c2f/attachment.htm>

From jlay at slave-tothe-box.net  Sat May 30 22:04:25 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 30 May 2015 16:04:25 -0600
Subject: [squid-users] Conditional question
In-Reply-To: <556A2172.6050708@treenet.co.nz>
References: <1433004525.3683.31.camel@JamesiMac>
 <556A2172.6050708@treenet.co.nz>
Message-ID: <1433023465.3683.33.camel@JamesiMac>

On Sun, 2015-05-31 at 08:45 +1200, Amos Jeffries wrote:

> On 31/05/2015 4:48 a.m., James Lay wrote:
> > Per the docs:
> > 
> > #  Conditional configuration
> > #
> > #       If-statements can be used to make configuration directives
> > #       depend on conditions:
> > #
> > #           if <CONDITION>
> > #               ... regular configuration directives ...
> > #           [else
> > #               ... regular configuration directives ...]
> > #           endif
> > #
> > #       The else part is optional. The keywords "if", "else", and
> > "endif"
> > #       must be typed on their own lines, as if they were regular
> > #       configuration directives.
> > #
> > #       NOTE: An else-if condition is not supported.
> > #
> > #       These individual conditions types are supported:
> > #
> > #           true
> > #               Always evaluates to true.
> > #           false
> > #               Always evaluates to false.
> > #           <integer> = <integer>
> > #               Equality comparison of two integer numbers.
> > 
> > Anyone have any examples, documentation, heck ANYTHING that can show how
> > this works?  I can't seem to find a thing besides the above.
> 
> Those are for process controls (SMP, named services, etc).
> 
> >  My goal is
> > something like the below:
> > 
> > if port = 80
> >     http_access deny all
> > else
> >     http_access allow all
> > endif
> > 
> > But nothing I'm trying as the condition expression is working.  Thank
> > you.
> 
> The default Squid configuration should "just work"...
> 
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_Ports
>   ...
>   # this one permits the CONNECT *:443 requests to get bumped
>   http_access allow localnet
>   ..
>   http_access deny all
> 
> If you are using any other access controls on your client traffic you
> need to keep in mind that Squid is dealing with "CONNECT raw-IP:443 ..."
> requests in http_access / adapted_http_access / url_rewrite_access /
> adaptation_access / ssl_bump prior to bumping them.
> 
> Amos
> _______________________________________________


Thanks Amos....in starting from scratch I completely neglected to even
allow localnets...yugh!  Continuing on and will post my final results.

James

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/f68bcfb8/attachment.htm>

From jlay at slave-tothe-box.net  Sat May 30 22:24:00 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 30 May 2015 16:24:00 -0600
Subject: [squid-users] Conditional question
In-Reply-To: <556A2172.6050708@treenet.co.nz>
References: <1433004525.3683.31.camel@JamesiMac>
 <556A2172.6050708@treenet.co.nz>
Message-ID: <1433024640.3683.43.camel@JamesiMac>

On Sun, 2015-05-31 at 08:45 +1200, Amos Jeffries wrote:

> On 31/05/2015 4:48 a.m., James Lay wrote:
> > Per the docs:
> > 
> > #  Conditional configuration
> > #
> > #       If-statements can be used to make configuration directives
> > #       depend on conditions:
> > #
> > #           if <CONDITION>
> > #               ... regular configuration directives ...
> > #           [else
> > #               ... regular configuration directives ...]
> > #           endif
> > #
> > #       The else part is optional. The keywords "if", "else", and
> > "endif"
> > #       must be typed on their own lines, as if they were regular
> > #       configuration directives.
> > #
> > #       NOTE: An else-if condition is not supported.
> > #
> > #       These individual conditions types are supported:
> > #
> > #           true
> > #               Always evaluates to true.
> > #           false
> > #               Always evaluates to false.
> > #           <integer> = <integer>
> > #               Equality comparison of two integer numbers.
> > 
> > Anyone have any examples, documentation, heck ANYTHING that can show how
> > this works?  I can't seem to find a thing besides the above.
> 
> Those are for process controls (SMP, named services, etc).
> 
> >  My goal is
> > something like the below:
> > 
> > if port = 80
> >     http_access deny all
> > else
> >     http_access allow all
> > endif
> > 
> > But nothing I'm trying as the condition expression is working.  Thank
> > you.
> 
> The default Squid configuration should "just work"...
> 
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_Ports
>   ...
>   # this one permits the CONNECT *:443 requests to get bumped
>   http_access allow localnet
>   ..
>   http_access deny all
> 
> If you are using any other access controls on your client traffic you
> need to keep in mind that Squid is dealing with "CONNECT raw-IP:443 ..."
> requests in http_access / adapted_http_access / url_rewrite_access /
> adaptation_access / ssl_bump prior to bumping them.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Hi again Amos,

So...my method of access control might be weird.  I have a regex list of
sites that work fine via http (say \.acer\.com).  So, I allow access to
this list via:

acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt
http_access allow allowed_http_sites
http_access deny !allowed_http_sites

This works well for allowing access to the list of sites....the lack of
http_access allow localnet makes this happen.  With the above however,
ssl_bumping stops working as I get:

[16:18:22 jlay at powerbook:~/test$ wget
--ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
https://www.msn.com
DEBUG output created by Wget 1.16 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-30 16:19:46--  https://www.msn.com/
Certificates loaded: 173
Resolving www.msn.com (www.msn.com)... 204.79.197.203
Caching www.msn.com => 204.79.197.203
Connecting to www.msn.com (www.msn.com)|204.79.197.203|:443...
connected.
Created socket 4.
Releasing 0x10c3ef98 (new refcount 1).
The certificate's owner does not match hostname ?www.msn.com?

May 30 16:19:46 analysis squid: 192.168.1.73 - - [30/May/2015:16:19:46
-0600] "CONNECT 204.79.197.203:443 HTTP/1.1" - 200 0
TCP_DENIED:HIER_NONE peek

Adding http_access alllow localnet makes ssl_bumping work correctly, but
then the http_access deny !allowed_http_sites does not work.  I'm having
a hard time getting both http and https filtering to play well together
with one instance of squid.  I'd like to try and just go with one, but
if I have to I'll go with two.  Anyway thanks again for looking...I hope
I'm explaining this well.

James

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/50599eda/attachment.htm>

From jlay at slave-tothe-box.net  Sat May 30 22:39:47 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 30 May 2015 16:39:47 -0600
Subject: [squid-users] Conditional question
In-Reply-To: <1433024640.3683.43.camel@JamesiMac>
References: <1433004525.3683.31.camel@JamesiMac>
 <556A2172.6050708@treenet.co.nz> <1433024640.3683.43.camel@JamesiMac>
Message-ID: <1433025587.3683.45.camel@JamesiMac>

On Sat, 2015-05-30 at 16:24 -0600, James Lay wrote:

> On Sun, 2015-05-31 at 08:45 +1200, Amos Jeffries wrote: 
> 
> > On 31/05/2015 4:48 a.m., James Lay wrote:
> > > Per the docs:
> > > 
> > > #  Conditional configuration
> > > #
> > > #       If-statements can be used to make configuration directives
> > > #       depend on conditions:
> > > #
> > > #           if <CONDITION>
> > > #               ... regular configuration directives ...
> > > #           [else
> > > #               ... regular configuration directives ...]
> > > #           endif
> > > #
> > > #       The else part is optional. The keywords "if", "else", and
> > > "endif"
> > > #       must be typed on their own lines, as if they were regular
> > > #       configuration directives.
> > > #
> > > #       NOTE: An else-if condition is not supported.
> > > #
> > > #       These individual conditions types are supported:
> > > #
> > > #           true
> > > #               Always evaluates to true.
> > > #           false
> > > #               Always evaluates to false.
> > > #           <integer> = <integer>
> > > #               Equality comparison of two integer numbers.
> > > 
> > > Anyone have any examples, documentation, heck ANYTHING that can show how
> > > this works?  I can't seem to find a thing besides the above.
> > 
> > Those are for process controls (SMP, named services, etc).
> > 
> > >  My goal is
> > > something like the below:
> > > 
> > > if port = 80
> > >     http_access deny all
> > > else
> > >     http_access allow all
> > > endif
> > > 
> > > But nothing I'm trying as the condition expression is working.  Thank
> > > you.
> > 
> > The default Squid configuration should "just work"...
> > 
> >   http_access deny !Safe_ports
> >   http_access deny CONNECT !SSL_Ports
> >   ...
> >   # this one permits the CONNECT *:443 requests to get bumped
> >   http_access allow localnet
> >   ..
> >   http_access deny all
> > 
> > If you are using any other access controls on your client traffic you
> > need to keep in mind that Squid is dealing with "CONNECT raw-IP:443 ..."
> > requests in http_access / adapted_http_access / url_rewrite_access /
> > adaptation_access / ssl_bump prior to bumping them.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> Hi again Amos,
> 
> So...my method of access control might be weird.  I have a regex list
> of sites that work fine via http (say \.acer\.com).  So, I allow
> access to this list via:
> 
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt
> http_access allow allowed_http_sites
> http_access deny !allowed_http_sites
> 
> This works well for allowing access to the list of sites....the lack
> of http_access allow localnet makes this happen.  With the above
> however, ssl_bumping stops working as I get:
> 
> [16:18:22 jlay at powerbook:~/test$ wget
> --ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
> https://www.msn.com
> DEBUG output created by Wget 1.16 on linux-gnu.
> 
> URI encoding = ?UTF-8?
> --2015-05-30 16:19:46--  https://www.msn.com/
> Certificates loaded: 173
> Resolving www.msn.com (www.msn.com)... 204.79.197.203
> Caching www.msn.com => 204.79.197.203
> Connecting to www.msn.com (www.msn.com)|204.79.197.203|:443...
> connected.
> Created socket 4.
> Releasing 0x10c3ef98 (new refcount 1).
> The certificate's owner does not match hostname ?www.msn.com?
> 
> May 30 16:19:46 analysis squid: 192.168.1.73 - - [30/May/2015:16:19:46
> -0600] "CONNECT 204.79.197.203:443 HTTP/1.1" - 200 0
> TCP_DENIED:HIER_NONE peek
> 
> Adding http_access alllow localnet makes ssl_bumping work correctly,
> but then the http_access deny !allowed_http_sites does not work.  I'm
> having a hard time getting both http and https filtering to play well
> together with one instance of squid.  I'd like to try and just go with
> one, but if I have to I'll go with two.  Anyway thanks again for
> looking...I hope I'm explaining this well.
> 
> James
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Ok I think I got it...added:

acl allow_https port 443
...
http_access allow allow_https

Now my clients are allowed full port 443 access, which gets a decision
of allow or block later on, and this also allows my "usual" http access
list....woo hoo!  I'll post the full info later.  Thanks so much.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150530/fe5b8522/attachment.htm>

From marcel at guineanet.net  Sun May 31 11:38:34 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Sun, 31 May 2015 04:38:34 -0700 (PDT)
Subject: [squid-users] TOS squid-3.5.0.4
Message-ID: <1433072314426-4671459.post@n4.nabble.com>

Hi All
let see if some of you can help me troubleshoot the issue I have with
squid-3.5.0.4
on centos 6.6 configure with tproxy
in fact the issue is relate to qos stuff  i just set things according to
manual

qos_flows tos local-hit=0x30
qos_flows mark local-hit=0x30
qos_flows tos sibling-hit=0x31
qos_flows mark sibling-hit=0x31
qos_flows tos parent-hit=0x32
qos_flows mark parent-hit=0x32
qos_flows tos disable-preserve-miss
tcpdump output

tcpdump -vni eth1 | grep 'tos 0x30'

tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 65535
bytes

01:37:24.787867 IP (tos 0x30, ttl 64, id 38723, offset 0, flags [DF], proto
TCP (6), length 534)

01:37:24.788003 IP (tos 0x30, ttl 64, id 38724, offset 0, flags [DF], proto
TCP (6), length 2920)

01:37:24.788019 IP (tos 0x30, ttl 64, id 38726, offset 0, flags [DF], proto
TCP (6), length 1256)
01:37:24.788141 IP (tos 0x30, ttl 64, id 38727, offset 0, flags [DF], proto
TCP (6), length 2920)

but for sure it's not marking anything while send traffic to my pppoe BRAS
(MK)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun May 31 23:45:59 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 01 Jun 2015 11:45:59 +1200
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <1433072314426-4671459.post@n4.nabble.com>
References: <1433072314426-4671459.post@n4.nabble.com>
Message-ID: <556B9D37.7050802@treenet.co.nz>

On 31/05/2015 11:38 p.m., Marcel Fossua wrote:
> Hi All
> let see if some of you can help me troubleshoot the issue I have with
> squid-3.5.0.4

Please upgrade. 3.5.5 was released the other day, and Eliezer has new
CentOS 6/7 packages available already.

> on centos 6.6 configure with tproxy
> in fact the issue is relate to qos stuff  i just set things according to
> manual
> 
> qos_flows tos local-hit=0x30
> qos_flows mark local-hit=0x30
> qos_flows tos sibling-hit=0x31
> qos_flows mark sibling-hit=0x31
> qos_flows tos parent-hit=0x32
> qos_flows mark parent-hit=0x32

As the manual states only the TOS values in multiples of 4 are available
for use. The final two bits in the TOS field are used by the TCP ECN
feature. You should be seeing log warnings as Squid masks away those 0x1
/ 0x2 bits to produce a valid DiffServ value.

Amos



From marcel at guineanet.net  Sun May 31 23:35:08 2015
From: marcel at guineanet.net (Marcel Fossua)
Date: Sun, 31 May 2015 16:35:08 -0700 (PDT)
Subject: [squid-users] TOS squid-3.5.0.4
In-Reply-To: <556B9D37.7050802@treenet.co.nz>
References: <1433072314426-4671459.post@n4.nabble.com>
 <556B9D37.7050802@treenet.co.nz>
Message-ID: <1433115308548-4671461.post@n4.nabble.com>

HI Amos thanks for your reply I just upgrade to 3.5.5 but compiling from
source to get --enable-ecap
but I can't figure out what you means exactly concerning the TOS part
did you means what I set is ok or not?
qos_flows tos
qos_flows local-hit=0x30
qos_flows parent-hit=0x32
qos_flows disable-preserve-miss

Bests Rgds



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TOS-squid-3-5-0-4-tp4671459p4671461.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Sun May 31 23:55:41 2015
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 01 Jun 2015 11:55:41 +1200
Subject: [squid-users] Conditional question
In-Reply-To: <1433024640.3683.43.camel@JamesiMac>
References: <1433004525.3683.31.camel@JamesiMac>
 <556A2172.6050708@treenet.co.nz> <1433024640.3683.43.camel@JamesiMac>
Message-ID: <556B9F7D.30006@treenet.co.nz>

On 31/05/2015 10:24 a.m., James Lay wrote:
> On Sun, 2015-05-31 at 08:45 +1200, Amos Jeffries wrote:
> 
>> On 31/05/2015 4:48 a.m., James Lay wrote:
>>> Per the docs:
>>>
>>> #  Conditional configuration
>>> #
>>> #       If-statements can be used to make configuration directives
>>> #       depend on conditions:
>>> #
>>> #           if <CONDITION>
>>> #               ... regular configuration directives ...
>>> #           [else
>>> #               ... regular configuration directives ...]
>>> #           endif
>>> #
>>> #       The else part is optional. The keywords "if", "else", and
>>> "endif"
>>> #       must be typed on their own lines, as if they were regular
>>> #       configuration directives.
>>> #
>>> #       NOTE: An else-if condition is not supported.
>>> #
>>> #       These individual conditions types are supported:
>>> #
>>> #           true
>>> #               Always evaluates to true.
>>> #           false
>>> #               Always evaluates to false.
>>> #           <integer> = <integer>
>>> #               Equality comparison of two integer numbers.
>>>
>>> Anyone have any examples, documentation, heck ANYTHING that can show how
>>> this works?  I can't seem to find a thing besides the above.
>>
>> Those are for process controls (SMP, named services, etc).
>>
>>>  My goal is
>>> something like the below:
>>>
>>> if port = 80
>>>     http_access deny all
>>> else
>>>     http_access allow all
>>> endif
>>>
>>> But nothing I'm trying as the condition expression is working.  Thank
>>> you.
>>
>> The default Squid configuration should "just work"...
>>
>>   http_access deny !Safe_ports
>>   http_access deny CONNECT !SSL_Ports
>>   ...
>>   # this one permits the CONNECT *:443 requests to get bumped
>>   http_access allow localnet
>>   ..
>>   http_access deny all
>>
>> If you are using any other access controls on your client traffic you
>> need to keep in mind that Squid is dealing with "CONNECT raw-IP:443 ..."
>> requests in http_access / adapted_http_access / url_rewrite_access /
>> adaptation_access / ssl_bump prior to bumping them.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> Hi again Amos,
> 
> So...my method of access control might be weird.  I have a regex list of
> sites that work fine via http (say \.acer\.com).  So, I allow access to
> this list via:
> 
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt
> http_access allow allowed_http_sites
> http_access deny !allowed_http_sites

By using url_regex to match domain names you are preventing any chance
for Squid to perfom reverse-DNS lookup on the raw-IP CONNECT requests
and see if the rDNS site name matches an entry in your list.

If you made that ACL a dstdomain (which is the right type to be matching
domains with) you would see this rDNS behaviour and only have to add
domain entries for sites where the rDNS is different from the popular
domain names.

> 
> This works well for allowing access to the list of sites....the lack of
> http_access allow localnet makes this happen.  With the above however,
> ssl_bumping stops working as I get:
> 
> [16:18:22 jlay at powerbook:~/test$ wget
> --ca-certificate=/etc/ssl/certs/sslsplit_ca_cert.pem -d
> https://www.msn.com
> DEBUG output created by Wget 1.16 on linux-gnu.
> 
> URI encoding = ?UTF-8?
> --2015-05-30 16:19:46--  https://www.msn.com/
> Certificates loaded: 173
> Resolving www.msn.com (www.msn.com)... 204.79.197.203
> Caching www.msn.com => 204.79.197.203
> Connecting to www.msn.com (www.msn.com)|204.79.197.203|:443...
> connected.
> Created socket 4.
> Releasing 0x10c3ef98 (new refcount 1).
> The certificate's owner does not match hostname ?www.msn.com?
> 
> May 30 16:19:46 analysis squid: 192.168.1.73 - - [30/May/2015:16:19:46
> -0600] "CONNECT 204.79.197.203:443 HTTP/1.1" - 200 0
> TCP_DENIED:HIER_NONE peek
> 
> Adding http_access alllow localnet makes ssl_bumping work correctly, but
> then the http_access deny !allowed_http_sites does not work.  I'm having
> a hard time getting both http and https filtering to play well together
> with one instance of squid.  I'd like to try and just go with one, but
> if I have to I'll go with two.  Anyway thanks again for looking...I hope
> I'm explaining this well.

The above mentioned solution, OR as you found allowing all port 443
traffic through to at least the bumping stage will do it.

Amos


From jlay at slave-tothe-box.net  Sun May 31 23:56:12 2015
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 31 May 2015 17:56:12 -0600
Subject: [squid-users] Ssl-bump deep dive (intercept last post and final
	thoughts)
Message-ID: <1433116572.3710.63.camel@JamesiMac>

So this has been REALLY good!  The tl;dr:  ssl-bumping is pretty easy
even with intercept, ssl-bumping with access control is a little more
difficult...jump to the config to skip the chit chat.

My goal has always been to a content filter based on url regex.  This
works just fine for http traffic, but is much more difficult for https
traffic just for the case of you may or may not know the host you're
going to, depending on the site/app.  I'll be real honest here....I'm
only doing this to protect/filter the traffic of two kids, on laptops,
iPhone, and Android phone, so it's a mixed bag of content and, since
it's just the two of them in a home environment, I get to play around
and see what works and what doesn't.

Below is a close as I can get transparent intercept ssl-bump with
content filtering with using a list of domains/urls with both http and
https.  I still have to use a list of broken sites, which are large
netblocks (17.0.0.0/8..Apple anyone?) because some of these I just can't
seem to get host/domain information during the ssl handshake.  As I
discovered after attempting to put this into "production", I have not
been able to emulate using wget or curl an https session that doesn't
have any SNI information, so that threw me for a loop.  TextNow is a
great example (I'm including a packet capture of this in this post).
There's no host information in the client hello....there's no host
information in the server hello.....buried deep in the certificate ONLY
is the "commonName=.*textnow.me"...that's it.  This dashed my hopes of
using an url_regex for access control with all https sessions.  I have
"%ssl::>cert_subject" in my logging, and I never did see this log in any
of my tests...and I tested a BUNCH of different peek/stare/splice/bump
cominations..so I don't think squid is actually seeing this from the
certificate.

Another challenge is getting http url_regex filtering to work with https
filtering.  My method of filtering means not having an "http_access
allow localnet", which directly conflicted with also trying to filter
https.  The solution was to add an acl for port 443, then http_access to
just allow it, as our filtering was going to happen for https further
down.

I know there's a fair amount of people who just want to plop in some
config files, run a few commands, and be up and running.  The below
configuration has two additional files it references, http_url.txt,
which is an a list of domains/urls (\.apple\.com for example), and the
aptly named broken, which is a IP list (17.0.0.0/8).  The broken list
should be (semi) trusted and are sites that we just can't get SNI or
hostname information from.  If you've created a single cert/key pair
from the Squid documentation, you won't need the key= line in your
https_port directive.  If you've followed along in my posts, you already
have the configure line from my previous posts.  Change the
commands/config to fir where your squid config and ssl_db are.  So after
configuring, make sure you:

sudo /opt/libexec/ssl_crtd -c -s /opt/var/ssl_db
sudo chown -R nobody /opt/var/ssl_db/

As I believe in a lot of logging, and actually looking at said logging,
below is what you can expect to see in your logs (mine logs to syslog,
again, change this if you log to a different file):

Allowed http to .apple.com in http_url.txt:
May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
[31/May/2015:17:03:48 -0600] "GET
http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag? HTTP/1.1" - -
200 5243 TCP_MISS:ORIGINAL_DST -
Denied http to symcb.com not in http_url.txt
May 31 17:03:48 gateway (squid-1): 192.168.1.100 - -
[31/May/2015:17:03:48 -0600] "GET http://sd.symcb.com/sd.crt HTTP/1.1" -
- 403 3618 TCP_DENIED:HIER_NONE -
Spliced https IP in broken.txt (google block 216.58.192.0/19)
May 31 17:04:34 gateway (squid-1): 192.168.1.101 - -
[31/May/2015:17:04:34 -0600] "CONNECT 216.58.216.138:443 HTTP/1.1" - -
200 568 TCP_TUNNEL:ORIGINAL_DST peek
Spliced https IP in broken.txt that we got SNI or bumped site in
http_url.txt look exactly the same
May 31 17:09:45 gateway (squid-1): 192.168.1.100 - -
[31/May/2015:17:09:45 -0600] "CONNECT 23.222.157.21:443 HTTP/1.1"
init.itunes.apple.com - 200 30314 TCP_TUNNEL:ORIGINAL_DST peek

The only drag with the configuration is you won't see when an https
session is terminated when the IP/url is not in the broken.txt, or the
http_url.txt:

[17:20:53 jlay at analysis:~$] wget -d
--ca-certificate=/etc/ssl/certs/sslsplit.crt https://www.yahoo.com
Setting --ca-certificate (cacertificate) to /etc/ssl/certs/sslsplit.crt
DEBUG output created by Wget 1.16.1 on linux-gnu.

URI encoding = ?UTF-8?
--2015-05-31 17:20:59--  https://www.yahoo.com/
Resolving www.yahoo.com (www.yahoo.com)... 206.190.36.45,
206.190.36.105, 2001:4998:c:a06::2:4008
Caching www.yahoo.com => 206.190.36.45 206.190.36.105
2001:4998:c:a06::2:4008
Connecting to www.yahoo.com (www.yahoo.com)|206.190.36.45|:443...
connected.
Created socket 3.
Releasing 0x00007fdf67eecdd0 (new refcount 1).
Initiating SSL handshake.
SSL handshake failed.
Closed fd 3
Unable to establish SSL connection.

May 31 17:20:59 gateway (squid-1): 192.168.1.6 - - [31/May/2015:17:20:59
-0600] "CONNECT 206.190.36.45:443 HTTP/1.1" www.yahoo.com - 200 0
TAG_NONE:ORIGINAL_DST peek 

Full config below:
####################################
acl localnet src 192.168.1.0/24

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

acl CONNECT method CONNECT

acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
acl allow_https port 443
acl broken dst "/opt/etc/squid/broken.txt"

http_access deny !Safe_ports
http_access deny CONNECT !SSL_Ports

http_access allow allow_https
http_access allow allowed_http_sites
http_access deny !allowed_http_sites

http_access deny all

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 broken
ssl_bump peek step2 broken
ssl_bump splice broken
ssl_bump peek step1 all
ssl_bump peek step2 all
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/http_url.txt"
ssl_bump bump allowed_https_sites
ssl_bump terminate !allowed_https_sites

sslproxy_cert_error allow all
sslproxy_capath /etc/ssl/certs
sslproxy_flags DONT_VERIFY_PEER 
sslproxy_options ALL

sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

http_port 3128 intercept
https_port 3129 intercept ssl-bump
cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE

logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni %
ssl::>cert_subject %>Hs %<st %Ss:%Sh %ssl::bump_mode 

access_log syslog:daemon.info mine

refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

coredump_dir /opt/var
##############################

Thanks all for being patient while I continued to post my learning and
all my mistakes.  If there's anything that I've missed, or if there's
another method for trying to accomplish what I've tried to do I'm all
eyes.

James

P.S. Things I'd love to see in Squid some day:

acl's being AND'd (http_access allow allowed_sites AND localnet)
Full on separate http_access, https_access directives


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150531/6abddb21/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: textnow.pcapng
Type: application/octet-stream
Size: 7736 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150531/6abddb21/attachment.obj>

From marcel at guineanet.net  Sun May 31 11:24:41 2015
From: marcel at guineanet.net (Marcel)
Date: Sun, 31 May 2015 12:24:41 +0100
Subject: [squid-users] Fwd: TOS squid-3.5.0.4
In-Reply-To: <5568D233.8050900@guineanet.net>
References: <5568D233.8050900@guineanet.net>
Message-ID: <556AEF79.4050404@guineanet.net>

Hi All
let see if some of you can help me troubleshoot the issue I have with 
squid-3.5.0.4
on centos 6.6 configure with tproxy
in fact the issue is relate to qos stuff  i just set things according to 
manual
*
qos_flows tos local-hit=0x30
qos_flows mark local-hit=0x30
qos_flows tos sibling-hit=0x31
qos_flows mark sibling-hit=0x31
qos_flows tos parent-hit=0x32
qos_flows mark parent-hit=0x32
qos_flows tos disable-preserve-miss*
tcpdump output

*tcpdump -vni eth1 | grep 'tos 0x30'*

*tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 
65535 bytes***

*01:37:24.787867 IP (tos 0x30, ttl 64, id 38723, offset 0, flags [DF], 
proto TCP (6), length 534)*

*01:37:24.788003 IP (tos 0x30, ttl 64, id 38724, offset 0, flags [DF], 
proto TCP (6), length 2920)*

*01:37:24.788019 IP (tos 0x30, ttl 64, id 38726, offset 0, flags [DF], 
proto TCP (6), length 1256)*

*01:37:24.788141 IP (tos 0x30, ttl 64, id 38727, offset 0, flags [DF], 
proto TCP (6), length 2920)*

but for sure it's not marking anything while send traffic to my pppoe 
BRAS (MK)

any trick to make me solve this will be higly appreciate
Bests Rgds
-- 
Fossua-vcard

	 Marcel Fossua
Unix/Linux Network Administrator










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20150531/bbd4c2b6/attachment.htm>

